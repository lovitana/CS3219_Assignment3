<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.995219">
Three Dependency-and-Boundary Models for Grammar Induction
</title>
<author confidence="0.988224">
Valentin I. Spitkovsky Hiyan Alshawi
</author>
<affiliation confidence="0.984024">
Stanford University and Google Inc. Google Inc., Mountain View, CA, 94043
</affiliation>
<email confidence="0.986555">
valentin@cs.stanford.edu hiyan@google.com
</email>
<author confidence="0.975723">
Daniel Jurafsky
</author>
<affiliation confidence="0.875381">
Stanford University, Stanford, CA, 94305
</affiliation>
<email confidence="0.985456">
jurafsky@stanford.edu
</email>
<figure confidence="0.514315">
DT NN VBZ IN DT NN
[The check] is in [the mail].
Abstract
</figure>
<figureCaption confidence="0.6778488">
We present a new family of models for unsu-
pervised parsing, Dependency and Boundary
models, that use cues at constituent bound-
aries to inform head-outward dependency tree
generation. We build on three intuitions that
are explicit in phrase-structure grammars but
only implicit in standard dependency formu-
lations: (i) Distributions of words that oc-
cur at sentence boundaries — such as English
determiners — resemble constituent edges.
</figureCaption>
<bodyText confidence="0.989957416666667">
(ii) Punctuation at sentence boundaries fur-
ther helps distinguish full sentences from
fragments like headlines and titles, allow-
ing us to model grammatical differences be-
tween complete and incomplete sentences.
(iii) Sentence-internal punctuation boundaries
help with longer-distance dependencies, since
punctuation correlates with constituent edges.
Our models induce state-of-the-art depen-
dency grammars for many languages without
special knowledge of optimal input sentence
lengths or biased, manually-tuned initializers.
</bodyText>
<sectionHeader confidence="0.999132" genericHeader="abstract">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999917846153846">
Natural language is ripe with all manner of bound-
aries at the surface level that align with hierarchical
syntactic structure. From the significance of func-
tion words (Berant et al., 2006) and punctuation
marks (Seginer, 2007; Ponvert et al., 2010) as sepa-
rators between constituents in longer sentences — to
the importance of isolated words in children’s early
vocabulary acquisition (Brent and Siskind, 2001)
— word boundaries play a crucial role in language
learning. We will show that boundary information
can also be useful in dependency grammar induc-
tion models, which traditionally focus on head rather
than fringe words (Carroll and Charniak, 1992).
</bodyText>
<figure confidence="0.2702045">
� �� � � �� �
Subject Object
</figure>
<figureCaption confidence="0.999749">
Figure 1: A partial analysis of our running example.
</figureCaption>
<bodyText confidence="0.99998428125">
Consider the example in Figure 1. Because the
determiner (DT) appears at the left edge of the sen-
tence, it should be possible to learn that determiners
may generally be present at left edges of phrases.
This information could then be used to correctly
parse the sentence-internal determiner in the mail.
Similarly, the fact that the noun head (NN) of the ob-
ject the mail appears at the right edge of the sentence
could help identify the noun check as the right edge
of the subject NP. As with jigsaw puzzles, working
inwards from boundaries helps determine sentence-
internal structures of both noun phrases, neither of
which would be quite so clear if viewed separately.
Furthermore, properties of noun-phrase edges are
partially shared with prepositional- and verb-phrase
units that contain these nouns. Because typical head-
driven grammars model valence separately for each
class of head, however, they cannot see that the left
fringe boundary, The check, of the verb-phrase is
shared with its daughter’s, check. Neither of these
insights is available to traditional dependency for-
mulations, which could learn from the boundaries
of this sentence only that determiners might have no
left- and that nouns might have no right-dependents.
We propose a family of dependency parsing mod-
els that are capable of inducing longer-range im-
plications from sentence edges than just fertilities
of their fringe words. Our ideas conveniently lend
themselves to implementations that can reuse much
of the standard grammar induction machinery, in-
cluding efficient dynamic programming routines for
the relevant expectation-maximization algorithms.
</bodyText>
<page confidence="0.969235">
688
</page>
<note confidence="0.806229666666667">
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 688–698, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics
2 The Dependency and Boundary Models
</note>
<bodyText confidence="0.999133023255814">
Our models follow a standard generative story for
head-outward automata (Alshawi, 1996a), restricted
to the split-head case (see below),1 over lexical word
classes {cw}: first, a sentence root c, is chosen, with
probability PATTACS(c,  |o; L); o is a special start
symbol that, by convention (Klein and Manning,
2004; Eisner, 1996), produces exactly one child, to
its left. Next, the process recurses. Each (head)
word ch generates a left-dependent with probability
1 − PSTOP( ·  |L; · · · ), where dots represent additional
parameterization on which it may be conditioned. If
the child is indeed generated, its identity cd is cho-
sen with probability PATTACS(cd  |ch; · · · ), influenced
by the identity of the parent ch and possibly other pa-
rameters (again represented by dots). The child then
generates its own subtree recursively and the whole
process continues, moving away from the head, un-
til ch fails to generate a left-dependent. At that point,
an analogous procedure is repeated to ch’s right, this
time using stopping factors PSTOP( ·  |x; · · · ). All parse
trees derived in this way are guaranteed to be projec-
tive and can be described by split-head grammars.
Instances of these split-head automata have been
heavily used in grammar induction (Paskin, 2001b;
Klein and Manning, 2004; Headden et al., 2009,
inter alia), in part because they allow for efficient
implementations (Eisner and Satta, 1999, §8) of
the inside-outside re-estimation algorithm (Baker,
1979). The basic tenet of split-head grammars is
that every head word generates its left-dependents
independently of its right-dependents. This as-
sumption implies, for instance, that words’ left-
and right-valences — their numbers of children
to each side — are also independent. But it does
not imply that descendants that are closer to the
head cannot influence the generation of farther
dependents on the same side. Nevertheless, many
popular grammars for unsupervised parsing behave
as if a word had to generate all of its children
(to one side) — or at least their count — before
allowing any of these children themselves to recurse.
For example, Klein and Manning’s (2004) depen-
dency model with valence (DMV) could be imple-
</bodyText>
<footnote confidence="0.851506333333333">
1Unrestricted head-outward automata are strictly more pow-
erful (e.g., they recognize the language a&apos;P in finite state) than
the split-head variants, which process one side before the other.
</footnote>
<bodyText confidence="0.999874625">
mented as both head-outward and head-inward au-
tomata. (In fact, arbitrary permutations of siblings
to a given side of their parent would not affect the
likelihood of the modified tree, with the DMV.) We
propose to make fuller use of split-head automata’s
head-outward nature by drawing on information in
partially-generated parses, which contain useful pre-
dictors that, until now, had not been exploited even
in featurized systems for grammar induction (Cohen
and Smith, 2009; Berg-Kirkpatrick et al., 2010).
Some of these predictors, including the identity
— or even number (McClosky, 2008) — of already-
generated siblings, can be prohibitively expensive in
sentences above a short length k. For example, they
break certain modularity constraints imposed by the
charts used in O(k3)-optimized algorithms (Paskin,
2001a; Eisner, 2000). However, in bottom-up pars-
ing and training from text, everything about the yield
— i.e., the ordered sequence of all already-generated
descendants, on the side of the head that is in the
process of spawning off an additional child — is not
only known but also readily accessible. Taking ad-
vantage of this availability, we designed three new
models for dependency grammar induction.
</bodyText>
<subsectionHeader confidence="0.989139">
2.1 Dependency and Boundary Model One
</subsectionHeader>
<bodyText confidence="0.99967725">
DBM-1 conditions all stopping decisions on adja-
cency and the identity of the fringe word ce — the
currently-farthest descendant (edge) derived by head
ch in the given head-outward direction (dir E {L, x}):
</bodyText>
<equation confidence="0.819959">
PSTOP( ·  |dir; adj, c,).
</equation>
<bodyText confidence="0.999957928571429">
In the adjacent case (adj = T), ch is deciding whether
to have any children on a given side: a first child’s
subtree would be right next to the head, so the head
and the fringe words coincide (ch = ce). In the non-
adjacent case (adj = F), these will be different words
and their classes will, in general, not be the same.2
Thus, non-adjacent stopping decisions will be made
independently of a head word’s identity. Therefore,
all word classes will be equally likely to continue to
grow or not, for a specific proposed fringe boundary.
For example, production of The check is involves
two non-adjacent stopping decisions on the left: one
by the noun check and one by the verb is, both of
which stop after generating a first child. In DBM-1,
</bodyText>
<footnote confidence="0.877721">
2Fringe words differ also from other standard dependency
features (Eisner, 1996, §2.3): parse siblings and adjacent words.
</footnote>
<page confidence="0.985859">
689
</page>
<equation confidence="0.982386888888889">
DT NN VBZ IN DT NN Q
The check is in the mail .
0
� �� �
P = (1 − PSTOP(⋄  |L; T)) × PATTACH(VBZ  |⋄; L)
× (1 − PSTOP(· |L; T, VBZ)) × PATTACH(NN  |VBZ; L)
× (1 − PSTOP( · |R; T, VBZ)) × PATTACH(IN  |VBZ; R)
× PSTOP( ·  |L; F, DT) // VBZ × PSTOP( ·  |R; F, NN) // VBZ
× (1 − PSTOP( · |L; T, NN))2 × P2ATTACH(DT  |NN; L)
× (1 − PSTOP( · |R; T, IN)) × PATTACH(NN  |IN; R)
× P2
STOP( ·  |R; T, NN) × P2STOP( ·  |L; F, DT) // NN
× PSTOP( ·  |L; T, IN) × PSTOP( · |R; F, NN) // IN
× P2 P2
STOP ( · |L; T, DT) ×STOP( · |R; T, DT)
× PSTOP(⋄  |R; T)
� �� �
1
</equation>
<figureCaption confidence="0.85190225">
Figure 2: Our running example — a simple sentence and
its unlabeled dependency parse structure’s probability, as
factored by DBM-1; highlighted comments specify heads
associated to non-adjacent stopping probability factors.
</figureCaption>
<bodyText confidence="0.9304004">
this outcome is captured by squaring a shared pa-
rameter belonging to the left-fringe determiner The:
PSTOP( ·  |L; F, DT)2 — instead of by a product of two
factors, such as PSTOP( ·  |L; F, NN) · PSTOP( ·  |L; F, VBZ).
In these grammars, dependents’ attachment prob-
abilities, given heads, are additionally conditioned
only on their relative positions — as in traditional
models (Klein and Manning, 2004; Paskin, 2001b):
PATTACH(cd  |ch; dir).
Figure 2 shows a completely factored example.
</bodyText>
<subsectionHeader confidence="0.981389">
2.2 Dependency and Boundary Model Two
</subsectionHeader>
<bodyText confidence="0.986961033333333">
DBM-2 allows different but related grammars to co-
exist in a single model. Specifically, we presuppose
that all sentences are assigned to one of two classes:
complete and incomplete (comp ∈ {T, F}, for now
taken as exogenous). This model assumes that word-
word (i.e., head-dependent) interactions in the two
domains are the same. However, sentence lengths
— for which stopping probabilities are responsible
— and distributions of root words may be different.
Consequently, an additional comp parameter is
added to the context of two relevant types of factors:
PSTOP( ·  |dir; adj, ce, comp );
and PATTACH(cr  |O; L, comp ).
For example, the new stopping factors could capture
the fact that incomplete fragments — such as the
noun-phrases George Morton, headlines Energy and
Odds and Ends, a line item c - Domestic car, dollar
quantity Revenue: $3.57 billion, the time 1:11am,
and the like — tend to be much shorter than com-
plete sentences. The new root-attachment factors
could further track that incomplete sentences gener-
ally lack verbs, in contrast to other short sentences,
e.g., Excerpts follow:, Are you kidding?, Yes, he
did., It’s huge., Indeed it is., I said, ‘NOW?’, `Ab-
solutely,” he said., I am waiting., Mrs. Yeargin de-
clined., McGraw-Hill was outraged., `It happens.”,
I’m OK, Jack., Who cares?, Never mind. and so on.
All other attachment probabilities PATTACH(cd  |ch; dir)
remain unchanged, as in DBM-1. In practice, comp
can indicate presence of sentence-final punctuation.
</bodyText>
<subsectionHeader confidence="0.995732">
2.3 Dependency and Boundary Model Three
</subsectionHeader>
<bodyText confidence="0.915636466666667">
DBM-3 adds further conditioning on punctuation
context. We introduce another boolean parameter,
cross, which indicates the presence of intervening
punctuation between a proposed head word ch and
its dependent cd. Using this information, longer-
distance punctuation-crossing arcs can be modeled
separately from other, lower-level dependencies, via
PATTACH(cd  |ch; dir, cross).
For instance, in Continentals believe that the
strongest growth area will be southern Europe., four
words appear between that and will. Conditioning
on (the absence of) intervening punctuation could
help tell true long-distance relations from impostors.
All other probabilities, PSTOP( ·  |dir; adj, ce, comp) and
PATTACH(cr  |⋄; L, comp), remain the same as in DBM-2.
</bodyText>
<subsectionHeader confidence="0.999478">
2.4 Summary of DBMs and Related Models
</subsectionHeader>
<bodyText confidence="0.997523625">
Head-outward automata (Alshawi, 1996a; Alshawi,
1996b; Alshawi et al., 2000) played a central part as
generative models for probabilistic grammars, start-
ing with their early adoption in supervised split-head
constituent parsers (Collins, 1997; Collins, 2003).
Table 1 lists some parameterizations that have since
been used by unsupervised dependency grammar in-
ducers sharing their backbone split-head process.
</bodyText>
<sectionHeader confidence="0.963513" genericHeader="keywords">
3 Experimental Set-Up and Methodology
</sectionHeader>
<bodyText confidence="0.999612333333333">
We first motivate each model by analyzing the Wall
Street Journal (WSJ) portion of the Penn English
Treebank (Marcus et al., 1993),3 before delving into
</bodyText>
<footnote confidence="0.954352">
3We converted labeled constituents into unlabeled depen-
dencies using deterministic “head-percolation” rules (Collins,
</footnote>
<equation confidence="0.98989775">
× PSTOP(⋄  |L; F)
� �� �
1
�
</equation>
<page confidence="0.977962">
690
</page>
<subsectionHeader confidence="0.219584">
Split-Head Dependency Grammar PATTACH (head-root) PATTACH (dependent-head) PSTOP (adjacent and not)
</subsectionHeader>
<note confidence="0.805347333333333">
GB (Paskin, 2001b) 1 / |M |d  |h; dir 1 / 2
DMV (Klein and Manning, 2004) c,.  |o; L cd  |ch; dir ·  |dir; adj, ch
EVG (Headden et al., 2009) c,.  |o; L cd  |ch; dir, adj ·  |dir; adj, ch
</note>
<equation confidence="0.817034333333333">
DBM-1 (§2.1) c,.  |o; L cd  |ch; dir ·  |dir; adj, ce
DBM-2 (§2.2) c,.  |o; L, comp cd  |ch; dir ·  |dir; adj, ce, comp
DBM-3 (§2.3) c,.  |o; L, comp cd  |ch; dir, cross ·  |dir; adj, ce, comp
</equation>
<tableCaption confidence="0.996384">
Table 1: Parameterizations of the split-head-outward generative process used by DBMs and in previous models.
</tableCaption>
<bodyText confidence="0.98775486">
grammar induction experiments. Although motivat-
ing solely from this treebank biases our discussion
towards a very specific genre of just one language,
it has the advantage of allowing us to make concrete
claims that are backed up by significant statistics.
In the grammar induction experiments that follow,
we will test each model’s incremental contribution
to accuracies empirically, across many disparate lan-
guages. We worked with all 23 (disjoint) train/test
splits from the 2006/7 CoNLL shared tasks (Buch-
holz and Marsi, 2006; Nivre et al., 2007), span-
ning 19 languages.4 For each data set, we induced
a baseline grammar using the DMV. We excluded
all training sentences with more than 15 tokens to
create a conservative bias, because in this set-up the
baseline is known to excel (Spitkovsky et al., 2009).
Grammar inducers were initialized using (the same)
uniformly-at-random chosen parse trees of training
sentences (Cohen and Smith, 2010); thereafter, we
applied “add one” smoothing at every training step.
To fairly compare the models under considera-
tion — which could have quite different starting
perplexities and ensuing consecutive relative like-
lihoods — we experimented with two termination
strategies. In one case, we blindly ran each learner
through 40 steps of inside-outside re-estimation, ig-
noring any convergence criteria; in the other case,
we ran until numerical convergence of soft EM’s ob-
jective function or until the likelihood of resulting
Viterbi parse trees suffered — an “early-stopping la-
teen EM” strategy (Spitkovsky et al., 2011a, §2.3).
We evaluated against all sentences of the blind test
sets (except one 145-token item in Arabic ’07 data).
Table 2 shows experimental results, averaged over
1999), discarding any empty nodes, etc., as is standard practice.
4We did not test on WSJ data because such evaluation would
not be blind, as parse trees from the PTB are our motivating ex-
amples; instead, performance on WSJ serves as a strong base-
line in a separate study (Spitkovsky et al., 2012a): bootstrapping
of DBMs from mostly incomplete inter-punctuation fragments.
all 19 languages, for the DMV baselines and DBM-1
and 2. We did not test DBM-3 in this set-up because
most sentence-internal punctuation occurs in longer
sentences; instead, DBM-3 will be tested later (see
§7), using most sentences,5 in the final training step
of a curriculum strategy (Bengio et al., 2009) that we
will propose for DBMs. For the three models tested
on shorter inputs (up to 15 tokens) both terminating
criteria exhibited the same trend; lateen EM consis-
tently scored slightly higher than 40 EM iterations.
</bodyText>
<subsectionHeader confidence="0.33965">
Termination Criterion DMV
</subsectionHeader>
<construct confidence="0.599111333333333">
40 steps of EM
early-stopping lateen EM
Table 2: Directed dependency accuracies, averaged over
all 2006/7 CoNLL evaluation sets (all sentences), for the
DMV and two new dependency-and-boundary grammar
inducers (DBM-1,2) — using two termination strategies.6
</construct>
<sectionHeader confidence="0.910767" genericHeader="introduction">
4 Dependency and Boundary Model One
</sectionHeader>
<bodyText confidence="0.99991575">
The primary difference between DBM-1 and tradi-
tional models, such as the DMV, is that DBM-1 con-
ditions non-adjacent stopping decisions on the iden-
tities of fringe words in partial yields (see §2.1).
</bodyText>
<subsectionHeader confidence="0.997913">
4.1 Analytical Motivation
</subsectionHeader>
<bodyText confidence="0.9999845">
Treebank data suggests that the class of the fringe
word — its part-of-speech, ce — is a better predic-
tor of (non-adjacent) stopping decisions, in a given
direction dir, than the head’s own class ch. A statis-
tical analysis of logistic regressions fitted to the data
shows that the (ch, dir) predictor explains only about
7% of the total variation (see Table 3). This seems
low, although it is much better compared to direction
alone (which explains less than 2%) and slightly bet-
ter than using the (current) number of the head’s de-
</bodyText>
<footnote confidence="0.965212">
5Results for DBM-3 — given only standard input sentences,
up to length fifteen — would be nearly identical to DBM-2’s.
6We down-weighed the four languages appearing in both
CoNLL years (see Table 8) by 50% in all reported averages.
</footnote>
<figure confidence="0.9681508">
33.5
34.0
DBM-1 DBM-2
38.8 40.7
39.0 40.9
</figure>
<page confidence="0.96648">
691
</page>
<table confidence="0.451042428571429">
Non-Adjacent Stop Predictor �2 AICc
adj
(dir) 0.0149 1,120,200
(n, dir) 0.0726 1,049,175
(ch, dir) 0.0728 1,047,157
(ce, dir) 0.2361 904,102.4
(ch, ce, dir) 0.3320 789,594.3
</table>
<tableCaption confidence="0.648479333333333">
Table 3: Coefficients of determination (R2) and Akaike
information criteria (AIC), both adjusted for the number
of parameters, for several single-predictor logistic models
</tableCaption>
<bodyText confidence="0.987919689655172">
of non-adjacent stops, given direction dir; ch is the class
of the head, n is its number of descendants (so far) to that
side, and ce represents the farthest descendant (the edge).
scendants on that side, n, instead of the head’s class.
In contrast, using ce in place of ch boosts explanatory
power to 24%, keeping the number of parameters the
same. If one were willing to roughly square the size
of the model, explanatory power could be improved
further, to 33% (see Table 3), using both ce and ch.
Fringe boundaries thus appear to be informative
even in the supervised case, which is not surprising,
since using just one probability factor (and its com-
plement) to generate very short (geometric coin-flip)
sequences is a recipe for high entropy. But as sug-
gested earlier, fringes should be extra attractive in
unsupervised settings because yields are observable,
whereas heads almost always remain hidden. More-
over, every sentence exposes two true edges (H¨anig,
2010): integrated over many sample sentence begin-
nings and ends, cumulative knowledge about such
markers can guide a grammar inducer inside long in-
puts, where structure is murky. Table 4 shows distri-
butions of all part-of-speech (POS) tags in the tree-
bank versus in sentence-initial, sentence-final and
sentence-root positions. WSJ often leads with deter-
miners, proper nouns, prepositions and pronouns —
all good candidates for starting English phrases; and
its sentences usually end with various noun types,
again consistent with our running example.
</bodyText>
<subsectionHeader confidence="0.992295">
4.2 Experimental Results
</subsectionHeader>
<bodyText confidence="0.945937125">
Table 2 shows DBM-1 to be substantially more ac-
curate than the DMV, on average: 38.8 versus 33.5%
after 40 steps of EM.7 Lateen termination improved
both models’ accuracies slightly, to 39.0 and 34.0%,
respectively, with DBM-1 scoring five points higher.
7DBM-1’s 39% average accuracy with uniform-at-random
initialization is two points above DMV’s scores with the “ad-
hoc harmonic” strategy, 37% (Spitkovsky et al., 2011a, Table 5).
</bodyText>
<figure confidence="0.998271916666666">
First Last
Tokens Tokens
4.33 3.74
1.26 0.64
0.05 0.61
0.07 0.05
0.00 0.11
0.90 0.00
0.08 0.00
0.18 0.43
0.00 0.42
0.20 0.56
0.42 0.01
0.78 0.02
0.27 0.06
0.20 0.54
0.75 0.00
0.06 0.01
0.08 0.00
0.01 0.05
0.00 0.00
0.08 0.05
0.11 0.01
0.09 0.00
</figure>
<tableCaption confidence="0.932645363636364">
Table 4: Empirical distributions for non-punctuationpart-
of-speech tags in WSJ, ordered by overall frequency, as
well as distributions for sentence boundaries and for the
roots of complete and incomplete sentences. (A uniform
distribution would have 1/36 = 2.7% for all POS-tags.)
Table 5: A distance matrix for all pairs of probability dis-
tributions over POS-tags shown in Table 4 and the uni-
form distribution; the BC- (or Hellinger) distance (Bhat-
tacharyya, 1943; Nikulin, 2002) between discrete distri-
butions p and q (over x E X) ranges from zero (iff p = q)
to one (iff p · q = 0, i.e., when they do not overlap at all).
</tableCaption>
<figure confidence="0.9954061875">
4.31 36.67
20.49 12.85
23.34 0.34
13.54 0.57
4.49 20.64
1.29 6.92
5.96 3.88
0.09 3.52
0.44 1.67
5.93 0.00
0.37 0.05
0.17 1.65
0.61 2.57
9.04 1.34
V/1 − � �pxqx
x
All First Last
Sent. Frag.
Uniform
Sent.
First
Last
All
0.48
0.58 0.64
0.35 0.40
0.59
0.79 0.65
0.79 0.42
0.94 0.57
0.83 0.29
0.86
</figure>
<table confidence="0.931162789473684">
POS % ofAll
Tokens
NN 15.94
IN 11.85
NNP 11.09
DT 9.84
JJ 7.32
NNS 7.19
CD 4.37
RB 3.71
VBD 3.65
VB 3.17
CC 2.86
TO 2.67
VBZ 2.57
VBN 2.42
PRP 2.08
VBG 1.77
VBP 1.50
MD 1.17
POS 1.05
PRP$ 1.00
WDT 0.52
JJR 0.39
RP 0.32
NNPS 0.30
WP 0.28
WRB 0.26
JJS 0.23
RBR 0.21
EX 0.10
RBS 0.05
PDT 0.04
FW 0.03
WP$ 0.02
UH 0.01
SYM 0.01
LS 0.01
</table>
<figure confidence="0.993145327868852">
0.00 0.04
0.01 1.15
0.15 17.12
0.00 3.27
0.00 1.50
46.65 0.93
0.48 6.81
0.00 0.00
0.02 0.44
28.31 0.93
0.65 1.28
0.00 0.00
0.10 0.97
14.33 0.71
8.88 0.57
0.01 0.04
0.00 0.00
0.01 0.13
0.00 0.09
0.00 0.00
0.00 2.96
0.01 0.04
0.01 0.31
0.00 0.00
0.00 0.04
0.00 0.00
0.00 0.00
0.00 0.00
0.00 0.09
0.00 0.00
0.00 0.62
0.00 0.18
0.00 0.00
Sent. Frag.
Roots Roots
0.10 23.40
0.24 4.33
0.02 32.02
692
3
17
20
14
27
Distributions of Sentence Lengths (l) in WSJ
7
1
76
1
171
(Box-and-whiskers quartile diagrams.)
l
2,000
1,750
1,500
1,250
1,000
750
500
250
5 10 15 20 25 30 35 40 45 50 55 60 65 70 75
</figure>
<figureCaption confidence="0.998859">
Figure 3: Histograms of lengths (in tokens) for 2,261 non-clausal fragments (red) and other sentences (blue) in WSJ.
</figureCaption>
<sectionHeader confidence="0.94457" genericHeader="method">
5 Dependency and Boundary Model Two
</sectionHeader>
<bodyText confidence="0.99747875">
DBM-2 adapts DBM-1 grammars to two classes
of inputs (complete sentences and incomplete frag-
ments) by forking off new, separate multinomials for
stopping decisions and root-distributions (see §2.2).
</bodyText>
<subsectionHeader confidence="0.987211">
5.1 Analytical Motivation
</subsectionHeader>
<bodyText confidence="0.998027432432432">
Unrepresentative short sentences — such as head-
lines and titles — are common in news-style data
and pose a known nuisance to grammar inducers.
Previous research sometimes took radical measures
to combat the problem: for example, Gillenwater
et al. (2009) excluded all sentences with three or
fewer tokens from their experiments; and Mareˇcek
and Zabokrtsk´y (2011) enforced an “anti-noun-root”
policy to steer their Gibbs sampler away from the
undercurrents caused by the many short noun-phrase
fragments (among sentences up to length 15, in
Czech data). We refer to such snippets of text as
“incomplete sentences” and focus our study of WSJ
on non-clausal data (as signaled by top-level con-
stituent annotations whose first character is not S).8
Table 4 shows that roots of incomplete sentences,
which are dominated by nouns, barely resemble the
other roots, drawn from more traditional verb and
modal types. In fact, these two empirical root dis-
tributions are more distant from one another than ei-
ther is from the uniform distribution, in the space of
discrete probability distributions over POS-tags (see
Table 5). Of the distributions we considered, only
sentence boundaries are as or more different from
8I.e., separating top-level types {S, SINV, SBARQ, SQ, SBAR}
from the rest (ordered by frequency): {NP, FRAG, X, PP, ...}.
(complete) roots, suggesting that heads of fragments
too may warrant their own multinomial in the model.
Further, incomplete sentences are uncharacteris-
tically short (see Figure 3). It is this property that
makes them particularly treacherous to grammar in-
ducers, since by offering few options of root posi-
tions they increase the chances that a learner will
incorrectly induce nouns to be heads. Given that ex-
pected lengths are directly related to stopping deci-
sions, it could make sense to also model the stopping
probabilities of incomplete sentences separately.
</bodyText>
<subsectionHeader confidence="0.995583">
5.2 Experimental Results
</subsectionHeader>
<bodyText confidence="0.999046272727273">
Since it is not possible to consult parse trees during
grammar induction (to check whether an input sen-
tence is clausal), we opted for a proxy: presence of
sentence-final punctuation. Using punctuation to di-
vide input sentences into two groups, DBM-2 scored
higher: 40.9, up from 39.0% accuracy (see Table 2).
After evaluating these multi-lingual experiments,
we checked how well our proxy corresponds to ac-
tual clausal sentences in WSJ. Table 6 shows the bi-
nary confusion matrix having a fairly low (but posi-
tive) Pearson correlation coefficient. False positives
</bodyText>
<table confidence="0.95305625">
ro ≈ 0.31 Clausal non-Clausal Total
Punctuation 46,829 1,936 48,765
no Punctuation 118 325 443
Total 46,947 2,261 49,208
</table>
<tableCaption confidence="0.9980162">
Table 6: A contingency table for clausal sentences and
trailing punctuation in WSJ; the mean square contingency
coefficient ro signifies a low degree of correlation. (For
two binary variables, ro is equivalent to Karl Pearson’s
better-known product-moment correlation coefficient, p.)
</tableCaption>
<page confidence="0.999287">
693
</page>
<bodyText confidence="0.999480727272727">
include parenthesized expressions that are marked
as noun-phrases, such as (See related story: “Fed
Ready to Inject Big Funds”: WSJ Oct. 16, 1989);
false negatives can be headlines having a main verb,
e.g., Population Drain Ends For Midwestern States.
Thus, our proxy is not perfect but seems to be toler-
able in practice. We suspect that identities of punc-
tuation marks (Collins, 2003, Footnote 13) — both
sentence-final and sentence-initial — could be of ex-
tra assistance in grammar induction, specifically for
grouping imperatives, questions, and so forth.
</bodyText>
<sectionHeader confidence="0.974959" genericHeader="method">
6 Dependency and Boundary Model Three
</sectionHeader>
<bodyText confidence="0.990225666666667">
DBM-3 exploits sentence-internal punctuation con-
texts by modeling punctuation-crossing dependency
arcs separately from other attachments (see §2.3).
</bodyText>
<subsectionHeader confidence="0.994429">
6.1 Analytical Motivation
</subsectionHeader>
<bodyText confidence="0.999928">
Many common syntactic relations, such as between
a determiner and a noun, are unlikely to hold over
long distances. (In fact, 45% of all head-percolated
dependencies in WSJ are between adjacent words.)
However, some common constructions are more re-
mote: e.g., subordinating conjunctions are, on av-
erage, 4.8 tokens away from their dependent modal
verbs. Sometimes longer-distance dependencies can
be vetted using sentence-internal punctuation marks.
It happens that the presence of punctuation be-
tween such conjunction (IN) and verb (MD) types
serves as a clue that they are not connected (see Ta-
ble 7a); by contrast, a simpler cue — whether these
words are adjacent — is, in this case, hardly of any
use (see Table 7b). Conditioning on crossing punc-
tuation could be of help then, playing a role simi-
lar to that of comma-counting (Collins, 1997, §2.1)
— and “verb intervening” (Bikel, 2004, §5.1) — in
early head-outward models for supervised parsing.
</bodyText>
<figure confidence="0.790958428571429">
Attached not Attached
337 7,645
2,144 4,040
2,481 11,685
2,478 11,673
3 12
b) ry ≈ +0.00
</figure>
<tableCaption confidence="0.90861975">
Table 7: Contingency tables for IN right-attaching MD,
among closest ordered pairs of these tokens in WSJ sen-
tences with punctuation, versus: (a) presence of interven-
ing punctuation; and (b) presence of intermediate words.
</tableCaption>
<subsectionHeader confidence="0.997779">
6.2 Experimental Results Postponed
</subsectionHeader>
<bodyText confidence="0.9999344">
As we mentioned earlier (see §3), there is little point
in testing DBM-3 with shorter sentences, since most
sentence-internal punctuation occurs in longer in-
puts. Instead, we will test this model in a final step of
a staged training strategy, with more data (see §7.3).
</bodyText>
<sectionHeader confidence="0.954757" genericHeader="method">
7 A Curriculum Strategy for DBMs
</sectionHeader>
<bodyText confidence="0.999995466666667">
We propose to train up to DBM-3 iteratively —
by beginning with DBM-1 and gradually increasing
model complexity through DBM-2, drawing on the
intuitions of IBM translation models 1–4 (Brown et
al., 1993). Instead of using sentences of up to 15 to-
kens, as in all previous experiments (§4–5), we will
now make use of nearly all available training data:
up to length 45 (out of concern for efficiency), dur-
ing later stages. In the first stage, however, we will
use only a subset of the data with DBM-1, in a pro-
cess sometimes called curriculum learning (Bengio
et al., 2009; Krueger and Dayan, 2009, inter alia).
Our grammar inducers will thus be “starting small”
in both senses suggested by Elman (1993): simulta-
neously scaffolding on model- and data-complexity.
</bodyText>
<subsectionHeader confidence="0.999029">
7.1 Scaffolding Stage #1: DBM-1
</subsectionHeader>
<bodyText confidence="0.999964444444444">
We begin by training DBM-1 on sentences with-
out sentence-internal punctuation but with at least
one trailing punctuation mark. Our goal is to avoid,
when possible, overly specific arbitrary parameters
like the “15 tokens or less” threshold used to select
training sentences. Unlike DBM-2 and 3, DBM-1
does not model punctuation or sentence fragments,
so we instead explicitly restrict its attention to this
cleaner subset of the training data, which takes ad-
vantage of the fact that punctuation may generally
correlate with sentence complexity (Frank, 2000).9
Aside from input sentence selection, our exper-
imental set-up here remained identical to previous
training of DBMs (§4–5). Using this new input data,
DBM-1 averaged 40.7% accuracy (see Table 8).
This is slightly higher than the 39.0% when using
sentences up to length 15, suggesting that our heuris-
tic for clean, simple sentences may be a useful one.
</bodyText>
<footnote confidence="0.8396765">
9More incremental training strategies are the subject of an
unpublished companion manuscript (Spitkovsky et al., 2012a).
</footnote>
<figure confidence="0.997514571428571">
a) ry ≈ −0.40
Punctuation
no Punctuation
Total
non-Adjacent
Adjacent
Attached not Attached
Total
7,982
6,184
14,166
14,151
15
Total
</figure>
<page confidence="0.993059">
694
</page>
<table confidence="0.993911925925926">
Directed Dependency Accuracies for: Best of State-of-the-Art Systems
this Work (@10)
DMV DBM-1 DBM-2 DBM-3 +inference
12.9 10.6 11.0 11.1 10.9 (34.5)
36.6 43.9 44.0 44.4 44.9 (48.8)
32.7 34.1 33.0 32.7 33.3 (36.5)
24.7 59.4 63.6 64.6 65.2 (70.4)
41.1 61.3 61.1 61.1 62.1 (78.1)
50.4 63.1 63.0 63.2 63.2 (65.7)
55.3 56.8 57.0 57.1 57.0 (59.8)
31.5 51.3 52.8 53.0 55.1 (61.8)
34.5 50.5 51.2 53.3 54.2 (67.3)
22.4 21.3 19.9 21.8 22.2 (27.4)
44.9 45.9 46.5 46.0 46.6 (48.6)
32.3 29.2 28.6 29.0 29.6 (51.4)
27.7 36.3 37.9 38.4 39.1 (52.1)
36.3 28.1 26.1 26.1 26.9 (36.8)
23.6 43.2 52.1 57.4 58.2 (68.4)
25.5 41.7 39.8 39.9 40.7 (41.8)
42.2 22.8 22.7 22.7 22.7 (32.5)
37.1 68.9 72.3 71.1 72.4 (80.6)
33.4 30.4 33.0 34.1 35.2 (36.8)
22.0 25.0 26.7 27.1 28.2 (51.8)
30.7 48.6 50.3 50.0 50.7 (63.2)
43.4 32.9 33.7 33.4 34.4 (38.1)
58.5 44.6 44.2 43.7 44.8 (44.4)
33.6 40.7 41.7 42.2 42.9 (51.9)
</table>
<figure confidence="0.978692388888889">
Monolingual; POS-
(i) Agnostic (ii) Identified
38.2 SCAJ6 (best average, not an average of bests)
CoNLL Year
&amp; Language
Arabic 2006
’7
Basque ’7
Bulgarian ’7
Catalan ’7
Chinese ’6
’7
Czech ’6
’7
Danish ’6
Dutch ’6
English ’7
German ’6
Greek ’6
Hungarian ’7
Italian ’7
Japanese ’6
Portuguese ’6
Slovenian ’6
Spanish ’6
Swedish ’6
Turkish ’6
’7
Average:
Cross-Lingual
(iii) Transfer
50.2 Sbg
—
—
70.3 Spt
—
—
—
—
—
56.5 Sar
65.7 MPHm:p
45.7 MPHel
56.7 MPHm:d
65.1 MPHm:p
—
69.1 MPHpt
—
76.9 Sbg
—
68.4 MPHit
68.0 MPHm:p
—
—
</figure>
<table confidence="0.81915247826087">
33.4 SCAJ6 —
55.6 RF 54.6 RFH1
43.6 SCAJ5 34.7 MZNR
44.3 SCAJ5 53.9 RFH1&amp;2
63.8 SCAJ5 56.3 MZNR
63.6 SCAJ6 —
58.5 SCAJ6 34.6 MZNR
50.5 SCAJ5 —
49.8 SCAJ5 42.4 RFH1&amp;2
46.0 RF 53.1 RFH1&amp;2
32.5 SCAJ5 48.8 RFH1&amp;2
50.3 SAJ 23.8 MZNR
33.5 SCAJ5 21.8 MZNR
39.0 MZ 33.4 MZNR
48.0 MZ 48.1 MZNR
57.5 MZ 60.6 MZNR
56.6 SCAJ5 53.5 MZNR
43.2 MZ 55.8 RFH1&amp;2
33.6 SCAJ5 34.6 MZNR
53.0 MZ 54.6 MZNR
50.0 SCAJ6 34.3 RFH1&amp;2
40.9 SAJ 61.3 RFH1
48.8 SCAJ6 —
</table>
<tableCaption confidence="0.956135571428572">
Table 8: Average accuracies over CoNLL evaluation sets (all sentences), for the DMV baseline and DBM1–3 trained
with a curriculum strategy, and state-of-the-art results for systems that: (i) are also POS-agnostic and monolingual,
including SCAJ (Spitkovsky et al., 2011a, Tables 5–6) and SAJ (Spitkovsky et al., 2011b); (ii) rely on gold POS-tag
identities to discourage noun roots (Mareˇcek and Zabokrtsk´y, 2011, MZ) or to encourage verbs (Rasooli and Faili,
2012, RF); and (iii) transfer delexicalized parsers (Søgaard, 2011a, S) from resource-rich languages with transla-
tions (McDonald et al., 2011, MPH). DMV and DBM-1 trained on simple sentences, from uniform; DBM-2 and 3
trained on most sentences, from DBM-1 and 2, respectively; +inference is DBM-3 with punctuation constraints.
</tableCaption>
<subsectionHeader confidence="0.998294">
7.2 Scaffolding Stage #2: DBM-2 ← DBM-1
</subsectionHeader>
<bodyText confidence="0.9999771">
Next, we trained on all sentences up to length 45.
Since these inputs are punctuation-rich, in both re-
maining stages we used the constrained Viterbi EM
set-up suggested by Spitkovsky et al. (2011b) in-
stead of plain soft EM; we employ an early termina-
tion strategy, quitting hard EM as soon as soft EM’s
objective suffers (Spitkovsky et al., 2011a). Punc-
tuation was converted into Viterbi-decoding con-
straints during training using the so-called loose
method, which stipulates that all words in an inter-
punctuation fragment must be dominated by a single
(head) word, also from that fragment — with only
these head words allowed to attach the head words
of other fragments, across punctuation boundaries.
To adapt to full data, we initialized DBM-2 using
Viterbi parses from the previous stage (§7.1), plus
uniformly-at-random chosen dependency trees for
the new complex and incomplete sentences, subject
to punctuation-induced constraints. This approach
improved parsing accuracies to 41.7% (see Table 8).
</bodyText>
<subsectionHeader confidence="0.99965">
7.3 Scaffolding Stage #3: DBM-3 ← DBM-2
</subsectionHeader>
<bodyText confidence="0.999996">
Next, we repeated the training process of the pre-
vious stage (§7.2) using DBM-3. To initialize this
model, we combined the final instance of DBM-2
with uniform multinomials for punctuation-crossing
attachment probabilities (see §2.3). As a result, av-
erage performance improved to 42.2% (see Table 8).
Lastly, we applied punctuation constraints also in
inference. Here we used the sprawl method — a
more relaxed approach than in training, allowing ar-
bitrary words to attach inter-punctuation fragments
(provided that each entire fragment still be derived
</bodyText>
<page confidence="0.99802">
695
</page>
<bodyText confidence="0.9995998">
by one of its words) — as suggested by Spitkovsky
et al. (2011b). This technique increased DBM-3’s
average accuracy to 42.9% (see Table 8). Our fi-
nal result substantially improves over the baseline’s
33.6% and compares favorably to previous work.10
</bodyText>
<sectionHeader confidence="0.469437" genericHeader="evaluation">
8 Discussion and the State-of-the-Art
</sectionHeader>
<bodyText confidence="0.9975647">
DBMs come from a long line of head-outward mod-
els for dependency grammar induction yet their gen-
erative processes feature important novelties. One
is conditioning on more observable state — specifi-
cally, the left and right end words of a phrase being
constructed — than in previous work. Another is al-
lowing multiple grammars — e.g., of complete and
incomplete sentences — to coexist in a single model.
These improvements could make DBMs quick-and-
easy to bootstrap directly from any available partial
bracketings (Pereira and Schabes, 1992), for exam-
ple capitalized phrases (Spitkovsky et al., 2012b).
The second part of our work — the use of a cur-
riculum strategy to train DBM-1 through 3 — elim-
inates having to know tuned cut-offs, such as sen-
tences with up to a predetermined number of tokens.
Although this approach adds some complexity, we
chose conservatively, to avoid overfitting settings
of sentence length, convergence criteria, etc.: stage
one’s data is dictated by DBM-1 (which ignores
punctuation); subsequent stages initialize additional
pieces uniformly: uniform-at-random parses for new
data and uniform multinomials for new parameters.
Even without curriculum learning — trained with
vanilla EM — DBM-2 and 1 are already strong.
Further boosts to accuracy could come from em-
ploying more sophisticated optimization algorithms,
e.g., better EM (Samdani et al., 2012), constrained
Gibbs sampling (Mareˇcek and Zabokrtsk´y, 2011) or
locally-normalized features (Berg-Kirkpatrick et al.,
2010). Other orthogonal dependency grammar in-
duction techniques — including ones based on uni-
versal rules (Naseem et al., 2010) — may also ben-
efit in combination with DBMs. Direct comparisons
to previous work require some care, however, as
there are several classes of systems that make dif-
ferent assumptions about training data (see Table 8).
10Note that DBM-1’s 39% average accuracy with standard
training (see Table 2) was already nearly a full point higher than
that of any single previous best system (SCAJ6 — see Table 8).
</bodyText>
<subsectionHeader confidence="0.997502">
8.1 Monolingual POS-Agnostic Inducers
</subsectionHeader>
<bodyText confidence="0.999995037037037">
The first type of grammar inducers, including our
own approach, uses standard training and test data
sets for each language, with gold part-of-speech tags
as anonymized word classes. For the purposes of
this discussion, we also include in this group trans-
ductive learners that may train on data from the test
sets. Our DBM-3 (decoded with punctuation con-
straints) does well among such systems — for which
accuracies on all sentence lengths of the evaluation
sets are reported — attaining highest scores for 8 of
19 languages; the DMV baseline is still state-of-the-
art for one language; and the remaining 10 bests are
split among five other recent systems (see Table 8).11
Half of the five came from various lateen EM strate-
gies (Spitkovsky et al., 2011a) for escaping and/or
avoiding local optima. These heuristics are compat-
ible with how we trained our DBMs and could po-
tentially provide further improvement to accuracies.
Overall, the final scores of DBM-3 were better, on
average, than those of any other single system: 42.9
versus 38.2% (Spitkovsky et al., 2011a, Table 6).
The progression of scores for DBM-1 through 3
without using punctuation constraints in inference
— 40.7, 41.7 and 42.2% — fell entirely above this
previous state-of-the-art result as well; the DMV
baseline — also trained on sentences without inter-
nal but with final punctuation — averaged 33.6%.
</bodyText>
<subsectionHeader confidence="0.99829">
8.2 Monolingual POS-Identified Inducers
</subsectionHeader>
<bodyText confidence="0.9999446">
The second class of techniques assumes knowledge
about identities of part-of-speech tags (Naseem et
al., 2010), i.e., which word tokens are verbs, which
ones are nouns, etc. Such grammar inducers gener-
ally do better than the first kind — e.g., by encour-
aging verbocentricity (Gimpel and Smith, 2011) —
though even here our results appear to be compet-
itive. In fact, to our surprise, only in 5 of 19 lan-
guages a “POS-identified” system performed better
than all of the “POS-agnostic” ones (see Table 8).
</bodyText>
<subsectionHeader confidence="0.992001">
8.3 Multi-Lingual Semi-Supervised Parsers
</subsectionHeader>
<bodyText confidence="0.992898833333333">
The final broad class of related algorithms we con-
sidered extends beyond monolingual data and uses
11For Turkish ’06, the “right-attach” baseline outperforms
even the DMV, at 65.4% (Rasooli and Faili, 2012, Table 1); an
important difference between 2006 and 2007 CoNLL data sets
has to do with segmentation of morphologically-rich languages.
</bodyText>
<page confidence="0.997006">
696
</page>
<bodyText confidence="0.999977166666667">
both identities of POS-tags and/or parallel bitexts
to transfer (supervised) delexicalized parsers across
languages. Parser projection is by far the most suc-
cessful approach to date and we hope that it too
may stand to gain from our modeling improvements.
Of the 10 languages for which we found results
in the literature, transferred parsers underperformed
the grammar inducers in only one case: on En-
glish (see Table 8). The unsupervised system that
performed better used a special “weighted” initial-
izer (Spitkovsky et al., 2011b, §3.1) that worked well
for English (but less so for many other languages).
DBMs may be able to improve initialization. For
example, modeling of incomplete sentences could
help in incremental initialization strategies like baby
steps (Spitkovsky et al., 2009), which are likely sen-
sitive to the proverbial “bum steer” from unrepresen-
tative short fragments, pace Tu and Honavar (2011).
</bodyText>
<subsectionHeader confidence="0.98832">
8.4 Miscellaneous Systems on Short Sentences
</subsectionHeader>
<bodyText confidence="0.99995375">
Several recent systems (Cohen et al., 2011; Søgaard,
2011b; Naseem et al., 2010; Gillenwater et al., 2010;
Berg-Kirkpatrick and Klein, 2010, inter alia) are ab-
sent from Table 8 because they do not report perfor-
mance for all sentence lengths. To facilitate com-
parison with this body of important previous work,
we also tabulated final accuracies for the “up-to-ten
words” task under heading @10: 51.9%, on average.
</bodyText>
<sectionHeader confidence="0.996767" genericHeader="conclusions">
9 Conclusion
</sectionHeader>
<bodyText confidence="0.99997885">
Although a dependency parse for a sentence can be
mapped to a constituency parse (Xia and Palmer,
2001), the probabilistic models generating them use
different conditioning: dependency grammars focus
on the relationship between arguments and heads,
constituency grammars on the coherence of chunks
covered by non-terminals. Since redundant views of
data can make learning easier (Blum and Mitchell,
1998), integrating aspects of both constituency and
dependency ought to be able to help grammar in-
duction. We have shown that this insight is correct:
dependency grammar inducers can gain from mod-
eling boundary information that is fundamental to
constituency (i.e., phrase-structure) formalisms.
DBMs are a step in the direction towards mod-
eling constituent boundaries jointly with head de-
pendencies. Further steps must involve more tightly
coupling the two frameworks, as well as showing
ways to incorporate both kinds of information in
other state-of-the art grammar induction paradigms.
</bodyText>
<sectionHeader confidence="0.995489" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99945225">
We thank Roi Reichart and Marta Recasens, for many helpful
comments on draft versions of this paper, and Marie-Catherine
de Marneffe, Roy Schwartz, Mengqiu Wang and the anonymous
reviewers, for their apt recommendations. Funded, in part, by
Defense Advanced Research Projects Agency (DARPA) Ma-
chine Reading Program, under Air Force Research Labora-
tory (AFRL) prime contract no. FA8750-09-C-0181. Any opin-
ions, findings, and conclusion or recommendations expressed
in this material are those of the authors and do not necessarily
reflect the view of the DARPA, AFRL, or the US government.
First author is grateful to Cindy Chan for her friendship and
support over many long months leading up to this publication.
</bodyText>
<sectionHeader confidence="0.998773" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998793194444444">
H. Alshawi, S. Bangalore, and S. Douglas. 2000. Learning
dependency translation models as collections of finite-state
head transducers. Computational Linguistics, 26.
H. Alshawi. 1996a. Head automata for speech translation. In
ICSLP.
H. Alshawi. 1996b. Method and apparatus for an improved
language recognition system. US Patent 1999/5870706.
J. K. Baker. 1979. Trainable grammars for speech recognition.
In Speech Communication Papers for the 97th Meeting of the
Acoustical Society ofAmerica.
Y. Bengio, J. Louradour, R. Collobert, and J. Weston. 2009.
Curriculum learning. In ICML.
J. Berant, Y. Gross, M. Mussel, B. Sandbank, E. Ruppin, and
S. Edelman. 2006. Boosting unsupervised grammar induc-
tion by splitting complex sentences on function words. In
BUCLD.
T. Berg-Kirkpatrick and D. Klein. 2010. Phylogenetic gram-
mar induction. In ACL.
T. Berg-Kirkpatrick, A. Bouchard-Cˆot´e, J. DeNero, and
D. Klein. 2010. Painless unsupervised learning with fea-
tures. In NAACL-HLT.
A. Bhattacharyya. 1943. On a measure of divergence between
two statistical populations defined by their probability distri-
butions. BCMS, 35.
D. M. Bikel. 2004. Intricacies of Collins’ parsing model. Com-
putational Linguistics, 30.
A. Blum and T. Mitchell. 1998. Combining labeled and unla-
beled data with co-training. In COLT.
M. R. Brent and J. M. Siskind. 2001. The role of exposure to
isolated words in early vocabulary development. Cognition,
81.
P. F. Brown, V. J. Della Pietra, S. A. Della Pietra, and R. L. Mer-
cer. 1993. The mathematics of statistical machine transla-
tion: Parameter estimation. Computational Linguistics, 19.
S. Buchholz and E. Marsi. 2006. CoNLL-X shared task on
multilingual dependency parsing. In CoNLL.
</reference>
<page confidence="0.976084">
697
</page>
<reference confidence="0.999938166666667">
G. Carroll and E. Charniak. 1992. Two experiments on learning
probabilistic dependency grammars from corpora. Technical
report, Brown University.
S. B. Cohen and N. A. Smith. 2009. Shared logistic normal dis-
tributions for soft parameter tying in unsupervised grammar
induction. In IAACL-HLT.
S. B. Cohen and N. A. Smith. 2010. Viterbi training for PCFGs:
Hardness results and competitiveness of uniform initializa-
tion. In ACL.
S. B. Cohen, D. Das, and N. A. Smith. 2011. Unsupervised
structure prediction with non-parallel multilingual guidance.
In EMILP.
M. Collins. 1997. Three generative, lexicalised models for sta-
tistical parsing. In ACL.
M. Collins. 1999. Head-Driven Statistical Models for Iatural
Language Parsing. Ph.D. thesis, University of Pennsylvania.
M. Collins. 2003. Head-driven statistical models for natural
language parsing. Computational Linguistics, 29.
J. Eisner and G. Satta. 1999. Efficient parsing for bilexical
context-free grammars and head-automaton grammars. In
ACL.
J. M. Eisner. 1996. An empirical comparison of probability
models for dependency grammar. Technical report, IRCS.
J. Eisner. 2000. Bilexical grammars and their cubic-time
parsing algorithms. In H. C. Bunt and A. Nijholt, editors,
Advances in Probabilistic and Other Parsing Technologies.
Kluwer Academic Publishers.
J. L. Elman. 1993. Learning and development in neural net-
works: The importance of starting small. Cognition, 48.
R. Frank. 2000. From regular to context-free to mildly context-
sensitive tree rewriting systems: The path of child language
acquisition. In A. Abeill´e and O. Rambow, editors, Tree
Adjoining Grammars: Formalisms, Linguistic Analysis and
Processing. CSLI Publications.
J. Gillenwater, K. Ganchev, J. Grac¸a, B. Taskar, and F. Pereira.
2009. Sparsity in grammar induction. In IIPS: Gram-
mar Induction, Representation of Language and Language
Learning.
J. Gillenwater, K. Ganchev, J. Grac¸a, F. Pereira, and B. Taskar.
2010. Posterior sparsity in unsupervised dependency pars-
ing. Technical report, University of Pennsylvania.
K. Gimpel and N. A. Smith. 2011. Concavity and initialization
for unsupervised dependency grammar induction. Technical
report, CMU.
C. H¨anig. 2010. Improvements in unsupervised co-occurrence
based parsing. In CoILL.
W. P. Headden, III, M. Johnson, and D. McClosky. 2009. Im-
proving unsupervised dependency parsing with richer con-
texts and smoothing. In IAACL-HLT.
D. Klein and C. D. Manning. 2004. Corpus-based induction of
syntactic structure: Models of dependency and constituency.
In ACL.
K. A. Krueger and P. Dayan. 2009. Flexible shaping: How
learning in small steps helps. Cognition, 110.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz. 1993.
Building a large annotated corpus of English: The Penn
Treebank. Computational Linguistics, 19.
D. Mareˇcek and Z. Zabokrtsk´y. 2011. Gibbs sampling with
treeness constraint in unsupervised dependency parsing. In
ROBUS.
D. McClosky. 2008. Modeling valence effects in unsupervised
grammar induction. Technical report, Brown University.
R. McDonald, S. Petrov, and K. Hall. 2011. Multi-source trans-
fer of delexicalized dependency parsers. In EMILP.
T. Naseem, H. Chen, R. Barzilay, and M. Johnson. 2010. Using
universal linguistic knowledge to guide grammar induction.
In EMILP.
M. S. Nikulin. 2002. Hellinger distance. In M. Hazewinkel,
editor, Encyclopaedia of Mathematics. Kluwer Academic
Publishers.
J. Nivre, J. Hall, S. K¨ubler, R. McDonald, J. Nilsson, S. Riedel,
and D. Yuret. 2007. The CoNLL 2007 shared task on de-
pendency parsing. In EMILP-CoILL.
M. A. Paskin. 2001a. Cubic-time parsing and learning algo-
rithms for grammatical bigram models. Technical report,
UCB.
M. A. Paskin. 2001b. Grammatical bigrams. In IIPS.
F. Pereira and Y. Schabes. 1992. Inside-outside reestimation
from partially bracketed corpora. In ACL.
E. Ponvert, J. Baldridge, and K. Erk. 2010. Simple unsuper-
vised identification of low-level constituents. In ICSC.
M. S. Rasooli and H. Faili. 2012. Fast unsupervised depen-
dency parsing with arc-standard transitions. In ROBUS-
UISUP.
R. Samdani, M.-W. Chang, and D. Roth. 2012. Unified expec-
tation maximization. In IAACL-HLT.
Y. Seginer. 2007. Learning Syntactic Structure. Ph.D. thesis,
University of Amsterdam.
A. Søgaard. 2011a. Datapoint selection for cross-language
adaptation of dependency parsers. In ACL.
A. Søgaard. 2011b. From ranked words to dependency trees:
two-stage unsupervised non-projective dependency parsing.
In TextGraphs.
V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2009. Baby
Steps: How “Less is More” in unsupervised dependency
parsing. In IIPS: Grammar Induction, Representation of
Language and Language Learning.
V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2011a. Lateen
EM: Unsupervised training with multiple objectives, applied
to dependency grammar induction. In EMILP.
V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2011b. Punctu-
ation: Making a point in unsupervised dependency parsing.
In CoILL.
V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2012a. Boot-
strapping dependency grammar inducers from incomplete
sentence fragments via austere models – the “wabi-sabi” of
unsupervised parsing. In submission.
V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2012b. Capi-
talization cues improve dependency grammar induction. In
WILS.
K. Tu and V. Honavar. 2011. On the utility of curricula in
unsupervised learning of probabilistic grammars. In IJCAI.
F. Xia and M. Palmer. 2001. Converting dependency structures
to phrase structures. In HLT.
</reference>
<page confidence="0.997287">
698
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.126006">
<title confidence="0.999838">Three Dependency-and-Boundary Models for Grammar Induction</title>
<author confidence="0.991924">Valentin I Spitkovsky Hiyan Alshawi</author>
<affiliation confidence="0.566797">Stanford University and Google Inc. Google Inc., Mountain View, CA, 94043</affiliation>
<email confidence="0.989359">valentin@cs.stanford.eduhiyan@google.com</email>
<author confidence="0.988745">Daniel Jurafsky</author>
<affiliation confidence="0.673954">Stanford University, Stanford, CA, 94305</affiliation>
<email confidence="0.996456">jurafsky@stanford.edu</email>
<note confidence="0.434434">NN DT NN check] is in [the</note>
<abstract confidence="0.999310826086957">We present a new family of models for unsuparsing, and Boundary models, that use cues at constituent boundaries to inform head-outward dependency tree generation. We build on three intuitions that are explicit in phrase-structure grammars but only implicit in standard dependency formulations: (i) Distributions of words that occur at sentence boundaries — such as English determiners — resemble constituent edges. (ii) Punctuation at sentence boundaries further helps distinguish full sentences from fragments like headlines and titles, allowing us to model grammatical differences between complete and incomplete sentences. (iii) Sentence-internal punctuation boundaries help with longer-distance dependencies, since punctuation correlates with constituent edges. Our models induce state-of-the-art dependency grammars for many languages without special knowledge of optimal input sentence lengths or biased, manually-tuned initializers.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>H Alshawi</author>
<author>S Bangalore</author>
<author>S Douglas</author>
</authors>
<title>Learning dependency translation models as collections of finite-state head transducers.</title>
<date>2000</date>
<journal>Computational Linguistics,</journal>
<volume>26</volume>
<contexts>
<context position="12379" citStr="Alshawi et al., 2000" startWordPosition="1989" endWordPosition="1992"> longerdistance punctuation-crossing arcs can be modeled separately from other, lower-level dependencies, via PATTACH(cd |ch; dir, cross). For instance, in Continentals believe that the strongest growth area will be southern Europe., four words appear between that and will. Conditioning on (the absence of) intervening punctuation could help tell true long-distance relations from impostors. All other probabilities, PSTOP( · |dir; adj, ce, comp) and PATTACH(cr |⋄; L, comp), remain the same as in DBM-2. 2.4 Summary of DBMs and Related Models Head-outward automata (Alshawi, 1996a; Alshawi, 1996b; Alshawi et al., 2000) played a central part as generative models for probabilistic grammars, starting with their early adoption in supervised split-head constituent parsers (Collins, 1997; Collins, 2003). Table 1 lists some parameterizations that have since been used by unsupervised dependency grammar inducers sharing their backbone split-head process. 3 Experimental Set-Up and Methodology We first motivate each model by analyzing the Wall Street Journal (WSJ) portion of the Penn English Treebank (Marcus et al., 1993),3 before delving into 3We converted labeled constituents into unlabeled dependencies using determ</context>
</contexts>
<marker>Alshawi, Bangalore, Douglas, 2000</marker>
<rawString>H. Alshawi, S. Bangalore, and S. Douglas. 2000. Learning dependency translation models as collections of finite-state head transducers. Computational Linguistics, 26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Alshawi</author>
</authors>
<title>Head automata for speech translation.</title>
<date>1996</date>
<booktitle>In ICSLP.</booktitle>
<contexts>
<context position="4060" citStr="Alshawi, 1996" startWordPosition="601" endWordPosition="602"> their fringe words. Our ideas conveniently lend themselves to implementations that can reuse much of the standard grammar induction machinery, including efficient dynamic programming routines for the relevant expectation-maximization algorithms. 688 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 688–698, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics 2 The Dependency and Boundary Models Our models follow a standard generative story for head-outward automata (Alshawi, 1996a), restricted to the split-head case (see below),1 over lexical word classes {cw}: first, a sentence root c, is chosen, with probability PATTACS(c, |o; L); o is a special start symbol that, by convention (Klein and Manning, 2004; Eisner, 1996), produces exactly one child, to its left. Next, the process recurses. Each (head) word ch generates a left-dependent with probability 1 − PSTOP( · |L; · · · ), where dots represent additional parameterization on which it may be conditioned. If the child is indeed generated, its identity cd is chosen with probability PATTACS(cd |ch; · · · ), influenced b</context>
<context position="12339" citStr="Alshawi, 1996" startWordPosition="1985" endWordPosition="1986">dent cd. Using this information, longerdistance punctuation-crossing arcs can be modeled separately from other, lower-level dependencies, via PATTACH(cd |ch; dir, cross). For instance, in Continentals believe that the strongest growth area will be southern Europe., four words appear between that and will. Conditioning on (the absence of) intervening punctuation could help tell true long-distance relations from impostors. All other probabilities, PSTOP( · |dir; adj, ce, comp) and PATTACH(cr |⋄; L, comp), remain the same as in DBM-2. 2.4 Summary of DBMs and Related Models Head-outward automata (Alshawi, 1996a; Alshawi, 1996b; Alshawi et al., 2000) played a central part as generative models for probabilistic grammars, starting with their early adoption in supervised split-head constituent parsers (Collins, 1997; Collins, 2003). Table 1 lists some parameterizations that have since been used by unsupervised dependency grammar inducers sharing their backbone split-head process. 3 Experimental Set-Up and Methodology We first motivate each model by analyzing the Wall Street Journal (WSJ) portion of the Penn English Treebank (Marcus et al., 1993),3 before delving into 3We converted labeled constituents </context>
</contexts>
<marker>Alshawi, 1996</marker>
<rawString>H. Alshawi. 1996a. Head automata for speech translation. In ICSLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Alshawi</author>
</authors>
<title>Method and apparatus for an improved language recognition system.</title>
<date>1996</date>
<tech>US Patent</tech>
<contexts>
<context position="4060" citStr="Alshawi, 1996" startWordPosition="601" endWordPosition="602"> their fringe words. Our ideas conveniently lend themselves to implementations that can reuse much of the standard grammar induction machinery, including efficient dynamic programming routines for the relevant expectation-maximization algorithms. 688 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 688–698, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics 2 The Dependency and Boundary Models Our models follow a standard generative story for head-outward automata (Alshawi, 1996a), restricted to the split-head case (see below),1 over lexical word classes {cw}: first, a sentence root c, is chosen, with probability PATTACS(c, |o; L); o is a special start symbol that, by convention (Klein and Manning, 2004; Eisner, 1996), produces exactly one child, to its left. Next, the process recurses. Each (head) word ch generates a left-dependent with probability 1 − PSTOP( · |L; · · · ), where dots represent additional parameterization on which it may be conditioned. If the child is indeed generated, its identity cd is chosen with probability PATTACS(cd |ch; · · · ), influenced b</context>
<context position="12339" citStr="Alshawi, 1996" startWordPosition="1985" endWordPosition="1986">dent cd. Using this information, longerdistance punctuation-crossing arcs can be modeled separately from other, lower-level dependencies, via PATTACH(cd |ch; dir, cross). For instance, in Continentals believe that the strongest growth area will be southern Europe., four words appear between that and will. Conditioning on (the absence of) intervening punctuation could help tell true long-distance relations from impostors. All other probabilities, PSTOP( · |dir; adj, ce, comp) and PATTACH(cr |⋄; L, comp), remain the same as in DBM-2. 2.4 Summary of DBMs and Related Models Head-outward automata (Alshawi, 1996a; Alshawi, 1996b; Alshawi et al., 2000) played a central part as generative models for probabilistic grammars, starting with their early adoption in supervised split-head constituent parsers (Collins, 1997; Collins, 2003). Table 1 lists some parameterizations that have since been used by unsupervised dependency grammar inducers sharing their backbone split-head process. 3 Experimental Set-Up and Methodology We first motivate each model by analyzing the Wall Street Journal (WSJ) portion of the Penn English Treebank (Marcus et al., 1993),3 before delving into 3We converted labeled constituents </context>
</contexts>
<marker>Alshawi, 1996</marker>
<rawString>H. Alshawi. 1996b. Method and apparatus for an improved language recognition system. US Patent 1999/5870706.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J K Baker</author>
</authors>
<title>Trainable grammars for speech recognition.</title>
<date>1979</date>
<booktitle>In Speech Communication Papers for the 97th Meeting of the Acoustical Society ofAmerica.</booktitle>
<contexts>
<context position="5446" citStr="Baker, 1979" startWordPosition="830" endWordPosition="831">s, moving away from the head, until ch fails to generate a left-dependent. At that point, an analogous procedure is repeated to ch’s right, this time using stopping factors PSTOP( · |x; · · · ). All parse trees derived in this way are guaranteed to be projective and can be described by split-head grammars. Instances of these split-head automata have been heavily used in grammar induction (Paskin, 2001b; Klein and Manning, 2004; Headden et al., 2009, inter alia), in part because they allow for efficient implementations (Eisner and Satta, 1999, §8) of the inside-outside re-estimation algorithm (Baker, 1979). The basic tenet of split-head grammars is that every head word generates its left-dependents independently of its right-dependents. This assumption implies, for instance, that words’ leftand right-valences — their numbers of children to each side — are also independent. But it does not imply that descendants that are closer to the head cannot influence the generation of farther dependents on the same side. Nevertheless, many popular grammars for unsupervised parsing behave as if a word had to generate all of its children (to one side) — or at least their count — before allowing any of these </context>
</contexts>
<marker>Baker, 1979</marker>
<rawString>J. K. Baker. 1979. Trainable grammars for speech recognition. In Speech Communication Papers for the 97th Meeting of the Acoustical Society ofAmerica.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Bengio</author>
<author>J Louradour</author>
<author>R Collobert</author>
<author>J Weston</author>
</authors>
<title>Curriculum learning.</title>
<date>2009</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="16036" citStr="Bengio et al., 2009" startWordPosition="2578" endWordPosition="2581">ice. 4We did not test on WSJ data because such evaluation would not be blind, as parse trees from the PTB are our motivating examples; instead, performance on WSJ serves as a strong baseline in a separate study (Spitkovsky et al., 2012a): bootstrapping of DBMs from mostly incomplete inter-punctuation fragments. all 19 languages, for the DMV baselines and DBM-1 and 2. We did not test DBM-3 in this set-up because most sentence-internal punctuation occurs in longer sentences; instead, DBM-3 will be tested later (see §7), using most sentences,5 in the final training step of a curriculum strategy (Bengio et al., 2009) that we will propose for DBMs. For the three models tested on shorter inputs (up to 15 tokens) both terminating criteria exhibited the same trend; lateen EM consistently scored slightly higher than 40 EM iterations. Termination Criterion DMV 40 steps of EM early-stopping lateen EM Table 2: Directed dependency accuracies, averaged over all 2006/7 CoNLL evaluation sets (all sentences), for the DMV and two new dependency-and-boundary grammar inducers (DBM-1,2) — using two termination strategies.6 4 Dependency and Boundary Model One The primary difference between DBM-1 and traditional models, suc</context>
<context position="28230" citStr="Bengio et al., 2009" startWordPosition="4592" endWordPosition="4595">ata (see §7.3). 7 A Curriculum Strategy for DBMs We propose to train up to DBM-3 iteratively — by beginning with DBM-1 and gradually increasing model complexity through DBM-2, drawing on the intuitions of IBM translation models 1–4 (Brown et al., 1993). Instead of using sentences of up to 15 tokens, as in all previous experiments (§4–5), we will now make use of nearly all available training data: up to length 45 (out of concern for efficiency), during later stages. In the first stage, however, we will use only a subset of the data with DBM-1, in a process sometimes called curriculum learning (Bengio et al., 2009; Krueger and Dayan, 2009, inter alia). Our grammar inducers will thus be “starting small” in both senses suggested by Elman (1993): simultaneously scaffolding on model- and data-complexity. 7.1 Scaffolding Stage #1: DBM-1 We begin by training DBM-1 on sentences without sentence-internal punctuation but with at least one trailing punctuation mark. Our goal is to avoid, when possible, overly specific arbitrary parameters like the “15 tokens or less” threshold used to select training sentences. Unlike DBM-2 and 3, DBM-1 does not model punctuation or sentence fragments, so we instead explicitly r</context>
</contexts>
<marker>Bengio, Louradour, Collobert, Weston, 2009</marker>
<rawString>Y. Bengio, J. Louradour, R. Collobert, and J. Weston. 2009. Curriculum learning. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Berant</author>
<author>Y Gross</author>
<author>M Mussel</author>
<author>B Sandbank</author>
<author>E Ruppin</author>
<author>S Edelman</author>
</authors>
<title>Boosting unsupervised grammar induction by splitting complex sentences on function words.</title>
<date>2006</date>
<booktitle>In BUCLD.</booktitle>
<contexts>
<context position="1511" citStr="Berant et al., 2006" startWordPosition="206" endWordPosition="209">nes and titles, allowing us to model grammatical differences between complete and incomplete sentences. (iii) Sentence-internal punctuation boundaries help with longer-distance dependencies, since punctuation correlates with constituent edges. Our models induce state-of-the-art dependency grammars for many languages without special knowledge of optimal input sentence lengths or biased, manually-tuned initializers. 1 Introduction Natural language is ripe with all manner of boundaries at the surface level that align with hierarchical syntactic structure. From the significance of function words (Berant et al., 2006) and punctuation marks (Seginer, 2007; Ponvert et al., 2010) as separators between constituents in longer sentences — to the importance of isolated words in children’s early vocabulary acquisition (Brent and Siskind, 2001) — word boundaries play a crucial role in language learning. We will show that boundary information can also be useful in dependency grammar induction models, which traditionally focus on head rather than fringe words (Carroll and Charniak, 1992). � �� � � �� � Subject Object Figure 1: A partial analysis of our running example. Consider the example in Figure 1. Because the de</context>
</contexts>
<marker>Berant, Gross, Mussel, Sandbank, Ruppin, Edelman, 2006</marker>
<rawString>J. Berant, Y. Gross, M. Mussel, B. Sandbank, E. Ruppin, and S. Edelman. 2006. Boosting unsupervised grammar induction by splitting complex sentences on function words. In BUCLD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Berg-Kirkpatrick</author>
<author>D Klein</author>
</authors>
<title>Phylogenetic grammar induction.</title>
<date>2010</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="39655" citStr="Berg-Kirkpatrick and Klein, 2010" startWordPosition="6435" endWordPosition="6438">sed a special “weighted” initializer (Spitkovsky et al., 2011b, §3.1) that worked well for English (but less so for many other languages). DBMs may be able to improve initialization. For example, modeling of incomplete sentences could help in incremental initialization strategies like baby steps (Spitkovsky et al., 2009), which are likely sensitive to the proverbial “bum steer” from unrepresentative short fragments, pace Tu and Honavar (2011). 8.4 Miscellaneous Systems on Short Sentences Several recent systems (Cohen et al., 2011; Søgaard, 2011b; Naseem et al., 2010; Gillenwater et al., 2010; Berg-Kirkpatrick and Klein, 2010, inter alia) are absent from Table 8 because they do not report performance for all sentence lengths. To facilitate comparison with this body of important previous work, we also tabulated final accuracies for the “up-to-ten words” task under heading @10: 51.9%, on average. 9 Conclusion Although a dependency parse for a sentence can be mapped to a constituency parse (Xia and Palmer, 2001), the probabilistic models generating them use different conditioning: dependency grammars focus on the relationship between arguments and heads, constituency grammars on the coherence of chunks covered by non</context>
</contexts>
<marker>Berg-Kirkpatrick, Klein, 2010</marker>
<rawString>T. Berg-Kirkpatrick and D. Klein. 2010. Phylogenetic grammar induction. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Berg-Kirkpatrick</author>
<author>A Bouchard-Cˆot´e</author>
<author>J DeNero</author>
<author>D Klein</author>
</authors>
<title>Painless unsupervised learning with features.</title>
<date>2010</date>
<booktitle>In NAACL-HLT.</booktitle>
<marker>Berg-Kirkpatrick, Bouchard-Cˆot´e, DeNero, Klein, 2010</marker>
<rawString>T. Berg-Kirkpatrick, A. Bouchard-Cˆot´e, J. DeNero, and D. Klein. 2010. Painless unsupervised learning with features. In NAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Bhattacharyya</author>
</authors>
<title>On a measure of divergence between two statistical populations defined by their probability distributions.</title>
<date>1943</date>
<journal>BCMS,</journal>
<volume>35</volume>
<contexts>
<context position="20635" citStr="Bhattacharyya, 1943" startWordPosition="3320" endWordPosition="3322"> 0.00 0.18 0.43 0.00 0.42 0.20 0.56 0.42 0.01 0.78 0.02 0.27 0.06 0.20 0.54 0.75 0.00 0.06 0.01 0.08 0.00 0.01 0.05 0.00 0.00 0.08 0.05 0.11 0.01 0.09 0.00 Table 4: Empirical distributions for non-punctuationpartof-speech tags in WSJ, ordered by overall frequency, as well as distributions for sentence boundaries and for the roots of complete and incomplete sentences. (A uniform distribution would have 1/36 = 2.7% for all POS-tags.) Table 5: A distance matrix for all pairs of probability distributions over POS-tags shown in Table 4 and the uniform distribution; the BC- (or Hellinger) distance (Bhattacharyya, 1943; Nikulin, 2002) between discrete distributions p and q (over x E X) ranges from zero (iff p = q) to one (iff p · q = 0, i.e., when they do not overlap at all). 4.31 36.67 20.49 12.85 23.34 0.34 13.54 0.57 4.49 20.64 1.29 6.92 5.96 3.88 0.09 3.52 0.44 1.67 5.93 0.00 0.37 0.05 0.17 1.65 0.61 2.57 9.04 1.34 V/1 − � �pxqx x All First Last Sent. Frag. Uniform Sent. First Last All 0.48 0.58 0.64 0.35 0.40 0.59 0.79 0.65 0.79 0.42 0.94 0.57 0.83 0.29 0.86 POS % ofAll Tokens NN 15.94 IN 11.85 NNP 11.09 DT 9.84 JJ 7.32 NNS 7.19 CD 4.37 RB 3.71 VBD 3.65 VB 3.17 CC 2.86 TO 2.67 VBZ 2.57 VBN 2.42 PRP 2.0</context>
</contexts>
<marker>Bhattacharyya, 1943</marker>
<rawString>A. Bhattacharyya. 1943. On a measure of divergence between two statistical populations defined by their probability distributions. BCMS, 35.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D M Bikel</author>
</authors>
<date>2004</date>
<journal>Intricacies of Collins’ parsing model. Computational Linguistics,</journal>
<volume>30</volume>
<contexts>
<context position="26947" citStr="Bikel, 2004" startWordPosition="4378" endWordPosition="4379">unctions are, on average, 4.8 tokens away from their dependent modal verbs. Sometimes longer-distance dependencies can be vetted using sentence-internal punctuation marks. It happens that the presence of punctuation between such conjunction (IN) and verb (MD) types serves as a clue that they are not connected (see Table 7a); by contrast, a simpler cue — whether these words are adjacent — is, in this case, hardly of any use (see Table 7b). Conditioning on crossing punctuation could be of help then, playing a role similar to that of comma-counting (Collins, 1997, §2.1) — and “verb intervening” (Bikel, 2004, §5.1) — in early head-outward models for supervised parsing. Attached not Attached 337 7,645 2,144 4,040 2,481 11,685 2,478 11,673 3 12 b) ry ≈ +0.00 Table 7: Contingency tables for IN right-attaching MD, among closest ordered pairs of these tokens in WSJ sentences with punctuation, versus: (a) presence of intervening punctuation; and (b) presence of intermediate words. 6.2 Experimental Results Postponed As we mentioned earlier (see §3), there is little point in testing DBM-3 with shorter sentences, since most sentence-internal punctuation occurs in longer inputs. Instead, we will test this </context>
</contexts>
<marker>Bikel, 2004</marker>
<rawString>D. M. Bikel. 2004. Intricacies of Collins’ parsing model. Computational Linguistics, 30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Blum</author>
<author>T Mitchell</author>
</authors>
<title>Combining labeled and unlabeled data with co-training.</title>
<date>1998</date>
<booktitle>In COLT.</booktitle>
<contexts>
<context position="40347" citStr="Blum and Mitchell, 1998" startWordPosition="6542" endWordPosition="6545">formance for all sentence lengths. To facilitate comparison with this body of important previous work, we also tabulated final accuracies for the “up-to-ten words” task under heading @10: 51.9%, on average. 9 Conclusion Although a dependency parse for a sentence can be mapped to a constituency parse (Xia and Palmer, 2001), the probabilistic models generating them use different conditioning: dependency grammars focus on the relationship between arguments and heads, constituency grammars on the coherence of chunks covered by non-terminals. Since redundant views of data can make learning easier (Blum and Mitchell, 1998), integrating aspects of both constituency and dependency ought to be able to help grammar induction. We have shown that this insight is correct: dependency grammar inducers can gain from modeling boundary information that is fundamental to constituency (i.e., phrase-structure) formalisms. DBMs are a step in the direction towards modeling constituent boundaries jointly with head dependencies. Further steps must involve more tightly coupling the two frameworks, as well as showing ways to incorporate both kinds of information in other state-of-the art grammar induction paradigms. Acknowledgments</context>
</contexts>
<marker>Blum, Mitchell, 1998</marker>
<rawString>A. Blum and T. Mitchell. 1998. Combining labeled and unlabeled data with co-training. In COLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M R Brent</author>
<author>J M Siskind</author>
</authors>
<title>The role of exposure to isolated words in early vocabulary development.</title>
<date>2001</date>
<journal>Cognition,</journal>
<volume>81</volume>
<contexts>
<context position="1733" citStr="Brent and Siskind, 2001" startWordPosition="239" endWordPosition="242">s with constituent edges. Our models induce state-of-the-art dependency grammars for many languages without special knowledge of optimal input sentence lengths or biased, manually-tuned initializers. 1 Introduction Natural language is ripe with all manner of boundaries at the surface level that align with hierarchical syntactic structure. From the significance of function words (Berant et al., 2006) and punctuation marks (Seginer, 2007; Ponvert et al., 2010) as separators between constituents in longer sentences — to the importance of isolated words in children’s early vocabulary acquisition (Brent and Siskind, 2001) — word boundaries play a crucial role in language learning. We will show that boundary information can also be useful in dependency grammar induction models, which traditionally focus on head rather than fringe words (Carroll and Charniak, 1992). � �� � � �� � Subject Object Figure 1: A partial analysis of our running example. Consider the example in Figure 1. Because the determiner (DT) appears at the left edge of the sentence, it should be possible to learn that determiners may generally be present at left edges of phrases. This information could then be used to correctly parse the sentence</context>
</contexts>
<marker>Brent, Siskind, 2001</marker>
<rawString>M. R. Brent and J. M. Siskind. 2001. The role of exposure to isolated words in early vocabulary development. Cognition, 81.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>V J Della Pietra</author>
<author>S A Della Pietra</author>
<author>R L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<contexts>
<context position="27863" citStr="Brown et al., 1993" startWordPosition="4524" endWordPosition="4527"> presence of intervening punctuation; and (b) presence of intermediate words. 6.2 Experimental Results Postponed As we mentioned earlier (see §3), there is little point in testing DBM-3 with shorter sentences, since most sentence-internal punctuation occurs in longer inputs. Instead, we will test this model in a final step of a staged training strategy, with more data (see §7.3). 7 A Curriculum Strategy for DBMs We propose to train up to DBM-3 iteratively — by beginning with DBM-1 and gradually increasing model complexity through DBM-2, drawing on the intuitions of IBM translation models 1–4 (Brown et al., 1993). Instead of using sentences of up to 15 tokens, as in all previous experiments (§4–5), we will now make use of nearly all available training data: up to length 45 (out of concern for efficiency), during later stages. In the first stage, however, we will use only a subset of the data with DBM-1, in a process sometimes called curriculum learning (Bengio et al., 2009; Krueger and Dayan, 2009, inter alia). Our grammar inducers will thus be “starting small” in both senses suggested by Elman (1993): simultaneously scaffolding on model- and data-complexity. 7.1 Scaffolding Stage #1: DBM-1 We begin b</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>P. F. Brown, V. J. Della Pietra, S. A. Della Pietra, and R. L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Buchholz</author>
<author>E Marsi</author>
</authors>
<title>CoNLL-X shared task on multilingual dependency parsing.</title>
<date>2006</date>
<booktitle>In CoNLL.</booktitle>
<contexts>
<context position="14157" citStr="Buchholz and Marsi, 2006" startWordPosition="2278" endWordPosition="2282">eterizations of the split-head-outward generative process used by DBMs and in previous models. grammar induction experiments. Although motivating solely from this treebank biases our discussion towards a very specific genre of just one language, it has the advantage of allowing us to make concrete claims that are backed up by significant statistics. In the grammar induction experiments that follow, we will test each model’s incremental contribution to accuracies empirically, across many disparate languages. We worked with all 23 (disjoint) train/test splits from the 2006/7 CoNLL shared tasks (Buchholz and Marsi, 2006; Nivre et al., 2007), spanning 19 languages.4 For each data set, we induced a baseline grammar using the DMV. We excluded all training sentences with more than 15 tokens to create a conservative bias, because in this set-up the baseline is known to excel (Spitkovsky et al., 2009). Grammar inducers were initialized using (the same) uniformly-at-random chosen parse trees of training sentences (Cohen and Smith, 2010); thereafter, we applied “add one” smoothing at every training step. To fairly compare the models under consideration — which could have quite different starting perplexities and ens</context>
</contexts>
<marker>Buchholz, Marsi, 2006</marker>
<rawString>S. Buchholz and E. Marsi. 2006. CoNLL-X shared task on multilingual dependency parsing. In CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Carroll</author>
<author>E Charniak</author>
</authors>
<title>Two experiments on learning probabilistic dependency grammars from corpora.</title>
<date>1992</date>
<tech>Technical report,</tech>
<institution>Brown University.</institution>
<contexts>
<context position="1979" citStr="Carroll and Charniak, 1992" startWordPosition="278" endWordPosition="281">h all manner of boundaries at the surface level that align with hierarchical syntactic structure. From the significance of function words (Berant et al., 2006) and punctuation marks (Seginer, 2007; Ponvert et al., 2010) as separators between constituents in longer sentences — to the importance of isolated words in children’s early vocabulary acquisition (Brent and Siskind, 2001) — word boundaries play a crucial role in language learning. We will show that boundary information can also be useful in dependency grammar induction models, which traditionally focus on head rather than fringe words (Carroll and Charniak, 1992). � �� � � �� � Subject Object Figure 1: A partial analysis of our running example. Consider the example in Figure 1. Because the determiner (DT) appears at the left edge of the sentence, it should be possible to learn that determiners may generally be present at left edges of phrases. This information could then be used to correctly parse the sentence-internal determiner in the mail. Similarly, the fact that the noun head (NN) of the object the mail appears at the right edge of the sentence could help identify the noun check as the right edge of the subject NP. As with jigsaw puzzles, working</context>
</contexts>
<marker>Carroll, Charniak, 1992</marker>
<rawString>G. Carroll and E. Charniak. 1992. Two experiments on learning probabilistic dependency grammars from corpora. Technical report, Brown University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S B Cohen</author>
<author>N A Smith</author>
</authors>
<title>Shared logistic normal distributions for soft parameter tying in unsupervised grammar induction.</title>
<date>2009</date>
<booktitle>In IAACL-HLT.</booktitle>
<contexts>
<context position="6833" citStr="Cohen and Smith, 2009" startWordPosition="1048" endWordPosition="1051">ctly more powerful (e.g., they recognize the language a&apos;P in finite state) than the split-head variants, which process one side before the other. mented as both head-outward and head-inward automata. (In fact, arbitrary permutations of siblings to a given side of their parent would not affect the likelihood of the modified tree, with the DMV.) We propose to make fuller use of split-head automata’s head-outward nature by drawing on information in partially-generated parses, which contain useful predictors that, until now, had not been exploited even in featurized systems for grammar induction (Cohen and Smith, 2009; Berg-Kirkpatrick et al., 2010). Some of these predictors, including the identity — or even number (McClosky, 2008) — of alreadygenerated siblings, can be prohibitively expensive in sentences above a short length k. For example, they break certain modularity constraints imposed by the charts used in O(k3)-optimized algorithms (Paskin, 2001a; Eisner, 2000). However, in bottom-up parsing and training from text, everything about the yield — i.e., the ordered sequence of all already-generated descendants, on the side of the head that is in the process of spawning off an additional child — is not </context>
</contexts>
<marker>Cohen, Smith, 2009</marker>
<rawString>S. B. Cohen and N. A. Smith. 2009. Shared logistic normal distributions for soft parameter tying in unsupervised grammar induction. In IAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S B Cohen</author>
<author>N A Smith</author>
</authors>
<title>Viterbi training for PCFGs: Hardness results and competitiveness of uniform initialization.</title>
<date>2010</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="14575" citStr="Cohen and Smith, 2010" startWordPosition="2346" endWordPosition="2349">model’s incremental contribution to accuracies empirically, across many disparate languages. We worked with all 23 (disjoint) train/test splits from the 2006/7 CoNLL shared tasks (Buchholz and Marsi, 2006; Nivre et al., 2007), spanning 19 languages.4 For each data set, we induced a baseline grammar using the DMV. We excluded all training sentences with more than 15 tokens to create a conservative bias, because in this set-up the baseline is known to excel (Spitkovsky et al., 2009). Grammar inducers were initialized using (the same) uniformly-at-random chosen parse trees of training sentences (Cohen and Smith, 2010); thereafter, we applied “add one” smoothing at every training step. To fairly compare the models under consideration — which could have quite different starting perplexities and ensuing consecutive relative likelihoods — we experimented with two termination strategies. In one case, we blindly ran each learner through 40 steps of inside-outside re-estimation, ignoring any convergence criteria; in the other case, we ran until numerical convergence of soft EM’s objective function or until the likelihood of resulting Viterbi parse trees suffered — an “early-stopping lateen EM” strategy (Spitkovsk</context>
</contexts>
<marker>Cohen, Smith, 2010</marker>
<rawString>S. B. Cohen and N. A. Smith. 2010. Viterbi training for PCFGs: Hardness results and competitiveness of uniform initialization. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S B Cohen</author>
<author>D Das</author>
<author>N A Smith</author>
</authors>
<title>Unsupervised structure prediction with non-parallel multilingual guidance.</title>
<date>2011</date>
<booktitle>In EMILP.</booktitle>
<contexts>
<context position="39558" citStr="Cohen et al., 2011" startWordPosition="6421" endWordPosition="6424">one case: on English (see Table 8). The unsupervised system that performed better used a special “weighted” initializer (Spitkovsky et al., 2011b, §3.1) that worked well for English (but less so for many other languages). DBMs may be able to improve initialization. For example, modeling of incomplete sentences could help in incremental initialization strategies like baby steps (Spitkovsky et al., 2009), which are likely sensitive to the proverbial “bum steer” from unrepresentative short fragments, pace Tu and Honavar (2011). 8.4 Miscellaneous Systems on Short Sentences Several recent systems (Cohen et al., 2011; Søgaard, 2011b; Naseem et al., 2010; Gillenwater et al., 2010; Berg-Kirkpatrick and Klein, 2010, inter alia) are absent from Table 8 because they do not report performance for all sentence lengths. To facilitate comparison with this body of important previous work, we also tabulated final accuracies for the “up-to-ten words” task under heading @10: 51.9%, on average. 9 Conclusion Although a dependency parse for a sentence can be mapped to a constituency parse (Xia and Palmer, 2001), the probabilistic models generating them use different conditioning: dependency grammars focus on the relation</context>
</contexts>
<marker>Cohen, Das, Smith, 2011</marker>
<rawString>S. B. Cohen, D. Das, and N. A. Smith. 2011. Unsupervised structure prediction with non-parallel multilingual guidance. In EMILP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Three generative, lexicalised models for statistical parsing.</title>
<date>1997</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="12545" citStr="Collins, 1997" startWordPosition="2014" endWordPosition="2015">ve that the strongest growth area will be southern Europe., four words appear between that and will. Conditioning on (the absence of) intervening punctuation could help tell true long-distance relations from impostors. All other probabilities, PSTOP( · |dir; adj, ce, comp) and PATTACH(cr |⋄; L, comp), remain the same as in DBM-2. 2.4 Summary of DBMs and Related Models Head-outward automata (Alshawi, 1996a; Alshawi, 1996b; Alshawi et al., 2000) played a central part as generative models for probabilistic grammars, starting with their early adoption in supervised split-head constituent parsers (Collins, 1997; Collins, 2003). Table 1 lists some parameterizations that have since been used by unsupervised dependency grammar inducers sharing their backbone split-head process. 3 Experimental Set-Up and Methodology We first motivate each model by analyzing the Wall Street Journal (WSJ) portion of the Penn English Treebank (Marcus et al., 1993),3 before delving into 3We converted labeled constituents into unlabeled dependencies using deterministic “head-percolation” rules (Collins, × PSTOP(⋄ |L; F) � �� � 1 � 690 Split-Head Dependency Grammar PATTACH (head-root) PATTACH (dependent-head) PSTOP (adjacent </context>
<context position="26902" citStr="Collins, 1997" startWordPosition="4371" endWordPosition="4372">tions are more remote: e.g., subordinating conjunctions are, on average, 4.8 tokens away from their dependent modal verbs. Sometimes longer-distance dependencies can be vetted using sentence-internal punctuation marks. It happens that the presence of punctuation between such conjunction (IN) and verb (MD) types serves as a clue that they are not connected (see Table 7a); by contrast, a simpler cue — whether these words are adjacent — is, in this case, hardly of any use (see Table 7b). Conditioning on crossing punctuation could be of help then, playing a role similar to that of comma-counting (Collins, 1997, §2.1) — and “verb intervening” (Bikel, 2004, §5.1) — in early head-outward models for supervised parsing. Attached not Attached 337 7,645 2,144 4,040 2,481 11,685 2,478 11,673 3 12 b) ry ≈ +0.00 Table 7: Contingency tables for IN right-attaching MD, among closest ordered pairs of these tokens in WSJ sentences with punctuation, versus: (a) presence of intervening punctuation; and (b) presence of intermediate words. 6.2 Experimental Results Postponed As we mentioned earlier (see §3), there is little point in testing DBM-3 with shorter sentences, since most sentence-internal punctuation occurs </context>
</contexts>
<marker>Collins, 1997</marker>
<rawString>M. Collins. 1997. Three generative, lexicalised models for statistical parsing. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Head-Driven Statistical Models for Iatural Language Parsing.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<marker>Collins, 1999</marker>
<rawString>M. Collins. 1999. Head-Driven Statistical Models for Iatural Language Parsing. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Head-driven statistical models for natural language parsing.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<contexts>
<context position="12561" citStr="Collins, 2003" startWordPosition="2016" endWordPosition="2017">ongest growth area will be southern Europe., four words appear between that and will. Conditioning on (the absence of) intervening punctuation could help tell true long-distance relations from impostors. All other probabilities, PSTOP( · |dir; adj, ce, comp) and PATTACH(cr |⋄; L, comp), remain the same as in DBM-2. 2.4 Summary of DBMs and Related Models Head-outward automata (Alshawi, 1996a; Alshawi, 1996b; Alshawi et al., 2000) played a central part as generative models for probabilistic grammars, starting with their early adoption in supervised split-head constituent parsers (Collins, 1997; Collins, 2003). Table 1 lists some parameterizations that have since been used by unsupervised dependency grammar inducers sharing their backbone split-head process. 3 Experimental Set-Up and Methodology We first motivate each model by analyzing the Wall Street Journal (WSJ) portion of the Penn English Treebank (Marcus et al., 1993),3 before delving into 3We converted labeled constituents into unlabeled dependencies using deterministic “head-percolation” rules (Collins, × PSTOP(⋄ |L; F) � �� � 1 � 690 Split-Head Dependency Grammar PATTACH (head-root) PATTACH (dependent-head) PSTOP (adjacent and not) GB (Pas</context>
<context position="25670" citStr="Collins, 2003" startWordPosition="4182" endWordPosition="4183">ing punctuation in WSJ; the mean square contingency coefficient ro signifies a low degree of correlation. (For two binary variables, ro is equivalent to Karl Pearson’s better-known product-moment correlation coefficient, p.) 693 include parenthesized expressions that are marked as noun-phrases, such as (See related story: “Fed Ready to Inject Big Funds”: WSJ Oct. 16, 1989); false negatives can be headlines having a main verb, e.g., Population Drain Ends For Midwestern States. Thus, our proxy is not perfect but seems to be tolerable in practice. We suspect that identities of punctuation marks (Collins, 2003, Footnote 13) — both sentence-final and sentence-initial — could be of extra assistance in grammar induction, specifically for grouping imperatives, questions, and so forth. 6 Dependency and Boundary Model Three DBM-3 exploits sentence-internal punctuation contexts by modeling punctuation-crossing dependency arcs separately from other attachments (see §2.3). 6.1 Analytical Motivation Many common syntactic relations, such as between a determiner and a noun, are unlikely to hold over long distances. (In fact, 45% of all head-percolated dependencies in WSJ are between adjacent words.) However, s</context>
</contexts>
<marker>Collins, 2003</marker>
<rawString>M. Collins. 2003. Head-driven statistical models for natural language parsing. Computational Linguistics, 29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Eisner</author>
<author>G Satta</author>
</authors>
<title>Efficient parsing for bilexical context-free grammars and head-automaton grammars.</title>
<date>1999</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="5381" citStr="Eisner and Satta, 1999" startWordPosition="820" endWordPosition="823">d then generates its own subtree recursively and the whole process continues, moving away from the head, until ch fails to generate a left-dependent. At that point, an analogous procedure is repeated to ch’s right, this time using stopping factors PSTOP( · |x; · · · ). All parse trees derived in this way are guaranteed to be projective and can be described by split-head grammars. Instances of these split-head automata have been heavily used in grammar induction (Paskin, 2001b; Klein and Manning, 2004; Headden et al., 2009, inter alia), in part because they allow for efficient implementations (Eisner and Satta, 1999, §8) of the inside-outside re-estimation algorithm (Baker, 1979). The basic tenet of split-head grammars is that every head word generates its left-dependents independently of its right-dependents. This assumption implies, for instance, that words’ leftand right-valences — their numbers of children to each side — are also independent. But it does not imply that descendants that are closer to the head cannot influence the generation of farther dependents on the same side. Nevertheless, many popular grammars for unsupervised parsing behave as if a word had to generate all of its children (to on</context>
</contexts>
<marker>Eisner, Satta, 1999</marker>
<rawString>J. Eisner and G. Satta. 1999. Efficient parsing for bilexical context-free grammars and head-automaton grammars. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J M Eisner</author>
</authors>
<title>An empirical comparison of probability models for dependency grammar.</title>
<date>1996</date>
<tech>Technical report, IRCS.</tech>
<contexts>
<context position="4304" citStr="Eisner, 1996" startWordPosition="640" endWordPosition="641">s. 688 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 688–698, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics 2 The Dependency and Boundary Models Our models follow a standard generative story for head-outward automata (Alshawi, 1996a), restricted to the split-head case (see below),1 over lexical word classes {cw}: first, a sentence root c, is chosen, with probability PATTACS(c, |o; L); o is a special start symbol that, by convention (Klein and Manning, 2004; Eisner, 1996), produces exactly one child, to its left. Next, the process recurses. Each (head) word ch generates a left-dependent with probability 1 − PSTOP( · |L; · · · ), where dots represent additional parameterization on which it may be conditioned. If the child is indeed generated, its identity cd is chosen with probability PATTACS(cd |ch; · · · ), influenced by the identity of the parent ch and possibly other parameters (again represented by dots). The child then generates its own subtree recursively and the whole process continues, moving away from the head, until ch fails to generate a left-depend</context>
<context position="8662" citStr="Eisner, 1996" startWordPosition="1355" endWordPosition="1356">the nonadjacent case (adj = F), these will be different words and their classes will, in general, not be the same.2 Thus, non-adjacent stopping decisions will be made independently of a head word’s identity. Therefore, all word classes will be equally likely to continue to grow or not, for a specific proposed fringe boundary. For example, production of The check is involves two non-adjacent stopping decisions on the left: one by the noun check and one by the verb is, both of which stop after generating a first child. In DBM-1, 2Fringe words differ also from other standard dependency features (Eisner, 1996, §2.3): parse siblings and adjacent words. 689 DT NN VBZ IN DT NN Q The check is in the mail . 0 � �� � P = (1 − PSTOP(⋄ |L; T)) × PATTACH(VBZ |⋄; L) × (1 − PSTOP(· |L; T, VBZ)) × PATTACH(NN |VBZ; L) × (1 − PSTOP( · |R; T, VBZ)) × PATTACH(IN |VBZ; R) × PSTOP( · |L; F, DT) // VBZ × PSTOP( · |R; F, NN) // VBZ × (1 − PSTOP( · |L; T, NN))2 × P2ATTACH(DT |NN; L) × (1 − PSTOP( · |R; T, IN)) × PATTACH(NN |IN; R) × P2 STOP( · |R; T, NN) × P2STOP( · |L; F, DT) // NN × PSTOP( · |L; T, IN) × PSTOP( · |R; F, NN) // IN × P2 P2 STOP ( · |L; T, DT) ×STOP( · |R; T, DT) × PSTOP(⋄ |R; T) � �� � 1 Figure 2: Our</context>
</contexts>
<marker>Eisner, 1996</marker>
<rawString>J. M. Eisner. 1996. An empirical comparison of probability models for dependency grammar. Technical report, IRCS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Eisner</author>
</authors>
<title>Bilexical grammars and their cubic-time parsing algorithms. In</title>
<date>2000</date>
<booktitle>Advances in Probabilistic and Other Parsing Technologies.</booktitle>
<editor>H. C. Bunt and A. Nijholt, editors,</editor>
<publisher>Kluwer Academic Publishers.</publisher>
<contexts>
<context position="7191" citStr="Eisner, 2000" startWordPosition="1102" endWordPosition="1103">uller use of split-head automata’s head-outward nature by drawing on information in partially-generated parses, which contain useful predictors that, until now, had not been exploited even in featurized systems for grammar induction (Cohen and Smith, 2009; Berg-Kirkpatrick et al., 2010). Some of these predictors, including the identity — or even number (McClosky, 2008) — of alreadygenerated siblings, can be prohibitively expensive in sentences above a short length k. For example, they break certain modularity constraints imposed by the charts used in O(k3)-optimized algorithms (Paskin, 2001a; Eisner, 2000). However, in bottom-up parsing and training from text, everything about the yield — i.e., the ordered sequence of all already-generated descendants, on the side of the head that is in the process of spawning off an additional child — is not only known but also readily accessible. Taking advantage of this availability, we designed three new models for dependency grammar induction. 2.1 Dependency and Boundary Model One DBM-1 conditions all stopping decisions on adjacency and the identity of the fringe word ce — the currently-farthest descendant (edge) derived by head ch in the given head-outwar</context>
</contexts>
<marker>Eisner, 2000</marker>
<rawString>J. Eisner. 2000. Bilexical grammars and their cubic-time parsing algorithms. In H. C. Bunt and A. Nijholt, editors, Advances in Probabilistic and Other Parsing Technologies. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J L Elman</author>
</authors>
<title>Learning and development in neural networks: The importance of starting small.</title>
<date>1993</date>
<journal>Cognition,</journal>
<volume>48</volume>
<contexts>
<context position="28361" citStr="Elman (1993)" startWordPosition="4615" endWordPosition="4616">asing model complexity through DBM-2, drawing on the intuitions of IBM translation models 1–4 (Brown et al., 1993). Instead of using sentences of up to 15 tokens, as in all previous experiments (§4–5), we will now make use of nearly all available training data: up to length 45 (out of concern for efficiency), during later stages. In the first stage, however, we will use only a subset of the data with DBM-1, in a process sometimes called curriculum learning (Bengio et al., 2009; Krueger and Dayan, 2009, inter alia). Our grammar inducers will thus be “starting small” in both senses suggested by Elman (1993): simultaneously scaffolding on model- and data-complexity. 7.1 Scaffolding Stage #1: DBM-1 We begin by training DBM-1 on sentences without sentence-internal punctuation but with at least one trailing punctuation mark. Our goal is to avoid, when possible, overly specific arbitrary parameters like the “15 tokens or less” threshold used to select training sentences. Unlike DBM-2 and 3, DBM-1 does not model punctuation or sentence fragments, so we instead explicitly restrict its attention to this cleaner subset of the training data, which takes advantage of the fact that punctuation may generally</context>
</contexts>
<marker>Elman, 1993</marker>
<rawString>J. L. Elman. 1993. Learning and development in neural networks: The importance of starting small. Cognition, 48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Frank</author>
</authors>
<title>From regular to context-free to mildly contextsensitive tree rewriting systems: The path of child language acquisition.</title>
<date>2000</date>
<editor>In A. Abeill´e and O. Rambow, editors,</editor>
<publisher>CSLI Publications.</publisher>
<contexts>
<context position="29010" citStr="Frank, 2000" startWordPosition="4713" endWordPosition="4714">- and data-complexity. 7.1 Scaffolding Stage #1: DBM-1 We begin by training DBM-1 on sentences without sentence-internal punctuation but with at least one trailing punctuation mark. Our goal is to avoid, when possible, overly specific arbitrary parameters like the “15 tokens or less” threshold used to select training sentences. Unlike DBM-2 and 3, DBM-1 does not model punctuation or sentence fragments, so we instead explicitly restrict its attention to this cleaner subset of the training data, which takes advantage of the fact that punctuation may generally correlate with sentence complexity (Frank, 2000).9 Aside from input sentence selection, our experimental set-up here remained identical to previous training of DBMs (§4–5). Using this new input data, DBM-1 averaged 40.7% accuracy (see Table 8). This is slightly higher than the 39.0% when using sentences up to length 15, suggesting that our heuristic for clean, simple sentences may be a useful one. 9More incremental training strategies are the subject of an unpublished companion manuscript (Spitkovsky et al., 2012a). a) ry ≈ −0.40 Punctuation no Punctuation Total non-Adjacent Adjacent Attached not Attached Total 7,982 6,184 14,166 14,151 15 </context>
</contexts>
<marker>Frank, 2000</marker>
<rawString>R. Frank. 2000. From regular to context-free to mildly contextsensitive tree rewriting systems: The path of child language acquisition. In A. Abeill´e and O. Rambow, editors, Tree Adjoining Grammars: Formalisms, Linguistic Analysis and Processing. CSLI Publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Gillenwater</author>
<author>K Ganchev</author>
<author>J Grac¸a</author>
<author>B Taskar</author>
<author>F Pereira</author>
</authors>
<title>Sparsity in grammar induction.</title>
<date>2009</date>
<booktitle>In IIPS: Grammar Induction, Representation of Language and Language Learning.</booktitle>
<marker>Gillenwater, Ganchev, Grac¸a, Taskar, Pereira, 2009</marker>
<rawString>J. Gillenwater, K. Ganchev, J. Grac¸a, B. Taskar, and F. Pereira. 2009. Sparsity in grammar induction. In IIPS: Grammar Induction, Representation of Language and Language Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Gillenwater</author>
<author>K Ganchev</author>
<author>J Grac¸a</author>
<author>F Pereira</author>
<author>B Taskar</author>
</authors>
<title>Posterior sparsity in unsupervised dependency parsing.</title>
<date>2010</date>
<tech>Technical report,</tech>
<institution>University of Pennsylvania.</institution>
<marker>Gillenwater, Ganchev, Grac¸a, Pereira, Taskar, 2010</marker>
<rawString>J. Gillenwater, K. Ganchev, J. Grac¸a, F. Pereira, and B. Taskar. 2010. Posterior sparsity in unsupervised dependency parsing. Technical report, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Gimpel</author>
<author>N A Smith</author>
</authors>
<title>Concavity and initialization for unsupervised dependency grammar induction.</title>
<date>2011</date>
<tech>Technical report, CMU.</tech>
<contexts>
<context position="37962" citStr="Gimpel and Smith, 2011" startWordPosition="6171" endWordPosition="6174">ion of scores for DBM-1 through 3 without using punctuation constraints in inference — 40.7, 41.7 and 42.2% — fell entirely above this previous state-of-the-art result as well; the DMV baseline — also trained on sentences without internal but with final punctuation — averaged 33.6%. 8.2 Monolingual POS-Identified Inducers The second class of techniques assumes knowledge about identities of part-of-speech tags (Naseem et al., 2010), i.e., which word tokens are verbs, which ones are nouns, etc. Such grammar inducers generally do better than the first kind — e.g., by encouraging verbocentricity (Gimpel and Smith, 2011) — though even here our results appear to be competitive. In fact, to our surprise, only in 5 of 19 languages a “POS-identified” system performed better than all of the “POS-agnostic” ones (see Table 8). 8.3 Multi-Lingual Semi-Supervised Parsers The final broad class of related algorithms we considered extends beyond monolingual data and uses 11For Turkish ’06, the “right-attach” baseline outperforms even the DMV, at 65.4% (Rasooli and Faili, 2012, Table 1); an important difference between 2006 and 2007 CoNLL data sets has to do with segmentation of morphologically-rich languages. 696 both ide</context>
</contexts>
<marker>Gimpel, Smith, 2011</marker>
<rawString>K. Gimpel and N. A. Smith. 2011. Concavity and initialization for unsupervised dependency grammar induction. Technical report, CMU.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C H¨anig</author>
</authors>
<title>Improvements in unsupervised co-occurrence based parsing.</title>
<date>2010</date>
<booktitle>In CoILL.</booktitle>
<marker>H¨anig, 2010</marker>
<rawString>C. H¨anig. 2010. Improvements in unsupervised co-occurrence based parsing. In CoILL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W P Headden</author>
<author>M Johnson</author>
<author>D McClosky</author>
</authors>
<title>Improving unsupervised dependency parsing with richer contexts and smoothing.</title>
<date>2009</date>
<booktitle>In IAACL-HLT.</booktitle>
<contexts>
<context position="5286" citStr="Headden et al., 2009" startWordPosition="806" endWordPosition="809">identity of the parent ch and possibly other parameters (again represented by dots). The child then generates its own subtree recursively and the whole process continues, moving away from the head, until ch fails to generate a left-dependent. At that point, an analogous procedure is repeated to ch’s right, this time using stopping factors PSTOP( · |x; · · · ). All parse trees derived in this way are guaranteed to be projective and can be described by split-head grammars. Instances of these split-head automata have been heavily used in grammar induction (Paskin, 2001b; Klein and Manning, 2004; Headden et al., 2009, inter alia), in part because they allow for efficient implementations (Eisner and Satta, 1999, §8) of the inside-outside re-estimation algorithm (Baker, 1979). The basic tenet of split-head grammars is that every head word generates its left-dependents independently of its right-dependents. This assumption implies, for instance, that words’ leftand right-valences — their numbers of children to each side — are also independent. But it does not imply that descendants that are closer to the head cannot influence the generation of farther dependents on the same side. Nevertheless, many popular g</context>
<context position="13291" citStr="Headden et al., 2009" startWordPosition="2131" endWordPosition="2134">sharing their backbone split-head process. 3 Experimental Set-Up and Methodology We first motivate each model by analyzing the Wall Street Journal (WSJ) portion of the Penn English Treebank (Marcus et al., 1993),3 before delving into 3We converted labeled constituents into unlabeled dependencies using deterministic “head-percolation” rules (Collins, × PSTOP(⋄ |L; F) � �� � 1 � 690 Split-Head Dependency Grammar PATTACH (head-root) PATTACH (dependent-head) PSTOP (adjacent and not) GB (Paskin, 2001b) 1 / |M |d |h; dir 1 / 2 DMV (Klein and Manning, 2004) c,. |o; L cd |ch; dir · |dir; adj, ch EVG (Headden et al., 2009) c,. |o; L cd |ch; dir, adj · |dir; adj, ch DBM-1 (§2.1) c,. |o; L cd |ch; dir · |dir; adj, ce DBM-2 (§2.2) c,. |o; L, comp cd |ch; dir · |dir; adj, ce, comp DBM-3 (§2.3) c,. |o; L, comp cd |ch; dir, cross · |dir; adj, ce, comp Table 1: Parameterizations of the split-head-outward generative process used by DBMs and in previous models. grammar induction experiments. Although motivating solely from this treebank biases our discussion towards a very specific genre of just one language, it has the advantage of allowing us to make concrete claims that are backed up by significant statistics. In the</context>
</contexts>
<marker>Headden, Johnson, McClosky, 2009</marker>
<rawString>W. P. Headden, III, M. Johnson, and D. McClosky. 2009. Improving unsupervised dependency parsing with richer contexts and smoothing. In IAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>C D Manning</author>
</authors>
<title>Corpus-based induction of syntactic structure: Models of dependency and constituency.</title>
<date>2004</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="4289" citStr="Klein and Manning, 2004" startWordPosition="636" endWordPosition="639">on-maximization algorithms. 688 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 688–698, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics 2 The Dependency and Boundary Models Our models follow a standard generative story for head-outward automata (Alshawi, 1996a), restricted to the split-head case (see below),1 over lexical word classes {cw}: first, a sentence root c, is chosen, with probability PATTACS(c, |o; L); o is a special start symbol that, by convention (Klein and Manning, 2004; Eisner, 1996), produces exactly one child, to its left. Next, the process recurses. Each (head) word ch generates a left-dependent with probability 1 − PSTOP( · |L; · · · ), where dots represent additional parameterization on which it may be conditioned. If the child is indeed generated, its identity cd is chosen with probability PATTACS(cd |ch; · · · ), influenced by the identity of the parent ch and possibly other parameters (again represented by dots). The child then generates its own subtree recursively and the whole process continues, moving away from the head, until ch fails to generat</context>
<context position="9872" citStr="Klein and Manning, 2004" startWordPosition="1602" endWordPosition="1605">Figure 2: Our running example — a simple sentence and its unlabeled dependency parse structure’s probability, as factored by DBM-1; highlighted comments specify heads associated to non-adjacent stopping probability factors. this outcome is captured by squaring a shared parameter belonging to the left-fringe determiner The: PSTOP( · |L; F, DT)2 — instead of by a product of two factors, such as PSTOP( · |L; F, NN) · PSTOP( · |L; F, VBZ). In these grammars, dependents’ attachment probabilities, given heads, are additionally conditioned only on their relative positions — as in traditional models (Klein and Manning, 2004; Paskin, 2001b): PATTACH(cd |ch; dir). Figure 2 shows a completely factored example. 2.2 Dependency and Boundary Model Two DBM-2 allows different but related grammars to coexist in a single model. Specifically, we presuppose that all sentences are assigned to one of two classes: complete and incomplete (comp ∈ {T, F}, for now taken as exogenous). This model assumes that wordword (i.e., head-dependent) interactions in the two domains are the same. However, sentence lengths — for which stopping probabilities are responsible — and distributions of root words may be different. Consequently, an ad</context>
<context position="13226" citStr="Klein and Manning, 2004" startWordPosition="2116" endWordPosition="2119">at have since been used by unsupervised dependency grammar inducers sharing their backbone split-head process. 3 Experimental Set-Up and Methodology We first motivate each model by analyzing the Wall Street Journal (WSJ) portion of the Penn English Treebank (Marcus et al., 1993),3 before delving into 3We converted labeled constituents into unlabeled dependencies using deterministic “head-percolation” rules (Collins, × PSTOP(⋄ |L; F) � �� � 1 � 690 Split-Head Dependency Grammar PATTACH (head-root) PATTACH (dependent-head) PSTOP (adjacent and not) GB (Paskin, 2001b) 1 / |M |d |h; dir 1 / 2 DMV (Klein and Manning, 2004) c,. |o; L cd |ch; dir · |dir; adj, ch EVG (Headden et al., 2009) c,. |o; L cd |ch; dir, adj · |dir; adj, ch DBM-1 (§2.1) c,. |o; L cd |ch; dir · |dir; adj, ce DBM-2 (§2.2) c,. |o; L, comp cd |ch; dir · |dir; adj, ce, comp DBM-3 (§2.3) c,. |o; L, comp cd |ch; dir, cross · |dir; adj, ce, comp Table 1: Parameterizations of the split-head-outward generative process used by DBMs and in previous models. grammar induction experiments. Although motivating solely from this treebank biases our discussion towards a very specific genre of just one language, it has the advantage of allowing us to make con</context>
</contexts>
<marker>Klein, Manning, 2004</marker>
<rawString>D. Klein and C. D. Manning. 2004. Corpus-based induction of syntactic structure: Models of dependency and constituency. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K A Krueger</author>
<author>P Dayan</author>
</authors>
<title>Flexible shaping: How learning in small steps helps.</title>
<date>2009</date>
<journal>Cognition,</journal>
<volume>110</volume>
<contexts>
<context position="28255" citStr="Krueger and Dayan, 2009" startWordPosition="4596" endWordPosition="4599">urriculum Strategy for DBMs We propose to train up to DBM-3 iteratively — by beginning with DBM-1 and gradually increasing model complexity through DBM-2, drawing on the intuitions of IBM translation models 1–4 (Brown et al., 1993). Instead of using sentences of up to 15 tokens, as in all previous experiments (§4–5), we will now make use of nearly all available training data: up to length 45 (out of concern for efficiency), during later stages. In the first stage, however, we will use only a subset of the data with DBM-1, in a process sometimes called curriculum learning (Bengio et al., 2009; Krueger and Dayan, 2009, inter alia). Our grammar inducers will thus be “starting small” in both senses suggested by Elman (1993): simultaneously scaffolding on model- and data-complexity. 7.1 Scaffolding Stage #1: DBM-1 We begin by training DBM-1 on sentences without sentence-internal punctuation but with at least one trailing punctuation mark. Our goal is to avoid, when possible, overly specific arbitrary parameters like the “15 tokens or less” threshold used to select training sentences. Unlike DBM-2 and 3, DBM-1 does not model punctuation or sentence fragments, so we instead explicitly restrict its attention to </context>
</contexts>
<marker>Krueger, Dayan, 2009</marker>
<rawString>K. A. Krueger and P. Dayan. 2009. Flexible shaping: How learning in small steps helps. Cognition, 110.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M P Marcus</author>
<author>B Santorini</author>
<author>M A Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<contexts>
<context position="12881" citStr="Marcus et al., 1993" startWordPosition="2062" endWordPosition="2065">2. 2.4 Summary of DBMs and Related Models Head-outward automata (Alshawi, 1996a; Alshawi, 1996b; Alshawi et al., 2000) played a central part as generative models for probabilistic grammars, starting with their early adoption in supervised split-head constituent parsers (Collins, 1997; Collins, 2003). Table 1 lists some parameterizations that have since been used by unsupervised dependency grammar inducers sharing their backbone split-head process. 3 Experimental Set-Up and Methodology We first motivate each model by analyzing the Wall Street Journal (WSJ) portion of the Penn English Treebank (Marcus et al., 1993),3 before delving into 3We converted labeled constituents into unlabeled dependencies using deterministic “head-percolation” rules (Collins, × PSTOP(⋄ |L; F) � �� � 1 � 690 Split-Head Dependency Grammar PATTACH (head-root) PATTACH (dependent-head) PSTOP (adjacent and not) GB (Paskin, 2001b) 1 / |M |d |h; dir 1 / 2 DMV (Klein and Manning, 2004) c,. |o; L cd |ch; dir · |dir; adj, ch EVG (Headden et al., 2009) c,. |o; L cd |ch; dir, adj · |dir; adj, ch DBM-1 (§2.1) c,. |o; L cd |ch; dir · |dir; adj, ce DBM-2 (§2.2) c,. |o; L, comp cd |ch; dir · |dir; adj, ce, comp DBM-3 (§2.3) c,. |o; L, comp cd </context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Mareˇcek</author>
<author>Z Zabokrtsk´y</author>
</authors>
<title>Gibbs sampling with treeness constraint in unsupervised dependency parsing.</title>
<date>2011</date>
<booktitle>In ROBUS.</booktitle>
<marker>Mareˇcek, Zabokrtsk´y, 2011</marker>
<rawString>D. Mareˇcek and Z. Zabokrtsk´y. 2011. Gibbs sampling with treeness constraint in unsupervised dependency parsing. In ROBUS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D McClosky</author>
</authors>
<title>Modeling valence effects in unsupervised grammar induction.</title>
<date>2008</date>
<tech>Technical report,</tech>
<institution>Brown University.</institution>
<contexts>
<context position="6949" citStr="McClosky, 2008" startWordPosition="1067" endWordPosition="1068"> side before the other. mented as both head-outward and head-inward automata. (In fact, arbitrary permutations of siblings to a given side of their parent would not affect the likelihood of the modified tree, with the DMV.) We propose to make fuller use of split-head automata’s head-outward nature by drawing on information in partially-generated parses, which contain useful predictors that, until now, had not been exploited even in featurized systems for grammar induction (Cohen and Smith, 2009; Berg-Kirkpatrick et al., 2010). Some of these predictors, including the identity — or even number (McClosky, 2008) — of alreadygenerated siblings, can be prohibitively expensive in sentences above a short length k. For example, they break certain modularity constraints imposed by the charts used in O(k3)-optimized algorithms (Paskin, 2001a; Eisner, 2000). However, in bottom-up parsing and training from text, everything about the yield — i.e., the ordered sequence of all already-generated descendants, on the side of the head that is in the process of spawning off an additional child — is not only known but also readily accessible. Taking advantage of this availability, we designed three new models for depe</context>
</contexts>
<marker>McClosky, 2008</marker>
<rawString>D. McClosky. 2008. Modeling valence effects in unsupervised grammar induction. Technical report, Brown University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>S Petrov</author>
<author>K Hall</author>
</authors>
<title>Multi-source transfer of delexicalized dependency parsers.</title>
<date>2011</date>
<booktitle>In EMILP.</booktitle>
<contexts>
<context position="32066" citStr="McDonald et al., 2011" startWordPosition="5242" endWordPosition="5245"> RFH1 48.8 SCAJ6 — Table 8: Average accuracies over CoNLL evaluation sets (all sentences), for the DMV baseline and DBM1–3 trained with a curriculum strategy, and state-of-the-art results for systems that: (i) are also POS-agnostic and monolingual, including SCAJ (Spitkovsky et al., 2011a, Tables 5–6) and SAJ (Spitkovsky et al., 2011b); (ii) rely on gold POS-tag identities to discourage noun roots (Mareˇcek and Zabokrtsk´y, 2011, MZ) or to encourage verbs (Rasooli and Faili, 2012, RF); and (iii) transfer delexicalized parsers (Søgaard, 2011a, S) from resource-rich languages with translations (McDonald et al., 2011, MPH). DMV and DBM-1 trained on simple sentences, from uniform; DBM-2 and 3 trained on most sentences, from DBM-1 and 2, respectively; +inference is DBM-3 with punctuation constraints. 7.2 Scaffolding Stage #2: DBM-2 ← DBM-1 Next, we trained on all sentences up to length 45. Since these inputs are punctuation-rich, in both remaining stages we used the constrained Viterbi EM set-up suggested by Spitkovsky et al. (2011b) instead of plain soft EM; we employ an early termination strategy, quitting hard EM as soon as soft EM’s objective suffers (Spitkovsky et al., 2011a). Punctuation was converted</context>
</contexts>
<marker>McDonald, Petrov, Hall, 2011</marker>
<rawString>R. McDonald, S. Petrov, and K. Hall. 2011. Multi-source transfer of delexicalized dependency parsers. In EMILP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Naseem</author>
<author>H Chen</author>
<author>R Barzilay</author>
<author>M Johnson</author>
</authors>
<title>Using universal linguistic knowledge to guide grammar induction.</title>
<date>2010</date>
<booktitle>In EMILP.</booktitle>
<contexts>
<context position="35809" citStr="Naseem et al., 2010" startWordPosition="5821" endWordPosition="5824">on); subsequent stages initialize additional pieces uniformly: uniform-at-random parses for new data and uniform multinomials for new parameters. Even without curriculum learning — trained with vanilla EM — DBM-2 and 1 are already strong. Further boosts to accuracy could come from employing more sophisticated optimization algorithms, e.g., better EM (Samdani et al., 2012), constrained Gibbs sampling (Mareˇcek and Zabokrtsk´y, 2011) or locally-normalized features (Berg-Kirkpatrick et al., 2010). Other orthogonal dependency grammar induction techniques — including ones based on universal rules (Naseem et al., 2010) — may also benefit in combination with DBMs. Direct comparisons to previous work require some care, however, as there are several classes of systems that make different assumptions about training data (see Table 8). 10Note that DBM-1’s 39% average accuracy with standard training (see Table 2) was already nearly a full point higher than that of any single previous best system (SCAJ6 — see Table 8). 8.1 Monolingual POS-Agnostic Inducers The first type of grammar inducers, including our own approach, uses standard training and test data sets for each language, with gold part-of-speech tags as an</context>
<context position="37773" citStr="Naseem et al., 2010" startWordPosition="6139" endWordPosition="6142">ment to accuracies. Overall, the final scores of DBM-3 were better, on average, than those of any other single system: 42.9 versus 38.2% (Spitkovsky et al., 2011a, Table 6). The progression of scores for DBM-1 through 3 without using punctuation constraints in inference — 40.7, 41.7 and 42.2% — fell entirely above this previous state-of-the-art result as well; the DMV baseline — also trained on sentences without internal but with final punctuation — averaged 33.6%. 8.2 Monolingual POS-Identified Inducers The second class of techniques assumes knowledge about identities of part-of-speech tags (Naseem et al., 2010), i.e., which word tokens are verbs, which ones are nouns, etc. Such grammar inducers generally do better than the first kind — e.g., by encouraging verbocentricity (Gimpel and Smith, 2011) — though even here our results appear to be competitive. In fact, to our surprise, only in 5 of 19 languages a “POS-identified” system performed better than all of the “POS-agnostic” ones (see Table 8). 8.3 Multi-Lingual Semi-Supervised Parsers The final broad class of related algorithms we considered extends beyond monolingual data and uses 11For Turkish ’06, the “right-attach” baseline outperforms even th</context>
<context position="39595" citStr="Naseem et al., 2010" startWordPosition="6427" endWordPosition="6430">The unsupervised system that performed better used a special “weighted” initializer (Spitkovsky et al., 2011b, §3.1) that worked well for English (but less so for many other languages). DBMs may be able to improve initialization. For example, modeling of incomplete sentences could help in incremental initialization strategies like baby steps (Spitkovsky et al., 2009), which are likely sensitive to the proverbial “bum steer” from unrepresentative short fragments, pace Tu and Honavar (2011). 8.4 Miscellaneous Systems on Short Sentences Several recent systems (Cohen et al., 2011; Søgaard, 2011b; Naseem et al., 2010; Gillenwater et al., 2010; Berg-Kirkpatrick and Klein, 2010, inter alia) are absent from Table 8 because they do not report performance for all sentence lengths. To facilitate comparison with this body of important previous work, we also tabulated final accuracies for the “up-to-ten words” task under heading @10: 51.9%, on average. 9 Conclusion Although a dependency parse for a sentence can be mapped to a constituency parse (Xia and Palmer, 2001), the probabilistic models generating them use different conditioning: dependency grammars focus on the relationship between arguments and heads, con</context>
</contexts>
<marker>Naseem, Chen, Barzilay, Johnson, 2010</marker>
<rawString>T. Naseem, H. Chen, R. Barzilay, and M. Johnson. 2010. Using universal linguistic knowledge to guide grammar induction. In EMILP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M S Nikulin</author>
</authors>
<date>2002</date>
<booktitle>Encyclopaedia of Mathematics.</booktitle>
<editor>Hellinger distance. In M. Hazewinkel, editor,</editor>
<publisher>Kluwer Academic Publishers.</publisher>
<contexts>
<context position="20651" citStr="Nikulin, 2002" startWordPosition="3323" endWordPosition="3324">0.42 0.20 0.56 0.42 0.01 0.78 0.02 0.27 0.06 0.20 0.54 0.75 0.00 0.06 0.01 0.08 0.00 0.01 0.05 0.00 0.00 0.08 0.05 0.11 0.01 0.09 0.00 Table 4: Empirical distributions for non-punctuationpartof-speech tags in WSJ, ordered by overall frequency, as well as distributions for sentence boundaries and for the roots of complete and incomplete sentences. (A uniform distribution would have 1/36 = 2.7% for all POS-tags.) Table 5: A distance matrix for all pairs of probability distributions over POS-tags shown in Table 4 and the uniform distribution; the BC- (or Hellinger) distance (Bhattacharyya, 1943; Nikulin, 2002) between discrete distributions p and q (over x E X) ranges from zero (iff p = q) to one (iff p · q = 0, i.e., when they do not overlap at all). 4.31 36.67 20.49 12.85 23.34 0.34 13.54 0.57 4.49 20.64 1.29 6.92 5.96 3.88 0.09 3.52 0.44 1.67 5.93 0.00 0.37 0.05 0.17 1.65 0.61 2.57 9.04 1.34 V/1 − � �pxqx x All First Last Sent. Frag. Uniform Sent. First Last All 0.48 0.58 0.64 0.35 0.40 0.59 0.79 0.65 0.79 0.42 0.94 0.57 0.83 0.29 0.86 POS % ofAll Tokens NN 15.94 IN 11.85 NNP 11.09 DT 9.84 JJ 7.32 NNS 7.19 CD 4.37 RB 3.71 VBD 3.65 VB 3.17 CC 2.86 TO 2.67 VBZ 2.57 VBN 2.42 PRP 2.08 VBG 1.77 VBP 1</context>
</contexts>
<marker>Nikulin, 2002</marker>
<rawString>M. S. Nikulin. 2002. Hellinger distance. In M. Hazewinkel, editor, Encyclopaedia of Mathematics. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
<author>J Hall</author>
<author>S K¨ubler</author>
<author>R McDonald</author>
<author>J Nilsson</author>
<author>S Riedel</author>
<author>D Yuret</author>
</authors>
<title>The CoNLL</title>
<date>2007</date>
<booktitle>In EMILP-CoILL.</booktitle>
<marker>Nivre, Hall, K¨ubler, McDonald, Nilsson, Riedel, Yuret, 2007</marker>
<rawString>J. Nivre, J. Hall, S. K¨ubler, R. McDonald, J. Nilsson, S. Riedel, and D. Yuret. 2007. The CoNLL 2007 shared task on dependency parsing. In EMILP-CoILL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Paskin</author>
</authors>
<title>Cubic-time parsing and learning algorithms for grammatical bigram models.</title>
<date>2001</date>
<tech>Technical report, UCB.</tech>
<contexts>
<context position="5238" citStr="Paskin, 2001" startWordPosition="800" endWordPosition="801">TACS(cd |ch; · · · ), influenced by the identity of the parent ch and possibly other parameters (again represented by dots). The child then generates its own subtree recursively and the whole process continues, moving away from the head, until ch fails to generate a left-dependent. At that point, an analogous procedure is repeated to ch’s right, this time using stopping factors PSTOP( · |x; · · · ). All parse trees derived in this way are guaranteed to be projective and can be described by split-head grammars. Instances of these split-head automata have been heavily used in grammar induction (Paskin, 2001b; Klein and Manning, 2004; Headden et al., 2009, inter alia), in part because they allow for efficient implementations (Eisner and Satta, 1999, §8) of the inside-outside re-estimation algorithm (Baker, 1979). The basic tenet of split-head grammars is that every head word generates its left-dependents independently of its right-dependents. This assumption implies, for instance, that words’ leftand right-valences — their numbers of children to each side — are also independent. But it does not imply that descendants that are closer to the head cannot influence the generation of farther dependent</context>
<context position="7175" citStr="Paskin, 2001" startWordPosition="1100" endWordPosition="1101">opose to make fuller use of split-head automata’s head-outward nature by drawing on information in partially-generated parses, which contain useful predictors that, until now, had not been exploited even in featurized systems for grammar induction (Cohen and Smith, 2009; Berg-Kirkpatrick et al., 2010). Some of these predictors, including the identity — or even number (McClosky, 2008) — of alreadygenerated siblings, can be prohibitively expensive in sentences above a short length k. For example, they break certain modularity constraints imposed by the charts used in O(k3)-optimized algorithms (Paskin, 2001a; Eisner, 2000). However, in bottom-up parsing and training from text, everything about the yield — i.e., the ordered sequence of all already-generated descendants, on the side of the head that is in the process of spawning off an additional child — is not only known but also readily accessible. Taking advantage of this availability, we designed three new models for dependency grammar induction. 2.1 Dependency and Boundary Model One DBM-1 conditions all stopping decisions on adjacency and the identity of the fringe word ce — the currently-farthest descendant (edge) derived by head ch in the g</context>
<context position="9886" citStr="Paskin, 2001" startWordPosition="1606" endWordPosition="1607">mple — a simple sentence and its unlabeled dependency parse structure’s probability, as factored by DBM-1; highlighted comments specify heads associated to non-adjacent stopping probability factors. this outcome is captured by squaring a shared parameter belonging to the left-fringe determiner The: PSTOP( · |L; F, DT)2 — instead of by a product of two factors, such as PSTOP( · |L; F, NN) · PSTOP( · |L; F, VBZ). In these grammars, dependents’ attachment probabilities, given heads, are additionally conditioned only on their relative positions — as in traditional models (Klein and Manning, 2004; Paskin, 2001b): PATTACH(cd |ch; dir). Figure 2 shows a completely factored example. 2.2 Dependency and Boundary Model Two DBM-2 allows different but related grammars to coexist in a single model. Specifically, we presuppose that all sentences are assigned to one of two classes: complete and incomplete (comp ∈ {T, F}, for now taken as exogenous). This model assumes that wordword (i.e., head-dependent) interactions in the two domains are the same. However, sentence lengths — for which stopping probabilities are responsible — and distributions of root words may be different. Consequently, an additional comp </context>
<context position="13170" citStr="Paskin, 2001" startWordPosition="2104" endWordPosition="2105">03). Table 1 lists some parameterizations that have since been used by unsupervised dependency grammar inducers sharing their backbone split-head process. 3 Experimental Set-Up and Methodology We first motivate each model by analyzing the Wall Street Journal (WSJ) portion of the Penn English Treebank (Marcus et al., 1993),3 before delving into 3We converted labeled constituents into unlabeled dependencies using deterministic “head-percolation” rules (Collins, × PSTOP(⋄ |L; F) � �� � 1 � 690 Split-Head Dependency Grammar PATTACH (head-root) PATTACH (dependent-head) PSTOP (adjacent and not) GB (Paskin, 2001b) 1 / |M |d |h; dir 1 / 2 DMV (Klein and Manning, 2004) c,. |o; L cd |ch; dir · |dir; adj, ch EVG (Headden et al., 2009) c,. |o; L cd |ch; dir, adj · |dir; adj, ch DBM-1 (§2.1) c,. |o; L cd |ch; dir · |dir; adj, ce DBM-2 (§2.2) c,. |o; L, comp cd |ch; dir · |dir; adj, ce, comp DBM-3 (§2.3) c,. |o; L, comp cd |ch; dir, cross · |dir; adj, ce, comp Table 1: Parameterizations of the split-head-outward generative process used by DBMs and in previous models. grammar induction experiments. Although motivating solely from this treebank biases our discussion towards a very specific genre of just one l</context>
</contexts>
<marker>Paskin, 2001</marker>
<rawString>M. A. Paskin. 2001a. Cubic-time parsing and learning algorithms for grammatical bigram models. Technical report, UCB.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Paskin</author>
</authors>
<title>Grammatical bigrams. In</title>
<date>2001</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="5238" citStr="Paskin, 2001" startWordPosition="800" endWordPosition="801">TACS(cd |ch; · · · ), influenced by the identity of the parent ch and possibly other parameters (again represented by dots). The child then generates its own subtree recursively and the whole process continues, moving away from the head, until ch fails to generate a left-dependent. At that point, an analogous procedure is repeated to ch’s right, this time using stopping factors PSTOP( · |x; · · · ). All parse trees derived in this way are guaranteed to be projective and can be described by split-head grammars. Instances of these split-head automata have been heavily used in grammar induction (Paskin, 2001b; Klein and Manning, 2004; Headden et al., 2009, inter alia), in part because they allow for efficient implementations (Eisner and Satta, 1999, §8) of the inside-outside re-estimation algorithm (Baker, 1979). The basic tenet of split-head grammars is that every head word generates its left-dependents independently of its right-dependents. This assumption implies, for instance, that words’ leftand right-valences — their numbers of children to each side — are also independent. But it does not imply that descendants that are closer to the head cannot influence the generation of farther dependent</context>
<context position="7175" citStr="Paskin, 2001" startWordPosition="1100" endWordPosition="1101">opose to make fuller use of split-head automata’s head-outward nature by drawing on information in partially-generated parses, which contain useful predictors that, until now, had not been exploited even in featurized systems for grammar induction (Cohen and Smith, 2009; Berg-Kirkpatrick et al., 2010). Some of these predictors, including the identity — or even number (McClosky, 2008) — of alreadygenerated siblings, can be prohibitively expensive in sentences above a short length k. For example, they break certain modularity constraints imposed by the charts used in O(k3)-optimized algorithms (Paskin, 2001a; Eisner, 2000). However, in bottom-up parsing and training from text, everything about the yield — i.e., the ordered sequence of all already-generated descendants, on the side of the head that is in the process of spawning off an additional child — is not only known but also readily accessible. Taking advantage of this availability, we designed three new models for dependency grammar induction. 2.1 Dependency and Boundary Model One DBM-1 conditions all stopping decisions on adjacency and the identity of the fringe word ce — the currently-farthest descendant (edge) derived by head ch in the g</context>
<context position="9886" citStr="Paskin, 2001" startWordPosition="1606" endWordPosition="1607">mple — a simple sentence and its unlabeled dependency parse structure’s probability, as factored by DBM-1; highlighted comments specify heads associated to non-adjacent stopping probability factors. this outcome is captured by squaring a shared parameter belonging to the left-fringe determiner The: PSTOP( · |L; F, DT)2 — instead of by a product of two factors, such as PSTOP( · |L; F, NN) · PSTOP( · |L; F, VBZ). In these grammars, dependents’ attachment probabilities, given heads, are additionally conditioned only on their relative positions — as in traditional models (Klein and Manning, 2004; Paskin, 2001b): PATTACH(cd |ch; dir). Figure 2 shows a completely factored example. 2.2 Dependency and Boundary Model Two DBM-2 allows different but related grammars to coexist in a single model. Specifically, we presuppose that all sentences are assigned to one of two classes: complete and incomplete (comp ∈ {T, F}, for now taken as exogenous). This model assumes that wordword (i.e., head-dependent) interactions in the two domains are the same. However, sentence lengths — for which stopping probabilities are responsible — and distributions of root words may be different. Consequently, an additional comp </context>
<context position="13170" citStr="Paskin, 2001" startWordPosition="2104" endWordPosition="2105">03). Table 1 lists some parameterizations that have since been used by unsupervised dependency grammar inducers sharing their backbone split-head process. 3 Experimental Set-Up and Methodology We first motivate each model by analyzing the Wall Street Journal (WSJ) portion of the Penn English Treebank (Marcus et al., 1993),3 before delving into 3We converted labeled constituents into unlabeled dependencies using deterministic “head-percolation” rules (Collins, × PSTOP(⋄ |L; F) � �� � 1 � 690 Split-Head Dependency Grammar PATTACH (head-root) PATTACH (dependent-head) PSTOP (adjacent and not) GB (Paskin, 2001b) 1 / |M |d |h; dir 1 / 2 DMV (Klein and Manning, 2004) c,. |o; L cd |ch; dir · |dir; adj, ch EVG (Headden et al., 2009) c,. |o; L cd |ch; dir, adj · |dir; adj, ch DBM-1 (§2.1) c,. |o; L cd |ch; dir · |dir; adj, ce DBM-2 (§2.2) c,. |o; L, comp cd |ch; dir · |dir; adj, ce, comp DBM-3 (§2.3) c,. |o; L, comp cd |ch; dir, cross · |dir; adj, ce, comp Table 1: Parameterizations of the split-head-outward generative process used by DBMs and in previous models. grammar induction experiments. Although motivating solely from this treebank biases our discussion towards a very specific genre of just one l</context>
</contexts>
<marker>Paskin, 2001</marker>
<rawString>M. A. Paskin. 2001b. Grammatical bigrams. In IIPS. F. Pereira and Y. Schabes. 1992. Inside-outside reestimation from partially bracketed corpora. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Ponvert</author>
<author>J Baldridge</author>
<author>K Erk</author>
</authors>
<title>Simple unsupervised identification of low-level constituents.</title>
<date>2010</date>
<booktitle>In ICSC.</booktitle>
<contexts>
<context position="1571" citStr="Ponvert et al., 2010" startWordPosition="215" endWordPosition="218">s between complete and incomplete sentences. (iii) Sentence-internal punctuation boundaries help with longer-distance dependencies, since punctuation correlates with constituent edges. Our models induce state-of-the-art dependency grammars for many languages without special knowledge of optimal input sentence lengths or biased, manually-tuned initializers. 1 Introduction Natural language is ripe with all manner of boundaries at the surface level that align with hierarchical syntactic structure. From the significance of function words (Berant et al., 2006) and punctuation marks (Seginer, 2007; Ponvert et al., 2010) as separators between constituents in longer sentences — to the importance of isolated words in children’s early vocabulary acquisition (Brent and Siskind, 2001) — word boundaries play a crucial role in language learning. We will show that boundary information can also be useful in dependency grammar induction models, which traditionally focus on head rather than fringe words (Carroll and Charniak, 1992). � �� � � �� � Subject Object Figure 1: A partial analysis of our running example. Consider the example in Figure 1. Because the determiner (DT) appears at the left edge of the sentence, it s</context>
</contexts>
<marker>Ponvert, Baldridge, Erk, 2010</marker>
<rawString>E. Ponvert, J. Baldridge, and K. Erk. 2010. Simple unsupervised identification of low-level constituents. In ICSC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M S Rasooli</author>
<author>H Faili</author>
</authors>
<title>Fast unsupervised dependency parsing with arc-standard transitions.</title>
<date>2012</date>
<booktitle>In ROBUSUISUP.</booktitle>
<contexts>
<context position="31929" citStr="Rasooli and Faili, 2012" startWordPosition="5223" endWordPosition="5226">MZNR 57.5 MZ 60.6 MZNR 56.6 SCAJ5 53.5 MZNR 43.2 MZ 55.8 RFH1&amp;2 33.6 SCAJ5 34.6 MZNR 53.0 MZ 54.6 MZNR 50.0 SCAJ6 34.3 RFH1&amp;2 40.9 SAJ 61.3 RFH1 48.8 SCAJ6 — Table 8: Average accuracies over CoNLL evaluation sets (all sentences), for the DMV baseline and DBM1–3 trained with a curriculum strategy, and state-of-the-art results for systems that: (i) are also POS-agnostic and monolingual, including SCAJ (Spitkovsky et al., 2011a, Tables 5–6) and SAJ (Spitkovsky et al., 2011b); (ii) rely on gold POS-tag identities to discourage noun roots (Mareˇcek and Zabokrtsk´y, 2011, MZ) or to encourage verbs (Rasooli and Faili, 2012, RF); and (iii) transfer delexicalized parsers (Søgaard, 2011a, S) from resource-rich languages with translations (McDonald et al., 2011, MPH). DMV and DBM-1 trained on simple sentences, from uniform; DBM-2 and 3 trained on most sentences, from DBM-1 and 2, respectively; +inference is DBM-3 with punctuation constraints. 7.2 Scaffolding Stage #2: DBM-2 ← DBM-1 Next, we trained on all sentences up to length 45. Since these inputs are punctuation-rich, in both remaining stages we used the constrained Viterbi EM set-up suggested by Spitkovsky et al. (2011b) instead of plain soft EM; we employ an </context>
<context position="38413" citStr="Rasooli and Faili, 2012" startWordPosition="6244" endWordPosition="6247">d tokens are verbs, which ones are nouns, etc. Such grammar inducers generally do better than the first kind — e.g., by encouraging verbocentricity (Gimpel and Smith, 2011) — though even here our results appear to be competitive. In fact, to our surprise, only in 5 of 19 languages a “POS-identified” system performed better than all of the “POS-agnostic” ones (see Table 8). 8.3 Multi-Lingual Semi-Supervised Parsers The final broad class of related algorithms we considered extends beyond monolingual data and uses 11For Turkish ’06, the “right-attach” baseline outperforms even the DMV, at 65.4% (Rasooli and Faili, 2012, Table 1); an important difference between 2006 and 2007 CoNLL data sets has to do with segmentation of morphologically-rich languages. 696 both identities of POS-tags and/or parallel bitexts to transfer (supervised) delexicalized parsers across languages. Parser projection is by far the most successful approach to date and we hope that it too may stand to gain from our modeling improvements. Of the 10 languages for which we found results in the literature, transferred parsers underperformed the grammar inducers in only one case: on English (see Table 8). The unsupervised system that performe</context>
</contexts>
<marker>Rasooli, Faili, 2012</marker>
<rawString>M. S. Rasooli and H. Faili. 2012. Fast unsupervised dependency parsing with arc-standard transitions. In ROBUSUISUP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Samdani</author>
<author>M-W Chang</author>
<author>D Roth</author>
</authors>
<title>Unified expectation maximization.</title>
<date>2012</date>
<booktitle>In IAACL-HLT.</booktitle>
<contexts>
<context position="35563" citStr="Samdani et al., 2012" startWordPosition="5788" endWordPosition="5791">to a predetermined number of tokens. Although this approach adds some complexity, we chose conservatively, to avoid overfitting settings of sentence length, convergence criteria, etc.: stage one’s data is dictated by DBM-1 (which ignores punctuation); subsequent stages initialize additional pieces uniformly: uniform-at-random parses for new data and uniform multinomials for new parameters. Even without curriculum learning — trained with vanilla EM — DBM-2 and 1 are already strong. Further boosts to accuracy could come from employing more sophisticated optimization algorithms, e.g., better EM (Samdani et al., 2012), constrained Gibbs sampling (Mareˇcek and Zabokrtsk´y, 2011) or locally-normalized features (Berg-Kirkpatrick et al., 2010). Other orthogonal dependency grammar induction techniques — including ones based on universal rules (Naseem et al., 2010) — may also benefit in combination with DBMs. Direct comparisons to previous work require some care, however, as there are several classes of systems that make different assumptions about training data (see Table 8). 10Note that DBM-1’s 39% average accuracy with standard training (see Table 2) was already nearly a full point higher than that of any sin</context>
</contexts>
<marker>Samdani, Chang, Roth, 2012</marker>
<rawString>R. Samdani, M.-W. Chang, and D. Roth. 2012. Unified expectation maximization. In IAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Seginer</author>
</authors>
<title>Learning Syntactic Structure.</title>
<date>2007</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Amsterdam.</institution>
<contexts>
<context position="1548" citStr="Seginer, 2007" startWordPosition="213" endWordPosition="214">ical differences between complete and incomplete sentences. (iii) Sentence-internal punctuation boundaries help with longer-distance dependencies, since punctuation correlates with constituent edges. Our models induce state-of-the-art dependency grammars for many languages without special knowledge of optimal input sentence lengths or biased, manually-tuned initializers. 1 Introduction Natural language is ripe with all manner of boundaries at the surface level that align with hierarchical syntactic structure. From the significance of function words (Berant et al., 2006) and punctuation marks (Seginer, 2007; Ponvert et al., 2010) as separators between constituents in longer sentences — to the importance of isolated words in children’s early vocabulary acquisition (Brent and Siskind, 2001) — word boundaries play a crucial role in language learning. We will show that boundary information can also be useful in dependency grammar induction models, which traditionally focus on head rather than fringe words (Carroll and Charniak, 1992). � �� � � �� � Subject Object Figure 1: A partial analysis of our running example. Consider the example in Figure 1. Because the determiner (DT) appears at the left edg</context>
</contexts>
<marker>Seginer, 2007</marker>
<rawString>Y. Seginer. 2007. Learning Syntactic Structure. Ph.D. thesis, University of Amsterdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Søgaard</author>
</authors>
<title>Datapoint selection for cross-language adaptation of dependency parsers.</title>
<date>2011</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="31991" citStr="Søgaard, 2011" startWordPosition="5233" endWordPosition="5234">J5 34.6 MZNR 53.0 MZ 54.6 MZNR 50.0 SCAJ6 34.3 RFH1&amp;2 40.9 SAJ 61.3 RFH1 48.8 SCAJ6 — Table 8: Average accuracies over CoNLL evaluation sets (all sentences), for the DMV baseline and DBM1–3 trained with a curriculum strategy, and state-of-the-art results for systems that: (i) are also POS-agnostic and monolingual, including SCAJ (Spitkovsky et al., 2011a, Tables 5–6) and SAJ (Spitkovsky et al., 2011b); (ii) rely on gold POS-tag identities to discourage noun roots (Mareˇcek and Zabokrtsk´y, 2011, MZ) or to encourage verbs (Rasooli and Faili, 2012, RF); and (iii) transfer delexicalized parsers (Søgaard, 2011a, S) from resource-rich languages with translations (McDonald et al., 2011, MPH). DMV and DBM-1 trained on simple sentences, from uniform; DBM-2 and 3 trained on most sentences, from DBM-1 and 2, respectively; +inference is DBM-3 with punctuation constraints. 7.2 Scaffolding Stage #2: DBM-2 ← DBM-1 Next, we trained on all sentences up to length 45. Since these inputs are punctuation-rich, in both remaining stages we used the constrained Viterbi EM set-up suggested by Spitkovsky et al. (2011b) instead of plain soft EM; we employ an early termination strategy, quitting hard EM as soon as soft E</context>
<context position="39573" citStr="Søgaard, 2011" startWordPosition="6425" endWordPosition="6426"> (see Table 8). The unsupervised system that performed better used a special “weighted” initializer (Spitkovsky et al., 2011b, §3.1) that worked well for English (but less so for many other languages). DBMs may be able to improve initialization. For example, modeling of incomplete sentences could help in incremental initialization strategies like baby steps (Spitkovsky et al., 2009), which are likely sensitive to the proverbial “bum steer” from unrepresentative short fragments, pace Tu and Honavar (2011). 8.4 Miscellaneous Systems on Short Sentences Several recent systems (Cohen et al., 2011; Søgaard, 2011b; Naseem et al., 2010; Gillenwater et al., 2010; Berg-Kirkpatrick and Klein, 2010, inter alia) are absent from Table 8 because they do not report performance for all sentence lengths. To facilitate comparison with this body of important previous work, we also tabulated final accuracies for the “up-to-ten words” task under heading @10: 51.9%, on average. 9 Conclusion Although a dependency parse for a sentence can be mapped to a constituency parse (Xia and Palmer, 2001), the probabilistic models generating them use different conditioning: dependency grammars focus on the relationship between ar</context>
</contexts>
<marker>Søgaard, 2011</marker>
<rawString>A. Søgaard. 2011a. Datapoint selection for cross-language adaptation of dependency parsers. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Søgaard</author>
</authors>
<title>From ranked words to dependency trees: two-stage unsupervised non-projective dependency parsing. In TextGraphs.</title>
<date>2011</date>
<contexts>
<context position="31991" citStr="Søgaard, 2011" startWordPosition="5233" endWordPosition="5234">J5 34.6 MZNR 53.0 MZ 54.6 MZNR 50.0 SCAJ6 34.3 RFH1&amp;2 40.9 SAJ 61.3 RFH1 48.8 SCAJ6 — Table 8: Average accuracies over CoNLL evaluation sets (all sentences), for the DMV baseline and DBM1–3 trained with a curriculum strategy, and state-of-the-art results for systems that: (i) are also POS-agnostic and monolingual, including SCAJ (Spitkovsky et al., 2011a, Tables 5–6) and SAJ (Spitkovsky et al., 2011b); (ii) rely on gold POS-tag identities to discourage noun roots (Mareˇcek and Zabokrtsk´y, 2011, MZ) or to encourage verbs (Rasooli and Faili, 2012, RF); and (iii) transfer delexicalized parsers (Søgaard, 2011a, S) from resource-rich languages with translations (McDonald et al., 2011, MPH). DMV and DBM-1 trained on simple sentences, from uniform; DBM-2 and 3 trained on most sentences, from DBM-1 and 2, respectively; +inference is DBM-3 with punctuation constraints. 7.2 Scaffolding Stage #2: DBM-2 ← DBM-1 Next, we trained on all sentences up to length 45. Since these inputs are punctuation-rich, in both remaining stages we used the constrained Viterbi EM set-up suggested by Spitkovsky et al. (2011b) instead of plain soft EM; we employ an early termination strategy, quitting hard EM as soon as soft E</context>
<context position="39573" citStr="Søgaard, 2011" startWordPosition="6425" endWordPosition="6426"> (see Table 8). The unsupervised system that performed better used a special “weighted” initializer (Spitkovsky et al., 2011b, §3.1) that worked well for English (but less so for many other languages). DBMs may be able to improve initialization. For example, modeling of incomplete sentences could help in incremental initialization strategies like baby steps (Spitkovsky et al., 2009), which are likely sensitive to the proverbial “bum steer” from unrepresentative short fragments, pace Tu and Honavar (2011). 8.4 Miscellaneous Systems on Short Sentences Several recent systems (Cohen et al., 2011; Søgaard, 2011b; Naseem et al., 2010; Gillenwater et al., 2010; Berg-Kirkpatrick and Klein, 2010, inter alia) are absent from Table 8 because they do not report performance for all sentence lengths. To facilitate comparison with this body of important previous work, we also tabulated final accuracies for the “up-to-ten words” task under heading @10: 51.9%, on average. 9 Conclusion Although a dependency parse for a sentence can be mapped to a constituency parse (Xia and Palmer, 2001), the probabilistic models generating them use different conditioning: dependency grammars focus on the relationship between ar</context>
</contexts>
<marker>Søgaard, 2011</marker>
<rawString>A. Søgaard. 2011b. From ranked words to dependency trees: two-stage unsupervised non-projective dependency parsing. In TextGraphs.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V I Spitkovsky</author>
<author>H Alshawi</author>
<author>D Jurafsky</author>
</authors>
<title>Baby Steps: How “Less is More” in unsupervised dependency parsing.</title>
<date>2009</date>
<booktitle>In IIPS: Grammar Induction, Representation of Language and Language Learning.</booktitle>
<contexts>
<context position="14438" citStr="Spitkovsky et al., 2009" startWordPosition="2328" endWordPosition="2331"> to make concrete claims that are backed up by significant statistics. In the grammar induction experiments that follow, we will test each model’s incremental contribution to accuracies empirically, across many disparate languages. We worked with all 23 (disjoint) train/test splits from the 2006/7 CoNLL shared tasks (Buchholz and Marsi, 2006; Nivre et al., 2007), spanning 19 languages.4 For each data set, we induced a baseline grammar using the DMV. We excluded all training sentences with more than 15 tokens to create a conservative bias, because in this set-up the baseline is known to excel (Spitkovsky et al., 2009). Grammar inducers were initialized using (the same) uniformly-at-random chosen parse trees of training sentences (Cohen and Smith, 2010); thereafter, we applied “add one” smoothing at every training step. To fairly compare the models under consideration — which could have quite different starting perplexities and ensuing consecutive relative likelihoods — we experimented with two termination strategies. In one case, we blindly ran each learner through 40 steps of inside-outside re-estimation, ignoring any convergence criteria; in the other case, we ran until numerical convergence of soft EM’s</context>
<context position="39345" citStr="Spitkovsky et al., 2009" startWordPosition="6388" endWordPosition="6391">proach to date and we hope that it too may stand to gain from our modeling improvements. Of the 10 languages for which we found results in the literature, transferred parsers underperformed the grammar inducers in only one case: on English (see Table 8). The unsupervised system that performed better used a special “weighted” initializer (Spitkovsky et al., 2011b, §3.1) that worked well for English (but less so for many other languages). DBMs may be able to improve initialization. For example, modeling of incomplete sentences could help in incremental initialization strategies like baby steps (Spitkovsky et al., 2009), which are likely sensitive to the proverbial “bum steer” from unrepresentative short fragments, pace Tu and Honavar (2011). 8.4 Miscellaneous Systems on Short Sentences Several recent systems (Cohen et al., 2011; Søgaard, 2011b; Naseem et al., 2010; Gillenwater et al., 2010; Berg-Kirkpatrick and Klein, 2010, inter alia) are absent from Table 8 because they do not report performance for all sentence lengths. To facilitate comparison with this body of important previous work, we also tabulated final accuracies for the “up-to-ten words” task under heading @10: 51.9%, on average. 9 Conclusion Al</context>
</contexts>
<marker>Spitkovsky, Alshawi, Jurafsky, 2009</marker>
<rawString>V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2009. Baby Steps: How “Less is More” in unsupervised dependency parsing. In IIPS: Grammar Induction, Representation of Language and Language Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V I Spitkovsky</author>
<author>H Alshawi</author>
<author>D Jurafsky</author>
</authors>
<title>Lateen EM: Unsupervised training with multiple objectives, applied to dependency grammar induction.</title>
<date>2011</date>
<booktitle>In EMILP.</booktitle>
<contexts>
<context position="15189" citStr="Spitkovsky et al., 2011" startWordPosition="2440" endWordPosition="2443">th, 2010); thereafter, we applied “add one” smoothing at every training step. To fairly compare the models under consideration — which could have quite different starting perplexities and ensuing consecutive relative likelihoods — we experimented with two termination strategies. In one case, we blindly ran each learner through 40 steps of inside-outside re-estimation, ignoring any convergence criteria; in the other case, we ran until numerical convergence of soft EM’s objective function or until the likelihood of resulting Viterbi parse trees suffered — an “early-stopping lateen EM” strategy (Spitkovsky et al., 2011a, §2.3). We evaluated against all sentences of the blind test sets (except one 145-token item in Arabic ’07 data). Table 2 shows experimental results, averaged over 1999), discarding any empty nodes, etc., as is standard practice. 4We did not test on WSJ data because such evaluation would not be blind, as parse trees from the PTB are our motivating examples; instead, performance on WSJ serves as a strong baseline in a separate study (Spitkovsky et al., 2012a): bootstrapping of DBMs from mostly incomplete inter-punctuation fragments. all 19 languages, for the DMV baselines and DBM-1 and 2. We </context>
<context position="19914" citStr="Spitkovsky et al., 2011" startWordPosition="3197" endWordPosition="3200">, prepositions and pronouns — all good candidates for starting English phrases; and its sentences usually end with various noun types, again consistent with our running example. 4.2 Experimental Results Table 2 shows DBM-1 to be substantially more accurate than the DMV, on average: 38.8 versus 33.5% after 40 steps of EM.7 Lateen termination improved both models’ accuracies slightly, to 39.0 and 34.0%, respectively, with DBM-1 scoring five points higher. 7DBM-1’s 39% average accuracy with uniform-at-random initialization is two points above DMV’s scores with the “adhoc harmonic” strategy, 37% (Spitkovsky et al., 2011a, Table 5). First Last Tokens Tokens 4.33 3.74 1.26 0.64 0.05 0.61 0.07 0.05 0.00 0.11 0.90 0.00 0.08 0.00 0.18 0.43 0.00 0.42 0.20 0.56 0.42 0.01 0.78 0.02 0.27 0.06 0.20 0.54 0.75 0.00 0.06 0.01 0.08 0.00 0.01 0.05 0.00 0.00 0.08 0.05 0.11 0.01 0.09 0.00 Table 4: Empirical distributions for non-punctuationpartof-speech tags in WSJ, ordered by overall frequency, as well as distributions for sentence boundaries and for the roots of complete and incomplete sentences. (A uniform distribution would have 1/36 = 2.7% for all POS-tags.) Table 5: A distance matrix for all pairs of probability distri</context>
<context position="31733" citStr="Spitkovsky et al., 2011" startWordPosition="5192" endWordPosition="5195">5 56.3 MZNR 63.6 SCAJ6 — 58.5 SCAJ6 34.6 MZNR 50.5 SCAJ5 — 49.8 SCAJ5 42.4 RFH1&amp;2 46.0 RF 53.1 RFH1&amp;2 32.5 SCAJ5 48.8 RFH1&amp;2 50.3 SAJ 23.8 MZNR 33.5 SCAJ5 21.8 MZNR 39.0 MZ 33.4 MZNR 48.0 MZ 48.1 MZNR 57.5 MZ 60.6 MZNR 56.6 SCAJ5 53.5 MZNR 43.2 MZ 55.8 RFH1&amp;2 33.6 SCAJ5 34.6 MZNR 53.0 MZ 54.6 MZNR 50.0 SCAJ6 34.3 RFH1&amp;2 40.9 SAJ 61.3 RFH1 48.8 SCAJ6 — Table 8: Average accuracies over CoNLL evaluation sets (all sentences), for the DMV baseline and DBM1–3 trained with a curriculum strategy, and state-of-the-art results for systems that: (i) are also POS-agnostic and monolingual, including SCAJ (Spitkovsky et al., 2011a, Tables 5–6) and SAJ (Spitkovsky et al., 2011b); (ii) rely on gold POS-tag identities to discourage noun roots (Mareˇcek and Zabokrtsk´y, 2011, MZ) or to encourage verbs (Rasooli and Faili, 2012, RF); and (iii) transfer delexicalized parsers (Søgaard, 2011a, S) from resource-rich languages with translations (McDonald et al., 2011, MPH). DMV and DBM-1 trained on simple sentences, from uniform; DBM-2 and 3 trained on most sentences, from DBM-1 and 2, respectively; +inference is DBM-3 with punctuation constraints. 7.2 Scaffolding Stage #2: DBM-2 ← DBM-1 Next, we trained on all sentences up to l</context>
<context position="33957" citStr="Spitkovsky et al. (2011" startWordPosition="5539" endWordPosition="5542">-2 Next, we repeated the training process of the previous stage (§7.2) using DBM-3. To initialize this model, we combined the final instance of DBM-2 with uniform multinomials for punctuation-crossing attachment probabilities (see §2.3). As a result, average performance improved to 42.2% (see Table 8). Lastly, we applied punctuation constraints also in inference. Here we used the sprawl method — a more relaxed approach than in training, allowing arbitrary words to attach inter-punctuation fragments (provided that each entire fragment still be derived 695 by one of its words) — as suggested by Spitkovsky et al. (2011b). This technique increased DBM-3’s average accuracy to 42.9% (see Table 8). Our final result substantially improves over the baseline’s 33.6% and compares favorably to previous work.10 8 Discussion and the State-of-the-Art DBMs come from a long line of head-outward models for dependency grammar induction yet their generative processes feature important novelties. One is conditioning on more observable state — specifically, the left and right end words of a phrase being constructed — than in previous work. Another is allowing multiple grammars — e.g., of complete and incomplete sentences — to</context>
<context position="37001" citStr="Spitkovsky et al., 2011" startWordPosition="6020" endWordPosition="6023"> gold part-of-speech tags as anonymized word classes. For the purposes of this discussion, we also include in this group transductive learners that may train on data from the test sets. Our DBM-3 (decoded with punctuation constraints) does well among such systems — for which accuracies on all sentence lengths of the evaluation sets are reported — attaining highest scores for 8 of 19 languages; the DMV baseline is still state-of-theart for one language; and the remaining 10 bests are split among five other recent systems (see Table 8).11 Half of the five came from various lateen EM strategies (Spitkovsky et al., 2011a) for escaping and/or avoiding local optima. These heuristics are compatible with how we trained our DBMs and could potentially provide further improvement to accuracies. Overall, the final scores of DBM-3 were better, on average, than those of any other single system: 42.9 versus 38.2% (Spitkovsky et al., 2011a, Table 6). The progression of scores for DBM-1 through 3 without using punctuation constraints in inference — 40.7, 41.7 and 42.2% — fell entirely above this previous state-of-the-art result as well; the DMV baseline — also trained on sentences without internal but with final punctuat</context>
<context position="39084" citStr="Spitkovsky et al., 2011" startWordPosition="6349" endWordPosition="6352"> and 2007 CoNLL data sets has to do with segmentation of morphologically-rich languages. 696 both identities of POS-tags and/or parallel bitexts to transfer (supervised) delexicalized parsers across languages. Parser projection is by far the most successful approach to date and we hope that it too may stand to gain from our modeling improvements. Of the 10 languages for which we found results in the literature, transferred parsers underperformed the grammar inducers in only one case: on English (see Table 8). The unsupervised system that performed better used a special “weighted” initializer (Spitkovsky et al., 2011b, §3.1) that worked well for English (but less so for many other languages). DBMs may be able to improve initialization. For example, modeling of incomplete sentences could help in incremental initialization strategies like baby steps (Spitkovsky et al., 2009), which are likely sensitive to the proverbial “bum steer” from unrepresentative short fragments, pace Tu and Honavar (2011). 8.4 Miscellaneous Systems on Short Sentences Several recent systems (Cohen et al., 2011; Søgaard, 2011b; Naseem et al., 2010; Gillenwater et al., 2010; Berg-Kirkpatrick and Klein, 2010, inter alia) are absent from</context>
</contexts>
<marker>Spitkovsky, Alshawi, Jurafsky, 2011</marker>
<rawString>V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2011a. Lateen EM: Unsupervised training with multiple objectives, applied to dependency grammar induction. In EMILP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V I Spitkovsky</author>
<author>H Alshawi</author>
<author>D Jurafsky</author>
</authors>
<title>Punctuation: Making a point in unsupervised dependency parsing.</title>
<date>2011</date>
<booktitle>In CoILL.</booktitle>
<contexts>
<context position="15189" citStr="Spitkovsky et al., 2011" startWordPosition="2440" endWordPosition="2443">th, 2010); thereafter, we applied “add one” smoothing at every training step. To fairly compare the models under consideration — which could have quite different starting perplexities and ensuing consecutive relative likelihoods — we experimented with two termination strategies. In one case, we blindly ran each learner through 40 steps of inside-outside re-estimation, ignoring any convergence criteria; in the other case, we ran until numerical convergence of soft EM’s objective function or until the likelihood of resulting Viterbi parse trees suffered — an “early-stopping lateen EM” strategy (Spitkovsky et al., 2011a, §2.3). We evaluated against all sentences of the blind test sets (except one 145-token item in Arabic ’07 data). Table 2 shows experimental results, averaged over 1999), discarding any empty nodes, etc., as is standard practice. 4We did not test on WSJ data because such evaluation would not be blind, as parse trees from the PTB are our motivating examples; instead, performance on WSJ serves as a strong baseline in a separate study (Spitkovsky et al., 2012a): bootstrapping of DBMs from mostly incomplete inter-punctuation fragments. all 19 languages, for the DMV baselines and DBM-1 and 2. We </context>
<context position="19914" citStr="Spitkovsky et al., 2011" startWordPosition="3197" endWordPosition="3200">, prepositions and pronouns — all good candidates for starting English phrases; and its sentences usually end with various noun types, again consistent with our running example. 4.2 Experimental Results Table 2 shows DBM-1 to be substantially more accurate than the DMV, on average: 38.8 versus 33.5% after 40 steps of EM.7 Lateen termination improved both models’ accuracies slightly, to 39.0 and 34.0%, respectively, with DBM-1 scoring five points higher. 7DBM-1’s 39% average accuracy with uniform-at-random initialization is two points above DMV’s scores with the “adhoc harmonic” strategy, 37% (Spitkovsky et al., 2011a, Table 5). First Last Tokens Tokens 4.33 3.74 1.26 0.64 0.05 0.61 0.07 0.05 0.00 0.11 0.90 0.00 0.08 0.00 0.18 0.43 0.00 0.42 0.20 0.56 0.42 0.01 0.78 0.02 0.27 0.06 0.20 0.54 0.75 0.00 0.06 0.01 0.08 0.00 0.01 0.05 0.00 0.00 0.08 0.05 0.11 0.01 0.09 0.00 Table 4: Empirical distributions for non-punctuationpartof-speech tags in WSJ, ordered by overall frequency, as well as distributions for sentence boundaries and for the roots of complete and incomplete sentences. (A uniform distribution would have 1/36 = 2.7% for all POS-tags.) Table 5: A distance matrix for all pairs of probability distri</context>
<context position="31733" citStr="Spitkovsky et al., 2011" startWordPosition="5192" endWordPosition="5195">5 56.3 MZNR 63.6 SCAJ6 — 58.5 SCAJ6 34.6 MZNR 50.5 SCAJ5 — 49.8 SCAJ5 42.4 RFH1&amp;2 46.0 RF 53.1 RFH1&amp;2 32.5 SCAJ5 48.8 RFH1&amp;2 50.3 SAJ 23.8 MZNR 33.5 SCAJ5 21.8 MZNR 39.0 MZ 33.4 MZNR 48.0 MZ 48.1 MZNR 57.5 MZ 60.6 MZNR 56.6 SCAJ5 53.5 MZNR 43.2 MZ 55.8 RFH1&amp;2 33.6 SCAJ5 34.6 MZNR 53.0 MZ 54.6 MZNR 50.0 SCAJ6 34.3 RFH1&amp;2 40.9 SAJ 61.3 RFH1 48.8 SCAJ6 — Table 8: Average accuracies over CoNLL evaluation sets (all sentences), for the DMV baseline and DBM1–3 trained with a curriculum strategy, and state-of-the-art results for systems that: (i) are also POS-agnostic and monolingual, including SCAJ (Spitkovsky et al., 2011a, Tables 5–6) and SAJ (Spitkovsky et al., 2011b); (ii) rely on gold POS-tag identities to discourage noun roots (Mareˇcek and Zabokrtsk´y, 2011, MZ) or to encourage verbs (Rasooli and Faili, 2012, RF); and (iii) transfer delexicalized parsers (Søgaard, 2011a, S) from resource-rich languages with translations (McDonald et al., 2011, MPH). DMV and DBM-1 trained on simple sentences, from uniform; DBM-2 and 3 trained on most sentences, from DBM-1 and 2, respectively; +inference is DBM-3 with punctuation constraints. 7.2 Scaffolding Stage #2: DBM-2 ← DBM-1 Next, we trained on all sentences up to l</context>
<context position="33957" citStr="Spitkovsky et al. (2011" startWordPosition="5539" endWordPosition="5542">-2 Next, we repeated the training process of the previous stage (§7.2) using DBM-3. To initialize this model, we combined the final instance of DBM-2 with uniform multinomials for punctuation-crossing attachment probabilities (see §2.3). As a result, average performance improved to 42.2% (see Table 8). Lastly, we applied punctuation constraints also in inference. Here we used the sprawl method — a more relaxed approach than in training, allowing arbitrary words to attach inter-punctuation fragments (provided that each entire fragment still be derived 695 by one of its words) — as suggested by Spitkovsky et al. (2011b). This technique increased DBM-3’s average accuracy to 42.9% (see Table 8). Our final result substantially improves over the baseline’s 33.6% and compares favorably to previous work.10 8 Discussion and the State-of-the-Art DBMs come from a long line of head-outward models for dependency grammar induction yet their generative processes feature important novelties. One is conditioning on more observable state — specifically, the left and right end words of a phrase being constructed — than in previous work. Another is allowing multiple grammars — e.g., of complete and incomplete sentences — to</context>
<context position="37001" citStr="Spitkovsky et al., 2011" startWordPosition="6020" endWordPosition="6023"> gold part-of-speech tags as anonymized word classes. For the purposes of this discussion, we also include in this group transductive learners that may train on data from the test sets. Our DBM-3 (decoded with punctuation constraints) does well among such systems — for which accuracies on all sentence lengths of the evaluation sets are reported — attaining highest scores for 8 of 19 languages; the DMV baseline is still state-of-theart for one language; and the remaining 10 bests are split among five other recent systems (see Table 8).11 Half of the five came from various lateen EM strategies (Spitkovsky et al., 2011a) for escaping and/or avoiding local optima. These heuristics are compatible with how we trained our DBMs and could potentially provide further improvement to accuracies. Overall, the final scores of DBM-3 were better, on average, than those of any other single system: 42.9 versus 38.2% (Spitkovsky et al., 2011a, Table 6). The progression of scores for DBM-1 through 3 without using punctuation constraints in inference — 40.7, 41.7 and 42.2% — fell entirely above this previous state-of-the-art result as well; the DMV baseline — also trained on sentences without internal but with final punctuat</context>
<context position="39084" citStr="Spitkovsky et al., 2011" startWordPosition="6349" endWordPosition="6352"> and 2007 CoNLL data sets has to do with segmentation of morphologically-rich languages. 696 both identities of POS-tags and/or parallel bitexts to transfer (supervised) delexicalized parsers across languages. Parser projection is by far the most successful approach to date and we hope that it too may stand to gain from our modeling improvements. Of the 10 languages for which we found results in the literature, transferred parsers underperformed the grammar inducers in only one case: on English (see Table 8). The unsupervised system that performed better used a special “weighted” initializer (Spitkovsky et al., 2011b, §3.1) that worked well for English (but less so for many other languages). DBMs may be able to improve initialization. For example, modeling of incomplete sentences could help in incremental initialization strategies like baby steps (Spitkovsky et al., 2009), which are likely sensitive to the proverbial “bum steer” from unrepresentative short fragments, pace Tu and Honavar (2011). 8.4 Miscellaneous Systems on Short Sentences Several recent systems (Cohen et al., 2011; Søgaard, 2011b; Naseem et al., 2010; Gillenwater et al., 2010; Berg-Kirkpatrick and Klein, 2010, inter alia) are absent from</context>
</contexts>
<marker>Spitkovsky, Alshawi, Jurafsky, 2011</marker>
<rawString>V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2011b. Punctuation: Making a point in unsupervised dependency parsing. In CoILL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V I Spitkovsky</author>
<author>H Alshawi</author>
<author>D Jurafsky</author>
</authors>
<title>Bootstrapping dependency grammar inducers from incomplete sentence fragments via austere models – the “wabi-sabi” of unsupervised parsing.</title>
<date>2012</date>
<booktitle>In submission.</booktitle>
<contexts>
<context position="15651" citStr="Spitkovsky et al., 2012" startWordPosition="2519" endWordPosition="2522"> soft EM’s objective function or until the likelihood of resulting Viterbi parse trees suffered — an “early-stopping lateen EM” strategy (Spitkovsky et al., 2011a, §2.3). We evaluated against all sentences of the blind test sets (except one 145-token item in Arabic ’07 data). Table 2 shows experimental results, averaged over 1999), discarding any empty nodes, etc., as is standard practice. 4We did not test on WSJ data because such evaluation would not be blind, as parse trees from the PTB are our motivating examples; instead, performance on WSJ serves as a strong baseline in a separate study (Spitkovsky et al., 2012a): bootstrapping of DBMs from mostly incomplete inter-punctuation fragments. all 19 languages, for the DMV baselines and DBM-1 and 2. We did not test DBM-3 in this set-up because most sentence-internal punctuation occurs in longer sentences; instead, DBM-3 will be tested later (see §7), using most sentences,5 in the final training step of a curriculum strategy (Bengio et al., 2009) that we will propose for DBMs. For the three models tested on shorter inputs (up to 15 tokens) both terminating criteria exhibited the same trend; lateen EM consistently scored slightly higher than 40 EM iterations</context>
<context position="29480" citStr="Spitkovsky et al., 2012" startWordPosition="4785" endWordPosition="4788">is cleaner subset of the training data, which takes advantage of the fact that punctuation may generally correlate with sentence complexity (Frank, 2000).9 Aside from input sentence selection, our experimental set-up here remained identical to previous training of DBMs (§4–5). Using this new input data, DBM-1 averaged 40.7% accuracy (see Table 8). This is slightly higher than the 39.0% when using sentences up to length 15, suggesting that our heuristic for clean, simple sentences may be a useful one. 9More incremental training strategies are the subject of an unpublished companion manuscript (Spitkovsky et al., 2012a). a) ry ≈ −0.40 Punctuation no Punctuation Total non-Adjacent Adjacent Attached not Attached Total 7,982 6,184 14,166 14,151 15 Total 694 Directed Dependency Accuracies for: Best of State-of-the-Art Systems this Work (@10) DMV DBM-1 DBM-2 DBM-3 +inference 12.9 10.6 11.0 11.1 10.9 (34.5) 36.6 43.9 44.0 44.4 44.9 (48.8) 32.7 34.1 33.0 32.7 33.3 (36.5) 24.7 59.4 63.6 64.6 65.2 (70.4) 41.1 61.3 61.1 61.1 62.1 (78.1) 50.4 63.1 63.0 63.2 63.2 (65.7) 55.3 56.8 57.0 57.1 57.0 (59.8) 31.5 51.3 52.8 53.0 55.1 (61.8) 34.5 50.5 51.2 53.3 54.2 (67.3) 22.4 21.3 19.9 21.8 22.2 (27.4) 44.9 45.9 46.5 46.0 46</context>
<context position="34780" citStr="Spitkovsky et al., 2012" startWordPosition="5667" endWordPosition="5670">and the State-of-the-Art DBMs come from a long line of head-outward models for dependency grammar induction yet their generative processes feature important novelties. One is conditioning on more observable state — specifically, the left and right end words of a phrase being constructed — than in previous work. Another is allowing multiple grammars — e.g., of complete and incomplete sentences — to coexist in a single model. These improvements could make DBMs quick-andeasy to bootstrap directly from any available partial bracketings (Pereira and Schabes, 1992), for example capitalized phrases (Spitkovsky et al., 2012b). The second part of our work — the use of a curriculum strategy to train DBM-1 through 3 — eliminates having to know tuned cut-offs, such as sentences with up to a predetermined number of tokens. Although this approach adds some complexity, we chose conservatively, to avoid overfitting settings of sentence length, convergence criteria, etc.: stage one’s data is dictated by DBM-1 (which ignores punctuation); subsequent stages initialize additional pieces uniformly: uniform-at-random parses for new data and uniform multinomials for new parameters. Even without curriculum learning — trained wi</context>
</contexts>
<marker>Spitkovsky, Alshawi, Jurafsky, 2012</marker>
<rawString>V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2012a. Bootstrapping dependency grammar inducers from incomplete sentence fragments via austere models – the “wabi-sabi” of unsupervised parsing. In submission.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V I Spitkovsky</author>
<author>H Alshawi</author>
<author>D Jurafsky</author>
</authors>
<title>Capitalization cues improve dependency grammar induction.</title>
<date>2012</date>
<booktitle>In WILS.</booktitle>
<contexts>
<context position="15651" citStr="Spitkovsky et al., 2012" startWordPosition="2519" endWordPosition="2522"> soft EM’s objective function or until the likelihood of resulting Viterbi parse trees suffered — an “early-stopping lateen EM” strategy (Spitkovsky et al., 2011a, §2.3). We evaluated against all sentences of the blind test sets (except one 145-token item in Arabic ’07 data). Table 2 shows experimental results, averaged over 1999), discarding any empty nodes, etc., as is standard practice. 4We did not test on WSJ data because such evaluation would not be blind, as parse trees from the PTB are our motivating examples; instead, performance on WSJ serves as a strong baseline in a separate study (Spitkovsky et al., 2012a): bootstrapping of DBMs from mostly incomplete inter-punctuation fragments. all 19 languages, for the DMV baselines and DBM-1 and 2. We did not test DBM-3 in this set-up because most sentence-internal punctuation occurs in longer sentences; instead, DBM-3 will be tested later (see §7), using most sentences,5 in the final training step of a curriculum strategy (Bengio et al., 2009) that we will propose for DBMs. For the three models tested on shorter inputs (up to 15 tokens) both terminating criteria exhibited the same trend; lateen EM consistently scored slightly higher than 40 EM iterations</context>
<context position="29480" citStr="Spitkovsky et al., 2012" startWordPosition="4785" endWordPosition="4788">is cleaner subset of the training data, which takes advantage of the fact that punctuation may generally correlate with sentence complexity (Frank, 2000).9 Aside from input sentence selection, our experimental set-up here remained identical to previous training of DBMs (§4–5). Using this new input data, DBM-1 averaged 40.7% accuracy (see Table 8). This is slightly higher than the 39.0% when using sentences up to length 15, suggesting that our heuristic for clean, simple sentences may be a useful one. 9More incremental training strategies are the subject of an unpublished companion manuscript (Spitkovsky et al., 2012a). a) ry ≈ −0.40 Punctuation no Punctuation Total non-Adjacent Adjacent Attached not Attached Total 7,982 6,184 14,166 14,151 15 Total 694 Directed Dependency Accuracies for: Best of State-of-the-Art Systems this Work (@10) DMV DBM-1 DBM-2 DBM-3 +inference 12.9 10.6 11.0 11.1 10.9 (34.5) 36.6 43.9 44.0 44.4 44.9 (48.8) 32.7 34.1 33.0 32.7 33.3 (36.5) 24.7 59.4 63.6 64.6 65.2 (70.4) 41.1 61.3 61.1 61.1 62.1 (78.1) 50.4 63.1 63.0 63.2 63.2 (65.7) 55.3 56.8 57.0 57.1 57.0 (59.8) 31.5 51.3 52.8 53.0 55.1 (61.8) 34.5 50.5 51.2 53.3 54.2 (67.3) 22.4 21.3 19.9 21.8 22.2 (27.4) 44.9 45.9 46.5 46.0 46</context>
<context position="34780" citStr="Spitkovsky et al., 2012" startWordPosition="5667" endWordPosition="5670">and the State-of-the-Art DBMs come from a long line of head-outward models for dependency grammar induction yet their generative processes feature important novelties. One is conditioning on more observable state — specifically, the left and right end words of a phrase being constructed — than in previous work. Another is allowing multiple grammars — e.g., of complete and incomplete sentences — to coexist in a single model. These improvements could make DBMs quick-andeasy to bootstrap directly from any available partial bracketings (Pereira and Schabes, 1992), for example capitalized phrases (Spitkovsky et al., 2012b). The second part of our work — the use of a curriculum strategy to train DBM-1 through 3 — eliminates having to know tuned cut-offs, such as sentences with up to a predetermined number of tokens. Although this approach adds some complexity, we chose conservatively, to avoid overfitting settings of sentence length, convergence criteria, etc.: stage one’s data is dictated by DBM-1 (which ignores punctuation); subsequent stages initialize additional pieces uniformly: uniform-at-random parses for new data and uniform multinomials for new parameters. Even without curriculum learning — trained wi</context>
</contexts>
<marker>Spitkovsky, Alshawi, Jurafsky, 2012</marker>
<rawString>V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2012b. Capitalization cues improve dependency grammar induction. In WILS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Tu</author>
<author>V Honavar</author>
</authors>
<title>On the utility of curricula in unsupervised learning of probabilistic grammars.</title>
<date>2011</date>
<booktitle>In IJCAI.</booktitle>
<contexts>
<context position="39469" citStr="Tu and Honavar (2011)" startWordPosition="6408" endWordPosition="6411"> results in the literature, transferred parsers underperformed the grammar inducers in only one case: on English (see Table 8). The unsupervised system that performed better used a special “weighted” initializer (Spitkovsky et al., 2011b, §3.1) that worked well for English (but less so for many other languages). DBMs may be able to improve initialization. For example, modeling of incomplete sentences could help in incremental initialization strategies like baby steps (Spitkovsky et al., 2009), which are likely sensitive to the proverbial “bum steer” from unrepresentative short fragments, pace Tu and Honavar (2011). 8.4 Miscellaneous Systems on Short Sentences Several recent systems (Cohen et al., 2011; Søgaard, 2011b; Naseem et al., 2010; Gillenwater et al., 2010; Berg-Kirkpatrick and Klein, 2010, inter alia) are absent from Table 8 because they do not report performance for all sentence lengths. To facilitate comparison with this body of important previous work, we also tabulated final accuracies for the “up-to-ten words” task under heading @10: 51.9%, on average. 9 Conclusion Although a dependency parse for a sentence can be mapped to a constituency parse (Xia and Palmer, 2001), the probabilistic mod</context>
</contexts>
<marker>Tu, Honavar, 2011</marker>
<rawString>K. Tu and V. Honavar. 2011. On the utility of curricula in unsupervised learning of probabilistic grammars. In IJCAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Xia</author>
<author>M Palmer</author>
</authors>
<title>Converting dependency structures to phrase structures.</title>
<date>2001</date>
<booktitle>In HLT.</booktitle>
<contexts>
<context position="40046" citStr="Xia and Palmer, 2001" startWordPosition="6501" endWordPosition="6504"> short fragments, pace Tu and Honavar (2011). 8.4 Miscellaneous Systems on Short Sentences Several recent systems (Cohen et al., 2011; Søgaard, 2011b; Naseem et al., 2010; Gillenwater et al., 2010; Berg-Kirkpatrick and Klein, 2010, inter alia) are absent from Table 8 because they do not report performance for all sentence lengths. To facilitate comparison with this body of important previous work, we also tabulated final accuracies for the “up-to-ten words” task under heading @10: 51.9%, on average. 9 Conclusion Although a dependency parse for a sentence can be mapped to a constituency parse (Xia and Palmer, 2001), the probabilistic models generating them use different conditioning: dependency grammars focus on the relationship between arguments and heads, constituency grammars on the coherence of chunks covered by non-terminals. Since redundant views of data can make learning easier (Blum and Mitchell, 1998), integrating aspects of both constituency and dependency ought to be able to help grammar induction. We have shown that this insight is correct: dependency grammar inducers can gain from modeling boundary information that is fundamental to constituency (i.e., phrase-structure) formalisms. DBMs are</context>
</contexts>
<marker>Xia, Palmer, 2001</marker>
<rawString>F. Xia and M. Palmer. 2001. Converting dependency structures to phrase structures. In HLT.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
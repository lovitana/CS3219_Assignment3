<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000072">
<title confidence="0.994124">
Polarity Inducing Latent Semantic Analysis
</title>
<author confidence="0.984911">
Wen-tau Yih Geoffrey Zweig John C. Platt
</author>
<affiliation confidence="0.953816">
Microsoft Research
</affiliation>
<address confidence="0.942601">
One Microsoft Way
Redmond, WA 98052, USA
</address>
<email confidence="0.999409">
{scottyih,gzweig,jplatt}@microsoft.com
</email>
<sectionHeader confidence="0.995645" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999764964285714">
Existing vector space models typically map
synonyms and antonyms to similar word vec-
tors, and thus fail to represent antonymy. We
introduce a new vector space representation
where antonyms lie on opposite sides of a
sphere: in the word vector space, synonyms
have cosine similarities close to one, while
antonyms are close to minus one.
We derive this representation with the aid of a
thesaurus and latent semantic analysis (LSA).
Each entry in the thesaurus – a word sense
along with its synonyms and antonyms – is
treated as a “document,” and the resulting doc-
ument collection is subjected to LSA. The key
contribution of this work is to show how to as-
sign signs to the entries in the co-occurrence
matrix on which LSA operates, so as to induce
a subspace with the desired property.
We evaluate this procedure with the Grad-
uate Record Examination questions of (Mo-
hammed et al., 2008) and find that the method
improves on the results of that study. Further
improvements result from refining the sub-
space representation with discriminative train-
ing, and augmenting the training data with
general newspaper text. Altogether, we im-
prove on the best previous results by 11 points
absolute in F measure.
</bodyText>
<sectionHeader confidence="0.999335" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999955289473685">
Vector space representations have proven useful
across a wide variety of text processing applications
ranging from document clustering to search rele-
vance measurement. In these applications, text is
represented as a vector in a multi-dimensional con-
tinuous space, and a similarity metric such as co-
sine similarity can be used to measure the related-
ness of different items. Vector space representations
have been used both at the document and word lev-
els. At the document level, they are effective for
applications including information retrieval (Salton
and McGill, 1983; Deerwester et al., 1990), docu-
ment clustering (Deerwester et al., 1990; Xu et al.,
2003), search relevance measurement (Baeza-Yates
and Ribiero-Neto, 1999) and cross-lingual docu-
ment retrieval (Platt et al., 2010). At the word level,
vector representations have been used to measure
word similarity (Deerwester et al., 1990; Turney and
Littman, 2005; Turney, 2006; Turney, 2001; Lin,
1998; Agirre et al., 2009; Reisinger and Mooney,
2010) and for language modeling (Bellegarda, 2000;
Coccaro and Jurafsky, 1998). While quite success-
ful, these applications have typically been consistent
with a very general notion of similarity in which
basic association is measured, and finer shades of
meaning need not be distinguished. For example,
latent semantic analysis might assign a high degree
of similarity to opposites as well as synonyms (Lan-
dauer and Laham, 1998; Landauer, 2002).
Independent of vector-space representations, a
number of authors have focused on identifying dif-
ferent kinds of relatedness. At the simplest level,
we may wish to distinguish between synonyms and
antonyms, which can be further differentiated. For
example, in synonymy, we may wish to distinguish
hyponyms and hypernyms. Moreover, Cruse (1986)
notes that numerous kinds of antonymy are possible,
for example antipodal pairs like “top-bottom” or
</bodyText>
<page confidence="0.944478">
1212
</page>
<note confidence="0.958285857142857">
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1212–1222, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics
gradable opposites like “light-heavy.” Work in this
area includes (Turney, 2001; Lin et al., 2003; Tur-
ney and Littman, 2005; Turney, 2006; Curran and
Moens, 2002; van der Plas and Tiedemann, 2006;
Mohammed et al., 2008; Mohammed et al., 2011).
</note>
<bodyText confidence="0.997212297872341">
Despite the existence of a large amount of related
work in the literature, distinguishing synonyms and
antonyms is still considered as a difficult open prob-
lem in general (Poon and Domingos, 2009).
In this paper, we fuse these two strands of re-
search in an attempt to develop a vector space rep-
resentation in which the synonymy and antonymy
are naturally differentiated. We follow Schwab et
al. (2002) in requiring a representation in which
two lexical items in an antonymy relation should lie
at opposite ends of an axis. However, in contrast
to the logical axes used previously, we desire that
antonyms should lie at the opposite ends of a sphere
lying in a continuous and automatically induced vec-
tor space. To generate this vector space, we present
a novel method for assigning both negative and pos-
itive values to the TF-IDF weights used in latent se-
mantic analysis.
To determine these signed values, we exploit the
information present in a thesaurus. The result is a
vector space representation in which synonyms clus-
ter together, and the opposites of a word tend to clus-
ter together at the opposite end of a sphere.
This representation provides several advantages
over the raw thesaurus. First, by finding the items
most and least similar to a word, we are able to dis-
cover new synonyms and antonyms. Second, as dis-
cussed in Section 5, the representation provides a
natural starting point for gradient-descent based op-
timization. Thirdly, as we discuss in Section 6, it is
straightforward to embed new words into the derived
subspace by using information from a large unsuper-
vised text corpus such as Wikipedia.
The remainder of this paper is organized as fol-
lows. Section 2 describes previous work. Section 3
presents the classical LSA approach and analyzes
some of its limitations. In Section 4 we present our
polarity inducing extension to LSA. Section 5 fur-
ther extends the approach by optimizing the vector
space representation with supervised discriminative
training. Section 6 describes the proposed method of
embedding new words in the thesaurus-derived sub-
space. The experimental results of Section 7 indi-
cate that the proposed method outperforms previous
approaches on a GRE test of closest-opposites (Mo-
hammed et al., 2008). Finally, Section 8 concludes
the paper.
</bodyText>
<sectionHeader confidence="0.999345" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999980317073171">
The detection of antonymy has been studied in a
number of previous papers. Mohammed et al. (2008)
approach the problem by combining information
from a published thesaurus with corpus statistics de-
rived from the Google n-gram corpus (Brants and
Franz, 2006). Their method consists of two main
steps: first, detecting contrasting word categories
(e.g. “WORK” vs. “ACTIVITY FOR FUN”) and
then determining the degree of antonymy. Cate-
gories are defined by a thesaurus; contrasting cat-
egories are found by using affix rules (e.g., un- &amp;
dis-) and WordNet antonymy links. Words belong-
ing to contrasting categories are treated as antonyms
and the degree of contrast is determined by distri-
butional similarity. Mohammed et al. (2008) also
provides a publicly available dataset for detection of
antonymy, which we have adopted. This work has
been extended in (Mohammed et al., 2011) to in-
clude a study of antonymy based on crowd-sourcing
experiments.
Turney (2008) proposes a unified approach to
handling analogies, synonyms, antonyms and asso-
ciations by transforming the last three cases into
cases of analogy. A supervised learning method
is then used to solve the resulting analogical prob-
lems. This is evaluated on a set of 136 ESL ques-
tions. Lin et al. (2003) builds on (Lin, 1998) and
identifies antonyms as semantically related words
which also happen to be found together in a database
in pre-identified phrases indicating opposition. Lin
et al. (2003) further note that whereas synonyms
will tend to translate to the same word in another
language, antonyms will not. This observation is
used to select antonyms from amongst distribution-
ally similar words. Antonymy is used in (de Si-
mone and Kazakov, 2005) for document clustering
and (Harabagiu et al., 2006) to find contradiction.
The automatic detection of synonyms has been
more extensively studied. Lin (1998) presents
a thorough comparison of word-similarity metrics
based on distributional similarity, where this is de-
</bodyText>
<page confidence="0.971673">
1213
</page>
<bodyText confidence="0.9999325">
termined from co-occurrence statistics in depen-
dency triples extracted by parsing a large dataset.
Related studies are described in (Curran and Moens,
2002; van der Plas and Bouma, 2005). Later, van
der Plas and Tiedemann (2006) extend the use of
multilingual data present in Lin et al. (2003) by mea-
suring distributional similarity based on the contexts
that a word occurs in once translated into a new lan-
guage. This is used to improve the precision/recall
characteristics on synonym pairs. Structured infor-
mation can be important in determining relatedness,
and thesauri and Wikipedia links have been studied
in (Milne and Witten, 2008; Jarmasz and Szpakow-
icz, 2003). Combinations of approaches are studied
in (Turney et al., 2003).
Vector-space models and latent semantic analysis
in particular have a long history of use in synonym
detection, which in fact was suggested in some of
the earliest LSA papers. Deerwester et al. (1990)
defines a metric for measuring word similarity based
on LSA, and it has been used in (Landauer and Du-
mais, 1997; Landauer et al., 1998) to answer word
similarity questions derived from the Test of English
as a Foreign Language (TOEFL). Turney (2001)
proposes the use of point-wise mutual information in
conjunction with LSA, and again presents results on
synonym questions derived from the TOEFL. Vari-
ants of vector space models are further analyzed
in (Turney and Littman, 2005; Turney, 2006; Tur-
ney and Pantel, 2010).
</bodyText>
<sectionHeader confidence="0.945501" genericHeader="method">
3 Latent Semantic Analysis
</sectionHeader>
<bodyText confidence="0.990694217391304">
Latent Semantic Analysis (Deerwester et al., 1990)
is a widely used method for representing words and
documents in a low dimensional vector space. The
method is based on applying singular value decom-
position (SVD) to a matrix W which indicates the
occurrence of words in documents. To perform
LSA, one proceeds as follows. The input is a col-
lection of d documents which are expressed in terms
of words from a vocabulary of size n. These docu-
ments may be actual documents such as newspaper
articles, or simply notional documents such as sen-
tences, or any other collection in which words are
grouped together. Next, a d x n document-term ma-
trix W is formed1. At its simplest form, the ijth
entry contains the number of times word j has oc-
curred in document i – its term frequency or TF
value. More conventionally, the entry is weighted
by some notion of the importance of word j, for ex-
ample the negative logarithm of the fraction of doc-
uments that contain it, resulting in a TF-IDF weight-
ing (Salton et al., 1975). The similarity between two
documents can be computed using the cosine simi-
larity of their corresponding row vectors:
</bodyText>
<equation confidence="0.9929255">
x · y
sim(x, y) =
</equation>
<bodyText confidence="0.999217">
Similarly, the cosine similarity of two column vec-
tors can be used to judge the similarity of the corre-
sponding words. Finally, to obtain a subspace repre-
sentation of dimension k, W is decomposed as
</bodyText>
<equation confidence="0.709988">
W Pz� USVT
</equation>
<bodyText confidence="0.999901">
where U is d x k, V T is k x n, and S is a k x k
diagonal matrix. In applications, k « n and k « d;
for example one might have a 50, 000 word vocab-
ulary and 1, 000, 000 documents and use a 300 di-
mensional subspace representation.
An important property of SVD is that the columns
of SVT – which now represent the words – behave
similarly to the original columns of W, in the sense
that the cosine similarity between two columns in
SVT approximates the cosine similarity between the
corresponding columns in W. This follows from
the observation that WTW = VSIVT, and the fact
that the ijth entry of WT W is the dot product of
the ith and jth columns (words) in W. We will
use this observation subsequently in the derivation
of polarity-inducing LSA. For efficiency, we nor-
malize the columns of SVT to unit length, allow-
ing the cosine similarity between two words to be
computed with a single dot-product; this also has the
property of mapping each word to a point on a multi-
dimensional sphere.
A second important property of LSA is that in the
word representations which result can by viewed as
the result of applying a projection matrix U to the
original vectors as:
</bodyText>
<equation confidence="0.716352">
UTW = SVT
</equation>
<bodyText confidence="0.800846333333333">
1(Bellegarda, 2000) constructs the transpose of this, but we
have found it convenient in data processing for documents to
represent rows.
</bodyText>
<equation confidence="0.59356">
II x IIII y II
</equation>
<page confidence="0.915627">
1214
</page>
<bodyText confidence="0.999887">
In Section 5, we will view U simply as a dxk matrix
learned through gradient descent so as to optimize
an objective function.
</bodyText>
<subsectionHeader confidence="0.999816">
3.1 Limitation of LSA
</subsectionHeader>
<bodyText confidence="0.998780966666667">
Word similarity as determined by LSA assigns high
values to words which tend to co-occur in doc-
uments. However, as noted by (Landauer and
Laham, 1998; Landauer, 2002), there is no no-
tion of antonymy; words with low or negative co-
sine scores are simply unrelated. In comparison,
words with high cosine similarity scores are typi-
cally semantically related, which includes both syn-
onyms and antonyms, as contrasting words often co-
occur (Murphy and Andrew, 1993; Mohammed et
al., 2008). To illustrate this, we have performed
SVD with the aid of the Encarta thesaurus developed
by Bloomsbury Publishing Plc. This thesaurus con-
tains approximately 47k word senses and a vocab-
ulary of 50k words and phrases. Each “document”
is taken to be the thesaurus entry for a word-sense,
including synonyms and antonyms. For example,
the word “admirable” induces a document consist-
ing of {admirable, estimable, commendable, vener-
able, good, splendid, worthy, marvelous, excellent,
unworthy}. Note that the last word in this set is its
antonym. Performing SVD on this set of thesaurus
derived “meaning-documents” results in a subspace
representation for each word. This form of LSA is
similar to the use of Wikipedia in (Gabrilovich and
Markovitch, 2007).
Table 1 shows some words, their original the-
saurus documents, and the most and least similar
words in the LSA subspace. Several properties are
apparent:
</bodyText>
<listItem confidence="0.996974222222222">
• The vector-space representation of words is
able to identify related words that are not ex-
plicitly present in the original thesaurus. For
example, “meritorious” for “admirable” – ar-
guably better than any of the words given in the
thesaurus itself.
• Similarity is based on co-occurrence, so the
co-occurrence of antonyms in the thesaurus-
derived documents induces their presence as
</listItem>
<bodyText confidence="0.8476845">
LSA-similar words. For example, “con-
temptible” is identified as similar to “ad-
mirable.” In the case of “mourning,” opposites
acrimony rancor goodwill affection
acrimony 1 1 1 1
affection 1 1 1 1
</bodyText>
<tableCaption confidence="0.925224333333333">
Table 2: The W matrix for two thesaurus entries in
its original form. Rows represent documents; columns
words.
</tableCaption>
<table confidence="0.942759333333333">
acrimony rancor goodwill affection
acrimony 1 1 -1 -1
affection -1 -1 1 1
</table>
<tableCaption confidence="0.9959825">
Table 3: The W matrix for two thesaurus entries in its
polarity-inducing form.
</tableCaption>
<bodyText confidence="0.950821916666667">
such as “joy” and “elation” actually dominate
the list of LSA-similar words.
• The LSA-least-similar words have no relation-
ship at all to the word they are least-similar to.
For example, the least-similar word to “consid-
ered” is “ready-made-meal.”
In the next section, we will present a method for
inducing polarity in LSA subspaces, where opposite
words will tend to have negative cosine similarities,
analogous to the positive similarities of synonyms.
Thus, the least-similar words to a given word will be
its opposites.
</bodyText>
<sectionHeader confidence="0.970607" genericHeader="method">
4 Polarity Inducing LSA
</sectionHeader>
<bodyText confidence="0.999981705882353">
We modify LSA so that we may exploit a thesaurus
to embed meaningful axes in the induced subspace
representation. Words with opposite meaning will
lie at opposite positions on a sphere. Recall that the
cosine similarity between word-vectors in the orig-
inal matrix W are preserved in the subspace repre-
sentation of words. Thus, if we construct the original
matrix so that the columns representing antonyms
will tend to have negative cosine similarities while
columns representing synonyms will tend to have
positive similarities, we will achieve the desired be-
havior.
This can be achieved by negating the TF-IDF en-
tries for the antonyms of a word when constructing
W from the thesaurus, which is illustrated in Ta-
bles 2 and 3. The two rows in these tables corre-
spond to thesaurus entries for the sense-categories
</bodyText>
<page confidence="0.622462">
1215
</page>
<bodyText confidence="0.9578334">
Word Thesaurus Entry LSA Most-Similar Words LSA Least-Similar Words
admirable estimable, commendable, commendable, creditable, easy-on-the-eye, peace-
venerable, good, splen- laudable, praiseworthy, keeper, peace-lover,
did, worthy, marvelous, worthy, meritorious, conscientious-objector,
excellent, unworthy scurvy, contemptible, uninviting, dishy, dessert,
despicable, estimable pudding, seductive
considered careful, measured, well- calculated, premeditated, ready-made-meal, ready-
thought-out, painstaking, planned, tactical, strate- meal, disposed-to, apt-to,
rash gic, thought-through, in- wild-animals, big-game,
tentional, fortuitous, pur- game-birds, game-fish,
poseful, unpremeditated rugger, rugby
mourning grief, bereavement, sor- sorrowfulness, anguish, muckiness, turn-the-
row, sadness, lamenta- exultation, rejoicing, ju- corner, impassibility,
tion, woe, grieving, exul- bilation, glee, heartache, filminess, pellucidity,
tation travail, joy, elation limpidity, sheerness
</bodyText>
<tableCaption confidence="0.997899">
Table 1: LSA on a thesaurus. Thesaurus entries include antonyms in italics.
</tableCaption>
<bodyText confidence="0.999611678571429">
“acrimony,” and “affection.” The thesaurus entries
induce two “documents” containing the words and
their synonyms and antonyms. The complete set of
words is acrimony, rancor, goodwill, affection. For
simplicity, we assume that all TF-IDF weights are
1. In the original LSA formulation, we have the rep-
resentation of Table 2. “Rancor” is listed as a syn-
onym of “acrimony,” which has “goodwill” and “af-
fection” as its antonyms. This results in the first row.
Note that the cosine similarity between every pair of
words (columns) is 1.
Table 3 shows the polarity-inducing representa-
tion. Here, the cosine similarity between synony-
mous words (columns) is 1, and the cosine similarity
between antonymous words is -1. Since LSA tends
to preserve cosine similarities between words, in the
resulting subspace we may expect to find meaning-
ful axes, where opposite senses map to opposite ex-
tremes. We refer to this as polarity-inducing LSA or
PILSA.
In Table 4, we show the PILSA-similar and
PILSA-least-similar words for the same words as in
Table 1. We see now that words which are least
similar in the sense of having the lowest cosine-
similarity are indeed opposites. In this table gen-
erally the most similar words have similarities in the
range of 0.7 to 1.0 and the least similar in the range
of -0.7 to -1.0.
</bodyText>
<sectionHeader confidence="0.997361" genericHeader="method">
5 Discriminative Training
</sectionHeader>
<bodyText confidence="0.999910206896552">
Although the cosine similarity of LSA-derived word
vectors are generally very effective in applications
such as judging the relevance of words or docu-
ments, or detecting antonyms as in our construction,
the process of singular value decomposition in LSA
does not explicitly try to achieve such goals. In this
section, we see that when supervised training data is
available, the projection matrix of LSA can be en-
hanced through a discriminative training technique
explicitly designed to create a representation suited
to a specific task.
Because LSA is closely related to principle com-
ponent analysis (PCA), extensions of PCA such as
canonical correlation analysis (CCA) and oriented
principle component analysis (OPCA) can leverage
the labeled data and produce the projection matrix
through general eigen-decomposition (Platt et al.,
2010). Along this line of work, Yih et al. (2011)
proposed a Siamese neural network approach called
S2Net, which tunes the projection matrix directly
through gradient descent, and has shown to outper-
form other methods in several tasks. Below we de-
scribe briefly this technique and explain how we
adopt it for the task of antonym detection.
The goal of S2Net is to learn a concept vector
representation of the original sparse term vectors.
Although such transformation can be non-linear in
general, its current design chooses the model form
to be a linear projection matrix, which is identical to
</bodyText>
<page confidence="0.893848">
1216
</page>
<table confidence="0.99988225">
Word PILSA-Similar Words PILSA-Least-Similar Words
admirable commendable, creditable, laudable, scurvy, contemptible, despicable,
praiseworthy, worthy, meritorious, es- lamentable, shameful, reprehensible,
timable, deserving, tiptop, valued unworthy, disgraceful, discreditable,
undeserving
considered calculated, premeditated, planned, tac- fortuitous, unpremeditated, unconsid-
tical, strategic, thought-through, inten- ered, off-your-own-bat, unintended,
tional, purposeful, intended, psycho- undirected, objectiveless, hit-and-miss,
logical unforced, involuntary
mourning sorrowful, doleful, sad, miserable, smiley, happy, blissful, wooden, mirth-
wistful, pitiful, wailing, sobbing, ful, joyful, deadpan, fulfilled, straight-
heavy-hearted, forlorn faced, content
</table>
<tableCaption confidence="0.999893">
Table 4: PILSA on a thesaurus. Thesaurus entries are as in Table 1.
</tableCaption>
<bodyText confidence="0.999508333333333">
that of LSA, PCA, OPCA or CCA. Given a d-by-1
input vector f, the model of S2Net is a d-by-k ma-
trix A = [aij]dxk, which maps f to a k-by-1 output
vector g = ATf. The fact that the transformation
can be viewed as a two-layer neural network leads
to the method’s name.
What differentiates S2Net from other approaches
is its loss function and optimization process. In
the “parallel text” setting, the labeled data con-
sists of pairs of similar text objects such as doc-
uments. The objective of the training process is
to assign higher cosine similarities to these pairs
compared to others. More specifically, suppose the
training set consists of m pairs of raw input vectors
{(fp1, fq1), (fp2, fq2), ··· , (fpm, fqm)}. Given a pro-
jection matrix A, the similarity score of any pair of
objects is simA(fpi, fqj) = cosine(AT fpi, AT fqj).
Let Δij = simA(fpi, fqi) − simA(fpi, fqj) be the
difference of the similarity scores of (fpi, fqi) and
(fpi, fqj). The learning procedure tries to increase
Δij by using the following logistic loss:
</bodyText>
<equation confidence="0.853376">
L(Δij; A) = log(1 + exp(−-&apos;Δij)),
</equation>
<bodyText confidence="0.999453148148148">
where -y is a scaling factor that adjusts the loss func-
tion2. The loss of the whole training set is thus:
by standard gradient-based methods, such as L-
BFGS (Nocedal and Wright, 2006).
The original setting of S2Net can be directly ap-
plied to finding synonymous words, where the train-
ing data consists of pairs of vectors representing
two synonyms. It is also easy to modify the loss
function to apply it to the antonym detection prob-
lem. We first sample pairs of antonyms from the
thesaurus to create the training data. The raw input
vector f of a selected word is its corresponding col-
umn vector of the document-term matrix W (Sec-
tion 3) after inducing polarity (Section 4). When
each pair of vectors in the training data represents
two antonyms, we can redefine Δij by flipping the
sign: Δij = simA(fpi, fqj) − simA(fpi, fqi), and
leave others unchanged. As the loss function encour-
ages Δij to be larger, an antonym pair will tend to
have a smaller cosine similarity than other pairs. Be-
cause S2Net uses a gradient descent technique and a
non-convex objective function, it is sensitive to ini-
tialization, and we have found that the PILSA pro-
jection matrix U (Section 3) provides an excellent
starting point. As illustrated in Section 7, learning
the word vectors with S2Net produces a significant
improvement over PILSA alone.
</bodyText>
<figure confidence="0.918659">
1
L(Δij; A)
m(m − 1)
6 Extending PILSA to Out-of-thesaurus
Words
1&lt;i,j&lt;M,i=,4j
</figure>
<footnote confidence="0.790495666666667">
Parameter learning (i.e., tuning A) can be done
2As suggested in (Yih et al., 2011), -y is set to 10 in our
experiments.
</footnote>
<bodyText confidence="0.6426035">
While PILSA is effective at representing synonym
and antonym information, in its pure form, it is lim-
ited to the vocabulary of the thesaurus. In order to
extend PILSA to operate on out-of-thesaurus words,
</bodyText>
<page confidence="0.98866">
1217
</page>
<bodyText confidence="0.999951875">
we employ a two-stage strategy. We first conduct
some lexical analysis and try to match an unknown
word to one or more in-thesaurus words in their lem-
matized forms. If no such match can be found,
we then attempt to find semantically related in-
thesaurus words by leveraging co-occurrence statis-
tics from general text data. These two steps are de-
scribed in detail below.
</bodyText>
<subsectionHeader confidence="0.99911">
6.1 Matching via Lexical Analysis
</subsectionHeader>
<bodyText confidence="0.999884423076923">
When a target word is not included in a thesaurus, it
is quite often that some of its morphological varia-
tions are covered. For example, although the Encarta
thesaurus does not have the word “corruptibility,”
it does contain other forms like “corruptible” and
“corruption.” Replacing the out-of-thesaurus target
word with these morphological variations may alter
the part-of-speech but typically does not change the
meaning3.
Given an out-of-thesaurus target word, we first
apply a morphological analyzer for English devel-
oped by Minnen et al. (2001), which removes the
inflectional affixes and returns the lemma. If the
lemma still does not exist in the thesaurus, we then
apply Porter’s stemmer (Porter, 1980) and check
whether the target word can match any of the in-
thesaurus words in their stemmed forms. A sim-
ple rule that checks whether removing hyphens from
words can lead to a match and whether the target
word occurs as part of a compound word in the the-
saurus is applied when both morphological analysis
and stemming fail to find a match. When there are
more than one matched words, the centroid of their
PILSA vectors is used to represent the target word.
When there is only one matched word, the matched
word is treated as the target word.
</bodyText>
<subsectionHeader confidence="0.999762">
6.2 Leveraging General Text Data
</subsectionHeader>
<bodyText confidence="0.99975625">
If no words in the thesaurus can be linked to the
target word through the simple lexical analysis pro-
cedure, we try to find matched words by creating
a context vector space model from a large docu-
ment collection, and then mapping from this space
to the PILSA space. We use contexts because of the
distributional hypothesis – words that occur in the
same contexts tend to have similar meaning (Harris,
</bodyText>
<footnote confidence="0.9462945">
3The rules we use based on lexical analysis are moderately
conservative to avoid mistakes like mapping hopeless to hope.
</footnote>
<bodyText confidence="0.9977422">
1954). When a word is not in the thesaurus but ap-
pears in the corpus, we predict its PILSA vector rep-
resentation from the context vector space model by
using its k-nearest neighbors which are in the the-
saurus and consistent with each other.
</bodyText>
<subsectionHeader confidence="0.929969">
6.2.1 Context Vector Space Model
</subsectionHeader>
<bodyText confidence="0.999982642857143">
Given a corpus of documents, we construct the
raw context vectors as follows. For each target word,
we first create a bag of words by collecting all the
terms within a window of [-10,+10] centered at each
occurrence of the target word in the corpus. The
non-identical terms form a term-vector, where each
term is weighted using its TF-IDF value. We then
perform LSA on the context-word matrix. The se-
mantic similarity/relatedness of two words can then
be determined using the cosine similarity of their
corresponding LSA word vectors. In the following
text, we refer to this LSA context vector space model
as the corpus space, in contrast to the PILSA the-
saurus space.
</bodyText>
<subsectionHeader confidence="0.940707">
6.2.2 Embedding Out-of-Vocabulary Words
</subsectionHeader>
<bodyText confidence="0.99980448">
Given the context space model, we may use a
linear regression or a k-nearest neighbors approach
to embed out-of-thesaurus words into the thesaurus-
space representation. However, as near words in the
context space may be synonyms in addition to other
semantically related words (including antonyms),
such approaches can potentially be noisy. For ex-
ample, words like “hot” and “cold” may be close
to each other in the context space due to their sim-
ilar usage in text. An affine transform cannot “tear
space” and map them to opposite poles in the the-
saurus space.
Therefore, we propose a revised k-nearest neigh-
bors approach. Suppose we are interested in an out-
of-thesaurus word w. We first find K-nearest in-
thesaurus neighbors to w in the context space. We
then select a subset of k members of these K words
such that the pairwise similarity of each of the k
members with every other member is positive. The
thesaurus-space centroid of these k items is com-
puted as w’s representation. This procedure has the
property that the k nearby words used to form the
embedding of a non-thesaurus word are selected to
be consistent with each other. In practice, we used
K = 10 and k = 3, which requires only around
</bodyText>
<page confidence="0.979947">
1218
</page>
<bodyText confidence="0.999534666666667">
1000 pairwise computations even done in a brute-
force way. To provide a concrete example, if we
had the out-of-thesaurus word “sweltering” with in-
thesaurus neighbors “hot, cold, burning, scorching,
...” the procedure would return the centroid of “hot,
burning, scorching” and exclude “cold.”
</bodyText>
<sectionHeader confidence="0.996576" genericHeader="method">
7 Experimental Validation
</sectionHeader>
<bodyText confidence="0.999854666666667">
In this section, we present our experimental results
on applying PILSA and its extensions to answering
the closest-opposite GRE questions.
</bodyText>
<subsectionHeader confidence="0.995653">
7.1 Data Resources
</subsectionHeader>
<bodyText confidence="0.999989142857143">
The primary thesaurus we use is the Encarta The-
saurus developed by Bloomsbury Publishing Plc4.
Our version of this has approximately 47k word
senses and a vocabulary of 50k words, and con-
tains 125,724 pairs of antonyms. To experiment with
the effect of using a different thesaurus, we used
WordNet as an information source. Each synset in
WordNet maps to a row in the document-term ma-
trix; synonyms in a synset are weighted with posi-
tive TFIDF values, and antonyms are weighted neg-
ative TFIDF values. Entries corresponding to other
words in the vocabulary are 0. WordNet provides
significantly greater coverage with approximately
227k synsets involving multiple words, and a vo-
cabulary of about 190k words. However, it is also
much sparser, with 5.3 words per sense on average
as opposed to 10.3 in the thesaurus, and has only
62,821 pairs of antonyms. As general text data for
use in embedding out-of-vocabulary words, we used
a Nov-2010 dump of English Wikipedia, which con-
tains approximately 917M words.
</bodyText>
<subsectionHeader confidence="0.998142">
7.2 Development and Test Data
</subsectionHeader>
<bodyText confidence="0.997872">
For testing, we use the closest-opposite questions
from GRE tests provided by (Mohammed et al.,
2008). Each question contains a target word and
five choices, and asks which of the choice words has
the most opposite meaning to the target word. Two
datasets are made publicly available by Mohammad
et al. (2008): the development set, which consists of
162 questions, and the test set, which has 950 ques-
tions5. We considered making our own, more exten-
</bodyText>
<footnote confidence="0.911991333333333">
4http://www.bloomsbury.com/
5http://www.umiacs.umd.edu/—saif/WebDocs/LC-
data/{devset,testset}.txt
</footnote>
<table confidence="0.999867272727273">
Dimensions Bloomsbury Prec. WordNet Prec.
50 0.778 0.475
100 0.850 0.563
200 0.856 0.569
300 0.863 0.625
400 0.843 0.625
500 0.843 0.613
750 0.830 0.613
1000 0.837 0.544
2000 0.784 0.519
3000 0.778 0.494
</table>
<tableCaption confidence="0.749690666666667">
Table 5: The performance of PILSA vs. the number of di-
mensions when applied to the closest-opposite questions
from the GRE development set. Out of the 162 ques-
</tableCaption>
<bodyText confidence="0.910915482758621">
tions, using the Bloomsbury thesaurus data we are able
to answer 153 of them. Using 300 dimensions gives the
best precision (132/153 = 0.863). This dimension set-
ting is also optimal when using the WordNet data, which
answers 100 questions correctly out of the 160 attempts
(100/160 = 0.625).
sive, test – for example one which would require the
use of sentence context to choose between related
yet distributionally different antonyms (e.g. “little,
small” as antonyms of “big”) but chose to stick to a
previously used benchmark. This allows the direct
comparison with previously reported methods.
Some of these questions contain very rarely used
target or choice words, which are not included in
the thesaurus vocabulary. In order to provide a fair
comparison to existing methods, we do not try to
randomly answer these questions. Instead, when the
target word is out of vocabulary, we skip the whole
question. When the target word is in vocabulary but
one or more choices are unknown words, we ignore
those unknown words and pick the word with the
lowest cosine similarity from the rest as the answer.
The results of our methods are reported in precision
(the number of questions answered correctly divided
by the number of questions attempted), recall (the
number of questions answered correctly divided by
the number of all questions) and Fi (the harmonic
mean of precision and recall)6. We now turn to an
in-depth evaluation.
</bodyText>
<footnote confidence="0.579405">
6Precision/recall/F, were used in (Mohammed et al., 2008)
as when their system “did not find any evidence of antonymy
between the target and any of its alternatives, then it refrained
from attempting that question.” We adopt this convention to
provide a fair comparison to their system.
</footnote>
<page confidence="0.962235">
1219
</page>
<table confidence="0.999775181818182">
Dev. Set Test Set
Prec Rec F1 Prec Rec F1
WordNet lookup 0.40 0.40 0.40 0.42 0.41 0.42
WordNet signed-TFIDF w/o LSA 0.41 0.41 0.41 0.43 0.42 0.43
WordNet PILSA 0.63 0.62 0.62 0.60 0.60 0.60
Bloomsbury lookup 0.65 0.61 0.63 0.61 0.56 0.59
Bloomsbury signed TFIDF w/o LSA 0.68 0.64 0.66 0.63 0.57 0.60
Bloomsbury PILSA 0.86 0.81 0.84 0.81 0.74 0.77
Bloomsbury PILSA + S2Net 0.89 0.84 0.86 0.84 0.77 0.80
Bloomsbury PILSA + S2Net + Embedding 0.88 0.87 0.87 0.81 0.80 0.81
(Mohammed et al., 2008) 0.76 0.66 0.70 0.76 0.64 0.70
</table>
<tableCaption confidence="0.999781">
Table 6: The overall results. PILSA performs LSA on the signed TF-IDF vectors.
</tableCaption>
<subsectionHeader confidence="0.994156">
7.3 Basic PILSA
</subsectionHeader>
<bodyText confidence="0.966424772727273">
When applying PILSA, we need to determine the
number of dimensions in the projected space. Eval-
uated on the GRE development set, Table 5 shows
the precision of PILSA, using two different training
datasets, Bloomsbury and WordNet, at different di-
mensions.
The Bloomsbury-based system is able to answer
153 questions, and the best dimension setting is
300, which answers 132 questions correctly and thus
archives 0.863 in precision. In contrast, the larger
vocabulary in WordNet helps the system answer 160
questions but the quality is not as good. We find
dimensions 300 and 400 are equally good, where
both answer 100 questions correctly (0.625 in pre-
cision)7. Because a lower number of dimensions
is preferred for saving storage space and computing
time, we choose 300 as the number of dimensions in
PILSA.
We now compare the proposed methods. All re-
sults are summarized in Table 6. When evaluated on
the GRE test set, the Bloomsbury thesaurus-based
methods (Lines 4–7) attempted 865 questions. The
precision, recall and F1 of the Bloomsbury-based
PILSA model (Line 6) are 0.81, 0.74 and 0.77,
which are all better than the best reported method
in (Mohammed et al., 2008)8. In contrast, the
WordNet-based methods (Lines 1–3) attempted 936
7Note that the number of questions attempted is not a func-
tion of the number of dimensions.
8We take a conservative approach and assume that skipped
questions are answered incorrectly. The difference is statisti-
cally significant at 99% confidence level using a binomial test.
questions. However, consistent with what we ob-
served on the development set, the WordNet-based
model is inferior. Its precision, recall and F1 on
the test set are 0.60, 0.60 and 0.60 (Line 3). Al-
though the quality of the data source plays an im-
portant role, we need to emphasize that performing
LSA using our polarity inducing construction is in
fact a critical step in enhancing the model perfor-
mance. For example, directly using the antonym sets
in the Bloomsbury thesaurus gives 0.59 in F1 (Line
4), while using cosine similarity on the signed vec-
tors prior to LSA only reaches 0.60 in F1 (Line 5).
</bodyText>
<subsectionHeader confidence="0.4965695">
7.4 Improving Precision with Discriminative
Training
</subsectionHeader>
<bodyText confidence="0.999985363636364">
Building on the success of the unsupervised PILSA
model, we refine the projection matrix. As described
in Section 5, we take the PILSA projection matrix
as the initial model in S2Net and train the model
using 20,517 pairs of antonyms sampled from the
Bloomsbury thesaurus. A separate sample of 5,000
antonym pairs is used as the validation set for hyper-
parameter tuning in regularization. Encouragingly,
we found that the already strong results of PILSA
can indeed be improved, which gives 3 more points
in both precision (0.84), recall (0.77) and F1 (0.80).
</bodyText>
<subsectionHeader confidence="0.965168">
7.5 Improving Recall with Unsupervised Data
</subsectionHeader>
<bodyText confidence="0.9998882">
We next evaluate our approach of extending the
word coverage with the help of an external text cor-
pus, as well as the lexical analysis procedure. Using
the Bloomsbury PILSA-S2Net thesaurus space and
the Wikipedia corpus space, our method increases
</bodyText>
<page confidence="0.958602">
1220
</page>
<bodyText confidence="0.99996055">
recall by 3 points on the test set. Compared to the in-
vocabulary only setting, it attempted 75 more ques-
tions (865 —* 940) and had 33 of them correctly an-
swered.
While the accuracy on these questions is much
higher than random, the fact that it is substantially
below the precision of the original indicates some
room for improvement. We notice that the out-of-
thesaurus words are either offensive words excluded
in the thesaurus (e.g., moronic) or some very rarely
used words (e.g., froward). When the lexical analy-
sis procedure fails to match the target word to some
in-thesaurus words, the context vector embedding
approach solves the former case, but has difficulty
in handling the latter. The main reason is that such
words occur very infrequently in a general corpus,
which result in significant uncertainty in their se-
mantic vectors. Other than using a much larger
corpus, approaches that leverage character n-grams
may help. We leave this as future work.
</bodyText>
<sectionHeader confidence="0.997306" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999638523809524">
In this paper we have tackled the problem of find-
ing a vector-space representation of words where,
by construction, synonyms and antonyms are easy
to distinguish. Specifically, we have defined a way
of assigning sign to the entries in the co-occurrence
matrix on which LSA operates, such that synonyms
will tend to have positive cosine similarity, and
antonyms will tend to have negative similarities. To
the best of our knowledge, our method of inducing
polarity to the document-term matrix before apply-
ing LSA is novel and has shown to effectively pre-
serve and generalize the synonymous/antonymous
information in the projected space. With this vector
space representation, we were able to bring to bear
the machinery of discriminative training in order to
further optimize the word representations. Finally,
by using the notion of closeness in this space, we
were able to embed new out-of-vocabulary words
into the space. On a standard test set, the proposed
methods improved the F measure by 11 points abso-
lute over previous results.
</bodyText>
<sectionHeader confidence="0.998299" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999074">
We thank Susan Dumais for her thoughtful com-
ments, Silviu-Petru Cucerzan for preparing the
Wikipedia data, and Saif Mohammed for sharing the
GRE datasets. We are also grateful to the anony-
mous reviewers for their useful suggestions.
</bodyText>
<sectionHeader confidence="0.9896" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.992599911111111">
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana
Kravalova, Marius Pas¸ca, and Aitor Soroa. 2009.
A study on similarity and relatedness using distribu-
tional and wordnet-based approaches. In Proceedings
of HLT-NAACL, pages 19–27.
Ricardo Baeza-Yates and Berthier Ribiero-Neto. 1999.
Modern Information Retrieval. Addison-Wesley.
J. Bellegarda. 2000. Exploiting latent semantic informa-
tion in statistical language modeling. Proceedings of
the IEEE, 88(8).
Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram
Version 1. Linguistic Data Consortium.
N. Coccaro and D. Jurafsky. 1998. Towards better in-
tegration of semantic predictors in statistical language
modeling. In Proceedings, International Conference
on Spoken Language Processing (ICSLP-98).
D. A. Cruse. 1986. Lexical Semantics. Cambridge Uni-
versity Press.
James R. Curran and Marc Moens. 2002. Improvements
in automatic thesaurus extraction. In Proceedings of
the ACL-02 workshop on Unsupervised lexical acqui-
sition - Volume 9, pages 59–66. Association for Com-
putational Linguistics.
Thomas de Simone and Dimitar Kazakov. 2005. Using
wordnet similarity and antonymy relations to aid doc-
ument retrieval. In Recent Advances in Natural Lan-
guage Processing (RANLP).
S. Deerwester, S.T. Dumais, G.W. Furnas, T.K. Landauer,
and R. Harshman. 1990. Indexing by latent semantic
analysis. Journal of the American Society forInforma-
tion Science, 41(96).
E. Gabrilovich and S. Markovitch. 2007. Computing se-
mantic relatedness using wikipedia-based explicit se-
mantic analysis. In AAAI Conference on Artificial In-
telligence (AAAI).
Sanda Harabagiu, Andrew Hickl, and Finley Lacatusu.
2006. Negation, contrast and contradiction in text pro-
cessing. In AAAI Conference on Artificial Intelligence
(AAAI).
Zelig Harris. 1954. Distributional structure. Word,
10(23):146–162.
Mario Jarmasz and Stan Szpakowicz. 2003. Rogets the-
saurus and semantic similarity. In Proceedings of the
International Conference on Recent Advances in Nat-
ural Language Processing (RANLP-2003).
</reference>
<page confidence="0.780081">
1221
</page>
<reference confidence="0.999714560747663">
Thomas Landauer and Susan Dumais. 1997. A solution
to plato’s problem: The latent semantic analysis the-
ory of the acquisition, induction, and representation of
knowledge. Psychological Review, 104(2), pages 211–
240.
T.K. Landauer and D. Laham. 1998. Learning human-
like knowledge by singular value decomposition: A
progress report. In Neural Information Processing
Systems (NIPS).
Thomas K. Landauer, Peter W. Foltz, and Darrell La-
ham. 1998. An introduction to latent semantic analy-
sis. Discourse Processes, 25, pages 259–284.
T.K. Landauer. 2002. On the computational basis of
learning and cognition: Arguments from lsa. Psychol-
ogy of Learning and Motivation, 41:43–84.
Dekang Lin, Shaojun Zhao, Lijuan Qin, and Ming Zhou.
2003. Identifying synonyms among distributionally
similar words. In International Joint Conference on
Artificial Intelligence (IJCAI).
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of the 36th Annual
Meeting of the Association for Computational Linguis-
tics and 17th International Conference on Computa-
tional Linguistics - Volume 2, ACL ’98, pages 768–
774, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
David Milne and Ian H. Witten. 2008. An effective low-
cost measure of semantic relatedness obtained from
wikipedia links. In Proceedings of the AAAI 2008
Workshop on Wikipedia and Artificial Intelligence.
G. Minnen, J. Carroll, and D. Pearce. 2001. Applied
morphological processing of english. Natural Lan-
guage Engineering, 7(3):207–223.
Saif Mohammed, Bonnie Dorr, and Graeme Hirst. 2008.
Computing word pair antonymy. In Empirical Meth-
ods in Natural Language Processing (EMNLP).
Saif M. Mohammed, Bonnie J. Dorr, Graeme Hirst, and
Peter D. Turney. 2011. Measuring degrees of seman-
tic opposition. Technical report, National Research
Council Canada.
Gregory L. Murphy and Jane M. Andrew. 1993. The
conceptual basis of antonymy and synonymy in adjec-
tives. Journal of Memory and Language, 32(3):1–19.
Jorge Nocedal and Stephen Wright. 2006. Numerical
Optimization. Springer, 2nd edition.
John Platt, Kristina Toutanova, and Wen-tau Yih. 2010.
Translingual document representations from discrimi-
native projections. In Proceedings of EMNLP, pages
251–261.
Hoifung Poon and Pedro Domingos. 2009. Unsuper-
vised semantic parsing. In Empirical Methods in Nat-
ural Language Processing (EMNLP).
Martin F. Porter. 1980. An algorithm for suffix stripping.
Program, 14(3):130–137.
Joseph Reisinger and Raymond J. Mooney. 2010. Multi-
prototype vector-space models of word meaning. In
Proceedings of HLT-NAACL, pages 109–117.
Gerard Salton and Michael J. McGill. 1983. Introduction
to Modern Information Retrieval. McGraw Hill.
G. Salton, A. Wong, and C. S. Yang. 1975. A Vector
Space Model for Automatic Indexing. Communica-
tions of the ACM, 18(11).
D. Schwab, M. Lafourcade, and V. Prince. 2002.
Antonymy and conceptual vectors. In International
Conference on Computational Linguistics (COLING).
Peter Turney and Michael Littman. 2005. Corpus-based
learning of analogies and semantic relations. Machine
Learning, 60 (1-3), pages 251–278.
Peter D. Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of semantics.
Journal of Artificial Intelligence Research, (37).
Peter D. Turney, Michael L. Littman, Jeffrey Bigham,
and Victor Shnayder. 2003. Combining independent
modules to solve multiple-choice synonym and anal-
ogy problems. In Recent Advances in Natural Lan-
guage Processing (RANLP).
Peter D. Turney. 2001. Mining the web for synonyms:
Pmi-ir versus lsa on toefl. In European Conference on
Machine Learning (ECML).
P. D. Turney. 2006. Similarity of semantic relations.
Computational Linguistics, 32(3):379–416.
Peter Turney. 2008. A uniform approach to analo-
gies, synonyms, antonyms, and associations. In In-
ternational Conference on Computational Linguistics
(COLING).
Lonneke van der Plas and Gosse Bouma. 2005. Syntac-
tic contexts for finding semantically similar words. In
Proceedings of the Meeting of Computational Linguis-
tics in the Netherlands 2004 (CLIN).
Lonneke van der Plas and J¨org Tiedemann. 2006. Find-
ing synonyms using automatic word alignment and
measures of distributional similarity. In Proceedings
of the COLING/ACL on Main conference poster ses-
sions, COLING-ACL ’06, pages 866–873. Associa-
tion for Computational Linguistics.
Wei Xu, Xin Liu, and Yihong Gong. 2003. Document
clustering based on non-negative matrix factorization.
In Proceedings of the 26th annual international ACM
SIGIR conference on Research and development in in-
formaion retrieval, pages 267–273, New York, NY,
USA. ACM.
Wen-tau Yih, Kristina Toutanova, John C. Platt, and
Christopher Meek. 2011. Learning discriminative
projections for text similarity measures. In Proceed-
ings of the Fifteen Conference on Computational Nat-
ural Language Learning (CoNLL), pages 247–256,
Portland, Oregon, USA.
</reference>
<page confidence="0.992104">
1222
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.734160">
<title confidence="0.999545">Polarity Inducing Latent Semantic Analysis</title>
<author confidence="0.998432">Wen-tau Yih Geoffrey Zweig John C Platt</author>
<affiliation confidence="0.967765">Microsoft</affiliation>
<address confidence="0.891464">One Microsoft Redmond, WA 98052,</address>
<abstract confidence="0.998464275862069">Existing vector space models typically map synonyms and antonyms to similar word vectors, and thus fail to represent antonymy. We introduce a new vector space representation where antonyms lie on opposite sides of a sphere: in the word vector space, synonyms have cosine similarities close to one, while antonyms are close to minus one. We derive this representation with the aid of a thesaurus and latent semantic analysis (LSA). Each entry in the thesaurus – a word sense along with its synonyms and antonyms – is treated as a “document,” and the resulting document collection is subjected to LSA. The key contribution of this work is to show how to assign signs to the entries in the co-occurrence matrix on which LSA operates, so as to induce a subspace with the desired property. We evaluate this procedure with the Graduate Record Examination questions of (Mohammed et al., 2008) and find that the method improves on the results of that study. Further improvements result from refining the subspace representation with discriminative training, and augmenting the training data with general newspaper text. Altogether, we improve on the best previous results by 11 points absolute in F measure.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Enrique Alfonseca</author>
<author>Keith Hall</author>
<author>Jana Kravalova</author>
<author>Marius Pas¸ca</author>
<author>Aitor Soroa</author>
</authors>
<title>A study on similarity and relatedness using distributional and wordnet-based approaches.</title>
<date>2009</date>
<booktitle>In Proceedings of HLT-NAACL,</booktitle>
<pages>pages</pages>
<marker>Agirre, Alfonseca, Hall, Kravalova, Pas¸ca, Soroa, 2009</marker>
<rawString>Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana Kravalova, Marius Pas¸ca, and Aitor Soroa. 2009. A study on similarity and relatedness using distributional and wordnet-based approaches. In Proceedings of HLT-NAACL, pages 19–27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ricardo Baeza-Yates</author>
<author>Berthier Ribiero-Neto</author>
</authors>
<title>Modern Information Retrieval.</title>
<date>1999</date>
<publisher>Addison-Wesley.</publisher>
<contexts>
<context position="2134" citStr="Baeza-Yates and Ribiero-Neto, 1999" startWordPosition="334" endWordPosition="337">lications ranging from document clustering to search relevance measurement. In these applications, text is represented as a vector in a multi-dimensional continuous space, and a similarity metric such as cosine similarity can be used to measure the relatedness of different items. Vector space representations have been used both at the document and word levels. At the document level, they are effective for applications including information retrieval (Salton and McGill, 1983; Deerwester et al., 1990), document clustering (Deerwester et al., 1990; Xu et al., 2003), search relevance measurement (Baeza-Yates and Ribiero-Neto, 1999) and cross-lingual document retrieval (Platt et al., 2010). At the word level, vector representations have been used to measure word similarity (Deerwester et al., 1990; Turney and Littman, 2005; Turney, 2006; Turney, 2001; Lin, 1998; Agirre et al., 2009; Reisinger and Mooney, 2010) and for language modeling (Bellegarda, 2000; Coccaro and Jurafsky, 1998). While quite successful, these applications have typically been consistent with a very general notion of similarity in which basic association is measured, and finer shades of meaning need not be distinguished. For example, latent semantic ana</context>
</contexts>
<marker>Baeza-Yates, Ribiero-Neto, 1999</marker>
<rawString>Ricardo Baeza-Yates and Berthier Ribiero-Neto. 1999. Modern Information Retrieval. Addison-Wesley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Bellegarda</author>
</authors>
<title>Exploiting latent semantic information in statistical language modeling.</title>
<date>2000</date>
<booktitle>Proceedings of the IEEE,</booktitle>
<volume>88</volume>
<issue>8</issue>
<contexts>
<context position="2461" citStr="Bellegarda, 2000" startWordPosition="386" endWordPosition="387">and word levels. At the document level, they are effective for applications including information retrieval (Salton and McGill, 1983; Deerwester et al., 1990), document clustering (Deerwester et al., 1990; Xu et al., 2003), search relevance measurement (Baeza-Yates and Ribiero-Neto, 1999) and cross-lingual document retrieval (Platt et al., 2010). At the word level, vector representations have been used to measure word similarity (Deerwester et al., 1990; Turney and Littman, 2005; Turney, 2006; Turney, 2001; Lin, 1998; Agirre et al., 2009; Reisinger and Mooney, 2010) and for language modeling (Bellegarda, 2000; Coccaro and Jurafsky, 1998). While quite successful, these applications have typically been consistent with a very general notion of similarity in which basic association is measured, and finer shades of meaning need not be distinguished. For example, latent semantic analysis might assign a high degree of similarity to opposites as well as synonyms (Landauer and Laham, 1998; Landauer, 2002). Independent of vector-space representations, a number of authors have focused on identifying different kinds of relatedness. At the simplest level, we may wish to distinguish between synonyms and antonym</context>
<context position="12107" citStr="Bellegarda, 2000" startWordPosition="1988" endWordPosition="1989"> the ijth entry of WT W is the dot product of the ith and jth columns (words) in W. We will use this observation subsequently in the derivation of polarity-inducing LSA. For efficiency, we normalize the columns of SVT to unit length, allowing the cosine similarity between two words to be computed with a single dot-product; this also has the property of mapping each word to a point on a multidimensional sphere. A second important property of LSA is that in the word representations which result can by viewed as the result of applying a projection matrix U to the original vectors as: UTW = SVT 1(Bellegarda, 2000) constructs the transpose of this, but we have found it convenient in data processing for documents to represent rows. II x IIII y II 1214 In Section 5, we will view U simply as a dxk matrix learned through gradient descent so as to optimize an objective function. 3.1 Limitation of LSA Word similarity as determined by LSA assigns high values to words which tend to co-occur in documents. However, as noted by (Landauer and Laham, 1998; Landauer, 2002), there is no notion of antonymy; words with low or negative cosine scores are simply unrelated. In comparison, words with high cosine similarity s</context>
</contexts>
<marker>Bellegarda, 2000</marker>
<rawString>J. Bellegarda. 2000. Exploiting latent semantic information in statistical language modeling. Proceedings of the IEEE, 88(8).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
<author>Alex Franz</author>
</authors>
<title>Web 1T 5-gram Version 1. Linguistic Data Consortium.</title>
<date>2006</date>
<contexts>
<context position="6346" citStr="Brants and Franz, 2006" startWordPosition="1003" endWordPosition="1006">entation with supervised discriminative training. Section 6 describes the proposed method of embedding new words in the thesaurus-derived subspace. The experimental results of Section 7 indicate that the proposed method outperforms previous approaches on a GRE test of closest-opposites (Mohammed et al., 2008). Finally, Section 8 concludes the paper. 2 Related Work The detection of antonymy has been studied in a number of previous papers. Mohammed et al. (2008) approach the problem by combining information from a published thesaurus with corpus statistics derived from the Google n-gram corpus (Brants and Franz, 2006). Their method consists of two main steps: first, detecting contrasting word categories (e.g. “WORK” vs. “ACTIVITY FOR FUN”) and then determining the degree of antonymy. Categories are defined by a thesaurus; contrasting categories are found by using affix rules (e.g., un- &amp; dis-) and WordNet antonymy links. Words belonging to contrasting categories are treated as antonyms and the degree of contrast is determined by distributional similarity. Mohammed et al. (2008) also provides a publicly available dataset for detection of antonymy, which we have adopted. This work has been extended in (Moham</context>
</contexts>
<marker>Brants, Franz, 2006</marker>
<rawString>Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram Version 1. Linguistic Data Consortium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Coccaro</author>
<author>D Jurafsky</author>
</authors>
<title>Towards better integration of semantic predictors in statistical language modeling.</title>
<date>1998</date>
<booktitle>In Proceedings, International Conference on Spoken Language Processing (ICSLP-98).</booktitle>
<contexts>
<context position="2490" citStr="Coccaro and Jurafsky, 1998" startWordPosition="388" endWordPosition="391">t the document level, they are effective for applications including information retrieval (Salton and McGill, 1983; Deerwester et al., 1990), document clustering (Deerwester et al., 1990; Xu et al., 2003), search relevance measurement (Baeza-Yates and Ribiero-Neto, 1999) and cross-lingual document retrieval (Platt et al., 2010). At the word level, vector representations have been used to measure word similarity (Deerwester et al., 1990; Turney and Littman, 2005; Turney, 2006; Turney, 2001; Lin, 1998; Agirre et al., 2009; Reisinger and Mooney, 2010) and for language modeling (Bellegarda, 2000; Coccaro and Jurafsky, 1998). While quite successful, these applications have typically been consistent with a very general notion of similarity in which basic association is measured, and finer shades of meaning need not be distinguished. For example, latent semantic analysis might assign a high degree of similarity to opposites as well as synonyms (Landauer and Laham, 1998; Landauer, 2002). Independent of vector-space representations, a number of authors have focused on identifying different kinds of relatedness. At the simplest level, we may wish to distinguish between synonyms and antonyms, which can be further diffe</context>
</contexts>
<marker>Coccaro, Jurafsky, 1998</marker>
<rawString>N. Coccaro and D. Jurafsky. 1998. Towards better integration of semantic predictors in statistical language modeling. In Proceedings, International Conference on Spoken Language Processing (ICSLP-98).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D A Cruse</author>
</authors>
<title>Lexical Semantics.</title>
<date>1986</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="3200" citStr="Cruse (1986)" startWordPosition="497" endWordPosition="498">tion of similarity in which basic association is measured, and finer shades of meaning need not be distinguished. For example, latent semantic analysis might assign a high degree of similarity to opposites as well as synonyms (Landauer and Laham, 1998; Landauer, 2002). Independent of vector-space representations, a number of authors have focused on identifying different kinds of relatedness. At the simplest level, we may wish to distinguish between synonyms and antonyms, which can be further differentiated. For example, in synonymy, we may wish to distinguish hyponyms and hypernyms. Moreover, Cruse (1986) notes that numerous kinds of antonymy are possible, for example antipodal pairs like “top-bottom” or 1212 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1212–1222, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics gradable opposites like “light-heavy.” Work in this area includes (Turney, 2001; Lin et al., 2003; Turney and Littman, 2005; Turney, 2006; Curran and Moens, 2002; van der Plas and Tiedemann, 2006; Mohammed et al., 2008; Mohammed et al., 2011). Despite </context>
</contexts>
<marker>Cruse, 1986</marker>
<rawString>D. A. Cruse. 1986. Lexical Semantics. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James R Curran</author>
<author>Marc Moens</author>
</authors>
<title>Improvements in automatic thesaurus extraction.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL-02 workshop on Unsupervised lexical acquisition - Volume 9,</booktitle>
<pages>59--66</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="3709" citStr="Curran and Moens, 2002" startWordPosition="568" endWordPosition="571">fferentiated. For example, in synonymy, we may wish to distinguish hyponyms and hypernyms. Moreover, Cruse (1986) notes that numerous kinds of antonymy are possible, for example antipodal pairs like “top-bottom” or 1212 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1212–1222, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics gradable opposites like “light-heavy.” Work in this area includes (Turney, 2001; Lin et al., 2003; Turney and Littman, 2005; Turney, 2006; Curran and Moens, 2002; van der Plas and Tiedemann, 2006; Mohammed et al., 2008; Mohammed et al., 2011). Despite the existence of a large amount of related work in the literature, distinguishing synonyms and antonyms is still considered as a difficult open problem in general (Poon and Domingos, 2009). In this paper, we fuse these two strands of research in an attempt to develop a vector space representation in which the synonymy and antonymy are naturally differentiated. We follow Schwab et al. (2002) in requiring a representation in which two lexical items in an antonymy relation should lie at opposite ends of an </context>
<context position="8224" citStr="Curran and Moens, 2002" startWordPosition="1299" endWordPosition="1302"> the same word in another language, antonyms will not. This observation is used to select antonyms from amongst distributionally similar words. Antonymy is used in (de Simone and Kazakov, 2005) for document clustering and (Harabagiu et al., 2006) to find contradiction. The automatic detection of synonyms has been more extensively studied. Lin (1998) presents a thorough comparison of word-similarity metrics based on distributional similarity, where this is de1213 termined from co-occurrence statistics in dependency triples extracted by parsing a large dataset. Related studies are described in (Curran and Moens, 2002; van der Plas and Bouma, 2005). Later, van der Plas and Tiedemann (2006) extend the use of multilingual data present in Lin et al. (2003) by measuring distributional similarity based on the contexts that a word occurs in once translated into a new language. This is used to improve the precision/recall characteristics on synonym pairs. Structured information can be important in determining relatedness, and thesauri and Wikipedia links have been studied in (Milne and Witten, 2008; Jarmasz and Szpakowicz, 2003). Combinations of approaches are studied in (Turney et al., 2003). Vector-space models</context>
</contexts>
<marker>Curran, Moens, 2002</marker>
<rawString>James R. Curran and Marc Moens. 2002. Improvements in automatic thesaurus extraction. In Proceedings of the ACL-02 workshop on Unsupervised lexical acquisition - Volume 9, pages 59–66. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas de Simone</author>
<author>Dimitar Kazakov</author>
</authors>
<title>Using wordnet similarity and antonymy relations to aid document retrieval.</title>
<date>2005</date>
<booktitle>In Recent Advances in Natural Language Processing (RANLP).</booktitle>
<marker>de Simone, Kazakov, 2005</marker>
<rawString>Thomas de Simone and Dimitar Kazakov. 2005. Using wordnet similarity and antonymy relations to aid document retrieval. In Recent Advances in Natural Language Processing (RANLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Deerwester</author>
<author>S T Dumais</author>
<author>G W Furnas</author>
<author>T K Landauer</author>
<author>R Harshman</author>
</authors>
<title>Indexing by latent semantic analysis.</title>
<date>1990</date>
<journal>Journal of the American Society forInformation Science,</journal>
<volume>41</volume>
<issue>96</issue>
<contexts>
<context position="2003" citStr="Deerwester et al., 1990" startWordPosition="316" endWordPosition="319">n F measure. 1 Introduction Vector space representations have proven useful across a wide variety of text processing applications ranging from document clustering to search relevance measurement. In these applications, text is represented as a vector in a multi-dimensional continuous space, and a similarity metric such as cosine similarity can be used to measure the relatedness of different items. Vector space representations have been used both at the document and word levels. At the document level, they are effective for applications including information retrieval (Salton and McGill, 1983; Deerwester et al., 1990), document clustering (Deerwester et al., 1990; Xu et al., 2003), search relevance measurement (Baeza-Yates and Ribiero-Neto, 1999) and cross-lingual document retrieval (Platt et al., 2010). At the word level, vector representations have been used to measure word similarity (Deerwester et al., 1990; Turney and Littman, 2005; Turney, 2006; Turney, 2001; Lin, 1998; Agirre et al., 2009; Reisinger and Mooney, 2010) and for language modeling (Bellegarda, 2000; Coccaro and Jurafsky, 1998). While quite successful, these applications have typically been consistent with a very general notion of similar</context>
<context position="9005" citStr="Deerwester et al. (1990)" startWordPosition="1426" endWordPosition="1429">ibutional similarity based on the contexts that a word occurs in once translated into a new language. This is used to improve the precision/recall characteristics on synonym pairs. Structured information can be important in determining relatedness, and thesauri and Wikipedia links have been studied in (Milne and Witten, 2008; Jarmasz and Szpakowicz, 2003). Combinations of approaches are studied in (Turney et al., 2003). Vector-space models and latent semantic analysis in particular have a long history of use in synonym detection, which in fact was suggested in some of the earliest LSA papers. Deerwester et al. (1990) defines a metric for measuring word similarity based on LSA, and it has been used in (Landauer and Dumais, 1997; Landauer et al., 1998) to answer word similarity questions derived from the Test of English as a Foreign Language (TOEFL). Turney (2001) proposes the use of point-wise mutual information in conjunction with LSA, and again presents results on synonym questions derived from the TOEFL. Variants of vector space models are further analyzed in (Turney and Littman, 2005; Turney, 2006; Turney and Pantel, 2010). 3 Latent Semantic Analysis Latent Semantic Analysis (Deerwester et al., 1990) i</context>
</contexts>
<marker>Deerwester, Dumais, Furnas, Landauer, Harshman, 1990</marker>
<rawString>S. Deerwester, S.T. Dumais, G.W. Furnas, T.K. Landauer, and R. Harshman. 1990. Indexing by latent semantic analysis. Journal of the American Society forInformation Science, 41(96).</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Gabrilovich</author>
<author>S Markovitch</author>
</authors>
<title>Computing semantic relatedness using wikipedia-based explicit semantic analysis.</title>
<date>2007</date>
<booktitle>In AAAI Conference on Artificial Intelligence (AAAI).</booktitle>
<contexts>
<context position="13628" citStr="Gabrilovich and Markovitch, 2007" startWordPosition="2236" endWordPosition="2239">. This thesaurus contains approximately 47k word senses and a vocabulary of 50k words and phrases. Each “document” is taken to be the thesaurus entry for a word-sense, including synonyms and antonyms. For example, the word “admirable” induces a document consisting of {admirable, estimable, commendable, venerable, good, splendid, worthy, marvelous, excellent, unworthy}. Note that the last word in this set is its antonym. Performing SVD on this set of thesaurus derived “meaning-documents” results in a subspace representation for each word. This form of LSA is similar to the use of Wikipedia in (Gabrilovich and Markovitch, 2007). Table 1 shows some words, their original thesaurus documents, and the most and least similar words in the LSA subspace. Several properties are apparent: • The vector-space representation of words is able to identify related words that are not explicitly present in the original thesaurus. For example, “meritorious” for “admirable” – arguably better than any of the words given in the thesaurus itself. • Similarity is based on co-occurrence, so the co-occurrence of antonyms in the thesaurusderived documents induces their presence as LSA-similar words. For example, “contemptible” is identified a</context>
</contexts>
<marker>Gabrilovich, Markovitch, 2007</marker>
<rawString>E. Gabrilovich and S. Markovitch. 2007. Computing semantic relatedness using wikipedia-based explicit semantic analysis. In AAAI Conference on Artificial Intelligence (AAAI).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanda Harabagiu</author>
<author>Andrew Hickl</author>
<author>Finley Lacatusu</author>
</authors>
<title>Negation, contrast and contradiction in text processing.</title>
<date>2006</date>
<booktitle>In AAAI Conference on Artificial Intelligence (AAAI).</booktitle>
<contexts>
<context position="7848" citStr="Harabagiu et al., 2006" startWordPosition="1245" endWordPosition="1248">then used to solve the resulting analogical problems. This is evaluated on a set of 136 ESL questions. Lin et al. (2003) builds on (Lin, 1998) and identifies antonyms as semantically related words which also happen to be found together in a database in pre-identified phrases indicating opposition. Lin et al. (2003) further note that whereas synonyms will tend to translate to the same word in another language, antonyms will not. This observation is used to select antonyms from amongst distributionally similar words. Antonymy is used in (de Simone and Kazakov, 2005) for document clustering and (Harabagiu et al., 2006) to find contradiction. The automatic detection of synonyms has been more extensively studied. Lin (1998) presents a thorough comparison of word-similarity metrics based on distributional similarity, where this is de1213 termined from co-occurrence statistics in dependency triples extracted by parsing a large dataset. Related studies are described in (Curran and Moens, 2002; van der Plas and Bouma, 2005). Later, van der Plas and Tiedemann (2006) extend the use of multilingual data present in Lin et al. (2003) by measuring distributional similarity based on the contexts that a word occurs in on</context>
</contexts>
<marker>Harabagiu, Hickl, Lacatusu, 2006</marker>
<rawString>Sanda Harabagiu, Andrew Hickl, and Finley Lacatusu. 2006. Negation, contrast and contradiction in text processing. In AAAI Conference on Artificial Intelligence (AAAI).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zelig Harris</author>
</authors>
<date>1954</date>
<journal>Distributional structure. Word,</journal>
<volume>10</volume>
<issue>23</issue>
<marker>Harris, 1954</marker>
<rawString>Zelig Harris. 1954. Distributional structure. Word, 10(23):146–162.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mario Jarmasz</author>
<author>Stan Szpakowicz</author>
</authors>
<title>Rogets thesaurus and semantic similarity.</title>
<date>2003</date>
<booktitle>In Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP-2003).</booktitle>
<contexts>
<context position="8738" citStr="Jarmasz and Szpakowicz, 2003" startWordPosition="1382" endWordPosition="1386">n dependency triples extracted by parsing a large dataset. Related studies are described in (Curran and Moens, 2002; van der Plas and Bouma, 2005). Later, van der Plas and Tiedemann (2006) extend the use of multilingual data present in Lin et al. (2003) by measuring distributional similarity based on the contexts that a word occurs in once translated into a new language. This is used to improve the precision/recall characteristics on synonym pairs. Structured information can be important in determining relatedness, and thesauri and Wikipedia links have been studied in (Milne and Witten, 2008; Jarmasz and Szpakowicz, 2003). Combinations of approaches are studied in (Turney et al., 2003). Vector-space models and latent semantic analysis in particular have a long history of use in synonym detection, which in fact was suggested in some of the earliest LSA papers. Deerwester et al. (1990) defines a metric for measuring word similarity based on LSA, and it has been used in (Landauer and Dumais, 1997; Landauer et al., 1998) to answer word similarity questions derived from the Test of English as a Foreign Language (TOEFL). Turney (2001) proposes the use of point-wise mutual information in conjunction with LSA, and aga</context>
</contexts>
<marker>Jarmasz, Szpakowicz, 2003</marker>
<rawString>Mario Jarmasz and Stan Szpakowicz. 2003. Rogets thesaurus and semantic similarity. In Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP-2003).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Landauer</author>
<author>Susan Dumais</author>
</authors>
<title>A solution to plato’s problem: The latent semantic analysis theory of the acquisition, induction, and representation of knowledge.</title>
<date>1997</date>
<journal>Psychological Review,</journal>
<volume>104</volume>
<issue>2</issue>
<pages>211--240</pages>
<contexts>
<context position="9117" citStr="Landauer and Dumais, 1997" startWordPosition="1446" endWordPosition="1450">sed to improve the precision/recall characteristics on synonym pairs. Structured information can be important in determining relatedness, and thesauri and Wikipedia links have been studied in (Milne and Witten, 2008; Jarmasz and Szpakowicz, 2003). Combinations of approaches are studied in (Turney et al., 2003). Vector-space models and latent semantic analysis in particular have a long history of use in synonym detection, which in fact was suggested in some of the earliest LSA papers. Deerwester et al. (1990) defines a metric for measuring word similarity based on LSA, and it has been used in (Landauer and Dumais, 1997; Landauer et al., 1998) to answer word similarity questions derived from the Test of English as a Foreign Language (TOEFL). Turney (2001) proposes the use of point-wise mutual information in conjunction with LSA, and again presents results on synonym questions derived from the TOEFL. Variants of vector space models are further analyzed in (Turney and Littman, 2005; Turney, 2006; Turney and Pantel, 2010). 3 Latent Semantic Analysis Latent Semantic Analysis (Deerwester et al., 1990) is a widely used method for representing words and documents in a low dimensional vector space. The method is bas</context>
</contexts>
<marker>Landauer, Dumais, 1997</marker>
<rawString>Thomas Landauer and Susan Dumais. 1997. A solution to plato’s problem: The latent semantic analysis theory of the acquisition, induction, and representation of knowledge. Psychological Review, 104(2), pages 211– 240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T K Landauer</author>
<author>D Laham</author>
</authors>
<title>Learning humanlike knowledge by singular value decomposition: A progress report.</title>
<date>1998</date>
<booktitle>In Neural Information Processing Systems (NIPS).</booktitle>
<contexts>
<context position="2839" citStr="Landauer and Laham, 1998" startWordPosition="442" endWordPosition="446">ctor representations have been used to measure word similarity (Deerwester et al., 1990; Turney and Littman, 2005; Turney, 2006; Turney, 2001; Lin, 1998; Agirre et al., 2009; Reisinger and Mooney, 2010) and for language modeling (Bellegarda, 2000; Coccaro and Jurafsky, 1998). While quite successful, these applications have typically been consistent with a very general notion of similarity in which basic association is measured, and finer shades of meaning need not be distinguished. For example, latent semantic analysis might assign a high degree of similarity to opposites as well as synonyms (Landauer and Laham, 1998; Landauer, 2002). Independent of vector-space representations, a number of authors have focused on identifying different kinds of relatedness. At the simplest level, we may wish to distinguish between synonyms and antonyms, which can be further differentiated. For example, in synonymy, we may wish to distinguish hyponyms and hypernyms. Moreover, Cruse (1986) notes that numerous kinds of antonymy are possible, for example antipodal pairs like “top-bottom” or 1212 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Lear</context>
<context position="12543" citStr="Landauer and Laham, 1998" startWordPosition="2064" endWordPosition="2067">ant property of LSA is that in the word representations which result can by viewed as the result of applying a projection matrix U to the original vectors as: UTW = SVT 1(Bellegarda, 2000) constructs the transpose of this, but we have found it convenient in data processing for documents to represent rows. II x IIII y II 1214 In Section 5, we will view U simply as a dxk matrix learned through gradient descent so as to optimize an objective function. 3.1 Limitation of LSA Word similarity as determined by LSA assigns high values to words which tend to co-occur in documents. However, as noted by (Landauer and Laham, 1998; Landauer, 2002), there is no notion of antonymy; words with low or negative cosine scores are simply unrelated. In comparison, words with high cosine similarity scores are typically semantically related, which includes both synonyms and antonyms, as contrasting words often cooccur (Murphy and Andrew, 1993; Mohammed et al., 2008). To illustrate this, we have performed SVD with the aid of the Encarta thesaurus developed by Bloomsbury Publishing Plc. This thesaurus contains approximately 47k word senses and a vocabulary of 50k words and phrases. Each “document” is taken to be the thesaurus entr</context>
</contexts>
<marker>Landauer, Laham, 1998</marker>
<rawString>T.K. Landauer and D. Laham. 1998. Learning humanlike knowledge by singular value decomposition: A progress report. In Neural Information Processing Systems (NIPS).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas K Landauer</author>
<author>Peter W Foltz</author>
<author>Darrell Laham</author>
</authors>
<title>An introduction to latent semantic analysis.</title>
<date>1998</date>
<booktitle>Discourse Processes,</booktitle>
<volume>25</volume>
<pages>259--284</pages>
<contexts>
<context position="9141" citStr="Landauer et al., 1998" startWordPosition="1451" endWordPosition="1454">n/recall characteristics on synonym pairs. Structured information can be important in determining relatedness, and thesauri and Wikipedia links have been studied in (Milne and Witten, 2008; Jarmasz and Szpakowicz, 2003). Combinations of approaches are studied in (Turney et al., 2003). Vector-space models and latent semantic analysis in particular have a long history of use in synonym detection, which in fact was suggested in some of the earliest LSA papers. Deerwester et al. (1990) defines a metric for measuring word similarity based on LSA, and it has been used in (Landauer and Dumais, 1997; Landauer et al., 1998) to answer word similarity questions derived from the Test of English as a Foreign Language (TOEFL). Turney (2001) proposes the use of point-wise mutual information in conjunction with LSA, and again presents results on synonym questions derived from the TOEFL. Variants of vector space models are further analyzed in (Turney and Littman, 2005; Turney, 2006; Turney and Pantel, 2010). 3 Latent Semantic Analysis Latent Semantic Analysis (Deerwester et al., 1990) is a widely used method for representing words and documents in a low dimensional vector space. The method is based on applying singular </context>
</contexts>
<marker>Landauer, Foltz, Laham, 1998</marker>
<rawString>Thomas K. Landauer, Peter W. Foltz, and Darrell Laham. 1998. An introduction to latent semantic analysis. Discourse Processes, 25, pages 259–284.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T K Landauer</author>
</authors>
<title>On the computational basis of learning and cognition: Arguments from lsa.</title>
<date>2002</date>
<booktitle>Psychology of Learning and Motivation,</booktitle>
<pages>41--43</pages>
<contexts>
<context position="2856" citStr="Landauer, 2002" startWordPosition="447" endWordPosition="448">been used to measure word similarity (Deerwester et al., 1990; Turney and Littman, 2005; Turney, 2006; Turney, 2001; Lin, 1998; Agirre et al., 2009; Reisinger and Mooney, 2010) and for language modeling (Bellegarda, 2000; Coccaro and Jurafsky, 1998). While quite successful, these applications have typically been consistent with a very general notion of similarity in which basic association is measured, and finer shades of meaning need not be distinguished. For example, latent semantic analysis might assign a high degree of similarity to opposites as well as synonyms (Landauer and Laham, 1998; Landauer, 2002). Independent of vector-space representations, a number of authors have focused on identifying different kinds of relatedness. At the simplest level, we may wish to distinguish between synonyms and antonyms, which can be further differentiated. For example, in synonymy, we may wish to distinguish hyponyms and hypernyms. Moreover, Cruse (1986) notes that numerous kinds of antonymy are possible, for example antipodal pairs like “top-bottom” or 1212 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1212–</context>
<context position="12560" citStr="Landauer, 2002" startWordPosition="2068" endWordPosition="2069">t in the word representations which result can by viewed as the result of applying a projection matrix U to the original vectors as: UTW = SVT 1(Bellegarda, 2000) constructs the transpose of this, but we have found it convenient in data processing for documents to represent rows. II x IIII y II 1214 In Section 5, we will view U simply as a dxk matrix learned through gradient descent so as to optimize an objective function. 3.1 Limitation of LSA Word similarity as determined by LSA assigns high values to words which tend to co-occur in documents. However, as noted by (Landauer and Laham, 1998; Landauer, 2002), there is no notion of antonymy; words with low or negative cosine scores are simply unrelated. In comparison, words with high cosine similarity scores are typically semantically related, which includes both synonyms and antonyms, as contrasting words often cooccur (Murphy and Andrew, 1993; Mohammed et al., 2008). To illustrate this, we have performed SVD with the aid of the Encarta thesaurus developed by Bloomsbury Publishing Plc. This thesaurus contains approximately 47k word senses and a vocabulary of 50k words and phrases. Each “document” is taken to be the thesaurus entry for a word-sens</context>
</contexts>
<marker>Landauer, 2002</marker>
<rawString>T.K. Landauer. 2002. On the computational basis of learning and cognition: Arguments from lsa. Psychology of Learning and Motivation, 41:43–84.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
<author>Shaojun Zhao</author>
<author>Lijuan Qin</author>
<author>Ming Zhou</author>
</authors>
<title>Identifying synonyms among distributionally similar words.</title>
<date>2003</date>
<booktitle>In International Joint Conference on Artificial Intelligence (IJCAI).</booktitle>
<contexts>
<context position="3645" citStr="Lin et al., 2003" startWordPosition="557" endWordPosition="560">ish between synonyms and antonyms, which can be further differentiated. For example, in synonymy, we may wish to distinguish hyponyms and hypernyms. Moreover, Cruse (1986) notes that numerous kinds of antonymy are possible, for example antipodal pairs like “top-bottom” or 1212 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1212–1222, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics gradable opposites like “light-heavy.” Work in this area includes (Turney, 2001; Lin et al., 2003; Turney and Littman, 2005; Turney, 2006; Curran and Moens, 2002; van der Plas and Tiedemann, 2006; Mohammed et al., 2008; Mohammed et al., 2011). Despite the existence of a large amount of related work in the literature, distinguishing synonyms and antonyms is still considered as a difficult open problem in general (Poon and Domingos, 2009). In this paper, we fuse these two strands of research in an attempt to develop a vector space representation in which the synonymy and antonymy are naturally differentiated. We follow Schwab et al. (2002) in requiring a representation in which two lexical </context>
<context position="7345" citStr="Lin et al. (2003)" startWordPosition="1165" endWordPosition="1168">e degree of contrast is determined by distributional similarity. Mohammed et al. (2008) also provides a publicly available dataset for detection of antonymy, which we have adopted. This work has been extended in (Mohammed et al., 2011) to include a study of antonymy based on crowd-sourcing experiments. Turney (2008) proposes a unified approach to handling analogies, synonyms, antonyms and associations by transforming the last three cases into cases of analogy. A supervised learning method is then used to solve the resulting analogical problems. This is evaluated on a set of 136 ESL questions. Lin et al. (2003) builds on (Lin, 1998) and identifies antonyms as semantically related words which also happen to be found together in a database in pre-identified phrases indicating opposition. Lin et al. (2003) further note that whereas synonyms will tend to translate to the same word in another language, antonyms will not. This observation is used to select antonyms from amongst distributionally similar words. Antonymy is used in (de Simone and Kazakov, 2005) for document clustering and (Harabagiu et al., 2006) to find contradiction. The automatic detection of synonyms has been more extensively studied. Li</context>
</contexts>
<marker>Lin, Zhao, Qin, Zhou, 2003</marker>
<rawString>Dekang Lin, Shaojun Zhao, Lijuan Qin, and Ming Zhou. 2003. Identifying synonyms among distributionally similar words. In International Joint Conference on Artificial Intelligence (IJCAI).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar words.</title>
<date>1998</date>
<booktitle>In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics - Volume 2, ACL ’98,</booktitle>
<pages>768--774</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="2367" citStr="Lin, 1998" startWordPosition="372" endWordPosition="373">s of different items. Vector space representations have been used both at the document and word levels. At the document level, they are effective for applications including information retrieval (Salton and McGill, 1983; Deerwester et al., 1990), document clustering (Deerwester et al., 1990; Xu et al., 2003), search relevance measurement (Baeza-Yates and Ribiero-Neto, 1999) and cross-lingual document retrieval (Platt et al., 2010). At the word level, vector representations have been used to measure word similarity (Deerwester et al., 1990; Turney and Littman, 2005; Turney, 2006; Turney, 2001; Lin, 1998; Agirre et al., 2009; Reisinger and Mooney, 2010) and for language modeling (Bellegarda, 2000; Coccaro and Jurafsky, 1998). While quite successful, these applications have typically been consistent with a very general notion of similarity in which basic association is measured, and finer shades of meaning need not be distinguished. For example, latent semantic analysis might assign a high degree of similarity to opposites as well as synonyms (Landauer and Laham, 1998; Landauer, 2002). Independent of vector-space representations, a number of authors have focused on identifying different kinds </context>
<context position="7367" citStr="Lin, 1998" startWordPosition="1171" endWordPosition="1172">mined by distributional similarity. Mohammed et al. (2008) also provides a publicly available dataset for detection of antonymy, which we have adopted. This work has been extended in (Mohammed et al., 2011) to include a study of antonymy based on crowd-sourcing experiments. Turney (2008) proposes a unified approach to handling analogies, synonyms, antonyms and associations by transforming the last three cases into cases of analogy. A supervised learning method is then used to solve the resulting analogical problems. This is evaluated on a set of 136 ESL questions. Lin et al. (2003) builds on (Lin, 1998) and identifies antonyms as semantically related words which also happen to be found together in a database in pre-identified phrases indicating opposition. Lin et al. (2003) further note that whereas synonyms will tend to translate to the same word in another language, antonyms will not. This observation is used to select antonyms from amongst distributionally similar words. Antonymy is used in (de Simone and Kazakov, 2005) for document clustering and (Harabagiu et al., 2006) to find contradiction. The automatic detection of synonyms has been more extensively studied. Lin (1998) presents a th</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. Automatic retrieval and clustering of similar words. In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics - Volume 2, ACL ’98, pages 768– 774, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Milne</author>
<author>Ian H Witten</author>
</authors>
<title>An effective lowcost measure of semantic relatedness obtained from wikipedia links.</title>
<date>2008</date>
<booktitle>In Proceedings of the AAAI 2008 Workshop on Wikipedia and Artificial Intelligence.</booktitle>
<contexts>
<context position="8707" citStr="Milne and Witten, 2008" startWordPosition="1378" endWordPosition="1381">-occurrence statistics in dependency triples extracted by parsing a large dataset. Related studies are described in (Curran and Moens, 2002; van der Plas and Bouma, 2005). Later, van der Plas and Tiedemann (2006) extend the use of multilingual data present in Lin et al. (2003) by measuring distributional similarity based on the contexts that a word occurs in once translated into a new language. This is used to improve the precision/recall characteristics on synonym pairs. Structured information can be important in determining relatedness, and thesauri and Wikipedia links have been studied in (Milne and Witten, 2008; Jarmasz and Szpakowicz, 2003). Combinations of approaches are studied in (Turney et al., 2003). Vector-space models and latent semantic analysis in particular have a long history of use in synonym detection, which in fact was suggested in some of the earliest LSA papers. Deerwester et al. (1990) defines a metric for measuring word similarity based on LSA, and it has been used in (Landauer and Dumais, 1997; Landauer et al., 1998) to answer word similarity questions derived from the Test of English as a Foreign Language (TOEFL). Turney (2001) proposes the use of point-wise mutual information i</context>
</contexts>
<marker>Milne, Witten, 2008</marker>
<rawString>David Milne and Ian H. Witten. 2008. An effective lowcost measure of semantic relatedness obtained from wikipedia links. In Proceedings of the AAAI 2008 Workshop on Wikipedia and Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Minnen</author>
<author>J Carroll</author>
<author>D Pearce</author>
</authors>
<title>Applied morphological processing of english.</title>
<date>2001</date>
<journal>Natural Language Engineering,</journal>
<volume>7</volume>
<issue>3</issue>
<contexts>
<context position="24401" citStr="Minnen et al. (2001)" startWordPosition="3917" endWordPosition="3920">teps are described in detail below. 6.1 Matching via Lexical Analysis When a target word is not included in a thesaurus, it is quite often that some of its morphological variations are covered. For example, although the Encarta thesaurus does not have the word “corruptibility,” it does contain other forms like “corruptible” and “corruption.” Replacing the out-of-thesaurus target word with these morphological variations may alter the part-of-speech but typically does not change the meaning3. Given an out-of-thesaurus target word, we first apply a morphological analyzer for English developed by Minnen et al. (2001), which removes the inflectional affixes and returns the lemma. If the lemma still does not exist in the thesaurus, we then apply Porter’s stemmer (Porter, 1980) and check whether the target word can match any of the inthesaurus words in their stemmed forms. A simple rule that checks whether removing hyphens from words can lead to a match and whether the target word occurs as part of a compound word in the thesaurus is applied when both morphological analysis and stemming fail to find a match. When there are more than one matched words, the centroid of their PILSA vectors is used to represent </context>
</contexts>
<marker>Minnen, Carroll, Pearce, 2001</marker>
<rawString>G. Minnen, J. Carroll, and D. Pearce. 2001. Applied morphological processing of english. Natural Language Engineering, 7(3):207–223.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saif Mohammed</author>
<author>Bonnie Dorr</author>
<author>Graeme Hirst</author>
</authors>
<title>Computing word pair antonymy.</title>
<date>2008</date>
<booktitle>In Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="1077" citStr="Mohammed et al., 2008" startWordPosition="171" endWordPosition="175"> cosine similarities close to one, while antonyms are close to minus one. We derive this representation with the aid of a thesaurus and latent semantic analysis (LSA). Each entry in the thesaurus – a word sense along with its synonyms and antonyms – is treated as a “document,” and the resulting document collection is subjected to LSA. The key contribution of this work is to show how to assign signs to the entries in the co-occurrence matrix on which LSA operates, so as to induce a subspace with the desired property. We evaluate this procedure with the Graduate Record Examination questions of (Mohammed et al., 2008) and find that the method improves on the results of that study. Further improvements result from refining the subspace representation with discriminative training, and augmenting the training data with general newspaper text. Altogether, we improve on the best previous results by 11 points absolute in F measure. 1 Introduction Vector space representations have proven useful across a wide variety of text processing applications ranging from document clustering to search relevance measurement. In these applications, text is represented as a vector in a multi-dimensional continuous space, and a </context>
<context position="3766" citStr="Mohammed et al., 2008" startWordPosition="578" endWordPosition="581">tinguish hyponyms and hypernyms. Moreover, Cruse (1986) notes that numerous kinds of antonymy are possible, for example antipodal pairs like “top-bottom” or 1212 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1212–1222, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics gradable opposites like “light-heavy.” Work in this area includes (Turney, 2001; Lin et al., 2003; Turney and Littman, 2005; Turney, 2006; Curran and Moens, 2002; van der Plas and Tiedemann, 2006; Mohammed et al., 2008; Mohammed et al., 2011). Despite the existence of a large amount of related work in the literature, distinguishing synonyms and antonyms is still considered as a difficult open problem in general (Poon and Domingos, 2009). In this paper, we fuse these two strands of research in an attempt to develop a vector space representation in which the synonymy and antonymy are naturally differentiated. We follow Schwab et al. (2002) in requiring a representation in which two lexical items in an antonymy relation should lie at opposite ends of an axis. However, in contrast to the logical axes used previ</context>
<context position="6033" citStr="Mohammed et al., 2008" startWordPosition="952" endWordPosition="956">The remainder of this paper is organized as follows. Section 2 describes previous work. Section 3 presents the classical LSA approach and analyzes some of its limitations. In Section 4 we present our polarity inducing extension to LSA. Section 5 further extends the approach by optimizing the vector space representation with supervised discriminative training. Section 6 describes the proposed method of embedding new words in the thesaurus-derived subspace. The experimental results of Section 7 indicate that the proposed method outperforms previous approaches on a GRE test of closest-opposites (Mohammed et al., 2008). Finally, Section 8 concludes the paper. 2 Related Work The detection of antonymy has been studied in a number of previous papers. Mohammed et al. (2008) approach the problem by combining information from a published thesaurus with corpus statistics derived from the Google n-gram corpus (Brants and Franz, 2006). Their method consists of two main steps: first, detecting contrasting word categories (e.g. “WORK” vs. “ACTIVITY FOR FUN”) and then determining the degree of antonymy. Categories are defined by a thesaurus; contrasting categories are found by using affix rules (e.g., un- &amp; dis-) and W</context>
<context position="12875" citStr="Mohammed et al., 2008" startWordPosition="2118" endWordPosition="2121">ion 5, we will view U simply as a dxk matrix learned through gradient descent so as to optimize an objective function. 3.1 Limitation of LSA Word similarity as determined by LSA assigns high values to words which tend to co-occur in documents. However, as noted by (Landauer and Laham, 1998; Landauer, 2002), there is no notion of antonymy; words with low or negative cosine scores are simply unrelated. In comparison, words with high cosine similarity scores are typically semantically related, which includes both synonyms and antonyms, as contrasting words often cooccur (Murphy and Andrew, 1993; Mohammed et al., 2008). To illustrate this, we have performed SVD with the aid of the Encarta thesaurus developed by Bloomsbury Publishing Plc. This thesaurus contains approximately 47k word senses and a vocabulary of 50k words and phrases. Each “document” is taken to be the thesaurus entry for a word-sense, including synonyms and antonyms. For example, the word “admirable” induces a document consisting of {admirable, estimable, commendable, venerable, good, splendid, worthy, marvelous, excellent, unworthy}. Note that the last word in this set is its antonym. Performing SVD on this set of thesaurus derived “meaning</context>
<context position="29461" citStr="Mohammed et al., 2008" startWordPosition="4774" endWordPosition="4777">onding to other words in the vocabulary are 0. WordNet provides significantly greater coverage with approximately 227k synsets involving multiple words, and a vocabulary of about 190k words. However, it is also much sparser, with 5.3 words per sense on average as opposed to 10.3 in the thesaurus, and has only 62,821 pairs of antonyms. As general text data for use in embedding out-of-vocabulary words, we used a Nov-2010 dump of English Wikipedia, which contains approximately 917M words. 7.2 Development and Test Data For testing, we use the closest-opposite questions from GRE tests provided by (Mohammed et al., 2008). Each question contains a target word and five choices, and asks which of the choice words has the most opposite meaning to the target word. Two datasets are made publicly available by Mohammad et al. (2008): the development set, which consists of 162 questions, and the test set, which has 950 questions5. We considered making our own, more exten4http://www.bloomsbury.com/ 5http://www.umiacs.umd.edu/—saif/WebDocs/LCdata/{devset,testset}.txt Dimensions Bloomsbury Prec. WordNet Prec. 50 0.778 0.475 100 0.850 0.563 200 0.856 0.569 300 0.863 0.625 400 0.843 0.625 500 0.843 0.613 750 0.830 0.613 10</context>
<context position="31757" citStr="Mohammed et al., 2008" startWordPosition="5144" endWordPosition="5147">ut of vocabulary, we skip the whole question. When the target word is in vocabulary but one or more choices are unknown words, we ignore those unknown words and pick the word with the lowest cosine similarity from the rest as the answer. The results of our methods are reported in precision (the number of questions answered correctly divided by the number of questions attempted), recall (the number of questions answered correctly divided by the number of all questions) and Fi (the harmonic mean of precision and recall)6. We now turn to an in-depth evaluation. 6Precision/recall/F, were used in (Mohammed et al., 2008) as when their system “did not find any evidence of antonymy between the target and any of its alternatives, then it refrained from attempting that question.” We adopt this convention to provide a fair comparison to their system. 1219 Dev. Set Test Set Prec Rec F1 Prec Rec F1 WordNet lookup 0.40 0.40 0.40 0.42 0.41 0.42 WordNet signed-TFIDF w/o LSA 0.41 0.41 0.41 0.43 0.42 0.43 WordNet PILSA 0.63 0.62 0.62 0.60 0.60 0.60 Bloomsbury lookup 0.65 0.61 0.63 0.61 0.56 0.59 Bloomsbury signed TFIDF w/o LSA 0.68 0.64 0.66 0.63 0.57 0.60 Bloomsbury PILSA 0.86 0.81 0.84 0.81 0.74 0.77 Bloomsbury PILSA +</context>
<context position="33782" citStr="Mohammed et al., 2008" startWordPosition="5487" endWordPosition="5490">find dimensions 300 and 400 are equally good, where both answer 100 questions correctly (0.625 in precision)7. Because a lower number of dimensions is preferred for saving storage space and computing time, we choose 300 as the number of dimensions in PILSA. We now compare the proposed methods. All results are summarized in Table 6. When evaluated on the GRE test set, the Bloomsbury thesaurus-based methods (Lines 4–7) attempted 865 questions. The precision, recall and F1 of the Bloomsbury-based PILSA model (Line 6) are 0.81, 0.74 and 0.77, which are all better than the best reported method in (Mohammed et al., 2008)8. In contrast, the WordNet-based methods (Lines 1–3) attempted 936 7Note that the number of questions attempted is not a function of the number of dimensions. 8We take a conservative approach and assume that skipped questions are answered incorrectly. The difference is statistically significant at 99% confidence level using a binomial test. questions. However, consistent with what we observed on the development set, the WordNet-based model is inferior. Its precision, recall and F1 on the test set are 0.60, 0.60 and 0.60 (Line 3). Although the quality of the data source plays an important role</context>
</contexts>
<marker>Mohammed, Dorr, Hirst, 2008</marker>
<rawString>Saif Mohammed, Bonnie Dorr, and Graeme Hirst. 2008. Computing word pair antonymy. In Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saif M Mohammed</author>
<author>Bonnie J Dorr</author>
<author>Graeme Hirst</author>
<author>Peter D Turney</author>
</authors>
<title>Measuring degrees of semantic opposition.</title>
<date>2011</date>
<tech>Technical report,</tech>
<institution>National Research Council Canada.</institution>
<contexts>
<context position="3790" citStr="Mohammed et al., 2011" startWordPosition="582" endWordPosition="585">ypernyms. Moreover, Cruse (1986) notes that numerous kinds of antonymy are possible, for example antipodal pairs like “top-bottom” or 1212 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1212–1222, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics gradable opposites like “light-heavy.” Work in this area includes (Turney, 2001; Lin et al., 2003; Turney and Littman, 2005; Turney, 2006; Curran and Moens, 2002; van der Plas and Tiedemann, 2006; Mohammed et al., 2008; Mohammed et al., 2011). Despite the existence of a large amount of related work in the literature, distinguishing synonyms and antonyms is still considered as a difficult open problem in general (Poon and Domingos, 2009). In this paper, we fuse these two strands of research in an attempt to develop a vector space representation in which the synonymy and antonymy are naturally differentiated. We follow Schwab et al. (2002) in requiring a representation in which two lexical items in an antonymy relation should lie at opposite ends of an axis. However, in contrast to the logical axes used previously, we desire that an</context>
<context position="6963" citStr="Mohammed et al., 2011" startWordPosition="1101" endWordPosition="1104">2006). Their method consists of two main steps: first, detecting contrasting word categories (e.g. “WORK” vs. “ACTIVITY FOR FUN”) and then determining the degree of antonymy. Categories are defined by a thesaurus; contrasting categories are found by using affix rules (e.g., un- &amp; dis-) and WordNet antonymy links. Words belonging to contrasting categories are treated as antonyms and the degree of contrast is determined by distributional similarity. Mohammed et al. (2008) also provides a publicly available dataset for detection of antonymy, which we have adopted. This work has been extended in (Mohammed et al., 2011) to include a study of antonymy based on crowd-sourcing experiments. Turney (2008) proposes a unified approach to handling analogies, synonyms, antonyms and associations by transforming the last three cases into cases of analogy. A supervised learning method is then used to solve the resulting analogical problems. This is evaluated on a set of 136 ESL questions. Lin et al. (2003) builds on (Lin, 1998) and identifies antonyms as semantically related words which also happen to be found together in a database in pre-identified phrases indicating opposition. Lin et al. (2003) further note that whe</context>
</contexts>
<marker>Mohammed, Dorr, Hirst, Turney, 2011</marker>
<rawString>Saif M. Mohammed, Bonnie J. Dorr, Graeme Hirst, and Peter D. Turney. 2011. Measuring degrees of semantic opposition. Technical report, National Research Council Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregory L Murphy</author>
<author>Jane M Andrew</author>
</authors>
<title>The conceptual basis of antonymy and synonymy in adjectives.</title>
<date>1993</date>
<journal>Journal of Memory and Language,</journal>
<volume>32</volume>
<issue>3</issue>
<contexts>
<context position="12851" citStr="Murphy and Andrew, 1993" startWordPosition="2114" endWordPosition="2117"> x IIII y II 1214 In Section 5, we will view U simply as a dxk matrix learned through gradient descent so as to optimize an objective function. 3.1 Limitation of LSA Word similarity as determined by LSA assigns high values to words which tend to co-occur in documents. However, as noted by (Landauer and Laham, 1998; Landauer, 2002), there is no notion of antonymy; words with low or negative cosine scores are simply unrelated. In comparison, words with high cosine similarity scores are typically semantically related, which includes both synonyms and antonyms, as contrasting words often cooccur (Murphy and Andrew, 1993; Mohammed et al., 2008). To illustrate this, we have performed SVD with the aid of the Encarta thesaurus developed by Bloomsbury Publishing Plc. This thesaurus contains approximately 47k word senses and a vocabulary of 50k words and phrases. Each “document” is taken to be the thesaurus entry for a word-sense, including synonyms and antonyms. For example, the word “admirable” induces a document consisting of {admirable, estimable, commendable, venerable, good, splendid, worthy, marvelous, excellent, unworthy}. Note that the last word in this set is its antonym. Performing SVD on this set of th</context>
</contexts>
<marker>Murphy, Andrew, 1993</marker>
<rawString>Gregory L. Murphy and Jane M. Andrew. 1993. The conceptual basis of antonymy and synonymy in adjectives. Journal of Memory and Language, 32(3):1–19.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jorge Nocedal</author>
<author>Stephen Wright</author>
</authors>
<title>Numerical Optimization.</title>
<date>2006</date>
<publisher>Springer,</publisher>
<note>2nd edition.</note>
<contexts>
<context position="21892" citStr="Nocedal and Wright, 2006" startWordPosition="3496" endWordPosition="3499">ning set consists of m pairs of raw input vectors {(fp1, fq1), (fp2, fq2), ··· , (fpm, fqm)}. Given a projection matrix A, the similarity score of any pair of objects is simA(fpi, fqj) = cosine(AT fpi, AT fqj). Let Δij = simA(fpi, fqi) − simA(fpi, fqj) be the difference of the similarity scores of (fpi, fqi) and (fpi, fqj). The learning procedure tries to increase Δij by using the following logistic loss: L(Δij; A) = log(1 + exp(−-&apos;Δij)), where -y is a scaling factor that adjusts the loss function2. The loss of the whole training set is thus: by standard gradient-based methods, such as LBFGS (Nocedal and Wright, 2006). The original setting of S2Net can be directly applied to finding synonymous words, where the training data consists of pairs of vectors representing two synonyms. It is also easy to modify the loss function to apply it to the antonym detection problem. We first sample pairs of antonyms from the thesaurus to create the training data. The raw input vector f of a selected word is its corresponding column vector of the document-term matrix W (Section 3) after inducing polarity (Section 4). When each pair of vectors in the training data represents two antonyms, we can redefine Δij by flipping the</context>
</contexts>
<marker>Nocedal, Wright, 2006</marker>
<rawString>Jorge Nocedal and Stephen Wright. 2006. Numerical Optimization. Springer, 2nd edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Platt</author>
<author>Kristina Toutanova</author>
<author>Wen-tau Yih</author>
</authors>
<title>Translingual document representations from discriminative projections.</title>
<date>2010</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>251--261</pages>
<contexts>
<context position="2192" citStr="Platt et al., 2010" startWordPosition="343" endWordPosition="346">. In these applications, text is represented as a vector in a multi-dimensional continuous space, and a similarity metric such as cosine similarity can be used to measure the relatedness of different items. Vector space representations have been used both at the document and word levels. At the document level, they are effective for applications including information retrieval (Salton and McGill, 1983; Deerwester et al., 1990), document clustering (Deerwester et al., 1990; Xu et al., 2003), search relevance measurement (Baeza-Yates and Ribiero-Neto, 1999) and cross-lingual document retrieval (Platt et al., 2010). At the word level, vector representations have been used to measure word similarity (Deerwester et al., 1990; Turney and Littman, 2005; Turney, 2006; Turney, 2001; Lin, 1998; Agirre et al., 2009; Reisinger and Mooney, 2010) and for language modeling (Bellegarda, 2000; Coccaro and Jurafsky, 1998). While quite successful, these applications have typically been consistent with a very general notion of similarity in which basic association is measured, and finer shades of meaning need not be distinguished. For example, latent semantic analysis might assign a high degree of similarity to opposite</context>
<context position="19220" citStr="Platt et al., 2010" startWordPosition="3087" endWordPosition="3090">ngular value decomposition in LSA does not explicitly try to achieve such goals. In this section, we see that when supervised training data is available, the projection matrix of LSA can be enhanced through a discriminative training technique explicitly designed to create a representation suited to a specific task. Because LSA is closely related to principle component analysis (PCA), extensions of PCA such as canonical correlation analysis (CCA) and oriented principle component analysis (OPCA) can leverage the labeled data and produce the projection matrix through general eigen-decomposition (Platt et al., 2010). Along this line of work, Yih et al. (2011) proposed a Siamese neural network approach called S2Net, which tunes the projection matrix directly through gradient descent, and has shown to outperform other methods in several tasks. Below we describe briefly this technique and explain how we adopt it for the task of antonym detection. The goal of S2Net is to learn a concept vector representation of the original sparse term vectors. Although such transformation can be non-linear in general, its current design chooses the model form to be a linear projection matrix, which is identical to 1216 Word</context>
</contexts>
<marker>Platt, Toutanova, Yih, 2010</marker>
<rawString>John Platt, Kristina Toutanova, and Wen-tau Yih. 2010. Translingual document representations from discriminative projections. In Proceedings of EMNLP, pages 251–261.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hoifung Poon</author>
<author>Pedro Domingos</author>
</authors>
<title>Unsupervised semantic parsing.</title>
<date>2009</date>
<booktitle>In Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="3988" citStr="Poon and Domingos, 2009" startWordPosition="614" endWordPosition="617">hods in Natural Language Processing and Computational Natural Language Learning, pages 1212–1222, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics gradable opposites like “light-heavy.” Work in this area includes (Turney, 2001; Lin et al., 2003; Turney and Littman, 2005; Turney, 2006; Curran and Moens, 2002; van der Plas and Tiedemann, 2006; Mohammed et al., 2008; Mohammed et al., 2011). Despite the existence of a large amount of related work in the literature, distinguishing synonyms and antonyms is still considered as a difficult open problem in general (Poon and Domingos, 2009). In this paper, we fuse these two strands of research in an attempt to develop a vector space representation in which the synonymy and antonymy are naturally differentiated. We follow Schwab et al. (2002) in requiring a representation in which two lexical items in an antonymy relation should lie at opposite ends of an axis. However, in contrast to the logical axes used previously, we desire that antonyms should lie at the opposite ends of a sphere lying in a continuous and automatically induced vector space. To generate this vector space, we present a novel method for assigning both negative </context>
</contexts>
<marker>Poon, Domingos, 2009</marker>
<rawString>Hoifung Poon and Pedro Domingos. 2009. Unsupervised semantic parsing. In Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin F Porter</author>
</authors>
<title>An algorithm for suffix stripping.</title>
<date>1980</date>
<journal>Program,</journal>
<volume>14</volume>
<issue>3</issue>
<contexts>
<context position="24562" citStr="Porter, 1980" startWordPosition="3945" endWordPosition="3946">l variations are covered. For example, although the Encarta thesaurus does not have the word “corruptibility,” it does contain other forms like “corruptible” and “corruption.” Replacing the out-of-thesaurus target word with these morphological variations may alter the part-of-speech but typically does not change the meaning3. Given an out-of-thesaurus target word, we first apply a morphological analyzer for English developed by Minnen et al. (2001), which removes the inflectional affixes and returns the lemma. If the lemma still does not exist in the thesaurus, we then apply Porter’s stemmer (Porter, 1980) and check whether the target word can match any of the inthesaurus words in their stemmed forms. A simple rule that checks whether removing hyphens from words can lead to a match and whether the target word occurs as part of a compound word in the thesaurus is applied when both morphological analysis and stemming fail to find a match. When there are more than one matched words, the centroid of their PILSA vectors is used to represent the target word. When there is only one matched word, the matched word is treated as the target word. 6.2 Leveraging General Text Data If no words in the thesaur</context>
</contexts>
<marker>Porter, 1980</marker>
<rawString>Martin F. Porter. 1980. An algorithm for suffix stripping. Program, 14(3):130–137.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Reisinger</author>
<author>Raymond J Mooney</author>
</authors>
<title>Multiprototype vector-space models of word meaning.</title>
<date>2010</date>
<booktitle>In Proceedings of HLT-NAACL,</booktitle>
<pages>109--117</pages>
<contexts>
<context position="2417" citStr="Reisinger and Mooney, 2010" startWordPosition="378" endWordPosition="381">ce representations have been used both at the document and word levels. At the document level, they are effective for applications including information retrieval (Salton and McGill, 1983; Deerwester et al., 1990), document clustering (Deerwester et al., 1990; Xu et al., 2003), search relevance measurement (Baeza-Yates and Ribiero-Neto, 1999) and cross-lingual document retrieval (Platt et al., 2010). At the word level, vector representations have been used to measure word similarity (Deerwester et al., 1990; Turney and Littman, 2005; Turney, 2006; Turney, 2001; Lin, 1998; Agirre et al., 2009; Reisinger and Mooney, 2010) and for language modeling (Bellegarda, 2000; Coccaro and Jurafsky, 1998). While quite successful, these applications have typically been consistent with a very general notion of similarity in which basic association is measured, and finer shades of meaning need not be distinguished. For example, latent semantic analysis might assign a high degree of similarity to opposites as well as synonyms (Landauer and Laham, 1998; Landauer, 2002). Independent of vector-space representations, a number of authors have focused on identifying different kinds of relatedness. At the simplest level, we may wish</context>
</contexts>
<marker>Reisinger, Mooney, 2010</marker>
<rawString>Joseph Reisinger and Raymond J. Mooney. 2010. Multiprototype vector-space models of word meaning. In Proceedings of HLT-NAACL, pages 109–117.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerard Salton</author>
<author>Michael J McGill</author>
</authors>
<title>Introduction to Modern Information Retrieval.</title>
<date>1983</date>
<publisher>McGraw Hill.</publisher>
<contexts>
<context position="1977" citStr="Salton and McGill, 1983" startWordPosition="312" endWordPosition="315">s by 11 points absolute in F measure. 1 Introduction Vector space representations have proven useful across a wide variety of text processing applications ranging from document clustering to search relevance measurement. In these applications, text is represented as a vector in a multi-dimensional continuous space, and a similarity metric such as cosine similarity can be used to measure the relatedness of different items. Vector space representations have been used both at the document and word levels. At the document level, they are effective for applications including information retrieval (Salton and McGill, 1983; Deerwester et al., 1990), document clustering (Deerwester et al., 1990; Xu et al., 2003), search relevance measurement (Baeza-Yates and Ribiero-Neto, 1999) and cross-lingual document retrieval (Platt et al., 2010). At the word level, vector representations have been used to measure word similarity (Deerwester et al., 1990; Turney and Littman, 2005; Turney, 2006; Turney, 2001; Lin, 1998; Agirre et al., 2009; Reisinger and Mooney, 2010) and for language modeling (Bellegarda, 2000; Coccaro and Jurafsky, 1998). While quite successful, these applications have typically been consistent with a very</context>
</contexts>
<marker>Salton, McGill, 1983</marker>
<rawString>Gerard Salton and Michael J. McGill. 1983. Introduction to Modern Information Retrieval. McGraw Hill.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>A Wong</author>
<author>C S Yang</author>
</authors>
<title>A Vector Space Model for Automatic Indexing.</title>
<date>1975</date>
<journal>Communications of the ACM,</journal>
<volume>18</volume>
<issue>11</issue>
<contexts>
<context position="10565" citStr="Salton et al., 1975" startWordPosition="1698" endWordPosition="1701"> of words from a vocabulary of size n. These documents may be actual documents such as newspaper articles, or simply notional documents such as sentences, or any other collection in which words are grouped together. Next, a d x n document-term matrix W is formed1. At its simplest form, the ijth entry contains the number of times word j has occurred in document i – its term frequency or TF value. More conventionally, the entry is weighted by some notion of the importance of word j, for example the negative logarithm of the fraction of documents that contain it, resulting in a TF-IDF weighting (Salton et al., 1975). The similarity between two documents can be computed using the cosine similarity of their corresponding row vectors: x · y sim(x, y) = Similarly, the cosine similarity of two column vectors can be used to judge the similarity of the corresponding words. Finally, to obtain a subspace representation of dimension k, W is decomposed as W Pz� USVT where U is d x k, V T is k x n, and S is a k x k diagonal matrix. In applications, k « n and k « d; for example one might have a 50, 000 word vocabulary and 1, 000, 000 documents and use a 300 dimensional subspace representation. An important property o</context>
</contexts>
<marker>Salton, Wong, Yang, 1975</marker>
<rawString>G. Salton, A. Wong, and C. S. Yang. 1975. A Vector Space Model for Automatic Indexing. Communications of the ACM, 18(11).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Schwab</author>
<author>M Lafourcade</author>
<author>V Prince</author>
</authors>
<title>Antonymy and conceptual vectors.</title>
<date>2002</date>
<booktitle>In International Conference on Computational Linguistics (COLING).</booktitle>
<contexts>
<context position="4193" citStr="Schwab et al. (2002)" startWordPosition="650" endWordPosition="653">light-heavy.” Work in this area includes (Turney, 2001; Lin et al., 2003; Turney and Littman, 2005; Turney, 2006; Curran and Moens, 2002; van der Plas and Tiedemann, 2006; Mohammed et al., 2008; Mohammed et al., 2011). Despite the existence of a large amount of related work in the literature, distinguishing synonyms and antonyms is still considered as a difficult open problem in general (Poon and Domingos, 2009). In this paper, we fuse these two strands of research in an attempt to develop a vector space representation in which the synonymy and antonymy are naturally differentiated. We follow Schwab et al. (2002) in requiring a representation in which two lexical items in an antonymy relation should lie at opposite ends of an axis. However, in contrast to the logical axes used previously, we desire that antonyms should lie at the opposite ends of a sphere lying in a continuous and automatically induced vector space. To generate this vector space, we present a novel method for assigning both negative and positive values to the TF-IDF weights used in latent semantic analysis. To determine these signed values, we exploit the information present in a thesaurus. The result is a vector space representation </context>
</contexts>
<marker>Schwab, Lafourcade, Prince, 2002</marker>
<rawString>D. Schwab, M. Lafourcade, and V. Prince. 2002. Antonymy and conceptual vectors. In International Conference on Computational Linguistics (COLING).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Turney</author>
<author>Michael Littman</author>
</authors>
<title>Corpus-based learning of analogies and semantic relations.</title>
<date>2005</date>
<booktitle>Machine Learning,</booktitle>
<volume>60</volume>
<pages>1--3</pages>
<contexts>
<context position="2328" citStr="Turney and Littman, 2005" startWordPosition="364" endWordPosition="367">osine similarity can be used to measure the relatedness of different items. Vector space representations have been used both at the document and word levels. At the document level, they are effective for applications including information retrieval (Salton and McGill, 1983; Deerwester et al., 1990), document clustering (Deerwester et al., 1990; Xu et al., 2003), search relevance measurement (Baeza-Yates and Ribiero-Neto, 1999) and cross-lingual document retrieval (Platt et al., 2010). At the word level, vector representations have been used to measure word similarity (Deerwester et al., 1990; Turney and Littman, 2005; Turney, 2006; Turney, 2001; Lin, 1998; Agirre et al., 2009; Reisinger and Mooney, 2010) and for language modeling (Bellegarda, 2000; Coccaro and Jurafsky, 1998). While quite successful, these applications have typically been consistent with a very general notion of similarity in which basic association is measured, and finer shades of meaning need not be distinguished. For example, latent semantic analysis might assign a high degree of similarity to opposites as well as synonyms (Landauer and Laham, 1998; Landauer, 2002). Independent of vector-space representations, a number of authors have </context>
<context position="3671" citStr="Turney and Littman, 2005" startWordPosition="561" endWordPosition="565">ms and antonyms, which can be further differentiated. For example, in synonymy, we may wish to distinguish hyponyms and hypernyms. Moreover, Cruse (1986) notes that numerous kinds of antonymy are possible, for example antipodal pairs like “top-bottom” or 1212 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1212–1222, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics gradable opposites like “light-heavy.” Work in this area includes (Turney, 2001; Lin et al., 2003; Turney and Littman, 2005; Turney, 2006; Curran and Moens, 2002; van der Plas and Tiedemann, 2006; Mohammed et al., 2008; Mohammed et al., 2011). Despite the existence of a large amount of related work in the literature, distinguishing synonyms and antonyms is still considered as a difficult open problem in general (Poon and Domingos, 2009). In this paper, we fuse these two strands of research in an attempt to develop a vector space representation in which the synonymy and antonymy are naturally differentiated. We follow Schwab et al. (2002) in requiring a representation in which two lexical items in an antonymy relat</context>
<context position="9484" citStr="Turney and Littman, 2005" startWordPosition="1505" endWordPosition="1508">ticular have a long history of use in synonym detection, which in fact was suggested in some of the earliest LSA papers. Deerwester et al. (1990) defines a metric for measuring word similarity based on LSA, and it has been used in (Landauer and Dumais, 1997; Landauer et al., 1998) to answer word similarity questions derived from the Test of English as a Foreign Language (TOEFL). Turney (2001) proposes the use of point-wise mutual information in conjunction with LSA, and again presents results on synonym questions derived from the TOEFL. Variants of vector space models are further analyzed in (Turney and Littman, 2005; Turney, 2006; Turney and Pantel, 2010). 3 Latent Semantic Analysis Latent Semantic Analysis (Deerwester et al., 1990) is a widely used method for representing words and documents in a low dimensional vector space. The method is based on applying singular value decomposition (SVD) to a matrix W which indicates the occurrence of words in documents. To perform LSA, one proceeds as follows. The input is a collection of d documents which are expressed in terms of words from a vocabulary of size n. These documents may be actual documents such as newspaper articles, or simply notional documents suc</context>
</contexts>
<marker>Turney, Littman, 2005</marker>
<rawString>Peter Turney and Michael Littman. 2005. Corpus-based learning of analogies and semantic relations. Machine Learning, 60 (1-3), pages 251–278.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
<author>Patrick Pantel</author>
</authors>
<title>From frequency to meaning: Vector space models of semantics.</title>
<date>2010</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<volume>37</volume>
<contexts>
<context position="9524" citStr="Turney and Pantel, 2010" startWordPosition="1511" endWordPosition="1515">nonym detection, which in fact was suggested in some of the earliest LSA papers. Deerwester et al. (1990) defines a metric for measuring word similarity based on LSA, and it has been used in (Landauer and Dumais, 1997; Landauer et al., 1998) to answer word similarity questions derived from the Test of English as a Foreign Language (TOEFL). Turney (2001) proposes the use of point-wise mutual information in conjunction with LSA, and again presents results on synonym questions derived from the TOEFL. Variants of vector space models are further analyzed in (Turney and Littman, 2005; Turney, 2006; Turney and Pantel, 2010). 3 Latent Semantic Analysis Latent Semantic Analysis (Deerwester et al., 1990) is a widely used method for representing words and documents in a low dimensional vector space. The method is based on applying singular value decomposition (SVD) to a matrix W which indicates the occurrence of words in documents. To perform LSA, one proceeds as follows. The input is a collection of d documents which are expressed in terms of words from a vocabulary of size n. These documents may be actual documents such as newspaper articles, or simply notional documents such as sentences, or any other collection </context>
</contexts>
<marker>Turney, Pantel, 2010</marker>
<rawString>Peter D. Turney and Patrick Pantel. 2010. From frequency to meaning: Vector space models of semantics. Journal of Artificial Intelligence Research, (37).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
<author>Michael L Littman</author>
<author>Jeffrey Bigham</author>
<author>Victor Shnayder</author>
</authors>
<title>Combining independent modules to solve multiple-choice synonym and analogy problems.</title>
<date>2003</date>
<booktitle>In Recent Advances in Natural Language Processing (RANLP).</booktitle>
<contexts>
<context position="8803" citStr="Turney et al., 2003" startWordPosition="1393" endWordPosition="1396"> are described in (Curran and Moens, 2002; van der Plas and Bouma, 2005). Later, van der Plas and Tiedemann (2006) extend the use of multilingual data present in Lin et al. (2003) by measuring distributional similarity based on the contexts that a word occurs in once translated into a new language. This is used to improve the precision/recall characteristics on synonym pairs. Structured information can be important in determining relatedness, and thesauri and Wikipedia links have been studied in (Milne and Witten, 2008; Jarmasz and Szpakowicz, 2003). Combinations of approaches are studied in (Turney et al., 2003). Vector-space models and latent semantic analysis in particular have a long history of use in synonym detection, which in fact was suggested in some of the earliest LSA papers. Deerwester et al. (1990) defines a metric for measuring word similarity based on LSA, and it has been used in (Landauer and Dumais, 1997; Landauer et al., 1998) to answer word similarity questions derived from the Test of English as a Foreign Language (TOEFL). Turney (2001) proposes the use of point-wise mutual information in conjunction with LSA, and again presents results on synonym questions derived from the TOEFL. </context>
</contexts>
<marker>Turney, Littman, Bigham, Shnayder, 2003</marker>
<rawString>Peter D. Turney, Michael L. Littman, Jeffrey Bigham, and Victor Shnayder. 2003. Combining independent modules to solve multiple-choice synonym and analogy problems. In Recent Advances in Natural Language Processing (RANLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>Mining the web for synonyms: Pmi-ir versus lsa on toefl.</title>
<date>2001</date>
<booktitle>In European Conference on Machine Learning (ECML).</booktitle>
<contexts>
<context position="2356" citStr="Turney, 2001" startWordPosition="370" endWordPosition="371">the relatedness of different items. Vector space representations have been used both at the document and word levels. At the document level, they are effective for applications including information retrieval (Salton and McGill, 1983; Deerwester et al., 1990), document clustering (Deerwester et al., 1990; Xu et al., 2003), search relevance measurement (Baeza-Yates and Ribiero-Neto, 1999) and cross-lingual document retrieval (Platt et al., 2010). At the word level, vector representations have been used to measure word similarity (Deerwester et al., 1990; Turney and Littman, 2005; Turney, 2006; Turney, 2001; Lin, 1998; Agirre et al., 2009; Reisinger and Mooney, 2010) and for language modeling (Bellegarda, 2000; Coccaro and Jurafsky, 1998). While quite successful, these applications have typically been consistent with a very general notion of similarity in which basic association is measured, and finer shades of meaning need not be distinguished. For example, latent semantic analysis might assign a high degree of similarity to opposites as well as synonyms (Landauer and Laham, 1998; Landauer, 2002). Independent of vector-space representations, a number of authors have focused on identifying diffe</context>
<context position="3627" citStr="Turney, 2001" startWordPosition="555" endWordPosition="556">sh to distinguish between synonyms and antonyms, which can be further differentiated. For example, in synonymy, we may wish to distinguish hyponyms and hypernyms. Moreover, Cruse (1986) notes that numerous kinds of antonymy are possible, for example antipodal pairs like “top-bottom” or 1212 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1212–1222, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics gradable opposites like “light-heavy.” Work in this area includes (Turney, 2001; Lin et al., 2003; Turney and Littman, 2005; Turney, 2006; Curran and Moens, 2002; van der Plas and Tiedemann, 2006; Mohammed et al., 2008; Mohammed et al., 2011). Despite the existence of a large amount of related work in the literature, distinguishing synonyms and antonyms is still considered as a difficult open problem in general (Poon and Domingos, 2009). In this paper, we fuse these two strands of research in an attempt to develop a vector space representation in which the synonymy and antonymy are naturally differentiated. We follow Schwab et al. (2002) in requiring a representation in </context>
<context position="9255" citStr="Turney (2001)" startWordPosition="1471" endWordPosition="1472"> and Wikipedia links have been studied in (Milne and Witten, 2008; Jarmasz and Szpakowicz, 2003). Combinations of approaches are studied in (Turney et al., 2003). Vector-space models and latent semantic analysis in particular have a long history of use in synonym detection, which in fact was suggested in some of the earliest LSA papers. Deerwester et al. (1990) defines a metric for measuring word similarity based on LSA, and it has been used in (Landauer and Dumais, 1997; Landauer et al., 1998) to answer word similarity questions derived from the Test of English as a Foreign Language (TOEFL). Turney (2001) proposes the use of point-wise mutual information in conjunction with LSA, and again presents results on synonym questions derived from the TOEFL. Variants of vector space models are further analyzed in (Turney and Littman, 2005; Turney, 2006; Turney and Pantel, 2010). 3 Latent Semantic Analysis Latent Semantic Analysis (Deerwester et al., 1990) is a widely used method for representing words and documents in a low dimensional vector space. The method is based on applying singular value decomposition (SVD) to a matrix W which indicates the occurrence of words in documents. To perform LSA, one </context>
</contexts>
<marker>Turney, 2001</marker>
<rawString>Peter D. Turney. 2001. Mining the web for synonyms: Pmi-ir versus lsa on toefl. In European Conference on Machine Learning (ECML).</rawString>
</citation>
<citation valid="true">
<authors>
<author>P D Turney</author>
</authors>
<title>Similarity of semantic relations.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>3</issue>
<contexts>
<context position="2342" citStr="Turney, 2006" startWordPosition="368" endWordPosition="369">ed to measure the relatedness of different items. Vector space representations have been used both at the document and word levels. At the document level, they are effective for applications including information retrieval (Salton and McGill, 1983; Deerwester et al., 1990), document clustering (Deerwester et al., 1990; Xu et al., 2003), search relevance measurement (Baeza-Yates and Ribiero-Neto, 1999) and cross-lingual document retrieval (Platt et al., 2010). At the word level, vector representations have been used to measure word similarity (Deerwester et al., 1990; Turney and Littman, 2005; Turney, 2006; Turney, 2001; Lin, 1998; Agirre et al., 2009; Reisinger and Mooney, 2010) and for language modeling (Bellegarda, 2000; Coccaro and Jurafsky, 1998). While quite successful, these applications have typically been consistent with a very general notion of similarity in which basic association is measured, and finer shades of meaning need not be distinguished. For example, latent semantic analysis might assign a high degree of similarity to opposites as well as synonyms (Landauer and Laham, 1998; Landauer, 2002). Independent of vector-space representations, a number of authors have focused on ide</context>
<context position="3685" citStr="Turney, 2006" startWordPosition="566" endWordPosition="567"> be further differentiated. For example, in synonymy, we may wish to distinguish hyponyms and hypernyms. Moreover, Cruse (1986) notes that numerous kinds of antonymy are possible, for example antipodal pairs like “top-bottom” or 1212 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1212–1222, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics gradable opposites like “light-heavy.” Work in this area includes (Turney, 2001; Lin et al., 2003; Turney and Littman, 2005; Turney, 2006; Curran and Moens, 2002; van der Plas and Tiedemann, 2006; Mohammed et al., 2008; Mohammed et al., 2011). Despite the existence of a large amount of related work in the literature, distinguishing synonyms and antonyms is still considered as a difficult open problem in general (Poon and Domingos, 2009). In this paper, we fuse these two strands of research in an attempt to develop a vector space representation in which the synonymy and antonymy are naturally differentiated. We follow Schwab et al. (2002) in requiring a representation in which two lexical items in an antonymy relation should lie</context>
<context position="9498" citStr="Turney, 2006" startWordPosition="1509" endWordPosition="1510">y of use in synonym detection, which in fact was suggested in some of the earliest LSA papers. Deerwester et al. (1990) defines a metric for measuring word similarity based on LSA, and it has been used in (Landauer and Dumais, 1997; Landauer et al., 1998) to answer word similarity questions derived from the Test of English as a Foreign Language (TOEFL). Turney (2001) proposes the use of point-wise mutual information in conjunction with LSA, and again presents results on synonym questions derived from the TOEFL. Variants of vector space models are further analyzed in (Turney and Littman, 2005; Turney, 2006; Turney and Pantel, 2010). 3 Latent Semantic Analysis Latent Semantic Analysis (Deerwester et al., 1990) is a widely used method for representing words and documents in a low dimensional vector space. The method is based on applying singular value decomposition (SVD) to a matrix W which indicates the occurrence of words in documents. To perform LSA, one proceeds as follows. The input is a collection of d documents which are expressed in terms of words from a vocabulary of size n. These documents may be actual documents such as newspaper articles, or simply notional documents such as sentences</context>
</contexts>
<marker>Turney, 2006</marker>
<rawString>P. D. Turney. 2006. Similarity of semantic relations. Computational Linguistics, 32(3):379–416.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Turney</author>
</authors>
<title>A uniform approach to analogies, synonyms, antonyms, and associations.</title>
<date>2008</date>
<booktitle>In International Conference on Computational Linguistics (COLING).</booktitle>
<contexts>
<context position="7045" citStr="Turney (2008)" startWordPosition="1116" endWordPosition="1117">s (e.g. “WORK” vs. “ACTIVITY FOR FUN”) and then determining the degree of antonymy. Categories are defined by a thesaurus; contrasting categories are found by using affix rules (e.g., un- &amp; dis-) and WordNet antonymy links. Words belonging to contrasting categories are treated as antonyms and the degree of contrast is determined by distributional similarity. Mohammed et al. (2008) also provides a publicly available dataset for detection of antonymy, which we have adopted. This work has been extended in (Mohammed et al., 2011) to include a study of antonymy based on crowd-sourcing experiments. Turney (2008) proposes a unified approach to handling analogies, synonyms, antonyms and associations by transforming the last three cases into cases of analogy. A supervised learning method is then used to solve the resulting analogical problems. This is evaluated on a set of 136 ESL questions. Lin et al. (2003) builds on (Lin, 1998) and identifies antonyms as semantically related words which also happen to be found together in a database in pre-identified phrases indicating opposition. Lin et al. (2003) further note that whereas synonyms will tend to translate to the same word in another language, antonym</context>
</contexts>
<marker>Turney, 2008</marker>
<rawString>Peter Turney. 2008. A uniform approach to analogies, synonyms, antonyms, and associations. In International Conference on Computational Linguistics (COLING).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lonneke van der Plas</author>
<author>Gosse Bouma</author>
</authors>
<title>Syntactic contexts for finding semantically similar words.</title>
<date>2005</date>
<booktitle>In Proceedings of the Meeting of Computational Linguistics in the Netherlands</booktitle>
<marker>van der Plas, Bouma, 2005</marker>
<rawString>Lonneke van der Plas and Gosse Bouma. 2005. Syntactic contexts for finding semantically similar words. In Proceedings of the Meeting of Computational Linguistics in the Netherlands 2004 (CLIN).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lonneke van der Plas</author>
<author>J¨org Tiedemann</author>
</authors>
<title>Finding synonyms using automatic word alignment and measures of distributional similarity.</title>
<date>2006</date>
<booktitle>In Proceedings of the COLING/ACL on Main conference poster sessions, COLING-ACL ’06,</booktitle>
<pages>866--873</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>van der Plas, Tiedemann, 2006</marker>
<rawString>Lonneke van der Plas and J¨org Tiedemann. 2006. Finding synonyms using automatic word alignment and measures of distributional similarity. In Proceedings of the COLING/ACL on Main conference poster sessions, COLING-ACL ’06, pages 866–873. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Xu</author>
<author>Xin Liu</author>
<author>Yihong Gong</author>
</authors>
<title>Document clustering based on non-negative matrix factorization.</title>
<date>2003</date>
<booktitle>In Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval,</booktitle>
<pages>267--273</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="2067" citStr="Xu et al., 2003" startWordPosition="327" endWordPosition="330">ful across a wide variety of text processing applications ranging from document clustering to search relevance measurement. In these applications, text is represented as a vector in a multi-dimensional continuous space, and a similarity metric such as cosine similarity can be used to measure the relatedness of different items. Vector space representations have been used both at the document and word levels. At the document level, they are effective for applications including information retrieval (Salton and McGill, 1983; Deerwester et al., 1990), document clustering (Deerwester et al., 1990; Xu et al., 2003), search relevance measurement (Baeza-Yates and Ribiero-Neto, 1999) and cross-lingual document retrieval (Platt et al., 2010). At the word level, vector representations have been used to measure word similarity (Deerwester et al., 1990; Turney and Littman, 2005; Turney, 2006; Turney, 2001; Lin, 1998; Agirre et al., 2009; Reisinger and Mooney, 2010) and for language modeling (Bellegarda, 2000; Coccaro and Jurafsky, 1998). While quite successful, these applications have typically been consistent with a very general notion of similarity in which basic association is measured, and finer shades of </context>
</contexts>
<marker>Xu, Liu, Gong, 2003</marker>
<rawString>Wei Xu, Xin Liu, and Yihong Gong. 2003. Document clustering based on non-negative matrix factorization. In Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval, pages 267–273, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wen-tau Yih</author>
<author>Kristina Toutanova</author>
<author>John C Platt</author>
<author>Christopher Meek</author>
</authors>
<title>Learning discriminative projections for text similarity measures.</title>
<date>2011</date>
<booktitle>In Proceedings of the Fifteen Conference on Computational Natural Language Learning (CoNLL),</booktitle>
<pages>247--256</pages>
<location>Portland, Oregon, USA.</location>
<contexts>
<context position="19264" citStr="Yih et al. (2011)" startWordPosition="3096" endWordPosition="3099">licitly try to achieve such goals. In this section, we see that when supervised training data is available, the projection matrix of LSA can be enhanced through a discriminative training technique explicitly designed to create a representation suited to a specific task. Because LSA is closely related to principle component analysis (PCA), extensions of PCA such as canonical correlation analysis (CCA) and oriented principle component analysis (OPCA) can leverage the labeled data and produce the projection matrix through general eigen-decomposition (Platt et al., 2010). Along this line of work, Yih et al. (2011) proposed a Siamese neural network approach called S2Net, which tunes the projection matrix directly through gradient descent, and has shown to outperform other methods in several tasks. Below we describe briefly this technique and explain how we adopt it for the task of antonym detection. The goal of S2Net is to learn a concept vector representation of the original sparse term vectors. Although such transformation can be non-linear in general, its current design chooses the model form to be a linear projection matrix, which is identical to 1216 Word PILSA-Similar Words PILSA-Least-Similar Wor</context>
<context position="23200" citStr="Yih et al., 2011" startWordPosition="3721" endWordPosition="3724">ion encourages Δij to be larger, an antonym pair will tend to have a smaller cosine similarity than other pairs. Because S2Net uses a gradient descent technique and a non-convex objective function, it is sensitive to initialization, and we have found that the PILSA projection matrix U (Section 3) provides an excellent starting point. As illustrated in Section 7, learning the word vectors with S2Net produces a significant improvement over PILSA alone. 1 L(Δij; A) m(m − 1) 6 Extending PILSA to Out-of-thesaurus Words 1&lt;i,j&lt;M,i=,4j Parameter learning (i.e., tuning A) can be done 2As suggested in (Yih et al., 2011), -y is set to 10 in our experiments. While PILSA is effective at representing synonym and antonym information, in its pure form, it is limited to the vocabulary of the thesaurus. In order to extend PILSA to operate on out-of-thesaurus words, 1217 we employ a two-stage strategy. We first conduct some lexical analysis and try to match an unknown word to one or more in-thesaurus words in their lemmatized forms. If no such match can be found, we then attempt to find semantically related inthesaurus words by leveraging co-occurrence statistics from general text data. These two steps are described </context>
</contexts>
<marker>Yih, Toutanova, Platt, Meek, 2011</marker>
<rawString>Wen-tau Yih, Kristina Toutanova, John C. Platt, and Christopher Meek. 2011. Learning discriminative projections for text similarity measures. In Proceedings of the Fifteen Conference on Computational Natural Language Learning (CoNLL), pages 247–256, Portland, Oregon, USA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001325">
<title confidence="0.995633">
A Comparison of Vector-based Representations for Semantic Composition
</title>
<author confidence="0.994489">
William Blacoe and Mirella Lapata
</author>
<affiliation confidence="0.9994995">
Institute for Language, Cognition and Computation
School of Informatics, University of Edinburgh
</affiliation>
<address confidence="0.993809">
10 Crichton Street, Edinburgh EH8 9AB
</address>
<email confidence="0.997603">
w.b.blacoe@sms.ed.ac.uk, mlap@inf.ed.ac.uk
</email>
<sectionHeader confidence="0.99682" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.995700235294118">
In this paper we address the problem of
modeling compositional meaning for phrases
and sentences using distributional methods.
We experiment with several possible com-
binations of representation and composition,
exhibiting varying degrees of sophistication.
Some are shallow while others operate over
syntactic structure, rely on parameter learn-
ing, or require access to very large corpora.
We find that shallow approaches are as good
as more computationally intensive alternatives
with regards to two particular tests: (1) phrase
similarity and (2) paraphrase detection. The
sizes of the involved training corpora and the
generated vectors are not as important as the
fit between the meaning representation and
compositional method.
</bodyText>
<sectionHeader confidence="0.99878" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999848375">
Distributional models of semantics have seen con-
siderable success at simulating a wide range of be-
havioral data in tasks involving semantic cognition
and also in practical applications. For example, they
have been used to model judgments of semantic sim-
ilarity (McDonald, 2000) and association (Denhire
and Lemaire, 2004; Griffiths et al., 2007) and have
been shown to achieve human level performance
on synonymy tests (Landauer and Dumais, 1997;
Griffiths et al., 2007) such as those included in the
Test of English as a Foreign Language (TOEFL).
This ability has been put to practical use in numer-
ous natural language processing tasks such as au-
tomatic thesaurus extraction (Grefenstette, 1994),
word sense discrimination (Sch¨utze, 1998), lan-
guage modeling (Bellegarda, 2000), and the iden-
tification of analogical relations (Turney, 2006).
While much research has been directed at the
most effective ways of constructing representations
for individual words, there has been far less con-
sensus regarding the representation of larger con-
structions such as phrases and sentences. The prob-
lem has received some attention in the connection-
ist literature, particularly in response to criticisms of
the ability of connectionist representations to handle
complex structures (Smolensky, 1990; Plate, 1995).
More recently, several proposals have been put for-
ward for computing the meaning of word combina-
tions in vector spaces. This renewed interest is partly
due to the popularity of distributional methods and
their application potential to tasks that require an un-
derstanding of larger phrases or complete sentences.
For example, Mitchell and Lapata (2010) intro-
duce a general framework for studying vector com-
position, which they formulate as a function f of
two vectors u and v. Different composition mod-
els arise, depending on how f is chosen. Assuming
that composition is a linear function of the Cartesian
product of u and v allows to specify additive mod-
els which are by far the most common method of
vector combination in the literature (Landauer and
Dumais, 1997; Foltz et al., 1998; Kintsch, 2001).
Alternatively, assuming that composition is a linear
function of the tensor product of u and v, gives rise
to models based on multiplication.
One of the most sophisticated proposals for se-
mantic composition is that of Clark et al. (2008) and
the more recent implementation of Grefenstette and
</bodyText>
<page confidence="0.974704">
546
</page>
<note confidence="0.790359">
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 546–556, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.999940597560976">
Sadrzadeh (2011a). Using techniques from logic,
category theory, and quantum information they de-
velop a compositional distributional semantics that
brings type-logical and distributional vector space
models together. In their framework, words belong
to different type-based categories and different cate-
gories exist in different dimensional spaces. The cat-
egory of a word is decided by the number and type of
adjoints (arguments) it can take and the composition
of a sentence results in a vector which exists in sen-
tential space. Verbs, adjectives and adverbs act as re-
lational functions, are represented by matrices, and
modify the properties of nouns, that are represented
by vectors (see also Baroni and Zamparelli (2010)
for a proposal similar in spirit). Clarke (2012) intro-
duces context-theoretic semantics, a general frame-
work for combining vector representations, based on
a mathematical theory of meaning as context, and
shows that it can be used to describe a variety of
models including that of Clark et al. (2008).
Socher et al. (2011a) and Socher et al. (2011b)
present a framework based on recursive neural net-
works that learns vector space representations for
multi-word phrases and sentences. The network is
given a list of word vectors as input and a binary
tree representing their syntactic structure. Then, it
computes an n-dimensional representation p of two
n-dimensional children and the process is repeated
at every parent node until a representation for a full
tree is constructed. Parent representations are com-
puted essentially by concatenating the representa-
tions of their children. During training, the model
tries to minimize the reconstruction errors between
the n-dimensional parent vectors and those repre-
senting their children. This model can also compute
compositional representations when the tree struc-
ture is not given, e.g., by greedily inferring a binary
tree.
Although the type of function used for vector
composition has attracted much attention, relatively
less emphasis has been placed on the basic distri-
butional representations on which the composition
functions operate. In this paper, we examine three
types of distributional representation of increasing
sophistication and their effect on semantic composi-
tion. These include a simple semantic space, where
a word’s vector represents its co-occurrence with
neighboring words (Mitchell and Lapata, 2010),
a syntax-aware space based on weighted distribu-
tional tuples that encode typed co-occurrence rela-
tions among words (Baroni and Lenci, 2010), and
word embeddings computed with a neural language
model (Bengio, 2001; Collobert and Weston, 2008).
Word embeddings are distributed representations,
low-dimensional and real-valued. Each dimension
of the embedding represents a latent feature of the
word, hopefully capturing useful syntactic and se-
mantic properties.
Using these representations, we construct several
compositional models, based on addition, multipli-
cation, and recursive neural networks. We assess
the effectiveness of these models using two evalua-
tion protocols. The first one involves modeling sim-
ilarity judgments for short phrases gathered in hu-
man experiments (Mitchell and Lapata, 2010). The
second one is paraphrase detection, i.e., the task of
examining two sentences and determining whether
they have the same meaning (Socher et al., 2011a).
We find that shallow approaches are as good as
more computationally intensive alternatives. They
achieve considerable semantic expressivity without
any learning, sophisticated linguistic processing, or
access to very large corpora.
Our contributions in this work are three-fold: an
empirical comparison of a broad range of composi-
tional models, some of which are introduced here for
the first time; the use of an evaluation methodology
that takes into account the full spectrum of compo-
sitionality from phrases to sentences; and the em-
pirical finding that relatively simple compositional
models can be used to perform competitively on the
paraphrase detection and phrase similarity tasks.
</bodyText>
<sectionHeader confidence="0.944244" genericHeader="introduction">
2 Modeling
</sectionHeader>
<bodyText confidence="0.999960363636364">
The elementary objects that we operate on are vec-
tors associated with words. We instantiate these
word representations following three distinct seman-
tic space models which we describe in Section 2.1
below. Analogously, in Section 2.2 we consider
three methods of vector composition, i.e., how a
phrase or a sentence can be represented as a vector
using the vectors of its constituent words. Combin-
ing different vector representations and composition
methods gives rise to several compositional models
whose performance we evaluate in Sections 3 and 4.
</bodyText>
<page confidence="0.994427">
547
</page>
<subsectionHeader confidence="0.990726">
2.1 Word Representations
</subsectionHeader>
<bodyText confidence="0.999976875">
For all of our experiments we employ column vec-
tors from a Cartesian, finitely-dimensional space.
The dimensionality will depend on the source of
the vectors involved. Similarly, the component val-
ues inside each source’s vectors are not to be inter-
preted in the same manner. Nonetheless, they have
in common that they originate from distributive cor-
pus statistics.
</bodyText>
<subsectionHeader confidence="0.606874">
Co-occurence-based Semantic Space Word
</subsectionHeader>
<bodyText confidence="0.9992443">
meaning is commonly represented in a high-
dimensional space, where each component corre-
sponds to some contextual element in which the
word is found. The contextual elements can be
words themselves, or larger linguistic units such as
sentences or documents, or even more complex lin-
guistic representations such as the argument slots of
predicates. A semantic space that is often employed
in studying compositionality across a variety of
tasks (Mitchell and Lapata, 2010; Grefenstette and
Sadrzadeh, 2011a) uses a context window of five
words on either side of the target word, and 2,000
vector dimensions. These are the common context
words in the British National Corpus (BNC), a
corpus of about 100 million tokens. Their values
are set to the ratio of the probability of the context
word given the target word to the probability of the
context word overall.
More formally, let us consider the BNC as a set of
sentences:
</bodyText>
<equation confidence="0.942121333333333">
BNC = {Sen(BNC)
1 ,...,Sen(BNC)
nBNC } (1)
where the i-th sentence is a sequence of words
Seni = (w(i)
1 ,...,w(i)
</equation>
<bodyText confidence="0.999721625">
ni ) from the BNC’s vocabulary
VocBNC. Then f reqw is the amount of times
that each word w ∈ VocBNC appears in the BNC.
Mitchell and Lapata (2010) collect the M most
frequent non-stoplist words in the set ctxttop =
{w(top)1, , wM(top) } and let them consitute the word
vectors’ dimensions. Each dimension’s value is ob-
tained from a co-occurrence count:
</bodyText>
<equation confidence="0.9901185">
coCountw[ j] =
|{k ∈ [t − 5;t + 5]|w(i) t= w, w(i) k= w(top) j}|
</equation>
<bodyText confidence="0.9395735">
for w ∈ VocBNC and j = 1,...,M. Using these counts,
they define word vectors component-wise.
</bodyText>
<equation confidence="0.995362142857143">
j |w) (3)
wdVec(rp)
w [ j] = p(w(top)
p(w(top)
j )
f reqw(top)
j
</equation>
<bodyText confidence="0.999909695652174">
for j = 1,...,M, where totalCount is the total num-
ber of words in the BNC.
This space is relatively simple, it has few param-
eters, requires no preprocessing other than tokeniza-
tion and involves no syntactic information or param-
eter learning. Despite its simplicity, it is a good start-
ing point for studying representations for composi-
tional models as a baseline against which to evaluate
more elaborate models.
Neural Language Model Another perhaps less
well-known approach to meaning representation is
to represent words as continuous vectors of param-
eters. Such word vectors can be obtained with an
unsupervised neural language model (NLM, Bengio
(2001); Collobert and Weston (2008)) which jointly
learns an embedding of words into a vector space
and uses these vectors to predict how likely a word
is, given its context.
We induced word embeddings with Collobert
and Weston (2008)’s neural language model. The
model is discriminative and non-probabilistic. Each
word i ∈ D (the vocabulary) is embedded into a
d-dimensional space using a lookup table LTW(·):
</bodyText>
<equation confidence="0.999345">
LTW(i) = Wi (4)
</equation>
<bodyText confidence="0.999503923076923">
where W ∈ Rd×|D |is a matrix of parameters to be
learned. Wi ∈ Rd is the i-th column of W and d is
the word vector size to be chosen by the user. The
parameters W are automatically trained during the
learning process using backpropagation.
Specifically, at each training update, the model
reads an n-gram x = (w1,...,wn) from the cor-
pus. The n-gram is paired with a corrupted n-gram
x˜ = (w1,..., ˜wn) where ˜wn =6 wn is chosen uniformly
from the vocabulary. The model concatenates the
learned embeddings of the n words and predicts a
score for the n-gram sequence using the learned em-
beddings as features. The training criterion is that
</bodyText>
<figure confidence="0.997817454545455">
(2)
nBNC
�
i=1
ni
E
t=1
coCountw[j]
freqw
×
totalCount
</figure>
<page confidence="0.980168">
548
</page>
<bodyText confidence="0.993188744680851">
n-grams that are present in the training corpus must
have a score at least some margin higher than the
corrupted n-grams. The model learns via gradient
descent over the neural network parameters and the
embedding lookup table. Word vectors are stored in
a word embedding matrix which captures syntactic
and semantic information from co-occurrence statis-
tics. As these representations are learned, albeit in
an unsupervised manner, one would hope that they
capture word meanings more succinctly, compared
to the simpler distributional representations that are
merely based on co-occurrence.
We trained the neural language model on the
BNC. We optimized the model’s parameters on a
word similarity task using 4% of the BNC as de-
velopment data. Specifically, we used WordSim353,
a benchmark dataset (Finkelstein et al., 2001), con-
sisting of relatedness judgments (on a scale of 0
to 10) for 353 word pairs. We experimented with
vectors of varying dimensionality (ranging from 50
to 200, with a step size of 50). The size of the target
word’s context window was 2, 3 and 4 in turn. The
rate at which embeddings were learned ranged from
3.4 x 10−10 to 6.7 x 10−10 to 10−9. We ran each
training process for 1.1 x 108 to 2.7 x 108 iterations
(ca. 2 days). We obtained the best results with 50
dimensions, a context window of size 4, and a em-
bedding learning rate of 10−9. The NLM with these
parameters was then trained for 1.51x109 iterations
(ca. 2 weeks).
Figure 1 illustrates a two-dimensional projection
of the embeddings for the 500 most common words
in the BNC. We only show two out of the actual
50 dimensions involved, but one can already begin
to see clusterings of a syntactic and semantic na-
ture. In one corner, for example, we encounter a
grouping of possessive pronouns together with the
possessive clitic ’s. The singular ones my, her and
his are closely positioned, as are the plural ones our,
your and their. Also, there is a clustering of socio-
political terms, such as international, country, na-
tional, government, and council.
Distributional Memory Tensor Baroni and Lenci
(2010) present Distributional Memory, a general-
ized framework for distributional semantics from
which several special-purpose models can be de-
rived. In their framework distributional information
</bodyText>
<figureCaption confidence="0.9976976">
Figure 1: A two-dimensional projection of the word em-
beddings we trained on the BNC using Turian et al.’s
(2010) implementation of the NLM. Two small sections
have been blown up to a legible scale. They show exam-
ples of syntactic and semantic clustering, respectively.
</figureCaption>
<table confidence="0.997280125">
word w link l co-word v value c
1950s-n of essence-n 2.4880
1950s-n during bring-v 16.4636
Anyone-n nmod reaction-n 1.2161
American-n coord-1 athlete-n 5.6485
American-j nmod wasp-n 3.4945
American-n such as-1 country-n 14.4269
American-n sbj tr build-v 23.1014
</table>
<tableCaption confidence="0.9707045">
Table 1: Example entries in Baroni and Lenci (2010)’s
tensor
</tableCaption>
<bodyText confidence="0.999940666666667">
is extracted from the corpus once, in the form of a
set of weighted word-link-word tuples arranged into
a third-order tensor. Different matrices are then gen-
erated from the tensor, and their rows and columns
give rise to different semantic spaces appropriate for
capturing different semantic problems. In this way,
the same distributional information can be shared
across tasks such as word similarity or analogical
learning.
More formally, Baroni and Lenci (2010) con-
struct a 3-dimensional tensor T assigning a value c
to instances of word pairs w,v and a connecting
link-word l. This representation operates over a
dependency-parsed corpus and the scores c are ob-
tained via counting the occurrences of tuples, and
weighting the raw counts by mutual information.
Table 1 presents examples of tensor entries. These
were taken from a distributional memory tensor1
</bodyText>
<footnote confidence="0.99837">
1Available at http://clic.cimec.unitn.it/dm/.
</footnote>
<page confidence="0.991015">
549
</page>
<table confidence="0.999700333333333">
frequency link l co-word v
17059 obj include-v
16713 obj use-v
16573 obj call-v
16475 obj see-v
15962 obj make-v
15707 nmod-1 other-j
15554 nmod-1 new-j
15224 obj find-v
15221 nmod-1 more-j
14715 nmod-1 first-j
14348 obj give-v
</table>
<tableCaption confidence="0.982823666666667">
Table 2: The 11 most frequent contexts in Baroni and
Lenci (2010)’s tensor (v and j represent verbs and adjec-
tives, respectively).
</tableCaption>
<bodyText confidence="0.999312538461538">
that Baroni and Lenci obtained via preprocessing
several corpora: the web-derived ukWac corpus of
about 1.915 billion words, a mid-2009 dump of
the English Wikipedia containing about 820 million
words, and the BNC.
Extracting a 3-dimensional tensor from the BNC
alone would create very sparse representations.
We therefore extract so-called word-fibres, essen-
tially projections onto a lower-dimensional sub-
space, from the same tensor Baroni and Lenci (2010)
collectively derived from the 3 billion word corpus
just described (henceforth 3-BWC). We view the
3-dimensional tensor
</bodyText>
<equation confidence="0.962199">
T = {(w(T) l(T) v(T) c(T))...} (5)
1 , 1 , 1 , 1 ,
</equation>
<bodyText confidence="0.999334285714286">
as a mapping which assigns each target word w a
non-zero value c, given the context (l,v). All word-
context combinations not listed in T are implicitly
assigned a zero value.
Now we consider two possible approaches for
obtaining vectors, depending on their application.
First, we let the D most frequent contexts
</bodyText>
<equation confidence="0.998825">
ctxtD = {(l1,v1),...,(lD,vD)} (6)
</equation>
<bodyText confidence="0.99292825">
constitute the D dimensions that each word vec-
tor will have. Table 2 shows the 11 contexts (l,v)
that appear most frequently in T. Thus, each target
word’s vector is defined component-wise as:
</bodyText>
<equation confidence="0.926298333333333">
� c, if (w,lj,vj,c) ∈ T
wdVecw[ j] = (7)
0, otherwise
</equation>
<bodyText confidence="0.966767625">
for j = 1,...,D. This approach is used when a fixed
vector dimensionality is necessary.
A more dynamic approach is possible when very
few words w1,...,wn are involved in a test. Their
representations can then have a denser format, that
is, with no zero-valued components. For this we
identify the set of contexts common to the words in-
volved, ctxtdyn = {(l(dyn)
</bodyText>
<equation confidence="0.9247492">
1 ,v(dyn)
1 ),(l(dyn)
2 ,v(dyn)
2 ),...} (8)
= {(l,v)|(wi,l,v,c) ∈ T,c ∈ R,i = 1,...,n}
</equation>
<bodyText confidence="0.998567857142857">
Each context (l,v) again constitutes a vector dimen-
sion. The dimensionality varies strongly depend-
ing on the selection of words, but if n does not ex-
ceed 4, the dimensionality |ctxtdyn |will typically be
substantial enough. In this approach, each word’s
vector consists of the values c found along with that
word and its context in the tensor.
</bodyText>
<equation confidence="0.95892475">
wdVecwi[j] = c, (9)
where (wi,l(dyn)
j ,v(dyn)
j ,c) ∈ T, for j = 1,...,|ctxtdyn|.
</equation>
<subsectionHeader confidence="0.838096">
2.2 Composition Methods
</subsectionHeader>
<bodyText confidence="0.9997936">
In our experiments we compose word vectors to cre-
ate representations for phrase vectors and sentence
vectors. The phrases we are interested in consist of
two words each: an adjective and a noun like black
hair, a compound noun made up of two nouns such
as oil industry, or a verbal phrase with a transitive
verb and an object noun, e.g., pour tea.
Conceiving of a phrase phr = (w1,w2) as a binary
tuple of words, we obtain its vector from its words’
vectors either by addition:
</bodyText>
<equation confidence="0.997577">
phrVec(w1,w2) = wdVecw1 + wdVecw2 (10)
or by point-wise multiplication:
phrVec(w1,w2) = wdVecw1 OwdVecw2 (11)
</equation>
<bodyText confidence="0.8960825">
In the same way we acquire a vector senVeci rep-
resenting a sentence Seni = (w(i)
</bodyText>
<equation confidence="0.694532">
1 ,...,w(i)
</equation>
<bodyText confidence="0.994389">
ni ) from the
vectors for w1,...,wni. We simply sum the existing
word vectors, that is, vectors obtained via the respec-
tive corpus for words that are not on our stoplist:
</bodyText>
<equation confidence="0.996642">
senVec(+)
i [j] = �
k=1,...,ni
wdVecwkexists
wdVecwk[j] (12)
</equation>
<page confidence="0.95914">
550
</page>
<bodyText confidence="0.973976">
And do the same with point-wise multiplication:
</bodyText>
<equation confidence="0.99339525">
senVec(o)
i [j] = �
k=1,...,ni
wdVecwkexists
</equation>
<bodyText confidence="0.999453655172414">
The multiplication model in (13) can be seen as an
instantiation of the categorical compositional frame-
work put forward by Clark et al. (2008). In fact,
a variety of multiplication-based models can be de-
rived from this framework; and comparisons against
component-wise multiplication on phrase similar-
ity tasks yield comparable results (Grefenstette
and Sadrzadeh, 2011a; Grefenstette and Sadrzadeh,
2011b). We thus opt for the model (13) as an exam-
ple of compositional models based on multiplication
due to its good performance across a variety of tasks,
including language modeling and prediction of read-
ing difficulty (Mitchell, 2011).
Our third method, for creating phrase and sen-
tence vectors alike, is the application of Socher et
al. (2011a)’s model. They use the Stanford parser
(Klein and Manning, 2003) to create a binary parse
tree for each input phrase or sentence. This tree is
then used as the basis for a deep recursive autoen-
coder (RAE). The aim is to construct a vector rep-
resentation for the tree’s root bottom-up where the
leaves contain word vectors. The latter can in the-
ory be provided by any type of semantic space, how-
ever Socher et al. use word embeddings provided by
the neural language model (Collobert and Weston,
2008).
Given the binary tree input structure, the model
computes parent representations p from their chil-
dren (c1,c2) using a standard neural network layer:
</bodyText>
<equation confidence="0.982576">
p = f(W(1)[c1;c2]+b(1)), (14)
</equation>
<bodyText confidence="0.999525571428571">
where [c1;c2] is the concatenation of the two chil-
dren, f is an element-wise activation function such
as tanh, b is a bias term, and W E Rnx2n is an en-
coding matrix that we want to learn during training.
One way of assessing how well p represents its di-
rect children is to decode their vectors in a recon-
struction layer:
</bodyText>
<equation confidence="0.8932605">
[c,
1;c,2] = f(W(2)p+b(2)) (15)
</equation>
<bodyText confidence="0.9904754">
During training, the goal is to minimize the re-
construction errors of all input pairs at nontermi-
nal nodes p in a given parse tree by computing the
square of the Euclidean distance between the origi-
nal input and its reconstruction:
</bodyText>
<equation confidence="0.9973215">
1
Erec([c1;c2]) = 2|[c1;c2] − [c,1;c,2]|2 (16)
</equation>
<bodyText confidence="0.999926558823529">
Socher et al. (2011a) extend the standard re-
cursive autoencoder sketched above in two ways.
Firstly, they present an unfolding autoencoder that
tries to reconstruct all leaf nodes underneath each
node rather than only its direct children. And sec-
ondly, instead of transforming the two children di-
rectly into a parent p, they introduce another hidden
layer inbetween.
We obtained three compositional models per rep-
resentation resulting in nine compositional mod-
els overall. Plugging different representations into
the additive and multiplicative models is relatively
straightforward. The RAE can also be used with
arbitrary word vectors. Socher et al. (2011a) ob-
tain best results with 100-dimensional vectors which
we also used in our experiments. NLM vectors
were trained with this dimensionality on the BNC
for 7.9 x 108 iterations (with window size 4 and an
embedding learning rate of 10−9). We constructed
a simple distributional space with M = 100 dimen-
sions, i.e., those connected to the 100 most frequent
co-occurrence words. In the case of vectors obtained
from Baroni and Lenci (2010)’s DM tensor, we dif-
ferentiated between phrases and sentences, due to
the disparate amount of words contained in them
(see Section 2.1). To represent phrases, we used
vectors of dynamic dimensionality, since these form
a richer and denser representation. The sentences
considered in Section 4 are too large for this ap-
proach and all word vectors must be members of
the same vector space. Hence, these sentence vec-
tors have fixed dimensionality D = 100, consisting
of the “most significant” 100 dimensions, i.e., those
reflecting the 100 most frequent contexts.
</bodyText>
<sectionHeader confidence="0.989393" genericHeader="method">
3 Experiment 1: Phrase Similarity
</sectionHeader>
<bodyText confidence="0.999975857142857">
Our first experiment focused on modeling similarity
judgments for short phrases gathered in human ex-
periments. Distributional representations of individ-
ual words are commonly evaluated on tasks based
on their ability to model semantic similarity rela-
tions, e.g., synonymy or priming. Thus, it seems
appropriate to evaluate phrase representations in a
</bodyText>
<equation confidence="0.835991">
wdVecwk[ j] (13)
</equation>
<page confidence="0.976283">
551
</page>
<table confidence="0.9983286">
dim. c.m. Adj-N N-N V-Obj
SDS 2000 + 0.37 0.38 0.28
(BNC) 2000 � 0.48 0.50 0.35
100 RAE 0.31 0.30 0.28
DM vary + 0.37 0.30 0.29
(3-BWC) vary � 0.21 0.37 0.33
100 RAE 0.25 0.26 0.09
NLM 50 + 0.28 0.26 0.24
(BNC) 50 � 0.26 0.22 0.18
100 RAE 0.19 0.24 0.28
</table>
<tableCaption confidence="0.8954905">
Table 3: Correlation coefficients of model predictions
with subject similarity ratings (Spearman’s p); columns
show dimensionality: fixed or varying (see Section 2.1),
composition method: + is additive vector composition,
O is component-wise multiplicative vector composition,
RAE is Socher et al. (2011a)’s recursive auto-encoder.
</tableCaption>
<bodyText confidence="0.998877666666667">
similar manner. Specifically, we used the dataset
from Mitchell and Lapata (2010) which contains
similarity judgments for adjective-noun, noun-noun
and verb-object phrases, respectively.2 Each item is
a phrase pair phr1, phr2 which has a human rating
from 1 (very low similarity) to 7 (very high similar-
ity).
Using the composition models described above,
we compute the cosine similarity of phr1 and phr2:
</bodyText>
<equation confidence="0.9998215">
phrSimphr1,phr2 = phrVecphr1 · phrVecphr2
|phrVecphr1 |× |phrVecphr2 |(17)
</equation>
<bodyText confidence="0.9999716">
Model similarities were evaluated against the human
similarity ratings using Spearman’s p correlation co-
efficient.
Table 3 summarizes the performance of the vari-
ous models on the phrase similarity dataset. Rows
in the table correspond to different vector repre-
sentations: the simple distributional semantic space
(SDS) from Mitchell and Lapata (2010), Baroni and
Lenci’s (2010) distributional memory tensor (DM)
and the neural language model (NLM), for each
phrase combination: adjective noun (Adj-N), noun-
noun (N-N) and verb object (V-Obj). For each
phrase type we report results for each compositional
model, namely additive (+), multiplicative (0) and
recursive autoencoder (RAE). The table also shows
</bodyText>
<footnote confidence="0.976875">
2The dataset is publicly available from http:
//homepages.inf.ed.ac.uk/s0453356/share
</footnote>
<bodyText confidence="0.998982833333333">
the dimensionality of the input vectors next to the
vector representation.
As can be seen, for SDS the best performing
model is multiplication, as it is mostly for DM. With
regard to NLM, vector addition yields overall better
results. In general, neither DM or NLM in any com-
positional configuration are able to outperform SDS
with multiplication. All models in Table 3 are sig-
nificantly correlated with the human similarity judg-
ments (p &lt; 0.01). Spearman’s p differences of 0.3
or more are significant at the 0.01 level, using a t-
test (Cohen and Cohen, 1983).
</bodyText>
<sectionHeader confidence="0.991649" genericHeader="method">
4 Experiment 2: Paraphrase Detection
</sectionHeader>
<bodyText confidence="0.99992596969697">
Although the phrase similarity task gives a fairly
direct insight into semantic similarity and compo-
sitional representations, it is somewhat limited in
scope as it only considers two-word constructions
rather than naturally occurring sentences. Ideally,
we would like to augment our evaluation with a task
which is based on large quantities of natural data and
for which vector composition has practical conse-
quences. For these reasons, we used the Microsoft
Research Paraphrase Corpus (MSRPC) introduced
by Dolan et al. (2004). The corpus consists of sen-
tence pairs Seni1,Seni2 and labels indicating whether
they are in a paraphrase relationship or not. The vec-
tor representations obtained from our various com-
positional models were used as features for the para-
phrase classification task.
The MSRPC dataset contains 5,801 sentence
pairs, we used the standard split of 4,076 training
pairs (67.5% of which are paraphrases) and 1,725
test pairs (66.5% of which are paraphrases). In order
to judge whether two sentences have the same mean-
ing we employ Fan et al. (2008)’s liblinear classifier.
For each of our three vector sources and three differ-
ent compositional methods, we create the following
features: (a) a vector representing the pair of input
sentences either via concatenation (“con”) or sub-
traction (“sub”); (b) a vector encoding which words
appear therein (“enc”); and (c) a vector made up of
the following four other pieces of information: the
cosine similarity of the sentence vectors, the length
of Seni1, the length of Seni2, and the unigram overlap
among the two sentences.
In order to encode which words appear in
</bodyText>
<page confidence="0.992052">
552
</page>
<table confidence="0.999644">
NLM DM SDS
(BNC) (3-BWC) (BNC)
+ 69.04 73.51 72.93
(con, other) (other) (other)
O 67.83 67.54 73.04
(sub, other) (other) (other)
RAE 70.26 68.29 69.10
(con, other) (sub, other) (con, other)
</table>
<tableCaption confidence="0.94091725">
Table 4: Paraphrase classification accuracy in %. In-
cluded features are in parentheses: “con” is sentence vec-
tor concatenation, “sub” is sentence vector subtraction,
“other” stands for 4 other features (see Section 4)
</tableCaption>
<bodyText confidence="0.974205666666667">
each sentence and how often, we define a vec-
tor wdCounti for sentence Seni and enumerate all
words occuring in the MSRPC:
</bodyText>
<equation confidence="0.996514">
VocMSRPC = {w(MSRPC) ,...,wnMSRPC }
(MSRPC)
1
</equation>
<bodyText confidence="0.999034">
giving the word count vectors nMSRPC dimensions.
Thus the k-th component of wdCounti is the fre-
quency with which the word w(MSRPC) appears in
</bodyText>
<equation confidence="0.998407142857143">
k
Seni = (w(i)
1 ,...,w(i)
ni ):
wdCounti[k] = |{ j E [1;ni]|w(MSRPC) = wj } |(19)
(i)
k
</equation>
<bodyText confidence="0.999107875">
for k = 1,...,nMSRPC. Even though nMSRPC may be
large, the computer files storing our feature vectors
do not explode in size because wdCount contains
many zeros and the classifier allows a sparse nota-
tion of (non-zero) feature values.
Regarding the last four features, we measured the
similarity between sentences the same way as we did
with phrases in section 3.
</bodyText>
<equation confidence="0.999547">
senSim _ senVeci1 · senVeci2 (20)
ti,4z  |senVeci1  |x  |senVeci2 |
</equation>
<bodyText confidence="0.9999261">
Note that this is the cosine of the angle between
senVeci1 and senVeci2. This enables us to observe
the similarity or dissimilarity of two sentences inde-
pendent of their sentence length. Even though each
contained word increases or decreases the norm of
the resulting sentence vector, this does not distort
the overall similarity value, due to normalization.
The lengths of Seni1 and Seni2 are simply the
number of words they contain. The unigram over-
lap feature value may be viewed as the cardinal-
</bodyText>
<table confidence="0.99934375">
NLM DM SDS
(BNC) (3-BWC) (BNC)
+ 81.00 82.16 80.76
(con, other) (other) (other)
O 80.41 80.18 82.33
(sub, other) (other) (other)
RAE 81.28 80.43 80.68
(con, other) (sub, other) (con, other)
</table>
<tableCaption confidence="0.998306">
Table 5: Paraphrase classification F1-score in %. The
involved features are exactly the same as in Table 4.
</tableCaption>
<bodyText confidence="0.999898333333333">
ity of the intersection of each sentence’s multiset-
bag-of-words. The latter is encoded in the already-
introduced wdCount vectors. Therefore,
</bodyText>
<equation confidence="0.707506">
min {wdCountis[k]} (21)
s=1,2
</equation>
<bodyText confidence="0.999981344827586">
In order to establish which features work best for
each representation and composition method, we ex-
haustively explored all combinations on a develop-
ment set (20% of the original MSRPC training set).
Tables 4 (accuracy) and 5 (F1) show our results on
the test set with the best feature combinations for
each model (shown in parentheses). Each row cor-
responds to a different type of composition and each
column to a different word representation model.
As can be seen, the distributional memory (DM)
is the best performing representation for the addi-
tive composition model. The neural language model
(NLM) gives best results for the recursive autoen-
coder (RAE), although the other two representations
come close. And finally the simple distributional
semantic space (SDS) works best with multiplica-
tion. Also note that the best performing models,
namely DM with addition and SDS with multipli-
cation, use a basic feature space consisting only of
the cosine similarity of the composed sentence vec-
tors, the length of the two sentences involved, and
their unigram word overlap.
Although our intention was to use the paraphrase
detection task as a test-bed for evaluating composi-
tional models rather than achieving state-of-the-art
results, Table 6 compares our approach against pre-
vious work on the same task and dataset. Initial re-
search concentrated on individual words rather than
sentential representations. Several approaches used
</bodyText>
<equation confidence="0.990322666666667">
nMSRPC
uniOverlapi1,i2 = E
(18) k=1
</equation>
<page confidence="0.998997">
553
</page>
<note confidence="0.815047166666667">
Model Acc. F1
Baseline 66.5 79.9
Mihalcea et al. (2006) 70.3 81.3
Rus et al. (2008) 70.6 80.5
Qiu et al. (2006) 72.0 81.6
Islam and Inkpen (2007) 72.6 81.3
Mitchell and Lapata (2010) ((D) 73.0 82.3
Baroni and Lenci (2010) (+) 73.5 82.2
Fernando and Stevenson (2008) 74.1 82.4
Wan et al. (2006) 75.6 83.0
Das and Smith (2009) 76.1 82.7
Socher et al. (2011a) 76.8 83.6
</note>
<tableCaption confidence="0.974988333333333">
Table 6: Overview of results on the MSRCP (test corpus).
Accuracy differences of 3.3 or more are significant at the
0.01 level (using the x2 statistic).
</tableCaption>
<bodyText confidence="0.941777322580645">
WordNet in conjunction with distributional similar-
ity in an attempt to detect meaning conveyed by syn-
onymous words (Islam and Inkpen, 2007; Mihalcea
et al., 2006; Fernando and Stevenson, 2008). More
recently, the addition of syntactic features based
on dependency parse trees (Wan et al., 2006; Das
and Smith, 2009) has been shown to substantially
boost performance. The model of Das and Smith
(2009), for example, uses quasi-synchronous depen-
dency grammar to model the structure of the sen-
tences involved in the comparison and their corre-
spondences. Socher et al. (2011a) obtain an accu-
racy that is higher than previously published results.
This model is more sophisticated than the one we
used in our experiments (see Table 4 and 5). Rather
than using the output of the RAE as features for the
classifier, it applies dynamic pooling, a procedure
that takes a similarity matrix as input (e.g., created
by sentences with differing lengths) and maps it to
a matrix of fixed size that represents more faithfully
the global similarity structure.3
Overall, we observe that our own models do as
well as some of the models that employ WordNet
and more sophisticated syntactic features. With re-
gard to F1, we are comparable with Das and Smith
(2009) and Socher et al. (2011a) without using elab-
orate features, or any additional manipulations over
and above the output of the composition functions
3Without dynamic pooling, their model yields an accuracy
of 74.2.
which if added could increase performance.
</bodyText>
<sectionHeader confidence="0.996466" genericHeader="conclusions">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999902136363637">
In this paper we systematically compared three types
of distributional representation and their effect on
semantic composition. Our comparisons involved
a simple distributional semantic space (Mitchell
and Lapata, 2010), word embeddings computed
with a neural language model (Collobert and We-
ston, 2008) and a representation based on weighted
word-link-word tuples arranged into a third-order
tensor (Baroni and Lenci, 2010). These represen-
tations vary in many respects: the amount of pre-
processing and linguistic information involved (the
third-order tensor computes semantic representa-
tions over parsed corpora), whether the semantic
space is the by-product of a learning process (in the
neural language model the parameters of the lookup
table must be learned), and data requirements (the
third-order tensor involves processing billions of
words). These representations served as input to
three composition methods involving addition, mul-
tiplication and a deep recursive autoencoder. Again
these methods differ in terms of how they imple-
ment compositionality: addition and multiplication
are commutative and associative operations and thus
ignore word order and, more generally, syntactic
structure. In contrast, the recursive autoencoder is
syntax-aware as it operates over a parse tree. How-
ever, the composed representations must be learned
with a neural network.
We evaluated nine models on the complementary
tasks of phrase similarity and paraphrase detection.
The former task simplifies the challenge of find-
ing an adequate method of composition and places
more emphasis on the representation, whereas the
latter poses, in a sense, the ultimate challenge for
composition models. It involves entire sentences
exhibiting varied syntactic constructions and in the
limit involves genuine natural language undertand-
ing. Across both tasks our results deliver a consis-
tent message: simple is best. Despite being in the-
ory more expressive, the representations obtained by
the neural language model and the third-order ten-
sor cannot match the simple semantic space on the
phrase similarity task. In this task syntax-oblivious
composition models are superior to the more sophis-
</bodyText>
<page confidence="0.994819">
554
</page>
<bodyText confidence="0.999951208333334">
ticated recursive autoencoder. The latter performs
better on the paraphrase detection task when its out-
put is fed to a classifier. The simple semantic space
may not take word order or sentence structure into
account, but nevertheless achieves considerable se-
mantic expressivity: it is on par with the third-order
tensor without having access to as much data (3 bil-
lion words) or a syntactically parsed corpus.
What do these findings tell us about the future of
compositional models for distributional semantics?
The problem of finding the right methods of vec-
tor composition cannot be pursued independent of
the choice of lexical representation. Having tested
many model combinations, we argue that in a good
model of distributive semantics representation and
composition must go hand in hand, i.e., they must
be mutually learned.
Acknowledgments We are grateful to Jeff
Mitchell for his help with the re-implementation
of his models. Thanks to Frank Keller and Micha
Elsner for their input on earlier versions of this work
and to Richard Socher for technical assistance. We
acknowledge the support of EPSRC through project
grant EP/I032916/1.
</bodyText>
<sectionHeader confidence="0.997852" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999810973333334">
Marco Baroni and Alessandro Lenci. 2010. Distribu-
tional memory: A general framework for corpus-based
semantics. Computational Linguistics, 36(4):673–
721.
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of the 2010 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1183–1193, Cambridge, MA.
Jerome R. Bellegarda. 2000. Exploiting latent seman-
tic information in statistical language modeling. Pro-
ceedings of the Institute of of Electrical and Electron-
ics Engineers, 88(8):1279–1296.
Yoshua Bengio. 2001. Neural net language models.
Scholarpedia, 3(1):3881.
Stephen Clark, Bob Coecke, and Mehrnoosh Sadrzadeh.
2008. A compositional distributional model of mean-
ing. In Proceedings of the 2nd Quantum Interaction
Symposium, pages 133–140, Oxford, UK.
Daoud Clarke. 2012. A context-theoretic framework for
compositionality in distributional semantics. Compu-
tational Linguistics, 38(1):41–71.
J Cohen and P Cohen. 1983. Applied Multiple Regres-
sion/Correlation Analysis for the Behavioral Sciences.
Hillsdale, NJ: Erlbaum.
Ronan Collobert and Jason Weston. 2008. A unified ar-
chitecture for natural language processing: deep neural
networks with multitask learning. In Proceedings of
the 25th International Conference on Machine Learn-
ing, pages 160–167, New York, NY. ACM.
Dipanjan Das and Noah A. Smith. 2009. Paraphrase
identification as probabilistic quasi-synchronous
recognition. In Proceedings of the Joint Conference
of the 47th Annual Meeting of the ACL and the 4th
International Joint Conference on Natural Language
Processing of the AFNLP, pages 468–476, Suntec,
Singapore.
G. Denhire and B. Lemaire. 2004. A computational
model of children’s semantic memory. In Proceedings
of the 26th Annual Meeting of the Cognitive Science
Society, pages 297–302, Mahwah, NJ. Lawrence Erl-
baum Associates.
Bill Dolan, Chris Quirk, and Chris Brockett. 2004.
Unsupervised construction of large paraphrase cor-
pora: Exploiting massively parallel news sources. In
Proceedings of the 20th International Conference on
Computational Linguistics, pages 350–356, Geneva,
Switzerland. COLING.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A
library for large linear classification. Journal of Ma-
chine Learning Research, 9:1871–1874.
Samuel Fernando and Mark Stevenson. 2008. A seman-
tic similarity approach to paraphrase detection. Tech-
nology.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan
Ruppin. 2001. Placing search in context: the concept
revisited. In WWW, pages 406–414.
Peter Foltz, Walter Kintsch, and Thomas Landauer.
1998. The measurement of textual coherence with la-
tent semantic analysis. Discourse Process, 15:285–
307.
Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011a.
Experimental support for a categorical compositional
distributional model of meaning. In Proceedings of
the 2011 Conference on Empirical Methods in Natural
Language Processing, pages 1394–1404, Edinburgh,
Scotland.
Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011b.
Experimenting with transitive verbs in a DisCoCat. In
Proceedings of the GEMS 2011 Workshop on GEomet-
rical Models of Natural Language Semantics, pages
62–66, Edinburgh, UK.
</reference>
<page confidence="0.981785">
555
</page>
<reference confidence="0.999907410256411">
Gregory Grefenstette. 1994. Explorations in Automatic
Thesaurus Discovery. Kluwer Academic Publishers,
Norwell, MA.
Thomas L. Griffiths, Mark Steyvers, and Joshua B.
Tenenbaum. 2007. Topics in semantic representation.
Psychological Review, 114(2):211–244.
Aminul Islam and Diana Inkpen. 2007. Semantic sim-
ilarity of short texts. In Proceedings of the Interna-
tional Conference on RecentAdvances in Natural Lan-
guage Processing, Borovets, Bulgaria.
Walter Kintsch. 2001. Predication. Cognitive Science,
25(2):173–202.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In Proceedings of the 41st
Annual Meeting of the Association for Computational
Linguistics, pages 423–430, Sapporo, Japan.
T. K. Landauer and S. T. Dumais. 1997. A solution to
Plato’s problem: the latent semantic analysis theory
of acquisition, induction and representation of knowl-
edge. Psychological Review, 104(2):211–240.
Scott McDonald. 2000. Environmental Determinants of
Lexical Processing Effort. Ph.D. thesis, University of
Edinburgh.
Rada Mihalcea, Courtney Corley, and Carlo Strapparava,
2006. Corpus-based and knowledge-based measures
of text semantic similarity, pages 775–780. AAAI
Press.
Jeff Mitchell and Mirella Lapata. 2010. Composition in
distributional models of semantics. Cognitive Science,
38(8):1388–1429.
Jeff Mitchell. 2011. Composition in distributional mod-
els of semantics. Ph.D. thesis, University of Edin-
burgh.
Tony A. Plate. 1995. Holographic reduced repre-
sentations. IEEE Transactions on Neural Networks,
6(3):623–641.
Long Qiu, Min-Yen Kan, and Tat-Seng Chua. 2006.
Paraphrase recognition via dissimilarity significance
classification. In Proceedings of the 2006 Conference
on Empirical Methods in Natural Language Process-
ing, pages 18–26, Sydney, Australia.
Vasile Rus, Philip M. McCarthy, Mihai C. Lintean,
Danielle S. McNamara, and Arthur C. Graesser. 2008.
Paraphrase identification with lexico-syntactic graph
subsumption. In David Wilson and H. Chad Lane, ed-
itors, Florida Artificial Intelligence Research Society
Conference, pages 201–206. AAAI Press.
Hinrich Sch¨utze. 1998. Automatic word sense discrimi-
nation. Computational Linguistics, 24(1):97–124.
Paul Smolensky. 1990. Tensor product variable bind-
ing and the representation of symbolic structures in
connectionist systems. Artificial Intelligence, 46:159–
216.
Richard Socher, Eric H. Huang, Jeffrey Pennin, An-
drew Y. Ng, and Christopher D. Manning. 2011a.
Dynamic Pooling and Unfolding Recursive Autoen-
coders for Paraphrase Detection. In Advances in Neu-
ral Information Processing Systems 24, pages 801–
809. Granada, Spain.
Richard Socher, Jeffrey Pennington, Eric H. Huang, An-
drew Y. Ng, and Christopher D. Manning. 2011b.
Semi-supervised recursive autoencoders for predicting
sentiment distributions. In Proceedings of the 2011
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 151–161, Edinburgh, Scot-
land.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method for
semi-supervised learning. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics, pages 384–394, Uppsala, Sweden.
Peter D. Turney. 2006. Similarity of semantic relations.
Computational Linguistics, 32(3):379–416.
Stephen Wan, Mark Dras, Robert Dale, and Cecile Paris.
2006. Using dependency-based features to take the
“para-farce” out of paraphrase. In Proceedings of the
Australasian Language Technology Workshop 2006,
pages 131–138, Sydney, Australia.
</reference>
<page confidence="0.998556">
556
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.775298">
<title confidence="0.998416">A Comparison of Vector-based Representations for Semantic Composition</title>
<author confidence="0.826476">Blacoe</author>
<affiliation confidence="0.9905925">Institute for Language, Cognition and School of Informatics, University of</affiliation>
<address confidence="0.983086">10 Crichton Street, Edinburgh EH8</address>
<email confidence="0.998621">w.b.blacoe@sms.ed.ac.uk,mlap@inf.ed.ac.uk</email>
<abstract confidence="0.997467388888889">In this paper we address the problem of modeling compositional meaning for phrases and sentences using distributional methods. We experiment with several possible combinations of representation and composition, exhibiting varying degrees of sophistication. Some are shallow while others operate over syntactic structure, rely on parameter learning, or require access to very large corpora. We find that shallow approaches are as good as more computationally intensive alternatives with regards to two particular tests: (1) phrase similarity and (2) paraphrase detection. The sizes of the involved training corpora and the generated vectors are not as important as the fit between the meaning representation and compositional method.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Alessandro Lenci</author>
</authors>
<title>Distributional memory: A general framework for corpus-based semantics.</title>
<date>2010</date>
<journal>Computational Linguistics,</journal>
<volume>36</volume>
<issue>4</issue>
<pages>721</pages>
<contexts>
<context position="6219" citStr="Baroni and Lenci, 2010" startWordPosition="932" endWordPosition="935"> of function used for vector composition has attracted much attention, relatively less emphasis has been placed on the basic distributional representations on which the composition functions operate. In this paper, we examine three types of distributional representation of increasing sophistication and their effect on semantic composition. These include a simple semantic space, where a word’s vector represents its co-occurrence with neighboring words (Mitchell and Lapata, 2010), a syntax-aware space based on weighted distributional tuples that encode typed co-occurrence relations among words (Baroni and Lenci, 2010), and word embeddings computed with a neural language model (Bengio, 2001; Collobert and Weston, 2008). Word embeddings are distributed representations, low-dimensional and real-valued. Each dimension of the embedding represents a latent feature of the word, hopefully capturing useful syntactic and semantic properties. Using these representations, we construct several compositional models, based on addition, multiplication, and recursive neural networks. We assess the effectiveness of these models using two evaluation protocols. The first one involves modeling similarity judgments for short ph</context>
<context position="14201" citStr="Baroni and Lenci (2010)" startWordPosition="2243" endWordPosition="2246">ates a two-dimensional projection of the embeddings for the 500 most common words in the BNC. We only show two out of the actual 50 dimensions involved, but one can already begin to see clusterings of a syntactic and semantic nature. In one corner, for example, we encounter a grouping of possessive pronouns together with the possessive clitic ’s. The singular ones my, her and his are closely positioned, as are the plural ones our, your and their. Also, there is a clustering of sociopolitical terms, such as international, country, national, government, and council. Distributional Memory Tensor Baroni and Lenci (2010) present Distributional Memory, a generalized framework for distributional semantics from which several special-purpose models can be derived. In their framework distributional information Figure 1: A two-dimensional projection of the word embeddings we trained on the BNC using Turian et al.’s (2010) implementation of the NLM. Two small sections have been blown up to a legible scale. They show examples of syntactic and semantic clustering, respectively. word w link l co-word v value c 1950s-n of essence-n 2.4880 1950s-n during bring-v 16.4636 Anyone-n nmod reaction-n 1.2161 American-n coord-1 </context>
<context position="15446" citStr="Baroni and Lenci (2010)" startWordPosition="2432" endWordPosition="2435">can-j nmod wasp-n 3.4945 American-n such as-1 country-n 14.4269 American-n sbj tr build-v 23.1014 Table 1: Example entries in Baroni and Lenci (2010)’s tensor is extracted from the corpus once, in the form of a set of weighted word-link-word tuples arranged into a third-order tensor. Different matrices are then generated from the tensor, and their rows and columns give rise to different semantic spaces appropriate for capturing different semantic problems. In this way, the same distributional information can be shared across tasks such as word similarity or analogical learning. More formally, Baroni and Lenci (2010) construct a 3-dimensional tensor T assigning a value c to instances of word pairs w,v and a connecting link-word l. This representation operates over a dependency-parsed corpus and the scores c are obtained via counting the occurrences of tuples, and weighting the raw counts by mutual information. Table 1 presents examples of tensor entries. These were taken from a distributional memory tensor1 1Available at http://clic.cimec.unitn.it/dm/. 549 frequency link l co-word v 17059 obj include-v 16713 obj use-v 16573 obj call-v 16475 obj see-v 15962 obj make-v 15707 nmod-1 other-j 15554 nmod-1 new-</context>
<context position="16711" citStr="Baroni and Lenci (2010)" startWordPosition="2627" endWordPosition="2630">15 nmod-1 first-j 14348 obj give-v Table 2: The 11 most frequent contexts in Baroni and Lenci (2010)’s tensor (v and j represent verbs and adjectives, respectively). that Baroni and Lenci obtained via preprocessing several corpora: the web-derived ukWac corpus of about 1.915 billion words, a mid-2009 dump of the English Wikipedia containing about 820 million words, and the BNC. Extracting a 3-dimensional tensor from the BNC alone would create very sparse representations. We therefore extract so-called word-fibres, essentially projections onto a lower-dimensional subspace, from the same tensor Baroni and Lenci (2010) collectively derived from the 3 billion word corpus just described (henceforth 3-BWC). We view the 3-dimensional tensor T = {(w(T) l(T) v(T) c(T))...} (5) 1 , 1 , 1 , 1 , as a mapping which assigns each target word w a non-zero value c, given the context (l,v). All wordcontext combinations not listed in T are implicitly assigned a zero value. Now we consider two possible approaches for obtaining vectors, depending on their application. First, we let the D most frequent contexts ctxtD = {(l1,v1),...,(lD,vD)} (6) constitute the D dimensions that each word vector will have. Table 2 shows the 11 </context>
<context position="22540" citStr="Baroni and Lenci (2010)" startWordPosition="3610" endWordPosition="3613">ll. Plugging different representations into the additive and multiplicative models is relatively straightforward. The RAE can also be used with arbitrary word vectors. Socher et al. (2011a) obtain best results with 100-dimensional vectors which we also used in our experiments. NLM vectors were trained with this dimensionality on the BNC for 7.9 x 108 iterations (with window size 4 and an embedding learning rate of 10−9). We constructed a simple distributional space with M = 100 dimensions, i.e., those connected to the 100 most frequent co-occurrence words. In the case of vectors obtained from Baroni and Lenci (2010)’s DM tensor, we differentiated between phrases and sentences, due to the disparate amount of words contained in them (see Section 2.1). To represent phrases, we used vectors of dynamic dimensionality, since these form a richer and denser representation. The sentences considered in Section 4 are too large for this approach and all word vectors must be members of the same vector space. Hence, these sentence vectors have fixed dimensionality D = 100, consisting of the “most significant” 100 dimensions, i.e., those reflecting the 100 most frequent contexts. 3 Experiment 1: Phrase Similarity Our f</context>
<context position="31493" citStr="Baroni and Lenci (2010)" startWordPosition="5043" endWordPosition="5046">ap. Although our intention was to use the paraphrase detection task as a test-bed for evaluating compositional models rather than achieving state-of-the-art results, Table 6 compares our approach against previous work on the same task and dataset. Initial research concentrated on individual words rather than sentential representations. Several approaches used nMSRPC uniOverlapi1,i2 = E (18) k=1 553 Model Acc. F1 Baseline 66.5 79.9 Mihalcea et al. (2006) 70.3 81.3 Rus et al. (2008) 70.6 80.5 Qiu et al. (2006) 72.0 81.6 Islam and Inkpen (2007) 72.6 81.3 Mitchell and Lapata (2010) ((D) 73.0 82.3 Baroni and Lenci (2010) (+) 73.5 82.2 Fernando and Stevenson (2008) 74.1 82.4 Wan et al. (2006) 75.6 83.0 Das and Smith (2009) 76.1 82.7 Socher et al. (2011a) 76.8 83.6 Table 6: Overview of results on the MSRCP (test corpus). Accuracy differences of 3.3 or more are significant at the 0.01 level (using the x2 statistic). WordNet in conjunction with distributional similarity in an attempt to detect meaning conveyed by synonymous words (Islam and Inkpen, 2007; Mihalcea et al., 2006; Fernando and Stevenson, 2008). More recently, the addition of syntactic features based on dependency parse trees (Wan et al., 2006; Das an</context>
<context position="33728" citStr="Baroni and Lenci, 2010" startWordPosition="5402" endWordPosition="5405">itional manipulations over and above the output of the composition functions 3Without dynamic pooling, their model yields an accuracy of 74.2. which if added could increase performance. 5 Discussion In this paper we systematically compared three types of distributional representation and their effect on semantic composition. Our comparisons involved a simple distributional semantic space (Mitchell and Lapata, 2010), word embeddings computed with a neural language model (Collobert and Weston, 2008) and a representation based on weighted word-link-word tuples arranged into a third-order tensor (Baroni and Lenci, 2010). These representations vary in many respects: the amount of preprocessing and linguistic information involved (the third-order tensor computes semantic representations over parsed corpora), whether the semantic space is the by-product of a learning process (in the neural language model the parameters of the lookup table must be learned), and data requirements (the third-order tensor involves processing billions of words). These representations served as input to three composition methods involving addition, multiplication and a deep recursive autoencoder. Again these methods differ in terms o</context>
</contexts>
<marker>Baroni, Lenci, 2010</marker>
<rawString>Marco Baroni and Alessandro Lenci. 2010. Distributional memory: A general framework for corpus-based semantics. Computational Linguistics, 36(4):673– 721.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Roberto Zamparelli</author>
</authors>
<title>Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1183--1193</pages>
<location>Cambridge, MA.</location>
<contexts>
<context position="4403" citStr="Baroni and Zamparelli (2010)" startWordPosition="658" endWordPosition="661">velop a compositional distributional semantics that brings type-logical and distributional vector space models together. In their framework, words belong to different type-based categories and different categories exist in different dimensional spaces. The category of a word is decided by the number and type of adjoints (arguments) it can take and the composition of a sentence results in a vector which exists in sentential space. Verbs, adjectives and adverbs act as relational functions, are represented by matrices, and modify the properties of nouns, that are represented by vectors (see also Baroni and Zamparelli (2010) for a proposal similar in spirit). Clarke (2012) introduces context-theoretic semantics, a general framework for combining vector representations, based on a mathematical theory of meaning as context, and shows that it can be used to describe a variety of models including that of Clark et al. (2008). Socher et al. (2011a) and Socher et al. (2011b) present a framework based on recursive neural networks that learns vector space representations for multi-word phrases and sentences. The network is given a list of word vectors as input and a binary tree representing their syntactic structure. Then</context>
</contexts>
<marker>Baroni, Zamparelli, 2010</marker>
<rawString>Marco Baroni and Roberto Zamparelli. 2010. Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1183–1193, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerome R Bellegarda</author>
</authors>
<title>Exploiting latent semantic information in statistical language modeling.</title>
<date>2000</date>
<booktitle>Proceedings of the Institute of of Electrical and Electronics Engineers,</booktitle>
<volume>88</volume>
<issue>8</issue>
<contexts>
<context position="1817" citStr="Bellegarda, 2000" startWordPosition="260" endWordPosition="261"> practical applications. For example, they have been used to model judgments of semantic similarity (McDonald, 2000) and association (Denhire and Lemaire, 2004; Griffiths et al., 2007) and have been shown to achieve human level performance on synonymy tests (Landauer and Dumais, 1997; Griffiths et al., 2007) such as those included in the Test of English as a Foreign Language (TOEFL). This ability has been put to practical use in numerous natural language processing tasks such as automatic thesaurus extraction (Grefenstette, 1994), word sense discrimination (Sch¨utze, 1998), language modeling (Bellegarda, 2000), and the identification of analogical relations (Turney, 2006). While much research has been directed at the most effective ways of constructing representations for individual words, there has been far less consensus regarding the representation of larger constructions such as phrases and sentences. The problem has received some attention in the connectionist literature, particularly in response to criticisms of the ability of connectionist representations to handle complex structures (Smolensky, 1990; Plate, 1995). More recently, several proposals have been put forward for computing the mean</context>
</contexts>
<marker>Bellegarda, 2000</marker>
<rawString>Jerome R. Bellegarda. 2000. Exploiting latent semantic information in statistical language modeling. Proceedings of the Institute of of Electrical and Electronics Engineers, 88(8):1279–1296.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
</authors>
<title>Neural net language models.</title>
<date>2001</date>
<journal>Scholarpedia,</journal>
<volume>3</volume>
<issue>1</issue>
<contexts>
<context position="6292" citStr="Bengio, 2001" startWordPosition="945" endWordPosition="946">ss emphasis has been placed on the basic distributional representations on which the composition functions operate. In this paper, we examine three types of distributional representation of increasing sophistication and their effect on semantic composition. These include a simple semantic space, where a word’s vector represents its co-occurrence with neighboring words (Mitchell and Lapata, 2010), a syntax-aware space based on weighted distributional tuples that encode typed co-occurrence relations among words (Baroni and Lenci, 2010), and word embeddings computed with a neural language model (Bengio, 2001; Collobert and Weston, 2008). Word embeddings are distributed representations, low-dimensional and real-valued. Each dimension of the embedding represents a latent feature of the word, hopefully capturing useful syntactic and semantic properties. Using these representations, we construct several compositional models, based on addition, multiplication, and recursive neural networks. We assess the effectiveness of these models using two evaluation protocols. The first one involves modeling similarity judgments for short phrases gathered in human experiments (Mitchell and Lapata, 2010). The seco</context>
<context position="10992" citStr="Bengio (2001)" startWordPosition="1696" endWordPosition="1697">otal number of words in the BNC. This space is relatively simple, it has few parameters, requires no preprocessing other than tokenization and involves no syntactic information or parameter learning. Despite its simplicity, it is a good starting point for studying representations for compositional models as a baseline against which to evaluate more elaborate models. Neural Language Model Another perhaps less well-known approach to meaning representation is to represent words as continuous vectors of parameters. Such word vectors can be obtained with an unsupervised neural language model (NLM, Bengio (2001); Collobert and Weston (2008)) which jointly learns an embedding of words into a vector space and uses these vectors to predict how likely a word is, given its context. We induced word embeddings with Collobert and Weston (2008)’s neural language model. The model is discriminative and non-probabilistic. Each word i ∈ D (the vocabulary) is embedded into a d-dimensional space using a lookup table LTW(·): LTW(i) = Wi (4) where W ∈ Rd×|D |is a matrix of parameters to be learned. Wi ∈ Rd is the i-th column of W and d is the word vector size to be chosen by the user. The parameters W are automatical</context>
</contexts>
<marker>Bengio, 2001</marker>
<rawString>Yoshua Bengio. 2001. Neural net language models. Scholarpedia, 3(1):3881.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>Bob Coecke</author>
<author>Mehrnoosh Sadrzadeh</author>
</authors>
<title>A compositional distributional model of meaning.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2nd Quantum Interaction Symposium,</booktitle>
<pages>133--140</pages>
<location>Oxford, UK.</location>
<contexts>
<context position="3380" citStr="Clark et al. (2008)" startWordPosition="508" endWordPosition="511">they formulate as a function f of two vectors u and v. Different composition models arise, depending on how f is chosen. Assuming that composition is a linear function of the Cartesian product of u and v allows to specify additive models which are by far the most common method of vector combination in the literature (Landauer and Dumais, 1997; Foltz et al., 1998; Kintsch, 2001). Alternatively, assuming that composition is a linear function of the tensor product of u and v, gives rise to models based on multiplication. One of the most sophisticated proposals for semantic composition is that of Clark et al. (2008) and the more recent implementation of Grefenstette and 546 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 546–556, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics Sadrzadeh (2011a). Using techniques from logic, category theory, and quantum information they develop a compositional distributional semantics that brings type-logical and distributional vector space models together. In their framework, words belong to different type-based categories and different ca</context>
<context position="4704" citStr="Clark et al. (2008)" startWordPosition="707" endWordPosition="710">of adjoints (arguments) it can take and the composition of a sentence results in a vector which exists in sentential space. Verbs, adjectives and adverbs act as relational functions, are represented by matrices, and modify the properties of nouns, that are represented by vectors (see also Baroni and Zamparelli (2010) for a proposal similar in spirit). Clarke (2012) introduces context-theoretic semantics, a general framework for combining vector representations, based on a mathematical theory of meaning as context, and shows that it can be used to describe a variety of models including that of Clark et al. (2008). Socher et al. (2011a) and Socher et al. (2011b) present a framework based on recursive neural networks that learns vector space representations for multi-word phrases and sentences. The network is given a list of word vectors as input and a binary tree representing their syntactic structure. Then, it computes an n-dimensional representation p of two n-dimensional children and the process is repeated at every parent node until a representation for a full tree is constructed. Parent representations are computed essentially by concatenating the representations of their children. During training</context>
<context position="19529" citStr="Clark et al. (2008)" startWordPosition="3115" endWordPosition="3118"> multiplication: phrVec(w1,w2) = wdVecw1 OwdVecw2 (11) In the same way we acquire a vector senVeci representing a sentence Seni = (w(i) 1 ,...,w(i) ni ) from the vectors for w1,...,wni. We simply sum the existing word vectors, that is, vectors obtained via the respective corpus for words that are not on our stoplist: senVec(+) i [j] = � k=1,...,ni wdVecwkexists wdVecwk[j] (12) 550 And do the same with point-wise multiplication: senVec(o) i [j] = � k=1,...,ni wdVecwkexists The multiplication model in (13) can be seen as an instantiation of the categorical compositional framework put forward by Clark et al. (2008). In fact, a variety of multiplication-based models can be derived from this framework; and comparisons against component-wise multiplication on phrase similarity tasks yield comparable results (Grefenstette and Sadrzadeh, 2011a; Grefenstette and Sadrzadeh, 2011b). We thus opt for the model (13) as an example of compositional models based on multiplication due to its good performance across a variety of tasks, including language modeling and prediction of reading difficulty (Mitchell, 2011). Our third method, for creating phrase and sentence vectors alike, is the application of Socher et al. (</context>
</contexts>
<marker>Clark, Coecke, Sadrzadeh, 2008</marker>
<rawString>Stephen Clark, Bob Coecke, and Mehrnoosh Sadrzadeh. 2008. A compositional distributional model of meaning. In Proceedings of the 2nd Quantum Interaction Symposium, pages 133–140, Oxford, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daoud Clarke</author>
</authors>
<title>A context-theoretic framework for compositionality in distributional semantics.</title>
<date>2012</date>
<journal>Computational Linguistics,</journal>
<volume>38</volume>
<issue>1</issue>
<contexts>
<context position="4452" citStr="Clarke (2012)" startWordPosition="668" endWordPosition="669">logical and distributional vector space models together. In their framework, words belong to different type-based categories and different categories exist in different dimensional spaces. The category of a word is decided by the number and type of adjoints (arguments) it can take and the composition of a sentence results in a vector which exists in sentential space. Verbs, adjectives and adverbs act as relational functions, are represented by matrices, and modify the properties of nouns, that are represented by vectors (see also Baroni and Zamparelli (2010) for a proposal similar in spirit). Clarke (2012) introduces context-theoretic semantics, a general framework for combining vector representations, based on a mathematical theory of meaning as context, and shows that it can be used to describe a variety of models including that of Clark et al. (2008). Socher et al. (2011a) and Socher et al. (2011b) present a framework based on recursive neural networks that learns vector space representations for multi-word phrases and sentences. The network is given a list of word vectors as input and a binary tree representing their syntactic structure. Then, it computes an n-dimensional representation p o</context>
</contexts>
<marker>Clarke, 2012</marker>
<rawString>Daoud Clarke. 2012. A context-theoretic framework for compositionality in distributional semantics. Computational Linguistics, 38(1):41–71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Cohen</author>
<author>P Cohen</author>
</authors>
<title>Applied Multiple Regression/Correlation Analysis for the Behavioral Sciences.</title>
<date>1983</date>
<publisher>Erlbaum.</publisher>
<location>Hillsdale, NJ:</location>
<contexts>
<context position="25924" citStr="Cohen and Cohen, 1983" startWordPosition="4137" endWordPosition="4140">y available from http: //homepages.inf.ed.ac.uk/s0453356/share the dimensionality of the input vectors next to the vector representation. As can be seen, for SDS the best performing model is multiplication, as it is mostly for DM. With regard to NLM, vector addition yields overall better results. In general, neither DM or NLM in any compositional configuration are able to outperform SDS with multiplication. All models in Table 3 are significantly correlated with the human similarity judgments (p &lt; 0.01). Spearman’s p differences of 0.3 or more are significant at the 0.01 level, using a ttest (Cohen and Cohen, 1983). 4 Experiment 2: Paraphrase Detection Although the phrase similarity task gives a fairly direct insight into semantic similarity and compositional representations, it is somewhat limited in scope as it only considers two-word constructions rather than naturally occurring sentences. Ideally, we would like to augment our evaluation with a task which is based on large quantities of natural data and for which vector composition has practical consequences. For these reasons, we used the Microsoft Research Paraphrase Corpus (MSRPC) introduced by Dolan et al. (2004). The corpus consists of sentence </context>
</contexts>
<marker>Cohen, Cohen, 1983</marker>
<rawString>J Cohen and P Cohen. 1983. Applied Multiple Regression/Correlation Analysis for the Behavioral Sciences. Hillsdale, NJ: Erlbaum.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
</authors>
<title>A unified architecture for natural language processing: deep neural networks with multitask learning.</title>
<date>2008</date>
<booktitle>In Proceedings of the 25th International Conference on Machine Learning,</booktitle>
<pages>160--167</pages>
<publisher>ACM.</publisher>
<location>New York, NY.</location>
<contexts>
<context position="6321" citStr="Collobert and Weston, 2008" startWordPosition="947" endWordPosition="950">s been placed on the basic distributional representations on which the composition functions operate. In this paper, we examine three types of distributional representation of increasing sophistication and their effect on semantic composition. These include a simple semantic space, where a word’s vector represents its co-occurrence with neighboring words (Mitchell and Lapata, 2010), a syntax-aware space based on weighted distributional tuples that encode typed co-occurrence relations among words (Baroni and Lenci, 2010), and word embeddings computed with a neural language model (Bengio, 2001; Collobert and Weston, 2008). Word embeddings are distributed representations, low-dimensional and real-valued. Each dimension of the embedding represents a latent feature of the word, hopefully capturing useful syntactic and semantic properties. Using these representations, we construct several compositional models, based on addition, multiplication, and recursive neural networks. We assess the effectiveness of these models using two evaluation protocols. The first one involves modeling similarity judgments for short phrases gathered in human experiments (Mitchell and Lapata, 2010). The second one is paraphrase detectio</context>
<context position="11021" citStr="Collobert and Weston (2008)" startWordPosition="1698" endWordPosition="1701">words in the BNC. This space is relatively simple, it has few parameters, requires no preprocessing other than tokenization and involves no syntactic information or parameter learning. Despite its simplicity, it is a good starting point for studying representations for compositional models as a baseline against which to evaluate more elaborate models. Neural Language Model Another perhaps less well-known approach to meaning representation is to represent words as continuous vectors of parameters. Such word vectors can be obtained with an unsupervised neural language model (NLM, Bengio (2001); Collobert and Weston (2008)) which jointly learns an embedding of words into a vector space and uses these vectors to predict how likely a word is, given its context. We induced word embeddings with Collobert and Weston (2008)’s neural language model. The model is discriminative and non-probabilistic. Each word i ∈ D (the vocabulary) is embedded into a d-dimensional space using a lookup table LTW(·): LTW(i) = Wi (4) where W ∈ Rd×|D |is a matrix of parameters to be learned. Wi ∈ Rd is the i-th column of W and d is the word vector size to be chosen by the user. The parameters W are automatically trained during the learnin</context>
<context position="20634" citStr="Collobert and Weston, 2008" startWordPosition="3296" endWordPosition="3299"> (Mitchell, 2011). Our third method, for creating phrase and sentence vectors alike, is the application of Socher et al. (2011a)’s model. They use the Stanford parser (Klein and Manning, 2003) to create a binary parse tree for each input phrase or sentence. This tree is then used as the basis for a deep recursive autoencoder (RAE). The aim is to construct a vector representation for the tree’s root bottom-up where the leaves contain word vectors. The latter can in theory be provided by any type of semantic space, however Socher et al. use word embeddings provided by the neural language model (Collobert and Weston, 2008). Given the binary tree input structure, the model computes parent representations p from their children (c1,c2) using a standard neural network layer: p = f(W(1)[c1;c2]+b(1)), (14) where [c1;c2] is the concatenation of the two children, f is an element-wise activation function such as tanh, b is a bias term, and W E Rnx2n is an encoding matrix that we want to learn during training. One way of assessing how well p represents its direct children is to decode their vectors in a reconstruction layer: [c, 1;c,2] = f(W(2)p+b(2)) (15) During training, the goal is to minimize the reconstruction error</context>
<context position="33607" citStr="Collobert and Weston, 2008" startWordPosition="5384" endWordPosition="5388">ard to F1, we are comparable with Das and Smith (2009) and Socher et al. (2011a) without using elaborate features, or any additional manipulations over and above the output of the composition functions 3Without dynamic pooling, their model yields an accuracy of 74.2. which if added could increase performance. 5 Discussion In this paper we systematically compared three types of distributional representation and their effect on semantic composition. Our comparisons involved a simple distributional semantic space (Mitchell and Lapata, 2010), word embeddings computed with a neural language model (Collobert and Weston, 2008) and a representation based on weighted word-link-word tuples arranged into a third-order tensor (Baroni and Lenci, 2010). These representations vary in many respects: the amount of preprocessing and linguistic information involved (the third-order tensor computes semantic representations over parsed corpora), whether the semantic space is the by-product of a learning process (in the neural language model the parameters of the lookup table must be learned), and data requirements (the third-order tensor involves processing billions of words). These representations served as input to three compo</context>
</contexts>
<marker>Collobert, Weston, 2008</marker>
<rawString>Ronan Collobert and Jason Weston. 2008. A unified architecture for natural language processing: deep neural networks with multitask learning. In Proceedings of the 25th International Conference on Machine Learning, pages 160–167, New York, NY. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dipanjan Das</author>
<author>Noah A Smith</author>
</authors>
<title>Paraphrase identification as probabilistic quasi-synchronous recognition.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>468--476</pages>
<location>Suntec, Singapore.</location>
<contexts>
<context position="31596" citStr="Das and Smith (2009)" startWordPosition="5062" endWordPosition="5065">onal models rather than achieving state-of-the-art results, Table 6 compares our approach against previous work on the same task and dataset. Initial research concentrated on individual words rather than sentential representations. Several approaches used nMSRPC uniOverlapi1,i2 = E (18) k=1 553 Model Acc. F1 Baseline 66.5 79.9 Mihalcea et al. (2006) 70.3 81.3 Rus et al. (2008) 70.6 80.5 Qiu et al. (2006) 72.0 81.6 Islam and Inkpen (2007) 72.6 81.3 Mitchell and Lapata (2010) ((D) 73.0 82.3 Baroni and Lenci (2010) (+) 73.5 82.2 Fernando and Stevenson (2008) 74.1 82.4 Wan et al. (2006) 75.6 83.0 Das and Smith (2009) 76.1 82.7 Socher et al. (2011a) 76.8 83.6 Table 6: Overview of results on the MSRCP (test corpus). Accuracy differences of 3.3 or more are significant at the 0.01 level (using the x2 statistic). WordNet in conjunction with distributional similarity in an attempt to detect meaning conveyed by synonymous words (Islam and Inkpen, 2007; Mihalcea et al., 2006; Fernando and Stevenson, 2008). More recently, the addition of syntactic features based on dependency parse trees (Wan et al., 2006; Das and Smith, 2009) has been shown to substantially boost performance. The model of Das and Smith (2009), fo</context>
<context position="33034" citStr="Das and Smith (2009)" startWordPosition="5302" endWordPosition="5305"> published results. This model is more sophisticated than the one we used in our experiments (see Table 4 and 5). Rather than using the output of the RAE as features for the classifier, it applies dynamic pooling, a procedure that takes a similarity matrix as input (e.g., created by sentences with differing lengths) and maps it to a matrix of fixed size that represents more faithfully the global similarity structure.3 Overall, we observe that our own models do as well as some of the models that employ WordNet and more sophisticated syntactic features. With regard to F1, we are comparable with Das and Smith (2009) and Socher et al. (2011a) without using elaborate features, or any additional manipulations over and above the output of the composition functions 3Without dynamic pooling, their model yields an accuracy of 74.2. which if added could increase performance. 5 Discussion In this paper we systematically compared three types of distributional representation and their effect on semantic composition. Our comparisons involved a simple distributional semantic space (Mitchell and Lapata, 2010), word embeddings computed with a neural language model (Collobert and Weston, 2008) and a representation based</context>
</contexts>
<marker>Das, Smith, 2009</marker>
<rawString>Dipanjan Das and Noah A. Smith. 2009. Paraphrase identification as probabilistic quasi-synchronous recognition. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 468–476, Suntec, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Denhire</author>
<author>B Lemaire</author>
</authors>
<title>A computational model of children’s semantic memory.</title>
<date>2004</date>
<booktitle>In Proceedings of the 26th Annual Meeting of the Cognitive Science Society,</booktitle>
<pages>297--302</pages>
<location>Mahwah, NJ. Lawrence Erlbaum Associates.</location>
<contexts>
<context position="1359" citStr="Denhire and Lemaire, 2004" startWordPosition="187" endWordPosition="190">omputationally intensive alternatives with regards to two particular tests: (1) phrase similarity and (2) paraphrase detection. The sizes of the involved training corpora and the generated vectors are not as important as the fit between the meaning representation and compositional method. 1 Introduction Distributional models of semantics have seen considerable success at simulating a wide range of behavioral data in tasks involving semantic cognition and also in practical applications. For example, they have been used to model judgments of semantic similarity (McDonald, 2000) and association (Denhire and Lemaire, 2004; Griffiths et al., 2007) and have been shown to achieve human level performance on synonymy tests (Landauer and Dumais, 1997; Griffiths et al., 2007) such as those included in the Test of English as a Foreign Language (TOEFL). This ability has been put to practical use in numerous natural language processing tasks such as automatic thesaurus extraction (Grefenstette, 1994), word sense discrimination (Sch¨utze, 1998), language modeling (Bellegarda, 2000), and the identification of analogical relations (Turney, 2006). While much research has been directed at the most effective ways of construct</context>
</contexts>
<marker>Denhire, Lemaire, 2004</marker>
<rawString>G. Denhire and B. Lemaire. 2004. A computational model of children’s semantic memory. In Proceedings of the 26th Annual Meeting of the Cognitive Science Society, pages 297–302, Mahwah, NJ. Lawrence Erlbaum Associates.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bill Dolan</author>
<author>Chris Quirk</author>
<author>Chris Brockett</author>
</authors>
<title>Unsupervised construction of large paraphrase corpora: Exploiting massively parallel news sources.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th International Conference on Computational Linguistics,</booktitle>
<pages>350--356</pages>
<location>Geneva, Switzerland. COLING.</location>
<contexts>
<context position="26490" citStr="Dolan et al. (2004)" startWordPosition="4222" endWordPosition="4225">the 0.01 level, using a ttest (Cohen and Cohen, 1983). 4 Experiment 2: Paraphrase Detection Although the phrase similarity task gives a fairly direct insight into semantic similarity and compositional representations, it is somewhat limited in scope as it only considers two-word constructions rather than naturally occurring sentences. Ideally, we would like to augment our evaluation with a task which is based on large quantities of natural data and for which vector composition has practical consequences. For these reasons, we used the Microsoft Research Paraphrase Corpus (MSRPC) introduced by Dolan et al. (2004). The corpus consists of sentence pairs Seni1,Seni2 and labels indicating whether they are in a paraphrase relationship or not. The vector representations obtained from our various compositional models were used as features for the paraphrase classification task. The MSRPC dataset contains 5,801 sentence pairs, we used the standard split of 4,076 training pairs (67.5% of which are paraphrases) and 1,725 test pairs (66.5% of which are paraphrases). In order to judge whether two sentences have the same meaning we employ Fan et al. (2008)’s liblinear classifier. For each of our three vector sourc</context>
</contexts>
<marker>Dolan, Quirk, Brockett, 2004</marker>
<rawString>Bill Dolan, Chris Quirk, and Chris Brockett. 2004. Unsupervised construction of large paraphrase corpora: Exploiting massively parallel news sources. In Proceedings of the 20th International Conference on Computational Linguistics, pages 350–356, Geneva, Switzerland. COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rong-En Fan</author>
<author>Kai-Wei Chang</author>
<author>Cho-Jui Hsieh</author>
<author>XiangRui Wang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>Liblinear: A library for large linear classification.</title>
<date>2008</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>9--1871</pages>
<contexts>
<context position="27031" citStr="Fan et al. (2008)" startWordPosition="4311" endWordPosition="4314">crosoft Research Paraphrase Corpus (MSRPC) introduced by Dolan et al. (2004). The corpus consists of sentence pairs Seni1,Seni2 and labels indicating whether they are in a paraphrase relationship or not. The vector representations obtained from our various compositional models were used as features for the paraphrase classification task. The MSRPC dataset contains 5,801 sentence pairs, we used the standard split of 4,076 training pairs (67.5% of which are paraphrases) and 1,725 test pairs (66.5% of which are paraphrases). In order to judge whether two sentences have the same meaning we employ Fan et al. (2008)’s liblinear classifier. For each of our three vector sources and three different compositional methods, we create the following features: (a) a vector representing the pair of input sentences either via concatenation (“con”) or subtraction (“sub”); (b) a vector encoding which words appear therein (“enc”); and (c) a vector made up of the following four other pieces of information: the cosine similarity of the sentence vectors, the length of Seni1, the length of Seni2, and the unigram overlap among the two sentences. In order to encode which words appear in 552 NLM DM SDS (BNC) (3-BWC) (BNC) + </context>
</contexts>
<marker>Fan, Chang, Hsieh, Wang, Lin, 2008</marker>
<rawString>Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, XiangRui Wang, and Chih-Jen Lin. 2008. Liblinear: A library for large linear classification. Journal of Machine Learning Research, 9:1871–1874.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Samuel Fernando</author>
<author>Mark Stevenson</author>
</authors>
<title>A semantic similarity approach to paraphrase detection.</title>
<date>2008</date>
<publisher>Technology.</publisher>
<contexts>
<context position="31537" citStr="Fernando and Stevenson (2008)" startWordPosition="5050" endWordPosition="5053">the paraphrase detection task as a test-bed for evaluating compositional models rather than achieving state-of-the-art results, Table 6 compares our approach against previous work on the same task and dataset. Initial research concentrated on individual words rather than sentential representations. Several approaches used nMSRPC uniOverlapi1,i2 = E (18) k=1 553 Model Acc. F1 Baseline 66.5 79.9 Mihalcea et al. (2006) 70.3 81.3 Rus et al. (2008) 70.6 80.5 Qiu et al. (2006) 72.0 81.6 Islam and Inkpen (2007) 72.6 81.3 Mitchell and Lapata (2010) ((D) 73.0 82.3 Baroni and Lenci (2010) (+) 73.5 82.2 Fernando and Stevenson (2008) 74.1 82.4 Wan et al. (2006) 75.6 83.0 Das and Smith (2009) 76.1 82.7 Socher et al. (2011a) 76.8 83.6 Table 6: Overview of results on the MSRCP (test corpus). Accuracy differences of 3.3 or more are significant at the 0.01 level (using the x2 statistic). WordNet in conjunction with distributional similarity in an attempt to detect meaning conveyed by synonymous words (Islam and Inkpen, 2007; Mihalcea et al., 2006; Fernando and Stevenson, 2008). More recently, the addition of syntactic features based on dependency parse trees (Wan et al., 2006; Das and Smith, 2009) has been shown to substantial</context>
</contexts>
<marker>Fernando, Stevenson, 2008</marker>
<rawString>Samuel Fernando and Mark Stevenson. 2008. A semantic similarity approach to paraphrase detection. Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lev Finkelstein</author>
<author>Evgeniy Gabrilovich</author>
<author>Yossi Matias</author>
<author>Ehud Rivlin</author>
<author>Zach Solan</author>
<author>Gadi Wolfman</author>
<author>Eytan Ruppin</author>
</authors>
<title>Placing search in context: the concept revisited. In</title>
<date>2001</date>
<booktitle>WWW,</booktitle>
<pages>406--414</pages>
<contexts>
<context position="12933" citStr="Finkelstein et al., 2001" startWordPosition="2017" endWordPosition="2020">d the embedding lookup table. Word vectors are stored in a word embedding matrix which captures syntactic and semantic information from co-occurrence statistics. As these representations are learned, albeit in an unsupervised manner, one would hope that they capture word meanings more succinctly, compared to the simpler distributional representations that are merely based on co-occurrence. We trained the neural language model on the BNC. We optimized the model’s parameters on a word similarity task using 4% of the BNC as development data. Specifically, we used WordSim353, a benchmark dataset (Finkelstein et al., 2001), consisting of relatedness judgments (on a scale of 0 to 10) for 353 word pairs. We experimented with vectors of varying dimensionality (ranging from 50 to 200, with a step size of 50). The size of the target word’s context window was 2, 3 and 4 in turn. The rate at which embeddings were learned ranged from 3.4 x 10−10 to 6.7 x 10−10 to 10−9. We ran each training process for 1.1 x 108 to 2.7 x 108 iterations (ca. 2 days). We obtained the best results with 50 dimensions, a context window of size 4, and a embedding learning rate of 10−9. The NLM with these parameters was then trained for 1.51x1</context>
</contexts>
<marker>Finkelstein, Gabrilovich, Matias, Rivlin, Solan, Wolfman, Ruppin, 2001</marker>
<rawString>Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan Ruppin. 2001. Placing search in context: the concept revisited. In WWW, pages 406–414.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Foltz</author>
<author>Walter Kintsch</author>
<author>Thomas Landauer</author>
</authors>
<title>The measurement of textual coherence with latent semantic analysis.</title>
<date>1998</date>
<booktitle>Discourse Process,</booktitle>
<pages>15--285</pages>
<contexts>
<context position="3125" citStr="Foltz et al., 1998" startWordPosition="466" endWordPosition="469">larity of distributional methods and their application potential to tasks that require an understanding of larger phrases or complete sentences. For example, Mitchell and Lapata (2010) introduce a general framework for studying vector composition, which they formulate as a function f of two vectors u and v. Different composition models arise, depending on how f is chosen. Assuming that composition is a linear function of the Cartesian product of u and v allows to specify additive models which are by far the most common method of vector combination in the literature (Landauer and Dumais, 1997; Foltz et al., 1998; Kintsch, 2001). Alternatively, assuming that composition is a linear function of the tensor product of u and v, gives rise to models based on multiplication. One of the most sophisticated proposals for semantic composition is that of Clark et al. (2008) and the more recent implementation of Grefenstette and 546 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 546–556, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics Sadrzadeh (2011a). Using techniques from logic</context>
</contexts>
<marker>Foltz, Kintsch, Landauer, 1998</marker>
<rawString>Peter Foltz, Walter Kintsch, and Thomas Landauer. 1998. The measurement of textual coherence with latent semantic analysis. Discourse Process, 15:285– 307.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Grefenstette</author>
<author>Mehrnoosh Sadrzadeh</author>
</authors>
<title>Experimental support for a categorical compositional distributional model of meaning.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1394--1404</pages>
<location>Edinburgh, Scotland.</location>
<contexts>
<context position="9227" citStr="Grefenstette and Sadrzadeh, 2011" startWordPosition="1382" endWordPosition="1385">. Nonetheless, they have in common that they originate from distributive corpus statistics. Co-occurence-based Semantic Space Word meaning is commonly represented in a highdimensional space, where each component corresponds to some contextual element in which the word is found. The contextual elements can be words themselves, or larger linguistic units such as sentences or documents, or even more complex linguistic representations such as the argument slots of predicates. A semantic space that is often employed in studying compositionality across a variety of tasks (Mitchell and Lapata, 2010; Grefenstette and Sadrzadeh, 2011a) uses a context window of five words on either side of the target word, and 2,000 vector dimensions. These are the common context words in the British National Corpus (BNC), a corpus of about 100 million tokens. Their values are set to the ratio of the probability of the context word given the target word to the probability of the context word overall. More formally, let us consider the BNC as a set of sentences: BNC = {Sen(BNC) 1 ,...,Sen(BNC) nBNC } (1) where the i-th sentence is a sequence of words Seni = (w(i) 1 ,...,w(i) ni ) from the BNC’s vocabulary VocBNC. Then f reqw is the amount o</context>
<context position="19756" citStr="Grefenstette and Sadrzadeh, 2011" startWordPosition="3146" endWordPosition="3149">ord vectors, that is, vectors obtained via the respective corpus for words that are not on our stoplist: senVec(+) i [j] = � k=1,...,ni wdVecwkexists wdVecwk[j] (12) 550 And do the same with point-wise multiplication: senVec(o) i [j] = � k=1,...,ni wdVecwkexists The multiplication model in (13) can be seen as an instantiation of the categorical compositional framework put forward by Clark et al. (2008). In fact, a variety of multiplication-based models can be derived from this framework; and comparisons against component-wise multiplication on phrase similarity tasks yield comparable results (Grefenstette and Sadrzadeh, 2011a; Grefenstette and Sadrzadeh, 2011b). We thus opt for the model (13) as an example of compositional models based on multiplication due to its good performance across a variety of tasks, including language modeling and prediction of reading difficulty (Mitchell, 2011). Our third method, for creating phrase and sentence vectors alike, is the application of Socher et al. (2011a)’s model. They use the Stanford parser (Klein and Manning, 2003) to create a binary parse tree for each input phrase or sentence. This tree is then used as the basis for a deep recursive autoencoder (RAE). The aim is to c</context>
</contexts>
<marker>Grefenstette, Sadrzadeh, 2011</marker>
<rawString>Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011a. Experimental support for a categorical compositional distributional model of meaning. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1394–1404, Edinburgh, Scotland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Grefenstette</author>
<author>Mehrnoosh Sadrzadeh</author>
</authors>
<title>Experimenting with transitive verbs in a DisCoCat.</title>
<date>2011</date>
<booktitle>In Proceedings of the GEMS 2011 Workshop on GEometrical Models of Natural Language Semantics,</booktitle>
<pages>62--66</pages>
<location>Edinburgh, UK.</location>
<contexts>
<context position="9227" citStr="Grefenstette and Sadrzadeh, 2011" startWordPosition="1382" endWordPosition="1385">. Nonetheless, they have in common that they originate from distributive corpus statistics. Co-occurence-based Semantic Space Word meaning is commonly represented in a highdimensional space, where each component corresponds to some contextual element in which the word is found. The contextual elements can be words themselves, or larger linguistic units such as sentences or documents, or even more complex linguistic representations such as the argument slots of predicates. A semantic space that is often employed in studying compositionality across a variety of tasks (Mitchell and Lapata, 2010; Grefenstette and Sadrzadeh, 2011a) uses a context window of five words on either side of the target word, and 2,000 vector dimensions. These are the common context words in the British National Corpus (BNC), a corpus of about 100 million tokens. Their values are set to the ratio of the probability of the context word given the target word to the probability of the context word overall. More formally, let us consider the BNC as a set of sentences: BNC = {Sen(BNC) 1 ,...,Sen(BNC) nBNC } (1) where the i-th sentence is a sequence of words Seni = (w(i) 1 ,...,w(i) ni ) from the BNC’s vocabulary VocBNC. Then f reqw is the amount o</context>
<context position="19756" citStr="Grefenstette and Sadrzadeh, 2011" startWordPosition="3146" endWordPosition="3149">ord vectors, that is, vectors obtained via the respective corpus for words that are not on our stoplist: senVec(+) i [j] = � k=1,...,ni wdVecwkexists wdVecwk[j] (12) 550 And do the same with point-wise multiplication: senVec(o) i [j] = � k=1,...,ni wdVecwkexists The multiplication model in (13) can be seen as an instantiation of the categorical compositional framework put forward by Clark et al. (2008). In fact, a variety of multiplication-based models can be derived from this framework; and comparisons against component-wise multiplication on phrase similarity tasks yield comparable results (Grefenstette and Sadrzadeh, 2011a; Grefenstette and Sadrzadeh, 2011b). We thus opt for the model (13) as an example of compositional models based on multiplication due to its good performance across a variety of tasks, including language modeling and prediction of reading difficulty (Mitchell, 2011). Our third method, for creating phrase and sentence vectors alike, is the application of Socher et al. (2011a)’s model. They use the Stanford parser (Klein and Manning, 2003) to create a binary parse tree for each input phrase or sentence. This tree is then used as the basis for a deep recursive autoencoder (RAE). The aim is to c</context>
</contexts>
<marker>Grefenstette, Sadrzadeh, 2011</marker>
<rawString>Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011b. Experimenting with transitive verbs in a DisCoCat. In Proceedings of the GEMS 2011 Workshop on GEometrical Models of Natural Language Semantics, pages 62–66, Edinburgh, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregory Grefenstette</author>
</authors>
<title>Explorations in Automatic Thesaurus Discovery.</title>
<date>1994</date>
<publisher>Kluwer Academic Publishers,</publisher>
<location>Norwell, MA.</location>
<contexts>
<context position="1735" citStr="Grefenstette, 1994" startWordPosition="250" endWordPosition="251">ng a wide range of behavioral data in tasks involving semantic cognition and also in practical applications. For example, they have been used to model judgments of semantic similarity (McDonald, 2000) and association (Denhire and Lemaire, 2004; Griffiths et al., 2007) and have been shown to achieve human level performance on synonymy tests (Landauer and Dumais, 1997; Griffiths et al., 2007) such as those included in the Test of English as a Foreign Language (TOEFL). This ability has been put to practical use in numerous natural language processing tasks such as automatic thesaurus extraction (Grefenstette, 1994), word sense discrimination (Sch¨utze, 1998), language modeling (Bellegarda, 2000), and the identification of analogical relations (Turney, 2006). While much research has been directed at the most effective ways of constructing representations for individual words, there has been far less consensus regarding the representation of larger constructions such as phrases and sentences. The problem has received some attention in the connectionist literature, particularly in response to criticisms of the ability of connectionist representations to handle complex structures (Smolensky, 1990; Plate, 19</context>
</contexts>
<marker>Grefenstette, 1994</marker>
<rawString>Gregory Grefenstette. 1994. Explorations in Automatic Thesaurus Discovery. Kluwer Academic Publishers, Norwell, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas L Griffiths</author>
<author>Mark Steyvers</author>
<author>Joshua B Tenenbaum</author>
</authors>
<title>Topics in semantic representation.</title>
<date>2007</date>
<journal>Psychological Review,</journal>
<volume>114</volume>
<issue>2</issue>
<contexts>
<context position="1384" citStr="Griffiths et al., 2007" startWordPosition="191" endWordPosition="194">ternatives with regards to two particular tests: (1) phrase similarity and (2) paraphrase detection. The sizes of the involved training corpora and the generated vectors are not as important as the fit between the meaning representation and compositional method. 1 Introduction Distributional models of semantics have seen considerable success at simulating a wide range of behavioral data in tasks involving semantic cognition and also in practical applications. For example, they have been used to model judgments of semantic similarity (McDonald, 2000) and association (Denhire and Lemaire, 2004; Griffiths et al., 2007) and have been shown to achieve human level performance on synonymy tests (Landauer and Dumais, 1997; Griffiths et al., 2007) such as those included in the Test of English as a Foreign Language (TOEFL). This ability has been put to practical use in numerous natural language processing tasks such as automatic thesaurus extraction (Grefenstette, 1994), word sense discrimination (Sch¨utze, 1998), language modeling (Bellegarda, 2000), and the identification of analogical relations (Turney, 2006). While much research has been directed at the most effective ways of constructing representations for i</context>
</contexts>
<marker>Griffiths, Steyvers, Tenenbaum, 2007</marker>
<rawString>Thomas L. Griffiths, Mark Steyvers, and Joshua B. Tenenbaum. 2007. Topics in semantic representation. Psychological Review, 114(2):211–244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aminul Islam</author>
<author>Diana Inkpen</author>
</authors>
<title>Semantic similarity of short texts.</title>
<date>2007</date>
<booktitle>In Proceedings of the International Conference on RecentAdvances in Natural Language Processing,</booktitle>
<location>Borovets, Bulgaria.</location>
<contexts>
<context position="31417" citStr="Islam and Inkpen (2007)" startWordPosition="5030" endWordPosition="5033">tors, the length of the two sentences involved, and their unigram word overlap. Although our intention was to use the paraphrase detection task as a test-bed for evaluating compositional models rather than achieving state-of-the-art results, Table 6 compares our approach against previous work on the same task and dataset. Initial research concentrated on individual words rather than sentential representations. Several approaches used nMSRPC uniOverlapi1,i2 = E (18) k=1 553 Model Acc. F1 Baseline 66.5 79.9 Mihalcea et al. (2006) 70.3 81.3 Rus et al. (2008) 70.6 80.5 Qiu et al. (2006) 72.0 81.6 Islam and Inkpen (2007) 72.6 81.3 Mitchell and Lapata (2010) ((D) 73.0 82.3 Baroni and Lenci (2010) (+) 73.5 82.2 Fernando and Stevenson (2008) 74.1 82.4 Wan et al. (2006) 75.6 83.0 Das and Smith (2009) 76.1 82.7 Socher et al. (2011a) 76.8 83.6 Table 6: Overview of results on the MSRCP (test corpus). Accuracy differences of 3.3 or more are significant at the 0.01 level (using the x2 statistic). WordNet in conjunction with distributional similarity in an attempt to detect meaning conveyed by synonymous words (Islam and Inkpen, 2007; Mihalcea et al., 2006; Fernando and Stevenson, 2008). More recently, the addition of </context>
</contexts>
<marker>Islam, Inkpen, 2007</marker>
<rawString>Aminul Islam and Diana Inkpen. 2007. Semantic similarity of short texts. In Proceedings of the International Conference on RecentAdvances in Natural Language Processing, Borovets, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Walter Kintsch</author>
</authors>
<date>2001</date>
<journal>Predication. Cognitive Science,</journal>
<volume>25</volume>
<issue>2</issue>
<contexts>
<context position="3141" citStr="Kintsch, 2001" startWordPosition="470" endWordPosition="471">onal methods and their application potential to tasks that require an understanding of larger phrases or complete sentences. For example, Mitchell and Lapata (2010) introduce a general framework for studying vector composition, which they formulate as a function f of two vectors u and v. Different composition models arise, depending on how f is chosen. Assuming that composition is a linear function of the Cartesian product of u and v allows to specify additive models which are by far the most common method of vector combination in the literature (Landauer and Dumais, 1997; Foltz et al., 1998; Kintsch, 2001). Alternatively, assuming that composition is a linear function of the tensor product of u and v, gives rise to models based on multiplication. One of the most sophisticated proposals for semantic composition is that of Clark et al. (2008) and the more recent implementation of Grefenstette and 546 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 546–556, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics Sadrzadeh (2011a). Using techniques from logic, category theor</context>
</contexts>
<marker>Kintsch, 2001</marker>
<rawString>Walter Kintsch. 2001. Predication. Cognitive Science, 25(2):173–202.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>423--430</pages>
<location>Sapporo, Japan.</location>
<contexts>
<context position="20199" citStr="Klein and Manning, 2003" startWordPosition="3217" endWordPosition="3220">odels can be derived from this framework; and comparisons against component-wise multiplication on phrase similarity tasks yield comparable results (Grefenstette and Sadrzadeh, 2011a; Grefenstette and Sadrzadeh, 2011b). We thus opt for the model (13) as an example of compositional models based on multiplication due to its good performance across a variety of tasks, including language modeling and prediction of reading difficulty (Mitchell, 2011). Our third method, for creating phrase and sentence vectors alike, is the application of Socher et al. (2011a)’s model. They use the Stanford parser (Klein and Manning, 2003) to create a binary parse tree for each input phrase or sentence. This tree is then used as the basis for a deep recursive autoencoder (RAE). The aim is to construct a vector representation for the tree’s root bottom-up where the leaves contain word vectors. The latter can in theory be provided by any type of semantic space, however Socher et al. use word embeddings provided by the neural language model (Collobert and Weston, 2008). Given the binary tree input structure, the model computes parent representations p from their children (c1,c2) using a standard neural network layer: p = f(W(1)[c1</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Accurate unlexicalized parsing. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 423–430, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T K Landauer</author>
<author>S T Dumais</author>
</authors>
<title>A solution to Plato’s problem: the latent semantic analysis theory of acquisition, induction and representation of knowledge.</title>
<date>1997</date>
<journal>Psychological Review,</journal>
<volume>104</volume>
<issue>2</issue>
<contexts>
<context position="1484" citStr="Landauer and Dumais, 1997" startWordPosition="207" endWordPosition="210">n. The sizes of the involved training corpora and the generated vectors are not as important as the fit between the meaning representation and compositional method. 1 Introduction Distributional models of semantics have seen considerable success at simulating a wide range of behavioral data in tasks involving semantic cognition and also in practical applications. For example, they have been used to model judgments of semantic similarity (McDonald, 2000) and association (Denhire and Lemaire, 2004; Griffiths et al., 2007) and have been shown to achieve human level performance on synonymy tests (Landauer and Dumais, 1997; Griffiths et al., 2007) such as those included in the Test of English as a Foreign Language (TOEFL). This ability has been put to practical use in numerous natural language processing tasks such as automatic thesaurus extraction (Grefenstette, 1994), word sense discrimination (Sch¨utze, 1998), language modeling (Bellegarda, 2000), and the identification of analogical relations (Turney, 2006). While much research has been directed at the most effective ways of constructing representations for individual words, there has been far less consensus regarding the representation of larger constructi</context>
<context position="3105" citStr="Landauer and Dumais, 1997" startWordPosition="462" endWordPosition="465">t is partly due to the popularity of distributional methods and their application potential to tasks that require an understanding of larger phrases or complete sentences. For example, Mitchell and Lapata (2010) introduce a general framework for studying vector composition, which they formulate as a function f of two vectors u and v. Different composition models arise, depending on how f is chosen. Assuming that composition is a linear function of the Cartesian product of u and v allows to specify additive models which are by far the most common method of vector combination in the literature (Landauer and Dumais, 1997; Foltz et al., 1998; Kintsch, 2001). Alternatively, assuming that composition is a linear function of the tensor product of u and v, gives rise to models based on multiplication. One of the most sophisticated proposals for semantic composition is that of Clark et al. (2008) and the more recent implementation of Grefenstette and 546 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 546–556, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics Sadrzadeh (2011a). Using t</context>
</contexts>
<marker>Landauer, Dumais, 1997</marker>
<rawString>T. K. Landauer and S. T. Dumais. 1997. A solution to Plato’s problem: the latent semantic analysis theory of acquisition, induction and representation of knowledge. Psychological Review, 104(2):211–240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott McDonald</author>
</authors>
<title>Environmental Determinants of Lexical Processing Effort.</title>
<date>2000</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Edinburgh.</institution>
<contexts>
<context position="1316" citStr="McDonald, 2000" startWordPosition="183" endWordPosition="184"> approaches are as good as more computationally intensive alternatives with regards to two particular tests: (1) phrase similarity and (2) paraphrase detection. The sizes of the involved training corpora and the generated vectors are not as important as the fit between the meaning representation and compositional method. 1 Introduction Distributional models of semantics have seen considerable success at simulating a wide range of behavioral data in tasks involving semantic cognition and also in practical applications. For example, they have been used to model judgments of semantic similarity (McDonald, 2000) and association (Denhire and Lemaire, 2004; Griffiths et al., 2007) and have been shown to achieve human level performance on synonymy tests (Landauer and Dumais, 1997; Griffiths et al., 2007) such as those included in the Test of English as a Foreign Language (TOEFL). This ability has been put to practical use in numerous natural language processing tasks such as automatic thesaurus extraction (Grefenstette, 1994), word sense discrimination (Sch¨utze, 1998), language modeling (Bellegarda, 2000), and the identification of analogical relations (Turney, 2006). While much research has been direc</context>
</contexts>
<marker>McDonald, 2000</marker>
<rawString>Scott McDonald. 2000. Environmental Determinants of Lexical Processing Effort. Ph.D. thesis, University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Courtney Corley</author>
<author>Carlo Strapparava</author>
</authors>
<title>Corpus-based and knowledge-based measures of text semantic similarity,</title>
<date>2006</date>
<pages>775--780</pages>
<publisher>AAAI Press.</publisher>
<contexts>
<context position="31327" citStr="Mihalcea et al. (2006)" startWordPosition="5012" endWordPosition="5015">basic feature space consisting only of the cosine similarity of the composed sentence vectors, the length of the two sentences involved, and their unigram word overlap. Although our intention was to use the paraphrase detection task as a test-bed for evaluating compositional models rather than achieving state-of-the-art results, Table 6 compares our approach against previous work on the same task and dataset. Initial research concentrated on individual words rather than sentential representations. Several approaches used nMSRPC uniOverlapi1,i2 = E (18) k=1 553 Model Acc. F1 Baseline 66.5 79.9 Mihalcea et al. (2006) 70.3 81.3 Rus et al. (2008) 70.6 80.5 Qiu et al. (2006) 72.0 81.6 Islam and Inkpen (2007) 72.6 81.3 Mitchell and Lapata (2010) ((D) 73.0 82.3 Baroni and Lenci (2010) (+) 73.5 82.2 Fernando and Stevenson (2008) 74.1 82.4 Wan et al. (2006) 75.6 83.0 Das and Smith (2009) 76.1 82.7 Socher et al. (2011a) 76.8 83.6 Table 6: Overview of results on the MSRCP (test corpus). Accuracy differences of 3.3 or more are significant at the 0.01 level (using the x2 statistic). WordNet in conjunction with distributional similarity in an attempt to detect meaning conveyed by synonymous words (Islam and Inkpen, 2</context>
</contexts>
<marker>Mihalcea, Corley, Strapparava, 2006</marker>
<rawString>Rada Mihalcea, Courtney Corley, and Carlo Strapparava, 2006. Corpus-based and knowledge-based measures of text semantic similarity, pages 775–780. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Mitchell</author>
<author>Mirella Lapata</author>
</authors>
<title>Composition in distributional models of semantics.</title>
<date>2010</date>
<journal>Cognitive Science,</journal>
<volume>38</volume>
<issue>8</issue>
<contexts>
<context position="2691" citStr="Mitchell and Lapata (2010)" startWordPosition="389" endWordPosition="392">f larger constructions such as phrases and sentences. The problem has received some attention in the connectionist literature, particularly in response to criticisms of the ability of connectionist representations to handle complex structures (Smolensky, 1990; Plate, 1995). More recently, several proposals have been put forward for computing the meaning of word combinations in vector spaces. This renewed interest is partly due to the popularity of distributional methods and their application potential to tasks that require an understanding of larger phrases or complete sentences. For example, Mitchell and Lapata (2010) introduce a general framework for studying vector composition, which they formulate as a function f of two vectors u and v. Different composition models arise, depending on how f is chosen. Assuming that composition is a linear function of the Cartesian product of u and v allows to specify additive models which are by far the most common method of vector combination in the literature (Landauer and Dumais, 1997; Foltz et al., 1998; Kintsch, 2001). Alternatively, assuming that composition is a linear function of the tensor product of u and v, gives rise to models based on multiplication. One of</context>
<context position="6078" citStr="Mitchell and Lapata, 2010" startWordPosition="911" endWordPosition="914">an also compute compositional representations when the tree structure is not given, e.g., by greedily inferring a binary tree. Although the type of function used for vector composition has attracted much attention, relatively less emphasis has been placed on the basic distributional representations on which the composition functions operate. In this paper, we examine three types of distributional representation of increasing sophistication and their effect on semantic composition. These include a simple semantic space, where a word’s vector represents its co-occurrence with neighboring words (Mitchell and Lapata, 2010), a syntax-aware space based on weighted distributional tuples that encode typed co-occurrence relations among words (Baroni and Lenci, 2010), and word embeddings computed with a neural language model (Bengio, 2001; Collobert and Weston, 2008). Word embeddings are distributed representations, low-dimensional and real-valued. Each dimension of the embedding represents a latent feature of the word, hopefully capturing useful syntactic and semantic properties. Using these representations, we construct several compositional models, based on addition, multiplication, and recursive neural networks. </context>
<context position="9193" citStr="Mitchell and Lapata, 2010" startWordPosition="1378" endWordPosition="1381">erpreted in the same manner. Nonetheless, they have in common that they originate from distributive corpus statistics. Co-occurence-based Semantic Space Word meaning is commonly represented in a highdimensional space, where each component corresponds to some contextual element in which the word is found. The contextual elements can be words themselves, or larger linguistic units such as sentences or documents, or even more complex linguistic representations such as the argument slots of predicates. A semantic space that is often employed in studying compositionality across a variety of tasks (Mitchell and Lapata, 2010; Grefenstette and Sadrzadeh, 2011a) uses a context window of five words on either side of the target word, and 2,000 vector dimensions. These are the common context words in the British National Corpus (BNC), a corpus of about 100 million tokens. Their values are set to the ratio of the probability of the context word given the target word to the probability of the context word overall. More formally, let us consider the BNC as a set of sentences: BNC = {Sen(BNC) 1 ,...,Sen(BNC) nBNC } (1) where the i-th sentence is a sequence of words Seni = (w(i) 1 ,...,w(i) ni ) from the BNC’s vocabulary V</context>
<context position="24174" citStr="Mitchell and Lapata (2010)" startWordPosition="3871" endWordPosition="3874"> 0.38 0.28 (BNC) 2000 � 0.48 0.50 0.35 100 RAE 0.31 0.30 0.28 DM vary + 0.37 0.30 0.29 (3-BWC) vary � 0.21 0.37 0.33 100 RAE 0.25 0.26 0.09 NLM 50 + 0.28 0.26 0.24 (BNC) 50 � 0.26 0.22 0.18 100 RAE 0.19 0.24 0.28 Table 3: Correlation coefficients of model predictions with subject similarity ratings (Spearman’s p); columns show dimensionality: fixed or varying (see Section 2.1), composition method: + is additive vector composition, O is component-wise multiplicative vector composition, RAE is Socher et al. (2011a)’s recursive auto-encoder. similar manner. Specifically, we used the dataset from Mitchell and Lapata (2010) which contains similarity judgments for adjective-noun, noun-noun and verb-object phrases, respectively.2 Each item is a phrase pair phr1, phr2 which has a human rating from 1 (very low similarity) to 7 (very high similarity). Using the composition models described above, we compute the cosine similarity of phr1 and phr2: phrSimphr1,phr2 = phrVecphr1 · phrVecphr2 |phrVecphr1 |× |phrVecphr2 |(17) Model similarities were evaluated against the human similarity ratings using Spearman’s p correlation coefficient. Table 3 summarizes the performance of the various models on the phrase similarity dat</context>
<context position="31454" citStr="Mitchell and Lapata (2010)" startWordPosition="5036" endWordPosition="5039">ces involved, and their unigram word overlap. Although our intention was to use the paraphrase detection task as a test-bed for evaluating compositional models rather than achieving state-of-the-art results, Table 6 compares our approach against previous work on the same task and dataset. Initial research concentrated on individual words rather than sentential representations. Several approaches used nMSRPC uniOverlapi1,i2 = E (18) k=1 553 Model Acc. F1 Baseline 66.5 79.9 Mihalcea et al. (2006) 70.3 81.3 Rus et al. (2008) 70.6 80.5 Qiu et al. (2006) 72.0 81.6 Islam and Inkpen (2007) 72.6 81.3 Mitchell and Lapata (2010) ((D) 73.0 82.3 Baroni and Lenci (2010) (+) 73.5 82.2 Fernando and Stevenson (2008) 74.1 82.4 Wan et al. (2006) 75.6 83.0 Das and Smith (2009) 76.1 82.7 Socher et al. (2011a) 76.8 83.6 Table 6: Overview of results on the MSRCP (test corpus). Accuracy differences of 3.3 or more are significant at the 0.01 level (using the x2 statistic). WordNet in conjunction with distributional similarity in an attempt to detect meaning conveyed by synonymous words (Islam and Inkpen, 2007; Mihalcea et al., 2006; Fernando and Stevenson, 2008). More recently, the addition of syntactic features based on dependenc</context>
<context position="33523" citStr="Mitchell and Lapata, 2010" startWordPosition="5372" endWordPosition="5375"> the models that employ WordNet and more sophisticated syntactic features. With regard to F1, we are comparable with Das and Smith (2009) and Socher et al. (2011a) without using elaborate features, or any additional manipulations over and above the output of the composition functions 3Without dynamic pooling, their model yields an accuracy of 74.2. which if added could increase performance. 5 Discussion In this paper we systematically compared three types of distributional representation and their effect on semantic composition. Our comparisons involved a simple distributional semantic space (Mitchell and Lapata, 2010), word embeddings computed with a neural language model (Collobert and Weston, 2008) and a representation based on weighted word-link-word tuples arranged into a third-order tensor (Baroni and Lenci, 2010). These representations vary in many respects: the amount of preprocessing and linguistic information involved (the third-order tensor computes semantic representations over parsed corpora), whether the semantic space is the by-product of a learning process (in the neural language model the parameters of the lookup table must be learned), and data requirements (the third-order tensor involves</context>
</contexts>
<marker>Mitchell, Lapata, 2010</marker>
<rawString>Jeff Mitchell and Mirella Lapata. 2010. Composition in distributional models of semantics. Cognitive Science, 38(8):1388–1429.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Mitchell</author>
</authors>
<title>Composition in distributional models of semantics.</title>
<date>2011</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Edinburgh.</institution>
<contexts>
<context position="20024" citStr="Mitchell, 2011" startWordPosition="3190" endWordPosition="3191">el in (13) can be seen as an instantiation of the categorical compositional framework put forward by Clark et al. (2008). In fact, a variety of multiplication-based models can be derived from this framework; and comparisons against component-wise multiplication on phrase similarity tasks yield comparable results (Grefenstette and Sadrzadeh, 2011a; Grefenstette and Sadrzadeh, 2011b). We thus opt for the model (13) as an example of compositional models based on multiplication due to its good performance across a variety of tasks, including language modeling and prediction of reading difficulty (Mitchell, 2011). Our third method, for creating phrase and sentence vectors alike, is the application of Socher et al. (2011a)’s model. They use the Stanford parser (Klein and Manning, 2003) to create a binary parse tree for each input phrase or sentence. This tree is then used as the basis for a deep recursive autoencoder (RAE). The aim is to construct a vector representation for the tree’s root bottom-up where the leaves contain word vectors. The latter can in theory be provided by any type of semantic space, however Socher et al. use word embeddings provided by the neural language model (Collobert and Wes</context>
</contexts>
<marker>Mitchell, 2011</marker>
<rawString>Jeff Mitchell. 2011. Composition in distributional models of semantics. Ph.D. thesis, University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tony A Plate</author>
</authors>
<title>Holographic reduced representations.</title>
<date>1995</date>
<journal>IEEE Transactions on Neural Networks,</journal>
<volume>6</volume>
<issue>3</issue>
<contexts>
<context position="2338" citStr="Plate, 1995" startWordPosition="336" endWordPosition="337">te, 1994), word sense discrimination (Sch¨utze, 1998), language modeling (Bellegarda, 2000), and the identification of analogical relations (Turney, 2006). While much research has been directed at the most effective ways of constructing representations for individual words, there has been far less consensus regarding the representation of larger constructions such as phrases and sentences. The problem has received some attention in the connectionist literature, particularly in response to criticisms of the ability of connectionist representations to handle complex structures (Smolensky, 1990; Plate, 1995). More recently, several proposals have been put forward for computing the meaning of word combinations in vector spaces. This renewed interest is partly due to the popularity of distributional methods and their application potential to tasks that require an understanding of larger phrases or complete sentences. For example, Mitchell and Lapata (2010) introduce a general framework for studying vector composition, which they formulate as a function f of two vectors u and v. Different composition models arise, depending on how f is chosen. Assuming that composition is a linear function of the Ca</context>
</contexts>
<marker>Plate, 1995</marker>
<rawString>Tony A. Plate. 1995. Holographic reduced representations. IEEE Transactions on Neural Networks, 6(3):623–641.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Long Qiu</author>
<author>Min-Yen Kan</author>
<author>Tat-Seng Chua</author>
</authors>
<title>Paraphrase recognition via dissimilarity significance classification.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>18--26</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="31383" citStr="Qiu et al. (2006)" startWordPosition="5024" endWordPosition="5027">of the composed sentence vectors, the length of the two sentences involved, and their unigram word overlap. Although our intention was to use the paraphrase detection task as a test-bed for evaluating compositional models rather than achieving state-of-the-art results, Table 6 compares our approach against previous work on the same task and dataset. Initial research concentrated on individual words rather than sentential representations. Several approaches used nMSRPC uniOverlapi1,i2 = E (18) k=1 553 Model Acc. F1 Baseline 66.5 79.9 Mihalcea et al. (2006) 70.3 81.3 Rus et al. (2008) 70.6 80.5 Qiu et al. (2006) 72.0 81.6 Islam and Inkpen (2007) 72.6 81.3 Mitchell and Lapata (2010) ((D) 73.0 82.3 Baroni and Lenci (2010) (+) 73.5 82.2 Fernando and Stevenson (2008) 74.1 82.4 Wan et al. (2006) 75.6 83.0 Das and Smith (2009) 76.1 82.7 Socher et al. (2011a) 76.8 83.6 Table 6: Overview of results on the MSRCP (test corpus). Accuracy differences of 3.3 or more are significant at the 0.01 level (using the x2 statistic). WordNet in conjunction with distributional similarity in an attempt to detect meaning conveyed by synonymous words (Islam and Inkpen, 2007; Mihalcea et al., 2006; Fernando and Stevenson, 2008</context>
</contexts>
<marker>Qiu, Kan, Chua, 2006</marker>
<rawString>Long Qiu, Min-Yen Kan, and Tat-Seng Chua. 2006. Paraphrase recognition via dissimilarity significance classification. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, pages 18–26, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasile Rus</author>
<author>Philip M McCarthy</author>
<author>Mihai C Lintean</author>
<author>Danielle S McNamara</author>
<author>Arthur C Graesser</author>
</authors>
<title>Paraphrase identification with lexico-syntactic graph subsumption.</title>
<date>2008</date>
<booktitle>Florida Artificial Intelligence Research Society Conference,</booktitle>
<pages>201--206</pages>
<editor>In David Wilson and H. Chad Lane, editors,</editor>
<publisher>AAAI Press.</publisher>
<contexts>
<context position="31355" citStr="Rus et al. (2008)" startWordPosition="5018" endWordPosition="5021">ly of the cosine similarity of the composed sentence vectors, the length of the two sentences involved, and their unigram word overlap. Although our intention was to use the paraphrase detection task as a test-bed for evaluating compositional models rather than achieving state-of-the-art results, Table 6 compares our approach against previous work on the same task and dataset. Initial research concentrated on individual words rather than sentential representations. Several approaches used nMSRPC uniOverlapi1,i2 = E (18) k=1 553 Model Acc. F1 Baseline 66.5 79.9 Mihalcea et al. (2006) 70.3 81.3 Rus et al. (2008) 70.6 80.5 Qiu et al. (2006) 72.0 81.6 Islam and Inkpen (2007) 72.6 81.3 Mitchell and Lapata (2010) ((D) 73.0 82.3 Baroni and Lenci (2010) (+) 73.5 82.2 Fernando and Stevenson (2008) 74.1 82.4 Wan et al. (2006) 75.6 83.0 Das and Smith (2009) 76.1 82.7 Socher et al. (2011a) 76.8 83.6 Table 6: Overview of results on the MSRCP (test corpus). Accuracy differences of 3.3 or more are significant at the 0.01 level (using the x2 statistic). WordNet in conjunction with distributional similarity in an attempt to detect meaning conveyed by synonymous words (Islam and Inkpen, 2007; Mihalcea et al., 2006; </context>
</contexts>
<marker>Rus, McCarthy, Lintean, McNamara, Graesser, 2008</marker>
<rawString>Vasile Rus, Philip M. McCarthy, Mihai C. Lintean, Danielle S. McNamara, and Arthur C. Graesser. 2008. Paraphrase identification with lexico-syntactic graph subsumption. In David Wilson and H. Chad Lane, editors, Florida Artificial Intelligence Research Society Conference, pages 201–206. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Automatic word sense discrimination.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>1</issue>
<marker>Sch¨utze, 1998</marker>
<rawString>Hinrich Sch¨utze. 1998. Automatic word sense discrimination. Computational Linguistics, 24(1):97–124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Smolensky</author>
</authors>
<title>Tensor product variable binding and the representation of symbolic structures in connectionist systems.</title>
<date>1990</date>
<journal>Artificial Intelligence,</journal>
<volume>46</volume>
<pages>216</pages>
<contexts>
<context position="2324" citStr="Smolensky, 1990" startWordPosition="334" endWordPosition="335">ction (Grefenstette, 1994), word sense discrimination (Sch¨utze, 1998), language modeling (Bellegarda, 2000), and the identification of analogical relations (Turney, 2006). While much research has been directed at the most effective ways of constructing representations for individual words, there has been far less consensus regarding the representation of larger constructions such as phrases and sentences. The problem has received some attention in the connectionist literature, particularly in response to criticisms of the ability of connectionist representations to handle complex structures (Smolensky, 1990; Plate, 1995). More recently, several proposals have been put forward for computing the meaning of word combinations in vector spaces. This renewed interest is partly due to the popularity of distributional methods and their application potential to tasks that require an understanding of larger phrases or complete sentences. For example, Mitchell and Lapata (2010) introduce a general framework for studying vector composition, which they formulate as a function f of two vectors u and v. Different composition models arise, depending on how f is chosen. Assuming that composition is a linear func</context>
</contexts>
<marker>Smolensky, 1990</marker>
<rawString>Paul Smolensky. 1990. Tensor product variable binding and the representation of symbolic structures in connectionist systems. Artificial Intelligence, 46:159– 216.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Eric H Huang</author>
<author>Jeffrey Pennin</author>
<author>Andrew Y Ng</author>
<author>Christopher D Manning</author>
</authors>
<title>Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection.</title>
<date>2011</date>
<booktitle>In Advances in Neural Information Processing Systems 24,</booktitle>
<pages>801--809</pages>
<location>Granada,</location>
<contexts>
<context position="4725" citStr="Socher et al. (2011" startWordPosition="711" endWordPosition="714">s) it can take and the composition of a sentence results in a vector which exists in sentential space. Verbs, adjectives and adverbs act as relational functions, are represented by matrices, and modify the properties of nouns, that are represented by vectors (see also Baroni and Zamparelli (2010) for a proposal similar in spirit). Clarke (2012) introduces context-theoretic semantics, a general framework for combining vector representations, based on a mathematical theory of meaning as context, and shows that it can be used to describe a variety of models including that of Clark et al. (2008). Socher et al. (2011a) and Socher et al. (2011b) present a framework based on recursive neural networks that learns vector space representations for multi-word phrases and sentences. The network is given a list of word vectors as input and a binary tree representing their syntactic structure. Then, it computes an n-dimensional representation p of two n-dimensional children and the process is repeated at every parent node until a representation for a full tree is constructed. Parent representations are computed essentially by concatenating the representations of their children. During training, the model tries to </context>
<context position="7037" citStr="Socher et al., 2011" startWordPosition="1049" endWordPosition="1052">n of the embedding represents a latent feature of the word, hopefully capturing useful syntactic and semantic properties. Using these representations, we construct several compositional models, based on addition, multiplication, and recursive neural networks. We assess the effectiveness of these models using two evaluation protocols. The first one involves modeling similarity judgments for short phrases gathered in human experiments (Mitchell and Lapata, 2010). The second one is paraphrase detection, i.e., the task of examining two sentences and determining whether they have the same meaning (Socher et al., 2011a). We find that shallow approaches are as good as more computationally intensive alternatives. They achieve considerable semantic expressivity without any learning, sophisticated linguistic processing, or access to very large corpora. Our contributions in this work are three-fold: an empirical comparison of a broad range of compositional models, some of which are introduced here for the first time; the use of an evaluation methodology that takes into account the full spectrum of compositionality from phrases to sentences; and the empirical finding that relatively simple compositional models c</context>
<context position="20133" citStr="Socher et al. (2011" startWordPosition="3207" endWordPosition="3210">k et al. (2008). In fact, a variety of multiplication-based models can be derived from this framework; and comparisons against component-wise multiplication on phrase similarity tasks yield comparable results (Grefenstette and Sadrzadeh, 2011a; Grefenstette and Sadrzadeh, 2011b). We thus opt for the model (13) as an example of compositional models based on multiplication due to its good performance across a variety of tasks, including language modeling and prediction of reading difficulty (Mitchell, 2011). Our third method, for creating phrase and sentence vectors alike, is the application of Socher et al. (2011a)’s model. They use the Stanford parser (Klein and Manning, 2003) to create a binary parse tree for each input phrase or sentence. This tree is then used as the basis for a deep recursive autoencoder (RAE). The aim is to construct a vector representation for the tree’s root bottom-up where the leaves contain word vectors. The latter can in theory be provided by any type of semantic space, however Socher et al. use word embeddings provided by the neural language model (Collobert and Weston, 2008). Given the binary tree input structure, the model computes parent representations p from their chi</context>
<context position="21467" citStr="Socher et al. (2011" startWordPosition="3442" endWordPosition="3445">on of the two children, f is an element-wise activation function such as tanh, b is a bias term, and W E Rnx2n is an encoding matrix that we want to learn during training. One way of assessing how well p represents its direct children is to decode their vectors in a reconstruction layer: [c, 1;c,2] = f(W(2)p+b(2)) (15) During training, the goal is to minimize the reconstruction errors of all input pairs at nonterminal nodes p in a given parse tree by computing the square of the Euclidean distance between the original input and its reconstruction: 1 Erec([c1;c2]) = 2|[c1;c2] − [c,1;c,2]|2 (16) Socher et al. (2011a) extend the standard recursive autoencoder sketched above in two ways. Firstly, they present an unfolding autoencoder that tries to reconstruct all leaf nodes underneath each node rather than only its direct children. And secondly, instead of transforming the two children directly into a parent p, they introduce another hidden layer inbetween. We obtained three compositional models per representation resulting in nine compositional models overall. Plugging different representations into the additive and multiplicative models is relatively straightforward. The RAE can also be used with arbitr</context>
<context position="24064" citStr="Socher et al. (2011" startWordPosition="3857" endWordPosition="3860"> to evaluate phrase representations in a wdVecwk[ j] (13) 551 dim. c.m. Adj-N N-N V-Obj SDS 2000 + 0.37 0.38 0.28 (BNC) 2000 � 0.48 0.50 0.35 100 RAE 0.31 0.30 0.28 DM vary + 0.37 0.30 0.29 (3-BWC) vary � 0.21 0.37 0.33 100 RAE 0.25 0.26 0.09 NLM 50 + 0.28 0.26 0.24 (BNC) 50 � 0.26 0.22 0.18 100 RAE 0.19 0.24 0.28 Table 3: Correlation coefficients of model predictions with subject similarity ratings (Spearman’s p); columns show dimensionality: fixed or varying (see Section 2.1), composition method: + is additive vector composition, O is component-wise multiplicative vector composition, RAE is Socher et al. (2011a)’s recursive auto-encoder. similar manner. Specifically, we used the dataset from Mitchell and Lapata (2010) which contains similarity judgments for adjective-noun, noun-noun and verb-object phrases, respectively.2 Each item is a phrase pair phr1, phr2 which has a human rating from 1 (very low similarity) to 7 (very high similarity). Using the composition models described above, we compute the cosine similarity of phr1 and phr2: phrSimphr1,phr2 = phrVecphr1 · phrVecphr2 |phrVecphr1 |× |phrVecphr2 |(17) Model similarities were evaluated against the human similarity ratings using Spearman’s p </context>
<context position="31626" citStr="Socher et al. (2011" startWordPosition="5068" endWordPosition="5071">ng state-of-the-art results, Table 6 compares our approach against previous work on the same task and dataset. Initial research concentrated on individual words rather than sentential representations. Several approaches used nMSRPC uniOverlapi1,i2 = E (18) k=1 553 Model Acc. F1 Baseline 66.5 79.9 Mihalcea et al. (2006) 70.3 81.3 Rus et al. (2008) 70.6 80.5 Qiu et al. (2006) 72.0 81.6 Islam and Inkpen (2007) 72.6 81.3 Mitchell and Lapata (2010) ((D) 73.0 82.3 Baroni and Lenci (2010) (+) 73.5 82.2 Fernando and Stevenson (2008) 74.1 82.4 Wan et al. (2006) 75.6 83.0 Das and Smith (2009) 76.1 82.7 Socher et al. (2011a) 76.8 83.6 Table 6: Overview of results on the MSRCP (test corpus). Accuracy differences of 3.3 or more are significant at the 0.01 level (using the x2 statistic). WordNet in conjunction with distributional similarity in an attempt to detect meaning conveyed by synonymous words (Islam and Inkpen, 2007; Mihalcea et al., 2006; Fernando and Stevenson, 2008). More recently, the addition of syntactic features based on dependency parse trees (Wan et al., 2006; Das and Smith, 2009) has been shown to substantially boost performance. The model of Das and Smith (2009), for example, uses quasi-synchron</context>
<context position="33058" citStr="Socher et al. (2011" startWordPosition="5307" endWordPosition="5310">model is more sophisticated than the one we used in our experiments (see Table 4 and 5). Rather than using the output of the RAE as features for the classifier, it applies dynamic pooling, a procedure that takes a similarity matrix as input (e.g., created by sentences with differing lengths) and maps it to a matrix of fixed size that represents more faithfully the global similarity structure.3 Overall, we observe that our own models do as well as some of the models that employ WordNet and more sophisticated syntactic features. With regard to F1, we are comparable with Das and Smith (2009) and Socher et al. (2011a) without using elaborate features, or any additional manipulations over and above the output of the composition functions 3Without dynamic pooling, their model yields an accuracy of 74.2. which if added could increase performance. 5 Discussion In this paper we systematically compared three types of distributional representation and their effect on semantic composition. Our comparisons involved a simple distributional semantic space (Mitchell and Lapata, 2010), word embeddings computed with a neural language model (Collobert and Weston, 2008) and a representation based on weighted word-link-w</context>
</contexts>
<marker>Socher, Huang, Pennin, Ng, Manning, 2011</marker>
<rawString>Richard Socher, Eric H. Huang, Jeffrey Pennin, Andrew Y. Ng, and Christopher D. Manning. 2011a. Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection. In Advances in Neural Information Processing Systems 24, pages 801– 809. Granada, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Jeffrey Pennington</author>
<author>Eric H Huang</author>
<author>Andrew Y Ng</author>
<author>Christopher D Manning</author>
</authors>
<title>Semi-supervised recursive autoencoders for predicting sentiment distributions.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>151--161</pages>
<location>Edinburgh, Scotland.</location>
<contexts>
<context position="4725" citStr="Socher et al. (2011" startWordPosition="711" endWordPosition="714">s) it can take and the composition of a sentence results in a vector which exists in sentential space. Verbs, adjectives and adverbs act as relational functions, are represented by matrices, and modify the properties of nouns, that are represented by vectors (see also Baroni and Zamparelli (2010) for a proposal similar in spirit). Clarke (2012) introduces context-theoretic semantics, a general framework for combining vector representations, based on a mathematical theory of meaning as context, and shows that it can be used to describe a variety of models including that of Clark et al. (2008). Socher et al. (2011a) and Socher et al. (2011b) present a framework based on recursive neural networks that learns vector space representations for multi-word phrases and sentences. The network is given a list of word vectors as input and a binary tree representing their syntactic structure. Then, it computes an n-dimensional representation p of two n-dimensional children and the process is repeated at every parent node until a representation for a full tree is constructed. Parent representations are computed essentially by concatenating the representations of their children. During training, the model tries to </context>
<context position="7037" citStr="Socher et al., 2011" startWordPosition="1049" endWordPosition="1052">n of the embedding represents a latent feature of the word, hopefully capturing useful syntactic and semantic properties. Using these representations, we construct several compositional models, based on addition, multiplication, and recursive neural networks. We assess the effectiveness of these models using two evaluation protocols. The first one involves modeling similarity judgments for short phrases gathered in human experiments (Mitchell and Lapata, 2010). The second one is paraphrase detection, i.e., the task of examining two sentences and determining whether they have the same meaning (Socher et al., 2011a). We find that shallow approaches are as good as more computationally intensive alternatives. They achieve considerable semantic expressivity without any learning, sophisticated linguistic processing, or access to very large corpora. Our contributions in this work are three-fold: an empirical comparison of a broad range of compositional models, some of which are introduced here for the first time; the use of an evaluation methodology that takes into account the full spectrum of compositionality from phrases to sentences; and the empirical finding that relatively simple compositional models c</context>
<context position="20133" citStr="Socher et al. (2011" startWordPosition="3207" endWordPosition="3210">k et al. (2008). In fact, a variety of multiplication-based models can be derived from this framework; and comparisons against component-wise multiplication on phrase similarity tasks yield comparable results (Grefenstette and Sadrzadeh, 2011a; Grefenstette and Sadrzadeh, 2011b). We thus opt for the model (13) as an example of compositional models based on multiplication due to its good performance across a variety of tasks, including language modeling and prediction of reading difficulty (Mitchell, 2011). Our third method, for creating phrase and sentence vectors alike, is the application of Socher et al. (2011a)’s model. They use the Stanford parser (Klein and Manning, 2003) to create a binary parse tree for each input phrase or sentence. This tree is then used as the basis for a deep recursive autoencoder (RAE). The aim is to construct a vector representation for the tree’s root bottom-up where the leaves contain word vectors. The latter can in theory be provided by any type of semantic space, however Socher et al. use word embeddings provided by the neural language model (Collobert and Weston, 2008). Given the binary tree input structure, the model computes parent representations p from their chi</context>
<context position="21467" citStr="Socher et al. (2011" startWordPosition="3442" endWordPosition="3445">on of the two children, f is an element-wise activation function such as tanh, b is a bias term, and W E Rnx2n is an encoding matrix that we want to learn during training. One way of assessing how well p represents its direct children is to decode their vectors in a reconstruction layer: [c, 1;c,2] = f(W(2)p+b(2)) (15) During training, the goal is to minimize the reconstruction errors of all input pairs at nonterminal nodes p in a given parse tree by computing the square of the Euclidean distance between the original input and its reconstruction: 1 Erec([c1;c2]) = 2|[c1;c2] − [c,1;c,2]|2 (16) Socher et al. (2011a) extend the standard recursive autoencoder sketched above in two ways. Firstly, they present an unfolding autoencoder that tries to reconstruct all leaf nodes underneath each node rather than only its direct children. And secondly, instead of transforming the two children directly into a parent p, they introduce another hidden layer inbetween. We obtained three compositional models per representation resulting in nine compositional models overall. Plugging different representations into the additive and multiplicative models is relatively straightforward. The RAE can also be used with arbitr</context>
<context position="24064" citStr="Socher et al. (2011" startWordPosition="3857" endWordPosition="3860"> to evaluate phrase representations in a wdVecwk[ j] (13) 551 dim. c.m. Adj-N N-N V-Obj SDS 2000 + 0.37 0.38 0.28 (BNC) 2000 � 0.48 0.50 0.35 100 RAE 0.31 0.30 0.28 DM vary + 0.37 0.30 0.29 (3-BWC) vary � 0.21 0.37 0.33 100 RAE 0.25 0.26 0.09 NLM 50 + 0.28 0.26 0.24 (BNC) 50 � 0.26 0.22 0.18 100 RAE 0.19 0.24 0.28 Table 3: Correlation coefficients of model predictions with subject similarity ratings (Spearman’s p); columns show dimensionality: fixed or varying (see Section 2.1), composition method: + is additive vector composition, O is component-wise multiplicative vector composition, RAE is Socher et al. (2011a)’s recursive auto-encoder. similar manner. Specifically, we used the dataset from Mitchell and Lapata (2010) which contains similarity judgments for adjective-noun, noun-noun and verb-object phrases, respectively.2 Each item is a phrase pair phr1, phr2 which has a human rating from 1 (very low similarity) to 7 (very high similarity). Using the composition models described above, we compute the cosine similarity of phr1 and phr2: phrSimphr1,phr2 = phrVecphr1 · phrVecphr2 |phrVecphr1 |× |phrVecphr2 |(17) Model similarities were evaluated against the human similarity ratings using Spearman’s p </context>
<context position="31626" citStr="Socher et al. (2011" startWordPosition="5068" endWordPosition="5071">ng state-of-the-art results, Table 6 compares our approach against previous work on the same task and dataset. Initial research concentrated on individual words rather than sentential representations. Several approaches used nMSRPC uniOverlapi1,i2 = E (18) k=1 553 Model Acc. F1 Baseline 66.5 79.9 Mihalcea et al. (2006) 70.3 81.3 Rus et al. (2008) 70.6 80.5 Qiu et al. (2006) 72.0 81.6 Islam and Inkpen (2007) 72.6 81.3 Mitchell and Lapata (2010) ((D) 73.0 82.3 Baroni and Lenci (2010) (+) 73.5 82.2 Fernando and Stevenson (2008) 74.1 82.4 Wan et al. (2006) 75.6 83.0 Das and Smith (2009) 76.1 82.7 Socher et al. (2011a) 76.8 83.6 Table 6: Overview of results on the MSRCP (test corpus). Accuracy differences of 3.3 or more are significant at the 0.01 level (using the x2 statistic). WordNet in conjunction with distributional similarity in an attempt to detect meaning conveyed by synonymous words (Islam and Inkpen, 2007; Mihalcea et al., 2006; Fernando and Stevenson, 2008). More recently, the addition of syntactic features based on dependency parse trees (Wan et al., 2006; Das and Smith, 2009) has been shown to substantially boost performance. The model of Das and Smith (2009), for example, uses quasi-synchron</context>
<context position="33058" citStr="Socher et al. (2011" startWordPosition="5307" endWordPosition="5310">model is more sophisticated than the one we used in our experiments (see Table 4 and 5). Rather than using the output of the RAE as features for the classifier, it applies dynamic pooling, a procedure that takes a similarity matrix as input (e.g., created by sentences with differing lengths) and maps it to a matrix of fixed size that represents more faithfully the global similarity structure.3 Overall, we observe that our own models do as well as some of the models that employ WordNet and more sophisticated syntactic features. With regard to F1, we are comparable with Das and Smith (2009) and Socher et al. (2011a) without using elaborate features, or any additional manipulations over and above the output of the composition functions 3Without dynamic pooling, their model yields an accuracy of 74.2. which if added could increase performance. 5 Discussion In this paper we systematically compared three types of distributional representation and their effect on semantic composition. Our comparisons involved a simple distributional semantic space (Mitchell and Lapata, 2010), word embeddings computed with a neural language model (Collobert and Weston, 2008) and a representation based on weighted word-link-w</context>
</contexts>
<marker>Socher, Pennington, Huang, Ng, Manning, 2011</marker>
<rawString>Richard Socher, Jeffrey Pennington, Eric H. Huang, Andrew Y. Ng, and Christopher D. Manning. 2011b. Semi-supervised recursive autoencoders for predicting sentiment distributions. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 151–161, Edinburgh, Scotland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>Lev Ratinov</author>
<author>Yoshua Bengio</author>
</authors>
<title>Word representations: a simple and general method for semi-supervised learning.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>384--394</pages>
<location>Uppsala,</location>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: a simple and general method for semi-supervised learning. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 384–394, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>Similarity of semantic relations.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>3</issue>
<contexts>
<context position="1880" citStr="Turney, 2006" startWordPosition="269" endWordPosition="270">judgments of semantic similarity (McDonald, 2000) and association (Denhire and Lemaire, 2004; Griffiths et al., 2007) and have been shown to achieve human level performance on synonymy tests (Landauer and Dumais, 1997; Griffiths et al., 2007) such as those included in the Test of English as a Foreign Language (TOEFL). This ability has been put to practical use in numerous natural language processing tasks such as automatic thesaurus extraction (Grefenstette, 1994), word sense discrimination (Sch¨utze, 1998), language modeling (Bellegarda, 2000), and the identification of analogical relations (Turney, 2006). While much research has been directed at the most effective ways of constructing representations for individual words, there has been far less consensus regarding the representation of larger constructions such as phrases and sentences. The problem has received some attention in the connectionist literature, particularly in response to criticisms of the ability of connectionist representations to handle complex structures (Smolensky, 1990; Plate, 1995). More recently, several proposals have been put forward for computing the meaning of word combinations in vector spaces. This renewed interes</context>
</contexts>
<marker>Turney, 2006</marker>
<rawString>Peter D. Turney. 2006. Similarity of semantic relations. Computational Linguistics, 32(3):379–416.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Wan</author>
<author>Mark Dras</author>
<author>Robert Dale</author>
<author>Cecile Paris</author>
</authors>
<title>Using dependency-based features to take the “para-farce” out of paraphrase.</title>
<date>2006</date>
<booktitle>In Proceedings of the Australasian Language Technology Workshop</booktitle>
<pages>131--138</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="31565" citStr="Wan et al. (2006)" startWordPosition="5056" endWordPosition="5059">bed for evaluating compositional models rather than achieving state-of-the-art results, Table 6 compares our approach against previous work on the same task and dataset. Initial research concentrated on individual words rather than sentential representations. Several approaches used nMSRPC uniOverlapi1,i2 = E (18) k=1 553 Model Acc. F1 Baseline 66.5 79.9 Mihalcea et al. (2006) 70.3 81.3 Rus et al. (2008) 70.6 80.5 Qiu et al. (2006) 72.0 81.6 Islam and Inkpen (2007) 72.6 81.3 Mitchell and Lapata (2010) ((D) 73.0 82.3 Baroni and Lenci (2010) (+) 73.5 82.2 Fernando and Stevenson (2008) 74.1 82.4 Wan et al. (2006) 75.6 83.0 Das and Smith (2009) 76.1 82.7 Socher et al. (2011a) 76.8 83.6 Table 6: Overview of results on the MSRCP (test corpus). Accuracy differences of 3.3 or more are significant at the 0.01 level (using the x2 statistic). WordNet in conjunction with distributional similarity in an attempt to detect meaning conveyed by synonymous words (Islam and Inkpen, 2007; Mihalcea et al., 2006; Fernando and Stevenson, 2008). More recently, the addition of syntactic features based on dependency parse trees (Wan et al., 2006; Das and Smith, 2009) has been shown to substantially boost performance. The mo</context>
</contexts>
<marker>Wan, Dras, Dale, Paris, 2006</marker>
<rawString>Stephen Wan, Mark Dras, Robert Dale, and Cecile Paris. 2006. Using dependency-based features to take the “para-farce” out of paraphrase. In Proceedings of the Australasian Language Technology Workshop 2006, pages 131–138, Sydney, Australia.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
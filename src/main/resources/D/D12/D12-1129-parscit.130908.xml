<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.9989385">
A New Minimally-Supervised Framework
for Domain Word Sense Disambiguation
</title>
<author confidence="0.838381">
Stefano Faralli and Roberto Navigli
</author>
<affiliation confidence="0.556302">
Dipartimento di Informatica
</affiliation>
<address confidence="0.424655">
Sapienza Universit`a di Roma
</address>
<email confidence="0.983466">
{faralli,navigli}@di.uniroma1.it
</email>
<sectionHeader confidence="0.99539" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999543333333333">
We present a new minimally-supervised
framework for performing domain-driven
Word Sense Disambiguation (WSD). Glos-
saries for several domains are iteratively ac-
quired from the Web by means of a boot-
strapping technique. The acquired glosses are
then used as the sense inventory for fully-
unsupervised domain WSD. Our experiments,
on new and gold-standard datasets, show that
our wide-coverage framework enables high-
performance results on dozens of domains at
a coarse and fine-grained level.
</bodyText>
<sectionHeader confidence="0.998987" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.987067865384616">
Domain information pervades most of the text we
read every day. If we just think of the Web, the vast
majority of its textual content is domain oriented.
A case in point is Wikipedia, which provides ency-
clopedic coverage for a huge number of knowledge
domains (Medelyan et al., 2009), but most blogs,
Web sites and newspapers also provide a great deal
of information focused on specific areas of knowl-
edge. When it comes to automatic text understand-
ing, then, it is crucial to take into account the domain
specificity of a piece of text, so as to perform a fo-
cused and as-precise-as-possible analysis which, in
its turn, can enable domain-aware applications such
as question answering and information extraction.
Domain knowledge also has the potential to improve
open-text applications such as summarization (Cey-
lan et al., 2010) and machine translation (Foster et
al., 2010).
Research in Word Sense Disambiguation (Nav-
igli, 2009, WSD), the task aimed at the automatic
labeling of text with word senses, has been ori-
ented towards domain text understanding for sev-
eral years now. Many approaches have been devised,
including the identification of domain-specific pre-
dominant senses (McCarthy et al., 2007; Lapata and
Keller, 2007), the development of domain resources
(Magnini and Cavagli`a, 2000; Magnini et al., 2002),
their application to WSD (Gliozzo et al., 2004), and
the effective use of link analysis algorithms such as
Personalized PageRank (Agirre et al., 2009; Nav-
igli et al., 2011). More recently, semi-supervised ap-
proaches to domain WSD have been proposed which
aim at decreasing the amount of supervision needed
to carry out the task (Khapra et al., 2010).
High-performance domain WSD, however, has
been hampered by the widespread use of a general-
purpose sense inventory, i.e., WordNet (Miller et
al., 1990; Fellbaum, 1998). Unfortunately WordNet
does not contain many specialized terms, making
it difficult to use it in work on arbitrary special-
ized domains. While Wikipedia has recently been
considered a valid alternative (Mihalcea, 2007), it
is mainly focused on covering named entities and,
strictly speaking, does not contain a formal wide-
coverage sense inventory (not even in disambigua-
tion pages, which are often incomplete, especially
in the lexicographic sense).
In this paper we provide three main contributions:
• We tackle the above issues by introducing
a new framework based on the minimally-
supervised acquisition of specialized glossaries
for dozens of domains.
</bodyText>
<page confidence="0.926142">
1411
</page>
<note confidence="0.797282">
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1411–1422, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics
</note>
<listItem confidence="0.981988">
• In turn, we use the acquired domain glossaries
</listItem>
<bodyText confidence="0.624006625">
as a sense inventory for domain WSD. As a re-
sult, we redefine the domain WSD task as one
of picking out the most appropriate gloss (fine-
grained setting) or domain (coarse-grained set-
ting) from a multi-domain glossary.
• We show that our framework represents a con-
siderable departure from the common usage
of a general-purpose sense inventory such as
WordNet, in that, thanks to the wide cov-
erage of domain meanings, it enables high-
performance unsupervised WSD on many do-
mains in the range of 69-80% F1.
Furthermore, our approach can be customized to
any set of domains of interest, and new senses, i.e.,
glosses, can be added at any time (either manually or
automatically) to the multi-domain sense inventory.
</bodyText>
<sectionHeader confidence="0.999812" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999846626666667">
Domain WSD has been the focus of much interest
in the last few years. An important research direc-
tion identifies distributionally similar neighbors in
raw text as cues for determining the predominant
sense of a target word by means of a semantic simi-
larity measure (McCarthy et al., 2004; Koeling et al.,
2005; McCarthy et al., 2007). Other distributional
methods include the use of a word-category cooccur-
rence matrix, where categories are coarse senses ob-
tained from an existing thesaurus (Mohammad and
Hirst, 2006), and synonym-based word occurrence
counts (Lapata and Keller, 2007). Domain-informed
methods have also been proposed which make use of
domain labels as cues for disambiguation purposes
(Gliozzo et al., 2004).
Domain-driven approaches have been shown to
obtain the best performance among the unsupervised
alternatives (Strapparava et al., 2004), especially
when domain kernels are coupled with a syntag-
matic one (Gliozzo et al., 2005). However, their per-
formance is typically lower than supervised systems.
On the other hand, supervised systems fall short
of carrying out high-performance WSD within do-
mains, the main reason being the need for retraining
on each new specific knowledge domain. Nonethe-
less, the knowledge acquisition bottleneck can be
relieved by means of domain adaptation (Chan and
Ng, 2006; Chan and Ng, 2007; Agirre and de La-
calle, 2009) or by effectively injecting a general-
purpose corpus into a smaller domain-specific train-
ing set (Khapra et al., 2010).
However, as mentioned above, most work on
domain WSD uses WordNet as a sense inven-
tory. But even if WordNet senses have been en-
riched with topically-distinctive words and concepts
(Agirre and de Lacalle, 2004; Cuadros and Rigau,
2008), manually-developed domain labels (Magnini
et al., 2002), and disambiguated semantic relations
(Navigli, 2005), the main obstacle of being stuck
with an open-ended fine-grained sense inventory re-
mains. Recent results on the SPORTS and FINANCE
gold standard dataset (Koeling et al., 2005) show
that domain WSD can achieve accuracy in the 50-
60% ballpark when a state-of-the-art algorithm such
as Personalized PageRank is paired with a distribu-
tional approach (Agirre et al., 2009) or with seman-
tic model vectors acquired for many domains (Nav-
igli et al., 2011).
In this paper, we take domain WSD to the next
level by proposing a new framework based on
the minimally-supervised acquisition of large do-
main sense inventories thanks to which high per-
formance can be attained on virtually any domain
using unsupervised algorithms. Glossary acquisi-
tion approaches in the literature are mostly fo-
cused on pattern-based definition extraction (Fujii
and Ishikawa, 2000; Hovy et al., 2003; Fahmi and
Bouma, 2006, among others) and lattice-based su-
pervised models (Navigli and Velardi, 2010) start-
ing from an initial terminology, while we jointly
bootstrap the lexicon and the definitions for sev-
eral domains with minimal supervision and without
the requirement of domain-specific corpora. To do
so, we adapt bootstrapping techniques (Brin, 1998;
Agichtein and Gravano, 2000; Pasca et al., 2006) to
the novel task of domain glossary acquisition from
the Web.
Approaches to domain sense modeling have al-
ready been proposed which go beyond the WordNet
sense inventory (Duan and Yates, 2010). Distinc-
tive collocations are extracted from corpora and used
as features to bootstrap a supervised WSD system.
Experiments in the biomedical domain show good
performance, however only in-domain ambiguity is
addressed. In contrast, our approach tackles cross-
</bodyText>
<page confidence="0.997505">
1412
</page>
<figureCaption confidence="0.999881">
Figure 1: The bootstrapping process for glossary acquisition.
</figureCaption>
<bodyText confidence="0.999914928571428">
domain ambiguity, by working with virtually any set
of domains and minimizing the requirements by har-
vesting domain terms and definitions from the Web,
bootstrapped using a small number of seeds.
The existing approach closest to ours is that of
Huang and Riloff (2010), who devised a bootstrap-
ping approach to induce semantic class taggers from
domain text. The semantic classes are associated
with arbitrary NPs and must be established before-
hand. Our objective, instead, is to perform domain
disambiguation at the word level. To do this, we re-
define the domain WSD problem as one of selecting
the most suitable gloss from those available in our
full-fledged multi-domain glossary.
</bodyText>
<sectionHeader confidence="0.964879" genericHeader="method">
3 A Minimally-Supervised Framework for
Domain WSD
</sectionHeader>
<bodyText confidence="0.999865">
In this section we present our new framework for
performing domain WSD. The framework consists
of two phases: glossary bootstrapping (Section 3.1)
and domain WSD (Section 3.2).
</bodyText>
<subsectionHeader confidence="0.999605">
3.1 Phase 1: Bootstrapping Domain Glossaries
</subsectionHeader>
<bodyText confidence="0.999863363636364">
The objective of the first phase is to acquire a multi-
domain glossary from the Web with minimal super-
vision. We initially select a set D of domains of
interest. For each individual domain d E D we start
with an empty set of HTML patterns Pd (i.e., Pd :=
0), used for gloss harvesting. During this phase we
iteratively populate the pattern set by means of six
steps, described in the next six subsections and de-
picted in Figure 1. The final output of this phase will
be a glossary Gd consisting of domain terms and
their automatically-harvested glosses.
</bodyText>
<subsectionHeader confidence="0.958006">
3.1.1 Step 1: Initial seed selection
</subsectionHeader>
<bodyText confidence="0.999916368421053">
First, given the domain d, we manually
pick out K hypernymy relation seeds 5d =
{(ti, hi), ... , (tK, hK)}, where the pair (ti, hi)
contains a domain term ti and its generalization hi
(e.g., (firewall, security system)). The only con-
straint we impose is that the selected relations must
be distinctive for the domain d of interest. The cho-
sen hypernymy relations have to be as topical and
representative as possible for the given domain (e.g.,
(compiler, computer program) is an appropriate pair
for computer science, while (byte, unit of measure-
ment) is not, as it might cause the extraction of sev-
eral glossaries of various units and measures). Note
that this is the only human intervention in the entire
glossary acquisition process.
We now set the iteration counter k to 1 and start
the first iteration of the process (steps 2-5). After
each iteration k, we keep track of the set of glosses
Gd, acquired during iteration k.
</bodyText>
<subsectionHeader confidence="0.964561">
3.1.2 Step 2: Seed queries
</subsectionHeader>
<bodyText confidence="0.999845285714286">
For each seed pair (ti, hi), we submit the follow-
ing three queries to a Web search engine: “ti” “hi”
glossary1, “ti” “hi” definition, “ti” “hi”
dictionary and collect the 64 top-ranking results
for each query2. Each resulting page is a candidate
glossary for the domain d identified by our relation
seeds 5d.
</bodyText>
<subsectionHeader confidence="0.982854">
3.1.3 Step 3: Pattern and glossary extraction
</subsectionHeader>
<bodyText confidence="0.999848166666667">
We initialize the glossary for iteration k as fol-
lows: Gd := 0. Next, from each resulting page,
we harvest all the text snippets s starting with
ti and ending with hi (e.g., firewall&lt;/b&gt; -- a
&lt;i&gt;security system), i.e., s = ti ... hi. For each
such text snippet s, we perform five substeps:
</bodyText>
<subsectionHeader confidence="0.595331">
a) extraction of the term/gloss separator: we
</subsectionHeader>
<footnote confidence="0.99712">
1In what follows, we use the typewriter font for key-
words and term/gloss separators.
2We use the Google AJAX API, which returns 64 results.
</footnote>
<page confidence="0.922063">
1413
</page>
<table confidence="0.9960904">
Term Gloss Hypernym # seeds Gloss score
dynamic A firewall facility that monitors the state of connections and uses this firewall 2 0.75
packet filter
information to determine which network packets to allow through the firewall
peripheral Hardware that extends the capabilities of the computer, such as a printer, hardware 1 0.83
modem, or scanner.
die An integrated circuit chip cut from a finished wafer. integrated circuit 1 0.75
constructor a method used to help create a new object and initialise its data method 0 1.00
schema In database terminology, a schema is the organization of the tables, the fields in database 0 0.78
each table, and the relationships between fields and tables.
</table>
<tableCaption confidence="0.999917">
Table 1: Examples of extracted terms, glosses and hypernyms (seeds are in bold, domain terms are underlined).
</tableCaption>
<bodyText confidence="0.843072391304348">
start from ti and move right until we extract
the longest sequence pM of HTML tags and
non-alphanumeric characters, which we call the
term/gloss separator, between ti and the glossary
definition (e.g., “&lt;/b&gt; --” between “firewall”
and “a” in the above example);
b) gloss extraction: we expand the snippet s to
the right of hi in search of the entire gloss of
ti, i.e., until we reach a non-formatting tag el-
ement (e.g., &lt;span&gt;, &lt;p&gt;, &lt;div&gt;), while ig-
noring formatting elements such as &lt;b&gt;, &lt;i&gt;
and &lt;a&gt; which are typically included within a
definition sentence. As a result, we obtain the
sequence ti pM glosss(ti) pR, where glosss(ti)
is our gloss for seed term ti in snippet s (which
includes hi by construction) and pR is the non-
formatting HTML tag element to the right of
the extracted gloss. For example, we extend the
above definition for firewall to: “a &lt;i&gt;security
system&lt;/i&gt; for protecting against illegal entry
to a local area network.”.
c) pattern instance extraction: we extract the fol-
lowing pattern instance:
</bodyText>
<equation confidence="0.796355">
pL ti pM glosss(ti) pR,
</equation>
<bodyText confidence="0.945888">
where pL and pR are, respectively, the left bound-
ary of ti and the right boundary of glosss(ti), and
pM is the term/gloss separator extracted at step
3(a). The two boundaries pL and pR are obtained
by extracting the longest sequence of HTML
tags and non-alphanumeric characters obtained
when moving to the left of ti and the right of
glosss(ti), respectively3. For the above exam-
ple, we extract the following pattern instance:
3The minimum and maximum length of both PL and PR are
set to 4 and 50 characters, respectively, as a result of a tuning
phase described in Section 4.1.
pL = “&lt;p&gt;&lt;b&gt;”, ti = “firewall”, pM = “&lt;/b&gt;
--”, glosss(ti) = “a &lt;i&gt;security system&lt;/i&gt;
for protecting against illegal entry to a local area
network.”, pR =“&lt;/p&gt;”.
d) pattern extraction: we generalize the above pat-
tern instance to the following pattern:
</bodyText>
<equation confidence="0.975524">
pL * pM * pR,
</equation>
<bodyText confidence="0.9038">
i.e., we replace ti and glosss(ti) with *. In the
above example, we obtain the following pattern:
&lt;p&gt;&lt;b&gt; * &lt;/b&gt; -- * &lt;/p&gt;.
Finally, we add the generalized pattern to the set
of patterns Pd, i.e., we set Pd := Pd U {pL *
pM * pR}. We also add the first sentence of
the retrieved definition glosss(ti) to our glossary
Gkd, i.e., Gkd := Gkd U {(ti, first(glosss(ti)))},
where first(g) returns the first sentence of gloss
g.
e) pattern matching: we look for additional pairs
of terms/glosses in the Web page containing the
snippet s by matching the page against the gen-
eralized pattern pL * pM * pR. We then add
to Gkd the new (term, gloss) pairs matching the
generalized pattern.
As a result of this step, we obtain a glossary Gkd
for the terms discovered at iteration k.
</bodyText>
<subsectionHeader confidence="0.955377">
3.1.4 Step 4: Gloss ranking and filtering
</subsectionHeader>
<bodyText confidence="0.936358333333333">
Importantly not all the extracted definitions per-
tain to the domain of interest. In order to rank by
domain pertinence the glosses obtained at iteration
k, we define the terminology Tk−1
1 of the terms
accumulated up until iteration k − 1 as follows:
</bodyText>
<equation confidence="0.951396333333333">
Tk−1
1 := �k−1
i=1 Ti, where Ti := {t : I(t, g) E Gid}.
</equation>
<page confidence="0.745748">
1414
</page>
<bodyText confidence="0.994257428571429">
Gloss Domain
Measures undertaken to return a degraded ecosystem’s functions and values, including its hydrology, plant and... BIOLOGY
The renewing or repairing of a natural system so that its functions and qualities are comparable to its original... GEOGRAPHY
The reign of Charles II in England. ROYALTY
A goal of criminal sentencing that attempts to make the victim ”whole again.” LAW
The process and work of improving the degraded quality of the sound or image in terms of video and audio preservation. MEDIA
A process used by radio astronomers to eliminate the smoothing effect observed in radio maps that is caused by... PHYSICS
</bodyText>
<tableCaption confidence="0.978603">
Table 2: Examples of glosses harvested for the term restoration.
</tableCaption>
<bodyText confidence="0.999952384615385">
For the base step k = 1, we define T10 := T1, i.e.,
we use the first-iteration terminology itself. To rank
the glosses, we first transform each acquired gloss
g to its bag-of-words representation Bag(g), which
contains all the single- and multi-word expressions
in g. We then score each gloss g by the ratio of do-
main terms found in its bag of words:
In Table 1 we show some glosses in the computer
science domain (second column, domain terms are
underlined) together with their score (last column).
Next, we use a threshold 0 (tuned on a held-out do-
main, described in Section 4.1) to remove from Gd
those glosses g whose score(g) &lt; 0.
</bodyText>
<subsectionHeader confidence="0.801837">
3.1.5 Step 5: Seed selection for next iteration
</subsectionHeader>
<bodyText confidence="0.935783862068966">
We now aim at selecting the new set of hyper-
nymy relation seeds to be used to start the next it-
eration. We perform three substeps:
a) Hypernym extraction: for each newly-acquired
term/gloss pair (t, g) E Gd, we automatically ex-
tract a candidate hypernym h from the textual
gloss g. To do this we use a simple unsupervised
heuristic which just selects the first term in the
gloss. More sophisticated, supervised approaches
could have been used for hypernym extraction
from glosses (Navigli and Velardi, 2010). How-
ever, note that, for the purposes of our glossary
extraction task, it is not crucial to extract ac-
curate hypernyms, but rather to harvest terms h
which are very likely to occur in the glosses of t.
We show an example of hypernym extraction for
some terms in Table 1 (we report the term in col-
umn 1, the gloss in column 2 and the hypernyms
extracted by our hypernym extraction technique
in column 3).
b) (Term, Hypernym)-ranking: we sort all the
glosses in Gd by the number of seed terms found
in each gloss. In the case of ties (i.e., glosses with
the same number of seed terms), we further sort
the glosses by the score shown in Formula 1. We
show the number of seed terms and the scores
for some glosses in Table 1 (columns 4 and 5,
respectively), where seed terms are in bold and
domain terms (i.e., in T�−1
</bodyText>
<listItem confidence="0.67917275">
1 ) are underlined.
c) New seed selection: as new seeds we select the
(term, hypernym) pairs corresponding to the K
top-ranking glosses.
</listItem>
<bodyText confidence="0.9978685">
If k equals the maximum number of iterations, we
stop. Else, we increment the iteration counter (i.e.,
k := k + 1) and jump to step (2) of our glossary
bootstrapping algorithm after replacing 5d with the
new set of seeds.
The output of the glossary bootstrapping phase is
a domain glossary Gd := Ui=1,...,max Gid, where
max is the total number of iterations.
</bodyText>
<subsectionHeader confidence="0.830404">
3.1.6 Step 6: Increasing Coverage
</subsectionHeader>
<bodyText confidence="0.999984">
Given the nature of Web domain glossaries one
can rarely find terms and definitions for general
terms (e.g., jurisprudence for the LAW domain). In
order to cover this gap, we apply domain filtering
(see Section 3.1.4) to all the glosses contained in a
general-purpose dictionary (we use WordNet). We
then add the surviving term/gloss pairs to Gd.
</bodyText>
<subsectionHeader confidence="0.998304">
3.2 Phase 2: Domain WSD
</subsectionHeader>
<bodyText confidence="0.99992">
Now that we have acquired a glossary for each do-
main in our set D, we can create a multi-domain
glossary !9 := {((t, g), d) : d E D, (t, g) E Gd}.
Our glossary !9 is thus a set of term/gloss pairs
for many domains. Note that one pair might indi-
vidually belong to more than one domain, as glos-
sary bootstrapping is performed separately for each
domain. In Table 2 we show an example of the
</bodyText>
<equation confidence="0.99922">
score(g) = |Bag(g) n T �−1
1 |
|Bag(g)|
� (1)
</equation>
<page confidence="0.917901">
1415
</page>
<bodyText confidence="0.999958090909091">
glosses acquired for the term restoration. We ob-
serve that 5 out of 6 senses are not available in Word-
Net (namely: the BIOLOGY, GEOGRAPHY, LAW, ME-
DIA and PHYSICS senses). Many of them are domain-
specific meanings for the general concept of “the
act of restoring”, with the BIOLOGY and GEOGRA-
PHY senses being very similar. However, this is a
perfectly acceptable phenomenon as any of the two
senses, i.e., glosses, would be equally valid when
disambiguating a domain text dealing with ecosys-
tem restoration.
</bodyText>
<subsectionHeader confidence="0.55475">
3.2.1 Gloss-driven WSD
</subsectionHeader>
<bodyText confidence="0.999075944444444">
We redefine the task of domain WSD as one of
selecting the most suitable gloss, if one exists, for
an input term t. For instance, consider the sentence:
“He performed the restoration of heavily corrupted
images”. An appropriate option for this occurrence
would be the MEDIA sense of restoration in Table 2.
Our gloss-driven WSD paradigm has the desir-
able property of automatically providing two levels
of sense granularity: a domain, coarse-grained level,
similar in spirit to Word Domain Disambiguation
(Sanfilippo et al., 2006), in which the sense inven-
tory of a term t is just the set of domains for which t
is covered (e.g., BIOLOGY, GEOGRAPHY, ROYALTY, LAW,
MEDIA, PHYSICS in the example of Table 2), and a
fine-grained level, which requires the selection of
the gloss which best describes the sense denoted
by the given word occurrence. A second desirable
property of our gloss-driven WSD paradigm is that
it relies on a flexible framework, which allows for
the bootstrapping of new domain glossaries or the
expansion of existing ones. However, while these
two properties – i.e., double level of granularity dis-
tinctions and flexibility – are naturally inherent in
the gloss-driven paradigm, the same cannot be said
for mainstream open-text WSD in which general-
purpose static dictionaries are typically used.
In order to evaluate our framework for domain
WSD, we propose two fully unsupervised algo-
rithms for gloss-driven domain WSD. Ideally, high
performance could be obtained using state-of-the-art
supervised WSD systems. However, in order to train
such systems, a wide-coverage sense-labeled corpus
should be available for each domain, a heavy task
which we leave to future work. Instead, our objec-
tive is to show that high-performance domain WSD
can be enabled with little effort by our framework.
</bodyText>
<subsectionHeader confidence="0.659324">
3.2.2 Algorithm 1: WSD with Personalized
PageRank
</subsectionHeader>
<bodyText confidence="0.996659393939394">
Domain Glossaries as Graphs For each domain
d E D, we create an undirected graph Nd =
(Vd, Ed) as follows: Vd is the set of concepts identi-
fied by term/gloss pairs in the domain glossary Gd,
i.e., Vd := Gd; Ed is the set of edges between pairs
of concepts, where an edge {(t, g), (t&apos;, g&apos;)} exists if
and only if t&apos; is such that t&apos; =� t and t&apos; occurs in the
bag of words of the gloss g of t. In other words, t is
connected to all the domain senses of words used in
its definition g.
Graph-based WSD Given an input text, for each
domain d E D, we produce its bag of domain con-
tent words Cd = {w1, w2, ... , w,,,} by perform-
ing tokenization, lemmatization and compounding
based on the lexicon of domain d. Then, given a
target word t, we use Cd \ {t} as the context to dis-
ambiguate t within the domain d. In order to carry
out domain WSD, i.e., to pick out the most suit-
able sense of t across domains, we apply a state-of-
the-art graph-based algorithm, namely Personalized
PageRank (Haveliwala, 2002, PPR), to each domain
graph Nd. PPR is a variant of the popular PageRank
algorithm (Brin and Page, 1998) in which the damp-
ing probability mass is concentrated on a selected
number of graph nodes, instead of being uniformly
distributed across all nodes. Specifically, following
Agirre and Soroa (2009) we concentrate the proba-
bility mass on the nodes (t&apos;, g&apos;) E Vd for which the
term t&apos; is a context word, i.e., t&apos; E Cd. Next, for each
domain d E D, we run PPR for a given number of
iterations and obtain as output a probability distribu-
tion PPVd over the graph nodes. Finally, we select
the most suitable gloss of t as follows:
</bodyText>
<equation confidence="0.99367">
SensePPR(t) = arg max PPVd(t, g) (2)
g:�IdED,��,g)EVd
</equation>
<bodyText confidence="0.999899">
where PPVd(t, g) is the PPR probability for the
term/gloss pair (t, g) and SensePPR(t) contains the
best interpretation of t across all the domains D.
</bodyText>
<sectionHeader confidence="0.744742" genericHeader="method">
3.2.3 Algorithm 2: PPR Boosted with Domain
Distribution Information
</sectionHeader>
<bodyText confidence="0.9963165">
The words in a given text do not typically deal
with a single domain. Instead, they touch different
</bodyText>
<page confidence="0.962929">
1416
</page>
<table confidence="0.988800333333333">
ART BIOLOGY BUSINESS CHEMISTRY COMPUTING EDUCATION ENGINEERING ENVIRONMENT FOOD &amp; DRINK GEOGRAPHY
GEOLOGY HEALTH HISTORY LANGUAGE LAW LITERATURE MATHS MEDIA METEOROLOGY MUSIC
PHILOSOPHY PHYSICS POLITICS PSYCHOLOGY RELIGION ROYALTY SPORTS TOURISM VIDEOGAMES WARFARE
</table>
<tableCaption confidence="0.978149">
Table 3: List of the 30 domains used in our experiments.
</tableCaption>
<table confidence="0.999751555555556">
COMPUTING FOOD ENVIRONMENT BUSINESS
chip circuit timbale dish sewage waste eurobond bond
destructor method brioche bread acid rain rain asset play stock
compiler program macaroni pasta ecosystem system income stock security
html language pizza dish air monitoring sampling financial intermediary institution
firewall security system ice cream dessert global warming temperature derivative financial product
remote lan access process pasteurized milk milk fermentation decomposition arbitrage pricing theory economic theory
relational database tabular database salted butter butter attainment area area banker’s draft bill of exchange
admin console user interface prosecco wine fugitive dust matter working capital cash
</table>
<tableCaption confidence="0.999393">
Table 4: Hypernymy relation seeds used to bootstrap glossary acquisition in four of the 30 domains.
</tableCaption>
<bodyText confidence="0.998728166666667">
areas of knowledge which are intertwined with each
other within the discourse. For example, a text deal-
ing with VIDEOGAMES will often concern domains
such as BUSINESS, COMPUTING, SPORTS, etc. Given an
input text, we can capture its relevance for each do-
main by calculating the following domain score:
</bodyText>
<equation confidence="0.998644">
Qd = |Cd |(3)
Ed&apos;ED |Cd&apos;|
</equation>
<bodyText confidence="0.999982">
where, as above, Cd is the set of content words from
the input text which are covered by domain d. We
thus propose a second algorithm which synergisti-
cally combines the spreading effect of PPR with the
domain distribution information. The best sense for
a given term t is calculated as follows:
</bodyText>
<equation confidence="0.983159">
SenSeDomPPR(t) = argmax QdPPVd(t, 9)
g:�IdED,(t,g)EVd
</equation>
<bodyText confidence="0.908191666666667">
(4)
that is, we select as the most suitable gloss for t the
one which maximizes the product of its domain rel-
evance score by its domain PPVd value. Note that
the same gloss can occur in multiple domains and
that it might obtain different scores depending on the
domain. Again, since the approach is gloss-driven,
we do not see this as a problem, but rather as a natu-
ral characteristic of our framework.
</bodyText>
<sectionHeader confidence="0.998635" genericHeader="method">
4 Experimental Setup
</sectionHeader>
<subsectionHeader confidence="0.931942">
4.1 Domains
</subsectionHeader>
<bodyText confidence="0.9997275">
We selected 30 domains starting from the Wikipedia
featured articles4. We show the domain labels in Ta-
</bodyText>
<footnote confidence="0.984647">
4http://en.wikipedia.org/wiki/Wikipedia:Featured articles
</footnote>
<tableCaption confidence="0.99376">
Table 5: Statistics on the multi-domain acquired glossary.
</tableCaption>
<bodyText confidence="0.940505">
From the Web From WordNet From both Total
Terms 74,295 83,904 18,313 176,512
Glosses 153,920 68,731 596 223,247
ble 3 (some labels have been conveniently short-
ened, e.g., PHYSICS should read PHYSICS &amp; ASTRON-
OMY). We manually identified 8 hypernym/hyponym
seeds for each domain, totalizing 240 seeds. We
used two criteria for selecting a seed: i) it covers a
separate segment of the domain, and ii) it has to be
specialized enough to avoid ambiguity. We show the
seeds used in four of our domains in Table 4. We
bootstrapped our glossary acquisition technique (cf.
Section 3.1) on each domain and performed 5 itera-
tions. For increasing the coverage of domain terms
we used WordNet glosses (see Section 3.1.6). As a
result, we obtained 30 domain glossaries. We also
kept aside a 31st domain, namely FASHION, which
we employed for tuning the minimum and maximum
length of both pL and pR in Section 3.1.3 and the
threshold 0 used to filter out non-domain glosses in
Section 3.1.4.
In Table 5 we show the statistics for the ac-
quired multi-domain glossary by distinguishing
Web-derived and WordNet terms and glosses.
</bodyText>
<subsectionHeader confidence="0.994676">
4.2 Sense Inventory
</subsectionHeader>
<bodyText confidence="0.999963">
Our sense inventory is given by the 30-domain
glossary obtained as a result of our glossary boot-
strapping phase. Overall we collected 176,512 and
223,247 distinct terms and glosses, respectively,
with an important contribution from both the Web
</bodyText>
<page confidence="0.987525">
1417
</page>
<bodyText confidence="0.9999589375">
and WordNet (see Table 5). The average num-
ber of glosses per term in our inventory is 1.9 (3.6
glosses on polysemous terms). However, note that
a monosemous word in our domain sense inventory
does not necessarily make disambiguation easier,
as i) we might have missed other domain-specific
senses, ii) an uncovered, non-domain sense might fit
a word occurrence (in this case, the domain WSD
algorithms might be (wrongly) biased towards re-
turning the only possible choice if a non-zero dis-
ambiguation score is calculated for it).
In order to determine the suitability of our multi-
domain sense inventory, we compared it with the
latest version of WordNet Domains (Magnini et al.,
2002, WND 3.2), a well-known resource which
provides domain labels for almost 65,000 nomi-
nal WordNet synsets (we removed all the synsets
tagged with the FACTOTUM label, which indicates no
domain specificity). Since WND uses about 160
finer-grained domain labels, we manually mapped
them to our 30 labels when possible (e.g. SOCCER
and SWIMMING were mapped to SPORTS), totalizing
62,100 domain-labeled synsets.
We calculated the coverage of our sense inventory
against WND at the synset and the sense level, for
each non-FACTOTUM synset. Given a WordNet synset
5, let d = UsES ds be the union of the domains ds
provided for each synonym s E 5 by our sense in-
ventory (ds = 0 if not present), and let d� be the do-
main labels assigned to 5 by WND. A synset is cov-
ered if d and d� intersect. At the sense level, instead,
we consider a synonym s E 5 to be covered if ds and
d&apos; intersect. Our synset and sense coverage is 65.9%
(40,969/62,100) and 63.7% (71,950/112,875), re-
spectively. We also calculated an extra-coverage of
203.2% (229,384/112,875), that is the fraction of do-
main senses which are not available in WND, but
we are able to provide in our sense inventory (see
e.g. the example in Table 2) over the total number of
senses in WND. While coverage and extra-coverage
provide a good indicator of the completeness of our
sense inventory, we need to calculate its precision to
determine its correctness. To do so, we randomly
sampled 500 domain glosses of terms for which no
WordNet sense was tagged with the same domain in
WND. A manual validation of this sample resulted
in an 87.0% (435/500) estimate of the precision of
our sense inventory.
</bodyText>
<subsectionHeader confidence="0.987057">
4.3 Datasets
</subsectionHeader>
<bodyText confidence="0.999973068965517">
A dataset for 30 domains We used the Giga-
word corpus (Graff and Cieri, 2003) to extract a 6-
paragraph text snippet for each of the 30 domains.
As a result, we obtained a domain dataset made up
of 180 paragraphs to which we applied tokeniza-
tion, lemmatization and compounding, totaling 1432
domain content words overall (47.7 content words
per domain on average). The average polysemy of
the words in the dataset was of 9.7 glosses and 4.4
domains per word. Each content word was manu-
ally tagged with the most suitable glosses from our
multi-domain glossary (3.9 glosses, i.e., senses per
word were assigned on average). The annotation
task was performed by two annotators with adjudi-
cation.
Sports and Finance We also experimented with
the gold standard produced by Koeling et al. (2005).
The dataset covers two domains: SPORTS and FI-
NANCE. The dataset comprises 41 ambiguous words
(with an average polysemy of 6.7 senses), many
of which express different meanings in the two do-
mains. In each domain, and for each word, around
100 sentences were sense-annotated with WordNet.
Environment Finally, we also carried out an ex-
periment on the ENVIRONMENT dataset from the
Semeval-2010 domain WSD task (Agirre et al.,
2010). The dataset includes 1,398 content words (of
which 1,032 content nouns) tagged with WordNet
senses.
</bodyText>
<subsectionHeader confidence="0.996927">
4.4 Systems
</subsectionHeader>
<bodyText confidence="0.999516833333333">
We applied the two algorithms proposed in Section
3.2, namely vanilla PPR and domain-boosted PPR.
For both versions of PPR we employed UKB, a
readily-available implementation of PPR for WSD5,
successfully experimented by Agirre and Soroa
(2009) and Agirre et al. (2009).
</bodyText>
<subsectionHeader confidence="0.993902">
4.5 Baselines
</subsectionHeader>
<bodyText confidence="0.99976225">
Random baseline We compared our algorithms
with the random baseline, which associates a ran-
dom gloss among those available for each word oc-
currence according to a uniform distribution.
</bodyText>
<footnote confidence="0.982571">
5http://ixa2.si.ehu.es/ukb/
</footnote>
<page confidence="0.995002">
1418
</page>
<bodyText confidence="0.999897555555556">
Predominant domain We also compared our al-
gorithms with a predominant sense baseline which
assigns to each word occurrence the domain label
with the highest domain score Qd among those avail-
able for the word (cf. Formula 3). Note that this is
a strong baseline, because it aims at identifying the
domain covered by the majority of terms in the input
text, however it can disambiguate only at a coarse-
grained level, i.e., at the domain level.
</bodyText>
<sectionHeader confidence="0.987118" genericHeader="evaluation">
5 Experimental Results
</sectionHeader>
<bodyText confidence="0.999701305555555">
30 domains We ran our WSD systems and the
baselines on our 30-domain dataset, on a sentence-
by-sentence basis. We calculated results at the two
levels of granularity enabled by our WSD frame-
work: a coarse-grained setting where systems out-
put the most appropriate domain label for each word
item to be disambiguated; a fine-grained setting
where systems are required to output the most suit-
able gloss for the input word. The results are shown
in Table 6. Domain PPR outperforms Vanilla PPR
by some points in precision, recall and F1 in both the
coarse-grained and the fine-grained setting, achiev-
ing an F1 around 80% and 69%, respectively (dif-
ferences in recall performance are statistically sig-
nificant using a x2 test). The predominant domain
baseline, available only in the coarse-grained set-
ting, lags behind Domain PPR by more than 3 points
in precision and 2 in recall. While these differences
are not statistically significant, the variance across
domains is much higher, thus suggesting lower reli-
ability of the method.
These results were obtained in a fully unsuper-
vised setting in which no structured knowledge was
provided, unlike previous applications of PPR to
WSD (Agirre et al., 2009; Agirre and Soroa, 2009)
which relied on the underlying WordNet graph, a
manually created resource. Furthermore, our graph
contains “noisy” semantic relations, as we connect
each gloss to all the senses of its gloss words (cf.
Section 3.2.2). Finally, we note that the results
shown in Table 6 could never have been obtained
with WordNet. In fact, drawing on our domain map-
ping, we calculated that the correct domain sense is
not in WordNet for about 68% of the words in the
dataset. Instead, the results in Table 6 show that our
framework enables high-performance unsupervised
</bodyText>
<table confidence="0.999039">
Coarse-grained Fine-grained
P R F1 P R F1
Vanilla PPR 76.7 74.31 75.5 66.1 64.11 65.1
Domain PPR 81.2 78.71 79.9 69.7 67.61 68.6
Predom. domain 77.9 76.8 77.3 - - -
Random baseline 42.5 42.5 42.5 44.1 44.1 44.1
</table>
<tableCaption confidence="0.987544">
Table 6: Performance results on the 30-domain dataset
</tableCaption>
<bodyText confidence="0.978318564102564">
(† differences between the two systems are statistically
significant using a x2 test, p &lt; 0.05).
WSD thanks to the wide coverage of domain mean-
ings.
As regards the random baseline, this performs
42.5% and 44.1% in the two settings. Despite the
higher polysemy of glosses (9.7 glosses vs. 4.4 do-
mains per word in the dataset), the performance is
higher in the fine-grained setting because often there
is more than one gloss covering the same meaning of
a domain word.
Sports, Finance and Environment For the
SPORTS, FINANCE and ENVIRONMENT datasets (cf. Sec-
tion 4.3) we did not have gloss-based sense annota-
tions, so we could not perform a fine-grained evalu-
ation. Therefore, we first studied the different sys-
tems at a coarse level on the basis of the domain dis-
tribution of the senses returned for the word items
in the dataset. We show the 3 most frequent domain
labels for each system and each dataset in Figure 2.
The figure seems to confirm our results showing Do-
main PPR as being more robust than its Vanilla ver-
sion. Next, to get a more accurate evaluation, we
randomly sampled 200 sentences from each dataset
and manually validated the coarse-grained senses,
i.e., domain assignments, output by each system on
this set of sentences. We remark that several words
in the datasets did not pertain to the domain of inter-
est. For instance, will and share do not have any
sports sense in WordNet, while the same applies
to half and chip for the business domain. Table 7
shows the results of our validation, where a domain
output by a system was considered correct if a suit-
able gloss existed for that domain in our inventory.
The results show that our framework enables
coarse-grained recall in the 70-80% ballpark even
on difficult gold standard datasets for which fine-
grained recall with WordNet struggles to surpass the
50-60% range. For instance, the best performance
</bodyText>
<page confidence="0.991276">
1419
</page>
<table confidence="0.8058425">
Vanilla PPR Domain PPR Pred. dom. Vanilla PPR Domain PPR Pred. dom. Vanilla PPR Domain PPR Pred. dom.
FINANCE SPORTS ENVIRONMENT
Figure 2: Frequency of the most common domain labels returned by our 3 systems on standard domain datasets.
FINANCE SPORTS ENVIRONMENT
P R F1 P R F1 P R F1
Vanilla PPR 57.8 56.5 57.1 65.5 63.2 64.3 81.5 77.9 79.7
Domain PPR 77.8 76.1 76.9 72.1 71.3 71.7 83.1 79.4 81.2
Predom. domain 80.0 78.3 79.1 72.6 70.1 71.3 72.7 70.6 71.6
</table>
<tableCaption confidence="0.999905">
Table 7: Coarse-grained performance results on gold-standard domain datasets.
</tableCaption>
<bodyText confidence="0.999875285714286">
on the ENVIRONMENT dataset was around 60% re-
call (Kulkarni et al., 2010) using a semi-supervised
WSD system, trained on the domain. Similarly, both
the FINANCE and SPORTS datasets are notoriously dif-
ficult gold standards on which state-of-the-art recall
using WordNet is lower than 60% (Navigli et al.,
2011).
Interestingly, the predominant domain baseline
shows a bias towards BUSINESS, thus performing best
on the FINANCE dataset. This is because of the large
number of terms covered in our domain glossary,
and consequently the high overlap with cue words
in context. On the other two domains, we observe
performance in line with our 30-domain experiment.
</bodyText>
<sectionHeader confidence="0.999358" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999982171428571">
We have here presented a new framework for do-
main Word Sense Disambiguation. We depart from
the use of general-purpose sense inventories like
WordNet and propose a bootstrapping approach to
the acquisition of sense inventories for virtually any
domain. While we selected 30 domains for this
study, nothing would prevent us from using a smaller
or larger set of these domains, or a set of completely
different domains.
Our work provides three main contributions:
i) we propose a new, flexible approach to glossary
bootstrapping which harvests hundreds of thou-
sands of term/gloss pairs; the resulting multi-
domain glossary is shown to have wide cov-
erage across domains and to include a large
amount of terms not available in WordNet;
ii) we propose a novel framework for fully-
unsupervised domain WSD which uses the
multi-domain glossary as our sense inventory;
iii) we show that high performance can be achieved
by means of simple, unsupervised WSD algo-
rithms (around 80% and 69% in a coarse- and
fine-grained setting, respectively).
Note that our aim here has not been to determine
which system performs best, but rather to show that
a reliable, full-fledged framework for domain WSD
can be set up with minimal supervision. Addition-
ally, our framework can be applied to any language
of interest, provided enough glossaries are available
online, by simply translating the keywords used for
our queries.
The multi-domain glossary (and sense inven-
tory) together with the seeds used for bootstrapping
are available from http://lcl.uniroma1.
it/dwsd.
</bodyText>
<sectionHeader confidence="0.998439" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.866004333333333">
The authors gratefully acknowl-
edge the support of the ERC Start-
ing Grant MultiJEDI No. 259234.
</bodyText>
<page confidence="0.988395">
1420
</page>
<sectionHeader confidence="0.989235" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99205953271028">
Eugene Agichtein and Luis Gravano. 2000. Snowball:
extracting relations from large plain-text collections.
In Proceedings of the fifth ACM conference on Digi-
tal Libraries (DL 2000), pages 85–94, San Antonio,
Texas, United States.
Eneko Agirre and Oier Lopez de Lacalle. 2004. Pub-
licly available topic signatures for all WordNet nom-
inal senses. In Proceedings of the 4th International
Conference on Language Resources and Evaluation,
LREC 2004, pages 1123–1126, Lisbon, Portugal.
Eneko Agirre and Oier Lopez de Lacalle. 2009. Su-
pervised domain adaption for WSD. In Proceedings
of the 12th Conference of the European Chapter of
the Association for Computational Linguistics, EACL
2009, pages 42–50, Athens, Greece.
Eneko Agirre and Aitor Soroa. 2009. Personaliz-
ing PageRank for Word Sense Disambiguation. In
Proceedings of the 12th Conference of the European
Chapter of the Association for Computational Linguis-
tics, EACL 2009, pages 33–41, Athens, Greece.
Eneko Agirre, Oier Lopez de Lacalle, and Aitor Soroa.
2009. Knowledge-based WSD on specific domains:
performing better than generic supervised WSD. In
Proceedings of the 21st International Joint Conference
on Artificial Intelligence (IJCAI), pages 1501–1506,
Pasadena, California.
Eneko Agirre, Oier L´opez de Lacalle, Christiane Fell-
baum, Shu-Kai Hsieh, Maurizio Tesconi, Monica
Monachini, Piek Vossen, and Roxanne Segers. 2010.
Semeval-2010 task 17: All-words word sense disam-
biguation on a specific domain. In Proceedings of the
5th International Workshop on Semantic Evaluation,
pages 75–80, Uppsala, Sweden.
Sergey Brin and Michael Page. 1998. Anatomy of a
large-scale hypertextual web search engine. In Pro-
ceedings of the 7th Conference on World Wide Web,
WWW 2007, pages 107–117, Brisbane, Australia.
Sergey Brin. 1998. Extracting patterns and relations
from the world wide web. In Proceedings of the In-
ternational Workshop on The World Wide Web and
Databases (WebDB 1998), pages 172–183, London,
UK.
Hakan Ceylan, Rada Mihalcea, Umut ¨Ozertem, Elena
Lloret, and Manuel Palomar. 2010. Quantifying the
limits and success of extractive summarization sys-
tems across domains. In Human Language Technolo-
gies: The 2010Annual Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics, pages 903–911, Los Angeles, California.
Yee Seng Chan and Hwee Tou Ng. 2006. Estimating
class priors in domain adaptation for word sense dis-
ambiguation. In Proceedings of the 21st International
Conference on Computational Linguistics and the 44th
annual meeting of the Association for Computational
Linguistics, ACL 2006, pages 89–96, Sydney, Aus-
tralia.
Yee Seng Chan and Hwee Tou Ng. 2007. Domain adap-
tation with active learning for word sense disambigua-
tion. In Proceedings of the 45th Annual Meeting of the
Association of Computational Linguistics, ACL 2007,
pages 49–56, Prague, Czech Republic.
Montse Cuadros and German Rigau. 2008. KnowNet:
building a large net of knowledge from the Web.
In Proceedings of the 22nd International Conference
on Computational Linguistics, COLING 2008, pages
161–168, Manchester, U.K.
Weisi Duan and Alexander Yates. 2010. Extracting
glosses to disambiguate word senses. In Proceedings
of Human Language Technologies: Conference of the
North American Chapter of the Association of Com-
putational Linguistics, NAACL 2010, pages 627–635,
Los Angeles, California, USA.
Ismail Fahmi and Gosse Bouma. 2006. Learning to iden-
tify definitions using syntactic features. In Proceed-
ings of the EACL 2006 workshop on Learning Struc-
tured Information in Natural Language Applications,
pages 64–71, Trento, Italy.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Database. MIT Press, Cambridge, MA.
George Foster, Cyril Goutte, and Roland Kuhn. 2010.
Discriminative instance weighting for domain adapta-
tion in statistical machine translation. In Proceedings
of the 2010 Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP 2010), pages 451–
459, Cambridge, Massachusetts.
Atsushi Fujii and Tetsuya Ishikawa. 2000. Utilizing
the world wide web as an encyclopedia: extracting
term descriptions from semi-structured texts. In Pro-
ceedings of the 38th Annual Meeting on Association
for Computational Linguistics, ACL 2000, pages 488–
495, Hong Kong.
Alfio Gliozzo, Carlo Strapparava, and Ido Dagan. 2004.
Unsupervised and supervised exploitation of semantic
domains in lexical disambiguation. Computer Speech
and Language, 18(3):275–299.
Alfio Gliozzo, Claudio Giuliano, and Carlo Strapparava.
2005. Domain kernels for word sense disambiguation.
In Proceedings of the 43rd Annual Meeting on Associ-
ation for Computational Linguistics, ACL 2005, pages
403–410, Ann Arbor, Michigan.
David Graff and Christopher Cieri. 2003. English Giga-
word, LDC2003T05. In Linguistic Data Consortium,
Philadelphia.
Taher H. Haveliwala. 2002. Topic-sensitive PageRank.
In Proceedings of 11th International Conference on
World Wide Web, WWW 2002, pages 517–526, Hon-
olulu, Hawaii.
</reference>
<page confidence="0.824765">
1421
</page>
<reference confidence="0.999690863636363">
Eduard Hovy, Andrew Philpot, Judith Klavans, Ulrich
Germann, and Peter T. Davis. 2003. Extending meta-
data definitions by automatically extracting and orga-
nizing glossary definitions. In Proceedings of the 2003
Annual National Conference on Digital Government
Research, pages 1–6, Boston, MA.
Ruihong Huang and Ellen Riloff. 2010. Inducing
domain-specific semantic class taggers from (almost)
nothing. In Proceedings of the 48th Annual Meeting
of the Association for Computational Linguistics, ACL
2010, pages 275–285, Uppsala, Sweden.
Mitesh Khapra, Anup Kulkarni, Saurabh Sohoney, and
Pushpak Bhattacharyya. 2010. All words domain
adapted WSD: Finding a middle ground between su-
pervision and unsupervision. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 1532–1541, Sweden.
Rob Koeling, Diana McCarthy, and John Carroll. 2005.
Domain-specific sense distributions and predominant
sense acquisition. In Proceedings of the Human Lan-
guage Technology Conference and the 2005 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 419–426, Vancouver, B.C., Canada.
Anup Kulkarni, Mitesh Khapra, Saurabh Sohoney, and
Pushpak Bhattacharyya. 2010. CFILT: Resource con-
scious approaches for all-words domain specific WSD.
In Proceedings of the 5th International Workshop on
Semantic Evaluation (Semeval-2010), pages 421–426,
Stroudsburg, PA, USA.
Mirella Lapata and Frank Keller. 2007. An information
retrieval approach to sense ranking. In Proceedings of
Human Language Technologies 2007: The Conference
of the North American Chapter of the Association for
Computational Linguistics, HLT-NAACL 2007, pages
348–355, Rochester, USA.
Bernardo Magnini and Gabriela Cavagli`a. 2000. In-
tegrating subject field codes into WordNet. In Pro-
ceedings of the 2nd Conference on Language Re-
sources and Evaluation, LREC 2000, pages 1413–
1418, Athens, Greece.
Bernardo Magnini, Carlo Strapparava, Giovanni Pezzulo,
and Alfio Gliozzo. 2002. The role of domain informa-
tion in word sense disambiguation. Natural Language
Engineering, 8:359–373.
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2004. Finding predominant senses in un-
tagged text. In Proceedings of the 42nd Annual Meet-
ing of the Association for Computational Linguistics,
ACL 2004, pages 280–287, Barcelona, Spain.
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2007. Unsupervised acquisition of pre-
dominant word senses. Computational Linguistics,
33(4):553–590.
Olena Medelyan, David Milne, Catherine Legg, and
Ian H. Witten. 2009. Mining meaning from
Wikipedia. Int. J. Hum.-Comput. Stud., 67(9):716–
754.
Rada Mihalcea. 2007. Using Wikipedia for automatic
Word Sense Disambiguation. In Proceedings of Hu-
man Language Technologies 2007: The Conference
of the North American Chapter of the Association for
Computational Linguistics, HLT-NAACL, pages 196–
203, Rochester, N.Y.
George A. Miller, R.T. Beckwith, Christiane D. Fell-
baum, D. Gross, and K. Miller. 1990. WordNet: an
online lexical database. International Journal of Lexi-
cography, 3(4):235–244.
Saif Mohammad and Graeme Hirst. 2006. Determining
word sense dominance using a thesaurus. In Proceed-
ings of the 11th Conference of the European Chap-
ter of the Association for Computational Linguistics,
EACL 2006, pages 121–128, Trento, Italy.
Roberto Navigli and Paola Velardi. 2010. Learning
Word-Class Lattices for definition and hypernym ex-
traction. In Proceedings of the 48th Annual Meeting
of the Association for Computational Linguistics, ACL
2010, pages 1318–1327, Uppsala, Sweden.
Roberto Navigli, Stefano Faralli, Aitor Soroa, Oier de La-
calle, and Eneko Agirre. 2011. Two birds with one
stone: Learning semantic models for text categoriza-
tion and Word Sense Disambiguation. In Proceed-
ings of the 20th ACM Conference on Information and
Knowledge Management, CIKM 2011, pages 2317–
2320, Glasgow, UK.
Roberto Navigli. 2005. Semi-automatic extension of
large-scale linguistic knowledge bases. In Proceed-
ings of the 18th Internationa Florida AI Research Sym-
posium Conference (FLAIRS), 15–17 May 2005, pages
548–553, Clearwater Beach, Florida.
Roberto Navigli. 2009. Word Sense Disambiguation: A
survey. ACM Computing Surveys, 41(2):1–69.
Marius Pasca, Dekang Lin, Jeffrey Bigham, Andrei Lif-
chits, and Alpa Jain. 2006. Organizing and searching
the world wide web of facts - step one: the one-million
fact extraction challenge. In Proceedings of the 21st
National Conference on Artificial intelligence (AAAI
2006), pages 1400–1405, Boston, MA.
Antonio Sanfilippo, Stephen Tratz, and Michelle Gre-
gory. 2006. Word domain disambiguation via word
sense disambiguation. In Proceedings of the Hu-
man Language Technology Conference of the NAACL,
Companion Volume: Short Papers, NAACL 2006,
pages 141–144, New York, USA.
Carlo Strapparava, Alfio Gliozzo, and Claudio Giuliano.
2004. Pattern abstraction and term similarity for
Word Sense Disambiguation: IRST at Senseval-3.
In Proceedings of the 3rd International Workshop on
the Evaluation of Systems for the Semantic Analy-
sis of Text (SENSEVAL-3), pages 229–234, Barcelona,
Spain.
</reference>
<page confidence="0.993188">
1422
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.620888">
<title confidence="0.944130666666667">A New Minimally-Supervised for Domain Word Sense Disambiguation Faralli</title>
<author confidence="0.834339">Dipartimento di_Sapienza Universit`a di</author>
<abstract confidence="0.998152923076923">We present a new minimally-supervised framework for performing domain-driven Word Sense Disambiguation (WSD). Glossaries for several domains are iteratively acquired from the Web by means of a bootstrapping technique. The acquired glosses are then used as the sense inventory for fullyunsupervised domain WSD. Our experiments, on new and gold-standard datasets, show that our wide-coverage framework enables highperformance results on dozens of domains at a coarse and fine-grained level.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eugene Agichtein</author>
<author>Luis Gravano</author>
</authors>
<title>Snowball: extracting relations from large plain-text collections.</title>
<date>2000</date>
<booktitle>In Proceedings of the fifth ACM conference on Digital Libraries (DL</booktitle>
<pages>85--94</pages>
<location>San Antonio, Texas, United States.</location>
<contexts>
<context position="7319" citStr="Agichtein and Gravano, 2000" startWordPosition="1137" endWordPosition="1140">h high performance can be attained on virtually any domain using unsupervised algorithms. Glossary acquisition approaches in the literature are mostly focused on pattern-based definition extraction (Fujii and Ishikawa, 2000; Hovy et al., 2003; Fahmi and Bouma, 2006, among others) and lattice-based supervised models (Navigli and Velardi, 2010) starting from an initial terminology, while we jointly bootstrap the lexicon and the definitions for several domains with minimal supervision and without the requirement of domain-specific corpora. To do so, we adapt bootstrapping techniques (Brin, 1998; Agichtein and Gravano, 2000; Pasca et al., 2006) to the novel task of domain glossary acquisition from the Web. Approaches to domain sense modeling have already been proposed which go beyond the WordNet sense inventory (Duan and Yates, 2010). Distinctive collocations are extracted from corpora and used as features to bootstrap a supervised WSD system. Experiments in the biomedical domain show good performance, however only in-domain ambiguity is addressed. In contrast, our approach tackles cross1412 Figure 1: The bootstrapping process for glossary acquisition. domain ambiguity, by working with virtually any set of domai</context>
</contexts>
<marker>Agichtein, Gravano, 2000</marker>
<rawString>Eugene Agichtein and Luis Gravano. 2000. Snowball: extracting relations from large plain-text collections. In Proceedings of the fifth ACM conference on Digital Libraries (DL 2000), pages 85–94, San Antonio, Texas, United States.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Oier Lopez de Lacalle</author>
</authors>
<title>Publicly available topic signatures for all WordNet nominal senses.</title>
<date>2004</date>
<booktitle>In Proceedings of the 4th International Conference on Language Resources and Evaluation, LREC 2004,</booktitle>
<pages>1123--1126</pages>
<location>Lisbon, Portugal.</location>
<marker>Agirre, de Lacalle, 2004</marker>
<rawString>Eneko Agirre and Oier Lopez de Lacalle. 2004. Publicly available topic signatures for all WordNet nominal senses. In Proceedings of the 4th International Conference on Language Resources and Evaluation, LREC 2004, pages 1123–1126, Lisbon, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Oier Lopez de Lacalle</author>
</authors>
<title>Supervised domain adaption for WSD.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics, EACL</booktitle>
<pages>42--50</pages>
<location>Athens, Greece.</location>
<marker>Agirre, de Lacalle, 2009</marker>
<rawString>Eneko Agirre and Oier Lopez de Lacalle. 2009. Supervised domain adaption for WSD. In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics, EACL 2009, pages 42–50, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Aitor Soroa</author>
</authors>
<title>Personalizing PageRank for Word Sense Disambiguation.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics, EACL</booktitle>
<pages>33--41</pages>
<location>Athens, Greece.</location>
<contexts>
<context position="22741" citStr="Agirre and Soroa (2009)" startWordPosition="3805" endWordPosition="3808">d on the lexicon of domain d. Then, given a target word t, we use Cd \ {t} as the context to disambiguate t within the domain d. In order to carry out domain WSD, i.e., to pick out the most suitable sense of t across domains, we apply a state-ofthe-art graph-based algorithm, namely Personalized PageRank (Haveliwala, 2002, PPR), to each domain graph Nd. PPR is a variant of the popular PageRank algorithm (Brin and Page, 1998) in which the damping probability mass is concentrated on a selected number of graph nodes, instead of being uniformly distributed across all nodes. Specifically, following Agirre and Soroa (2009) we concentrate the probability mass on the nodes (t&apos;, g&apos;) E Vd for which the term t&apos; is a context word, i.e., t&apos; E Cd. Next, for each domain d E D, we run PPR for a given number of iterations and obtain as output a probability distribution PPVd over the graph nodes. Finally, we select the most suitable gloss of t as follows: SensePPR(t) = arg max PPVd(t, g) (2) g:�IdED,��,g)EVd where PPVd(t, g) is the PPR probability for the term/gloss pair (t, g) and SensePPR(t) contains the best interpretation of t across all the domains D. 3.2.3 Algorithm 2: PPR Boosted with Domain Distribution Information</context>
<context position="31215" citStr="Agirre and Soroa (2009)" startWordPosition="5196" endWordPosition="5199">ent meanings in the two domains. In each domain, and for each word, around 100 sentences were sense-annotated with WordNet. Environment Finally, we also carried out an experiment on the ENVIRONMENT dataset from the Semeval-2010 domain WSD task (Agirre et al., 2010). The dataset includes 1,398 content words (of which 1,032 content nouns) tagged with WordNet senses. 4.4 Systems We applied the two algorithms proposed in Section 3.2, namely vanilla PPR and domain-boosted PPR. For both versions of PPR we employed UKB, a readily-available implementation of PPR for WSD5, successfully experimented by Agirre and Soroa (2009) and Agirre et al. (2009). 4.5 Baselines Random baseline We compared our algorithms with the random baseline, which associates a random gloss among those available for each word occurrence according to a uniform distribution. 5http://ixa2.si.ehu.es/ukb/ 1418 Predominant domain We also compared our algorithms with a predominant sense baseline which assigns to each word occurrence the domain label with the highest domain score Qd among those available for the word (cf. Formula 3). Note that this is a strong baseline, because it aims at identifying the domain covered by the majority of terms in t</context>
<context position="33160" citStr="Agirre and Soroa, 2009" startWordPosition="5515" endWordPosition="5518">an F1 around 80% and 69%, respectively (differences in recall performance are statistically significant using a x2 test). The predominant domain baseline, available only in the coarse-grained setting, lags behind Domain PPR by more than 3 points in precision and 2 in recall. While these differences are not statistically significant, the variance across domains is much higher, thus suggesting lower reliability of the method. These results were obtained in a fully unsupervised setting in which no structured knowledge was provided, unlike previous applications of PPR to WSD (Agirre et al., 2009; Agirre and Soroa, 2009) which relied on the underlying WordNet graph, a manually created resource. Furthermore, our graph contains “noisy” semantic relations, as we connect each gloss to all the senses of its gloss words (cf. Section 3.2.2). Finally, we note that the results shown in Table 6 could never have been obtained with WordNet. In fact, drawing on our domain mapping, we calculated that the correct domain sense is not in WordNet for about 68% of the words in the dataset. Instead, the results in Table 6 show that our framework enables high-performance unsupervised Coarse-grained Fine-grained P R F1 P R F1 Vani</context>
</contexts>
<marker>Agirre, Soroa, 2009</marker>
<rawString>Eneko Agirre and Aitor Soroa. 2009. Personalizing PageRank for Word Sense Disambiguation. In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics, EACL 2009, pages 33–41, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Oier Lopez de Lacalle</author>
<author>Aitor Soroa</author>
</authors>
<title>Knowledge-based WSD on specific domains: performing better than generic supervised WSD.</title>
<date>2009</date>
<booktitle>In Proceedings of the 21st International Joint Conference on Artificial Intelligence (IJCAI),</booktitle>
<pages>1501--1506</pages>
<location>Pasadena, California.</location>
<marker>Agirre, de Lacalle, Soroa, 2009</marker>
<rawString>Eneko Agirre, Oier Lopez de Lacalle, and Aitor Soroa. 2009. Knowledge-based WSD on specific domains: performing better than generic supervised WSD. In Proceedings of the 21st International Joint Conference on Artificial Intelligence (IJCAI), pages 1501–1506, Pasadena, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Oier L´opez de Lacalle</author>
<author>Christiane Fellbaum</author>
<author>Shu-Kai Hsieh</author>
<author>Maurizio Tesconi</author>
<author>Monica Monachini</author>
<author>Piek Vossen</author>
<author>Roxanne Segers</author>
</authors>
<title>Semeval-2010 task 17: All-words word sense disambiguation on a specific domain.</title>
<date>2010</date>
<booktitle>In Proceedings of the 5th International Workshop on Semantic Evaluation,</booktitle>
<pages>75--80</pages>
<location>Uppsala,</location>
<marker>Agirre, de Lacalle, Fellbaum, Hsieh, Tesconi, Monachini, Vossen, Segers, 2010</marker>
<rawString>Eneko Agirre, Oier L´opez de Lacalle, Christiane Fellbaum, Shu-Kai Hsieh, Maurizio Tesconi, Monica Monachini, Piek Vossen, and Roxanne Segers. 2010. Semeval-2010 task 17: All-words word sense disambiguation on a specific domain. In Proceedings of the 5th International Workshop on Semantic Evaluation, pages 75–80, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sergey Brin</author>
<author>Michael Page</author>
</authors>
<title>Anatomy of a large-scale hypertextual web search engine.</title>
<date>1998</date>
<booktitle>In Proceedings of the 7th Conference on World Wide Web, WWW</booktitle>
<pages>107--117</pages>
<location>Brisbane, Australia.</location>
<contexts>
<context position="22545" citStr="Brin and Page, 1998" startWordPosition="3775" endWordPosition="3778">. Graph-based WSD Given an input text, for each domain d E D, we produce its bag of domain content words Cd = {w1, w2, ... , w,,,} by performing tokenization, lemmatization and compounding based on the lexicon of domain d. Then, given a target word t, we use Cd \ {t} as the context to disambiguate t within the domain d. In order to carry out domain WSD, i.e., to pick out the most suitable sense of t across domains, we apply a state-ofthe-art graph-based algorithm, namely Personalized PageRank (Haveliwala, 2002, PPR), to each domain graph Nd. PPR is a variant of the popular PageRank algorithm (Brin and Page, 1998) in which the damping probability mass is concentrated on a selected number of graph nodes, instead of being uniformly distributed across all nodes. Specifically, following Agirre and Soroa (2009) we concentrate the probability mass on the nodes (t&apos;, g&apos;) E Vd for which the term t&apos; is a context word, i.e., t&apos; E Cd. Next, for each domain d E D, we run PPR for a given number of iterations and obtain as output a probability distribution PPVd over the graph nodes. Finally, we select the most suitable gloss of t as follows: SensePPR(t) = arg max PPVd(t, g) (2) g:�IdED,��,g)EVd where PPVd(t, g) is th</context>
</contexts>
<marker>Brin, Page, 1998</marker>
<rawString>Sergey Brin and Michael Page. 1998. Anatomy of a large-scale hypertextual web search engine. In Proceedings of the 7th Conference on World Wide Web, WWW 2007, pages 107–117, Brisbane, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sergey Brin</author>
</authors>
<title>Extracting patterns and relations from the world wide web.</title>
<date>1998</date>
<booktitle>In Proceedings of the International Workshop on The World Wide Web and Databases (WebDB</booktitle>
<pages>172--183</pages>
<location>London, UK.</location>
<contexts>
<context position="7290" citStr="Brin, 1998" startWordPosition="1135" endWordPosition="1136">anks to which high performance can be attained on virtually any domain using unsupervised algorithms. Glossary acquisition approaches in the literature are mostly focused on pattern-based definition extraction (Fujii and Ishikawa, 2000; Hovy et al., 2003; Fahmi and Bouma, 2006, among others) and lattice-based supervised models (Navigli and Velardi, 2010) starting from an initial terminology, while we jointly bootstrap the lexicon and the definitions for several domains with minimal supervision and without the requirement of domain-specific corpora. To do so, we adapt bootstrapping techniques (Brin, 1998; Agichtein and Gravano, 2000; Pasca et al., 2006) to the novel task of domain glossary acquisition from the Web. Approaches to domain sense modeling have already been proposed which go beyond the WordNet sense inventory (Duan and Yates, 2010). Distinctive collocations are extracted from corpora and used as features to bootstrap a supervised WSD system. Experiments in the biomedical domain show good performance, however only in-domain ambiguity is addressed. In contrast, our approach tackles cross1412 Figure 1: The bootstrapping process for glossary acquisition. domain ambiguity, by working wi</context>
</contexts>
<marker>Brin, 1998</marker>
<rawString>Sergey Brin. 1998. Extracting patterns and relations from the world wide web. In Proceedings of the International Workshop on The World Wide Web and Databases (WebDB 1998), pages 172–183, London, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hakan Ceylan</author>
<author>Rada Mihalcea</author>
<author>Umut ¨Ozertem</author>
<author>Elena Lloret</author>
<author>Manuel Palomar</author>
</authors>
<title>Quantifying the limits and success of extractive summarization systems across domains. In Human Language Technologies:</title>
<date>2010</date>
<booktitle>The 2010Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>903--911</pages>
<location>Los Angeles, California.</location>
<marker>Ceylan, Mihalcea, ¨Ozertem, Lloret, Palomar, 2010</marker>
<rawString>Hakan Ceylan, Rada Mihalcea, Umut ¨Ozertem, Elena Lloret, and Manuel Palomar. 2010. Quantifying the limits and success of extractive summarization systems across domains. In Human Language Technologies: The 2010Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 903–911, Los Angeles, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Seng Chan</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Estimating class priors in domain adaptation for word sense disambiguation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, ACL</booktitle>
<pages>89--96</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="5551" citStr="Chan and Ng, 2006" startWordPosition="859" endWordPosition="862">zzo et al., 2004). Domain-driven approaches have been shown to obtain the best performance among the unsupervised alternatives (Strapparava et al., 2004), especially when domain kernels are coupled with a syntagmatic one (Gliozzo et al., 2005). However, their performance is typically lower than supervised systems. On the other hand, supervised systems fall short of carrying out high-performance WSD within domains, the main reason being the need for retraining on each new specific knowledge domain. Nonetheless, the knowledge acquisition bottleneck can be relieved by means of domain adaptation (Chan and Ng, 2006; Chan and Ng, 2007; Agirre and de Lacalle, 2009) or by effectively injecting a generalpurpose corpus into a smaller domain-specific training set (Khapra et al., 2010). However, as mentioned above, most work on domain WSD uses WordNet as a sense inventory. But even if WordNet senses have been enriched with topically-distinctive words and concepts (Agirre and de Lacalle, 2004; Cuadros and Rigau, 2008), manually-developed domain labels (Magnini et al., 2002), and disambiguated semantic relations (Navigli, 2005), the main obstacle of being stuck with an open-ended fine-grained sense inventory rem</context>
</contexts>
<marker>Chan, Ng, 2006</marker>
<rawString>Yee Seng Chan and Hwee Tou Ng. 2006. Estimating class priors in domain adaptation for word sense disambiguation. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, ACL 2006, pages 89–96, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Seng Chan</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Domain adaptation with active learning for word sense disambiguation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, ACL</booktitle>
<pages>49--56</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="5570" citStr="Chan and Ng, 2007" startWordPosition="863" endWordPosition="866">Domain-driven approaches have been shown to obtain the best performance among the unsupervised alternatives (Strapparava et al., 2004), especially when domain kernels are coupled with a syntagmatic one (Gliozzo et al., 2005). However, their performance is typically lower than supervised systems. On the other hand, supervised systems fall short of carrying out high-performance WSD within domains, the main reason being the need for retraining on each new specific knowledge domain. Nonetheless, the knowledge acquisition bottleneck can be relieved by means of domain adaptation (Chan and Ng, 2006; Chan and Ng, 2007; Agirre and de Lacalle, 2009) or by effectively injecting a generalpurpose corpus into a smaller domain-specific training set (Khapra et al., 2010). However, as mentioned above, most work on domain WSD uses WordNet as a sense inventory. But even if WordNet senses have been enriched with topically-distinctive words and concepts (Agirre and de Lacalle, 2004; Cuadros and Rigau, 2008), manually-developed domain labels (Magnini et al., 2002), and disambiguated semantic relations (Navigli, 2005), the main obstacle of being stuck with an open-ended fine-grained sense inventory remains. Recent result</context>
</contexts>
<marker>Chan, Ng, 2007</marker>
<rawString>Yee Seng Chan and Hwee Tou Ng. 2007. Domain adaptation with active learning for word sense disambiguation. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, ACL 2007, pages 49–56, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Montse Cuadros</author>
<author>German Rigau</author>
</authors>
<title>KnowNet: building a large net of knowledge from the Web.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics, COLING</booktitle>
<pages>161--168</pages>
<location>Manchester, U.K.</location>
<contexts>
<context position="5954" citStr="Cuadros and Rigau, 2008" startWordPosition="927" endWordPosition="930">e WSD within domains, the main reason being the need for retraining on each new specific knowledge domain. Nonetheless, the knowledge acquisition bottleneck can be relieved by means of domain adaptation (Chan and Ng, 2006; Chan and Ng, 2007; Agirre and de Lacalle, 2009) or by effectively injecting a generalpurpose corpus into a smaller domain-specific training set (Khapra et al., 2010). However, as mentioned above, most work on domain WSD uses WordNet as a sense inventory. But even if WordNet senses have been enriched with topically-distinctive words and concepts (Agirre and de Lacalle, 2004; Cuadros and Rigau, 2008), manually-developed domain labels (Magnini et al., 2002), and disambiguated semantic relations (Navigli, 2005), the main obstacle of being stuck with an open-ended fine-grained sense inventory remains. Recent results on the SPORTS and FINANCE gold standard dataset (Koeling et al., 2005) show that domain WSD can achieve accuracy in the 50- 60% ballpark when a state-of-the-art algorithm such as Personalized PageRank is paired with a distributional approach (Agirre et al., 2009) or with semantic model vectors acquired for many domains (Navigli et al., 2011). In this paper, we take domain WSD to </context>
</contexts>
<marker>Cuadros, Rigau, 2008</marker>
<rawString>Montse Cuadros and German Rigau. 2008. KnowNet: building a large net of knowledge from the Web. In Proceedings of the 22nd International Conference on Computational Linguistics, COLING 2008, pages 161–168, Manchester, U.K.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Weisi Duan</author>
<author>Alexander Yates</author>
</authors>
<title>Extracting glosses to disambiguate word senses.</title>
<date>2010</date>
<booktitle>In Proceedings of Human Language Technologies: Conference of the North American Chapter of the Association of Computational Linguistics, NAACL 2010,</booktitle>
<pages>627--635</pages>
<location>Los Angeles, California, USA.</location>
<contexts>
<context position="7533" citStr="Duan and Yates, 2010" startWordPosition="1173" endWordPosition="1176">000; Hovy et al., 2003; Fahmi and Bouma, 2006, among others) and lattice-based supervised models (Navigli and Velardi, 2010) starting from an initial terminology, while we jointly bootstrap the lexicon and the definitions for several domains with minimal supervision and without the requirement of domain-specific corpora. To do so, we adapt bootstrapping techniques (Brin, 1998; Agichtein and Gravano, 2000; Pasca et al., 2006) to the novel task of domain glossary acquisition from the Web. Approaches to domain sense modeling have already been proposed which go beyond the WordNet sense inventory (Duan and Yates, 2010). Distinctive collocations are extracted from corpora and used as features to bootstrap a supervised WSD system. Experiments in the biomedical domain show good performance, however only in-domain ambiguity is addressed. In contrast, our approach tackles cross1412 Figure 1: The bootstrapping process for glossary acquisition. domain ambiguity, by working with virtually any set of domains and minimizing the requirements by harvesting domain terms and definitions from the Web, bootstrapped using a small number of seeds. The existing approach closest to ours is that of Huang and Riloff (2010), who </context>
</contexts>
<marker>Duan, Yates, 2010</marker>
<rawString>Weisi Duan and Alexander Yates. 2010. Extracting glosses to disambiguate word senses. In Proceedings of Human Language Technologies: Conference of the North American Chapter of the Association of Computational Linguistics, NAACL 2010, pages 627–635, Los Angeles, California, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ismail Fahmi</author>
<author>Gosse Bouma</author>
</authors>
<title>Learning to identify definitions using syntactic features.</title>
<date>2006</date>
<booktitle>In Proceedings of the EACL 2006 workshop on Learning Structured Information in Natural Language Applications,</booktitle>
<pages>64--71</pages>
<location>Trento, Italy.</location>
<contexts>
<context position="6957" citStr="Fahmi and Bouma, 2006" startWordPosition="1084" endWordPosition="1087">orithm such as Personalized PageRank is paired with a distributional approach (Agirre et al., 2009) or with semantic model vectors acquired for many domains (Navigli et al., 2011). In this paper, we take domain WSD to the next level by proposing a new framework based on the minimally-supervised acquisition of large domain sense inventories thanks to which high performance can be attained on virtually any domain using unsupervised algorithms. Glossary acquisition approaches in the literature are mostly focused on pattern-based definition extraction (Fujii and Ishikawa, 2000; Hovy et al., 2003; Fahmi and Bouma, 2006, among others) and lattice-based supervised models (Navigli and Velardi, 2010) starting from an initial terminology, while we jointly bootstrap the lexicon and the definitions for several domains with minimal supervision and without the requirement of domain-specific corpora. To do so, we adapt bootstrapping techniques (Brin, 1998; Agichtein and Gravano, 2000; Pasca et al., 2006) to the novel task of domain glossary acquisition from the Web. Approaches to domain sense modeling have already been proposed which go beyond the WordNet sense inventory (Duan and Yates, 2010). Distinctive collocatio</context>
</contexts>
<marker>Fahmi, Bouma, 2006</marker>
<rawString>Ismail Fahmi and Gosse Bouma. 2006. Learning to identify definitions using syntactic features. In Proceedings of the EACL 2006 workshop on Learning Structured Information in Natural Language Applications, pages 64–71, Trento, Italy.</rawString>
</citation>
<citation valid="true">
<title>WordNet: An Electronic Database.</title>
<date>1998</date>
<editor>Christiane Fellbaum, editor.</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<marker>1998</marker>
<rawString>Christiane Fellbaum, editor. 1998. WordNet: An Electronic Database. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Foster</author>
<author>Cyril Goutte</author>
<author>Roland Kuhn</author>
</authors>
<title>Discriminative instance weighting for domain adaptation in statistical machine translation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing (EMNLP 2010),</booktitle>
<pages>451--459</pages>
<location>Cambridge, Massachusetts.</location>
<contexts>
<context position="1589" citStr="Foster et al., 2010" startWordPosition="239" endWordPosition="242">ins (Medelyan et al., 2009), but most blogs, Web sites and newspapers also provide a great deal of information focused on specific areas of knowledge. When it comes to automatic text understanding, then, it is crucial to take into account the domain specificity of a piece of text, so as to perform a focused and as-precise-as-possible analysis which, in its turn, can enable domain-aware applications such as question answering and information extraction. Domain knowledge also has the potential to improve open-text applications such as summarization (Ceylan et al., 2010) and machine translation (Foster et al., 2010). Research in Word Sense Disambiguation (Navigli, 2009, WSD), the task aimed at the automatic labeling of text with word senses, has been oriented towards domain text understanding for several years now. Many approaches have been devised, including the identification of domain-specific predominant senses (McCarthy et al., 2007; Lapata and Keller, 2007), the development of domain resources (Magnini and Cavagli`a, 2000; Magnini et al., 2002), their application to WSD (Gliozzo et al., 2004), and the effective use of link analysis algorithms such as Personalized PageRank (Agirre et al., 2009; Navi</context>
</contexts>
<marker>Foster, Goutte, Kuhn, 2010</marker>
<rawString>George Foster, Cyril Goutte, and Roland Kuhn. 2010. Discriminative instance weighting for domain adaptation in statistical machine translation. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing (EMNLP 2010), pages 451– 459, Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Atsushi Fujii</author>
<author>Tetsuya Ishikawa</author>
</authors>
<title>Utilizing the world wide web as an encyclopedia: extracting term descriptions from semi-structured texts.</title>
<date>2000</date>
<booktitle>In Proceedings of the 38th Annual Meeting on Association for Computational Linguistics, ACL</booktitle>
<pages>488--495</pages>
<location>Hong Kong.</location>
<contexts>
<context position="6915" citStr="Fujii and Ishikawa, 2000" startWordPosition="1076" endWordPosition="1079"> 50- 60% ballpark when a state-of-the-art algorithm such as Personalized PageRank is paired with a distributional approach (Agirre et al., 2009) or with semantic model vectors acquired for many domains (Navigli et al., 2011). In this paper, we take domain WSD to the next level by proposing a new framework based on the minimally-supervised acquisition of large domain sense inventories thanks to which high performance can be attained on virtually any domain using unsupervised algorithms. Glossary acquisition approaches in the literature are mostly focused on pattern-based definition extraction (Fujii and Ishikawa, 2000; Hovy et al., 2003; Fahmi and Bouma, 2006, among others) and lattice-based supervised models (Navigli and Velardi, 2010) starting from an initial terminology, while we jointly bootstrap the lexicon and the definitions for several domains with minimal supervision and without the requirement of domain-specific corpora. To do so, we adapt bootstrapping techniques (Brin, 1998; Agichtein and Gravano, 2000; Pasca et al., 2006) to the novel task of domain glossary acquisition from the Web. Approaches to domain sense modeling have already been proposed which go beyond the WordNet sense inventory (Dua</context>
</contexts>
<marker>Fujii, Ishikawa, 2000</marker>
<rawString>Atsushi Fujii and Tetsuya Ishikawa. 2000. Utilizing the world wide web as an encyclopedia: extracting term descriptions from semi-structured texts. In Proceedings of the 38th Annual Meeting on Association for Computational Linguistics, ACL 2000, pages 488– 495, Hong Kong.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alfio Gliozzo</author>
<author>Carlo Strapparava</author>
<author>Ido Dagan</author>
</authors>
<title>Unsupervised and supervised exploitation of semantic domains in lexical disambiguation.</title>
<date>2004</date>
<journal>Computer Speech and Language,</journal>
<volume>18</volume>
<issue>3</issue>
<contexts>
<context position="2081" citStr="Gliozzo et al., 2004" startWordPosition="315" endWordPosition="318">ntial to improve open-text applications such as summarization (Ceylan et al., 2010) and machine translation (Foster et al., 2010). Research in Word Sense Disambiguation (Navigli, 2009, WSD), the task aimed at the automatic labeling of text with word senses, has been oriented towards domain text understanding for several years now. Many approaches have been devised, including the identification of domain-specific predominant senses (McCarthy et al., 2007; Lapata and Keller, 2007), the development of domain resources (Magnini and Cavagli`a, 2000; Magnini et al., 2002), their application to WSD (Gliozzo et al., 2004), and the effective use of link analysis algorithms such as Personalized PageRank (Agirre et al., 2009; Navigli et al., 2011). More recently, semi-supervised approaches to domain WSD have been proposed which aim at decreasing the amount of supervision needed to carry out the task (Khapra et al., 2010). High-performance domain WSD, however, has been hampered by the widespread use of a generalpurpose sense inventory, i.e., WordNet (Miller et al., 1990; Fellbaum, 1998). Unfortunately WordNet does not contain many specialized terms, making it difficult to use it in work on arbitrary specialized do</context>
<context position="4951" citStr="Gliozzo et al., 2004" startWordPosition="768" endWordPosition="771">entifies distributionally similar neighbors in raw text as cues for determining the predominant sense of a target word by means of a semantic similarity measure (McCarthy et al., 2004; Koeling et al., 2005; McCarthy et al., 2007). Other distributional methods include the use of a word-category cooccurrence matrix, where categories are coarse senses obtained from an existing thesaurus (Mohammad and Hirst, 2006), and synonym-based word occurrence counts (Lapata and Keller, 2007). Domain-informed methods have also been proposed which make use of domain labels as cues for disambiguation purposes (Gliozzo et al., 2004). Domain-driven approaches have been shown to obtain the best performance among the unsupervised alternatives (Strapparava et al., 2004), especially when domain kernels are coupled with a syntagmatic one (Gliozzo et al., 2005). However, their performance is typically lower than supervised systems. On the other hand, supervised systems fall short of carrying out high-performance WSD within domains, the main reason being the need for retraining on each new specific knowledge domain. Nonetheless, the knowledge acquisition bottleneck can be relieved by means of domain adaptation (Chan and Ng, 2006</context>
</contexts>
<marker>Gliozzo, Strapparava, Dagan, 2004</marker>
<rawString>Alfio Gliozzo, Carlo Strapparava, and Ido Dagan. 2004. Unsupervised and supervised exploitation of semantic domains in lexical disambiguation. Computer Speech and Language, 18(3):275–299.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alfio Gliozzo</author>
<author>Claudio Giuliano</author>
<author>Carlo Strapparava</author>
</authors>
<title>Domain kernels for word sense disambiguation.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL</booktitle>
<pages>403--410</pages>
<location>Ann Arbor, Michigan.</location>
<contexts>
<context position="5177" citStr="Gliozzo et al., 2005" startWordPosition="801" endWordPosition="804">07). Other distributional methods include the use of a word-category cooccurrence matrix, where categories are coarse senses obtained from an existing thesaurus (Mohammad and Hirst, 2006), and synonym-based word occurrence counts (Lapata and Keller, 2007). Domain-informed methods have also been proposed which make use of domain labels as cues for disambiguation purposes (Gliozzo et al., 2004). Domain-driven approaches have been shown to obtain the best performance among the unsupervised alternatives (Strapparava et al., 2004), especially when domain kernels are coupled with a syntagmatic one (Gliozzo et al., 2005). However, their performance is typically lower than supervised systems. On the other hand, supervised systems fall short of carrying out high-performance WSD within domains, the main reason being the need for retraining on each new specific knowledge domain. Nonetheless, the knowledge acquisition bottleneck can be relieved by means of domain adaptation (Chan and Ng, 2006; Chan and Ng, 2007; Agirre and de Lacalle, 2009) or by effectively injecting a generalpurpose corpus into a smaller domain-specific training set (Khapra et al., 2010). However, as mentioned above, most work on domain WSD uses</context>
</contexts>
<marker>Gliozzo, Giuliano, Strapparava, 2005</marker>
<rawString>Alfio Gliozzo, Claudio Giuliano, and Carlo Strapparava. 2005. Domain kernels for word sense disambiguation. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL 2005, pages 403–410, Ann Arbor, Michigan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Graff</author>
<author>Christopher Cieri</author>
</authors>
<date>2003</date>
<booktitle>English Gigaword, LDC2003T05. In Linguistic Data Consortium,</booktitle>
<location>Philadelphia.</location>
<contexts>
<context position="29715" citStr="Graff and Cieri, 2003" startWordPosition="4953" endWordPosition="4956">but we are able to provide in our sense inventory (see e.g. the example in Table 2) over the total number of senses in WND. While coverage and extra-coverage provide a good indicator of the completeness of our sense inventory, we need to calculate its precision to determine its correctness. To do so, we randomly sampled 500 domain glosses of terms for which no WordNet sense was tagged with the same domain in WND. A manual validation of this sample resulted in an 87.0% (435/500) estimate of the precision of our sense inventory. 4.3 Datasets A dataset for 30 domains We used the Gigaword corpus (Graff and Cieri, 2003) to extract a 6- paragraph text snippet for each of the 30 domains. As a result, we obtained a domain dataset made up of 180 paragraphs to which we applied tokenization, lemmatization and compounding, totaling 1432 domain content words overall (47.7 content words per domain on average). The average polysemy of the words in the dataset was of 9.7 glosses and 4.4 domains per word. Each content word was manually tagged with the most suitable glosses from our multi-domain glossary (3.9 glosses, i.e., senses per word were assigned on average). The annotation task was performed by two annotators wit</context>
</contexts>
<marker>Graff, Cieri, 2003</marker>
<rawString>David Graff and Christopher Cieri. 2003. English Gigaword, LDC2003T05. In Linguistic Data Consortium, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taher H Haveliwala</author>
</authors>
<title>Topic-sensitive PageRank.</title>
<date>2002</date>
<booktitle>In Proceedings of 11th International Conference on World Wide Web, WWW 2002,</booktitle>
<pages>517--526</pages>
<location>Honolulu, Hawaii.</location>
<contexts>
<context position="22440" citStr="Haveliwala, 2002" startWordPosition="3758" endWordPosition="3759">oss g of t. In other words, t is connected to all the domain senses of words used in its definition g. Graph-based WSD Given an input text, for each domain d E D, we produce its bag of domain content words Cd = {w1, w2, ... , w,,,} by performing tokenization, lemmatization and compounding based on the lexicon of domain d. Then, given a target word t, we use Cd \ {t} as the context to disambiguate t within the domain d. In order to carry out domain WSD, i.e., to pick out the most suitable sense of t across domains, we apply a state-ofthe-art graph-based algorithm, namely Personalized PageRank (Haveliwala, 2002, PPR), to each domain graph Nd. PPR is a variant of the popular PageRank algorithm (Brin and Page, 1998) in which the damping probability mass is concentrated on a selected number of graph nodes, instead of being uniformly distributed across all nodes. Specifically, following Agirre and Soroa (2009) we concentrate the probability mass on the nodes (t&apos;, g&apos;) E Vd for which the term t&apos; is a context word, i.e., t&apos; E Cd. Next, for each domain d E D, we run PPR for a given number of iterations and obtain as output a probability distribution PPVd over the graph nodes. Finally, we select the most sui</context>
</contexts>
<marker>Haveliwala, 2002</marker>
<rawString>Taher H. Haveliwala. 2002. Topic-sensitive PageRank. In Proceedings of 11th International Conference on World Wide Web, WWW 2002, pages 517–526, Honolulu, Hawaii.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eduard Hovy</author>
<author>Andrew Philpot</author>
<author>Judith Klavans</author>
<author>Ulrich Germann</author>
<author>Peter T Davis</author>
</authors>
<title>Extending metadata definitions by automatically extracting and organizing glossary definitions.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Annual National Conference on Digital Government Research,</booktitle>
<pages>1--6</pages>
<location>Boston, MA.</location>
<contexts>
<context position="6934" citStr="Hovy et al., 2003" startWordPosition="1080" endWordPosition="1083">tate-of-the-art algorithm such as Personalized PageRank is paired with a distributional approach (Agirre et al., 2009) or with semantic model vectors acquired for many domains (Navigli et al., 2011). In this paper, we take domain WSD to the next level by proposing a new framework based on the minimally-supervised acquisition of large domain sense inventories thanks to which high performance can be attained on virtually any domain using unsupervised algorithms. Glossary acquisition approaches in the literature are mostly focused on pattern-based definition extraction (Fujii and Ishikawa, 2000; Hovy et al., 2003; Fahmi and Bouma, 2006, among others) and lattice-based supervised models (Navigli and Velardi, 2010) starting from an initial terminology, while we jointly bootstrap the lexicon and the definitions for several domains with minimal supervision and without the requirement of domain-specific corpora. To do so, we adapt bootstrapping techniques (Brin, 1998; Agichtein and Gravano, 2000; Pasca et al., 2006) to the novel task of domain glossary acquisition from the Web. Approaches to domain sense modeling have already been proposed which go beyond the WordNet sense inventory (Duan and Yates, 2010).</context>
</contexts>
<marker>Hovy, Philpot, Klavans, Germann, Davis, 2003</marker>
<rawString>Eduard Hovy, Andrew Philpot, Judith Klavans, Ulrich Germann, and Peter T. Davis. 2003. Extending metadata definitions by automatically extracting and organizing glossary definitions. In Proceedings of the 2003 Annual National Conference on Digital Government Research, pages 1–6, Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruihong Huang</author>
<author>Ellen Riloff</author>
</authors>
<title>Inducing domain-specific semantic class taggers from (almost) nothing.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL 2010,</booktitle>
<pages>275--285</pages>
<location>Uppsala,</location>
<contexts>
<context position="8127" citStr="Huang and Riloff (2010)" startWordPosition="1263" endWordPosition="1266">entory (Duan and Yates, 2010). Distinctive collocations are extracted from corpora and used as features to bootstrap a supervised WSD system. Experiments in the biomedical domain show good performance, however only in-domain ambiguity is addressed. In contrast, our approach tackles cross1412 Figure 1: The bootstrapping process for glossary acquisition. domain ambiguity, by working with virtually any set of domains and minimizing the requirements by harvesting domain terms and definitions from the Web, bootstrapped using a small number of seeds. The existing approach closest to ours is that of Huang and Riloff (2010), who devised a bootstrapping approach to induce semantic class taggers from domain text. The semantic classes are associated with arbitrary NPs and must be established beforehand. Our objective, instead, is to perform domain disambiguation at the word level. To do this, we redefine the domain WSD problem as one of selecting the most suitable gloss from those available in our full-fledged multi-domain glossary. 3 A Minimally-Supervised Framework for Domain WSD In this section we present our new framework for performing domain WSD. The framework consists of two phases: glossary bootstrapping (S</context>
</contexts>
<marker>Huang, Riloff, 2010</marker>
<rawString>Ruihong Huang and Ellen Riloff. 2010. Inducing domain-specific semantic class taggers from (almost) nothing. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL 2010, pages 275–285, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitesh Khapra</author>
<author>Anup Kulkarni</author>
<author>Saurabh Sohoney</author>
<author>Pushpak Bhattacharyya</author>
</authors>
<title>All words domain adapted WSD: Finding a middle ground between supervision and unsupervision.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1532--1541</pages>
<contexts>
<context position="2383" citStr="Khapra et al., 2010" startWordPosition="365" endWordPosition="368">anding for several years now. Many approaches have been devised, including the identification of domain-specific predominant senses (McCarthy et al., 2007; Lapata and Keller, 2007), the development of domain resources (Magnini and Cavagli`a, 2000; Magnini et al., 2002), their application to WSD (Gliozzo et al., 2004), and the effective use of link analysis algorithms such as Personalized PageRank (Agirre et al., 2009; Navigli et al., 2011). More recently, semi-supervised approaches to domain WSD have been proposed which aim at decreasing the amount of supervision needed to carry out the task (Khapra et al., 2010). High-performance domain WSD, however, has been hampered by the widespread use of a generalpurpose sense inventory, i.e., WordNet (Miller et al., 1990; Fellbaum, 1998). Unfortunately WordNet does not contain many specialized terms, making it difficult to use it in work on arbitrary specialized domains. While Wikipedia has recently been considered a valid alternative (Mihalcea, 2007), it is mainly focused on covering named entities and, strictly speaking, does not contain a formal widecoverage sense inventory (not even in disambiguation pages, which are often incomplete, especially in the lexi</context>
<context position="5718" citStr="Khapra et al., 2010" startWordPosition="888" endWordPosition="891">y when domain kernels are coupled with a syntagmatic one (Gliozzo et al., 2005). However, their performance is typically lower than supervised systems. On the other hand, supervised systems fall short of carrying out high-performance WSD within domains, the main reason being the need for retraining on each new specific knowledge domain. Nonetheless, the knowledge acquisition bottleneck can be relieved by means of domain adaptation (Chan and Ng, 2006; Chan and Ng, 2007; Agirre and de Lacalle, 2009) or by effectively injecting a generalpurpose corpus into a smaller domain-specific training set (Khapra et al., 2010). However, as mentioned above, most work on domain WSD uses WordNet as a sense inventory. But even if WordNet senses have been enriched with topically-distinctive words and concepts (Agirre and de Lacalle, 2004; Cuadros and Rigau, 2008), manually-developed domain labels (Magnini et al., 2002), and disambiguated semantic relations (Navigli, 2005), the main obstacle of being stuck with an open-ended fine-grained sense inventory remains. Recent results on the SPORTS and FINANCE gold standard dataset (Koeling et al., 2005) show that domain WSD can achieve accuracy in the 50- 60% ballpark when a st</context>
</contexts>
<marker>Khapra, Kulkarni, Sohoney, Bhattacharyya, 2010</marker>
<rawString>Mitesh Khapra, Anup Kulkarni, Saurabh Sohoney, and Pushpak Bhattacharyya. 2010. All words domain adapted WSD: Finding a middle ground between supervision and unsupervision. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1532–1541, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rob Koeling</author>
<author>Diana McCarthy</author>
<author>John Carroll</author>
</authors>
<title>Domain-specific sense distributions and predominant sense acquisition.</title>
<date>2005</date>
<booktitle>In Proceedings of the Human Language Technology Conference and the 2005 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>419--426</pages>
<location>Vancouver, B.C.,</location>
<contexts>
<context position="4535" citStr="Koeling et al., 2005" startWordPosition="707" endWordPosition="710">, it enables highperformance unsupervised WSD on many domains in the range of 69-80% F1. Furthermore, our approach can be customized to any set of domains of interest, and new senses, i.e., glosses, can be added at any time (either manually or automatically) to the multi-domain sense inventory. 2 Related Work Domain WSD has been the focus of much interest in the last few years. An important research direction identifies distributionally similar neighbors in raw text as cues for determining the predominant sense of a target word by means of a semantic similarity measure (McCarthy et al., 2004; Koeling et al., 2005; McCarthy et al., 2007). Other distributional methods include the use of a word-category cooccurrence matrix, where categories are coarse senses obtained from an existing thesaurus (Mohammad and Hirst, 2006), and synonym-based word occurrence counts (Lapata and Keller, 2007). Domain-informed methods have also been proposed which make use of domain labels as cues for disambiguation purposes (Gliozzo et al., 2004). Domain-driven approaches have been shown to obtain the best performance among the unsupervised alternatives (Strapparava et al., 2004), especially when domain kernels are coupled wit</context>
<context position="6242" citStr="Koeling et al., 2005" startWordPosition="968" endWordPosition="971">jecting a generalpurpose corpus into a smaller domain-specific training set (Khapra et al., 2010). However, as mentioned above, most work on domain WSD uses WordNet as a sense inventory. But even if WordNet senses have been enriched with topically-distinctive words and concepts (Agirre and de Lacalle, 2004; Cuadros and Rigau, 2008), manually-developed domain labels (Magnini et al., 2002), and disambiguated semantic relations (Navigli, 2005), the main obstacle of being stuck with an open-ended fine-grained sense inventory remains. Recent results on the SPORTS and FINANCE gold standard dataset (Koeling et al., 2005) show that domain WSD can achieve accuracy in the 50- 60% ballpark when a state-of-the-art algorithm such as Personalized PageRank is paired with a distributional approach (Agirre et al., 2009) or with semantic model vectors acquired for many domains (Navigli et al., 2011). In this paper, we take domain WSD to the next level by proposing a new framework based on the minimally-supervised acquisition of large domain sense inventories thanks to which high performance can be attained on virtually any domain using unsupervised algorithms. Glossary acquisition approaches in the literature are mostly</context>
<context position="30427" citStr="Koeling et al. (2005)" startWordPosition="5072" endWordPosition="5075">d a domain dataset made up of 180 paragraphs to which we applied tokenization, lemmatization and compounding, totaling 1432 domain content words overall (47.7 content words per domain on average). The average polysemy of the words in the dataset was of 9.7 glosses and 4.4 domains per word. Each content word was manually tagged with the most suitable glosses from our multi-domain glossary (3.9 glosses, i.e., senses per word were assigned on average). The annotation task was performed by two annotators with adjudication. Sports and Finance We also experimented with the gold standard produced by Koeling et al. (2005). The dataset covers two domains: SPORTS and FINANCE. The dataset comprises 41 ambiguous words (with an average polysemy of 6.7 senses), many of which express different meanings in the two domains. In each domain, and for each word, around 100 sentences were sense-annotated with WordNet. Environment Finally, we also carried out an experiment on the ENVIRONMENT dataset from the Semeval-2010 domain WSD task (Agirre et al., 2010). The dataset includes 1,398 content words (of which 1,032 content nouns) tagged with WordNet senses. 4.4 Systems We applied the two algorithms proposed in Section 3.2, n</context>
</contexts>
<marker>Koeling, McCarthy, Carroll, 2005</marker>
<rawString>Rob Koeling, Diana McCarthy, and John Carroll. 2005. Domain-specific sense distributions and predominant sense acquisition. In Proceedings of the Human Language Technology Conference and the 2005 Conference on Empirical Methods in Natural Language Processing, pages 419–426, Vancouver, B.C., Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anup Kulkarni</author>
<author>Mitesh Khapra</author>
<author>Saurabh Sohoney</author>
<author>Pushpak Bhattacharyya</author>
</authors>
<title>CFILT: Resource conscious approaches for all-words domain specific WSD.</title>
<date>2010</date>
<booktitle>In Proceedings of the 5th International Workshop on Semantic Evaluation (Semeval-2010),</booktitle>
<pages>421--426</pages>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="36466" citStr="Kulkarni et al., 2010" startWordPosition="6085" endWordPosition="6088">formance 1419 Vanilla PPR Domain PPR Pred. dom. Vanilla PPR Domain PPR Pred. dom. Vanilla PPR Domain PPR Pred. dom. FINANCE SPORTS ENVIRONMENT Figure 2: Frequency of the most common domain labels returned by our 3 systems on standard domain datasets. FINANCE SPORTS ENVIRONMENT P R F1 P R F1 P R F1 Vanilla PPR 57.8 56.5 57.1 65.5 63.2 64.3 81.5 77.9 79.7 Domain PPR 77.8 76.1 76.9 72.1 71.3 71.7 83.1 79.4 81.2 Predom. domain 80.0 78.3 79.1 72.6 70.1 71.3 72.7 70.6 71.6 Table 7: Coarse-grained performance results on gold-standard domain datasets. on the ENVIRONMENT dataset was around 60% recall (Kulkarni et al., 2010) using a semi-supervised WSD system, trained on the domain. Similarly, both the FINANCE and SPORTS datasets are notoriously difficult gold standards on which state-of-the-art recall using WordNet is lower than 60% (Navigli et al., 2011). Interestingly, the predominant domain baseline shows a bias towards BUSINESS, thus performing best on the FINANCE dataset. This is because of the large number of terms covered in our domain glossary, and consequently the high overlap with cue words in context. On the other two domains, we observe performance in line with our 30-domain experiment. 6 Conclusion </context>
</contexts>
<marker>Kulkarni, Khapra, Sohoney, Bhattacharyya, 2010</marker>
<rawString>Anup Kulkarni, Mitesh Khapra, Saurabh Sohoney, and Pushpak Bhattacharyya. 2010. CFILT: Resource conscious approaches for all-words domain specific WSD. In Proceedings of the 5th International Workshop on Semantic Evaluation (Semeval-2010), pages 421–426, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mirella Lapata</author>
<author>Frank Keller</author>
</authors>
<title>An information retrieval approach to sense ranking.</title>
<date>2007</date>
<booktitle>In Proceedings of Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics, HLT-NAACL</booktitle>
<pages>348--355</pages>
<location>Rochester, USA.</location>
<contexts>
<context position="1943" citStr="Lapata and Keller, 2007" startWordPosition="294" endWordPosition="297">, in its turn, can enable domain-aware applications such as question answering and information extraction. Domain knowledge also has the potential to improve open-text applications such as summarization (Ceylan et al., 2010) and machine translation (Foster et al., 2010). Research in Word Sense Disambiguation (Navigli, 2009, WSD), the task aimed at the automatic labeling of text with word senses, has been oriented towards domain text understanding for several years now. Many approaches have been devised, including the identification of domain-specific predominant senses (McCarthy et al., 2007; Lapata and Keller, 2007), the development of domain resources (Magnini and Cavagli`a, 2000; Magnini et al., 2002), their application to WSD (Gliozzo et al., 2004), and the effective use of link analysis algorithms such as Personalized PageRank (Agirre et al., 2009; Navigli et al., 2011). More recently, semi-supervised approaches to domain WSD have been proposed which aim at decreasing the amount of supervision needed to carry out the task (Khapra et al., 2010). High-performance domain WSD, however, has been hampered by the widespread use of a generalpurpose sense inventory, i.e., WordNet (Miller et al., 1990; Fellbau</context>
<context position="4811" citStr="Lapata and Keller, 2007" startWordPosition="747" endWordPosition="750">domain sense inventory. 2 Related Work Domain WSD has been the focus of much interest in the last few years. An important research direction identifies distributionally similar neighbors in raw text as cues for determining the predominant sense of a target word by means of a semantic similarity measure (McCarthy et al., 2004; Koeling et al., 2005; McCarthy et al., 2007). Other distributional methods include the use of a word-category cooccurrence matrix, where categories are coarse senses obtained from an existing thesaurus (Mohammad and Hirst, 2006), and synonym-based word occurrence counts (Lapata and Keller, 2007). Domain-informed methods have also been proposed which make use of domain labels as cues for disambiguation purposes (Gliozzo et al., 2004). Domain-driven approaches have been shown to obtain the best performance among the unsupervised alternatives (Strapparava et al., 2004), especially when domain kernels are coupled with a syntagmatic one (Gliozzo et al., 2005). However, their performance is typically lower than supervised systems. On the other hand, supervised systems fall short of carrying out high-performance WSD within domains, the main reason being the need for retraining on each new s</context>
</contexts>
<marker>Lapata, Keller, 2007</marker>
<rawString>Mirella Lapata and Frank Keller. 2007. An information retrieval approach to sense ranking. In Proceedings of Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics, HLT-NAACL 2007, pages 348–355, Rochester, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernardo Magnini</author>
<author>Gabriela Cavagli`a</author>
</authors>
<title>Integrating subject field codes into WordNet.</title>
<date>2000</date>
<booktitle>In Proceedings of the 2nd Conference on Language Resources and Evaluation, LREC</booktitle>
<pages>1413--1418</pages>
<location>Athens, Greece.</location>
<marker>Magnini, Cavagli`a, 2000</marker>
<rawString>Bernardo Magnini and Gabriela Cavagli`a. 2000. Integrating subject field codes into WordNet. In Proceedings of the 2nd Conference on Language Resources and Evaluation, LREC 2000, pages 1413– 1418, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernardo Magnini</author>
<author>Carlo Strapparava</author>
<author>Giovanni Pezzulo</author>
<author>Alfio Gliozzo</author>
</authors>
<title>The role of domain information in word sense disambiguation.</title>
<date>2002</date>
<journal>Natural Language Engineering,</journal>
<pages>8--359</pages>
<contexts>
<context position="2032" citStr="Magnini et al., 2002" startWordPosition="307" endWordPosition="310">on extraction. Domain knowledge also has the potential to improve open-text applications such as summarization (Ceylan et al., 2010) and machine translation (Foster et al., 2010). Research in Word Sense Disambiguation (Navigli, 2009, WSD), the task aimed at the automatic labeling of text with word senses, has been oriented towards domain text understanding for several years now. Many approaches have been devised, including the identification of domain-specific predominant senses (McCarthy et al., 2007; Lapata and Keller, 2007), the development of domain resources (Magnini and Cavagli`a, 2000; Magnini et al., 2002), their application to WSD (Gliozzo et al., 2004), and the effective use of link analysis algorithms such as Personalized PageRank (Agirre et al., 2009; Navigli et al., 2011). More recently, semi-supervised approaches to domain WSD have been proposed which aim at decreasing the amount of supervision needed to carry out the task (Khapra et al., 2010). High-performance domain WSD, however, has been hampered by the widespread use of a generalpurpose sense inventory, i.e., WordNet (Miller et al., 1990; Fellbaum, 1998). Unfortunately WordNet does not contain many specialized terms, making it diffic</context>
<context position="6011" citStr="Magnini et al., 2002" startWordPosition="934" endWordPosition="937">raining on each new specific knowledge domain. Nonetheless, the knowledge acquisition bottleneck can be relieved by means of domain adaptation (Chan and Ng, 2006; Chan and Ng, 2007; Agirre and de Lacalle, 2009) or by effectively injecting a generalpurpose corpus into a smaller domain-specific training set (Khapra et al., 2010). However, as mentioned above, most work on domain WSD uses WordNet as a sense inventory. But even if WordNet senses have been enriched with topically-distinctive words and concepts (Agirre and de Lacalle, 2004; Cuadros and Rigau, 2008), manually-developed domain labels (Magnini et al., 2002), and disambiguated semantic relations (Navigli, 2005), the main obstacle of being stuck with an open-ended fine-grained sense inventory remains. Recent results on the SPORTS and FINANCE gold standard dataset (Koeling et al., 2005) show that domain WSD can achieve accuracy in the 50- 60% ballpark when a state-of-the-art algorithm such as Personalized PageRank is paired with a distributional approach (Agirre et al., 2009) or with semantic model vectors acquired for many domains (Navigli et al., 2011). In this paper, we take domain WSD to the next level by proposing a new framework based on the </context>
<context position="27982" citStr="Magnini et al., 2002" startWordPosition="4648" endWordPosition="4651">erm in our inventory is 1.9 (3.6 glosses on polysemous terms). However, note that a monosemous word in our domain sense inventory does not necessarily make disambiguation easier, as i) we might have missed other domain-specific senses, ii) an uncovered, non-domain sense might fit a word occurrence (in this case, the domain WSD algorithms might be (wrongly) biased towards returning the only possible choice if a non-zero disambiguation score is calculated for it). In order to determine the suitability of our multidomain sense inventory, we compared it with the latest version of WordNet Domains (Magnini et al., 2002, WND 3.2), a well-known resource which provides domain labels for almost 65,000 nominal WordNet synsets (we removed all the synsets tagged with the FACTOTUM label, which indicates no domain specificity). Since WND uses about 160 finer-grained domain labels, we manually mapped them to our 30 labels when possible (e.g. SOCCER and SWIMMING were mapped to SPORTS), totalizing 62,100 domain-labeled synsets. We calculated the coverage of our sense inventory against WND at the synset and the sense level, for each non-FACTOTUM synset. Given a WordNet synset 5, let d = UsES ds be the union of the domai</context>
</contexts>
<marker>Magnini, Strapparava, Pezzulo, Gliozzo, 2002</marker>
<rawString>Bernardo Magnini, Carlo Strapparava, Giovanni Pezzulo, and Alfio Gliozzo. 2002. The role of domain information in word sense disambiguation. Natural Language Engineering, 8:359–373.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diana McCarthy</author>
<author>Rob Koeling</author>
<author>Julie Weeds</author>
<author>John Carroll</author>
</authors>
<title>Finding predominant senses in untagged text.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics, ACL</booktitle>
<pages>280--287</pages>
<location>Barcelona,</location>
<contexts>
<context position="4513" citStr="McCarthy et al., 2004" startWordPosition="703" endWordPosition="706">rage of domain meanings, it enables highperformance unsupervised WSD on many domains in the range of 69-80% F1. Furthermore, our approach can be customized to any set of domains of interest, and new senses, i.e., glosses, can be added at any time (either manually or automatically) to the multi-domain sense inventory. 2 Related Work Domain WSD has been the focus of much interest in the last few years. An important research direction identifies distributionally similar neighbors in raw text as cues for determining the predominant sense of a target word by means of a semantic similarity measure (McCarthy et al., 2004; Koeling et al., 2005; McCarthy et al., 2007). Other distributional methods include the use of a word-category cooccurrence matrix, where categories are coarse senses obtained from an existing thesaurus (Mohammad and Hirst, 2006), and synonym-based word occurrence counts (Lapata and Keller, 2007). Domain-informed methods have also been proposed which make use of domain labels as cues for disambiguation purposes (Gliozzo et al., 2004). Domain-driven approaches have been shown to obtain the best performance among the unsupervised alternatives (Strapparava et al., 2004), especially when domain k</context>
</contexts>
<marker>McCarthy, Koeling, Weeds, Carroll, 2004</marker>
<rawString>Diana McCarthy, Rob Koeling, Julie Weeds, and John Carroll. 2004. Finding predominant senses in untagged text. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics, ACL 2004, pages 280–287, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diana McCarthy</author>
<author>Rob Koeling</author>
<author>Julie Weeds</author>
<author>John Carroll</author>
</authors>
<title>Unsupervised acquisition of predominant word senses.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>4</issue>
<contexts>
<context position="1917" citStr="McCarthy et al., 2007" startWordPosition="290" endWordPosition="293">possible analysis which, in its turn, can enable domain-aware applications such as question answering and information extraction. Domain knowledge also has the potential to improve open-text applications such as summarization (Ceylan et al., 2010) and machine translation (Foster et al., 2010). Research in Word Sense Disambiguation (Navigli, 2009, WSD), the task aimed at the automatic labeling of text with word senses, has been oriented towards domain text understanding for several years now. Many approaches have been devised, including the identification of domain-specific predominant senses (McCarthy et al., 2007; Lapata and Keller, 2007), the development of domain resources (Magnini and Cavagli`a, 2000; Magnini et al., 2002), their application to WSD (Gliozzo et al., 2004), and the effective use of link analysis algorithms such as Personalized PageRank (Agirre et al., 2009; Navigli et al., 2011). More recently, semi-supervised approaches to domain WSD have been proposed which aim at decreasing the amount of supervision needed to carry out the task (Khapra et al., 2010). High-performance domain WSD, however, has been hampered by the widespread use of a generalpurpose sense inventory, i.e., WordNet (Mi</context>
<context position="4559" citStr="McCarthy et al., 2007" startWordPosition="711" endWordPosition="714">rmance unsupervised WSD on many domains in the range of 69-80% F1. Furthermore, our approach can be customized to any set of domains of interest, and new senses, i.e., glosses, can be added at any time (either manually or automatically) to the multi-domain sense inventory. 2 Related Work Domain WSD has been the focus of much interest in the last few years. An important research direction identifies distributionally similar neighbors in raw text as cues for determining the predominant sense of a target word by means of a semantic similarity measure (McCarthy et al., 2004; Koeling et al., 2005; McCarthy et al., 2007). Other distributional methods include the use of a word-category cooccurrence matrix, where categories are coarse senses obtained from an existing thesaurus (Mohammad and Hirst, 2006), and synonym-based word occurrence counts (Lapata and Keller, 2007). Domain-informed methods have also been proposed which make use of domain labels as cues for disambiguation purposes (Gliozzo et al., 2004). Domain-driven approaches have been shown to obtain the best performance among the unsupervised alternatives (Strapparava et al., 2004), especially when domain kernels are coupled with a syntagmatic one (Gli</context>
</contexts>
<marker>McCarthy, Koeling, Weeds, Carroll, 2007</marker>
<rawString>Diana McCarthy, Rob Koeling, Julie Weeds, and John Carroll. 2007. Unsupervised acquisition of predominant word senses. Computational Linguistics, 33(4):553–590.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Olena Medelyan</author>
</authors>
<location>David Milne, Catherine Legg, and</location>
<marker>Medelyan, </marker>
<rawString>Olena Medelyan, David Milne, Catherine Legg, and</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian H Witten</author>
</authors>
<title>Mining meaning from Wikipedia.</title>
<date>2009</date>
<journal>Int. J. Hum.-Comput. Stud.,</journal>
<volume>67</volume>
<issue>9</issue>
<pages>754</pages>
<marker>Witten, 2009</marker>
<rawString>Ian H. Witten. 2009. Mining meaning from Wikipedia. Int. J. Hum.-Comput. Stud., 67(9):716– 754.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
</authors>
<title>Using Wikipedia for automatic Word Sense Disambiguation.</title>
<date>2007</date>
<booktitle>In Proceedings of Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics, HLT-NAACL,</booktitle>
<pages>196--203</pages>
<location>Rochester, N.Y.</location>
<contexts>
<context position="2769" citStr="Mihalcea, 2007" startWordPosition="424" endWordPosition="425">PageRank (Agirre et al., 2009; Navigli et al., 2011). More recently, semi-supervised approaches to domain WSD have been proposed which aim at decreasing the amount of supervision needed to carry out the task (Khapra et al., 2010). High-performance domain WSD, however, has been hampered by the widespread use of a generalpurpose sense inventory, i.e., WordNet (Miller et al., 1990; Fellbaum, 1998). Unfortunately WordNet does not contain many specialized terms, making it difficult to use it in work on arbitrary specialized domains. While Wikipedia has recently been considered a valid alternative (Mihalcea, 2007), it is mainly focused on covering named entities and, strictly speaking, does not contain a formal widecoverage sense inventory (not even in disambiguation pages, which are often incomplete, especially in the lexicographic sense). In this paper we provide three main contributions: • We tackle the above issues by introducing a new framework based on the minimallysupervised acquisition of specialized glossaries for dozens of domains. 1411 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1411–1422, Jej</context>
</contexts>
<marker>Mihalcea, 2007</marker>
<rawString>Rada Mihalcea. 2007. Using Wikipedia for automatic Word Sense Disambiguation. In Proceedings of Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics, HLT-NAACL, pages 196– 203, Rochester, N.Y.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
<author>R T Beckwith</author>
<author>Christiane D Fellbaum</author>
<author>D Gross</author>
<author>K Miller</author>
</authors>
<title>WordNet: an online lexical database.</title>
<date>1990</date>
<journal>International Journal of Lexicography,</journal>
<volume>3</volume>
<issue>4</issue>
<contexts>
<context position="2534" citStr="Miller et al., 1990" startWordPosition="388" endWordPosition="391">07; Lapata and Keller, 2007), the development of domain resources (Magnini and Cavagli`a, 2000; Magnini et al., 2002), their application to WSD (Gliozzo et al., 2004), and the effective use of link analysis algorithms such as Personalized PageRank (Agirre et al., 2009; Navigli et al., 2011). More recently, semi-supervised approaches to domain WSD have been proposed which aim at decreasing the amount of supervision needed to carry out the task (Khapra et al., 2010). High-performance domain WSD, however, has been hampered by the widespread use of a generalpurpose sense inventory, i.e., WordNet (Miller et al., 1990; Fellbaum, 1998). Unfortunately WordNet does not contain many specialized terms, making it difficult to use it in work on arbitrary specialized domains. While Wikipedia has recently been considered a valid alternative (Mihalcea, 2007), it is mainly focused on covering named entities and, strictly speaking, does not contain a formal widecoverage sense inventory (not even in disambiguation pages, which are often incomplete, especially in the lexicographic sense). In this paper we provide three main contributions: • We tackle the above issues by introducing a new framework based on the minimally</context>
</contexts>
<marker>Miller, Beckwith, Fellbaum, Gross, Miller, 1990</marker>
<rawString>George A. Miller, R.T. Beckwith, Christiane D. Fellbaum, D. Gross, and K. Miller. 1990. WordNet: an online lexical database. International Journal of Lexicography, 3(4):235–244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saif Mohammad</author>
<author>Graeme Hirst</author>
</authors>
<title>Determining word sense dominance using a thesaurus.</title>
<date>2006</date>
<booktitle>In Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics, EACL</booktitle>
<pages>121--128</pages>
<location>Trento, Italy.</location>
<contexts>
<context position="4743" citStr="Mohammad and Hirst, 2006" startWordPosition="738" endWordPosition="741">be added at any time (either manually or automatically) to the multi-domain sense inventory. 2 Related Work Domain WSD has been the focus of much interest in the last few years. An important research direction identifies distributionally similar neighbors in raw text as cues for determining the predominant sense of a target word by means of a semantic similarity measure (McCarthy et al., 2004; Koeling et al., 2005; McCarthy et al., 2007). Other distributional methods include the use of a word-category cooccurrence matrix, where categories are coarse senses obtained from an existing thesaurus (Mohammad and Hirst, 2006), and synonym-based word occurrence counts (Lapata and Keller, 2007). Domain-informed methods have also been proposed which make use of domain labels as cues for disambiguation purposes (Gliozzo et al., 2004). Domain-driven approaches have been shown to obtain the best performance among the unsupervised alternatives (Strapparava et al., 2004), especially when domain kernels are coupled with a syntagmatic one (Gliozzo et al., 2005). However, their performance is typically lower than supervised systems. On the other hand, supervised systems fall short of carrying out high-performance WSD within </context>
</contexts>
<marker>Mohammad, Hirst, 2006</marker>
<rawString>Saif Mohammad and Graeme Hirst. 2006. Determining word sense dominance using a thesaurus. In Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics, EACL 2006, pages 121–128, Trento, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
<author>Paola Velardi</author>
</authors>
<title>Learning Word-Class Lattices for definition and hypernym extraction.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL 2010,</booktitle>
<pages>1318--1327</pages>
<location>Uppsala,</location>
<contexts>
<context position="7036" citStr="Navigli and Velardi, 2010" startWordPosition="1095" endWordPosition="1098">ach (Agirre et al., 2009) or with semantic model vectors acquired for many domains (Navigli et al., 2011). In this paper, we take domain WSD to the next level by proposing a new framework based on the minimally-supervised acquisition of large domain sense inventories thanks to which high performance can be attained on virtually any domain using unsupervised algorithms. Glossary acquisition approaches in the literature are mostly focused on pattern-based definition extraction (Fujii and Ishikawa, 2000; Hovy et al., 2003; Fahmi and Bouma, 2006, among others) and lattice-based supervised models (Navigli and Velardi, 2010) starting from an initial terminology, while we jointly bootstrap the lexicon and the definitions for several domains with minimal supervision and without the requirement of domain-specific corpora. To do so, we adapt bootstrapping techniques (Brin, 1998; Agichtein and Gravano, 2000; Pasca et al., 2006) to the novel task of domain glossary acquisition from the Web. Approaches to domain sense modeling have already been proposed which go beyond the WordNet sense inventory (Duan and Yates, 2010). Distinctive collocations are extracted from corpora and used as features to bootstrap a supervised WS</context>
<context position="16900" citStr="Navigli and Velardi, 2010" startWordPosition="2772" endWordPosition="2775"> in Section 4.1) to remove from Gd those glosses g whose score(g) &lt; 0. 3.1.5 Step 5: Seed selection for next iteration We now aim at selecting the new set of hypernymy relation seeds to be used to start the next iteration. We perform three substeps: a) Hypernym extraction: for each newly-acquired term/gloss pair (t, g) E Gd, we automatically extract a candidate hypernym h from the textual gloss g. To do this we use a simple unsupervised heuristic which just selects the first term in the gloss. More sophisticated, supervised approaches could have been used for hypernym extraction from glosses (Navigli and Velardi, 2010). However, note that, for the purposes of our glossary extraction task, it is not crucial to extract accurate hypernyms, but rather to harvest terms h which are very likely to occur in the glosses of t. We show an example of hypernym extraction for some terms in Table 1 (we report the term in column 1, the gloss in column 2 and the hypernyms extracted by our hypernym extraction technique in column 3). b) (Term, Hypernym)-ranking: we sort all the glosses in Gd by the number of seed terms found in each gloss. In the case of ties (i.e., glosses with the same number of seed terms), we further sort</context>
</contexts>
<marker>Navigli, Velardi, 2010</marker>
<rawString>Roberto Navigli and Paola Velardi. 2010. Learning Word-Class Lattices for definition and hypernym extraction. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL 2010, pages 1318–1327, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
<author>Stefano Faralli</author>
</authors>
<title>Aitor Soroa, Oier de Lacalle, and Eneko Agirre.</title>
<date>2011</date>
<booktitle>In Proceedings of the 20th ACM Conference on Information and Knowledge Management, CIKM 2011,</booktitle>
<pages>2317--2320</pages>
<location>Glasgow, UK.</location>
<marker>Navigli, Faralli, 2011</marker>
<rawString>Roberto Navigli, Stefano Faralli, Aitor Soroa, Oier de Lacalle, and Eneko Agirre. 2011. Two birds with one stone: Learning semantic models for text categorization and Word Sense Disambiguation. In Proceedings of the 20th ACM Conference on Information and Knowledge Management, CIKM 2011, pages 2317– 2320, Glasgow, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
</authors>
<title>Semi-automatic extension of large-scale linguistic knowledge bases.</title>
<date>2005</date>
<booktitle>In Proceedings of the 18th Internationa Florida AI Research Symposium Conference (FLAIRS),</booktitle>
<pages>15--17</pages>
<location>Clearwater Beach, Florida.</location>
<contexts>
<context position="6065" citStr="Navigli, 2005" startWordPosition="942" endWordPosition="943">he knowledge acquisition bottleneck can be relieved by means of domain adaptation (Chan and Ng, 2006; Chan and Ng, 2007; Agirre and de Lacalle, 2009) or by effectively injecting a generalpurpose corpus into a smaller domain-specific training set (Khapra et al., 2010). However, as mentioned above, most work on domain WSD uses WordNet as a sense inventory. But even if WordNet senses have been enriched with topically-distinctive words and concepts (Agirre and de Lacalle, 2004; Cuadros and Rigau, 2008), manually-developed domain labels (Magnini et al., 2002), and disambiguated semantic relations (Navigli, 2005), the main obstacle of being stuck with an open-ended fine-grained sense inventory remains. Recent results on the SPORTS and FINANCE gold standard dataset (Koeling et al., 2005) show that domain WSD can achieve accuracy in the 50- 60% ballpark when a state-of-the-art algorithm such as Personalized PageRank is paired with a distributional approach (Agirre et al., 2009) or with semantic model vectors acquired for many domains (Navigli et al., 2011). In this paper, we take domain WSD to the next level by proposing a new framework based on the minimally-supervised acquisition of large domain sense</context>
</contexts>
<marker>Navigli, 2005</marker>
<rawString>Roberto Navigli. 2005. Semi-automatic extension of large-scale linguistic knowledge bases. In Proceedings of the 18th Internationa Florida AI Research Symposium Conference (FLAIRS), 15–17 May 2005, pages 548–553, Clearwater Beach, Florida.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
</authors>
<title>Word Sense Disambiguation: A survey.</title>
<date>2009</date>
<journal>ACM Computing Surveys,</journal>
<volume>41</volume>
<issue>2</issue>
<contexts>
<context position="1643" citStr="Navigli, 2009" startWordPosition="248" endWordPosition="250">wspapers also provide a great deal of information focused on specific areas of knowledge. When it comes to automatic text understanding, then, it is crucial to take into account the domain specificity of a piece of text, so as to perform a focused and as-precise-as-possible analysis which, in its turn, can enable domain-aware applications such as question answering and information extraction. Domain knowledge also has the potential to improve open-text applications such as summarization (Ceylan et al., 2010) and machine translation (Foster et al., 2010). Research in Word Sense Disambiguation (Navigli, 2009, WSD), the task aimed at the automatic labeling of text with word senses, has been oriented towards domain text understanding for several years now. Many approaches have been devised, including the identification of domain-specific predominant senses (McCarthy et al., 2007; Lapata and Keller, 2007), the development of domain resources (Magnini and Cavagli`a, 2000; Magnini et al., 2002), their application to WSD (Gliozzo et al., 2004), and the effective use of link analysis algorithms such as Personalized PageRank (Agirre et al., 2009; Navigli et al., 2011). More recently, semi-supervised appr</context>
</contexts>
<marker>Navigli, 2009</marker>
<rawString>Roberto Navigli. 2009. Word Sense Disambiguation: A survey. ACM Computing Surveys, 41(2):1–69.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marius Pasca</author>
<author>Dekang Lin</author>
<author>Jeffrey Bigham</author>
<author>Andrei Lifchits</author>
<author>Alpa Jain</author>
</authors>
<title>Organizing and searching the world wide web of facts - step one: the one-million fact extraction challenge.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st National Conference on Artificial intelligence (AAAI</booktitle>
<pages>1400--1405</pages>
<location>Boston, MA.</location>
<contexts>
<context position="7340" citStr="Pasca et al., 2006" startWordPosition="1141" endWordPosition="1144">ained on virtually any domain using unsupervised algorithms. Glossary acquisition approaches in the literature are mostly focused on pattern-based definition extraction (Fujii and Ishikawa, 2000; Hovy et al., 2003; Fahmi and Bouma, 2006, among others) and lattice-based supervised models (Navigli and Velardi, 2010) starting from an initial terminology, while we jointly bootstrap the lexicon and the definitions for several domains with minimal supervision and without the requirement of domain-specific corpora. To do so, we adapt bootstrapping techniques (Brin, 1998; Agichtein and Gravano, 2000; Pasca et al., 2006) to the novel task of domain glossary acquisition from the Web. Approaches to domain sense modeling have already been proposed which go beyond the WordNet sense inventory (Duan and Yates, 2010). Distinctive collocations are extracted from corpora and used as features to bootstrap a supervised WSD system. Experiments in the biomedical domain show good performance, however only in-domain ambiguity is addressed. In contrast, our approach tackles cross1412 Figure 1: The bootstrapping process for glossary acquisition. domain ambiguity, by working with virtually any set of domains and minimizing the</context>
</contexts>
<marker>Pasca, Lin, Bigham, Lifchits, Jain, 2006</marker>
<rawString>Marius Pasca, Dekang Lin, Jeffrey Bigham, Andrei Lifchits, and Alpa Jain. 2006. Organizing and searching the world wide web of facts - step one: the one-million fact extraction challenge. In Proceedings of the 21st National Conference on Artificial intelligence (AAAI 2006), pages 1400–1405, Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antonio Sanfilippo</author>
<author>Stephen Tratz</author>
<author>Michelle Gregory</author>
</authors>
<title>Word domain disambiguation via word sense disambiguation.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume: Short Papers, NAACL</booktitle>
<pages>141--144</pages>
<location>New York, USA.</location>
<contexts>
<context position="20115" citStr="Sanfilippo et al., 2006" startWordPosition="3345" endWordPosition="3348">d when disambiguating a domain text dealing with ecosystem restoration. 3.2.1 Gloss-driven WSD We redefine the task of domain WSD as one of selecting the most suitable gloss, if one exists, for an input term t. For instance, consider the sentence: “He performed the restoration of heavily corrupted images”. An appropriate option for this occurrence would be the MEDIA sense of restoration in Table 2. Our gloss-driven WSD paradigm has the desirable property of automatically providing two levels of sense granularity: a domain, coarse-grained level, similar in spirit to Word Domain Disambiguation (Sanfilippo et al., 2006), in which the sense inventory of a term t is just the set of domains for which t is covered (e.g., BIOLOGY, GEOGRAPHY, ROYALTY, LAW, MEDIA, PHYSICS in the example of Table 2), and a fine-grained level, which requires the selection of the gloss which best describes the sense denoted by the given word occurrence. A second desirable property of our gloss-driven WSD paradigm is that it relies on a flexible framework, which allows for the bootstrapping of new domain glossaries or the expansion of existing ones. However, while these two properties – i.e., double level of granularity distinctions an</context>
</contexts>
<marker>Sanfilippo, Tratz, Gregory, 2006</marker>
<rawString>Antonio Sanfilippo, Stephen Tratz, and Michelle Gregory. 2006. Word domain disambiguation via word sense disambiguation. In Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume: Short Papers, NAACL 2006, pages 141–144, New York, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carlo Strapparava</author>
<author>Alfio Gliozzo</author>
<author>Claudio Giuliano</author>
</authors>
<title>Pattern abstraction and term similarity for Word Sense Disambiguation: IRST at Senseval-3.</title>
<date>2004</date>
<booktitle>In Proceedings of the 3rd International Workshop on the Evaluation of Systems for the Semantic Analysis of Text (SENSEVAL-3),</booktitle>
<pages>229--234</pages>
<location>Barcelona,</location>
<contexts>
<context position="5087" citStr="Strapparava et al., 2004" startWordPosition="786" endWordPosition="789"> semantic similarity measure (McCarthy et al., 2004; Koeling et al., 2005; McCarthy et al., 2007). Other distributional methods include the use of a word-category cooccurrence matrix, where categories are coarse senses obtained from an existing thesaurus (Mohammad and Hirst, 2006), and synonym-based word occurrence counts (Lapata and Keller, 2007). Domain-informed methods have also been proposed which make use of domain labels as cues for disambiguation purposes (Gliozzo et al., 2004). Domain-driven approaches have been shown to obtain the best performance among the unsupervised alternatives (Strapparava et al., 2004), especially when domain kernels are coupled with a syntagmatic one (Gliozzo et al., 2005). However, their performance is typically lower than supervised systems. On the other hand, supervised systems fall short of carrying out high-performance WSD within domains, the main reason being the need for retraining on each new specific knowledge domain. Nonetheless, the knowledge acquisition bottleneck can be relieved by means of domain adaptation (Chan and Ng, 2006; Chan and Ng, 2007; Agirre and de Lacalle, 2009) or by effectively injecting a generalpurpose corpus into a smaller domain-specific tra</context>
</contexts>
<marker>Strapparava, Gliozzo, Giuliano, 2004</marker>
<rawString>Carlo Strapparava, Alfio Gliozzo, and Claudio Giuliano. 2004. Pattern abstraction and term similarity for Word Sense Disambiguation: IRST at Senseval-3. In Proceedings of the 3rd International Workshop on the Evaluation of Systems for the Semantic Analysis of Text (SENSEVAL-3), pages 229–234, Barcelona, Spain.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
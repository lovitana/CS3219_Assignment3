<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000790">
<title confidence="0.972417">
User Demographics and Language in an Implicit Social Network
</title>
<author confidence="0.873646">
Katja Filippova
</author>
<affiliation confidence="0.785391">
Google Inc.
</affiliation>
<address confidence="0.8736055">
Brandschenkestr. 110
Z¨urich, 8004 Switzerland
</address>
<email confidence="0.999031">
katjaf@google.com
</email>
<sectionHeader confidence="0.995642" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999971777777778">
We consider the task of predicting the gender
of the YouTube1 users and contrast two infor-
mation sources: the comments they leave and
the social environment induced from the af-
filiation graph of users and videos. We prop-
agate gender information through the videos
and show that a user’s gender can be predicted
from her social environment with the accuracy
above 90%. We also show that the gender can
be predicted from language alone (89%). A
surprising result of our study is that the latter
predictions correlate more strongly with the
gender predominant in the user’s environment
than with the sex of the person as reported in
the profile. We also investigate how the two
views (linguistic and social) can be combined
and analyse how prediction accuracy changes
over different age groups.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999361">
Over the past decade the web has become more and
more social. The number of people having an iden-
tity on one of the Internet social networks (Face-
book2, Google+3, Twitter4, etc.) has been steadily
growing, many users communicate online on a daily
basis. Their interactions open new possibilities for
social sciences, and linguistics is no exception. For
example, with the development and growth of Web
2.0, it has become possible to get access to masses
of text data labeled with respect to different social
</bodyText>
<footnote confidence="0.9999725">
1www.youtube.com
2www.facebook.com
3www.plus.google.com
4www.twitter.com
</footnote>
<bodyText confidence="0.999576970588235">
parameters such as country, age, gender, profession
or religion. The study of language varieties between
groups separated by a certain social variable belongs
to the field of sociolinguistics which more generally
investigates the effect of society on how language is
used (Coulmas, 1998). Historically, sociolinguistics
is connected to dialectology whose focus has been
primarily on the phonetic aspect of the regional di-
alects but was later extended to sociolects (Cham-
bers &amp; Trudgill, 1998). A usual study would involve
sampling speakers from a population, interviewing
them and analyzing the linguistic items with respect
to social variables (Hudson, 1980).
The last decade has seen several studies inves-
tigating the relationship between the language and
the demographics of the users of blogs or Twitter
(see Sec. 2 for references). Most of those studies
used social network sites to collect labeled data–
samples of text together with the demographics vari-
able. However, they did not analyse how social en-
vironment affects language, although very similar
questions have been recently posed (but not yet an-
swered) by Ellist (2009). In our work we attempt to
address precisely this issue. In particular, we con-
sider the task of user gender prediction on YouTube
and contrast two information sources: (1) the com-
ments written by the user and (2) her social neigh-
borhood as defined by the bipartite user-video graph.
We use the comments to train a gender classifier on
a variety of linguistic features. We also introduce a
simple gender propagation procedure to predict per-
son’s gender from the user-video graph.
In what follows we will argue that although lan-
guage does provide us with signals indicative of the
</bodyText>
<page confidence="0.936232">
1478
</page>
<note confidence="0.775184">
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1478–1488, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.998738407407407">
user’s gender5 (as reported in the user’s profile), it
is in fact more indicative of a socially defined gen-
der. Leaving aside the debate on the intricate rela-
tionship between language and gender (see Eckert &amp;
McConnell-Ginet (2003) for a thorough discussion
of the subject), we simply demonstrate that a classi-
fier trained to predict the predominant gender in the
user’s social environment, as approximated by the
YouTube graph of users and videos, achieves higher
accuracy for both genders than the one trained to
predict the user’s inborn gender. We also investi-
gate ways of how the language-based and the so-
cial views can be combined to improve prediction
accuracy. Finally, we look at three age groups –
teenagers, people in their twenties and people over
thirty – and show that gender identity is more evi-
dent in the language of younger people but also that
there is a higher correlation between their inborn
gender and the predominant gender in their social
environment.
The paper is organized as follows: we first re-
view related work on the language of social media
and user demographics (Sec. 2) and elaborate on the
goals of our research (Sec. 3). Then we describe
our data (Sec. 4), introduce the demographics prop-
agation experiments (Sec. 5) and the experiments on
supervised learning gender from language (Sec. 6).
</bodyText>
<sectionHeader confidence="0.999493" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.9317111">
Previous studies on language and demographics
which looked at online data can be distinguished
with respect to their aims. (1) Studies coming from
the sociolinguistic community aim at empirically
confirming hypotheses, such as that female speakers
use more pronouns, or that males tend to use longer
words. (2) A standard goal of an NLP study is to
build an automatic system which accurately solves
a given task which in the case of demographics is
predicting user age, gender or country of origin. In
this section we start by reviewing the first kind of
studies, which are about data analysis and hypothe-
ses checking. These are relevant for our choice of
features. Then we briefly summarize a selection of
5Although it might be more correct to talk about the user’s
sex in place of gender (Eckert &amp; McConnell-Ginet, 2003), we
stick to the terminology adopted in previous NLP research on
gender prediction.
studies on demographics prediction to better situate
and motivate our approach.
</bodyText>
<subsectionHeader confidence="0.994077">
2.1 Language and demographics analysis
</subsectionHeader>
<bodyText confidence="0.99997915625">
Previous sociolinguistic studies mostly checked hy-
potheses formulated before the widespread use of
the Internet, such as that women use hedges more
often (Lakoff, 1973) or that men use more negations
(Mulac et al., 2000), or looked at specific words or
word classes. Newman et al. (2008) provide a com-
prehensive review of such work and a description of
the non-web corpora used therein. Some of those
hypotheses were confirmed by empirical evidence,
some not.
For example, Herring &amp; Paolillo (2006) analyse
gender- and genre-specific use of language in on-
line communication on a sample of about 130 blog
entries. Looking at a number of stylistic features
which had previously been claimed to be predic-
tive of gender (Argamon et al., 2003; Koppel et al.,
2004), such as personal pronouns, determiners and
other function words, they find no gender effect. Un-
like them, Kapidzic &amp; Herring (2011) analyse re-
cent chat communications and find that they are gen-
dered. Similarly, Huffaker &amp; Calvert (2005) inves-
tigate the question of identity of teenager bloggers
(e.g., age, gender, sexuality) and find language fea-
tures indicative of gender (e.g., use of emoticons by
males). Burger &amp; Henderson (2006) consider the
relationship between different linguistic (e.g., text
length, use of capital and punctuation letters) and
non-linguistic (e.g., interests, mood) features and
blogger’s age and location. They find that many fea-
tures correlate with the age and run an experiment
with the goal of predicting whether the blog author
is over 18.
</bodyText>
<subsectionHeader confidence="0.99958">
2.2 Demographics prediction from language
</subsectionHeader>
<bodyText confidence="0.999968">
The studies we review here used supervised ma-
chine learning to obtain models for predicting gen-
der or age. Other demographic attributes, like lo-
cation, ethnicity, or educational level, have also
been predicted automatically (Gillick, 2010; Rao &amp;
Yarowsky, 2011, inter alia). Also, generative ap-
proaches have been applied to discover associations
between language and demographics of social media
users (Eisenstein et al., 2011, inter alia) but these are
of less direct relevance for the present work. For su-
</bodyText>
<page confidence="0.993114">
1479
</page>
<bodyText confidence="0.999962934782609">
pervised approaches, major feature sources are the
text the user has written and also her profile which
may list the name, interests, friends, etc. There have
also been studies which did not look at the language
at all but considered the social environment only.
For example, MacKinnon &amp; Warren (2006) aim at
predicting the age and the location of the LiveJour-
nal6 users. What they found is that there is a remark-
able correlation between the age and the location of
the user and those of her friends, although there are
interesting exceptions.
Burger et al. (2011) train a gender classifier on
tweets with word and character-based ngram fea-
tures achieving accuracy of 75.5%. Adding the full
name feature alone gives a boost to 89.1%, fur-
ther features like self-written description and screen
name further help to get 92%. Also, a self-training
method exploring unlabeled data is described but its
performance is worse. Other kinds of sociolinguistic
features and a different classifier have been applied
to gender prediction on tweets by Rao &amp; Yarowsky
(2010).
Nowson &amp; Oberlander (2006) achieve 92% accu-
racy on the gender prediction task using ngram fea-
tures only. Their corpus consist of 1,400/450 posts
written by 47 females and 24 males, respectively.
However, the ngram features were preselected based
on whether they occurred with significant relative
frequency in the language of one gender over the
other. Since the complete dataset was used to pre-
select features, the results are inconclusive.
Yan &amp; Yan (2006) train a Naive Bayes classifier
to predict the gender of a blog entry author. In to-
tal they looked at 75,000 individual blog entries au-
thored by 3,000 bloggers, all of them posted their
genders on the profile page. They measure precision
and recall w.r.t. the minority class (males) and get
the best f-measure of 0.64 (precision and recall are
65% and 71%, respectively).
Rosenthal &amp; McKeown (2011) predict the age of
a blogger, most features they use are extracted from
the blog posts, other features include blogger’s inter-
ests, the number of friends, the usual time of post-
ing, etc. Similarly to Schler et al. (2006), they run
a classification experiment with three age classes re-
moving intermediate ages and use the majority-class
</bodyText>
<footnote confidence="0.934454">
6www.livejournal.com
</footnote>
<bodyText confidence="0.999750636363636">
baseline for comparison. In their other experiment
they experiment with a binary classifier for age dis-
tinguishing between the pre- and post-social media
generations and using the years from 1975-1988 as a
boundary. The prediction accuracy increases as later
years are taken.
Interestingly, it has been shown that demograph-
ics can be predicted in more restricted genres than
the personal blog or tweets and from text frag-
ments even shorter than tweets (Otterbacher, 2010;
Popescu &amp; Grefenstette, 2010).
</bodyText>
<sectionHeader confidence="0.970412" genericHeader="method">
3 Motivation for the present study
</sectionHeader>
<bodyText confidence="0.999977147058823">
Similarly to previous NLP studies, our starting goal
is to predict the self-reported user gender. The first
novelty of our research is that in doing so we con-
trast two sources of information: the user’s social
environment and the text she has written. Indeed,
a topic which has not yet been investigated much
in the reviewed studies on language and user demo-
graphics is the relationship between the language of
the user and her social environment. The data analy-
sis studies (Sec. 2.1) verified hypotheses concerning
the dependency between a language trait (e.g., aver-
age sentence length) and a demographic parameter
(e.g., gender). The demographics prediction studies
(Sec. 2.2) mostly relied on language and user pro-
file features and considered users in isolation. An
exception to this is Garera &amp; Yarowsky (2009) who
showed that, for gender prediction in a dialogue, it
helps to know the interlocutor’s gender. However,
we aim at investigating the impact of the social en-
vironment in a much broader sense than the immedi-
ate interlocutors and in a much broader context than
a conversation.
Language is a social phenomenon, and it is this
fact that motivates all the sociolinguistic research.
Many if not most language traits are not hard-wired
or inborn but can be explained by looking at who
the person interacts most with. Since every lan-
guage speaker can be seen as a member of multiple
overlapping communities (e.g., computer scientists,
French, males, runners), the language of the person
may reflect her membership in different communi-
ties to various degrees. Repeated interactions with
other language speakers influence the way the per-
son speaks (Baxter et al., 2006; Bybee, 2010), and
</bodyText>
<page confidence="0.948688">
1480
</page>
<bodyText confidence="0.99995052">
the influence is observable on all the levels of the
language representation (Croft, 2000). For exam-
ple, it has been shown that the more a person is in-
tegrated in a certain community and the tighter the
ties of the social network are, the more prominent
are the representative traits of that community in
the language of the person (Milroy &amp; Milroy, 1992;
Labov, 1994). In our study we adopt a similar view
and analyse the implications it has for gender pre-
diction. Given its social nature, does the language
reflect the norms of a community the user belongs
to or the actual value of a demographic variable?
In our study we address this issue with a particular
modeling technique: we assume that the observed
online behavior adequately reflects the offline life of
a user (more on this in Sec. 4 and 5) and based on
this assumption make inferences about the user’s so-
cial environment. We use language-based features
and a supervised approach to gender prediction to
analyse the relationship between the language and
the variable to be predicted. To our knowledge, we
are the first to question whether it is really the in-
born gender that language-based classifiers learn to
predict. More concrete questions we are going to
suggest answers to are as follows:
</bodyText>
<listItem confidence="0.610129">
1. Previous studies which looked at online data re-
lied on self-reported demographics. The pro-
file data are known to be noisy, although it is
</listItem>
<bodyText confidence="0.976998714285714">
hard to estimate the proportion of false profiles
(Burger et al., 2011). Concerning the predic-
tion task, how can we make use of what we
know about the user’s social environment to re-
duce the effect of noise? How can we bene-
fit from the language samples from the users
whose gender we do not know at all?
</bodyText>
<listItem confidence="0.9964426">
2. When analyzing the language of a user, how
much are its gender-specific traits due to the
user’s inborn gender and to which extent can
they be explained by her social environment?
Using our modeling technique and a language-
based gender classifier, how is its performance
affected by what we know about the online so-
cial environment of the user?
3. Concerning gender predictions across different
age groups, how does classifier performance
</listItem>
<bodyText confidence="0.9914167">
change? Judging from the online communica-
tion, do teenagers signal their gender identity
more than older people? In terms of classifier
accuracy, is it easier to predict a teenager’s gen-
der than the gender of an adult?
The final novelty of our study is that we are the
first to demonstrate how YouTube can be used as
a valuable resource for sociolinguistic research. In
the following section we highlight the points which
make YouTube interesting and unique.
</bodyText>
<sectionHeader confidence="0.995926" genericHeader="method">
4 Data
</sectionHeader>
<bodyText confidence="0.999760096774193">
Most social networks strive to protect user privacy
and by default do not expose profile information or
reveal user activity (e.g., posts, comments, votes,
etc.). To obtain data for our experiments we use
YouTube, a video sharing site. Most of the YouTube
registered users list their gender, age and location on
their profile pages which, like their comments, are
publicly available. YouTube is an interesting domain
for sociolinguistic research for several reasons:
High diversity: it is not restricted to any particular
topic (e.g., like political blogs) but covers a vast va-
riety of topics attracting a very broad audience, from
children interested in cartoons to academics watch-
ing lectures on philosophy7.
Spontaneous speech: the user comments are ar-
guably more spontaneous than blogs which are more
likely to conform to the norms of written language.
At the same time they are less restricted than tweets
written under the length constraint which encour-
ages highly compressed utterances.
Data availability: all the comments are publicly
available, so we have do not get a biased subset of
what a user has written for the public. Moreover,
we observe users’ interactions in different environ-
ments because every video targets particular groups
of people who may share origin (e.g., elections in
Greece) or possession (e.g., how to unlock iPhone)
or any other property. Some videos attract a well-
defined group of people (e.g., the family of a new-
born child), whereas some videos appeal to a very
broad audience (e.g., a kitten video).
</bodyText>
<footnote confidence="0.98491">
7For more information and statistics see the official
YouTube demographics on http://www.youtube.com/
yt/advertise/affinities.html.
</footnote>
<page confidence="0.949759">
1481
</page>
<table confidence="0.5115895">
female male nn
26% 62% 12%
</table>
<tableCaption confidence="0.998783">
Table 1: Gender distribution for the extracted 6.9M users.
</tableCaption>
<bodyText confidence="0.999862861111111">
From the users, videos and the comment relation-
ship we build an affiliation graph (Easley &amp; Klein-
berg, 2010): a user and a video are connected if the
user commented on the video (Fig. 1(a)). Our graph
is unweighted although the number of comments
could be used to weight edges. The co-comment
graph is a stricter version of a more popular co-view
graph used in, e.g., video recommendation studies
(Baluja et al., 2008, inter alia).
We obtained a random sample of videos by con-
sidering all the videos whose YouTube ID has a spe-
cific prefix8. From those, we collected the profiles of
the users whose commented on the videos. In total,
we extracted about 6.9M profiles of users who have
written at least 20 comments, not more than 30 com-
ments were collected for every user. The threshold
on the minimum number of comments is set in or-
der to reduce the proportion of users who have used
YouTube only a few times and possibly followed
the suggestions of the site in their video choice.
The users’ gender distributions is presented in Table
1. Although females, in particular teenagers, have
been reported to be more likely to blog than males
(Herring et al., 2004), males are predominant in our
dataset. A random sample from a pool of users with-
out the 20-comments threshold showed that there are
more male commenters overall, although the differ-
ence is less remarkable for teenagers: 58% of the
teenagers with known gender are male as opposed
to 74% and 79% for the age groups 20-29 and 30+.
Teenagers are also more numerous accounting for
about 35% in our data.
Although we did not filter users based on their lo-
cation or mother tongue as many users comment in
multiple languages, the comment set is overwhelm-
ingly English.
</bodyText>
<footnote confidence="0.9912935">
8The YouTube API (http://code.google.com/
apis/youtube/getting_started.html) can be used
to retrieve user profiles and video metadata as well as the com-
ments.
</footnote>
<sectionHeader confidence="0.937194" genericHeader="method">
5 Gender propagation
</sectionHeader>
<bodyText confidence="0.998351625">
We first consider the user’s social environment to see
whether there is any correlation between the gender
of a user and the gender distribution in her vicinity,
independent of the language. We use a simple prop-
agation procedure to reach the closest neighbors of
a user, that is, other users “affiliated” with the same
videos. Specifically, we perform the following two
steps:
</bodyText>
<listItem confidence="0.691814823529412">
1. We send the gender information (female, male
or unknown) to all the videos the user has com-
mented on. This way for every video we obtain
a multinomial distribution over three classes
(see Fig. 1(b)).
2. We send the gender distributions from every
video back to all the users who commented on
it and average over all the videos the user is
connected with (see Fig. 1(c)). However, in do-
ing so we adjust the distribution for every user
so that her own demographics is excluded. This
way we have a fair setting where the original
gender of the user is never included in what
she gets back from the connected videos. Thus,
the gender of a user contributes to the vicinity
distributions of all the neighbors but not to her
own final gender distribution.
</listItem>
<bodyText confidence="0.999955157894737">
In line with our motivation and modeling tech-
nique, we chose such a simple method (and not,
say, classification) in order to approximate the of-
fline encounters of the user: does she more often
meet women or men? The way we think of the
videos is that they correspond to places (e.g., a cin-
ema, a cosmetic shop, a pub) visited by the user
where she is unintentionally or deliberately exposed
to how other speakers use the language. Similar to
Baxter et al. (2006), we assume that these encoun-
ters influence the way the person speaks. Note that
if the user’s gender has no influence on her choice
of videos, then, on average, we would expect every
video to have the same distribution as in our data
overall: 62% male, 26% female and 12% unknown
(Table 1).
To obtain a single gender prediction from the
propagated distribution, for a given user we select
the gender class (female or male) which got more
</bodyText>
<page confidence="0.979421">
1482
</page>
<figure confidence="0.978625">
(a) Color represents gender information:
blue=male, red=female, grey=unknown.
(b) Propagating gender from users to
videos.
(c) Propagating gender distribution from
videos to users.
</figure>
<figureCaption confidence="0.999979">
Figure 1: Affiliation graph of users (circles) and videos (rectangles).
</figureCaption>
<bodyText confidence="0.999019029411765">
of the distribution mass. The exact procedure is as
follows: given user u connected with videos Vu =
{vi, ..., vm}, there are m gender distributions sent
to u: PV (u) = {p(glvi) : 1 G i G m, g E
{f, m, n}}. A single distribution is obtained from
PV (u): Ag�u) = Ei p(gIvi)/m.
To address the skewness in the data, i.e., the fact
that 70% of our users (62/(26 + 62)) with known
gender are male, we select the female gender if (a) it
got more than zero mass and at least as much mass
as male: p(f) &gt; 0 n p(f) &gt; p(m), or (b) it got at
least τ of the mass: p(f) &gt; τ. We set τ = 0.26 ini-
tially because it corresponds to the expected propor-
tion of females (26%) but further experimented with
different τ values in the range of 0.25-0.4. We ob-
tained best accuracy and f-measures with the thresh-
old of 0.33, the difference in accuracy from the ini-
tial threshold of 0.26 being less than 2%. The fact
that the optimal τ value is different from the overall
proportion of females (26%) is not surprising given
that we aggregate per video distributions and not raw
user counts.
The predictions obtained with the described prop-
agation method are remarkably accurate, reaching
90% accuracy (Table 2). The baseline of assigning
all the users the majority class (all male) provides us
with the accuracy of 70% – the proportion of males
among the users with known gender.
Although the purpose of this section is not to
present a gender prediction method, we find it worth
emphasizing that 90% accuracy is remarkable given
that we only look at the immediate user vicinity.
In the following section we are going to investigate
how this social view on demographics can help us in
</bodyText>
<equation confidence="0.981179333333333">
Acc% P% R% F1
Baseline
90 - - -
</equation>
<bodyText confidence="0.74313">
- 84.3 80.8 83
- 92.2 93.8 93
</bodyText>
<tableCaption confidence="0.986028">
Table 2: Precision and recall for propagated gender.
</tableCaption>
<bodyText confidence="0.622852">
predicting gender from language.
</bodyText>
<sectionHeader confidence="0.97585" genericHeader="method">
6 Supervised learning of gender
</sectionHeader>
<bodyText confidence="0.999957">
In this section we start by describing our first gen-
der prediction experiment and several extensions to
it and then turn to the results.
</bodyText>
<subsectionHeader confidence="0.917342">
6.1 Experiments
</subsectionHeader>
<bodyText confidence="0.999986888888889">
Similar to previous studies on demographics predic-
tion, we start with a supervised approach and only
look at the text (comments) written by the user. We
do not rely on any information from the social en-
vironment of the user and do not use any features
extracted from the user profile, like name, which
would make the gender prediction task consider-
ably easier (Burger et al., 2011). Finally, we do
not extract any features from the videos the user has
commented on because our goal here is to explore
the language as a sole source of information. Here
we simply want to investigate the extent to which
the language of the user is indicative of her gender
which is found in the profile and which, ignoring the
noise, corresponds to the inborn gender.
In our experiments we use a distributed imple-
mentation of the maximum entropy learner (Berger
et al., 1996; McDonald et al., 2010) which outputs
</bodyText>
<figure confidence="0.7167365">
70 - - -
all
fem
male
</figure>
<page confidence="0.955878">
1483
</page>
<bodyText confidence="0.999421101449275">
a distribution over the classes, the final prediction is
the class with the greater probability. We take 80%
of the users for training and generate a training in-
stance for every user who made her gender visible on
the profile page (4.9M). The remaining 20% of the
data are used for testing (1.2M). We use the follow-
ing three groups of features: (1) character-based:
average comment length, ratio of capital letters to
the total number of letters, ratio of punctuation to the
total number of characters; (2) token-based: average
comment length in words, ratio of unique words to
the total tokens, lowercase unigrams with total count
over all the comments (10K most frequent unigrams
were used, the frequencies were computed on a sep-
arate comment set), use of pronouns, determiners,
function words; (3) sentence-based: average com-
ment length in sentences, average sentence length in
words.
Enhancing the training set. The first question we
consider is how the affiliation graph and propagated
gender can be used to enhance our data for the super-
vised experiments. One possibility would be to train
a classifier on a refined set of users by eliminating
all those whose reported gender did not match the
gender predicted by the neighborhood. This would
presumably reduce the amount of noise by discard-
ing the users who intentionally provided false infor-
mation on their profiles. Another possibility would
be to extend the training set with the users who did
not make their gender visible to the public but whose
gender we can predict from their vicinity. The idea
here is similar to co-training where one has two in-
dependent views on the data. In this case a social
graph view would be combined with the language-
based view.
Profile vs. vicinity gender prediction. The next
question posed in the motivation section is as fol-
lows: Does the fact that language is a social phe-
nomenon and that it is being shaped by the social
environment of the speaker impact our gender clas-
sifier? If there are truly gender-specific language
traits and they are reflected in our features, then
we should not observe any significant difference be-
tween the prediction results on the users whose gen-
der matches the gender propagated from the vicinity
and those whose gender does not match. A contrary
hypothesis would be that what the classifier actually
learns to predict is not as much the inborn but a so-
cial gender. In this case, the classifier trained on the
propagated gender labels should be more accurate
than the one trained on the labels extracted from the
profiles.
To address these questions we contrast two classi-
fiers: (1) the one described in the beginning of the
section which is trained on the gender labels col-
lected from the user profiles; (2) a classifier trained
on the vicinity gender, that is the dominating gender
of the environment of a speaker as obtained with the
procedure described in Section 5.
Age groups and gender prediction. Finally, we
look at how gender predictions change with age and
train three age-specific models to predict gender for
teenagers (13-19), people in their twenties (20-29)
and people over thirty (30+), the age is also ex-
tracted from the profiles. These groups are identified
in order to check whether teenagers tend to signal-
ize their gender identity more than older people, a
hypothesis investigated earlier on a sample of blog
posts (Huffaker &amp; Calvert, 2005).
</bodyText>
<subsectionHeader confidence="0.941404">
6.2 Results
</subsectionHeader>
<bodyText confidence="0.999945">
We report the results of the supervised experiments
for all the settings described above. As an estimate
of the lowest bound we also give the results of the
majority class baseline (all male) which guarantees
70% accuracy. For the supervised classifiers we re-
port accuracy and per-gender precision, recall and f-
measure. Table 3 presents the results for the starting
classifier trained to predict profile gender.
</bodyText>
<table confidence="0.9988604">
Acc% P% R% F1 Total
Baseline 70 - - - 619K
all 89 - - - 619K
fem - 83 78 80 182K
male - 91 94 93 437K
</table>
<tableCaption confidence="0.999947">
Table 3: Results on the test set.
</tableCaption>
<bodyText confidence="0.999948142857143">
In order to investigate the relationship between the
social environment of a person, her gender and the
language, we split the users from the test set into
two groups: those whose profile gender matched the
gender propagated from the vicinity and those for
whom there was a mismatch. Thus Table 4 presents
the same results as Table 3 but separated for these
</bodyText>
<page confidence="0.989258">
1484
</page>
<bodyText confidence="0.852398">
two groups of users. It also gives user counts w.r.t.
the profile gender.
</bodyText>
<table confidence="0.999434285714286">
Acc% P% R% F1 Total
94 - - - 557K
- 89 87 88 147K
- 95 96 96 410K
47 - - - 62K
- 54 39 45 35K
- 42 56 48 27K
</table>
<tableCaption confidence="0.9958375">
Table 4: Results for users whose profile gender
matches/differs from the vicinity gender.
</tableCaption>
<bodyText confidence="0.999700214285714">
Enhanced training set. In the next experiment we
refined the training set by removing all the users
whose vicinity gender did not match the gender re-
ported in the profile. The evaluation was done on
the unmodified set (Table 5). The predictions made
by the model trained on a refined set of users turned
out to be slightly less accurate than those made by
the model trained on the full training set (Table 3).
The refined model performed slightly (&lt; 1%) bet-
ter than the starting one on the users whose vicinity
and the profile genders matched but got very poor
results on the users with a gender mismatch, the ac-
curacy being as low as 37%. The accuracy of the
starting model on those users is 47% (Table 4).
</bodyText>
<tableCaption confidence="0.988722">
Table 5: Results of the models trained on the refined
training set.
</tableCaption>
<bodyText confidence="0.999754586206897">
In another experiment we extended the training
data with the users whose gender was unknown but
was predicted with the propagation method. How-
ever, a larger training set makes a difference only if
there is a substantial performance gain over the in-
creasing size of the training set. We observed only a
minor gain in performance (&lt; 1%) when the train-
ing data size was increased by an order of magni-
tude. Given that, it is not surprising that adding 12%
did not affect the results.
Language, the vicinity and the profile genders.
The gap in accuracies of predictions for the two user
groups in Table 4 is remarkable: 47% vs. 94%. If
we extrapolate what we observe in the affiliation
graph to other online and offline life, then this re-
sult may suggest that gender traits are more promi-
nent in the language of people spending more time
with the people of their gender than in that of the
people who spend more time with the people of the
opposite gender. Given the remarkable difference,
a further question arises whether the classifier actu-
ally learns to predict a kind of socially rather than the
profile gender. To investigate this, we looked at the
results of the model which knew nothing about the
profile gender but was trained to predict the vicinity
gender instead (Table 6). This model relied on the
exact same set of features but both for training and
testing it used the gender labels obtained from the
propagation procedure described in Section 5.
</bodyText>
<table confidence="0.5986905">
Acc% P% R% F1
all
- 86 80 83
- 92 95 94
</table>
<tableCaption confidence="0.990379">
Table 6: Results of the models trained and tested on the
propagated gender.
</tableCaption>
<bodyText confidence="0.9998715">
According to all the evaluation metrics, for both gen-
ders the performance of the classifier trained and
tested on the propagated gender is higher (cf. Ta-
ble 3): the differences in f-measure for female and
male are four and two points respectively, both sta-
tistically significant. This indicates that it is the pre-
dominant environment gender that a language-based
classifier is better at learning rather than the inborn
gender.
Predictions across age groups. Finally, to ad-
dress the question of whether gender differences are
more prominent and thus easer to identify in the lan-
guage of younger people, we looked at the accu-
racy of gender predictions across three age groups.
Table 7 summarizes the results and gives the accu-
racy of the all male baseline as well as of the prop-
agation procedure (Prop-acc). Although the over-
all accuracy over the three groups does not degrade
much, from 89% to 87%, both precision and recall
do decrease significantly for females. This is not
</bodyText>
<figure confidence="0.995756230769231">
all (same)
fem (same)
male (same)
all (diff)
fem (diff)
male (diff)
Acc% P% R% F1
all 88 - - -
fem - 83 76 79
male - 90 94 92
fem
male
91 - - -
</figure>
<page confidence="0.980401">
1485
</page>
<bodyText confidence="0.999314444444445">
directly reflected in the accuracy because the num-
ber of females drops dramatically from 42% among
teenagers to 26% and then 21% in the latter groups.
For a comparison, the accuracy of the propagated
gender (Prop-acc) also decreases from younger to
older age groups although it is slightly higher than
that of language-based predictions. One conclusion
we can make at this point is that a teenager’s gen-
der is easier to predict from the language which is
in line with the hypothesis that younger people sig-
nalize their gender identities more than older people.
Another observation is that, as the person gets older,
we can be less sure about her gender by looking at
her social environment. This in turn might be an
explanation of why there are less gender signals in
the language of a person: the environment becomes
more mixed, and the influence of both genders be-
comes more balanced.
</bodyText>
<table confidence="0.9985632">
13-19 20-29 30+ Overall
Base-acc% 58 74 79 70
Prop-acc% 91 90 88 90
Accuracy% 89 89 87 89
Fem-P% 87 81 74 83
Fem-R% 87 76 62 78
Fem-F1 87 78 68 80
Male-P% 90 92 90 91
Male-R% 90 94 94 94
Male-F1 90 93 92 93
</table>
<tableCaption confidence="0.999465">
Table 7: Results across the age groups.
</tableCaption>
<sectionHeader confidence="0.998171" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999426444444444">
In our study we addressed the gender prediction task
from two perspectives: (1) the social one where we
looked at an affiliation graph of users and videos and
propagated gender information between users, and
(2) the language one where we trained a classifier
on features which have been claimed to be indica-
tive of gender. We demonstrated that both perspec-
tives provide us with comparably accurate predic-
tions (around 90%) but that they are far from be-
ing independent. We also investigated a few ways of
how the performance of a language-based classifier
can be enhanced by the social aspect, compared the
accuracy of predictions across different age groups
and found support for hypotheses made in earlier so-
ciolinguistic studies.
We are not the first to predict gender from lan-
guage features with online data. However, to our
knowledge, we are the first to contrast the two views,
social and language-based, using online data and to
question whether there is a clear understanding of
what gender classifiers actually learn to predict from
language. Our results indicate that from the standard
language cues we are better at predicting a social
gender, that is the gender defined by the environment
of a person, rather than the inborn gender.
The theoretical significance of this result is that
it provides support for the usage-based view on lan-
guage (Bybee, 2010), namely that the person’s lan-
guage is largely shaped by the interactions with her
social environment. On the practical side, it may
have implications for targeted advertisement as it en-
riches the understanding of what gender classifiers
predict.
Acknowledgements: I am thankful to Enrique Al-
fonseca, Keith Hall and the EMNLP reviewers for
their feedback.
</bodyText>
<sectionHeader confidence="0.997995" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.996425">
Argamon, S., M. Koppel, J. Fine &amp; A. R. Shimoni
(2003). Gender, genre, and writing style in formal
written texts. Text, 23(3).
Baluja, S., R. Seth, D. Sivakumar, Y. Jing, J. Yag-
nik, S. Kumar, D. Ravichandran &amp; M. Aly (2008).
Video suggestion and discovery for YouTube:
Taking random walks through the view graph. In
Proc. of WWW-08, pp. 895–904.
Baxter, G. J., R. A. Blythe, W. Croft &amp; A. J. McKane
(2006). Utterance selection model of language
change. Physical Review, E73.046118.
Berger, A., S. A. Della Pietra &amp; V. J. Della Pietra
(1996). A maximum entropy approach to natural
language processing. Computational Linguistics,
22(1):39–71.
Burger, J. D., J. Henderson, G. Kim &amp; G. Zarrella
(2011). Discriminating gender on Twitter. In
Proc. of EMNLP-11, pp. 1301–1309.
</reference>
<page confidence="0.929314">
1486
</page>
<reference confidence="0.998748037974684">
Burger, J. D. &amp; J. C. Henderson (2006). An ex-
ploration of observable features related to blogger
age. In Proceedings of the AAAI Spring Sympo-
sium on Computational Approaches forAnalyzing
Weblogs, Stanford, CA, 27-29 March 2006, pp.
15–20.
Bybee, J. (2010). Language, Usage and Cognition.
Cambridge University Press.
Chambers, J. &amp; P. Trudgill (1998). Dialectology.
Cambridge University Press.
Coulmas, F. (Ed.) (1998). The Handbook of Soci-
olinguistics. Blackwell.
Croft, W. (2000). Explaining Language Change: An
Evolutionary Approach. London: Longman.
Easley, D. &amp; J. Kleinberg (2010). Network, crowds,
and markets: Reasoning about a highly connected
world. Cambridge University Press.
Eckert, P. &amp; S. McConnell-Ginet (2003). Language
and Gender. Cambridge University Press.
Eisenstein, J., N. A. Smith &amp; E. P. Xing (2011). Dis-
covering sociolinguistic associations with struc-
tured sparsity. In Proc. of ACL-11, pp. 1365–
1374.
Ellist, D. (2009). Social (distributed) language mod-
eling, clustering and dialectometry. In Proc. of
TextGraphs atACL-IJCNLP-09, pp. 1–4.
Garera, N. &amp; D. Yarowsky (2009). Modeling latent
biographic attributes in conversational genres. In
Proc. of ACL-IJCNLP-09, pp. 710–718.
Gillick, D. (2010). Can conversational word usage
be used to predict speaker demographics? In Pro-
ceedings of Interspeech, Makuhari, Japan, 26-30
September 2010.
Herring, S. C. &amp; J. C. Paolillo (2006). Gender and
genre variation in weblogs. Journal of Sociolin-
guistics, 10(4):439–459.
Herring, S. C., L. A. Scheidt, S. Bonus &amp; E. Wright
(2004). Bridging the gap: A genre analysis of
weblogs. In HICSS-04.
Hudson, R. A. (1980). Sociolinguistics. Cambridge
University Press.
Huffaker, D. A. &amp; S. L. Calvert (2005). Gen-
der, identity and language use in teenager blogs.
Journal of Computer-Mediated Communication,
10(2).
Kapidzic, S. &amp; S. C. Herring (2011). Gen-
der, communication, and self-presentation in teen
chatrooms revisited: Have patterns changed?
Journal of Computer-Mediated Communication,
17(1):39–59.
Koppel, M., S. Argamon &amp; A. R. Shimoni (2004).
Automatically categorizing written text by au-
thor gender. Literary and Linguistic Computing,
17(4):401–412.
Labov, W. (1994). Principles of Linguistic Change:
Internal Factors. Blackwell.
Lakoff, R. (1973). Language and woman’s place.
Language in Society, 2(1):45–80.
MacKinnon, I. &amp; R. Warren (2006). Age and geo-
graphic inferences of the LiveJournal social net-
work. In Statistical Network Analysis: Models,
Issues, and New Directions Workshop at ICML-
2006, Pittsburgh, PA, 29 June, 2006.
McDonald, R., K. Hall &amp; G. Mann (2010). Dis-
tributed training strategies for the structured per-
ceptron. In Proc. of NAACL-HLT-10, pp. 456–
464.
Milroy, L. &amp; J. Milroy (1992). Social network and
social class: Toward an integrated sociolinguistic
model. Language in Society, 21:1–26.
Mulac, A., D. R. Seibold &amp; J. R. Farris (2000). Fe-
male and male managers’ and professionals’ crit-
icism giving: Differences in language use and ef-
fects. Journal of Language and Social Psychol-
ogy, 19(4):389–415.
Newman, M. L., C. J. Groom, L. D. Handelman &amp;
J. W. Pennebaker (2008). Gender differences in
language use: An analysis of 14,000 text samples.
Discourse Processes, 45:211–236.
</reference>
<page confidence="0.844176">
1487
</page>
<reference confidence="0.999762060606061">
Nowson, S. &amp; J. Oberlander (2006). The identity of
bloggers: Openness and gender in personal we-
blogs. In Proceedings of the AAAI Spring Sympo-
sium on Computational Approaches forAnalyzing
Weblogs, Stanford, CA, 27-29 March 2006, pp.
163–167.
Otterbacher, J. (2010). Inferring gender of movie
reviewers: Exploiting writing style, content and
metadata. In Proc. of CIKM-10.
Popescu, A. &amp; G. Grefenstette (2010). Mining user
home location and gender from Flickr tags. In
Proc. of ICWSM-10, pp. 1873–1876.
Rao, D. &amp; D. Yarowsky (2010). Detecting latent
user properties in social media. In Proc. of the
NIPS MLSN Workshop.
Rao, D. &amp; D. Yarowsky (2011). Typed graph models
for semi-supervised learning of name ethnicity. In
Proc. of ACL-11, pp. 514–518.
Rosenthal, S. &amp; K. McKeown (2011). Age predic-
tion in blogs: A study of style, content, and on-
line behavior in pre- and post-social media gener-
ations. In Proc. ofACL-11, pp. 763–772.
Schler, J., M. Koppel, S. Argamon &amp; J. Pennebaker
(2006). Effects of age and gender on blogging.
In Proceedings of the AAAI Spring Symposium
on Computational Approaches for Analyzing We-
blogs, Stanford, CA, 27-29 March 2006, pp. 199–
205.
Yan, X. &amp; L. Yan (2006). Gender classifica-
tion of weblogs authors. In Proceedings of the
AAAI Spring Symposium on Computational Ap-
proaches for Analyzing Weblogs, Stanford, CA,
27-29 March 2006, pp. 228–230.
</reference>
<page confidence="0.994054">
1488
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.158493">
<title confidence="0.993072">User Demographics and Language in an Implicit Social Network</title>
<author confidence="0.443977">Katja</author>
<affiliation confidence="0.3672625">Google Brandschenkestr.</affiliation>
<address confidence="0.952154">Z¨urich, 8004</address>
<email confidence="0.999661">katjaf@google.com</email>
<abstract confidence="0.999673157894737">We consider the task of predicting the gender the users and contrast two information sources: the comments they leave and the social environment induced from the affiliation graph of users and videos. We propagate gender information through the videos and show that a user’s gender can be predicted from her social environment with the accuracy above 90%. We also show that the gender can be predicted from language alone (89%). A surprising result of our study is that the latter predictions correlate more strongly with the gender predominant in the user’s environment than with the sex of the person as reported in the profile. We also investigate how the two views (linguistic and social) can be combined and analyse how prediction accuracy changes over different age groups.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Argamon</author>
<author>M Koppel</author>
<author>J Fine</author>
<author>A R Shimoni</author>
</authors>
<title>Gender, genre, and writing style in formal written texts.</title>
<date>2003</date>
<journal>Text,</journal>
<volume>23</volume>
<issue>3</issue>
<contexts>
<context position="6616" citStr="Argamon et al., 2003" startWordPosition="1059" endWordPosition="1062"> as that women use hedges more often (Lakoff, 1973) or that men use more negations (Mulac et al., 2000), or looked at specific words or word classes. Newman et al. (2008) provide a comprehensive review of such work and a description of the non-web corpora used therein. Some of those hypotheses were confirmed by empirical evidence, some not. For example, Herring &amp; Paolillo (2006) analyse gender- and genre-specific use of language in online communication on a sample of about 130 blog entries. Looking at a number of stylistic features which had previously been claimed to be predictive of gender (Argamon et al., 2003; Koppel et al., 2004), such as personal pronouns, determiners and other function words, they find no gender effect. Unlike them, Kapidzic &amp; Herring (2011) analyse recent chat communications and find that they are gendered. Similarly, Huffaker &amp; Calvert (2005) investigate the question of identity of teenager bloggers (e.g., age, gender, sexuality) and find language features indicative of gender (e.g., use of emoticons by males). Burger &amp; Henderson (2006) consider the relationship between different linguistic (e.g., text length, use of capital and punctuation letters) and non-linguistic (e.g., </context>
</contexts>
<marker>Argamon, Koppel, Fine, Shimoni, 2003</marker>
<rawString>Argamon, S., M. Koppel, J. Fine &amp; A. R. Shimoni (2003). Gender, genre, and writing style in formal written texts. Text, 23(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Baluja</author>
<author>R Seth</author>
<author>D Sivakumar</author>
<author>Y Jing</author>
<author>J Yagnik</author>
<author>S Kumar</author>
<author>D Ravichandran</author>
<author>M Aly</author>
</authors>
<title>Video suggestion and discovery for YouTube: Taking random walks through the view graph.</title>
<date>2008</date>
<booktitle>In Proc. of WWW-08,</booktitle>
<pages>895--904</pages>
<contexts>
<context position="17267" citStr="Baluja et al., 2008" startWordPosition="2804" endWordPosition="2807"> and statistics see the official YouTube demographics on http://www.youtube.com/ yt/advertise/affinities.html. 1481 female male nn 26% 62% 12% Table 1: Gender distribution for the extracted 6.9M users. From the users, videos and the comment relationship we build an affiliation graph (Easley &amp; Kleinberg, 2010): a user and a video are connected if the user commented on the video (Fig. 1(a)). Our graph is unweighted although the number of comments could be used to weight edges. The co-comment graph is a stricter version of a more popular co-view graph used in, e.g., video recommendation studies (Baluja et al., 2008, inter alia). We obtained a random sample of videos by considering all the videos whose YouTube ID has a specific prefix8. From those, we collected the profiles of the users whose commented on the videos. In total, we extracted about 6.9M profiles of users who have written at least 20 comments, not more than 30 comments were collected for every user. The threshold on the minimum number of comments is set in order to reduce the proportion of users who have used YouTube only a few times and possibly followed the suggestions of the site in their video choice. The users’ gender distributions is p</context>
</contexts>
<marker>Baluja, Seth, Sivakumar, Jing, Yagnik, Kumar, Ravichandran, Aly, 2008</marker>
<rawString>Baluja, S., R. Seth, D. Sivakumar, Y. Jing, J. Yagnik, S. Kumar, D. Ravichandran &amp; M. Aly (2008). Video suggestion and discovery for YouTube: Taking random walks through the view graph. In Proc. of WWW-08, pp. 895–904.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G J Baxter</author>
<author>R A Blythe</author>
<author>W Croft</author>
<author>A J McKane</author>
</authors>
<title>Utterance selection model of language change. Physical Review,</title>
<date>2006</date>
<contexts>
<context position="12453" citStr="Baxter et al., 2006" startWordPosition="2000" endWordPosition="2003">roader context than a conversation. Language is a social phenomenon, and it is this fact that motivates all the sociolinguistic research. Many if not most language traits are not hard-wired or inborn but can be explained by looking at who the person interacts most with. Since every language speaker can be seen as a member of multiple overlapping communities (e.g., computer scientists, French, males, runners), the language of the person may reflect her membership in different communities to various degrees. Repeated interactions with other language speakers influence the way the person speaks (Baxter et al., 2006; Bybee, 2010), and 1480 the influence is observable on all the levels of the language representation (Croft, 2000). For example, it has been shown that the more a person is integrated in a certain community and the tighter the ties of the social network are, the more prominent are the representative traits of that community in the language of the person (Milroy &amp; Milroy, 1992; Labov, 1994). In our study we adopt a similar view and analyse the implications it has for gender prediction. Given its social nature, does the language reflect the norms of a community the user belongs to or the actual</context>
<context position="20342" citStr="Baxter et al. (2006)" startWordPosition="3340" endWordPosition="3343">the connected videos. Thus, the gender of a user contributes to the vicinity distributions of all the neighbors but not to her own final gender distribution. In line with our motivation and modeling technique, we chose such a simple method (and not, say, classification) in order to approximate the offline encounters of the user: does she more often meet women or men? The way we think of the videos is that they correspond to places (e.g., a cinema, a cosmetic shop, a pub) visited by the user where she is unintentionally or deliberately exposed to how other speakers use the language. Similar to Baxter et al. (2006), we assume that these encounters influence the way the person speaks. Note that if the user’s gender has no influence on her choice of videos, then, on average, we would expect every video to have the same distribution as in our data overall: 62% male, 26% female and 12% unknown (Table 1). To obtain a single gender prediction from the propagated distribution, for a given user we select the gender class (female or male) which got more 1482 (a) Color represents gender information: blue=male, red=female, grey=unknown. (b) Propagating gender from users to videos. (c) Propagating gender distributi</context>
</contexts>
<marker>Baxter, Blythe, Croft, McKane, 2006</marker>
<rawString>Baxter, G. J., R. A. Blythe, W. Croft &amp; A. J. McKane (2006). Utterance selection model of language change. Physical Review, E73.046118.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Berger</author>
<author>S A Della Pietra</author>
<author>V J Della Pietra</author>
</authors>
<title>A maximum entropy approach to natural language processing.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>1</issue>
<contexts>
<context position="23886" citStr="Berger et al., 1996" startWordPosition="3972" endWordPosition="3975">e any features extracted from the user profile, like name, which would make the gender prediction task considerably easier (Burger et al., 2011). Finally, we do not extract any features from the videos the user has commented on because our goal here is to explore the language as a sole source of information. Here we simply want to investigate the extent to which the language of the user is indicative of her gender which is found in the profile and which, ignoring the noise, corresponds to the inborn gender. In our experiments we use a distributed implementation of the maximum entropy learner (Berger et al., 1996; McDonald et al., 2010) which outputs 70 - - - all fem male 1483 a distribution over the classes, the final prediction is the class with the greater probability. We take 80% of the users for training and generate a training instance for every user who made her gender visible on the profile page (4.9M). The remaining 20% of the data are used for testing (1.2M). We use the following three groups of features: (1) character-based: average comment length, ratio of capital letters to the total number of letters, ratio of punctuation to the total number of characters; (2) token-based: average commen</context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>Berger, A., S. A. Della Pietra &amp; V. J. Della Pietra (1996). A maximum entropy approach to natural language processing. Computational Linguistics, 22(1):39–71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J D Burger</author>
<author>J Henderson</author>
<author>G Kim</author>
<author>G Zarrella</author>
</authors>
<title>Discriminating gender on Twitter.</title>
<date>2011</date>
<booktitle>In Proc. of EMNLP-11,</booktitle>
<pages>1301--1309</pages>
<contexts>
<context position="8530" citStr="Burger et al. (2011)" startWordPosition="1366" endWordPosition="1369"> less direct relevance for the present work. For su1479 pervised approaches, major feature sources are the text the user has written and also her profile which may list the name, interests, friends, etc. There have also been studies which did not look at the language at all but considered the social environment only. For example, MacKinnon &amp; Warren (2006) aim at predicting the age and the location of the LiveJournal6 users. What they found is that there is a remarkable correlation between the age and the location of the user and those of her friends, although there are interesting exceptions. Burger et al. (2011) train a gender classifier on tweets with word and character-based ngram features achieving accuracy of 75.5%. Adding the full name feature alone gives a boost to 89.1%, further features like self-written description and screen name further help to get 92%. Also, a self-training method exploring unlabeled data is described but its performance is worse. Other kinds of sociolinguistic features and a different classifier have been applied to gender prediction on tweets by Rao &amp; Yarowsky (2010). Nowson &amp; Oberlander (2006) achieve 92% accuracy on the gender prediction task using ngram features only</context>
<context position="13948" citStr="Burger et al., 2011" startWordPosition="2257" endWordPosition="2260">bout the user’s social environment. We use language-based features and a supervised approach to gender prediction to analyse the relationship between the language and the variable to be predicted. To our knowledge, we are the first to question whether it is really the inborn gender that language-based classifiers learn to predict. More concrete questions we are going to suggest answers to are as follows: 1. Previous studies which looked at online data relied on self-reported demographics. The profile data are known to be noisy, although it is hard to estimate the proportion of false profiles (Burger et al., 2011). Concerning the prediction task, how can we make use of what we know about the user’s social environment to reduce the effect of noise? How can we benefit from the language samples from the users whose gender we do not know at all? 2. When analyzing the language of a user, how much are its gender-specific traits due to the user’s inborn gender and to which extent can they be explained by her social environment? Using our modeling technique and a languagebased gender classifier, how is its performance affected by what we know about the online social environment of the user? 3. Concerning gende</context>
<context position="23411" citStr="Burger et al., 2011" startWordPosition="3888" endWordPosition="3891">r propagated gender. predicting gender from language. 6 Supervised learning of gender In this section we start by describing our first gender prediction experiment and several extensions to it and then turn to the results. 6.1 Experiments Similar to previous studies on demographics prediction, we start with a supervised approach and only look at the text (comments) written by the user. We do not rely on any information from the social environment of the user and do not use any features extracted from the user profile, like name, which would make the gender prediction task considerably easier (Burger et al., 2011). Finally, we do not extract any features from the videos the user has commented on because our goal here is to explore the language as a sole source of information. Here we simply want to investigate the extent to which the language of the user is indicative of her gender which is found in the profile and which, ignoring the noise, corresponds to the inborn gender. In our experiments we use a distributed implementation of the maximum entropy learner (Berger et al., 1996; McDonald et al., 2010) which outputs 70 - - - all fem male 1483 a distribution over the classes, the final prediction is th</context>
</contexts>
<marker>Burger, Henderson, Kim, Zarrella, 2011</marker>
<rawString>Burger, J. D., J. Henderson, G. Kim &amp; G. Zarrella (2011). Discriminating gender on Twitter. In Proc. of EMNLP-11, pp. 1301–1309.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J D Burger</author>
<author>J C Henderson</author>
</authors>
<title>An exploration of observable features related to blogger age.</title>
<date>2006</date>
<booktitle>In Proceedings of the AAAI Spring Symposium on Computational Approaches forAnalyzing Weblogs,</booktitle>
<pages>15--20</pages>
<location>Stanford, CA,</location>
<contexts>
<context position="7074" citStr="Burger &amp; Henderson (2006)" startWordPosition="1132" endWordPosition="1135">ion on a sample of about 130 blog entries. Looking at a number of stylistic features which had previously been claimed to be predictive of gender (Argamon et al., 2003; Koppel et al., 2004), such as personal pronouns, determiners and other function words, they find no gender effect. Unlike them, Kapidzic &amp; Herring (2011) analyse recent chat communications and find that they are gendered. Similarly, Huffaker &amp; Calvert (2005) investigate the question of identity of teenager bloggers (e.g., age, gender, sexuality) and find language features indicative of gender (e.g., use of emoticons by males). Burger &amp; Henderson (2006) consider the relationship between different linguistic (e.g., text length, use of capital and punctuation letters) and non-linguistic (e.g., interests, mood) features and blogger’s age and location. They find that many features correlate with the age and run an experiment with the goal of predicting whether the blog author is over 18. 2.2 Demographics prediction from language The studies we review here used supervised machine learning to obtain models for predicting gender or age. Other demographic attributes, like location, ethnicity, or educational level, have also been predicted automatica</context>
</contexts>
<marker>Burger, Henderson, 2006</marker>
<rawString>Burger, J. D. &amp; J. C. Henderson (2006). An exploration of observable features related to blogger age. In Proceedings of the AAAI Spring Symposium on Computational Approaches forAnalyzing Weblogs, Stanford, CA, 27-29 March 2006, pp. 15–20.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Bybee</author>
</authors>
<title>Language, Usage and Cognition.</title>
<date>2010</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="12467" citStr="Bybee, 2010" startWordPosition="2004" endWordPosition="2005"> conversation. Language is a social phenomenon, and it is this fact that motivates all the sociolinguistic research. Many if not most language traits are not hard-wired or inborn but can be explained by looking at who the person interacts most with. Since every language speaker can be seen as a member of multiple overlapping communities (e.g., computer scientists, French, males, runners), the language of the person may reflect her membership in different communities to various degrees. Repeated interactions with other language speakers influence the way the person speaks (Baxter et al., 2006; Bybee, 2010), and 1480 the influence is observable on all the levels of the language representation (Croft, 2000). For example, it has been shown that the more a person is integrated in a certain community and the tighter the ties of the social network are, the more prominent are the representative traits of that community in the language of the person (Milroy &amp; Milroy, 1992; Labov, 1994). In our study we adopt a similar view and analyse the implications it has for gender prediction. Given its social nature, does the language reflect the norms of a community the user belongs to or the actual value of a de</context>
</contexts>
<marker>Bybee, 2010</marker>
<rawString>Bybee, J. (2010). Language, Usage and Cognition. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Chambers</author>
<author>P Trudgill</author>
</authors>
<date>1998</date>
<publisher>Dialectology. Cambridge University Press.</publisher>
<contexts>
<context position="2043" citStr="Chambers &amp; Trudgill, 1998" startWordPosition="312" endWordPosition="316">masses of text data labeled with respect to different social 1www.youtube.com 2www.facebook.com 3www.plus.google.com 4www.twitter.com parameters such as country, age, gender, profession or religion. The study of language varieties between groups separated by a certain social variable belongs to the field of sociolinguistics which more generally investigates the effect of society on how language is used (Coulmas, 1998). Historically, sociolinguistics is connected to dialectology whose focus has been primarily on the phonetic aspect of the regional dialects but was later extended to sociolects (Chambers &amp; Trudgill, 1998). A usual study would involve sampling speakers from a population, interviewing them and analyzing the linguistic items with respect to social variables (Hudson, 1980). The last decade has seen several studies investigating the relationship between the language and the demographics of the users of blogs or Twitter (see Sec. 2 for references). Most of those studies used social network sites to collect labeled data– samples of text together with the demographics variable. However, they did not analyse how social environment affects language, although very similar questions have been recently pos</context>
</contexts>
<marker>Chambers, Trudgill, 1998</marker>
<rawString>Chambers, J. &amp; P. Trudgill (1998). Dialectology. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Coulmas</author>
</authors>
<title>The Handbook of Sociolinguistics.</title>
<date>1998</date>
<publisher>Blackwell.</publisher>
<contexts>
<context position="1838" citStr="Coulmas, 1998" startWordPosition="284" endWordPosition="285">Their interactions open new possibilities for social sciences, and linguistics is no exception. For example, with the development and growth of Web 2.0, it has become possible to get access to masses of text data labeled with respect to different social 1www.youtube.com 2www.facebook.com 3www.plus.google.com 4www.twitter.com parameters such as country, age, gender, profession or religion. The study of language varieties between groups separated by a certain social variable belongs to the field of sociolinguistics which more generally investigates the effect of society on how language is used (Coulmas, 1998). Historically, sociolinguistics is connected to dialectology whose focus has been primarily on the phonetic aspect of the regional dialects but was later extended to sociolects (Chambers &amp; Trudgill, 1998). A usual study would involve sampling speakers from a population, interviewing them and analyzing the linguistic items with respect to social variables (Hudson, 1980). The last decade has seen several studies investigating the relationship between the language and the demographics of the users of blogs or Twitter (see Sec. 2 for references). Most of those studies used social network sites to</context>
</contexts>
<marker>Coulmas, 1998</marker>
<rawString>Coulmas, F. (Ed.) (1998). The Handbook of Sociolinguistics. Blackwell.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Croft</author>
</authors>
<title>Explaining Language Change: An Evolutionary Approach.</title>
<date>2000</date>
<publisher>Longman.</publisher>
<location>London:</location>
<contexts>
<context position="12568" citStr="Croft, 2000" startWordPosition="2020" endWordPosition="2021">istic research. Many if not most language traits are not hard-wired or inborn but can be explained by looking at who the person interacts most with. Since every language speaker can be seen as a member of multiple overlapping communities (e.g., computer scientists, French, males, runners), the language of the person may reflect her membership in different communities to various degrees. Repeated interactions with other language speakers influence the way the person speaks (Baxter et al., 2006; Bybee, 2010), and 1480 the influence is observable on all the levels of the language representation (Croft, 2000). For example, it has been shown that the more a person is integrated in a certain community and the tighter the ties of the social network are, the more prominent are the representative traits of that community in the language of the person (Milroy &amp; Milroy, 1992; Labov, 1994). In our study we adopt a similar view and analyse the implications it has for gender prediction. Given its social nature, does the language reflect the norms of a community the user belongs to or the actual value of a demographic variable? In our study we address this issue with a particular modeling technique: we assum</context>
</contexts>
<marker>Croft, 2000</marker>
<rawString>Croft, W. (2000). Explaining Language Change: An Evolutionary Approach. London: Longman.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Easley</author>
<author>J Kleinberg</author>
</authors>
<title>Network, crowds, and markets: Reasoning about a highly connected world.</title>
<date>2010</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="16958" citStr="Easley &amp; Kleinberg, 2010" startWordPosition="2749" endWordPosition="2753">ups of people who may share origin (e.g., elections in Greece) or possession (e.g., how to unlock iPhone) or any other property. Some videos attract a welldefined group of people (e.g., the family of a newborn child), whereas some videos appeal to a very broad audience (e.g., a kitten video). 7For more information and statistics see the official YouTube demographics on http://www.youtube.com/ yt/advertise/affinities.html. 1481 female male nn 26% 62% 12% Table 1: Gender distribution for the extracted 6.9M users. From the users, videos and the comment relationship we build an affiliation graph (Easley &amp; Kleinberg, 2010): a user and a video are connected if the user commented on the video (Fig. 1(a)). Our graph is unweighted although the number of comments could be used to weight edges. The co-comment graph is a stricter version of a more popular co-view graph used in, e.g., video recommendation studies (Baluja et al., 2008, inter alia). We obtained a random sample of videos by considering all the videos whose YouTube ID has a specific prefix8. From those, we collected the profiles of the users whose commented on the videos. In total, we extracted about 6.9M profiles of users who have written at least 20 comm</context>
</contexts>
<marker>Easley, Kleinberg, 2010</marker>
<rawString>Easley, D. &amp; J. Kleinberg (2010). Network, crowds, and markets: Reasoning about a highly connected world. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Eckert</author>
<author>S McConnell-Ginet</author>
</authors>
<title>Language and Gender.</title>
<date>2003</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="3742" citStr="Eckert &amp; McConnell-Ginet (2003)" startWordPosition="583" endWordPosition="586">to predict person’s gender from the user-video graph. In what follows we will argue that although language does provide us with signals indicative of the 1478 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1478–1488, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics user’s gender5 (as reported in the user’s profile), it is in fact more indicative of a socially defined gender. Leaving aside the debate on the intricate relationship between language and gender (see Eckert &amp; McConnell-Ginet (2003) for a thorough discussion of the subject), we simply demonstrate that a classifier trained to predict the predominant gender in the user’s social environment, as approximated by the YouTube graph of users and videos, achieves higher accuracy for both genders than the one trained to predict the user’s inborn gender. We also investigate ways of how the language-based and the social views can be combined to improve prediction accuracy. Finally, we look at three age groups – teenagers, people in their twenties and people over thirty – and show that gender identity is more evident in the language </context>
<context position="5675" citStr="Eckert &amp; McConnell-Ginet, 2003" startWordPosition="908" endWordPosition="911">ally confirming hypotheses, such as that female speakers use more pronouns, or that males tend to use longer words. (2) A standard goal of an NLP study is to build an automatic system which accurately solves a given task which in the case of demographics is predicting user age, gender or country of origin. In this section we start by reviewing the first kind of studies, which are about data analysis and hypotheses checking. These are relevant for our choice of features. Then we briefly summarize a selection of 5Although it might be more correct to talk about the user’s sex in place of gender (Eckert &amp; McConnell-Ginet, 2003), we stick to the terminology adopted in previous NLP research on gender prediction. studies on demographics prediction to better situate and motivate our approach. 2.1 Language and demographics analysis Previous sociolinguistic studies mostly checked hypotheses formulated before the widespread use of the Internet, such as that women use hedges more often (Lakoff, 1973) or that men use more negations (Mulac et al., 2000), or looked at specific words or word classes. Newman et al. (2008) provide a comprehensive review of such work and a description of the non-web corpora used therein. Some of t</context>
</contexts>
<marker>Eckert, McConnell-Ginet, 2003</marker>
<rawString>Eckert, P. &amp; S. McConnell-Ginet (2003). Language and Gender. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Eisenstein</author>
<author>N A Smith</author>
<author>E P Xing</author>
</authors>
<title>Discovering sociolinguistic associations with structured sparsity.</title>
<date>2011</date>
<booktitle>In Proc. of ACL-11,</booktitle>
<pages>1365--1374</pages>
<contexts>
<context position="7880" citStr="Eisenstein et al., 2011" startWordPosition="1253" endWordPosition="1256">ge and location. They find that many features correlate with the age and run an experiment with the goal of predicting whether the blog author is over 18. 2.2 Demographics prediction from language The studies we review here used supervised machine learning to obtain models for predicting gender or age. Other demographic attributes, like location, ethnicity, or educational level, have also been predicted automatically (Gillick, 2010; Rao &amp; Yarowsky, 2011, inter alia). Also, generative approaches have been applied to discover associations between language and demographics of social media users (Eisenstein et al., 2011, inter alia) but these are of less direct relevance for the present work. For su1479 pervised approaches, major feature sources are the text the user has written and also her profile which may list the name, interests, friends, etc. There have also been studies which did not look at the language at all but considered the social environment only. For example, MacKinnon &amp; Warren (2006) aim at predicting the age and the location of the LiveJournal6 users. What they found is that there is a remarkable correlation between the age and the location of the user and those of her friends, although ther</context>
</contexts>
<marker>Eisenstein, Smith, Xing, 2011</marker>
<rawString>Eisenstein, J., N. A. Smith &amp; E. P. Xing (2011). Discovering sociolinguistic associations with structured sparsity. In Proc. of ACL-11, pp. 1365– 1374.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Ellist</author>
</authors>
<title>Social (distributed) language modeling, clustering and dialectometry.</title>
<date>2009</date>
<booktitle>In Proc. of TextGraphs atACL-IJCNLP-09,</booktitle>
<pages>1--4</pages>
<contexts>
<context position="2685" citStr="Ellist (2009)" startWordPosition="416" endWordPosition="417"> sampling speakers from a population, interviewing them and analyzing the linguistic items with respect to social variables (Hudson, 1980). The last decade has seen several studies investigating the relationship between the language and the demographics of the users of blogs or Twitter (see Sec. 2 for references). Most of those studies used social network sites to collect labeled data– samples of text together with the demographics variable. However, they did not analyse how social environment affects language, although very similar questions have been recently posed (but not yet answered) by Ellist (2009). In our work we attempt to address precisely this issue. In particular, we consider the task of user gender prediction on YouTube and contrast two information sources: (1) the comments written by the user and (2) her social neighborhood as defined by the bipartite user-video graph. We use the comments to train a gender classifier on a variety of linguistic features. We also introduce a simple gender propagation procedure to predict person’s gender from the user-video graph. In what follows we will argue that although language does provide us with signals indicative of the 1478 Proceedings of </context>
</contexts>
<marker>Ellist, 2009</marker>
<rawString>Ellist, D. (2009). Social (distributed) language modeling, clustering and dialectometry. In Proc. of TextGraphs atACL-IJCNLP-09, pp. 1–4.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Garera</author>
<author>D Yarowsky</author>
</authors>
<title>Modeling latent biographic attributes in conversational genres.</title>
<date>2009</date>
<booktitle>In Proc. of ACL-IJCNLP-09,</booktitle>
<pages>710--718</pages>
<contexts>
<context position="11593" citStr="Garera &amp; Yarowsky (2009)" startWordPosition="1859" endWordPosition="1862">: the user’s social environment and the text she has written. Indeed, a topic which has not yet been investigated much in the reviewed studies on language and user demographics is the relationship between the language of the user and her social environment. The data analysis studies (Sec. 2.1) verified hypotheses concerning the dependency between a language trait (e.g., average sentence length) and a demographic parameter (e.g., gender). The demographics prediction studies (Sec. 2.2) mostly relied on language and user profile features and considered users in isolation. An exception to this is Garera &amp; Yarowsky (2009) who showed that, for gender prediction in a dialogue, it helps to know the interlocutor’s gender. However, we aim at investigating the impact of the social environment in a much broader sense than the immediate interlocutors and in a much broader context than a conversation. Language is a social phenomenon, and it is this fact that motivates all the sociolinguistic research. Many if not most language traits are not hard-wired or inborn but can be explained by looking at who the person interacts most with. Since every language speaker can be seen as a member of multiple overlapping communities</context>
</contexts>
<marker>Garera, Yarowsky, 2009</marker>
<rawString>Garera, N. &amp; D. Yarowsky (2009). Modeling latent biographic attributes in conversational genres. In Proc. of ACL-IJCNLP-09, pp. 710–718.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Gillick</author>
</authors>
<title>Can conversational word usage be used to predict speaker demographics?</title>
<date>2010</date>
<booktitle>In Proceedings of Interspeech, Makuhari,</booktitle>
<contexts>
<context position="7692" citStr="Gillick, 2010" startWordPosition="1227" endWordPosition="1228">ider the relationship between different linguistic (e.g., text length, use of capital and punctuation letters) and non-linguistic (e.g., interests, mood) features and blogger’s age and location. They find that many features correlate with the age and run an experiment with the goal of predicting whether the blog author is over 18. 2.2 Demographics prediction from language The studies we review here used supervised machine learning to obtain models for predicting gender or age. Other demographic attributes, like location, ethnicity, or educational level, have also been predicted automatically (Gillick, 2010; Rao &amp; Yarowsky, 2011, inter alia). Also, generative approaches have been applied to discover associations between language and demographics of social media users (Eisenstein et al., 2011, inter alia) but these are of less direct relevance for the present work. For su1479 pervised approaches, major feature sources are the text the user has written and also her profile which may list the name, interests, friends, etc. There have also been studies which did not look at the language at all but considered the social environment only. For example, MacKinnon &amp; Warren (2006) aim at predicting the ag</context>
</contexts>
<marker>Gillick, 2010</marker>
<rawString>Gillick, D. (2010). Can conversational word usage be used to predict speaker demographics? In Proceedings of Interspeech, Makuhari, Japan, 26-30 September 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S C Herring</author>
<author>J C Paolillo</author>
</authors>
<title>Gender and genre variation in weblogs.</title>
<date>2006</date>
<journal>Journal of Sociolinguistics,</journal>
<volume>10</volume>
<issue>4</issue>
<contexts>
<context position="6377" citStr="Herring &amp; Paolillo (2006)" startWordPosition="1018" endWordPosition="1021">iction. studies on demographics prediction to better situate and motivate our approach. 2.1 Language and demographics analysis Previous sociolinguistic studies mostly checked hypotheses formulated before the widespread use of the Internet, such as that women use hedges more often (Lakoff, 1973) or that men use more negations (Mulac et al., 2000), or looked at specific words or word classes. Newman et al. (2008) provide a comprehensive review of such work and a description of the non-web corpora used therein. Some of those hypotheses were confirmed by empirical evidence, some not. For example, Herring &amp; Paolillo (2006) analyse gender- and genre-specific use of language in online communication on a sample of about 130 blog entries. Looking at a number of stylistic features which had previously been claimed to be predictive of gender (Argamon et al., 2003; Koppel et al., 2004), such as personal pronouns, determiners and other function words, they find no gender effect. Unlike them, Kapidzic &amp; Herring (2011) analyse recent chat communications and find that they are gendered. Similarly, Huffaker &amp; Calvert (2005) investigate the question of identity of teenager bloggers (e.g., age, gender, sexuality) and find la</context>
</contexts>
<marker>Herring, Paolillo, 2006</marker>
<rawString>Herring, S. C. &amp; J. C. Paolillo (2006). Gender and genre variation in weblogs. Journal of Sociolinguistics, 10(4):439–459.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S C Herring</author>
<author>L A Scheidt</author>
<author>S Bonus</author>
<author>E Wright</author>
</authors>
<title>Bridging the gap: A genre analysis of weblogs.</title>
<date>2004</date>
<booktitle>In HICSS-04.</booktitle>
<contexts>
<context position="18009" citStr="Herring et al., 2004" startWordPosition="2936" endWordPosition="2939">8. From those, we collected the profiles of the users whose commented on the videos. In total, we extracted about 6.9M profiles of users who have written at least 20 comments, not more than 30 comments were collected for every user. The threshold on the minimum number of comments is set in order to reduce the proportion of users who have used YouTube only a few times and possibly followed the suggestions of the site in their video choice. The users’ gender distributions is presented in Table 1. Although females, in particular teenagers, have been reported to be more likely to blog than males (Herring et al., 2004), males are predominant in our dataset. A random sample from a pool of users without the 20-comments threshold showed that there are more male commenters overall, although the difference is less remarkable for teenagers: 58% of the teenagers with known gender are male as opposed to 74% and 79% for the age groups 20-29 and 30+. Teenagers are also more numerous accounting for about 35% in our data. Although we did not filter users based on their location or mother tongue as many users comment in multiple languages, the comment set is overwhelmingly English. 8The YouTube API (http://code.google.c</context>
</contexts>
<marker>Herring, Scheidt, Bonus, Wright, 2004</marker>
<rawString>Herring, S. C., L. A. Scheidt, S. Bonus &amp; E. Wright (2004). Bridging the gap: A genre analysis of weblogs. In HICSS-04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R A Hudson</author>
</authors>
<title>Sociolinguistics.</title>
<date>1980</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="2210" citStr="Hudson, 1980" startWordPosition="339" endWordPosition="340">ssion or religion. The study of language varieties between groups separated by a certain social variable belongs to the field of sociolinguistics which more generally investigates the effect of society on how language is used (Coulmas, 1998). Historically, sociolinguistics is connected to dialectology whose focus has been primarily on the phonetic aspect of the regional dialects but was later extended to sociolects (Chambers &amp; Trudgill, 1998). A usual study would involve sampling speakers from a population, interviewing them and analyzing the linguistic items with respect to social variables (Hudson, 1980). The last decade has seen several studies investigating the relationship between the language and the demographics of the users of blogs or Twitter (see Sec. 2 for references). Most of those studies used social network sites to collect labeled data– samples of text together with the demographics variable. However, they did not analyse how social environment affects language, although very similar questions have been recently posed (but not yet answered) by Ellist (2009). In our work we attempt to address precisely this issue. In particular, we consider the task of user gender prediction on Yo</context>
</contexts>
<marker>Hudson, 1980</marker>
<rawString>Hudson, R. A. (1980). Sociolinguistics. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D A Huffaker</author>
<author>S L Calvert</author>
</authors>
<title>Gender, identity and language use in teenager blogs.</title>
<date>2005</date>
<journal>Journal of Computer-Mediated Communication,</journal>
<volume>10</volume>
<issue>2</issue>
<contexts>
<context position="6876" citStr="Huffaker &amp; Calvert (2005)" startWordPosition="1101" endWordPosition="1104"> used therein. Some of those hypotheses were confirmed by empirical evidence, some not. For example, Herring &amp; Paolillo (2006) analyse gender- and genre-specific use of language in online communication on a sample of about 130 blog entries. Looking at a number of stylistic features which had previously been claimed to be predictive of gender (Argamon et al., 2003; Koppel et al., 2004), such as personal pronouns, determiners and other function words, they find no gender effect. Unlike them, Kapidzic &amp; Herring (2011) analyse recent chat communications and find that they are gendered. Similarly, Huffaker &amp; Calvert (2005) investigate the question of identity of teenager bloggers (e.g., age, gender, sexuality) and find language features indicative of gender (e.g., use of emoticons by males). Burger &amp; Henderson (2006) consider the relationship between different linguistic (e.g., text length, use of capital and punctuation letters) and non-linguistic (e.g., interests, mood) features and blogger’s age and location. They find that many features correlate with the age and run an experiment with the goal of predicting whether the blog author is over 18. 2.2 Demographics prediction from language The studies we review </context>
<context position="27343" citStr="Huffaker &amp; Calvert, 2005" startWordPosition="4557" endWordPosition="4560">icinity gender, that is the dominating gender of the environment of a speaker as obtained with the procedure described in Section 5. Age groups and gender prediction. Finally, we look at how gender predictions change with age and train three age-specific models to predict gender for teenagers (13-19), people in their twenties (20-29) and people over thirty (30+), the age is also extracted from the profiles. These groups are identified in order to check whether teenagers tend to signalize their gender identity more than older people, a hypothesis investigated earlier on a sample of blog posts (Huffaker &amp; Calvert, 2005). 6.2 Results We report the results of the supervised experiments for all the settings described above. As an estimate of the lowest bound we also give the results of the majority class baseline (all male) which guarantees 70% accuracy. For the supervised classifiers we report accuracy and per-gender precision, recall and fmeasure. Table 3 presents the results for the starting classifier trained to predict profile gender. Acc% P% R% F1 Total Baseline 70 - - - 619K all 89 - - - 619K fem - 83 78 80 182K male - 91 94 93 437K Table 3: Results on the test set. In order to investigate the relationsh</context>
</contexts>
<marker>Huffaker, Calvert, 2005</marker>
<rawString>Huffaker, D. A. &amp; S. L. Calvert (2005). Gender, identity and language use in teenager blogs. Journal of Computer-Mediated Communication, 10(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kapidzic</author>
<author>S C Herring</author>
</authors>
<title>Gender, communication, and self-presentation in teen chatrooms revisited: Have patterns changed?</title>
<date>2011</date>
<journal>Journal of Computer-Mediated Communication,</journal>
<volume>17</volume>
<issue>1</issue>
<contexts>
<context position="6771" citStr="Kapidzic &amp; Herring (2011)" startWordPosition="1084" endWordPosition="1087">Newman et al. (2008) provide a comprehensive review of such work and a description of the non-web corpora used therein. Some of those hypotheses were confirmed by empirical evidence, some not. For example, Herring &amp; Paolillo (2006) analyse gender- and genre-specific use of language in online communication on a sample of about 130 blog entries. Looking at a number of stylistic features which had previously been claimed to be predictive of gender (Argamon et al., 2003; Koppel et al., 2004), such as personal pronouns, determiners and other function words, they find no gender effect. Unlike them, Kapidzic &amp; Herring (2011) analyse recent chat communications and find that they are gendered. Similarly, Huffaker &amp; Calvert (2005) investigate the question of identity of teenager bloggers (e.g., age, gender, sexuality) and find language features indicative of gender (e.g., use of emoticons by males). Burger &amp; Henderson (2006) consider the relationship between different linguistic (e.g., text length, use of capital and punctuation letters) and non-linguistic (e.g., interests, mood) features and blogger’s age and location. They find that many features correlate with the age and run an experiment with the goal of predic</context>
</contexts>
<marker>Kapidzic, Herring, 2011</marker>
<rawString>Kapidzic, S. &amp; S. C. Herring (2011). Gender, communication, and self-presentation in teen chatrooms revisited: Have patterns changed? Journal of Computer-Mediated Communication, 17(1):39–59.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Koppel</author>
<author>S Argamon</author>
<author>A R Shimoni</author>
</authors>
<title>Automatically categorizing written text by author gender.</title>
<date>2004</date>
<booktitle>Literary and Linguistic Computing,</booktitle>
<pages>17--4</pages>
<contexts>
<context position="6638" citStr="Koppel et al., 2004" startWordPosition="1063" endWordPosition="1066">ges more often (Lakoff, 1973) or that men use more negations (Mulac et al., 2000), or looked at specific words or word classes. Newman et al. (2008) provide a comprehensive review of such work and a description of the non-web corpora used therein. Some of those hypotheses were confirmed by empirical evidence, some not. For example, Herring &amp; Paolillo (2006) analyse gender- and genre-specific use of language in online communication on a sample of about 130 blog entries. Looking at a number of stylistic features which had previously been claimed to be predictive of gender (Argamon et al., 2003; Koppel et al., 2004), such as personal pronouns, determiners and other function words, they find no gender effect. Unlike them, Kapidzic &amp; Herring (2011) analyse recent chat communications and find that they are gendered. Similarly, Huffaker &amp; Calvert (2005) investigate the question of identity of teenager bloggers (e.g., age, gender, sexuality) and find language features indicative of gender (e.g., use of emoticons by males). Burger &amp; Henderson (2006) consider the relationship between different linguistic (e.g., text length, use of capital and punctuation letters) and non-linguistic (e.g., interests, mood) featu</context>
</contexts>
<marker>Koppel, Argamon, Shimoni, 2004</marker>
<rawString>Koppel, M., S. Argamon &amp; A. R. Shimoni (2004). Automatically categorizing written text by author gender. Literary and Linguistic Computing, 17(4):401–412.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Labov</author>
</authors>
<title>Principles of Linguistic Change: Internal Factors.</title>
<date>1994</date>
<publisher>Blackwell.</publisher>
<contexts>
<context position="12846" citStr="Labov, 1994" startWordPosition="2071" endWordPosition="2072">s, runners), the language of the person may reflect her membership in different communities to various degrees. Repeated interactions with other language speakers influence the way the person speaks (Baxter et al., 2006; Bybee, 2010), and 1480 the influence is observable on all the levels of the language representation (Croft, 2000). For example, it has been shown that the more a person is integrated in a certain community and the tighter the ties of the social network are, the more prominent are the representative traits of that community in the language of the person (Milroy &amp; Milroy, 1992; Labov, 1994). In our study we adopt a similar view and analyse the implications it has for gender prediction. Given its social nature, does the language reflect the norms of a community the user belongs to or the actual value of a demographic variable? In our study we address this issue with a particular modeling technique: we assume that the observed online behavior adequately reflects the offline life of a user (more on this in Sec. 4 and 5) and based on this assumption make inferences about the user’s social environment. We use language-based features and a supervised approach to gender prediction to a</context>
</contexts>
<marker>Labov, 1994</marker>
<rawString>Labov, W. (1994). Principles of Linguistic Change: Internal Factors. Blackwell.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Lakoff</author>
</authors>
<title>Language and woman’s place.</title>
<date>1973</date>
<journal>Language in Society,</journal>
<volume>2</volume>
<issue>1</issue>
<contexts>
<context position="6047" citStr="Lakoff, 1973" startWordPosition="964" endWordPosition="965">ata analysis and hypotheses checking. These are relevant for our choice of features. Then we briefly summarize a selection of 5Although it might be more correct to talk about the user’s sex in place of gender (Eckert &amp; McConnell-Ginet, 2003), we stick to the terminology adopted in previous NLP research on gender prediction. studies on demographics prediction to better situate and motivate our approach. 2.1 Language and demographics analysis Previous sociolinguistic studies mostly checked hypotheses formulated before the widespread use of the Internet, such as that women use hedges more often (Lakoff, 1973) or that men use more negations (Mulac et al., 2000), or looked at specific words or word classes. Newman et al. (2008) provide a comprehensive review of such work and a description of the non-web corpora used therein. Some of those hypotheses were confirmed by empirical evidence, some not. For example, Herring &amp; Paolillo (2006) analyse gender- and genre-specific use of language in online communication on a sample of about 130 blog entries. Looking at a number of stylistic features which had previously been claimed to be predictive of gender (Argamon et al., 2003; Koppel et al., 2004), such as</context>
</contexts>
<marker>Lakoff, 1973</marker>
<rawString>Lakoff, R. (1973). Language and woman’s place. Language in Society, 2(1):45–80.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I MacKinnon</author>
<author>R Warren</author>
</authors>
<title>Age and geographic inferences of the LiveJournal social network.</title>
<date>2006</date>
<booktitle>In Statistical Network Analysis: Models, Issues, and New Directions Workshop at ICML2006,</booktitle>
<location>Pittsburgh, PA,</location>
<contexts>
<context position="8267" citStr="MacKinnon &amp; Warren (2006)" startWordPosition="1319" endWordPosition="1322">e also been predicted automatically (Gillick, 2010; Rao &amp; Yarowsky, 2011, inter alia). Also, generative approaches have been applied to discover associations between language and demographics of social media users (Eisenstein et al., 2011, inter alia) but these are of less direct relevance for the present work. For su1479 pervised approaches, major feature sources are the text the user has written and also her profile which may list the name, interests, friends, etc. There have also been studies which did not look at the language at all but considered the social environment only. For example, MacKinnon &amp; Warren (2006) aim at predicting the age and the location of the LiveJournal6 users. What they found is that there is a remarkable correlation between the age and the location of the user and those of her friends, although there are interesting exceptions. Burger et al. (2011) train a gender classifier on tweets with word and character-based ngram features achieving accuracy of 75.5%. Adding the full name feature alone gives a boost to 89.1%, further features like self-written description and screen name further help to get 92%. Also, a self-training method exploring unlabeled data is described but its perf</context>
</contexts>
<marker>MacKinnon, Warren, 2006</marker>
<rawString>MacKinnon, I. &amp; R. Warren (2006). Age and geographic inferences of the LiveJournal social network. In Statistical Network Analysis: Models, Issues, and New Directions Workshop at ICML2006, Pittsburgh, PA, 29 June, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>K Hall</author>
<author>G Mann</author>
</authors>
<title>Distributed training strategies for the structured perceptron.</title>
<date>2010</date>
<booktitle>In Proc. of NAACL-HLT-10,</booktitle>
<pages>456--464</pages>
<contexts>
<context position="23910" citStr="McDonald et al., 2010" startWordPosition="3976" endWordPosition="3979">ted from the user profile, like name, which would make the gender prediction task considerably easier (Burger et al., 2011). Finally, we do not extract any features from the videos the user has commented on because our goal here is to explore the language as a sole source of information. Here we simply want to investigate the extent to which the language of the user is indicative of her gender which is found in the profile and which, ignoring the noise, corresponds to the inborn gender. In our experiments we use a distributed implementation of the maximum entropy learner (Berger et al., 1996; McDonald et al., 2010) which outputs 70 - - - all fem male 1483 a distribution over the classes, the final prediction is the class with the greater probability. We take 80% of the users for training and generate a training instance for every user who made her gender visible on the profile page (4.9M). The remaining 20% of the data are used for testing (1.2M). We use the following three groups of features: (1) character-based: average comment length, ratio of capital letters to the total number of letters, ratio of punctuation to the total number of characters; (2) token-based: average comment length in words, ratio</context>
</contexts>
<marker>McDonald, Hall, Mann, 2010</marker>
<rawString>McDonald, R., K. Hall &amp; G. Mann (2010). Distributed training strategies for the structured perceptron. In Proc. of NAACL-HLT-10, pp. 456– 464.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Milroy</author>
<author>J Milroy</author>
</authors>
<title>Social network and social class: Toward an integrated sociolinguistic model. Language in Society,</title>
<date>1992</date>
<pages>21--1</pages>
<contexts>
<context position="12832" citStr="Milroy &amp; Milroy, 1992" startWordPosition="2067" endWordPosition="2070">cientists, French, males, runners), the language of the person may reflect her membership in different communities to various degrees. Repeated interactions with other language speakers influence the way the person speaks (Baxter et al., 2006; Bybee, 2010), and 1480 the influence is observable on all the levels of the language representation (Croft, 2000). For example, it has been shown that the more a person is integrated in a certain community and the tighter the ties of the social network are, the more prominent are the representative traits of that community in the language of the person (Milroy &amp; Milroy, 1992; Labov, 1994). In our study we adopt a similar view and analyse the implications it has for gender prediction. Given its social nature, does the language reflect the norms of a community the user belongs to or the actual value of a demographic variable? In our study we address this issue with a particular modeling technique: we assume that the observed online behavior adequately reflects the offline life of a user (more on this in Sec. 4 and 5) and based on this assumption make inferences about the user’s social environment. We use language-based features and a supervised approach to gender p</context>
</contexts>
<marker>Milroy, Milroy, 1992</marker>
<rawString>Milroy, L. &amp; J. Milroy (1992). Social network and social class: Toward an integrated sociolinguistic model. Language in Society, 21:1–26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Mulac</author>
<author>D R Seibold</author>
<author>J R Farris</author>
</authors>
<title>Female and male managers’ and professionals’ criticism giving: Differences in language use and effects.</title>
<date>2000</date>
<journal>Journal of Language and Social Psychology,</journal>
<volume>19</volume>
<issue>4</issue>
<contexts>
<context position="6099" citStr="Mulac et al., 2000" startWordPosition="972" endWordPosition="975">e relevant for our choice of features. Then we briefly summarize a selection of 5Although it might be more correct to talk about the user’s sex in place of gender (Eckert &amp; McConnell-Ginet, 2003), we stick to the terminology adopted in previous NLP research on gender prediction. studies on demographics prediction to better situate and motivate our approach. 2.1 Language and demographics analysis Previous sociolinguistic studies mostly checked hypotheses formulated before the widespread use of the Internet, such as that women use hedges more often (Lakoff, 1973) or that men use more negations (Mulac et al., 2000), or looked at specific words or word classes. Newman et al. (2008) provide a comprehensive review of such work and a description of the non-web corpora used therein. Some of those hypotheses were confirmed by empirical evidence, some not. For example, Herring &amp; Paolillo (2006) analyse gender- and genre-specific use of language in online communication on a sample of about 130 blog entries. Looking at a number of stylistic features which had previously been claimed to be predictive of gender (Argamon et al., 2003; Koppel et al., 2004), such as personal pronouns, determiners and other function w</context>
</contexts>
<marker>Mulac, Seibold, Farris, 2000</marker>
<rawString>Mulac, A., D. R. Seibold &amp; J. R. Farris (2000). Female and male managers’ and professionals’ criticism giving: Differences in language use and effects. Journal of Language and Social Psychology, 19(4):389–415.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M L Newman</author>
<author>C J Groom</author>
<author>L D Handelman</author>
<author>J W Pennebaker</author>
</authors>
<title>Gender differences in language use: An analysis of 14,000 text samples.</title>
<date>2008</date>
<booktitle>Discourse Processes,</booktitle>
<pages>45--211</pages>
<contexts>
<context position="6166" citStr="Newman et al. (2008)" startWordPosition="984" endWordPosition="987"> selection of 5Although it might be more correct to talk about the user’s sex in place of gender (Eckert &amp; McConnell-Ginet, 2003), we stick to the terminology adopted in previous NLP research on gender prediction. studies on demographics prediction to better situate and motivate our approach. 2.1 Language and demographics analysis Previous sociolinguistic studies mostly checked hypotheses formulated before the widespread use of the Internet, such as that women use hedges more often (Lakoff, 1973) or that men use more negations (Mulac et al., 2000), or looked at specific words or word classes. Newman et al. (2008) provide a comprehensive review of such work and a description of the non-web corpora used therein. Some of those hypotheses were confirmed by empirical evidence, some not. For example, Herring &amp; Paolillo (2006) analyse gender- and genre-specific use of language in online communication on a sample of about 130 blog entries. Looking at a number of stylistic features which had previously been claimed to be predictive of gender (Argamon et al., 2003; Koppel et al., 2004), such as personal pronouns, determiners and other function words, they find no gender effect. Unlike them, Kapidzic &amp; Herring (</context>
</contexts>
<marker>Newman, Groom, Handelman, Pennebaker, 2008</marker>
<rawString>Newman, M. L., C. J. Groom, L. D. Handelman &amp; J. W. Pennebaker (2008). Gender differences in language use: An analysis of 14,000 text samples. Discourse Processes, 45:211–236.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Nowson</author>
<author>J Oberlander</author>
</authors>
<title>The identity of bloggers: Openness and gender in personal weblogs.</title>
<date>2006</date>
<booktitle>In Proceedings of the AAAI Spring Symposium on Computational Approaches forAnalyzing Weblogs,</booktitle>
<pages>163--167</pages>
<location>Stanford, CA,</location>
<contexts>
<context position="9053" citStr="Nowson &amp; Oberlander (2006)" startWordPosition="1448" endWordPosition="1451">n of the user and those of her friends, although there are interesting exceptions. Burger et al. (2011) train a gender classifier on tweets with word and character-based ngram features achieving accuracy of 75.5%. Adding the full name feature alone gives a boost to 89.1%, further features like self-written description and screen name further help to get 92%. Also, a self-training method exploring unlabeled data is described but its performance is worse. Other kinds of sociolinguistic features and a different classifier have been applied to gender prediction on tweets by Rao &amp; Yarowsky (2010). Nowson &amp; Oberlander (2006) achieve 92% accuracy on the gender prediction task using ngram features only. Their corpus consist of 1,400/450 posts written by 47 females and 24 males, respectively. However, the ngram features were preselected based on whether they occurred with significant relative frequency in the language of one gender over the other. Since the complete dataset was used to preselect features, the results are inconclusive. Yan &amp; Yan (2006) train a Naive Bayes classifier to predict the gender of a blog entry author. In total they looked at 75,000 individual blog entries authored by 3,000 bloggers, all of </context>
</contexts>
<marker>Nowson, Oberlander, 2006</marker>
<rawString>Nowson, S. &amp; J. Oberlander (2006). The identity of bloggers: Openness and gender in personal weblogs. In Proceedings of the AAAI Spring Symposium on Computational Approaches forAnalyzing Weblogs, Stanford, CA, 27-29 March 2006, pp. 163–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Otterbacher</author>
</authors>
<title>Inferring gender of movie reviewers: Exploiting writing style, content and metadata.</title>
<date>2010</date>
<booktitle>In Proc. of CIKM-10.</booktitle>
<contexts>
<context position="10711" citStr="Otterbacher, 2010" startWordPosition="1719" endWordPosition="1720">al. (2006), they run a classification experiment with three age classes removing intermediate ages and use the majority-class 6www.livejournal.com baseline for comparison. In their other experiment they experiment with a binary classifier for age distinguishing between the pre- and post-social media generations and using the years from 1975-1988 as a boundary. The prediction accuracy increases as later years are taken. Interestingly, it has been shown that demographics can be predicted in more restricted genres than the personal blog or tweets and from text fragments even shorter than tweets (Otterbacher, 2010; Popescu &amp; Grefenstette, 2010). 3 Motivation for the present study Similarly to previous NLP studies, our starting goal is to predict the self-reported user gender. The first novelty of our research is that in doing so we contrast two sources of information: the user’s social environment and the text she has written. Indeed, a topic which has not yet been investigated much in the reviewed studies on language and user demographics is the relationship between the language of the user and her social environment. The data analysis studies (Sec. 2.1) verified hypotheses concerning the dependency b</context>
</contexts>
<marker>Otterbacher, 2010</marker>
<rawString>Otterbacher, J. (2010). Inferring gender of movie reviewers: Exploiting writing style, content and metadata. In Proc. of CIKM-10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Popescu</author>
<author>G Grefenstette</author>
</authors>
<title>Mining user home location and gender from Flickr tags.</title>
<date>2010</date>
<booktitle>In Proc. of ICWSM-10,</booktitle>
<pages>1873--1876</pages>
<contexts>
<context position="10742" citStr="Popescu &amp; Grefenstette, 2010" startWordPosition="1721" endWordPosition="1724">n a classification experiment with three age classes removing intermediate ages and use the majority-class 6www.livejournal.com baseline for comparison. In their other experiment they experiment with a binary classifier for age distinguishing between the pre- and post-social media generations and using the years from 1975-1988 as a boundary. The prediction accuracy increases as later years are taken. Interestingly, it has been shown that demographics can be predicted in more restricted genres than the personal blog or tweets and from text fragments even shorter than tweets (Otterbacher, 2010; Popescu &amp; Grefenstette, 2010). 3 Motivation for the present study Similarly to previous NLP studies, our starting goal is to predict the self-reported user gender. The first novelty of our research is that in doing so we contrast two sources of information: the user’s social environment and the text she has written. Indeed, a topic which has not yet been investigated much in the reviewed studies on language and user demographics is the relationship between the language of the user and her social environment. The data analysis studies (Sec. 2.1) verified hypotheses concerning the dependency between a language trait (e.g., </context>
</contexts>
<marker>Popescu, Grefenstette, 2010</marker>
<rawString>Popescu, A. &amp; G. Grefenstette (2010). Mining user home location and gender from Flickr tags. In Proc. of ICWSM-10, pp. 1873–1876.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Rao</author>
<author>D Yarowsky</author>
</authors>
<title>Detecting latent user properties in social media.</title>
<date>2010</date>
<booktitle>In Proc. of the NIPS MLSN Workshop.</booktitle>
<contexts>
<context position="9025" citStr="Rao &amp; Yarowsky (2010)" startWordPosition="1444" endWordPosition="1447">the age and the location of the user and those of her friends, although there are interesting exceptions. Burger et al. (2011) train a gender classifier on tweets with word and character-based ngram features achieving accuracy of 75.5%. Adding the full name feature alone gives a boost to 89.1%, further features like self-written description and screen name further help to get 92%. Also, a self-training method exploring unlabeled data is described but its performance is worse. Other kinds of sociolinguistic features and a different classifier have been applied to gender prediction on tweets by Rao &amp; Yarowsky (2010). Nowson &amp; Oberlander (2006) achieve 92% accuracy on the gender prediction task using ngram features only. Their corpus consist of 1,400/450 posts written by 47 females and 24 males, respectively. However, the ngram features were preselected based on whether they occurred with significant relative frequency in the language of one gender over the other. Since the complete dataset was used to preselect features, the results are inconclusive. Yan &amp; Yan (2006) train a Naive Bayes classifier to predict the gender of a blog entry author. In total they looked at 75,000 individual blog entries authore</context>
</contexts>
<marker>Rao, Yarowsky, 2010</marker>
<rawString>Rao, D. &amp; D. Yarowsky (2010). Detecting latent user properties in social media. In Proc. of the NIPS MLSN Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Rao</author>
<author>D Yarowsky</author>
</authors>
<title>Typed graph models for semi-supervised learning of name ethnicity.</title>
<date>2011</date>
<booktitle>In Proc. of ACL-11,</booktitle>
<pages>514--518</pages>
<contexts>
<context position="7714" citStr="Rao &amp; Yarowsky, 2011" startWordPosition="1229" endWordPosition="1232">onship between different linguistic (e.g., text length, use of capital and punctuation letters) and non-linguistic (e.g., interests, mood) features and blogger’s age and location. They find that many features correlate with the age and run an experiment with the goal of predicting whether the blog author is over 18. 2.2 Demographics prediction from language The studies we review here used supervised machine learning to obtain models for predicting gender or age. Other demographic attributes, like location, ethnicity, or educational level, have also been predicted automatically (Gillick, 2010; Rao &amp; Yarowsky, 2011, inter alia). Also, generative approaches have been applied to discover associations between language and demographics of social media users (Eisenstein et al., 2011, inter alia) but these are of less direct relevance for the present work. For su1479 pervised approaches, major feature sources are the text the user has written and also her profile which may list the name, interests, friends, etc. There have also been studies which did not look at the language at all but considered the social environment only. For example, MacKinnon &amp; Warren (2006) aim at predicting the age and the location of </context>
</contexts>
<marker>Rao, Yarowsky, 2011</marker>
<rawString>Rao, D. &amp; D. Yarowsky (2011). Typed graph models for semi-supervised learning of name ethnicity. In Proc. of ACL-11, pp. 514–518.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Rosenthal</author>
<author>K McKeown</author>
</authors>
<title>Age prediction in blogs: A study of style, content, and online behavior in pre- and post-social media generations.</title>
<date>2011</date>
<booktitle>In Proc. ofACL-11,</booktitle>
<pages>763--772</pages>
<contexts>
<context position="9883" citStr="Rosenthal &amp; McKeown (2011)" startWordPosition="1586" endWordPosition="1589">eselected based on whether they occurred with significant relative frequency in the language of one gender over the other. Since the complete dataset was used to preselect features, the results are inconclusive. Yan &amp; Yan (2006) train a Naive Bayes classifier to predict the gender of a blog entry author. In total they looked at 75,000 individual blog entries authored by 3,000 bloggers, all of them posted their genders on the profile page. They measure precision and recall w.r.t. the minority class (males) and get the best f-measure of 0.64 (precision and recall are 65% and 71%, respectively). Rosenthal &amp; McKeown (2011) predict the age of a blogger, most features they use are extracted from the blog posts, other features include blogger’s interests, the number of friends, the usual time of posting, etc. Similarly to Schler et al. (2006), they run a classification experiment with three age classes removing intermediate ages and use the majority-class 6www.livejournal.com baseline for comparison. In their other experiment they experiment with a binary classifier for age distinguishing between the pre- and post-social media generations and using the years from 1975-1988 as a boundary. The prediction accuracy in</context>
</contexts>
<marker>Rosenthal, McKeown, 2011</marker>
<rawString>Rosenthal, S. &amp; K. McKeown (2011). Age prediction in blogs: A study of style, content, and online behavior in pre- and post-social media generations. In Proc. ofACL-11, pp. 763–772.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Schler</author>
<author>M Koppel</author>
<author>S Argamon</author>
<author>J Pennebaker</author>
</authors>
<title>Effects of age and gender on blogging.</title>
<date>2006</date>
<booktitle>In Proceedings of the AAAI Spring Symposium on Computational Approaches for Analyzing Weblogs,</booktitle>
<pages>199--205</pages>
<location>Stanford, CA,</location>
<contexts>
<context position="10104" citStr="Schler et al. (2006)" startWordPosition="1625" endWordPosition="1628">) train a Naive Bayes classifier to predict the gender of a blog entry author. In total they looked at 75,000 individual blog entries authored by 3,000 bloggers, all of them posted their genders on the profile page. They measure precision and recall w.r.t. the minority class (males) and get the best f-measure of 0.64 (precision and recall are 65% and 71%, respectively). Rosenthal &amp; McKeown (2011) predict the age of a blogger, most features they use are extracted from the blog posts, other features include blogger’s interests, the number of friends, the usual time of posting, etc. Similarly to Schler et al. (2006), they run a classification experiment with three age classes removing intermediate ages and use the majority-class 6www.livejournal.com baseline for comparison. In their other experiment they experiment with a binary classifier for age distinguishing between the pre- and post-social media generations and using the years from 1975-1988 as a boundary. The prediction accuracy increases as later years are taken. Interestingly, it has been shown that demographics can be predicted in more restricted genres than the personal blog or tweets and from text fragments even shorter than tweets (Otterbache</context>
</contexts>
<marker>Schler, Koppel, Argamon, Pennebaker, 2006</marker>
<rawString>Schler, J., M. Koppel, S. Argamon &amp; J. Pennebaker (2006). Effects of age and gender on blogging. In Proceedings of the AAAI Spring Symposium on Computational Approaches for Analyzing Weblogs, Stanford, CA, 27-29 March 2006, pp. 199– 205.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Yan</author>
<author>L Yan</author>
</authors>
<title>Gender classification of weblogs authors.</title>
<date>2006</date>
<booktitle>In Proceedings of the AAAI Spring Symposium on Computational Approaches for Analyzing Weblogs,</booktitle>
<pages>228--230</pages>
<location>Stanford, CA,</location>
<contexts>
<context position="9485" citStr="Yan &amp; Yan (2006)" startWordPosition="1518" endWordPosition="1521">mance is worse. Other kinds of sociolinguistic features and a different classifier have been applied to gender prediction on tweets by Rao &amp; Yarowsky (2010). Nowson &amp; Oberlander (2006) achieve 92% accuracy on the gender prediction task using ngram features only. Their corpus consist of 1,400/450 posts written by 47 females and 24 males, respectively. However, the ngram features were preselected based on whether they occurred with significant relative frequency in the language of one gender over the other. Since the complete dataset was used to preselect features, the results are inconclusive. Yan &amp; Yan (2006) train a Naive Bayes classifier to predict the gender of a blog entry author. In total they looked at 75,000 individual blog entries authored by 3,000 bloggers, all of them posted their genders on the profile page. They measure precision and recall w.r.t. the minority class (males) and get the best f-measure of 0.64 (precision and recall are 65% and 71%, respectively). Rosenthal &amp; McKeown (2011) predict the age of a blogger, most features they use are extracted from the blog posts, other features include blogger’s interests, the number of friends, the usual time of posting, etc. Similarly to S</context>
</contexts>
<marker>Yan, Yan, 2006</marker>
<rawString>Yan, X. &amp; L. Yan (2006). Gender classification of weblogs authors. In Proceedings of the AAAI Spring Symposium on Computational Approaches for Analyzing Weblogs, Stanford, CA, 27-29 March 2006, pp. 228–230.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
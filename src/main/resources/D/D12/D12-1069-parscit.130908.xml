<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000323">
<title confidence="0.994741">
Weakly Supervised Training of Semantic Parsers
</title>
<author confidence="0.992751">
Jayant Krishnamurthy
</author>
<affiliation confidence="0.993015">
Carnegie Mellon University
</affiliation>
<address confidence="0.815351">
5000 Forbes Avenue
Pittsburgh, PA 15213
</address>
<email confidence="0.998077">
jayantk@cs.cmu.edu
</email>
<author confidence="0.991505">
Tom M. Mitchell
</author>
<affiliation confidence="0.990507">
Carnegie Mellon University
</affiliation>
<address confidence="0.815227">
5000 Forbes Avenue
Pittsburgh, PA 15213
</address>
<email confidence="0.998684">
tom.mitchell@cmu.edu
</email>
<sectionHeader confidence="0.997387" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999555739130435">
We present a method for training a semantic
parser using only a knowledge base and an un-
labeled text corpus, without any individually
annotated sentences. Our key observation is
that multiple forms of weak supervision can be
combined to train an accurate semantic parser:
semantic supervision from a knowledge base,
and syntactic supervision from dependency-
parsed sentences. We apply our approach
to train a semantic parser that uses 77 rela-
tions from Freebase in its knowledge repre-
sentation. This semantic parser extracts in-
stances of binary relations with state-of-the-
art accuracy, while simultaneously recovering
much richer semantic structures, such as con-
junctions of multiple relations with partially
shared arguments. We demonstrate recovery
of this richer structure by extracting logical
forms from natural language queries against
Freebase. On this task, the trained semantic
parser achieves 80% precision and 56% recall,
despite never having seen an annotated logical
form.
</bodyText>
<sectionHeader confidence="0.999472" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9998845">
Semantic parsing converts natural language state-
ments into logical forms in a meaning repre-
sentation language. For example, the phrase
“town in California” might be represented as
Ax.CITY(x) ∧ LOCATEDIN(x, CALIFORNIA), where
CITY, LOCATEDIN and CALIFORNIA are predicates
and entities from a knowledge base. The expressiv-
ity and utility of semantic parsing is derived from
this meaning representation, which is essentially a
program that is directly executable by a computer.
In this sense, broad coverage semantic parsing is the
goal of natural language understanding.
Unfortunately, due to data annotation constraints,
modern semantic parsers only operate in narrow do-
mains. The best performing semantic parsers are
trained using extensive manual annotation: typi-
cally, a number of sentences must be annotated with
their desired logical form. Although other forms of
supervision exist (Clarke et al., 2010; Liang et al.,
2011), these methods similarly require annotations
for individual sentences. More automated training
methods are required to produce semantic parsers
with richer meaning representations.
This paper presents an algorithm for training a se-
mantic parser without per-sentence annotations. In-
stead, our approach exploits two easily-obtainable
sources of supervision: a large knowledge base and
(automatically) dependency-parsed sentences. The
semantic parser is trained to identify relation in-
stances from the knowledge base while simulta-
neously producing parses that syntactically agree
with the dependency parses. Combining these two
sources of supervision allows us to train an accurate
semantic parser for any knowledge base without an-
notated training data.
We demonstrate our approach by training a Com-
binatory Categorial Grammar (CCG) (Steedman,
1996) that parses sentences into logical forms con-
taining any of 77 relations from Freebase. Our
training data consists of relation instances from
Freebase and automatically dependency-parsed sen-
tences from a web corpus. The trained semantic
parser extracts binary relations with state-of-the-art
performance, while recovering considerably richer
semantic structure. We demonstrate recovery of this
semantic structure using natural language queries
</bodyText>
<page confidence="0.979906">
754
</page>
<note confidence="0.848122666666667">
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 754–765, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics
in Lex California Lex
</note>
<equation confidence="0.990528333333333">
(N\N)/N : Af.Ag.Ax.3y. f (y) ∧ g(x) ∧ LOCATEDIN(x, y) N : Ax.x = CALIFORNIA
N\N : Ag.Ax.3y.y = CALIFORNIA ∧ g(x) ∧ LOCATEDIN(x, y)
N : Ax.3y.y = CALIFORNIA ∧ CITY(x) ∧ LOCATEDIN(x, y)
</equation>
<figureCaption confidence="0.898209666666667">
Figure 1: An example parse of “town in California” using the example CCG lexicon. The first stage in parsing
retrieves a category from each word from the lexicon, represented by the “Lex” entries. The second stage applies CCG
combination rules, in this case both forms of function application, to combine these categories into a semantic parse.
</figureCaption>
<figure confidence="0.61856475">
town
N : Ax.CITY(x)Lex
�
�
</figure>
<bodyText confidence="0.997443083333333">
against Freebase. Our weakly-supervised semantic
parser predicts the correct logical form for 56% of
queries, despite never seeing a labeled logical form.
This paper is structured as follows. We first pro-
vide some background information on CCG and the
structure of a knowledge base in Section 2. Section
3 formulates the weakly supervised training prob-
lem for semantic parsers and presents our algorithm.
Section 4 describes how we applied our algorithm to
construct a semantic parser for Freebase, and Sec-
tion 5 presents our results. We conclude with related
work and discussion.
</bodyText>
<sectionHeader confidence="0.99875" genericHeader="introduction">
2 Background
</sectionHeader>
<subsectionHeader confidence="0.97878">
2.1 Combinatory Categorial Grammar
</subsectionHeader>
<bodyText confidence="0.999415285714286">
Combinatory Categorial grammar (CCG) is a lin-
guistic formalism that represents both the syntax and
semantics of language (Steedman, 1996). CCG is a
lexicalized formalism that encodes all grammatical
information in a lexicon A. This lexicon contains
syntactic and semantic categories for each word. A
lexicon may include entries such as:
</bodyText>
<equation confidence="0.99938375">
town := N : Ax.CITY(x)
California := N : Ax.x = CALIFORNIA
in := (N\N)/N : Af.Ag.Ax.
Ely.f(y) ∧ g(x) ∧ LOCATEDIN(x, y)
</equation>
<bodyText confidence="0.993276272727273">
Each entry of the lexicon w := s : l maps a word or
short phrase w to a syntactic category s and a logical
form l. Syntactic categories s may be atomic (N) or
complex (N\N). Logical forms l are lambda calcu-
lus expressions constructed using predicates from a
knowledge base. These logical forms combine dur-
ing parsing to form a complete logical form for the
parsed text.
Parses are constructed by combining adjacent cat-
egories using several combination rules, such as for-
ward (&gt;) and backward (&lt;) application:
</bodyText>
<construct confidence="0.483473">
X/Y : f Y : g =� X : f(g) (&gt;)
Y : g X\Y : f =� X : f(g) (&lt;)
</construct>
<bodyText confidence="0.998865">
These rules mean that the complex category X/Y
(X\Y ) behaves like a function which accepts an ar-
gument of type Y on its right (left) and returns a
value of type X. Parsing amounts to sequentially
applying these two rules, as shown in Figure 1. The
result of parsing is an ordered pair, containing both
a syntactic parse tree and an associated logical form.
We refer to such an ordered pair as a semantic parse,
or by using the letter E.
Given a lexicon, there may be multiple seman-
tic parses E for a given phrase w. Like context-free
grammars (CFGs), CCGs can be extended to repre-
sent a probability distribution over parses P( |w; 0)
where 0 is a parameter vector.
</bodyText>
<subsectionHeader confidence="0.998813">
2.2 Knowledge Base
</subsectionHeader>
<bodyText confidence="0.999935375">
The main input to our system is a propositional
knowledge base K = (E, R, C, A), containing
entities E, categories C, relations R and relation
instances A. Categories and relations are pred-
icates which operate on entities and return truth
values; categories c E C are one-place predi-
cates (CITY(e)) and relations r E R are two-
place predicates (LOCATEDIN(e1, e2)). Entities e E
E represent real-world entities and have a set of
known text names. For example, CALIFORNIA
is an entity whose text names include “Califor-
nia” and “CA.” Relation instances r(e1, e2) E A
are facts asserted by the knowledge base, such
as LOCATEDIN(SACRAMENTO, CALIFORNIA). Ex-
amples of such knowledge bases include Freebase
(Bollacker et al., 2008), NELL (Carlson et al.,
2010), and YAGO (Suchanek et al., 2007).
The knowledge base influences the semantic
parser in two ways. First, CCG logical forms are
constructed by combining categories, relations and
entities from the knowledge base with logical con-
nectives; hence, the predicates in the knowledge
base determine the expressivity of the parser’s se-
mantic representation. Second, the known relation
</bodyText>
<page confidence="0.997501">
755
</page>
<bodyText confidence="0.9972525">
instances r(e1, e2) E A are used as weak supervi-
sion to train the semantic parser.
</bodyText>
<sectionHeader confidence="0.958279" genericHeader="method">
3 Weakly Supervised Semantic Parsing
</sectionHeader>
<bodyText confidence="0.995562333333333">
We define weakly supervised semantic parsing as
the following learning problem.
Input:
</bodyText>
<listItem confidence="0.945709416666667">
1. A knowledge base K = (E, R, C, A), as de-
fined above.
2. A corpus of dependency-parsed sentences S.
3. A CCG lexicon A that produces logical forms
containing predicates from K. Section 4.1 de-
scribes an approach to generate this lexicon.
4. A procedure for identifying mentions of enti-
ties from K in sentences from S. (e.g., simple
string matching).
Output:
1. Parameters θ for the CCG that produce correct
semantic parses ` for sentences s E S.
</listItem>
<bodyText confidence="0.9971995">
This problem is ill-posed without additional as-
sumptions: since the correct logical form for a sen-
tence is never observed, there is no a priori reason
to prefer one semantic parse to another. Our train-
ing algorithm makes two assumptions about correct
semantic parses, which are encoded as weak super-
vision constraints. These constraints make learning
possible by adding an inductive bias:
</bodyText>
<listItem confidence="0.993879833333334">
1. Every relation instance r(e1, e2) E A is ex-
pressed by at least one sentence in S (Riedel
et al., 2010; Hoffmann et al., 2011).
2. The correct semantic parse of a sentence s con-
tains a subset of the syntactic dependencies
contained in a dependency parse of s.
</listItem>
<bodyText confidence="0.999765">
Our weakly supervised training uses these con-
straints as a proxy for labeled semantic parses. The
training algorithm has two steps. First, the algo-
rithm constructs a graphical model that contains
both the semantic parser and constant factors en-
coding the above two constraints. This graphical
model is then used to estimate parameters θ for the
semantic parser, essentially optimizing θ to produce
parses that satisfy the weak supervision constraints.
If our assumptions are correct and sufficiently con-
strain the parameter space, then this procedure will
identify parameters for an accurate semantic parser.
</bodyText>
<subsectionHeader confidence="0.985263">
3.1 Encoding the Weak Supervision
Constraints
</subsectionHeader>
<bodyText confidence="0.993807192307692">
The first step of training constructs a graphical
model containing the semantic parser and two weak
supervision constraints. However, the first weak su-
pervision constraint couples the semantic parses for
every sentence s E S. Such coupling would result in
an undesirably large graphical model. We therefore
modify this constraint to enforce that every relation
r(e1, e2) is expressed at least once in S(e1,e2) C S,
the subset of sentences which mention both e1 and
e2. These mentions are detected using the provided
mention-identification procedure.
Figure 2 depicts the graphical model constructed
for training. The semantic constraint couples the ex-
tractions for all sentences S(e1,e2), so the graphical
model is instantiated once per (e1, e2) tuple. The
model has 4 types of random variables and values:
Si = si represents a sentence, Li = `i represents
a semantic parse, Zi = zi represents the satisfac-
tion of the syntactic constraint and Yr = yr repre-
sents the truth value of relation r. Si, Li and Zi are
replicated once for each sentence s E S(e1,e2), while
Yr is replicated once for each relation type r in the
knowledge base (all r E R).
For each entity pair (e1, e2), this graphical model
defines a conditional distribution over L, Y, Z given
S. This distribution factorizes as:
</bodyText>
<equation confidence="0.977952">
p(Y = y, Z = z, L = fjS = s; θ) =
1
11
Zs r
</equation>
<bodyText confidence="0.999944333333333">
The factorization contains three replicated fac-
tors. F represents the semantic parser, which is
parametrized by θ and produces a semantic parse
`i for each sentence si. IF and 4b are deterministic
factors representing the two weak supervision con-
straints. We now describe each factor in more detail.
</bodyText>
<subsectionHeader confidence="0.969492">
Semantic Parser
</subsectionHeader>
<bodyText confidence="0.999773166666667">
The factor F represents the semantic parser, which
is a log-linear probabilistic CCG using the input lex-
icon A. Given a sentence s and parameters θ, the
parser defines an unnormalized probability distribu-
tion over semantic parses `, each of which includes
both a syntactic CCG parse tree and logical form.
</bodyText>
<figure confidence="0.517214333333333">
11 4b(zi, `i, si)F(si, `i; θ)
(yr, �)
i
</figure>
<page confidence="0.898243">
756
</page>
<figureCaption confidence="0.935692166666667">
Figure 2: Factor graph containing the semantic parser
F and weak supervision constraints IF and 4), instanti-
ated for an (e1, e2) tuple occurring in 2 sentences S1 and
S2, with corresponding semantic parses L1 and L2. The
knowledge base contains 3 relations, represented by the
Y variables.
</figureCaption>
<bodyText confidence="0.995762333333333">
Let f(E, s) represent a feature function mapping se-
mantic parses to vectors of feature values1. The fac-
tor F is then defined as:
</bodyText>
<equation confidence="0.950278">
F(s, E; 0) = exp{BT f(E, s)}
</equation>
<bodyText confidence="0.999922285714286">
If the features f(E, s) factorize according to the
structure of the CCG parse tree, it is possible to
perform exact inference using a CKY-style dynamic
programming algorithm. However, other aspects of
the graphical model preclude exact inference, so we
perform approximate inference using beam search.
Inference is explained in more detail in Section 3.2.
</bodyText>
<subsectionHeader confidence="0.99008">
Semantic Constraint
</subsectionHeader>
<bodyText confidence="0.999984">
The semantic constraint states that, given an entity
tuple (e1, e2), every relation instance r(e1, e2) E A
must be expressed somewhere in S(11,12). Further-
more, no semantic parse can express a relation in-
stance which is not in the knowledge base. This con-
straint is identical to the multiple deterministic-OR
constraint used by Hoffmann et al. (2011) to train a
sentential relation extractor.
The graphical model contains a semantic con-
straint factor IF and one binary variable Yr for each
relation r in the knowledge base. Yr represents
whether r(e1,e2) is expressed by any sentence in
S(11,12). The IF factor determines whether each se-
mantic parse in f extracts a relation between e1 and
e2. It then aggregates these sentence-level extrac-
tions using a deterministic OR: if any sentence ex-
tracts r(e1, e2) then Yr = 1. Otherwise, Yr = 0.
</bodyText>
<footnote confidence="0.7762325">
1Section 4.3 describes the features used by our semantic
parser for Freebase.
</footnote>
<equation confidence="0.99797175">
IF(Yr,f) =
1 if Yr = 1 n li.EXTRACTS(Ei, r, e1, e2)
1 if Yr = 0 n 6li.EXTRACTS(Ei, r, e1, e2)
0 otherwise
</equation>
<bodyText confidence="0.999976583333333">
The EXTRACTS function determines the relation
instances that are asserted by a semantic parse E.
EXTRACTS(E, r, e1, e2) is true if E asserts the rela-
tion r(e1, e2) and false otherwise. This function es-
sentially converts the semantic parser into a senten-
tial relation extractor, and its implementation may
depend on the types of logical connectives included
in the lexicon A. Logical forms in our Freebase se-
mantic parser consist of conjunctions of predicates
from the knowledge base; we therefore define EX-
TRACTS(E, r, e1, e2) as true if E’s logical form con-
tains the clauses r(x, y), x = e1 and y = e2.
</bodyText>
<subsectionHeader confidence="0.992931">
Syntactic Constraint
</subsectionHeader>
<bodyText confidence="0.979252904761905">
A problem with the semantic constraint is that it
admits a large number of ungrammatical parses. The
syntactic constraint penalizes ungrammatical parses
by encouraging the semantic parser to produce parse
trees that agree with a dependency parse of the same
sentence. Specifically, the syntactic constraint re-
quires the predicate-argument structure of the CCG
parse to agree with the predicate-argument structure
of the dependency parse.
Agreement is defined as a function of each CCG
rule application in E. In the parse tree E, each rule
application combines two subtrees, Eh and E,, into a
single tree spanning a larger portion of the sentence.
A rule application is consistent with a dependency
parse t if the head words of Eh and E, have a depen-
dency edge between them in t. AGREE(E, t) is true
if and only if every rule application in E is consistent
with t. This syntactic constraint is encoded in the
graphical model by the 4b factors and Z variables:
4b(z, E, s) = 1 if z = AGREE(E, DEPPARSE(s))
0 otherwise
</bodyText>
<subsectionHeader confidence="0.999309">
3.2 Parameter Estimation
</subsectionHeader>
<bodyText confidence="0.999986">
To train the model, a single training example is con-
structed for every tuple of entities (e1, e2). The in-
put to the model is s = S(11,12), the set of sentences
</bodyText>
<figure confidence="0.933833">
YCAFITALOF
Ψ
YACQUIRED
YLOCATEDIN
Ψ
Ψ
L1
L2
Γ Z1
Γ Z2
Φ
Φ
S1
S2
</figure>
<page confidence="0.979712">
757
</page>
<bodyText confidence="0.981855977272728">
containing e1 and e2. The weak supervision vari-
ables, y, z, are the output of the model. y is con-
structed by setting yr = 1 if r(e1, e2) ∈ A, and 0
otherwise. This setting trains the semantic parser to
extract every true relation instance between (e1, e2)
from some sentence in S(e,,e,), while simultane-
ously avoiding incorrect instances. Finally, z = 1,
to encourage agreement between the semantic and
dependency parses. The training data for the model
is therefore a collection, {(sj,yj,zj)}n j�1, where j
indexes entity tuples (e1, e2).
Training optimizes the semantic parser parame-
ters θ to predict Y = yj, Z = zj given S = sj. The
parameters θ are estimated by running the structured
perceptron algorithm (Collins, 2002) on the training
data defined above. The structured perceptron al-
gorithm iteratively applies a simple update rule for
each example (sj, yj, zj) in the training data:
`predicted ←
`actual ←
θt+1 ←
Each iteration of training requires solving two
maximization problems. The first maximization,
maxe,y,z p(`, y, z|s; θt), is straightforward because y
and z are deterministic functions of `. Therefore,
it is solved by finding the maximum probability as-
signment `, then choosing values for y and z that
satisfy the weak supervision constraints.
The second maximization, maxe p(`|y, z, s; θt), is
more challenging. When y and z are given, the infer-
ence procedure must restrict its search to the parses
` which satisfy these weak supervision constraints.
The original formulation of the IF factors permitted
tractable inference (Hoffmann et al., 2011), but the
EXTRACTS function and the 4b factors preclude ef-
ficient inference. We approximate this maximiza-
tion using beam search over CCG parses `. For each
sentence s, we perform a beam search to produce
k = 300 possible semantic parses. We then check
the value of 4b for each generated parse and elimi-
nate parses which do not satisfy this syntactic con-
straint. Finally, we apply EXTRACTS to each parse,
then use the greedy approximate inference proce-
dure from Hoffmann et al. (2011) for the IF factors.
</bodyText>
<sectionHeader confidence="0.898563" genericHeader="method">
4 Building a Grammar for Freebase
</sectionHeader>
<bodyText confidence="0.9999845">
We apply the training algorithm from the previous
section to produce a semantic parser for a subset of
Freebase. This section describes details of the gram-
mar we construct for this task, including the con-
struction of the lexicon A, some extensions to the
CCG parser, and the features used during training.
In this section, we assume access to a knowledge
base K = (E, C, R, A), a corpus of dependency-
parsed sentences S and a procedure for identifying
mentions of entities in sentences.
</bodyText>
<subsectionHeader confidence="0.997652">
4.1 Constructing the Lexicon A
</subsectionHeader>
<bodyText confidence="0.999956838709677">
The first step in constructing the semantic parser
is defining a lexicon A. We construct A by ap-
plying simple dependency-parse-based heuristics to
sentences in the training corpus. The resulting lex-
icon A captures a variety of linguistic phenomena,
including verbs, common nouns (“city”), noun com-
pounds (“California city”) and prepositional modi-
fiers (“city in California”).
The first step in lexicon construction is to use the
mention identification procedure to identify all men-
tions of entities in the sentences S. This process
results in (e1, e2, s) triples, consisting of sentences
with two entity mentions. The dependency path be-
tween e1 and e2 in s is then matched against the de-
pendency parse patterns in Table 1. Each matched
pattern adds one or more lexical entries to A
Each pattern in Table 1 has a corresponding lexi-
cal category template, which is a CCG lexical cate-
gory containing parameters e, c and r that are chosen
at initialization time. Given the triple (e1, e2, s), re-
lations r are chosen such that r(e1, e2) ∈ A, and
categories c are chosen such that c(e1) ∈ A or
c(e2) ∈ A. The template is then instantiated with
every combination of these e, c and r values.
After instantiating lexical categories for each sen-
tence in S, we prune infrequent lexical categories to
improve parser efficiency. This pruning step is re-
quired because the common noun pattern generates
a large number of lexical categories, the majority
of which are incorrect. Therefore, we eliminate all
common noun categories instantiated by fewer than
</bodyText>
<figure confidence="0.966220444444445">
arg max max p(`,y,z|sj;θt)
e y,z
arg max p(`|yj, zj, sj; θt)
f(`actual i , si)
�− predicted
i f (`i , si)
� e
θt +
i
</figure>
<page confidence="0.973033">
758
</page>
<table confidence="0.999940851851852">
Part of Dependency Parse Pattern Lexical Category Template
Speech
Proper (name of entity e) w := N : λx.x = e
Noun Sacramento Sacramento := N : λx.x = SACRAMENTO
Common SBJ ⇐=== w w := N : λx.c(x)
Noun e1 ===⇒ [is, are, was, ...] OBJ capital := N : λx.CITY(x)
Sacramento is the capital
Noun NMOD Type change N : λx.c(x) to N|N : λf.λx.∃y.c(x) ∧ f(y) ∧ r(x, y)
Modifier e1 ⇐===== e2 N : λx.CITY(x) to N|N : λf.λx.∃y.CITY(x) ∧ f(y) ∧ LOCATEDIN(x, y)
Sacramento, California
Preposition P MOD w := (N\N)/N : λf.λg.λx.∃y.f(y) ∧ g(x) ∧ r(x, y)
NMOD ⇐===== e2 in := (N\N)/N : λf.λg.λx.∃y.f(y) ∧ g(x) ∧ LOCATEDIN(x, y)
e1 ⇐===== w w := PP/N : λf.λx.f(x)
Sacramento in California in := PP/N : λf.λx.f(x)
SBJ ⇐=== w P MOD
e1 ===⇒ VB* ADV ⇐===== e2
Sacramento is located in California
Verb ===⇒ e1SBJw* OBJ ⇐=== e2 w* :=(S\N)/N : λf.λg.∃x, y.f(y) ∧ g(x) ∧ r(x, y)
Sacramento governs California governs := (S\N)/N : λf.λg.∃x, y.f(y) ∧ g(x) ∧ LOCATEDIN(x, y)
SBJ ⇐=== [IN,TO] P MOD w* := (S\N)/PP : λf.λg.∃x, y.f(y) ∧ g(x) ∧ r(x, y)
e1 ===⇒ w* ADV ⇐===== e2 is located := (S\N)/PP : λf.λg.∃x, y.f(y) ∧ g(x) ∧ LOCATEDIN(x, y)
Sacramento is located in California w* := (N\N)/PP : λf.λg.λy.f(y) ∧ g(x) ∧ r(x, y)
NMOD ⇐=== [IN,TO] P MOD located := (N\N)/PP : λf.λg.λy.f(y) ∧ g(x) ∧ LOCATEDIN(x, y)
e1 ⇐===== w* ADV ⇐===== e2
Sacramento located in California
Forms of (none) w* := (S\N)/N : λf.λg.∃x.g(x) ∧ f(x)
“to be”
</table>
<tableCaption confidence="0.871503">
Table 1: Dependency parse patterns used to instantiate lexical categories for the semantic parser lexicon A. Each
pattern is followed by an example phrase that instantiates it. An * indicates a position that may be filled by multiple
consecutive words in the sentence. e1 and e2 are the entities identified in the sentence, r represents a relation where
r(e1, e2), and c represents a category where c(e1). Each template may be instantiated with multiple values for the
variables e, c, r.
</tableCaption>
<bodyText confidence="0.981999296296296">
5 sentences in S. The other rules are less fertile, so
we do not need to prune their output.
In addition to these categories, the grammar in-
cludes type-changing rules from N to NDN. These
rules capture noun compounds by allowing nouns to
become functions from nouns to nouns. There are
several such type-changing rules since the resulting
category includes a hidden relation r between the
noun and its modifier (see Table 1). As with lexical
categories, the set of type changing rules included
in the grammar is determined by matching depen-
dency parse patterns to the training data. Similar
rules for noun compounds are used in other CCG
parsers (Clark and Curran, 2007).
The instantiated lexicon represents the semantics
of words and phrases as conjunctions of predicates
from the knowledge base, possibly including exis-
tentially quantified variables and A expressions. The
syntactic types N and PP are semantically rep-
resented as functions from entities to truth values
(e.g., Ax.CITY(x)), while sentences S are statements
with no A terms, such as Elx, y.x = CALIFORNIA ∧
CITY(y) ∧ LOCATEDIN(x, y). Variables in the seman-
tic representation (x, y) range over entities from the
knowledge base. Intuitively, the N and PP cate-
gories represent sets of entities, while sentences rep-
resent assertions about the world.
</bodyText>
<subsectionHeader confidence="0.995546">
4.2 Extensions to CCG
</subsectionHeader>
<bodyText confidence="0.999946545454546">
The semantic parser is trained using sentences from
a web corpus, which contains many out-of-domain
words. As a consequence, many of the words en-
countered during training cannot be represented us-
ing the vocabulary of predicates from the knowl-
edge base. To handle these extraneous words, we
allow the CCG parser to skip words while parsing
a sentence. During parsing, the parser first decides
whether to retrieve a lexical category for each word
in the sentence. The sentence is then parsed as if
only the retrieved lexical categories existed.
</bodyText>
<subsectionHeader confidence="0.9841">
4.3 Features
</subsectionHeader>
<bodyText confidence="0.999712777777778">
The features f(E, s) for our probabilistic CCG con-
tain two sets of features. The first set contains lexi-
cal features, which count the number of times each
lexical entry is used in E. The second set contains
rule application features, which count the number
of times each combination rule is applied to each
possible set of arguments. An argument is defined
by its syntactic and semantic category, and in some
cases by the lexical entry which created it. We lex-
</bodyText>
<page confidence="0.997033">
759
</page>
<bodyText confidence="0.999918">
icalize arguments for prepositional phrases PP and
common nouns (initialized by the second rule in Ta-
ble 1). This lexicalization allows the parser to dis-
tinguish between prepositional phrases headed by
different prepositions, as well as between different
common nouns. All other types are distinguished
solely by syntactic and semantic category.
</bodyText>
<sectionHeader confidence="0.998783" genericHeader="method">
5 Evaluation
</sectionHeader>
<bodyText confidence="0.9999414375">
In this section, we evaluate the performance of
a semantic parser for Freebase, trained using our
weakly-supervised algorithm. Empirical compari-
son is somewhat difficult because the most compara-
ble previous work – weakly-supervised relation ex-
traction – uses a shallower semantic representation.
Our evaluation therefore has two components: (1) a
binary relation extraction task, to demonstrate that
the trained semantic parser extracts instances of bi-
nary relations with performance comparable to other
state-of-the-art systems, and (2) a natural language
database query task, to demonstrate the parser’s abil-
ity to extract more complex logical forms than bi-
nary relation instances, such as logical expressions
involving conjunctions of multiple categories and re-
lations with partially shared arguments.
</bodyText>
<subsectionHeader confidence="0.992082">
5.1 Corpus Construction
</subsectionHeader>
<bodyText confidence="0.999966470588235">
Our experiments use a subset of 77 relations2 from
Freebase3 as the knowledge base and a corpus of
web sentences. We constructed the sentence corpus
by first sampling sentences from a web crawl and
parsing them with MaltParser (Nivre et al., 2006).
Long sentences tended to have noisy parses while
also rarely expressing relations, so we discarded
sentences longer than 10 words. Entities were iden-
tified by performing a simple string match between
canonical entity names in Freebase and proper noun
phrases identified by the parser. In cases where a
single noun phrase matched multiple entities, we se-
lected the entity participating in the most relations.
The resulting corpus contains 2.5 million (e1, e2, s)
triples, from which we reserved 10% for validation
and 10% for testing. The validation set was used
to estimate performance during algorithm develop-
</bodyText>
<footnote confidence="0.999765333333333">
2These relations are defined by a set of MQL queries and
potentially traverse multiple relation links.
3http://www.freebase.com
</footnote>
<table confidence="0.999793916666667">
Relation Name Relation Sentences
Instances
CITYLOCATEDINSTATE 2951 13422
CITYLOCATEDINCOUNTRY 1696 7904
CITYOFPERSONBIRTH 397 440
COMPANIESHEADQUARTEREDHERE 326 432
MUSICARTISTMUSICIAN 251 291
CITYUNIVERSITIES 239 338
CITYCAPITALOFCOUNTRY 123 2529
HASHUSBAND 103 367
PARENTOFPERSON 85 356
HASSPOUSE 81 461
</table>
<tableCaption confidence="0.9930466">
Table 2: Occurrence statistics for the 10 most frequent
relations in the training data. “Relation Instances” shows
the number of entity tuples (e1, e2) that appear as positive
examples for each relation, and “Sentences” shows the
total number of sentences in which these tuples appear.
</tableCaption>
<bodyText confidence="0.999555307692308">
ment, while the test set was used to generate the fi-
nal experimental results. All triples for each (e1, e2)
tuple were placed in the same set.
Approximately 1% of the resulting (e1, e2, s)
triples are positive examples, meaning there exists
some relation r where r(e1, e2) E 04. To improve
training efficiency and prediction performance, we
subsample 5% of the negative examples for training,
producing a training set of 125k sentences with 27k
positive examples. The validation and test sets retain
the original positive/negative ratio. Table 2 shows
some statistics of the most frequent relations in the
test set.
</bodyText>
<subsectionHeader confidence="0.998218">
5.2 Relation Extraction
</subsectionHeader>
<bodyText confidence="0.999301384615385">
The first experiment measures the semantic parser’s
ability to extract relations from sentences in our web
corpus. We compare our semantic parser to MUL-
TIR (Hoffmann et al., 2011), which is a state-of-
the-art weakly supervised relation extractor. This
method uses the same weak supervision constraint
and parameter estimation procedure, but replaces the
semantic parser by a linear classifier. The features
for this classifier include the dependency path be-
tween the entity mentions, the type of each mention,
and the intervening context (Mintz et al., 2009).
Both the semantic parser and MULTIR were
trained by running 5 iterations of the structured per-
</bodyText>
<footnote confidence="0.877385333333333">
4Note that the positive/negative ratio was much lower with-
out the length filter or entity disambiguation, which is partly
why filtering was performed.
</footnote>
<page confidence="0.992764">
760
</page>
<figureCaption confidence="0.995680666666667">
Figure 3: Aggregate precision as a function of recall, for
MULTIR (Hoffman et al., 2011) and our three semantic
parser variants.
</figureCaption>
<figure confidence="0.9818">
0 600 1200 1800 2400 3000
</figure>
<figureCaption confidence="0.999932">
Figure 4: Sentential precision as a function of the ex-
pected number of correct extractions for MULTIR (Hoff-
man et al., 2011) and our three semantic parser variants.
</figureCaption>
<bodyText confidence="0.999090055555556">
ceptron algorithm5. At test time, both models pre-
dicted a relation r E R or NONE for each (e1, e2, s)
triple in the test set. The parser parses the sen-
tence without considering the entities marked in the
sentence, then applies the EXTRACTS function de-
fined in Section 3.1 to identify a relation between e1
and e2. We compare three versions of the semantic
parser: PARSE, which is the basic semantic parser,
PARSE+DEP which additionally observes the cor-
rect dependency parse at test time, and PARSE-DEP
which is trained without the syntactic constraint.
Note that MULTIR uses the sentence’s dependency
parse to construct its feature vector.
Our evaluation considers two performance mea-
sures: aggregate and sentential precision/recall. Ag-
gregate precision takes the union of all extracted re-
lation instances r(e1, e2) from the test corpus and
compares these instances to Freebase. To pro-
</bodyText>
<footnote confidence="0.998972333333333">
5The structured perceptron algorithm does not converge to a
parameter estimate, and we empirically found that performance
did not improve beyond 5 iterations.
</footnote>
<figureCaption confidence="0.977314666666667">
Figure 5: Aggregate precision as a function of recall,
ignoring the two most frequent relations, CITYLOCATE-
DINSTATE and CITYLOCATEDINCOUNTRY.
</figureCaption>
<bodyText confidence="0.934832411764706">
duce a precision/recall curve, each extracted in-
stance r(e1, e2) is assigned the maximum score over
all sentences which extracted it. This metric is easy
to compute, but may be inaccurate due to inaccura-
cies and missing relations in Freebase.
Sentential precision computes the precision of ex-
tractions on individual (e1, e2, s) tuples. This met-
ric is evaluated by manually sampling and evaluat-
ing 100 test sentences from which a relation was ex-
tracted per model. Unfortunately, it is difficult to
compute recall for this metric, since the true number
of sentences expressing relations is unknown. We
instead report precision as a function of the expected
number of correct extractions, which is directly pro-
portional to recall.
Figure 3 displays aggregate precision/recall and
Figure 4 displays sentential precision/recall for all
4 models. Generally, PARSE behaves like MUL-
TIR with somewhat lower recall. In the sentential
evaluation, PARSE+DEP outperforms both PARSE
and MULTIR. The difference between PARSE+DEP’s
aggregate and sentential precision stems from the
fact that PARSE+DEP extracts each relation instance
from more sentences than either MULTIR or PARSE.
PARSE-DEP has the worst performance in both eval-
uations, suggesting the importance of syntactic su-
pervision. Precision in the aggregate experiment is
low partially due to examples with incorrect entity
disambiguation.
We found that the skewed distribution of relation
types hides interesting differences between the mod-
els. Therefore, we include Figure 5 comparing our
syntactically-supervised parsers to MULTIR, ignor-
ing the two most frequent relations (which together
</bodyText>
<figure confidence="0.998475419354839">
MULTIR
PARSE+DEP
PARSE
PARSE-DEP
0.4
0.2
0
0 0.2 0.4 0.6 0.8 1.0
1.0
0.8
0.6
MULTIR
PARSE+DEP
PARSE
PARSE-DEP
0.4
0.2
0
1.0
0.8
0.6
MULTIR
PARSE+DEP
PARSE
0.6
0.4
0.2
0
0 0.2 0.4 0.6 0.8 1.0
1.0
0.8
</figure>
<page confidence="0.992319">
761
</page>
<bodyText confidence="0.999781933333333">
make up over half of all relation instances). Both
PARSE and PARSE+DEP are considerably more pre-
cise than MULTIR on these less frequent relations
because their compositional meaning representation
shares parameter strength between relations. For
example, the semantic parsers learn that “in” often
combines with a city to form a prepositional phrase;
the parsers can apply this knowledge to identify city
arguments of any relation. However, MULTIR is ca-
pable of higher recall, since its dependency parse
features can represent syntactic dependencies that
cannot be represented by our semantic parsers. This
limitation is a consequence of our heuristic lexicon
initialization procedure, and could be rectified by a
more flexible initialization procedure.
</bodyText>
<subsectionHeader confidence="0.996948">
5.3 Natural Language Database Queries
</subsectionHeader>
<bodyText confidence="0.99957864516129">
The second experiment measures our trained
parser’s ability to correctly translate natural lan-
guage queries into logical queries against Freebase.
To avoid biasing the evaluation, we constructed
a test corpus of natural language queries in a data-
driven fashion. We searched the test data for sen-
tences with two related entities separated by an “is
a” expression. The portion of the sentence before the
“is a” expression was discarded and the remainder
retained as a candidate query. For example “Jesse is
an author from Austin, Texas,” was converted into
the candidate query “author from Austin, Texas.”
Each candidate query was then annotated with a log-
ical form using categories and relations from the
knowledge base; candidate queries without satisfac-
tory logical forms were discarded. We annotated 50
validation and 50 test queries in this fashion. The
validation set was used to estimate performance dur-
ing algorithm development and the test set was used
to generate the final results. Example queries with
their annotated logical forms are shown in Table 3.
Table 4 displays the results of the query evalua-
tion. For this evaluation, we forced the parser to in-
clude every word of the query in the parse. Precision
is the percentage of successfully parsed queries for
which the correct logical form was predicted. Re-
call is the percentage of all queries for which the
correct logical form was predicted. This evalua-
tion demonstrates that the semantic parser success-
fully interprets common nouns and identifies mul-
tiple relations with shared arguments. The perfor-
</bodyText>
<table confidence="0.999117111111111">
Example Query Logical Form
capital of Russia λx.CITYCAPITALOFCOUNTRY(x, RUSSIA)
wife of Abraham λx.HASHUSBAND(x, ABRAHAM)
vocalist from λx.MUSICIAN(x)∧
London, England PERSONBORNIN(x, LONDON)∧
CITYINCOUNTRY(LONDON, ENGLAND)
home of λx.HEADQUARTERS(CONOCOPHILLIPS, x)
ConocoPhillips ∧CITYINCOUNTRY(x, CANADA)
in Canada
</table>
<tableCaption confidence="0.942416">
Table 3: Example natural language queries and their cor-
rect annotated logical form.
</tableCaption>
<table confidence="0.999929333333333">
Precision Recall
PARSE 0.80 0.56
PARSE-DEP 0.45 0.32
</table>
<tableCaption confidence="0.984122">
Table 4: Precision and recall for predicting logical forms
of natural language queries against Freebase. The table
compares PARSE, trained with syntactic supervision to
PARSE-DEP, trained without syntactic supervision.
</tableCaption>
<bodyText confidence="0.997133727272727">
mance difference between PARSE and PARSE-DEP
also demonstrates the benefit of including syntactic
supervision.
Examining the system output, we find two ma-
jor sources of error. The first is missing lexical cat-
egories for uncommon words (e.g., “ex-guitarist”),
which negatively impact recall by making some
queries unparsable. The second is difficulty distin-
guishing between relations with similar type signa-
tures, such as CITYLOCATEDINCOUNTRY and CITY-
CAPITALOFCOUNTRY.
</bodyText>
<sectionHeader confidence="0.999987" genericHeader="method">
6 Related Work
</sectionHeader>
<bodyText confidence="0.9998930625">
There are many approaches to supervised seman-
tic parsing, including inductive logic programming
(Zelle and Mooney, 1996), probabilistic and syn-
chronous grammars (Ge and Mooney, 2005; Wong
and Mooney, 2006; Wong and Mooney, 2007; Lu et
al., 2008), and automatically learned transformation
rules (Kate et al., 2005). This work most closely
follows the work on semantic parsing using CCG
(Zettlemoyer and Collins, 2005; Zettlemoyer and
Collins, 2007; Kwiatkowski et al., 2010). These su-
pervised systems are all trained with annotated sen-
tence/logical form pairs; hence these approaches are
labor intensive and do not scale to broad domains
with large numbers of predicates.
Several recent papers have attempted to reduce
the amount of human supervision required to train
</bodyText>
<page confidence="0.987118">
762
</page>
<bodyText confidence="0.999903814814815">
a semantic parser. One line of work eliminates the
need for an annotated logical form, instead using
only the correct answer for a database query (Liang
et al., 2011) or even a binary correct/incorrect sig-
nal (Clarke et al., 2010). This type of feedback may
be easier to obtain than full logical forms, but still
requires individually annotated sentences. Other ap-
proaches are completely unsupervised, but do not tie
the language to an existing meaning representation
(Poon and Domingos, 2009). It is also possible to
self-train a semantic parser without any labeled data
(Goldwasser et al., 2011). However, this approach
does not perform as well as more supervised ap-
proaches, since the parser’s self-training predictions
are not constrained by the correct logical form.
Recent research has produced several weakly su-
pervised relation extractors (Craven and Kumlien,
1999; Mintz et al., 2009; Wu and Weld, 2010; Riedel
et al., 2010; Hoffmann et al., 2011). These sys-
tems scale up to hundreds of predicates, but have
much shallower semantic representations than se-
mantic parsers. For example, these systems can-
not be directly used to respond to natural language
queries. This work extends weakly supervised rela-
tion extraction to produce richer semantic structure,
using only slightly more supervision in the form of
dependency parses.
</bodyText>
<sectionHeader confidence="0.999744" genericHeader="discussions">
7 Discussion
</sectionHeader>
<bodyText confidence="0.998438345454546">
This paper presents a method for training a seman-
tic parser using only a knowledge base and a cor-
pus of unlabeled sentences. Our key observation is
that multiple forms of weak supervision can be com-
bined to train an accurate semantic parser: semantic
supervision from a knowledge base of facts, and syn-
tactic supervision in the form of a standard depen-
dency parser. We presented an algorithm for train-
ing a semantic parser in the form of a probabilistic
Combinatory Categorial Grammar, using these two
types of weak supervision. We used this algorithm
to train a semantic parser for an ontology of 77 Free-
base predicates, using Freebase itself as the weak se-
mantic supervision.
Experimental results show that our trained se-
mantic parser extracts binary relations as well as
a state-of-the-art weakly supervised relation extrac-
tor (Hoffmann et al., 2011). Further experiments
tested our trained parser’s ability to extract more
complex meanings from sentences, including logi-
cal forms involving conjunctions of multiple relation
and category predicates with shared arguments (e.g.,
Ax.MUSICIAN(x) ∧ PERSONBORNIN(x, LONDON) ∧
CITYINCOUNTRY(LONDON, ENGLAND)). To test this
capability, we applied the trained parser to natural
language queries against Freebase. The semantic
parser correctly interpreted 56% of these queries,
despite the broad domain and never having seen an
annotated logical form. Together, these two experi-
mental analyses suggest that the combination of syn-
tactic and semantic weak supervision is indeed a suf-
ficient basis for training semantic parsers for a di-
verse range of corpora and predicate ontologies.
One limitation of our method is the reliance on
hand-built dependency parse patterns for lexicon ini-
tialization. Although these patterns capture a va-
riety of linguistic phenomena, they require manual
engineering and may miss important relations. An
area for future work is developing an automated
way to produce this lexicon, perhaps by extend-
ing the recent work on automatic lexicon generation
(Kwiatkowski et al., 2010) to the weakly supervised
setting. Such an algorithm seems especially impor-
tant if one wishes to model phenomena such as ad-
jectives, which are difficult to initialize heuristically
without generating large numbers of lexical entries.
An elegant aspect of semantic parsing is that it is
easily extensible to include more complex linguis-
tic phenomena, such as quantification and events
(multi-argument relations). In the future, we plan
to increase the expressivity of our parser’s mean-
ing representation to capture more linguistic and se-
mantic phenomena. In this fashion, we can make
progress toward broad coverage semantic parsing,
and thus natural language understanding.
</bodyText>
<sectionHeader confidence="0.998137" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999934444444445">
This research has been supported in part by DARPA
under contract number FA8750-09-C-0179, and by a
grant from Google. Additionally, we thank Yahoo!
for use of their M45 cluster. We also gratefully ac-
knowledge the contributions of our colleagues on the
NELL project, Justin Betteridge for collecting the
Freebase relations, Jamie Callan and colleagues for
the web crawl, and Thomas Kollar and Matt Gardner
for helpful comments on earlier drafts of this paper.
</bodyText>
<page confidence="0.997572">
763
</page>
<sectionHeader confidence="0.998345" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999917761904762">
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: a col-
laboratively created graph database for structuring hu-
man knowledge. In Proceedings of the 2008 ACM
SIGMOD International Conference on Management of
Data, pages 1247–1250.
Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr
Settles, Estevam R. Hruschka Jr., and Tom M.
Mitchell. 2010. Toward an architecture for never-
ending language learning. In Proceedings of the
Twenty-Fourth AAAI Conference on Artificial Intelli-
gence.
Stephen Clark and James R. Curran. 2007. Wide-
coverage efficient statistical parsing with CCG and
log-linear models. Computational Linguistics,
33(4):493–552.
James Clarke, Dan Goldwasser, Ming-Wei Chang, and
Dan Roth. 2010. Driving semantic parsing from
the world’s response. In Proceedings of the Four-
teenth Conference on Computational Natural Lan-
guage Learning.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: theory and experi-
ments with perceptron algorithms. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing.
Mark Craven and Johan Kumlien. 1999. Constructing
biological knowledge bases by extracting information
from text sources. In Proceedings of the Seventh Inter-
national Conference on Intelligent Systems for Molec-
ular Biology.
Ruifang Ge and Raymond J. Mooney. 2005. A statistical
semantic parser that integrates syntax and semantics.
In Proceedings of the Ninth Conference on Computa-
tional Natural Language Learning.
Dan Goldwasser, Roi Reichart, James Clarke, and Dan
Roth. 2011. Confidence driven unsupervised semantic
parsing. Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics.
Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke S.
Zettlemoyer, and Daniel S. Weld. 2011. Knowledge-
based weak supervision for information extraction of
overlapping relations. In The 49th Annual Meeting
of the Association for Computational Linguistics: Hu-
man Language Technologies.
Rohit J. Kate, Yuk Wah Wong, and Raymond J. Mooney.
2005. Learning to transform natural to formal lan-
guages. In Proceedings, The Twentieth National Con-
ference on Artificial Intelligence and the Seventeenth
Innovative Applications of Artificial Intelligence Con-
ference.
Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwa-
ter, and Mark Steedman. 2010. Inducing probabilistic
CCG grammars from logical form with higher-order
unification. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Processing.
Percy Liang, Michael I. Jordan, and Dan Klein. 2011.
Learning dependency-based compositional semantics.
In Proceedings of the Association for Computational
Linguistics, Portland, Oregon. Association for Com-
putational Linguistics.
Wei Lu, Hwee Tou Ng, Wee Sun Lee, and Luke S. Zettle-
moyer. 2008. A generative model for parsing natural
language to meaning representations. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing.
Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky.
2009. Distant supervision for relation extraction with-
out labeled data. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and the 4th
International Joint Conference on Natural Language
Processing of the AFNLP.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006. Malt-
parser: A data-driven parser-generator for dependency
parsing. In Proceedings of the 21st International Con-
ference on Computational Linguistics and 44th Annual
Meeting of the Association for Computational Linguis-
tics.
Hoifung Poon and Pedro Domingos. 2009. Unsuper-
vised semantic parsing. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing.
Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling relations and their mentions with-
out labeled text. In Proceedings of the 2010 European
conference on Machine learning and Knowledge Dis-
covery in Databases.
Mark Steedman. 1996. Surface Structure and Interpre-
tation. The MIT Press.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: a core of semantic knowl-
edge. In Proceedings of the 16th international confer-
ence on World Wide Web, WWW ’07, pages 697–706,
New York, NY, USA. ACM.
Yuk Wah Wong and Raymond J. Mooney. 2006. Learn-
ing for semantic parsing with statistical machine trans-
lation. In Proceedings of the Human Language Tech-
nology Conference of the NAACL.
Yuk Wah Wong and Raymond J. Mooney. 2007. Learn-
ing synchronous grammars for semantic parsing with
lambda calculus. In Proceedings of the 45th Annual
Meeting of the Association for Computational Linguis-
tics.
Fei Wu and Daniel S. Weld. 2010. Open information
extraction using Wikipedia. In Proceedings of the 48th
</reference>
<page confidence="0.977857">
764
</page>
<reference confidence="0.9969995">
Annual Meeting of the Association for Computational
Linguistics.
John M. Zelle and Raymond J. Mooney. 1996. Learning
to parse database queries using inductive logic pro-
gramming. In Proceedings of the thirteenth national
conference on Artificial Intelligence.
Luke S. Zettlemoyer and Michael Collins. 2005. Learn-
ing to map sentences to logical form: structured clas-
sification with probabilistic categorial grammars. In
UAI ’05, Proceedings of the 21st Conference in Un-
certainty in Artificial Intelligence.
Luke S. Zettlemoyer and Michael Collins. 2007. Online
learning of relaxed ccg grammars for parsing to logical
form. In Proceedings of the 2007 Joint Conference on
Empirical Methods in Natural Language Processing
and Computational Natural Language Learning.
</reference>
<page confidence="0.998177">
765
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.845425">
<title confidence="0.999072">Weakly Supervised Training of Semantic Parsers</title>
<author confidence="0.951087">Jayant</author>
<affiliation confidence="0.99513">Carnegie Mellon</affiliation>
<address confidence="0.987541">5000 Forbes Pittsburgh, PA</address>
<email confidence="0.999613">jayantk@cs.cmu.edu</email>
<author confidence="0.999482">M Tom</author>
<affiliation confidence="0.995949">Carnegie Mellon</affiliation>
<address confidence="0.983287">5000 Forbes Pittsburgh, PA</address>
<email confidence="0.999552">tom.mitchell@cmu.edu</email>
<abstract confidence="0.99793225">We present a method for training a semantic parser using only a knowledge base and an unlabeled text corpus, without any individually annotated sentences. Our key observation is that multiple forms of weak supervision can be combined to train an accurate semantic parser: supervision a knowledge base, supervision dependencyparsed sentences. We apply our approach to train a semantic parser that uses 77 relations from Freebase in its knowledge representation. This semantic parser extracts instances of binary relations with state-of-theart accuracy, while simultaneously recovering much richer semantic structures, such as conjunctions of multiple relations with partially shared arguments. We demonstrate recovery of this richer structure by extracting logical forms from natural language queries against Freebase. On this task, the trained semantic parser achieves 80% precision and 56% recall, despite never having seen an annotated logical form.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Kurt Bollacker</author>
<author>Colin Evans</author>
<author>Praveen Paritosh</author>
<author>Tim Sturge</author>
<author>Jamie Taylor</author>
</authors>
<title>Freebase: a collaboratively created graph database for structuring human knowledge.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data,</booktitle>
<pages>1247--1250</pages>
<contexts>
<context position="7367" citStr="Bollacker et al., 2008" startWordPosition="1154" endWordPosition="1157">s E, categories C, relations R and relation instances A. Categories and relations are predicates which operate on entities and return truth values; categories c E C are one-place predicates (CITY(e)) and relations r E R are twoplace predicates (LOCATEDIN(e1, e2)). Entities e E E represent real-world entities and have a set of known text names. For example, CALIFORNIA is an entity whose text names include “California” and “CA.” Relation instances r(e1, e2) E A are facts asserted by the knowledge base, such as LOCATEDIN(SACRAMENTO, CALIFORNIA). Examples of such knowledge bases include Freebase (Bollacker et al., 2008), NELL (Carlson et al., 2010), and YAGO (Suchanek et al., 2007). The knowledge base influences the semantic parser in two ways. First, CCG logical forms are constructed by combining categories, relations and entities from the knowledge base with logical connectives; hence, the predicates in the knowledge base determine the expressivity of the parser’s semantic representation. Second, the known relation 755 instances r(e1, e2) E A are used as weak supervision to train the semantic parser. 3 Weakly Supervised Semantic Parsing We define weakly supervised semantic parsing as the following learning</context>
</contexts>
<marker>Bollacker, Evans, Paritosh, Sturge, Taylor, 2008</marker>
<rawString>Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. 2008. Freebase: a collaboratively created graph database for structuring human knowledge. In Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data, pages 1247–1250.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Carlson</author>
<author>Justin Betteridge</author>
<author>Bryan Kisiel</author>
<author>Burr Settles</author>
<author>Estevam R Hruschka Jr</author>
<author>Tom M Mitchell</author>
</authors>
<title>Toward an architecture for neverending language learning.</title>
<date>2010</date>
<booktitle>In Proceedings of the Twenty-Fourth AAAI Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="7396" citStr="Carlson et al., 2010" startWordPosition="1159" endWordPosition="1162">and relation instances A. Categories and relations are predicates which operate on entities and return truth values; categories c E C are one-place predicates (CITY(e)) and relations r E R are twoplace predicates (LOCATEDIN(e1, e2)). Entities e E E represent real-world entities and have a set of known text names. For example, CALIFORNIA is an entity whose text names include “California” and “CA.” Relation instances r(e1, e2) E A are facts asserted by the knowledge base, such as LOCATEDIN(SACRAMENTO, CALIFORNIA). Examples of such knowledge bases include Freebase (Bollacker et al., 2008), NELL (Carlson et al., 2010), and YAGO (Suchanek et al., 2007). The knowledge base influences the semantic parser in two ways. First, CCG logical forms are constructed by combining categories, relations and entities from the knowledge base with logical connectives; hence, the predicates in the knowledge base determine the expressivity of the parser’s semantic representation. Second, the known relation 755 instances r(e1, e2) E A are used as weak supervision to train the semantic parser. 3 Weakly Supervised Semantic Parsing We define weakly supervised semantic parsing as the following learning problem. Input: 1. A knowled</context>
</contexts>
<marker>Carlson, Betteridge, Kisiel, Settles, Jr, Mitchell, 2010</marker>
<rawString>Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr Settles, Estevam R. Hruschka Jr., and Tom M. Mitchell. 2010. Toward an architecture for neverending language learning. In Proceedings of the Twenty-Fourth AAAI Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>James R Curran</author>
</authors>
<title>Widecoverage efficient statistical parsing with CCG and log-linear models.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>4</issue>
<contexts>
<context position="22322" citStr="Clark and Curran, 2007" startWordPosition="3724" endWordPosition="3727">ile, so we do not need to prune their output. In addition to these categories, the grammar includes type-changing rules from N to NDN. These rules capture noun compounds by allowing nouns to become functions from nouns to nouns. There are several such type-changing rules since the resulting category includes a hidden relation r between the noun and its modifier (see Table 1). As with lexical categories, the set of type changing rules included in the grammar is determined by matching dependency parse patterns to the training data. Similar rules for noun compounds are used in other CCG parsers (Clark and Curran, 2007). The instantiated lexicon represents the semantics of words and phrases as conjunctions of predicates from the knowledge base, possibly including existentially quantified variables and A expressions. The syntactic types N and PP are semantically represented as functions from entities to truth values (e.g., Ax.CITY(x)), while sentences S are statements with no A terms, such as Elx, y.x = CALIFORNIA ∧ CITY(y) ∧ LOCATEDIN(x, y). Variables in the semantic representation (x, y) range over entities from the knowledge base. Intuitively, the N and PP categories represent sets of entities, while sente</context>
</contexts>
<marker>Clark, Curran, 2007</marker>
<rawString>Stephen Clark and James R. Curran. 2007. Widecoverage efficient statistical parsing with CCG and log-linear models. Computational Linguistics, 33(4):493–552.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Clarke</author>
<author>Dan Goldwasser</author>
<author>Ming-Wei Chang</author>
<author>Dan Roth</author>
</authors>
<title>Driving semantic parsing from the world’s response.</title>
<date>2010</date>
<booktitle>In Proceedings of the Fourteenth Conference on Computational Natural Language Learning.</booktitle>
<contexts>
<context position="2173" citStr="Clarke et al., 2010" startWordPosition="311" endWordPosition="314">m a knowledge base. The expressivity and utility of semantic parsing is derived from this meaning representation, which is essentially a program that is directly executable by a computer. In this sense, broad coverage semantic parsing is the goal of natural language understanding. Unfortunately, due to data annotation constraints, modern semantic parsers only operate in narrow domains. The best performing semantic parsers are trained using extensive manual annotation: typically, a number of sentences must be annotated with their desired logical form. Although other forms of supervision exist (Clarke et al., 2010; Liang et al., 2011), these methods similarly require annotations for individual sentences. More automated training methods are required to produce semantic parsers with richer meaning representations. This paper presents an algorithm for training a semantic parser without per-sentence annotations. Instead, our approach exploits two easily-obtainable sources of supervision: a large knowledge base and (automatically) dependency-parsed sentences. The semantic parser is trained to identify relation instances from the knowledge base while simultaneously producing parses that syntactically agree w</context>
<context position="36085" citStr="Clarke et al., 2010" startWordPosition="5844" endWordPosition="5847">CG (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2010). These supervised systems are all trained with annotated sentence/logical form pairs; hence these approaches are labor intensive and do not scale to broad domains with large numbers of predicates. Several recent papers have attempted to reduce the amount of human supervision required to train 762 a semantic parser. One line of work eliminates the need for an annotated logical form, instead using only the correct answer for a database query (Liang et al., 2011) or even a binary correct/incorrect signal (Clarke et al., 2010). This type of feedback may be easier to obtain than full logical forms, but still requires individually annotated sentences. Other approaches are completely unsupervised, but do not tie the language to an existing meaning representation (Poon and Domingos, 2009). It is also possible to self-train a semantic parser without any labeled data (Goldwasser et al., 2011). However, this approach does not perform as well as more supervised approaches, since the parser’s self-training predictions are not constrained by the correct logical form. Recent research has produced several weakly supervised rel</context>
</contexts>
<marker>Clarke, Goldwasser, Chang, Roth, 2010</marker>
<rawString>James Clarke, Dan Goldwasser, Ming-Wei Chang, and Dan Roth. 2010. Driving semantic parsing from the world’s response. In Proceedings of the Fourteenth Conference on Computational Natural Language Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden markov models: theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="16202" citStr="Collins, 2002" startWordPosition="2660" endWordPosition="2661">tting yr = 1 if r(e1, e2) ∈ A, and 0 otherwise. This setting trains the semantic parser to extract every true relation instance between (e1, e2) from some sentence in S(e,,e,), while simultaneously avoiding incorrect instances. Finally, z = 1, to encourage agreement between the semantic and dependency parses. The training data for the model is therefore a collection, {(sj,yj,zj)}n j�1, where j indexes entity tuples (e1, e2). Training optimizes the semantic parser parameters θ to predict Y = yj, Z = zj given S = sj. The parameters θ are estimated by running the structured perceptron algorithm (Collins, 2002) on the training data defined above. The structured perceptron algorithm iteratively applies a simple update rule for each example (sj, yj, zj) in the training data: `predicted ← `actual ← θt+1 ← Each iteration of training requires solving two maximization problems. The first maximization, maxe,y,z p(`, y, z|s; θt), is straightforward because y and z are deterministic functions of `. Therefore, it is solved by finding the maximum probability assignment `, then choosing values for y and z that satisfy the weak supervision constraints. The second maximization, maxe p(`|y, z, s; θt), is more chal</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for hidden markov models: theory and experiments with perceptron algorithms. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Craven</author>
<author>Johan Kumlien</author>
</authors>
<title>Constructing biological knowledge bases by extracting information from text sources.</title>
<date>1999</date>
<booktitle>In Proceedings of the Seventh International Conference on Intelligent Systems for Molecular Biology.</booktitle>
<contexts>
<context position="36727" citStr="Craven and Kumlien, 1999" startWordPosition="5941" endWordPosition="5944">dback may be easier to obtain than full logical forms, but still requires individually annotated sentences. Other approaches are completely unsupervised, but do not tie the language to an existing meaning representation (Poon and Domingos, 2009). It is also possible to self-train a semantic parser without any labeled data (Goldwasser et al., 2011). However, this approach does not perform as well as more supervised approaches, since the parser’s self-training predictions are not constrained by the correct logical form. Recent research has produced several weakly supervised relation extractors (Craven and Kumlien, 1999; Mintz et al., 2009; Wu and Weld, 2010; Riedel et al., 2010; Hoffmann et al., 2011). These systems scale up to hundreds of predicates, but have much shallower semantic representations than semantic parsers. For example, these systems cannot be directly used to respond to natural language queries. This work extends weakly supervised relation extraction to produce richer semantic structure, using only slightly more supervision in the form of dependency parses. 7 Discussion This paper presents a method for training a semantic parser using only a knowledge base and a corpus of unlabeled sentences</context>
</contexts>
<marker>Craven, Kumlien, 1999</marker>
<rawString>Mark Craven and Johan Kumlien. 1999. Constructing biological knowledge bases by extracting information from text sources. In Proceedings of the Seventh International Conference on Intelligent Systems for Molecular Biology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruifang Ge</author>
<author>Raymond J Mooney</author>
</authors>
<title>A statistical semantic parser that integrates syntax and semantics.</title>
<date>2005</date>
<booktitle>In Proceedings of the Ninth Conference on Computational Natural Language Learning.</booktitle>
<contexts>
<context position="35264" citStr="Ge and Mooney, 2005" startWordPosition="5712" endWordPosition="5715">also demonstrates the benefit of including syntactic supervision. Examining the system output, we find two major sources of error. The first is missing lexical categories for uncommon words (e.g., “ex-guitarist”), which negatively impact recall by making some queries unparsable. The second is difficulty distinguishing between relations with similar type signatures, such as CITYLOCATEDINCOUNTRY and CITYCAPITALOFCOUNTRY. 6 Related Work There are many approaches to supervised semantic parsing, including inductive logic programming (Zelle and Mooney, 1996), probabilistic and synchronous grammars (Ge and Mooney, 2005; Wong and Mooney, 2006; Wong and Mooney, 2007; Lu et al., 2008), and automatically learned transformation rules (Kate et al., 2005). This work most closely follows the work on semantic parsing using CCG (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2010). These supervised systems are all trained with annotated sentence/logical form pairs; hence these approaches are labor intensive and do not scale to broad domains with large numbers of predicates. Several recent papers have attempted to reduce the amount of human supervision required to train 762 a semanti</context>
</contexts>
<marker>Ge, Mooney, 2005</marker>
<rawString>Ruifang Ge and Raymond J. Mooney. 2005. A statistical semantic parser that integrates syntax and semantics. In Proceedings of the Ninth Conference on Computational Natural Language Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Goldwasser</author>
<author>Roi Reichart</author>
<author>James Clarke</author>
<author>Dan Roth</author>
</authors>
<title>Confidence driven unsupervised semantic parsing.</title>
<date>2011</date>
<booktitle>Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="36452" citStr="Goldwasser et al., 2011" startWordPosition="5901" endWordPosition="5904">sion required to train 762 a semantic parser. One line of work eliminates the need for an annotated logical form, instead using only the correct answer for a database query (Liang et al., 2011) or even a binary correct/incorrect signal (Clarke et al., 2010). This type of feedback may be easier to obtain than full logical forms, but still requires individually annotated sentences. Other approaches are completely unsupervised, but do not tie the language to an existing meaning representation (Poon and Domingos, 2009). It is also possible to self-train a semantic parser without any labeled data (Goldwasser et al., 2011). However, this approach does not perform as well as more supervised approaches, since the parser’s self-training predictions are not constrained by the correct logical form. Recent research has produced several weakly supervised relation extractors (Craven and Kumlien, 1999; Mintz et al., 2009; Wu and Weld, 2010; Riedel et al., 2010; Hoffmann et al., 2011). These systems scale up to hundreds of predicates, but have much shallower semantic representations than semantic parsers. For example, these systems cannot be directly used to respond to natural language queries. This work extends weakly s</context>
</contexts>
<marker>Goldwasser, Reichart, Clarke, Roth, 2011</marker>
<rawString>Dan Goldwasser, Roi Reichart, James Clarke, and Dan Roth. 2011. Confidence driven unsupervised semantic parsing. Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raphael Hoffmann</author>
<author>Congle Zhang</author>
<author>Xiao Ling</author>
<author>Luke S Zettlemoyer</author>
<author>Daniel S Weld</author>
</authors>
<title>Knowledgebased weak supervision for information extraction of overlapping relations.</title>
<date>2011</date>
<booktitle>In The 49th Annual Meeting of</booktitle>
<contexts>
<context position="8948" citStr="Hoffmann et al., 2011" startWordPosition="1418" endWordPosition="1421">tching). Output: 1. Parameters θ for the CCG that produce correct semantic parses ` for sentences s E S. This problem is ill-posed without additional assumptions: since the correct logical form for a sentence is never observed, there is no a priori reason to prefer one semantic parse to another. Our training algorithm makes two assumptions about correct semantic parses, which are encoded as weak supervision constraints. These constraints make learning possible by adding an inductive bias: 1. Every relation instance r(e1, e2) E A is expressed by at least one sentence in S (Riedel et al., 2010; Hoffmann et al., 2011). 2. The correct semantic parse of a sentence s contains a subset of the syntactic dependencies contained in a dependency parse of s. Our weakly supervised training uses these constraints as a proxy for labeled semantic parses. The training algorithm has two steps. First, the algorithm constructs a graphical model that contains both the semantic parser and constant factors encoding the above two constraints. This graphical model is then used to estimate parameters θ for the semantic parser, essentially optimizing θ to produce parses that satisfy the weak supervision constraints. If our assumpt</context>
<context position="12910" citStr="Hoffmann et al. (2011)" startWordPosition="2079" endWordPosition="2082">rform exact inference using a CKY-style dynamic programming algorithm. However, other aspects of the graphical model preclude exact inference, so we perform approximate inference using beam search. Inference is explained in more detail in Section 3.2. Semantic Constraint The semantic constraint states that, given an entity tuple (e1, e2), every relation instance r(e1, e2) E A must be expressed somewhere in S(11,12). Furthermore, no semantic parse can express a relation instance which is not in the knowledge base. This constraint is identical to the multiple deterministic-OR constraint used by Hoffmann et al. (2011) to train a sentential relation extractor. The graphical model contains a semantic constraint factor IF and one binary variable Yr for each relation r in the knowledge base. Yr represents whether r(e1,e2) is expressed by any sentence in S(11,12). The IF factor determines whether each semantic parse in f extracts a relation between e1 and e2. It then aggregates these sentence-level extractions using a deterministic OR: if any sentence extracts r(e1, e2) then Yr = 1. Otherwise, Yr = 0. 1Section 4.3 describes the features used by our semantic parser for Freebase. IF(Yr,f) = 1 if Yr = 1 n li.EXTRA</context>
<context position="17046" citStr="Hoffmann et al., 2011" startWordPosition="2793" endWordPosition="2796"> requires solving two maximization problems. The first maximization, maxe,y,z p(`, y, z|s; θt), is straightforward because y and z are deterministic functions of `. Therefore, it is solved by finding the maximum probability assignment `, then choosing values for y and z that satisfy the weak supervision constraints. The second maximization, maxe p(`|y, z, s; θt), is more challenging. When y and z are given, the inference procedure must restrict its search to the parses ` which satisfy these weak supervision constraints. The original formulation of the IF factors permitted tractable inference (Hoffmann et al., 2011), but the EXTRACTS function and the 4b factors preclude efficient inference. We approximate this maximization using beam search over CCG parses `. For each sentence s, we perform a beam search to produce k = 300 possible semantic parses. We then check the value of 4b for each generated parse and eliminate parses which do not satisfy this syntactic constraint. Finally, we apply EXTRACTS to each parse, then use the greedy approximate inference procedure from Hoffmann et al. (2011) for the IF factors. 4 Building a Grammar for Freebase We apply the training algorithm from the previous section to p</context>
<context position="27593" citStr="Hoffmann et al., 2011" startWordPosition="4540" endWordPosition="4543">are positive examples, meaning there exists some relation r where r(e1, e2) E 04. To improve training efficiency and prediction performance, we subsample 5% of the negative examples for training, producing a training set of 125k sentences with 27k positive examples. The validation and test sets retain the original positive/negative ratio. Table 2 shows some statistics of the most frequent relations in the test set. 5.2 Relation Extraction The first experiment measures the semantic parser’s ability to extract relations from sentences in our web corpus. We compare our semantic parser to MULTIR (Hoffmann et al., 2011), which is a state-ofthe-art weakly supervised relation extractor. This method uses the same weak supervision constraint and parameter estimation procedure, but replaces the semantic parser by a linear classifier. The features for this classifier include the dependency path between the entity mentions, the type of each mention, and the intervening context (Mintz et al., 2009). Both the semantic parser and MULTIR were trained by running 5 iterations of the structured per4Note that the positive/negative ratio was much lower without the length filter or entity disambiguation, which is partly why </context>
<context position="36811" citStr="Hoffmann et al., 2011" startWordPosition="5957" endWordPosition="5960"> annotated sentences. Other approaches are completely unsupervised, but do not tie the language to an existing meaning representation (Poon and Domingos, 2009). It is also possible to self-train a semantic parser without any labeled data (Goldwasser et al., 2011). However, this approach does not perform as well as more supervised approaches, since the parser’s self-training predictions are not constrained by the correct logical form. Recent research has produced several weakly supervised relation extractors (Craven and Kumlien, 1999; Mintz et al., 2009; Wu and Weld, 2010; Riedel et al., 2010; Hoffmann et al., 2011). These systems scale up to hundreds of predicates, but have much shallower semantic representations than semantic parsers. For example, these systems cannot be directly used to respond to natural language queries. This work extends weakly supervised relation extraction to produce richer semantic structure, using only slightly more supervision in the form of dependency parses. 7 Discussion This paper presents a method for training a semantic parser using only a knowledge base and a corpus of unlabeled sentences. Our key observation is that multiple forms of weak supervision can be combined to </context>
<context position="38057" citStr="Hoffmann et al., 2011" startWordPosition="6159" endWordPosition="6162">ic parser: semantic supervision from a knowledge base of facts, and syntactic supervision in the form of a standard dependency parser. We presented an algorithm for training a semantic parser in the form of a probabilistic Combinatory Categorial Grammar, using these two types of weak supervision. We used this algorithm to train a semantic parser for an ontology of 77 Freebase predicates, using Freebase itself as the weak semantic supervision. Experimental results show that our trained semantic parser extracts binary relations as well as a state-of-the-art weakly supervised relation extractor (Hoffmann et al., 2011). Further experiments tested our trained parser’s ability to extract more complex meanings from sentences, including logical forms involving conjunctions of multiple relation and category predicates with shared arguments (e.g., Ax.MUSICIAN(x) ∧ PERSONBORNIN(x, LONDON) ∧ CITYINCOUNTRY(LONDON, ENGLAND)). To test this capability, we applied the trained parser to natural language queries against Freebase. The semantic parser correctly interpreted 56% of these queries, despite the broad domain and never having seen an annotated logical form. Together, these two experimental analyses suggest that th</context>
</contexts>
<marker>Hoffmann, Zhang, Ling, Zettlemoyer, Weld, 2011</marker>
<rawString>Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke S. Zettlemoyer, and Daniel S. Weld. 2011. Knowledgebased weak supervision for information extraction of overlapping relations. In The 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rohit J Kate</author>
<author>Yuk Wah Wong</author>
<author>Raymond J Mooney</author>
</authors>
<title>Learning to transform natural to formal languages. In</title>
<date>2005</date>
<booktitle>Proceedings, The Twentieth National Conference on Artificial Intelligence and the Seventeenth Innovative Applications of Artificial Intelligence Conference.</booktitle>
<contexts>
<context position="35396" citStr="Kate et al., 2005" startWordPosition="5733" endWordPosition="5736">e first is missing lexical categories for uncommon words (e.g., “ex-guitarist”), which negatively impact recall by making some queries unparsable. The second is difficulty distinguishing between relations with similar type signatures, such as CITYLOCATEDINCOUNTRY and CITYCAPITALOFCOUNTRY. 6 Related Work There are many approaches to supervised semantic parsing, including inductive logic programming (Zelle and Mooney, 1996), probabilistic and synchronous grammars (Ge and Mooney, 2005; Wong and Mooney, 2006; Wong and Mooney, 2007; Lu et al., 2008), and automatically learned transformation rules (Kate et al., 2005). This work most closely follows the work on semantic parsing using CCG (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2010). These supervised systems are all trained with annotated sentence/logical form pairs; hence these approaches are labor intensive and do not scale to broad domains with large numbers of predicates. Several recent papers have attempted to reduce the amount of human supervision required to train 762 a semantic parser. One line of work eliminates the need for an annotated logical form, instead using only the correct answer for a database q</context>
</contexts>
<marker>Kate, Wong, Mooney, 2005</marker>
<rawString>Rohit J. Kate, Yuk Wah Wong, and Raymond J. Mooney. 2005. Learning to transform natural to formal languages. In Proceedings, The Twentieth National Conference on Artificial Intelligence and the Seventeenth Innovative Applications of Artificial Intelligence Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom Kwiatkowski</author>
<author>Luke Zettlemoyer</author>
<author>Sharon Goldwater</author>
<author>Mark Steedman</author>
</authors>
<title>Inducing probabilistic CCG grammars from logical form with higher-order unification.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="35556" citStr="Kwiatkowski et al., 2010" startWordPosition="5757" endWordPosition="5760">cond is difficulty distinguishing between relations with similar type signatures, such as CITYLOCATEDINCOUNTRY and CITYCAPITALOFCOUNTRY. 6 Related Work There are many approaches to supervised semantic parsing, including inductive logic programming (Zelle and Mooney, 1996), probabilistic and synchronous grammars (Ge and Mooney, 2005; Wong and Mooney, 2006; Wong and Mooney, 2007; Lu et al., 2008), and automatically learned transformation rules (Kate et al., 2005). This work most closely follows the work on semantic parsing using CCG (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2010). These supervised systems are all trained with annotated sentence/logical form pairs; hence these approaches are labor intensive and do not scale to broad domains with large numbers of predicates. Several recent papers have attempted to reduce the amount of human supervision required to train 762 a semantic parser. One line of work eliminates the need for an annotated logical form, instead using only the correct answer for a database query (Liang et al., 2011) or even a binary correct/incorrect signal (Clarke et al., 2010). This type of feedback may be easier to obtain than full logical forms</context>
<context position="39251" citStr="Kwiatkowski et al., 2010" startWordPosition="6334" endWordPosition="6337">mental analyses suggest that the combination of syntactic and semantic weak supervision is indeed a sufficient basis for training semantic parsers for a diverse range of corpora and predicate ontologies. One limitation of our method is the reliance on hand-built dependency parse patterns for lexicon initialization. Although these patterns capture a variety of linguistic phenomena, they require manual engineering and may miss important relations. An area for future work is developing an automated way to produce this lexicon, perhaps by extending the recent work on automatic lexicon generation (Kwiatkowski et al., 2010) to the weakly supervised setting. Such an algorithm seems especially important if one wishes to model phenomena such as adjectives, which are difficult to initialize heuristically without generating large numbers of lexical entries. An elegant aspect of semantic parsing is that it is easily extensible to include more complex linguistic phenomena, such as quantification and events (multi-argument relations). In the future, we plan to increase the expressivity of our parser’s meaning representation to capture more linguistic and semantic phenomena. In this fashion, we can make progress toward b</context>
</contexts>
<marker>Kwiatkowski, Zettlemoyer, Goldwater, Steedman, 2010</marker>
<rawString>Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwater, and Mark Steedman. 2010. Inducing probabilistic CCG grammars from logical form with higher-order unification. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Michael I Jordan</author>
<author>Dan Klein</author>
</authors>
<title>Learning dependency-based compositional semantics.</title>
<date>2011</date>
<booktitle>In Proceedings of the Association for Computational Linguistics,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon.</location>
<contexts>
<context position="2194" citStr="Liang et al., 2011" startWordPosition="315" endWordPosition="318">he expressivity and utility of semantic parsing is derived from this meaning representation, which is essentially a program that is directly executable by a computer. In this sense, broad coverage semantic parsing is the goal of natural language understanding. Unfortunately, due to data annotation constraints, modern semantic parsers only operate in narrow domains. The best performing semantic parsers are trained using extensive manual annotation: typically, a number of sentences must be annotated with their desired logical form. Although other forms of supervision exist (Clarke et al., 2010; Liang et al., 2011), these methods similarly require annotations for individual sentences. More automated training methods are required to produce semantic parsers with richer meaning representations. This paper presents an algorithm for training a semantic parser without per-sentence annotations. Instead, our approach exploits two easily-obtainable sources of supervision: a large knowledge base and (automatically) dependency-parsed sentences. The semantic parser is trained to identify relation instances from the knowledge base while simultaneously producing parses that syntactically agree with the dependency pa</context>
<context position="36021" citStr="Liang et al., 2011" startWordPosition="5833" endWordPosition="5836"> work most closely follows the work on semantic parsing using CCG (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2010). These supervised systems are all trained with annotated sentence/logical form pairs; hence these approaches are labor intensive and do not scale to broad domains with large numbers of predicates. Several recent papers have attempted to reduce the amount of human supervision required to train 762 a semantic parser. One line of work eliminates the need for an annotated logical form, instead using only the correct answer for a database query (Liang et al., 2011) or even a binary correct/incorrect signal (Clarke et al., 2010). This type of feedback may be easier to obtain than full logical forms, but still requires individually annotated sentences. Other approaches are completely unsupervised, but do not tie the language to an existing meaning representation (Poon and Domingos, 2009). It is also possible to self-train a semantic parser without any labeled data (Goldwasser et al., 2011). However, this approach does not perform as well as more supervised approaches, since the parser’s self-training predictions are not constrained by the correct logical </context>
</contexts>
<marker>Liang, Jordan, Klein, 2011</marker>
<rawString>Percy Liang, Michael I. Jordan, and Dan Klein. 2011. Learning dependency-based compositional semantics. In Proceedings of the Association for Computational Linguistics, Portland, Oregon. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Lu</author>
<author>Hwee Tou Ng</author>
<author>Wee Sun Lee</author>
<author>Luke S Zettlemoyer</author>
</authors>
<title>A generative model for parsing natural language to meaning representations.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="35328" citStr="Lu et al., 2008" startWordPosition="5724" endWordPosition="5727">xamining the system output, we find two major sources of error. The first is missing lexical categories for uncommon words (e.g., “ex-guitarist”), which negatively impact recall by making some queries unparsable. The second is difficulty distinguishing between relations with similar type signatures, such as CITYLOCATEDINCOUNTRY and CITYCAPITALOFCOUNTRY. 6 Related Work There are many approaches to supervised semantic parsing, including inductive logic programming (Zelle and Mooney, 1996), probabilistic and synchronous grammars (Ge and Mooney, 2005; Wong and Mooney, 2006; Wong and Mooney, 2007; Lu et al., 2008), and automatically learned transformation rules (Kate et al., 2005). This work most closely follows the work on semantic parsing using CCG (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2010). These supervised systems are all trained with annotated sentence/logical form pairs; hence these approaches are labor intensive and do not scale to broad domains with large numbers of predicates. Several recent papers have attempted to reduce the amount of human supervision required to train 762 a semantic parser. One line of work eliminates the need for an annotated </context>
</contexts>
<marker>Lu, Ng, Lee, Zettlemoyer, 2008</marker>
<rawString>Wei Lu, Hwee Tou Ng, Wee Sun Lee, and Luke S. Zettlemoyer. 2008. A generative model for parsing natural language to meaning representations. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mike Mintz</author>
<author>Steven Bills</author>
<author>Rion Snow</author>
<author>Dan Jurafsky</author>
</authors>
<title>Distant supervision for relation extraction without labeled data.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP.</booktitle>
<contexts>
<context position="27971" citStr="Mintz et al., 2009" startWordPosition="4597" endWordPosition="4600">t frequent relations in the test set. 5.2 Relation Extraction The first experiment measures the semantic parser’s ability to extract relations from sentences in our web corpus. We compare our semantic parser to MULTIR (Hoffmann et al., 2011), which is a state-ofthe-art weakly supervised relation extractor. This method uses the same weak supervision constraint and parameter estimation procedure, but replaces the semantic parser by a linear classifier. The features for this classifier include the dependency path between the entity mentions, the type of each mention, and the intervening context (Mintz et al., 2009). Both the semantic parser and MULTIR were trained by running 5 iterations of the structured per4Note that the positive/negative ratio was much lower without the length filter or entity disambiguation, which is partly why filtering was performed. 760 Figure 3: Aggregate precision as a function of recall, for MULTIR (Hoffman et al., 2011) and our three semantic parser variants. 0 600 1200 1800 2400 3000 Figure 4: Sentential precision as a function of the expected number of correct extractions for MULTIR (Hoffman et al., 2011) and our three semantic parser variants. ceptron algorithm5. At test t</context>
<context position="36747" citStr="Mintz et al., 2009" startWordPosition="5945" endWordPosition="5948">ain than full logical forms, but still requires individually annotated sentences. Other approaches are completely unsupervised, but do not tie the language to an existing meaning representation (Poon and Domingos, 2009). It is also possible to self-train a semantic parser without any labeled data (Goldwasser et al., 2011). However, this approach does not perform as well as more supervised approaches, since the parser’s self-training predictions are not constrained by the correct logical form. Recent research has produced several weakly supervised relation extractors (Craven and Kumlien, 1999; Mintz et al., 2009; Wu and Weld, 2010; Riedel et al., 2010; Hoffmann et al., 2011). These systems scale up to hundreds of predicates, but have much shallower semantic representations than semantic parsers. For example, these systems cannot be directly used to respond to natural language queries. This work extends weakly supervised relation extraction to produce richer semantic structure, using only slightly more supervision in the form of dependency parses. 7 Discussion This paper presents a method for training a semantic parser using only a knowledge base and a corpus of unlabeled sentences. Our key observatio</context>
</contexts>
<marker>Mintz, Bills, Snow, Jurafsky, 2009</marker>
<rawString>Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky. 2009. Distant supervision for relation extraction without labeled data. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Jens Nilsson</author>
</authors>
<title>Maltparser: A data-driven parser-generator for dependency parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="25442" citStr="Nivre et al., 2006" startWordPosition="4214" endWordPosition="4217">relations with performance comparable to other state-of-the-art systems, and (2) a natural language database query task, to demonstrate the parser’s ability to extract more complex logical forms than binary relation instances, such as logical expressions involving conjunctions of multiple categories and relations with partially shared arguments. 5.1 Corpus Construction Our experiments use a subset of 77 relations2 from Freebase3 as the knowledge base and a corpus of web sentences. We constructed the sentence corpus by first sampling sentences from a web crawl and parsing them with MaltParser (Nivre et al., 2006). Long sentences tended to have noisy parses while also rarely expressing relations, so we discarded sentences longer than 10 words. Entities were identified by performing a simple string match between canonical entity names in Freebase and proper noun phrases identified by the parser. In cases where a single noun phrase matched multiple entities, we selected the entity participating in the most relations. The resulting corpus contains 2.5 million (e1, e2, s) triples, from which we reserved 10% for validation and 10% for testing. The validation set was used to estimate performance during algor</context>
</contexts>
<marker>Nivre, Hall, Nilsson, 2006</marker>
<rawString>Joakim Nivre, Johan Hall, and Jens Nilsson. 2006. Maltparser: A data-driven parser-generator for dependency parsing. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hoifung Poon</author>
<author>Pedro Domingos</author>
</authors>
<title>Unsupervised semantic parsing.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="36348" citStr="Poon and Domingos, 2009" startWordPosition="5884" endWordPosition="5887"> large numbers of predicates. Several recent papers have attempted to reduce the amount of human supervision required to train 762 a semantic parser. One line of work eliminates the need for an annotated logical form, instead using only the correct answer for a database query (Liang et al., 2011) or even a binary correct/incorrect signal (Clarke et al., 2010). This type of feedback may be easier to obtain than full logical forms, but still requires individually annotated sentences. Other approaches are completely unsupervised, but do not tie the language to an existing meaning representation (Poon and Domingos, 2009). It is also possible to self-train a semantic parser without any labeled data (Goldwasser et al., 2011). However, this approach does not perform as well as more supervised approaches, since the parser’s self-training predictions are not constrained by the correct logical form. Recent research has produced several weakly supervised relation extractors (Craven and Kumlien, 1999; Mintz et al., 2009; Wu and Weld, 2010; Riedel et al., 2010; Hoffmann et al., 2011). These systems scale up to hundreds of predicates, but have much shallower semantic representations than semantic parsers. For example, </context>
</contexts>
<marker>Poon, Domingos, 2009</marker>
<rawString>Hoifung Poon and Pedro Domingos. 2009. Unsupervised semantic parsing. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Riedel</author>
<author>Limin Yao</author>
<author>Andrew McCallum</author>
</authors>
<title>Modeling relations and their mentions without labeled text.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 European conference on Machine learning and Knowledge Discovery in Databases.</booktitle>
<contexts>
<context position="8924" citStr="Riedel et al., 2010" startWordPosition="1414" endWordPosition="1417">.g., simple string matching). Output: 1. Parameters θ for the CCG that produce correct semantic parses ` for sentences s E S. This problem is ill-posed without additional assumptions: since the correct logical form for a sentence is never observed, there is no a priori reason to prefer one semantic parse to another. Our training algorithm makes two assumptions about correct semantic parses, which are encoded as weak supervision constraints. These constraints make learning possible by adding an inductive bias: 1. Every relation instance r(e1, e2) E A is expressed by at least one sentence in S (Riedel et al., 2010; Hoffmann et al., 2011). 2. The correct semantic parse of a sentence s contains a subset of the syntactic dependencies contained in a dependency parse of s. Our weakly supervised training uses these constraints as a proxy for labeled semantic parses. The training algorithm has two steps. First, the algorithm constructs a graphical model that contains both the semantic parser and constant factors encoding the above two constraints. This graphical model is then used to estimate parameters θ for the semantic parser, essentially optimizing θ to produce parses that satisfy the weak supervision con</context>
<context position="36787" citStr="Riedel et al., 2010" startWordPosition="5953" endWordPosition="5956">requires individually annotated sentences. Other approaches are completely unsupervised, but do not tie the language to an existing meaning representation (Poon and Domingos, 2009). It is also possible to self-train a semantic parser without any labeled data (Goldwasser et al., 2011). However, this approach does not perform as well as more supervised approaches, since the parser’s self-training predictions are not constrained by the correct logical form. Recent research has produced several weakly supervised relation extractors (Craven and Kumlien, 1999; Mintz et al., 2009; Wu and Weld, 2010; Riedel et al., 2010; Hoffmann et al., 2011). These systems scale up to hundreds of predicates, but have much shallower semantic representations than semantic parsers. For example, these systems cannot be directly used to respond to natural language queries. This work extends weakly supervised relation extraction to produce richer semantic structure, using only slightly more supervision in the form of dependency parses. 7 Discussion This paper presents a method for training a semantic parser using only a knowledge base and a corpus of unlabeled sentences. Our key observation is that multiple forms of weak supervi</context>
</contexts>
<marker>Riedel, Yao, McCallum, 2010</marker>
<rawString>Sebastian Riedel, Limin Yao, and Andrew McCallum. 2010. Modeling relations and their mentions without labeled text. In Proceedings of the 2010 European conference on Machine learning and Knowledge Discovery in Databases.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steedman</author>
</authors>
<title>Surface Structure and Interpretation.</title>
<date>1996</date>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="3041" citStr="Steedman, 1996" startWordPosition="434" endWordPosition="435">mantic parser without per-sentence annotations. Instead, our approach exploits two easily-obtainable sources of supervision: a large knowledge base and (automatically) dependency-parsed sentences. The semantic parser is trained to identify relation instances from the knowledge base while simultaneously producing parses that syntactically agree with the dependency parses. Combining these two sources of supervision allows us to train an accurate semantic parser for any knowledge base without annotated training data. We demonstrate our approach by training a Combinatory Categorial Grammar (CCG) (Steedman, 1996) that parses sentences into logical forms containing any of 77 relations from Freebase. Our training data consists of relation instances from Freebase and automatically dependency-parsed sentences from a web corpus. The trained semantic parser extracts binary relations with state-of-the-art performance, while recovering considerably richer semantic structure. We demonstrate recovery of this semantic structure using natural language queries 754 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 754–765,</context>
<context position="5072" citStr="Steedman, 1996" startWordPosition="744" endWordPosition="745">rm. This paper is structured as follows. We first provide some background information on CCG and the structure of a knowledge base in Section 2. Section 3 formulates the weakly supervised training problem for semantic parsers and presents our algorithm. Section 4 describes how we applied our algorithm to construct a semantic parser for Freebase, and Section 5 presents our results. We conclude with related work and discussion. 2 Background 2.1 Combinatory Categorial Grammar Combinatory Categorial grammar (CCG) is a linguistic formalism that represents both the syntax and semantics of language (Steedman, 1996). CCG is a lexicalized formalism that encodes all grammatical information in a lexicon A. This lexicon contains syntactic and semantic categories for each word. A lexicon may include entries such as: town := N : Ax.CITY(x) California := N : Ax.x = CALIFORNIA in := (N\N)/N : Af.Ag.Ax. Ely.f(y) ∧ g(x) ∧ LOCATEDIN(x, y) Each entry of the lexicon w := s : l maps a word or short phrase w to a syntactic category s and a logical form l. Syntactic categories s may be atomic (N) or complex (N\N). Logical forms l are lambda calculus expressions constructed using predicates from a knowledge base. These l</context>
</contexts>
<marker>Steedman, 1996</marker>
<rawString>Mark Steedman. 1996. Surface Structure and Interpretation. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabian M Suchanek</author>
<author>Gjergji Kasneci</author>
<author>Gerhard Weikum</author>
</authors>
<title>Yago: a core of semantic knowledge.</title>
<date>2007</date>
<booktitle>In Proceedings of the 16th international conference on World Wide Web, WWW ’07,</booktitle>
<pages>697--706</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="7430" citStr="Suchanek et al., 2007" startWordPosition="1165" endWordPosition="1168">ies and relations are predicates which operate on entities and return truth values; categories c E C are one-place predicates (CITY(e)) and relations r E R are twoplace predicates (LOCATEDIN(e1, e2)). Entities e E E represent real-world entities and have a set of known text names. For example, CALIFORNIA is an entity whose text names include “California” and “CA.” Relation instances r(e1, e2) E A are facts asserted by the knowledge base, such as LOCATEDIN(SACRAMENTO, CALIFORNIA). Examples of such knowledge bases include Freebase (Bollacker et al., 2008), NELL (Carlson et al., 2010), and YAGO (Suchanek et al., 2007). The knowledge base influences the semantic parser in two ways. First, CCG logical forms are constructed by combining categories, relations and entities from the knowledge base with logical connectives; hence, the predicates in the knowledge base determine the expressivity of the parser’s semantic representation. Second, the known relation 755 instances r(e1, e2) E A are used as weak supervision to train the semantic parser. 3 Weakly Supervised Semantic Parsing We define weakly supervised semantic parsing as the following learning problem. Input: 1. A knowledge base K = (E, R, C, A), as defin</context>
</contexts>
<marker>Suchanek, Kasneci, Weikum, 2007</marker>
<rawString>Fabian M. Suchanek, Gjergji Kasneci, and Gerhard Weikum. 2007. Yago: a core of semantic knowledge. In Proceedings of the 16th international conference on World Wide Web, WWW ’07, pages 697–706, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuk Wah Wong</author>
<author>Raymond J Mooney</author>
</authors>
<title>Learning for semantic parsing with statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the NAACL.</booktitle>
<contexts>
<context position="35287" citStr="Wong and Mooney, 2006" startWordPosition="5716" endWordPosition="5719"> benefit of including syntactic supervision. Examining the system output, we find two major sources of error. The first is missing lexical categories for uncommon words (e.g., “ex-guitarist”), which negatively impact recall by making some queries unparsable. The second is difficulty distinguishing between relations with similar type signatures, such as CITYLOCATEDINCOUNTRY and CITYCAPITALOFCOUNTRY. 6 Related Work There are many approaches to supervised semantic parsing, including inductive logic programming (Zelle and Mooney, 1996), probabilistic and synchronous grammars (Ge and Mooney, 2005; Wong and Mooney, 2006; Wong and Mooney, 2007; Lu et al., 2008), and automatically learned transformation rules (Kate et al., 2005). This work most closely follows the work on semantic parsing using CCG (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2010). These supervised systems are all trained with annotated sentence/logical form pairs; hence these approaches are labor intensive and do not scale to broad domains with large numbers of predicates. Several recent papers have attempted to reduce the amount of human supervision required to train 762 a semantic parser. One line of w</context>
</contexts>
<marker>Wong, Mooney, 2006</marker>
<rawString>Yuk Wah Wong and Raymond J. Mooney. 2006. Learning for semantic parsing with statistical machine translation. In Proceedings of the Human Language Technology Conference of the NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuk Wah Wong</author>
<author>Raymond J Mooney</author>
</authors>
<title>Learning synchronous grammars for semantic parsing with lambda calculus.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="35310" citStr="Wong and Mooney, 2007" startWordPosition="5720" endWordPosition="5723">yntactic supervision. Examining the system output, we find two major sources of error. The first is missing lexical categories for uncommon words (e.g., “ex-guitarist”), which negatively impact recall by making some queries unparsable. The second is difficulty distinguishing between relations with similar type signatures, such as CITYLOCATEDINCOUNTRY and CITYCAPITALOFCOUNTRY. 6 Related Work There are many approaches to supervised semantic parsing, including inductive logic programming (Zelle and Mooney, 1996), probabilistic and synchronous grammars (Ge and Mooney, 2005; Wong and Mooney, 2006; Wong and Mooney, 2007; Lu et al., 2008), and automatically learned transformation rules (Kate et al., 2005). This work most closely follows the work on semantic parsing using CCG (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2010). These supervised systems are all trained with annotated sentence/logical form pairs; hence these approaches are labor intensive and do not scale to broad domains with large numbers of predicates. Several recent papers have attempted to reduce the amount of human supervision required to train 762 a semantic parser. One line of work eliminates the need</context>
</contexts>
<marker>Wong, Mooney, 2007</marker>
<rawString>Yuk Wah Wong and Raymond J. Mooney. 2007. Learning synchronous grammars for semantic parsing with lambda calculus. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Wu</author>
<author>Daniel S Weld</author>
</authors>
<title>Open information extraction using Wikipedia.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="36766" citStr="Wu and Weld, 2010" startWordPosition="5949" endWordPosition="5952">l forms, but still requires individually annotated sentences. Other approaches are completely unsupervised, but do not tie the language to an existing meaning representation (Poon and Domingos, 2009). It is also possible to self-train a semantic parser without any labeled data (Goldwasser et al., 2011). However, this approach does not perform as well as more supervised approaches, since the parser’s self-training predictions are not constrained by the correct logical form. Recent research has produced several weakly supervised relation extractors (Craven and Kumlien, 1999; Mintz et al., 2009; Wu and Weld, 2010; Riedel et al., 2010; Hoffmann et al., 2011). These systems scale up to hundreds of predicates, but have much shallower semantic representations than semantic parsers. For example, these systems cannot be directly used to respond to natural language queries. This work extends weakly supervised relation extraction to produce richer semantic structure, using only slightly more supervision in the form of dependency parses. 7 Discussion This paper presents a method for training a semantic parser using only a knowledge base and a corpus of unlabeled sentences. Our key observation is that multiple </context>
</contexts>
<marker>Wu, Weld, 2010</marker>
<rawString>Fei Wu and Daniel S. Weld. 2010. Open information extraction using Wikipedia. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John M Zelle</author>
<author>Raymond J Mooney</author>
</authors>
<title>Learning to parse database queries using inductive logic programming.</title>
<date>1996</date>
<booktitle>In Proceedings of the thirteenth national conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="35203" citStr="Zelle and Mooney, 1996" startWordPosition="5703" endWordPosition="5706">tactic supervision. mance difference between PARSE and PARSE-DEP also demonstrates the benefit of including syntactic supervision. Examining the system output, we find two major sources of error. The first is missing lexical categories for uncommon words (e.g., “ex-guitarist”), which negatively impact recall by making some queries unparsable. The second is difficulty distinguishing between relations with similar type signatures, such as CITYLOCATEDINCOUNTRY and CITYCAPITALOFCOUNTRY. 6 Related Work There are many approaches to supervised semantic parsing, including inductive logic programming (Zelle and Mooney, 1996), probabilistic and synchronous grammars (Ge and Mooney, 2005; Wong and Mooney, 2006; Wong and Mooney, 2007; Lu et al., 2008), and automatically learned transformation rules (Kate et al., 2005). This work most closely follows the work on semantic parsing using CCG (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2010). These supervised systems are all trained with annotated sentence/logical form pairs; hence these approaches are labor intensive and do not scale to broad domains with large numbers of predicates. Several recent papers have attempted to reduce th</context>
</contexts>
<marker>Zelle, Mooney, 1996</marker>
<rawString>John M. Zelle and Raymond J. Mooney. 1996. Learning to parse database queries using inductive logic programming. In Proceedings of the thirteenth national conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luke S Zettlemoyer</author>
<author>Michael Collins</author>
</authors>
<title>Learning to map sentences to logical form: structured classification with probabilistic categorial grammars.</title>
<date>2005</date>
<booktitle>In UAI ’05, Proceedings of the 21st Conference in Uncertainty in Artificial Intelligence.</booktitle>
<contexts>
<context position="35498" citStr="Zettlemoyer and Collins, 2005" startWordPosition="5749" endWordPosition="5752">tively impact recall by making some queries unparsable. The second is difficulty distinguishing between relations with similar type signatures, such as CITYLOCATEDINCOUNTRY and CITYCAPITALOFCOUNTRY. 6 Related Work There are many approaches to supervised semantic parsing, including inductive logic programming (Zelle and Mooney, 1996), probabilistic and synchronous grammars (Ge and Mooney, 2005; Wong and Mooney, 2006; Wong and Mooney, 2007; Lu et al., 2008), and automatically learned transformation rules (Kate et al., 2005). This work most closely follows the work on semantic parsing using CCG (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2010). These supervised systems are all trained with annotated sentence/logical form pairs; hence these approaches are labor intensive and do not scale to broad domains with large numbers of predicates. Several recent papers have attempted to reduce the amount of human supervision required to train 762 a semantic parser. One line of work eliminates the need for an annotated logical form, instead using only the correct answer for a database query (Liang et al., 2011) or even a binary correct/incorrect signal (Clarke et al., 2010). This type o</context>
</contexts>
<marker>Zettlemoyer, Collins, 2005</marker>
<rawString>Luke S. Zettlemoyer and Michael Collins. 2005. Learning to map sentences to logical form: structured classification with probabilistic categorial grammars. In UAI ’05, Proceedings of the 21st Conference in Uncertainty in Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luke S Zettlemoyer</author>
<author>Michael Collins</author>
</authors>
<title>Online learning of relaxed ccg grammars for parsing to logical form.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning.</booktitle>
<contexts>
<context position="35529" citStr="Zettlemoyer and Collins, 2007" startWordPosition="5753" endWordPosition="5756">some queries unparsable. The second is difficulty distinguishing between relations with similar type signatures, such as CITYLOCATEDINCOUNTRY and CITYCAPITALOFCOUNTRY. 6 Related Work There are many approaches to supervised semantic parsing, including inductive logic programming (Zelle and Mooney, 1996), probabilistic and synchronous grammars (Ge and Mooney, 2005; Wong and Mooney, 2006; Wong and Mooney, 2007; Lu et al., 2008), and automatically learned transformation rules (Kate et al., 2005). This work most closely follows the work on semantic parsing using CCG (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2010). These supervised systems are all trained with annotated sentence/logical form pairs; hence these approaches are labor intensive and do not scale to broad domains with large numbers of predicates. Several recent papers have attempted to reduce the amount of human supervision required to train 762 a semantic parser. One line of work eliminates the need for an annotated logical form, instead using only the correct answer for a database query (Liang et al., 2011) or even a binary correct/incorrect signal (Clarke et al., 2010). This type of feedback may be easier to obt</context>
</contexts>
<marker>Zettlemoyer, Collins, 2007</marker>
<rawString>Luke S. Zettlemoyer and Michael Collins. 2007. Online learning of relaxed ccg grammars for parsing to logical form. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
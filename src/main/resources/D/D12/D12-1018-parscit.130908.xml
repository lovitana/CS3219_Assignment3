<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.005913">
<title confidence="0.635737">
Learning Verb Inference Rules from Linguistically-Motivated Evidence
</title>
<author confidence="0.832846">
Hila Weisman§, Jonathan Berant†, Idan Szpektort, Ido Dagan§
</author>
<affiliation confidence="0.781782">
§ Computer Science Department, Bar-Ilan University
t The Blavatnik School of Computer Science, Tel Aviv University
t Yahoo! Research Israel
</affiliation>
<email confidence="0.883207333333333">
{weismah1,dagan}@cs.biu.ac.il
{jonatha6}@post.tau.ac.il
{idan}@yahoo-inc.com
</email>
<sectionHeader confidence="0.996252" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9993483">
Learning inference relations between verbs is
at the heart of many semantic applications.
However, most prior work on learning such
rules focused on a rather narrow set of in-
formation sources: mainly distributional sim-
ilarity, and to a lesser extent manually con-
structed verb co-occurrence patterns. In this
paper, we claim that it is imperative to uti-
lize information from various textual scopes:
verb co-occurrence within a sentence, verb co-
occurrence within a document, as well as over-
all corpus statistics. To this end, we propose
a much richer novel set of linguistically mo-
tivated cues for detecting entailment between
verbs and combine them as features in a su-
pervised classification framework. We empir-
ically demonstrate that our model significantly
outperforms previous methods and that infor-
mation from each textual scope contributes to
the verb entailment learning task.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999971586956522">
Inference rules are an important building block of
many semantic applications, such as Question An-
swering (Ravichandran and Hovy, 2002) and In-
formation Extraction (Shinyama and Sekine, 2006).
For example, given the sentence “Churros are
coated with sugar”, one can use the rule ‘coat —*
cover’ to answer the question “What are Churros
covered with?”. Inference rules specify a directional
inference relation between two text fragments, and
we follow the Textual Entailment modeling of infer-
ence (Dagan et al., 2006), which refers to such rules
as entailment rules. In this work we focus on one
of the most important rule types, namely, lexical en-
tailment rules between verbs (verb entailment), e.g.,
‘whisper —* talk’, ‘win —* play’ and ‘buy —* own’.
The significance of such rules has led to active re-
search in automatic learning of entailment rules be-
tween verbs or verb-like structures (Zanzotto et al.,
2006; Abe et al., 2008; Schoenmackers et al., 2010).
Most prior efforts to learn verb entailment rules
from large corpora employed distributional similar-
ity methods, assuming that verbs are semantically
similar if they occur in similar contexts (Lin, 1998;
Berant et al., 2012). This led to the automatic ac-
quisition of large scale knowledge bases, but with
limited precision. Fewer works, such as VerbOcean
(Chklovski and Pantel, 2004), focused on identi-
fying verb entailment through verb instantiation of
manually constructed patterns. For example, the
sentence “he scared and even startled me” implies
that ‘startle —* scare’. This led to more precise rule
extraction, but with poor coverage since contrary
to nouns, in which patterns are common (Hearst,
1992), verbs do not co-occur often within rigid pat-
terns. However, verbs do tend to co-occur in the
same document, and also in different clauses of the
same sentence.
In this paper, we claim that on top of standard
pattern-based and distributional similarity methods,
corpus-based learning of verb entailment can greatly
benefit from exploiting additional linguistically-
motivated cues that are specific to verbs. For in-
stance, when verbs co-occur in different clauses of
the same sentence, the syntactic relation between the
clauses can be viewed as a proxy for the semantic re-
lation between the verbs. Moreover, we claim that to
</bodyText>
<page confidence="0.982461">
194
</page>
<note confidence="0.797618">
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 194–204, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.9999168">
improve performance it is crucial to combine infor-
mation sources from different textual scopes: verb
co-occurrence within a sentence and within a docu-
ment, distributional similarity over the entire corpus,
etc.
Our contribution in this paper is two-fold. First,
we suggest a novel set of entailment indicators that
help to detect the likelihood of verb entailment.
Our novel indicators are specific to verbs and are
linguistically-motivated. Second, we encode our
novel indicators as features within a supervised clas-
sification framework and integrate them with other
standard features adapted from prior work. This re-
sults in a supervised corpus-based learning method
that combines verb entailment information at the
sentence, document and corpus levels.
We test our model on a manually labeled data
set, and show that it outperforms the best perform-
ing previous work by 24%. In addition, we ex-
amine the effectiveness of indicators that operate at
the sentence-level, document-level and corpus-level.
This analysis reveals that using a rich and diverse
set of indicators that capture sentence-level interac-
tions between verbs substantially improves verb en-
tailment detection.
</bodyText>
<sectionHeader confidence="0.995482" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.999951603174603">
The main approach for learning entailment rules be-
tween verbs and verb-like structures has employed
the distributional hypothesis, which assumes that
words with similar meanings appear in similar con-
texts. For example, we expect the words ‘buy’ and
‘purchase’ to occur with similar subjects and objects
in a large corpus. This observation has led to ample
work on developing both symmetric and directional
similarity measures that attempt to capture semantic
relations between lexical items by comparing their
neighborhood context (Lin, 1998; Weeds and Weir,
2003; Geffet and Dagan, 2005; Szpektor and Dagan,
2008; Kotlerman et al., 2010).
A far less explored direction for learning verb en-
tailment involves exploiting verb co-occurrence in
a sentence or a document. One prominent work
is Chklovsky and Pantel’s VerbOcean (2004). In
VerbOcean, the authors manually constructed 33
patterns and divided them into five pattern groups,
where each group signals one of the following five
semantic relations: similarity, strength, antonymy,
enablement and happens-before. For example, the
pattern ‘Xed and later Yed’ signals the happens-
before relation between the verbs ‘X’ and ‘Y’. Start-
ing with candidate verb pairs based on a distribu-
tional similarity measure, the patterns are used to
choose a semantic relation per verb pair based on
the different patterns this pair instantiates. This
method is more precise than distributional similarity
approaches, but it is highly susceptible to sparseness
issues, since verbs do not typically co-occur within
rigid patterns. Utilizing verb co-occurrence at the
document level, Chambers and Jurafsky (2008) es-
timate whether a pair of verbs is narratively related
by counting the number of times the verbs share an
argument in the same document. In a similar man-
ner, Pekar (2008) detects entailment rules between
templates from shared arguments within discourse-
related clauses in the same document.
Recently, supervised classification has become
standard in performing various semantic tasks.
N irkin et al. (2006) introduced a system for learn-
ing entailment rules between nouns (e.g., ‘novel —*
book’) that combines distributional similarity and
Hearst patterns as features in a supervised clas-
sifier. Pennacchiotti and Pantel (2009) augment
N irkin et al’s features with web-based features for
the task of entity extraction. Hagiwara et al. (2009)
perform synonym identification based on both dis-
tributional and contextual features. Tremper (2010)
extract “loose” sentence-level features in order to
identify the presupposition relation (e.g., , the verb
‘win’ presupposes the verb ‘play’). Last, Be-
rant et al. (2012) utilized various distributional
similarity features to identify entailment between
lexical-syntactic predicates.
In this paper, we follow the supervised approach
for semantic relation detection in order to identify
verb entailment. While we utilize and adapt useful
features from prior work, we introduce a diverse set
of novel features for the task, effectively combining
verb co-occurrence information at the sentence, doc-
ument, and corpus levels.
</bodyText>
<sectionHeader confidence="0.999437" genericHeader="method">
3 Linguistically-Motivated Indicators
</sectionHeader>
<bodyText confidence="0.9902095">
As mentioned in Section 1, verbs behave quite dif-
ferently from nouns in corpora. In this section, we
</bodyText>
<page confidence="0.998412">
195
</page>
<bodyText confidence="0.999783060240964">
introduce linguistically motivated indicators that are
specific to verbs and may signal the semantic re-
lation between verb pairs. Then, in Section 4 we
describe how these indicators are exactly encoded
as features within a supervised classification frame-
work.
Verb co-occurrence When (non-auxiliary) verbs
co-occur in a sentence, they are often the main verbs
of different clauses. We thus aim to use information
about the relation between clauses to learn about
the relation between the clauses’ main verbs. Dis-
course markers (Hobbs, 1979; Schiffrin, 1988) are
lexical terms such as ‘because’ and ‘however’ that
indicate a semantic relation between discourse frag-
ments (i.e., propositions or speech acts). We suggest
that these markers can indicate semantic relations
between the main verbs of the connected clauses.
For example, in the sentence “He always snores
while he sleeps”, the marker ‘while’ indicates a tem-
poral relation between the clauses, indicating that
‘snoring’ occurs while ‘sleeping’ (and so ‘snore �
sleep’).
Often the relation between clauses is not ex-
pressed explicitly with an overt discourse marker,
but is still implied by the syntactic structure of
the sentence. For example, in dependency parsing
the relation can be captured by labeled dependency
edges expressing that one clause is an adverbial ad-
junct of the other, or that two clauses are coordi-
nated. This can indicate the existence (or lack) of
entailment between verbs. For instance, in the sen-
tence “When I walked into the room, he was working
out”, the verb ‘walk’ is an adverbial adjunct of the
verb ‘work out’. Such co-occurrence structure does
not indicate a deep semantic relation, such as entail-
ment, between the two verbs.
Verb classes Verb classes are sets of semantically-
related verbs sharing some linguistic properties
(Levin, 1993). One of the most general verb classes
are stative vs. event verbs (Jackendoff, 1983). Sta-
tive verb, such as ‘love’ and ‘think’, usually describe
a state that lasts some time. On the other hand, event
verbs, such as ‘run’ and ‘kiss’, describe an action.
We hypothesize that verb classes are relevant for de-
termining entailment, for example, that stative verbs
are not likely to entail event verbs.
Verb generality Verb-particle constructions are
multi-word expressions consisting of a head verb
and a particle, e.g., switch off (Baldwin and Villav-
icencio, 2002). We conjecture that the more gen-
eral a verb is, the more likely it is to appear with
many different particles. Detecting verb generality
can help us tackle an infamous property of distribu-
tional similarity methods, namely, the difficulty in
detecting the direction of entailment (Berant et al.,
2012). For example, the verb ’cover’ appears with
many different particles such as ’up’ and ’for’, while
the verb ’coat’ does not. Thus, assuming we have
evidence for an entailment relation between the two
verbs, this indicator can help us discern the direction
of entailment and determine that ‘coat —* cover’.
Typed Distributional Similarity As discussed in
section 2, distributional similarity is the most com-
mon source of information for learning semantic re-
lations between verbs. Yet, we suggest that on top
of standard distributional similarity measures, which
take several verbal arguments into account (such as
subject, object, etc.) simultaneously, we should also
focus on each type of argument independently. In
particular, we apply this approach to compute simi-
larity between verbs based on the set of adverbs that
modify them. Our hypothesis is that adverbs may
contain relevant information for capturing the direc-
tion of entailment. If a verb appears with a small set
of adverbs, it is more likely to be a specific verb that
already conveys a specific action or state, making an
additional adverb redundant. For example, the verb
‘whisper’ conveys a specific manner of talking and
will probably not appear with the adverb ‘loudly’,
while the verb ‘talk’ is more likely to appear with
such an adverb. Thus, measuring similarity based
solely on adverb modifiers could reveal this phe-
nomenon.
</bodyText>
<sectionHeader confidence="0.993969" genericHeader="method">
4 Supervised Entailment Detection
</sectionHeader>
<bodyText confidence="0.999975625">
In the previous section, we discussed linguistic ob-
servations regarding novel indicators that may help
in detecting entailment relations between verbs. We
next describe how to incorporate these indicators as
features within a supervised framework for learning
lexical entailment rules between verbs. We follow
prior work on supervised lexical semantics (Mirkin
et al., 2006; Hagiwara et al., 2009; Tremper, 2010)
</bodyText>
<page confidence="0.99492">
196
</page>
<bodyText confidence="0.999952777777778">
and address the rule learning task as a classification
task. Specifically, given an ordered verb pair (v1, v2)
as input, we learn a classifier that detects whether the
entailment relation ‘v1 —* v2’ holds for this pair.
We next detail how our novel indicators, as well
as other diverse sources of information found useful
in prior work, are encoded as features. Then, we
describe the learning model and our feature analysis
procedure.
</bodyText>
<subsectionHeader confidence="0.996927">
4.1 Entailment features
</subsectionHeader>
<bodyText confidence="0.9998747">
Most of our features are based on information ex-
tracted from the target verb pair co-occurring within
varying textual scopes (sentence, document, cor-
pus). Hence, we group the features according to
their related scope. Naturally, when the scope is
small, i.e., at a sentence level, the semantic rela-
tion between the verbs is easier to discern but the
information may be sparse. Conversely, when co-
occurrence is loose the relation is harder to discern
but coverage is increased.
</bodyText>
<subsectionHeader confidence="0.790249">
4.1.1 Sentence-level co-occurrence
</subsectionHeader>
<bodyText confidence="0.999652941176471">
We next detail features that address co-occurrence
of the target verb pair within a sentence. These in-
clude our novel linguistically-motivated indicators,
as well as features that were adapted from prior
work.
Discourse markers As discussed in Section 3,
discourse markers may signal relations between the
main verbs of adjacent clauses. The literature is
abundant with taxonomies that classify markers to
various discourse relations (Mann and Thompson,
1988; Hovy and Maier, 1993; Knott and Sanders,
1998). Inspired by Marcu and Echihabi (2002), we
employ markers that are mapped to four discourse
relations ’Contrast’, ’Cause’, ’Condition’ and ’Tem-
poral’, as specified in Table 1. This definition
can be viewed as a relaxed version of VerbOcean’s
(Chklovski and Pantel, 2004) patterns, although the
underlying intuition is different (see Section 3).
For a target verb pair (v1, v2) and each discourse
relation r, we count the number of times that v1 is
the main verb in the main clause, v2 is the main verb
in the subordinate clause, and the clauses are con-
nected via a marker mapped to r. For example, given
the sentence “You must enroll in the competition be-
fore you can participate in it”, the verb pair (‘en-
roll’,‘participate’) appears in the ’Temporal’ rela-
tion, indicated by the marker ‘before’, where ‘enroll’
is in the main clause. Each count is then normalized
by the total number of times (v1, v2) appear with any
marker. The same procedure is done when v1 is in
the subordinate clause and v2 in the main clause. We
term the features by the relevant discourse relation,
e.g., ‘v1-contrast-v2’ refers to v1 being in the main
clause and connected to the subordinate clause via a
contrast marker.
Dependency relations between clauses As noted
in Section 3, the syntactic structure of verb co-
occurrence can indicate the existence or lack of en-
tailment. In dependency parsing this may be ex-
pressed via the label of the dependency relation con-
necting the main and subordinate clauses. In our ex-
periments we used the ukWaC corpus1 (Baroni et al.,
2009) which was parsed by the MALT parser (Nivre
et al., 2006). Hence, we identified three MALT de-
pendency relations that connect a main clause with
its subordinate clause. The first relation is the object
complement relation ‘obj’. In this case the subor-
dinate clause is an object complement of the main
clause. For example, in “it surprised me that the
lizard could talk” the verb pair (‘surprise’,‘talk’) is
connected by the ‘obj’ relation. The second rela-
tion is the adverbial adjunct relation ‘adv’, in which
the subordinate clause is adverbial and describes the
time, place, manner, etc. of the main clause, e.g., “he
gave his consent without thinking about the reper-
cussions”. The last relation is the coordination rela-
tion ‘coord’, e.g., “every night my dog Lucky sleeps
on the bed and my cat Flippers naps in the bathtub”.
Similar to discourse markers, we compute for
each verb pair (v1,v2) and each dependency label d
the proportion of times that v1 is the main verb of the
main clause, v2 is the main verb of the subordinate
clause, and the clauses are connected by dependency
relation d, out of all the times they are connected by
any dependency relation. We term the features by
the dependency label, e.g., ‘v1-adv-v2’ refers to v1
being in the main clause and connected to the subor-
dinate clause via an adverbial adjunct.
</bodyText>
<footnote confidence="0.990633">
1http://wacky.sslmit.unibo.it/doku.php?
id=corpora
</footnote>
<page confidence="0.959904">
197
</page>
<table confidence="0.999925">
Discourse Rel. Discourse Markers
Contrast although , despite , but , whereas , notwithstanding , though
Cause because , therefore , thus
Condition if , unless
Temporal whenever , after , before , until , when , finally , during , afterwards , meanwhile
</table>
<tableCaption confidence="0.998875">
Table 1: Discourse relations and their mapped markers.
</tableCaption>
<bodyText confidence="0.999681971428572">
Pattern-based We follow Chklovski and Pan-
tel (2004) and extract occurrences of VerbOcean pat-
terns that are instantiated by the target verb pair. As
mentioned in Section 2, VerbOcean patterns were
originally grouped into five semantic classes. Based
on a preliminary study we conducted, we decided
to utilize only four strength-class patterns as posi-
tive indicators for entailment, e.g., “he scared and
even startled me”, and three antonym-class patterns
as negative indicators for entailment, e.g., “you can
either open or close the door”. We note that these
patterns are also commonly used by RTE systems2.
Since the corpus pattern counts were very sparse,
we defined for a target verb pair (v1, v2) two bi-
nary features: the first denotes whether the verb
pair instantiates at least one positive pattern, and
the second denotes whether the verb pair instanti-
ates at least one negative pattern. For example, given
the aforementioned sentences, the value of the pos-
itive feature for the verb pair (‘startle’,‘scare’) is
‘1’. Patterns are directional, and so the value of
(‘scare’,‘startle’) is ‘0’.
Polarity We compute the proportion of times that
the two verbs appear in different polarity. For exam-
ple, in “he didn’t say why he left”, the verb ’say’ ap-
pears in negative polarity and the verb ’leave’ in pos-
itive polarity. Such change in polarity is usually an
indicator of non-entailment between the two verbs.
Tense ordering The temporal relation between
verbs may provide information about their seman-
tic relation. For each verb pair co-occurrence, we
extract the verbs’ tenses and order them as follows:
past &lt; present &lt; future. We then add the fea-
tures ‘tense-v1&lt;tense-v2’, ‘tense-v1=tense-v2’, and
‘tense-v1&gt;tense-v2’, corresponding to the propor-
</bodyText>
<footnote confidence="0.72578">
2http://aclweb.org/aclwiki/index.php?
title=RTE_Knowledge_Resources#Ablation_
Tests
</footnote>
<bodyText confidence="0.999848620689655">
tion of times the tense of v1 is smaller, equal to,
or bigger than the tense of v2. This indicates the
prevalent temporal relation between the verbs in the
corpus and may assist in detecting the direction of
entailment. e.g., if tense-v1&gt;tense-v2, the verb pair
is less likely to entail.
Co-reference Following Tremper (2010), in every
co-occurrence of (v1,v2) we extract for each verb
the set of arguments at either the subject or object
positions, denoted A1 and A2 (for v1 and v2, re-
spectively). We then compute the proportion of co-
occurrences in which v1 and v2 share an argument,
i.e., A1 n A2 =� 0, out of all the co-occurrences in
which both A1 and A2 are non-empty. The intuition,
which is similar to distributional similarity, is that
semantically related verbs tend to share arguments.
Syntactic and lexical distance Following Trem-
per (2010) again, we compute the average distance
d in dependency edges between the co-occurring
verbs. We compute three features corresponding to
three bins indicating if d &lt; 3, 3 &lt; d &lt; 7, or
d &gt; 7. Similar features are computed for the dis-
tance in words (bins are 0 &lt; d &lt; 5, 5 &lt; d &lt; 10,
d &gt; 10). This feature provides insight into the syn-
tactic relatedness of the verbs.
Sentence-level pmi Pointwise mutual information
(pmi) between v1 and v2 is computed, where the co-
occurrence scope is a sentence. Higher pmi should
hint at semantically related verbs.
</bodyText>
<subsectionHeader confidence="0.534905">
4.1.2 Document-level co-occurrence
</subsectionHeader>
<bodyText confidence="0.999888285714286">
This group of features addresses co-occurrence of
a target verb pair within the same document. These
features are less sparse, but tend to capture coarser
semantic relations between the target verbs.
Narrative score Chambers and Jurafsky (2008)
suggested a method for learning sequences of ac-
tions or events (expressed by verbs) in which a sin-
</bodyText>
<page confidence="0.994292">
198
</page>
<bodyText confidence="0.9998319">
gle entity is involved. They proposed a pmi-like nar-
rative score (see Eq. (1) in their paper) that esti-
mates whether a pair consisting of a verb and one
of its dependency relations (v1, r1) is narratively-
related to another such pair (v2, r2). Their estima-
tion is based on quantifying the likelihood that two
verbs will share an argument that instantiates both
the dependency position (v1, r1) and (v2, r2) within
documents in which the two verbs co-occur. For ex-
ample, given the document “Lindsay was prosecuted
for DUI. Lindsay was convicted of DUI.” the pairs
(‘prosecute’,‘subj’) and (‘convict’,‘subj’) share the
argument ‘Lindsay’ and are part of a narrative chain.
Such narrative relations may provide cues to the se-
mantic relatedness of the verb pair.
We compute for every target verb pair nine fea-
tures using their narrative score. In four features,
r1 = r2 and the common dependency is either a sub-
ject, an object, a preposition complement (e.g., “we
meet at the station.), or an adverb (termed chamb-
subj, chamb-obj, and so on). In the next three fea-
tures, r1 =� r2 and r1, r2 denote either a subject,
object, or preposition complement3 (termed chamb-
subj-obj and so on). Last, we add as features the
average of the four features where r1 = r2 (termed
chamb-same), and the average of the three features
where r1 =� r2 (termed chamb-diff).
Document-level pmi Similar to sentence-level
pmi, we compute the pmi between v1 and v2, but
this time the co-occurrence scope is a document.
</bodyText>
<subsectionHeader confidence="0.540922">
4.1.3 Corpus-level statistics
</subsectionHeader>
<bodyText confidence="0.999891307692308">
The final group of features ignores sentence or
document boundaries and is based on overall corpus
statistics.
Distributional similarity Following our hypoth-
esis regarding typed distributional similarity (Sec-
tion 3), we first compute for each verb and each
argument (subject, object, preposition complement
and adverb) a separate vector that counts the num-
ber of times each word in the corpus instantiates
the argument of that verb. In addition, we also
compute a vector that is the concatenation of the
previous separate vectors, which captures the stan-
dard distributional similarity statistics. We then
</bodyText>
<footnote confidence="0.7424375">
3adverbs never instantiate the subject, object or preposition
complement positions.
</footnote>
<bodyText confidence="0.9976866">
apply three state-of-the-art distributional similarity
measures, Lin (Lin, 1998), Weeds precision (Weeds
and Weir, 2003) and BInc (Szpektor and Dagan,
2008), to compute for every verb pair a similarity
score between each of the five count vectors4. We
term each feature by the method and argument, e.g.,
weeds-prep and lin-all represent the Weeds measure
over prepositional complements and the Lin mea-
sure over all arguments.
Verb classes Following our discussion in Sec-
tion 3, we first measure for each target verb v a “sta-
tive” feature f by computing the proportion of times
it appears in progressive tense, since stative verbs
usually do not appear in the progressive tense (e.g.,
‘knowing’). Then, given a verb pair (v1,v2) and their
corresponding stative features f1 and f2, we add two
features f1 · f2 and ��
�� , which capture the interaction
between the verb classes of the two verbs.
Verb generality For each verb, we add as a feature
the number of different particles it appears with in
the corpus, following the hypothesis that this is a
cue to its generality. Then, given a verb pair (v1,v2)
and their corresponding features f1 and f2, we add
the feature ��
</bodyText>
<equation confidence="0.642683">
�� . We expect that when ��
�� is high, v1 is
</equation>
<bodyText confidence="0.997487">
more general than v2, which is a negative entailment
indicator.
</bodyText>
<subsectionHeader confidence="0.958107">
4.2 Learning model and feature analysis
</subsectionHeader>
<bodyText confidence="0.999906416666667">
The total number of features in our model as de-
scribed above is 63. We combine the features in
a supervised classification framework with a linear
SVM. Since our model contains many novel fea-
tures, it is important to investigate their utility for
detecting verb entailment. To that end, we employ
feature ranking methods as suggested by Guyon et
al. (2003). In feature ranking methods, features are
ranked by some score computed for each feature in-
dependently. In this paper we use Pearson correla-
tion between the feature values and the correspond-
ing labels as the ranking criterion.
</bodyText>
<footnote confidence="0.992929">
4We employ the common practice of using the pmi between
a verb and an argument rather than the argument count as the
argument’s weight.
</footnote>
<page confidence="0.998866">
199
</page>
<sectionHeader confidence="0.988729" genericHeader="evaluation">
5 Evaluation and Analysis
</sectionHeader>
<subsectionHeader confidence="0.963617">
5.1 Experimental Setting
</subsectionHeader>
<bodyText confidence="0.999960297297297">
To evaluate our proposed supervised model, we con-
structed a dataset containing labeled verb pairs. We
started by randomly sampling 50 verbs out of the
common verbs in the RCV1 corpus5, which we de-
note here as seed verbs. Next, we extracted the 20
most similar verbs to each seed verb according to
the Lin similarity measure (Lin, 1998), which was
computed on the RCV1 corpus. Then, for each seed
verb vs and one of its extracted similar verbs vs we
generated the two directed pairs (vs, vs) and (vs, vs),
which represent the candidate rules ‘vs —* vs’ and
‘vs vs’ respectively. To reduce noise, we filtered
out verb pairs where one of the verbs is an auxiliary
or a light verb such as ’do’, ’get’ and ’have’. This
step resulted in 812 verb pairs as our dataset6, which
were manually annotated by the authors as repre-
senting a valid entailment rule or not. To annotate
these pairs, we generally followed the rule-based ap-
proach for entailment rule annotation, where a rule
‘v1 —* v2’ is considered as correct if the annotator
could think of reasonable contexts under which the
rule holds (Dekang and Pantel, 2001; Szpektor et
al., 2004). In total 225 verb pairs were labeled as
entailing (the rule ‘v1 —* v2’ was judged as correct)
and 587 verb pairs were labeled as non-entailing (the
rule ‘v1 —* v2’ was judged as incorrect). The Inter-
Annotator Agreement (IAA) for a random sample of
100 pairs was moderate (0.47), as expected from the
rule-based approach (Szpektor et al., 2007).
For each verb pair, all 63 features within our
model (Section 4) were computed using the ukWaC
corpus (Baroni et al., 2009), which contains 2 billion
words. For classification, we utilized SVM-perf’s
(Joachims, 2005) linear SVM implementation with
default parameters, and evaluated our model by per-
forming 10-fold cross validation (CV) over the la-
beled dataset.
</bodyText>
<footnote confidence="0.9887168">
5http://trec.nist.gov/data/reuters/
reuters.html
6The data set is available at http://www.cs.biu.ac.
il/˜nlp/downloads/verb-pair-annotation.
html
</footnote>
<subsectionHeader confidence="0.990823">
5.2 Feature selection and analysis
</subsectionHeader>
<bodyText confidence="0.99997597826087">
As discussed in Section 4.2, we followed the feature
ranking method proposed by Guyon et al. (2003) to
investigate the utility of our proposed features. Ta-
ble 2 depicts the 10 most positively and negatively
correlated features with entailment according to the
Pearson correlation measure
From Table 2, it is clear that distributional simi-
larity features are amongst the most positively cor-
related with entailment, which is in line with prior
work (Geffet and Dagan, 2005; Kotlerman et al.,
2010). Looking more closely, our suggestion for
typed distributional similarity proved to be useful,
and indeed most of the highly correlated distribu-
tional similarity features are typed measures. Stand-
ing out are the adverb-typed measures, with two fea-
tures in the top 10, including the highest, ‘Weeds-
adverb’, and ‘BInc-adverb’. We also note that the
highly correlated distributional similarity measures
are directional, Weeds and BInc.
The table also indicates that document-level co-
occurrence contributes positively to entailment de-
tection. This includes both the Chambers narrative
measure, with the typed feature Chambers-obj, and
document-level PMI, which captures a more loose
co-occurrence relationship between verbs. Again,
we point at the significant correlation of our novel
typed measures with verb entailment, in this case the
typed narrative measure.
Last, our feature analysis shows that many of our
novel co-occurrence features at the sentence level
contribute useful negative information. For exam-
ple, verbs connected via an adverbial adjunct (‘v2-
adverb-v1’) or an object complement (‘v1-obj-v2’)
are negatively correlated with entailment. In addi-
tion, the novel ‘verb generality’ feature as well as
the tense difference feature (‘tense-v1 &gt; tense-v2’)
are also strong negative indicators. On the other
hand, ‘v2-coord-v1’ is positively correlated with en-
tailment. This shows that encoding various aspects
of verb co-occurrence at the sentence level can lead
to better prediction of verb entailment. Finally, we
note that PMI at the sentence level is highly corre-
lated with entailment even more than at the docu-
ment level, since the local textual scope is more in-
dicative, though sparser.
To conclude, our feature analysis shows that fea-
</bodyText>
<page confidence="0.983436">
200
</page>
<table confidence="0.9988435">
Rank Top Positive Top Negative
1 Weeds-adverb tense-v1 &gt; tense-v2
2 Sentence-level PMI v2-adverb-v1 co-occurrence
3 Weeds-subj v2-obj-v1 co-occurrence
4 Weeds-prep v1-obj-v2 co-occurrence
5 Weeds-all v1-adverb-v2 co-occurrence
6 Chambers-obj verb generality ��
��
7 v2-coord-v1 co-occurrence v1-contrast-v2
8 BInc-adverb tense-v1 &lt; tense-v2
9 Document-level PMI lexical-distance 0-5
10 Chambers-same Lin-subj
</table>
<tableCaption confidence="0.999781">
Table 2: Top 10 positive and negative features according to the Pearson correlation score.
</tableCaption>
<bodyText confidence="0.99441325">
tures at all levels: sentence, document and corpus,
contain useful information for entailment detection,
both positive and negative, and should be combined
together. Moreover, many of our novel features are
among the highly correlated features, showing that
devising a rich set of verb-specific and linguistically-
motivated features provides better discriminative ev-
idence for entailment detection.
</bodyText>
<subsectionHeader confidence="0.971543">
5.3 Results and Analysis
</subsectionHeader>
<bodyText confidence="0.999698571428571">
We compared our method to the following baselines
which were mostly taken from or inspired by prior
work:
Random: A simple decision rule: for any
pair (v1, v2), randomly classify as “yes” with a
probability equal to the number of entailing verb
pairs out of all verb pairs in the labeled dataset (i.e.,
</bodyText>
<equation confidence="0.799456333333333">
225
812 _ 0.277 .
812 - )
</equation>
<bodyText confidence="0.994790083333334">
VO-KB: A simple unsupervised rule: for any
pair (v1, v2), classify as “yes” if the pair appears in
the strength relation (corresponding to entailment)
in the VerbOcean knowledge-base, which was com-
puted over Web counts.
VO-ukWaC: A simple unsupervised rule: for any
pair (v1, v2), classify as “yes” if the value of the
positive VerbOcean feature is ‘1’ (Section 4.1, com-
puted over ukWaC).
TDS: Include only the 15 distributional similarity
features in our supervised model. This baseline ex-
tends Berant et al. (2012), who trained an entailment
</bodyText>
<table confidence="0.999839428571429">
Method P% R% AUC F1
All 40.2 71.0 0.65 0.51
TDS+VO 36.8 53.2 0.58 0.41
TDS 34.6 44.8 0.56 0.37
Random 27.9 28.8 0.51 0.28
VO-KB 33.1 14.8 0.53 0.2
VO-ukWaC 23.3 4.7 0.29 0.08
</table>
<tableCaption confidence="0.9947265">
Table 3: Average precision, recall, AUC and Fl for our
method and the baselines.
</tableCaption>
<bodyText confidence="0.99977055">
classifier over several distributional similarity fea-
tures, and provides an evaluation of the discrimina-
tive power of distributional similarity alone, without
co-occurrence features.
TDS+VO: Include only the 15 typed distribu-
tional similarity features and the two VerbOcean
features in our supervised model. This baseline
is inspired by Mirkin et al. (2006), who combined
distributional similarity features and Hearst pat-
terns (Hearst, 1992) for learning entailment between
nouns.
All: Our full-blown model, including all features
described in Section 4.1.
For all tested methods, we performed 10-fold
cross validation and averaged Precision, Recall,
Area under the ROC curve (AUC) and F1 over the 10
folds. Table 3 presents the results of our full-blown
model as well as the baselines.
First, we note that, as expected, the VerbOcean
baselines VO-KB and VO-ukWaC provide low recall,
</bodyText>
<page confidence="0.993079">
201
</page>
<table confidence="0.999862875">
Method P% R% AUC F1
All 40.2 71.0 0.65 0.51
Sent+Corpus-level 39.7 70.4 0.64 0.50
Sent+Doc-level 39.0 70.0 0.63 0.50
Doc+Corpus-level 37.7 64.0 0.62 0.47
Sent-level 35.8 63.8 0.59 0.46
Doc-level 30.0 45.4 0.52 0.35
Corpus-level 35.4 58.1 0.58 0.44
</table>
<tableCaption confidence="0.962791">
Table 4: Average precision, recall, AUC and Fl for each
subset of the feature groups.
</tableCaption>
<bodyText confidence="0.9999926">
due to the sparseness of rigid pattern instantiation
for verbs both in the ukWaC corpus and on the web.
Yet, VerbOcean positive and negative patterns do
add some discriminative power over only distribu-
tional similarity measures, as seen by the improve-
ment of TDS+VO over TDS in all criteria. But, it is
the combination of all types of information sources
that yields the best performance. Our complete
model, employing the full set of features, outper-
forms all other models in terms of both precision and
recall. Its improvement in terms of F1 over the sec-
ond best model (TDS+VO), which includes all distri-
butional similarity features as well as pattern-based
features, is by 24%. This result shows the benefits
of integrating linguistically motivated co-occurrence
features with traditional pattern-based and distribu-
tional similarity information.
To further investigate the contribution of fea-
tures at various co-occurrence levels, we trained
and tested our model with all possible combina-
tions of feature groups corresponding to a certain
co-occurrence scope (sentence, document and cor-
pus). Table 4 presents the results of these tests.
The most notable result of this analysis is that
sentence-level features play an important role within
our model. Indeed, removing either the document-
level features (Sent+Corpus-level) or the corpus-
level features (Sent+Doc-level) results in only a
slight decline in performance. Yet, removing the
sentence-level features (Doc+Corpus-level), ends in
a more substantial decline of 8.5% in F1. In addi-
tion, sentence-level features alone (Sent-level) pro-
vide the best discriminative power for verb entail-
ment, compared to document and corpus levels,
which include distributional similarity features. Yet,
we note that sentence-level features alone do not
capture all the information within our model, and
they should be combined with one of the other fea-
ture groups to reach performance close to the com-
plete model. This shows again the importance of
combining co-occurrence indicators at different lev-
els.
As an additional insight from Table 4, we point
out that document-level features are not good en-
tailment indicators by themselves (Doc-level in Ta-
ble 4), and they perform worse than the distribu-
tional similarity baseline (TDS at Table 3). Still, they
do complement each of the other feature groups. In
particular, since the Sent+Doc-level model performs
almost as good as the full model, this subset may
be a good substitute to the full model, since its fea-
tures are easier to extract from large corpora, as they
may be extracted in an on-line fashion, processing
one document at a time (contrary to corpus-level fea-
tures).
As a final analysis, we randomly sampled cor-
rect entailment rules learned by our model but
missed by the typed distributional similarity classi-
fier (TDS). Our overall impression is that employ-
ing co-occurrence information helps to better cap-
ture entailment relations other than synonymy and
troponymy. For example, our model learns that ac-
quire —* own, corresponding to the cause-effect en-
tailment relation, and that patent —* invent, corre-
sponding to the presupposition entailment relation.
</bodyText>
<sectionHeader confidence="0.999137" genericHeader="conclusions">
6 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999992866666667">
We presented a supervised classification model for
detecting lexical entailment between verbs. At the
heart of our model stand novel linguistically moti-
vated indicators that capture positive and negative
entailment information. These indicators encom-
pass co-occurrence relationships between verbs at
the sentence, document and corpus level, as well
as more fine-grained typed distributional similarity
measures. Our model incorporates these novel indi-
cators together with useful features from prior work,
combining co-occurrence and distributional similar-
ity information about verb pairs.
Our experiment over a manually labeled dataset
showed that our model significantly outperforms
several state-of-the-art models both in terms of Pre-
</bodyText>
<page confidence="0.992982">
202
</page>
<bodyText confidence="0.999848363636364">
cision and Recall. Further feature analysis indicated
that our novel indicators contribute greatly to the
performance of the model, and that co-occurrence
at multiple levels, combined with distributional sim-
ilarity features, is necessary to achieve the model’s
best performance.
In future work we’d like to investigate which in-
dicators may contribute to learning different fine-
grained types of entailment, such as presupposition
and cause-effect, and attempt to perform a more
fine-grained classification to subtypes of entailment.
</bodyText>
<sectionHeader confidence="0.998091" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999386285714286">
This work was partially supported by the Israel
Science Foundation grant 1112/08, the PASCAL-
2 Network of Excellence of the European Com-
munity FP7-ICT-2007-1-216886, and the Euro-
pean Community’s Seventh Framework Programme
(FP7/2007-2013) under grant agreement no. 287923
(EXCITEMENT).
</bodyText>
<sectionHeader confidence="0.99877" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999450417721519">
Shuya Abe, Kentaro Inui, and Yuji Matsumoto. 2008.
Acquiring event relation knowledge by learning cooc-
currence patterns and fertilizing cooccurrence samples
with verbal nouns. In Proceedings of IJCNLP.
Timothy Baldwin and Aline Villavicencio. 2002. Ex-
tracting the unextractable: a case study on verb-
particles. In proceedings of COLING.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and
Eros Zanchetta. 2009. The wacky wide web: A
collection of very large linguistically processed web-
crawled corpora. Language Resources and Evalua-
tion, 43(3):209–226.
Jonathan Berant, Ido Dagan, and Jacob Goldberger.
2012. Learning entailment relations by global graph
structure optimization. Computational Linguistics,
38(1):73–111.
Nathanael Chambers and Dan Jurafsky. 2008. Unsuper-
vised learning of narrative event chains. In Proceed-
ings of ACL.
Timothy Chklovski and Patrick Pantel. 2004. Verb
ocean: Mining the web for fine-grained semantic verb
relations. In Proceedings of EMNLP.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The pascal recognising textual entailment chal-
lenge. In Machine Learning Challenges, volume 3944
of Lecture Notes in Computer Science, pages 177–190.
Springer.
Lin Dekang and Patrick Pantel. 2001. Dirt - discovery
of inference rules from text. In In Proceedings of the
ACM SIGKDD Conference on Knowledge Discovery
and Data Mining.
Maayan Geffet and Ido Dagan. 2005. The distributional
inclusion hypotheses and lexical entailment. In Pro-
ceedings of ACL.
Isabelle Guyon and Andre Elisseeff. 2003. An intro-
duction to variable and feature selection. Journal of
Machine Learning Research, 3:1157–1182.
Masato Hagiwara, Yasuhiro Ogawa, and Katsuhiko
Toyama. 2009. Supervised synonym acquisition us-
ing distributional features and syntactic patterns. In
Journal of Natural Language Processing.
Marti Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proceedings of COLING.
Jerry Hobbs. 1979. Coherence and coreference. Cogni-
tive Science, 3:67–90.
Eduard Hovy and Elisabeth Maier. 1993. Organizing
Discourse Structure Relations using Metafunctions.
Pinter Publishing.
Ray Jackendoff. 1983. Semantics and Cognition. The
MIT Press.
T. Joachims. 2005. A support vector method for mul-
tivariate performance measures. In Proceedings of
ICML.
Alistair Knott and Ted Sanders. 1998. The classification
of coherence relations and their linguistic markers: An
exploration of two languages. In Journal of Pragmat-
ics.
Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan
Zhitomirsky-Geffet. 2010. Directional distributional
similarity for lexical inference. Natural Language En-
gineering, 16(4):359–389.
Beth Levin. 1993. English Verb Classes and Alter-
nations: A Preliminary Investigation. University Of
Chicago Press.
Dekang Lin. 1998. An information-theoretic definition
of similarity. In Proceedings of ICML.
William Mann and Sandra Thompson. 1988. Rhetorical
structure theory: Toward a functional theory of text
organization. Text, 8(3):243–281.
Daniel Marcu and Abdessamad Echihabi. 2002. An
unsupervised approach to recognizing discourse rela-
tions. In Proceedings of ACL.
Shachar Mirkin, Ido Dagan, and Maayan Geffet. 2006.
Integrating pattern-based and distributional similarity
methods for lexical entailment acquisition. In Pro-
ceedings of the COLING/ACL.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006. Malt-
parser: A data-driven parser-generator for dependency
parsing. In Proceedings of LREC.
</reference>
<page confidence="0.985113">
203
</page>
<reference confidence="0.999796135135135">
Viktor Pekar. 2008. Discovery of event entailment
knowledge from text corpora. Comput. Speech Lang.,
22(1):1–16.
Marco Pennacchiotti and Patrick Pantel. 2009. Entity
extraction via ensemble semantics. In Proceedings of
EMNLP.
Deepak Ravichandran and Eduard Hovy. 2002. Learning
surface text patterns for a question answering system.
In Proceedings of ACL.
Deborah Schiffrin. 1988. Discourse Markers. Cam-
bridge University Press.
Stefan Schoenmackers, Jesse Davis, Oren Etzioni, and
Daniel S. Weld. 2010. Learning first-order horn
clauses from web text. In Proceedings of EMNLP.
Yusuke Shinyama and Satoshi Sekine. 2006. Preemp-
tive information extraction using unrestricted relation
discovery. In Proceedings of NAACL-HLT.
Idan Szpektor and Ido Dagan. 2008. Learning entailment
rules for unary templates. In Proceedings of COLING.
Idan Szpektor, Hristo Tanev, and Ido Dagan. 2004. Scal-
ing web-based acquisition of entailment relations. In
In Proceedings of EMNLP.
Idan Szpektor, Eyal Shnarch, and Ido Dagan. 2007. In-
stance based evaluation of entailment rule acquisition.
In Proceedings of the 45th Annual Meeting of the As-
sociation of Computational Linguistics.
Galina Tremper. 2010. Weakly supervised learning of
presupposition relations between verbs. In Proceed-
ings of ACL student workshop.
Julie Weeds and David Weir. 2003. A general frame-
work for distributional similarity. In Proceedings of
EMNLP.
Fabio Massimo Zanzotto, Marco Pennacchiotti, and
Maria Teresa Pazienza. 2006. Discovering asym-
metric entailment relations between verbs using se-
lectional preferences. In Proceedings of the COL-
ING/ACL.
</reference>
<page confidence="0.998889">
204
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.131174">
<title confidence="0.999918">Learning Verb Inference Rules from Linguistically-Motivated Evidence</title>
<author confidence="0.991489">Jonathan Idan Ido</author>
<affiliation confidence="0.356748333333333">Science Department, Bar-Ilan Blavatnik School of Computer Science, Tel Aviv Research</affiliation>
<abstract confidence="0.99940980952381">Learning inference relations between verbs is at the heart of many semantic applications. However, most prior work on learning such rules focused on a rather narrow set of information sources: mainly distributional similarity, and to a lesser extent manually constructed verb co-occurrence patterns. In this paper, we claim that it is imperative to utilize information from various textual scopes: verb co-occurrence within a sentence, verb cooccurrence within a document, as well as overall corpus statistics. To this end, we propose a much richer novel set of linguistically motivated cues for detecting entailment between verbs and combine them as features in a supervised classification framework. We empirically demonstrate that our model significantly outperforms previous methods and that information from each textual scope contributes to the verb entailment learning task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Shuya Abe</author>
<author>Kentaro Inui</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Acquiring event relation knowledge by learning cooccurrence patterns and fertilizing cooccurrence samples with verbal nouns.</title>
<date>2008</date>
<booktitle>In Proceedings of IJCNLP.</booktitle>
<contexts>
<context position="2181" citStr="Abe et al., 2008" startWordPosition="327" endWordPosition="330">ion “What are Churros covered with?”. Inference rules specify a directional inference relation between two text fragments, and we follow the Textual Entailment modeling of inference (Dagan et al., 2006), which refers to such rules as entailment rules. In this work we focus on one of the most important rule types, namely, lexical entailment rules between verbs (verb entailment), e.g., ‘whisper —* talk’, ‘win —* play’ and ‘buy —* own’. The significance of such rules has led to active research in automatic learning of entailment rules between verbs or verb-like structures (Zanzotto et al., 2006; Abe et al., 2008; Schoenmackers et al., 2010). Most prior efforts to learn verb entailment rules from large corpora employed distributional similarity methods, assuming that verbs are semantically similar if they occur in similar contexts (Lin, 1998; Berant et al., 2012). This led to the automatic acquisition of large scale knowledge bases, but with limited precision. Fewer works, such as VerbOcean (Chklovski and Pantel, 2004), focused on identifying verb entailment through verb instantiation of manually constructed patterns. For example, the sentence “he scared and even startled me” implies that ‘startle —* </context>
</contexts>
<marker>Abe, Inui, Matsumoto, 2008</marker>
<rawString>Shuya Abe, Kentaro Inui, and Yuji Matsumoto. 2008. Acquiring event relation knowledge by learning cooccurrence patterns and fertilizing cooccurrence samples with verbal nouns. In Proceedings of IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timothy Baldwin</author>
<author>Aline Villavicencio</author>
</authors>
<title>Extracting the unextractable: a case study on verbparticles.</title>
<date>2002</date>
<booktitle>In proceedings of COLING.</booktitle>
<contexts>
<context position="10617" citStr="Baldwin and Villavicencio, 2002" startWordPosition="1615" endWordPosition="1619">anticallyrelated verbs sharing some linguistic properties (Levin, 1993). One of the most general verb classes are stative vs. event verbs (Jackendoff, 1983). Stative verb, such as ‘love’ and ‘think’, usually describe a state that lasts some time. On the other hand, event verbs, such as ‘run’ and ‘kiss’, describe an action. We hypothesize that verb classes are relevant for determining entailment, for example, that stative verbs are not likely to entail event verbs. Verb generality Verb-particle constructions are multi-word expressions consisting of a head verb and a particle, e.g., switch off (Baldwin and Villavicencio, 2002). We conjecture that the more general a verb is, the more likely it is to appear with many different particles. Detecting verb generality can help us tackle an infamous property of distributional similarity methods, namely, the difficulty in detecting the direction of entailment (Berant et al., 2012). For example, the verb ’cover’ appears with many different particles such as ’up’ and ’for’, while the verb ’coat’ does not. Thus, assuming we have evidence for an entailment relation between the two verbs, this indicator can help us discern the direction of entailment and determine that ‘coat —* </context>
</contexts>
<marker>Baldwin, Villavicencio, 2002</marker>
<rawString>Timothy Baldwin and Aline Villavicencio. 2002. Extracting the unextractable: a case study on verbparticles. In proceedings of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Silvia Bernardini</author>
<author>Adriano Ferraresi</author>
<author>Eros Zanchetta</author>
</authors>
<title>The wacky wide web: A collection of very large linguistically processed webcrawled corpora. Language Resources and Evaluation,</title>
<date>2009</date>
<pages>43--3</pages>
<contexts>
<context position="15795" citStr="Baroni et al., 2009" startWordPosition="2452" endWordPosition="2455"> same procedure is done when v1 is in the subordinate clause and v2 in the main clause. We term the features by the relevant discourse relation, e.g., ‘v1-contrast-v2’ refers to v1 being in the main clause and connected to the subordinate clause via a contrast marker. Dependency relations between clauses As noted in Section 3, the syntactic structure of verb cooccurrence can indicate the existence or lack of entailment. In dependency parsing this may be expressed via the label of the dependency relation connecting the main and subordinate clauses. In our experiments we used the ukWaC corpus1 (Baroni et al., 2009) which was parsed by the MALT parser (Nivre et al., 2006). Hence, we identified three MALT dependency relations that connect a main clause with its subordinate clause. The first relation is the object complement relation ‘obj’. In this case the subordinate clause is an object complement of the main clause. For example, in “it surprised me that the lizard could talk” the verb pair (‘surprise’,‘talk’) is connected by the ‘obj’ relation. The second relation is the adverbial adjunct relation ‘adv’, in which the subordinate clause is adverbial and describes the time, place, manner, etc. of the main</context>
<context position="27010" citStr="Baroni et al., 2009" startWordPosition="4321" endWordPosition="4324"> considered as correct if the annotator could think of reasonable contexts under which the rule holds (Dekang and Pantel, 2001; Szpektor et al., 2004). In total 225 verb pairs were labeled as entailing (the rule ‘v1 —* v2’ was judged as correct) and 587 verb pairs were labeled as non-entailing (the rule ‘v1 —* v2’ was judged as incorrect). The InterAnnotator Agreement (IAA) for a random sample of 100 pairs was moderate (0.47), as expected from the rule-based approach (Szpektor et al., 2007). For each verb pair, all 63 features within our model (Section 4) were computed using the ukWaC corpus (Baroni et al., 2009), which contains 2 billion words. For classification, we utilized SVM-perf’s (Joachims, 2005) linear SVM implementation with default parameters, and evaluated our model by performing 10-fold cross validation (CV) over the labeled dataset. 5http://trec.nist.gov/data/reuters/ reuters.html 6The data set is available at http://www.cs.biu.ac. il/˜nlp/downloads/verb-pair-annotation. html 5.2 Feature selection and analysis As discussed in Section 4.2, we followed the feature ranking method proposed by Guyon et al. (2003) to investigate the utility of our proposed features. Table 2 depicts the 10 most</context>
</contexts>
<marker>Baroni, Bernardini, Ferraresi, Zanchetta, 2009</marker>
<rawString>Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and Eros Zanchetta. 2009. The wacky wide web: A collection of very large linguistically processed webcrawled corpora. Language Resources and Evaluation, 43(3):209–226.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Berant</author>
<author>Ido Dagan</author>
<author>Jacob Goldberger</author>
</authors>
<title>Learning entailment relations by global graph structure optimization.</title>
<date>2012</date>
<journal>Computational Linguistics,</journal>
<volume>38</volume>
<issue>1</issue>
<contexts>
<context position="2436" citStr="Berant et al., 2012" startWordPosition="365" endWordPosition="368"> In this work we focus on one of the most important rule types, namely, lexical entailment rules between verbs (verb entailment), e.g., ‘whisper —* talk’, ‘win —* play’ and ‘buy —* own’. The significance of such rules has led to active research in automatic learning of entailment rules between verbs or verb-like structures (Zanzotto et al., 2006; Abe et al., 2008; Schoenmackers et al., 2010). Most prior efforts to learn verb entailment rules from large corpora employed distributional similarity methods, assuming that verbs are semantically similar if they occur in similar contexts (Lin, 1998; Berant et al., 2012). This led to the automatic acquisition of large scale knowledge bases, but with limited precision. Fewer works, such as VerbOcean (Chklovski and Pantel, 2004), focused on identifying verb entailment through verb instantiation of manually constructed patterns. For example, the sentence “he scared and even startled me” implies that ‘startle —* scare’. This led to more precise rule extraction, but with poor coverage since contrary to nouns, in which patterns are common (Hearst, 1992), verbs do not co-occur often within rigid patterns. However, verbs do tend to co-occur in the same document, and </context>
<context position="7635" citStr="Berant et al. (2012)" startWordPosition="1151" endWordPosition="1155">n et al. (2006) introduced a system for learning entailment rules between nouns (e.g., ‘novel —* book’) that combines distributional similarity and Hearst patterns as features in a supervised classifier. Pennacchiotti and Pantel (2009) augment N irkin et al’s features with web-based features for the task of entity extraction. Hagiwara et al. (2009) perform synonym identification based on both distributional and contextual features. Tremper (2010) extract “loose” sentence-level features in order to identify the presupposition relation (e.g., , the verb ‘win’ presupposes the verb ‘play’). Last, Berant et al. (2012) utilized various distributional similarity features to identify entailment between lexical-syntactic predicates. In this paper, we follow the supervised approach for semantic relation detection in order to identify verb entailment. While we utilize and adapt useful features from prior work, we introduce a diverse set of novel features for the task, effectively combining verb co-occurrence information at the sentence, document, and corpus levels. 3 Linguistically-Motivated Indicators As mentioned in Section 1, verbs behave quite differently from nouns in corpora. In this section, we 195 introd</context>
<context position="10918" citStr="Berant et al., 2012" startWordPosition="1665" endWordPosition="1668"> an action. We hypothesize that verb classes are relevant for determining entailment, for example, that stative verbs are not likely to entail event verbs. Verb generality Verb-particle constructions are multi-word expressions consisting of a head verb and a particle, e.g., switch off (Baldwin and Villavicencio, 2002). We conjecture that the more general a verb is, the more likely it is to appear with many different particles. Detecting verb generality can help us tackle an infamous property of distributional similarity methods, namely, the difficulty in detecting the direction of entailment (Berant et al., 2012). For example, the verb ’cover’ appears with many different particles such as ’up’ and ’for’, while the verb ’coat’ does not. Thus, assuming we have evidence for an entailment relation between the two verbs, this indicator can help us discern the direction of entailment and determine that ‘coat —* cover’. Typed Distributional Similarity As discussed in section 2, distributional similarity is the most common source of information for learning semantic relations between verbs. Yet, we suggest that on top of standard distributional similarity measures, which take several verbal arguments into acc</context>
<context position="31449" citStr="Berant et al. (2012)" startWordPosition="4982" endWordPosition="4985">r of entailing verb pairs out of all verb pairs in the labeled dataset (i.e., 225 812 _ 0.277 . 812 - ) VO-KB: A simple unsupervised rule: for any pair (v1, v2), classify as “yes” if the pair appears in the strength relation (corresponding to entailment) in the VerbOcean knowledge-base, which was computed over Web counts. VO-ukWaC: A simple unsupervised rule: for any pair (v1, v2), classify as “yes” if the value of the positive VerbOcean feature is ‘1’ (Section 4.1, computed over ukWaC). TDS: Include only the 15 distributional similarity features in our supervised model. This baseline extends Berant et al. (2012), who trained an entailment Method P% R% AUC F1 All 40.2 71.0 0.65 0.51 TDS+VO 36.8 53.2 0.58 0.41 TDS 34.6 44.8 0.56 0.37 Random 27.9 28.8 0.51 0.28 VO-KB 33.1 14.8 0.53 0.2 VO-ukWaC 23.3 4.7 0.29 0.08 Table 3: Average precision, recall, AUC and Fl for our method and the baselines. classifier over several distributional similarity features, and provides an evaluation of the discriminative power of distributional similarity alone, without co-occurrence features. TDS+VO: Include only the 15 typed distributional similarity features and the two VerbOcean features in our supervised model. This bas</context>
</contexts>
<marker>Berant, Dagan, Goldberger, 2012</marker>
<rawString>Jonathan Berant, Ido Dagan, and Jacob Goldberger. 2012. Learning entailment relations by global graph structure optimization. Computational Linguistics, 38(1):73–111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nathanael Chambers</author>
<author>Dan Jurafsky</author>
</authors>
<title>Unsupervised learning of narrative event chains.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="6623" citStr="Chambers and Jurafsky (2008)" startWordPosition="998" endWordPosition="1001">strength, antonymy, enablement and happens-before. For example, the pattern ‘Xed and later Yed’ signals the happensbefore relation between the verbs ‘X’ and ‘Y’. Starting with candidate verb pairs based on a distributional similarity measure, the patterns are used to choose a semantic relation per verb pair based on the different patterns this pair instantiates. This method is more precise than distributional similarity approaches, but it is highly susceptible to sparseness issues, since verbs do not typically co-occur within rigid patterns. Utilizing verb co-occurrence at the document level, Chambers and Jurafsky (2008) estimate whether a pair of verbs is narratively related by counting the number of times the verbs share an argument in the same document. In a similar manner, Pekar (2008) detects entailment rules between templates from shared arguments within discourserelated clauses in the same document. Recently, supervised classification has become standard in performing various semantic tasks. N irkin et al. (2006) introduced a system for learning entailment rules between nouns (e.g., ‘novel —* book’) that combines distributional similarity and Hearst patterns as features in a supervised classifier. Penn</context>
<context position="21001" citStr="Chambers and Jurafsky (2008)" startWordPosition="3303" endWordPosition="3306"> features are computed for the distance in words (bins are 0 &lt; d &lt; 5, 5 &lt; d &lt; 10, d &gt; 10). This feature provides insight into the syntactic relatedness of the verbs. Sentence-level pmi Pointwise mutual information (pmi) between v1 and v2 is computed, where the cooccurrence scope is a sentence. Higher pmi should hint at semantically related verbs. 4.1.2 Document-level co-occurrence This group of features addresses co-occurrence of a target verb pair within the same document. These features are less sparse, but tend to capture coarser semantic relations between the target verbs. Narrative score Chambers and Jurafsky (2008) suggested a method for learning sequences of actions or events (expressed by verbs) in which a sin198 gle entity is involved. They proposed a pmi-like narrative score (see Eq. (1) in their paper) that estimates whether a pair consisting of a verb and one of its dependency relations (v1, r1) is narrativelyrelated to another such pair (v2, r2). Their estimation is based on quantifying the likelihood that two verbs will share an argument that instantiates both the dependency position (v1, r1) and (v2, r2) within documents in which the two verbs co-occur. For example, given the document “Lindsay </context>
</contexts>
<marker>Chambers, Jurafsky, 2008</marker>
<rawString>Nathanael Chambers and Dan Jurafsky. 2008. Unsupervised learning of narrative event chains. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timothy Chklovski</author>
<author>Patrick Pantel</author>
</authors>
<title>Verb ocean: Mining the web for fine-grained semantic verb relations.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="2595" citStr="Chklovski and Pantel, 2004" startWordPosition="390" endWordPosition="393">’, ‘win —* play’ and ‘buy —* own’. The significance of such rules has led to active research in automatic learning of entailment rules between verbs or verb-like structures (Zanzotto et al., 2006; Abe et al., 2008; Schoenmackers et al., 2010). Most prior efforts to learn verb entailment rules from large corpora employed distributional similarity methods, assuming that verbs are semantically similar if they occur in similar contexts (Lin, 1998; Berant et al., 2012). This led to the automatic acquisition of large scale knowledge bases, but with limited precision. Fewer works, such as VerbOcean (Chklovski and Pantel, 2004), focused on identifying verb entailment through verb instantiation of manually constructed patterns. For example, the sentence “he scared and even startled me” implies that ‘startle —* scare’. This led to more precise rule extraction, but with poor coverage since contrary to nouns, in which patterns are common (Hearst, 1992), verbs do not co-occur often within rigid patterns. However, verbs do tend to co-occur in the same document, and also in different clauses of the same sentence. In this paper, we claim that on top of standard pattern-based and distributional similarity methods, corpus-bas</context>
<context position="14515" citStr="Chklovski and Pantel, 2004" startWordPosition="2228" endWordPosition="2231">ors, as well as features that were adapted from prior work. Discourse markers As discussed in Section 3, discourse markers may signal relations between the main verbs of adjacent clauses. The literature is abundant with taxonomies that classify markers to various discourse relations (Mann and Thompson, 1988; Hovy and Maier, 1993; Knott and Sanders, 1998). Inspired by Marcu and Echihabi (2002), we employ markers that are mapped to four discourse relations ’Contrast’, ’Cause’, ’Condition’ and ’Temporal’, as specified in Table 1. This definition can be viewed as a relaxed version of VerbOcean’s (Chklovski and Pantel, 2004) patterns, although the underlying intuition is different (see Section 3). For a target verb pair (v1, v2) and each discourse relation r, we count the number of times that v1 is the main verb in the main clause, v2 is the main verb in the subordinate clause, and the clauses are connected via a marker mapped to r. For example, given the sentence “You must enroll in the competition before you can participate in it”, the verb pair (‘enroll’,‘participate’) appears in the ’Temporal’ relation, indicated by the marker ‘before’, where ‘enroll’ is in the main clause. Each count is then normalized by th</context>
<context position="17540" citStr="Chklovski and Pantel (2004)" startWordPosition="2742" endWordPosition="2746">ey are connected by any dependency relation. We term the features by the dependency label, e.g., ‘v1-adv-v2’ refers to v1 being in the main clause and connected to the subordinate clause via an adverbial adjunct. 1http://wacky.sslmit.unibo.it/doku.php? id=corpora 197 Discourse Rel. Discourse Markers Contrast although , despite , but , whereas , notwithstanding , though Cause because , therefore , thus Condition if , unless Temporal whenever , after , before , until , when , finally , during , afterwards , meanwhile Table 1: Discourse relations and their mapped markers. Pattern-based We follow Chklovski and Pantel (2004) and extract occurrences of VerbOcean patterns that are instantiated by the target verb pair. As mentioned in Section 2, VerbOcean patterns were originally grouped into five semantic classes. Based on a preliminary study we conducted, we decided to utilize only four strength-class patterns as positive indicators for entailment, e.g., “he scared and even startled me”, and three antonym-class patterns as negative indicators for entailment, e.g., “you can either open or close the door”. We note that these patterns are also commonly used by RTE systems2. Since the corpus pattern counts were very s</context>
</contexts>
<marker>Chklovski, Pantel, 2004</marker>
<rawString>Timothy Chklovski and Patrick Pantel. 2004. Verb ocean: Mining the web for fine-grained semantic verb relations. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Oren Glickman</author>
<author>Bernardo Magnini</author>
</authors>
<title>The pascal recognising textual entailment challenge.</title>
<date>2006</date>
<booktitle>In Machine Learning Challenges,</booktitle>
<volume>3944</volume>
<pages>177--190</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="1767" citStr="Dagan et al., 2006" startWordPosition="255" endWordPosition="258">at information from each textual scope contributes to the verb entailment learning task. 1 Introduction Inference rules are an important building block of many semantic applications, such as Question Answering (Ravichandran and Hovy, 2002) and Information Extraction (Shinyama and Sekine, 2006). For example, given the sentence “Churros are coated with sugar”, one can use the rule ‘coat —* cover’ to answer the question “What are Churros covered with?”. Inference rules specify a directional inference relation between two text fragments, and we follow the Textual Entailment modeling of inference (Dagan et al., 2006), which refers to such rules as entailment rules. In this work we focus on one of the most important rule types, namely, lexical entailment rules between verbs (verb entailment), e.g., ‘whisper —* talk’, ‘win —* play’ and ‘buy —* own’. The significance of such rules has led to active research in automatic learning of entailment rules between verbs or verb-like structures (Zanzotto et al., 2006; Abe et al., 2008; Schoenmackers et al., 2010). Most prior efforts to learn verb entailment rules from large corpora employed distributional similarity methods, assuming that verbs are semantically simil</context>
</contexts>
<marker>Dagan, Glickman, Magnini, 2006</marker>
<rawString>Ido Dagan, Oren Glickman, and Bernardo Magnini. 2006. The pascal recognising textual entailment challenge. In Machine Learning Challenges, volume 3944 of Lecture Notes in Computer Science, pages 177–190. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lin Dekang</author>
<author>Patrick Pantel</author>
</authors>
<title>Dirt - discovery of inference rules from text. In</title>
<date>2001</date>
<booktitle>In Proceedings of the ACM SIGKDD Conference on Knowledge Discovery and Data Mining.</booktitle>
<contexts>
<context position="26516" citStr="Dekang and Pantel, 2001" startWordPosition="4235" endWordPosition="4238">s) and (vs, vs), which represent the candidate rules ‘vs —* vs’ and ‘vs vs’ respectively. To reduce noise, we filtered out verb pairs where one of the verbs is an auxiliary or a light verb such as ’do’, ’get’ and ’have’. This step resulted in 812 verb pairs as our dataset6, which were manually annotated by the authors as representing a valid entailment rule or not. To annotate these pairs, we generally followed the rule-based approach for entailment rule annotation, where a rule ‘v1 —* v2’ is considered as correct if the annotator could think of reasonable contexts under which the rule holds (Dekang and Pantel, 2001; Szpektor et al., 2004). In total 225 verb pairs were labeled as entailing (the rule ‘v1 —* v2’ was judged as correct) and 587 verb pairs were labeled as non-entailing (the rule ‘v1 —* v2’ was judged as incorrect). The InterAnnotator Agreement (IAA) for a random sample of 100 pairs was moderate (0.47), as expected from the rule-based approach (Szpektor et al., 2007). For each verb pair, all 63 features within our model (Section 4) were computed using the ukWaC corpus (Baroni et al., 2009), which contains 2 billion words. For classification, we utilized SVM-perf’s (Joachims, 2005) linear SVM i</context>
</contexts>
<marker>Dekang, Pantel, 2001</marker>
<rawString>Lin Dekang and Patrick Pantel. 2001. Dirt - discovery of inference rules from text. In In Proceedings of the ACM SIGKDD Conference on Knowledge Discovery and Data Mining.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maayan Geffet</author>
<author>Ido Dagan</author>
</authors>
<title>The distributional inclusion hypotheses and lexical entailment.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="5567" citStr="Geffet and Dagan, 2005" startWordPosition="840" endWordPosition="843">entailment detection. 2 Background The main approach for learning entailment rules between verbs and verb-like structures has employed the distributional hypothesis, which assumes that words with similar meanings appear in similar contexts. For example, we expect the words ‘buy’ and ‘purchase’ to occur with similar subjects and objects in a large corpus. This observation has led to ample work on developing both symmetric and directional similarity measures that attempt to capture semantic relations between lexical items by comparing their neighborhood context (Lin, 1998; Weeds and Weir, 2003; Geffet and Dagan, 2005; Szpektor and Dagan, 2008; Kotlerman et al., 2010). A far less explored direction for learning verb entailment involves exploiting verb co-occurrence in a sentence or a document. One prominent work is Chklovsky and Pantel’s VerbOcean (2004). In VerbOcean, the authors manually constructed 33 patterns and divided them into five pattern groups, where each group signals one of the following five semantic relations: similarity, strength, antonymy, enablement and happens-before. For example, the pattern ‘Xed and later Yed’ signals the happensbefore relation between the verbs ‘X’ and ‘Y’. Starting w</context>
<context position="27900" citStr="Geffet and Dagan, 2005" startWordPosition="4447" endWordPosition="4450">uters/ reuters.html 6The data set is available at http://www.cs.biu.ac. il/˜nlp/downloads/verb-pair-annotation. html 5.2 Feature selection and analysis As discussed in Section 4.2, we followed the feature ranking method proposed by Guyon et al. (2003) to investigate the utility of our proposed features. Table 2 depicts the 10 most positively and negatively correlated features with entailment according to the Pearson correlation measure From Table 2, it is clear that distributional similarity features are amongst the most positively correlated with entailment, which is in line with prior work (Geffet and Dagan, 2005; Kotlerman et al., 2010). Looking more closely, our suggestion for typed distributional similarity proved to be useful, and indeed most of the highly correlated distributional similarity features are typed measures. Standing out are the adverb-typed measures, with two features in the top 10, including the highest, ‘Weedsadverb’, and ‘BInc-adverb’. We also note that the highly correlated distributional similarity measures are directional, Weeds and BInc. The table also indicates that document-level cooccurrence contributes positively to entailment detection. This includes both the Chambers nar</context>
</contexts>
<marker>Geffet, Dagan, 2005</marker>
<rawString>Maayan Geffet and Ido Dagan. 2005. The distributional inclusion hypotheses and lexical entailment. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Isabelle Guyon</author>
<author>Andre Elisseeff</author>
</authors>
<title>An introduction to variable and feature selection.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--1157</pages>
<marker>Guyon, Elisseeff, 2003</marker>
<rawString>Isabelle Guyon and Andre Elisseeff. 2003. An introduction to variable and feature selection. Journal of Machine Learning Research, 3:1157–1182.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masato Hagiwara</author>
<author>Yasuhiro Ogawa</author>
<author>Katsuhiko Toyama</author>
</authors>
<title>Supervised synonym acquisition using distributional features and syntactic patterns.</title>
<date>2009</date>
<journal>In Journal of Natural Language Processing.</journal>
<contexts>
<context position="7365" citStr="Hagiwara et al. (2009)" startWordPosition="1113" endWordPosition="1116">the same document. In a similar manner, Pekar (2008) detects entailment rules between templates from shared arguments within discourserelated clauses in the same document. Recently, supervised classification has become standard in performing various semantic tasks. N irkin et al. (2006) introduced a system for learning entailment rules between nouns (e.g., ‘novel —* book’) that combines distributional similarity and Hearst patterns as features in a supervised classifier. Pennacchiotti and Pantel (2009) augment N irkin et al’s features with web-based features for the task of entity extraction. Hagiwara et al. (2009) perform synonym identification based on both distributional and contextual features. Tremper (2010) extract “loose” sentence-level features in order to identify the presupposition relation (e.g., , the verb ‘win’ presupposes the verb ‘play’). Last, Berant et al. (2012) utilized various distributional similarity features to identify entailment between lexical-syntactic predicates. In this paper, we follow the supervised approach for semantic relation detection in order to identify verb entailment. While we utilize and adapt useful features from prior work, we introduce a diverse set of novel f</context>
<context position="12746" citStr="Hagiwara et al., 2009" startWordPosition="1951" endWordPosition="1954">appear with the adverb ‘loudly’, while the verb ‘talk’ is more likely to appear with such an adverb. Thus, measuring similarity based solely on adverb modifiers could reveal this phenomenon. 4 Supervised Entailment Detection In the previous section, we discussed linguistic observations regarding novel indicators that may help in detecting entailment relations between verbs. We next describe how to incorporate these indicators as features within a supervised framework for learning lexical entailment rules between verbs. We follow prior work on supervised lexical semantics (Mirkin et al., 2006; Hagiwara et al., 2009; Tremper, 2010) 196 and address the rule learning task as a classification task. Specifically, given an ordered verb pair (v1, v2) as input, we learn a classifier that detects whether the entailment relation ‘v1 —* v2’ holds for this pair. We next detail how our novel indicators, as well as other diverse sources of information found useful in prior work, are encoded as features. Then, we describe the learning model and our feature analysis procedure. 4.1 Entailment features Most of our features are based on information extracted from the target verb pair co-occurring within varying textual sc</context>
</contexts>
<marker>Hagiwara, Ogawa, Toyama, 2009</marker>
<rawString>Masato Hagiwara, Yasuhiro Ogawa, and Katsuhiko Toyama. 2009. Supervised synonym acquisition using distributional features and syntactic patterns. In Journal of Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti Hearst</author>
</authors>
<title>Automatic acquisition of hyponyms from large text corpora.</title>
<date>1992</date>
<booktitle>In Proceedings of COLING.</booktitle>
<contexts>
<context position="2922" citStr="Hearst, 1992" startWordPosition="442" endWordPosition="443">larity methods, assuming that verbs are semantically similar if they occur in similar contexts (Lin, 1998; Berant et al., 2012). This led to the automatic acquisition of large scale knowledge bases, but with limited precision. Fewer works, such as VerbOcean (Chklovski and Pantel, 2004), focused on identifying verb entailment through verb instantiation of manually constructed patterns. For example, the sentence “he scared and even startled me” implies that ‘startle —* scare’. This led to more precise rule extraction, but with poor coverage since contrary to nouns, in which patterns are common (Hearst, 1992), verbs do not co-occur often within rigid patterns. However, verbs do tend to co-occur in the same document, and also in different clauses of the same sentence. In this paper, we claim that on top of standard pattern-based and distributional similarity methods, corpus-based learning of verb entailment can greatly benefit from exploiting additional linguisticallymotivated cues that are specific to verbs. For instance, when verbs co-occur in different clauses of the same sentence, the syntactic relation between the clauses can be viewed as a proxy for the semantic relation between the verbs. Mo</context>
<context position="32174" citStr="Hearst, 1992" startWordPosition="5099" endWordPosition="5100"> 0.56 0.37 Random 27.9 28.8 0.51 0.28 VO-KB 33.1 14.8 0.53 0.2 VO-ukWaC 23.3 4.7 0.29 0.08 Table 3: Average precision, recall, AUC and Fl for our method and the baselines. classifier over several distributional similarity features, and provides an evaluation of the discriminative power of distributional similarity alone, without co-occurrence features. TDS+VO: Include only the 15 typed distributional similarity features and the two VerbOcean features in our supervised model. This baseline is inspired by Mirkin et al. (2006), who combined distributional similarity features and Hearst patterns (Hearst, 1992) for learning entailment between nouns. All: Our full-blown model, including all features described in Section 4.1. For all tested methods, we performed 10-fold cross validation and averaged Precision, Recall, Area under the ROC curve (AUC) and F1 over the 10 folds. Table 3 presents the results of our full-blown model as well as the baselines. First, we note that, as expected, the VerbOcean baselines VO-KB and VO-ukWaC provide low recall, 201 Method P% R% AUC F1 All 40.2 71.0 0.65 0.51 Sent+Corpus-level 39.7 70.4 0.64 0.50 Sent+Doc-level 39.0 70.0 0.63 0.50 Doc+Corpus-level 37.7 64.0 0.62 0.47</context>
</contexts>
<marker>Hearst, 1992</marker>
<rawString>Marti Hearst. 1992. Automatic acquisition of hyponyms from large text corpora. In Proceedings of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerry Hobbs</author>
</authors>
<title>Coherence and coreference.</title>
<date>1979</date>
<journal>Cognitive Science,</journal>
<pages>3--67</pages>
<contexts>
<context position="8768" citStr="Hobbs, 1979" startWordPosition="1322" endWordPosition="1323">behave quite differently from nouns in corpora. In this section, we 195 introduce linguistically motivated indicators that are specific to verbs and may signal the semantic relation between verb pairs. Then, in Section 4 we describe how these indicators are exactly encoded as features within a supervised classification framework. Verb co-occurrence When (non-auxiliary) verbs co-occur in a sentence, they are often the main verbs of different clauses. We thus aim to use information about the relation between clauses to learn about the relation between the clauses’ main verbs. Discourse markers (Hobbs, 1979; Schiffrin, 1988) are lexical terms such as ‘because’ and ‘however’ that indicate a semantic relation between discourse fragments (i.e., propositions or speech acts). We suggest that these markers can indicate semantic relations between the main verbs of the connected clauses. For example, in the sentence “He always snores while he sleeps”, the marker ‘while’ indicates a temporal relation between the clauses, indicating that ‘snoring’ occurs while ‘sleeping’ (and so ‘snore � sleep’). Often the relation between clauses is not expressed explicitly with an overt discourse marker, but is still im</context>
</contexts>
<marker>Hobbs, 1979</marker>
<rawString>Jerry Hobbs. 1979. Coherence and coreference. Cognitive Science, 3:67–90.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eduard Hovy</author>
<author>Elisabeth Maier</author>
</authors>
<title>Organizing Discourse Structure Relations using Metafunctions.</title>
<date>1993</date>
<publisher>Pinter Publishing.</publisher>
<contexts>
<context position="14218" citStr="Hovy and Maier, 1993" startWordPosition="2182" endWordPosition="2185">rse. Conversely, when cooccurrence is loose the relation is harder to discern but coverage is increased. 4.1.1 Sentence-level co-occurrence We next detail features that address co-occurrence of the target verb pair within a sentence. These include our novel linguistically-motivated indicators, as well as features that were adapted from prior work. Discourse markers As discussed in Section 3, discourse markers may signal relations between the main verbs of adjacent clauses. The literature is abundant with taxonomies that classify markers to various discourse relations (Mann and Thompson, 1988; Hovy and Maier, 1993; Knott and Sanders, 1998). Inspired by Marcu and Echihabi (2002), we employ markers that are mapped to four discourse relations ’Contrast’, ’Cause’, ’Condition’ and ’Temporal’, as specified in Table 1. This definition can be viewed as a relaxed version of VerbOcean’s (Chklovski and Pantel, 2004) patterns, although the underlying intuition is different (see Section 3). For a target verb pair (v1, v2) and each discourse relation r, we count the number of times that v1 is the main verb in the main clause, v2 is the main verb in the subordinate clause, and the clauses are connected via a marker m</context>
</contexts>
<marker>Hovy, Maier, 1993</marker>
<rawString>Eduard Hovy and Elisabeth Maier. 1993. Organizing Discourse Structure Relations using Metafunctions. Pinter Publishing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ray Jackendoff</author>
</authors>
<title>Semantics and Cognition.</title>
<date>1983</date>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="10141" citStr="Jackendoff, 1983" startWordPosition="1542" endWordPosition="1543">t one clause is an adverbial adjunct of the other, or that two clauses are coordinated. This can indicate the existence (or lack) of entailment between verbs. For instance, in the sentence “When I walked into the room, he was working out”, the verb ‘walk’ is an adverbial adjunct of the verb ‘work out’. Such co-occurrence structure does not indicate a deep semantic relation, such as entailment, between the two verbs. Verb classes Verb classes are sets of semanticallyrelated verbs sharing some linguistic properties (Levin, 1993). One of the most general verb classes are stative vs. event verbs (Jackendoff, 1983). Stative verb, such as ‘love’ and ‘think’, usually describe a state that lasts some time. On the other hand, event verbs, such as ‘run’ and ‘kiss’, describe an action. We hypothesize that verb classes are relevant for determining entailment, for example, that stative verbs are not likely to entail event verbs. Verb generality Verb-particle constructions are multi-word expressions consisting of a head verb and a particle, e.g., switch off (Baldwin and Villavicencio, 2002). We conjecture that the more general a verb is, the more likely it is to appear with many different particles. Detecting ve</context>
</contexts>
<marker>Jackendoff, 1983</marker>
<rawString>Ray Jackendoff. 1983. Semantics and Cognition. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>A support vector method for multivariate performance measures.</title>
<date>2005</date>
<booktitle>In Proceedings of ICML.</booktitle>
<contexts>
<context position="27103" citStr="Joachims, 2005" startWordPosition="4335" endWordPosition="4336">lds (Dekang and Pantel, 2001; Szpektor et al., 2004). In total 225 verb pairs were labeled as entailing (the rule ‘v1 —* v2’ was judged as correct) and 587 verb pairs were labeled as non-entailing (the rule ‘v1 —* v2’ was judged as incorrect). The InterAnnotator Agreement (IAA) for a random sample of 100 pairs was moderate (0.47), as expected from the rule-based approach (Szpektor et al., 2007). For each verb pair, all 63 features within our model (Section 4) were computed using the ukWaC corpus (Baroni et al., 2009), which contains 2 billion words. For classification, we utilized SVM-perf’s (Joachims, 2005) linear SVM implementation with default parameters, and evaluated our model by performing 10-fold cross validation (CV) over the labeled dataset. 5http://trec.nist.gov/data/reuters/ reuters.html 6The data set is available at http://www.cs.biu.ac. il/˜nlp/downloads/verb-pair-annotation. html 5.2 Feature selection and analysis As discussed in Section 4.2, we followed the feature ranking method proposed by Guyon et al. (2003) to investigate the utility of our proposed features. Table 2 depicts the 10 most positively and negatively correlated features with entailment according to the Pearson corre</context>
</contexts>
<marker>Joachims, 2005</marker>
<rawString>T. Joachims. 2005. A support vector method for multivariate performance measures. In Proceedings of ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alistair Knott</author>
<author>Ted Sanders</author>
</authors>
<title>The classification of coherence relations and their linguistic markers: An exploration of two languages.</title>
<date>1998</date>
<journal>In Journal of Pragmatics.</journal>
<contexts>
<context position="14244" citStr="Knott and Sanders, 1998" startWordPosition="2186" endWordPosition="2189">cooccurrence is loose the relation is harder to discern but coverage is increased. 4.1.1 Sentence-level co-occurrence We next detail features that address co-occurrence of the target verb pair within a sentence. These include our novel linguistically-motivated indicators, as well as features that were adapted from prior work. Discourse markers As discussed in Section 3, discourse markers may signal relations between the main verbs of adjacent clauses. The literature is abundant with taxonomies that classify markers to various discourse relations (Mann and Thompson, 1988; Hovy and Maier, 1993; Knott and Sanders, 1998). Inspired by Marcu and Echihabi (2002), we employ markers that are mapped to four discourse relations ’Contrast’, ’Cause’, ’Condition’ and ’Temporal’, as specified in Table 1. This definition can be viewed as a relaxed version of VerbOcean’s (Chklovski and Pantel, 2004) patterns, although the underlying intuition is different (see Section 3). For a target verb pair (v1, v2) and each discourse relation r, we count the number of times that v1 is the main verb in the main clause, v2 is the main verb in the subordinate clause, and the clauses are connected via a marker mapped to r. For example, g</context>
</contexts>
<marker>Knott, Sanders, 1998</marker>
<rawString>Alistair Knott and Ted Sanders. 1998. The classification of coherence relations and their linguistic markers: An exploration of two languages. In Journal of Pragmatics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lili Kotlerman</author>
<author>Ido Dagan</author>
<author>Idan Szpektor</author>
<author>Maayan Zhitomirsky-Geffet</author>
</authors>
<title>Directional distributional similarity for lexical inference.</title>
<date>2010</date>
<journal>Natural Language Engineering,</journal>
<volume>16</volume>
<issue>4</issue>
<contexts>
<context position="5618" citStr="Kotlerman et al., 2010" startWordPosition="848" endWordPosition="851">ch for learning entailment rules between verbs and verb-like structures has employed the distributional hypothesis, which assumes that words with similar meanings appear in similar contexts. For example, we expect the words ‘buy’ and ‘purchase’ to occur with similar subjects and objects in a large corpus. This observation has led to ample work on developing both symmetric and directional similarity measures that attempt to capture semantic relations between lexical items by comparing their neighborhood context (Lin, 1998; Weeds and Weir, 2003; Geffet and Dagan, 2005; Szpektor and Dagan, 2008; Kotlerman et al., 2010). A far less explored direction for learning verb entailment involves exploiting verb co-occurrence in a sentence or a document. One prominent work is Chklovsky and Pantel’s VerbOcean (2004). In VerbOcean, the authors manually constructed 33 patterns and divided them into five pattern groups, where each group signals one of the following five semantic relations: similarity, strength, antonymy, enablement and happens-before. For example, the pattern ‘Xed and later Yed’ signals the happensbefore relation between the verbs ‘X’ and ‘Y’. Starting with candidate verb pairs based on a distributional </context>
<context position="27925" citStr="Kotlerman et al., 2010" startWordPosition="4451" endWordPosition="4454"> data set is available at http://www.cs.biu.ac. il/˜nlp/downloads/verb-pair-annotation. html 5.2 Feature selection and analysis As discussed in Section 4.2, we followed the feature ranking method proposed by Guyon et al. (2003) to investigate the utility of our proposed features. Table 2 depicts the 10 most positively and negatively correlated features with entailment according to the Pearson correlation measure From Table 2, it is clear that distributional similarity features are amongst the most positively correlated with entailment, which is in line with prior work (Geffet and Dagan, 2005; Kotlerman et al., 2010). Looking more closely, our suggestion for typed distributional similarity proved to be useful, and indeed most of the highly correlated distributional similarity features are typed measures. Standing out are the adverb-typed measures, with two features in the top 10, including the highest, ‘Weedsadverb’, and ‘BInc-adverb’. We also note that the highly correlated distributional similarity measures are directional, Weeds and BInc. The table also indicates that document-level cooccurrence contributes positively to entailment detection. This includes both the Chambers narrative measure, with the </context>
</contexts>
<marker>Kotlerman, Dagan, Szpektor, Zhitomirsky-Geffet, 2010</marker>
<rawString>Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan Zhitomirsky-Geffet. 2010. Directional distributional similarity for lexical inference. Natural Language Engineering, 16(4):359–389.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beth Levin</author>
</authors>
<title>English Verb Classes and Alternations: A Preliminary Investigation.</title>
<date>1993</date>
<publisher>University Of Chicago Press.</publisher>
<contexts>
<context position="10056" citStr="Levin, 1993" startWordPosition="1528" endWordPosition="1529"> parsing the relation can be captured by labeled dependency edges expressing that one clause is an adverbial adjunct of the other, or that two clauses are coordinated. This can indicate the existence (or lack) of entailment between verbs. For instance, in the sentence “When I walked into the room, he was working out”, the verb ‘walk’ is an adverbial adjunct of the verb ‘work out’. Such co-occurrence structure does not indicate a deep semantic relation, such as entailment, between the two verbs. Verb classes Verb classes are sets of semanticallyrelated verbs sharing some linguistic properties (Levin, 1993). One of the most general verb classes are stative vs. event verbs (Jackendoff, 1983). Stative verb, such as ‘love’ and ‘think’, usually describe a state that lasts some time. On the other hand, event verbs, such as ‘run’ and ‘kiss’, describe an action. We hypothesize that verb classes are relevant for determining entailment, for example, that stative verbs are not likely to entail event verbs. Verb generality Verb-particle constructions are multi-word expressions consisting of a head verb and a particle, e.g., switch off (Baldwin and Villavicencio, 2002). We conjecture that the more general a</context>
</contexts>
<marker>Levin, 1993</marker>
<rawString>Beth Levin. 1993. English Verb Classes and Alternations: A Preliminary Investigation. University Of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>An information-theoretic definition of similarity.</title>
<date>1998</date>
<booktitle>In Proceedings of ICML.</booktitle>
<contexts>
<context position="2414" citStr="Lin, 1998" startWordPosition="363" endWordPosition="364">ment rules. In this work we focus on one of the most important rule types, namely, lexical entailment rules between verbs (verb entailment), e.g., ‘whisper —* talk’, ‘win —* play’ and ‘buy —* own’. The significance of such rules has led to active research in automatic learning of entailment rules between verbs or verb-like structures (Zanzotto et al., 2006; Abe et al., 2008; Schoenmackers et al., 2010). Most prior efforts to learn verb entailment rules from large corpora employed distributional similarity methods, assuming that verbs are semantically similar if they occur in similar contexts (Lin, 1998; Berant et al., 2012). This led to the automatic acquisition of large scale knowledge bases, but with limited precision. Fewer works, such as VerbOcean (Chklovski and Pantel, 2004), focused on identifying verb entailment through verb instantiation of manually constructed patterns. For example, the sentence “he scared and even startled me” implies that ‘startle —* scare’. This led to more precise rule extraction, but with poor coverage since contrary to nouns, in which patterns are common (Hearst, 1992), verbs do not co-occur often within rigid patterns. However, verbs do tend to co-occur in t</context>
<context position="5521" citStr="Lin, 1998" startWordPosition="834" endWordPosition="835">erbs substantially improves verb entailment detection. 2 Background The main approach for learning entailment rules between verbs and verb-like structures has employed the distributional hypothesis, which assumes that words with similar meanings appear in similar contexts. For example, we expect the words ‘buy’ and ‘purchase’ to occur with similar subjects and objects in a large corpus. This observation has led to ample work on developing both symmetric and directional similarity measures that attempt to capture semantic relations between lexical items by comparing their neighborhood context (Lin, 1998; Weeds and Weir, 2003; Geffet and Dagan, 2005; Szpektor and Dagan, 2008; Kotlerman et al., 2010). A far less explored direction for learning verb entailment involves exploiting verb co-occurrence in a sentence or a document. One prominent work is Chklovsky and Pantel’s VerbOcean (2004). In VerbOcean, the authors manually constructed 33 patterns and divided them into five pattern groups, where each group signals one of the following five semantic relations: similarity, strength, antonymy, enablement and happens-before. For example, the pattern ‘Xed and later Yed’ signals the happensbefore rela</context>
<context position="23390" citStr="Lin, 1998" startWordPosition="3693" endWordPosition="3694">is regarding typed distributional similarity (Section 3), we first compute for each verb and each argument (subject, object, preposition complement and adverb) a separate vector that counts the number of times each word in the corpus instantiates the argument of that verb. In addition, we also compute a vector that is the concatenation of the previous separate vectors, which captures the standard distributional similarity statistics. We then 3adverbs never instantiate the subject, object or preposition complement positions. apply three state-of-the-art distributional similarity measures, Lin (Lin, 1998), Weeds precision (Weeds and Weir, 2003) and BInc (Szpektor and Dagan, 2008), to compute for every verb pair a similarity score between each of the five count vectors4. We term each feature by the method and argument, e.g., weeds-prep and lin-all represent the Weeds measure over prepositional complements and the Lin measure over all arguments. Verb classes Following our discussion in Section 3, we first measure for each target verb v a “stative” feature f by computing the proportion of times it appears in progressive tense, since stative verbs usually do not appear in the progressive tense (e.</context>
<context position="25740" citStr="Lin, 1998" startWordPosition="4099" endWordPosition="4100">n between the feature values and the corresponding labels as the ranking criterion. 4We employ the common practice of using the pmi between a verb and an argument rather than the argument count as the argument’s weight. 199 5 Evaluation and Analysis 5.1 Experimental Setting To evaluate our proposed supervised model, we constructed a dataset containing labeled verb pairs. We started by randomly sampling 50 verbs out of the common verbs in the RCV1 corpus5, which we denote here as seed verbs. Next, we extracted the 20 most similar verbs to each seed verb according to the Lin similarity measure (Lin, 1998), which was computed on the RCV1 corpus. Then, for each seed verb vs and one of its extracted similar verbs vs we generated the two directed pairs (vs, vs) and (vs, vs), which represent the candidate rules ‘vs —* vs’ and ‘vs vs’ respectively. To reduce noise, we filtered out verb pairs where one of the verbs is an auxiliary or a light verb such as ’do’, ’get’ and ’have’. This step resulted in 812 verb pairs as our dataset6, which were manually annotated by the authors as representing a valid entailment rule or not. To annotate these pairs, we generally followed the rule-based approach for enta</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. An information-theoretic definition of similarity. In Proceedings of ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Mann</author>
<author>Sandra Thompson</author>
</authors>
<title>Rhetorical structure theory: Toward a functional theory of text organization.</title>
<date>1988</date>
<tech>Text, 8(3):243–281.</tech>
<contexts>
<context position="14196" citStr="Mann and Thompson, 1988" startWordPosition="2178" endWordPosition="2181">he information may be sparse. Conversely, when cooccurrence is loose the relation is harder to discern but coverage is increased. 4.1.1 Sentence-level co-occurrence We next detail features that address co-occurrence of the target verb pair within a sentence. These include our novel linguistically-motivated indicators, as well as features that were adapted from prior work. Discourse markers As discussed in Section 3, discourse markers may signal relations between the main verbs of adjacent clauses. The literature is abundant with taxonomies that classify markers to various discourse relations (Mann and Thompson, 1988; Hovy and Maier, 1993; Knott and Sanders, 1998). Inspired by Marcu and Echihabi (2002), we employ markers that are mapped to four discourse relations ’Contrast’, ’Cause’, ’Condition’ and ’Temporal’, as specified in Table 1. This definition can be viewed as a relaxed version of VerbOcean’s (Chklovski and Pantel, 2004) patterns, although the underlying intuition is different (see Section 3). For a target verb pair (v1, v2) and each discourse relation r, we count the number of times that v1 is the main verb in the main clause, v2 is the main verb in the subordinate clause, and the clauses are co</context>
</contexts>
<marker>Mann, Thompson, 1988</marker>
<rawString>William Mann and Sandra Thompson. 1988. Rhetorical structure theory: Toward a functional theory of text organization. Text, 8(3):243–281.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
<author>Abdessamad Echihabi</author>
</authors>
<title>An unsupervised approach to recognizing discourse relations.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="14283" citStr="Marcu and Echihabi (2002)" startWordPosition="2192" endWordPosition="2195">harder to discern but coverage is increased. 4.1.1 Sentence-level co-occurrence We next detail features that address co-occurrence of the target verb pair within a sentence. These include our novel linguistically-motivated indicators, as well as features that were adapted from prior work. Discourse markers As discussed in Section 3, discourse markers may signal relations between the main verbs of adjacent clauses. The literature is abundant with taxonomies that classify markers to various discourse relations (Mann and Thompson, 1988; Hovy and Maier, 1993; Knott and Sanders, 1998). Inspired by Marcu and Echihabi (2002), we employ markers that are mapped to four discourse relations ’Contrast’, ’Cause’, ’Condition’ and ’Temporal’, as specified in Table 1. This definition can be viewed as a relaxed version of VerbOcean’s (Chklovski and Pantel, 2004) patterns, although the underlying intuition is different (see Section 3). For a target verb pair (v1, v2) and each discourse relation r, we count the number of times that v1 is the main verb in the main clause, v2 is the main verb in the subordinate clause, and the clauses are connected via a marker mapped to r. For example, given the sentence “You must enroll in t</context>
</contexts>
<marker>Marcu, Echihabi, 2002</marker>
<rawString>Daniel Marcu and Abdessamad Echihabi. 2002. An unsupervised approach to recognizing discourse relations. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shachar Mirkin</author>
<author>Ido Dagan</author>
<author>Maayan Geffet</author>
</authors>
<title>Integrating pattern-based and distributional similarity methods for lexical entailment acquisition.</title>
<date>2006</date>
<booktitle>In Proceedings of the COLING/ACL.</booktitle>
<contexts>
<context position="12723" citStr="Mirkin et al., 2006" startWordPosition="1947" endWordPosition="1950">nd will probably not appear with the adverb ‘loudly’, while the verb ‘talk’ is more likely to appear with such an adverb. Thus, measuring similarity based solely on adverb modifiers could reveal this phenomenon. 4 Supervised Entailment Detection In the previous section, we discussed linguistic observations regarding novel indicators that may help in detecting entailment relations between verbs. We next describe how to incorporate these indicators as features within a supervised framework for learning lexical entailment rules between verbs. We follow prior work on supervised lexical semantics (Mirkin et al., 2006; Hagiwara et al., 2009; Tremper, 2010) 196 and address the rule learning task as a classification task. Specifically, given an ordered verb pair (v1, v2) as input, we learn a classifier that detects whether the entailment relation ‘v1 —* v2’ holds for this pair. We next detail how our novel indicators, as well as other diverse sources of information found useful in prior work, are encoded as features. Then, we describe the learning model and our feature analysis procedure. 4.1 Entailment features Most of our features are based on information extracted from the target verb pair co-occurring wi</context>
<context position="32090" citStr="Mirkin et al. (2006)" startWordPosition="5086" endWordPosition="5089">ilment Method P% R% AUC F1 All 40.2 71.0 0.65 0.51 TDS+VO 36.8 53.2 0.58 0.41 TDS 34.6 44.8 0.56 0.37 Random 27.9 28.8 0.51 0.28 VO-KB 33.1 14.8 0.53 0.2 VO-ukWaC 23.3 4.7 0.29 0.08 Table 3: Average precision, recall, AUC and Fl for our method and the baselines. classifier over several distributional similarity features, and provides an evaluation of the discriminative power of distributional similarity alone, without co-occurrence features. TDS+VO: Include only the 15 typed distributional similarity features and the two VerbOcean features in our supervised model. This baseline is inspired by Mirkin et al. (2006), who combined distributional similarity features and Hearst patterns (Hearst, 1992) for learning entailment between nouns. All: Our full-blown model, including all features described in Section 4.1. For all tested methods, we performed 10-fold cross validation and averaged Precision, Recall, Area under the ROC curve (AUC) and F1 over the 10 folds. Table 3 presents the results of our full-blown model as well as the baselines. First, we note that, as expected, the VerbOcean baselines VO-KB and VO-ukWaC provide low recall, 201 Method P% R% AUC F1 All 40.2 71.0 0.65 0.51 Sent+Corpus-level 39.7 70</context>
</contexts>
<marker>Mirkin, Dagan, Geffet, 2006</marker>
<rawString>Shachar Mirkin, Ido Dagan, and Maayan Geffet. 2006. Integrating pattern-based and distributional similarity methods for lexical entailment acquisition. In Proceedings of the COLING/ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Jens Nilsson</author>
</authors>
<title>Maltparser: A data-driven parser-generator for dependency parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of LREC.</booktitle>
<contexts>
<context position="15852" citStr="Nivre et al., 2006" startWordPosition="2463" endWordPosition="2466">se and v2 in the main clause. We term the features by the relevant discourse relation, e.g., ‘v1-contrast-v2’ refers to v1 being in the main clause and connected to the subordinate clause via a contrast marker. Dependency relations between clauses As noted in Section 3, the syntactic structure of verb cooccurrence can indicate the existence or lack of entailment. In dependency parsing this may be expressed via the label of the dependency relation connecting the main and subordinate clauses. In our experiments we used the ukWaC corpus1 (Baroni et al., 2009) which was parsed by the MALT parser (Nivre et al., 2006). Hence, we identified three MALT dependency relations that connect a main clause with its subordinate clause. The first relation is the object complement relation ‘obj’. In this case the subordinate clause is an object complement of the main clause. For example, in “it surprised me that the lizard could talk” the verb pair (‘surprise’,‘talk’) is connected by the ‘obj’ relation. The second relation is the adverbial adjunct relation ‘adv’, in which the subordinate clause is adverbial and describes the time, place, manner, etc. of the main clause, e.g., “he gave his consent without thinking abou</context>
</contexts>
<marker>Nivre, Hall, Nilsson, 2006</marker>
<rawString>Joakim Nivre, Johan Hall, and Jens Nilsson. 2006. Maltparser: A data-driven parser-generator for dependency parsing. In Proceedings of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Viktor Pekar</author>
</authors>
<title>Discovery of event entailment knowledge from text corpora.</title>
<date>2008</date>
<journal>Comput. Speech Lang.,</journal>
<volume>22</volume>
<issue>1</issue>
<contexts>
<context position="6795" citStr="Pekar (2008)" startWordPosition="1032" endWordPosition="1033">rb pairs based on a distributional similarity measure, the patterns are used to choose a semantic relation per verb pair based on the different patterns this pair instantiates. This method is more precise than distributional similarity approaches, but it is highly susceptible to sparseness issues, since verbs do not typically co-occur within rigid patterns. Utilizing verb co-occurrence at the document level, Chambers and Jurafsky (2008) estimate whether a pair of verbs is narratively related by counting the number of times the verbs share an argument in the same document. In a similar manner, Pekar (2008) detects entailment rules between templates from shared arguments within discourserelated clauses in the same document. Recently, supervised classification has become standard in performing various semantic tasks. N irkin et al. (2006) introduced a system for learning entailment rules between nouns (e.g., ‘novel —* book’) that combines distributional similarity and Hearst patterns as features in a supervised classifier. Pennacchiotti and Pantel (2009) augment N irkin et al’s features with web-based features for the task of entity extraction. Hagiwara et al. (2009) perform synonym identificatio</context>
</contexts>
<marker>Pekar, 2008</marker>
<rawString>Viktor Pekar. 2008. Discovery of event entailment knowledge from text corpora. Comput. Speech Lang., 22(1):1–16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Pennacchiotti</author>
<author>Patrick Pantel</author>
</authors>
<title>Entity extraction via ensemble semantics.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="7250" citStr="Pennacchiotti and Pantel (2009)" startWordPosition="1094" endWordPosition="1097">008) estimate whether a pair of verbs is narratively related by counting the number of times the verbs share an argument in the same document. In a similar manner, Pekar (2008) detects entailment rules between templates from shared arguments within discourserelated clauses in the same document. Recently, supervised classification has become standard in performing various semantic tasks. N irkin et al. (2006) introduced a system for learning entailment rules between nouns (e.g., ‘novel —* book’) that combines distributional similarity and Hearst patterns as features in a supervised classifier. Pennacchiotti and Pantel (2009) augment N irkin et al’s features with web-based features for the task of entity extraction. Hagiwara et al. (2009) perform synonym identification based on both distributional and contextual features. Tremper (2010) extract “loose” sentence-level features in order to identify the presupposition relation (e.g., , the verb ‘win’ presupposes the verb ‘play’). Last, Berant et al. (2012) utilized various distributional similarity features to identify entailment between lexical-syntactic predicates. In this paper, we follow the supervised approach for semantic relation detection in order to identify</context>
</contexts>
<marker>Pennacchiotti, Pantel, 2009</marker>
<rawString>Marco Pennacchiotti and Patrick Pantel. 2009. Entity extraction via ensemble semantics. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deepak Ravichandran</author>
<author>Eduard Hovy</author>
</authors>
<title>Learning surface text patterns for a question answering system.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="1387" citStr="Ravichandran and Hovy, 2002" startWordPosition="195" endWordPosition="198">o-occurrence within a sentence, verb cooccurrence within a document, as well as overall corpus statistics. To this end, we propose a much richer novel set of linguistically motivated cues for detecting entailment between verbs and combine them as features in a supervised classification framework. We empirically demonstrate that our model significantly outperforms previous methods and that information from each textual scope contributes to the verb entailment learning task. 1 Introduction Inference rules are an important building block of many semantic applications, such as Question Answering (Ravichandran and Hovy, 2002) and Information Extraction (Shinyama and Sekine, 2006). For example, given the sentence “Churros are coated with sugar”, one can use the rule ‘coat —* cover’ to answer the question “What are Churros covered with?”. Inference rules specify a directional inference relation between two text fragments, and we follow the Textual Entailment modeling of inference (Dagan et al., 2006), which refers to such rules as entailment rules. In this work we focus on one of the most important rule types, namely, lexical entailment rules between verbs (verb entailment), e.g., ‘whisper —* talk’, ‘win —* play’ an</context>
</contexts>
<marker>Ravichandran, Hovy, 2002</marker>
<rawString>Deepak Ravichandran and Eduard Hovy. 2002. Learning surface text patterns for a question answering system. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deborah Schiffrin</author>
</authors>
<title>Discourse Markers.</title>
<date>1988</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="8786" citStr="Schiffrin, 1988" startWordPosition="1324" endWordPosition="1325">differently from nouns in corpora. In this section, we 195 introduce linguistically motivated indicators that are specific to verbs and may signal the semantic relation between verb pairs. Then, in Section 4 we describe how these indicators are exactly encoded as features within a supervised classification framework. Verb co-occurrence When (non-auxiliary) verbs co-occur in a sentence, they are often the main verbs of different clauses. We thus aim to use information about the relation between clauses to learn about the relation between the clauses’ main verbs. Discourse markers (Hobbs, 1979; Schiffrin, 1988) are lexical terms such as ‘because’ and ‘however’ that indicate a semantic relation between discourse fragments (i.e., propositions or speech acts). We suggest that these markers can indicate semantic relations between the main verbs of the connected clauses. For example, in the sentence “He always snores while he sleeps”, the marker ‘while’ indicates a temporal relation between the clauses, indicating that ‘snoring’ occurs while ‘sleeping’ (and so ‘snore � sleep’). Often the relation between clauses is not expressed explicitly with an overt discourse marker, but is still implied by the synta</context>
</contexts>
<marker>Schiffrin, 1988</marker>
<rawString>Deborah Schiffrin. 1988. Discourse Markers. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Schoenmackers</author>
<author>Jesse Davis</author>
<author>Oren Etzioni</author>
<author>Daniel S Weld</author>
</authors>
<title>Learning first-order horn clauses from web text.</title>
<date>2010</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="2210" citStr="Schoenmackers et al., 2010" startWordPosition="331" endWordPosition="334">ros covered with?”. Inference rules specify a directional inference relation between two text fragments, and we follow the Textual Entailment modeling of inference (Dagan et al., 2006), which refers to such rules as entailment rules. In this work we focus on one of the most important rule types, namely, lexical entailment rules between verbs (verb entailment), e.g., ‘whisper —* talk’, ‘win —* play’ and ‘buy —* own’. The significance of such rules has led to active research in automatic learning of entailment rules between verbs or verb-like structures (Zanzotto et al., 2006; Abe et al., 2008; Schoenmackers et al., 2010). Most prior efforts to learn verb entailment rules from large corpora employed distributional similarity methods, assuming that verbs are semantically similar if they occur in similar contexts (Lin, 1998; Berant et al., 2012). This led to the automatic acquisition of large scale knowledge bases, but with limited precision. Fewer works, such as VerbOcean (Chklovski and Pantel, 2004), focused on identifying verb entailment through verb instantiation of manually constructed patterns. For example, the sentence “he scared and even startled me” implies that ‘startle —* scare’. This led to more prec</context>
</contexts>
<marker>Schoenmackers, Davis, Etzioni, Weld, 2010</marker>
<rawString>Stefan Schoenmackers, Jesse Davis, Oren Etzioni, and Daniel S. Weld. 2010. Learning first-order horn clauses from web text. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yusuke Shinyama</author>
<author>Satoshi Sekine</author>
</authors>
<title>Preemptive information extraction using unrestricted relation discovery.</title>
<date>2006</date>
<booktitle>In Proceedings of NAACL-HLT.</booktitle>
<contexts>
<context position="1442" citStr="Shinyama and Sekine, 2006" startWordPosition="203" endWordPosition="206">a document, as well as overall corpus statistics. To this end, we propose a much richer novel set of linguistically motivated cues for detecting entailment between verbs and combine them as features in a supervised classification framework. We empirically demonstrate that our model significantly outperforms previous methods and that information from each textual scope contributes to the verb entailment learning task. 1 Introduction Inference rules are an important building block of many semantic applications, such as Question Answering (Ravichandran and Hovy, 2002) and Information Extraction (Shinyama and Sekine, 2006). For example, given the sentence “Churros are coated with sugar”, one can use the rule ‘coat —* cover’ to answer the question “What are Churros covered with?”. Inference rules specify a directional inference relation between two text fragments, and we follow the Textual Entailment modeling of inference (Dagan et al., 2006), which refers to such rules as entailment rules. In this work we focus on one of the most important rule types, namely, lexical entailment rules between verbs (verb entailment), e.g., ‘whisper —* talk’, ‘win —* play’ and ‘buy —* own’. The significance of such rules has led </context>
</contexts>
<marker>Shinyama, Sekine, 2006</marker>
<rawString>Yusuke Shinyama and Satoshi Sekine. 2006. Preemptive information extraction using unrestricted relation discovery. In Proceedings of NAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Idan Szpektor</author>
<author>Ido Dagan</author>
</authors>
<title>Learning entailment rules for unary templates.</title>
<date>2008</date>
<booktitle>In Proceedings of COLING.</booktitle>
<contexts>
<context position="5593" citStr="Szpektor and Dagan, 2008" startWordPosition="844" endWordPosition="847">Background The main approach for learning entailment rules between verbs and verb-like structures has employed the distributional hypothesis, which assumes that words with similar meanings appear in similar contexts. For example, we expect the words ‘buy’ and ‘purchase’ to occur with similar subjects and objects in a large corpus. This observation has led to ample work on developing both symmetric and directional similarity measures that attempt to capture semantic relations between lexical items by comparing their neighborhood context (Lin, 1998; Weeds and Weir, 2003; Geffet and Dagan, 2005; Szpektor and Dagan, 2008; Kotlerman et al., 2010). A far less explored direction for learning verb entailment involves exploiting verb co-occurrence in a sentence or a document. One prominent work is Chklovsky and Pantel’s VerbOcean (2004). In VerbOcean, the authors manually constructed 33 patterns and divided them into five pattern groups, where each group signals one of the following five semantic relations: similarity, strength, antonymy, enablement and happens-before. For example, the pattern ‘Xed and later Yed’ signals the happensbefore relation between the verbs ‘X’ and ‘Y’. Starting with candidate verb pairs b</context>
<context position="23466" citStr="Szpektor and Dagan, 2008" startWordPosition="3703" endWordPosition="3706">first compute for each verb and each argument (subject, object, preposition complement and adverb) a separate vector that counts the number of times each word in the corpus instantiates the argument of that verb. In addition, we also compute a vector that is the concatenation of the previous separate vectors, which captures the standard distributional similarity statistics. We then 3adverbs never instantiate the subject, object or preposition complement positions. apply three state-of-the-art distributional similarity measures, Lin (Lin, 1998), Weeds precision (Weeds and Weir, 2003) and BInc (Szpektor and Dagan, 2008), to compute for every verb pair a similarity score between each of the five count vectors4. We term each feature by the method and argument, e.g., weeds-prep and lin-all represent the Weeds measure over prepositional complements and the Lin measure over all arguments. Verb classes Following our discussion in Section 3, we first measure for each target verb v a “stative” feature f by computing the proportion of times it appears in progressive tense, since stative verbs usually do not appear in the progressive tense (e.g., ‘knowing’). Then, given a verb pair (v1,v2) and their corresponding stat</context>
</contexts>
<marker>Szpektor, Dagan, 2008</marker>
<rawString>Idan Szpektor and Ido Dagan. 2008. Learning entailment rules for unary templates. In Proceedings of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Idan Szpektor</author>
<author>Hristo Tanev</author>
<author>Ido Dagan</author>
</authors>
<title>Scaling web-based acquisition of entailment relations.</title>
<date>2004</date>
<booktitle>In In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="26540" citStr="Szpektor et al., 2004" startWordPosition="4239" endWordPosition="4242">present the candidate rules ‘vs —* vs’ and ‘vs vs’ respectively. To reduce noise, we filtered out verb pairs where one of the verbs is an auxiliary or a light verb such as ’do’, ’get’ and ’have’. This step resulted in 812 verb pairs as our dataset6, which were manually annotated by the authors as representing a valid entailment rule or not. To annotate these pairs, we generally followed the rule-based approach for entailment rule annotation, where a rule ‘v1 —* v2’ is considered as correct if the annotator could think of reasonable contexts under which the rule holds (Dekang and Pantel, 2001; Szpektor et al., 2004). In total 225 verb pairs were labeled as entailing (the rule ‘v1 —* v2’ was judged as correct) and 587 verb pairs were labeled as non-entailing (the rule ‘v1 —* v2’ was judged as incorrect). The InterAnnotator Agreement (IAA) for a random sample of 100 pairs was moderate (0.47), as expected from the rule-based approach (Szpektor et al., 2007). For each verb pair, all 63 features within our model (Section 4) were computed using the ukWaC corpus (Baroni et al., 2009), which contains 2 billion words. For classification, we utilized SVM-perf’s (Joachims, 2005) linear SVM implementation with defau</context>
</contexts>
<marker>Szpektor, Tanev, Dagan, 2004</marker>
<rawString>Idan Szpektor, Hristo Tanev, and Ido Dagan. 2004. Scaling web-based acquisition of entailment relations. In In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Idan Szpektor</author>
<author>Eyal Shnarch</author>
<author>Ido Dagan</author>
</authors>
<title>Instance based evaluation of entailment rule acquisition.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics.</booktitle>
<contexts>
<context position="26885" citStr="Szpektor et al., 2007" startWordPosition="4299" endWordPosition="4302"> annotate these pairs, we generally followed the rule-based approach for entailment rule annotation, where a rule ‘v1 —* v2’ is considered as correct if the annotator could think of reasonable contexts under which the rule holds (Dekang and Pantel, 2001; Szpektor et al., 2004). In total 225 verb pairs were labeled as entailing (the rule ‘v1 —* v2’ was judged as correct) and 587 verb pairs were labeled as non-entailing (the rule ‘v1 —* v2’ was judged as incorrect). The InterAnnotator Agreement (IAA) for a random sample of 100 pairs was moderate (0.47), as expected from the rule-based approach (Szpektor et al., 2007). For each verb pair, all 63 features within our model (Section 4) were computed using the ukWaC corpus (Baroni et al., 2009), which contains 2 billion words. For classification, we utilized SVM-perf’s (Joachims, 2005) linear SVM implementation with default parameters, and evaluated our model by performing 10-fold cross validation (CV) over the labeled dataset. 5http://trec.nist.gov/data/reuters/ reuters.html 6The data set is available at http://www.cs.biu.ac. il/˜nlp/downloads/verb-pair-annotation. html 5.2 Feature selection and analysis As discussed in Section 4.2, we followed the feature ra</context>
</contexts>
<marker>Szpektor, Shnarch, Dagan, 2007</marker>
<rawString>Idan Szpektor, Eyal Shnarch, and Ido Dagan. 2007. Instance based evaluation of entailment rule acquisition. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Galina Tremper</author>
</authors>
<title>Weakly supervised learning of presupposition relations between verbs.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL student workshop.</booktitle>
<contexts>
<context position="7465" citStr="Tremper (2010)" startWordPosition="1128" endWordPosition="1129">arguments within discourserelated clauses in the same document. Recently, supervised classification has become standard in performing various semantic tasks. N irkin et al. (2006) introduced a system for learning entailment rules between nouns (e.g., ‘novel —* book’) that combines distributional similarity and Hearst patterns as features in a supervised classifier. Pennacchiotti and Pantel (2009) augment N irkin et al’s features with web-based features for the task of entity extraction. Hagiwara et al. (2009) perform synonym identification based on both distributional and contextual features. Tremper (2010) extract “loose” sentence-level features in order to identify the presupposition relation (e.g., , the verb ‘win’ presupposes the verb ‘play’). Last, Berant et al. (2012) utilized various distributional similarity features to identify entailment between lexical-syntactic predicates. In this paper, we follow the supervised approach for semantic relation detection in order to identify verb entailment. While we utilize and adapt useful features from prior work, we introduce a diverse set of novel features for the task, effectively combining verb co-occurrence information at the sentence, document</context>
<context position="12762" citStr="Tremper, 2010" startWordPosition="1955" endWordPosition="1956">‘loudly’, while the verb ‘talk’ is more likely to appear with such an adverb. Thus, measuring similarity based solely on adverb modifiers could reveal this phenomenon. 4 Supervised Entailment Detection In the previous section, we discussed linguistic observations regarding novel indicators that may help in detecting entailment relations between verbs. We next describe how to incorporate these indicators as features within a supervised framework for learning lexical entailment rules between verbs. We follow prior work on supervised lexical semantics (Mirkin et al., 2006; Hagiwara et al., 2009; Tremper, 2010) 196 and address the rule learning task as a classification task. Specifically, given an ordered verb pair (v1, v2) as input, we learn a classifier that detects whether the entailment relation ‘v1 —* v2’ holds for this pair. We next detail how our novel indicators, as well as other diverse sources of information found useful in prior work, are encoded as features. Then, we describe the learning model and our feature analysis procedure. 4.1 Entailment features Most of our features are based on information extracted from the target verb pair co-occurring within varying textual scopes (sentence, </context>
<context position="19650" citStr="Tremper (2010)" startWordPosition="3073" endWordPosition="3074">nce, we extract the verbs’ tenses and order them as follows: past &lt; present &lt; future. We then add the features ‘tense-v1&lt;tense-v2’, ‘tense-v1=tense-v2’, and ‘tense-v1&gt;tense-v2’, corresponding to the propor2http://aclweb.org/aclwiki/index.php? title=RTE_Knowledge_Resources#Ablation_ Tests tion of times the tense of v1 is smaller, equal to, or bigger than the tense of v2. This indicates the prevalent temporal relation between the verbs in the corpus and may assist in detecting the direction of entailment. e.g., if tense-v1&gt;tense-v2, the verb pair is less likely to entail. Co-reference Following Tremper (2010), in every co-occurrence of (v1,v2) we extract for each verb the set of arguments at either the subject or object positions, denoted A1 and A2 (for v1 and v2, respectively). We then compute the proportion of cooccurrences in which v1 and v2 share an argument, i.e., A1 n A2 =� 0, out of all the co-occurrences in which both A1 and A2 are non-empty. The intuition, which is similar to distributional similarity, is that semantically related verbs tend to share arguments. Syntactic and lexical distance Following Tremper (2010) again, we compute the average distance d in dependency edges between the </context>
</contexts>
<marker>Tremper, 2010</marker>
<rawString>Galina Tremper. 2010. Weakly supervised learning of presupposition relations between verbs. In Proceedings of ACL student workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julie Weeds</author>
<author>David Weir</author>
</authors>
<title>A general framework for distributional similarity.</title>
<date>2003</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="5543" citStr="Weeds and Weir, 2003" startWordPosition="836" endWordPosition="839">ntially improves verb entailment detection. 2 Background The main approach for learning entailment rules between verbs and verb-like structures has employed the distributional hypothesis, which assumes that words with similar meanings appear in similar contexts. For example, we expect the words ‘buy’ and ‘purchase’ to occur with similar subjects and objects in a large corpus. This observation has led to ample work on developing both symmetric and directional similarity measures that attempt to capture semantic relations between lexical items by comparing their neighborhood context (Lin, 1998; Weeds and Weir, 2003; Geffet and Dagan, 2005; Szpektor and Dagan, 2008; Kotlerman et al., 2010). A far less explored direction for learning verb entailment involves exploiting verb co-occurrence in a sentence or a document. One prominent work is Chklovsky and Pantel’s VerbOcean (2004). In VerbOcean, the authors manually constructed 33 patterns and divided them into five pattern groups, where each group signals one of the following five semantic relations: similarity, strength, antonymy, enablement and happens-before. For example, the pattern ‘Xed and later Yed’ signals the happensbefore relation between the verbs</context>
<context position="23430" citStr="Weeds and Weir, 2003" startWordPosition="3697" endWordPosition="3700">onal similarity (Section 3), we first compute for each verb and each argument (subject, object, preposition complement and adverb) a separate vector that counts the number of times each word in the corpus instantiates the argument of that verb. In addition, we also compute a vector that is the concatenation of the previous separate vectors, which captures the standard distributional similarity statistics. We then 3adverbs never instantiate the subject, object or preposition complement positions. apply three state-of-the-art distributional similarity measures, Lin (Lin, 1998), Weeds precision (Weeds and Weir, 2003) and BInc (Szpektor and Dagan, 2008), to compute for every verb pair a similarity score between each of the five count vectors4. We term each feature by the method and argument, e.g., weeds-prep and lin-all represent the Weeds measure over prepositional complements and the Lin measure over all arguments. Verb classes Following our discussion in Section 3, we first measure for each target verb v a “stative” feature f by computing the proportion of times it appears in progressive tense, since stative verbs usually do not appear in the progressive tense (e.g., ‘knowing’). Then, given a verb pair </context>
</contexts>
<marker>Weeds, Weir, 2003</marker>
<rawString>Julie Weeds and David Weir. 2003. A general framework for distributional similarity. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabio Massimo Zanzotto</author>
<author>Marco Pennacchiotti</author>
<author>Maria Teresa Pazienza</author>
</authors>
<title>Discovering asymmetric entailment relations between verbs using selectional preferences.</title>
<date>2006</date>
<booktitle>In Proceedings of the COLING/ACL.</booktitle>
<contexts>
<context position="2163" citStr="Zanzotto et al., 2006" startWordPosition="323" endWordPosition="326">er’ to answer the question “What are Churros covered with?”. Inference rules specify a directional inference relation between two text fragments, and we follow the Textual Entailment modeling of inference (Dagan et al., 2006), which refers to such rules as entailment rules. In this work we focus on one of the most important rule types, namely, lexical entailment rules between verbs (verb entailment), e.g., ‘whisper —* talk’, ‘win —* play’ and ‘buy —* own’. The significance of such rules has led to active research in automatic learning of entailment rules between verbs or verb-like structures (Zanzotto et al., 2006; Abe et al., 2008; Schoenmackers et al., 2010). Most prior efforts to learn verb entailment rules from large corpora employed distributional similarity methods, assuming that verbs are semantically similar if they occur in similar contexts (Lin, 1998; Berant et al., 2012). This led to the automatic acquisition of large scale knowledge bases, but with limited precision. Fewer works, such as VerbOcean (Chklovski and Pantel, 2004), focused on identifying verb entailment through verb instantiation of manually constructed patterns. For example, the sentence “he scared and even startled me” implies</context>
</contexts>
<marker>Zanzotto, Pennacchiotti, Pazienza, 2006</marker>
<rawString>Fabio Massimo Zanzotto, Marco Pennacchiotti, and Maria Teresa Pazienza. 2006. Discovering asymmetric entailment relations between verbs using selectional preferences. In Proceedings of the COLING/ACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000028">
<title confidence="0.999608">
Aligning Predicates across Monolingual Comparable Texts
using Graph-based Clustering
</title>
<author confidence="0.981603">
Michael Roth and Anette Frank
</author>
<affiliation confidence="0.972808">
Department of Computational Linguistics
Heidelberg University
</affiliation>
<address confidence="0.542809">
Germany
</address>
<email confidence="0.99122">
{mroth,frank}@cl.uni-heidelberg.de
</email>
<sectionHeader confidence="0.995443" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.8964444">
Generating coherent discourse is an important
aspect in natural language generation. Our
aim is to learn factors that constitute coherent
discourse from data, with a focus on how to re-
alize predicate-argument structures in a model
that exceeds the sentence level. We present
an important subtask for this overall goal, in
which we align predicates across compara-
ble texts, admitting partial argument struc-
ture correspondence. The contribution of this
work is two-fold: We first construct a large
corpus resource of comparable texts, includ-
ing an evaluation set with manual predicate
alignments. Secondly, we present a novel ap-
proach for aligning predicates across compa-
rable texts using graph-based clustering with
Mincuts. Our method significantly outper-
forms other alignment techniques when ap-
plied to this novel alignment task, by a margin
of at least 6.5 percentage points in Fl-score.
</bodyText>
<sectionHeader confidence="0.998884" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999557152173913">
Discourse coherence is an important aspect in natu-
ral language generation (NLG) applications. A num-
ber of theories have investigated coherence inducing
factors. A prominent example is Centering Theory
(Grosz et al., 1995), which models local coherence
by relating the choice of referring expressions to the
importance of an entity at a certain stage of a dis-
course. A data-driven model based on this theory
is the entity-based approach by Barzilay and Lap-
ata (2008), which models coherence phenomena by
observing sentence-to-sentence transitions of entity
occurrences.
Barzilay and Lapata show that their approach can
discriminate between a coherent and a non-coherent
set of ordered sentences. However, their model is
not able to generate alternative entity realizations by
itself. Furthermore, the entity-based approach only
investigates realization patterns for individual enti-
ties in discourse in terms of core grammatical func-
tions. It does not investigate the interplay between
entity transitions and realization patterns for full-
fledged semantic structures. This interplay, how-
ever, is an important factor for a semantics-based,
generative model of discourse coherence.
The main hypothesis of our work is that we can
automatically learn context-specific realization pat-
terns for predicate argument structures (PAS) from a
semantically parsed corpus of comparable text pairs.
Our assumption builds on the success of previous
research, where comparable and parallel texts have
been exploited for a range of related learning tasks,
e.g., unsupervised discourse segmentation (Barzilay
and Lee, 2004) and bootstrapping semantic analyz-
ers (Titov and Kozhevnikov, 2010).
For our purposes, we are interested in finding cor-
responding PAS across comparable texts that are
known to talk about the same events, and hence in-
volve the same set of underlying event participants.
By aligning predicates in such texts, we can inves-
tigate the factors that determine discourse coher-
ence in the realization patterns for the involved argu-
ments. These include the specific forms of argument
realization, as a pronoun or a specific type of refer-
ential expression, as studied in prior work in NLG
(Belz et al., 2009, inter alia). The specific set-up
we examine, however, allows us to further investi-
</bodyText>
<page confidence="0.975611">
171
</page>
<note confidence="0.7302785">
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 171–182, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.999338083333333">
gate the factors that govern the non-realization of
an argument position, as a special form of coher-
ence inducing element in discourse. Example (1),
extracted from our corpus of aligned texts,illustrates
this point: Both texts report on the same event of
locating victims in an avalanche. While (1.a) explic-
itly talks about the location of this event, the role re-
mains implicit in the second sentence of (1.b), given
that it can be recovered from the preceding sentence.
In fact, realization of this argument role would im-
pede the fluency of discourse by being overly repet-
itive.
</bodyText>
<figureCaption confidence="0.7003362">
(1) a. ...The official said that [no bodies]Arg1 had
been recovered [from the avalanches]Arg2 which
occurred late Friday in the Central Asian coun-
try near the Afghan border some 300 kilometers
(185 miles) southeast of the capital Dushanbe.
</figureCaption>
<bodyText confidence="0.956747653846154">
b. Three other victims were trapped in an
avalanche in the village of Khichikh. [None
of the victims bodies]Arg1 have been found
[ ]Argm-loc.
This phenomenon clearly relates to the problem
of discourse-linking of implicit roles, a very chal-
lenging task in discourse processing.1 In our work,
we consider this problem from a content-based gen-
eration perspective, concentrating on the discourse
factors that allow for the omission of a role.
Thus, our aim is to identify comparable predica-
tions across aligned texts, and to study the discourse
coherence factors that determine the realization pat-
terns of arguments in the respective discourses. This
can be achieved by considering the full set of argu-
ments that can be recovered from the aligned pred-
ications. This paper focuses on the first of these
tasks, henceforth called predicate alignment.2
In line with data-driven approaches in NLP, we
automatically align predicates in a suitable corpus of
paired texts. The induced alignments will (i) serve to
identify events described in both comparable texts,
and (ii) provide information about the underlying ar-
gument structures and how they are realized in each
context to establish a coherent discourse. We in-
vestigate a graph-based clustering method for induc-
</bodyText>
<footnote confidence="0.9355378">
1See the recent SemEval 2010 task: Linking Events and
their Participants in Discourse, (Ruppenhofer et al., 2010).
2Note that we provide details regarding the construction of
a suitable data set and further examples involving non-realized
arguments in a complementary paper (Roth and Frank, 2012).
</footnote>
<bodyText confidence="0.9993">
ing such alignments as clustering provides a suitable
framework to implicitly relate alignment decisions
to one another, by exploiting global information en-
coded in a graph.
The remainder of this paper is structured as fol-
lows: In Section 2, we discuss previous work in re-
lated tasks. Section 3 describes our task and a suit-
able data set. Section 4 introduces a graph-based
clustering model using Mincuts for the alignment of
predicates. Section 5 outlines the experiments and
presents evaluation results. Finally, we conclude in
Section 6 and discuss future work.
</bodyText>
<sectionHeader confidence="0.999782" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999927787878788">
The task of aligning words in general has been stud-
ied extensively in previous work, for example as part
of research in statistical machine translation (SMT).
Typically, alignment models in SMT are trained by
observing and (re-)estimating co-occurrence counts
of word pairs in parallel sentences (Brown et al.,
1993). The same methods have also been applied
in monolingual settings, for example to align words
in paraphrases (Cohn et al., 2008). In contrast to
traditional word alignment tasks, our focus is not on
pairs of isolated sentences but on aligning predicates
within the discourse contexts in which they are sit-
uated. Furthermore, text pairs for our task should
not be strictly parallel as we are specifically inter-
ested in the impact of different discourse contexts.
In Section 5, we will show that this particular set-
ting indeed constitutes a more challenging task com-
pared to traditional word alignment in parallel or
paraphrasing sentences.
Another set of related tasks is found in the area of
textual inference. Since 2006, there have been reg-
ular challenges on the task of Recognizing Textual
Entailment (RTE). In the original task description,
Dagan et al. (2006) define textual entailment “as a
directional relationship between pairs of text expres-
sions, denoted by T - the entailing ‘Text’ -, and H
- the entailed ‘Hypothesis’. (... ) T entails H if the
meaning of H can be inferred from the meaning of
T, as would typically be interpreted by people.” Al-
though this relation does not necessarily require the
presence of corresponding predicates, previous work
by MacCartney et al. (2008) shows that word align-
ments can serve as a good indicator of entailment.
</bodyText>
<page confidence="0.996681">
172
</page>
<bodyText confidence="0.999838894736842">
As a matter of fact, the same holds true for the task
of detecting paraphrases. In contrast to RTE, this lat-
ter task requires bi-directional entailments, i.e., each
of the two phrases must entail the other. Wan et al.
(2006) show that a simple approach solely based on
word (and lemmatized n-gram) overlap can already
achieve an Fl-score of up to 83% for detecting para-
phrases in the Microsoft Research Paraphrase Cor-
pus (Dolan and Brockett, 2005, MSRPC). In fact,
this is just 0.6% points below the state-of-the-art re-
sults recently reported by Socher et al. (2011).
The MSRPC and data sets from the first RTE
challenges only consisted of isolated pairs of sen-
tences. The Fifth PASCAL Recognizing Textual En-
tailment Challenge (Bentivogli et al., 2009) intro-
duced a “Search Task”, where entailing sentences
for a hypothesis have to be found in a set of full
documents. This new task first opened the doors for
assessing the role of discourse (Mirkin et al., 2010a;
Mirkin et al., 2010b) in RTE. However, this setting is
still limited as discourse contexts are only provided
for the entailing part (T) of each text pair but not for
the hypothesis H.
A further task related to ours is the detection
of event coreference. The goal of this task is to
identify all mentions of the same event within a
document and, in some settings, also across docu-
ments. However, the task setting is typically more
restricted than ours in that its focus lies on iden-
tical events/references (cf. Walker et al. (2006),
Weischedel et al. (2011), inter alia). In particular,
verbalizations of different aspects of an event (e.g.,
‘buy’–‘sell’, ‘kill’–‘die’, ‘recover’–‘find’) are gen-
erally not linked in this paradigm. In contrast to co-
reference methods that identify chains of events, we
are interested in pairs of corresponding predicates
(and their argument structure), for which we can ob-
serve alternative realizations in discourse.
</bodyText>
<sectionHeader confidence="0.976238" genericHeader="method">
3 Aligning Predicates Across Texts
</sectionHeader>
<bodyText confidence="0.999932230769231">
This section summarizes how we built a large cor-
pus of comparable texts, as a basis for the predicate
alignment task. We motivate the choice of the cor-
pus and present a strategy for extracting comparable
text pairs. Subsequently, we report on the prepara-
tion of an evaluation data set with manual predicate
alignments across the paired texts. We conclude this
section with an example that showcases the poten-
tial of using aligned predicates for the study of co-
herence phenomena. More detailed information re-
garding corpus creation, annotation guidelines and
additional examples illustrating the potential of this
corpus can be found in Roth and Frank (2012).
</bodyText>
<subsectionHeader confidence="0.99934">
3.1 Corpus Creation
</subsectionHeader>
<bodyText confidence="0.99919953125">
The goal of our work is to investigate coherence fac-
tors for argument structure realization, using com-
parable texts that describe the same events, but that
include variation in textual presentation. This re-
quirement fits well with the news domain, for which
we can trace varying textual sources that describe
the same underlying events. The English Gigaword
Fifth Edition (Parker et al., 2011) corpus (henceforth
just Gigaword) is one of the largest corpus collec-
tions for English. It comprises a total of 9.8 million
newswire articles from seven distinct sources.
In previous work (Roth and Frank, 2012), we in-
troduced GigaPairs, a sub-corpus extracted from Gi-
gaword that includes over 160,000 pairs of newswire
articles from distinct sources. GigaPairs has been
derived from Gigaword using the pairwise similar-
ity method on headlines presented by Wubben et al.
(2009). In addition to calculating the similarity of
news titles, we impose an additional date constraint
to further increase the precision of extracted pairs of
texts. Random inspection of about 100 documents
revealed only two texts describing different events.
Overall, we extracted 167,728 document pairs con-
taining a total of 50 million word tokens. Each doc-
ument in this corpus consists of up to 7.564 words
with a mean and median of 301 and 213 words, re-
spectively. All texts have been pre-processed us-
ing MATE tools (Bj¨orkelund et al., 2010; Bohnet,
2010), a pipeline of NLP modules including a state-
of-the-art semantic role labeler that computes Prop-
Bank/NomBank annotations (Palmer et al., 2005;
Meyers et al., 2008).
</bodyText>
<subsectionHeader confidence="0.999761">
3.2 Gold Standard Annotation
</subsectionHeader>
<bodyText confidence="0.99994475">
We selected 70 text pairs from the GigaPairs cor-
pus for manual predicate alignment. All document
pairs were randomly chosen with the constraint that
each text consists of 100 to 300 words.3 Predi-
</bodyText>
<footnote confidence="0.988833">
3This constraint is satisfied by 75.3% of all documents in
GigaPairs.
</footnote>
<page confidence="0.998393">
173
</page>
<bodyText confidence="0.999768181818182">
cates identified by the semantic parser are provided
as pre-labeled annotations for alignment. We asked
two students4 to tag corresponding predicates across
each text pair. Following standard practice in word
alignment tasks (cf. Cohn et al. (2008)) the annota-
tors were instructed to distinguish between sure and
possible alignments, depending on how certainly, in
their opinion, two predicates describe verbalizations
of the same event. The following examples show
predicate pairings marked as sure (2) and as possi-
ble alignments (3).
</bodyText>
<listItem confidence="0.872780428571429">
(2) a. The regulator ruled on September 27 that Nas-
daq too was qualified to bid for OMX [... ]
b. The authority [... ] had already approved a sim-
ilar application by Nasdaq.
(3) a. Myanmar’s military government said earlier this
year it has released some 220 political prisoners
[. . . ]
</listItem>
<bodyText confidence="0.94475716">
b. The government has been regularly releasing
members of Suu Kyi’s National League for
Democracy party [... ]
In total, the annotators (A/B) aligned 487/451 sure
and 221/180 possible alignments with a Kappa score
(Cohen, 1960) of 0.86.5 For the construction of a
gold standard, we merged the alignments from both
annotators by taking the union of all possible align-
ments and the intersection of all sure alignments.
Cases which involved a sure alignment on which the
annotators disagreed were resolved in a group dis-
cussion with the first author.
We split the final corpus into a development set
of 10 document pairs and a test set of 60 document
pairs. The test set contains a total of 3,453 predicates
(1,531 nouns and 1,922 verbs). Its gold standard an-
notation consists of 446 sure and 361 possible align-
ments, which corresponds to an average of 7.4 sure
(6.0 possible) alignments per document pair. Most
of the gold alignments (82.4%) are between predi-
cates of the same part-of-speech (242 noun and 423
verb pairs). A total of 383 gold alignments (47.5%)
have been annotated between predicates with iden-
tical lemma form. Diverging numbers of realized
arguments can be observed in 320 pairs (39.7%).
</bodyText>
<footnote confidence="0.8224576">
4Both annotators are students in computational linguistics,
one undergraduate (A) and one postgraduate (B) student.
5Following Brockett (2007), we computed agreement on la-
beled annotations, including unaligned predicate pairs as an ad-
ditional null category.
</footnote>
<subsectionHeader confidence="0.962462">
3.3 Potential for Discourse Coherence
</subsectionHeader>
<bodyText confidence="0.999987857142857">
This section presents an example of an aligned
predicate pair from our development set that il-
lustrates the potential of aggregating corresponding
PAS across comparable texts. The example repre-
sents one of eleven cases involving unrealized argu-
ments that can be found in our development set of
only ten document pairs.
</bodyText>
<listItem confidence="0.940485">
(4) a. The Chadians said theyArg0 had fled in fear of
their lives.
</listItem>
<bodyText confidence="0.96156444">
b. The United Nations says some 20,000
refugeesArg0 have fled into CameroonArg1.
In both sentences, the Arg0 role of the predicate flee
is filled, but Arg1 (here: the goal) has not been real-
ized in (4.a). However, sentence (4.a) is still part of a
coherent discourse, as a role filler for the omitted ar-
gument can be inferred from the preceding context.
For the goal of our work, we are interested in factors
that license such omissions of an argument. Poten-
tial factors on the discourse level include the infor-
mation status of the entity filling an argument posi-
tion, and its salience at the corresponding point in
discourse. Roth and Frank (2012) discuss additional
examples that demonstrate the importance of fac-
tors on further linguistic levels, e.g., lexical choice
of predicates and their syntactic realization.
In the example above, the aggregation of aligned
PAS presents an effective means to identify appro-
priate fillers for unrealized roles. Hence, we can uti-
lize each such pair as one positive and one negative
training instance for a model of discourse coherence
that controls the omissibility of arguments. In what
follows, we introduce an alignment approach that
can be used to automatically acquire more training
data using the entire GigaPairs corpus.
</bodyText>
<sectionHeader confidence="0.992603" genericHeader="method">
4 Model
</sectionHeader>
<bodyText confidence="0.9999719">
For the automatic induction of predicate alignments
across texts, we opt for an unsupervised graph-based
clustering method. In this section, we first define a
graph representation for pairs of documents. In par-
ticular, predicates are represented as nodes in such a
graph and similarities between predicates as edges.
We then proceed to describe various similarity mea-
sures that can be used to identify similar predicate
instances. Finally, we introduce the clustering algo-
rithm that we apply to graphs (representing pairs of
</bodyText>
<page confidence="0.990758">
174
</page>
<bodyText confidence="0.9869565">
documents) in order to induce alignments between
corresponding predicates.
</bodyText>
<subsectionHeader confidence="0.99719">
4.1 Graph representation
</subsectionHeader>
<bodyText confidence="0.999997">
We build a bipartite graph representation for each
pair of texts, using as vertices the predicate argu-
ment structures assigned in pre-processing (cf. Sec-
tion 3.1). We represent each predicate as a node and
integrate information about arguments only implic-
itly. Given the sets of predicates P1 and P2 of two
comparable texts T1 and T2, respectively, we for-
mally define an undirected graph GP1,P2 as follows:
</bodyText>
<equation confidence="0.993542">
GP (V E) where V P1 U P2 (1)
1,p2 = &apos; E = Pi x P2
</equation>
<bodyText confidence="0.999542833333333">
Edge weights. We specify the edge weight be-
tween two nodes representing predicates p1 E P1
and p2 E P2 as a weighted linear combination of
four similarity measures described in the next sec-
tion: WordNet and VerbNet similarity, Distributional
similarity and Argument similarity.
</bodyText>
<equation confidence="0.99914075">
wp1p2 = λ1 * simWN(p1,p2)
+ λ2 * simVN(p1,p2)
+ λ3 * simDist(p1,p2)
+ λ4 * simArg(p1,p2)
</equation>
<bodyText confidence="0.99980975">
Initially we set all weighting parameters λ1 ... λ4 to
have uniform weights by default. In Section 5, we
define an optimized weighting setting for the indi-
vidual similarity measures.
</bodyText>
<subsectionHeader confidence="0.992685">
4.2 Similarity Measures
</subsectionHeader>
<bodyText confidence="0.999980909090909">
We employ a number of similarity measures
that make use of complementary information
that is type-based (simWN/VN/Dist) or token-based
(simArg).6 Given two lemmatized predicates p1, p2
and their set of arguments A1 = args(p1), A2 =
args(p2), we define the following measures.
WordNet similarity. Given all pairs of synsets s1,
s2 that contain the predicates p1, p2, respectively,
we compute the maximal similarity using the infor-
mation theoretic measure described in Lin (1998).
Our implementation exploits the WordNet hierarchy
</bodyText>
<footnote confidence="0.791435">
6All token-based frequency counts (i.e., freqO and idfO)
are computed over all documents from the AFP and APW parts
of the English Gigaword Fifth Edition.
</footnote>
<bodyText confidence="0.99814775">
(Fellbaum, 1998) to find the synset of the least com-
mon subsumer (lcs) and uses the pre-computed In-
formation Content (IC) files from Pedersen et al.
(2004) to compute Lin’s measure:
</bodyText>
<equation confidence="0.996093">
IC(lcs(s1, s2))
simWN(p1,p2) = (3)
IC(s1) * IC(s2)
</equation>
<bodyText confidence="0.999941190476191">
In order to compute similarities between verbal and
nominal predicates, we further use derivation infor-
mation from NomBank (Meyers et al., 2008): if a
noun represents a nominalization of a verbal pred-
icate, we resort to the corresponding verb synset.
If no relation can be found between two predicates,
we set a default value of simWN = 0. This applies
in particular to all cases that involve a predicate not
present in WordNet.
VerbNet similarity. To overcome systematic
problems with the WordNet verb hierarchy (cf.
Richens (2008)), we further compute similarity
between verbal predicates using VerbNet (Kipper
et al., 2008). Verbs in VerbNet are categorized into
semantic classes according to their syntactic behav-
ior. A class C can recursively embed sub-classes
C3 E sub(C) that represent finer semantic and
syntactic distinctions. We define a simple similarity
function that defines fixed similarity scores between
0 and 1 for pairs of predicates p1, p2 depending on
their relatedness within the VerbNet class hierarchy:
</bodyText>
<table confidence="0.6904215">
simVN(p1,p2) = I 1.0 if EIC : p1,p2 E C (4)
0.8 if EIC, Cs : Cs E sub(C)
∧ p1,p2 E C U Cs
0.0 else
</table>
<bodyText confidence="0.985555727272727">
Distributional similarity. As some predicates
may not be covered by the WordNet and VerbNet hi-
erarchies, we additionally calculate similarity based
on distributional meaning in a semantic space (Lan-
dauer and Dumais, 1997). Following the traditional
bag-of-words approach that has been applied in re-
lated tasks (Guo and Diab, 2011; Mitchell and La-
pata, 2010), we consider the 2,000 most frequent
context words c1, ... , c2000 E C as dimensions of
a vector space and define predicates as vectors using
their Pointwise Mutual Information (PMI):
</bodyText>
<equation confidence="0.970222166666667">
p~ = (PMI(p, c1), ... , PMI(p, c2000) (5)
(2)
175
freq(x, y)
with PMI(x,y) =
freq(x) * freq(y)
</equation>
<bodyText confidence="0.999970416666667">
Given the vector representations of two predicates,
we calculate their similarity as the cosine of the an-
gle between the two vectors:
Argument similarity. While the previous similar-
ity measures are purely type-based, argument simi-
larity integrates token-based, i.e., discourse-specific,
similarity information about predications by taking
into account the similarity of their arguments. This
measure calculates the association between the ar-
guments A1 of the first and the arguments A2 of the
second predicate by determining the ratio of over-
lapping words in both argument sets.
</bodyText>
<equation confidence="0.918973">
simArg(p1,p2) = EwEA1 idf(w) + EwEA2 idf(w)
E wEA1nA2 idf(w)
(7)
</equation>
<bodyText confidence="0.997649">
In order to give higher weight to (rare) content
words, we weight each word by its Inverse Docu-
ment Frequency (IDF), which we calculate over all
documents d from the AFP and APW sections of the
Gigaword corpus:
</bodyText>
<equation confidence="0.854729">
idf(w) = log |D |(8)
|{d : w E D|}
</equation>
<bodyText confidence="0.99989">
Normalization. In order to make the outputs of all
similarity measures comparable, we normalize their
value ranges on the development set to have a mean
and standard deviation of 1.0.
</bodyText>
<subsectionHeader confidence="0.992786">
4.3 Mincut-based Clustering
</subsectionHeader>
<bodyText confidence="0.999986272727273">
Our graph clustering method uses minimum cuts (or
Mincut) in order to partition the bipartite text graph
into clusters of aligned predicates. A Mincut op-
eration divides a given graph into two disjoint sub-
graphs. Each minimum cut is performed as a cut
between some source node s and some target node
t, such that (i) each of the two nodes will be in a
different sub-graph and (ii) the sum of weights of all
removed edges will be as small as possible. Our sys-
tem determines each Mincut using an implementa-
tion of the method by Goldberg and Tarjan (1986).7
</bodyText>
<footnote confidence="0.9002915">
7Basic graph operations are performed using the freely
available Java library JGraph, cf. http://jgrapht.org/.
</footnote>
<equation confidence="0.63185275">
function CLUSTER(G)
clusters 0
E GETEDGES(G) . Step 1
e GETEDGEWITHLOWESTWEIGHT(E)
s GETSOURCENODE(e)
t GETTARGETNODE(e)
G&apos; MINCUT(G, s, t) . Step 2
C GETCONNECTEDCOMPONENTS(G&apos;)
</equation>
<bodyText confidence="0.4246385">
for all G3 E C do . Step 3
if SIZE(G3) &lt;= 2 then
</bodyText>
<figure confidence="0.947325714285714">
clusters clusters U G3
else
clusters clusters U CLUSTER(G3)
end if
end for
return clusters;
end function
</figure>
<figureCaption confidence="0.999699">
Figure 2: Pseudo code of our clustering algorithm
</figureCaption>
<bodyText confidence="0.999947809523809">
As our goal is to induce clusters that correspond to
pairs of similar predicates, we set a maximum num-
ber of two nodes per cluster as stopping criterion.
Given an input graph G, our algorithm recursively
applies Mincuts in three steps as described in Figure
2. Step 1 identifies the edge e with lowest weight in
the given graph G. Step 2 performs the actual Min-
cut operation on G. Finally, the stopping criterion
and recursion are applied in Step 3. An example of
a clustered graph is illustrated in Figure 1.
The advantage of our method compared to off-
the-shelf clustering techniques is two-fold: On the
one hand, the clustering algorithm is free of any pa-
rameters, such as the number of clusters or a clus-
tering threshold, that require fine-tuning. On the
other hand, the approach makes use of a termina-
tion criterion that very well represents the nature of
the goal of our task, namely to align pairs of predi-
cates across comparable texts. The next section pro-
vides empirical evidence for the advantage of this
approach.
</bodyText>
<sectionHeader confidence="0.999609" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.9999755">
This section evaluates our graph-clustering model
on the task of aligning predicates across compara-
ble texts. For comparison to related tasks and meth-
ods, we describe different evaluation settings, vari-
</bodyText>
<equation confidence="0.99795475">
simDist(p1,p2) =
(6)
|~p1 |* |~p2|
~p1 · ~p2
</equation>
<page confidence="0.998008">
176
</page>
<figureCaption confidence="0.958469333333333">
Figure 1: The predicates of two sentences (white: “The company has said it plans to restate its earnings for 2000
through 2002.”; grey: “The company had announced in January that it would have to restate earnings (...)”) from the
Microsoft Research Paragraph Corpus are aligned by computing clusters with minimum cuts.
</figureCaption>
<bodyText confidence="0.953904">
ous baselines, as well as results for these baselines
and the model presented above.
</bodyText>
<subsectionHeader confidence="0.975753">
5.1 Settings
</subsectionHeader>
<bodyText confidence="0.999972142857143">
In order to benchmark our model against tradi-
tional methods for word alignment, we first apply
our graph-based alignment model (Full) on three
sentence-based paraphrase corpora. This model uses
the similarity measures defined in Section 4.2 and
the clustering algorithm introduced in Section 4.3.
In a second experiment, we evaluate Full on our
novel task of inducing predicate alignments across
comparable monolingual texts, using the GigaPairs
data set described in Section 3. We evaluate against
the manually annotated gold alignments in the test
data set described in Section 3.2. To gain more in-
sight into the performance of the various similar-
ity measures included in the Full model, we eval-
uate simplified versions that omit individual similar-
ity measures (Full–[measure name]).
The relative differences in performance against
various baselines will help us quantify the differ-
ences and difficulties between a traditional sentence-
based word alignment setting and our novel align-
ment task that operates on full texts.
</bodyText>
<subsubsectionHeader confidence="0.842833">
5.1.1 Sentence-level Alignment Setting
</subsubsectionHeader>
<bodyText confidence="0.993970227272727">
For sentence-based predicate alignment we make
use of the following three corpora that are word-
aligned subsets of the paraphrase collections de-
scribed in (Cohn et al., 2008): MTC consists of 100
sentence pairs from the Multiple-Translation Chi-
nese Corpus (Huang et al., 2002), Leagues contains
100 sentential paraphrases from two translations of
Jules Verne’s “Twenty Thousand Leagues Under
the Sea”, and MSR is a sub-set of the Microsoft
Research Paraphrase Corpus (Dolan and Brockett,
2005), consisting of 130 sentence pairs. All three
paraphrase collections are in English.
Results for these experiments are reported in Sec-
tion 5.3.1. Note that in order to determine alignment
candidates, we apply the same pre-processing steps
as used for the annotation of our corpus. The se-
mantic parser identified an average number of 3.8,
5.1 and 4.7 predicates per text (i.e., per paraphrase
sentence) in MTC, Leagues and MSR, respectively.
All models are evaluated against the subset of gold
standard alignments (cf. Cohn et al. (2008)) between
pairs of words marked as predicates.
</bodyText>
<subsubsectionHeader confidence="0.860948">
5.1.2 Text-level Alignment Setting
</subsubsectionHeader>
<bodyText confidence="0.999942727272727">
Results for our own data set, GigaPairs, are reported
in Section 5.3.2. In this setting, models are evaluated
against the annotated gold standard alignments be-
tween predicates as described in Section 3.2. Since
all text pairs in GigaPairs comprise multiple sen-
tences each, the average number of predicates per
text to consider (27.5) is much higher than in the
paraphrase settings. As the full graph representa-
tion becomes rather inefficient to handle (by default,
edges are inserted between all predicate pairs), we
use the development set of 10 text pairs to estimate
</bodyText>
<page confidence="0.993996">
177
</page>
<table confidence="0.999985333333333">
Precision MTC F1 Precision Leagues F1 Precision MSR F1
Recall Recall Recall
LemmaId 25.1** 74.9 37.6** 31.5** 67.2 42.9** 42.3** 90.8 57.7**
Greedy 74.8** 88.3** 81.0 75.0** 86.0** 80.1 80.7** 97.0** 88.1
WordAlign 99.3 86.6 92.5 98.7 78.5 87.4 99.5 96.0* 97.7*
Full 92.3 72.2 81.1 92.7 69.4 79.4 94.5 88.3 91.3
</table>
<tableCaption confidence="0.9901465">
Table 1: Results for sentence-based predicte alignment in the three benchmark settings MTC, Leagues and MSR (all
numbers in %); results that significantly differ from Full are marked with asterisks (* p&lt;0.05; ** p&lt;0.01).
</tableCaption>
<bodyText confidence="0.9979506">
a threshold on predicate similarity for adding edges.
We tested all thresholds from 1.5 to 4.0 with a step-
size of 0.25 and found 2.5 to perform best. This
threshold is applied in the evaluation of all graph-
based models.
</bodyText>
<subsectionHeader confidence="0.994518">
5.2 Baselines
</subsectionHeader>
<bodyText confidence="0.997582387096774">
A simple baseline for both settings is to align all
predicates whose lemmas are identical. This base-
line, henceforth called LemmaId, is computed as a
lower bound for all settings. In order to assess the
benefits of the clustering step, we propose a second
baseline that uses the same similarity measures and
thresholds as our Full model, but omits the cluster-
ing step described in Section 4.3. Instead, it greed-
ily computes as many 1-to-1 alignments as possible,
starting from the highest similarity to the learned
threshold (Greedy).
As a more sophisticated baseline, we make
use of alignment tools commonly used in sta-
tistical machine translation (SMT). For the three
sentence-based paraphrase settings MTC, Leagues
and MSR, Cohn et al. (2008) readily provide
GIZA++ (Och and Ney, 2003) alignments as part
of their word-aligned paraphrase corpus. For the
experiments in the GigaPairs setting, we train our
own word alignment model using the state-of-the-
art word alignment tool Berkeley Aligner (Liang et
al., 2006). As word alignment tools require pairs of
sentences as input, we first extract paraphrases in the
latter setting using a re-implementation of the para-
phrase detection system by Wan et al. (2006).8 In
the following section, we abbreviate both baselines
using SMT alignment tools as WordAlign.
8Note that the performance of this system lies slightly be-
low the state-of-the-art results reported by Socher et al. (2011)
However, we were not able to reproduce the results of Socher et
al. using the publicly available release of their software.
</bodyText>
<subsectionHeader confidence="0.655">
5.3 Results
</subsectionHeader>
<bodyText confidence="0.999995714285714">
We measure precision as the number of predicted
alignments that are annotated in the gold standard
divided by the total number of predictions. Recall
is measured as the number of correctly predicted
sure alignments divided by the total number of sure
alignments in the gold standard. This conforms to
evaluation measures used for word alignment mod-
els in SMT (Och and Ney, 2003). Following Cohn
et al. (2008), we subsequently compute the Fi-score
as the harmonic mean between precision and recall.
We compute statistical significance of result dif-
ferences with a paired t-test (Cohen, 1995) over the
affected test set documents and provide correspond-
ing significance levels where appropriate.
</bodyText>
<subsectionHeader confidence="0.808644">
5.3.1 Sentence-level Predicate Alignment
</subsectionHeader>
<bodyText confidence="0.999784523809524">
The results for MTC, Leagues and MSR are pre-
sented in Table 1. The numbers indicate that
WordAlign consistently outperforms all other mod-
els on the three data sets in terms of Fl-score. Sta-
tistical significance of result differences between
WordAlign and Full can only be observed for recall
and Fl-score on the MSR data set (p&lt;0.05). Other
differences are not significant due to high variance
of results compared to data set sizes.
The overall performance of WordAlign does not
come much as a surprise, seeing that all three data
sets consist of highly parallel sentence pairs. In
fact, the results for LemmaId show that by align-
ing all predicates with identical lemmas, most of the
sure alignments in the three settings are already cov-
ered. The reason for the low precision lies in the
fact that the same lemma can occur multiple times
in the same paraphrase, a phenomenon that is bet-
ter handled by WordAlign, Greedy and Full. In-
terestingly, the Greedy model achieves the highest
recall in all settings but it performs below our Full
</bodyText>
<page confidence="0.996217">
178
</page>
<bodyText confidence="0.99989725">
model in terms of precision and F1-score. The per-
formance differences between Greedy and Full are
statistically significant (p&lt;0.01) regarding precision
and recall.
</bodyText>
<subsectionHeader confidence="0.740466">
5.3.2 Text-level Predicate Alignment
</subsectionHeader>
<bodyText confidence="0.999980463414634">
We now turn to the experiments on our own data
set, GigaPairs, which comprises full documents
of unequal lengths instead of pairs of single sen-
tences. Table 2 presents the results for our full model
and the three baselines. From all four approaches,
WordAlign yields lowest performance. We observe
two main reasons for this: On the one hand, sen-
tence paraphrase detection does not perform per-
fectly. Hence, the extracted sentence pairs do not
always contain gold alignments. On the other hand,
even sentence pairs that contain gold alignments are
generally less parallel than in the previous settings,
which make them harder to align. The increased dif-
ficulty can also be seen in the results for the Greedy
baseline, which only achieves an F1-score of 20.1%
in this setting. In contrast, we observe that the ma-
jority of all sure alignments (60.3%) can be retrieved
by applying the LemmaId model.
The Full model achieves a recall of 46.6%, but
it significantly outperforms LemmaId (p&lt;0.01) in
terms of precision (58.7%, +18.4 percentage points).
This is an important factor for us, as we plan to use
the alignments in subsequent tasks. With 52.0%,
Full achieves the best overall F1-score.
Ablating similarity measures. All aforemen-
tioned results were conducted in experiments with
a uniform weighting scheme of similarity measures
as introduced in Section 4.3. Table 3 shows the per-
formance impact of individual similarity measures
by removing them completely (i.e., setting their
weight to 0.0). The numbers indicate that not all
measures contribute positively to the overall perfor-
mance when using equal weights. However, a signif-
icant difference can only be observed when remov-
ing the argument similarity measure, which drasti-
cally reduces the results. This clearly highlights the
importance of incorporating the context of individ-
ual predications in this task.
Tuning weights. Subsequently, we tested various
combinations of weights on our development set in
order to estimate a good overall weighting scheme.
</bodyText>
<table confidence="0.9999714">
Precision Recall F1
LemmaId 40.3** 60.3** 48.3
Greedy 19.6** 20.6** 20.1**
WordAlign 19.7** 15.2** 17.2**
Full 58.7 46.6 52.0
</table>
<tableCaption confidence="0.964805333333333">
Table 2: Results for GigaPairs (all numbers in %); re-
sults that significantly differ from Full are marked with
asterisks (* p&lt;0.05; ** p&lt;0.01).
</tableCaption>
<table confidence="0.999983428571429">
Precision Recall F1
Full–WN 58.9 48.0 52.9
Full–VN 57.3 48.7 52.6
Full–Dist 54.3 42.8 47.9
Full–Args 40.1** 24.0** 30.0**
Full 58.7 46.6 52.0
Full+tuned 59.7** 50.7** 54.8**
</table>
<tableCaption confidence="0.94055325">
Table 3: Impact of removing individual measures and us-
ing a tuned weighting scheme (all numbers in %); results
that significantly differ from Full are marked with aster-
isks (* p&lt;0.05; ** p&lt;0.01).
</tableCaption>
<bodyText confidence="0.999670642857143">
This tuning procedure is implemented as a brute-
force technique, in which we fix the weight of one
similarity measure and allow all other measures to
receive a weight assignment between 0.25 to 5.0
times the fixed weight. Finally, the resulting weights
are normalized to sum to 1.0. We found the best per-
forming weighting scheme to be 0.09, 0.48, 0.24 and
0.19 for A1, ... , A4, respectively (cf. Eq. (2), Section
4). The performance gains of the resulting model
(Full+tuned) can be seen in Table 3. Comput-
ing statistical significance of the result differences
between Full+tuned and all baseline models con-
firmed significant improvements (p&lt;0.01) for both
precision and F1-score.
</bodyText>
<subsectionHeader confidence="0.968334">
5.4 Error Analysis
</subsectionHeader>
<bodyText confidence="0.9999809">
We perform an error analysis on the output of
Full+tuned on the development set of GigaPairs
in order to determine re-occurring problems. In to-
tal, the model missed 13 out of 35 sure alignments
(Type I errors) and predicted 23 alignments not an-
notated in the gold standard (Type II errors).
Six Type I errors (46%) occurred when the lemma
of an affected predicate occurred more than once in a
text and the model missed a correct link. Vice versa,
identical predicates that refer to different events have
</bodyText>
<page confidence="0.99733">
179
</page>
<bodyText confidence="0.998129166666667">
been the source of 8 Type II errors (35%). We ob-
serve that these errors are frequently related to pred-
icates, such as “say” and “appear”, that often occur
in news texts. Altogether, we find 15 Type II errors
(65%) that are due to high predicate similarity de-
spite low argument overlap (cf. Example (5)).
</bodyText>
<listItem confidence="0.9447975">
(5) a. The US alert (...) followed intelligence reports
that ...
</listItem>
<bodyText confidence="0.9496031">
b. The Foreign Ministry announcement called on
Japanese citizens to be cautious ...
We observe that argument overlap itself can be low
even for correct alignments. This clearly indicates
that a better integration of context is needed. Ex-
ample (6.a) illustrates a case in which the agent of
a warning event is not realized. Here, contextual in-
formation is required to correctly align it to the first
warning event in (6.b). This involves inference be-
yond the local PAS.
</bodyText>
<listItem confidence="0.851143">
(6) a. The US alert (... ) is one step down from a full
[travel]Arg1 warning [ ]Arg0.
</listItem>
<bodyText confidence="0.9936454">
b. Japan has issued a travel alert ... (which)
follows similar warnings [from Ameri-
can and British authorities]Arg0. (... ) An offi-
cial said it was highly unusual for [Tokyo]Arg0
to issue such a warning ...
</bodyText>
<sectionHeader confidence="0.999358" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999987847457627">
We presented a novel task for predicate alignment
across comparable monolingual texts, which we ad-
dress using graph-based clustering with Mincuts.
The motivation for this task is to acquire empirical
data for studying discourse coherence factors related
to argument structure realization.
As a first step, we constructed a data set of com-
parable texts that provide full discourse contexts
for alternative verbalizations of the same underlying
events. The data set is derived from all newswire
pairs found in the English Gigaword Fifth Edition
and contains a total of more than 160,000 paired
documents.
A subset of these pairs forms an evaluation set,
annotated with gold alignments that relate predica-
tions, which exhibit a (possibly partial) correspond-
ing argument structure. We established that the an-
notation task, while difficult, can be performed with
good inter-annotator agreement (n at 0.86).
Our main contribution is a novel clustering ap-
proach using Mincuts for aligning predications
across comparable texts. Our experiments estab-
lished that recursive clustering improves on greedy
selection methods by profiting from global infor-
mation encoded in the graph representation. While
the Mincut-based method is in itself unsupervised, a
small amount of development data is needed to tune
parameters for the construction of particularly suit-
able input graphs.
We tested our full model against two additional
baselines: simple heuristic alignment based on iden-
tical lemma forms and a combination of techniques
from SMT and paraphrase detection. The evalua-
tion for our novel task was complemented by a tra-
ditional word alignment task using established para-
phrase data sets. We determined clear differences in
performance for all models for the two types of task
settings. While word alignment methods from SMT
outperform the competing models in the sentence-
based alignment tasks, they perform poorly in the
discourse setting.
In future work, we will enhance our model by
incorporating more refined similarity measures in-
cluding discourse-based criteria. We will further ex-
plore tuning techniques, e.g., a more suitable pre-
selection method for edges in graph construction, in
order to increase either precision or recall. The deci-
sion of optimizing towards one measure or another
is clearly task-dependent. In our case, high preci-
sion is favorable as we plan to learn accurate dis-
course model parameters from the computed align-
ments. Even though such an optimization will result
in an overall lower recall, application of the align-
ment model on the entire GigaPairs corpus can still
provide us with a large amount of precise predicate
alignments. Using this set of alignments, we will
then proceed to exploit contextual information in or-
der to learn a semantic model for discourse coher-
ence in argument structure realization.
</bodyText>
<sectionHeader confidence="0.996514" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.99763">
We are grateful to the Landesgraduiertenf¨orderung
Baden-W¨urttemberg for funding within the research
initiative “Coherence in language processing” at
Heidelberg University. We thank Danny Rehl and
Lukas Funk for annotation.
</bodyText>
<page confidence="0.996147">
180
</page>
<sectionHeader confidence="0.989077" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999786615384616">
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. 2012. SemEval-2012 Task 6: A pi-
lot on semantic textual similarity. In Proceedings of
the 6th International Workshop on Semantic Evalua-
tions, Montreal, Canada, June. to appear.
Regina Barzilay and Mirella Lapata. 2008. Modeling
local coherence: An entity-based approach. Computa-
tional Linguistics, 34(1):1–34.
Regina Barzilay and Lillian Lee. 2004. Catching the
drift: Probabilistic content models, with applications
to generation and summarization. In Proceedings of
the Human Language Technology Conference of the
North American Chapter of the Association for Com-
putational Linguistics, Boston, Mass., 2–7 May 2004,
pages 113–120.
Anja Belz, Eric Kow, Jette Viethen, and Albert Gatt.
2009. The grec main subject reference generation
challenge 2009: overview and evaluation results. In
Proceedings of the 2009 Workshop on Language Gen-
eration and Summarisation, pages 79–87.
Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo
Giampiccolo, and Bernardo Magnini. 2009. The fifth
pascal recognizing textual entailment challenge. In
Proceedings of TAC.
Anders Bj¨orkelund, Bernd Bohnet, Love Hafdell, and
Pierre Nugues. 2010. A high-performance syntac-
tic and semantic dependency parser. In Coling 2010:
Demonstration Volume, pages 33–36, Beijing, China,
August. Coling 2010 Organizing Committee.
Bernd Bohnet. 2010. Top accuracy and fast dependency
parsing is not a contradiction. In Proceedings of the
23rd International Conference on Computational Lin-
guistics (Coling 2010), pages 89–97, Beijing, China,
August. Coling 2010 Organizing Committee.
Chris Brockett. 2007. Aligning the RTE 2006 Corpus.
Microsoft Research.
Peter F. Brown, Vincent J. Della Pietra, Stephan A. Della
Pietra, and Robert L. Mercer. 1993. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19:263–311.
Jacob Cohen. 1960. A coefficient of agreement for nom-
inal scales. Educational and Psychological Measure-
ment, 20:37–46.
Paul R. Cohen. 1995. Empirical methods for artificial
intelligence. MIT Press, Cambridge, MA, USA.
Trevor Cohn, Chris Callison-Burch, and Mirella Lap-
ata. 2008. Constructing Corpora for Development and
Evaluation of Paraphrase Systems. 34(4).
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The PASCAL recognising textual entailment
challenge. In J. Qui˜nonero-Candela, I. Dagan, and
B. Magnini, editors, Machine Learning Challenges,
pages 177–190. Springer, Heidelberg, Germany.
William B. Dolan and Chris Brockett. 2005. Automat-
ically constructing a corpus of sentential paraphrases.
In Proceedings of the Third International Workshop on
Paraphrasing.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge,
Mass.
Adrew V. Goldberg and Robert E. Tarjan. 1986. A
new approach to the maximum flow problem. In Pro-
ceedings of the eighteenth annual ACM symposium on
Theory of computing, pages 136–146, New York, NY,
USA.
Barbara J. Grosz, Aravind K. Joshi, and Scott Weinstein.
1995. Centering: A framework for modeling the lo-
cal coherence of discourse. Computational Linguis-
tics, 21(2):203–225.
Weiwei Guo and Mona Diab. 2011. Semantic topic mod-
els: Combining word distributional statistics and dic-
tionary definitions. In Proceedings of the 2011 Con-
ference on Empirical Methods in Natural Language
Processing, pages 552–561, July.
Shudong Huang, David Graff, and George Doddington.
2002. Multiple-Translation Chinese Corpus. Linguis-
tic Data Consortium, Philadelphia.
Karin Kipper, Anna Korhonen, Neville Ryant, and
Martha Palmer. 2008. A Large-scale Classification
of English Verbs. 42(1):21–40.
Thomas K. Landauer and Susan T. Dumais. 1997. A so-
lution to Plato’s problem: The Latent Semantic Anal-
ysis theory of the acquisition, induction, and represen-
tation of knowledge. Psychological Review, 104:211–
240.
Percy Liang, Benjamin Taskar, and Dan Klein. 2006.
Alignment by agreement. In North American Associ-
ation for Computational Linguistics (NAACL), pages
104–111.
Dekang Lin. 1998. An information-theoretic defini-
tion of similarity. In Proceedings of the 15th Inter-
national Conference on Machine Learning, Madison,
Wisc., 24–27 July 1998, pages 296–304.
Bill MacCartney, Michael Galley, and Christopher D.
Manning. 2008. A phrase-based alignment model
for natural language inference. In Proceedings of the
2008 Conference on Empirical Methods in Natural
Language Processing, Waikiki, Honolulu, Hawaii, 25-
27 October 2008.
Adam Meyers, Ruth Reeves, and Catherine Macleod.
2008. NomBank v1.0. Linguistic Data Consortium,
Philadelphia.
Shachar Mirkin, Jonathan Berant, Ido Dagan, and Eyal
Shnarch. 2010a. Recognising entailment within dis-
</reference>
<page confidence="0.979652">
181
</page>
<reference confidence="0.999920103896104">
course. In Proceedings of the 23rd International Con-
ference on Computational Linguistics (Coling 2010),
Beijing, China, August. Coling 2010 Organizing Com-
mittee.
Shachar Mirkin, Ido Dagan, and Sebastian Pad´o. 2010b.
Assessing the role of discourse references in entail-
ment inference. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, Uppsala, Sweden, 11–16 July 2010.
Jeff Mitchell and Mirella Lapata. 2010. Composition
in Distributional Models of Semantics. 34(8):1388–
1429.
Franz Josef Och and Hermann Ney. 2003. A Systematic
Comparison of Various Statistical Alignment Models.
29(1):19–51.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated corpus of
semantic roles. Computational Linguistics, 31(1):71–
105.
Robert Parker, David Graff, Jumbo Kong, Ke Chen, and
Kazuaki Maeda. 2011. English Gigaword Fifth Edi-
tion. Linguistic Data Consortium, Philadelphia.
Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. WordNet::Similarity – Measuring the re-
latedness of concepts. In Companion Volume to the
Proceedings of the Human Language Technology Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics, Boston, Mass.,
2–7 May 2004, pages 267–270.
Tom Richens. 2008. Anomalies in the wordnet verb hier-
archy. In Proceedings of the 22nd International Con-
ference on Computational Linguistics (Coling 2008),
pages 729–736. Association for Computational Lin-
guistics.
Michael Roth and Anette Frank. 2012. Aligning pred-
icate argument structures in monolingual comparable
texts: A new corpus for a new task. In Proceedings
of the First Joint Conference on Lexical and Computa-
tional Semantics, Montreal, Canada, June.
Josef Ruppenhofer, Caroline Sporleder, Roser Morante,
Collin Baker, and Martha Palmer. 2010. SemEval-
2010 Task 10: Linking Events and Their Participants
in Discourse. In Proceedings of the 5th International
Workshop on Semantic Evaluations, pages 45–50, Up-
psala, Sweden, July.
Richard Socher, Eric H. Huang, Jeffrey Pennington, An-
drew Y. Ng, and Christopher D. Manning. 2011. Dy-
namic pooling and unfolding recursive autoencoders
for paraphrase detection. In Advances in Neural Infor-
mation Processing Systems (NIPS 2011).
Ivan Titov and Mikhail Kozhevnikov. 2010. Bootstrap-
ping semantic analyzers from non-contradictory texts.
In Proceedings of the 48th Annual Meeting of the Asso-
ciation for Computational Linguistics, Uppsala, Swe-
den, 11–16 July 2010, pages 958–967.
Christopher Walker, Stephanie Strassel, Julie Medero,
and Kazuaki Maeda. 2006. ACE 2005 Multilin-
gual Training Corpus. Linguistic Data Consortium,
Philadelphia.
Stephen Wan, Mark Dras, Robert Dale, and Cecile Paris.
2006. Using dependency-based features to take the
”Para-farce” out of paraphrase. In Proceedings of the
Australasian Language Technology Workshop, pages
131–138.
Ralph Weischedel, Martha Palmer, Mitchell Marcus, Ed-
uard Hovy, Sameer Pradhan, Lance Ramshaw, Nian-
wen Xue, Ann Taylor, Jeff Kaufman, Michelle Fran-
chini, Mohammed El-Bachouti, Robert Belvin, and
Ann Houston. 2011. OntoNotes Release 4.0. Lin-
guistic Data Consortium, Philadelphia.
Sander Wubben, Antal van den Bosch, Emiel Krahmer,
and Erwin Marsi. 2009. Clustering and matching
headlines for automatic paraphrase acquisition. In
Proceedings of the 12th European Workshop on Nat-
ural Language Generation (ENLG 2009), pages 122–
125, Athens, Greece, March. Association for Compu-
tational Linguistics.
</reference>
<page confidence="0.99801">
182
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.210310">
<title confidence="0.9986655">Aligning Predicates across Monolingual Comparable using Graph-based Clustering</title>
<author confidence="0.977281">Roth</author>
<affiliation confidence="0.998949">Department of Computational</affiliation>
<address confidence="0.726033">Heidelberg</address>
<abstract confidence="0.98360825">Generating coherent discourse is an important aspect in natural language generation. Our aim is to learn factors that constitute coherent discourse from data, with a focus on how to realize predicate-argument structures in a model that exceeds the sentence level. We present an important subtask for this overall goal, in which we align predicates across comparable texts, admitting partial argument structure correspondence. The contribution of this work is two-fold: We first construct a large corpus resource of comparable texts, including an evaluation set with manual predicate alignments. Secondly, we present a novel approach for aligning predicates across comparable texts using graph-based clustering with Mincuts. Our method significantly outperforms other alignment techniques when applied to this novel alignment task, by a margin</abstract>
<intro confidence="0.319065">at least 6.5 percentage points in</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Daniel Cer</author>
<author>Mona Diab</author>
<author>Aitor Gonzalez-Agirre</author>
</authors>
<title>SemEval-2012 Task 6: A pilot on semantic textual similarity.</title>
<date>2012</date>
<booktitle>In Proceedings of the 6th International Workshop on Semantic Evaluations,</booktitle>
<location>Montreal, Canada,</location>
<note>to appear.</note>
<marker>Agirre, Cer, Diab, Gonzalez-Agirre, 2012</marker>
<rawString>Eneko Agirre, Daniel Cer, Mona Diab, and Aitor Gonzalez-Agirre. 2012. SemEval-2012 Task 6: A pilot on semantic textual similarity. In Proceedings of the 6th International Workshop on Semantic Evaluations, Montreal, Canada, June. to appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Mirella Lapata</author>
</authors>
<title>Modeling local coherence: An entity-based approach.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>1</issue>
<contexts>
<context position="1599" citStr="Barzilay and Lapata (2008)" startWordPosition="232" endWordPosition="236">ntly outperforms other alignment techniques when applied to this novel alignment task, by a margin of at least 6.5 percentage points in Fl-score. 1 Introduction Discourse coherence is an important aspect in natural language generation (NLG) applications. A number of theories have investigated coherence inducing factors. A prominent example is Centering Theory (Grosz et al., 1995), which models local coherence by relating the choice of referring expressions to the importance of an entity at a certain stage of a discourse. A data-driven model based on this theory is the entity-based approach by Barzilay and Lapata (2008), which models coherence phenomena by observing sentence-to-sentence transitions of entity occurrences. Barzilay and Lapata show that their approach can discriminate between a coherent and a non-coherent set of ordered sentences. However, their model is not able to generate alternative entity realizations by itself. Furthermore, the entity-based approach only investigates realization patterns for individual entities in discourse in terms of core grammatical functions. It does not investigate the interplay between entity transitions and realization patterns for fullfledged semantic structures. </context>
</contexts>
<marker>Barzilay, Lapata, 2008</marker>
<rawString>Regina Barzilay and Mirella Lapata. 2008. Modeling local coherence: An entity-based approach. Computational Linguistics, 34(1):1–34.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Lillian Lee</author>
</authors>
<title>Catching the drift: Probabilistic content models, with applications to generation and summarization.</title>
<date>2004</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>113--120</pages>
<location>Boston, Mass.,</location>
<contexts>
<context position="2736" citStr="Barzilay and Lee, 2004" startWordPosition="392" endWordPosition="395">ween entity transitions and realization patterns for fullfledged semantic structures. This interplay, however, is an important factor for a semantics-based, generative model of discourse coherence. The main hypothesis of our work is that we can automatically learn context-specific realization patterns for predicate argument structures (PAS) from a semantically parsed corpus of comparable text pairs. Our assumption builds on the success of previous research, where comparable and parallel texts have been exploited for a range of related learning tasks, e.g., unsupervised discourse segmentation (Barzilay and Lee, 2004) and bootstrapping semantic analyzers (Titov and Kozhevnikov, 2010). For our purposes, we are interested in finding corresponding PAS across comparable texts that are known to talk about the same events, and hence involve the same set of underlying event participants. By aligning predicates in such texts, we can investigate the factors that determine discourse coherence in the realization patterns for the involved arguments. These include the specific forms of argument realization, as a pronoun or a specific type of referential expression, as studied in prior work in NLG (Belz et al., 2009, in</context>
</contexts>
<marker>Barzilay, Lee, 2004</marker>
<rawString>Regina Barzilay and Lillian Lee. 2004. Catching the drift: Probabilistic content models, with applications to generation and summarization. In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics, Boston, Mass., 2–7 May 2004, pages 113–120.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anja Belz</author>
<author>Eric Kow</author>
<author>Jette Viethen</author>
<author>Albert Gatt</author>
</authors>
<title>The grec main subject reference generation challenge 2009: overview and evaluation results.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Workshop on Language Generation and Summarisation,</booktitle>
<pages>79--87</pages>
<contexts>
<context position="3332" citStr="Belz et al., 2009" startWordPosition="491" endWordPosition="494">rzilay and Lee, 2004) and bootstrapping semantic analyzers (Titov and Kozhevnikov, 2010). For our purposes, we are interested in finding corresponding PAS across comparable texts that are known to talk about the same events, and hence involve the same set of underlying event participants. By aligning predicates in such texts, we can investigate the factors that determine discourse coherence in the realization patterns for the involved arguments. These include the specific forms of argument realization, as a pronoun or a specific type of referential expression, as studied in prior work in NLG (Belz et al., 2009, inter alia). The specific set-up we examine, however, allows us to further investi171 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 171–182, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics gate the factors that govern the non-realization of an argument position, as a special form of coherence inducing element in discourse. Example (1), extracted from our corpus of aligned texts,illustrates this point: Both texts report on the same event of locating victims i</context>
</contexts>
<marker>Belz, Kow, Viethen, Gatt, 2009</marker>
<rawString>Anja Belz, Eric Kow, Jette Viethen, and Albert Gatt. 2009. The grec main subject reference generation challenge 2009: overview and evaluation results. In Proceedings of the 2009 Workshop on Language Generation and Summarisation, pages 79–87.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luisa Bentivogli</author>
<author>Ido Dagan</author>
<author>Hoa Trang Dang</author>
<author>Danilo Giampiccolo</author>
<author>Bernardo Magnini</author>
</authors>
<title>The fifth pascal recognizing textual entailment challenge.</title>
<date>2009</date>
<booktitle>In Proceedings of TAC.</booktitle>
<contexts>
<context position="9050" citStr="Bentivogli et al., 2009" startWordPosition="1411" endWordPosition="1414">-directional entailments, i.e., each of the two phrases must entail the other. Wan et al. (2006) show that a simple approach solely based on word (and lemmatized n-gram) overlap can already achieve an Fl-score of up to 83% for detecting paraphrases in the Microsoft Research Paraphrase Corpus (Dolan and Brockett, 2005, MSRPC). In fact, this is just 0.6% points below the state-of-the-art results recently reported by Socher et al. (2011). The MSRPC and data sets from the first RTE challenges only consisted of isolated pairs of sentences. The Fifth PASCAL Recognizing Textual Entailment Challenge (Bentivogli et al., 2009) introduced a “Search Task”, where entailing sentences for a hypothesis have to be found in a set of full documents. This new task first opened the doors for assessing the role of discourse (Mirkin et al., 2010a; Mirkin et al., 2010b) in RTE. However, this setting is still limited as discourse contexts are only provided for the entailing part (T) of each text pair but not for the hypothesis H. A further task related to ours is the detection of event coreference. The goal of this task is to identify all mentions of the same event within a document and, in some settings, also across documents. H</context>
</contexts>
<marker>Bentivogli, Dagan, Dang, Giampiccolo, Magnini, 2009</marker>
<rawString>Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo Giampiccolo, and Bernardo Magnini. 2009. The fifth pascal recognizing textual entailment challenge. In Proceedings of TAC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anders Bj¨orkelund</author>
<author>Bernd Bohnet</author>
<author>Love Hafdell</author>
<author>Pierre Nugues</author>
</authors>
<title>A high-performance syntactic and semantic dependency parser.</title>
<date>2010</date>
<journal>Organizing Committee.</journal>
<booktitle>In Coling 2010: Demonstration Volume,</booktitle>
<pages>33--36</pages>
<location>Beijing, China,</location>
<marker>Bj¨orkelund, Bohnet, Hafdell, Nugues, 2010</marker>
<rawString>Anders Bj¨orkelund, Bernd Bohnet, Love Hafdell, and Pierre Nugues. 2010. A high-performance syntactic and semantic dependency parser. In Coling 2010: Demonstration Volume, pages 33–36, Beijing, China, August. Coling 2010 Organizing Committee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernd Bohnet</author>
</authors>
<title>Top accuracy and fast dependency parsing is not a contradiction.</title>
<date>2010</date>
<journal>Organizing Committee.</journal>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics (Coling</booktitle>
<pages>89--97</pages>
<location>Beijing, China,</location>
<contexts>
<context position="12351" citStr="Bohnet, 2010" startWordPosition="1950" endWordPosition="1951">ty method on headlines presented by Wubben et al. (2009). In addition to calculating the similarity of news titles, we impose an additional date constraint to further increase the precision of extracted pairs of texts. Random inspection of about 100 documents revealed only two texts describing different events. Overall, we extracted 167,728 document pairs containing a total of 50 million word tokens. Each document in this corpus consists of up to 7.564 words with a mean and median of 301 and 213 words, respectively. All texts have been pre-processed using MATE tools (Bj¨orkelund et al., 2010; Bohnet, 2010), a pipeline of NLP modules including a stateof-the-art semantic role labeler that computes PropBank/NomBank annotations (Palmer et al., 2005; Meyers et al., 2008). 3.2 Gold Standard Annotation We selected 70 text pairs from the GigaPairs corpus for manual predicate alignment. All document pairs were randomly chosen with the constraint that each text consists of 100 to 300 words.3 Predi3This constraint is satisfied by 75.3% of all documents in GigaPairs. 173 cates identified by the semantic parser are provided as pre-labeled annotations for alignment. We asked two students4 to tag correspondin</context>
</contexts>
<marker>Bohnet, 2010</marker>
<rawString>Bernd Bohnet. 2010. Top accuracy and fast dependency parsing is not a contradiction. In Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 89–97, Beijing, China, August. Coling 2010 Organizing Committee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Brockett</author>
</authors>
<title>Aligning the RTE</title>
<date>2007</date>
<institution>Corpus. Microsoft Research.</institution>
<contexts>
<context position="14983" citStr="Brockett (2007)" startWordPosition="2378" endWordPosition="2379">bs). Its gold standard annotation consists of 446 sure and 361 possible alignments, which corresponds to an average of 7.4 sure (6.0 possible) alignments per document pair. Most of the gold alignments (82.4%) are between predicates of the same part-of-speech (242 noun and 423 verb pairs). A total of 383 gold alignments (47.5%) have been annotated between predicates with identical lemma form. Diverging numbers of realized arguments can be observed in 320 pairs (39.7%). 4Both annotators are students in computational linguistics, one undergraduate (A) and one postgraduate (B) student. 5Following Brockett (2007), we computed agreement on labeled annotations, including unaligned predicate pairs as an additional null category. 3.3 Potential for Discourse Coherence This section presents an example of an aligned predicate pair from our development set that illustrates the potential of aggregating corresponding PAS across comparable texts. The example represents one of eleven cases involving unrealized arguments that can be found in our development set of only ten document pairs. (4) a. The Chadians said theyArg0 had fled in fear of their lives. b. The United Nations says some 20,000 refugeesArg0 have fle</context>
</contexts>
<marker>Brockett, 2007</marker>
<rawString>Chris Brockett. 2007. Aligning the RTE 2006 Corpus. Microsoft Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Vincent J Della Pietra</author>
<author>Stephan A Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="6931" citStr="Brown et al., 1993" startWordPosition="1057" endWordPosition="1060"> tasks. Section 3 describes our task and a suitable data set. Section 4 introduces a graph-based clustering model using Mincuts for the alignment of predicates. Section 5 outlines the experiments and presents evaluation results. Finally, we conclude in Section 6 and discuss future work. 2 Related Work The task of aligning words in general has been studied extensively in previous work, for example as part of research in statistical machine translation (SMT). Typically, alignment models in SMT are trained by observing and (re-)estimating co-occurrence counts of word pairs in parallel sentences (Brown et al., 1993). The same methods have also been applied in monolingual settings, for example to align words in paraphrases (Cohn et al., 2008). In contrast to traditional word alignment tasks, our focus is not on pairs of isolated sentences but on aligning predicates within the discourse contexts in which they are situated. Furthermore, text pairs for our task should not be strictly parallel as we are specifically interested in the impact of different discourse contexts. In Section 5, we will show that this particular setting indeed constitutes a more challenging task compared to traditional word alignment </context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Vincent J. Della Pietra, Stephan A. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19:263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Cohen</author>
</authors>
<title>A coefficient of agreement for nominal scales.</title>
<date>1960</date>
<booktitle>Educational and Psychological Measurement,</booktitle>
<pages>20--37</pages>
<contexts>
<context position="13864" citStr="Cohen, 1960" startWordPosition="2194" endWordPosition="2195">The following examples show predicate pairings marked as sure (2) and as possible alignments (3). (2) a. The regulator ruled on September 27 that Nasdaq too was qualified to bid for OMX [... ] b. The authority [... ] had already approved a similar application by Nasdaq. (3) a. Myanmar’s military government said earlier this year it has released some 220 political prisoners [. . . ] b. The government has been regularly releasing members of Suu Kyi’s National League for Democracy party [... ] In total, the annotators (A/B) aligned 487/451 sure and 221/180 possible alignments with a Kappa score (Cohen, 1960) of 0.86.5 For the construction of a gold standard, we merged the alignments from both annotators by taking the union of all possible alignments and the intersection of all sure alignments. Cases which involved a sure alignment on which the annotators disagreed were resolved in a group discussion with the first author. We split the final corpus into a development set of 10 document pairs and a test set of 60 document pairs. The test set contains a total of 3,453 predicates (1,531 nouns and 1,922 verbs). Its gold standard annotation consists of 446 sure and 361 possible alignments, which corres</context>
</contexts>
<marker>Cohen, 1960</marker>
<rawString>Jacob Cohen. 1960. A coefficient of agreement for nominal scales. Educational and Psychological Measurement, 20:37–46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul R Cohen</author>
</authors>
<title>Empirical methods for artificial intelligence.</title>
<date>1995</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA, USA.</location>
<contexts>
<context position="30667" citStr="Cohen, 1995" startWordPosition="4929" endWordPosition="4930">software. 5.3 Results We measure precision as the number of predicted alignments that are annotated in the gold standard divided by the total number of predictions. Recall is measured as the number of correctly predicted sure alignments divided by the total number of sure alignments in the gold standard. This conforms to evaluation measures used for word alignment models in SMT (Och and Ney, 2003). Following Cohn et al. (2008), we subsequently compute the Fi-score as the harmonic mean between precision and recall. We compute statistical significance of result differences with a paired t-test (Cohen, 1995) over the affected test set documents and provide corresponding significance levels where appropriate. 5.3.1 Sentence-level Predicate Alignment The results for MTC, Leagues and MSR are presented in Table 1. The numbers indicate that WordAlign consistently outperforms all other models on the three data sets in terms of Fl-score. Statistical significance of result differences between WordAlign and Full can only be observed for recall and Fl-score on the MSR data set (p&lt;0.05). Other differences are not significant due to high variance of results compared to data set sizes. The overall performance</context>
</contexts>
<marker>Cohen, 1995</marker>
<rawString>Paul R. Cohen. 1995. Empirical methods for artificial intelligence. MIT Press, Cambridge, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Cohn</author>
<author>Chris Callison-Burch</author>
<author>Mirella Lapata</author>
</authors>
<title>Constructing Corpora for Development and Evaluation of Paraphrase Systems.</title>
<date>2008</date>
<volume>34</volume>
<issue>4</issue>
<contexts>
<context position="7059" citStr="Cohn et al., 2008" startWordPosition="1078" endWordPosition="1081">or the alignment of predicates. Section 5 outlines the experiments and presents evaluation results. Finally, we conclude in Section 6 and discuss future work. 2 Related Work The task of aligning words in general has been studied extensively in previous work, for example as part of research in statistical machine translation (SMT). Typically, alignment models in SMT are trained by observing and (re-)estimating co-occurrence counts of word pairs in parallel sentences (Brown et al., 1993). The same methods have also been applied in monolingual settings, for example to align words in paraphrases (Cohn et al., 2008). In contrast to traditional word alignment tasks, our focus is not on pairs of isolated sentences but on aligning predicates within the discourse contexts in which they are situated. Furthermore, text pairs for our task should not be strictly parallel as we are specifically interested in the impact of different discourse contexts. In Section 5, we will show that this particular setting indeed constitutes a more challenging task compared to traditional word alignment in parallel or paraphrasing sentences. Another set of related tasks is found in the area of textual inference. Since 2006, there</context>
<context position="13062" citStr="Cohn et al. (2008)" startWordPosition="2059" endWordPosition="2062">ropBank/NomBank annotations (Palmer et al., 2005; Meyers et al., 2008). 3.2 Gold Standard Annotation We selected 70 text pairs from the GigaPairs corpus for manual predicate alignment. All document pairs were randomly chosen with the constraint that each text consists of 100 to 300 words.3 Predi3This constraint is satisfied by 75.3% of all documents in GigaPairs. 173 cates identified by the semantic parser are provided as pre-labeled annotations for alignment. We asked two students4 to tag corresponding predicates across each text pair. Following standard practice in word alignment tasks (cf. Cohn et al. (2008)) the annotators were instructed to distinguish between sure and possible alignments, depending on how certainly, in their opinion, two predicates describe verbalizations of the same event. The following examples show predicate pairings marked as sure (2) and as possible alignments (3). (2) a. The regulator ruled on September 27 that Nasdaq too was qualified to bid for OMX [... ] b. The authority [... ] had already approved a similar application by Nasdaq. (3) a. Myanmar’s military government said earlier this year it has released some 220 political prisoners [. . . ] b. The government has bee</context>
<context position="26230" citStr="Cohn et al., 2008" startWordPosition="4214" endWordPosition="4217">formance of the various similarity measures included in the Full model, we evaluate simplified versions that omit individual similarity measures (Full–[measure name]). The relative differences in performance against various baselines will help us quantify the differences and difficulties between a traditional sentencebased word alignment setting and our novel alignment task that operates on full texts. 5.1.1 Sentence-level Alignment Setting For sentence-based predicate alignment we make use of the following three corpora that are wordaligned subsets of the paraphrase collections described in (Cohn et al., 2008): MTC consists of 100 sentence pairs from the Multiple-Translation Chinese Corpus (Huang et al., 2002), Leagues contains 100 sentential paraphrases from two translations of Jules Verne’s “Twenty Thousand Leagues Under the Sea”, and MSR is a sub-set of the Microsoft Research Paraphrase Corpus (Dolan and Brockett, 2005), consisting of 130 sentence pairs. All three paraphrase collections are in English. Results for these experiments are reported in Section 5.3.1. Note that in order to determine alignment candidates, we apply the same pre-processing steps as used for the annotation of our corpus. </context>
<context position="29253" citStr="Cohn et al. (2008)" startWordPosition="4701" endWordPosition="4704">maId, is computed as a lower bound for all settings. In order to assess the benefits of the clustering step, we propose a second baseline that uses the same similarity measures and thresholds as our Full model, but omits the clustering step described in Section 4.3. Instead, it greedily computes as many 1-to-1 alignments as possible, starting from the highest similarity to the learned threshold (Greedy). As a more sophisticated baseline, we make use of alignment tools commonly used in statistical machine translation (SMT). For the three sentence-based paraphrase settings MTC, Leagues and MSR, Cohn et al. (2008) readily provide GIZA++ (Och and Ney, 2003) alignments as part of their word-aligned paraphrase corpus. For the experiments in the GigaPairs setting, we train our own word alignment model using the state-of-theart word alignment tool Berkeley Aligner (Liang et al., 2006). As word alignment tools require pairs of sentences as input, we first extract paraphrases in the latter setting using a re-implementation of the paraphrase detection system by Wan et al. (2006).8 In the following section, we abbreviate both baselines using SMT alignment tools as WordAlign. 8Note that the performance of this s</context>
<context position="30485" citStr="Cohn et al. (2008)" startWordPosition="4900" endWordPosition="4903">ghtly below the state-of-the-art results reported by Socher et al. (2011) However, we were not able to reproduce the results of Socher et al. using the publicly available release of their software. 5.3 Results We measure precision as the number of predicted alignments that are annotated in the gold standard divided by the total number of predictions. Recall is measured as the number of correctly predicted sure alignments divided by the total number of sure alignments in the gold standard. This conforms to evaluation measures used for word alignment models in SMT (Och and Ney, 2003). Following Cohn et al. (2008), we subsequently compute the Fi-score as the harmonic mean between precision and recall. We compute statistical significance of result differences with a paired t-test (Cohen, 1995) over the affected test set documents and provide corresponding significance levels where appropriate. 5.3.1 Sentence-level Predicate Alignment The results for MTC, Leagues and MSR are presented in Table 1. The numbers indicate that WordAlign consistently outperforms all other models on the three data sets in terms of Fl-score. Statistical significance of result differences between WordAlign and Full can only be ob</context>
</contexts>
<marker>Cohn, Callison-Burch, Lapata, 2008</marker>
<rawString>Trevor Cohn, Chris Callison-Burch, and Mirella Lapata. 2008. Constructing Corpora for Development and Evaluation of Paraphrase Systems. 34(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Oren Glickman</author>
<author>Bernardo Magnini</author>
</authors>
<title>The PASCAL recognising textual entailment challenge.</title>
<date>2006</date>
<booktitle>Machine Learning Challenges,</booktitle>
<pages>177--190</pages>
<editor>In J. Qui˜nonero-Candela, I. Dagan, and B. Magnini, editors,</editor>
<publisher>Springer,</publisher>
<location>Heidelberg, Germany.</location>
<contexts>
<context position="7795" citStr="Dagan et al. (2006)" startWordPosition="1198" endWordPosition="1201">edicates within the discourse contexts in which they are situated. Furthermore, text pairs for our task should not be strictly parallel as we are specifically interested in the impact of different discourse contexts. In Section 5, we will show that this particular setting indeed constitutes a more challenging task compared to traditional word alignment in parallel or paraphrasing sentences. Another set of related tasks is found in the area of textual inference. Since 2006, there have been regular challenges on the task of Recognizing Textual Entailment (RTE). In the original task description, Dagan et al. (2006) define textual entailment “as a directional relationship between pairs of text expressions, denoted by T - the entailing ‘Text’ -, and H - the entailed ‘Hypothesis’. (... ) T entails H if the meaning of H can be inferred from the meaning of T, as would typically be interpreted by people.” Although this relation does not necessarily require the presence of corresponding predicates, previous work by MacCartney et al. (2008) shows that word alignments can serve as a good indicator of entailment. 172 As a matter of fact, the same holds true for the task of detecting paraphrases. In contrast to RT</context>
</contexts>
<marker>Dagan, Glickman, Magnini, 2006</marker>
<rawString>Ido Dagan, Oren Glickman, and Bernardo Magnini. 2006. The PASCAL recognising textual entailment challenge. In J. Qui˜nonero-Candela, I. Dagan, and B. Magnini, editors, Machine Learning Challenges, pages 177–190. Springer, Heidelberg, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William B Dolan</author>
<author>Chris Brockett</author>
</authors>
<title>Automatically constructing a corpus of sentential paraphrases.</title>
<date>2005</date>
<booktitle>In Proceedings of the Third International Workshop on Paraphrasing.</booktitle>
<contexts>
<context position="8744" citStr="Dolan and Brockett, 2005" startWordPosition="1361" endWordPosition="1364">cessarily require the presence of corresponding predicates, previous work by MacCartney et al. (2008) shows that word alignments can serve as a good indicator of entailment. 172 As a matter of fact, the same holds true for the task of detecting paraphrases. In contrast to RTE, this latter task requires bi-directional entailments, i.e., each of the two phrases must entail the other. Wan et al. (2006) show that a simple approach solely based on word (and lemmatized n-gram) overlap can already achieve an Fl-score of up to 83% for detecting paraphrases in the Microsoft Research Paraphrase Corpus (Dolan and Brockett, 2005, MSRPC). In fact, this is just 0.6% points below the state-of-the-art results recently reported by Socher et al. (2011). The MSRPC and data sets from the first RTE challenges only consisted of isolated pairs of sentences. The Fifth PASCAL Recognizing Textual Entailment Challenge (Bentivogli et al., 2009) introduced a “Search Task”, where entailing sentences for a hypothesis have to be found in a set of full documents. This new task first opened the doors for assessing the role of discourse (Mirkin et al., 2010a; Mirkin et al., 2010b) in RTE. However, this setting is still limited as discourse</context>
<context position="26549" citStr="Dolan and Brockett, 2005" startWordPosition="4262" endWordPosition="4265">entencebased word alignment setting and our novel alignment task that operates on full texts. 5.1.1 Sentence-level Alignment Setting For sentence-based predicate alignment we make use of the following three corpora that are wordaligned subsets of the paraphrase collections described in (Cohn et al., 2008): MTC consists of 100 sentence pairs from the Multiple-Translation Chinese Corpus (Huang et al., 2002), Leagues contains 100 sentential paraphrases from two translations of Jules Verne’s “Twenty Thousand Leagues Under the Sea”, and MSR is a sub-set of the Microsoft Research Paraphrase Corpus (Dolan and Brockett, 2005), consisting of 130 sentence pairs. All three paraphrase collections are in English. Results for these experiments are reported in Section 5.3.1. Note that in order to determine alignment candidates, we apply the same pre-processing steps as used for the annotation of our corpus. The semantic parser identified an average number of 3.8, 5.1 and 4.7 predicates per text (i.e., per paraphrase sentence) in MTC, Leagues and MSR, respectively. All models are evaluated against the subset of gold standard alignments (cf. Cohn et al. (2008)) between pairs of words marked as predicates. 5.1.2 Text-level </context>
</contexts>
<marker>Dolan, Brockett, 2005</marker>
<rawString>William B. Dolan and Chris Brockett. 2005. Automatically constructing a corpus of sentential paraphrases. In Proceedings of the Third International Workshop on Paraphrasing.</rawString>
</citation>
<citation valid="true">
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<editor>Christiane Fellbaum, editor.</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, Mass.</location>
<contexts>
<context position="18937" citStr="(1998)" startWordPosition="3020" endWordPosition="3020"> default. In Section 5, we define an optimized weighting setting for the individual similarity measures. 4.2 Similarity Measures We employ a number of similarity measures that make use of complementary information that is type-based (simWN/VN/Dist) or token-based (simArg).6 Given two lemmatized predicates p1, p2 and their set of arguments A1 = args(p1), A2 = args(p2), we define the following measures. WordNet similarity. Given all pairs of synsets s1, s2 that contain the predicates p1, p2, respectively, we compute the maximal similarity using the information theoretic measure described in Lin (1998). Our implementation exploits the WordNet hierarchy 6All token-based frequency counts (i.e., freqO and idfO) are computed over all documents from the AFP and APW parts of the English Gigaword Fifth Edition. (Fellbaum, 1998) to find the synset of the least common subsumer (lcs) and uses the pre-computed Information Content (IC) files from Pedersen et al. (2004) to compute Lin’s measure: IC(lcs(s1, s2)) simWN(p1,p2) = (3) IC(s1) * IC(s2) In order to compute similarities between verbal and nominal predicates, we further use derivation information from NomBank (Meyers et al., 2008): if a noun repr</context>
</contexts>
<marker>1998</marker>
<rawString>Christiane Fellbaum, editor. 1998. WordNet: An Electronic Lexical Database. MIT Press, Cambridge, Mass.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adrew V Goldberg</author>
<author>Robert E Tarjan</author>
</authors>
<title>A new approach to the maximum flow problem.</title>
<date>1986</date>
<booktitle>In Proceedings of the eighteenth annual ACM symposium on Theory of computing,</booktitle>
<pages>136--146</pages>
<location>New York, NY, USA.</location>
<contexts>
<context position="22792" citStr="Goldberg and Tarjan (1986)" startWordPosition="3659" endWordPosition="3662"> to have a mean and standard deviation of 1.0. 4.3 Mincut-based Clustering Our graph clustering method uses minimum cuts (or Mincut) in order to partition the bipartite text graph into clusters of aligned predicates. A Mincut operation divides a given graph into two disjoint subgraphs. Each minimum cut is performed as a cut between some source node s and some target node t, such that (i) each of the two nodes will be in a different sub-graph and (ii) the sum of weights of all removed edges will be as small as possible. Our system determines each Mincut using an implementation of the method by Goldberg and Tarjan (1986).7 7Basic graph operations are performed using the freely available Java library JGraph, cf. http://jgrapht.org/. function CLUSTER(G) clusters 0 E GETEDGES(G) . Step 1 e GETEDGEWITHLOWESTWEIGHT(E) s GETSOURCENODE(e) t GETTARGETNODE(e) G&apos; MINCUT(G, s, t) . Step 2 C GETCONNECTEDCOMPONENTS(G&apos;) for all G3 E C do . Step 3 if SIZE(G3) &lt;= 2 then clusters clusters U G3 else clusters clusters U CLUSTER(G3) end if end for return clusters; end function Figure 2: Pseudo code of our clustering algorithm As our goal is to induce clusters that correspond to pairs of similar predicates, we set a maximum numbe</context>
</contexts>
<marker>Goldberg, Tarjan, 1986</marker>
<rawString>Adrew V. Goldberg and Robert E. Tarjan. 1986. A new approach to the maximum flow problem. In Proceedings of the eighteenth annual ACM symposium on Theory of computing, pages 136–146, New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara J Grosz</author>
<author>Aravind K Joshi</author>
<author>Scott Weinstein</author>
</authors>
<title>Centering: A framework for modeling the local coherence of discourse.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<volume>21</volume>
<issue>2</issue>
<contexts>
<context position="1355" citStr="Grosz et al., 1995" startWordPosition="191" endWordPosition="194">urce of comparable texts, including an evaluation set with manual predicate alignments. Secondly, we present a novel approach for aligning predicates across comparable texts using graph-based clustering with Mincuts. Our method significantly outperforms other alignment techniques when applied to this novel alignment task, by a margin of at least 6.5 percentage points in Fl-score. 1 Introduction Discourse coherence is an important aspect in natural language generation (NLG) applications. A number of theories have investigated coherence inducing factors. A prominent example is Centering Theory (Grosz et al., 1995), which models local coherence by relating the choice of referring expressions to the importance of an entity at a certain stage of a discourse. A data-driven model based on this theory is the entity-based approach by Barzilay and Lapata (2008), which models coherence phenomena by observing sentence-to-sentence transitions of entity occurrences. Barzilay and Lapata show that their approach can discriminate between a coherent and a non-coherent set of ordered sentences. However, their model is not able to generate alternative entity realizations by itself. Furthermore, the entity-based approach</context>
</contexts>
<marker>Grosz, Joshi, Weinstein, 1995</marker>
<rawString>Barbara J. Grosz, Aravind K. Joshi, and Scott Weinstein. 1995. Centering: A framework for modeling the local coherence of discourse. Computational Linguistics, 21(2):203–225.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Weiwei Guo</author>
<author>Mona Diab</author>
</authors>
<title>Semantic topic models: Combining word distributional statistics and dictionary definitions.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>552--561</pages>
<contexts>
<context position="20831" citStr="Guo and Diab, 2011" startWordPosition="3325" endWordPosition="3328">We define a simple similarity function that defines fixed similarity scores between 0 and 1 for pairs of predicates p1, p2 depending on their relatedness within the VerbNet class hierarchy: simVN(p1,p2) = I 1.0 if EIC : p1,p2 E C (4) 0.8 if EIC, Cs : Cs E sub(C) ∧ p1,p2 E C U Cs 0.0 else Distributional similarity. As some predicates may not be covered by the WordNet and VerbNet hierarchies, we additionally calculate similarity based on distributional meaning in a semantic space (Landauer and Dumais, 1997). Following the traditional bag-of-words approach that has been applied in related tasks (Guo and Diab, 2011; Mitchell and Lapata, 2010), we consider the 2,000 most frequent context words c1, ... , c2000 E C as dimensions of a vector space and define predicates as vectors using their Pointwise Mutual Information (PMI): p~ = (PMI(p, c1), ... , PMI(p, c2000) (5) (2) 175 freq(x, y) with PMI(x,y) = freq(x) * freq(y) Given the vector representations of two predicates, we calculate their similarity as the cosine of the angle between the two vectors: Argument similarity. While the previous similarity measures are purely type-based, argument similarity integrates token-based, i.e., discourse-specific, simil</context>
</contexts>
<marker>Guo, Diab, 2011</marker>
<rawString>Weiwei Guo and Mona Diab. 2011. Semantic topic models: Combining word distributional statistics and dictionary definitions. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 552–561, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shudong Huang</author>
<author>David Graff</author>
<author>George Doddington</author>
</authors>
<title>Multiple-Translation Chinese Corpus. Linguistic Data Consortium,</title>
<date>2002</date>
<location>Philadelphia.</location>
<contexts>
<context position="26332" citStr="Huang et al., 2002" startWordPosition="4230" endWordPosition="4233">ns that omit individual similarity measures (Full–[measure name]). The relative differences in performance against various baselines will help us quantify the differences and difficulties between a traditional sentencebased word alignment setting and our novel alignment task that operates on full texts. 5.1.1 Sentence-level Alignment Setting For sentence-based predicate alignment we make use of the following three corpora that are wordaligned subsets of the paraphrase collections described in (Cohn et al., 2008): MTC consists of 100 sentence pairs from the Multiple-Translation Chinese Corpus (Huang et al., 2002), Leagues contains 100 sentential paraphrases from two translations of Jules Verne’s “Twenty Thousand Leagues Under the Sea”, and MSR is a sub-set of the Microsoft Research Paraphrase Corpus (Dolan and Brockett, 2005), consisting of 130 sentence pairs. All three paraphrase collections are in English. Results for these experiments are reported in Section 5.3.1. Note that in order to determine alignment candidates, we apply the same pre-processing steps as used for the annotation of our corpus. The semantic parser identified an average number of 3.8, 5.1 and 4.7 predicates per text (i.e., per pa</context>
</contexts>
<marker>Huang, Graff, Doddington, 2002</marker>
<rawString>Shudong Huang, David Graff, and George Doddington. 2002. Multiple-Translation Chinese Corpus. Linguistic Data Consortium, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karin Kipper</author>
<author>Anna Korhonen</author>
<author>Neville Ryant</author>
<author>Martha Palmer</author>
</authors>
<date>2008</date>
<journal>A Large-scale Classification of English Verbs.</journal>
<volume>42</volume>
<issue>1</issue>
<contexts>
<context position="20003" citStr="Kipper et al., 2008" startWordPosition="3187" endWordPosition="3190">er to compute similarities between verbal and nominal predicates, we further use derivation information from NomBank (Meyers et al., 2008): if a noun represents a nominalization of a verbal predicate, we resort to the corresponding verb synset. If no relation can be found between two predicates, we set a default value of simWN = 0. This applies in particular to all cases that involve a predicate not present in WordNet. VerbNet similarity. To overcome systematic problems with the WordNet verb hierarchy (cf. Richens (2008)), we further compute similarity between verbal predicates using VerbNet (Kipper et al., 2008). Verbs in VerbNet are categorized into semantic classes according to their syntactic behavior. A class C can recursively embed sub-classes C3 E sub(C) that represent finer semantic and syntactic distinctions. We define a simple similarity function that defines fixed similarity scores between 0 and 1 for pairs of predicates p1, p2 depending on their relatedness within the VerbNet class hierarchy: simVN(p1,p2) = I 1.0 if EIC : p1,p2 E C (4) 0.8 if EIC, Cs : Cs E sub(C) ∧ p1,p2 E C U Cs 0.0 else Distributional similarity. As some predicates may not be covered by the WordNet and VerbNet hierarchi</context>
</contexts>
<marker>Kipper, Korhonen, Ryant, Palmer, 2008</marker>
<rawString>Karin Kipper, Anna Korhonen, Neville Ryant, and Martha Palmer. 2008. A Large-scale Classification of English Verbs. 42(1):21–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas K Landauer</author>
<author>Susan T Dumais</author>
</authors>
<title>A solution to Plato’s problem: The Latent Semantic Analysis theory of the acquisition, induction, and representation of knowledge.</title>
<date>1997</date>
<journal>Psychological Review,</journal>
<volume>104</volume>
<pages>240</pages>
<contexts>
<context position="20723" citStr="Landauer and Dumais, 1997" startWordPosition="3307" endWordPosition="3311">. A class C can recursively embed sub-classes C3 E sub(C) that represent finer semantic and syntactic distinctions. We define a simple similarity function that defines fixed similarity scores between 0 and 1 for pairs of predicates p1, p2 depending on their relatedness within the VerbNet class hierarchy: simVN(p1,p2) = I 1.0 if EIC : p1,p2 E C (4) 0.8 if EIC, Cs : Cs E sub(C) ∧ p1,p2 E C U Cs 0.0 else Distributional similarity. As some predicates may not be covered by the WordNet and VerbNet hierarchies, we additionally calculate similarity based on distributional meaning in a semantic space (Landauer and Dumais, 1997). Following the traditional bag-of-words approach that has been applied in related tasks (Guo and Diab, 2011; Mitchell and Lapata, 2010), we consider the 2,000 most frequent context words c1, ... , c2000 E C as dimensions of a vector space and define predicates as vectors using their Pointwise Mutual Information (PMI): p~ = (PMI(p, c1), ... , PMI(p, c2000) (5) (2) 175 freq(x, y) with PMI(x,y) = freq(x) * freq(y) Given the vector representations of two predicates, we calculate their similarity as the cosine of the angle between the two vectors: Argument similarity. While the previous similarity</context>
</contexts>
<marker>Landauer, Dumais, 1997</marker>
<rawString>Thomas K. Landauer and Susan T. Dumais. 1997. A solution to Plato’s problem: The Latent Semantic Analysis theory of the acquisition, induction, and representation of knowledge. Psychological Review, 104:211– 240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Benjamin Taskar</author>
<author>Dan Klein</author>
</authors>
<title>Alignment by agreement.</title>
<date>2006</date>
<booktitle>In North American Association for Computational Linguistics (NAACL),</booktitle>
<pages>104--111</pages>
<contexts>
<context position="29524" citStr="Liang et al., 2006" startWordPosition="4743" endWordPosition="4746">tead, it greedily computes as many 1-to-1 alignments as possible, starting from the highest similarity to the learned threshold (Greedy). As a more sophisticated baseline, we make use of alignment tools commonly used in statistical machine translation (SMT). For the three sentence-based paraphrase settings MTC, Leagues and MSR, Cohn et al. (2008) readily provide GIZA++ (Och and Ney, 2003) alignments as part of their word-aligned paraphrase corpus. For the experiments in the GigaPairs setting, we train our own word alignment model using the state-of-theart word alignment tool Berkeley Aligner (Liang et al., 2006). As word alignment tools require pairs of sentences as input, we first extract paraphrases in the latter setting using a re-implementation of the paraphrase detection system by Wan et al. (2006).8 In the following section, we abbreviate both baselines using SMT alignment tools as WordAlign. 8Note that the performance of this system lies slightly below the state-of-the-art results reported by Socher et al. (2011) However, we were not able to reproduce the results of Socher et al. using the publicly available release of their software. 5.3 Results We measure precision as the number of predicted</context>
</contexts>
<marker>Liang, Taskar, Klein, 2006</marker>
<rawString>Percy Liang, Benjamin Taskar, and Dan Klein. 2006. Alignment by agreement. In North American Association for Computational Linguistics (NAACL), pages 104–111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>An information-theoretic definition of similarity.</title>
<date>1998</date>
<booktitle>In Proceedings of the 15th International Conference on Machine Learning,</booktitle>
<pages>296--304</pages>
<location>Madison, Wisc.,</location>
<contexts>
<context position="18937" citStr="Lin (1998)" startWordPosition="3019" endWordPosition="3020">s by default. In Section 5, we define an optimized weighting setting for the individual similarity measures. 4.2 Similarity Measures We employ a number of similarity measures that make use of complementary information that is type-based (simWN/VN/Dist) or token-based (simArg).6 Given two lemmatized predicates p1, p2 and their set of arguments A1 = args(p1), A2 = args(p2), we define the following measures. WordNet similarity. Given all pairs of synsets s1, s2 that contain the predicates p1, p2, respectively, we compute the maximal similarity using the information theoretic measure described in Lin (1998). Our implementation exploits the WordNet hierarchy 6All token-based frequency counts (i.e., freqO and idfO) are computed over all documents from the AFP and APW parts of the English Gigaword Fifth Edition. (Fellbaum, 1998) to find the synset of the least common subsumer (lcs) and uses the pre-computed Information Content (IC) files from Pedersen et al. (2004) to compute Lin’s measure: IC(lcs(s1, s2)) simWN(p1,p2) = (3) IC(s1) * IC(s2) In order to compute similarities between verbal and nominal predicates, we further use derivation information from NomBank (Meyers et al., 2008): if a noun repr</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. An information-theoretic definition of similarity. In Proceedings of the 15th International Conference on Machine Learning, Madison, Wisc., 24–27 July 1998, pages 296–304.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bill MacCartney</author>
<author>Michael Galley</author>
<author>Christopher D Manning</author>
</authors>
<title>A phrase-based alignment model for natural language inference.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<location>Waikiki, Honolulu, Hawaii,</location>
<contexts>
<context position="8221" citStr="MacCartney et al. (2008)" startWordPosition="1270" endWordPosition="1273">s is found in the area of textual inference. Since 2006, there have been regular challenges on the task of Recognizing Textual Entailment (RTE). In the original task description, Dagan et al. (2006) define textual entailment “as a directional relationship between pairs of text expressions, denoted by T - the entailing ‘Text’ -, and H - the entailed ‘Hypothesis’. (... ) T entails H if the meaning of H can be inferred from the meaning of T, as would typically be interpreted by people.” Although this relation does not necessarily require the presence of corresponding predicates, previous work by MacCartney et al. (2008) shows that word alignments can serve as a good indicator of entailment. 172 As a matter of fact, the same holds true for the task of detecting paraphrases. In contrast to RTE, this latter task requires bi-directional entailments, i.e., each of the two phrases must entail the other. Wan et al. (2006) show that a simple approach solely based on word (and lemmatized n-gram) overlap can already achieve an Fl-score of up to 83% for detecting paraphrases in the Microsoft Research Paraphrase Corpus (Dolan and Brockett, 2005, MSRPC). In fact, this is just 0.6% points below the state-of-the-art result</context>
</contexts>
<marker>MacCartney, Galley, Manning, 2008</marker>
<rawString>Bill MacCartney, Michael Galley, and Christopher D. Manning. 2008. A phrase-based alignment model for natural language inference. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, Waikiki, Honolulu, Hawaii, 25-27 October 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Meyers</author>
<author>Ruth Reeves</author>
<author>Catherine Macleod</author>
</authors>
<date>2008</date>
<booktitle>NomBank v1.0. Linguistic Data Consortium,</booktitle>
<location>Philadelphia.</location>
<contexts>
<context position="12514" citStr="Meyers et al., 2008" startWordPosition="1973" endWordPosition="1976">to further increase the precision of extracted pairs of texts. Random inspection of about 100 documents revealed only two texts describing different events. Overall, we extracted 167,728 document pairs containing a total of 50 million word tokens. Each document in this corpus consists of up to 7.564 words with a mean and median of 301 and 213 words, respectively. All texts have been pre-processed using MATE tools (Bj¨orkelund et al., 2010; Bohnet, 2010), a pipeline of NLP modules including a stateof-the-art semantic role labeler that computes PropBank/NomBank annotations (Palmer et al., 2005; Meyers et al., 2008). 3.2 Gold Standard Annotation We selected 70 text pairs from the GigaPairs corpus for manual predicate alignment. All document pairs were randomly chosen with the constraint that each text consists of 100 to 300 words.3 Predi3This constraint is satisfied by 75.3% of all documents in GigaPairs. 173 cates identified by the semantic parser are provided as pre-labeled annotations for alignment. We asked two students4 to tag corresponding predicates across each text pair. Following standard practice in word alignment tasks (cf. Cohn et al. (2008)) the annotators were instructed to distinguish betw</context>
<context position="19521" citStr="Meyers et al., 2008" startWordPosition="3109" endWordPosition="3112">etic measure described in Lin (1998). Our implementation exploits the WordNet hierarchy 6All token-based frequency counts (i.e., freqO and idfO) are computed over all documents from the AFP and APW parts of the English Gigaword Fifth Edition. (Fellbaum, 1998) to find the synset of the least common subsumer (lcs) and uses the pre-computed Information Content (IC) files from Pedersen et al. (2004) to compute Lin’s measure: IC(lcs(s1, s2)) simWN(p1,p2) = (3) IC(s1) * IC(s2) In order to compute similarities between verbal and nominal predicates, we further use derivation information from NomBank (Meyers et al., 2008): if a noun represents a nominalization of a verbal predicate, we resort to the corresponding verb synset. If no relation can be found between two predicates, we set a default value of simWN = 0. This applies in particular to all cases that involve a predicate not present in WordNet. VerbNet similarity. To overcome systematic problems with the WordNet verb hierarchy (cf. Richens (2008)), we further compute similarity between verbal predicates using VerbNet (Kipper et al., 2008). Verbs in VerbNet are categorized into semantic classes according to their syntactic behavior. A class C can recursiv</context>
</contexts>
<marker>Meyers, Reeves, Macleod, 2008</marker>
<rawString>Adam Meyers, Ruth Reeves, and Catherine Macleod. 2008. NomBank v1.0. Linguistic Data Consortium, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shachar Mirkin</author>
<author>Jonathan Berant</author>
<author>Ido Dagan</author>
<author>Eyal Shnarch</author>
</authors>
<title>Recognising entailment within discourse.</title>
<date>2010</date>
<journal>Organizing Committee.</journal>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010),</booktitle>
<location>Beijing, China,</location>
<contexts>
<context position="9260" citStr="Mirkin et al., 2010" startWordPosition="1449" endWordPosition="1452"> to 83% for detecting paraphrases in the Microsoft Research Paraphrase Corpus (Dolan and Brockett, 2005, MSRPC). In fact, this is just 0.6% points below the state-of-the-art results recently reported by Socher et al. (2011). The MSRPC and data sets from the first RTE challenges only consisted of isolated pairs of sentences. The Fifth PASCAL Recognizing Textual Entailment Challenge (Bentivogli et al., 2009) introduced a “Search Task”, where entailing sentences for a hypothesis have to be found in a set of full documents. This new task first opened the doors for assessing the role of discourse (Mirkin et al., 2010a; Mirkin et al., 2010b) in RTE. However, this setting is still limited as discourse contexts are only provided for the entailing part (T) of each text pair but not for the hypothesis H. A further task related to ours is the detection of event coreference. The goal of this task is to identify all mentions of the same event within a document and, in some settings, also across documents. However, the task setting is typically more restricted than ours in that its focus lies on identical events/references (cf. Walker et al. (2006), Weischedel et al. (2011), inter alia). In particular, verbalizati</context>
</contexts>
<marker>Mirkin, Berant, Dagan, Shnarch, 2010</marker>
<rawString>Shachar Mirkin, Jonathan Berant, Ido Dagan, and Eyal Shnarch. 2010a. Recognising entailment within discourse. In Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), Beijing, China, August. Coling 2010 Organizing Committee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shachar Mirkin</author>
<author>Ido Dagan</author>
<author>Sebastian Pad´o</author>
</authors>
<title>Assessing the role of discourse references in entailment inference.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Uppsala,</location>
<marker>Mirkin, Dagan, Pad´o, 2010</marker>
<rawString>Shachar Mirkin, Ido Dagan, and Sebastian Pad´o. 2010b. Assessing the role of discourse references in entailment inference. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, Uppsala, Sweden, 11–16 July 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Mitchell</author>
<author>Mirella Lapata</author>
</authors>
<date>2010</date>
<booktitle>Composition in Distributional Models of Semantics.</booktitle>
<volume>34</volume>
<issue>8</issue>
<pages>1429</pages>
<contexts>
<context position="20859" citStr="Mitchell and Lapata, 2010" startWordPosition="3329" endWordPosition="3333">imilarity function that defines fixed similarity scores between 0 and 1 for pairs of predicates p1, p2 depending on their relatedness within the VerbNet class hierarchy: simVN(p1,p2) = I 1.0 if EIC : p1,p2 E C (4) 0.8 if EIC, Cs : Cs E sub(C) ∧ p1,p2 E C U Cs 0.0 else Distributional similarity. As some predicates may not be covered by the WordNet and VerbNet hierarchies, we additionally calculate similarity based on distributional meaning in a semantic space (Landauer and Dumais, 1997). Following the traditional bag-of-words approach that has been applied in related tasks (Guo and Diab, 2011; Mitchell and Lapata, 2010), we consider the 2,000 most frequent context words c1, ... , c2000 E C as dimensions of a vector space and define predicates as vectors using their Pointwise Mutual Information (PMI): p~ = (PMI(p, c1), ... , PMI(p, c2000) (5) (2) 175 freq(x, y) with PMI(x,y) = freq(x) * freq(y) Given the vector representations of two predicates, we calculate their similarity as the cosine of the angle between the two vectors: Argument similarity. While the previous similarity measures are purely type-based, argument similarity integrates token-based, i.e., discourse-specific, similarity information about pred</context>
</contexts>
<marker>Mitchell, Lapata, 2010</marker>
<rawString>Jeff Mitchell and Mirella Lapata. 2010. Composition in Distributional Models of Semantics. 34(8):1388– 1429.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A Systematic Comparison of Various Statistical Alignment Models.</title>
<date>2003</date>
<contexts>
<context position="29296" citStr="Och and Ney, 2003" startWordPosition="4708" endWordPosition="4711">settings. In order to assess the benefits of the clustering step, we propose a second baseline that uses the same similarity measures and thresholds as our Full model, but omits the clustering step described in Section 4.3. Instead, it greedily computes as many 1-to-1 alignments as possible, starting from the highest similarity to the learned threshold (Greedy). As a more sophisticated baseline, we make use of alignment tools commonly used in statistical machine translation (SMT). For the three sentence-based paraphrase settings MTC, Leagues and MSR, Cohn et al. (2008) readily provide GIZA++ (Och and Ney, 2003) alignments as part of their word-aligned paraphrase corpus. For the experiments in the GigaPairs setting, we train our own word alignment model using the state-of-theart word alignment tool Berkeley Aligner (Liang et al., 2006). As word alignment tools require pairs of sentences as input, we first extract paraphrases in the latter setting using a re-implementation of the paraphrase detection system by Wan et al. (2006).8 In the following section, we abbreviate both baselines using SMT alignment tools as WordAlign. 8Note that the performance of this system lies slightly below the state-of-the-</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A Systematic Comparison of Various Statistical Alignment Models. 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Daniel Gildea</author>
<author>Paul Kingsbury</author>
</authors>
<title>The proposition bank: An annotated corpus of semantic roles.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>1</issue>
<pages>105</pages>
<contexts>
<context position="12492" citStr="Palmer et al., 2005" startWordPosition="1969" endWordPosition="1972">onal date constraint to further increase the precision of extracted pairs of texts. Random inspection of about 100 documents revealed only two texts describing different events. Overall, we extracted 167,728 document pairs containing a total of 50 million word tokens. Each document in this corpus consists of up to 7.564 words with a mean and median of 301 and 213 words, respectively. All texts have been pre-processed using MATE tools (Bj¨orkelund et al., 2010; Bohnet, 2010), a pipeline of NLP modules including a stateof-the-art semantic role labeler that computes PropBank/NomBank annotations (Palmer et al., 2005; Meyers et al., 2008). 3.2 Gold Standard Annotation We selected 70 text pairs from the GigaPairs corpus for manual predicate alignment. All document pairs were randomly chosen with the constraint that each text consists of 100 to 300 words.3 Predi3This constraint is satisfied by 75.3% of all documents in GigaPairs. 173 cates identified by the semantic parser are provided as pre-labeled annotations for alignment. We asked two students4 to tag corresponding predicates across each text pair. Following standard practice in word alignment tasks (cf. Cohn et al. (2008)) the annotators were instruct</context>
</contexts>
<marker>Palmer, Gildea, Kingsbury, 2005</marker>
<rawString>Martha Palmer, Daniel Gildea, and Paul Kingsbury. 2005. The proposition bank: An annotated corpus of semantic roles. Computational Linguistics, 31(1):71– 105.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Parker</author>
<author>David Graff</author>
<author>Jumbo Kong</author>
<author>Ke Chen</author>
<author>Kazuaki Maeda</author>
</authors>
<title>English Gigaword Fifth Edition. Linguistic Data Consortium,</title>
<date>2011</date>
<location>Philadelphia.</location>
<contexts>
<context position="11318" citStr="Parker et al., 2011" startWordPosition="1781" endWordPosition="1784">dy of coherence phenomena. More detailed information regarding corpus creation, annotation guidelines and additional examples illustrating the potential of this corpus can be found in Roth and Frank (2012). 3.1 Corpus Creation The goal of our work is to investigate coherence factors for argument structure realization, using comparable texts that describe the same events, but that include variation in textual presentation. This requirement fits well with the news domain, for which we can trace varying textual sources that describe the same underlying events. The English Gigaword Fifth Edition (Parker et al., 2011) corpus (henceforth just Gigaword) is one of the largest corpus collections for English. It comprises a total of 9.8 million newswire articles from seven distinct sources. In previous work (Roth and Frank, 2012), we introduced GigaPairs, a sub-corpus extracted from Gigaword that includes over 160,000 pairs of newswire articles from distinct sources. GigaPairs has been derived from Gigaword using the pairwise similarity method on headlines presented by Wubben et al. (2009). In addition to calculating the similarity of news titles, we impose an additional date constraint to further increase the </context>
</contexts>
<marker>Parker, Graff, Kong, Chen, Maeda, 2011</marker>
<rawString>Robert Parker, David Graff, Jumbo Kong, Ke Chen, and Kazuaki Maeda. 2011. English Gigaword Fifth Edition. Linguistic Data Consortium, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Pedersen</author>
<author>Siddharth Patwardhan</author>
<author>Jason Michelizzi</author>
</authors>
<title>WordNet::Similarity – Measuring the relatedness of concepts.</title>
<date>2004</date>
<booktitle>In Companion Volume to the Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>267--270</pages>
<location>Boston, Mass.,</location>
<contexts>
<context position="19299" citStr="Pedersen et al. (2004)" startWordPosition="3075" endWordPosition="3078">args(p1), A2 = args(p2), we define the following measures. WordNet similarity. Given all pairs of synsets s1, s2 that contain the predicates p1, p2, respectively, we compute the maximal similarity using the information theoretic measure described in Lin (1998). Our implementation exploits the WordNet hierarchy 6All token-based frequency counts (i.e., freqO and idfO) are computed over all documents from the AFP and APW parts of the English Gigaword Fifth Edition. (Fellbaum, 1998) to find the synset of the least common subsumer (lcs) and uses the pre-computed Information Content (IC) files from Pedersen et al. (2004) to compute Lin’s measure: IC(lcs(s1, s2)) simWN(p1,p2) = (3) IC(s1) * IC(s2) In order to compute similarities between verbal and nominal predicates, we further use derivation information from NomBank (Meyers et al., 2008): if a noun represents a nominalization of a verbal predicate, we resort to the corresponding verb synset. If no relation can be found between two predicates, we set a default value of simWN = 0. This applies in particular to all cases that involve a predicate not present in WordNet. VerbNet similarity. To overcome systematic problems with the WordNet verb hierarchy (cf. Rich</context>
</contexts>
<marker>Pedersen, Patwardhan, Michelizzi, 2004</marker>
<rawString>Ted Pedersen, Siddharth Patwardhan, and Jason Michelizzi. 2004. WordNet::Similarity – Measuring the relatedness of concepts. In Companion Volume to the Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics, Boston, Mass., 2–7 May 2004, pages 267–270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom Richens</author>
</authors>
<title>Anomalies in the wordnet verb hierarchy.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics (Coling</booktitle>
<pages>729--736</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="19909" citStr="Richens (2008)" startWordPosition="3176" endWordPosition="3177">004) to compute Lin’s measure: IC(lcs(s1, s2)) simWN(p1,p2) = (3) IC(s1) * IC(s2) In order to compute similarities between verbal and nominal predicates, we further use derivation information from NomBank (Meyers et al., 2008): if a noun represents a nominalization of a verbal predicate, we resort to the corresponding verb synset. If no relation can be found between two predicates, we set a default value of simWN = 0. This applies in particular to all cases that involve a predicate not present in WordNet. VerbNet similarity. To overcome systematic problems with the WordNet verb hierarchy (cf. Richens (2008)), we further compute similarity between verbal predicates using VerbNet (Kipper et al., 2008). Verbs in VerbNet are categorized into semantic classes according to their syntactic behavior. A class C can recursively embed sub-classes C3 E sub(C) that represent finer semantic and syntactic distinctions. We define a simple similarity function that defines fixed similarity scores between 0 and 1 for pairs of predicates p1, p2 depending on their relatedness within the VerbNet class hierarchy: simVN(p1,p2) = I 1.0 if EIC : p1,p2 E C (4) 0.8 if EIC, Cs : Cs E sub(C) ∧ p1,p2 E C U Cs 0.0 else Distrib</context>
</contexts>
<marker>Richens, 2008</marker>
<rawString>Tom Richens. 2008. Anomalies in the wordnet verb hierarchy. In Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 729–736. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Roth</author>
<author>Anette Frank</author>
</authors>
<title>Aligning predicate argument structures in monolingual comparable texts: A new corpus for a new task.</title>
<date>2012</date>
<booktitle>In Proceedings of the First Joint Conference on Lexical and Computational Semantics,</booktitle>
<location>Montreal, Canada,</location>
<contexts>
<context position="6033" citStr="Roth and Frank, 2012" startWordPosition="914" endWordPosition="917">able corpus of paired texts. The induced alignments will (i) serve to identify events described in both comparable texts, and (ii) provide information about the underlying argument structures and how they are realized in each context to establish a coherent discourse. We investigate a graph-based clustering method for induc1See the recent SemEval 2010 task: Linking Events and their Participants in Discourse, (Ruppenhofer et al., 2010). 2Note that we provide details regarding the construction of a suitable data set and further examples involving non-realized arguments in a complementary paper (Roth and Frank, 2012). ing such alignments as clustering provides a suitable framework to implicitly relate alignment decisions to one another, by exploiting global information encoded in a graph. The remainder of this paper is structured as follows: In Section 2, we discuss previous work in related tasks. Section 3 describes our task and a suitable data set. Section 4 introduces a graph-based clustering model using Mincuts for the alignment of predicates. Section 5 outlines the experiments and presents evaluation results. Finally, we conclude in Section 6 and discuss future work. 2 Related Work The task of aligni</context>
<context position="10903" citStr="Roth and Frank (2012)" startWordPosition="1715" endWordPosition="1718"> a large corpus of comparable texts, as a basis for the predicate alignment task. We motivate the choice of the corpus and present a strategy for extracting comparable text pairs. Subsequently, we report on the preparation of an evaluation data set with manual predicate alignments across the paired texts. We conclude this section with an example that showcases the potential of using aligned predicates for the study of coherence phenomena. More detailed information regarding corpus creation, annotation guidelines and additional examples illustrating the potential of this corpus can be found in Roth and Frank (2012). 3.1 Corpus Creation The goal of our work is to investigate coherence factors for argument structure realization, using comparable texts that describe the same events, but that include variation in textual presentation. This requirement fits well with the news domain, for which we can trace varying textual sources that describe the same underlying events. The English Gigaword Fifth Edition (Parker et al., 2011) corpus (henceforth just Gigaword) is one of the largest corpus collections for English. It comprises a total of 9.8 million newswire articles from seven distinct sources. In previous w</context>
<context position="16171" citStr="Roth and Frank (2012)" startWordPosition="2575" endWordPosition="2578">some 20,000 refugeesArg0 have fled into CameroonArg1. In both sentences, the Arg0 role of the predicate flee is filled, but Arg1 (here: the goal) has not been realized in (4.a). However, sentence (4.a) is still part of a coherent discourse, as a role filler for the omitted argument can be inferred from the preceding context. For the goal of our work, we are interested in factors that license such omissions of an argument. Potential factors on the discourse level include the information status of the entity filling an argument position, and its salience at the corresponding point in discourse. Roth and Frank (2012) discuss additional examples that demonstrate the importance of factors on further linguistic levels, e.g., lexical choice of predicates and their syntactic realization. In the example above, the aggregation of aligned PAS presents an effective means to identify appropriate fillers for unrealized roles. Hence, we can utilize each such pair as one positive and one negative training instance for a model of discourse coherence that controls the omissibility of arguments. In what follows, we introduce an alignment approach that can be used to automatically acquire more training data using the enti</context>
</contexts>
<marker>Roth, Frank, 2012</marker>
<rawString>Michael Roth and Anette Frank. 2012. Aligning predicate argument structures in monolingual comparable texts: A new corpus for a new task. In Proceedings of the First Joint Conference on Lexical and Computational Semantics, Montreal, Canada, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Josef Ruppenhofer</author>
<author>Caroline Sporleder</author>
<author>Roser Morante</author>
<author>Collin Baker</author>
<author>Martha Palmer</author>
</authors>
<title>SemEval2010 Task 10: Linking Events and Their Participants in Discourse.</title>
<date>2010</date>
<booktitle>In Proceedings of the 5th International Workshop on Semantic Evaluations,</booktitle>
<pages>45--50</pages>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="5850" citStr="Ruppenhofer et al., 2010" startWordPosition="887" endWordPosition="890">edications. This paper focuses on the first of these tasks, henceforth called predicate alignment.2 In line with data-driven approaches in NLP, we automatically align predicates in a suitable corpus of paired texts. The induced alignments will (i) serve to identify events described in both comparable texts, and (ii) provide information about the underlying argument structures and how they are realized in each context to establish a coherent discourse. We investigate a graph-based clustering method for induc1See the recent SemEval 2010 task: Linking Events and their Participants in Discourse, (Ruppenhofer et al., 2010). 2Note that we provide details regarding the construction of a suitable data set and further examples involving non-realized arguments in a complementary paper (Roth and Frank, 2012). ing such alignments as clustering provides a suitable framework to implicitly relate alignment decisions to one another, by exploiting global information encoded in a graph. The remainder of this paper is structured as follows: In Section 2, we discuss previous work in related tasks. Section 3 describes our task and a suitable data set. Section 4 introduces a graph-based clustering model using Mincuts for the al</context>
</contexts>
<marker>Ruppenhofer, Sporleder, Morante, Baker, Palmer, 2010</marker>
<rawString>Josef Ruppenhofer, Caroline Sporleder, Roser Morante, Collin Baker, and Martha Palmer. 2010. SemEval2010 Task 10: Linking Events and Their Participants in Discourse. In Proceedings of the 5th International Workshop on Semantic Evaluations, pages 45–50, Uppsala, Sweden, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Eric H Huang</author>
<author>Jeffrey Pennington</author>
<author>Andrew Y Ng</author>
<author>Christopher D Manning</author>
</authors>
<title>Dynamic pooling and unfolding recursive autoencoders for paraphrase detection.</title>
<date>2011</date>
<booktitle>In Advances in Neural Information Processing Systems (NIPS</booktitle>
<contexts>
<context position="8864" citStr="Socher et al. (2011)" startWordPosition="1381" endWordPosition="1384">ents can serve as a good indicator of entailment. 172 As a matter of fact, the same holds true for the task of detecting paraphrases. In contrast to RTE, this latter task requires bi-directional entailments, i.e., each of the two phrases must entail the other. Wan et al. (2006) show that a simple approach solely based on word (and lemmatized n-gram) overlap can already achieve an Fl-score of up to 83% for detecting paraphrases in the Microsoft Research Paraphrase Corpus (Dolan and Brockett, 2005, MSRPC). In fact, this is just 0.6% points below the state-of-the-art results recently reported by Socher et al. (2011). The MSRPC and data sets from the first RTE challenges only consisted of isolated pairs of sentences. The Fifth PASCAL Recognizing Textual Entailment Challenge (Bentivogli et al., 2009) introduced a “Search Task”, where entailing sentences for a hypothesis have to be found in a set of full documents. This new task first opened the doors for assessing the role of discourse (Mirkin et al., 2010a; Mirkin et al., 2010b) in RTE. However, this setting is still limited as discourse contexts are only provided for the entailing part (T) of each text pair but not for the hypothesis H. A further task re</context>
<context position="29940" citStr="Socher et al. (2011)" startWordPosition="4809" endWordPosition="4812">heir word-aligned paraphrase corpus. For the experiments in the GigaPairs setting, we train our own word alignment model using the state-of-theart word alignment tool Berkeley Aligner (Liang et al., 2006). As word alignment tools require pairs of sentences as input, we first extract paraphrases in the latter setting using a re-implementation of the paraphrase detection system by Wan et al. (2006).8 In the following section, we abbreviate both baselines using SMT alignment tools as WordAlign. 8Note that the performance of this system lies slightly below the state-of-the-art results reported by Socher et al. (2011) However, we were not able to reproduce the results of Socher et al. using the publicly available release of their software. 5.3 Results We measure precision as the number of predicted alignments that are annotated in the gold standard divided by the total number of predictions. Recall is measured as the number of correctly predicted sure alignments divided by the total number of sure alignments in the gold standard. This conforms to evaluation measures used for word alignment models in SMT (Och and Ney, 2003). Following Cohn et al. (2008), we subsequently compute the Fi-score as the harmonic </context>
</contexts>
<marker>Socher, Huang, Pennington, Ng, Manning, 2011</marker>
<rawString>Richard Socher, Eric H. Huang, Jeffrey Pennington, Andrew Y. Ng, and Christopher D. Manning. 2011. Dynamic pooling and unfolding recursive autoencoders for paraphrase detection. In Advances in Neural Information Processing Systems (NIPS 2011).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan Titov</author>
<author>Mikhail Kozhevnikov</author>
</authors>
<title>Bootstrapping semantic analyzers from non-contradictory texts.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>958--967</pages>
<location>Uppsala,</location>
<contexts>
<context position="2803" citStr="Titov and Kozhevnikov, 2010" startWordPosition="401" endWordPosition="404">ed semantic structures. This interplay, however, is an important factor for a semantics-based, generative model of discourse coherence. The main hypothesis of our work is that we can automatically learn context-specific realization patterns for predicate argument structures (PAS) from a semantically parsed corpus of comparable text pairs. Our assumption builds on the success of previous research, where comparable and parallel texts have been exploited for a range of related learning tasks, e.g., unsupervised discourse segmentation (Barzilay and Lee, 2004) and bootstrapping semantic analyzers (Titov and Kozhevnikov, 2010). For our purposes, we are interested in finding corresponding PAS across comparable texts that are known to talk about the same events, and hence involve the same set of underlying event participants. By aligning predicates in such texts, we can investigate the factors that determine discourse coherence in the realization patterns for the involved arguments. These include the specific forms of argument realization, as a pronoun or a specific type of referential expression, as studied in prior work in NLG (Belz et al., 2009, inter alia). The specific set-up we examine, however, allows us to fu</context>
</contexts>
<marker>Titov, Kozhevnikov, 2010</marker>
<rawString>Ivan Titov and Mikhail Kozhevnikov. 2010. Bootstrapping semantic analyzers from non-contradictory texts. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, Uppsala, Sweden, 11–16 July 2010, pages 958–967.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher Walker</author>
<author>Stephanie Strassel</author>
<author>Julie Medero</author>
<author>Kazuaki Maeda</author>
</authors>
<title>Multilingual Training Corpus. Linguistic Data Consortium,</title>
<date>2006</date>
<publisher>ACE</publisher>
<location>Philadelphia.</location>
<contexts>
<context position="9793" citStr="Walker et al. (2006)" startWordPosition="1543" endWordPosition="1546">ew task first opened the doors for assessing the role of discourse (Mirkin et al., 2010a; Mirkin et al., 2010b) in RTE. However, this setting is still limited as discourse contexts are only provided for the entailing part (T) of each text pair but not for the hypothesis H. A further task related to ours is the detection of event coreference. The goal of this task is to identify all mentions of the same event within a document and, in some settings, also across documents. However, the task setting is typically more restricted than ours in that its focus lies on identical events/references (cf. Walker et al. (2006), Weischedel et al. (2011), inter alia). In particular, verbalizations of different aspects of an event (e.g., ‘buy’–‘sell’, ‘kill’–‘die’, ‘recover’–‘find’) are generally not linked in this paradigm. In contrast to coreference methods that identify chains of events, we are interested in pairs of corresponding predicates (and their argument structure), for which we can observe alternative realizations in discourse. 3 Aligning Predicates Across Texts This section summarizes how we built a large corpus of comparable texts, as a basis for the predicate alignment task. We motivate the choice of the</context>
</contexts>
<marker>Walker, Strassel, Medero, Maeda, 2006</marker>
<rawString>Christopher Walker, Stephanie Strassel, Julie Medero, and Kazuaki Maeda. 2006. ACE 2005 Multilingual Training Corpus. Linguistic Data Consortium, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Wan</author>
<author>Mark Dras</author>
<author>Robert Dale</author>
<author>Cecile Paris</author>
</authors>
<title>Using dependency-based features to take the ”Para-farce” out of paraphrase.</title>
<date>2006</date>
<booktitle>In Proceedings of the Australasian Language Technology Workshop,</booktitle>
<pages>131--138</pages>
<contexts>
<context position="8522" citStr="Wan et al. (2006)" startWordPosition="1324" endWordPosition="1327">e entailing ‘Text’ -, and H - the entailed ‘Hypothesis’. (... ) T entails H if the meaning of H can be inferred from the meaning of T, as would typically be interpreted by people.” Although this relation does not necessarily require the presence of corresponding predicates, previous work by MacCartney et al. (2008) shows that word alignments can serve as a good indicator of entailment. 172 As a matter of fact, the same holds true for the task of detecting paraphrases. In contrast to RTE, this latter task requires bi-directional entailments, i.e., each of the two phrases must entail the other. Wan et al. (2006) show that a simple approach solely based on word (and lemmatized n-gram) overlap can already achieve an Fl-score of up to 83% for detecting paraphrases in the Microsoft Research Paraphrase Corpus (Dolan and Brockett, 2005, MSRPC). In fact, this is just 0.6% points below the state-of-the-art results recently reported by Socher et al. (2011). The MSRPC and data sets from the first RTE challenges only consisted of isolated pairs of sentences. The Fifth PASCAL Recognizing Textual Entailment Challenge (Bentivogli et al., 2009) introduced a “Search Task”, where entailing sentences for a hypothesis </context>
<context position="29719" citStr="Wan et al. (2006)" startWordPosition="4775" endWordPosition="4778">tools commonly used in statistical machine translation (SMT). For the three sentence-based paraphrase settings MTC, Leagues and MSR, Cohn et al. (2008) readily provide GIZA++ (Och and Ney, 2003) alignments as part of their word-aligned paraphrase corpus. For the experiments in the GigaPairs setting, we train our own word alignment model using the state-of-theart word alignment tool Berkeley Aligner (Liang et al., 2006). As word alignment tools require pairs of sentences as input, we first extract paraphrases in the latter setting using a re-implementation of the paraphrase detection system by Wan et al. (2006).8 In the following section, we abbreviate both baselines using SMT alignment tools as WordAlign. 8Note that the performance of this system lies slightly below the state-of-the-art results reported by Socher et al. (2011) However, we were not able to reproduce the results of Socher et al. using the publicly available release of their software. 5.3 Results We measure precision as the number of predicted alignments that are annotated in the gold standard divided by the total number of predictions. Recall is measured as the number of correctly predicted sure alignments divided by the total number</context>
</contexts>
<marker>Wan, Dras, Dale, Paris, 2006</marker>
<rawString>Stephen Wan, Mark Dras, Robert Dale, and Cecile Paris. 2006. Using dependency-based features to take the ”Para-farce” out of paraphrase. In Proceedings of the Australasian Language Technology Workshop, pages 131–138.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralph Weischedel</author>
<author>Martha Palmer</author>
<author>Mitchell Marcus</author>
<author>Eduard Hovy</author>
<author>Sameer Pradhan</author>
<author>Lance Ramshaw</author>
</authors>
<title>OntoNotes Release 4.0. Linguistic Data Consortium,</title>
<date>2011</date>
<location>Nianwen Xue, Ann Taylor, Jeff Kaufman, Michelle Franchini, Mohammed El-Bachouti, Robert</location>
<contexts>
<context position="9819" citStr="Weischedel et al. (2011)" startWordPosition="1547" endWordPosition="1550">he doors for assessing the role of discourse (Mirkin et al., 2010a; Mirkin et al., 2010b) in RTE. However, this setting is still limited as discourse contexts are only provided for the entailing part (T) of each text pair but not for the hypothesis H. A further task related to ours is the detection of event coreference. The goal of this task is to identify all mentions of the same event within a document and, in some settings, also across documents. However, the task setting is typically more restricted than ours in that its focus lies on identical events/references (cf. Walker et al. (2006), Weischedel et al. (2011), inter alia). In particular, verbalizations of different aspects of an event (e.g., ‘buy’–‘sell’, ‘kill’–‘die’, ‘recover’–‘find’) are generally not linked in this paradigm. In contrast to coreference methods that identify chains of events, we are interested in pairs of corresponding predicates (and their argument structure), for which we can observe alternative realizations in discourse. 3 Aligning Predicates Across Texts This section summarizes how we built a large corpus of comparable texts, as a basis for the predicate alignment task. We motivate the choice of the corpus and present a stra</context>
</contexts>
<marker>Weischedel, Palmer, Marcus, Hovy, Pradhan, Ramshaw, 2011</marker>
<rawString>Ralph Weischedel, Martha Palmer, Mitchell Marcus, Eduard Hovy, Sameer Pradhan, Lance Ramshaw, Nianwen Xue, Ann Taylor, Jeff Kaufman, Michelle Franchini, Mohammed El-Bachouti, Robert Belvin, and Ann Houston. 2011. OntoNotes Release 4.0. Linguistic Data Consortium, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sander Wubben</author>
<author>Antal van den Bosch</author>
<author>Emiel Krahmer</author>
<author>Erwin Marsi</author>
</authors>
<title>Clustering and matching headlines for automatic paraphrase acquisition.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th European Workshop on Natural Language Generation (ENLG</booktitle>
<pages>122--125</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Athens, Greece,</location>
<marker>Wubben, van den Bosch, Krahmer, Marsi, 2009</marker>
<rawString>Sander Wubben, Antal van den Bosch, Emiel Krahmer, and Erwin Marsi. 2009. Clustering and matching headlines for automatic paraphrase acquisition. In Proceedings of the 12th European Workshop on Natural Language Generation (ENLG 2009), pages 122– 125, Athens, Greece, March. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.011781">
<title confidence="0.987719">
Syntactic surprisal affects spoken word duration in conversational contexts
</title>
<author confidence="0.995264">
Vera Demberg, Asad B. Sayeed, Philip J. Gorinski, and Nikolaos Engonopoulos
</author>
<affiliation confidence="0.996253666666667">
M2CI Cluster of Excellence and
Department of Computational Linguistics and Phonetics
Saarland University
</affiliation>
<address confidence="0.822083">
66143 Saarbr¨ucken, Germany
</address>
<email confidence="0.999191">
{vera,asayeed,philipg,nikolaos}@coli.uni-saarland.de
</email>
<sectionHeader confidence="0.998601" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9994813">
We present results of a novel experiment to in-
vestigate speech production in conversational
data that links speech rate to information den-
sity. We provide the first evidence for an asso-
ciation between syntactic surprisal and word
duration in recorded speech. Using the AMI
corpus which contains transcriptions of focus
group meetings with precise word durations,
we show that word durations correlate with
syntactic surprisal estimated from the incre-
mental Roark parser over and above simpler
measures, such as word duration estimated
from a state-of-the-art text-to-speech system
and word frequencies, and that the syntac-
tic surprisal estimates are better predictors of
word durations than a simpler version of sur-
prisal based on trigram probabilities. This re-
sult supports the uniform information density
(UID) hypothesis and points a way to more re-
alistic artificial speech generation.
</bodyText>
<sectionHeader confidence="0.999473" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999960793103448">
The uniform information density (UID) hypothesis
suggests that speakers try to distribute information
uniformly across their utterances (Frank and Jaeger,
2008). Information density can be measured in
terms of the surprisal incurred at each word, where
surprisal is defined as the negative log-probability
of an event. This paper sets out to test whether UID
holds across different linguistic levels, i.e. whether
speakers adapt word duration during production to
syntactic surprisal, such that words with higher sur-
prisal have longer durations than words with lower
surprisal. We investigate this question in a corpus
of transcribed speech from a mix of native and non-
native English speakers, a population that is a non-
trivial component of the user base for language tech-
nologies developed for English. This data reflects a
casual, uncontrolled conversational environment.
Using linear mixed-effects modeling, we found
that syntactic surprisal as calculated from a top-
down incremental PCFG parser accounts for a sig-
nificant amount of variation in spoken word dura-
tion, using an HMM-trained text-to-speech system
as a baseline. The findings of this paper provide ad-
ditional support the uniform information density hy-
pothesis and furthermore have implications for the
design of text-to-speech systems, which currently
do not take into account higher-level linguistic in-
formation such as syntactic surprisal (or even word
frequencies) for their word duration models.
</bodyText>
<sectionHeader confidence="0.910612" genericHeader="introduction">
1.1 Related work
</sectionHeader>
<bodyText confidence="0.9997174">
The use of word-level surprisal as a predictor of pro-
cessing difficulty is based on the notion that pro-
cessing difficulty results when a word is encountered
that is unexpected given its preceding context. The
amount of surprisal on a word wi can be formal-
ized as the log of the inverse conditional probabil-
ity of wi given the preceding words in the sentence
w1 ... wi−1, or − log P(wi|w1...i−1). If this proba-
bility is low, then the word is unexpected, and sur-
prisal is high. Surprisal can be estimated in different
ways, e.g. from word sequences (n-grams) or with
respect to the possible syntactic structures covering
a sentence prefix (see Section 4).
Hale (2001) showed that surprisal calculated from
a probabilistic Earley parser correctly predicts well-
</bodyText>
<page confidence="0.981987">
356
</page>
<note confidence="0.770716">
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 356–367, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.999984989583333">
known processing phenomena that were believed
to emerge from structural ambiguities (e.g., garden
paths) and Levy (2008) further demonstrated the rel-
evance of surprisal to human sentence processing
difficulty on a range of syntactic processing diffi-
culty phenomena.
There is existing work in correlating information-
theoretic measures of linguistic redundancy to the
observed duration of speech units. Aylett and Turk
(2006) demonstrate that the contextual predictability
of a syllable (n-gram log probability) has an inverse
relationship to syllable duration in speech. Their ex-
periments were performed using a carefully articu-
lated speech synthesis training corpus.
This type of work fits into a larger programme of
understanding how speakers schedule utterances to
avoid high variation in the transmission of linguis-
tic information over time, also known as the Uni-
form Information Density (UID) hypothesis (Flo-
rian Jaeger, 2010). Levy and Jaeger (2007) show
that the reduction of optional that-complementizers
in English is related to trigram surprisal; low sur-
prisal predicts a high likelihood of reduction. Flo-
rian Jaeger (2010) shows the same result of in-
creased reduction when the complementizer is more
predictable according to information density calcu-
lated in terms of the main verb’s subcategorization
frequency.
Frank and Jaeger (2008) provide evidence that a
UID account can predict the use of reduced forms
of “be”, “have”, and “not” in English. They use the
surprisal of the candidate word itself as well as sur-
prisals of the word before and after, computing bi-
gram and trigram estimates directly from the corpus
without smoothing or backoff.
Jurafsky et al. (2001) report a corpus study sim-
ilar to ours, showing that words that are more pre-
dictable from context are reduced. As measures
of word predictability, they use bigram and trigram
models, as well as joint probabilities, but not syntac-
tic surprisal.
Within the same theme of utterance duration
vs. information content, Piantadosi et al. (2011)
performed a study using Google-derived n-gram
datasets on the lexica of multiple languages, includ-
ing English, Portuguese, and Czech. For every word
in a given language’s lexicon, they calculated 2-, 3-,
and 4-gram surprisal values using the Google dataset
for every occurrence of the word, and then they
took the mean surprisal for that word over all oc-
currences. The 3-gram surprisal values in particular
were a better predictor of orthographic length than
unigram frequency, providing evidence for the use
of information content and contextual predictability
as improvement over a Zipf’s Law view of commu-
nicative efficiency. This is an n-gram approach to
supporting the UID hypothesis.
However, there is some counter-evidence for the
UID-based view. Kuperman et al. (2007) analyzed
the relationship between linguistic unit predictabil-
ity and syllable duration in read-aloud speech in
Dutch. Dutch makes use of interfix morphemes
-s- and -e(n)- in certain contexts to make com-
pound nouns, preferring a null interfix in most
cases. For example, the Dutch noun kandidaatsex-
amen (“Bachelor’s examination”) is composed of
kandidaat-, -s-, and -examen.
Kuperman et al. find that the greater the pre-
dictability of the interfix from the morphological
context (i.e., the surrounding members of the com-
pound), the longer the duration of the pronuncia-
tion of the interfix. To illustrate, if -s- is more ex-
pected after kandidaat or if kandidaatsexamen is a
frequent compound, we would therefore expect the
-s- to be pronounced longer, given the correlations
they found. Their finding runs counter to a strong
view of UID’s fine-grained control over speech rate,
but it is focused on the morphological level. They
hypothesize that this counter-intuitive result may be
driven by complex paradigmatic constraints in the
choice of morpheme.
Our work, however, focuses on the syntactic level
rather than the paradigmatic. What we seek to an-
swer in our work is the extent to which an infor-
mation density-based analysis can not only be ap-
plied to real speech data in context but also be de-
rived from higher-level syntactic analyses, a com-
bination hitherto little explored. Existing broad-
coverage work on syntactic surprisal has largely fo-
cused on comprehension phenomena, such as Dem-
berg and Keller (2008), Roark et al. (2009), and
Frank (2010). We provide a production study in a
vein similar to that of Kuperman et al., but show that
frequency effects work in the expected direction at
the syntactic level. This in turn expands upon the
view supported by n-gram-based work such as that
</bodyText>
<page confidence="0.996894">
357
</page>
<bodyText confidence="0.9992745">
of Piantadosi et al. (2011); Levy and Jaeger (2007);
Jurafsky et al. (2001), showing that information con-
tent above the n-gram level is important in guiding
spoken language production in humans.
</bodyText>
<subsectionHeader confidence="0.918563">
1.2 Implications for Potential Applications
</subsectionHeader>
<bodyText confidence="0.999964863636364">
Spoken dialogue systems are of increasing eco-
nomic and technological importance in recent times,
particularly as it is now feasible to include this tech-
nology in everything from small consumer devices
to industrial equipment. With this increase in impor-
tance, there is also unsurprisingly growing scientific
emphasis in understanding its usability and safety
characteristics. Recent work (Fang et al., 2009;
Taube-Schiff and Segalowitz, 2005) has shown that
linguistic information presentation has an effect on
user behaviour, but the overall granularity of this be-
haviour is still not well-understood.
Other potential applications exist in any place
where text-to-speech technologies can be applied,
such as in real-time spoken machine translation and
communications systems for the disabled.
In demonstrating that we can observe speakers be-
having in the manner predicted by the UID hypoth-
esis in conversational contexts, we provide evidence
for a finer-level of granularity necessary for control-
ling the rate of information presentation in artificial
systems.
</bodyText>
<subsectionHeader confidence="0.994035">
1.3 AMI corpus
</subsectionHeader>
<bodyText confidence="0.999963789473684">
The Augmented Multi-Party Interaction (AMI) cor-
pus is a collection of recorded, transcribed con-
versations spanning 100 hours of simulated meet-
ings. The corpus contains a number of data streams
including speech, video, and whiteboard writing.
Transcription of the meetings was performed man-
ually, and the transcripts contain word-level time
bounds that were produced by an automatic speech
recognition system.
The freely-available AMI corpus is one of a very
small number of efforts that contain orthographic
transcriptions that are time-aligned at a word level.
We chose it for the realism of the setting in which
it was recorded; the physical presence of multiple
speakers in an unstructured discussion reflects a po-
tentially high level of noise in which we would be
looking for surprisal correspondences, potentially
increasing the application value of the correspon-
dences we find.
</bodyText>
<subsectionHeader confidence="0.987769">
1.4 Organization
</subsectionHeader>
<bodyText confidence="0.999740533333333">
The remainder of this paper proceeds as follows. In
section 2, we describe at a high level the procedure
we used to test our hypothesis that parser-derived
surprisal values can partly account for utterance-
duration variation. Then (section 3.2) we discuss the
MARY text-to-speech system, from which we derive
“canonical” word utterance durations. We describe
the way we process and filter the AMI meeting cor-
pus in section 3.1. In section 4, we describe in detail
our predictors, frequency counts, trigram surprisal,
and Roark parser surprisal. Sections 5 and 6 de-
scribe how we use linear mixed effects modeling to
find significant correlations between our predictors
and the response variable, and we finally make some
concluding remarks in section 7.
</bodyText>
<sectionHeader confidence="0.998305" genericHeader="method">
2 Design
</sectionHeader>
<bodyText confidence="0.999970391304348">
The overall design of our experiment is schemati-
cally depicted in Figure 1. We extract the words
and the word-by-word timings from the AMI corpus,
keeping track of each word’s position in the corpus
by conversation ID, speaker turn, and chronological
order. As we describe in the next section, we filter
the words for anomalies.
After pre-processing, for each word in the cor-
pus, we extract the following predictors: canoni-
cal speech durations from the MARY text-to-speech
system, logarithmic word frequencies, n-gram sur-
prisal, and surprisal values produced by the Roark
(2001a); Roark et al. (2009) parser (see Section 4).
The next sections describe how and from where
these values are obtained1.
Finally, we run mixed effects regression model
analyses (Baayen et al., 2008) with the observed
durations as a response variable and the predictors
mentioned above in order to detect whether syntac-
tic surprisal is a significant positive predictor of spo-
ken word durations above and beyond the more ba-
sic effects of canonical word duration and word fre-
quency.
</bodyText>
<footnote confidence="0.956221">
1We will make this data widely available upon publication.
</footnote>
<page confidence="0.986232">
358
</page>
<figure confidence="0.999526975">
Observations
Observed
timings
AMI
corpus
Computed
timings
MARY
Gigaword
Gigaword
freq.
AMI
word
freq.
Word
filtration
and
selection
PTB
n-gram
surprisal
Gigaword
n-gram
surprisal
CMU
toolkit
Penn
Treebank
AMI
n-gram
surprisal
Roark
syntactic
surprisal
Roark
parser
Relative
significance
Regression
analysis
</figure>
<figureCaption confidence="0.999974">
Figure 1: Schematic overview of experiment.
</figureCaption>
<sectionHeader confidence="0.993431" genericHeader="method">
3 Experimental materials
</sectionHeader>
<subsectionHeader confidence="0.999763">
3.1 Corpus preparation
</subsectionHeader>
<bodyText confidence="0.999626575757576">
The AMI corpus is provided in the NITE XML
Toolkit (NXT) format. We developed a custom inter-
preter to assemble the relevant data streams: words,
meeting IDs, speaker IDs, speaker turns, and ob-
served word durations.
In addition to grouping and re-ordering the infor-
mation found in the original XML corpus, two more
steps were taken to eliminate confounding noise
from the data. Non-words (e.g. “uhm”, “uh-hmm”,
etc.) were filtered out, as were incomplete words or
incorrectly transcribed words (e.g. “recogn”, “some-
thi”, etc); the criterion for rejection was presence in
the English Gigaword corpus with subsequent mi-
nor corrections by hand, e.g., mapping unseen verbs
back into the corpus and correcting obvious com-
mon misspellings.2
Finally, turns that did not make for complete sen-
tences, e.g., utterances that were interrupted in mid-
2A reviewer asks about the extent to which our Gigaword fil-
tering process may remove words we might want to keep but ad-
mit words we want to reject. As Gigaword is mostly newswire
text, we do not expect the latter case to hold often. AMI is
hand-transcribed and uses consistent spellings for non-word in-
terjections (easy to remove), and any spelling mistakes would
have to coincide exactly with a Gigaword mistake.
The other way around (rejecting what should be allowed) is
easier to check, and we find that of 13K word types in AMI,
about 7.2% are rejected for non-appearance in Gigaword, after
filtering for interjections like “mm-hmm”. However, we man-
ually checked them and returned all but 2.9% of word types to
the corpus. These tend to be very low-frequency types. The
manual check suggests that ultimately there would be few false
rejections.
</bodyText>
<page confidence="0.994953">
359
</page>
<bodyText confidence="0.999927666666667">
sentence, were filtered out in order to maximize the
proportion of complete parses in surprisal calcula-
tion.
</bodyText>
<subsectionHeader confidence="0.998335">
3.2 Word duration model
</subsectionHeader>
<bodyText confidence="0.998417035714286">
In order to investigate whether there is an association
between high/low surprisal and increased/decreased
word duration, one needs to have a baseline mea-
sure of what constitutes the “canonical” duration of
each word—in other words, to account for the fact
that some words have longer pronunciations than
others. As one reviewer notes, one way of estimat-
ing word durations would be to calculate the aver-
age duration of each word in the corpus. However,
this approach would be insensitive to the phonolog-
ical, syllabic and phrasal context that a word oc-
curs in, which can have a large effect on word du-
ration. Therefore, we use word duration estimates
from the state-of-the-art open-source text-to-speech
system MARY (Schr¨oder et al., 2008, version 4.3.1),
with the default voice package included in this ver-
sion (cmu-slt-hsmm).
The cmu-slt-hsmm voice package uses
a Hidden Markov model, trained on the fe-
male US English section of the CMU ARCTIC
database (Kominek and Black, 2003), to predict
prosodic attributes of each individual synthesized
phone, including duration. Training was carried
out using a version of the HTS system (Zen et al.,
2007), modified for using the MARY context
features (Schr¨oder et al., 2008) for estimating the
parameters of the model and for decoding. Those
features include3:
</bodyText>
<listItem confidence="0.99080025">
• phonological features of the current and neigh-
boring phonemes
• syllabic and lexical features (e.g. syllable
stress, (estimated) part-of-speech, position of
syllable in word)
• phrasal / sentential features (e.g. sen-
tence/phrase boundaries, neighboring pauses
and punctuation)
</listItem>
<bodyText confidence="0.976997">
For each word in the AMI corpus, we ob-
tained two alternative estimates of word duration:
</bodyText>
<footnote confidence="0.785035">
3For further information about how HMM-based voices for
MARY TTS are trained, see http://mary.opendfki.
de/wiki/HMMVoiceCreation
</footnote>
<bodyText confidence="0.999910066666667">
one version which is independent of a word’s
sentential context, and a second version which
does take into account the sentential context (such
as phrasal/sentential and across-word-boundaries
phonological features) the word occurs in. In other
words, we obtain MARY word duration estimates
in the second version by running individual whole
sentences through MARY, segmented by standard
punctuation marks used in the AMI corpus transcrip-
tions. For each version, we obtained phone dura-
tions using MARY and calculate the total duration of
a word as the sum of the estimated phone durations
for that word. These durations serve as the “canoni-
cal” baselines to which the observed durations of the
words in the AMI corpus are compared.
</bodyText>
<subsectionHeader confidence="0.998852">
3.3 Word frequency baselines
</subsectionHeader>
<bodyText confidence="0.9999448125">
In order to account for the effects of simple word
frequency on utterance duration, we extracted two
types of frequency counts. One was taken di-
rectly from the AMI corpus alone. The other was
taken from a 151 million-word (4.3 million full-
paragraph) sample of the English Gigaword cor-
pus. These came from the following newswire
sources: Agence France Press, Associated Press
Worldstream, New York Times Newswire, and the
Xinhua News Agency English Service. These
sources are organized by month-of-year. We se-
lected the subset of Gigaword by randomly select-
ing month-of-year files from those sources with uni-
form probability. Punctuation was stripped from the
beginnings and ends of words before taking the fre-
quency counts.
</bodyText>
<sectionHeader confidence="0.996799" genericHeader="method">
4 Surprisal models
</sectionHeader>
<bodyText confidence="0.999834">
For predicting the surprisal of utterances in context,
two different types of models were used— n-gram
probabilities models, as well as Roark’s 2001 incre-
mental top-down parser capable of calculating pre-
fix probabilities. We also estimated word frequen-
cies to account for words being spoken more quickly
due to their higher frequency which is independent
of structural surprisal.
The n-gram probabilities models, while being fast
in both training and application, inherently capture
very limited contextual influences on surprisal. The
full-fledged parser, on the other hand, quantifies sur-
</bodyText>
<page confidence="0.987351">
360
</page>
<bodyText confidence="0.987910852941176">
prisal based in the prefix probability of the complete
sentence prefix and captures long-distance effects
by conditioning on c-commanding lexical items as
well as non-local node labels such as parents, grand-
parents and siblings from the left context.
CMU n-grams We used the CMU Statistical Nat-
ural Language Modeling Toolkit to provide a con-
venient way to calculate n-grams probabilities. For
the prediction of surprisal, we calculated 3-gram
models, 4-gram models and 5-gram models with
Witten-Bell smoothing. Different n-gram models
were trained on the full Gigaword corpus, as well
as the AMI corpus.
To avoid overfitting, the AMI text corpus was split
into 10 sub-corpora of equal word counts, preserv-
ing coherence of meetings. N-gram probabilities
were then calculated for each of the sub-corpora us-
ing models trained on the 9 others.
We also produced a trigram model using the text
of chapter 2–21 of the Penn Treebank’s (PTB) un-
derlying Wall Street Journal corpus. This consists
of approximately one million tokens. We generated
this model because it is the underlying training data
for the Roark parser, described below.
Syntactic Surprisal from Roark parser In order
to capture the effect of syntactically expected vs. un-
expected events, we can calculate the syntactic sur-
prisal of each word in a sentence. The syntactic sur-
prisal at word S,,,, is defined as the difference be-
tween the prefix probability at word wi and the pre-
fix probability at word wi−1. The prefix probability
at word wi is the sum of the probabilities of all trees
T spanning words w1 ... wi; see also (Levy, 2008;
Demberg and Keller, 2008).
</bodyText>
<equation confidence="0.9980335">
�S�i = log �P (T, w1..wi�1) − log P(T, w1..wi)
T T
</equation>
<bodyText confidence="0.999945857142857">
The top-down incremental Roark parser (Roark,
2001a) has the characteristic that all partial left-to-
right parses are rooted: they form a single tree with
one root. A set of heuristics ensures that rule appli-
cation occurs only through node expansion within
the connected structure.4 The grammar-derived pre-
fix probabilities of a given sentence prefix can there-
</bodyText>
<footnote confidence="0.9990755">
4The formulae for the calculation of the prefix probabilities
from the PCFG rules can be found in Roark et al. (2009).
</footnote>
<figureCaption confidence="0.994796">
Figure 2: Top-ranked partial parse of A puppy is to a dog
what a kitten is to a cat., stopping at the second a and
providing the Roark parser surprisal values by word. The
branch with dashed lines and struck-out symbols repre-
sents an analysis abandoned at the appearance of the a.
</figureCaption>
<bodyText confidence="0.999732333333333">
fore be calculated directly by multiplying the prob-
abilities of all rules used to generate the prefix tree.
The Roark parser shares this characteristic of gener-
ating fully connected structures with Earley parsers
(Earley, 1970) and left corner parsers (Rosenkrantz
and II, 1970).
The Roark parser uses a beam search. As the
amount of probability mass lost has been shown
to be small (Roark, 2001b), the surprisal estimates
can be assumed to be a good approximation. The
beam width of the parser search is controlled by a
“base parsing threshold”, which defines the distance
in terms of natural log-probability between the most
probable parse and the least probable parse within
the beam. For the experiments reported here, the
parsing beam was set to 21 (default setting is 12). A
wider beam also reduces the effects of pruning.
The parser was trained on Wall Street Journal sec-
tions 2–21 and applied to parse the full sentences
of the AMI corpus, collecting predicted surprisal at
each word (see Figure 2 for an example).
The syntactic surprisal can be furthermore be de-
composed into a structural and a lexical part: some-
times, high surprisal might be due to a word be-
ing incompatible with the high-probability syntactic
structures, other times high surprisal might just be
due to a lexical item being unexpected. It is inter-
</bodyText>
<figure confidence="0.99924348">
NP
VP
TO
to
3.873
to
3.873
DT
a
5.973
S
DT
A
3.989
NN
puppy
4.570
AUX
is
3.089
PP
TO
NP
S
VP
</figure>
<page confidence="0.995366">
361
</page>
<bodyText confidence="0.999981875">
esting to evaluate these two aspects of syntactic sur-
prisal separately, and the Roark parser conveniently
outputs both surprisal estimates. Structural surprisal
is estimated from the occurrence counts of the appli-
cation of syntactic rules during the parse discount-
ing the effect of lexical probabilities, while lexical
surprisal is calculated from the probabilities of the
derivational step from the POS-tag to lexical item.
</bodyText>
<sectionHeader confidence="0.983604" genericHeader="method">
5 Linear mixed effects modelling
</sectionHeader>
<bodyText confidence="0.999150897435898">
In order to test whether surprisal estimates correlate
with speech durations, we use linear mixed effects
models (LME, Pinheiro and Bates (2000)). This
type of model can be thought of as a generalization
of linear regression that allows the inclusion of ran-
dom factors as well as fixed factors.We treat speak-
ers as a random factor, which means that our mod-
els contain an intercept term for each speaker, rep-
resenting the individual differences in speech rates.
Furthermore, we include a random slope for the
predictors (e.g. frequency, canonical duration, sur-
prisal), essentially accounting for idiosyncrasies of
a participant with respect to the predictor, such that
only the part of the variance that is common to all
participants and is attributed to that predictor.
In a first step, we fit a baseline model with all pre-
dictors related to a word’s canonical duration and its
frequency as well as their random slopes to the ob-
served word durations. Models with more than two
random slopes generally did not converge. We there-
fore included in the baseline model only the two best
random slopes (in terms of model fit). We then cal-
culated the residuals of that model, the part of the
observed word durations that cannot be accounted
for through canonical word durations or word fre-
quency.
For each of our predictors of interest (n-gram sur-
prisal, syntactic surprisal), we then fit another lin-
ear mixed-effects model with random slopes to the
residuals of the baseline model. This two-step pro-
cedure allows us to make sure to avoid problems
of collinearity between e.g. surprisal and word fre-
quency or canonical duration. A simpler (but less
conservative) method is to directly add the predic-
tors of interest to the baseline model. Results for
both modelling variants lead to the same conclusions
for our model, so we here report the more conserva-
tive two-step model. We compare models based on
the Akaike Information Criterion (AIC).
</bodyText>
<sectionHeader confidence="0.999923" genericHeader="evaluation">
6 Results
</sectionHeader>
<bodyText confidence="0.999939190476191">
Our baseline model uses speech durations from the
AMI corpus as the response variable and canoni-
cal duration estimates from the MARY TTS system
and log word frequencies as predictors. We exclude
from the analysis all data points with zero duration
(effectively, punctuation) or a real duration longer
than 2 seconds. Furthermore, we exclude all words
which were never seen in Gigaword and any words
for which syntactic surprisal couldn’t be estimated.
This leaves us with 771,234 out of the 799,997 data
points with positive duration.
MARY duration models As mentioned in the
earlier sections, we have calculated different ver-
sions of the MARY estimated word durations: one
model without the sentential context and one model
with the sentential context. In our regression analy-
ses, we find, as expected, that the model which in-
cludes sentential context achieves a much better fit
with the actually measured word durations from the
AMI corpus (AIC = 32167) than the model without
context (AIC = 70917).
Word frequency estimates We estimated word
frequencies from several different resources, from
the AMI corpus to have a spoken domain frequency
and from Gigaword as a very large resource. We
find that both frequency estimates significantly im-
prove model fit over a model that does not contain
frequency estimates. Including both frequency esti-
mates improves model fit with respect to a model
that includes just one of the predictors (all p &lt;
0.0001).
Furthermore, including into the regression an in-
teraction of estimated word duration and word fre-
quency also significantly increases model fit (p &lt;
0.0001). This means that words which are short and
frequent have longer duration than would be esti-
mated by adding up their length and frequency ef-
fects.
Baseline model Fixed effects of the fitted model
are shown in Table 2. We see a highly significant ef-
fect in the expected direction for both the canonical
duration estimate and word frequency. The positive
</bodyText>
<page confidence="0.995773">
362
</page>
<bodyText confidence="0.9997964">
coefficient for MARY CONTEXT means that TTS
duration estimates are positively correlated with the
measured word durations. The negative coefficient
for WORDFREQUENCY means that more frequent
words are spoken faster than less frequent words.
Finally, the negative coefficient for the interaction
between word durations and frequencies means that
the duration estimate for short frequent and long in-
frequent words is less extreme than otherwise pre-
dicted by the main effects of duration and frequency.
</bodyText>
<table confidence="0.9983306">
Ami Mary Mary Giga PTB AMI AMI Giga
Dur Word Cntxt Freq Freq Freq 3grm 4grm
Mary Word .36 1
Mary Cntxt .42 .72 1
GigaFreq -.35 -.52 -.65 1
PTBFreq -.33 -.48 -.62 .98 1
AMIFreq -.33 -.61 -.57 .65 .62 1
AMI3gram .21 .40 .41 -.41 -.39 -.68 1
Giga4gram .24 .33 .44 -.59 -.59 -.44 .61 1
Srprsl .29 .40 .48 -.71 -.73 -.50 .50 .73
</table>
<tableCaption confidence="0.999935">
Table 1: Correlations (pearson) of model predictors.
</tableCaption>
<bodyText confidence="0.998697">
Note though that the predictors are also correlated
(for correlations of the main predictors used in these
analyses, see Table 1), so there is some collinearity
in the below model. Since we are less interested in
the exact coefficients and significance sizes for these
baseline predictors, this does not have to bother us
too much. What is more important, is that we re-
move any collinearity between the baseline predic-
tors and our predictors of interest, i.e. the surprisal
estimates from the ngram models and parser. There-
fore, we run separate regression models for these
predictors on the residuals of the baseline model.
N-gram estimates We estimated 3-gram, 4-gram
and 5-gram models on the AMI corpus (9-fold-
</bodyText>
<table confidence="0.999138166666667">
Predictor Coef t-value Sig
INTERCEPT 0.3098 212.11 ***
MARY CONTEXT 0.4987 95.48 ***
AMIWORDFREQUENCY -0.0282 -32.28 ***
GIGAWORDFREQUENCY -0.0275 -62.44 ***
MARY CNTXT:GIGAFREQ -0.0922 -45.41 ***
</table>
<tableCaption confidence="0.9858304">
Table 2: Baseline linear mixed effects model of speech
durations on the AMI corpus data for MARY CONTEXT
(including the sentential context), WORDFREQUENCY
under speaker with random intercept for speaker and ran-
dom slopes under speaker. Predictors are centered.
</tableCaption>
<table confidence="0.999825">
Predictor Coef t-value Sig
INTERCEPT 0.3099 212.94 ***
MARY CONTEXT 0.4970 94.60 ***
AMIWORDFREQUENCY -0.0279 -31.98 ***
GIGAWORDFREQUENCY -0.0254 -53.68 ***
GIGA4GRAMSURPRISAL 0.0027 11.81 ***
MARY CNTXT:GIGAFREQ -0.0912 -44.87 ***
</table>
<tableCaption confidence="0.990926">
Table 3: Linear mixed effects model of speech durations
including 4-gram surprisal trained on gigaword as a pre-
dictor.
</tableCaption>
<bodyText confidence="0.999954694444445">
cross), the Penn Treebank and the Gigaword Cor-
pus. We found that coefficient estimates and signif-
icance levels of the resulting models were compara-
ble. This is not surprising, given that 4-gram and 5-
gram models were backing of to 3-grams or smaller
contexts for more than 95% of cases on the AMI and
PTB corpora (both ca. 1m words), and thus were
correlated at p &gt; .98. On the Gigaword Corpus,
the larger contexts were seen more often (5-grams:
11%, 4-grams: 36%), but still correlation with 3-
grams were high at (p &gt; .96).
N-gram model surprisal estimated on newspaper
texts from PTB or Gigaword were statistically sig-
nificant positive predictors of spoken word durations
beyond simple word frequencies (but PTB ngram
surprisal did not improve fit over models containing
Gigaword frequency estimates). Counter-intuitively
however, ngram models estimated based on the AMI
corpus have a small negative coefficient in models
that already include word frequency as a predictor
– residuals of an AMI-estimated ngram model with
respect to word frequency are very noisy and do not
show a clear correlation anymore with word dura-
tions.
Surprisal Surprisal effects were found to have a
robust significant positive coefficient, meaning that
words with higher surprisal are spoken more slowly /
clearly than expected when taking into account only
canonical word duration and word frequency. Sur-
prisal achieves a better model fit than any of the
n-gram models, based on a comparsion of AICs,
and Surprisal significantly improved model fit over
a model including frequencies and ngram models
based on AMI and Gigaword. Table 4 shows the es-
timate for SURPRISAL on the residuals of the model
in Table 2.
</bodyText>
<page confidence="0.997549">
363
</page>
<table confidence="0.897522333333333">
Predictor Coef t-value Sig
INTERCEPT -0.0154 -23.45 ***
SURPRISAL 0.0024 26.09 ***
</table>
<tableCaption confidence="0.93800275">
Table 4: Linear mixed effects model of surprisal (based
on Roark parser) with random intercept for speaker and
random slope. The response variable is residual word du-
rations from the model shown in Table 3.
</tableCaption>
<bodyText confidence="0.99996325">
Surprisal estimated from the Roark parser also
remains a significant positive predictor when re-
gressed against the residuals of a baseline model in-
cluding both 3-gram surprisal from the AMI corpus
and 4-gram surprisal from the Gigaword corpus. In
order to make really sure that the observed surprisal
effect has indeed to do with syntax and can not be
explained away as a frequency effect, we also cal-
culated frequency estimates for the corpus based on
the Penn Treebank. The significant positive surprisal
effect remains stable, also when run on the residuals
of a model which includes PTB trigrams and PTB
frequencies.
It is difficult from these regression models to in-
tuitively grasp the size of the effect of a particular
predictor on reading times, since one would have to
know the exact range and distribution of each pre-
dictor. To provide some intuition, we calculate the
estimated effect size of Roark surprisal on speech
durations. Per Roark surprisal “unit”, the model es-
timates a 7 msec difference5. The range of Roark
surprisal in our data set is roughly from 0 to 25,
with most values between 2 and 15. For a word
like “thing” which in one instance in the AMI cor-
pus was estimated with a surprisal of 2.179 and in
another instance as 16.277, the estimated difference
in duration between these instances would thus be
104msec, which is certainly an audible difference.
(Full range for Roark surprisal: 174msec, whereas
full range for gigaword 4gram surprisal is 35 msec.)
When analysing the surprisal effect in more detail,
we find that both the syntactic component of sur-
prisal and its lexical component are significant pos-
itive predictors of word durations, as well as the in-
teraction between them, which has a negative slope.
A model with the separate components and their in-
</bodyText>
<footnote confidence="0.892784">
52.4msec for a unit of residualized Roark surprisal, but it
is even less intuitive what that means, hence we calculate with
non-residualized surprisal here.
</footnote>
<table confidence="0.9878488">
Predictor Coef t-value Sig
INTERCEPT -0.0219 -18.77 ***
STRUCTSURPRISAL 0.0009 2.71 **
LEXICALSURPRISAL 0.0044 24.00 ***
STRUCT:LEXICAL -0.0004 - 6.83 ***
</table>
<tableCaption confidence="0.990552">
Table 5: Linear mixed effects model of residual speech
durations wrt. baseline model from Table 3, with random
intercept for speaker and random slope for structural and
lexical component of surprisal, estimated using the Roark
parser.
</tableCaption>
<bodyText confidence="0.999833038461538">
teraction achieves a better model fit (in AIC and BIC
scores) than a model with only the full surprisal ef-
fect. The detailed model is shown in Table 5.
To summarize, the positive coefficient of surprisal
means that words which carry a lot of information
from a structural point of view are spoken more
slowly than words that carry less such information.
These results thus provide good evidence for our
hypothesis that the predictability of syntactic struc-
ture affects phonetic realization and that speakers
use speech rate to achieve more uniform information
density.
Native vs. non-native speakers Finally, we also
compared effects in our native vs. non-native
speaker populations, see Table 6. Both populations
show the same effects and tell the same story (note
that significance values can’t be compared as the
sample sizes are different). It might be possible to
interpret the findings in the sense that native speak-
ers are more proficient at adapting their speech rate
to (syntactic) complexity to achieve more uniform
information density, given the slightly higher coeffi-
cient and significance for Surprisal for native speak-
ers. Since the effects are statistically significant
for both groups, we don’t want to make too strong
claims about differences between the groups.
</bodyText>
<sectionHeader confidence="0.996287" genericHeader="conclusions">
7 Conclusions and future work
</sectionHeader>
<bodyText confidence="0.999961714285714">
We have shown evidence in this work that syntac-
tic surprisal effects in transcribed speech data can
be detected through word utterance duration in both
native and non-native speech, and we did so using
a meeting corpus not specifically designed to iso-
late these effects. This result is the potential foun-
dation for futher work in applied, experimental, and
</bodyText>
<page confidence="0.995157">
364
</page>
<table confidence="0.996440545454545">
Predictor Native English Sig Non-native Sig
Coef t-value Coef t-value
INTERCEPT 0.2947 149.74 *** 0.3221 175.38 ***
MARY CONTEXT 0.5304 69.27 *** 0.4699 67.77 ***
AMIWORDFREQUENCY -0.0226 -18.10 *** -0.0321 -28.00 ***
GIGAWORDFREQUENCY -0.0264 -41.19 *** -0.0248 -39.58 ***
GIGAWORD4-GRAMS 0.0018 5.36 *** 0.0033 10.85 ***
MARY CONTEXT:GIGAFREQ -0.0810 -27.20 *** -0.0993 -35.71 ***
SURPRISAL 0.0033 24.21 *** 0.0018 15.09 ***
no of data points 320,592 391,106
*p &lt; 0.05, **p &lt; 0.01, ***p &lt; 0.001
</table>
<tableCaption confidence="0.986054">
Table 6: Native speakers are possibly slightly better at adapting their speech rate to syntactic surprisal than non-native
speakers. Surprisal value is for model with residuals of other predictors as dependent variable.
</tableCaption>
<bodyText confidence="0.999898833333333">
theoretical psycholinguistics. It provides additional
direct support for approaches based on the UID hy-
pothesis.
From an applied perspective, the fact that fre-
quency and syntactic surprisal have a significant ef-
fect beyond what a HMM-trained TTS model would
predict for individual words is a case for further
research into incorporating syntactic models into
speech production systems. Our methodology im-
mediately provides a framework for estimating the
word-by-word effect on duration for increased nat-
uralness in TTS output. This is relevant to spo-
ken dialogue systems because it appears that syn-
thesized speech requires a greater level of attention
from the dialogue system users when compared to
the same words delivered in natural speech (Delogu
et al., 1998). Some of this effect may be attributable
to peaks in information density which are caused by
current generation systems not compensating for ar-
eas of high information density through speech rate,
lexical and structural choice.
Furthermore, syntax and semantics have been ob-
served to interact with the mode of speech deliv-
ery. Eye-tracking experiments by Swift et al. (2002)
showed that there was a synthetic vs. natural speech
difference in the time required to pay attention to
an object referred to using definite articles, but not
indefinite articles. Our result points a way towards
a direction for explaining of this phenomenon by
demonstrating that the differences between current-
technology artificial speech and natural speech can
be partially explained through higher-level syntactic
features.
However, further experimentation is required on
other measures of syntactic complexity (e.g. DLT,
Gibson (2000)) as well as other levels of representa-
tion such as the semantic level. From a theoretical
and neuroanatomical perspective, the finding that a
measure of syntactic ambiguity reduction has an ef-
fect on the phonological layer of production has ad-
ditional implications for the organization of the hu-
man language production system.
</bodyText>
<sectionHeader confidence="0.995037" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.868416842105263">
Aylett, M. and Turk, A. (2006). Language redun-
dancy predicts syllabic duration and the spec-
tral characteristics of vocalic syllable nuclei.
Journal of the acoustical society of America,
119(5):3048–3059.
Baayen, R., Davidson, D., and Bates, D. (2008).
Mixed-effects modeling with crossed random ef-
fects for subjects and items. Journal of memory
and language, 59(4):390–412.
Delogu, C., Conte, S., and Sementina, C. (1998).
Cognitive factors in the evaluation of synthetic
speech. Speech Communication, 24(2):153–168.
Demberg, V. and Keller, F. (2008). Data from
eye-tracking corpora as evidence for theories
of syntactic processing complexity. Cognition,
109:193–210.
Earley, J. (1970). An efficient context-free parsing
algorithm. Commun. ACM, 13(2):94–102.
Fang, R., Chai, J. Y., and Ferreira, F. (2009). Be-
</reference>
<page confidence="0.986721">
365
</page>
<reference confidence="0.96069051948052">
tween linguistic attention and gaze fixations in-
multimodal conversational interfaces. In Inter-
national Conference on Multimodal Interfaces,
pages 143–150.
Florian Jaeger, T. (2010). Redundancy and reduc-
tion: Speakers manage syntactic information den-
sity. Cognitive Psychology, 61(1):23–62.
Frank, A. and Jaeger, T. F. (2008). Speaking ra-
tionally: uniform information density as an opti-
mal strategy for language production. In The 30th
annual meeting of the Cognitive Science Society,
pages 939—944.
Frank, S. (2010). Uncertainty reduction as a mea-
sure of cognitive processing effort. In Proceed-
ings of the 2010 Workshop on Cognitive Model-
ing and Computational Linguistics, pages 81–89,
Uppsala, Sweden.
Gibson, E. (2000). Dependency locality theory: A
distance-dased theory of linguistic complexity. In
Marantz, A., Miyashita, Y., and O’Neil, W., ed-
itors, Image, Language, Brain: Papers from the
First Mind Articulation Project Symposium, pages
95–126. MIT Press, Cambridge, MA.
Hale, J. (2001). A probabilistic Earley parser as
a psycholinguistic model. In Proceedings of the
2nd Conference of the North American Chapter
of the Association for Computational Linguistics,
volume 2, pages 159–166, Pittsburgh, PA.
Jurafsky, D., Bell, A., Gregory, M., and Raymond,
W. (2001). Evidence from reduction in lexical
production. Frequency and the emergence of lin-
guistic structure, 45:229.
Kominek, J. and Black, A. (2003). The cmu
arctic speech databases for speech synthesis
research. Language Technologies Institute,
Carnegie Mellon University, Pittsburgh, PA, Tech.
Rep. CMULTI-03-177 http://festvox. org/cmuarc-
tic.
Kuperman, V., Pluymaekers, M., Ernestus, M., and
Baayen, H. (2007). Morphological predictability
and acoustic duration of interfixes in dutch com-
pounds. The Journal of the Acoustical Society of
America, 121(4):2261–2271.
Levy, R. (2008). Expectation-based syntactic com-
prehension. Cognition, 106(3):1126–1177.
Levy, R. and Jaeger, T. F. (2007). Speakers opti-
mize information density through syntactic reduc-
tion. In Advances in Neural Information Process-
ing Systems.
Piantadosi, S., Tily, H., and Gibson, E. (2011). Word
lengths are optimized for efficient communica-
tion. Proceedings of the National Academy of Sci-
ences, 108(9).
Pinheiro, J. C. and Bates, D. M. (2000). Mixed-
effects models in S and S-PLUS. Statistics and
computing series. Springer-Verlag.
Roark, B. (2001a). Probabilistic top-down parsing
and language modeling. Computational linguis-
tics, 27(2):249–276.
Roark, B. (2001b). Robust probabilistic predictive
syntactic processing: motivations, models, and
applications. PhD thesis, Brown University.
Roark, B., Bachrach, A., Cardenas, C., and Pal-
lier, C. (2009). Deriving lexical and syntactic
expectation-based measures for psycholinguistic
modeling via incremental top-down parsing. In
Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing, pages
324–333, Singapore. Association for Computa-
tional Linguistics.
Rosenkrantz, D. J. and II, P. M. L. (1970). Deter-
ministic left corner parsing (extended abstract). In
SWAT (FOCS), pages 139—152.
Schr¨oder, M., Charfuelan, M., Pammi, S., and T¨urk,
O. (2008). The MARY TTS entry in the Bliz-
zard Challenge 2008. In Proc. Blizzard Chal-
lenge. Citeseer.
</reference>
<bodyText confidence="0.915477538461538">
Swift, M. D., Campana, E., Allen, J. F., and Tanen-
haus, M. K. (2002). Monitoring eye movements
as an evaluation of synthesized speech. In Pro-
ceedings of the IEEE 2002 Workshop on Speech
Synthesis.
Taube-Schiff, M. and Segalowitz, N. (2005). Lin-
guistic attention control: attention shifting gov-
erned by grammaticized elements of language.
Journal of experimental psychology Learning
memory and cognition, 31(3):508–519.
Zen, H., Nose, T., Yamagishi, J., Sako, S., Masuko,
T., Black, A., and Tokuda, K. (2007). The HMM-
based speech synthesis system (HTS) version 2.0.
</bodyText>
<page confidence="0.997405">
366
</page>
<reference confidence="0.7646715">
In Proc. of Sixth ISCA Workshop on Speech Syn-
thesis, pages 294–299.
</reference>
<page confidence="0.998028">
367
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.199433">
<title confidence="0.818632">Syntactic surprisal affects spoken word duration in conversational contexts</title>
<author confidence="0.621486">Vera Demberg</author>
<author confidence="0.621486">Asad B Sayeed</author>
<author confidence="0.621486">Philip J Gorinski</author>
<author confidence="0.621486">Nikolaos MCI Cluster of Excellence</author>
<affiliation confidence="0.767389">Department of Computational Linguistics and Saarland</affiliation>
<address confidence="0.826561">66143 Saarbr¨ucken,</address>
<abstract confidence="0.999214809523809">We present results of a novel experiment to investigate speech production in conversational data that links speech rate to information density. We provide the first evidence for an association between syntactic surprisal and word duration in recorded speech. Using the AMI corpus which contains transcriptions of focus group meetings with precise word durations, we show that word durations correlate with syntactic surprisal estimated from the incremental Roark parser over and above simpler measures, such as word duration estimated from a state-of-the-art text-to-speech system and word frequencies, and that the syntactic surprisal estimates are better predictors of word durations than a simpler version of surprisal based on trigram probabilities. This result supports the uniform information density (UID) hypothesis and points a way to more realistic artificial speech generation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Aylett</author>
<author>A Turk</author>
</authors>
<title>Language redundancy predicts syllabic duration and the spectral characteristics of vocalic syllable nuclei.</title>
<date>2006</date>
<journal>Journal of the acoustical society of America,</journal>
<volume>119</volume>
<issue>5</issue>
<contexts>
<context position="4156" citStr="Aylett and Turk (2006)" startWordPosition="615" endWordPosition="618"> on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 356–367, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics known processing phenomena that were believed to emerge from structural ambiguities (e.g., garden paths) and Levy (2008) further demonstrated the relevance of surprisal to human sentence processing difficulty on a range of syntactic processing difficulty phenomena. There is existing work in correlating informationtheoretic measures of linguistic redundancy to the observed duration of speech units. Aylett and Turk (2006) demonstrate that the contextual predictability of a syllable (n-gram log probability) has an inverse relationship to syllable duration in speech. Their experiments were performed using a carefully articulated speech synthesis training corpus. This type of work fits into a larger programme of understanding how speakers schedule utterances to avoid high variation in the transmission of linguistic information over time, also known as the Uniform Information Density (UID) hypothesis (Florian Jaeger, 2010). Levy and Jaeger (2007) show that the reduction of optional that-complementizers in English </context>
</contexts>
<marker>Aylett, Turk, 2006</marker>
<rawString>Aylett, M. and Turk, A. (2006). Language redundancy predicts syllabic duration and the spectral characteristics of vocalic syllable nuclei. Journal of the acoustical society of America, 119(5):3048–3059.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Baayen</author>
<author>D Davidson</author>
<author>D Bates</author>
</authors>
<title>Mixed-effects modeling with crossed random effects for subjects and items.</title>
<date>2008</date>
<journal>Journal of memory and language,</journal>
<pages>59--4</pages>
<contexts>
<context position="12081" citStr="Baayen et al., 2008" startWordPosition="1860" endWordPosition="1863">rack of each word’s position in the corpus by conversation ID, speaker turn, and chronological order. As we describe in the next section, we filter the words for anomalies. After pre-processing, for each word in the corpus, we extract the following predictors: canonical speech durations from the MARY text-to-speech system, logarithmic word frequencies, n-gram surprisal, and surprisal values produced by the Roark (2001a); Roark et al. (2009) parser (see Section 4). The next sections describe how and from where these values are obtained1. Finally, we run mixed effects regression model analyses (Baayen et al., 2008) with the observed durations as a response variable and the predictors mentioned above in order to detect whether syntactic surprisal is a significant positive predictor of spoken word durations above and beyond the more basic effects of canonical word duration and word frequency. 1We will make this data widely available upon publication. 358 Observations Observed timings AMI corpus Computed timings MARY Gigaword Gigaword freq. AMI word freq. Word filtration and selection PTB n-gram surprisal Gigaword n-gram surprisal CMU toolkit Penn Treebank AMI n-gram surprisal Roark syntactic surprisal Roa</context>
</contexts>
<marker>Baayen, Davidson, Bates, 2008</marker>
<rawString>Baayen, R., Davidson, D., and Bates, D. (2008). Mixed-effects modeling with crossed random effects for subjects and items. Journal of memory and language, 59(4):390–412.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Delogu</author>
<author>S Conte</author>
<author>C Sementina</author>
</authors>
<title>Cognitive factors in the evaluation of synthetic speech.</title>
<date>1998</date>
<journal>Speech Communication,</journal>
<volume>24</volume>
<issue>2</issue>
<contexts>
<context position="36644" citStr="Delogu et al., 1998" startWordPosition="5831" endWordPosition="5834">e fact that frequency and syntactic surprisal have a significant effect beyond what a HMM-trained TTS model would predict for individual words is a case for further research into incorporating syntactic models into speech production systems. Our methodology immediately provides a framework for estimating the word-by-word effect on duration for increased naturalness in TTS output. This is relevant to spoken dialogue systems because it appears that synthesized speech requires a greater level of attention from the dialogue system users when compared to the same words delivered in natural speech (Delogu et al., 1998). Some of this effect may be attributable to peaks in information density which are caused by current generation systems not compensating for areas of high information density through speech rate, lexical and structural choice. Furthermore, syntax and semantics have been observed to interact with the mode of speech delivery. Eye-tracking experiments by Swift et al. (2002) showed that there was a synthetic vs. natural speech difference in the time required to pay attention to an object referred to using definite articles, but not indefinite articles. Our result points a way towards a direction </context>
</contexts>
<marker>Delogu, Conte, Sementina, 1998</marker>
<rawString>Delogu, C., Conte, S., and Sementina, C. (1998). Cognitive factors in the evaluation of synthetic speech. Speech Communication, 24(2):153–168.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Demberg</author>
<author>F Keller</author>
</authors>
<title>Data from eye-tracking corpora as evidence for theories of syntactic processing complexity.</title>
<date>2008</date>
<journal>Cognition,</journal>
<pages>109--193</pages>
<contexts>
<context position="8040" citStr="Demberg and Keller (2008)" startWordPosition="1230" endWordPosition="1234">ocused on the morphological level. They hypothesize that this counter-intuitive result may be driven by complex paradigmatic constraints in the choice of morpheme. Our work, however, focuses on the syntactic level rather than the paradigmatic. What we seek to answer in our work is the extent to which an information density-based analysis can not only be applied to real speech data in context but also be derived from higher-level syntactic analyses, a combination hitherto little explored. Existing broadcoverage work on syntactic surprisal has largely focused on comprehension phenomena, such as Demberg and Keller (2008), Roark et al. (2009), and Frank (2010). We provide a production study in a vein similar to that of Kuperman et al., but show that frequency effects work in the expected direction at the syntactic level. This in turn expands upon the view supported by n-gram-based work such as that 357 of Piantadosi et al. (2011); Levy and Jaeger (2007); Jurafsky et al. (2001), showing that information content above the n-gram level is important in guiding spoken language production in humans. 1.2 Implications for Potential Applications Spoken dialogue systems are of increasing economic and technological impor</context>
<context position="20168" citStr="Demberg and Keller, 2008" startWordPosition="3141" endWordPosition="3144">imately one million tokens. We generated this model because it is the underlying training data for the Roark parser, described below. Syntactic Surprisal from Roark parser In order to capture the effect of syntactically expected vs. unexpected events, we can calculate the syntactic surprisal of each word in a sentence. The syntactic surprisal at word S,,,, is defined as the difference between the prefix probability at word wi and the prefix probability at word wi−1. The prefix probability at word wi is the sum of the probabilities of all trees T spanning words w1 ... wi; see also (Levy, 2008; Demberg and Keller, 2008). �S�i = log �P (T, w1..wi�1) − log P(T, w1..wi) T T The top-down incremental Roark parser (Roark, 2001a) has the characteristic that all partial left-toright parses are rooted: they form a single tree with one root. A set of heuristics ensures that rule application occurs only through node expansion within the connected structure.4 The grammar-derived prefix probabilities of a given sentence prefix can there4The formulae for the calculation of the prefix probabilities from the PCFG rules can be found in Roark et al. (2009). Figure 2: Top-ranked partial parse of A puppy is to a dog what a kitt</context>
</contexts>
<marker>Demberg, Keller, 2008</marker>
<rawString>Demberg, V. and Keller, F. (2008). Data from eye-tracking corpora as evidence for theories of syntactic processing complexity. Cognition, 109:193–210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Earley</author>
</authors>
<title>An efficient context-free parsing algorithm.</title>
<date>1970</date>
<journal>Commun. ACM,</journal>
<volume>13</volume>
<issue>2</issue>
<contexts>
<context position="21207" citStr="Earley, 1970" startWordPosition="3319" endWordPosition="3320">for the calculation of the prefix probabilities from the PCFG rules can be found in Roark et al. (2009). Figure 2: Top-ranked partial parse of A puppy is to a dog what a kitten is to a cat., stopping at the second a and providing the Roark parser surprisal values by word. The branch with dashed lines and struck-out symbols represents an analysis abandoned at the appearance of the a. fore be calculated directly by multiplying the probabilities of all rules used to generate the prefix tree. The Roark parser shares this characteristic of generating fully connected structures with Earley parsers (Earley, 1970) and left corner parsers (Rosenkrantz and II, 1970). The Roark parser uses a beam search. As the amount of probability mass lost has been shown to be small (Roark, 2001b), the surprisal estimates can be assumed to be a good approximation. The beam width of the parser search is controlled by a “base parsing threshold”, which defines the distance in terms of natural log-probability between the most probable parse and the least probable parse within the beam. For the experiments reported here, the parsing beam was set to 21 (default setting is 12). A wider beam also reduces the effects of pruning</context>
</contexts>
<marker>Earley, 1970</marker>
<rawString>Earley, J. (1970). An efficient context-free parsing algorithm. Commun. ACM, 13(2):94–102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Fang</author>
<author>J Y Chai</author>
<author>F Ferreira</author>
</authors>
<title>Between linguistic attention and gaze fixations inmultimodal conversational interfaces.</title>
<date>2009</date>
<booktitle>In International Conference on Multimodal Interfaces,</booktitle>
<pages>143--150</pages>
<contexts>
<context position="8972" citStr="Fang et al., 2009" startWordPosition="1378" endWordPosition="1381"> Levy and Jaeger (2007); Jurafsky et al. (2001), showing that information content above the n-gram level is important in guiding spoken language production in humans. 1.2 Implications for Potential Applications Spoken dialogue systems are of increasing economic and technological importance in recent times, particularly as it is now feasible to include this technology in everything from small consumer devices to industrial equipment. With this increase in importance, there is also unsurprisingly growing scientific emphasis in understanding its usability and safety characteristics. Recent work (Fang et al., 2009; Taube-Schiff and Segalowitz, 2005) has shown that linguistic information presentation has an effect on user behaviour, but the overall granularity of this behaviour is still not well-understood. Other potential applications exist in any place where text-to-speech technologies can be applied, such as in real-time spoken machine translation and communications systems for the disabled. In demonstrating that we can observe speakers behaving in the manner predicted by the UID hypothesis in conversational contexts, we provide evidence for a finer-level of granularity necessary for controlling the </context>
</contexts>
<marker>Fang, Chai, Ferreira, 2009</marker>
<rawString>Fang, R., Chai, J. Y., and Ferreira, F. (2009). Between linguistic attention and gaze fixations inmultimodal conversational interfaces. In International Conference on Multimodal Interfaces, pages 143–150.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Florian Jaeger</author>
<author>T</author>
</authors>
<title>Redundancy and reduction: Speakers manage syntactic information density.</title>
<date>2010</date>
<journal>Cognitive Psychology,</journal>
<volume>61</volume>
<issue>1</issue>
<marker>Jaeger, T, 2010</marker>
<rawString>Florian Jaeger, T. (2010). Redundancy and reduction: Speakers manage syntactic information density. Cognitive Psychology, 61(1):23–62.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Frank</author>
<author>T F Jaeger</author>
</authors>
<title>Speaking rationally: uniform information density as an optimal strategy for language production.</title>
<date>2008</date>
<booktitle>In The 30th annual meeting of the Cognitive Science Society,</booktitle>
<pages>939--944</pages>
<contexts>
<context position="1411" citStr="Frank and Jaeger, 2008" startWordPosition="192" endWordPosition="195">the incremental Roark parser over and above simpler measures, such as word duration estimated from a state-of-the-art text-to-speech system and word frequencies, and that the syntactic surprisal estimates are better predictors of word durations than a simpler version of surprisal based on trigram probabilities. This result supports the uniform information density (UID) hypothesis and points a way to more realistic artificial speech generation. 1 Introduction The uniform information density (UID) hypothesis suggests that speakers try to distribute information uniformly across their utterances (Frank and Jaeger, 2008). Information density can be measured in terms of the surprisal incurred at each word, where surprisal is defined as the negative log-probability of an event. This paper sets out to test whether UID holds across different linguistic levels, i.e. whether speakers adapt word duration during production to syntactic surprisal, such that words with higher surprisal have longer durations than words with lower surprisal. We investigate this question in a corpus of transcribed speech from a mix of native and nonnative English speakers, a population that is a nontrivial component of the user base for l</context>
<context position="5079" citStr="Frank and Jaeger (2008)" startWordPosition="754" endWordPosition="757">tanding how speakers schedule utterances to avoid high variation in the transmission of linguistic information over time, also known as the Uniform Information Density (UID) hypothesis (Florian Jaeger, 2010). Levy and Jaeger (2007) show that the reduction of optional that-complementizers in English is related to trigram surprisal; low surprisal predicts a high likelihood of reduction. Florian Jaeger (2010) shows the same result of increased reduction when the complementizer is more predictable according to information density calculated in terms of the main verb’s subcategorization frequency. Frank and Jaeger (2008) provide evidence that a UID account can predict the use of reduced forms of “be”, “have”, and “not” in English. They use the surprisal of the candidate word itself as well as surprisals of the word before and after, computing bigram and trigram estimates directly from the corpus without smoothing or backoff. Jurafsky et al. (2001) report a corpus study similar to ours, showing that words that are more predictable from context are reduced. As measures of word predictability, they use bigram and trigram models, as well as joint probabilities, but not syntactic surprisal. Within the same theme o</context>
</contexts>
<marker>Frank, Jaeger, 2008</marker>
<rawString>Frank, A. and Jaeger, T. F. (2008). Speaking rationally: uniform information density as an optimal strategy for language production. In The 30th annual meeting of the Cognitive Science Society, pages 939—944.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Frank</author>
</authors>
<title>Uncertainty reduction as a measure of cognitive processing effort.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Workshop on Cognitive Modeling and Computational Linguistics,</booktitle>
<pages>81--89</pages>
<location>Uppsala,</location>
<contexts>
<context position="8079" citStr="Frank (2010)" startWordPosition="1240" endWordPosition="1241">that this counter-intuitive result may be driven by complex paradigmatic constraints in the choice of morpheme. Our work, however, focuses on the syntactic level rather than the paradigmatic. What we seek to answer in our work is the extent to which an information density-based analysis can not only be applied to real speech data in context but also be derived from higher-level syntactic analyses, a combination hitherto little explored. Existing broadcoverage work on syntactic surprisal has largely focused on comprehension phenomena, such as Demberg and Keller (2008), Roark et al. (2009), and Frank (2010). We provide a production study in a vein similar to that of Kuperman et al., but show that frequency effects work in the expected direction at the syntactic level. This in turn expands upon the view supported by n-gram-based work such as that 357 of Piantadosi et al. (2011); Levy and Jaeger (2007); Jurafsky et al. (2001), showing that information content above the n-gram level is important in guiding spoken language production in humans. 1.2 Implications for Potential Applications Spoken dialogue systems are of increasing economic and technological importance in recent times, particularly as </context>
</contexts>
<marker>Frank, 2010</marker>
<rawString>Frank, S. (2010). Uncertainty reduction as a measure of cognitive processing effort. In Proceedings of the 2010 Workshop on Cognitive Modeling and Computational Linguistics, pages 81–89, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Gibson</author>
</authors>
<title>Dependency locality theory: A distance-dased theory of linguistic complexity.</title>
<date>2000</date>
<booktitle>Papers from the First Mind Articulation Project Symposium,</booktitle>
<pages>95--126</pages>
<editor>In Marantz, A., Miyashita, Y., and O’Neil, W., editors, Image, Language, Brain:</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<marker>Gibson, 2000</marker>
<rawString>Gibson, E. (2000). Dependency locality theory: A distance-dased theory of linguistic complexity. In Marantz, A., Miyashita, Y., and O’Neil, W., editors, Image, Language, Brain: Papers from the First Mind Articulation Project Symposium, pages 95–126. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hale</author>
</authors>
<title>A probabilistic Earley parser as a psycholinguistic model.</title>
<date>2001</date>
<booktitle>In Proceedings of the 2nd Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<volume>2</volume>
<pages>159--166</pages>
<location>Pittsburgh, PA.</location>
<contexts>
<context position="3398" citStr="Hale (2001)" startWordPosition="512" endWordPosition="513">essing difficulty is based on the notion that processing difficulty results when a word is encountered that is unexpected given its preceding context. The amount of surprisal on a word wi can be formalized as the log of the inverse conditional probability of wi given the preceding words in the sentence w1 ... wi−1, or − log P(wi|w1...i−1). If this probability is low, then the word is unexpected, and surprisal is high. Surprisal can be estimated in different ways, e.g. from word sequences (n-grams) or with respect to the possible syntactic structures covering a sentence prefix (see Section 4). Hale (2001) showed that surprisal calculated from a probabilistic Earley parser correctly predicts well356 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 356–367, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics known processing phenomena that were believed to emerge from structural ambiguities (e.g., garden paths) and Levy (2008) further demonstrated the relevance of surprisal to human sentence processing difficulty on a range of syntactic processing difficulty phenomena.</context>
</contexts>
<marker>Hale, 2001</marker>
<rawString>Hale, J. (2001). A probabilistic Earley parser as a psycholinguistic model. In Proceedings of the 2nd Conference of the North American Chapter of the Association for Computational Linguistics, volume 2, pages 159–166, Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Jurafsky</author>
<author>A Bell</author>
<author>M Gregory</author>
<author>W Raymond</author>
</authors>
<title>Evidence from reduction in lexical production. Frequency and the emergence of linguistic structure,</title>
<date>2001</date>
<pages>45--229</pages>
<contexts>
<context position="5412" citStr="Jurafsky et al. (2001)" startWordPosition="812" endWordPosition="815">ow surprisal predicts a high likelihood of reduction. Florian Jaeger (2010) shows the same result of increased reduction when the complementizer is more predictable according to information density calculated in terms of the main verb’s subcategorization frequency. Frank and Jaeger (2008) provide evidence that a UID account can predict the use of reduced forms of “be”, “have”, and “not” in English. They use the surprisal of the candidate word itself as well as surprisals of the word before and after, computing bigram and trigram estimates directly from the corpus without smoothing or backoff. Jurafsky et al. (2001) report a corpus study similar to ours, showing that words that are more predictable from context are reduced. As measures of word predictability, they use bigram and trigram models, as well as joint probabilities, but not syntactic surprisal. Within the same theme of utterance duration vs. information content, Piantadosi et al. (2011) performed a study using Google-derived n-gram datasets on the lexica of multiple languages, including English, Portuguese, and Czech. For every word in a given language’s lexicon, they calculated 2-, 3-, and 4-gram surprisal values using the Google dataset for e</context>
<context position="8402" citStr="Jurafsky et al. (2001)" startWordPosition="1295" endWordPosition="1298">o real speech data in context but also be derived from higher-level syntactic analyses, a combination hitherto little explored. Existing broadcoverage work on syntactic surprisal has largely focused on comprehension phenomena, such as Demberg and Keller (2008), Roark et al. (2009), and Frank (2010). We provide a production study in a vein similar to that of Kuperman et al., but show that frequency effects work in the expected direction at the syntactic level. This in turn expands upon the view supported by n-gram-based work such as that 357 of Piantadosi et al. (2011); Levy and Jaeger (2007); Jurafsky et al. (2001), showing that information content above the n-gram level is important in guiding spoken language production in humans. 1.2 Implications for Potential Applications Spoken dialogue systems are of increasing economic and technological importance in recent times, particularly as it is now feasible to include this technology in everything from small consumer devices to industrial equipment. With this increase in importance, there is also unsurprisingly growing scientific emphasis in understanding its usability and safety characteristics. Recent work (Fang et al., 2009; Taube-Schiff and Segalowitz,</context>
</contexts>
<marker>Jurafsky, Bell, Gregory, Raymond, 2001</marker>
<rawString>Jurafsky, D., Bell, A., Gregory, M., and Raymond, W. (2001). Evidence from reduction in lexical production. Frequency and the emergence of linguistic structure, 45:229.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Kominek</author>
<author>A Black</author>
</authors>
<title>The cmu arctic speech databases for speech synthesis research.</title>
<date>2003</date>
<tech>Tech. Rep. CMULTI-03-177</tech>
<institution>Language Technologies Institute, Carnegie Mellon University,</institution>
<location>Pittsburgh, PA,</location>
<note>http://festvox. org/cmuarctic.</note>
<contexts>
<context position="15628" citStr="Kominek and Black, 2003" startWordPosition="2428" endWordPosition="2431"> word durations would be to calculate the average duration of each word in the corpus. However, this approach would be insensitive to the phonological, syllabic and phrasal context that a word occurs in, which can have a large effect on word duration. Therefore, we use word duration estimates from the state-of-the-art open-source text-to-speech system MARY (Schr¨oder et al., 2008, version 4.3.1), with the default voice package included in this version (cmu-slt-hsmm). The cmu-slt-hsmm voice package uses a Hidden Markov model, trained on the female US English section of the CMU ARCTIC database (Kominek and Black, 2003), to predict prosodic attributes of each individual synthesized phone, including duration. Training was carried out using a version of the HTS system (Zen et al., 2007), modified for using the MARY context features (Schr¨oder et al., 2008) for estimating the parameters of the model and for decoding. Those features include3: • phonological features of the current and neighboring phonemes • syllabic and lexical features (e.g. syllable stress, (estimated) part-of-speech, position of syllable in word) • phrasal / sentential features (e.g. sentence/phrase boundaries, neighboring pauses and punctuat</context>
</contexts>
<marker>Kominek, Black, 2003</marker>
<rawString>Kominek, J. and Black, A. (2003). The cmu arctic speech databases for speech synthesis research. Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA, Tech. Rep. CMULTI-03-177 http://festvox. org/cmuarctic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Kuperman</author>
<author>M Pluymaekers</author>
<author>M Ernestus</author>
<author>H Baayen</author>
</authors>
<title>Morphological predictability and acoustic duration of interfixes in dutch compounds.</title>
<date>2007</date>
<journal>The Journal of the Acoustical Society of America,</journal>
<volume>121</volume>
<issue>4</issue>
<contexts>
<context position="6526" citStr="Kuperman et al. (2007)" startWordPosition="987" endWordPosition="990">given language’s lexicon, they calculated 2-, 3-, and 4-gram surprisal values using the Google dataset for every occurrence of the word, and then they took the mean surprisal for that word over all occurrences. The 3-gram surprisal values in particular were a better predictor of orthographic length than unigram frequency, providing evidence for the use of information content and contextual predictability as improvement over a Zipf’s Law view of communicative efficiency. This is an n-gram approach to supporting the UID hypothesis. However, there is some counter-evidence for the UID-based view. Kuperman et al. (2007) analyzed the relationship between linguistic unit predictability and syllable duration in read-aloud speech in Dutch. Dutch makes use of interfix morphemes -s- and -e(n)- in certain contexts to make compound nouns, preferring a null interfix in most cases. For example, the Dutch noun kandidaatsexamen (“Bachelor’s examination”) is composed of kandidaat-, -s-, and -examen. Kuperman et al. find that the greater the predictability of the interfix from the morphological context (i.e., the surrounding members of the compound), the longer the duration of the pronunciation of the interfix. To illustr</context>
</contexts>
<marker>Kuperman, Pluymaekers, Ernestus, Baayen, 2007</marker>
<rawString>Kuperman, V., Pluymaekers, M., Ernestus, M., and Baayen, H. (2007). Morphological predictability and acoustic duration of interfixes in dutch compounds. The Journal of the Acoustical Society of America, 121(4):2261–2271.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Levy</author>
</authors>
<title>Expectation-based syntactic comprehension.</title>
<date>2008</date>
<journal>Cognition,</journal>
<volume>106</volume>
<issue>3</issue>
<contexts>
<context position="3853" citStr="Levy (2008)" startWordPosition="573" endWordPosition="574">ferent ways, e.g. from word sequences (n-grams) or with respect to the possible syntactic structures covering a sentence prefix (see Section 4). Hale (2001) showed that surprisal calculated from a probabilistic Earley parser correctly predicts well356 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 356–367, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics known processing phenomena that were believed to emerge from structural ambiguities (e.g., garden paths) and Levy (2008) further demonstrated the relevance of surprisal to human sentence processing difficulty on a range of syntactic processing difficulty phenomena. There is existing work in correlating informationtheoretic measures of linguistic redundancy to the observed duration of speech units. Aylett and Turk (2006) demonstrate that the contextual predictability of a syllable (n-gram log probability) has an inverse relationship to syllable duration in speech. Their experiments were performed using a carefully articulated speech synthesis training corpus. This type of work fits into a larger programme of und</context>
<context position="20141" citStr="Levy, 2008" startWordPosition="3139" endWordPosition="3140">ts of approximately one million tokens. We generated this model because it is the underlying training data for the Roark parser, described below. Syntactic Surprisal from Roark parser In order to capture the effect of syntactically expected vs. unexpected events, we can calculate the syntactic surprisal of each word in a sentence. The syntactic surprisal at word S,,,, is defined as the difference between the prefix probability at word wi and the prefix probability at word wi−1. The prefix probability at word wi is the sum of the probabilities of all trees T spanning words w1 ... wi; see also (Levy, 2008; Demberg and Keller, 2008). �S�i = log �P (T, w1..wi�1) − log P(T, w1..wi) T T The top-down incremental Roark parser (Roark, 2001a) has the characteristic that all partial left-toright parses are rooted: they form a single tree with one root. A set of heuristics ensures that rule application occurs only through node expansion within the connected structure.4 The grammar-derived prefix probabilities of a given sentence prefix can there4The formulae for the calculation of the prefix probabilities from the PCFG rules can be found in Roark et al. (2009). Figure 2: Top-ranked partial parse of A pu</context>
</contexts>
<marker>Levy, 2008</marker>
<rawString>Levy, R. (2008). Expectation-based syntactic comprehension. Cognition, 106(3):1126–1177.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Levy</author>
<author>T F Jaeger</author>
</authors>
<title>Speakers optimize information density through syntactic reduction.</title>
<date>2007</date>
<booktitle>In Advances in Neural Information Processing Systems.</booktitle>
<contexts>
<context position="4687" citStr="Levy and Jaeger (2007)" startWordPosition="695" endWordPosition="698">s of linguistic redundancy to the observed duration of speech units. Aylett and Turk (2006) demonstrate that the contextual predictability of a syllable (n-gram log probability) has an inverse relationship to syllable duration in speech. Their experiments were performed using a carefully articulated speech synthesis training corpus. This type of work fits into a larger programme of understanding how speakers schedule utterances to avoid high variation in the transmission of linguistic information over time, also known as the Uniform Information Density (UID) hypothesis (Florian Jaeger, 2010). Levy and Jaeger (2007) show that the reduction of optional that-complementizers in English is related to trigram surprisal; low surprisal predicts a high likelihood of reduction. Florian Jaeger (2010) shows the same result of increased reduction when the complementizer is more predictable according to information density calculated in terms of the main verb’s subcategorization frequency. Frank and Jaeger (2008) provide evidence that a UID account can predict the use of reduced forms of “be”, “have”, and “not” in English. They use the surprisal of the candidate word itself as well as surprisals of the word before an</context>
<context position="8378" citStr="Levy and Jaeger (2007)" startWordPosition="1291" endWordPosition="1294">an not only be applied to real speech data in context but also be derived from higher-level syntactic analyses, a combination hitherto little explored. Existing broadcoverage work on syntactic surprisal has largely focused on comprehension phenomena, such as Demberg and Keller (2008), Roark et al. (2009), and Frank (2010). We provide a production study in a vein similar to that of Kuperman et al., but show that frequency effects work in the expected direction at the syntactic level. This in turn expands upon the view supported by n-gram-based work such as that 357 of Piantadosi et al. (2011); Levy and Jaeger (2007); Jurafsky et al. (2001), showing that information content above the n-gram level is important in guiding spoken language production in humans. 1.2 Implications for Potential Applications Spoken dialogue systems are of increasing economic and technological importance in recent times, particularly as it is now feasible to include this technology in everything from small consumer devices to industrial equipment. With this increase in importance, there is also unsurprisingly growing scientific emphasis in understanding its usability and safety characteristics. Recent work (Fang et al., 2009; Taub</context>
</contexts>
<marker>Levy, Jaeger, 2007</marker>
<rawString>Levy, R. and Jaeger, T. F. (2007). Speakers optimize information density through syntactic reduction. In Advances in Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Piantadosi</author>
<author>H Tily</author>
<author>E Gibson</author>
</authors>
<title>Word lengths are optimized for efficient communication.</title>
<date>2011</date>
<booktitle>Proceedings of the National Academy of Sciences,</booktitle>
<volume>108</volume>
<issue>9</issue>
<contexts>
<context position="5749" citStr="Piantadosi et al. (2011)" startWordPosition="867" endWordPosition="870">ct the use of reduced forms of “be”, “have”, and “not” in English. They use the surprisal of the candidate word itself as well as surprisals of the word before and after, computing bigram and trigram estimates directly from the corpus without smoothing or backoff. Jurafsky et al. (2001) report a corpus study similar to ours, showing that words that are more predictable from context are reduced. As measures of word predictability, they use bigram and trigram models, as well as joint probabilities, but not syntactic surprisal. Within the same theme of utterance duration vs. information content, Piantadosi et al. (2011) performed a study using Google-derived n-gram datasets on the lexica of multiple languages, including English, Portuguese, and Czech. For every word in a given language’s lexicon, they calculated 2-, 3-, and 4-gram surprisal values using the Google dataset for every occurrence of the word, and then they took the mean surprisal for that word over all occurrences. The 3-gram surprisal values in particular were a better predictor of orthographic length than unigram frequency, providing evidence for the use of information content and contextual predictability as improvement over a Zipf’s Law view</context>
<context position="8354" citStr="Piantadosi et al. (2011)" startWordPosition="1287" endWordPosition="1290">n density-based analysis can not only be applied to real speech data in context but also be derived from higher-level syntactic analyses, a combination hitherto little explored. Existing broadcoverage work on syntactic surprisal has largely focused on comprehension phenomena, such as Demberg and Keller (2008), Roark et al. (2009), and Frank (2010). We provide a production study in a vein similar to that of Kuperman et al., but show that frequency effects work in the expected direction at the syntactic level. This in turn expands upon the view supported by n-gram-based work such as that 357 of Piantadosi et al. (2011); Levy and Jaeger (2007); Jurafsky et al. (2001), showing that information content above the n-gram level is important in guiding spoken language production in humans. 1.2 Implications for Potential Applications Spoken dialogue systems are of increasing economic and technological importance in recent times, particularly as it is now feasible to include this technology in everything from small consumer devices to industrial equipment. With this increase in importance, there is also unsurprisingly growing scientific emphasis in understanding its usability and safety characteristics. Recent work </context>
</contexts>
<marker>Piantadosi, Tily, Gibson, 2011</marker>
<rawString>Piantadosi, S., Tily, H., and Gibson, E. (2011). Word lengths are optimized for efficient communication. Proceedings of the National Academy of Sciences, 108(9).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J C Pinheiro</author>
<author>D M Bates</author>
</authors>
<title>Mixedeffects models in S and S-PLUS. Statistics and computing series.</title>
<date>2000</date>
<publisher>Springer-Verlag.</publisher>
<contexts>
<context position="23005" citStr="Pinheiro and Bates (2000)" startWordPosition="3620" endWordPosition="3623">.089 PP TO NP S VP 361 esting to evaluate these two aspects of syntactic surprisal separately, and the Roark parser conveniently outputs both surprisal estimates. Structural surprisal is estimated from the occurrence counts of the application of syntactic rules during the parse discounting the effect of lexical probabilities, while lexical surprisal is calculated from the probabilities of the derivational step from the POS-tag to lexical item. 5 Linear mixed effects modelling In order to test whether surprisal estimates correlate with speech durations, we use linear mixed effects models (LME, Pinheiro and Bates (2000)). This type of model can be thought of as a generalization of linear regression that allows the inclusion of random factors as well as fixed factors.We treat speakers as a random factor, which means that our models contain an intercept term for each speaker, representing the individual differences in speech rates. Furthermore, we include a random slope for the predictors (e.g. frequency, canonical duration, surprisal), essentially accounting for idiosyncrasies of a participant with respect to the predictor, such that only the part of the variance that is common to all participants and is attr</context>
</contexts>
<marker>Pinheiro, Bates, 2000</marker>
<rawString>Pinheiro, J. C. and Bates, D. M. (2000). Mixedeffects models in S and S-PLUS. Statistics and computing series. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Roark</author>
</authors>
<title>Probabilistic top-down parsing and language modeling.</title>
<date>2001</date>
<booktitle>Computational linguistics,</booktitle>
<pages>27--2</pages>
<contexts>
<context position="11882" citStr="Roark (2001" startWordPosition="1830" endWordPosition="1831">ng remarks in section 7. 2 Design The overall design of our experiment is schematically depicted in Figure 1. We extract the words and the word-by-word timings from the AMI corpus, keeping track of each word’s position in the corpus by conversation ID, speaker turn, and chronological order. As we describe in the next section, we filter the words for anomalies. After pre-processing, for each word in the corpus, we extract the following predictors: canonical speech durations from the MARY text-to-speech system, logarithmic word frequencies, n-gram surprisal, and surprisal values produced by the Roark (2001a); Roark et al. (2009) parser (see Section 4). The next sections describe how and from where these values are obtained1. Finally, we run mixed effects regression model analyses (Baayen et al., 2008) with the observed durations as a response variable and the predictors mentioned above in order to detect whether syntactic surprisal is a significant positive predictor of spoken word durations above and beyond the more basic effects of canonical word duration and word frequency. 1We will make this data widely available upon publication. 358 Observations Observed timings AMI corpus Computed timing</context>
<context position="20271" citStr="Roark, 2001" startWordPosition="3162" endWordPosition="3163">described below. Syntactic Surprisal from Roark parser In order to capture the effect of syntactically expected vs. unexpected events, we can calculate the syntactic surprisal of each word in a sentence. The syntactic surprisal at word S,,,, is defined as the difference between the prefix probability at word wi and the prefix probability at word wi−1. The prefix probability at word wi is the sum of the probabilities of all trees T spanning words w1 ... wi; see also (Levy, 2008; Demberg and Keller, 2008). �S�i = log �P (T, w1..wi�1) − log P(T, w1..wi) T T The top-down incremental Roark parser (Roark, 2001a) has the characteristic that all partial left-toright parses are rooted: they form a single tree with one root. A set of heuristics ensures that rule application occurs only through node expansion within the connected structure.4 The grammar-derived prefix probabilities of a given sentence prefix can there4The formulae for the calculation of the prefix probabilities from the PCFG rules can be found in Roark et al. (2009). Figure 2: Top-ranked partial parse of A puppy is to a dog what a kitten is to a cat., stopping at the second a and providing the Roark parser surprisal values by word. The </context>
</contexts>
<marker>Roark, 2001</marker>
<rawString>Roark, B. (2001a). Probabilistic top-down parsing and language modeling. Computational linguistics, 27(2):249–276.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Roark</author>
</authors>
<title>Robust probabilistic predictive syntactic processing: motivations, models, and applications.</title>
<date>2001</date>
<tech>PhD thesis,</tech>
<institution>Brown University.</institution>
<contexts>
<context position="11882" citStr="Roark (2001" startWordPosition="1830" endWordPosition="1831">ng remarks in section 7. 2 Design The overall design of our experiment is schematically depicted in Figure 1. We extract the words and the word-by-word timings from the AMI corpus, keeping track of each word’s position in the corpus by conversation ID, speaker turn, and chronological order. As we describe in the next section, we filter the words for anomalies. After pre-processing, for each word in the corpus, we extract the following predictors: canonical speech durations from the MARY text-to-speech system, logarithmic word frequencies, n-gram surprisal, and surprisal values produced by the Roark (2001a); Roark et al. (2009) parser (see Section 4). The next sections describe how and from where these values are obtained1. Finally, we run mixed effects regression model analyses (Baayen et al., 2008) with the observed durations as a response variable and the predictors mentioned above in order to detect whether syntactic surprisal is a significant positive predictor of spoken word durations above and beyond the more basic effects of canonical word duration and word frequency. 1We will make this data widely available upon publication. 358 Observations Observed timings AMI corpus Computed timing</context>
<context position="20271" citStr="Roark, 2001" startWordPosition="3162" endWordPosition="3163">described below. Syntactic Surprisal from Roark parser In order to capture the effect of syntactically expected vs. unexpected events, we can calculate the syntactic surprisal of each word in a sentence. The syntactic surprisal at word S,,,, is defined as the difference between the prefix probability at word wi and the prefix probability at word wi−1. The prefix probability at word wi is the sum of the probabilities of all trees T spanning words w1 ... wi; see also (Levy, 2008; Demberg and Keller, 2008). �S�i = log �P (T, w1..wi�1) − log P(T, w1..wi) T T The top-down incremental Roark parser (Roark, 2001a) has the characteristic that all partial left-toright parses are rooted: they form a single tree with one root. A set of heuristics ensures that rule application occurs only through node expansion within the connected structure.4 The grammar-derived prefix probabilities of a given sentence prefix can there4The formulae for the calculation of the prefix probabilities from the PCFG rules can be found in Roark et al. (2009). Figure 2: Top-ranked partial parse of A puppy is to a dog what a kitten is to a cat., stopping at the second a and providing the Roark parser surprisal values by word. The </context>
</contexts>
<marker>Roark, 2001</marker>
<rawString>Roark, B. (2001b). Robust probabilistic predictive syntactic processing: motivations, models, and applications. PhD thesis, Brown University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Roark</author>
<author>A Bachrach</author>
<author>C Cardenas</author>
<author>C Pallier</author>
</authors>
<title>Deriving lexical and syntactic expectation-based measures for psycholinguistic modeling via incremental top-down parsing.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>324--333</pages>
<institution>Singapore. Association for Computational Linguistics.</institution>
<contexts>
<context position="8061" citStr="Roark et al. (2009)" startWordPosition="1235" endWordPosition="1238"> level. They hypothesize that this counter-intuitive result may be driven by complex paradigmatic constraints in the choice of morpheme. Our work, however, focuses on the syntactic level rather than the paradigmatic. What we seek to answer in our work is the extent to which an information density-based analysis can not only be applied to real speech data in context but also be derived from higher-level syntactic analyses, a combination hitherto little explored. Existing broadcoverage work on syntactic surprisal has largely focused on comprehension phenomena, such as Demberg and Keller (2008), Roark et al. (2009), and Frank (2010). We provide a production study in a vein similar to that of Kuperman et al., but show that frequency effects work in the expected direction at the syntactic level. This in turn expands upon the view supported by n-gram-based work such as that 357 of Piantadosi et al. (2011); Levy and Jaeger (2007); Jurafsky et al. (2001), showing that information content above the n-gram level is important in guiding spoken language production in humans. 1.2 Implications for Potential Applications Spoken dialogue systems are of increasing economic and technological importance in recent times</context>
<context position="11905" citStr="Roark et al. (2009)" startWordPosition="1832" endWordPosition="1835">ection 7. 2 Design The overall design of our experiment is schematically depicted in Figure 1. We extract the words and the word-by-word timings from the AMI corpus, keeping track of each word’s position in the corpus by conversation ID, speaker turn, and chronological order. As we describe in the next section, we filter the words for anomalies. After pre-processing, for each word in the corpus, we extract the following predictors: canonical speech durations from the MARY text-to-speech system, logarithmic word frequencies, n-gram surprisal, and surprisal values produced by the Roark (2001a); Roark et al. (2009) parser (see Section 4). The next sections describe how and from where these values are obtained1. Finally, we run mixed effects regression model analyses (Baayen et al., 2008) with the observed durations as a response variable and the predictors mentioned above in order to detect whether syntactic surprisal is a significant positive predictor of spoken word durations above and beyond the more basic effects of canonical word duration and word frequency. 1We will make this data widely available upon publication. 358 Observations Observed timings AMI corpus Computed timings MARY Gigaword Gigawor</context>
<context position="20697" citStr="Roark et al. (2009)" startWordPosition="3230" endWordPosition="3233">s of all trees T spanning words w1 ... wi; see also (Levy, 2008; Demberg and Keller, 2008). �S�i = log �P (T, w1..wi�1) − log P(T, w1..wi) T T The top-down incremental Roark parser (Roark, 2001a) has the characteristic that all partial left-toright parses are rooted: they form a single tree with one root. A set of heuristics ensures that rule application occurs only through node expansion within the connected structure.4 The grammar-derived prefix probabilities of a given sentence prefix can there4The formulae for the calculation of the prefix probabilities from the PCFG rules can be found in Roark et al. (2009). Figure 2: Top-ranked partial parse of A puppy is to a dog what a kitten is to a cat., stopping at the second a and providing the Roark parser surprisal values by word. The branch with dashed lines and struck-out symbols represents an analysis abandoned at the appearance of the a. fore be calculated directly by multiplying the probabilities of all rules used to generate the prefix tree. The Roark parser shares this characteristic of generating fully connected structures with Earley parsers (Earley, 1970) and left corner parsers (Rosenkrantz and II, 1970). The Roark parser uses a beam search. </context>
</contexts>
<marker>Roark, Bachrach, Cardenas, Pallier, 2009</marker>
<rawString>Roark, B., Bachrach, A., Cardenas, C., and Pallier, C. (2009). Deriving lexical and syntactic expectation-based measures for psycholinguistic modeling via incremental top-down parsing. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 324–333, Singapore. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D J Rosenkrantz</author>
<author>P M L</author>
</authors>
<title>Deterministic left corner parsing (extended abstract).</title>
<date>1970</date>
<booktitle>In SWAT (FOCS),</booktitle>
<pages>139--152</pages>
<marker>Rosenkrantz, L, 1970</marker>
<rawString>Rosenkrantz, D. J. and II, P. M. L. (1970). Deterministic left corner parsing (extended abstract). In SWAT (FOCS), pages 139—152.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Schr¨oder</author>
<author>M Charfuelan</author>
<author>S Pammi</author>
<author>O T¨urk</author>
</authors>
<date>2008</date>
<booktitle>The MARY TTS entry in the Blizzard Challenge</booktitle>
<marker>Schr¨oder, Charfuelan, Pammi, T¨urk, 2008</marker>
<rawString>Schr¨oder, M., Charfuelan, M., Pammi, S., and T¨urk, O. (2008). The MARY TTS entry in the Blizzard Challenge 2008. In Proc. Blizzard Challenge. Citeseer.</rawString>
</citation>
<citation valid="false">
<booktitle>In Proc. of Sixth ISCA Workshop on Speech Synthesis,</booktitle>
<pages>294--299</pages>
<marker></marker>
<rawString>In Proc. of Sixth ISCA Workshop on Speech Synthesis, pages 294–299.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
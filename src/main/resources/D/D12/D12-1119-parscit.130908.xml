<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000731">
<title confidence="0.920652">
Multi-Domain Learning: When Do Domains Matter?
</title>
<author confidence="0.99369">
Mahesh Joshi
</author>
<affiliation confidence="0.877189333333333">
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA, 15213, USA
</affiliation>
<email confidence="0.995335">
maheshj@cs.cmu.edu
</email>
<author confidence="0.995474">
William W. Cohen
</author>
<affiliation confidence="0.987089">
School of Computer Science
Carnegie Mellon University
</affiliation>
<address confidence="0.572925">
Pittsburgh, PA, 15213, USA
</address>
<email confidence="0.999439">
wcohen@cs.cmu.edu
</email>
<author confidence="0.993421">
Mark Dredze
</author>
<affiliation confidence="0.9339935">
Human Language Technology Center of Excellence
Johns Hopkins University
</affiliation>
<address confidence="0.715085">
Baltimore, Maryland 21211
</address>
<email confidence="0.995679">
mdredze@cs.jhu.edu
</email>
<author confidence="0.998476">
Carolyn P. Ros´e
</author>
<affiliation confidence="0.867654">
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA, 15213, USA
</affiliation>
<email confidence="0.998707">
cprose@cs.cmu.edu
</email>
<sectionHeader confidence="0.995642" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999910631578948">
We present a systematic analysis of exist-
ing multi-domain learning approaches with re-
spect to two questions. First, many multi-
domain learning algorithms resemble ensem-
ble learning algorithms. (1) Are multi-domain
learning improvements the result of ensemble
learning effects? Second, these algorithms are
traditionally evaluated in a balanced class la-
bel setting, although in practice many multi-
domain settings have domain-specific class
label biases. When multi-domain learning
is applied to these settings, (2) are multi-
domain methods improving because they cap-
ture domain-specific class biases? An under-
standing of these two issues presents a clearer
idea about where the field has had success in
multi-domain learning, and it suggests some
important open questions for improving be-
yond the current state of the art.
</bodyText>
<sectionHeader confidence="0.999105" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999700022727273">
Research efforts in recent years have demonstrated
the importance of domains in statistical natural lan-
guage processing. A mismatch between training and
test domains can negatively impact system accuracy
as it violates a core assumption in many machine
learning algorithms: that data points are indepen-
dent and identically distributed (i.i.d.). As a result,
numerous domain adaptation methods (Chelba and
Acero, 2004; Daum´e III and Marcu, 2006; Blitzer et
al., 2007) target settings with a training set from one
domain and a test set from another.
Often times the training set itself violates the i.i.d.
assumption and contains multiple domains. In this
case, training a single model obscures domain dis-
tinctions, and separating the dataset by domains re-
duces training data. Instead, multi-domain learn-
ing (MDL) can take advantage of these domain la-
bels to improve learning (Daum´e III, 2007; Dredze
and Crammer, 2008; Arnold et al., 2008; Finkel and
Manning, 2009; Zhang and Yeung, 2010; Saha et al.,
2011). One such example is sentiment classification
of product reviews. Training data is available from
many product categories and while all data should
be used to learn a model, there are important differ-
ences between the categories (Blitzer et al., 2007)1.
While much prior research has shown improve-
ments using MDL, this paper explores what prop-
erties of an MDL setting matter. Are previous im-
provements from MDL algorithms discovering im-
portant distinctions between features in different do-
mains, as we would hope, or are other factors con-
tributing to learning success? The key question of
this paper is: when do domains matter?
Towards this goal we explore two issues. First,
we explore the question of whether domain distinc-
tions are used by existing MDL algorithms in mean-
ingful ways. While differences in feature behaviors
between domains will hurt performance (Blitzer et
al., 2008; Ben-David et al., 2009), it is not clear
if the improvements in MDL algorithms can be at-
tributed to correcting these errors, or whether they
are benefiting from something else. In particular,
there are many similarities between MDL and en-
semble methods, with connections to instance bag-
</bodyText>
<footnote confidence="0.988235333333333">
1Blitzer et al. (2007) do not consider the MDL setup, they
consider a single source domain, and a single target domain,
with little or no labeled data available for the target domain.
</footnote>
<page confidence="0.922796">
1302
</page>
<note confidence="0.792139">
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1302–1312, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.999794095238096">
ging, feature bagging and classifier combination. It
may be that gains in MDL are the usual ensemble
learning improvements.
Second, one simple way in which domains can
change is the distribution of the prior over the la-
bels. For example, reviews of some products may be
more positive on average than reviews of other prod-
uct types. Simply capturing this bias may account
for significant gains in accuracy, even though noth-
ing is learned about the behavior of domain-specific
features. Most prior work considers datasets with
balanced labels. However, in real world applica-
tions, where labels may be biased toward some val-
ues, gains from MDL could be attributed to simply
modeling domain-specific bias. A practical advan-
tage of such a result is ease of implementation and
the ability to scale to many domains.
Overall, irrespective of the answers to these ques-
tions, a better understanding of the performance of
existing MDL algorithms in different settings will
provide intuitions for improving the state of the art.
</bodyText>
<sectionHeader confidence="0.996267" genericHeader="method">
2 Multi-Domain Learning
</sectionHeader>
<bodyText confidence="0.999900303030303">
In the multi-domain learning (MDL) setting, exam-
ples are accompanied by both a class label and a do-
main indicator. Examples are of the form (xi, y,di),
where xi E R&apos;, di is a domain indicator, xi is
drawn according to a fixed domain-specific distri-
bution Ddz, and yi is the label (e.g. yi E {−1, +1}
for binary labels). Standard learning ignores di, but
MDL uses these to improve learning accuracy.
Why should we care about the domain label? Do-
main differences can introduce errors in a number
of ways (Ben-David et al., 2007; Ben-David et al.,
2009). First, the domain-specific distributions Ddz
can differ such that they favor different features, i.e.
p(x) changes between domains. As a result, some
features may only appear in one domain. This aspect
of domain difference is typically the focus of un-
supervised domain adaptation (Blitzer et al., 2006;
Blitzer et al., 2007). Second, the features may be-
have differently with respect to the label in each do-
main, i.e. p(y|x) changes between domains. As a
result, a learning algorithm cannot generalize the be-
havior of features from one domain to another. The
key idea behind many MDL algorithms is to target
one or both of these properties of domain difference
to improve performance.
Prior approaches to MDL can be broadly catego-
rized into two classes. The first set of approaches
(Daum´e III, 2007; Dredze et al., 2008) introduce pa-
rameters to capture domain-specific behaviors while
preserving features that learn domain-general be-
haviors. A key of these methods is that they do not
explicitly model any relationship between the do-
mains. Daum´e III (2007) proposes a very simple
“easy adapt” approach, which was originally pro-
posed in the context of adapting to a specific target
domain, but easily generalizes to MDL. Dredze et al.
(2008) consider the problem of learning how to com-
bine different domain-specific classifiers such that
behaviors common to several domains can be cap-
tured by a shared classifier, while domain-specific
behavior is still captured by the individual classi-
fiers. We describe both of these approaches in § 3.2.
The second set of approaches to MDL introduce
an explicit notion of relationship between domains.
For example, Cavallanti et al. (2008) assume a fixed
task relationship matrix in the context of online
multi-task learning. The key assumption is that in-
stances from two different domains are half as much
related to each other as two instances from the same
domain. Saha et al. (2011) improve upon the idea
of simply using a fixed task relationship matrix by
instead learning it adaptively. They derive an online
algorithm for updating the task interaction matrix.
Zhang and Yeung (2010) derive a convex formu-
lation for adaptively learning domain relationships.
We describe their approach in § 3.2. Finally, Daum´e
III (2009) proposes a joint task clustering and multi-
task/multi-domain learning setup, where instead of
just learning pairwise domain relationships, a hier-
archical structure among them is inferred. Hierar-
chical clustering of tasks is performed in a Bayesian
framework, by imposing a hierarchical prior on the
structure of the task relationships.
In all of these settings, the key idea is to learn
both domain-specific behaviors and behaviors that
generalize between (possibly related) domains.
</bodyText>
<sectionHeader confidence="0.997579" genericHeader="method">
3 Data
</sectionHeader>
<bodyText confidence="0.998389666666667">
To support our analysis we develop several empir-
ical experiments. We first summarize the datasets
and methods that we use in our experiments, then
</bodyText>
<page confidence="0.974092">
1303
</page>
<bodyText confidence="0.982036">
proceed to our exploration of MDL.
</bodyText>
<subsectionHeader confidence="0.982612">
3.1 Datasets
</subsectionHeader>
<bodyText confidence="0.998803370370371">
A variety of multi-domain datasets have been used
for demonstrating MDL improvements. In this pa-
per, we focus on two datasets representative of many
of the properties of MDL.
Amazon (AMAZON) Our first dataset is the Multi-
Domain Amazon data (version 2.0), first introduced
by Blitzer et al. (2007). The task is binary sentiment
classification, in which Amazon product reviews are
labeled as positive or negative. Domains are defined
by product categories. We select the four domains
used in most studies: books, dvd, electronics
and kitchen appliances.
The original dataset contained 2,000 reviews for
each of the four domains, with 1,000 positive and
1,000 negative reviews per domain. Feature extrac-
tion follows Blitzer et al. (2007): we use case insen-
sitive unigrams and bigrams, although we remove
rare features (those that appear less than five times
in the training set). The reduced feature set was se-
lected given the sensitivity to feature size of some of
the MDL methods.
ConVote (CONVOTE) Our second dataset is taken
from segments of speech from United States
Congress floor debates, first introduced by Thomas
et al. (2006). The binary classification task on this
dataset is that of predicting whether a given speech
segment supports or opposes a bill under discus-
sion in the floor debate. We select this dataset be-
cause, unlike the AMAZON data, CONVOTE can be
divided into domains in several ways based on dif-
ferent metadata attributes available with the dataset.
We consider two types of domain divisions: the bill
identifier and the political party of the speaker. Di-
vision based on the bill creates domain differences
in that each bill has its own topic. Division based on
political party implies preference for different issues
and concerns, which manifest as different language.
We refer to these datasets as BILL and PARTY.
We use Version 1.1 of the CONVOTE dataset,
available at http://www.cs.cornell.edu/
home/llee/data/convote.html. More
specifically, we combine the training, development
and test folds from the data stage three/ ver-
sion, and sub-sample to generate different versions
of the dataset required for our experiments. For
BILL we randomly sample speech segments from
three different bills. The three bills and the number
of instances for each were chosen such that we have
sufficient data in each fold for every experiment.
For PARTY we randomly sample speech segments
from the two major political parties (Democrats and
Republicans). Feature processing was identical to
AMAZON, except that the threshold for feature re-
moval was two.
</bodyText>
<subsectionHeader confidence="0.999587">
3.2 Learning Methods and Features
</subsectionHeader>
<bodyText confidence="0.999796264705882">
We consider three MDL algorithms, two are repre-
sentative of the first approach and one of the second
approach (learning domain similarities) (§2). We fa-
vored algorithms with available code or that were
straightforward to implement, so as to ensure repro-
ducibility of our results.
FEDA Frustratingly easy domain adaptation
(FEDA) (Daum´e III, 2007; Daum´e III et al., 2010b;
Daum´e III et al., 2010a) is an example of a classifier
combination approach to MDL. The feature space
is a cross-product of the domain and input features,
augmented with the original input features (shared
features). Prediction is effectively a linear combina-
tion of a set of domain-specific weights and shared
weights. We combine FEDA with both the SVM
and logistic regression algorithms described below
to obtain FEDA-SVM and FEDA-LR.
MDR Multi-domain regularization (MDR) (Dredze
and Crammer, 2008; Dredze et al., 2009) extends the
idea behind classifier combination by explicitly for-
mulating a classifier combination scheme based on
Confidence-Weighted learning (Dredze et al., 2008).
Additionally, classifier updates (which happen in
an online framework) contain an explicit constraint
that the combined classifier should perform well on
the example. Dredze et al. (2009) consider several
variants of MDR. We select the two best perform-
ing methods: MDR-L2, which uses the underlying
algorithm of Crammer et al. (2008), and MDR-KL,
which uses the underlying algorithm of Dredze et al.
(2008). We follow their approach to classifier train-
ing and parameter optimization.
MTRL The multi-task relationship learning
(MTRL) approach proposed by Zhang and Yeung
</bodyText>
<page confidence="0.981169">
1304
</page>
<bodyText confidence="0.999960088888889">
(2010) achieves states of the art performance on
many MDL tasks. This method is representative
of methods that learn similarities between domains
and in turn regularize domain-specific parameters
accordingly. The key idea in their work is the use
of a matrix-normal distribution p(X|M, fl, E) as
a prior on the matrix W created by column-wise
stacking of the domain-specific classifier weight
vectors. fl represents the covariance matrix for the
variables along the columns of X. When used as
a prior over W it models the covariance between
the domain-specific classifiers (and therefore the
tasks). fl is learned jointly with the domain-specific
classifiers. This method has similar benefits to
FEDA in terms of classifier combination, but also
attempts to model domain relationships. We use
the implementation of MTRL made available by the
authors2. For parameter tuning, we perform a grid
search over the parameters A1 and A2, using the fol-
lowing values for each (a total of 36 combinations):
10.00001, 0.0001, 0.001, 0.01, 0.1, 11.
In addition to these multi-task learning methods,
we consider a common baseline: ignoring the do-
main distinctions and learning a single classifier
over all the data. This reflects single-domain learn-
ing, in which no domain knowledge is used and will
indicate baseline performance for all experiments.
While some earlier research has included a sepa-
rate one classifier per domain baseline, it almost al-
ways performs worse, since splitting the domains
provides much less data to each classifier (Dredze
et al., 2009). So we omit this baseline for simplicity.
To obtain a single classifier we use two classifica-
tion algorithms: SVMs and logistic regression.
Support Vector Machines A single SVM run
over all the training data, ignoring domain labels.
We use the SVM implementation available in the LI-
BLINEAR package (Fan et al., 2008). In particular,
we use the L2-regularized L2-loss SVM (option -s
1 in version 1.8 of LIBLINEAR, and also option -B
1 for including a standard bias feature). We tune
the SVM using five-fold stratified cross-validation
on the training set, using the following values for
the trade-off parameter C: 10.0001, 0.001, 0.01, 0.1,
0.2, 0.3, 0.5, 11.
</bodyText>
<footnote confidence="0.7667835">
2http://www.cse.ust.hk/˜zhangyu/codes/
MTRL.zip
</footnote>
<bodyText confidence="0.999272181818182">
Logistic Regression (LR) A single logistic re-
gression model run over all the training data, ignor-
ing domain labels. Again, we use the L2-regularized
LR implementation available in the LIBLINEAR
package (option -s 0, and also option -B 1). We
tune the LR model using the same strategy as the
one used for SVM above, including the values of the
trade-off parameter C.
For all experiments, we measure average accu-
racy over K-fold cross-validation, using 10 folds for
AMAZON, and 5 folds for both BILL and PARTY.
</bodyText>
<sectionHeader confidence="0.991445" genericHeader="method">
4 When Do Domains Matter?
</sectionHeader>
<bodyText confidence="0.999793">
We now empirically explore two questions regarding
the behavior of MDL.
</bodyText>
<subsectionHeader confidence="0.989966">
4.1 Ensemble Learning
</subsectionHeader>
<bodyText confidence="0.995441466666667">
Question: Are MDL improvements the result of
ensemble learning effects?
Many of the MDL approaches bear a striking
resemblance to ensemble learning. Traditionally,
ensemble learning combines the output from sev-
eral different classifiers to obtain a single improved
model (Maclin and Opitz, 1999). It is well estab-
lished that ensemble learning, applied on top of a
diverse array of quality classifiers, can improve re-
sults for a variety of tasks. The key idea behind
ensemble learning, that of combining a diverse ar-
ray of models, has been applied to settings in which
data preprocessing is used to create many different
classifiers. Examples include instance bagging and
feature bagging (Dietterich, 2000).
The core idea of using diverse inputs in making
classification decisions is common in the MDL liter-
ature. In fact, the top performing and only success-
ful entry to the 2007 CoNLL shared task on domain
adaptation for dependency parsing was a straightfor-
ward implementation of ensemble learning by cre-
ating variants of parsers (Sagae and Tsujii, 2007).
Many MDL algorithms, among them Dredze and
Crammer (2008), Daum´e III (2009), Zhang and Ye-
ung (2010) and Saha et al. (2011), all include some
notion of learning domain-specific classifiers on the
training data, and combining them in the best way
possible. To be clear, we do not claim that these
approaches can be reduced to an existing ensem-
ble learning algorithm. There are crucial elements
</bodyText>
<page confidence="0.9605">
1305
</page>
<bodyText confidence="0.999367582417583">
in each of these algorithms that separate them from
existing ensemble learning algorithms. One exam-
ple of such a distinction is the learning of domain
relationships by both Zhang and Yeung (2010) and
Saha et al. (2011). However, we argue that their
core approach, that of combining parameters that are
trained on variants of the data (all data or individual
domains), is an ensemble learning idea.
Consider instance bagging, in which multiple
classifiers are each trained on random subsets of the
data. The resulting classifiers are then combined
to form a final model. In MDL, we can consider
each domain a subset of the data, albeit non-random
and non-overlapping. The final model combines the
domain-specific parameters and parameters trained
on other instances, which in the case of FEDA are the
shared parameters. In this light, these methods are a
complex form of instance bagging, and their devel-
opment could be justified from this perspective.
However, given this justification, are improve-
ments from MDL simply the result of standard en-
semble learning effects, or are these methods re-
ally learning something about domain behavior? If
knowledge of domain was withheld from the algo-
rithm, could we expect similar improvements? As
we will do in each empirical experiment, we propose
a contrarian hypothesis:
Hypothesis: Knowledge of domains is irrelevant
for MDL.
Empirical Evaluation We evaluate this hypothe-
sis as follows. We begin by constructing a true MDL
setting, in which we attempt to improve accuracy
through knowledge of the domains. We will apply
three MDL algorithms (FEDA, MDR, and MTRL) to
our three multi-domain datasets (AMAZON, BILL,
and PARTY) and compare them against a single clas-
sifier baseline. We will then withhold knowledge
of the true domains from these algorithms and in-
stead provide them with random “pseudo-domains,”
and then evaluate the change in their behavior. The
question is whether we can obtain similar benefits
by ignoring domain labels and relying strictly on an
ensemble learning motivation (instance bagging).
For the “True Domain” setting, we apply the
MDL algorithms as normal. For the “Random Do-
main” setting, we randomly shuffle the domain la-
bels within a given class label within each fold, thus
maintaining the same number of examples for each
domain label, and also retaining the same class dis-
tribution within each randomized domain. The re-
sulting “pseudo-domains” are then similar to ran-
dom subsets of the data used in ensemble learning.
Following the standard practice in previous work,
for this experiment we use a balanced number of
examples from each domain and a balanced num-
ber of positive and negative labels (no class bias).
For AMAZON (4 domains), we have 10 folds of 400
examples per fold, for BILL (3 domains) 5 folds of
60 examples per fold, and for PARTY (2 domains) 5
folds of 80 examples per fold. In the “Random Do-
main” setting, since we are randomizing the domain
labels, we increase the number of trials. We repeat
each cross-validation experiment 5 times with differ-
ent randomization of the domain labels each time.
Results Results are shown in Table 1. The first
row shows absolute (average) accuracy for a single
classifier trained on all data, ignoring domain dis-
tinctions. The remaining cells indicate absolute im-
provements against the baseline.
First, we note for the well-studied AMAZON
dataset that our results with true domains are con-
sistent with the previous literature. FEDA is known
to not improve upon a single classifier baseline for
that dataset (Dredze et al., 2009). Both MDR-L2 and
MDR-KL improve upon the single classifier baseline,
again as per Dredze et al. (2009). And finally, MTRL
also improves upon the single classifier baseline. Al-
though the MTRL improvement is not as dramatic as
in the original paper3, the average accuracy that we
achieve for MTRL (84.2%) is better than the best av-
erage accuracy in the original paper (83.65%).
The main comparison to make in Table 1 is be-
tween having knowledge of true domains or not.
“Random Domain” in the table is the case where do-
main identifiers are randomly shuffled within a given
fold. Ignoring the significance test results for now,
overall the results indicate that knowing the true do-
mains is useful for MDL algorithms. Randomiz-
ing the domains does not work better than knowing
true domains in any case. However, in all except
one case, the improvements of MDL algorithms are
</bodyText>
<footnote confidence="0.999549333333333">
3This might be due to a different version of the dataset being
used in a cross-validation setup, rather than their train/test setup,
and also because of differences in baseline approaches.
</footnote>
<page confidence="0.84583">
1306
</page>
<table confidence="0.998743125">
SVM AMAZON BILL LR PARTY LR
LR SVM SVM
Single Classifier
83.93% 83.78% 66.67% 68.00% 62.75% 64.00%
FEDA
True Domain -0.35 -0.10 +2.33 + 1.00 +4.25 N +1.25
Random Domain -1.30 H -1.02 H -1.20 -2.07 -2.05 -2.10
MDR-L2
True Domain +1.87 N +2.02 N +0.00 -1.33 +2.25 +1.00
Random Domain +0.91 N +1.07 N -2.67 -4.00 -2.80 -4.05
MDR-KL
True Domain +1.85 N +2.00 N +1.00 -0.33 +3.00 +1.75
Random Domain +1.36 N +1.51 N +0.60 -0.73 -1.30 -2.55 H
MTRL
True Domain +0.27 +0.42 +0.67 -0.67 +1.50 +0.25
Random Domain -0.37 -0.21 -1.47 -2.80 -3.55 -4.80
</table>
<tableCaption confidence="0.999124">
Table 1: A comparison between MDL methods with access to the “True Domain” labels and methods that
</tableCaption>
<bodyText confidence="0.983023185185185">
use “Random Domain” information, essentially ensemble learning. The first row has raw accuracy numbers,
whereas the remaining entries are absolute improvements over the baseline. N: Significantly better than the
corresponding SVM or LR baseline, with p &lt; 0.05, using a paired t-test. H: Significantly worse than
corresponding baseline, with p &lt; 0.05, using a paired t-test.
significantly better only for the AMAZON dataset4.
And interestingly, exactly in the same case, ran-
domly shuffling the domains also gives significant
improvements compared to the baseline, showing
that there is an ensemble learning effect in operation
for MDR-L2 and MDR-KL on the AMAZON dataset.
For FEDA, randomizing the domains significantly
hurts its performance on the AMAZON data, as is
the case for MDR-KL on the PARTY data. Therefore,
while our contrarian hypothesis about irrelevance of
domains is not completely true, it is indeed the case
that some MDL methods benefit from the ensemble
learning effect.
A second observation to be made from these re-
sults is that, while all of empirical research on MDL
assumes the definition of domains as a given, the
question of how to split a dataset into domains given
various metadata attributes is still open. For exam-
ple, in our experiments, in general, using the po-
litical party as a domain distinction gives us more
improvements over the corresponding baseline ap-
proach5.
We provide a detailed comparison of using true
</bodyText>
<footnote confidence="0.9680358">
4Some numbers in Table 1 might appear to be significant,
but are not. That is because of high variance in the performance
of the methods across the different folds.
5The BILL and the PARTY datasets are not directly compa-
rable to each other, although the prediction task is the same.
</footnote>
<bodyText confidence="0.9920225">
vs. randomized domains in Table 6, after presenting
the second set of experimental results.
</bodyText>
<subsectionHeader confidence="0.978402">
4.2 Domain-specific Class Bias
</subsectionHeader>
<bodyText confidence="0.989950434782609">
Question: Are MDL methods improving because
they capture domain-specific class biases?
In previous work, and the above section, experi-
ments have assumed a balanced dataset in terms of
class labels. It has been in these settings that MDL
methods improve. However, this is an unrealistic as-
sumption. Even in our datasets, the original versions
demonstrated class bias: Amazon product reviews
are generally positive, votes on bills are rarely tied,
and political parties vote in blocs. While it is com-
mon to evaluate learning methods on balanced data,
and then adjust for imbalanced real world datasets, it
is unclear what effect domain-specific class bias will
have on MDL methods. Domains can differ in their
proportion of examples of different classes. For ex-
ample, it is quite likely that less controversial bills in
the United States Congress will have more yes votes
than controversial bills. Similarly, if instead of the
category of a product, its brand is considered as a do-
main, it is likely that some brands receive a higher
proportion of positive reviews than others.
Improvements from MDL in such settings may
simply be capturing domain-specific class biases.
</bodyText>
<page confidence="0.990836">
1307
</page>
<table confidence="0.973484045454545">
domain class cb1 cb2 cb3 cb4
AMAZON
b - 20 80 60 40
+ 80 20 40 60
d - 40 20 80 60
+ 60 80 20 40
e - 60 40 20 80
+ 40 60 80 20
k - 80 60 40 20
+ 20 40 60 80
BILL
031 N 16 4 8 12
Y 4 16 12 8
088 N 12 16 4 8
Y 8 4 16 12
132 N 8 12 16 4
Y 12 8 4 16
PARTY
D N 10 30 15 25
Y 30 10 25 15
R N 30 10 25 15
Y 10 30 15 25
</table>
<tableCaption confidence="0.993987">
Table 2: The table shows the distribution of in-
</tableCaption>
<bodyText confidence="0.990693069444445">
stances across domains and class labels within one
fold of each of the datasets, for four different class
bias trials. These datasets with varying class bias
across domains were used for the experiments de-
scribed in §4.2
Consider two domains, where each domain is biased
towards the opposite label. In this case, domain-
specific parameters may simply be capturing the bias
towards the class label, increasing the weight uni-
formly of features predictive of the dominant class.
Similarly, methods that learn domain similarity may
be learning class bias similarity.
Why does the effectiveness of these domain-
specific bias parameters matter? First, if capturing
domain-specific class bias is the source of improve-
ment, there are much simpler methods for learning
that can be just as effective. This would be espe-
cially important in settings where we have many do-
mains, and learning domain-specific parameters for
each feature becomes infeasible. Second, if class
bias accounted for most of the improvement in learn-
ing, it suggests that such settings could be amenable
to unsupervised adaptation of the bias parameters.
Hypothesis: MDL largely capitalizes on
domain-specific class bias.
Empirical Evaluation To evaluate our hypothe-
sis, for each of our three datasets we create 4 random
versions, each with some domain-specific class-bias.
A summary of the dataset partitions is shown in
Table 2. For example, for the AMAZON dataset,
we create 4 versions (cb1 ... cb4), where each do-
main has 100 examples per fold and each domain
has a different balance between positive and nega-
tive classes. For each of these settings, we conduct
a 10-fold cross validation experiment, then average
the CV results for each of the 4 settings. The re-
sulting accuracy numbers therefore reflect an aver-
age across many types of bias, each evaluated many
times. We do a similar experiment for the BILL and
PARTY datasets, except we use 5-fold CV.
In addition to the multi-domain and baseline
methods, we add a new baseline: DOM-ID. In this
setting, we augment the baseline classifier (which
ignores domain labels) with a new feature that in-
dicates the domain label. While we already include
a general bias feature, as is common in classifica-
tion tasks, these new features will capture domain-
specific bias. This is the only change to the base-
line classifier, so improvements over the baseline are
indicative of the change in domain-bias that can be
captured using these simple features.
Results Results are shown in Table 3. The table
follows the same structure as Table 1, with the ad-
dition of the results for the DOM-ID approach. We
first examine the efficacy of MDL in this setting. An
observation that is hard to miss is that MDL results
in these experiments show significant improvements
in almost all cases, as compared to only a few cases
in Table 1, despite the fact that even the baseline ap-
proaches have a higher accuracy. This shows that
MDL results can be highly influenced by systematic
differences in class bias across domains. Note that
there is also a significant negative influence of class
bias on MTRL for the AMAZON data.
A comparison of the MDL results on true domains
to the DOM-ID baseline gives us an idea of how
much MDL benefits purely from class bias differ-
ences across domains. We see that in most cases,
about half of the improvement seen in MDL is ac-
counted for by a simple baseline of using the do-
main identifier as a feature, and all but one of the
improvements from DOM-ID are significant. This
</bodyText>
<page confidence="0.968738">
1308
</page>
<table confidence="0.999626631578947">
AMAZON LR BILL LR PARTY LR
SVM SVM SVM
Single Classifier
85.52% 85.46% 70.50% 70.67% 65.44% 65.81%
FEDA
True Domain +0.11 +0.31 +4.25 N +4.00 N +4.81 N +4.69 N
Random Domain +0.94 N +1.03 N +3.68 N +4.03 N +4.24 +3.73
MDR-L2
True Domain +0.92 N +0.98 N +4.42 N +4.25 N +1.31 +0.94
Random Domain +1.86 N +1.92 N +3.93 N +3.77 N +0.65 +0.28
MDR-KL
True Domain +1.54 N +1.59 N +5.17 N +5.00 N +4.25 N +3.88 N
Random Domain +2.84 N +2.90 N +4.13 N +3.97 N +3.81 N +3.44
MTRL
True Domain -1.22 H -1.17 H +4.50 N +4.33 N +6.44 N +6.06 N
Random Domain -0.69 H -0.63 H +3.53 N +3.37 N +4.87 N +4.50 N
DOM-ID
True Domain +0.36 +0.38 N +2.83 N +2.75 N +3.75 N +4.00 N
Random Domain +1.73 N +1.76 N +4.50 N +4.98 N +5.24 N +5.31 N
</table>
<tableCaption confidence="0.998784">
Table 3: A comparison between MDL methods with class biased data. Similar to the setup where we
</tableCaption>
<bodyText confidence="0.991042689655173">
evaluate the ensemble learning effect, we have a setting of using randomized domains. N: Significantly
better than the corresponding SVM or LR baseline, with p &lt; 0.05, using a paired t-test. H: Significantly
worse than corresponding baseline, with p &lt; 0.05, using a paired t-test.
suggests that in a real-world scenario where differ-
ence in class bias across domains is quite likely, it is
useful to consider DOM-ID as a simple baseline that
gives good empirical performance. To our knowl-
edge, using this approach as a baseline is not stan-
dard practice in MDL literature.
Finally, we also include the “Random Domain”
evaluation in the our class biased version of exper-
iments. Each “Random Domain” result in Table 3
is an average over 20 cross-validation runs (5 ran-
domized trials for each of the four class biased tri-
als cb1 ... cb4). This setup combines the effects
of ensemble learning and bias difference across do-
mains. As seen in the table, for MDL algorithms the
results are consistently better as compared to know-
ing the true domains for the AMAZON dataset. For
the other datasets, the performance after randomiz-
ing the domains is still significantly better than the
baseline. This evaluation on randomized domains
further strengthens the conclusion that differences in
bias across domains play an important role, even in
the case of noisy domains. Looking at the perfor-
mance of DOM-ID with randomized domains, we
see that in all cases the DOM-ID baseline performs
better with randomized domains. While the dif-
ference is significant mostly only on the AMAZON
</bodyText>
<table confidence="0.9268178">
domain class cb5 cb6 cb7 cb8
AMAZON
b - 20 40 60 80
+ 80 60 40 20
d - 20 40 60 80
+ 80 60 40 20
e - 20 40 60 80
+ 80 60 40 20
k - 20 40 60 80
+ 20 40 60 80
</table>
<tableCaption confidence="0.989266">
Table 4: The table shows the distribution of in-
</tableCaption>
<bodyText confidence="0.995740583333333">
stances across domains and class labels within one
fold of the AMAZON dataset, for four different class
bias trials. For the BILL and PARTY datasets, similar
folds with consistent bias were created (number of
examples used was different). These datasets with
consistent class bias across domains were used for
the experiments described in §4.2.1
dataset (details in Table 6, columns under “Varying
Class Bias,”) this trend is still counter-intuitive. We
suspect this might be because randomization creates
a noisy version of the domain labels, which helps
learners to avoid over-fitting that single feature.
</bodyText>
<page confidence="0.990441">
1309
</page>
<subsectionHeader confidence="0.932829">
4.2.1 Consistent Class Bias
</subsectionHeader>
<bodyText confidence="0.99999775">
We also performed a set of experiments that ap-
ply MDL algorithms to a setting where the datasets
have different class biases (unlike the experiments
reported in Table 1, where the classes are balanced),
but, unlike the experiments reported in Table 3, the
class bias is the same within each of the domains.
We refer to this as the case of consistent class bias
across domains. The distribution of classes within
each domain within each fold is shown in Table 4.
The results for this set of experiments are reported
in Table 5. The structure of Table 5 is identical to
that of Table 1. Comparing these results to those
in Table 1, we can see that in most cases the im-
provements seen using MDL algorithms are lower
than those seen in Table 1. This is likely due to
the higher baseline performance in the consistent
class bias case. A notable difference is in the per-
formance of MTRL — it is significantly worse for
the AMAZON dataset, and significantly better for the
PARTY dataset. For the AMAZON dataset, we be-
lieve that the domain distinctions are less meaning-
ful, and hence forcing MTRL to learn the relation-
ships results in lower performance. For the PARTY
dataset, in the case of a class-biased setup, know-
ing the party is highly predictive of the vote (in the
original CONVOTE dataset, Democrats mostly vote
“no” and Republicans mostly vote “yes”), and this
is rightly exploited by MTRL.
</bodyText>
<subsectionHeader confidence="0.505252">
4.2.2 True vs. Randomized Domains
</subsectionHeader>
<bodyText confidence="0.999976828571428">
In Table 6 we analyze the difference in perfor-
mance of MDL methods when using true vs. ran-
domized domain information. For the three sets of
results reported earlier, we evaluated whether using
true domains as compared to randomized domains
gives significantly better, significantly worse or
equal performance. Significance testing was done
using a paired t-test with α = 0.05 as before. As the
table shows, for the first set of results where the class
labels were balanced (overall, as well as within each
domain), using true domains was significantly better
mostly only for the AMAZON dataset. FEDA-SVM
was the only approach that was consistently better
with true domains across all datasets. Note, how-
ever, that it was significantly better than the baseline
approach only for PARTY.
For the second set of results (Table 3) where the
class bias varied across the different domains, us-
ing true domains was either no different from using
randomized domains, or it was significantly worse.
In particular, it was consistently significantly worse
to use true domains on the AMAZON dataset. This
questions the utility of domains on the AMAZON
dataset in the context of MDL in a domain-specific
class bias scenario. Since randomizing the domains
works better for all of the MDL methods on AMA-
ZON, it suggests that an ensemble learning effect
is primarily responsible for the significant improve-
ments seen on the AMAZON data, when evaluated in
a domain-specific class bias setting.
Finally, for the case of consistent class bias across
domains, the trend is similar to the case of no class
bias — using true domains is useful. This table
further supports the conclusion that domain-specific
class bias highly influences multi-domain learning.
</bodyText>
<sectionHeader confidence="0.99726" genericHeader="conclusions">
5 Discussion and Open Questions
</sectionHeader>
<bodyText confidence="0.987358346153846">
Our analysis of MDL algorithms revealed new
trends that suggest further avenues of exploration.
We suggest three open questions in response.
Question: When are MDL methods most effective?
Our empirical results suggest that MDL can be more
effective in settings with domain-specific class bi-
ases. However, we also saw differences in im-
provements for each method, and for different do-
mains. Differences emerge between the AMAZON
and CONVOTE datasets in terms of the ensemble
learning hypothesis. While there has been some the-
oretical analyses on the topic of MDL (Ben-David
et al., 2007; Ben-David et al., 2009; Mansour et
al., 2009; Daum´e III et al., 2010a), our results sug-
gest performing new analyses that relate ensemble
learning results with the MDL setting. These anal-
yses could provide insights into new algorithms that
can take advantage of the specific properties of each
multi-domain setting.
Question: What makes a good domain for MDL?
To the best of our knowledge, previous work has
assumed that domain identities are provided to the
learning algorithm. However, in reality, there may
be many ways to split a dataset into domains. For
example, consider the CONVOTE dataset, which we
split both by BILL and PARTY. The choice of splits
</bodyText>
<page confidence="0.950543">
1310
</page>
<table confidence="0.9979041875">
AMAZON LR BILL LR PARTY LR
SVM SVM SVM
Single Classifier
86.06% 86.22% 76.42% 75.58% 69.31% 68.38%
FEDA
True Domain -0.25 -0.33 -0.83 +0.25 +0.88 +1.25
Random Domain -1.17 H -1.26 H -1.33 -0.82 -0.55 -0.04
MDR-L2
True Domain +0.39 N +0.23 -0.42 +0.42 -2.12 -1.19
Random Domain -0.38 -0.53 H -3.57 -2.73 -4.30 H -3.36 H
MDR-KL
True Domain +0.81 N +0.65 N -0.83 +0.00 +1.31 +2.25 N
Random Domain +0.22 +0.06 -1.90 -1.07 -0.60 +0.34
MTRL
True Domain -1.52 H -1.68 H -1.92 -1.08 +3.12 N +4.06 N
Random Domain -2.12 H -2.28 H -0.95 -0.12 +0.19 +1.12 N
</table>
<tableCaption confidence="0.9870005">
Table 5: A comparison between MDL methods with data that have a consistent class bias across domains.
Similar to the setup where we evaluate the ensemble learning effect, we have a setting of using randomized
domains. N: Significantly better than the corresponding SVM or LR baseline, with p &lt; 0.05, using a paired
t-test. H: Significantly worse than corresponding baseline, with p &lt; 0.05, using a paired t-test.
</tableCaption>
<table confidence="0.999407444444444">
MDL Method No Class Bias (Tab. 1) Varying Class Bias (Tab. 3) Consistent Class Bias (Tab. 5)
better worse equal better worse equal better worse equal
FEDA-SVM AM, BI, PA – BI, PA AM BI, PA AM, PA PA BI
FEDA-LR AM – BI, PA AM BI, PA AM, BI BI PA
MDR-L2 AM AM, BI AM BI, PA AM, BI – BI
MDR-KL PA BI, PA AM BI, PA AM, PA – –
MTRL AM – AM BI, PA AM, PA –
DOM-ID-SVM – – AM BI, PA –
DOM-ID-LR – AM, BI PA –
</table>
<tableCaption confidence="0.970916">
Table 6: The table shows the datasets (AM:AMAZON, BI:BILL, PA:PARTY) for which a given MDL method
</tableCaption>
<bodyText confidence="0.989237">
using true domain information was significantly better, significantly worse, or not significantly different
(equal) as compared to using randomized domain information with the same MDL method.
impacted MDL. This poses new questions: what
makes a good domain? How should we choose to di-
vide data along possible metadata properties? If we
can gain improvements simply by randomly creat-
ing new domains (“Random Domain” setting in our
experiments) then there may be better ways to take
advantage of the provided metadata for MDL.
Question: Can we learn class-bias for
unsupervised domain adaptation?
Experiments with domain-specific class biases re-
vealed that a significant part of the improvements
could be achieved by adding domain-specific bias
features. Limiting the multi-domain improvements
to a small set of parameters raises an interesting
question: can these parameters be adapted to a new
domain without labeled data? Traditionally, domain
adaptation without target domain labeled data has
focused on learning the behavior of new features;
beliefs about existing feature behaviors could not be
corrected without new training data. However, by
collapsing the adaptation into a single bias parame-
ter, we may be able to learn how to adjust this pa-
rameter in a fully unsupervised way. This would
open the door to improvements in this challenging
setting for real world problems where class bias was
a significant factor.
</bodyText>
<sectionHeader confidence="0.99896" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.883137">
Research presented here is supported by the Office
of Naval Research grant number N000141110221.
</bodyText>
<page confidence="0.989403">
1311
</page>
<sectionHeader confidence="0.995849" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99993985576923">
Andrew Arnold, Ramesh Nallapati, and William W. Co-
hen. 2008. Exploiting Feature Hierarchy for Transfer
Learning in Named Entity Recognition. In Proceed-
ings of ACL-08: HLT, pages 245–253.
Shai Ben-David, John Blitzer, Koby Crammer, and Fer-
nando Pereira. 2007. Analysis of representations for
domain adaptation. In Proceedings of NIPS 2006.
Shai Ben-David, John Blitzer, Koby Crammer, Alex
Kulesza, Fernando Pereira, and Jennifer Wortman
Vaughan. 2009. A theory of learning from different
domains. Machine Learning.
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain Adaptation with Structural Correspon-
dence Learning. In Proceedings of the 2006 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 120–128.
John Blitzer, Mark Dredze, and Fernando Pereira. 2007.
Biographies, Bollywood, Boom-boxes and Blenders:
Domain Adaptation for Sentiment Classification. In
Proceedings of the 45th Annual Meeting of the Associ-
ation of Computational Linguistics, pages 440–447.
John Blitzer, Koby Crammer, Alex Kulesza, Fernando
Pereira, and Jennifer Wortman. 2008. Learning
Bounds for Domain Adaptation. In Advances in Neu-
ral Information Processing Systems (NIPS 2007).
Giovanni Cavallanti, Nicol`o Cesa-Bianchi, and Claudio
Gentile. 2008. Linear Algorithms for Online Multi-
task Classification. In Proceedings of COLT.
Ciprian Chelba and Alex Acero. 2004. Adaptation of
Maximum Entropy Capitalizer: Little Data Can Help
a Lot. In Dekang Lin and Dekai Wu, editors, Proceed-
ings of EMNLP 2004, pages 285–292.
Koby Crammer, Mark Dredze, and Fernando Pereira.
2008. Exact convex confidence-weighted learning. In
Advances in Neural Information Processing Systems
(NIPS).
Hal Daum´e III and Daniel Marcu. 2006. Domain adap-
tation for statistical classifiers. Journal of Artificial
Intelligence Research, 26(1):101–126.
Hal Daum´e III, Abhishek Kumar, and Avishek Saha.
2010a. A Co-regularization Based Semi-supervised
Domain Adaptation. In Neural Information Process-
ing Systems.
Hal Daum´e III, Abhishek Kumar, and Avishek Saha.
2010b. Frustratingly Easy Semi-Supervised Domain
Adaptation. In Proceedings of the ACL 2010 Work-
shop on Domain Adaptation for Natural Language
Processing, pages 53–59.
Hal Daum´e III. 2007. Frustratingly Easy Domain Adap-
tation. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
256–263.
Hal Daum´e III. 2009. Bayesian multitask learning with
latent hierarchies. In Proceedings of the Twenty-Fifth
Conference on Uncertainty in Artificial Intelligence.
Thomas G. Dietterich. 2000. An experimental compar-
ison of three methods for constructing ensembles of
decision trees: Bagging, boosting, and randomization.
Machine Learning, 40:139–157.
Mark Dredze and Koby Crammer. 2008. Online meth-
ods for multi-domain learning and adaptation. Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing - EMNLP ’08.
Mark Dredze, Koby Crammer, and Fernando Pereira.
2008. Confidence-weighted linear classification. Pro-
ceedings of the 25th international conference on Ma-
chine learning - ICML ’08.
Mark Dredze, Alex Kulesza, and Koby Crammer. 2009.
Multi-domain learning by confidence-weighted pa-
rameter combination. Machine Learning, 79(1-2).
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-rui
Wang, and Chih-Jen Lin. 2008. LIBLINEAR : A Li-
brary for Large Linear Classification. Journal of Ma-
chine Learning Research, 9:1871–1874.
Jenny R. Finkel and Christopher D. Manning. 2009. Hi-
erarchical Bayesian Domain Adaptation. In Proceed-
ings of Human Language Technologies: The 2009 An-
nual Conference of the North American Chapter of
the Association for Computational Linguistics, pages
602–610.
Richard Maclin and David Opitz. 1999. Popular Ensem-
ble Methods: An Empirical Study. Journal of Artifi-
cial Intelligence Research, 11:169–198.
Yishay Mansour, Mehryar Mohri, and Afshin Ros-
tamizadeh. 2009. Domain Adaptation with Multiple
Sources. In Proceedings of NIPS 2008, pages 1041–
1048.
Kenji Sagae and Jun’ichi Tsujii. 2007. Dependency
parsing and domain adaptation with lr models and
parser ensembles. In Conference on Natural Language
Learning (Shared Task).
Avishek Saha, Piyush Rai, Hal Daum´e III, and Suresh
Venkatasubramanian. 2011. Online learning of mul-
tiple tasks and their relationships. In Proceedings of
AISTATS 2011.
Matt Thomas, Bo Pang, and Lillian Lee. 2006. Get
out the vote: Determining support or opposition from
Congressional floor-debate transcripts. In Proceed-
ings of EMNLP, pages 327–335.
Yu Zhang and Dit-Yan Yeung. 2010. A Convex Formu-
lation for Learning Task Relationships in Multi-Task
Learning. In Proceedings of the Proceedings of the
Twenty-Sixth Conference Annual Conference on Un-
certainty in Artificial Intelligence (UAI-10).
</reference>
<page confidence="0.99461">
1312
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.617050">
<title confidence="0.999282">Multi-Domain Learning: When Do Domains Matter?</title>
<author confidence="0.999841">Mahesh Joshi</author>
<affiliation confidence="0.9999635">School of Computer Science Carnegie Mellon University</affiliation>
<address confidence="0.998283">Pittsburgh, PA, 15213, USA</address>
<email confidence="0.999682">maheshj@cs.cmu.edu</email>
<author confidence="0.999934">William W Cohen</author>
<affiliation confidence="0.999939">School of Computer Science Carnegie Mellon University</affiliation>
<address confidence="0.999512">Pittsburgh, PA, 15213, USA</address>
<email confidence="0.999813">wcohen@cs.cmu.edu</email>
<author confidence="0.986531">Mark</author>
<affiliation confidence="0.842846">Human Language Technology Center of Johns Hopkins</affiliation>
<address confidence="0.91499">Baltimore, Maryland</address>
<email confidence="0.999574">mdredze@cs.jhu.edu</email>
<author confidence="0.999916">Carolyn P Ros´e</author>
<affiliation confidence="0.999911">School of Computer Science Carnegie Mellon University</affiliation>
<address confidence="0.999528">Pittsburgh, PA, 15213, USA</address>
<email confidence="0.999728">cprose@cs.cmu.edu</email>
<abstract confidence="0.99832275">We present a systematic analysis of existing multi-domain learning approaches with respect to two questions. First, many multidomain learning algorithms resemble ensemble learning algorithms. (1) Are multi-domain learning improvements the result of ensemble learning effects? Second, these algorithms are traditionally evaluated in a balanced class label setting, although in practice many multidomain settings have domain-specific class label biases. When multi-domain learning is applied to these settings, (2) are multidomain methods improving because they capture domain-specific class biases? An understanding of these two issues presents a clearer idea about where the field has had success in multi-domain learning, and it suggests some important open questions for improving beyond the current state of the art.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Andrew Arnold</author>
<author>Ramesh Nallapati</author>
<author>William W Cohen</author>
</authors>
<title>Exploiting Feature Hierarchy for Transfer Learning in Named Entity Recognition.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>245--253</pages>
<contexts>
<context position="2304" citStr="Arnold et al., 2008" startWordPosition="334" endWordPosition="337">ly distributed (i.i.d.). As a result, numerous domain adaptation methods (Chelba and Acero, 2004; Daum´e III and Marcu, 2006; Blitzer et al., 2007) target settings with a training set from one domain and a test set from another. Often times the training set itself violates the i.i.d. assumption and contains multiple domains. In this case, training a single model obscures domain distinctions, and separating the dataset by domains reduces training data. Instead, multi-domain learning (MDL) can take advantage of these domain labels to improve learning (Daum´e III, 2007; Dredze and Crammer, 2008; Arnold et al., 2008; Finkel and Manning, 2009; Zhang and Yeung, 2010; Saha et al., 2011). One such example is sentiment classification of product reviews. Training data is available from many product categories and while all data should be used to learn a model, there are important differences between the categories (Blitzer et al., 2007)1. While much prior research has shown improvements using MDL, this paper explores what properties of an MDL setting matter. Are previous improvements from MDL algorithms discovering important distinctions between features in different domains, as we would hope, or are other fac</context>
</contexts>
<marker>Arnold, Nallapati, Cohen, 2008</marker>
<rawString>Andrew Arnold, Ramesh Nallapati, and William W. Cohen. 2008. Exploiting Feature Hierarchy for Transfer Learning in Named Entity Recognition. In Proceedings of ACL-08: HLT, pages 245–253.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shai Ben-David</author>
<author>John Blitzer</author>
<author>Koby Crammer</author>
<author>Fernando Pereira</author>
</authors>
<title>Analysis of representations for domain adaptation.</title>
<date>2007</date>
<booktitle>In Proceedings of NIPS</booktitle>
<contexts>
<context position="5543" citStr="Ben-David et al., 2007" startWordPosition="864" endWordPosition="867">ent settings will provide intuitions for improving the state of the art. 2 Multi-Domain Learning In the multi-domain learning (MDL) setting, examples are accompanied by both a class label and a domain indicator. Examples are of the form (xi, y,di), where xi E R&apos;, di is a domain indicator, xi is drawn according to a fixed domain-specific distribution Ddz, and yi is the label (e.g. yi E {−1, +1} for binary labels). Standard learning ignores di, but MDL uses these to improve learning accuracy. Why should we care about the domain label? Domain differences can introduce errors in a number of ways (Ben-David et al., 2007; Ben-David et al., 2009). First, the domain-specific distributions Ddz can differ such that they favor different features, i.e. p(x) changes between domains. As a result, some features may only appear in one domain. This aspect of domain difference is typically the focus of unsupervised domain adaptation (Blitzer et al., 2006; Blitzer et al., 2007). Second, the features may behave differently with respect to the label in each domain, i.e. p(y|x) changes between domains. As a result, a learning algorithm cannot generalize the behavior of features from one domain to another. The key idea behind</context>
<context position="36260" citStr="Ben-David et al., 2007" startWordPosition="5988" endWordPosition="5991">ning. 5 Discussion and Open Questions Our analysis of MDL algorithms revealed new trends that suggest further avenues of exploration. We suggest three open questions in response. Question: When are MDL methods most effective? Our empirical results suggest that MDL can be more effective in settings with domain-specific class biases. However, we also saw differences in improvements for each method, and for different domains. Differences emerge between the AMAZON and CONVOTE datasets in terms of the ensemble learning hypothesis. While there has been some theoretical analyses on the topic of MDL (Ben-David et al., 2007; Ben-David et al., 2009; Mansour et al., 2009; Daum´e III et al., 2010a), our results suggest performing new analyses that relate ensemble learning results with the MDL setting. These analyses could provide insights into new algorithms that can take advantage of the specific properties of each multi-domain setting. Question: What makes a good domain for MDL? To the best of our knowledge, previous work has assumed that domain identities are provided to the learning algorithm. However, in reality, there may be many ways to split a dataset into domains. For example, consider the CONVOTE dataset,</context>
</contexts>
<marker>Ben-David, Blitzer, Crammer, Pereira, 2007</marker>
<rawString>Shai Ben-David, John Blitzer, Koby Crammer, and Fernando Pereira. 2007. Analysis of representations for domain adaptation. In Proceedings of NIPS 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shai Ben-David</author>
<author>John Blitzer</author>
<author>Koby Crammer</author>
<author>Alex Kulesza</author>
<author>Fernando Pereira</author>
<author>Jennifer Wortman Vaughan</author>
</authors>
<title>A theory of learning from different domains.</title>
<date>2009</date>
<journal>Machine Learning.</journal>
<contexts>
<context position="3284" citStr="Ben-David et al., 2009" startWordPosition="493" endWordPosition="496">wn improvements using MDL, this paper explores what properties of an MDL setting matter. Are previous improvements from MDL algorithms discovering important distinctions between features in different domains, as we would hope, or are other factors contributing to learning success? The key question of this paper is: when do domains matter? Towards this goal we explore two issues. First, we explore the question of whether domain distinctions are used by existing MDL algorithms in meaningful ways. While differences in feature behaviors between domains will hurt performance (Blitzer et al., 2008; Ben-David et al., 2009), it is not clear if the improvements in MDL algorithms can be attributed to correcting these errors, or whether they are benefiting from something else. In particular, there are many similarities between MDL and ensemble methods, with connections to instance bag1Blitzer et al. (2007) do not consider the MDL setup, they consider a single source domain, and a single target domain, with little or no labeled data available for the target domain. 1302 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1302</context>
<context position="5568" citStr="Ben-David et al., 2009" startWordPosition="868" endWordPosition="871">e intuitions for improving the state of the art. 2 Multi-Domain Learning In the multi-domain learning (MDL) setting, examples are accompanied by both a class label and a domain indicator. Examples are of the form (xi, y,di), where xi E R&apos;, di is a domain indicator, xi is drawn according to a fixed domain-specific distribution Ddz, and yi is the label (e.g. yi E {−1, +1} for binary labels). Standard learning ignores di, but MDL uses these to improve learning accuracy. Why should we care about the domain label? Domain differences can introduce errors in a number of ways (Ben-David et al., 2007; Ben-David et al., 2009). First, the domain-specific distributions Ddz can differ such that they favor different features, i.e. p(x) changes between domains. As a result, some features may only appear in one domain. This aspect of domain difference is typically the focus of unsupervised domain adaptation (Blitzer et al., 2006; Blitzer et al., 2007). Second, the features may behave differently with respect to the label in each domain, i.e. p(y|x) changes between domains. As a result, a learning algorithm cannot generalize the behavior of features from one domain to another. The key idea behind many MDL algorithms is t</context>
<context position="36284" citStr="Ben-David et al., 2009" startWordPosition="5992" endWordPosition="5995">pen Questions Our analysis of MDL algorithms revealed new trends that suggest further avenues of exploration. We suggest three open questions in response. Question: When are MDL methods most effective? Our empirical results suggest that MDL can be more effective in settings with domain-specific class biases. However, we also saw differences in improvements for each method, and for different domains. Differences emerge between the AMAZON and CONVOTE datasets in terms of the ensemble learning hypothesis. While there has been some theoretical analyses on the topic of MDL (Ben-David et al., 2007; Ben-David et al., 2009; Mansour et al., 2009; Daum´e III et al., 2010a), our results suggest performing new analyses that relate ensemble learning results with the MDL setting. These analyses could provide insights into new algorithms that can take advantage of the specific properties of each multi-domain setting. Question: What makes a good domain for MDL? To the best of our knowledge, previous work has assumed that domain identities are provided to the learning algorithm. However, in reality, there may be many ways to split a dataset into domains. For example, consider the CONVOTE dataset, which we split both by </context>
</contexts>
<marker>Ben-David, Blitzer, Crammer, Kulesza, Pereira, Vaughan, 2009</marker>
<rawString>Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wortman Vaughan. 2009. A theory of learning from different domains. Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Blitzer</author>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
</authors>
<title>Domain Adaptation with Structural Correspondence Learning.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>120--128</pages>
<contexts>
<context position="5871" citStr="Blitzer et al., 2006" startWordPosition="915" endWordPosition="918">specific distribution Ddz, and yi is the label (e.g. yi E {−1, +1} for binary labels). Standard learning ignores di, but MDL uses these to improve learning accuracy. Why should we care about the domain label? Domain differences can introduce errors in a number of ways (Ben-David et al., 2007; Ben-David et al., 2009). First, the domain-specific distributions Ddz can differ such that they favor different features, i.e. p(x) changes between domains. As a result, some features may only appear in one domain. This aspect of domain difference is typically the focus of unsupervised domain adaptation (Blitzer et al., 2006; Blitzer et al., 2007). Second, the features may behave differently with respect to the label in each domain, i.e. p(y|x) changes between domains. As a result, a learning algorithm cannot generalize the behavior of features from one domain to another. The key idea behind many MDL algorithms is to target one or both of these properties of domain difference to improve performance. Prior approaches to MDL can be broadly categorized into two classes. The first set of approaches (Daum´e III, 2007; Dredze et al., 2008) introduce parameters to capture domain-specific behaviors while preserving featu</context>
</contexts>
<marker>Blitzer, McDonald, Pereira, 2006</marker>
<rawString>John Blitzer, Ryan McDonald, and Fernando Pereira. 2006. Domain Adaptation with Structural Correspondence Learning. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, pages 120–128.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Blitzer</author>
<author>Mark Dredze</author>
<author>Fernando Pereira</author>
</authors>
<title>Biographies, Bollywood, Boom-boxes and Blenders: Domain Adaptation for Sentiment Classification.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>440--447</pages>
<contexts>
<context position="1832" citStr="Blitzer et al., 2007" startWordPosition="257" endWordPosition="260">has had success in multi-domain learning, and it suggests some important open questions for improving beyond the current state of the art. 1 Introduction Research efforts in recent years have demonstrated the importance of domains in statistical natural language processing. A mismatch between training and test domains can negatively impact system accuracy as it violates a core assumption in many machine learning algorithms: that data points are independent and identically distributed (i.i.d.). As a result, numerous domain adaptation methods (Chelba and Acero, 2004; Daum´e III and Marcu, 2006; Blitzer et al., 2007) target settings with a training set from one domain and a test set from another. Often times the training set itself violates the i.i.d. assumption and contains multiple domains. In this case, training a single model obscures domain distinctions, and separating the dataset by domains reduces training data. Instead, multi-domain learning (MDL) can take advantage of these domain labels to improve learning (Daum´e III, 2007; Dredze and Crammer, 2008; Arnold et al., 2008; Finkel and Manning, 2009; Zhang and Yeung, 2010; Saha et al., 2011). One such example is sentiment classification of product r</context>
<context position="3569" citStr="Blitzer et al. (2007)" startWordPosition="539" endWordPosition="543">ey question of this paper is: when do domains matter? Towards this goal we explore two issues. First, we explore the question of whether domain distinctions are used by existing MDL algorithms in meaningful ways. While differences in feature behaviors between domains will hurt performance (Blitzer et al., 2008; Ben-David et al., 2009), it is not clear if the improvements in MDL algorithms can be attributed to correcting these errors, or whether they are benefiting from something else. In particular, there are many similarities between MDL and ensemble methods, with connections to instance bag1Blitzer et al. (2007) do not consider the MDL setup, they consider a single source domain, and a single target domain, with little or no labeled data available for the target domain. 1302 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1302–1312, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics ging, feature bagging and classifier combination. It may be that gains in MDL are the usual ensemble learning improvements. Second, one simple way in which domains can change is the distributi</context>
<context position="5894" citStr="Blitzer et al., 2007" startWordPosition="919" endWordPosition="922">Ddz, and yi is the label (e.g. yi E {−1, +1} for binary labels). Standard learning ignores di, but MDL uses these to improve learning accuracy. Why should we care about the domain label? Domain differences can introduce errors in a number of ways (Ben-David et al., 2007; Ben-David et al., 2009). First, the domain-specific distributions Ddz can differ such that they favor different features, i.e. p(x) changes between domains. As a result, some features may only appear in one domain. This aspect of domain difference is typically the focus of unsupervised domain adaptation (Blitzer et al., 2006; Blitzer et al., 2007). Second, the features may behave differently with respect to the label in each domain, i.e. p(y|x) changes between domains. As a result, a learning algorithm cannot generalize the behavior of features from one domain to another. The key idea behind many MDL algorithms is to target one or both of these properties of domain difference to improve performance. Prior approaches to MDL can be broadly categorized into two classes. The first set of approaches (Daum´e III, 2007; Dredze et al., 2008) introduce parameters to capture domain-specific behaviors while preserving features that learn domain-g</context>
<context position="8821" citStr="Blitzer et al. (2007)" startWordPosition="1390" endWordPosition="1393">key idea is to learn both domain-specific behaviors and behaviors that generalize between (possibly related) domains. 3 Data To support our analysis we develop several empirical experiments. We first summarize the datasets and methods that we use in our experiments, then 1303 proceed to our exploration of MDL. 3.1 Datasets A variety of multi-domain datasets have been used for demonstrating MDL improvements. In this paper, we focus on two datasets representative of many of the properties of MDL. Amazon (AMAZON) Our first dataset is the MultiDomain Amazon data (version 2.0), first introduced by Blitzer et al. (2007). The task is binary sentiment classification, in which Amazon product reviews are labeled as positive or negative. Domains are defined by product categories. We select the four domains used in most studies: books, dvd, electronics and kitchen appliances. The original dataset contained 2,000 reviews for each of the four domains, with 1,000 positive and 1,000 negative reviews per domain. Feature extraction follows Blitzer et al. (2007): we use case insensitive unigrams and bigrams, although we remove rare features (those that appear less than five times in the training set). The reduced feature</context>
</contexts>
<marker>Blitzer, Dredze, Pereira, 2007</marker>
<rawString>John Blitzer, Mark Dredze, and Fernando Pereira. 2007. Biographies, Bollywood, Boom-boxes and Blenders: Domain Adaptation for Sentiment Classification. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 440–447.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Blitzer</author>
<author>Koby Crammer</author>
<author>Alex Kulesza</author>
<author>Fernando Pereira</author>
<author>Jennifer Wortman</author>
</authors>
<title>Learning Bounds for Domain Adaptation.</title>
<date>2008</date>
<booktitle>In Advances in Neural Information Processing Systems (NIPS</booktitle>
<contexts>
<context position="3259" citStr="Blitzer et al., 2008" startWordPosition="489" endWordPosition="492">prior research has shown improvements using MDL, this paper explores what properties of an MDL setting matter. Are previous improvements from MDL algorithms discovering important distinctions between features in different domains, as we would hope, or are other factors contributing to learning success? The key question of this paper is: when do domains matter? Towards this goal we explore two issues. First, we explore the question of whether domain distinctions are used by existing MDL algorithms in meaningful ways. While differences in feature behaviors between domains will hurt performance (Blitzer et al., 2008; Ben-David et al., 2009), it is not clear if the improvements in MDL algorithms can be attributed to correcting these errors, or whether they are benefiting from something else. In particular, there are many similarities between MDL and ensemble methods, with connections to instance bag1Blitzer et al. (2007) do not consider the MDL setup, they consider a single source domain, and a single target domain, with little or no labeled data available for the target domain. 1302 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Lang</context>
</contexts>
<marker>Blitzer, Crammer, Kulesza, Pereira, Wortman, 2008</marker>
<rawString>John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wortman. 2008. Learning Bounds for Domain Adaptation. In Advances in Neural Information Processing Systems (NIPS 2007).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Giovanni Cavallanti</author>
<author>Nicol`o Cesa-Bianchi</author>
<author>Claudio Gentile</author>
</authors>
<title>Linear Algorithms for Online Multitask Classification.</title>
<date>2008</date>
<booktitle>In Proceedings of COLT.</booktitle>
<contexts>
<context position="7246" citStr="Cavallanti et al. (2008)" startWordPosition="1139" endWordPosition="1142">2007) proposes a very simple “easy adapt” approach, which was originally proposed in the context of adapting to a specific target domain, but easily generalizes to MDL. Dredze et al. (2008) consider the problem of learning how to combine different domain-specific classifiers such that behaviors common to several domains can be captured by a shared classifier, while domain-specific behavior is still captured by the individual classifiers. We describe both of these approaches in § 3.2. The second set of approaches to MDL introduce an explicit notion of relationship between domains. For example, Cavallanti et al. (2008) assume a fixed task relationship matrix in the context of online multi-task learning. The key assumption is that instances from two different domains are half as much related to each other as two instances from the same domain. Saha et al. (2011) improve upon the idea of simply using a fixed task relationship matrix by instead learning it adaptively. They derive an online algorithm for updating the task interaction matrix. Zhang and Yeung (2010) derive a convex formulation for adaptively learning domain relationships. We describe their approach in § 3.2. Finally, Daum´e III (2009) proposes a </context>
</contexts>
<marker>Cavallanti, Cesa-Bianchi, Gentile, 2008</marker>
<rawString>Giovanni Cavallanti, Nicol`o Cesa-Bianchi, and Claudio Gentile. 2008. Linear Algorithms for Online Multitask Classification. In Proceedings of COLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ciprian Chelba</author>
<author>Alex Acero</author>
</authors>
<title>Adaptation of Maximum Entropy Capitalizer: Little Data Can Help a Lot.</title>
<date>2004</date>
<booktitle>In Dekang Lin and Dekai Wu, editors, Proceedings of EMNLP 2004,</booktitle>
<pages>285--292</pages>
<contexts>
<context position="1781" citStr="Chelba and Acero, 2004" startWordPosition="248" endWordPosition="251">ssues presents a clearer idea about where the field has had success in multi-domain learning, and it suggests some important open questions for improving beyond the current state of the art. 1 Introduction Research efforts in recent years have demonstrated the importance of domains in statistical natural language processing. A mismatch between training and test domains can negatively impact system accuracy as it violates a core assumption in many machine learning algorithms: that data points are independent and identically distributed (i.i.d.). As a result, numerous domain adaptation methods (Chelba and Acero, 2004; Daum´e III and Marcu, 2006; Blitzer et al., 2007) target settings with a training set from one domain and a test set from another. Often times the training set itself violates the i.i.d. assumption and contains multiple domains. In this case, training a single model obscures domain distinctions, and separating the dataset by domains reduces training data. Instead, multi-domain learning (MDL) can take advantage of these domain labels to improve learning (Daum´e III, 2007; Dredze and Crammer, 2008; Arnold et al., 2008; Finkel and Manning, 2009; Zhang and Yeung, 2010; Saha et al., 2011). One su</context>
</contexts>
<marker>Chelba, Acero, 2004</marker>
<rawString>Ciprian Chelba and Alex Acero. 2004. Adaptation of Maximum Entropy Capitalizer: Little Data Can Help a Lot. In Dekang Lin and Dekai Wu, editors, Proceedings of EMNLP 2004, pages 285–292.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koby Crammer</author>
<author>Mark Dredze</author>
<author>Fernando Pereira</author>
</authors>
<title>Exact convex confidence-weighted learning.</title>
<date>2008</date>
<booktitle>In Advances in Neural Information Processing Systems (NIPS).</booktitle>
<contexts>
<context position="12523" citStr="Crammer et al. (2008)" startWordPosition="1968" endWordPosition="1971">low to obtain FEDA-SVM and FEDA-LR. MDR Multi-domain regularization (MDR) (Dredze and Crammer, 2008; Dredze et al., 2009) extends the idea behind classifier combination by explicitly formulating a classifier combination scheme based on Confidence-Weighted learning (Dredze et al., 2008). Additionally, classifier updates (which happen in an online framework) contain an explicit constraint that the combined classifier should perform well on the example. Dredze et al. (2009) consider several variants of MDR. We select the two best performing methods: MDR-L2, which uses the underlying algorithm of Crammer et al. (2008), and MDR-KL, which uses the underlying algorithm of Dredze et al. (2008). We follow their approach to classifier training and parameter optimization. MTRL The multi-task relationship learning (MTRL) approach proposed by Zhang and Yeung 1304 (2010) achieves states of the art performance on many MDL tasks. This method is representative of methods that learn similarities between domains and in turn regularize domain-specific parameters accordingly. The key idea in their work is the use of a matrix-normal distribution p(X|M, fl, E) as a prior on the matrix W created by column-wise stacking of the</context>
</contexts>
<marker>Crammer, Dredze, Pereira, 2008</marker>
<rawString>Koby Crammer, Mark Dredze, and Fernando Pereira. 2008. Exact convex confidence-weighted learning. In Advances in Neural Information Processing Systems (NIPS).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
<author>Daniel Marcu</author>
</authors>
<title>Domain adaptation for statistical classifiers.</title>
<date>2006</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<volume>26</volume>
<issue>1</issue>
<marker>Daum´e, Marcu, 2006</marker>
<rawString>Hal Daum´e III and Daniel Marcu. 2006. Domain adaptation for statistical classifiers. Journal of Artificial Intelligence Research, 26(1):101–126.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e Abhishek Kumar</author>
<author>Avishek Saha</author>
</authors>
<title>A Co-regularization Based Semi-supervised Domain Adaptation.</title>
<date>2010</date>
<booktitle>In Neural Information Processing Systems.</booktitle>
<marker>Kumar, Saha, 2010</marker>
<rawString>Hal Daum´e III, Abhishek Kumar, and Avishek Saha. 2010a. A Co-regularization Based Semi-supervised Domain Adaptation. In Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e Abhishek Kumar</author>
<author>Avishek Saha</author>
</authors>
<title>Frustratingly Easy Semi-Supervised Domain Adaptation.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACL 2010 Workshop on Domain Adaptation for Natural Language Processing,</booktitle>
<pages>53--59</pages>
<marker>Kumar, Saha, 2010</marker>
<rawString>Hal Daum´e III, Abhishek Kumar, and Avishek Saha. 2010b. Frustratingly Easy Semi-Supervised Domain Adaptation. In Proceedings of the ACL 2010 Workshop on Domain Adaptation for Natural Language Processing, pages 53–59.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
</authors>
<title>Frustratingly Easy Domain Adaptation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>256--263</pages>
<marker>Daum´e, 2007</marker>
<rawString>Hal Daum´e III. 2007. Frustratingly Easy Domain Adaptation. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 256–263.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
</authors>
<title>Bayesian multitask learning with latent hierarchies.</title>
<date>2009</date>
<booktitle>In Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence.</booktitle>
<marker>Daum´e, 2009</marker>
<rawString>Hal Daum´e III. 2009. Bayesian multitask learning with latent hierarchies. In Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas G Dietterich</author>
</authors>
<title>An experimental comparison of three methods for constructing ensembles of decision trees: Bagging, boosting, and randomization.</title>
<date>2000</date>
<booktitle>Machine Learning,</booktitle>
<pages>40--139</pages>
<contexts>
<context position="16357" citStr="Dietterich, 2000" startWordPosition="2577" endWordPosition="2578">approaches bear a striking resemblance to ensemble learning. Traditionally, ensemble learning combines the output from several different classifiers to obtain a single improved model (Maclin and Opitz, 1999). It is well established that ensemble learning, applied on top of a diverse array of quality classifiers, can improve results for a variety of tasks. The key idea behind ensemble learning, that of combining a diverse array of models, has been applied to settings in which data preprocessing is used to create many different classifiers. Examples include instance bagging and feature bagging (Dietterich, 2000). The core idea of using diverse inputs in making classification decisions is common in the MDL literature. In fact, the top performing and only successful entry to the 2007 CoNLL shared task on domain adaptation for dependency parsing was a straightforward implementation of ensemble learning by creating variants of parsers (Sagae and Tsujii, 2007). Many MDL algorithms, among them Dredze and Crammer (2008), Daum´e III (2009), Zhang and Yeung (2010) and Saha et al. (2011), all include some notion of learning domain-specific classifiers on the training data, and combining them in the best way po</context>
</contexts>
<marker>Dietterich, 2000</marker>
<rawString>Thomas G. Dietterich. 2000. An experimental comparison of three methods for constructing ensembles of decision trees: Bagging, boosting, and randomization. Machine Learning, 40:139–157.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Dredze</author>
<author>Koby Crammer</author>
</authors>
<title>Online methods for multi-domain learning and adaptation.</title>
<date>2008</date>
<booktitle>Proceedings of the Conference on Empirical Methods in Natural Language Processing - EMNLP ’08.</booktitle>
<contexts>
<context position="2283" citStr="Dredze and Crammer, 2008" startWordPosition="330" endWordPosition="333"> independent and identically distributed (i.i.d.). As a result, numerous domain adaptation methods (Chelba and Acero, 2004; Daum´e III and Marcu, 2006; Blitzer et al., 2007) target settings with a training set from one domain and a test set from another. Often times the training set itself violates the i.i.d. assumption and contains multiple domains. In this case, training a single model obscures domain distinctions, and separating the dataset by domains reduces training data. Instead, multi-domain learning (MDL) can take advantage of these domain labels to improve learning (Daum´e III, 2007; Dredze and Crammer, 2008; Arnold et al., 2008; Finkel and Manning, 2009; Zhang and Yeung, 2010; Saha et al., 2011). One such example is sentiment classification of product reviews. Training data is available from many product categories and while all data should be used to learn a model, there are important differences between the categories (Blitzer et al., 2007)1. While much prior research has shown improvements using MDL, this paper explores what properties of an MDL setting matter. Are previous improvements from MDL algorithms discovering important distinctions between features in different domains, as we would h</context>
<context position="12001" citStr="Dredze and Crammer, 2008" startWordPosition="1891" endWordPosition="1894">roducibility of our results. FEDA Frustratingly easy domain adaptation (FEDA) (Daum´e III, 2007; Daum´e III et al., 2010b; Daum´e III et al., 2010a) is an example of a classifier combination approach to MDL. The feature space is a cross-product of the domain and input features, augmented with the original input features (shared features). Prediction is effectively a linear combination of a set of domain-specific weights and shared weights. We combine FEDA with both the SVM and logistic regression algorithms described below to obtain FEDA-SVM and FEDA-LR. MDR Multi-domain regularization (MDR) (Dredze and Crammer, 2008; Dredze et al., 2009) extends the idea behind classifier combination by explicitly formulating a classifier combination scheme based on Confidence-Weighted learning (Dredze et al., 2008). Additionally, classifier updates (which happen in an online framework) contain an explicit constraint that the combined classifier should perform well on the example. Dredze et al. (2009) consider several variants of MDR. We select the two best performing methods: MDR-L2, which uses the underlying algorithm of Crammer et al. (2008), and MDR-KL, which uses the underlying algorithm of Dredze et al. (2008). We </context>
<context position="16766" citStr="Dredze and Crammer (2008)" startWordPosition="2642" endWordPosition="2645"> of combining a diverse array of models, has been applied to settings in which data preprocessing is used to create many different classifiers. Examples include instance bagging and feature bagging (Dietterich, 2000). The core idea of using diverse inputs in making classification decisions is common in the MDL literature. In fact, the top performing and only successful entry to the 2007 CoNLL shared task on domain adaptation for dependency parsing was a straightforward implementation of ensemble learning by creating variants of parsers (Sagae and Tsujii, 2007). Many MDL algorithms, among them Dredze and Crammer (2008), Daum´e III (2009), Zhang and Yeung (2010) and Saha et al. (2011), all include some notion of learning domain-specific classifiers on the training data, and combining them in the best way possible. To be clear, we do not claim that these approaches can be reduced to an existing ensemble learning algorithm. There are crucial elements 1305 in each of these algorithms that separate them from existing ensemble learning algorithms. One example of such a distinction is the learning of domain relationships by both Zhang and Yeung (2010) and Saha et al. (2011). However, we argue that their core appro</context>
</contexts>
<marker>Dredze, Crammer, 2008</marker>
<rawString>Mark Dredze and Koby Crammer. 2008. Online methods for multi-domain learning and adaptation. Proceedings of the Conference on Empirical Methods in Natural Language Processing - EMNLP ’08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Dredze</author>
<author>Koby Crammer</author>
<author>Fernando Pereira</author>
</authors>
<title>Confidence-weighted linear classification.</title>
<date>2008</date>
<booktitle>Proceedings of the 25th international conference on Machine learning - ICML ’08.</booktitle>
<contexts>
<context position="6390" citStr="Dredze et al., 2008" startWordPosition="1004" endWordPosition="1007">t of domain difference is typically the focus of unsupervised domain adaptation (Blitzer et al., 2006; Blitzer et al., 2007). Second, the features may behave differently with respect to the label in each domain, i.e. p(y|x) changes between domains. As a result, a learning algorithm cannot generalize the behavior of features from one domain to another. The key idea behind many MDL algorithms is to target one or both of these properties of domain difference to improve performance. Prior approaches to MDL can be broadly categorized into two classes. The first set of approaches (Daum´e III, 2007; Dredze et al., 2008) introduce parameters to capture domain-specific behaviors while preserving features that learn domain-general behaviors. A key of these methods is that they do not explicitly model any relationship between the domains. Daum´e III (2007) proposes a very simple “easy adapt” approach, which was originally proposed in the context of adapting to a specific target domain, but easily generalizes to MDL. Dredze et al. (2008) consider the problem of learning how to combine different domain-specific classifiers such that behaviors common to several domains can be captured by a shared classifier, while </context>
<context position="12188" citStr="Dredze et al., 2008" startWordPosition="1917" endWordPosition="1920">approach to MDL. The feature space is a cross-product of the domain and input features, augmented with the original input features (shared features). Prediction is effectively a linear combination of a set of domain-specific weights and shared weights. We combine FEDA with both the SVM and logistic regression algorithms described below to obtain FEDA-SVM and FEDA-LR. MDR Multi-domain regularization (MDR) (Dredze and Crammer, 2008; Dredze et al., 2009) extends the idea behind classifier combination by explicitly formulating a classifier combination scheme based on Confidence-Weighted learning (Dredze et al., 2008). Additionally, classifier updates (which happen in an online framework) contain an explicit constraint that the combined classifier should perform well on the example. Dredze et al. (2009) consider several variants of MDR. We select the two best performing methods: MDR-L2, which uses the underlying algorithm of Crammer et al. (2008), and MDR-KL, which uses the underlying algorithm of Dredze et al. (2008). We follow their approach to classifier training and parameter optimization. MTRL The multi-task relationship learning (MTRL) approach proposed by Zhang and Yeung 1304 (2010) achieves states </context>
</contexts>
<marker>Dredze, Crammer, Pereira, 2008</marker>
<rawString>Mark Dredze, Koby Crammer, and Fernando Pereira. 2008. Confidence-weighted linear classification. Proceedings of the 25th international conference on Machine learning - ICML ’08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Dredze</author>
<author>Alex Kulesza</author>
<author>Koby Crammer</author>
</authors>
<title>Multi-domain learning by confidence-weighted parameter combination.</title>
<date>2009</date>
<booktitle>Machine Learning,</booktitle>
<pages>79--1</pages>
<contexts>
<context position="12023" citStr="Dredze et al., 2009" startWordPosition="1895" endWordPosition="1898">s. FEDA Frustratingly easy domain adaptation (FEDA) (Daum´e III, 2007; Daum´e III et al., 2010b; Daum´e III et al., 2010a) is an example of a classifier combination approach to MDL. The feature space is a cross-product of the domain and input features, augmented with the original input features (shared features). Prediction is effectively a linear combination of a set of domain-specific weights and shared weights. We combine FEDA with both the SVM and logistic regression algorithms described below to obtain FEDA-SVM and FEDA-LR. MDR Multi-domain regularization (MDR) (Dredze and Crammer, 2008; Dredze et al., 2009) extends the idea behind classifier combination by explicitly formulating a classifier combination scheme based on Confidence-Weighted learning (Dredze et al., 2008). Additionally, classifier updates (which happen in an online framework) contain an explicit constraint that the combined classifier should perform well on the example. Dredze et al. (2009) consider several variants of MDR. We select the two best performing methods: MDR-L2, which uses the underlying algorithm of Crammer et al. (2008), and MDR-KL, which uses the underlying algorithm of Dredze et al. (2008). We follow their approach </context>
<context position="14315" citStr="Dredze et al., 2009" startWordPosition="2249" endWordPosition="2252">using the following values for each (a total of 36 combinations): 10.00001, 0.0001, 0.001, 0.01, 0.1, 11. In addition to these multi-task learning methods, we consider a common baseline: ignoring the domain distinctions and learning a single classifier over all the data. This reflects single-domain learning, in which no domain knowledge is used and will indicate baseline performance for all experiments. While some earlier research has included a separate one classifier per domain baseline, it almost always performs worse, since splitting the domains provides much less data to each classifier (Dredze et al., 2009). So we omit this baseline for simplicity. To obtain a single classifier we use two classification algorithms: SVMs and logistic regression. Support Vector Machines A single SVM run over all the training data, ignoring domain labels. We use the SVM implementation available in the LIBLINEAR package (Fan et al., 2008). In particular, we use the L2-regularized L2-loss SVM (option -s 1 in version 1.8 of LIBLINEAR, and also option -B 1 for including a standard bias feature). We tune the SVM using five-fold stratified cross-validation on the training set, using the following values for the trade-off</context>
<context position="20664" citStr="Dredze et al., 2009" startWordPosition="3281" endWordPosition="3284">labels, we increase the number of trials. We repeat each cross-validation experiment 5 times with different randomization of the domain labels each time. Results Results are shown in Table 1. The first row shows absolute (average) accuracy for a single classifier trained on all data, ignoring domain distinctions. The remaining cells indicate absolute improvements against the baseline. First, we note for the well-studied AMAZON dataset that our results with true domains are consistent with the previous literature. FEDA is known to not improve upon a single classifier baseline for that dataset (Dredze et al., 2009). Both MDR-L2 and MDR-KL improve upon the single classifier baseline, again as per Dredze et al. (2009). And finally, MTRL also improves upon the single classifier baseline. Although the MTRL improvement is not as dramatic as in the original paper3, the average accuracy that we achieve for MTRL (84.2%) is better than the best average accuracy in the original paper (83.65%). The main comparison to make in Table 1 is between having knowledge of true domains or not. “Random Domain” in the table is the case where domain identifiers are randomly shuffled within a given fold. Ignoring the significan</context>
</contexts>
<marker>Dredze, Kulesza, Crammer, 2009</marker>
<rawString>Mark Dredze, Alex Kulesza, and Koby Crammer. 2009. Multi-domain learning by confidence-weighted parameter combination. Machine Learning, 79(1-2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rong-En Fan</author>
<author>Kai-Wei Chang</author>
<author>Cho-Jui Hsieh</author>
<author>Xiang-rui Wang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBLINEAR : A Library for Large Linear Classification.</title>
<date>2008</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>9--1871</pages>
<contexts>
<context position="14632" citStr="Fan et al., 2008" startWordPosition="2301" endWordPosition="2304">o domain knowledge is used and will indicate baseline performance for all experiments. While some earlier research has included a separate one classifier per domain baseline, it almost always performs worse, since splitting the domains provides much less data to each classifier (Dredze et al., 2009). So we omit this baseline for simplicity. To obtain a single classifier we use two classification algorithms: SVMs and logistic regression. Support Vector Machines A single SVM run over all the training data, ignoring domain labels. We use the SVM implementation available in the LIBLINEAR package (Fan et al., 2008). In particular, we use the L2-regularized L2-loss SVM (option -s 1 in version 1.8 of LIBLINEAR, and also option -B 1 for including a standard bias feature). We tune the SVM using five-fold stratified cross-validation on the training set, using the following values for the trade-off parameter C: 10.0001, 0.001, 0.01, 0.1, 0.2, 0.3, 0.5, 11. 2http://www.cse.ust.hk/˜zhangyu/codes/ MTRL.zip Logistic Regression (LR) A single logistic regression model run over all the training data, ignoring domain labels. Again, we use the L2-regularized LR implementation available in the LIBLINEAR package (option</context>
</contexts>
<marker>Fan, Chang, Hsieh, Wang, Lin, 2008</marker>
<rawString>Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR : A Library for Large Linear Classification. Journal of Machine Learning Research, 9:1871–1874.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny R Finkel</author>
<author>Christopher D Manning</author>
</authors>
<title>Hierarchical Bayesian Domain Adaptation.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>602--610</pages>
<contexts>
<context position="2330" citStr="Finkel and Manning, 2009" startWordPosition="338" endWordPosition="341">.). As a result, numerous domain adaptation methods (Chelba and Acero, 2004; Daum´e III and Marcu, 2006; Blitzer et al., 2007) target settings with a training set from one domain and a test set from another. Often times the training set itself violates the i.i.d. assumption and contains multiple domains. In this case, training a single model obscures domain distinctions, and separating the dataset by domains reduces training data. Instead, multi-domain learning (MDL) can take advantage of these domain labels to improve learning (Daum´e III, 2007; Dredze and Crammer, 2008; Arnold et al., 2008; Finkel and Manning, 2009; Zhang and Yeung, 2010; Saha et al., 2011). One such example is sentiment classification of product reviews. Training data is available from many product categories and while all data should be used to learn a model, there are important differences between the categories (Blitzer et al., 2007)1. While much prior research has shown improvements using MDL, this paper explores what properties of an MDL setting matter. Are previous improvements from MDL algorithms discovering important distinctions between features in different domains, as we would hope, or are other factors contributing to learn</context>
</contexts>
<marker>Finkel, Manning, 2009</marker>
<rawString>Jenny R. Finkel and Christopher D. Manning. 2009. Hierarchical Bayesian Domain Adaptation. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 602–610.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Maclin</author>
<author>David Opitz</author>
</authors>
<title>Popular Ensemble Methods: An Empirical Study.</title>
<date>1999</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>11--169</pages>
<contexts>
<context position="15947" citStr="Maclin and Opitz, 1999" startWordPosition="2508" endWordPosition="2511">or SVM above, including the values of the trade-off parameter C. For all experiments, we measure average accuracy over K-fold cross-validation, using 10 folds for AMAZON, and 5 folds for both BILL and PARTY. 4 When Do Domains Matter? We now empirically explore two questions regarding the behavior of MDL. 4.1 Ensemble Learning Question: Are MDL improvements the result of ensemble learning effects? Many of the MDL approaches bear a striking resemblance to ensemble learning. Traditionally, ensemble learning combines the output from several different classifiers to obtain a single improved model (Maclin and Opitz, 1999). It is well established that ensemble learning, applied on top of a diverse array of quality classifiers, can improve results for a variety of tasks. The key idea behind ensemble learning, that of combining a diverse array of models, has been applied to settings in which data preprocessing is used to create many different classifiers. Examples include instance bagging and feature bagging (Dietterich, 2000). The core idea of using diverse inputs in making classification decisions is common in the MDL literature. In fact, the top performing and only successful entry to the 2007 CoNLL shared tas</context>
</contexts>
<marker>Maclin, Opitz, 1999</marker>
<rawString>Richard Maclin and David Opitz. 1999. Popular Ensemble Methods: An Empirical Study. Journal of Artificial Intelligence Research, 11:169–198.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yishay Mansour</author>
<author>Mehryar Mohri</author>
<author>Afshin Rostamizadeh</author>
</authors>
<title>Domain Adaptation with Multiple Sources.</title>
<date>2009</date>
<booktitle>In Proceedings of NIPS</booktitle>
<pages>1041--1048</pages>
<contexts>
<context position="36306" citStr="Mansour et al., 2009" startWordPosition="5996" endWordPosition="5999">is of MDL algorithms revealed new trends that suggest further avenues of exploration. We suggest three open questions in response. Question: When are MDL methods most effective? Our empirical results suggest that MDL can be more effective in settings with domain-specific class biases. However, we also saw differences in improvements for each method, and for different domains. Differences emerge between the AMAZON and CONVOTE datasets in terms of the ensemble learning hypothesis. While there has been some theoretical analyses on the topic of MDL (Ben-David et al., 2007; Ben-David et al., 2009; Mansour et al., 2009; Daum´e III et al., 2010a), our results suggest performing new analyses that relate ensemble learning results with the MDL setting. These analyses could provide insights into new algorithms that can take advantage of the specific properties of each multi-domain setting. Question: What makes a good domain for MDL? To the best of our knowledge, previous work has assumed that domain identities are provided to the learning algorithm. However, in reality, there may be many ways to split a dataset into domains. For example, consider the CONVOTE dataset, which we split both by BILL and PARTY. The ch</context>
</contexts>
<marker>Mansour, Mohri, Rostamizadeh, 2009</marker>
<rawString>Yishay Mansour, Mehryar Mohri, and Afshin Rostamizadeh. 2009. Domain Adaptation with Multiple Sources. In Proceedings of NIPS 2008, pages 1041– 1048.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Sagae</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Dependency parsing and domain adaptation with lr models and parser ensembles.</title>
<date>2007</date>
<booktitle>In Conference on Natural Language Learning (Shared Task).</booktitle>
<contexts>
<context position="16707" citStr="Sagae and Tsujii, 2007" startWordPosition="2633" endWordPosition="2636">ety of tasks. The key idea behind ensemble learning, that of combining a diverse array of models, has been applied to settings in which data preprocessing is used to create many different classifiers. Examples include instance bagging and feature bagging (Dietterich, 2000). The core idea of using diverse inputs in making classification decisions is common in the MDL literature. In fact, the top performing and only successful entry to the 2007 CoNLL shared task on domain adaptation for dependency parsing was a straightforward implementation of ensemble learning by creating variants of parsers (Sagae and Tsujii, 2007). Many MDL algorithms, among them Dredze and Crammer (2008), Daum´e III (2009), Zhang and Yeung (2010) and Saha et al. (2011), all include some notion of learning domain-specific classifiers on the training data, and combining them in the best way possible. To be clear, we do not claim that these approaches can be reduced to an existing ensemble learning algorithm. There are crucial elements 1305 in each of these algorithms that separate them from existing ensemble learning algorithms. One example of such a distinction is the learning of domain relationships by both Zhang and Yeung (2010) and </context>
</contexts>
<marker>Sagae, Tsujii, 2007</marker>
<rawString>Kenji Sagae and Jun’ichi Tsujii. 2007. Dependency parsing and domain adaptation with lr models and parser ensembles. In Conference on Natural Language Learning (Shared Task).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Avishek Saha</author>
<author>Piyush Rai</author>
<author>Hal Daum´e</author>
<author>Suresh Venkatasubramanian</author>
</authors>
<title>Online learning of multiple tasks and their relationships.</title>
<date>2011</date>
<booktitle>In Proceedings of AISTATS</booktitle>
<marker>Saha, Rai, Daum´e, Venkatasubramanian, 2011</marker>
<rawString>Avishek Saha, Piyush Rai, Hal Daum´e III, and Suresh Venkatasubramanian. 2011. Online learning of multiple tasks and their relationships. In Proceedings of AISTATS 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matt Thomas</author>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Get out the vote: Determining support or opposition from Congressional floor-debate transcripts.</title>
<date>2006</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>327--335</pages>
<contexts>
<context position="9658" citStr="Thomas et al. (2006)" startWordPosition="1524" endWordPosition="1527">d, electronics and kitchen appliances. The original dataset contained 2,000 reviews for each of the four domains, with 1,000 positive and 1,000 negative reviews per domain. Feature extraction follows Blitzer et al. (2007): we use case insensitive unigrams and bigrams, although we remove rare features (those that appear less than five times in the training set). The reduced feature set was selected given the sensitivity to feature size of some of the MDL methods. ConVote (CONVOTE) Our second dataset is taken from segments of speech from United States Congress floor debates, first introduced by Thomas et al. (2006). The binary classification task on this dataset is that of predicting whether a given speech segment supports or opposes a bill under discussion in the floor debate. We select this dataset because, unlike the AMAZON data, CONVOTE can be divided into domains in several ways based on different metadata attributes available with the dataset. We consider two types of domain divisions: the bill identifier and the political party of the speaker. Division based on the bill creates domain differences in that each bill has its own topic. Division based on political party implies preference for differe</context>
</contexts>
<marker>Thomas, Pang, Lee, 2006</marker>
<rawString>Matt Thomas, Bo Pang, and Lillian Lee. 2006. Get out the vote: Determining support or opposition from Congressional floor-debate transcripts. In Proceedings of EMNLP, pages 327–335.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yu Zhang</author>
<author>Dit-Yan Yeung</author>
</authors>
<title>A Convex Formulation for Learning Task Relationships in Multi-Task Learning.</title>
<date>2010</date>
<booktitle>In Proceedings of the Proceedings of the Twenty-Sixth Conference Annual Conference on Uncertainty in Artificial Intelligence (UAI-10).</booktitle>
<contexts>
<context position="2353" citStr="Zhang and Yeung, 2010" startWordPosition="342" endWordPosition="345">domain adaptation methods (Chelba and Acero, 2004; Daum´e III and Marcu, 2006; Blitzer et al., 2007) target settings with a training set from one domain and a test set from another. Often times the training set itself violates the i.i.d. assumption and contains multiple domains. In this case, training a single model obscures domain distinctions, and separating the dataset by domains reduces training data. Instead, multi-domain learning (MDL) can take advantage of these domain labels to improve learning (Daum´e III, 2007; Dredze and Crammer, 2008; Arnold et al., 2008; Finkel and Manning, 2009; Zhang and Yeung, 2010; Saha et al., 2011). One such example is sentiment classification of product reviews. Training data is available from many product categories and while all data should be used to learn a model, there are important differences between the categories (Blitzer et al., 2007)1. While much prior research has shown improvements using MDL, this paper explores what properties of an MDL setting matter. Are previous improvements from MDL algorithms discovering important distinctions between features in different domains, as we would hope, or are other factors contributing to learning success? The key qu</context>
<context position="7696" citStr="Zhang and Yeung (2010)" startWordPosition="1214" endWordPosition="1217">e both of these approaches in § 3.2. The second set of approaches to MDL introduce an explicit notion of relationship between domains. For example, Cavallanti et al. (2008) assume a fixed task relationship matrix in the context of online multi-task learning. The key assumption is that instances from two different domains are half as much related to each other as two instances from the same domain. Saha et al. (2011) improve upon the idea of simply using a fixed task relationship matrix by instead learning it adaptively. They derive an online algorithm for updating the task interaction matrix. Zhang and Yeung (2010) derive a convex formulation for adaptively learning domain relationships. We describe their approach in § 3.2. Finally, Daum´e III (2009) proposes a joint task clustering and multitask/multi-domain learning setup, where instead of just learning pairwise domain relationships, a hierarchical structure among them is inferred. Hierarchical clustering of tasks is performed in a Bayesian framework, by imposing a hierarchical prior on the structure of the task relationships. In all of these settings, the key idea is to learn both domain-specific behaviors and behaviors that generalize between (possi</context>
<context position="16809" citStr="Zhang and Yeung (2010)" startWordPosition="2649" endWordPosition="2653">een applied to settings in which data preprocessing is used to create many different classifiers. Examples include instance bagging and feature bagging (Dietterich, 2000). The core idea of using diverse inputs in making classification decisions is common in the MDL literature. In fact, the top performing and only successful entry to the 2007 CoNLL shared task on domain adaptation for dependency parsing was a straightforward implementation of ensemble learning by creating variants of parsers (Sagae and Tsujii, 2007). Many MDL algorithms, among them Dredze and Crammer (2008), Daum´e III (2009), Zhang and Yeung (2010) and Saha et al. (2011), all include some notion of learning domain-specific classifiers on the training data, and combining them in the best way possible. To be clear, we do not claim that these approaches can be reduced to an existing ensemble learning algorithm. There are crucial elements 1305 in each of these algorithms that separate them from existing ensemble learning algorithms. One example of such a distinction is the learning of domain relationships by both Zhang and Yeung (2010) and Saha et al. (2011). However, we argue that their core approach, that of combining parameters that are </context>
</contexts>
<marker>Zhang, Yeung, 2010</marker>
<rawString>Yu Zhang and Dit-Yan Yeung. 2010. A Convex Formulation for Learning Task Relationships in Multi-Task Learning. In Proceedings of the Proceedings of the Twenty-Sixth Conference Annual Conference on Uncertainty in Artificial Intelligence (UAI-10).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
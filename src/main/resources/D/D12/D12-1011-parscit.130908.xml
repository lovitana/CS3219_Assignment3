<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.856802">
Linking Named Entities to Any Database
</title>
<author confidence="0.896495">
Avirup Sil*
</author>
<affiliation confidence="0.971549">
Temple University
</affiliation>
<address confidence="0.554296">
Philadelphia, PA
</address>
<email confidence="0.985617">
avi@temple.edu
</email>
<author confidence="0.994247">
Yinfei Yang
</author>
<affiliation confidence="0.983626">
St. Joseph’s University
</affiliation>
<address confidence="0.597872">
Philadelphia, PA
</address>
<email confidence="0.966163">
yangyin7@gmail.com
</email>
<author confidence="0.688981">
Ernest Cronin*
</author>
<affiliation confidence="0.621831">
St. Joseph’s University
</affiliation>
<address confidence="0.416527">
Philadelphia, PA
</address>
<email confidence="0.940397">
ernest.cronin@gmail.com
</email>
<author confidence="0.764075">
Ana-Maria Popescu
</author>
<affiliation confidence="0.652617">
Yahoo! Labs
</affiliation>
<address confidence="0.335199">
Sunnyvale, CA
</address>
<email confidence="0.561846">
amp@yahoo-inc.com
</email>
<author confidence="0.836616">
Penghai Nie
</author>
<affiliation confidence="0.815357">
St. Joseph’s University
</affiliation>
<address confidence="0.475017">
Philadelphia, PA
</address>
<email confidence="0.884478">
nph87903@gmail.com
</email>
<author confidence="0.99411">
Alexander Yates
</author>
<affiliation confidence="0.996891">
Temple University
</affiliation>
<address confidence="0.60784">
Philadelphia, PA
</address>
<email confidence="0.998061">
yates@temple.edu
</email>
<sectionHeader confidence="0.998593" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9996987">
Existing techniques for disambiguating named
entities in text mostly focus on Wikipedia as
a target catalog of entities. Yet for many
types of entities, such as restaurants and
cult movies, relational databases exist that
contain far more extensive information than
Wikipedia. This paper introduces a new task,
called Open-Database Named-Entity Disam-
biguation (Open-DB NED), in which a system
must be able to resolve named entities to sym-
bols in an arbitrary database, without requir-
ing labeled data for each new database. We
introduce two techniques for Open-DB NED,
one based on distant supervision and the other
based on domain adaptation. In experiments
on two domains, one with poor coverage by
Wikipedia and the other with near-perfect cov-
erage, our Open-DB NED strategies outper-
form a state-of-the-art Wikipedia NED system
by over 25% in accuracy.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999961434782609">
Named-entity disambiguation (NED) is the task of
linking names mentioned in text with an established
catalog of entities (Bunescu and Pasca, 2006; Rati-
nov et al., 2011). It is a vital first step for se-
mantic understanding of text, such as in grounded
semantic parsing (Kwiatkowski et al., 2011), as
well as for information retrieval tasks like person
name search (Chen and Martin, 2007; Mann and
Yarowsky, 2003).
NED requires a catalog of symbols, called refer-
ents, to which named-entities will be resolved. Most
NED systems today use Wikipedia as the catalog of
referents, but exclusive focus on Wikipedia as a tar-
get for NED systems has significant drawbacks: de-
spite its breadth, Wikipedia still does not contain all
or even most real-world entities mentioned in text.
As one example, it has poor coverage of entities that
are mostly important in a small geographical region,
such as hotels and restaurants, which are widely dis-
cussed on the Web. 57% of the named-entities in
the Text Analysis Conference’s (TAC) 2009 entity
linking task refer to an entity that does not appear
in Wikipedia (McNamee et al., 2009). Wikipedia is
clearly a highly valuable resource, but it should not
be thought of as the only one.
Instead of relying solely on Wikipedia, we pro-
pose a novel approach to NED, which we refer to
as Open-DB NED: the task is to resolve an en-
tity to Wikipedia or to any relational database that
meets mild conditions about the format of the data,
described below. Leveraging structured, relational
data should allow systems to achieve strong accu-
racy, as with domain-specific or database-specific
NED techniques like Hoffart et al.’s NED system
for YAGO (Hoffart et al., 2011). And because of
the availability of huge numbers of databases on
the Web, many for specialized domains, a success-
ful system for this task will cover entities that a
Wikipedia NED or database-specific system cannot.
We investigate two complementary learning
strategies for Open-DB NED, both of which signifi-
cantly relax the assumptions of traditional NED sys-
tems. The first strategy, a distant supervision ap-
proach, uses the relational information in a given
database and a large corpus of unlabeled text to
learn a database-specific model. The second strat-
</bodyText>
<page confidence="0.984627">
116
</page>
<note confidence="0.7952765">
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 116–127, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.999871833333333">
egy, a domain adaptation approach, assumes a sin-
gle source database that has accompanying labeled
data. Classifiers in this setting must learn a model
that transfers from the source database to any new
database, without requiring new training data for the
new database. Experiments show that both strategies
outperform a state-of-the-art Wikipedia NED sys-
tem by wide margins without requiring any labeled
data from the test domain, highlighting the signifi-
cant advantage of having domain-specific relational
data.
The next section contrasts Open-DB NED with
previous work. Section 3 formalizes the task. Sec-
tions 4 and 5 present our distant supervision strategy
and domain-adaptation strategy, respectively. Sec-
tion 6 introduces a technique that is a hybrid of the
two learning strategies. Section 7 describes our ex-
periments, and Section 8 concludes.
</bodyText>
<sectionHeader confidence="0.998946" genericHeader="introduction">
2 Previous Work
</sectionHeader>
<bodyText confidence="0.999842964912281">
As mentioned above, restricting the catalog of ref-
erents to Wikipedia, as most recent NED systems
do (Bunescu and Pasca, 2006; Mihalcea and Cso-
mai, 2007; Fader et al., 2009; Han and Zhao, 2009;
Kulkarni et al., 2009; Ratinov et al., 2011), can re-
strict the coverage of the system. Zhou et al. (2010)
estimate that 23% of names in Yahoo! news arti-
cles have no referent in Wikipedia, and Cucerzan
(2007) estimates the rate at 16% in MSNBC news
articles. There is reason to suspect that these esti-
mates are on the low side, however, as news tends to
cover popular entities, which are most likely to ap-
pear in Wikipedia; the mentions in TAC’s 2009 en-
tity linking task are drawn from both newswire and
blogs, and have a far higher rate (57%) of missing
Wikipedia entries. Lin et al. (2012) find that 33% of
mentions in a corpus of 500 million Web documents
cannot be linked to Wikipedia.
NED systems that are focused on specific do-
mains (or verticals) greatly benefit from reposito-
ries of domain-specific knowledge, only a subset
of which may be found in Wikipedia. For exam-
ple, Pantel and Fuxman (2011) use a query-click
graph to resolve names in search engine queries to a
large product catalog from a commercial search en-
gine, while Dalvi et al. (2009; 2012) focus on movie
and restaurant databases. Bellare and McCallum
(2009) use the sequence information available in ci-
tation text to link author, title, and venue names to a
publication database. Open-DB NED systems work
on any database, so they can serve as baselines for
domain-specific NED tasks, as well as provide dis-
ambiguation for domains where no domain-specific
NED system exists.
Numerous previous studies have considered dis-
tant or weak supervision from a single relational
database as an alternative to manual supervision for
information extraction (Hoffmann et al., 2011; Weld
et al., 2009; Bellare and McCallum, 2007; Bunescu
and Mooney, 2007; Mintz et al., 2009; Riedel et al.,
2010; Yao et al., 2010). In contrast to these sys-
tems, our distant supervision NED system provides
a meta-algorithm for generating an NED system for
any database and any entity type.
Existing domain adaptation or transfer learning
approaches are inappropriate for the Open-DB NED
task, either because they require labeled data in both
the source and target domains (Daum´e III et al.,
2010; Ben-David et al., 2010), or because they lever-
age some notion of distributional similarity between
words in the source and target domains (Blitzer et
al., 2006; Huang and Yates, 2009), which does not
apply to the database symbols across the two do-
mains. Instead, our domain adaptation technique
uses domain-independent features of relational data,
which apply regardless of the actual contents of the
database, as explained further below.
</bodyText>
<sectionHeader confidence="0.9991015" genericHeader="method">
3 The Open-DB NED Problem and
Assumptions
</sectionHeader>
<subsectionHeader confidence="0.999909">
3.1 Problem Formulation
</subsectionHeader>
<bodyText confidence="0.999716555555555">
A mention is an occurrence of a named-entity
in a document. Formally, a mention m =
(d, start, end) is a triple consisting of a document
d, as well as a start and end position for the men-
tion within the document. We say that d is the
context of m. A relational database is a 2-tuple
(5, R). Here, 5 is a set of symbols for constants,
attributes, and relations in the database, and R =
{r1, ... , rnI is a set of relation instances of the form
</bodyText>
<equation confidence="0.808384">
ri = {(c1,1, ... , c1,ki), ... , (cni,1, ... , cni,ki)I,
</equation>
<bodyText confidence="0.998613666666667">
where each cj is taken from 5, ki is the arity of re-
lation ri and ni is the number of known instances
of ri. We will write example database symbols in
</bodyText>
<page confidence="0.982934">
117
</page>
<figure confidence="0.996520714285714">
id
1
2
3
4
5
...
</figure>
<bodyText confidence="0.990327714285714">
Normal Form (BCNF) (Silberschatz et al., 2010).
A relational schema is said to be in BCNF when
all redundancy based on functional dependency has
been removed, although other types of redundancy
may still exist. Formally, a schema R is said to
be in BCNF with respect to a set of functional de-
pendencies F if for every one of the dependencies
</bodyText>
<figure confidence="0.991500247524753">
(X —* Y ) E F, either
1. Y C X, meaning this is a trivial functional de-
pendency, or
id
...
4 Chris Johnson
2
3 Chris Johnson
5 Chris Johnson
1
Rob Bironas
Carlos Lee
name
...
player
height position
5’11”
6’2”
6’0”
6’3”
6’1”
...
3B
DB
RB
LF
K
id
...
4
2
3
5
1
San Diego Padres
Tennessee Titans
Oakland Raiders
Houston Texans
Houston Astros
team
name
...
player_id team_id
...
4
5
3
2
1
plays_for
...
3
2
5
5
3
movie
actor
acted_in
title
year
id
name
movie_id actor_id
role
Next Door 1975
1
Nicole Kreux
5
1
Evelyn
Next Door 2005
2
Richard Ryan
5
2
Bruce
Next Door 2008
3 Kristoffer Joner
2
3
John
Next Door 2008
4
Lee Perkins
1
4
Kid
Next Door 2010
5 Carla Valentine
3
5
Elana
...
...
...
...
...
...
...
2. X is a superkey, meaning that X is a set of at-
</figure>
<figureCaption confidence="0.999968">
Figure 1: Example movie database (above) and sports tributes that together define a unique ID for the
</figureCaption>
<bodyText confidence="0.997771111111111">
database (below) in BCNF. relation.
teletype, and mentions in “quotations.” For a
particular database DB, we refer to its components
as DB.5 and DB.R. For a set of databases D, de-
fine the set of referents as 5D = (UDBED DB.5) U
{OOD}, where OOD is a special symbol indicat-
ing something that is “out of database”, or not found
in any of the databases in D.
Given a corpus C, a set of mentions M that oc-
cur in C, and a set of databases D, the Open-DB
NED task is to produce a function f : M —* 5D,
which identifies an appropriate target symbol from
one of the databases in D, or determines that the
mention is OOD. Note that this problem formula-
tion assumes no labeled data. This is significantly
more challenging than traditional NED settings, but
allows the system to generalize easily to any new
database. In the domain adaptation section below,
we relax this condition somewhat, to allow labeled
data for a small number of initial databases; the sys-
tem must then transfer what it learns from the la-
beled domains to any new database. Also note that
the focus for this paper is disambiguation; we as-
sume that the set of mentions are correctly demar-
cated in the input text. Previous systems, such as
Lex (Downey et al., 2007), have investigated the task
of finding correct named-entity boundaries in text.
</bodyText>
<subsectionHeader confidence="0.999582">
3.2 Assumptions
</subsectionHeader>
<bodyText confidence="0.999986233333333">
To allow our systems to handle arbitrary databases,
we need to make some assumptions about a standard
format for the data. We will assume that databases
are provided in a particular form, called Boyce-Codd
In practice, this is a relatively safe assumption as
database designers often aim for even stricter normal
forms. For databases not in BCNF, such as tables
extracted from Web pages, standard algorithms ex-
ist for converting them into BCNF, given appropri-
ate functional dependencies, although there are sets
of functional dependencies for which BCNF is not
achievable. Figure 1 shows two example databases
in BCNF. We use these tables as examples through-
out the paper.
We will additionally assume that all attributes, in-
cluding names and nicknames, of entities that are
covered by the database are treated as functional de-
pendencies of the entity. Again, in practice, this
is a fairly safe assumption as this is part of good
database design, but if a database does not con-
form to this, then there will be some entities in the
database that our algorithms cannot resolve to. This
assumption implies that it is enough to use the set of
superkeys for relations as the set of possible refer-
ents; our algorithms make use of this fact.
Finally, we will assume the existence of a func-
tion µ(s, t) which indicates whether the text t is a
valid surface form of database symbol s. Our exper-
iments in Section 7.3 explore several possible simple
definitions for this function.
</bodyText>
<sectionHeader confidence="0.996983" genericHeader="method">
4 A Distant Supervision Strategy for
Open-DB NED
</sectionHeader>
<bodyText confidence="0.999971">
Our first approach to the Open-DB NED problem re-
lies on the fact that, while many mentions are indeed
ambiguous and difficult to resolve correctly, most
</bodyText>
<page confidence="0.992839">
118
</page>
<bodyText confidence="0.9999625">
mentions have only a very small number of possi-
ble referents in a given database. “Chris Johnson”
is the name of doubtless thousands of people, but
for articles that are reasonably well-aligned with our
sports database, most of the time the name will refer
to just three different people. Most sports names are
in fact less ambiguous still. Thus, taking a corpus of
unlabeled sports articles, we use the information in
the database to provide (uncertain) labels, and then
train a log-linear model from this probabilistically-
labeled data.
This strategy requires a set of features for the
model. Traditionally, such features would be hand-
crafted for a particular domain and database. As a
first step towards our Open-DB system, we present
a log-linear model for disambiguation, as well as a
simple feature-generation algorithm that produces a
large set of useful features from a BCNF database.
We then present a distant-supervision learning pro-
cedure for this model.
</bodyText>
<subsectionHeader confidence="0.951473">
4.1 Disambiguation Model
</subsectionHeader>
<bodyText confidence="0.999965714285714">
Let 5D be the set of possible referents. We construct
a vector of feature functions f(m, s) describing the
degree to which m and s ∈ 5D appear to match
one another. The feature functions are described be-
low. The model includes a vector of weights w, one
weight per feature function, and sets the probability
of entity s given m and w as:
</bodyText>
<equation confidence="0.7604405">
P s m, w = exp (w · f(m, s)) (1)
(I ) Es,∈SD exp (w · f(m, s0))
</equation>
<subsectionHeader confidence="0.996436">
4.2 Database-driven Feature Generation
</subsectionHeader>
<bodyText confidence="0.999652461538462">
Figure 2 shows our algorithm for automatically gen-
erating feature functions fi(m, s) from a BCNF
database. As mentioned above, we only need to con-
sider resolving to database symbols s that are keys,
or unique IDs, for some tuple in a database. For
an entity in the database with key id, the feature
generation algorithm generates two types of feature
functions: attribute counts and similar entity counts.
Each of these features measures the similarity be-
tween the information stored in the database about
the entity id, and the information in the text in d sur-
rounding mention m.
An attribute count feature function fatt
</bodyText>
<equation confidence="0.304834">
i,j (m, id)
</equation>
<bodyText confidence="0.990131">
for the jth attribute of relation ri counts how many
</bodyText>
<sectionHeader confidence="0.63627" genericHeader="method">
Algorithm: Feature Generation
</sectionHeader>
<bodyText confidence="0.860022">
Input: DB, a database in BCNF
Output: F, a set of feature functions
Initialization: F ← ∅
</bodyText>
<subsectionHeader confidence="0.462787">
Attribute Count Feature Functions:
</subsectionHeader>
<bodyText confidence="0.388183333333333">
For each relation ri ∈ DB.R
For each j in {1,... , ki}
Define function fatt
</bodyText>
<equation confidence="0.974227259259259">
i,j (m, id):
count ← 0
Identify the tuple t ∈ ri containing id
val ← tj
count ← count +
ContextMatches(val, m)
return count
F ← F ∪ {fatt
i,j }
Similar-Entity Count Feature Functions:
For each relation ri ∈ DB.R
For each j in {1,... , ki}
Define function fsim
i,j (m, id):
count ← 0
Identify the tuple t ∈ ri containing id
val ← tj
Identify the set of similar tuples T0:
T0 = {t0|t0 ∈ ri, t0j = val}
For each tuple t0 ∈ T0
For each j0 ∈ {1, ... , ki}
val0 ← t0j
count ← count +
ContextMatches(val0, m)
return count
F ← F ∪ {fsim
i,j }
</equation>
<figureCaption confidence="0.99305">
Figure 2: Feature generation algorithm. The
</figureCaption>
<bodyText confidence="0.99765625">
ContextMatches(s, m) function counts how many
times a string that matches database symbol s appears
in the context of m. In our implementation, we use all
of d(m) as the context. Matching between strings and
database symbols is discussed in Sec. 7.3.
attributes of the entity id appear near m. For exam-
ple, if id is 5 in the movie relation in Figure 1, the
feature function for attribute year would count how
often 2010 matches the text surrounding mention
m. Defining precisely whether a database symbol
“matches” a word or phrase is a subtle issue; we ex-
plore several possibilities in Section 7.3. In addition
</bodyText>
<page confidence="0.995951">
119
</page>
<bodyText confidence="0.99710409375">
to attribute counts for attributes within a single rela-
tion, we also use attributes from relations that have
been inner-joined on primary key and foreign key
pairs. For example, for movies, we include attributes
such as director name, genre, and actor name. High
values for these attribute count features indicate that
the text around m closely matches the information
in the database about entity id, and therefore id is a
strong candidate for the referent of m. We use the
whole document as the context for finding matches,
although other variants are worth future investiga-
tion.
A similar entity count feature function
f�i�
i,� (m, id) for the jth attribute in relation ri
counts how many entities similar to id are men-
tioned in the neighborhood of m. As an example,
consider a mention of “Chris Johnson”, id = 3,
and the similar entity feature for the position
attribute of the players relation in the sports
database. The feature function would first identify
that 3B is the position of the player with id = 3. It
would then identify all players that had the same
position. Finally, it would count how often any
attributes of this set of players appear near “Chris
Johnson”. Likewise, the similar entity feature for
the team id attribute would count how many
teammates of the player with id = 3 appear near
“Chris Johnson”. A high count for this teammate
feature is a strong clue that id is the correct referent
for m, while a high count for players of the same
position is a weak but still valuable clue.
</bodyText>
<subsectionHeader confidence="0.7002435">
4.3 Parameter Estimation via Distant
Supervision
</subsectionHeader>
<bodyText confidence="0.9998335">
Using string similarity, we can heuristically deter-
mine that three IDs with name attribute Chris
Johnson are highly likely to be the correct target
for a mention of “Chris Johnson”. Our distant su-
pervision parameter estimation strategy is to move
as much probability mass as possible onto the set
of realistic referents obtained via string similarity.
Since our features rely on finding attributes and sim-
ilar entities, the side effect of this strategy is that
most of the probability mass for a particular mention
is moved onto the one target ID with high attribute
count and similar entity count features, thus disam-
biguating the entity. Although the string-similarity
heuristic is typically noisy, the strong information in
the database and the fact that many entity mentions
are typically not ambiguous allows the technique to
learn effectively from unlabeled text.
Let 0(m, DB) be a heuristic string-matching
function that returns a set of plausible ID values in
database DB for mention m. The objective function
for this training procedure is a modified marginal log
likelihood (MLL) function that encourages probabil-
ity mass to be placed on the heuristically-matched
targets:
</bodyText>
<equation confidence="0.9912565">
MLL(M, w) = � log � P(id|m, w)
MEM idEO(M,DB)
</equation>
<bodyText confidence="0.996384">
This objective is smooth but non-convex. We use
a gradient-based optimization procedure that finds a
local maximum. Our implementation uses an open-
source version of the LBFG-S optimization tech-
nique (Liu and Nocedal, 1989). The gradient of our
objective is given by
</bodyText>
<equation confidence="0.955742">
EidEO(M,DB) Ifi(m, id)]
− EidEDB.S Ifi(m, id)]
</equation>
<bodyText confidence="0.948347">
where the expectations are taken according to
P(id|m, w).
</bodyText>
<sectionHeader confidence="0.9989935" genericHeader="method">
5 A Domain-Adaptation Strategy for
Open-DB NED
</sectionHeader>
<bodyText confidence="0.999977277777778">
Our domain-adaptation strategy builds an Open-DB
NED system by training it on labeled examples from
an initial database or small set of initial databases.
Unlike traditional NED, however, the purpose in
Open-DB NED is to resolve to any database. Thus
the strategy must take care to build a model that
can transfer what it has learned to a new database,
without requiring additional labeled data for the new
database.
At first, the problem seems intractable — just
because a system can disambiguate between “Next
Door”, the 2005 Norwegian film, and “Next Door”,
the 1975 short film by director Andrew Silver, that
seems to provide little benefit for disambiguating be-
tween different athletes named “Andre Smith.” The
crux of the problem lies in the fact that database-
driven features are domain-specific. Counting how
many times the director of a movie appears is highly
</bodyText>
<figure confidence="0.575089">
aLL(M, w) �=
awi MEM
</figure>
<page confidence="0.947301">
120
</page>
<bodyText confidence="0.992164291666667">
useful in the movie domain, but worthless in the
sports domain.
Our solution works by re-defining the problem in
such a way that we can define domain-independent
and database-independent features. For example,
rather than counting how often the director of
a movie appears in the context around a movie
mention, we create a domain-independent Count
Att(m, s) feature function that counts how often any
attribute of s appears in the context of m. For
movies, Count Att will add together counts for ap-
pearances of a movie’s production year and IMDB
rating, among other attributes. In the sports domain,
Count Att will add together counts for appearances
of a player’s height, position, salary, etc.. But in ei-
ther domain, the feature is well-defined, and in either
domain, larger values of the feature indicate a better
match between m and s. Thus there is a hope for
training a model with domain-independent features
like Count Att on labeled data from one domain, say
movies, and producing a model that has high accu-
racy on the sports domain.
We first formalize the notion of a domain adap-
tation NED model, and then describe our algorithm
for producing such a model. We say that a domain
consists of a database DB as well as a distribution
D(M), where M is the space of mentions. For in-
stance, the movie domain might consist of the Inter-
net Movie Database (IMDB) and a distribution that
places most probability mass on documents about
movies and Hollywood stars. In domain adapta-
tion, a system observes a set of training examples
(m, s, g(m, s)), where instances m E M are drawn
from a source domain’s distribution DS and refer-
ents s are drawn from the source domain’s database
DBS. The labels g(m, s) are boolean values in-
dicating a correct or incorrect match between the
mention and referent. The system must then learn
a hypothesis for classifying examples (m, s) drawn
from a target domain’s distribution DT and database
DBT. Note that for domain adaptation, we can-
not use the more traditional problem formulation in
which the referent s is a label (i.e., s = g(m)) for the
mention, since the set of possible referents changes
from domain to domain, and therefore the output of
g would be completely different from one domain to
the next.
Table 1 lists the domain-independent features
</bodyText>
<figure confidence="0.952661583333334">
Domain-Independent Feature Functions
Count Att: Ei,j fatt
i,j (m, s)
Count Sim: Ei,j fsim
i,j (m, s)
Count All: Count Att + Count Sim
�
0 if f tt(m, s) = 0,
Count Unique: E i,j 1 if fatt
i,j (m, s) &gt; 0.
Count Num: Ei,j|jis a numeric att. fatt
i,j (m, s)
</figure>
<tableCaption confidence="0.922755857142857">
Table 1: Primary feature functions for a domain adapta-
tion approach to NED. These features made the biggest
difference in our experiments, but we also tested varia-
tions such as counting unique numeric attribute appear-
ances, counting unique similar entities, counting relation
name appearances, counting extended attributed appear-
ances, and others.
</tableCaption>
<bodyText confidence="0.999946548387097">
used in our domain adaptation model. These fea-
tures use the attribute counts and similar entity
counts from the distant supervision model as subrou-
tines. By aggregating over those domain-dependent
feature functions, the domain adaptation system ar-
rives at feature functions that can be defined for any
database, rather than for a specific database.
Note that there is a tradeoff between the do-
main adaptation technique and the distant super-
vision technique. The domain adaptation model
has access to labeled data, unlike the distant su-
pervision model. In addition, the domain adapta-
tion model requires no text whatsoever from the tar-
get domain, not even an unlabeled corpus, to set
weights for the target domain. Once trained, it is
ready for NED over any database that meets our as-
sumptions, out of the box. However, because the
model needs to be able to transfer to arbitrary new
domains, the domain adaptation model is restricted
to domain-independent features, which are “coarser-
grained.” That is, the distant supervision model has
the ability to place more weight on attributes like
director rather than genre, or team rather than po-
sition, if those attributes are more discriminative.
The domain adaptation model cannot place differ-
ent weights on the different attributes, since those
weights would not transfer across databases.
As with distant supervision, the domain adapta-
tion strategy uses a log-linear model over these fea-
ture functions. We use standard techniques for train-
ing the model using labeled data from the source do-
</bodyText>
<page confidence="0.996208">
121
</page>
<bodyText confidence="0.996194">
main: conditional log likelihood (CLL) as the objec-
tive function, and LBFG-S for convex optimization.
</bodyText>
<equation confidence="0.9912655">
CLL(L, w) = � log P(label|m, id, w)
(m,id,label)EL
</equation>
<bodyText confidence="0.999928333333333">
The training algorithm is guaranteed to converge to
the globally optimal parameter setting for this objec-
tive function over the training data. The manually
annotated data contains only positive examples; to
generate negative examples, we use the same name-
matching heuristic 0(m, DB) to identify a set of po-
tentially confusing bad matches. On test data, we
use the trained model to choose the id for a given m
with the highest probability of being correct.
</bodyText>
<sectionHeader confidence="0.99887" genericHeader="method">
6 A Hybrid Model
</sectionHeader>
<bodyText confidence="0.98055252">
The distant supervision and domain adaptation
strategies use two very different sources of evidence
for training a disambiguation classifier: the string-
matching heuristic and unlabeled text from the target
domain for the the distant supervision model, and
aggregate features over labeled text from a separate
domain for domain adaptation. This begs the ques-
tion, do these sources of evidence complement one
another? To address this question, we design a Hy-
brid model with features and training strategies from
both distant supervision and domain adaptation.
The training data consists of a set LS of labeled
mentions from a source domain, a source database
DBS, a set of unlabeled mentions MT from the tar-
get domain, and the target-domain database DBT.
The full feature set of the Hybrid model is the union
of the distant supervision feature functions for the
target domain and the domain-independent domain
adaptation feature functions. Note that the distant
supervision feature functions are domain-specific,
so they almost always will be uniformly zero on LS,
but the domain adaptation feature functions will be
activated on both LS and MT. The combined train-
ing objective for the Hybrid model is:
LL(LS, MT, w) = CLL(LS, w) + MLL(MT, w)
</bodyText>
<sectionHeader confidence="0.99928" genericHeader="evaluation">
7 Experiments
</sectionHeader>
<bodyText confidence="0.9998195">
Our experiments compare our strategies for Open-
DB NED against one another, as well as against a
Wikipedia NED system from previous work, on two
domains: sports and movies.
</bodyText>
<subsectionHeader confidence="0.989554">
7.1 Data
</subsectionHeader>
<bodyText confidence="0.999951695652174">
For the movie domain, we collected a set of
156 cult movie titles from an online movie site
(www.olivefilms.com). For each movie title, we ex-
ecuted a Web search using a commercial search en-
gine, and collected the top five documents for each
title from the search engine’s results. Nearly all top-
five results included at least one mention of an en-
tity not found in Wikipedia; overall, only 16% of the
mentions could be linked to Wikipedia. After strip-
ping javascript and html annotations, we removed
documents with fewer than 50 words, leaving a to-
tal of 770 documents. We select one occurrence of
any of the 156 movie titles from each document as
our set of mentions. Many titles are ambiguous not
just among different movies with the same name, but
also among novels, plays, geographical entities, and
assorted other types of entities. To provide labels for
these mentions, we use both a movie database and
Wikipedia. We downloaded the complete data dump
from the online Internet Movie Database (IMDB,
www.imdb.com). For our set of possible referents,
we use the set of all key values in IMDB, and the set
of all Wikipedia articles. Annotators manually la-
beled each mention using this set of referents. Table
2 shows summary statistics about this labeled data.
For the sports domain, we downloaded all player
data from Yahoo!, Inc.’s sports database for the
years 2011-2012 and two American sports leagues,
the National Football League (NFL) and Major
League Baseball (MLB). From the database, we ex-
tracted ambiguous player names and team names,
including names like “Philadelphia” which may re-
fer to Philadelphia Eagles in the NFL data,
Philadelphia Phillies in the MLB data, or
the city of Philadelphia itself (in both types of
data). We then collected 1300 Yahoo! news arti-
cles which include a mention that partially matches
at least one of these database symbols. We manu-
ally labeled a random sample of 564 mentions from
this data, including 279 player name mentions and
285 city name mentions. Many player name and
place name mentions are ambiguous between the
two sports leagues, as well as with teams or play-
ers from other leagues. In order to focus on the
hardest cases, we specifically exclude mentions like
“Philadelphia” from the labeled data if any of their
</bodyText>
<page confidence="0.990689">
122
</page>
<table confidence="0.985077333333333">
domain IMI E|0(m,DB) |OOD Wiki
movies 770 2.6 13% 16%
sports 549 4.5 0% 100%
</table>
<tableCaption confidence="0.969397">
Table 2: Number of mentions, average number of refer-
ents per mention, % of mentions that are OOD, and %
of mentions that are in Wikipedia in our movie and sports
data.
</tableCaption>
<table confidence="0.992709">
System Accuracy
No-Wikipedia Domain Adapt. 0.61
DocSim-Wikipedia Domain Adapt. 0.69
</table>
<tableCaption confidence="0.949281">
Table 3: Including a simple document-similarity feature
for comparing a mention’s context with a Wikipedia page
provides an 8% improvement over ignoring Wikipedia in-
formation.
</tableCaption>
<bodyText confidence="0.999626">
unambiguous completions appears in the same arti-
cle (that is, if either of the team names “Philadelphia
Eagles” or “Philadelphia Phillies” appears in the
same article, we exclude the “Philadelphia” men-
tion). As before, the set of possible referents in-
cludes the symbol OOD, key values from the sports
database, and Wikipedia articles, and a given men-
tion may be labeled with both a sports entity and a
Wikipedia article, if appropriate. All of our data is
available from the last author’s website.
</bodyText>
<subsectionHeader confidence="0.997227">
7.2 Evaluation Metric
</subsectionHeader>
<bodyText confidence="0.999995333333333">
We report on a version of exact-match accuracy. The
system chooses the most likely label s� for each m.
This is judged correct if s� matches the correct label
s exactly, or (in cases where both a Wikipedia and a
database entity are considered correct) if one of the
labels matches s� exactly. This metric allows systems
to resolve against either reference, Wikipedia or an-
other database, without requiring it to match both if
the same entity appears in both references.
</bodyText>
<subsectionHeader confidence="0.995918">
7.3 Exact or Partial Matching?
</subsectionHeader>
<bodyText confidence="0.999612">
One important question in the design of our systems
is how to determine the “match” between database
symbols and text. This question comes into play in
two components of our systems: it affects the com-
putation of feature functions that count how often a
match of some attribute is found in text, and it af-
fects which set of heuristically-determined database
entities are considered to be possible matches for a
given mention.
We experiment with two different matching
strategies between a symbol s and text t, exact
matching and partial matching. Exact matching
µexact(s, t) requires the sequence of characters in s
to appear exactly (modulo character encoding) in t.
For instance, the database value Chris Johnson
would match “Chris Johnson”, but not “C. John-
son” or “Johnson” in text. For partial matching,
we used different tests for numeric and textual en-
tities. For numeric entities, µpartial matched s and
t if the numeric value of one was within 10% of
the other, so that 5312 would match “5,000.” We
made no attempt to convert numeric phrases, such
as “3.6 million”, into numeric values. For textual
entities, µpartial matched s and t if at least one
token from each matched exactly. Thus Chris
Johnson matches both “Chris” and “C. Johnson”.
We found µpartial to be consistently superior for
computing 0(m, DB), since it has much better re-
call for mentions like “Philadelphia”. On the other
hand, if we use µpartial for computing our models’
feature functions, like the Count Att(m, s) in the do-
main adaptation model, counts varied widely across
domains. A simple version of the domain adapta-
tion classifier (only the Count All and Count Unique
features) trained on sports data and tested on movies
achieved an accuracy of 24% using µpartial, com-
pared with 61% using µexact. For all remaining
tests, we used µexact for computing features, and
µpartial for computing 0(m, DB).
</bodyText>
<subsectionHeader confidence="0.998445">
7.4 Incorporating Wikipedia referents
</subsectionHeader>
<bodyText confidence="0.99993925">
Thus far, all of our features work on relational data,
not Wikipedia. In order to allow our systems to link
to Wikipedia, we create a single “document simi-
larity” feature describing the similarity between the
text around a mention and the text appearing on a
Wikipedia page. We build a vector space model of
both the document containing the mention and the
Wikipedia page, remove stopwords, and use cosine
similarity to compute this feature.
To evaluate the effectiveness of this Wikipedia
feature, we tested two versions of our domain adap-
tation system, both trained on sports data and tested
</bodyText>
<page confidence="0.998637">
123
</page>
<figureCaption confidence="0.976342">
Figure 3: All three Open-DB NED strategies out-
</figureCaption>
<bodyText confidence="0.8621277">
perform a state-of-the-art Wikipedia NED system by
25% or more on sports and movies, and outperform
a Wikipedia NED system with oracle information by
14% or more on the movie data. Differences between
the Modified Zhou Wikifier and the Open-DB strategies
are statistically significant (p &lt; 0.01, Fisher’s exact test)
on both domains.
on the movies domain. The first version involves
no Wikipedia information whatsoever, thus it has no
reason to select a Wikipedia article over OOD. The
second system includes the document similarity fea-
ture. Table 3 shows the results of these systems. En-
couragingly, our single document similarity feature
produces a significant improvement over the model
without Wikipedia information, so we use this fea-
ture in all of our systems tested below. More so-
phisticated use of Wikipedia is certainly possible,
and an important question for future work is how
to combine Open-DB NED more seamlessly with
Wikipedia NED.
</bodyText>
<subsectionHeader confidence="0.998253">
7.5 Comparing Open-DB NED Strategies
</subsectionHeader>
<bodyText confidence="0.992375933333334">
For each domain, we compare our domain-
adaptation strategy, distant supervision, and hy-
brid strategies. The domain-adaptation model is
trained on the labeled data for sports when testing
on movies, and vice versa. We use a movies test set
of 180 mentions that is separate from the develop-
ment data used for the above tests. For the distant
supervision strategy, we use the entire collection of
texts from each domain as input (1300 articles for
sports, 770 articles for movies), with the labels re-
moved during training.
We compare against a state-of-the-art Wikipedia
NED system used in production by a major Web
company. This system is a modified version of the
system described by Zhou et al. (2010), where cer-
tain features have been removed for efficiency. We
refer to this as the Modified-Zhou Wikifier. This
system uses a gradient-boosted decision tree and
multiple local and global features for computing
the similarity between a mention’s context and a
Wikipedia article. We also test a hypothetical sys-
tem, Oracle Wikifier, which is given no information
about entities in IMDB, but is assumed to be able
to correctly resolve any mention that refers to an
entity found in Wikipedia. Thus, this system has
perfect accuracy on mentions that can be found in
Wikipedia, and accuracy similar to a baseline that
predicts randomly on all mentions that fall outside
of Wikipedia1. Oracle-Wikifier serves as an upper
bound on systems that have no access to a domain-
specific database. In addition, we compare against
two standard baselines: a classifier that always pre-
dicts OOD, and a classifier that chooses randomly.
Finally, we compare against a system that trains the
domain adaptation model using distant supervision
(“DA Trained with DS”).
Figure 3 shows our results. All three Open-DB
approaches outperform the baseline techniques on
this test by wide margins, with the Hybrid model in-
creasing by 30% or more over the random baseline.
On the movie domain, the Hybrid model outper-
forms the Oracle Wikifier by nearly 20%. Encour-
agingly, the Hybrid model consistently outperforms
both distant supervision and domain adaptation, sug-
gesting that the two sources of evidence are partially
complementary. Distant supervision performs better
on the movies test, whereas domain adaptation has
the advantage on sports. The differences among all
three Open-DB approaches is relatively small, com-
pared with the difference between these approaches
and Oracle Wikifier on the movie data.
The domain adaptation system outperforms DA
Trained with DS on both domains, suggesting
that labeled data from a separate domain is bet-
ter evidence for parameter estimates than unlabeled
data from the same domain. The distant super-
vision system also outperforms DA Trained with
1Alternatively, one could make the oracle system predict
OOD on all mentions that fall outside of Wikipedia. Random
predictions perform better on our data.
</bodyText>
<figure confidence="0.999096703703703">
Open-DB NED Test
1
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
0.13
0
Movies Sports
0.4 0.4
0.21
0.33
0.54
.6
0.54
.71 0.72 0.7
0.63
0.62
0.66
Accuracy
</figure>
<page confidence="0.992414">
124
</page>
<bodyText confidence="0.992918319444444">
DS on both domains, suggesting that the fine-
grained, domain-specific features do in fact provide
more helpful information than the coarser-grained,
domain-independent features of the domain adapta-
tion model.
All of the Open-DB NED systems outperform the
Modified Zhou Wikifier on both data sets by a wide
margin. In fact the Modified Zhou Wikifier has sim-
ilar results on both domains, despite the fact that
Wikipedia has far greater coverage on sports than
movies. In part, the poor performance of the Modi-
fied Zhou Wikifier reflects the difficult nature of the
task. In previous experiments on an MSNBC news
test set it reached 85% accuracy, but a random clas-
sifier there achieved 60% accuracy compared with
21% on our sports data. Another difficulty with
the Modified Zhou Wikifier is its strong preference
for globally common entities. It consistently clas-
sifies mentions that are ambiguous between a city
and a team (like “Chicago” in “Chicago sweeps the
Red Sox”) as cities when they should be resolved
to teams, in large part because Chicago is a more
common referent in general text than either of the
baseball teams that play in that city. In sports arti-
cles, however, both meanings are common, and only
the surrounding context can help determine the cor-
rect referent.
Besides wikifiers, NED systems may also be
compared with dictionary-based word sense disam-
biguation techniques like the Lesk algorithm2 (Lesk,
1986). The Lesk algorithm is “open” in the sense
that it works for arbitrary dictionaries, and it defines
a vector space model of the dictionary definitions
that may be likened to the attribute-value model in
our representation of entities in the database. Our
approach, however, estimates parameters for a sta-
tistical model from data, whereas the Lesk algorithm
uses an equal weight for all attributes. To make an
empirical comparison, we created a variant of the
Lesk algorithm for relational data: we took the dis-
ambiguation model from Eqn. 1, supplied all of
the features from the distant supervision model, and
manually set w = 1. This “relational Lesk” model
achieves an accuracy of 0.11 on movies, and 0.15
on sports, significantly below the random baseline.
Giving equal weight to noisy attributes like genre
2We thank the reviewers for making this connection.
and more discriminative attributes like director
significantly hurts the performance.
For both the movie and sports domain, approx-
imately 80% of the Hybrid model’s errors are be-
cause of predicting database symbols, when the cor-
rect referent is a Wikipedia page or OOD. This
nearly always occurs because some words in the
context of a mention match an attribute of an in-
correct database referent. For instance, the crime
genre is an attribute for several movies, but it also
matches in contexts surrounding book titles and nu-
merous other entities. In the movie domain, most of
the remaining errors are incorrect OOD predictions
for mentions that should resolve to the database, but
the article contains no attributes or similar entities
to the database entity. In the sports domain, many
of the remaining errors were due to predicting in-
correct player referents. Quite often, this was be-
cause the document discusses a fantasy sports league
or team, where players from different professional
sports teams are mixed together on a “fantasy team”
belonging to a fan of the sport. Since players in the
fantasy leagues have different teammates than they
do in the database, these articles consistently con-
fuse our methods.
</bodyText>
<sectionHeader confidence="0.993808" genericHeader="conclusions">
8 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.99991725">
This paper introduces the task of Open-DB Named
Entity Disambiguation, and presents two distinct
strategies for solving this task. Experiments indicate
that a mixture of the two strategies significantly out-
performs a state-of-the-art Wikipedia NED system,
on a dataset where Wikipedia has good coverage and
on another dataset where Wikipedia has poor cover-
age. The results indicate that there is a significant
benefit to leveraging other sources of knowledge in
addition to Wikipedia, and that it is possible to lever-
age this knowledge without requiring labeled data
for each new source. The initial success of these
Open-DB NED approaches indicates that this task is
a promising area for future research, including ex-
citing extensions that link large numbers of domain-
specific databases to text.
</bodyText>
<sectionHeader confidence="0.998903" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.911686">
This work was supported in part by a gift from Ya-
hoo!, Inc.
</bodyText>
<page confidence="0.998266">
125
</page>
<sectionHeader confidence="0.998322" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999622638095238">
Kedar Bellare and Andrew McCallum. 2007. Learn-
ing extractors from unlabeled text using relevant data-
bases. In Sixth International Workshop on Information
Integration on the Web.
Kedar Bellare and Andrew McCallum. 2009. General-
ized Expectation Criteria for Bootstrapping Extractors
using Record-Text Alignment. In Empirical Methods
in Natural Language Processing (EMNLP-09).
Shai Ben-David, John Blitzer, Koby Crammer, Alex
Kulesza, Fernando Pereira, and Jennifer Wortman
Vaughan. 2010. A theory of learning from different
domains. Machine Learning, 79:151–175.
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In EMNLP.
Razvan Bunescu and Raymond Mooney. 2007. Learning
to extract relations from the web using minimal super-
vision. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics (ACL-
07).
R. Bunescu and M. Pasca. 2006. Using encyclope-
dic knowledge for named entity disambiguation. In
Proceedings of the 11th Conference of the European
Chapter of the Association for Computational Linguis-
tics (EACL-06).
Ying Chen and James Martin. 2007. Towards Ro-
bust Unsupervised Personal Name Disambiguation. In
EMNLP, pages 190–198.
Silviu Cucerzan. 2007. Large-scale named entity dis-
ambiguation based on wikipedia data. In Proceedings
of the 2007 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning (EMNLP-CoNLL), pages
708–716.
Nilesh N. Dalvi, Ravi Kumar, Bo Pang, and Andrew
Tomkins. 2009. Matching Reviews to Objects using a
Language Model. In EMNLP, pages 609–618.
Nilesh N. Dalvi, Ravi Kumar, and Bo Pang. 2012. Object
matching in tweets with spatial models. In WSDM,
pages 43–52.
Hal Daum´e III, Abhishek Kumar, and Avishek Saha.
2010. Frustratingly easy semi-supervised domain
adaptation. In Proceedings of the ACL Workshop on
Domain Adaptation (DANLP).
D. Downey, M. Broadhead, and O. Etzioni. 2007. Lo-
cating complex named entities in web text. In Procs.
of the 20th International Joint Conference on Artificial
Intelligence (IJCAI 2007).
Anthony Fader, Stephen Soderland, and Oren Etzioni.
2009. Scaling wikipedia-based named entity disam-
biguation to arbitrary web text. In Proceedings of
the WikiAI 09 - IJCAI Workshop: User Contributed
Knowledge and Artificial Intelligence: An Evolving
Synergy.
Xianpei Han and Jun Zhao. 2009. Named entity dis-
ambiguation by leveraging Wikipedia semantic knowl-
edge. In Proceeding of the 18th ACM Conference
on Information and Knowledge Management (CIKM),
pages 215–224.
Johannes Hoffart, Mohamed Amir Yosef, Ilaria Bordino,
Hagen Furstenau, Manfred Pinkal, Marc Spaniol,
Bilyana Taneva, Stefan Thater, and Gerhard Weikum1.
2011. Robust Disambiguation of Named Entities in
Text. In EMNLP, pages 782–792.
Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke
Zettlemoyer, and Daniel S. Weld. 2011. Knowledge-
Based Weak Supervision for Information Extraction of
Overlapping Relations. In Proceedings of the Annual
Meeting of the Association for Computational Linguis-
tics (ACL).
Fei Huang and Alexander Yates. 2009. Distributional
representations for handling sparsity in supervised se-
quence labeling. In Proceedings of the Annual Meet-
ing of the Association for Computational Linguistics
(ACL).
Sayali Kulkarni, Amit Singh, Ganesh Ramakrishnan, and
Soumen Chakrabarti. 2009. Collective annotation
of wikipedia entities in web text. In Proceedings of
the 15th ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining (KDD), pages
457–466.
Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwater,
and Mark Steedman. 2011. Lexical Generalization
in CCG Grammar Induction for Semantic Parsing. In
Proceedings of the Conference on Empirical Methods
in Natural Language Processing (EMNLP).
M.E. Lesk. 1986. Automatic sense disambiguation us-
ing machine readable dictionaries: How to tell a pine
cone from an ice cream cone. In Proceedings of the
SIGDOC Conference.
Thomas Lin, Mausam, and Oren Etzioni. 2012. Entity
linking at web scale. In Knowledge Extraction Work-
shop (AKBC-WEKEX), 2012.
D.C. Liu and J. Nocedal. 1989. On the limited mem-
ory method for large scale optimization. Mathemati-
cal Programming B, 45(3):503–528.
G.S. Mann and D. Yarowsky. 2003. Unsupervised per-
sonal name disambiguation. In CoNLL.
Paul McNamee, Mark Dredze, Adam Gerber, Nikesh
Garera, Tim Finin, James Mayfield, Christine Pi-
atko, Delip Rao, David Yarowsky, and Markus Dreyer.
2009. HLTCOE Approaches to Knowledge Base Pop-
ulation at TAC 2009. In Text Analysis Conference.
Rada Mihalcea and Andras Csomai. 2007. Wikify!:
Linking documents to encyclopedic knowledge. In
</reference>
<page confidence="0.983428">
126
</page>
<reference confidence="0.998842833333333">
Proceedings of the Sixteenth ACM Conference on
Information and Knowledge Management (CIKM),
pages 233–242.
Mike Mintz, Steven Bills, Rion Snow, and Daniel Juraf-
sky. 2009. Distant supervision for relation extrac-
tion without labeled data. In Proceedings of the 47th
Annual Meeting of the Association for Computational
Linguistics (ACL-2009), pages 1003–1011.
Patrick Pantel and Ariel Fuxman. 2011. Jigs and Lures:
Associating Web Queries with Structured Entities. In
ACL.
L. Ratinov, D. Roth, D. Downey, and M. Anderson.
2011. Local and global algorithms for disambiguation
to wikipedia. In Proc. of the Annual Meeting of the
Association of Computational Linguistics (ACL).
Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling relations and their mentions without
labeled text. In Proceedings of the Sixteenth Euro-
pean Conference on Machine Learning (ECML-2010),
pages 148–163.
Avi Silberschatz, Henry F. Korth, and S. Sudarshan.
2010. Database System Concepts. McGraw-Hill,
sixth edition.
Daniel S. Weld, Raphael Hoffmann, and Fei Wu. 2009.
Using Wikipedia to Bootstrap Open Information Ex-
traction. In ACM SIGMOD Record.
Limin Yao, Sebastian Riedel, and Andrew McCallum.
2010. Collective cross-document relation extraction
without labelled data. In Proceedings of the 2010 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP-2010), pages 1013–1023.
Yiping Zhou, Lan Nie, Omid Rouhani-Kalleh, Flavian
Vasile, and Scott Gaffney. 2010. Resolving surface
forms to wikipedia topics. In Proceedings of the 23rd
International Conference on Computational Linguis-
tics (Coling), pages 1335–1343.
</reference>
<page confidence="0.997322">
127
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.060392">
<title confidence="0.993622">Linking Named Entities to Any Database</title>
<author confidence="0.86507">Temple</author>
<affiliation confidence="0.830021">Philadelphia,</affiliation>
<email confidence="0.999622">avi@temple.edu</email>
<author confidence="0.79888">Joseph’s</author>
<affiliation confidence="0.94659">Philadelphia,</affiliation>
<email confidence="0.996054">yangyin7@gmail.com</email>
<author confidence="0.933052">Joseph’s</author>
<affiliation confidence="0.867759">Philadelphia,</affiliation>
<email confidence="0.999426">ernest.cronin@gmail.com</email>
<note confidence="0.419996333333333">Ana-Maria Yahoo! Sunnyvale,</note>
<email confidence="0.999309">amp@yahoo-inc.com</email>
<author confidence="0.775349">Joseph’s</author>
<affiliation confidence="0.938412">Philadelphia,</affiliation>
<email confidence="0.996746">nph87903@gmail.com</email>
<author confidence="0.997319">Alexander Yates</author>
<affiliation confidence="0.998953">Temple University</affiliation>
<address confidence="0.962182">Philadelphia, PA</address>
<email confidence="0.999553">yates@temple.edu</email>
<abstract confidence="0.996448857142857">Existing techniques for disambiguating named entities in text mostly focus on Wikipedia as a target catalog of entities. Yet for many types of entities, such as restaurants and cult movies, relational databases exist that contain far more extensive information than Wikipedia. This paper introduces a new task, called Open-Database Named-Entity Disambiguation (Open-DB NED), in which a system must be able to resolve named entities to symbols in an arbitrary database, without requiring labeled data for each new database. We introduce two techniques for Open-DB NED, one based on distant supervision and the other based on domain adaptation. In experiments on two domains, one with poor coverage by Wikipedia and the other with near-perfect coverage, our Open-DB NED strategies outperform a state-of-the-art Wikipedia NED system by over 25% in accuracy.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Kedar Bellare</author>
<author>Andrew McCallum</author>
</authors>
<title>Learning extractors from unlabeled text using relevant databases.</title>
<date>2007</date>
<booktitle>In Sixth International Workshop on Information Integration on the Web.</booktitle>
<contexts>
<context position="6572" citStr="Bellare and McCallum, 2007" startWordPosition="1042" endWordPosition="1045"> focus on movie and restaurant databases. Bellare and McCallum (2009) use the sequence information available in citation text to link author, title, and venue names to a publication database. Open-DB NED systems work on any database, so they can serve as baselines for domain-specific NED tasks, as well as provide disambiguation for domains where no domain-specific NED system exists. Numerous previous studies have considered distant or weak supervision from a single relational database as an alternative to manual supervision for information extraction (Hoffmann et al., 2011; Weld et al., 2009; Bellare and McCallum, 2007; Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Yao et al., 2010). In contrast to these systems, our distant supervision NED system provides a meta-algorithm for generating an NED system for any database and any entity type. Existing domain adaptation or transfer learning approaches are inappropriate for the Open-DB NED task, either because they require labeled data in both the source and target domains (Daum´e III et al., 2010; Ben-David et al., 2010), or because they leverage some notion of distributional similarity between words in the source and target domains (Blitzer</context>
</contexts>
<marker>Bellare, McCallum, 2007</marker>
<rawString>Kedar Bellare and Andrew McCallum. 2007. Learning extractors from unlabeled text using relevant databases. In Sixth International Workshop on Information Integration on the Web.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kedar Bellare</author>
<author>Andrew McCallum</author>
</authors>
<title>Generalized Expectation Criteria for Bootstrapping Extractors using Record-Text Alignment.</title>
<date>2009</date>
<booktitle>In Empirical Methods in Natural Language Processing (EMNLP-09).</booktitle>
<contexts>
<context position="6015" citStr="Bellare and McCallum (2009)" startWordPosition="955" endWordPosition="958">logs, and have a far higher rate (57%) of missing Wikipedia entries. Lin et al. (2012) find that 33% of mentions in a corpus of 500 million Web documents cannot be linked to Wikipedia. NED systems that are focused on specific domains (or verticals) greatly benefit from repositories of domain-specific knowledge, only a subset of which may be found in Wikipedia. For example, Pantel and Fuxman (2011) use a query-click graph to resolve names in search engine queries to a large product catalog from a commercial search engine, while Dalvi et al. (2009; 2012) focus on movie and restaurant databases. Bellare and McCallum (2009) use the sequence information available in citation text to link author, title, and venue names to a publication database. Open-DB NED systems work on any database, so they can serve as baselines for domain-specific NED tasks, as well as provide disambiguation for domains where no domain-specific NED system exists. Numerous previous studies have considered distant or weak supervision from a single relational database as an alternative to manual supervision for information extraction (Hoffmann et al., 2011; Weld et al., 2009; Bellare and McCallum, 2007; Bunescu and Mooney, 2007; Mintz et al., 2</context>
</contexts>
<marker>Bellare, McCallum, 2009</marker>
<rawString>Kedar Bellare and Andrew McCallum. 2009. Generalized Expectation Criteria for Bootstrapping Extractors using Record-Text Alignment. In Empirical Methods in Natural Language Processing (EMNLP-09).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shai Ben-David</author>
<author>John Blitzer</author>
<author>Koby Crammer</author>
<author>Alex Kulesza</author>
<author>Fernando Pereira</author>
<author>Jennifer Wortman Vaughan</author>
</authors>
<title>A theory of learning from different domains.</title>
<date>2010</date>
<booktitle>Machine Learning,</booktitle>
<pages>79--151</pages>
<contexts>
<context position="7049" citStr="Ben-David et al., 2010" startWordPosition="1120" endWordPosition="1123"> database as an alternative to manual supervision for information extraction (Hoffmann et al., 2011; Weld et al., 2009; Bellare and McCallum, 2007; Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Yao et al., 2010). In contrast to these systems, our distant supervision NED system provides a meta-algorithm for generating an NED system for any database and any entity type. Existing domain adaptation or transfer learning approaches are inappropriate for the Open-DB NED task, either because they require labeled data in both the source and target domains (Daum´e III et al., 2010; Ben-David et al., 2010), or because they leverage some notion of distributional similarity between words in the source and target domains (Blitzer et al., 2006; Huang and Yates, 2009), which does not apply to the database symbols across the two domains. Instead, our domain adaptation technique uses domain-independent features of relational data, which apply regardless of the actual contents of the database, as explained further below. 3 The Open-DB NED Problem and Assumptions 3.1 Problem Formulation A mention is an occurrence of a named-entity in a document. Formally, a mention m = (d, start, end) is a triple consis</context>
</contexts>
<marker>Ben-David, Blitzer, Crammer, Kulesza, Pereira, Vaughan, 2010</marker>
<rawString>Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wortman Vaughan. 2010. A theory of learning from different domains. Machine Learning, 79:151–175.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Blitzer</author>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
</authors>
<title>Domain adaptation with structural correspondence learning.</title>
<date>2006</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="7185" citStr="Blitzer et al., 2006" startWordPosition="1142" endWordPosition="1145">m, 2007; Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Yao et al., 2010). In contrast to these systems, our distant supervision NED system provides a meta-algorithm for generating an NED system for any database and any entity type. Existing domain adaptation or transfer learning approaches are inappropriate for the Open-DB NED task, either because they require labeled data in both the source and target domains (Daum´e III et al., 2010; Ben-David et al., 2010), or because they leverage some notion of distributional similarity between words in the source and target domains (Blitzer et al., 2006; Huang and Yates, 2009), which does not apply to the database symbols across the two domains. Instead, our domain adaptation technique uses domain-independent features of relational data, which apply regardless of the actual contents of the database, as explained further below. 3 The Open-DB NED Problem and Assumptions 3.1 Problem Formulation A mention is an occurrence of a named-entity in a document. Formally, a mention m = (d, start, end) is a triple consisting of a document d, as well as a start and end position for the mention within the document. We say that d is the context of m. A rela</context>
</contexts>
<marker>Blitzer, McDonald, Pereira, 2006</marker>
<rawString>John Blitzer, Ryan McDonald, and Fernando Pereira. 2006. Domain adaptation with structural correspondence learning. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Razvan Bunescu</author>
<author>Raymond Mooney</author>
</authors>
<title>Learning to extract relations from the web using minimal supervision.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL07).</booktitle>
<contexts>
<context position="6598" citStr="Bunescu and Mooney, 2007" startWordPosition="1046" endWordPosition="1049">nt databases. Bellare and McCallum (2009) use the sequence information available in citation text to link author, title, and venue names to a publication database. Open-DB NED systems work on any database, so they can serve as baselines for domain-specific NED tasks, as well as provide disambiguation for domains where no domain-specific NED system exists. Numerous previous studies have considered distant or weak supervision from a single relational database as an alternative to manual supervision for information extraction (Hoffmann et al., 2011; Weld et al., 2009; Bellare and McCallum, 2007; Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Yao et al., 2010). In contrast to these systems, our distant supervision NED system provides a meta-algorithm for generating an NED system for any database and any entity type. Existing domain adaptation or transfer learning approaches are inappropriate for the Open-DB NED task, either because they require labeled data in both the source and target domains (Daum´e III et al., 2010; Ben-David et al., 2010), or because they leverage some notion of distributional similarity between words in the source and target domains (Blitzer et al., 2006; Huang and Y</context>
</contexts>
<marker>Bunescu, Mooney, 2007</marker>
<rawString>Razvan Bunescu and Raymond Mooney. 2007. Learning to extract relations from the web using minimal supervision. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL07).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Bunescu</author>
<author>M Pasca</author>
</authors>
<title>Using encyclopedic knowledge for named entity disambiguation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics (EACL-06).</booktitle>
<contexts>
<context position="1479" citStr="Bunescu and Pasca, 2006" startWordPosition="206" endWordPosition="209"> must be able to resolve named entities to symbols in an arbitrary database, without requiring labeled data for each new database. We introduce two techniques for Open-DB NED, one based on distant supervision and the other based on domain adaptation. In experiments on two domains, one with poor coverage by Wikipedia and the other with near-perfect coverage, our Open-DB NED strategies outperform a state-of-the-art Wikipedia NED system by over 25% in accuracy. 1 Introduction Named-entity disambiguation (NED) is the task of linking names mentioned in text with an established catalog of entities (Bunescu and Pasca, 2006; Ratinov et al., 2011). It is a vital first step for semantic understanding of text, such as in grounded semantic parsing (Kwiatkowski et al., 2011), as well as for information retrieval tasks like person name search (Chen and Martin, 2007; Mann and Yarowsky, 2003). NED requires a catalog of symbols, called referents, to which named-entities will be resolved. Most NED systems today use Wikipedia as the catalog of referents, but exclusive focus on Wikipedia as a target for NED systems has significant drawbacks: despite its breadth, Wikipedia still does not contain all or even most real-world e</context>
<context position="4815" citStr="Bunescu and Pasca, 2006" startWordPosition="741" endWordPosition="744">gins without requiring any labeled data from the test domain, highlighting the significant advantage of having domain-specific relational data. The next section contrasts Open-DB NED with previous work. Section 3 formalizes the task. Sections 4 and 5 present our distant supervision strategy and domain-adaptation strategy, respectively. Section 6 introduces a technique that is a hybrid of the two learning strategies. Section 7 describes our experiments, and Section 8 concludes. 2 Previous Work As mentioned above, restricting the catalog of referents to Wikipedia, as most recent NED systems do (Bunescu and Pasca, 2006; Mihalcea and Csomai, 2007; Fader et al., 2009; Han and Zhao, 2009; Kulkarni et al., 2009; Ratinov et al., 2011), can restrict the coverage of the system. Zhou et al. (2010) estimate that 23% of names in Yahoo! news articles have no referent in Wikipedia, and Cucerzan (2007) estimates the rate at 16% in MSNBC news articles. There is reason to suspect that these estimates are on the low side, however, as news tends to cover popular entities, which are most likely to appear in Wikipedia; the mentions in TAC’s 2009 entity linking task are drawn from both newswire and blogs, and have a far higher</context>
</contexts>
<marker>Bunescu, Pasca, 2006</marker>
<rawString>R. Bunescu and M. Pasca. 2006. Using encyclopedic knowledge for named entity disambiguation. In Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics (EACL-06).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ying Chen</author>
<author>James Martin</author>
</authors>
<title>Towards Robust Unsupervised Personal Name Disambiguation. In</title>
<date>2007</date>
<booktitle>EMNLP,</booktitle>
<pages>190--198</pages>
<contexts>
<context position="1719" citStr="Chen and Martin, 2007" startWordPosition="248" endWordPosition="251">ptation. In experiments on two domains, one with poor coverage by Wikipedia and the other with near-perfect coverage, our Open-DB NED strategies outperform a state-of-the-art Wikipedia NED system by over 25% in accuracy. 1 Introduction Named-entity disambiguation (NED) is the task of linking names mentioned in text with an established catalog of entities (Bunescu and Pasca, 2006; Ratinov et al., 2011). It is a vital first step for semantic understanding of text, such as in grounded semantic parsing (Kwiatkowski et al., 2011), as well as for information retrieval tasks like person name search (Chen and Martin, 2007; Mann and Yarowsky, 2003). NED requires a catalog of symbols, called referents, to which named-entities will be resolved. Most NED systems today use Wikipedia as the catalog of referents, but exclusive focus on Wikipedia as a target for NED systems has significant drawbacks: despite its breadth, Wikipedia still does not contain all or even most real-world entities mentioned in text. As one example, it has poor coverage of entities that are mostly important in a small geographical region, such as hotels and restaurants, which are widely discussed on the Web. 57% of the named-entities in the Te</context>
</contexts>
<marker>Chen, Martin, 2007</marker>
<rawString>Ying Chen and James Martin. 2007. Towards Robust Unsupervised Personal Name Disambiguation. In EMNLP, pages 190–198.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Silviu Cucerzan</author>
</authors>
<title>Large-scale named entity disambiguation based on wikipedia data.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</booktitle>
<pages>708--716</pages>
<contexts>
<context position="5091" citStr="Cucerzan (2007)" startWordPosition="794" endWordPosition="795">trategy and domain-adaptation strategy, respectively. Section 6 introduces a technique that is a hybrid of the two learning strategies. Section 7 describes our experiments, and Section 8 concludes. 2 Previous Work As mentioned above, restricting the catalog of referents to Wikipedia, as most recent NED systems do (Bunescu and Pasca, 2006; Mihalcea and Csomai, 2007; Fader et al., 2009; Han and Zhao, 2009; Kulkarni et al., 2009; Ratinov et al., 2011), can restrict the coverage of the system. Zhou et al. (2010) estimate that 23% of names in Yahoo! news articles have no referent in Wikipedia, and Cucerzan (2007) estimates the rate at 16% in MSNBC news articles. There is reason to suspect that these estimates are on the low side, however, as news tends to cover popular entities, which are most likely to appear in Wikipedia; the mentions in TAC’s 2009 entity linking task are drawn from both newswire and blogs, and have a far higher rate (57%) of missing Wikipedia entries. Lin et al. (2012) find that 33% of mentions in a corpus of 500 million Web documents cannot be linked to Wikipedia. NED systems that are focused on specific domains (or verticals) greatly benefit from repositories of domain-specific k</context>
</contexts>
<marker>Cucerzan, 2007</marker>
<rawString>Silviu Cucerzan. 2007. Large-scale named entity disambiguation based on wikipedia data. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 708–716.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nilesh N Dalvi</author>
<author>Ravi Kumar</author>
<author>Bo Pang</author>
<author>Andrew Tomkins</author>
</authors>
<title>Matching Reviews to Objects using a Language Model. In EMNLP,</title>
<date>2009</date>
<pages>609--618</pages>
<contexts>
<context position="5939" citStr="Dalvi et al. (2009" startWordPosition="944" endWordPosition="947">n TAC’s 2009 entity linking task are drawn from both newswire and blogs, and have a far higher rate (57%) of missing Wikipedia entries. Lin et al. (2012) find that 33% of mentions in a corpus of 500 million Web documents cannot be linked to Wikipedia. NED systems that are focused on specific domains (or verticals) greatly benefit from repositories of domain-specific knowledge, only a subset of which may be found in Wikipedia. For example, Pantel and Fuxman (2011) use a query-click graph to resolve names in search engine queries to a large product catalog from a commercial search engine, while Dalvi et al. (2009; 2012) focus on movie and restaurant databases. Bellare and McCallum (2009) use the sequence information available in citation text to link author, title, and venue names to a publication database. Open-DB NED systems work on any database, so they can serve as baselines for domain-specific NED tasks, as well as provide disambiguation for domains where no domain-specific NED system exists. Numerous previous studies have considered distant or weak supervision from a single relational database as an alternative to manual supervision for information extraction (Hoffmann et al., 2011; Weld et al.,</context>
</contexts>
<marker>Dalvi, Kumar, Pang, Tomkins, 2009</marker>
<rawString>Nilesh N. Dalvi, Ravi Kumar, Bo Pang, and Andrew Tomkins. 2009. Matching Reviews to Objects using a Language Model. In EMNLP, pages 609–618.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nilesh N Dalvi</author>
<author>Ravi Kumar</author>
<author>Bo Pang</author>
</authors>
<title>Object matching in tweets with spatial models.</title>
<date>2012</date>
<booktitle>In WSDM,</booktitle>
<pages>43--52</pages>
<marker>Dalvi, Kumar, Pang, 2012</marker>
<rawString>Nilesh N. Dalvi, Ravi Kumar, and Bo Pang. 2012. Object matching in tweets with spatial models. In WSDM, pages 43–52.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e Abhishek Kumar</author>
<author>Avishek Saha</author>
</authors>
<title>Frustratingly easy semi-supervised domain adaptation.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACL Workshop on Domain Adaptation (DANLP).</booktitle>
<marker>Kumar, Saha, 2010</marker>
<rawString>Hal Daum´e III, Abhishek Kumar, and Avishek Saha. 2010. Frustratingly easy semi-supervised domain adaptation. In Proceedings of the ACL Workshop on Domain Adaptation (DANLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Downey</author>
<author>M Broadhead</author>
<author>O Etzioni</author>
</authors>
<title>Locating complex named entities in web text.</title>
<date>2007</date>
<booktitle>In Procs. of the 20th International Joint Conference on Artificial Intelligence (IJCAI</booktitle>
<contexts>
<context position="10624" citStr="Downey et al., 2007" startWordPosition="1804" endWordPosition="1807"> is OOD. Note that this problem formulation assumes no labeled data. This is significantly more challenging than traditional NED settings, but allows the system to generalize easily to any new database. In the domain adaptation section below, we relax this condition somewhat, to allow labeled data for a small number of initial databases; the system must then transfer what it learns from the labeled domains to any new database. Also note that the focus for this paper is disambiguation; we assume that the set of mentions are correctly demarcated in the input text. Previous systems, such as Lex (Downey et al., 2007), have investigated the task of finding correct named-entity boundaries in text. 3.2 Assumptions To allow our systems to handle arbitrary databases, we need to make some assumptions about a standard format for the data. We will assume that databases are provided in a particular form, called Boyce-Codd In practice, this is a relatively safe assumption as database designers often aim for even stricter normal forms. For databases not in BCNF, such as tables extracted from Web pages, standard algorithms exist for converting them into BCNF, given appropriate functional dependencies, although there </context>
</contexts>
<marker>Downey, Broadhead, Etzioni, 2007</marker>
<rawString>D. Downey, M. Broadhead, and O. Etzioni. 2007. Locating complex named entities in web text. In Procs. of the 20th International Joint Conference on Artificial Intelligence (IJCAI 2007).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anthony Fader</author>
<author>Stephen Soderland</author>
<author>Oren Etzioni</author>
</authors>
<title>Scaling wikipedia-based named entity disambiguation to arbitrary web text.</title>
<date>2009</date>
<booktitle>In Proceedings of the WikiAI</booktitle>
<volume>09</volume>
<contexts>
<context position="4862" citStr="Fader et al., 2009" startWordPosition="750" endWordPosition="753">t domain, highlighting the significant advantage of having domain-specific relational data. The next section contrasts Open-DB NED with previous work. Section 3 formalizes the task. Sections 4 and 5 present our distant supervision strategy and domain-adaptation strategy, respectively. Section 6 introduces a technique that is a hybrid of the two learning strategies. Section 7 describes our experiments, and Section 8 concludes. 2 Previous Work As mentioned above, restricting the catalog of referents to Wikipedia, as most recent NED systems do (Bunescu and Pasca, 2006; Mihalcea and Csomai, 2007; Fader et al., 2009; Han and Zhao, 2009; Kulkarni et al., 2009; Ratinov et al., 2011), can restrict the coverage of the system. Zhou et al. (2010) estimate that 23% of names in Yahoo! news articles have no referent in Wikipedia, and Cucerzan (2007) estimates the rate at 16% in MSNBC news articles. There is reason to suspect that these estimates are on the low side, however, as news tends to cover popular entities, which are most likely to appear in Wikipedia; the mentions in TAC’s 2009 entity linking task are drawn from both newswire and blogs, and have a far higher rate (57%) of missing Wikipedia entries. Lin e</context>
</contexts>
<marker>Fader, Soderland, Etzioni, 2009</marker>
<rawString>Anthony Fader, Stephen Soderland, and Oren Etzioni. 2009. Scaling wikipedia-based named entity disambiguation to arbitrary web text. In Proceedings of the WikiAI 09 - IJCAI Workshop: User Contributed Knowledge and Artificial Intelligence: An Evolving Synergy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xianpei Han</author>
<author>Jun Zhao</author>
</authors>
<title>Named entity disambiguation by leveraging Wikipedia semantic knowledge.</title>
<date>2009</date>
<booktitle>In Proceeding of the 18th ACM Conference on Information and Knowledge Management (CIKM),</booktitle>
<pages>215--224</pages>
<contexts>
<context position="4882" citStr="Han and Zhao, 2009" startWordPosition="754" endWordPosition="757">ng the significant advantage of having domain-specific relational data. The next section contrasts Open-DB NED with previous work. Section 3 formalizes the task. Sections 4 and 5 present our distant supervision strategy and domain-adaptation strategy, respectively. Section 6 introduces a technique that is a hybrid of the two learning strategies. Section 7 describes our experiments, and Section 8 concludes. 2 Previous Work As mentioned above, restricting the catalog of referents to Wikipedia, as most recent NED systems do (Bunescu and Pasca, 2006; Mihalcea and Csomai, 2007; Fader et al., 2009; Han and Zhao, 2009; Kulkarni et al., 2009; Ratinov et al., 2011), can restrict the coverage of the system. Zhou et al. (2010) estimate that 23% of names in Yahoo! news articles have no referent in Wikipedia, and Cucerzan (2007) estimates the rate at 16% in MSNBC news articles. There is reason to suspect that these estimates are on the low side, however, as news tends to cover popular entities, which are most likely to appear in Wikipedia; the mentions in TAC’s 2009 entity linking task are drawn from both newswire and blogs, and have a far higher rate (57%) of missing Wikipedia entries. Lin et al. (2012) find th</context>
</contexts>
<marker>Han, Zhao, 2009</marker>
<rawString>Xianpei Han and Jun Zhao. 2009. Named entity disambiguation by leveraging Wikipedia semantic knowledge. In Proceeding of the 18th ACM Conference on Information and Knowledge Management (CIKM), pages 215–224.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johannes Hoffart</author>
</authors>
<title>Mohamed Amir Yosef, Ilaria Bordino, Hagen Furstenau, Manfred Pinkal, Marc Spaniol, Bilyana Taneva, Stefan Thater, and Gerhard Weikum1.</title>
<date>2011</date>
<booktitle>EMNLP,</booktitle>
<pages>782--792</pages>
<marker>Hoffart, 2011</marker>
<rawString>Johannes Hoffart, Mohamed Amir Yosef, Ilaria Bordino, Hagen Furstenau, Manfred Pinkal, Marc Spaniol, Bilyana Taneva, Stefan Thater, and Gerhard Weikum1. 2011. Robust Disambiguation of Named Entities in Text. In EMNLP, pages 782–792.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raphael Hoffmann</author>
<author>Congle Zhang</author>
<author>Xiao Ling</author>
<author>Luke Zettlemoyer</author>
<author>Daniel S Weld</author>
</authors>
<title>KnowledgeBased Weak Supervision for Information Extraction of Overlapping Relations.</title>
<date>2011</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="6525" citStr="Hoffmann et al., 2011" startWordPosition="1034" endWordPosition="1037">ch engine, while Dalvi et al. (2009; 2012) focus on movie and restaurant databases. Bellare and McCallum (2009) use the sequence information available in citation text to link author, title, and venue names to a publication database. Open-DB NED systems work on any database, so they can serve as baselines for domain-specific NED tasks, as well as provide disambiguation for domains where no domain-specific NED system exists. Numerous previous studies have considered distant or weak supervision from a single relational database as an alternative to manual supervision for information extraction (Hoffmann et al., 2011; Weld et al., 2009; Bellare and McCallum, 2007; Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Yao et al., 2010). In contrast to these systems, our distant supervision NED system provides a meta-algorithm for generating an NED system for any database and any entity type. Existing domain adaptation or transfer learning approaches are inappropriate for the Open-DB NED task, either because they require labeled data in both the source and target domains (Daum´e III et al., 2010; Ben-David et al., 2010), or because they leverage some notion of distributional similarity between </context>
</contexts>
<marker>Hoffmann, Zhang, Ling, Zettlemoyer, Weld, 2011</marker>
<rawString>Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke Zettlemoyer, and Daniel S. Weld. 2011. KnowledgeBased Weak Supervision for Information Extraction of Overlapping Relations. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Huang</author>
<author>Alexander Yates</author>
</authors>
<title>Distributional representations for handling sparsity in supervised sequence labeling.</title>
<date>2009</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="7209" citStr="Huang and Yates, 2009" startWordPosition="1146" endWordPosition="1149">ooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Yao et al., 2010). In contrast to these systems, our distant supervision NED system provides a meta-algorithm for generating an NED system for any database and any entity type. Existing domain adaptation or transfer learning approaches are inappropriate for the Open-DB NED task, either because they require labeled data in both the source and target domains (Daum´e III et al., 2010; Ben-David et al., 2010), or because they leverage some notion of distributional similarity between words in the source and target domains (Blitzer et al., 2006; Huang and Yates, 2009), which does not apply to the database symbols across the two domains. Instead, our domain adaptation technique uses domain-independent features of relational data, which apply regardless of the actual contents of the database, as explained further below. 3 The Open-DB NED Problem and Assumptions 3.1 Problem Formulation A mention is an occurrence of a named-entity in a document. Formally, a mention m = (d, start, end) is a triple consisting of a document d, as well as a start and end position for the mention within the document. We say that d is the context of m. A relational database is a 2-t</context>
</contexts>
<marker>Huang, Yates, 2009</marker>
<rawString>Fei Huang and Alexander Yates. 2009. Distributional representations for handling sparsity in supervised sequence labeling. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sayali Kulkarni</author>
<author>Amit Singh</author>
<author>Ganesh Ramakrishnan</author>
<author>Soumen Chakrabarti</author>
</authors>
<title>Collective annotation of wikipedia entities in web text.</title>
<date>2009</date>
<booktitle>In Proceedings of the 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD),</booktitle>
<pages>457--466</pages>
<contexts>
<context position="4905" citStr="Kulkarni et al., 2009" startWordPosition="758" endWordPosition="761">dvantage of having domain-specific relational data. The next section contrasts Open-DB NED with previous work. Section 3 formalizes the task. Sections 4 and 5 present our distant supervision strategy and domain-adaptation strategy, respectively. Section 6 introduces a technique that is a hybrid of the two learning strategies. Section 7 describes our experiments, and Section 8 concludes. 2 Previous Work As mentioned above, restricting the catalog of referents to Wikipedia, as most recent NED systems do (Bunescu and Pasca, 2006; Mihalcea and Csomai, 2007; Fader et al., 2009; Han and Zhao, 2009; Kulkarni et al., 2009; Ratinov et al., 2011), can restrict the coverage of the system. Zhou et al. (2010) estimate that 23% of names in Yahoo! news articles have no referent in Wikipedia, and Cucerzan (2007) estimates the rate at 16% in MSNBC news articles. There is reason to suspect that these estimates are on the low side, however, as news tends to cover popular entities, which are most likely to appear in Wikipedia; the mentions in TAC’s 2009 entity linking task are drawn from both newswire and blogs, and have a far higher rate (57%) of missing Wikipedia entries. Lin et al. (2012) find that 33% of mentions in a</context>
</contexts>
<marker>Kulkarni, Singh, Ramakrishnan, Chakrabarti, 2009</marker>
<rawString>Sayali Kulkarni, Amit Singh, Ganesh Ramakrishnan, and Soumen Chakrabarti. 2009. Collective annotation of wikipedia entities in web text. In Proceedings of the 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD), pages 457–466.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom Kwiatkowski</author>
<author>Luke Zettlemoyer</author>
<author>Sharon Goldwater</author>
<author>Mark Steedman</author>
</authors>
<title>Lexical Generalization in CCG Grammar Induction for Semantic Parsing.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="1628" citStr="Kwiatkowski et al., 2011" startWordPosition="233" endWordPosition="236"> techniques for Open-DB NED, one based on distant supervision and the other based on domain adaptation. In experiments on two domains, one with poor coverage by Wikipedia and the other with near-perfect coverage, our Open-DB NED strategies outperform a state-of-the-art Wikipedia NED system by over 25% in accuracy. 1 Introduction Named-entity disambiguation (NED) is the task of linking names mentioned in text with an established catalog of entities (Bunescu and Pasca, 2006; Ratinov et al., 2011). It is a vital first step for semantic understanding of text, such as in grounded semantic parsing (Kwiatkowski et al., 2011), as well as for information retrieval tasks like person name search (Chen and Martin, 2007; Mann and Yarowsky, 2003). NED requires a catalog of symbols, called referents, to which named-entities will be resolved. Most NED systems today use Wikipedia as the catalog of referents, but exclusive focus on Wikipedia as a target for NED systems has significant drawbacks: despite its breadth, Wikipedia still does not contain all or even most real-world entities mentioned in text. As one example, it has poor coverage of entities that are mostly important in a small geographical region, such as hotels </context>
</contexts>
<marker>Kwiatkowski, Zettlemoyer, Goldwater, Steedman, 2011</marker>
<rawString>Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwater, and Mark Steedman. 2011. Lexical Generalization in CCG Grammar Induction for Semantic Parsing. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M E Lesk</author>
</authors>
<title>Automatic sense disambiguation using machine readable dictionaries: How to tell a pine cone from an ice cream cone.</title>
<date>1986</date>
<booktitle>In Proceedings of the SIGDOC Conference.</booktitle>
<contexts>
<context position="38375" citStr="Lesk, 1986" startWordPosition="6460" endWordPosition="6461">lobally common entities. It consistently classifies mentions that are ambiguous between a city and a team (like “Chicago” in “Chicago sweeps the Red Sox”) as cities when they should be resolved to teams, in large part because Chicago is a more common referent in general text than either of the baseball teams that play in that city. In sports articles, however, both meanings are common, and only the surrounding context can help determine the correct referent. Besides wikifiers, NED systems may also be compared with dictionary-based word sense disambiguation techniques like the Lesk algorithm2 (Lesk, 1986). The Lesk algorithm is “open” in the sense that it works for arbitrary dictionaries, and it defines a vector space model of the dictionary definitions that may be likened to the attribute-value model in our representation of entities in the database. Our approach, however, estimates parameters for a statistical model from data, whereas the Lesk algorithm uses an equal weight for all attributes. To make an empirical comparison, we created a variant of the Lesk algorithm for relational data: we took the disambiguation model from Eqn. 1, supplied all of the features from the distant supervision </context>
</contexts>
<marker>Lesk, 1986</marker>
<rawString>M.E. Lesk. 1986. Automatic sense disambiguation using machine readable dictionaries: How to tell a pine cone from an ice cream cone. In Proceedings of the SIGDOC Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Lin</author>
<author>Mausam</author>
<author>Oren Etzioni</author>
</authors>
<title>Entity linking at web scale.</title>
<date>2012</date>
<booktitle>In Knowledge Extraction Workshop (AKBC-WEKEX),</booktitle>
<contexts>
<context position="5474" citStr="Lin et al. (2012)" startWordPosition="863" endWordPosition="866"> 2009; Han and Zhao, 2009; Kulkarni et al., 2009; Ratinov et al., 2011), can restrict the coverage of the system. Zhou et al. (2010) estimate that 23% of names in Yahoo! news articles have no referent in Wikipedia, and Cucerzan (2007) estimates the rate at 16% in MSNBC news articles. There is reason to suspect that these estimates are on the low side, however, as news tends to cover popular entities, which are most likely to appear in Wikipedia; the mentions in TAC’s 2009 entity linking task are drawn from both newswire and blogs, and have a far higher rate (57%) of missing Wikipedia entries. Lin et al. (2012) find that 33% of mentions in a corpus of 500 million Web documents cannot be linked to Wikipedia. NED systems that are focused on specific domains (or verticals) greatly benefit from repositories of domain-specific knowledge, only a subset of which may be found in Wikipedia. For example, Pantel and Fuxman (2011) use a query-click graph to resolve names in search engine queries to a large product catalog from a commercial search engine, while Dalvi et al. (2009; 2012) focus on movie and restaurant databases. Bellare and McCallum (2009) use the sequence information available in citation text to</context>
</contexts>
<marker>Lin, Mausam, Etzioni, 2012</marker>
<rawString>Thomas Lin, Mausam, and Oren Etzioni. 2012. Entity linking at web scale. In Knowledge Extraction Workshop (AKBC-WEKEX), 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D C Liu</author>
<author>J Nocedal</author>
</authors>
<title>On the limited memory method for large scale optimization.</title>
<date>1989</date>
<journal>Mathematical Programming B,</journal>
<volume>45</volume>
<issue>3</issue>
<contexts>
<context position="18951" citStr="Liu and Nocedal, 1989" startWordPosition="3237" endWordPosition="3240">o learn effectively from unlabeled text. Let 0(m, DB) be a heuristic string-matching function that returns a set of plausible ID values in database DB for mention m. The objective function for this training procedure is a modified marginal log likelihood (MLL) function that encourages probability mass to be placed on the heuristically-matched targets: MLL(M, w) = � log � P(id|m, w) MEM idEO(M,DB) This objective is smooth but non-convex. We use a gradient-based optimization procedure that finds a local maximum. Our implementation uses an opensource version of the LBFG-S optimization technique (Liu and Nocedal, 1989). The gradient of our objective is given by EidEO(M,DB) Ifi(m, id)] − EidEDB.S Ifi(m, id)] where the expectations are taken according to P(id|m, w). 5 A Domain-Adaptation Strategy for Open-DB NED Our domain-adaptation strategy builds an Open-DB NED system by training it on labeled examples from an initial database or small set of initial databases. Unlike traditional NED, however, the purpose in Open-DB NED is to resolve to any database. Thus the strategy must take care to build a model that can transfer what it has learned to a new database, without requiring additional labeled data for the n</context>
</contexts>
<marker>Liu, Nocedal, 1989</marker>
<rawString>D.C. Liu and J. Nocedal. 1989. On the limited memory method for large scale optimization. Mathematical Programming B, 45(3):503–528.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G S Mann</author>
<author>D Yarowsky</author>
</authors>
<title>Unsupervised personal name disambiguation.</title>
<date>2003</date>
<booktitle>In CoNLL.</booktitle>
<contexts>
<context position="1745" citStr="Mann and Yarowsky, 2003" startWordPosition="252" endWordPosition="255"> on two domains, one with poor coverage by Wikipedia and the other with near-perfect coverage, our Open-DB NED strategies outperform a state-of-the-art Wikipedia NED system by over 25% in accuracy. 1 Introduction Named-entity disambiguation (NED) is the task of linking names mentioned in text with an established catalog of entities (Bunescu and Pasca, 2006; Ratinov et al., 2011). It is a vital first step for semantic understanding of text, such as in grounded semantic parsing (Kwiatkowski et al., 2011), as well as for information retrieval tasks like person name search (Chen and Martin, 2007; Mann and Yarowsky, 2003). NED requires a catalog of symbols, called referents, to which named-entities will be resolved. Most NED systems today use Wikipedia as the catalog of referents, but exclusive focus on Wikipedia as a target for NED systems has significant drawbacks: despite its breadth, Wikipedia still does not contain all or even most real-world entities mentioned in text. As one example, it has poor coverage of entities that are mostly important in a small geographical region, such as hotels and restaurants, which are widely discussed on the Web. 57% of the named-entities in the Text Analysis Conference’s (</context>
</contexts>
<marker>Mann, Yarowsky, 2003</marker>
<rawString>G.S. Mann and D. Yarowsky. 2003. Unsupervised personal name disambiguation. In CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul McNamee</author>
<author>Mark Dredze</author>
<author>Adam Gerber</author>
<author>Nikesh Garera</author>
<author>Tim Finin</author>
<author>James Mayfield</author>
<author>Christine Piatko</author>
<author>Delip Rao</author>
<author>David Yarowsky</author>
<author>Markus Dreyer</author>
</authors>
<title>HLTCOE Approaches to Knowledge Base Population at TAC</title>
<date>2009</date>
<booktitle>In Text Analysis Conference.</booktitle>
<contexts>
<context position="2450" citStr="McNamee et al., 2009" startWordPosition="370" endWordPosition="373">be resolved. Most NED systems today use Wikipedia as the catalog of referents, but exclusive focus on Wikipedia as a target for NED systems has significant drawbacks: despite its breadth, Wikipedia still does not contain all or even most real-world entities mentioned in text. As one example, it has poor coverage of entities that are mostly important in a small geographical region, such as hotels and restaurants, which are widely discussed on the Web. 57% of the named-entities in the Text Analysis Conference’s (TAC) 2009 entity linking task refer to an entity that does not appear in Wikipedia (McNamee et al., 2009). Wikipedia is clearly a highly valuable resource, but it should not be thought of as the only one. Instead of relying solely on Wikipedia, we propose a novel approach to NED, which we refer to as Open-DB NED: the task is to resolve an entity to Wikipedia or to any relational database that meets mild conditions about the format of the data, described below. Leveraging structured, relational data should allow systems to achieve strong accuracy, as with domain-specific or database-specific NED techniques like Hoffart et al.’s NED system for YAGO (Hoffart et al., 2011). And because of the availab</context>
</contexts>
<marker>McNamee, Dredze, Gerber, Garera, Finin, Mayfield, Piatko, Rao, Yarowsky, Dreyer, 2009</marker>
<rawString>Paul McNamee, Mark Dredze, Adam Gerber, Nikesh Garera, Tim Finin, James Mayfield, Christine Piatko, Delip Rao, David Yarowsky, and Markus Dreyer. 2009. HLTCOE Approaches to Knowledge Base Population at TAC 2009. In Text Analysis Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Andras Csomai</author>
</authors>
<title>Wikify!: Linking documents to encyclopedic knowledge.</title>
<date>2007</date>
<booktitle>In Proceedings of the Sixteenth ACM Conference on Information and Knowledge Management (CIKM),</booktitle>
<pages>233--242</pages>
<contexts>
<context position="4842" citStr="Mihalcea and Csomai, 2007" startWordPosition="745" endWordPosition="749">y labeled data from the test domain, highlighting the significant advantage of having domain-specific relational data. The next section contrasts Open-DB NED with previous work. Section 3 formalizes the task. Sections 4 and 5 present our distant supervision strategy and domain-adaptation strategy, respectively. Section 6 introduces a technique that is a hybrid of the two learning strategies. Section 7 describes our experiments, and Section 8 concludes. 2 Previous Work As mentioned above, restricting the catalog of referents to Wikipedia, as most recent NED systems do (Bunescu and Pasca, 2006; Mihalcea and Csomai, 2007; Fader et al., 2009; Han and Zhao, 2009; Kulkarni et al., 2009; Ratinov et al., 2011), can restrict the coverage of the system. Zhou et al. (2010) estimate that 23% of names in Yahoo! news articles have no referent in Wikipedia, and Cucerzan (2007) estimates the rate at 16% in MSNBC news articles. There is reason to suspect that these estimates are on the low side, however, as news tends to cover popular entities, which are most likely to appear in Wikipedia; the mentions in TAC’s 2009 entity linking task are drawn from both newswire and blogs, and have a far higher rate (57%) of missing Wiki</context>
</contexts>
<marker>Mihalcea, Csomai, 2007</marker>
<rawString>Rada Mihalcea and Andras Csomai. 2007. Wikify!: Linking documents to encyclopedic knowledge. In Proceedings of the Sixteenth ACM Conference on Information and Knowledge Management (CIKM), pages 233–242.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mike Mintz</author>
<author>Steven Bills</author>
<author>Rion Snow</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Distant supervision for relation extraction without labeled data.</title>
<date>2009</date>
<booktitle>In Proceedings of the 47th Annual Meeting of the Association for Computational Linguistics (ACL-2009),</booktitle>
<pages>1003--1011</pages>
<contexts>
<context position="6618" citStr="Mintz et al., 2009" startWordPosition="1050" endWordPosition="1053">McCallum (2009) use the sequence information available in citation text to link author, title, and venue names to a publication database. Open-DB NED systems work on any database, so they can serve as baselines for domain-specific NED tasks, as well as provide disambiguation for domains where no domain-specific NED system exists. Numerous previous studies have considered distant or weak supervision from a single relational database as an alternative to manual supervision for information extraction (Hoffmann et al., 2011; Weld et al., 2009; Bellare and McCallum, 2007; Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Yao et al., 2010). In contrast to these systems, our distant supervision NED system provides a meta-algorithm for generating an NED system for any database and any entity type. Existing domain adaptation or transfer learning approaches are inappropriate for the Open-DB NED task, either because they require labeled data in both the source and target domains (Daum´e III et al., 2010; Ben-David et al., 2010), or because they leverage some notion of distributional similarity between words in the source and target domains (Blitzer et al., 2006; Huang and Yates, 2009), which d</context>
</contexts>
<marker>Mintz, Bills, Snow, Jurafsky, 2009</marker>
<rawString>Mike Mintz, Steven Bills, Rion Snow, and Daniel Jurafsky. 2009. Distant supervision for relation extraction without labeled data. In Proceedings of the 47th Annual Meeting of the Association for Computational Linguistics (ACL-2009), pages 1003–1011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Pantel</author>
<author>Ariel Fuxman</author>
</authors>
<title>Jigs and Lures: Associating Web Queries with Structured Entities.</title>
<date>2011</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="5788" citStr="Pantel and Fuxman (2011)" startWordPosition="917" endWordPosition="920">pect that these estimates are on the low side, however, as news tends to cover popular entities, which are most likely to appear in Wikipedia; the mentions in TAC’s 2009 entity linking task are drawn from both newswire and blogs, and have a far higher rate (57%) of missing Wikipedia entries. Lin et al. (2012) find that 33% of mentions in a corpus of 500 million Web documents cannot be linked to Wikipedia. NED systems that are focused on specific domains (or verticals) greatly benefit from repositories of domain-specific knowledge, only a subset of which may be found in Wikipedia. For example, Pantel and Fuxman (2011) use a query-click graph to resolve names in search engine queries to a large product catalog from a commercial search engine, while Dalvi et al. (2009; 2012) focus on movie and restaurant databases. Bellare and McCallum (2009) use the sequence information available in citation text to link author, title, and venue names to a publication database. Open-DB NED systems work on any database, so they can serve as baselines for domain-specific NED tasks, as well as provide disambiguation for domains where no domain-specific NED system exists. Numerous previous studies have considered distant or wea</context>
</contexts>
<marker>Pantel, Fuxman, 2011</marker>
<rawString>Patrick Pantel and Ariel Fuxman. 2011. Jigs and Lures: Associating Web Queries with Structured Entities. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Ratinov</author>
<author>D Roth</author>
<author>D Downey</author>
<author>M Anderson</author>
</authors>
<title>Local and global algorithms for disambiguation to wikipedia.</title>
<date>2011</date>
<booktitle>In Proc. of the Annual Meeting of the Association of Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="1502" citStr="Ratinov et al., 2011" startWordPosition="210" endWordPosition="214">named entities to symbols in an arbitrary database, without requiring labeled data for each new database. We introduce two techniques for Open-DB NED, one based on distant supervision and the other based on domain adaptation. In experiments on two domains, one with poor coverage by Wikipedia and the other with near-perfect coverage, our Open-DB NED strategies outperform a state-of-the-art Wikipedia NED system by over 25% in accuracy. 1 Introduction Named-entity disambiguation (NED) is the task of linking names mentioned in text with an established catalog of entities (Bunescu and Pasca, 2006; Ratinov et al., 2011). It is a vital first step for semantic understanding of text, such as in grounded semantic parsing (Kwiatkowski et al., 2011), as well as for information retrieval tasks like person name search (Chen and Martin, 2007; Mann and Yarowsky, 2003). NED requires a catalog of symbols, called referents, to which named-entities will be resolved. Most NED systems today use Wikipedia as the catalog of referents, but exclusive focus on Wikipedia as a target for NED systems has significant drawbacks: despite its breadth, Wikipedia still does not contain all or even most real-world entities mentioned in te</context>
<context position="4928" citStr="Ratinov et al., 2011" startWordPosition="762" endWordPosition="765">in-specific relational data. The next section contrasts Open-DB NED with previous work. Section 3 formalizes the task. Sections 4 and 5 present our distant supervision strategy and domain-adaptation strategy, respectively. Section 6 introduces a technique that is a hybrid of the two learning strategies. Section 7 describes our experiments, and Section 8 concludes. 2 Previous Work As mentioned above, restricting the catalog of referents to Wikipedia, as most recent NED systems do (Bunescu and Pasca, 2006; Mihalcea and Csomai, 2007; Fader et al., 2009; Han and Zhao, 2009; Kulkarni et al., 2009; Ratinov et al., 2011), can restrict the coverage of the system. Zhou et al. (2010) estimate that 23% of names in Yahoo! news articles have no referent in Wikipedia, and Cucerzan (2007) estimates the rate at 16% in MSNBC news articles. There is reason to suspect that these estimates are on the low side, however, as news tends to cover popular entities, which are most likely to appear in Wikipedia; the mentions in TAC’s 2009 entity linking task are drawn from both newswire and blogs, and have a far higher rate (57%) of missing Wikipedia entries. Lin et al. (2012) find that 33% of mentions in a corpus of 500 million </context>
</contexts>
<marker>Ratinov, Roth, Downey, Anderson, 2011</marker>
<rawString>L. Ratinov, D. Roth, D. Downey, and M. Anderson. 2011. Local and global algorithms for disambiguation to wikipedia. In Proc. of the Annual Meeting of the Association of Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Riedel</author>
<author>Limin Yao</author>
<author>Andrew McCallum</author>
</authors>
<title>Modeling relations and their mentions without labeled text.</title>
<date>2010</date>
<booktitle>In Proceedings of the Sixteenth European Conference on Machine Learning (ECML-2010),</booktitle>
<pages>148--163</pages>
<contexts>
<context position="6639" citStr="Riedel et al., 2010" startWordPosition="1054" endWordPosition="1057">the sequence information available in citation text to link author, title, and venue names to a publication database. Open-DB NED systems work on any database, so they can serve as baselines for domain-specific NED tasks, as well as provide disambiguation for domains where no domain-specific NED system exists. Numerous previous studies have considered distant or weak supervision from a single relational database as an alternative to manual supervision for information extraction (Hoffmann et al., 2011; Weld et al., 2009; Bellare and McCallum, 2007; Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Yao et al., 2010). In contrast to these systems, our distant supervision NED system provides a meta-algorithm for generating an NED system for any database and any entity type. Existing domain adaptation or transfer learning approaches are inappropriate for the Open-DB NED task, either because they require labeled data in both the source and target domains (Daum´e III et al., 2010; Ben-David et al., 2010), or because they leverage some notion of distributional similarity between words in the source and target domains (Blitzer et al., 2006; Huang and Yates, 2009), which does not apply to the </context>
</contexts>
<marker>Riedel, Yao, McCallum, 2010</marker>
<rawString>Sebastian Riedel, Limin Yao, and Andrew McCallum. 2010. Modeling relations and their mentions without labeled text. In Proceedings of the Sixteenth European Conference on Machine Learning (ECML-2010), pages 148–163.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Avi Silberschatz</author>
<author>Henry F Korth</author>
<author>S Sudarshan</author>
</authors>
<title>Database System Concepts. McGraw-Hill, sixth edition.</title>
<date>2010</date>
<contexts>
<context position="8249" citStr="Silberschatz et al., 2010" startWordPosition="1343" endWordPosition="1346">t, end) is a triple consisting of a document d, as well as a start and end position for the mention within the document. We say that d is the context of m. A relational database is a 2-tuple (5, R). Here, 5 is a set of symbols for constants, attributes, and relations in the database, and R = {r1, ... , rnI is a set of relation instances of the form ri = {(c1,1, ... , c1,ki), ... , (cni,1, ... , cni,ki)I, where each cj is taken from 5, ki is the arity of relation ri and ni is the number of known instances of ri. We will write example database symbols in 117 id 1 2 3 4 5 ... Normal Form (BCNF) (Silberschatz et al., 2010). A relational schema is said to be in BCNF when all redundancy based on functional dependency has been removed, although other types of redundancy may still exist. Formally, a schema R is said to be in BCNF with respect to a set of functional dependencies F if for every one of the dependencies (X —* Y ) E F, either 1. Y C X, meaning this is a trivial functional dependency, or id ... 4 Chris Johnson 2 3 Chris Johnson 5 Chris Johnson 1 Rob Bironas Carlos Lee name ... player height position 5’11” 6’2” 6’0” 6’3” 6’1” ... 3B DB RB LF K id ... 4 2 3 5 1 San Diego Padres Tennessee Titans Oakland Rai</context>
</contexts>
<marker>Silberschatz, Korth, Sudarshan, 2010</marker>
<rawString>Avi Silberschatz, Henry F. Korth, and S. Sudarshan. 2010. Database System Concepts. McGraw-Hill, sixth edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel S Weld</author>
<author>Raphael Hoffmann</author>
<author>Fei Wu</author>
</authors>
<title>Using Wikipedia to Bootstrap Open Information Extraction. In</title>
<date>2009</date>
<journal>ACM SIGMOD Record.</journal>
<contexts>
<context position="6544" citStr="Weld et al., 2009" startWordPosition="1038" endWordPosition="1041">et al. (2009; 2012) focus on movie and restaurant databases. Bellare and McCallum (2009) use the sequence information available in citation text to link author, title, and venue names to a publication database. Open-DB NED systems work on any database, so they can serve as baselines for domain-specific NED tasks, as well as provide disambiguation for domains where no domain-specific NED system exists. Numerous previous studies have considered distant or weak supervision from a single relational database as an alternative to manual supervision for information extraction (Hoffmann et al., 2011; Weld et al., 2009; Bellare and McCallum, 2007; Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Yao et al., 2010). In contrast to these systems, our distant supervision NED system provides a meta-algorithm for generating an NED system for any database and any entity type. Existing domain adaptation or transfer learning approaches are inappropriate for the Open-DB NED task, either because they require labeled data in both the source and target domains (Daum´e III et al., 2010; Ben-David et al., 2010), or because they leverage some notion of distributional similarity between words in the source</context>
</contexts>
<marker>Weld, Hoffmann, Wu, 2009</marker>
<rawString>Daniel S. Weld, Raphael Hoffmann, and Fei Wu. 2009. Using Wikipedia to Bootstrap Open Information Extraction. In ACM SIGMOD Record.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Limin Yao</author>
<author>Sebastian Riedel</author>
<author>Andrew McCallum</author>
</authors>
<title>Collective cross-document relation extraction without labelled data.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing (EMNLP-2010),</booktitle>
<pages>1013--1023</pages>
<contexts>
<context position="6658" citStr="Yao et al., 2010" startWordPosition="1058" endWordPosition="1061">ion available in citation text to link author, title, and venue names to a publication database. Open-DB NED systems work on any database, so they can serve as baselines for domain-specific NED tasks, as well as provide disambiguation for domains where no domain-specific NED system exists. Numerous previous studies have considered distant or weak supervision from a single relational database as an alternative to manual supervision for information extraction (Hoffmann et al., 2011; Weld et al., 2009; Bellare and McCallum, 2007; Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Yao et al., 2010). In contrast to these systems, our distant supervision NED system provides a meta-algorithm for generating an NED system for any database and any entity type. Existing domain adaptation or transfer learning approaches are inappropriate for the Open-DB NED task, either because they require labeled data in both the source and target domains (Daum´e III et al., 2010; Ben-David et al., 2010), or because they leverage some notion of distributional similarity between words in the source and target domains (Blitzer et al., 2006; Huang and Yates, 2009), which does not apply to the database symbols ac</context>
</contexts>
<marker>Yao, Riedel, McCallum, 2010</marker>
<rawString>Limin Yao, Sebastian Riedel, and Andrew McCallum. 2010. Collective cross-document relation extraction without labelled data. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing (EMNLP-2010), pages 1013–1023.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yiping Zhou</author>
<author>Lan Nie</author>
<author>Omid Rouhani-Kalleh</author>
<author>Flavian Vasile</author>
<author>Scott Gaffney</author>
</authors>
<title>Resolving surface forms to wikipedia topics.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics (Coling),</booktitle>
<pages>1335--1343</pages>
<contexts>
<context position="4989" citStr="Zhou et al. (2010)" startWordPosition="774" endWordPosition="777">NED with previous work. Section 3 formalizes the task. Sections 4 and 5 present our distant supervision strategy and domain-adaptation strategy, respectively. Section 6 introduces a technique that is a hybrid of the two learning strategies. Section 7 describes our experiments, and Section 8 concludes. 2 Previous Work As mentioned above, restricting the catalog of referents to Wikipedia, as most recent NED systems do (Bunescu and Pasca, 2006; Mihalcea and Csomai, 2007; Fader et al., 2009; Han and Zhao, 2009; Kulkarni et al., 2009; Ratinov et al., 2011), can restrict the coverage of the system. Zhou et al. (2010) estimate that 23% of names in Yahoo! news articles have no referent in Wikipedia, and Cucerzan (2007) estimates the rate at 16% in MSNBC news articles. There is reason to suspect that these estimates are on the low side, however, as news tends to cover popular entities, which are most likely to appear in Wikipedia; the mentions in TAC’s 2009 entity linking task are drawn from both newswire and blogs, and have a far higher rate (57%) of missing Wikipedia entries. Lin et al. (2012) find that 33% of mentions in a corpus of 500 million Web documents cannot be linked to Wikipedia. NED systems that</context>
<context position="34586" citStr="Zhou et al. (2010)" startWordPosition="5843" endWordPosition="5846">d hybrid strategies. The domain-adaptation model is trained on the labeled data for sports when testing on movies, and vice versa. We use a movies test set of 180 mentions that is separate from the development data used for the above tests. For the distant supervision strategy, we use the entire collection of texts from each domain as input (1300 articles for sports, 770 articles for movies), with the labels removed during training. We compare against a state-of-the-art Wikipedia NED system used in production by a major Web company. This system is a modified version of the system described by Zhou et al. (2010), where certain features have been removed for efficiency. We refer to this as the Modified-Zhou Wikifier. This system uses a gradient-boosted decision tree and multiple local and global features for computing the similarity between a mention’s context and a Wikipedia article. We also test a hypothetical system, Oracle Wikifier, which is given no information about entities in IMDB, but is assumed to be able to correctly resolve any mention that refers to an entity found in Wikipedia. Thus, this system has perfect accuracy on mentions that can be found in Wikipedia, and accuracy similar to a ba</context>
</contexts>
<marker>Zhou, Nie, Rouhani-Kalleh, Vasile, Gaffney, 2010</marker>
<rawString>Yiping Zhou, Lan Nie, Omid Rouhani-Kalleh, Flavian Vasile, and Scott Gaffney. 2010. Resolving surface forms to wikipedia topics. In Proceedings of the 23rd International Conference on Computational Linguistics (Coling), pages 1335–1343.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000055">
<title confidence="0.9882525">
Regularized Interlingual Projections:
Evaluation on Multilingual Transliteration
</title>
<author confidence="0.997467">
Jagadeesh Jagarlamudi Hal Daumé III
</author>
<affiliation confidence="0.999943">
University of Maryland University of Maryland
</affiliation>
<address confidence="0.860843">
College Park, USA, 20742 College Park, USA, 20742
</address>
<email confidence="0.999645">
jags@umiacs.umd.edu hal@umiacs.umd.edu
</email>
<sectionHeader confidence="0.998606" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999796583333333">
In this paper, we address the problem of build-
ing a multilingual transliteration system using
an interlingual representation. Our approach
uses international phonetic alphabet (IPA) to
learn the interlingual representation and thus
allows us to use any word and its IPA repre-
sentation as a training example. Thus, our ap-
proach requires only monolingual resources: a
phoneme dictionary that lists words and their
IPA representations.1 By adding a phoneme
dictionary of a new language, we can readily
build a transliteration system into any of the
existing previous languages, without the ex-
pense of all-pairs data or computation. We
also propose a regularization framework for
learning the interlingual representation, which
accounts for language specific phonemic vari-
ability, and thus it can find better mappings
between languages. Experimental results on
the name transliteration task in five diverse
languages show a maximum improvement of
29% accuracy and an average improvement of
17% accuracy compared to a state-of-the-art
baseline system.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99983375">
Because of the wide usage of English, many natu-
ral language processing (NLP) tasks have bilingual
resources from English into other languages. For ex-
ample, significantly larger parallel texts are available
</bodyText>
<footnote confidence="0.999452">
1It is arguable that getting words and their IPA representa-
tion require knowledge about both words and IPA symbols, but
it still is specific to one language and, in this sense, we refer to
it as a monolingual resource.
</footnote>
<page confidence="0.992948">
12
</page>
<bodyText confidence="0.999884235294118">
between English and other languages. Similarly,
bilingual dictionaries and transliteration data sets are
more accessible from a language into English than
into a different language. This situation has caused
the NLP community to develop approaches which
use a resource rich language (Q say English) as pivot
to build resources/applications between a new lan-
guage pair P and R. Previous studies in machine
translation (Utiyama and Isahara, 2007; Paul and
Sumita, 2011), transliteration (Khapra et al., 2010),
and dictionary mining (Saralegi et al., 2011) show
that these bridge language approaches perform com-
petitively with approaches that use resources be-
tween P and R. In this paper, we propose a regular-
ization framework for bridge language approaches
and show its effectiveness for name transliteration
task. The key idea of our approach is that it accounts
for language specific variation in the bridge lan-
guage resources (i.e. between P H Q and Q H R)
and aims to minimize this variation as much as pos-
sible. Though our technique is general, for clarity
we describe it in the context of named entity (NE)
transliteration.
Named entity (NE) transliteration involves
transliterating a name in one language into another
language and is shown to be crucial for machine
translation (MT) (Knight and Graehl, 1998; Al-
Onaizan and Knight, 2002; Hermjakob et al.,
2008; Li et al., 2009) and cross-lingual information
retrieval (CLIR) (AbdulJaleel and Larkey, 2003;
Mandl and Womser-Hacker, 2005; Udupa et al.,
2009). There exists a large body of literature in
transliteration, especially in the bilingual setting,
well summarized by Ravi and Knight (2009). We
</bodyText>
<note confidence="0.9580595">
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 12–23, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics
</note>
<table confidence="0.979538857142857">
English Bulgarian
Word IPA Word IPA
bashful /&apos;bæʃfol/ шi46аM /&apos;ʃibom/
tuesday /&apos;tuːzdeɪ/ nyK /luk/
craft /kɹæft/ KаK /kak/
book /bʊk/ Myзей /mʊ&apos;zej/
head /hEd/ criеKа /spE&apos;kɤ/
</table>
<tableCaption confidence="0.996203">
Table 1: Example phoneme dictionaries in English and
Bulgarian. The English translations for the Bulgarian
words are switch, onion, how, museum, and spekle.
</tableCaption>
<bodyText confidence="0.999946076923077">
summarize the approaches that are most relevant to
us in Sec. 5. In this paper, we operate in the context
of transliteration mining (Klementiev and Roth,
2006; Sproat et al., 2006) where we assume that we
are given a source language name and a list of target
language candidate transliterations and the task is to
identify the correct transliteration.
Given a set of l languages, we address the prob-
lem of building a transliteration system between
every pair of languages. A straight forward su-
pervised learning approach would require training
data of name pairs between every pair of languages
(Knight and Graehl, 1998) or a set of common
names transliterated from every language into a
pivot language. Though it is relatively easy to ob-
tain names transliterated into a pivot language (such
as English), it is unlikely that such data sets contain
the same names. Bridge language approaches over-
come the need for common names and build translit-
eration systems for resource poor languages (Khapra
et al., 2010). However, such approaches still require
training data consisting of bilingual name translit-
erations (orthographic name-to-name mappings). In
this paper, we relax the need for name translitera-
tions by using international phonetic alphabet (IPA)
in a manner akin to a “bridge language.”
</bodyText>
<sectionHeader confidence="0.997578" genericHeader="method">
2 IPA for Transliteration
</sectionHeader>
<bodyText confidence="0.999962679245283">
We assume that we have a list of words and their
IPA representations in each of the l languages. The
words in different languages need not have any rela-
tionship to each other. Table 1 shows few words and
their IPA representations in English and Bulgarian
languages. We refer to the set of (word, IPA) pairs
as phoneme dictionary in this paper. Notice that the
common symbols in the IPA sequences indicate a
vague phonetic correspondence between the charac-
ter sequences of English and Bulgarian. For exam-
ple, both the words ‘bashful’ and ‘шi46аM’ have the
symbol ‘ʃ’ in their IPA sequences which indicate a
possible mapping between the character sequences
‘sh’ and ‘ш’.
The use of IPA as the bridge language offers mul-
tiple advantages. As shown in Table 1, it allows us
to include any (word, IPA) pair in the training data
and thus it relaxes the need for name pairs as the
training data. Since we only need a phoneme dic-
tionary in each language, our approach does not re-
quire any bilingual resources to build the transliter-
ation system. Moreover, since our training data can
contain any word (not only the NEs), it is easier to
obtain such a resource, for e.g. the phoneme dic-
tionaries obtained from Wiktionary contain at least
2000 words in 21 languages and we will see in Sec. 6
that we can build a decent transliteration system with
2000 words.2 Finally, unlike other transliteration ap-
proaches, by simply adding a phoneme dictionary of
(l+1)st language we can readily get a transliteration
system into any of the existing l languages and thus
avoid the need for all-pairs data or computation.
Using IPA as the bridge language poses some
new challenges such as the language specific phone-
mic inventory. For example, Mandarin doesn’t
have /v/, so it is frequently substituted with /w/ or
/f/. Similarly, !Xóõ (Southern Khoisan, spoken in
Botswana) has 122 consonants, mostly consisting
of a large inventory of different word-initial click
sounds (Haspelmath et al., 2005), many of which
do not exist in any other documented languages.
Besides this language specific phonemic inventory,
names have different IPA representations in differ-
ent languages. For example, as shown in Table 2,
the IPA sequences for ‘China’ in English and Dutch
have common IPA symbols but the English IPA se-
quence has additional symbols. Moreover, a name
can have multiple pronunciations with in a language,
e.g. ‘France’ has two different IPA sequences in En-
glish (Table 2).
In order to handle this phonemic diversity, our
method explicitly models language-specific variabil-
ity and attempts to minimize this phonemic variabil-
</bodyText>
<footnote confidence="0.9844945">
2In our experiments, we consider languages with small
(2000) and big (&gt;30K) phoneme dictionaries.
</footnote>
<page confidence="0.997525">
13
</page>
<table confidence="0.9989855">
Word IPA sequence
China /&apos;tʃaɪ.nə/ (En), /&apos;ʃina/ (Du), /&apos;çiːnaː/ (De)
America /ə&apos;mErɪkə/ (En), /a&apos;me.ri.ka/ (Ro)
France /&apos;fɹciːns/ (En), /&apos;fɹænts/ (En), /frscĩs/ (Fr)
</table>
<tableCaption confidence="0.96580325">
Table 2: IPA sequences of few words in different lan-
guages indicated using language codes in the parenthesis
(‘En’ for English, ‘Du’ for Dutch, ‘De’ for German, ‘Ro’
for Romanian, and ‘Fr’ for French).
</tableCaption>
<bodyText confidence="0.999982333333333">
ity as much as possible. At a high level, our ap-
proach uses the phoneme dictionaries of each lan-
guage to learn mapping functions into an interlin-
gual representation (also referred as common sub-
space). Subsequently, given a pair of languages, a
query name in one of the languages and a list of
candidate transliterations in the other language, we
use the mapping functions of those two language to
identify the correct name transliteration. The map-
ping functions explicitly model the language specific
variability and thus account for fine grained differ-
ences. Our experimental results on four language
pairs from two different language families show a
maximum improvement of 29% accuracy and an av-
erage improvement of 17% accuracy compared to
a state-of-the-art baseline approach. An important
advantage of our approach is that, it extends eas-
ily to more than two languages and in fact adding
phoneme dictionary from a different, but related,
language improves the accuracies of a given lan-
guage pair. Our main contributions are: 1) build-
ing a transliteration system using (word, IPA) pairs
and hence using only monolingual resources and 2)
proposing a regularization framework which is more
general and applies to other bridge language applica-
tions such as lexicon mining (Mann and Yarowsky,
2001).
</bodyText>
<sectionHeader confidence="0.957967" genericHeader="method">
3 Low Dimensional Projections
</sectionHeader>
<bodyText confidence="0.99993924137931">
Our approach is inspired by the Canonical Correla-
tion Analysis (CCA) (Hotelling, 1936) and its appli-
cation to transliteration mining (Udupa and Khapra,
2010).
First, we convert the phoneme dictionary of each
language into feature vectors, i.e. we convert each
word into a feature vector of n-gram character se-
quences and similarly, we also, convert the IPA
representations into feature vectors of n-gram IPA
symbol sequences. For example, if we use uni-
gram and bigram sequences as features, then the
feature vectors of ‘head’ and its IPA sequence
`hEd&apos; are given by {h, e, a, d, #h, he, ea, ad, d$l
and {h,E, d, #h, hE,Ed, d$l. For brevity, we refer
to the spaces of n-gram character and IPA symbol
sequences as character and phonemic spaces respec-
tively. The character space is specific to each lan-
guage while the phonemic space is shared across all
the languages. Since we use IPA as bridge, even
though two languages share orthography (e.g. En-
glish and French) it is irrelevant for our approach.
Then, for each language, we find mappings (Ai
and Ui) from the character and phonemic spaces
into a common k-dimensional subspace such that the
correct transliterations lie closer to each other in this
subspace. Before moving into the details of our ap-
proach, we will describe the notation and then give
an overview of the process by which our approach
finds the transliteration.
</bodyText>
<subsectionHeader confidence="0.984509">
3.1 Notation
</subsectionHeader>
<bodyText confidence="0.973556083333333">
Letx(-) iE Rdi and p(-)
i E R&apos; be the feature vec-
tors of the mth word and its IPA sequence in the
ith(1 · · · l) language, where di is the size (i.e. no. of
features) of the character space of the language and
c is the size of the common phonemic space. Let
Xi (di x ni) and Pi (c x ni) denote the ith language
data matrices with x(-) iand p(-)
i m = 1 · · · ni as the
columns respectively. We consistently use subscript
to indicate the language and superscript to indicate
the index of an example point.
</bodyText>
<subsectionHeader confidence="0.999963">
3.2 Method Overview
</subsectionHeader>
<bodyText confidence="0.953539">
During the training stage, for each language, we find
mappings (or projection directions) Ai E R(dixk)
and Ui E R(&apos;xk) from the character and phonemic
spaces into a k-dimensional subspace (or an interlin-
gual representation) such that a name gets mapped
to the same k-dimensional vector irrespective of the
language. That is, given a name xi it gets mapped
to the vector AT xi and similarly its IPA sequence
pi gets mapped to UT pi. During the testing stage,
given a name xi in the source (ith) language, we find
its transliteration in the target (jth) language xj by
solving the following decoding problem:
arg min L(xi,xj) (1)
xi
</bodyText>
<page confidence="0.994472">
14
</page>
<figureCaption confidence="0.8769895">
Figure 1: A single name (Gandhi) is shown in all the in-
put feature spaces. The alignment between the character
</figureCaption>
<bodyText confidence="0.987951739130435">
and phonemic space is indicated with double dimensional
arrows. Bridge-CCA uses a single mapping function U
from the phonemic space into the common subspace (the
2-dimensional green space at the top), where as our ap-
proach uses two mapping functions U1 and U2, one for
each language, to map the IPA sequences into the com-
mon subspace.
where L(xi7 xj) is given by
min IAT xi - UT p�2 + �A�j xj - Uj p�2 (2)
pERc
This formulation uses the source language mappings
(Ai and Ui) to find the IPA sequence p that is clos-
est to the source name and then uses it, along with
the target language mappings (Aj and Uj), to iden-
tify the correct transliteration from a list of candidate
transliterations.
At a high level, existing bridge language ap-
proaches such as Bridge-CCA (Khapra et al., 2010)
assume that Ui - Uj thus ignoring the language
specific variation. To understand its implication
consider the example shown in Fig. 1. The mid-
dle portion of the Fig. shows the name Gandhi
(represented as point) in the character spaces of
English and Hindi, three-dimensional spaces, and
its IPA sequences in the phonemic space (the two-
dimensional space in the middle). Notice that, be-
cause of the phonemic variation, the same name is
represented by two distinct points in the common
phonemic space.3 Now, since Bridge-CCA uses a
single mapping function for both the IPA sequences,
it fails to map these two distinct points into a com-
mon point in the interlingual subspace.
Our new formulation, as explained above, relaxes
this hard constraint and learns different mapping
functions (Ui and Uj) and hence our approach can
potentially map both the distinct IPA sequences into
a single point. As a result our approach success-
fully handles the language specific phonemic vari-
ation. At the same time we constrain the projec-
tion directions such that they behave similarly for
the phonemic sounds that are observed in majority
of the languages. In the example shown in Fig. 1,
our model (called Regularized Projections) finds two
different mapping functions U1 and U2, one for each
language, from the phonemic space into the com-
mon two-dimensional space at the bottom.
</bodyText>
<subsectionHeader confidence="0.994362">
3.3 Regularized Projections
</subsectionHeader>
<bodyText confidence="0.91814647826087">
In this section we first formulate the problem of find-
ing the mapping functions (Ai and Ui) of each lan-
guage as an optimization problem. In the following
section (Sec. 4), we develop a method for solving the
optimization problem and also derive closed form
solution for the prediction problem given in Eq. 1.
For simplicity, we describe our approach in terms of
single projection vectors, ai E Rdz and ui E R1,
rather than full matrices, but the generalization is
trivial.
Inspired by the Canonical Correlation Analysis
(CCA) (Hotelling, 1936), we find projection direc-
tions in the character and phonemic spaces of each
language such that, after projection, a word is closer
to its aligned IPA sequence. To understand this, as-
sume that we have a name (say “Barack Obama”) in
all the languages4 and its feature vectors are given
by xi and pi i = 1 · · · l in the character and phone-
3In reality, as explained in the previous section, the phone-
mic variation that is commonly observed is that different fea-
tures are triggered for different languages. But for visualization
purpose, we showed the IPA sequences as if they differ in the
feature values.
</bodyText>
<footnote confidence="0.9754995">
4Our model does not require same names in different lan-
guages; this is used only for easier understanding.
</footnote>
<page confidence="0.997533">
15
</page>
<bodyText confidence="0.999743666666667">
The constraints of the above optimization problem
avoid the trivial solution of setting all the vectors to
zero and are referred to as length constraints.
mic spaces respectively. Then, we might try to find
projection directions az in each language and u in
the common phonemic space such that:
</bodyText>
<equation confidence="0.7339476">
arg min
ai,u
∑l
z=1
((xz, az) − (pz, u)) 2 (3)
</equation>
<bodyText confidence="0.9999771">
where (·, ·) denotes the dot product between two
vectors. This model assumes that the projection di-
rection u is same for the phonemic space of all the
languages. This is a hard constraint and does not
handle the language specific variability as discussed
in the previous section. We model the language
specificity by relaxing this hard constraint.
In our model, intuitively, the parameters corre-
sponding to the phonemic sounds that occur in ma-
jority of the languages are shared across the lan-
guages while the parameters of the language spe-
cific sounds are modeled per each language. This
is achieved by modeling the projection directions of
the ith language phonemic space uz +— u + rz. The
vector u E R&apos; is common to the phonemic spaces
of all the languages and thus handles sounds that
are observed in multiple languages while rz E R&apos;,
the residual vector, is specific to each language and
accounts for the language specific phonemic varia-
tions. Then the new formulation is given by:
</bodyText>
<sectionHeader confidence="0.97393" genericHeader="method">
4 Model Optimization
</sectionHeader>
<bodyText confidence="0.999909">
In this section, we derive the solutions for the opti-
mization problems presented in the previous section.
</bodyText>
<subsectionHeader confidence="0.994748">
4.1 Training the Model
</subsectionHeader>
<bodyText confidence="0.99880575">
We follow the standard procedure of forming the La-
grangian and setting its derivative to zero. The La-
grangian L of the optimization problem in Eq. 4 is
given by:
</bodyText>
<equation confidence="0.99347025">
+α ( ∑
z nz nz
1 llXTzazll2−1)+β(∑ 1 llPzTull2−1)
z
</equation>
<bodyText confidence="0.9990665">
where α and β are Lagrangian multipliers corre-
sponding to the length constraints. Differentiating L
with respect to az, rz and u and setting the derivatives
to zero yields the following equations, respectively:
</bodyText>
<equation confidence="0.736821136363636">
(1 + α)XzXTzaz − XzPzTrz = XzPzTu
−PzXTzaz + (1 + λnz)PzPzTrz = −PzPzTu
∑
L =
z
1 ∑
||XT z az−P z T(u+rz)||2+λ llPzTrzll2
n z
z
ll(xz, az) − (pz, u + rz)ll2 + λ(pz, rz)2
∑l
z=1
∑
z
1(PzXzTaz−PzPzTrz)
= (1+β) ∑1
PzPzT
nz
z
nz
arg min
ai,u,ri
</equation>
<bodyText confidence="0.994023896551724">
where λ is the residual parameter. The first term of
this summation ensures that a word and its IPA se-
quence gets mapped to closer points in the subspace
while the second term forces the residual vectors to
be as small as possible. By enforcing the residual
vectors to be small, this formulation encourages the
sounds that occur in majority of the languages to be
accounted by u and the sounds that are specific to the
given language by rz. The final optimization prob-
lem is obtained by summing these terms over all the
examples and all the languages and is given by:
u
We can rewrite these equations in matrix form, as
shown in Eqs. 5 and 6, since the solution becomes
clear in this form. For brevity, let Ez = (1 +
α)XzXTz, Fz = −XzPzT and Gz = (1 + λnz)PzPzT.
Then, u can be solved for using the generalized
eigenvalue problem shown in Eq. 7. This step in-
volves computing an inverse of a (dz +c) matrix and
an eigenvalue problem of size c which can be ex-
pensive since solving each of these problems involve
cubic time. This can be reduced further into a prob-
lem of smaller size by using inverse of a partitioned
matrix as shown in Eq. 8. This identity reduces the
matrix inverse computation from a problem of size
dz + c into two smaller problems of size dz and c
each. This reduces the time complexity considerably
since the inverse computation is cubic in the size of
the matrix.
</bodyText>
<figure confidence="0.952841875">
(||XT z az −P z T (u+rz)||2 +λ llPzTrzll2) (4)
nz
s.t. ∑l 1llXTz azll2 = 1 and ∑l 1llPzTull2=1
z=1 nz z=1 nz
∑l
z=1
min
ai,u,ri
</figure>
<page confidence="0.81849">
16
</page>
<equation confidence="0.988921903225807">
[(1 + α)XiXT −XiPT ai = XiPiT l u 5
−Pi XTi (1 + λni)PiPT] [ri −PiPTJ ( ) i
] [ai ]
1 ∑
[PiXT 1
i −PiP T = (1 + β) PiPi T u (6)
i
ni ri ni
i
[ ] [Ei Fi ]−1 [ −Fi ] ∑1
−F T −Gi PiPT
i 1+λni F T −Gi u = (1 + β) i u (7)
i Gi ni
1+λni i
]−1
If Mi = (Ei − FiG−1
i FT i
)−1, then [ Ei Fi [ Mi ]
−MiFiG−1
= (8)
i Fi T Gi −G−1
i F i T Mi G−1
i + G−1
i F i T MiFiG−1
i
∑
i
1
ni
∑
i
</equation>
<bodyText confidence="0.696706">
Substituting Eq. 8 into Eq. 7 and further simplify-
ing results in the following eigenvalue problem for
solving u:
</bodyText>
<equation confidence="0.865116333333333">
∑ Gi + (λni)2FiTMiFi ∑ PiPiT
i ni(1 + λni)2 u = (1 + β) u ni
i
</equation>
<bodyText confidence="0.9207075">
where Mi = (Ei − FiG−1
i FT )−1. Notice that the
i
term Ei = (1+α)XiXTi depends on the Lagrangian
multiplier α. Because of this, we cannot solve for
both the parameters and the Lagrangian multipliers
at the same time. One possible approach is to do an
alternate optimization over the parameters and La-
grangian multipliers, but in this paper we fix α and
solve for u. The value of α denotes the correlation
and its maximum value is 1. In practice, we often
observe that the top few correlations take the value
of 1. Based on this observation we fix the value of α
to 1 (Sec. 6).
Subsequently, we use u to solve for ai and ri as
follows:
</bodyText>
<equation confidence="0.9995176">
λniMiFi u (9)
1 + λni
λniG−1
i Fi T MiFi − I
1 + λni
</equation>
<bodyText confidence="0.9999855">
In order to increase the stability of the system we
regularize Gi and Ei by adding τI. We use the top k
eigenvectors u and their corresponding ai and ri vec-
tors as columns and form the mappings U, Ai and Ri
respectively. These mappings are used in predicting
the transliteration of a name in one language into
any other language, which will be described in the
following section.
</bodyText>
<subsectionHeader confidence="0.943687">
4.2 Transliteration Mining (Prediction)
</subsectionHeader>
<bodyText confidence="0.999520230769231">
During the testing phase, given a source name and
a list of candidate transliterations, we solve the de-
coding problem shown in Eq. 1 to find the appropri-
ate target language transliteration. Formally, given
a word xi in ith language we find its transliteration
into jth language xj, by solving the optimization
problem shown in Eq. 1, where Ui = U + Ri and
Uj = U + Rj. Similar to the previous case, the
closed form solution can be found by computing the
first derivative with respect to the unknown phoneme
sequence and the target language transliteration and
setting it to zero. First, the IPA sequence p∗ that
minimizes L(xi, xj) is given by:
</bodyText>
<equation confidence="0.9828355">
p∗ = C−1(UiAT i xi + UjAT j xj) (11)
ij
</equation>
<bodyText confidence="0.999935666666667">
where Cij = UiUTi +UjUTj . We substitute this back
in Eq. 2 and then solve for xj, the best transliteration
in the jth language, as:
</bodyText>
<equation confidence="0.885785">
)AT
Aj (I−UT j C−1
ij Uj j xj = AjUT j C−1
ij UiAT i xi (12)
</equation>
<bodyText confidence="0.999580666666667">
Since Ui and Uj are not full rank matrices, to in-
crease the numerical stability of the prediction step,
we use Cij = UiUTi +UjUjT +0.001 I where I is an
identity matrix. Notice that this solution doesn’t de-
pend on the p∗ and hence we don’t need to compute
it explicitly.
</bodyText>
<sectionHeader confidence="0.999978" genericHeader="method">
5 Related Work
</sectionHeader>
<bodyText confidence="0.99996325">
There is a large body of the literature in named entity
transliteration, so we will describe only the most rel-
evant ones to our approach. In transliteration, gener-
ative approaches aim to generate the target language
</bodyText>
<equation confidence="0.999594333333333">
ai =
ri =
u (10)
</equation>
<page confidence="0.992207">
17
</page>
<bodyText confidence="0.999965960784314">
transliteration of a given source name (Knight and
Graehl, 1998; Jung et al., 2000; Haizhou et al., 2004;
Al-Onaizan and Knight, 2002) while discriminative
approaches assume a list of target language names,
obtained from other sources, and try to identify the
correct transliteration (Klementiev and Roth, 2006;
Sproat et al., 2006). The effectiveness of the dis-
criminative approaches depend on the list of target
language candidates. Sproat et al. (2006) report an
oracle accuracy of 85%, but it depends on the source
of the candidate transliterations. Nevertheless, all
these approaches require either bilingual name pairs
or phoneme sequences to learn to transliterate be-
tween two languages. Thus, if we want to build
a transliteration system between every pair of lan-
guages in a given set of languages then these ap-
proaches need resources between every pair of lan-
guages which can be prohibitive.
Bridge language approaches propose an alterna-
tive and use a resource rich language such as English
as common language (Khapra et al., 2010) but they
still need bilingual resources. Moreover Bridge-
CCA (Khapra et al., 2010) uses a single mapping
function for the phonemic space of all the languages
and thus it can not handle language specific variabil-
ity. In the original setting, authors use English as the
pivot and since the feature space of English is fixed,
irrespective of the target language, this may not be a
serious concern but it becomes crucial when we use
IPA as the bridge language.
Approaches that map words in different languages
into the common phonemic space have also been
well studied. But most of these approaches use lan-
guage specific resources such as CMU pronuncia-
tion dictionary (Gao et al., 2004) or a carefully con-
structed cost matrices for addition, substitution, and
deletion of phonemes between a pair of languages
(Tao et al., 2006; Yoon et al., 2007). Variants of
soundex algorithm (Odel and Russel, 1918) such as
Kodex (Kang and Choi, 2000) use hand constructed
consonant to soundex code tables for name translit-
eration. Similar to our approach these variants only
require soundex mappings of a new language to
build transliteration system, but our model does not
require explicit mapping between n-gram characters
and the IPA symbols instead it learns them auto-
matically using phoneme dictionaries. Alternatively
unsupervised approaches have also been explored
(Ravi and Knight, 2009), but their accuracies are
fairly low compared to the supervised and weekly
supervised approaches.
</bodyText>
<sectionHeader confidence="0.999422" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<bodyText confidence="0.999856423076923">
Our experiments are designed to evaluate the follow-
ing three aspects of our model, and of our approach
to transliteration in general:
IPA as bridge: Unlike other phonemic based ap-
proaches (Sec. 5), we do not explicitly model the
phoneme modifications between pairs of languages.
Moreover, the phoneme dictionary in each language
is crawled from Wiktionary (Sec. 6.1), which is
likely to be noisy. So, the first aspect we want
to evaluate is the effectiveness of using IPA as the
bridge language. Here, we also compare our method
with other bridge language approaches and establish
the importance of modeling language specific vari-
ance.
Multilinguality: In our method, simply adding a
phoneme dictionary of a new language allows us to
extend our transliteration system into any of the ex-
isting languages. We evaluate the effect of data from
a different, but related, languages on a transliteration
system between a given pair.
Complementarity: Using IPA as bridge language
allows us to build transliteration system into re-
source poor languages. But we also want to eval-
uate whether such an approach can help improving
a transliteration system trained directly on bilingual
name-pairs.
</bodyText>
<subsectionHeader confidence="0.997968">
6.1 Data Sets
</subsectionHeader>
<bodyText confidence="0.999851071428571">
We use data sets from five languages in order to eval-
uate the effectiveness of our approach. The phoneme
dictionaries (list of words and their IPA represen-
tations as shown in Table 1) are obtained from
Wiktionary. The Wiktionary dump downloaded in
October 2011 has at least 2000 (word, IPA) pairs
in 21 languages which also includes some resource
poor languages (e.g. Armenian, Taiwanese, Turkish,
etc.).
In principle, our method allows us to build
transliteration system into any of these language
pairs without any additional information. But, in this
paper, we use English (En), Bulgarian (Bg), Rus-
sian (Ru), French (Fr), and Romanian (Ro) for eval-
</bodyText>
<page confidence="0.997179">
18
</page>
<table confidence="0.99481">
En. Bg. Ru. Fr. Ro.
Train 31K 36K 1141 36K 5211
Dev. – 1264 2000 2717 430
Test – 1264 2000 2717 431
# Features 5000 3998 2900 5000 3465
</table>
<tableCaption confidence="0.67657175">
Table 3: Statistics of different data sets. Training
data is monolingual phoneme dictionaries while develop-
ment/test sets are bilingual name pairs between English
and the respective language.
</tableCaption>
<bodyText confidence="0.999842">
uation purposes, as they suffice to showcase all the
three aspects mentioned in the previous section. Ta-
ble 3 shows the sizes of phoneme dictionaries used
for training the models. The phoneme dictionar-
ies of English, Bulgarian, and Russian contain more
than 30K (word,IPA) pairs while the remaining two
languages have smaller phoneme dictionaries. The
development and test sets between English and the
remaining language pairs are obtained from geon-
ames data base.5 These are geographic location
names from different countries written in multiple
languages.
</bodyText>
<subsectionHeader confidence="0.998588">
6.2 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.999930285714286">
We convert the phoneme dictionaries of each lan-
guage into feature vectors. We use unigram and
bigram features in the phonemic space and uni-
gram, bigram and trigram features in the character
space. An example for feature generation is shown
in Sec. 3. After converting the data into feature vec-
tors, we retain the most frequent 5000 features. We
only keep the frequent 5000 features since we ob-
served, elsewhere, that including infrequent features
leads CCA based methods to learn projection direc-
tions with perfect correlations which are not effec-
tive for downstream applications. The last row of
Table 3 shows the number of features in the char-
acter space of each of the languages. The phone-
mic space is common to all the languages and has
3777 features. Though the phonemic features are
common to all the languages, as discussed in Sec. 2,
only a subset of features will be observed in a given
language. For example, in our data sets, of the total
3777 common phonetic features only 3312, 882, and
1009 features are observed in English, Bulgarian,
</bodyText>
<footnote confidence="0.72135">
5http://www.geonames.org/
</footnote>
<figure confidence="0.6955375">
0 10 20 30 40 50 60 70 80 90 100
lambda
</figure>
<figureCaption confidence="0.999271333333333">
Figure 2: Performance of transliteration system with
residual parameter A on English-Bulgarian development
data set.
</figureCaption>
<bodyText confidence="0.999823523809524">
and Russian languages respectively. This indicates
the diversity in the phonemic inventory of different
languages.
We compare our approach against Bridge-CCA, a
state-of-the-art bridge language transliteration sys-
tem which is known to perform competitively with
other discriminative approaches (Khapra et al.,
2010). We use the phoneme dictionaries in each lan-
guage to train our approach, as well as the baseline
system. The projection directions learnt during the
training are used to find the transliteration for a test
name as described in Sec. 4.2. We report the perfor-
mance in terms of the accuracy (exact match) of the
top ranked transliteration and the mean reciprocal
rank (MRR) of the correct transliteration. We find
transliterations in both the directions (i.e. target lan-
guage transliterations given a source name and vice
versa) and report average accuracies. The regular-
ization parameter (T) and the size of the interlingual
representation (k) in both our approach and Bridge-
CCA are tuned on the development set.
</bodyText>
<subsectionHeader confidence="0.999892">
6.3 Description of Results
</subsectionHeader>
<bodyText confidence="0.996579">
In this section we report experimental results on the
three aspects mentioned above.
</bodyText>
<subsectionHeader confidence="0.823251">
6.3.1 IPA as Bridge
</subsectionHeader>
<bodyText confidence="0.9241365">
Fig. 2 shows the performance of our system with
the residual parameter A (in Eq. 4) on the develop-
</bodyText>
<figure confidence="0.994312222222222">
0.9
0.85
0.8
0.75
0.7
0.65
0.6
Valid. Acc.
Valid. MRR
</figure>
<page confidence="0.984755">
19
</page>
<table confidence="0.999299285714286">
En-Bg En-Ru En-Fr En-Ro
Acc. MRR Acc. MRR Acc. MRR Acc. MRR
1 Bridge-CCA 68.83 77.22 44.50 53.22 41.55 52.89 71.69 79.59
2 Ours (cosine) 67.68 76.52 45.07 53.63 42.45 53.06 74.13 81.28
3 Ours (Eq. 12) 83.70 88.32 63.47 73.01 70.68 78.13 77.38 84.22
4 Ours (cosine + Multi.) 68.91 77.44 49.15 57.20 42.55 53.02 77.49 84.04
5 Ours (Eq. 12 + Multi.) 84.45 88.43 66.70 75.85 71.09 78.43 77.49 84.04
</table>
<tableCaption confidence="0.959523">
Table 4: Results of our approach and the baseline system on the test set. The second block shows the results when our
approach is trained only on phoneme dictionaries of the language pair, the third block shows results when we include
other language data as well.
</tableCaption>
<bodyText confidence="0.995419678571429">
ment data set. When A is small, the model does not
attempt to constrain the projection directions Ui’s
and hence they tend to map names to completely
unrelated vectors. As we increase the residual pa-
rameter, it forces the residual vectors (Ri) to be
smaller and thus the subspaces identified for each
language are closely tied together. Thus, it models
the commonalities across languages and also the lan-
guage specific variability. Based on the performance
curves on the development data, we fix A = 50 in the
rest of the experiments.
Table 4 shows the results of Bridge-CCA and our
approach on the four language pairs. We report the
results of our approach with the decoding proposed
in Sec. 4.2 and a simple cosine similarity measure
in the common-subspace, i.e. cos (AT ).
i xi, AT j xj
Comparison of the accuracies in rows 1, 2 and 3,
shows that simply using cosine similarity performs
almost same as the Bridge-CCA approach. How-
ever, using the decoding suggested in Eq. 12 gives
significant improvements. To understand why the
cosine angle between ATi xi and ATj xj is not the ap-
propriate measure, assume that the vectors xi and
xj are feature vectors of same name in two lan-
guages and let p be its true IPA representation. Then,
since our model learns projection directions such
that ATi xi � UTi p,
</bodyText>
<equation confidence="0.743878">
cos(ATi xi,ATj xj) = cos ((U +Ri)Tp, (U +Rj)Tp)
</equation>
<bodyText confidence="0.999995263157895">
The additional residual matrices Ri and Rj make
the cosine measure inappropriate. At the same time,
our model forces the residual matrices to be small
and this is probably the reason why it performs
competitively with the Bridge-CCA. On the other
hand, our decoding method, as shown in Eq. 1, in-
tegrates over the best possible phoneme sequence
and thus yields significant improvements. In the rest
of the paper, we report results with the decoding
in Eq. 12 unless specified explicitly. Our approach
achieves a maximum improvement of 29.13% ac-
curacy over Bridge-CCA in English-French and on
an average it achieves 17.17% and 15.19% improve-
ment in accuracy and MRR respectively. Notice that
even though our Russian phoneme dictionary has
only 1141 (word, IPA) pairs, our approach is able
to achieve an accuracy of 63.47% and an MRR of
73% indicating that the correct name transliteration
is, on an average, at rank 1 or 2.
</bodyText>
<subsectionHeader confidence="0.797738">
6.3.2 Multilinguality
</subsectionHeader>
<bodyText confidence="0.999981071428571">
The fourth and fifth rows of Table 4 also show the
multilingual results. In particular, we train our sys-
tem on data from the three languages En, Bg, and
Ru and test it on En-Bg and En-Ru test sets. Simi-
larly, we train a different system on data from En, Fr
and Ro and evaluate it on En-Fr and En-Ro test sets.
We split the languages based on the language family,
Russian and Bulgarian are Slavonic languages while
French and Romanian are Romance languages, and
expect that languages in same family have similar
pronunciations. Comparing the performance of our
system with and without the multilingual data set, it
is clear that having data from other languages helps
improve the accuracy.
</bodyText>
<subsectionHeader confidence="0.690197">
6.3.3 Complementarity
</subsectionHeader>
<bodyText confidence="0.9999766">
In the final experiment, we want to compare
the performance of our approach, which uses only
monolingual resources, with a transliteration system
trained using bilingual name pairs. We train a CCA
based transliteration system (Udupa and Khapra,
</bodyText>
<page confidence="0.969613">
20
</page>
<table confidence="0.999406571428571">
En-Bg En-Fr
Acc. MRR Acc. MRR
CCA 95.57 96.76 95.82 96.67
Ours+CCA 95.69 96.90 96.14 96.90
∆ Err 2.7% 4.2% 7.5% 6.8%
Ours+CCA(t) 95.80 96.95 96.34 97.04
∆ Err 5.4% 5.8% 12.3% 11.3%
</table>
<tableCaption confidence="0.9853076">
Table 5: Comparison with a system trained on bilingual
name pairs. The (t) in the third row indicates parame-
ters are tuned for test set. We also show the percentage
error reduction achieved by a linear combination of our
approach and CCA.
</tableCaption>
<bodyText confidence="0.999667909090909">
2010) on a training data of 3792 and 8151 location
name pairs. Notice that the training and test data for
this system are from the same domain and thus it has
an additional advantage over our approach, which is
trained on whatever happens to be on Wiktionary.
The second row of Table 5 shows the results of
CCA on English-Bulgarian and English-French lan-
guage pairs. CCA achieves high accuracies even
though the training data is relatively small, most
likely because of the domain match between train-
ing and test data sets. As another baseline, we tried
using Google machine translation API to transliter-
ate the English names of the En-Bg test set. We
hoped that since these are names, the translation en-
gine would simply transliterate them and return the
result. Of the output, we observed that about 500
names are passed through the MT system unchanged
and so we ignore them. On the remaining names,
it achieved an accuracy of 76.15% and the average
string edit distance of the returned transliteration to
the true transliteration is about 3.74. These accura-
cies are not directly comparable to the results shown
in Table 5 because, presumably, it is a transliteration
generation system unlike CCA which is a transliter-
ation mining approach. For lack fair comparison, we
don’t report the accuracies of the Google transliter-
ation output in Table 5.
Table 5 also shows the results of our system when
combined with the CCA approach. For a given En-
glish word, we score the candidate transliterations
using our approach and then linearly combine their
scores with the scores assigned by CCA. We per-
form a line search between [0, 1] for the appropriate
weight combination. The third and fourth rows of
Table 5 show the results of the linear combination
when the weight is tuned for the development and
test sets respectively. The improvements, though
not significant, are encouraging and suggest that a
sophisticated way of combining these different sys-
tems may yield significant improvements. This ex-
periment shows that a transliteration system trained
on word and IPA representations can actually aug-
ment a system trained on bilingual name pairs lead-
ing to an improved performance.
</bodyText>
<sectionHeader confidence="0.998877" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.99954475">
In this paper we proposed a regularization technique
for the bridge language approaches and showed
its effectiveness on the name transliteration task.
Our approach learns interlingual representation us-
ing only monolingual resources and hence can be
used to build transliteration system between re-
source poor languages. We show that, by account-
ing the language specific phonemic variation, we
can get a significant improvements. Our experimen-
tal results suggest that a transliteration system built
using IPA data can also help improve the accuracy
of a transliteration system trained on bilingual name
pairs.
Thought we used IPA as a bridge language there
are other viable options. For example, as shown
in Khapra et al. (2010) we can use English as the
bridge language. Since name transliteration prob-
lem is being studied for a considerable time, many
resources already exist between English and other
languages. So, one can argue the appropriateness of
IPA as bridge language compared to, say, English.
While this is an important question, in this paper,
we are primarily interested in showing the impor-
tance of handling language specific phenomenon in
the bridge language approaches. In future, we would
like to study the appropriateness of IPA vs. English
as the bridge language and also the generalizability
of our technique to other scenarios.
</bodyText>
<sectionHeader confidence="0.997754" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9924695">
This work is partially funded by NSF grant IIS-
1153487 and the BOLT program of the Defense
Advanced Research Projects Agency, Contract No.
HR0011-12-C-0015.
</bodyText>
<page confidence="0.998847">
21
</page>
<sectionHeader confidence="0.996226" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999486876190476">
Nasreen AbdulJaleel and Leah S. Larkey. 2003. Statis-
tical transliteration for english-arabic cross language
information retrieval. In Proceedings of the twelfth in-
ternational conference on Information and knowledge
management, CIKM ’03, pages 139–146, New York,
NY, USA. ACM.
Yaser Al-Onaizan and Kevin Knight. 2002. Machine
transliteration of names in arabic text. In Proceed-
ings of the ACL-02 workshop on Computational ap-
proaches to semitic languages, SEMITIC ’02, pages
1–13, Stroudsburg, PA, USA. ACL.
Wei Gao, Kam fai Wong, and Wai Lam. 2004. Phoneme-
based transliteration of foreign names for OOV prob-
lem. In Proceedings of the 1st International Joint Con-
ference on Natural Language Processing (IJCNLP),
pages 374–381.
Li Haizhou, Zhang Min, and Su Jian. 2004. A joint
source-channel model for machine transliteration. In
Proceedings of the 42nd Annual Meeting on Associa-
tion for Computational Linguistics, ACL ’04, Strouds-
burg, PA, USA. ACL.
Martin Haspelmath, Matthew Dryer, David Gil, and
Bernard Comrie, editors. 2005. The World Atlas of
Language Structures. Oxford University Press.
Ulf Hermjakob, Kevin Knight, and Hal Daumé III. 2008.
Name translation in statistical machine translation -
learning when to transliterate. In Proceedings ofACL-
08: HLT, pages 389–397, Columbus, Ohio, June.
ACL.
Harold Hotelling. 1936. Relation between two sets of
variables. Biometrica, 28:322–377.
Sung Young Jung, SungLim Hong, and Eunok Paek.
2000. An english to korean transliteration model of
extended markov window. In Proceedings of the 18th
conference on Computational linguistics - Volume 1,
COLING ’00, pages 383–389, Stroudsburg, PA, USA.
ACL.
Byung-Ju Kang and Key-Sun Choi. 2000. Two ap-
proaches for the resolution of word mismatch prob-
lem caused by english words and foreign words in ko-
rean information retrieval. In Proceedings of the 5th
international workshop on on Information retrieval
with Asian languages, IRAL ’00, pages 133–140, New
York, NY, USA. ACM.
Mitesh M. Khapra, Raghavendra Udupa, A. Kumaran,
and Pushpak Bhattacharyya. 2010. Pr + rq ;:t-.i pq:
Transliteration mining using bridge language. In Pro-
ceedings of the Twenty-Fourth AAAI Conference on
Artificial Intelligence, AAAI 2010, Atlanta, Georgia,
USA, July. AAAI Press.
Alexandre Klementiev and Dan Roth. 2006. Weakly
supervised named entity transliteration and discovery
from multilingual comparable corpora. In Proceed-
ings of the 21st International Conference on Compu-
tational Linguistics and the 44th Annual Meeting of
the Association for Computational Linguistics, ACL-
44, pages 817–824, Stroudsburg, PA, USA. ACL.
Kevin Knight and Jonathan Graehl. 1998. Machine
transliteration. Computational Linguistics, 24(4):599–
612.
Haizhou Li, A. Kumaran, Vladimir Pervouchine, and
Min Zhang. 2009. Report of news 2009 machine
transliteration shared task. In Proceedings of the 2009
Named Entities Workshop: Shared Task on Transliter-
ation, NEWS ’09, pages 1–18, Stroudsburg, PA, USA.
ACL.
Thomas Mandl and Christa Womser-Hacker. 2005. The
effect of named entities on effectiveness in cross-
language information retrieval evaluation. In Proceed-
ings of the 2005 ACM symposium on Applied comput-
ing, SAC ’05, pages 1059–1064, New York, NY, USA.
ACM.
Gideon S. Mann and David Yarowsky. 2001. Multipath
translation lexicon induction via bridge languages. In
Proceedings of the 2nd meeting of the North American
Chapter of the Association for Computational Linguis-
tics on Language technologies, NAACL ’01, pages 1–
8, Stroudsburg, PA, USA. ACL.
M. K. Odel and R. C. Russel. 1918. U.s. patent numbers,
1,261,167 (1918) and 1,435,663(1922).
Michael Paul and Eiichiro Sumita. 2011. Translation
quality indicators for pivot-based statistical mt. In
Proceedings of 5th International Joint Conference on
Natural Language Processing, pages 811–818, Chiang
Mai, Thailand, November. AFNLP.
Sujith Ravi and Kevin Knight. 2009. Learning phoneme
mappings for transliteration without parallel data. In
Proceedings of Human Language Technologies: The
2009 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 37–45, Boulder, Colorado, June. ACL.
Xabier Saralegi, Iker Manterola, and Iñaki San Vicente.
2011. Analyzing methods for improving precision of
pivot based bilingual dictionaries. In Proceedings of
the 2011 Conference on Empirical Methods in Natu-
ral Language Processing, pages 846–856, Edinburgh,
Scotland, UK., July. ACL.
Richard Sproat, Tao Tao, and ChengXiang Zhai. 2006.
Named entity transliteration with comparable corpora.
In Proceedings of the 21st International Conference on
Computational Linguistics and the 44th annual meet-
ing of the Association for Computational Linguistics,
ACL-44, pages 73–80, Stroudsburg, PA, USA. ACL.
Tao Tao, Su-Youn Yoon, Andrew Fister, Richard Sproat,
and ChengXiang Zhai. 2006. Unsupervised named
</reference>
<page confidence="0.973581">
22
</page>
<reference confidence="0.9975162">
entity transliteration using temporal and phonetic cor-
relation. In Proceedings of the 2006 Conference on
Empirical Methods in Natural Language Processing,
EMNLP ’06, pages 250–257, Stroudsburg, PA, USA.
ACL.
Raghavendra Udupa and Mitesh M. Khapra. 2010.
Transliteration equivalence using canonical correlation
analysis. In ECIR’10, pages 75–86.
Raghavendra Udupa, Saravanan K, Anton Bakalov, and
Abhijit Bhole. 2009. &amp;quot;they are out there, if you know
where to look&amp;quot;: Mining transliterations of oov query
terms for cross-language information retrieval. In Pro-
ceedings of the 31th European Conference on IR Re-
search on Advances in Information Retrieval, ECIR
’09, pages 437–448, Berlin, Heidelberg. Springer-
Verlag.
Masao Utiyama and Hitoshi Isahara. 2007. A com-
parison of pivot methods for phrase-based statisti-
cal machine translation. In Proceedings of Human
Language Technologies 2007: The Conference of the
North American Chapter of the Association for Com-
putational Linguistics; Proceedings of the Main Con-
ference, pages 484–491, Rochester, New York, April.
ACL.
Su-Youn Yoon, Kyoung-Young Kim, and Richard Sproat.
2007. Multilingual transliteration using feature based
phonetic method. In Proceedings of the 45th Annual
Meeting of the Association of Computational Linguis-
tics, pages 112–119, Prague, Czech Republic, June.
ACL.
</reference>
<page confidence="0.998919">
23
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.960143">
<title confidence="0.9983085">Regularized Interlingual Evaluation on Multilingual Transliteration</title>
<author confidence="0.976097">Jagadeesh Jagarlamudi Hal Daumé</author>
<affiliation confidence="0.999988">University of Maryland University of Maryland</affiliation>
<address confidence="0.999873">College Park, USA, 20742 College Park, USA, 20742</address>
<email confidence="0.999794">jags@umiacs.umd.eduhal@umiacs.umd.edu</email>
<abstract confidence="0.99948208">In this paper, we address the problem of building a multilingual transliteration system using an interlingual representation. Our approach uses international phonetic alphabet (IPA) to learn the interlingual representation and thus allows us to use any word and its IPA representation as a training example. Thus, our approach requires only monolingual resources: a dictionary lists words and their By adding a phoneme dictionary of a new language, we can readily build a transliteration system into any of the existing previous languages, without the expense of all-pairs data or computation. We also propose a regularization framework for learning the interlingual representation, which accounts for language specific phonemic variability, and thus it can find better mappings between languages. Experimental results on the name transliteration task in five diverse languages show a maximum improvement of 29% accuracy and an average improvement of 17% accuracy compared to a state-of-the-art baseline system.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Nasreen AbdulJaleel</author>
<author>Leah S Larkey</author>
</authors>
<title>Statistical transliteration for english-arabic cross language information retrieval.</title>
<date>2003</date>
<booktitle>In Proceedings of the twelfth international conference on Information and knowledge management, CIKM ’03,</booktitle>
<pages>139--146</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="3210" citStr="AbdulJaleel and Larkey, 2003" startWordPosition="485" endWordPosition="488">r approach is that it accounts for language specific variation in the bridge language resources (i.e. between P H Q and Q H R) and aims to minimize this variation as much as possible. Though our technique is general, for clarity we describe it in the context of named entity (NE) transliteration. Named entity (NE) transliteration involves transliterating a name in one language into another language and is shown to be crucial for machine translation (MT) (Knight and Graehl, 1998; AlOnaizan and Knight, 2002; Hermjakob et al., 2008; Li et al., 2009) and cross-lingual information retrieval (CLIR) (AbdulJaleel and Larkey, 2003; Mandl and Womser-Hacker, 2005; Udupa et al., 2009). There exists a large body of literature in transliteration, especially in the bilingual setting, well summarized by Ravi and Knight (2009). We Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 12–23, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics English Bulgarian Word IPA Word IPA bashful /&apos;bæʃfol/ шi46аM /&apos;ʃibom/ tuesday /&apos;tuːzdeɪ/ nyK /luk/ craft /kɹæft/ KаK /kak/ book /bʊk/ Myзей /mʊ&apos;zej/ head /hEd/ criеKа</context>
</contexts>
<marker>AbdulJaleel, Larkey, 2003</marker>
<rawString>Nasreen AbdulJaleel and Leah S. Larkey. 2003. Statistical transliteration for english-arabic cross language information retrieval. In Proceedings of the twelfth international conference on Information and knowledge management, CIKM ’03, pages 139–146, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yaser Al-Onaizan</author>
<author>Kevin Knight</author>
</authors>
<title>Machine transliteration of names in arabic text.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL-02 workshop on Computational approaches to semitic languages, SEMITIC ’02,</booktitle>
<pages>1--13</pages>
<publisher>ACL.</publisher>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="22648" citStr="Al-Onaizan and Knight, 2002" startWordPosition="3919" endWordPosition="3922"> rank matrices, to increase the numerical stability of the prediction step, we use Cij = UiUTi +UjUjT +0.001 I where I is an identity matrix. Notice that this solution doesn’t depend on the p∗ and hence we don’t need to compute it explicitly. 5 Related Work There is a large body of the literature in named entity transliteration, so we will describe only the most relevant ones to our approach. In transliteration, generative approaches aim to generate the target language ai = ri = u (10) 17 transliteration of a given source name (Knight and Graehl, 1998; Jung et al., 2000; Haizhou et al., 2004; Al-Onaizan and Knight, 2002) while discriminative approaches assume a list of target language names, obtained from other sources, and try to identify the correct transliteration (Klementiev and Roth, 2006; Sproat et al., 2006). The effectiveness of the discriminative approaches depend on the list of target language candidates. Sproat et al. (2006) report an oracle accuracy of 85%, but it depends on the source of the candidate transliterations. Nevertheless, all these approaches require either bilingual name pairs or phoneme sequences to learn to transliterate between two languages. Thus, if we want to build a translitera</context>
</contexts>
<marker>Al-Onaizan, Knight, 2002</marker>
<rawString>Yaser Al-Onaizan and Kevin Knight. 2002. Machine transliteration of names in arabic text. In Proceedings of the ACL-02 workshop on Computational approaches to semitic languages, SEMITIC ’02, pages 1–13, Stroudsburg, PA, USA. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Gao</author>
<author>Kam fai Wong</author>
<author>Wai Lam</author>
</authors>
<title>Phonemebased transliteration of foreign names for OOV problem.</title>
<date>2004</date>
<booktitle>In Proceedings of the 1st International Joint Conference on Natural Language Processing (IJCNLP),</booktitle>
<pages>374--381</pages>
<contexts>
<context position="24236" citStr="Gao et al., 2004" startWordPosition="4179" endWordPosition="4182">pra et al., 2010) uses a single mapping function for the phonemic space of all the languages and thus it can not handle language specific variability. In the original setting, authors use English as the pivot and since the feature space of English is fixed, irrespective of the target language, this may not be a serious concern but it becomes crucial when we use IPA as the bridge language. Approaches that map words in different languages into the common phonemic space have also been well studied. But most of these approaches use language specific resources such as CMU pronunciation dictionary (Gao et al., 2004) or a carefully constructed cost matrices for addition, substitution, and deletion of phonemes between a pair of languages (Tao et al., 2006; Yoon et al., 2007). Variants of soundex algorithm (Odel and Russel, 1918) such as Kodex (Kang and Choi, 2000) use hand constructed consonant to soundex code tables for name transliteration. Similar to our approach these variants only require soundex mappings of a new language to build transliteration system, but our model does not require explicit mapping between n-gram characters and the IPA symbols instead it learns them automatically using phoneme dic</context>
</contexts>
<marker>Gao, Wong, Lam, 2004</marker>
<rawString>Wei Gao, Kam fai Wong, and Wai Lam. 2004. Phonemebased transliteration of foreign names for OOV problem. In Proceedings of the 1st International Joint Conference on Natural Language Processing (IJCNLP), pages 374–381.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Li Haizhou</author>
<author>Zhang Min</author>
<author>Su Jian</author>
</authors>
<title>A joint source-channel model for machine transliteration.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, ACL ’04,</booktitle>
<publisher>ACL.</publisher>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="22618" citStr="Haizhou et al., 2004" startWordPosition="3915" endWordPosition="3918">Ui and Uj are not full rank matrices, to increase the numerical stability of the prediction step, we use Cij = UiUTi +UjUjT +0.001 I where I is an identity matrix. Notice that this solution doesn’t depend on the p∗ and hence we don’t need to compute it explicitly. 5 Related Work There is a large body of the literature in named entity transliteration, so we will describe only the most relevant ones to our approach. In transliteration, generative approaches aim to generate the target language ai = ri = u (10) 17 transliteration of a given source name (Knight and Graehl, 1998; Jung et al., 2000; Haizhou et al., 2004; Al-Onaizan and Knight, 2002) while discriminative approaches assume a list of target language names, obtained from other sources, and try to identify the correct transliteration (Klementiev and Roth, 2006; Sproat et al., 2006). The effectiveness of the discriminative approaches depend on the list of target language candidates. Sproat et al. (2006) report an oracle accuracy of 85%, but it depends on the source of the candidate transliterations. Nevertheless, all these approaches require either bilingual name pairs or phoneme sequences to learn to transliterate between two languages. Thus, if </context>
</contexts>
<marker>Haizhou, Min, Jian, 2004</marker>
<rawString>Li Haizhou, Zhang Min, and Su Jian. 2004. A joint source-channel model for machine transliteration. In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, ACL ’04, Stroudsburg, PA, USA. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Haspelmath</author>
<author>Matthew Dryer</author>
<author>David Gil</author>
<author>Bernard Comrie</author>
<author>editors</author>
</authors>
<title>The World Atlas of Language Structures.</title>
<date>2005</date>
<publisher>Oxford University Press.</publisher>
<contexts>
<context position="7271" citStr="Haspelmath et al., 2005" startWordPosition="1144" endWordPosition="1147">Finally, unlike other transliteration approaches, by simply adding a phoneme dictionary of (l+1)st language we can readily get a transliteration system into any of the existing l languages and thus avoid the need for all-pairs data or computation. Using IPA as the bridge language poses some new challenges such as the language specific phonemic inventory. For example, Mandarin doesn’t have /v/, so it is frequently substituted with /w/ or /f/. Similarly, !Xóõ (Southern Khoisan, spoken in Botswana) has 122 consonants, mostly consisting of a large inventory of different word-initial click sounds (Haspelmath et al., 2005), many of which do not exist in any other documented languages. Besides this language specific phonemic inventory, names have different IPA representations in different languages. For example, as shown in Table 2, the IPA sequences for ‘China’ in English and Dutch have common IPA symbols but the English IPA sequence has additional symbols. Moreover, a name can have multiple pronunciations with in a language, e.g. ‘France’ has two different IPA sequences in English (Table 2). In order to handle this phonemic diversity, our method explicitly models language-specific variability and attempts to m</context>
</contexts>
<marker>Haspelmath, Dryer, Gil, Comrie, editors, 2005</marker>
<rawString>Martin Haspelmath, Matthew Dryer, David Gil, and Bernard Comrie, editors. 2005. The World Atlas of Language Structures. Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ulf Hermjakob</author>
<author>Kevin Knight</author>
<author>Hal Daumé</author>
</authors>
<title>Name translation in statistical machine translation -learning when to transliterate.</title>
<date>2008</date>
<booktitle>In Proceedings ofACL08: HLT,</booktitle>
<pages>389--397</pages>
<publisher>ACL.</publisher>
<location>Columbus, Ohio,</location>
<contexts>
<context position="3115" citStr="Hermjakob et al., 2008" startWordPosition="472" endWordPosition="475">e approaches and show its effectiveness for name transliteration task. The key idea of our approach is that it accounts for language specific variation in the bridge language resources (i.e. between P H Q and Q H R) and aims to minimize this variation as much as possible. Though our technique is general, for clarity we describe it in the context of named entity (NE) transliteration. Named entity (NE) transliteration involves transliterating a name in one language into another language and is shown to be crucial for machine translation (MT) (Knight and Graehl, 1998; AlOnaizan and Knight, 2002; Hermjakob et al., 2008; Li et al., 2009) and cross-lingual information retrieval (CLIR) (AbdulJaleel and Larkey, 2003; Mandl and Womser-Hacker, 2005; Udupa et al., 2009). There exists a large body of literature in transliteration, especially in the bilingual setting, well summarized by Ravi and Knight (2009). We Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 12–23, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics English Bulgarian Word IPA Word IPA bashful /&apos;bæʃfol/ шi46аM /&apos;ʃibom/ t</context>
</contexts>
<marker>Hermjakob, Knight, Daumé, 2008</marker>
<rawString>Ulf Hermjakob, Kevin Knight, and Hal Daumé III. 2008. Name translation in statistical machine translation -learning when to transliterate. In Proceedings ofACL08: HLT, pages 389–397, Columbus, Ohio, June. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harold Hotelling</author>
</authors>
<title>Relation between two sets of variables.</title>
<date>1936</date>
<journal>Biometrica,</journal>
<pages>28--322</pages>
<contexts>
<context position="9788" citStr="Hotelling, 1936" startWordPosition="1542" endWordPosition="1543">age of our approach is that, it extends easily to more than two languages and in fact adding phoneme dictionary from a different, but related, language improves the accuracies of a given language pair. Our main contributions are: 1) building a transliteration system using (word, IPA) pairs and hence using only monolingual resources and 2) proposing a regularization framework which is more general and applies to other bridge language applications such as lexicon mining (Mann and Yarowsky, 2001). 3 Low Dimensional Projections Our approach is inspired by the Canonical Correlation Analysis (CCA) (Hotelling, 1936) and its application to transliteration mining (Udupa and Khapra, 2010). First, we convert the phoneme dictionary of each language into feature vectors, i.e. we convert each word into a feature vector of n-gram character sequences and similarly, we also, convert the IPA representations into feature vectors of n-gram IPA symbol sequences. For example, if we use unigram and bigram sequences as features, then the feature vectors of ‘head’ and its IPA sequence `hEd&apos; are given by {h, e, a, d, #h, he, ea, ad, d$l and {h,E, d, #h, hE,Ed, d$l. For brevity, we refer to the spaces of n-gram character an</context>
<context position="15081" citStr="Hotelling, 1936" startWordPosition="2463" endWordPosition="2464"> common two-dimensional space at the bottom. 3.3 Regularized Projections In this section we first formulate the problem of finding the mapping functions (Ai and Ui) of each language as an optimization problem. In the following section (Sec. 4), we develop a method for solving the optimization problem and also derive closed form solution for the prediction problem given in Eq. 1. For simplicity, we describe our approach in terms of single projection vectors, ai E Rdz and ui E R1, rather than full matrices, but the generalization is trivial. Inspired by the Canonical Correlation Analysis (CCA) (Hotelling, 1936), we find projection directions in the character and phonemic spaces of each language such that, after projection, a word is closer to its aligned IPA sequence. To understand this, assume that we have a name (say “Barack Obama”) in all the languages4 and its feature vectors are given by xi and pi i = 1 · · · l in the character and phone3In reality, as explained in the previous section, the phonemic variation that is commonly observed is that different features are triggered for different languages. But for visualization purpose, we showed the IPA sequences as if they differ in the feature valu</context>
</contexts>
<marker>Hotelling, 1936</marker>
<rawString>Harold Hotelling. 1936. Relation between two sets of variables. Biometrica, 28:322–377.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sung Young Jung</author>
<author>SungLim Hong</author>
<author>Eunok Paek</author>
</authors>
<title>An english to korean transliteration model of extended markov window.</title>
<date>2000</date>
<booktitle>In Proceedings of the 18th conference on Computational linguistics - Volume 1, COLING ’00,</booktitle>
<pages>383--389</pages>
<publisher>ACL.</publisher>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="22596" citStr="Jung et al., 2000" startWordPosition="3911" endWordPosition="3914">AT i xi (12) Since Ui and Uj are not full rank matrices, to increase the numerical stability of the prediction step, we use Cij = UiUTi +UjUjT +0.001 I where I is an identity matrix. Notice that this solution doesn’t depend on the p∗ and hence we don’t need to compute it explicitly. 5 Related Work There is a large body of the literature in named entity transliteration, so we will describe only the most relevant ones to our approach. In transliteration, generative approaches aim to generate the target language ai = ri = u (10) 17 transliteration of a given source name (Knight and Graehl, 1998; Jung et al., 2000; Haizhou et al., 2004; Al-Onaizan and Knight, 2002) while discriminative approaches assume a list of target language names, obtained from other sources, and try to identify the correct transliteration (Klementiev and Roth, 2006; Sproat et al., 2006). The effectiveness of the discriminative approaches depend on the list of target language candidates. Sproat et al. (2006) report an oracle accuracy of 85%, but it depends on the source of the candidate transliterations. Nevertheless, all these approaches require either bilingual name pairs or phoneme sequences to learn to transliterate between tw</context>
</contexts>
<marker>Jung, Hong, Paek, 2000</marker>
<rawString>Sung Young Jung, SungLim Hong, and Eunok Paek. 2000. An english to korean transliteration model of extended markov window. In Proceedings of the 18th conference on Computational linguistics - Volume 1, COLING ’00, pages 383–389, Stroudsburg, PA, USA. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Byung-Ju Kang</author>
<author>Key-Sun Choi</author>
</authors>
<title>Two approaches for the resolution of word mismatch problem caused by english words and foreign words in korean information retrieval.</title>
<date>2000</date>
<booktitle>In Proceedings of the 5th international workshop on on Information retrieval with Asian languages, IRAL ’00,</booktitle>
<pages>133--140</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="24487" citStr="Kang and Choi, 2000" startWordPosition="4221" endWordPosition="4224">is fixed, irrespective of the target language, this may not be a serious concern but it becomes crucial when we use IPA as the bridge language. Approaches that map words in different languages into the common phonemic space have also been well studied. But most of these approaches use language specific resources such as CMU pronunciation dictionary (Gao et al., 2004) or a carefully constructed cost matrices for addition, substitution, and deletion of phonemes between a pair of languages (Tao et al., 2006; Yoon et al., 2007). Variants of soundex algorithm (Odel and Russel, 1918) such as Kodex (Kang and Choi, 2000) use hand constructed consonant to soundex code tables for name transliteration. Similar to our approach these variants only require soundex mappings of a new language to build transliteration system, but our model does not require explicit mapping between n-gram characters and the IPA symbols instead it learns them automatically using phoneme dictionaries. Alternatively unsupervised approaches have also been explored (Ravi and Knight, 2009), but their accuracies are fairly low compared to the supervised and weekly supervised approaches. 6 Experiments Our experiments are designed to evaluate t</context>
</contexts>
<marker>Kang, Choi, 2000</marker>
<rawString>Byung-Ju Kang and Key-Sun Choi. 2000. Two approaches for the resolution of word mismatch problem caused by english words and foreign words in korean information retrieval. In Proceedings of the 5th international workshop on on Information retrieval with Asian languages, IRAL ’00, pages 133–140, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitesh M Khapra</author>
<author>Raghavendra Udupa</author>
<author>A Kumaran</author>
<author>Pushpak Bhattacharyya</author>
</authors>
<title>Pr + rq ;:t-.i pq: Transliteration mining using bridge language.</title>
<date>2010</date>
<booktitle>In Proceedings of the Twenty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2010,</booktitle>
<publisher>AAAI Press.</publisher>
<location>Atlanta, Georgia, USA,</location>
<contexts>
<context position="2257" citStr="Khapra et al., 2010" startWordPosition="330" endWordPosition="333">ymbols, but it still is specific to one language and, in this sense, we refer to it as a monolingual resource. 12 between English and other languages. Similarly, bilingual dictionaries and transliteration data sets are more accessible from a language into English than into a different language. This situation has caused the NLP community to develop approaches which use a resource rich language (Q say English) as pivot to build resources/applications between a new language pair P and R. Previous studies in machine translation (Utiyama and Isahara, 2007; Paul and Sumita, 2011), transliteration (Khapra et al., 2010), and dictionary mining (Saralegi et al., 2011) show that these bridge language approaches perform competitively with approaches that use resources between P and R. In this paper, we propose a regularization framework for bridge language approaches and show its effectiveness for name transliteration task. The key idea of our approach is that it accounts for language specific variation in the bridge language resources (i.e. between P H Q and Q H R) and aims to minimize this variation as much as possible. Though our technique is general, for clarity we describe it in the context of named entity </context>
<context position="4986" citStr="Khapra et al., 2010" startWordPosition="763" endWordPosition="766">s the problem of building a transliteration system between every pair of languages. A straight forward supervised learning approach would require training data of name pairs between every pair of languages (Knight and Graehl, 1998) or a set of common names transliterated from every language into a pivot language. Though it is relatively easy to obtain names transliterated into a pivot language (such as English), it is unlikely that such data sets contain the same names. Bridge language approaches overcome the need for common names and build transliteration systems for resource poor languages (Khapra et al., 2010). However, such approaches still require training data consisting of bilingual name transliterations (orthographic name-to-name mappings). In this paper, we relax the need for name transliterations by using international phonetic alphabet (IPA) in a manner akin to a “bridge language.” 2 IPA for Transliteration We assume that we have a list of words and their IPA representations in each of the l languages. The words in different languages need not have any relationship to each other. Table 1 shows few words and their IPA representations in English and Bulgarian languages. We refer to the set of</context>
<context position="13147" citStr="Khapra et al., 2010" startWordPosition="2142" endWordPosition="2145">-dimensional green space at the top), where as our approach uses two mapping functions U1 and U2, one for each language, to map the IPA sequences into the common subspace. where L(xi7 xj) is given by min IAT xi - UT p�2 + �A�j xj - Uj p�2 (2) pERc This formulation uses the source language mappings (Ai and Ui) to find the IPA sequence p that is closest to the source name and then uses it, along with the target language mappings (Aj and Uj), to identify the correct transliteration from a list of candidate transliterations. At a high level, existing bridge language approaches such as Bridge-CCA (Khapra et al., 2010) assume that Ui - Uj thus ignoring the language specific variation. To understand its implication consider the example shown in Fig. 1. The middle portion of the Fig. shows the name Gandhi (represented as point) in the character spaces of English and Hindi, three-dimensional spaces, and its IPA sequences in the phonemic space (the twodimensional space in the middle). Notice that, because of the phonemic variation, the same name is represented by two distinct points in the common phonemic space.3 Now, since Bridge-CCA uses a single mapping function for both the IPA sequences, it fails to map th</context>
<context position="23554" citStr="Khapra et al., 2010" startWordPosition="4063" endWordPosition="4066">ge candidates. Sproat et al. (2006) report an oracle accuracy of 85%, but it depends on the source of the candidate transliterations. Nevertheless, all these approaches require either bilingual name pairs or phoneme sequences to learn to transliterate between two languages. Thus, if we want to build a transliteration system between every pair of languages in a given set of languages then these approaches need resources between every pair of languages which can be prohibitive. Bridge language approaches propose an alternative and use a resource rich language such as English as common language (Khapra et al., 2010) but they still need bilingual resources. Moreover BridgeCCA (Khapra et al., 2010) uses a single mapping function for the phonemic space of all the languages and thus it can not handle language specific variability. In the original setting, authors use English as the pivot and since the feature space of English is fixed, irrespective of the target language, this may not be a serious concern but it becomes crucial when we use IPA as the bridge language. Approaches that map words in different languages into the common phonemic space have also been well studied. But most of these approaches use l</context>
<context position="29354" citStr="Khapra et al., 2010" startWordPosition="4996" endWordPosition="4999">ta sets, of the total 3777 common phonetic features only 3312, 882, and 1009 features are observed in English, Bulgarian, 5http://www.geonames.org/ 0 10 20 30 40 50 60 70 80 90 100 lambda Figure 2: Performance of transliteration system with residual parameter A on English-Bulgarian development data set. and Russian languages respectively. This indicates the diversity in the phonemic inventory of different languages. We compare our approach against Bridge-CCA, a state-of-the-art bridge language transliteration system which is known to perform competitively with other discriminative approaches (Khapra et al., 2010). We use the phoneme dictionaries in each language to train our approach, as well as the baseline system. The projection directions learnt during the training are used to find the transliteration for a test name as described in Sec. 4.2. We report the performance in terms of the accuracy (exact match) of the top ranked transliteration and the mean reciprocal rank (MRR) of the correct transliteration. We find transliterations in both the directions (i.e. target language transliterations given a source name and vice versa) and report average accuracies. The regularization parameter (T) and the s</context>
<context position="37607" citStr="Khapra et al. (2010)" startWordPosition="6393" endWordPosition="6396">ectiveness on the name transliteration task. Our approach learns interlingual representation using only monolingual resources and hence can be used to build transliteration system between resource poor languages. We show that, by accounting the language specific phonemic variation, we can get a significant improvements. Our experimental results suggest that a transliteration system built using IPA data can also help improve the accuracy of a transliteration system trained on bilingual name pairs. Thought we used IPA as a bridge language there are other viable options. For example, as shown in Khapra et al. (2010) we can use English as the bridge language. Since name transliteration problem is being studied for a considerable time, many resources already exist between English and other languages. So, one can argue the appropriateness of IPA as bridge language compared to, say, English. While this is an important question, in this paper, we are primarily interested in showing the importance of handling language specific phenomenon in the bridge language approaches. In future, we would like to study the appropriateness of IPA vs. English as the bridge language and also the generalizability of our techniq</context>
</contexts>
<marker>Khapra, Udupa, Kumaran, Bhattacharyya, 2010</marker>
<rawString>Mitesh M. Khapra, Raghavendra Udupa, A. Kumaran, and Pushpak Bhattacharyya. 2010. Pr + rq ;:t-.i pq: Transliteration mining using bridge language. In Proceedings of the Twenty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2010, Atlanta, Georgia, USA, July. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexandre Klementiev</author>
<author>Dan Roth</author>
</authors>
<title>Weakly supervised named entity transliteration and discovery from multilingual comparable corpora.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics, ACL44,</booktitle>
<pages>817--824</pages>
<publisher>ACL.</publisher>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="4135" citStr="Klementiev and Roth, 2006" startWordPosition="622" endWordPosition="625">ational Natural Language Learning, pages 12–23, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics English Bulgarian Word IPA Word IPA bashful /&apos;bæʃfol/ шi46аM /&apos;ʃibom/ tuesday /&apos;tuːzdeɪ/ nyK /luk/ craft /kɹæft/ KаK /kak/ book /bʊk/ Myзей /mʊ&apos;zej/ head /hEd/ criеKа /spE&apos;kɤ/ Table 1: Example phoneme dictionaries in English and Bulgarian. The English translations for the Bulgarian words are switch, onion, how, museum, and spekle. summarize the approaches that are most relevant to us in Sec. 5. In this paper, we operate in the context of transliteration mining (Klementiev and Roth, 2006; Sproat et al., 2006) where we assume that we are given a source language name and a list of target language candidate transliterations and the task is to identify the correct transliteration. Given a set of l languages, we address the problem of building a transliteration system between every pair of languages. A straight forward supervised learning approach would require training data of name pairs between every pair of languages (Knight and Graehl, 1998) or a set of common names transliterated from every language into a pivot language. Though it is relatively easy to obtain names translite</context>
<context position="22824" citStr="Klementiev and Roth, 2006" startWordPosition="3944" endWordPosition="3947">end on the p∗ and hence we don’t need to compute it explicitly. 5 Related Work There is a large body of the literature in named entity transliteration, so we will describe only the most relevant ones to our approach. In transliteration, generative approaches aim to generate the target language ai = ri = u (10) 17 transliteration of a given source name (Knight and Graehl, 1998; Jung et al., 2000; Haizhou et al., 2004; Al-Onaizan and Knight, 2002) while discriminative approaches assume a list of target language names, obtained from other sources, and try to identify the correct transliteration (Klementiev and Roth, 2006; Sproat et al., 2006). The effectiveness of the discriminative approaches depend on the list of target language candidates. Sproat et al. (2006) report an oracle accuracy of 85%, but it depends on the source of the candidate transliterations. Nevertheless, all these approaches require either bilingual name pairs or phoneme sequences to learn to transliterate between two languages. Thus, if we want to build a transliteration system between every pair of languages in a given set of languages then these approaches need resources between every pair of languages which can be prohibitive. Bridge la</context>
</contexts>
<marker>Klementiev, Roth, 2006</marker>
<rawString>Alexandre Klementiev and Dan Roth. 2006. Weakly supervised named entity transliteration and discovery from multilingual comparable corpora. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics, ACL44, pages 817–824, Stroudsburg, PA, USA. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
<author>Jonathan Graehl</author>
</authors>
<date>1998</date>
<booktitle>Machine transliteration. Computational Linguistics,</booktitle>
<volume>24</volume>
<issue>4</issue>
<pages>612</pages>
<contexts>
<context position="3063" citStr="Knight and Graehl, 1998" startWordPosition="463" endWordPosition="466">propose a regularization framework for bridge language approaches and show its effectiveness for name transliteration task. The key idea of our approach is that it accounts for language specific variation in the bridge language resources (i.e. between P H Q and Q H R) and aims to minimize this variation as much as possible. Though our technique is general, for clarity we describe it in the context of named entity (NE) transliteration. Named entity (NE) transliteration involves transliterating a name in one language into another language and is shown to be crucial for machine translation (MT) (Knight and Graehl, 1998; AlOnaizan and Knight, 2002; Hermjakob et al., 2008; Li et al., 2009) and cross-lingual information retrieval (CLIR) (AbdulJaleel and Larkey, 2003; Mandl and Womser-Hacker, 2005; Udupa et al., 2009). There exists a large body of literature in transliteration, especially in the bilingual setting, well summarized by Ravi and Knight (2009). We Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 12–23, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics English Bulgarian W</context>
<context position="4597" citStr="Knight and Graehl, 1998" startWordPosition="698" endWordPosition="701">le. summarize the approaches that are most relevant to us in Sec. 5. In this paper, we operate in the context of transliteration mining (Klementiev and Roth, 2006; Sproat et al., 2006) where we assume that we are given a source language name and a list of target language candidate transliterations and the task is to identify the correct transliteration. Given a set of l languages, we address the problem of building a transliteration system between every pair of languages. A straight forward supervised learning approach would require training data of name pairs between every pair of languages (Knight and Graehl, 1998) or a set of common names transliterated from every language into a pivot language. Though it is relatively easy to obtain names transliterated into a pivot language (such as English), it is unlikely that such data sets contain the same names. Bridge language approaches overcome the need for common names and build transliteration systems for resource poor languages (Khapra et al., 2010). However, such approaches still require training data consisting of bilingual name transliterations (orthographic name-to-name mappings). In this paper, we relax the need for name transliterations by using inte</context>
<context position="22577" citStr="Knight and Graehl, 1998" startWordPosition="3907" endWordPosition="3910">j j xj = AjUT j C−1 ij UiAT i xi (12) Since Ui and Uj are not full rank matrices, to increase the numerical stability of the prediction step, we use Cij = UiUTi +UjUjT +0.001 I where I is an identity matrix. Notice that this solution doesn’t depend on the p∗ and hence we don’t need to compute it explicitly. 5 Related Work There is a large body of the literature in named entity transliteration, so we will describe only the most relevant ones to our approach. In transliteration, generative approaches aim to generate the target language ai = ri = u (10) 17 transliteration of a given source name (Knight and Graehl, 1998; Jung et al., 2000; Haizhou et al., 2004; Al-Onaizan and Knight, 2002) while discriminative approaches assume a list of target language names, obtained from other sources, and try to identify the correct transliteration (Klementiev and Roth, 2006; Sproat et al., 2006). The effectiveness of the discriminative approaches depend on the list of target language candidates. Sproat et al. (2006) report an oracle accuracy of 85%, but it depends on the source of the candidate transliterations. Nevertheless, all these approaches require either bilingual name pairs or phoneme sequences to learn to trans</context>
</contexts>
<marker>Knight, Graehl, 1998</marker>
<rawString>Kevin Knight and Jonathan Graehl. 1998. Machine transliteration. Computational Linguistics, 24(4):599– 612.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haizhou Li</author>
<author>A Kumaran</author>
<author>Vladimir Pervouchine</author>
<author>Min Zhang</author>
</authors>
<title>Report of news 2009 machine transliteration shared task.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Named Entities Workshop: Shared Task on Transliteration, NEWS ’09,</booktitle>
<pages>1--18</pages>
<publisher>ACL.</publisher>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="3133" citStr="Li et al., 2009" startWordPosition="476" endWordPosition="479">s effectiveness for name transliteration task. The key idea of our approach is that it accounts for language specific variation in the bridge language resources (i.e. between P H Q and Q H R) and aims to minimize this variation as much as possible. Though our technique is general, for clarity we describe it in the context of named entity (NE) transliteration. Named entity (NE) transliteration involves transliterating a name in one language into another language and is shown to be crucial for machine translation (MT) (Knight and Graehl, 1998; AlOnaizan and Knight, 2002; Hermjakob et al., 2008; Li et al., 2009) and cross-lingual information retrieval (CLIR) (AbdulJaleel and Larkey, 2003; Mandl and Womser-Hacker, 2005; Udupa et al., 2009). There exists a large body of literature in transliteration, especially in the bilingual setting, well summarized by Ravi and Knight (2009). We Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 12–23, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics English Bulgarian Word IPA Word IPA bashful /&apos;bæʃfol/ шi46аM /&apos;ʃibom/ tuesday /&apos;tuːzdeɪ/ </context>
</contexts>
<marker>Li, Kumaran, Pervouchine, Zhang, 2009</marker>
<rawString>Haizhou Li, A. Kumaran, Vladimir Pervouchine, and Min Zhang. 2009. Report of news 2009 machine transliteration shared task. In Proceedings of the 2009 Named Entities Workshop: Shared Task on Transliteration, NEWS ’09, pages 1–18, Stroudsburg, PA, USA. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Mandl</author>
<author>Christa Womser-Hacker</author>
</authors>
<title>The effect of named entities on effectiveness in crosslanguage information retrieval evaluation.</title>
<date>2005</date>
<booktitle>In Proceedings of the 2005 ACM symposium on Applied computing, SAC ’05,</booktitle>
<pages>1059--1064</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="3241" citStr="Mandl and Womser-Hacker, 2005" startWordPosition="489" endWordPosition="492"> for language specific variation in the bridge language resources (i.e. between P H Q and Q H R) and aims to minimize this variation as much as possible. Though our technique is general, for clarity we describe it in the context of named entity (NE) transliteration. Named entity (NE) transliteration involves transliterating a name in one language into another language and is shown to be crucial for machine translation (MT) (Knight and Graehl, 1998; AlOnaizan and Knight, 2002; Hermjakob et al., 2008; Li et al., 2009) and cross-lingual information retrieval (CLIR) (AbdulJaleel and Larkey, 2003; Mandl and Womser-Hacker, 2005; Udupa et al., 2009). There exists a large body of literature in transliteration, especially in the bilingual setting, well summarized by Ravi and Knight (2009). We Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 12–23, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics English Bulgarian Word IPA Word IPA bashful /&apos;bæʃfol/ шi46аM /&apos;ʃibom/ tuesday /&apos;tuːzdeɪ/ nyK /luk/ craft /kɹæft/ KаK /kak/ book /bʊk/ Myзей /mʊ&apos;zej/ head /hEd/ criеKа /spE&apos;kɤ/ Table 1: Example phon</context>
</contexts>
<marker>Mandl, Womser-Hacker, 2005</marker>
<rawString>Thomas Mandl and Christa Womser-Hacker. 2005. The effect of named entities on effectiveness in crosslanguage information retrieval evaluation. In Proceedings of the 2005 ACM symposium on Applied computing, SAC ’05, pages 1059–1064, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gideon S Mann</author>
<author>David Yarowsky</author>
</authors>
<title>Multipath translation lexicon induction via bridge languages.</title>
<date>2001</date>
<booktitle>In Proceedings of the 2nd meeting of the North American Chapter of the Association for Computational Linguistics on Language technologies, NAACL ’01,</booktitle>
<volume>1</volume>
<pages>pages</pages>
<publisher>ACL.</publisher>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="9670" citStr="Mann and Yarowsky, 2001" startWordPosition="1523" endWordPosition="1526"> 29% accuracy and an average improvement of 17% accuracy compared to a state-of-the-art baseline approach. An important advantage of our approach is that, it extends easily to more than two languages and in fact adding phoneme dictionary from a different, but related, language improves the accuracies of a given language pair. Our main contributions are: 1) building a transliteration system using (word, IPA) pairs and hence using only monolingual resources and 2) proposing a regularization framework which is more general and applies to other bridge language applications such as lexicon mining (Mann and Yarowsky, 2001). 3 Low Dimensional Projections Our approach is inspired by the Canonical Correlation Analysis (CCA) (Hotelling, 1936) and its application to transliteration mining (Udupa and Khapra, 2010). First, we convert the phoneme dictionary of each language into feature vectors, i.e. we convert each word into a feature vector of n-gram character sequences and similarly, we also, convert the IPA representations into feature vectors of n-gram IPA symbol sequences. For example, if we use unigram and bigram sequences as features, then the feature vectors of ‘head’ and its IPA sequence `hEd&apos; are given by {h</context>
</contexts>
<marker>Mann, Yarowsky, 2001</marker>
<rawString>Gideon S. Mann and David Yarowsky. 2001. Multipath translation lexicon induction via bridge languages. In Proceedings of the 2nd meeting of the North American Chapter of the Association for Computational Linguistics on Language technologies, NAACL ’01, pages 1– 8, Stroudsburg, PA, USA. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M K Odel</author>
<author>R C Russel</author>
</authors>
<date>1918</date>
<pages>1--261</pages>
<note>U.s. patent numbers,</note>
<contexts>
<context position="24451" citStr="Odel and Russel, 1918" startWordPosition="4214" endWordPosition="4217">nd since the feature space of English is fixed, irrespective of the target language, this may not be a serious concern but it becomes crucial when we use IPA as the bridge language. Approaches that map words in different languages into the common phonemic space have also been well studied. But most of these approaches use language specific resources such as CMU pronunciation dictionary (Gao et al., 2004) or a carefully constructed cost matrices for addition, substitution, and deletion of phonemes between a pair of languages (Tao et al., 2006; Yoon et al., 2007). Variants of soundex algorithm (Odel and Russel, 1918) such as Kodex (Kang and Choi, 2000) use hand constructed consonant to soundex code tables for name transliteration. Similar to our approach these variants only require soundex mappings of a new language to build transliteration system, but our model does not require explicit mapping between n-gram characters and the IPA symbols instead it learns them automatically using phoneme dictionaries. Alternatively unsupervised approaches have also been explored (Ravi and Knight, 2009), but their accuracies are fairly low compared to the supervised and weekly supervised approaches. 6 Experiments Our ex</context>
</contexts>
<marker>Odel, Russel, 1918</marker>
<rawString>M. K. Odel and R. C. Russel. 1918. U.s. patent numbers, 1,261,167 (1918) and 1,435,663(1922).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Paul</author>
<author>Eiichiro Sumita</author>
</authors>
<title>Translation quality indicators for pivot-based statistical mt.</title>
<date>2011</date>
<booktitle>In Proceedings of 5th International Joint Conference on Natural Language Processing,</booktitle>
<pages>811--818</pages>
<publisher>AFNLP.</publisher>
<location>Chiang Mai, Thailand,</location>
<contexts>
<context position="2218" citStr="Paul and Sumita, 2011" startWordPosition="325" endWordPosition="328">uire knowledge about both words and IPA symbols, but it still is specific to one language and, in this sense, we refer to it as a monolingual resource. 12 between English and other languages. Similarly, bilingual dictionaries and transliteration data sets are more accessible from a language into English than into a different language. This situation has caused the NLP community to develop approaches which use a resource rich language (Q say English) as pivot to build resources/applications between a new language pair P and R. Previous studies in machine translation (Utiyama and Isahara, 2007; Paul and Sumita, 2011), transliteration (Khapra et al., 2010), and dictionary mining (Saralegi et al., 2011) show that these bridge language approaches perform competitively with approaches that use resources between P and R. In this paper, we propose a regularization framework for bridge language approaches and show its effectiveness for name transliteration task. The key idea of our approach is that it accounts for language specific variation in the bridge language resources (i.e. between P H Q and Q H R) and aims to minimize this variation as much as possible. Though our technique is general, for clarity we desc</context>
</contexts>
<marker>Paul, Sumita, 2011</marker>
<rawString>Michael Paul and Eiichiro Sumita. 2011. Translation quality indicators for pivot-based statistical mt. In Proceedings of 5th International Joint Conference on Natural Language Processing, pages 811–818, Chiang Mai, Thailand, November. AFNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sujith Ravi</author>
<author>Kevin Knight</author>
</authors>
<title>Learning phoneme mappings for transliteration without parallel data.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>37--45</pages>
<publisher>ACL.</publisher>
<location>Boulder, Colorado,</location>
<contexts>
<context position="3402" citStr="Ravi and Knight (2009)" startWordPosition="514" endWordPosition="517">hnique is general, for clarity we describe it in the context of named entity (NE) transliteration. Named entity (NE) transliteration involves transliterating a name in one language into another language and is shown to be crucial for machine translation (MT) (Knight and Graehl, 1998; AlOnaizan and Knight, 2002; Hermjakob et al., 2008; Li et al., 2009) and cross-lingual information retrieval (CLIR) (AbdulJaleel and Larkey, 2003; Mandl and Womser-Hacker, 2005; Udupa et al., 2009). There exists a large body of literature in transliteration, especially in the bilingual setting, well summarized by Ravi and Knight (2009). We Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 12–23, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics English Bulgarian Word IPA Word IPA bashful /&apos;bæʃfol/ шi46аM /&apos;ʃibom/ tuesday /&apos;tuːzdeɪ/ nyK /luk/ craft /kɹæft/ KаK /kak/ book /bʊk/ Myзей /mʊ&apos;zej/ head /hEd/ criеKа /spE&apos;kɤ/ Table 1: Example phoneme dictionaries in English and Bulgarian. The English translations for the Bulgarian words are switch, onion, how, museum, and spekle. summarize the approaches </context>
<context position="24932" citStr="Ravi and Knight, 2009" startWordPosition="4286" endWordPosition="4289">deletion of phonemes between a pair of languages (Tao et al., 2006; Yoon et al., 2007). Variants of soundex algorithm (Odel and Russel, 1918) such as Kodex (Kang and Choi, 2000) use hand constructed consonant to soundex code tables for name transliteration. Similar to our approach these variants only require soundex mappings of a new language to build transliteration system, but our model does not require explicit mapping between n-gram characters and the IPA symbols instead it learns them automatically using phoneme dictionaries. Alternatively unsupervised approaches have also been explored (Ravi and Knight, 2009), but their accuracies are fairly low compared to the supervised and weekly supervised approaches. 6 Experiments Our experiments are designed to evaluate the following three aspects of our model, and of our approach to transliteration in general: IPA as bridge: Unlike other phonemic based approaches (Sec. 5), we do not explicitly model the phoneme modifications between pairs of languages. Moreover, the phoneme dictionary in each language is crawled from Wiktionary (Sec. 6.1), which is likely to be noisy. So, the first aspect we want to evaluate is the effectiveness of using IPA as the bridge l</context>
</contexts>
<marker>Ravi, Knight, 2009</marker>
<rawString>Sujith Ravi and Kevin Knight. 2009. Learning phoneme mappings for transliteration without parallel data. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 37–45, Boulder, Colorado, June. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xabier Saralegi</author>
<author>Iker Manterola</author>
<author>Iñaki San Vicente</author>
</authors>
<title>Analyzing methods for improving precision of pivot based bilingual dictionaries.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>846--856</pages>
<publisher>ACL.</publisher>
<location>Edinburgh, Scotland, UK.,</location>
<contexts>
<context position="2304" citStr="Saralegi et al., 2011" startWordPosition="337" endWordPosition="340">age and, in this sense, we refer to it as a monolingual resource. 12 between English and other languages. Similarly, bilingual dictionaries and transliteration data sets are more accessible from a language into English than into a different language. This situation has caused the NLP community to develop approaches which use a resource rich language (Q say English) as pivot to build resources/applications between a new language pair P and R. Previous studies in machine translation (Utiyama and Isahara, 2007; Paul and Sumita, 2011), transliteration (Khapra et al., 2010), and dictionary mining (Saralegi et al., 2011) show that these bridge language approaches perform competitively with approaches that use resources between P and R. In this paper, we propose a regularization framework for bridge language approaches and show its effectiveness for name transliteration task. The key idea of our approach is that it accounts for language specific variation in the bridge language resources (i.e. between P H Q and Q H R) and aims to minimize this variation as much as possible. Though our technique is general, for clarity we describe it in the context of named entity (NE) transliteration. Named entity (NE) transli</context>
</contexts>
<marker>Saralegi, Manterola, Vicente, 2011</marker>
<rawString>Xabier Saralegi, Iker Manterola, and Iñaki San Vicente. 2011. Analyzing methods for improving precision of pivot based bilingual dictionaries. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 846–856, Edinburgh, Scotland, UK., July. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Sproat</author>
<author>Tao Tao</author>
<author>ChengXiang Zhai</author>
</authors>
<title>Named entity transliteration with comparable corpora.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, ACL-44,</booktitle>
<pages>73--80</pages>
<publisher>ACL.</publisher>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="4157" citStr="Sproat et al., 2006" startWordPosition="626" endWordPosition="629">arning, pages 12–23, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics English Bulgarian Word IPA Word IPA bashful /&apos;bæʃfol/ шi46аM /&apos;ʃibom/ tuesday /&apos;tuːzdeɪ/ nyK /luk/ craft /kɹæft/ KаK /kak/ book /bʊk/ Myзей /mʊ&apos;zej/ head /hEd/ criеKа /spE&apos;kɤ/ Table 1: Example phoneme dictionaries in English and Bulgarian. The English translations for the Bulgarian words are switch, onion, how, museum, and spekle. summarize the approaches that are most relevant to us in Sec. 5. In this paper, we operate in the context of transliteration mining (Klementiev and Roth, 2006; Sproat et al., 2006) where we assume that we are given a source language name and a list of target language candidate transliterations and the task is to identify the correct transliteration. Given a set of l languages, we address the problem of building a transliteration system between every pair of languages. A straight forward supervised learning approach would require training data of name pairs between every pair of languages (Knight and Graehl, 1998) or a set of common names transliterated from every language into a pivot language. Though it is relatively easy to obtain names transliterated into a pivot lan</context>
<context position="22846" citStr="Sproat et al., 2006" startWordPosition="3948" endWordPosition="3951">don’t need to compute it explicitly. 5 Related Work There is a large body of the literature in named entity transliteration, so we will describe only the most relevant ones to our approach. In transliteration, generative approaches aim to generate the target language ai = ri = u (10) 17 transliteration of a given source name (Knight and Graehl, 1998; Jung et al., 2000; Haizhou et al., 2004; Al-Onaizan and Knight, 2002) while discriminative approaches assume a list of target language names, obtained from other sources, and try to identify the correct transliteration (Klementiev and Roth, 2006; Sproat et al., 2006). The effectiveness of the discriminative approaches depend on the list of target language candidates. Sproat et al. (2006) report an oracle accuracy of 85%, but it depends on the source of the candidate transliterations. Nevertheless, all these approaches require either bilingual name pairs or phoneme sequences to learn to transliterate between two languages. Thus, if we want to build a transliteration system between every pair of languages in a given set of languages then these approaches need resources between every pair of languages which can be prohibitive. Bridge language approaches prop</context>
</contexts>
<marker>Sproat, Tao, Zhai, 2006</marker>
<rawString>Richard Sproat, Tao Tao, and ChengXiang Zhai. 2006. Named entity transliteration with comparable corpora. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, ACL-44, pages 73–80, Stroudsburg, PA, USA. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tao Tao</author>
<author>Su-Youn Yoon</author>
<author>Andrew Fister</author>
<author>Richard Sproat</author>
<author>ChengXiang Zhai</author>
</authors>
<title>Unsupervised named entity transliteration using temporal and phonetic correlation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, EMNLP ’06,</booktitle>
<pages>250--257</pages>
<publisher>ACL.</publisher>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="24376" citStr="Tao et al., 2006" startWordPosition="4202" endWordPosition="4205">iability. In the original setting, authors use English as the pivot and since the feature space of English is fixed, irrespective of the target language, this may not be a serious concern but it becomes crucial when we use IPA as the bridge language. Approaches that map words in different languages into the common phonemic space have also been well studied. But most of these approaches use language specific resources such as CMU pronunciation dictionary (Gao et al., 2004) or a carefully constructed cost matrices for addition, substitution, and deletion of phonemes between a pair of languages (Tao et al., 2006; Yoon et al., 2007). Variants of soundex algorithm (Odel and Russel, 1918) such as Kodex (Kang and Choi, 2000) use hand constructed consonant to soundex code tables for name transliteration. Similar to our approach these variants only require soundex mappings of a new language to build transliteration system, but our model does not require explicit mapping between n-gram characters and the IPA symbols instead it learns them automatically using phoneme dictionaries. Alternatively unsupervised approaches have also been explored (Ravi and Knight, 2009), but their accuracies are fairly low compar</context>
</contexts>
<marker>Tao, Yoon, Fister, Sproat, Zhai, 2006</marker>
<rawString>Tao Tao, Su-Youn Yoon, Andrew Fister, Richard Sproat, and ChengXiang Zhai. 2006. Unsupervised named entity transliteration using temporal and phonetic correlation. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, EMNLP ’06, pages 250–257, Stroudsburg, PA, USA. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raghavendra Udupa</author>
<author>Mitesh M Khapra</author>
</authors>
<title>Transliteration equivalence using canonical correlation analysis.</title>
<date>2010</date>
<booktitle>In ECIR’10,</booktitle>
<pages>75--86</pages>
<contexts>
<context position="9859" citStr="Udupa and Khapra, 2010" startWordPosition="1551" endWordPosition="1554">languages and in fact adding phoneme dictionary from a different, but related, language improves the accuracies of a given language pair. Our main contributions are: 1) building a transliteration system using (word, IPA) pairs and hence using only monolingual resources and 2) proposing a regularization framework which is more general and applies to other bridge language applications such as lexicon mining (Mann and Yarowsky, 2001). 3 Low Dimensional Projections Our approach is inspired by the Canonical Correlation Analysis (CCA) (Hotelling, 1936) and its application to transliteration mining (Udupa and Khapra, 2010). First, we convert the phoneme dictionary of each language into feature vectors, i.e. we convert each word into a feature vector of n-gram character sequences and similarly, we also, convert the IPA representations into feature vectors of n-gram IPA symbol sequences. For example, if we use unigram and bigram sequences as features, then the feature vectors of ‘head’ and its IPA sequence `hEd&apos; are given by {h, e, a, d, #h, he, ea, ad, d$l and {h,E, d, #h, hE,Ed, d$l. For brevity, we refer to the spaces of n-gram character and IPA symbol sequences as character and phonemic spaces respectively. T</context>
</contexts>
<marker>Udupa, Khapra, 2010</marker>
<rawString>Raghavendra Udupa and Mitesh M. Khapra. 2010. Transliteration equivalence using canonical correlation analysis. In ECIR’10, pages 75–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raghavendra Udupa</author>
<author>K Saravanan</author>
<author>Anton Bakalov</author>
<author>Abhijit Bhole</author>
</authors>
<title>they are out there, if you know where to look&amp;quot;: Mining transliterations of oov query terms for cross-language information retrieval.</title>
<date>2009</date>
<booktitle>In Proceedings of the 31th European Conference on IR Research on Advances in Information Retrieval, ECIR ’09,</booktitle>
<pages>437--448</pages>
<publisher>SpringerVerlag.</publisher>
<location>Berlin, Heidelberg.</location>
<contexts>
<context position="3262" citStr="Udupa et al., 2009" startWordPosition="493" endWordPosition="496">n in the bridge language resources (i.e. between P H Q and Q H R) and aims to minimize this variation as much as possible. Though our technique is general, for clarity we describe it in the context of named entity (NE) transliteration. Named entity (NE) transliteration involves transliterating a name in one language into another language and is shown to be crucial for machine translation (MT) (Knight and Graehl, 1998; AlOnaizan and Knight, 2002; Hermjakob et al., 2008; Li et al., 2009) and cross-lingual information retrieval (CLIR) (AbdulJaleel and Larkey, 2003; Mandl and Womser-Hacker, 2005; Udupa et al., 2009). There exists a large body of literature in transliteration, especially in the bilingual setting, well summarized by Ravi and Knight (2009). We Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 12–23, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics English Bulgarian Word IPA Word IPA bashful /&apos;bæʃfol/ шi46аM /&apos;ʃibom/ tuesday /&apos;tuːzdeɪ/ nyK /luk/ craft /kɹæft/ KаK /kak/ book /bʊk/ Myзей /mʊ&apos;zej/ head /hEd/ criеKа /spE&apos;kɤ/ Table 1: Example phoneme dictionaries in E</context>
</contexts>
<marker>Udupa, Saravanan, Bakalov, Bhole, 2009</marker>
<rawString>Raghavendra Udupa, Saravanan K, Anton Bakalov, and Abhijit Bhole. 2009. &amp;quot;they are out there, if you know where to look&amp;quot;: Mining transliterations of oov query terms for cross-language information retrieval. In Proceedings of the 31th European Conference on IR Research on Advances in Information Retrieval, ECIR ’09, pages 437–448, Berlin, Heidelberg. SpringerVerlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masao Utiyama</author>
<author>Hitoshi Isahara</author>
</authors>
<title>A comparison of pivot methods for phrase-based statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference,</booktitle>
<pages>484--491</pages>
<publisher>ACL.</publisher>
<location>Rochester, New York,</location>
<contexts>
<context position="2194" citStr="Utiyama and Isahara, 2007" startWordPosition="321" endWordPosition="324">heir IPA representation require knowledge about both words and IPA symbols, but it still is specific to one language and, in this sense, we refer to it as a monolingual resource. 12 between English and other languages. Similarly, bilingual dictionaries and transliteration data sets are more accessible from a language into English than into a different language. This situation has caused the NLP community to develop approaches which use a resource rich language (Q say English) as pivot to build resources/applications between a new language pair P and R. Previous studies in machine translation (Utiyama and Isahara, 2007; Paul and Sumita, 2011), transliteration (Khapra et al., 2010), and dictionary mining (Saralegi et al., 2011) show that these bridge language approaches perform competitively with approaches that use resources between P and R. In this paper, we propose a regularization framework for bridge language approaches and show its effectiveness for name transliteration task. The key idea of our approach is that it accounts for language specific variation in the bridge language resources (i.e. between P H Q and Q H R) and aims to minimize this variation as much as possible. Though our technique is gene</context>
</contexts>
<marker>Utiyama, Isahara, 2007</marker>
<rawString>Masao Utiyama and Hitoshi Isahara. 2007. A comparison of pivot methods for phrase-based statistical machine translation. In Proceedings of Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference, pages 484–491, Rochester, New York, April. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Su-Youn Yoon</author>
<author>Kyoung-Young Kim</author>
<author>Richard Sproat</author>
</authors>
<title>Multilingual transliteration using feature based phonetic method.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>112--119</pages>
<publisher>ACL.</publisher>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="24396" citStr="Yoon et al., 2007" startWordPosition="4206" endWordPosition="4209">riginal setting, authors use English as the pivot and since the feature space of English is fixed, irrespective of the target language, this may not be a serious concern but it becomes crucial when we use IPA as the bridge language. Approaches that map words in different languages into the common phonemic space have also been well studied. But most of these approaches use language specific resources such as CMU pronunciation dictionary (Gao et al., 2004) or a carefully constructed cost matrices for addition, substitution, and deletion of phonemes between a pair of languages (Tao et al., 2006; Yoon et al., 2007). Variants of soundex algorithm (Odel and Russel, 1918) such as Kodex (Kang and Choi, 2000) use hand constructed consonant to soundex code tables for name transliteration. Similar to our approach these variants only require soundex mappings of a new language to build transliteration system, but our model does not require explicit mapping between n-gram characters and the IPA symbols instead it learns them automatically using phoneme dictionaries. Alternatively unsupervised approaches have also been explored (Ravi and Knight, 2009), but their accuracies are fairly low compared to the supervised</context>
</contexts>
<marker>Yoon, Kim, Sproat, 2007</marker>
<rawString>Su-Youn Yoon, Kyoung-Young Kim, and Richard Sproat. 2007. Multilingual transliteration using feature based phonetic method. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 112–119, Prague, Czech Republic, June. ACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
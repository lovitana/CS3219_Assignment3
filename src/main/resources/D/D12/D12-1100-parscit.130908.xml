<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000168">
<title confidence="0.997452">
Sketch Algorithms for Estimating Point Queries in NLP
</title>
<author confidence="0.987999">
Amit Goyal and Hal Daum´e III Graham Cormode
</author>
<affiliation confidence="0.992719">
University of Maryland AT&amp;T Labs–Research
</affiliation>
<email confidence="0.996614">
{amit,hal}@umiacs.umd.edu graham@research.att.com
</email>
<sectionHeader confidence="0.997356" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999650583333333">
Many NLP tasks rely on accurate statis-
tics from large corpora. Tracking com-
plete statistics is memory intensive, so recent
work has proposed using compact approx-
imate “sketches” of frequency distributions.
We describe 10 sketch methods, including ex-
isting and novel variants. We compare and
study the errors (over-estimation and under-
estimation) made by the sketches. We evaluate
several sketches on three important NLP prob-
lems. Our experiments show that one sketch
performs best for all the three tasks.
</bodyText>
<sectionHeader confidence="0.999393" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999813807017544">
Since the emergence of the World Wide Web, so-
cial media and mobile devices, we have ever larger
and richer examples of text data. Such vast cor-
pora have led to leaps in the performance of many
language-based tasks: the concept is that simple
models trained on big data can outperform more
complex models with fewer examples. However,
this new view comes with its own challenges: prin-
cipally, how to effectively represent such large data
sets so that model parameters can be efficiently ex-
tracted? One answer is to adopt compact summaries
of corpora in the form of probabilistic “sketches”.
In recent years, the field of Natural Language Pro-
cessing (NLP) has seen tremendous growth and in-
terest in the use of approximation, randomization,
and streaming techniques for large-scale problems
(Brants et al., 2007; Turney, 2008). Much of this
work relies on tracking very many statistics. For ex-
ample, storing approximate counts (Talbot and Os-
borne, 2007; Van Durme and Lall, 2009a; Goyal
and Daum´e III, 2011a), computing approximate as-
sociation scores like Pointwise Mutual Information
(Li et al., 2008; Van Durme and Lall, 2009b; Goyal
and Daum´e III, 2011a), finding frequent items (like
n-grams) (Goyal et al., 2009), building streaming
language models (Talbot and Brants, 2008; Leven-
berg and Osborne, 2009), and distributional similar-
ity (Ravichandran et al., 2005; Van Durme and Lall,
2010). All these problems ultimately depend on ap-
proximate counts of items (such as n-grams, word
pairs and word-context pairs). Thus we focus on
solving this central problem in the context of NLP
applications.
Sketch algorithms (Charikar et al., 2004; Cor-
mode, 2011) are a memory- and time-efficient so-
lution to answering point queries. Recently in NLP,
we (Goyal and Daum´e III, 2011a) demonstrated that
a version of the Count-Min sketch (Cormode and
Muthukrishnan, 2004) accurately solves three large-
scale NLP problems using small bounded memory
footprint. However, there are several other sketch al-
gorithms, and it is not clear why this instance should
be preferred amongst these. In this work, we con-
duct a systematic study and compare many sketch
techniques which answer point queries with focus
on large-scale NLP tasks. While sketches have been
evaluated within the database community for find-
ing frequent items (Cormode and Hadjieleftheriou,
2008) and join-size estimation (Rusu and Dobra,
2007), this is the first comparative study for NLP
problems.
Our work includes three contributions: (1)
We propose novel variants of existing sketches
by extending the idea of conservative update to
them. We propose Count sketch (Charikar et al.,
2004) with conservative update (COUNT-CU) and
Count-mean-min sketch with conservative update
</bodyText>
<page confidence="0.8873">
1093
</page>
<note confidence="0.774586">
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1093–1103, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.999832533333333">
(CMM-CU). The motivation behind proposing new
sketches is inspired by the success of Count-Min
sketch with conservative update in our earlier work
(Goyal and Daum´e III, 2011a). (2) We empirically
compare and study the errors in approximate counts
for several sketches. Errors can be over-estimation,
under-estimation, or a combination of the two. We
also evaluate their performance via Pointwise Mu-
tual Information and LogLikelihood Ratio. (3) We
use sketches to solve three important NLP problems.
Our experiments show that sketches can be very ef-
fective for these tasks, and that the best results are
obtained using the “conservative update” technique.
Across all the three tasks, one sketch (CM-CU) per-
forms best.
</bodyText>
<sectionHeader confidence="0.998572" genericHeader="introduction">
2 Sketches
</sectionHeader>
<bodyText confidence="0.999910159090909">
In this section, we review existing sketch algorithms
from the literature, and propose novel variants based
on the idea of conservative update (Estan and Vargh-
ese, 2002). The term ‘sketch’ refers to a class of
algorithm that represents a large data set with a
compact summary, typically much smaller than the
full size of the input. Given an input of N items
(x1, x2 ... xN), each item x (where x is drawn from
some domain U) is mapped via hash functions into
a small sketch vector that records frequency infor-
mation. Thus, the sketch does not store the items
explicitly, but only information about the frequency
distribution. Sketches support fundamental queries
on their input such as point, range and inner product
queries to be quickly answered approximately. In
this paper, we focus on point queries, which ask for
the (approximate) count of a given item.
The algorithms we consider are randomized and
approximate. They have two user-chosen parame-
ters E and S. E controls the amount of tolerable error
in the returned count and S controls the probability
with which the error exceeds the bound E. These
values of E and S determine respectively the width
w and depth d of a two-dimensional array sk[·, ·] of
count information. The depth d denotes the num-
ber of hash functions employed by the sketch algo-
rithms.
Sketch Operations. Every sketch has two opera-
tions: UPDATE and QUERY to update and estimate
the count of an item. They all guarantee essentially
constant time operation (technically, this grows as
O(log(1�) but in practice this is set to a constant)
per UPDATE and QUERY. Moreover, sketches can be
combined: given two sketches s1 and s2 computed
(using the same parameters w and d, and same set
of d hash functions) over different inputs, a sketch
of the combined input is obtained by adding the in-
dividual sketches, entry-wise. The time to perform
the COMBINE operation on sketches is O(d x w),
independent of the data size. This property enables
sketches to be implemented in distributed setting,
where each machine computes the sketch over a
small portion of the corpus and makes it scalable
to large datasets.
</bodyText>
<subsectionHeader confidence="0.996002">
2.1 Existing sketch algorithms
</subsectionHeader>
<bodyText confidence="0.981399606060606">
This section describes sketches from the literature:
Count-Min sketch (CM): The CM (Cormode and
Muthukrishnan, 2004) sketch has been used effec-
tively for many large scale problems across sev-
eral areas, such as Security (Schechter et al., 2010),
Machine Learning (Shi et al., 2009; Aggarwal
and Yu, 2010), Privacy (Dwork et al., 2010), and
NLP (Goyal and Daum´e III, 2011a). The sketch
stores an array of size d x w counters, along with d
hash functions (drawn from a pairwise-independent
family), one for each row of the array. Given an in-
put of N items (x1, x2 ... xN), each of the hash func-
tions hk:U —* {1... w}, V1 &lt; k &lt; d, takes an item
from the input and maps it into a counter indexed by
the corresponding hash function.
UPDATE: For each new item “x” with count c, the
sketch is updated as:
sk[k, hk(x)] +-- sk[k, hk(x)] + c, V1 &lt; k &lt; d.
QUERY: Since multiple items are hashed to the
same index for each array row, the stored frequency
in each row is guaranteed to overestimate the true
count, making it a biased estimator. Therefore, to
answer the point query (QUERY (x)), CM returns the
minimum over all the d positions x is stored.
c(x) = mink sk[k, hk(x)], V1 &lt; k &lt; d.
Setting w=2
and d=log(1�) ensures all reported
frequencies by CM exceed the true frequencies by
at most EN with probability of at least 1 − S. This
makes the space used by the algorithm O(1� log 1).
Spectral Bloom Filters (SBF): Cohen and Matias
(2003) proposed SBF, an extension to Bloom
Filters (Bloom, 1970) to answer point queries. The
</bodyText>
<page confidence="0.991771">
1094
</page>
<bodyText confidence="0.919823">
UPDATE and QUERY procedures for SBF are the
same as Count-Min (CM) sketch, except that the
range of all the hash functions for SBF are the full
array: hk:U —* {1 ... w x d}, V1 &lt; k &lt; d. While
CM and SBF are very similar, only CM provides
guarantees on the query error.
Count-mean-min (CMM): The motivation behind
the CMM (Deng and Rafiei, 2007) sketch is to
provide an unbiased estimator for Count-Min
(CM) sketch. The construction of CMM sketch
is identical to the CM sketch, while the QUERY
procedure differs. Instead of returning the minimum
value over the d counters (indexed by d hash
functions), CMM deducts the value of estimated
noise from each of the d counters, and return the
median of the d residues. The noise is estimated
as (N − sk[k, hk(x)])/(w − 1). Nevertheless,
the median estimate ( i1) over the d residues can
overestimate more than the original CM sketch min
estimate ( f2), so we return min ( �f1, f2) as the final
estimate for CMM sketch. CMM gives the same
theoretical guarantees as Count sketch (below).
Count sketch (COUNT) (Charikar et al., 2004):
COUNT (aka Fast-AGMS) keeps two hash
functions for each row, hk maps items onto
[1,w], and gk maps items onto {−1,+1}. UP-
DATE: For each new item “x” with count c:
sk[k, hk(x)] &lt;-- sk[k, hk(x)] + c · gk(x), V1 &lt; k &lt; d.
QUERY: the median over the d rows is an unbiased
estimator of the point query:
</bodyText>
<equation confidence="0.9530855">
�
c(x) = mediank sk[k, hk(x)] · gk(x), V1 &lt; k &lt; d.
Setting w= E and d=log (4δ) ensures that all re-
E2
</equation>
<bodyText confidence="0.9466685">
frequencies have error at most E N 2 1/2
P q (�Z=1 fz )
&lt; cN with probability at least 1−δ. The space used
by the algorithm is O( 1E2 log δ1).
</bodyText>
<subsectionHeader confidence="0.998946">
2.2 Conservative Update sketch algorithms
</subsectionHeader>
<bodyText confidence="0.989569203389831">
In this section, we propose novel variants of existing
sketches (see Section 2) by combining them with the
conservative update process (Estan and Varghese,
2002). The idea of conservative update (also known
as Minimal Increase (Cohen and Matias, 2003)) is to
only increase counts in the sketch by the minimum
amount needed to ensure the estimate remains accu-
rate. It can easily be applied to Count-Min (CM)
sketch and Spectral Bloom Filters (SBF) to further
improve the estimate of a point query. Goyal and
Daum´e III (2011a) showed that CM sketch with
conservative update reduces the amount of over-
estimation error by a factor of at least 1.5, and also
improves performance on three NLP tasks.
Note that while conservative update for CM and
SBF never increases the error, there is no guaranteed
improvement. The method relies on seeing multiple
updates in sequence. When a large corpus is being
summarized in a distributed setting, we can apply
conservative update on each sketch independently
before combining the sketches together (see “Sketch
Operations” in Section 2).
Count-Min sketch with conservative update
(CM-CU): The QUERY procedure for CM-CU
(Cormode, 2009; Goyal and Daum´e III, 2011a) is
identical to Count-Min. However, to UPDATE an
item “x” with frequency c, we first compute the fre-
quency c(x) of this item from the existing data struc-
ture (V1 &lt; k &lt; d, c(x) = mink sk[k, hk(x)]) and
the counts are updated according to:
sk[k, hk(x)] &lt;-- max{sk[k, hk(x)], c(x) + c} (*).
The intuition is that, since the point query returns
the minimum of all the d values, we update a
counter only if it is necessary as indicated by (*).
This heuristic avoids unnecessarily updating counter
values to reduce the over-estimation error.
Spectral Bloom Filters with conservative update
(SBF-CU): The QUERY procedure for SBF-CU
(Cohen and Matias, 2003) is identical to SBF.
SBF-CU UPDATE procedure is similar to CM-CU,
with the difference that all d hash functions have the
common range d x w.
Count-mean-min with conservative update
(CMM-CU): We propose a new variant to reduce
the over-estimation error for CMM sketch. The
construction of CMM-CU is identical to CM-CU.
However, due to conservative update, each row of
the sketch is not updated for every update, hence
the sum of counts over each row (Ei sk[k, i],
V1 &lt; k &lt; d) is not equal to input size N.
Hence, the estimated noise to be subtracted here is
(Ei sk[k, i] − sk[k, hk(x)]) / (w − 1). CMM-CU
deducts the value of estimated noise from each of
the d counters, and returns the median of the d
residues as the point query.
Count sketch with conservative update (COUNT-
CU): We propose a new variant to reduce over-
estimation error for the COUNT sketch. The
QUERY procedure for COUNT-CU is the same as
</bodyText>
<page confidence="0.961468">
1095
</page>
<bodyText confidence="0.997156333333333">
COUNT. The UPDATE procedure follows the same
outline as CM-CU, but uses the current estimate
c(x) from the COUNT sketch, i.e.
</bodyText>
<equation confidence="0.956915">
c(x) = mediank sk[k, hk(x)] · A(x), b1 &lt; k &lt; d.
</equation>
<bodyText confidence="0.995571848484848">
Note, this heuristic is not as strong as for CM-CU
and SBF-CU because COUNT can have both over-
estimate and under-estimate errors.
Lossy counting with conservative update (LCU-
WS): LCU-WS (Goyal and Daum´e III, 2011b) was
proposed to reduce the amount of over-estimation
error for CM-CU sketch, without incurring too
much under-estimation error. This scheme is in-
spired by lossy counting (Manku and Motwani,
2002). In this approach, the input sequence is con-
ceptually divided into windows, each containing 1/-y
items. The size of each window is equal to size of
the sketch i.e. d x w. Note that there are -yN win-
dows; let t denote the index of current window. At
window boundaries, b 1 &lt; i &lt; d, 1 &lt; j &lt; w,
if (sk[i, j] &gt; 0 and sk[i, j] &lt; t), then sk[i, j] +—
sk[i, j]−1. The idea is to remove the contribution of
small items colliding in the same entry, while not al-
tering the count of frequent items. The current win-
dow index is used to draw this distinction. Here, all
reported frequencies f have both under and over es-
timation error: f − -yN &lt; f�&lt; f + EN.
Lossy counting with conservative update II
(LCU-SWS): This is a variant of the previous
scheme, where the counts of the sketch are de-
creased more conservatively. Hence, this scheme
has worse over-estimation error compared to LCU-
WS, with better under-estimation. Here, only those
counts are decremented which are at most the square
root of current window index, t. At window bound-
aries, b 1 &lt; i &lt; d, 1 &lt; j &lt; w, if (sk[i, j] &gt; 0 and
sk[i, j] &lt; F.\/t]), then sk[i, j] +— sk[i, j] − 1. LCU-
SWS has similar analytical bounds to LCU-WS.
</bodyText>
<sectionHeader confidence="0.99931" genericHeader="method">
3 Intrinsic Evaluations
</sectionHeader>
<bodyText confidence="0.998649166666667">
We empirically compare and study the errors in ap-
proximate counts for all 10 sketches. Errors can be
over-estimation, under-estimation, or a combination
of the two. We also study the behavior of approxi-
mate Pointwise Mutual Information and Log Likeli-
hood Ratio for the sketches.
</bodyText>
<subsectionHeader confidence="0.997019">
3.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.999761352941177">
DATA: We took 50 million random sentences from
Gigaword (Graff, 2003). We split this data in
10 chunks of 5 million sentences each. Since all
sketches have probabilistic bounds, we report aver-
age results over these 10 chunks. For each chunk,
we generate counts of all word pairs within a win-
dow size 7. This results in an average stream size of
194 million word pair tokens and 33.5 million word
pair types per chunk.
To compare error in various sketch counts, first we
compute the exact counts of all the word pairs. Sec-
ond, we store the counts of all the word pairs in all
the sketches. Third, we query sketches to generate
approximate counts of all the word pairs. Recall, we
do not store the word pairs explicitly in sketches but
only a compact summary of the associated counts.
We fix the size of each sketch to be w = 20×106
</bodyText>
<page confidence="0.542579">
3
</page>
<bodyText confidence="0.999987277777778">
and d = 3. We keep the size of sketches equal
to allow fair comparison among them. Prior work
(Deng and Rafiei, 2007; Goyal and Daum´e III,
2011a) showed with fixed sketch size, a small num-
ber of hash functions (d=number of hash functions)
with large w (or range) give rise to small error over
counts. Next, we group all word pairs with the
same true frequency into a single bucket. We then
compute the Mean Relative Error (MRE) in each of
these buckets. Because different sketches have dif-
ferent accuracy behavior on low, mid, and high fre-
quency counts, making this distinction based on fre-
quency lets us determine the regions in which dif-
ferent sketches perform best. Mean Relative Error
(MRE) is defined as the average of absolute differ-
ence between the predicted and the exact value di-
vided by the exact value over all the word pairs in
each bucket.
</bodyText>
<subsectionHeader confidence="0.999933">
3.2 Studying the Error in Counts
</subsectionHeader>
<bodyText confidence="0.999983888888889">
We study the errors produced by all 10 sketches.
Since various sketches result in different errors on
low, mid, and high frequency counts, we plot the re-
sults with a linear error scale (Fig. 1(a)) to highlight
the performance for low frequency counts, and with
a log error scale (Fig. 1(b)) for mid and high fre-
quency counts.
We make several observations on low frequency
counts from Fig. 1(a). (1) Count-Min (CM) and
</bodyText>
<page confidence="0.961411">
1096
</page>
<figure confidence="0.999263303030303">
Mean Relative Error (log scale)
(a) Focusing on low frequency counts
100 102 104
True frequency counts of word pairs (log scale)
(b) Focusing on mid and high frequency counts
CM
CM−CU
SBF
SBF−CU
COUNT
COUNT−CU
CMM
CMM−CU
LCU−SWS
LCU−WS
Mean Relative Error
4
6
2
0 100 101 102
True frequency counts of word pairs (log scale)
8
100
10−2
10−4
CM
CM−CU
COUNT
COUNT−CU
CMM
CMM−CU
LCU−SWS
LCU−WS
</figure>
<figureCaption confidence="0.999161">
Figure 1: Comparing several sketches for input size of 75 million word pairs. Size of each sketch: w = 20×106
</figureCaption>
<equation confidence="0.2669145">
3 and d = 3. All
items with same exact count are put in one bucket and we plot Mean Relative Error on the y-axis with exact counts on the x-axis.
</equation>
<listItem confidence="0.80037775">
Spectral Bloom Filters (SBF) have identical MRE
for word pairs. Using conservative update with CM
(CM-CU) and SBF (SBF-CU) reduces the MRE
by a factor of 1.5. MRE for CM-CU and SBF-
CU is also identical. (2) COUNT has better MRE
than CM-CU and using conservative update with
COUNT (COUNT-CU) further reduces the MRE.
(3) CMM has better MRE than COUNT and using
</listItem>
<bodyText confidence="0.980575636363637">
conservative update with CMM (CMM-CU) fur-
ther reduces the MRE. (4) Lossy counting with con-
servative update variants (LCU-SWS, LCU-WS)
have comparable MRE to COUNT-CU and CMM-
CU respectively.
In Fig. 1(b), we do not plot the SBF variants
as SBF and CM variants had identical MRE in
Fig. 1(a). From Figure 1(b), we observe that,
CM, COUNT, COUNT-CU, CMM, CMM-CU
sketches have worse MRE than CM-CU, LCU-
SWS, and LCU-WS for mid and high frequency
counts. CM-CU, LCU-SWS, and LCU-WS have
zero MRE for all the counts &gt; 1000.
To summarize the above observations, for those
NLP problems where we cannot afford to make
errors on mid and high frequency counts, we
should employ CM-CU, LCU-SWS, and LCU-
WS sketches. If we want to reduce the error on
low frequency counts, LCU-WS generates least er-
ror. For NLP tasks where we can allow error on mid
and high frequency counts but not on low frequency
True frequency counts of word pairs (log scale)
</bodyText>
<figureCaption confidence="0.972993">
Figure 2: Compare several sketches on over-estimation and
under-estimation errors with respect to exact counts.
</figureCaption>
<bodyText confidence="0.932241">
counts, CMM-CU sketch is best.
</bodyText>
<subsectionHeader confidence="0.999798">
3.3 Examining OE and UE errors
</subsectionHeader>
<bodyText confidence="0.999921555555555">
In many NLP applications, we are willing to tolerate
either over-estimation or under-estimation errors.
Hence we breakdown the error into over-estimation
(OE) and under-estimation (UE) errors for the six
best-performing sketches (COUNT, COUNT-CU,
CMM, CMM-CU, LCU-SWS, and LCU-WS). To
accomplish that, rather than using absolute error val-
ues, we divide the values into over-estimation (pos-
itive), and under-estimation (negative) error buck-
</bodyText>
<figure confidence="0.998320173913044">
COUNT−CU−OE
COUNT−CU−UE
CMM−CU−OE
CMM−CU−UE
LCU−SWS−OE
LCU−SWS−UE
LCU−WS−OE
LCU−WS−UE
0.5
0
−0.5
−1 100 101 102 103
Mean Relative Error
1.5
2
1
1097
Recall
Top−K
(a) PMI
1
0.8
0.6
CM−CU
COUNT−CU
CMM−CU
LCU−SWS
LCU−WS
0101 102 103 104 105
0.4
0.2
1
CM−CU
COUNT−CU
CMM−CU
LCU−SWS
LCU−WS
0.6
101 102 103 104 105
Top−K
(b) LLR
0.5
0.9
Recall
0.8
0.7
</figure>
<figureCaption confidence="0.999973">
Figure 3: Evaluate the approximate PMI and LLR rankings (obtained using various sketches) with the exact rankings.
</figureCaption>
<bodyText confidence="0.99978708">
ets. Hence, to compute the over-estimation MRE,
we take the average of positive values over all the
items in each bucket. For under-estimation, we
take the average over the negative values. We
can make several interesting observations from Fig-
ure 2: (1) Comparing COUNT-CU and LCU-
SWS, we learn that both have the same over-
estimation errors. However, LCU-SWS has less
under-estimation error than COUNT-CU. There-
fore, LCU-SWS is always better than COUNT-
CU. (2) LCU-WS has less over-estimation than
LCU-SWS but with more under-estimation error on
mid frequency counts. LCU-WS has less under-
estimation error than COUNT-CU. (3) CMM-CU
has the least over-estimation error and most under-
estimation error among all the compared sketches.
From the above experiments, we conclude that
tasks sensitive to under-estimation should use the
CM-CU sketch, which guarantees over-estimation.
However, if we are willing to make some under-
estimation error with less over-estimation error,
then LCU-WS and LCU-SWS are recommended.
Lastly, to have minimal over-estimation error with
willingness to accept large under-estimation error,
CMM-CU is recommended.
</bodyText>
<subsectionHeader confidence="0.985115">
3.4 Evaluating association scores ranking
</subsectionHeader>
<bodyText confidence="0.999957257142857">
Last, in many NLP problems, we are interested in as-
sociation rankings obtained using Pointwise Mutual
Information (PMI) and Log Likelihood Ratio (LLR).
In this experiment, we compare the word pairs asso-
ciation rankings obtained using PMI and LLR from
several sketches and exact word pair counts. We use
recall to measure the number of top-K sorted word
pairs that are found in both the rankings.
In Figure 3(a), we compute the recall for CM-
CU, COUNT-CU, CMM-CU, LCU-SWS, and
LCU-WS sketches at several top-K thresholds of
word pairs for approximate PMI ranking. We
can make several observations from Figure 3(a).
COUNT-CU has the worst recall for almost all the
top-K settings. For top-K values less than 750, all
sketches except COUNT-CU have comparable re-
call. Meanwhile, for K greater than 750, LCU-WS
has the best recall. The is because PMI is sensitive
to low frequency counts (Church and Hanks, 1989),
over-estimation of the counts of low frequency word
pairs can make their approximate PMI scores worse.
In Figure 3(b), we compare the LLR rankings. For
top-K values less than 1000, all the sketches have
comparable recall. For top-K values greater than
1000, CM-CU, LCU-SWS, and LCU-WS perform
better. The reason for such a behavior is due to LLR
favoring high frequency word pairs, and COUNT-
CU and CMM-CU making under-estimation error
on high frequency word pairs.
To summarize, to maintain top-K PMI rank-
ings making over-estimation error is not desirable.
Hence, LCU-WS is recommended for PMI rank-
ings. For LLR, producing under-estimation error is
not preferable and therefore, CM-CU, LCU-WS,
and LCU-SWS are recommended.
</bodyText>
<page confidence="0.972722">
1098
</page>
<table confidence="0.999915625">
Test Set Random Buckets Neighbor
Model CM-CU CMM-CU LCU-WS CM-CU CMM-CU LCU-WS CM-CU CMM-CU LCU-WS
50M 87.2 74.3 86.5 83.9 72.9 83.2 71.7 64.7 72.1
100M 90.4 79.0 91.0 86.5 76.9 86.9 73.4 67.2 74.7
200M 93.3 83.1 92.9 88.3 80.1 88.4 75.0 69.0 75.4
500M 94.4 86.6 94.1 89.3 83.4 89.3 75.7 70.8 75.5
1B 94.4 88.7 94.4 89.5 85.1 89.5 75.8 71.9 75.8
Exact 94.5 89.5 75.8
</table>
<tableCaption confidence="0.9697175">
Table 1: Pseudo-words evaluation on accuracy metric for selectional preferences using several sketches of different sizes against
the exact. There is no statistically significant difference (at P &lt; 0.05 using bootstrap resampling) among bolded numbers.
</tableCaption>
<sectionHeader confidence="0.992179" genericHeader="method">
4 Extrinsic Evaluation
</sectionHeader>
<subsectionHeader confidence="0.953945">
4.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.999993424242424">
We study three important NLP applications, and
compare the three best-performing sketches: Count-
Min sketch with conservative update (CM-CU),
Count-mean-min with conservative update (CMM-
CU), and Lossy counting with conservative update
(LCU-WS). The above mentioned 3 sketches are se-
lected from 10 sketches (see Section 2) considering
these sketches make errors on different ranges of the
counts: low, mid and, high frequency counts as seen
in our intrinsic evaluations in Section 3. The goal
of this experiment is to show the effectiveness of
sketches on large-scale language processing tasks.
These adhere to the premise that simple methods
using large data can dominate more complex mod-
els. We purposefully select simple methods as they
use approximate counts and associations directly to
solve these tasks. This allows us to have a fair com-
parison among different sketches, and to more di-
rectly see the impact of different choices of sketch
on the task outcome. Of course, sketches are still
broadly applicable to many NLP problems where we
want to count (many) items or compute associations:
e.g. language models, Statistical Machine Transla-
tion, paraphrasing, bootstrapping and label propaga-
tion for automatically creating a knowledge base and
finding interesting patterns in social media.
Data: We use Gigaword (Graff, 2003) and a 50%
portion of a copy of news web (GWB50) crawled
by (Ravichandran et al., 2005). The raw size of
Gigaword (GW) and GWB50 is 9.8 GB and 49
GB with 56.78 million and 462.60 sentences respec-
tively. For both the corpora, we split the text into
sentences, tokenize and convert into lower-case.
</bodyText>
<subsectionHeader confidence="0.95353">
4.2 Pseudo-Words Evaluation
</subsectionHeader>
<bodyText confidence="0.999719088235294">
In NLP, it is difficult and time consuming to create
annotated test sets. This problem has motivated the
use of pseudo-words to automatically create the test
sets without human annotation. The pseudo-words
are a common way to evaluate selectional prefer-
ences models (Erk, 2007; Bergsma et al., 2008) that
measure the strength of association between a predi-
cate and its argument filler, e.g., that the noun “song”
is likely to be the object of the verb “sing”.
A pseudo-word is the conflation of two words
(e.g. song/dance). One word is the original in a sen-
tence, and the second is the confounder. For exam-
ple, in our task of selectional preferences, the system
has to decide for the verb “sing” which is the correct
object between “song”/“dance”. Recently, Cham-
bers and Jurafsky (2010) proposed a simple baseline
based on co-occurrence counts of words, which has
state-of-the-art performance on pseudo-words eval-
uation for selectional preferences.
We use a simple approach (without any typed de-
pendency data) similar to Chambers and Jurafsky
(2010), where we count all word pairs (except word
pairs involving stop words) that appear within a win-
dow of size 3 from Gigaword (9.8 GB). That gen-
erates 970 million word pair tokens (stream size)
and 94 million word pair types. Counts of all the
94 million unique word pairs are stored in CM-
CU, CMM-CU, and LCU-WS. For a target verb,
we return that noun which has higher co-occurrence
count with it, as the correct selectional preference.
We evaluate on Chambers and Jurafsky’s three test
sets1 (excluding instances involving stop words) that
are based on different strategies in selecting con-
founders: Random (4081 instances), Buckets (4028
</bodyText>
<footnote confidence="0.9944945">
1http://www.usna.edu/Users/cs/nchamber/
data/pseudowords/
</footnote>
<page confidence="0.992661">
1099
</page>
<tableCaption confidence="0.347487">
True Frequency of word pairs (log−scale)
</tableCaption>
<figureCaption confidence="0.953712">
Figure 4: Determining the proportion of low, mid and high
frequency test word pairs in Gigaword (GW).
</figureCaption>
<bodyText confidence="0.999801848484849">
instances), and Neighbor (3881 instances). To eval-
uate against the exact counts, we compute exact
counts for only those word pairs that are present in
the test sets. Accuracy is used for evaluation and
is defined as the percentage of number of correctly
identified pseudo words.
In Fig. 4, we plot the cumulative proportion of
true frequency counts of all word pairs (from the
three tests) in Gigaword (GW). To include unseen
word pairs from test set in GW on log-scale in Fig.
4, we increment the true counts of all the word pairs
by 1. This plot demonstrates that 45% of word-
pairs are unseen in GW, and 67% of word pairs have
counts less than 10. Hence, to perform better on this
task, it is essential to accurately maintain counts of
rare word pairs.
In Table 1, we vary the size of all sketches (50
million (M), 100M, 200M, 500M and 1 billion
(1B) counters) with 3 hash functions to compare
them against the exact counts. It takes 1.8 GB un-
compressed space to maintain the exact counts on
the disk. Table 1 shows that with sketches of size
&gt; 200M on all the three test sets, CM-CU and
LCU-WS are comparable to exact. However, the
CMM-CU sketch performs less well. We conjec-
ture the reason for such a behavior is due to loss of
recall (information about low frequency word pairs)
by under-estimation error. For this task CM-CU and
LCU-WS scales to storing 94M unique word pairs
using 200M integer (4 bytes each) counters (using
800 MB) &lt; 1.8 GB to maintain exact counts. More-
over, these results are comparable to Chambers and
Jurafsky’s state-of-the-art framework.
</bodyText>
<table confidence="0.999641333333333">
Data Exact CM-CU CMM-CU LCU-WS
GW 74.2 74.0 65.3 72.9
GWB50 81.2 80.9 74.9 78.3
</table>
<tableCaption confidence="0.972084333333333">
Table 2: Evaluating Semantic Orientation on accuracy metric
using several sketches of 2 billion counters against exact. Bold
and italic numbers denote no statistically significant difference.
</tableCaption>
<subsectionHeader confidence="0.999693">
4.3 Finding Semantic Orientation of a word
</subsectionHeader>
<bodyText confidence="0.985979083333333">
Given a word, the task of finding its Semantic Ori-
entation (SO) (Turney and Littman, 2003) is to de-
termine if the word is more probable to be used in
positive or negative connotation. We use Turney and
Littman’s (2003) state-of-the-art framework to com-
pute the SO of a word. We use same seven pos-
itive words (good, nice, excellent, positive, fortu-
nate, correct, and superior) and same seven nega-
tive words (bad, nasty, poor, negative, unfortunate,
wrong, and inferior) from their framework as seeds.
The SO of a given word is computed based on the
strength of its association with the seven positive
words and the seven negative words. Association
scores are computed via Pointwise Mutual Informa-
tion (PMI). We compute the SO of a word “w” as:
SO(W) = EpEPos PMI(p, w)−EnENeg PMI(n, w)
where, Pos and Neg denote the seven positive and
negative seeds respectively. If this score is negative,
we predict the word as negative; otherwise, we pre-
dict it as positive. We use the General Inquirer lex-
icon2 (Stone et al., 1966) as a benchmark to eval-
uate the semantic orientation similar to Turney and
Littman’s (2003) work. Our test set consists of 1611
positive and 1987 negative words. Accuracy is used
for evaluation and is defined as the percentage of
number of correctly identified SO words.
We evaluate SO of words on two different
sized corpora (see Section 4.1): Gigaword (GW)
(9.8GB), and GW with 50% news web corpus
(GWB50) (49GB). We fix the size of all sketches
to 2 billion (2B) counters with 5 hash functions. We
store exact counts of all words in a hash table for
both GW and GWB50. We count all word pairs
(except word pairs involving stop words) that appear
within a window of size 7 from GW and GWB50.
This yields 2.67 billion(B) tokens and .19B types
</bodyText>
<footnote confidence="0.988311">
2The General Inquirer lexicon is freely available at http:
//www.wjh.harvard.edu/˜inquirer/
</footnote>
<figure confidence="0.655781714285714">
0.8
0.6
0.4100 102 104 106
1
Pseudo test sets
SO test set
Cumulative Proportion
</figure>
<page confidence="0.638459">
1100
</page>
<table confidence="0.9998783">
Test Set WS-203 MC-30
Model
LLR PMI 10M .58 .25 .28 .67 .20 .16
50M .44 .23 .41 .61 .22 .31
200M .53 .44 .47 .57 .28 .43
Exact .52 .50
10M .47 .27 .29 .50 .29 .10
50M .42 .31 .34 .48 .32 .35
200M .41 .35 .39 .40 .31 .40
Exact .42 .41
</table>
<tableCaption confidence="0.994957333333333">
Table 3: Evaluating distributional similarity using sketches.
Scores are evaluated using rank correlation p. Bold and italic
numbers denote no statistically significant difference.
</tableCaption>
<bodyText confidence="0.999624481481481">
from GW and 13.20B tokens and 0.8B types from
GWB50. Next, we compare the sketches against the
exact counts over two different size corpora.
Table 2 shows that increasing the amount of data
improves the accuracy of identifying the SO of a
word. We get an absolute increase of 7 percentage
points (with exact counts) in accuracy (The 95% sta-
tistical significance boundary for accuracy is about
+ 1.5.), when we add 50% web data (GWB50).
CM-CU results are equivalent to exact counts for all
the corpus sizes. These results are also comparable
to Turney’s (2003) accuracy of 82.84%. However,
CMM-CU results are worse by absolute 8.7 points
and 6 points on GW and GWB50 respectively with
respect to CM-CU. LCU-WS is better than CMM-
CU but worse than CM-CU. Using 2B integer (4
bytes each) counters (bounded memory footprint of
8 GB), CM-CU scales to 0.8B word pair types (It
takes 16 GB uncompressed disk space to store exact
counts of all the unique word pair types.).
Figure 4 has similar frequency distribution of
word pairs3 in SO test set as pseudo-words evalu-
ation test sets word pairs. Hence, CMM-CU again
has substantially worse results than CM-CU due to
loss of recall (information about low frequency word
pairs) by under-estimation error. We can conclude
that for this task CM-CU is best.
</bodyText>
<subsectionHeader confidence="0.988695">
4.4 Distributional Similarity
</subsectionHeader>
<bodyText confidence="0.8431655">
Distributional similarity is based on the distribu-
tional hypothesis that similar terms appear in simi-
</bodyText>
<footnote confidence="0.842496">
3Consider only those pairs in which one word appears in the
seed list and the other word appears in the test set.
</footnote>
<bodyText confidence="0.999945541666667">
lar contexts (Firth, 1968; Harris, 1954). The context
vector for each term is represented by the strength
of association between the term and each of the lex-
ical, semantic, syntactic, and/or dependency units
that co-occur with it. For this work, we define con-
text for a given term as the surrounding words ap-
pearing in a window of 2 words to the left and 2
words to the right. The context words are concate-
nated along with their positions -2, -1, +1, and +2.
We use PMI and LLR to compute the association
score (AS) between the term and each of the context
to generate the context vector. We use the cosine
similarity measure to find the distributional similar-
ity between the context vectors for each of the terms.
We use two test sets which consist of word pairs,
and their corresponding human rankings. We gener-
ate the word pair rankings using distributional sim-
ilarity. We report the Spearman’s rank correlation
coefficient (p) between the human and distributional
similarity rankings. We report results on two test
sets: WS-203: A set of 203 word pairs marked ac-
cording to similarity (Agirre et al., 2009). MC-30:
A set of 30 noun pairs (Miller and Charles, 1991).
We evaluate distributional similarity on Giga-
word (GW) (9.8GB) (see Section 4.1). First, we
store exact counts of all words and contexts in a hash
table from GW. Next, we count all the word-context
pairs and store them in CM-CU, CMM-CU, and
LCU-WS sketches. That generates a stream of
size 3.35 billion (3.35B) word-context pair tokens
and 215 million unique word-context pair types (It
takes 4.6 GB uncompressed disk space to store exact
counts of all these unique word-context pair types.).
For every target word in the test set, we maintain
top-1000 approximate AS scores contexts using a
priority queue, by passing over the corpus a second
time. Finally, we use cosine similarity with these
approximate top-K context vectors to compute dis-
tributional similarity.
In Table 3, we vary the size of all sketches across
10 million (M), 50M, and 200M counters with 3
hash functions. The results using PMI shows that
CM-CU has best p on both WS-203 and MC-30
test sets. The results for LLR in Table 3 show simi-
lar trends with CM-CU having best results on small
size sketches. Thus, CM-CU scales using 10M
counters (using fixed memory of 40 MB versus 4.6
GB to store exact counts). These results are compa-
</bodyText>
<page confidence="0.97785">
1101
</page>
<bodyText confidence="0.999915727272727">
rable against the state-of-the-art results for distribu-
tional similarity (Agirre et al., 2009).
On this task CM-CU is best as it avoids loss
of recall (information about low frequency word
pairs) due to under-estimation error. For a target
word that has low frequency, using CMM-CU will
not generate any contexts for it, as it will have
large under-estimation error for word-context pairs
counts. This phenomenon is demonstrated in Ta-
ble 3, where CMM-CU and LCU-WS have worse
result with small size sketches.
</bodyText>
<sectionHeader confidence="0.999261" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999961851851852">
In this work, we systematically studied the problem
of estimating point queries using different sketch al-
gorithms. As far as we know, this represents the
first comparative study to demonstrate the relative
behavior of sketches in the context of NLP applica-
tions. We proposed two novel sketch variants: Count
sketch (Charikar et al., 2004) with conservative up-
date (COUNT-CU) and Count-mean-min sketch
with conservative update (CMM-CU). We empiri-
cally showed that CMM-CU has under-estimation
error with small over-estimation error, CM-CU has
only over-estimation error, and LCU-WS has more
under-estimation error than over-estimation error.
Finally, we demonstrated CM-CU has better re-
sults on all three tasks: pseudo-words evaluation
for selectional preferences, finding semantic orien-
tation task, and distributional similarity. This shows
that maintaining information about low frequency
items (even with over-estimation error) is better than
throwing away information (under-estimation error)
about rare items.
Future work is to reduce the bit size of each
counter (instead of the number of counters), as has
been tried for other summaries (Talbot and Osborne,
2007; Talbot, 2009; Van Durme and Lall, 2009a) in
NLP. However, it may be challenging to combine
this with conservative update.
</bodyText>
<sectionHeader confidence="0.998812" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999762">
This work was partially supported by NSF Award
IIS-1139909. Thanks to Suresh Venkatasubrama-
nian for useful discussions and the anonymous re-
viewers for many helpful comments.
</bodyText>
<sectionHeader confidence="0.995848" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999702452830188">
Charu C. Aggarwal and Philip S. Yu. 2010. On classi-
fication of high-cardinality data streams. In SDM’10,
pages 802–813.
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana
Kravalova, Marius Pas¸ca, and Aitor Soroa. 2009. A
study on similarity and relatedness using distributional
and wordnet-based approaches. In NAACL ’09: Pro-
ceedings of HLT-NAACL.
Shane Bergsma, Dekang Lin, and Randy Goebel. 2008.
Discriminative learning of selectional preference from
unlabeled text. In Proc. EMNLP, pages 59–68, Hon-
olulu, Hawaii, October.
Burton H. Bloom. 1970. Space/time trade-offs in hash
coding with allowable errors. Communications of the
ACM, 13:422–426.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.
Och, and Jeffrey Dean. 2007. Large language mod-
els in machine translation. In Proceedings of EMNLP-
CoNLL.
Nathanael Chambers and Dan Jurafsky. 2010. Improv-
ing the use of pseudo-words for evaluating selectional
preferences. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguistics,
ACL ’10, pages 445–453. Association for Computa-
tional Linguistics.
Moses Charikar, Kevin Chen, and Martin Farach-Colton.
2004. Finding frequent items in data streams. Theor.
Comput. Sci., 312:3–15, January.
K. Church and P. Hanks. 1989. Word Associa-
tion Norms, Mutual Information and Lexicography.
In Proceedings of ACL, pages 76–83, Vancouver,
Canada, June.
Saar Cohen and Yossi Matias. 2003. Spectral bloom fil-
ters. In Proceedings of the 2003 ACM SIGMOD in-
ternational conference on Management of data, SIG-
MOD ’03, pages 241–252. ACM.
Graham Cormode and Marios Hadjieleftheriou. 2008.
Finding frequent items in data streams. In VLDB.
Graham Cormode and S. Muthukrishnan. 2004. An im-
proved data stream summary: The count-min sketch
and its applications. J. Algorithms.
Graham Cormode. 2009. Encyclopedia entry on ’Count-
Min Sketch’. In Encyclopedia of Database Systems,
pages 511–516. Springer.
Graham Cormode. 2011. Sketch techniques for approx-
imate query processing. Foundations and Trends in
Databases. NOW publishers.
Fan Deng and Davood Rafiei. 2007. New estimation al-
gorithms for streaming data: Count-min can do more.
http://webdocs.cs.ualberta.ca/.
Cynthia Dwork, Moni Naor, Toniann Pitassi, Guy N.
Rothblum, and Sergey Yekhanin. 2010. Pan-private
streaming algorithms. In Proceedings of ICS.
</reference>
<page confidence="0.942952">
1102
</page>
<reference confidence="0.999863113636364">
Katrin Erk. 2007. A simple, similarity-based model for
selectional preferences. In Proceedings of the 45th An-
nual Meeting of the Association of Computational Lin-
guistics, volume 45, pages 216–223. Association for
Computational Linguistics.
Cristian Estan and George Varghese. 2002. New di-
rections in traffic measurement and accounting. SIG-
COMM Comput. Commun. Rev., 32(4).
J. Firth. 1968. A synopsis of linguistic theory 1930-
1955. In F. Palmer, editor, Selected Papers of J. R.
Firth. Longman.
Amit Goyal and Hal Daum´e III. 2011a. Approximate
scalable bounded space sketch for large data NLP. In
Empirical Methods in Natural Language Processing
(EMNLP).
Amit Goyal and Hal Daum´e III. 2011b. Lossy con-
servative update (LCU) sketch: Succinct approximate
count storage. In Conference on Artificial Intelligence
(AAAI).
Amit Goyal, Hal Daum´e III, and Suresh Venkatasubra-
manian. 2009. Streaming for large scale NLP: Lan-
guage modeling. In NAACL.
D. Graff. 2003. English Gigaword. Linguistic Data Con-
sortium, Philadelphia, PA, January.
Z. Harris. 1954. Distributional structure. Word 10 (23),
pages 146–162.
Abby Levenberg and Miles Osborne. 2009. Stream-
based randomised language models for SMT. In
EMNLP, August.
Ping Li, Kenneth Ward Church, and Trevor Hastie. 2008.
One sketch for all: Theory and application of condi-
tional random sampling. In Neural Information Pro-
cessing Systems, pages 953–960.
G. S. Manku and R. Motwani. 2002. Approximate fre-
quency counts over data streams. In VLDB.
G.A. Miller and W.G. Charles. 1991. Contextual corre-
lates of semantic similarity. Language and Cognitive
Processes, 6(1):1–28.
Deepak Ravichandran, Patrick Pantel, and Eduard Hovy.
2005. Randomized algorithms and nlp: using locality
sensitive hash function for high speed noun clustering.
In Proceedings of ACL.
Florin Rusu and Alin Dobra. 2007. Statistical analysis of
sketch estimators. In SIGMOD ’07. ACM.
Stuart Schechter, Cormac Herley, and Michael Mitzen-
macher. 2010. Popularity is everything: a new
approach to protecting passwords from statistical-
guessing attacks. In Proceedings of the 5th USENIX
conference on Hot topics in security, HotSec’10, pages
1–8, Berkeley, CA, USA. USENIX Association.
Qinfeng Shi, James Petterson, Gideon Dror, John Lang-
ford, Alex Smola, and S.V.N. Vishwanathan. 2009.
Hash kernels for structured data. Journal Machine
Learning Research, 10:2615–2637, December.
Philip J. Stone, Dexter C. Dunphy, Marshall S. Smith,
and Daniel M. Ogilvie. 1966. The General Inquirer:
A Computer Approach to Content Analysis. MIT
Press.
David Talbot and Thorsten Brants. 2008. Randomized
language models via perfect hash functions. In Pro-
ceedings of ACL-08: HLT.
David Talbot and Miles Osborne. 2007. Smoothed
Bloom filter language models: Tera-scale LMs on the
cheap. In Proceedings of the 2007 Joint Conference on
Empirical Methods in Natural Language Processing
and Computational Natural Language Learning (EM
NLP-CoNLL).
David Talbot. 2009. Succinct approximate counting of
skewed data. In IJCAI’09: Proceedings of the 21st
international jont conference on Artifical intelligence.
Peter D. Turney and Michael L. Littman. 2003. Measur-
ing praise and criticism: Inference of semantic orienta-
tion from association. ACM Trans. Inf. Syst., 21:315–
346, October.
Peter D. Turney. 2008. A uniform approach to analogies,
synonyms, antonyms, and associations. In Proceed-
ings of COLING 2008.
Benjamin Van Durme and Ashwin Lall. 2009a. Prob-
abilistic counting with randomized storage. In IJ-
CAI’09: Proceedings of the 21st international jont
conference on Artifical intelligence.
Benjamin Van Durme and Ashwin Lall. 2009b. Stream-
ing pointwise mutual information. In Advances in
Neural Information Processing Systems 22.
Benjamin Van Durme and Ashwin Lall. 2010. Online
generation of locality sensitive hash signatures. In
Proceedings of the ACL 2010 Conference Short Pa-
pers, pages 231–235, July.
</reference>
<page confidence="0.985235">
1103
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.994289">
<title confidence="0.998753">Sketch Algorithms for Estimating Point Queries in NLP</title>
<author confidence="0.998822">Amit Goyal</author>
<author confidence="0.998822">Hal Daum´e Graham Cormode</author>
<affiliation confidence="0.999942">University of Maryland AT&amp;T Labs–Research</affiliation>
<email confidence="0.999218">graham@research.att.com</email>
<abstract confidence="0.9998">Many NLP tasks rely on accurate statistics from large corpora. Tracking complete statistics is memory intensive, so recent work has proposed using compact approximate “sketches” of frequency distributions. describe methods, including existing and novel variants. We compare and study the errors (over-estimation and underestimation) made by the sketches. We evaluate several sketches on three important NLP prob- Our experiments show that performs best for all the three tasks.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Charu C Aggarwal</author>
<author>Philip S Yu</author>
</authors>
<title>On classification of high-cardinality data streams.</title>
<date>2010</date>
<booktitle>In SDM’10,</booktitle>
<pages>802--813</pages>
<contexts>
<context position="6885" citStr="Aggarwal and Yu, 2010" startWordPosition="1091" endWordPosition="1094">e. The time to perform the COMBINE operation on sketches is O(d x w), independent of the data size. This property enables sketches to be implemented in distributed setting, where each machine computes the sketch over a small portion of the corpus and makes it scalable to large datasets. 2.1 Existing sketch algorithms This section describes sketches from the literature: Count-Min sketch (CM): The CM (Cormode and Muthukrishnan, 2004) sketch has been used effectively for many large scale problems across several areas, such as Security (Schechter et al., 2010), Machine Learning (Shi et al., 2009; Aggarwal and Yu, 2010), Privacy (Dwork et al., 2010), and NLP (Goyal and Daum´e III, 2011a). The sketch stores an array of size d x w counters, along with d hash functions (drawn from a pairwise-independent family), one for each row of the array. Given an input of N items (x1, x2 ... xN), each of the hash functions hk:U —* {1... w}, V1 &lt; k &lt; d, takes an item from the input and maps it into a counter indexed by the corresponding hash function. UPDATE: For each new item “x” with count c, the sketch is updated as: sk[k, hk(x)] +-- sk[k, hk(x)] + c, V1 &lt; k &lt; d. QUERY: Since multiple items are hashed to the same index f</context>
</contexts>
<marker>Aggarwal, Yu, 2010</marker>
<rawString>Charu C. Aggarwal and Philip S. Yu. 2010. On classification of high-cardinality data streams. In SDM’10, pages 802–813.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Enrique Alfonseca</author>
<author>Keith Hall</author>
<author>Jana Kravalova</author>
<author>Marius Pas¸ca</author>
<author>Aitor Soroa</author>
</authors>
<title>A study on similarity and relatedness using distributional and wordnet-based approaches.</title>
<date>2009</date>
<booktitle>In NAACL ’09: Proceedings of HLT-NAACL.</booktitle>
<marker>Agirre, Alfonseca, Hall, Kravalova, Pas¸ca, Soroa, 2009</marker>
<rawString>Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana Kravalova, Marius Pas¸ca, and Aitor Soroa. 2009. A study on similarity and relatedness using distributional and wordnet-based approaches. In NAACL ’09: Proceedings of HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shane Bergsma</author>
<author>Dekang Lin</author>
<author>Randy Goebel</author>
</authors>
<title>Discriminative learning of selectional preference from unlabeled text.</title>
<date>2008</date>
<booktitle>In Proc. EMNLP,</booktitle>
<pages>59--68</pages>
<location>Honolulu, Hawaii,</location>
<contexts>
<context position="25202" citStr="Bergsma et al., 2008" startWordPosition="4251" endWordPosition="4254">d a 50% portion of a copy of news web (GWB50) crawled by (Ravichandran et al., 2005). The raw size of Gigaword (GW) and GWB50 is 9.8 GB and 49 GB with 56.78 million and 462.60 sentences respectively. For both the corpora, we split the text into sentences, tokenize and convert into lower-case. 4.2 Pseudo-Words Evaluation In NLP, it is difficult and time consuming to create annotated test sets. This problem has motivated the use of pseudo-words to automatically create the test sets without human annotation. The pseudo-words are a common way to evaluate selectional preferences models (Erk, 2007; Bergsma et al., 2008) that measure the strength of association between a predicate and its argument filler, e.g., that the noun “song” is likely to be the object of the verb “sing”. A pseudo-word is the conflation of two words (e.g. song/dance). One word is the original in a sentence, and the second is the confounder. For example, in our task of selectional preferences, the system has to decide for the verb “sing” which is the correct object between “song”/“dance”. Recently, Chambers and Jurafsky (2010) proposed a simple baseline based on co-occurrence counts of words, which has state-of-the-art performance on pse</context>
</contexts>
<marker>Bergsma, Lin, Goebel, 2008</marker>
<rawString>Shane Bergsma, Dekang Lin, and Randy Goebel. 2008. Discriminative learning of selectional preference from unlabeled text. In Proc. EMNLP, pages 59–68, Honolulu, Hawaii, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Burton H Bloom</author>
</authors>
<title>Space/time trade-offs in hash coding with allowable errors.</title>
<date>1970</date>
<journal>Communications of the ACM,</journal>
<pages>13--422</pages>
<contexts>
<context position="8072" citStr="Bloom, 1970" startWordPosition="1319" endWordPosition="1320">ashed to the same index for each array row, the stored frequency in each row is guaranteed to overestimate the true count, making it a biased estimator. Therefore, to answer the point query (QUERY (x)), CM returns the minimum over all the d positions x is stored. c(x) = mink sk[k, hk(x)], V1 &lt; k &lt; d. Setting w=2 and d=log(1�) ensures all reported frequencies by CM exceed the true frequencies by at most EN with probability of at least 1 − S. This makes the space used by the algorithm O(1� log 1). Spectral Bloom Filters (SBF): Cohen and Matias (2003) proposed SBF, an extension to Bloom Filters (Bloom, 1970) to answer point queries. The 1094 UPDATE and QUERY procedures for SBF are the same as Count-Min (CM) sketch, except that the range of all the hash functions for SBF are the full array: hk:U —* {1 ... w x d}, V1 &lt; k &lt; d. While CM and SBF are very similar, only CM provides guarantees on the query error. Count-mean-min (CMM): The motivation behind the CMM (Deng and Rafiei, 2007) sketch is to provide an unbiased estimator for Count-Min (CM) sketch. The construction of CMM sketch is identical to the CM sketch, while the QUERY procedure differs. Instead of returning the minimum value over the d cou</context>
</contexts>
<marker>Bloom, 1970</marker>
<rawString>Burton H. Bloom. 1970. Space/time trade-offs in hash coding with allowable errors. Communications of the ACM, 13:422–426.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
<author>Ashok C Popat</author>
<author>Peng Xu</author>
<author>Franz J Och</author>
<author>Jeffrey Dean</author>
</authors>
<title>Large language models in machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLPCoNLL.</booktitle>
<contexts>
<context position="1529" citStr="Brants et al., 2007" startWordPosition="235" endWordPosition="238">any language-based tasks: the concept is that simple models trained on big data can outperform more complex models with fewer examples. However, this new view comes with its own challenges: principally, how to effectively represent such large data sets so that model parameters can be efficiently extracted? One answer is to adopt compact summaries of corpora in the form of probabilistic “sketches”. In recent years, the field of Natural Language Processing (NLP) has seen tremendous growth and interest in the use of approximation, randomization, and streaming techniques for large-scale problems (Brants et al., 2007; Turney, 2008). Much of this work relies on tracking very many statistics. For example, storing approximate counts (Talbot and Osborne, 2007; Van Durme and Lall, 2009a; Goyal and Daum´e III, 2011a), computing approximate association scores like Pointwise Mutual Information (Li et al., 2008; Van Durme and Lall, 2009b; Goyal and Daum´e III, 2011a), finding frequent items (like n-grams) (Goyal et al., 2009), building streaming language models (Talbot and Brants, 2008; Levenberg and Osborne, 2009), and distributional similarity (Ravichandran et al., 2005; Van Durme and Lall, 2010). All these prob</context>
</contexts>
<marker>Brants, Popat, Xu, Och, Dean, 2007</marker>
<rawString>Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och, and Jeffrey Dean. 2007. Large language models in machine translation. In Proceedings of EMNLPCoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nathanael Chambers</author>
<author>Dan Jurafsky</author>
</authors>
<title>Improving the use of pseudo-words for evaluating selectional preferences.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10,</booktitle>
<pages>445--453</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="25689" citStr="Chambers and Jurafsky (2010)" startWordPosition="4335" endWordPosition="4339">sets without human annotation. The pseudo-words are a common way to evaluate selectional preferences models (Erk, 2007; Bergsma et al., 2008) that measure the strength of association between a predicate and its argument filler, e.g., that the noun “song” is likely to be the object of the verb “sing”. A pseudo-word is the conflation of two words (e.g. song/dance). One word is the original in a sentence, and the second is the confounder. For example, in our task of selectional preferences, the system has to decide for the verb “sing” which is the correct object between “song”/“dance”. Recently, Chambers and Jurafsky (2010) proposed a simple baseline based on co-occurrence counts of words, which has state-of-the-art performance on pseudo-words evaluation for selectional preferences. We use a simple approach (without any typed dependency data) similar to Chambers and Jurafsky (2010), where we count all word pairs (except word pairs involving stop words) that appear within a window of size 3 from Gigaword (9.8 GB). That generates 970 million word pair tokens (stream size) and 94 million word pair types. Counts of all the 94 million unique word pairs are stored in CMCU, CMM-CU, and LCU-WS. For a target verb, we ret</context>
</contexts>
<marker>Chambers, Jurafsky, 2010</marker>
<rawString>Nathanael Chambers and Dan Jurafsky. 2010. Improving the use of pseudo-words for evaluating selectional preferences. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10, pages 445–453. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Moses Charikar</author>
<author>Kevin Chen</author>
<author>Martin Farach-Colton</author>
</authors>
<title>Finding frequent items in data streams.</title>
<date>2004</date>
<journal>Theor. Comput. Sci.,</journal>
<pages>312--3</pages>
<contexts>
<context position="2359" citStr="Charikar et al., 2004" startWordPosition="365" endWordPosition="368"> approximate association scores like Pointwise Mutual Information (Li et al., 2008; Van Durme and Lall, 2009b; Goyal and Daum´e III, 2011a), finding frequent items (like n-grams) (Goyal et al., 2009), building streaming language models (Talbot and Brants, 2008; Levenberg and Osborne, 2009), and distributional similarity (Ravichandran et al., 2005; Van Durme and Lall, 2010). All these problems ultimately depend on approximate counts of items (such as n-grams, word pairs and word-context pairs). Thus we focus on solving this central problem in the context of NLP applications. Sketch algorithms (Charikar et al., 2004; Cormode, 2011) are a memory- and time-efficient solution to answering point queries. Recently in NLP, we (Goyal and Daum´e III, 2011a) demonstrated that a version of the Count-Min sketch (Cormode and Muthukrishnan, 2004) accurately solves three largescale NLP problems using small bounded memory footprint. However, there are several other sketch algorithms, and it is not clear why this instance should be preferred amongst these. In this work, we conduct a systematic study and compare many sketch techniques which answer point queries with focus on large-scale NLP tasks. While sketches have bee</context>
<context position="9181" citStr="Charikar et al., 2004" startWordPosition="1516" endWordPosition="1519">ntical to the CM sketch, while the QUERY procedure differs. Instead of returning the minimum value over the d counters (indexed by d hash functions), CMM deducts the value of estimated noise from each of the d counters, and return the median of the d residues. The noise is estimated as (N − sk[k, hk(x)])/(w − 1). Nevertheless, the median estimate ( i1) over the d residues can overestimate more than the original CM sketch min estimate ( f2), so we return min ( �f1, f2) as the final estimate for CMM sketch. CMM gives the same theoretical guarantees as Count sketch (below). Count sketch (COUNT) (Charikar et al., 2004): COUNT (aka Fast-AGMS) keeps two hash functions for each row, hk maps items onto [1,w], and gk maps items onto {−1,+1}. UPDATE: For each new item “x” with count c: sk[k, hk(x)] &lt;-- sk[k, hk(x)] + c · gk(x), V1 &lt; k &lt; d. QUERY: the median over the d rows is an unbiased estimator of the point query: � c(x) = mediank sk[k, hk(x)] · gk(x), V1 &lt; k &lt; d. Setting w= E and d=log (4δ) ensures that all reE2 frequencies have error at most E N 2 1/2 P q (�Z=1 fz ) &lt; cN with probability at least 1−δ. The space used by the algorithm is O( 1E2 log δ1). 2.2 Conservative Update sketch algorithms In this section</context>
<context position="35810" citStr="Charikar et al., 2004" startWordPosition="6059" endWordPosition="6062">t word that has low frequency, using CMM-CU will not generate any contexts for it, as it will have large under-estimation error for word-context pairs counts. This phenomenon is demonstrated in Table 3, where CMM-CU and LCU-WS have worse result with small size sketches. 5 Conclusion In this work, we systematically studied the problem of estimating point queries using different sketch algorithms. As far as we know, this represents the first comparative study to demonstrate the relative behavior of sketches in the context of NLP applications. We proposed two novel sketch variants: Count sketch (Charikar et al., 2004) with conservative update (COUNT-CU) and Count-mean-min sketch with conservative update (CMM-CU). We empirically showed that CMM-CU has under-estimation error with small over-estimation error, CM-CU has only over-estimation error, and LCU-WS has more under-estimation error than over-estimation error. Finally, we demonstrated CM-CU has better results on all three tasks: pseudo-words evaluation for selectional preferences, finding semantic orientation task, and distributional similarity. This shows that maintaining information about low frequency items (even with over-estimation error) is better</context>
</contexts>
<marker>Charikar, Chen, Farach-Colton, 2004</marker>
<rawString>Moses Charikar, Kevin Chen, and Martin Farach-Colton. 2004. Finding frequent items in data streams. Theor. Comput. Sci., 312:3–15, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Church</author>
<author>P Hanks</author>
</authors>
<title>Word Association Norms, Mutual Information and Lexicography.</title>
<date>1989</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>76--83</pages>
<location>Vancouver, Canada,</location>
<contexts>
<context position="21853" citStr="Church and Hanks, 1989" startWordPosition="3716" endWordPosition="3719">pair counts. We use recall to measure the number of top-K sorted word pairs that are found in both the rankings. In Figure 3(a), we compute the recall for CMCU, COUNT-CU, CMM-CU, LCU-SWS, and LCU-WS sketches at several top-K thresholds of word pairs for approximate PMI ranking. We can make several observations from Figure 3(a). COUNT-CU has the worst recall for almost all the top-K settings. For top-K values less than 750, all sketches except COUNT-CU have comparable recall. Meanwhile, for K greater than 750, LCU-WS has the best recall. The is because PMI is sensitive to low frequency counts (Church and Hanks, 1989), over-estimation of the counts of low frequency word pairs can make their approximate PMI scores worse. In Figure 3(b), we compare the LLR rankings. For top-K values less than 1000, all the sketches have comparable recall. For top-K values greater than 1000, CM-CU, LCU-SWS, and LCU-WS perform better. The reason for such a behavior is due to LLR favoring high frequency word pairs, and COUNTCU and CMM-CU making under-estimation error on high frequency word pairs. To summarize, to maintain top-K PMI rankings making over-estimation error is not desirable. Hence, LCU-WS is recommended for PMI rank</context>
</contexts>
<marker>Church, Hanks, 1989</marker>
<rawString>K. Church and P. Hanks. 1989. Word Association Norms, Mutual Information and Lexicography. In Proceedings of ACL, pages 76–83, Vancouver, Canada, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saar Cohen</author>
<author>Yossi Matias</author>
</authors>
<title>Spectral bloom filters.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 ACM SIGMOD international conference on Management of data, SIGMOD ’03,</booktitle>
<pages>241--252</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="8014" citStr="Cohen and Matias (2003)" startWordPosition="1308" endWordPosition="1311">] +-- sk[k, hk(x)] + c, V1 &lt; k &lt; d. QUERY: Since multiple items are hashed to the same index for each array row, the stored frequency in each row is guaranteed to overestimate the true count, making it a biased estimator. Therefore, to answer the point query (QUERY (x)), CM returns the minimum over all the d positions x is stored. c(x) = mink sk[k, hk(x)], V1 &lt; k &lt; d. Setting w=2 and d=log(1�) ensures all reported frequencies by CM exceed the true frequencies by at most EN with probability of at least 1 − S. This makes the space used by the algorithm O(1� log 1). Spectral Bloom Filters (SBF): Cohen and Matias (2003) proposed SBF, an extension to Bloom Filters (Bloom, 1970) to answer point queries. The 1094 UPDATE and QUERY procedures for SBF are the same as Count-Min (CM) sketch, except that the range of all the hash functions for SBF are the full array: hk:U —* {1 ... w x d}, V1 &lt; k &lt; d. While CM and SBF are very similar, only CM provides guarantees on the query error. Count-mean-min (CMM): The motivation behind the CMM (Deng and Rafiei, 2007) sketch is to provide an unbiased estimator for Count-Min (CM) sketch. The construction of CMM sketch is identical to the CM sketch, while the QUERY procedure diff</context>
<context position="10017" citStr="Cohen and Matias, 2003" startWordPosition="1676" endWordPosition="1679">k &lt; d. QUERY: the median over the d rows is an unbiased estimator of the point query: � c(x) = mediank sk[k, hk(x)] · gk(x), V1 &lt; k &lt; d. Setting w= E and d=log (4δ) ensures that all reE2 frequencies have error at most E N 2 1/2 P q (�Z=1 fz ) &lt; cN with probability at least 1−δ. The space used by the algorithm is O( 1E2 log δ1). 2.2 Conservative Update sketch algorithms In this section, we propose novel variants of existing sketches (see Section 2) by combining them with the conservative update process (Estan and Varghese, 2002). The idea of conservative update (also known as Minimal Increase (Cohen and Matias, 2003)) is to only increase counts in the sketch by the minimum amount needed to ensure the estimate remains accurate. It can easily be applied to Count-Min (CM) sketch and Spectral Bloom Filters (SBF) to further improve the estimate of a point query. Goyal and Daum´e III (2011a) showed that CM sketch with conservative update reduces the amount of overestimation error by a factor of at least 1.5, and also improves performance on three NLP tasks. Note that while conservative update for CM and SBF never increases the error, there is no guaranteed improvement. The method relies on seeing multiple updat</context>
<context position="11615" citStr="Cohen and Matias, 2003" startWordPosition="1941" endWordPosition="1944"> However, to UPDATE an item “x” with frequency c, we first compute the frequency c(x) of this item from the existing data structure (V1 &lt; k &lt; d, c(x) = mink sk[k, hk(x)]) and the counts are updated according to: sk[k, hk(x)] &lt;-- max{sk[k, hk(x)], c(x) + c} (*). The intuition is that, since the point query returns the minimum of all the d values, we update a counter only if it is necessary as indicated by (*). This heuristic avoids unnecessarily updating counter values to reduce the over-estimation error. Spectral Bloom Filters with conservative update (SBF-CU): The QUERY procedure for SBF-CU (Cohen and Matias, 2003) is identical to SBF. SBF-CU UPDATE procedure is similar to CM-CU, with the difference that all d hash functions have the common range d x w. Count-mean-min with conservative update (CMM-CU): We propose a new variant to reduce the over-estimation error for CMM sketch. The construction of CMM-CU is identical to CM-CU. However, due to conservative update, each row of the sketch is not updated for every update, hence the sum of counts over each row (Ei sk[k, i], V1 &lt; k &lt; d) is not equal to input size N. Hence, the estimated noise to be subtracted here is (Ei sk[k, i] − sk[k, hk(x)]) / (w − 1). CM</context>
</contexts>
<marker>Cohen, Matias, 2003</marker>
<rawString>Saar Cohen and Yossi Matias. 2003. Spectral bloom filters. In Proceedings of the 2003 ACM SIGMOD international conference on Management of data, SIGMOD ’03, pages 241–252. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graham Cormode</author>
<author>Marios Hadjieleftheriou</author>
</authors>
<title>Finding frequent items in data streams.</title>
<date>2008</date>
<booktitle>In VLDB.</booktitle>
<contexts>
<context position="3064" citStr="Cormode and Hadjieleftheriou, 2008" startWordPosition="475" endWordPosition="478">g point queries. Recently in NLP, we (Goyal and Daum´e III, 2011a) demonstrated that a version of the Count-Min sketch (Cormode and Muthukrishnan, 2004) accurately solves three largescale NLP problems using small bounded memory footprint. However, there are several other sketch algorithms, and it is not clear why this instance should be preferred amongst these. In this work, we conduct a systematic study and compare many sketch techniques which answer point queries with focus on large-scale NLP tasks. While sketches have been evaluated within the database community for finding frequent items (Cormode and Hadjieleftheriou, 2008) and join-size estimation (Rusu and Dobra, 2007), this is the first comparative study for NLP problems. Our work includes three contributions: (1) We propose novel variants of existing sketches by extending the idea of conservative update to them. We propose Count sketch (Charikar et al., 2004) with conservative update (COUNT-CU) and Count-mean-min sketch with conservative update 1093 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1093–1103, Jeju Island, Korea, 12–14 July 2012. c�2012 Association f</context>
</contexts>
<marker>Cormode, Hadjieleftheriou, 2008</marker>
<rawString>Graham Cormode and Marios Hadjieleftheriou. 2008. Finding frequent items in data streams. In VLDB.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graham Cormode</author>
<author>S Muthukrishnan</author>
</authors>
<title>An improved data stream summary: The count-min sketch and its applications.</title>
<date>2004</date>
<journal>J. Algorithms.</journal>
<contexts>
<context position="2581" citStr="Cormode and Muthukrishnan, 2004" startWordPosition="400" endWordPosition="403">treaming language models (Talbot and Brants, 2008; Levenberg and Osborne, 2009), and distributional similarity (Ravichandran et al., 2005; Van Durme and Lall, 2010). All these problems ultimately depend on approximate counts of items (such as n-grams, word pairs and word-context pairs). Thus we focus on solving this central problem in the context of NLP applications. Sketch algorithms (Charikar et al., 2004; Cormode, 2011) are a memory- and time-efficient solution to answering point queries. Recently in NLP, we (Goyal and Daum´e III, 2011a) demonstrated that a version of the Count-Min sketch (Cormode and Muthukrishnan, 2004) accurately solves three largescale NLP problems using small bounded memory footprint. However, there are several other sketch algorithms, and it is not clear why this instance should be preferred amongst these. In this work, we conduct a systematic study and compare many sketch techniques which answer point queries with focus on large-scale NLP tasks. While sketches have been evaluated within the database community for finding frequent items (Cormode and Hadjieleftheriou, 2008) and join-size estimation (Rusu and Dobra, 2007), this is the first comparative study for NLP problems. Our work incl</context>
<context position="6698" citStr="Cormode and Muthukrishnan, 2004" startWordPosition="1059" endWordPosition="1062"> and s2 computed (using the same parameters w and d, and same set of d hash functions) over different inputs, a sketch of the combined input is obtained by adding the individual sketches, entry-wise. The time to perform the COMBINE operation on sketches is O(d x w), independent of the data size. This property enables sketches to be implemented in distributed setting, where each machine computes the sketch over a small portion of the corpus and makes it scalable to large datasets. 2.1 Existing sketch algorithms This section describes sketches from the literature: Count-Min sketch (CM): The CM (Cormode and Muthukrishnan, 2004) sketch has been used effectively for many large scale problems across several areas, such as Security (Schechter et al., 2010), Machine Learning (Shi et al., 2009; Aggarwal and Yu, 2010), Privacy (Dwork et al., 2010), and NLP (Goyal and Daum´e III, 2011a). The sketch stores an array of size d x w counters, along with d hash functions (drawn from a pairwise-independent family), one for each row of the array. Given an input of N items (x1, x2 ... xN), each of the hash functions hk:U —* {1... w}, V1 &lt; k &lt; d, takes an item from the input and maps it into a counter indexed by the corresponding has</context>
</contexts>
<marker>Cormode, Muthukrishnan, 2004</marker>
<rawString>Graham Cormode and S. Muthukrishnan. 2004. An improved data stream summary: The count-min sketch and its applications. J. Algorithms.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graham Cormode</author>
</authors>
<title>Encyclopedia entry on ’CountMin Sketch’.</title>
<date>2009</date>
<booktitle>In Encyclopedia of Database Systems,</booktitle>
<pages>511--516</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="10935" citStr="Cormode, 2009" startWordPosition="1826" endWordPosition="1827">servative update reduces the amount of overestimation error by a factor of at least 1.5, and also improves performance on three NLP tasks. Note that while conservative update for CM and SBF never increases the error, there is no guaranteed improvement. The method relies on seeing multiple updates in sequence. When a large corpus is being summarized in a distributed setting, we can apply conservative update on each sketch independently before combining the sketches together (see “Sketch Operations” in Section 2). Count-Min sketch with conservative update (CM-CU): The QUERY procedure for CM-CU (Cormode, 2009; Goyal and Daum´e III, 2011a) is identical to Count-Min. However, to UPDATE an item “x” with frequency c, we first compute the frequency c(x) of this item from the existing data structure (V1 &lt; k &lt; d, c(x) = mink sk[k, hk(x)]) and the counts are updated according to: sk[k, hk(x)] &lt;-- max{sk[k, hk(x)], c(x) + c} (*). The intuition is that, since the point query returns the minimum of all the d values, we update a counter only if it is necessary as indicated by (*). This heuristic avoids unnecessarily updating counter values to reduce the over-estimation error. Spectral Bloom Filters with conse</context>
</contexts>
<marker>Cormode, 2009</marker>
<rawString>Graham Cormode. 2009. Encyclopedia entry on ’CountMin Sketch’. In Encyclopedia of Database Systems, pages 511–516. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graham Cormode</author>
</authors>
<title>Sketch techniques for approximate query processing. Foundations and Trends in Databases.</title>
<date>2011</date>
<publisher>NOW publishers.</publisher>
<contexts>
<context position="2375" citStr="Cormode, 2011" startWordPosition="369" endWordPosition="371">n scores like Pointwise Mutual Information (Li et al., 2008; Van Durme and Lall, 2009b; Goyal and Daum´e III, 2011a), finding frequent items (like n-grams) (Goyal et al., 2009), building streaming language models (Talbot and Brants, 2008; Levenberg and Osborne, 2009), and distributional similarity (Ravichandran et al., 2005; Van Durme and Lall, 2010). All these problems ultimately depend on approximate counts of items (such as n-grams, word pairs and word-context pairs). Thus we focus on solving this central problem in the context of NLP applications. Sketch algorithms (Charikar et al., 2004; Cormode, 2011) are a memory- and time-efficient solution to answering point queries. Recently in NLP, we (Goyal and Daum´e III, 2011a) demonstrated that a version of the Count-Min sketch (Cormode and Muthukrishnan, 2004) accurately solves three largescale NLP problems using small bounded memory footprint. However, there are several other sketch algorithms, and it is not clear why this instance should be preferred amongst these. In this work, we conduct a systematic study and compare many sketch techniques which answer point queries with focus on large-scale NLP tasks. While sketches have been evaluated with</context>
</contexts>
<marker>Cormode, 2011</marker>
<rawString>Graham Cormode. 2011. Sketch techniques for approximate query processing. Foundations and Trends in Databases. NOW publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fan Deng</author>
<author>Davood Rafiei</author>
</authors>
<title>New estimation algorithms for streaming data: Count-min can do more.</title>
<date>2007</date>
<note>http://webdocs.cs.ualberta.ca/.</note>
<contexts>
<context position="8451" citStr="Deng and Rafiei, 2007" startWordPosition="1389" endWordPosition="1392">ceed the true frequencies by at most EN with probability of at least 1 − S. This makes the space used by the algorithm O(1� log 1). Spectral Bloom Filters (SBF): Cohen and Matias (2003) proposed SBF, an extension to Bloom Filters (Bloom, 1970) to answer point queries. The 1094 UPDATE and QUERY procedures for SBF are the same as Count-Min (CM) sketch, except that the range of all the hash functions for SBF are the full array: hk:U —* {1 ... w x d}, V1 &lt; k &lt; d. While CM and SBF are very similar, only CM provides guarantees on the query error. Count-mean-min (CMM): The motivation behind the CMM (Deng and Rafiei, 2007) sketch is to provide an unbiased estimator for Count-Min (CM) sketch. The construction of CMM sketch is identical to the CM sketch, while the QUERY procedure differs. Instead of returning the minimum value over the d counters (indexed by d hash functions), CMM deducts the value of estimated noise from each of the d counters, and return the median of the d residues. The noise is estimated as (N − sk[k, hk(x)])/(w − 1). Nevertheless, the median estimate ( i1) over the d residues can overestimate more than the original CM sketch min estimate ( f2), so we return min ( �f1, f2) as the final estima</context>
<context position="15563" citStr="Deng and Rafiei, 2007" startWordPosition="2665" endWordPosition="2668">e stream size of 194 million word pair tokens and 33.5 million word pair types per chunk. To compare error in various sketch counts, first we compute the exact counts of all the word pairs. Second, we store the counts of all the word pairs in all the sketches. Third, we query sketches to generate approximate counts of all the word pairs. Recall, we do not store the word pairs explicitly in sketches but only a compact summary of the associated counts. We fix the size of each sketch to be w = 20×106 3 and d = 3. We keep the size of sketches equal to allow fair comparison among them. Prior work (Deng and Rafiei, 2007; Goyal and Daum´e III, 2011a) showed with fixed sketch size, a small number of hash functions (d=number of hash functions) with large w (or range) give rise to small error over counts. Next, we group all word pairs with the same true frequency into a single bucket. We then compute the Mean Relative Error (MRE) in each of these buckets. Because different sketches have different accuracy behavior on low, mid, and high frequency counts, making this distinction based on frequency lets us determine the regions in which different sketches perform best. Mean Relative Error (MRE) is defined as the av</context>
</contexts>
<marker>Deng, Rafiei, 2007</marker>
<rawString>Fan Deng and Davood Rafiei. 2007. New estimation algorithms for streaming data: Count-min can do more. http://webdocs.cs.ualberta.ca/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cynthia Dwork</author>
<author>Moni Naor</author>
<author>Toniann Pitassi</author>
<author>Guy N Rothblum</author>
<author>Sergey Yekhanin</author>
</authors>
<title>Pan-private streaming algorithms.</title>
<date>2010</date>
<booktitle>In Proceedings of ICS.</booktitle>
<contexts>
<context position="6915" citStr="Dwork et al., 2010" startWordPosition="1096" endWordPosition="1099">E operation on sketches is O(d x w), independent of the data size. This property enables sketches to be implemented in distributed setting, where each machine computes the sketch over a small portion of the corpus and makes it scalable to large datasets. 2.1 Existing sketch algorithms This section describes sketches from the literature: Count-Min sketch (CM): The CM (Cormode and Muthukrishnan, 2004) sketch has been used effectively for many large scale problems across several areas, such as Security (Schechter et al., 2010), Machine Learning (Shi et al., 2009; Aggarwal and Yu, 2010), Privacy (Dwork et al., 2010), and NLP (Goyal and Daum´e III, 2011a). The sketch stores an array of size d x w counters, along with d hash functions (drawn from a pairwise-independent family), one for each row of the array. Given an input of N items (x1, x2 ... xN), each of the hash functions hk:U —* {1... w}, V1 &lt; k &lt; d, takes an item from the input and maps it into a counter indexed by the corresponding hash function. UPDATE: For each new item “x” with count c, the sketch is updated as: sk[k, hk(x)] +-- sk[k, hk(x)] + c, V1 &lt; k &lt; d. QUERY: Since multiple items are hashed to the same index for each array row, the stored </context>
</contexts>
<marker>Dwork, Naor, Pitassi, Rothblum, Yekhanin, 2010</marker>
<rawString>Cynthia Dwork, Moni Naor, Toniann Pitassi, Guy N. Rothblum, and Sergey Yekhanin. 2010. Pan-private streaming algorithms. In Proceedings of ICS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Erk</author>
</authors>
<title>A simple, similarity-based model for selectional preferences.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<volume>45</volume>
<pages>216--223</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="25179" citStr="Erk, 2007" startWordPosition="4249" endWordPosition="4250">f, 2003) and a 50% portion of a copy of news web (GWB50) crawled by (Ravichandran et al., 2005). The raw size of Gigaword (GW) and GWB50 is 9.8 GB and 49 GB with 56.78 million and 462.60 sentences respectively. For both the corpora, we split the text into sentences, tokenize and convert into lower-case. 4.2 Pseudo-Words Evaluation In NLP, it is difficult and time consuming to create annotated test sets. This problem has motivated the use of pseudo-words to automatically create the test sets without human annotation. The pseudo-words are a common way to evaluate selectional preferences models (Erk, 2007; Bergsma et al., 2008) that measure the strength of association between a predicate and its argument filler, e.g., that the noun “song” is likely to be the object of the verb “sing”. A pseudo-word is the conflation of two words (e.g. song/dance). One word is the original in a sentence, and the second is the confounder. For example, in our task of selectional preferences, the system has to decide for the verb “sing” which is the correct object between “song”/“dance”. Recently, Chambers and Jurafsky (2010) proposed a simple baseline based on co-occurrence counts of words, which has state-of-the</context>
</contexts>
<marker>Erk, 2007</marker>
<rawString>Katrin Erk. 2007. A simple, similarity-based model for selectional preferences. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, volume 45, pages 216–223. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cristian Estan</author>
<author>George Varghese</author>
</authors>
<title>New directions in traffic measurement and accounting.</title>
<date>2002</date>
<journal>SIGCOMM Comput. Commun. Rev.,</journal>
<volume>32</volume>
<issue>4</issue>
<contexts>
<context position="4591" citStr="Estan and Varghese, 2002" startWordPosition="702" endWordPosition="706">ors can be over-estimation, under-estimation, or a combination of the two. We also evaluate their performance via Pointwise Mutual Information and LogLikelihood Ratio. (3) We use sketches to solve three important NLP problems. Our experiments show that sketches can be very effective for these tasks, and that the best results are obtained using the “conservative update” technique. Across all the three tasks, one sketch (CM-CU) performs best. 2 Sketches In this section, we review existing sketch algorithms from the literature, and propose novel variants based on the idea of conservative update (Estan and Varghese, 2002). The term ‘sketch’ refers to a class of algorithm that represents a large data set with a compact summary, typically much smaller than the full size of the input. Given an input of N items (x1, x2 ... xN), each item x (where x is drawn from some domain U) is mapped via hash functions into a small sketch vector that records frequency information. Thus, the sketch does not store the items explicitly, but only information about the frequency distribution. Sketches support fundamental queries on their input such as point, range and inner product queries to be quickly answered approximately. In th</context>
<context position="9927" citStr="Estan and Varghese, 2002" startWordPosition="1662" endWordPosition="1665">UPDATE: For each new item “x” with count c: sk[k, hk(x)] &lt;-- sk[k, hk(x)] + c · gk(x), V1 &lt; k &lt; d. QUERY: the median over the d rows is an unbiased estimator of the point query: � c(x) = mediank sk[k, hk(x)] · gk(x), V1 &lt; k &lt; d. Setting w= E and d=log (4δ) ensures that all reE2 frequencies have error at most E N 2 1/2 P q (�Z=1 fz ) &lt; cN with probability at least 1−δ. The space used by the algorithm is O( 1E2 log δ1). 2.2 Conservative Update sketch algorithms In this section, we propose novel variants of existing sketches (see Section 2) by combining them with the conservative update process (Estan and Varghese, 2002). The idea of conservative update (also known as Minimal Increase (Cohen and Matias, 2003)) is to only increase counts in the sketch by the minimum amount needed to ensure the estimate remains accurate. It can easily be applied to Count-Min (CM) sketch and Spectral Bloom Filters (SBF) to further improve the estimate of a point query. Goyal and Daum´e III (2011a) showed that CM sketch with conservative update reduces the amount of overestimation error by a factor of at least 1.5, and also improves performance on three NLP tasks. Note that while conservative update for CM and SBF never increases</context>
</contexts>
<marker>Estan, Varghese, 2002</marker>
<rawString>Cristian Estan and George Varghese. 2002. New directions in traffic measurement and accounting. SIGCOMM Comput. Commun. Rev., 32(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Firth</author>
</authors>
<title>A synopsis of linguistic theory 1930-1955.</title>
<date>1968</date>
<editor>In F. Palmer, editor, Selected Papers of J. R. Firth.</editor>
<publisher>Longman.</publisher>
<contexts>
<context position="32600" citStr="Firth, 1968" startWordPosition="5516" endWordPosition="5517">r types.). Figure 4 has similar frequency distribution of word pairs3 in SO test set as pseudo-words evaluation test sets word pairs. Hence, CMM-CU again has substantially worse results than CM-CU due to loss of recall (information about low frequency word pairs) by under-estimation error. We can conclude that for this task CM-CU is best. 4.4 Distributional Similarity Distributional similarity is based on the distributional hypothesis that similar terms appear in simi3Consider only those pairs in which one word appears in the seed list and the other word appears in the test set. lar contexts (Firth, 1968; Harris, 1954). The context vector for each term is represented by the strength of association between the term and each of the lexical, semantic, syntactic, and/or dependency units that co-occur with it. For this work, we define context for a given term as the surrounding words appearing in a window of 2 words to the left and 2 words to the right. The context words are concatenated along with their positions -2, -1, +1, and +2. We use PMI and LLR to compute the association score (AS) between the term and each of the context to generate the context vector. We use the cosine similarity measure</context>
</contexts>
<marker>Firth, 1968</marker>
<rawString>J. Firth. 1968. A synopsis of linguistic theory 1930-1955. In F. Palmer, editor, Selected Papers of J. R. Firth. Longman.</rawString>
</citation>
<citation valid="false">
<authors>
<author>2011a</author>
</authors>
<title>Approximate scalable bounded space sketch for large data NLP.</title>
<booktitle>In Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<marker>2011a, </marker>
<rawString>Amit Goyal and Hal Daum´e III. 2011a. Approximate scalable bounded space sketch for large data NLP. In Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="false">
<authors>
<author>2011b</author>
</authors>
<title>Lossy conservative update (LCU) sketch: Succinct approximate count storage.</title>
<booktitle>In Conference on Artificial Intelligence (AAAI).</booktitle>
<marker>2011b, </marker>
<rawString>Amit Goyal and Hal Daum´e III. 2011b. Lossy conservative update (LCU) sketch: Succinct approximate count storage. In Conference on Artificial Intelligence (AAAI).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amit Goyal</author>
<author>Hal Daum´e</author>
<author>Suresh Venkatasubramanian</author>
</authors>
<title>Streaming for large scale NLP: Language modeling.</title>
<date>2009</date>
<booktitle>In NAACL.</booktitle>
<marker>Goyal, Daum´e, Venkatasubramanian, 2009</marker>
<rawString>Amit Goyal, Hal Daum´e III, and Suresh Venkatasubramanian. 2009. Streaming for large scale NLP: Language modeling. In NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Graff</author>
</authors>
<title>English Gigaword. Linguistic Data Consortium,</title>
<date>2003</date>
<location>Philadelphia, PA,</location>
<contexts>
<context position="14683" citStr="Graff, 2003" startWordPosition="2501" endWordPosition="2502">t most the square root of current window index, t. At window boundaries, b 1 &lt; i &lt; d, 1 &lt; j &lt; w, if (sk[i, j] &gt; 0 and sk[i, j] &lt; F.\/t]), then sk[i, j] +— sk[i, j] − 1. LCUSWS has similar analytical bounds to LCU-WS. 3 Intrinsic Evaluations We empirically compare and study the errors in approximate counts for all 10 sketches. Errors can be over-estimation, under-estimation, or a combination of the two. We also study the behavior of approximate Pointwise Mutual Information and Log Likelihood Ratio for the sketches. 3.1 Experimental Setup DATA: We took 50 million random sentences from Gigaword (Graff, 2003). We split this data in 10 chunks of 5 million sentences each. Since all sketches have probabilistic bounds, we report average results over these 10 chunks. For each chunk, we generate counts of all word pairs within a window size 7. This results in an average stream size of 194 million word pair tokens and 33.5 million word pair types per chunk. To compare error in various sketch counts, first we compute the exact counts of all the word pairs. Second, we store the counts of all the word pairs in all the sketches. Third, we query sketches to generate approximate counts of all the word pairs. R</context>
<context position="24578" citStr="Graff, 2003" startWordPosition="4148" endWordPosition="4149">mple methods as they use approximate counts and associations directly to solve these tasks. This allows us to have a fair comparison among different sketches, and to more directly see the impact of different choices of sketch on the task outcome. Of course, sketches are still broadly applicable to many NLP problems where we want to count (many) items or compute associations: e.g. language models, Statistical Machine Translation, paraphrasing, bootstrapping and label propagation for automatically creating a knowledge base and finding interesting patterns in social media. Data: We use Gigaword (Graff, 2003) and a 50% portion of a copy of news web (GWB50) crawled by (Ravichandran et al., 2005). The raw size of Gigaword (GW) and GWB50 is 9.8 GB and 49 GB with 56.78 million and 462.60 sentences respectively. For both the corpora, we split the text into sentences, tokenize and convert into lower-case. 4.2 Pseudo-Words Evaluation In NLP, it is difficult and time consuming to create annotated test sets. This problem has motivated the use of pseudo-words to automatically create the test sets without human annotation. The pseudo-words are a common way to evaluate selectional preferences models (Erk, 200</context>
</contexts>
<marker>Graff, 2003</marker>
<rawString>D. Graff. 2003. English Gigaword. Linguistic Data Consortium, Philadelphia, PA, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Harris</author>
</authors>
<title>Distributional structure.</title>
<date>1954</date>
<journal>Word</journal>
<volume>10</volume>
<issue>23</issue>
<pages>146--162</pages>
<contexts>
<context position="32615" citStr="Harris, 1954" startWordPosition="5518" endWordPosition="5519">gure 4 has similar frequency distribution of word pairs3 in SO test set as pseudo-words evaluation test sets word pairs. Hence, CMM-CU again has substantially worse results than CM-CU due to loss of recall (information about low frequency word pairs) by under-estimation error. We can conclude that for this task CM-CU is best. 4.4 Distributional Similarity Distributional similarity is based on the distributional hypothesis that similar terms appear in simi3Consider only those pairs in which one word appears in the seed list and the other word appears in the test set. lar contexts (Firth, 1968; Harris, 1954). The context vector for each term is represented by the strength of association between the term and each of the lexical, semantic, syntactic, and/or dependency units that co-occur with it. For this work, we define context for a given term as the surrounding words appearing in a window of 2 words to the left and 2 words to the right. The context words are concatenated along with their positions -2, -1, +1, and +2. We use PMI and LLR to compute the association score (AS) between the term and each of the context to generate the context vector. We use the cosine similarity measure to find the di</context>
</contexts>
<marker>Harris, 1954</marker>
<rawString>Z. Harris. 1954. Distributional structure. Word 10 (23), pages 146–162.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Abby Levenberg</author>
<author>Miles Osborne</author>
</authors>
<title>Streambased randomised language models for SMT.</title>
<date>2009</date>
<booktitle>In EMNLP,</booktitle>
<contexts>
<context position="2028" citStr="Levenberg and Osborne, 2009" startWordPosition="312" endWordPosition="316">h and interest in the use of approximation, randomization, and streaming techniques for large-scale problems (Brants et al., 2007; Turney, 2008). Much of this work relies on tracking very many statistics. For example, storing approximate counts (Talbot and Osborne, 2007; Van Durme and Lall, 2009a; Goyal and Daum´e III, 2011a), computing approximate association scores like Pointwise Mutual Information (Li et al., 2008; Van Durme and Lall, 2009b; Goyal and Daum´e III, 2011a), finding frequent items (like n-grams) (Goyal et al., 2009), building streaming language models (Talbot and Brants, 2008; Levenberg and Osborne, 2009), and distributional similarity (Ravichandran et al., 2005; Van Durme and Lall, 2010). All these problems ultimately depend on approximate counts of items (such as n-grams, word pairs and word-context pairs). Thus we focus on solving this central problem in the context of NLP applications. Sketch algorithms (Charikar et al., 2004; Cormode, 2011) are a memory- and time-efficient solution to answering point queries. Recently in NLP, we (Goyal and Daum´e III, 2011a) demonstrated that a version of the Count-Min sketch (Cormode and Muthukrishnan, 2004) accurately solves three largescale NLP problem</context>
</contexts>
<marker>Levenberg, Osborne, 2009</marker>
<rawString>Abby Levenberg and Miles Osborne. 2009. Streambased randomised language models for SMT. In EMNLP, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ping Li</author>
<author>Kenneth Ward Church</author>
<author>Trevor Hastie</author>
</authors>
<title>One sketch for all: Theory and application of conditional random sampling.</title>
<date>2008</date>
<booktitle>In Neural Information Processing Systems,</booktitle>
<pages>953--960</pages>
<contexts>
<context position="1820" citStr="Li et al., 2008" startWordPosition="281" endWordPosition="284">y extracted? One answer is to adopt compact summaries of corpora in the form of probabilistic “sketches”. In recent years, the field of Natural Language Processing (NLP) has seen tremendous growth and interest in the use of approximation, randomization, and streaming techniques for large-scale problems (Brants et al., 2007; Turney, 2008). Much of this work relies on tracking very many statistics. For example, storing approximate counts (Talbot and Osborne, 2007; Van Durme and Lall, 2009a; Goyal and Daum´e III, 2011a), computing approximate association scores like Pointwise Mutual Information (Li et al., 2008; Van Durme and Lall, 2009b; Goyal and Daum´e III, 2011a), finding frequent items (like n-grams) (Goyal et al., 2009), building streaming language models (Talbot and Brants, 2008; Levenberg and Osborne, 2009), and distributional similarity (Ravichandran et al., 2005; Van Durme and Lall, 2010). All these problems ultimately depend on approximate counts of items (such as n-grams, word pairs and word-context pairs). Thus we focus on solving this central problem in the context of NLP applications. Sketch algorithms (Charikar et al., 2004; Cormode, 2011) are a memory- and time-efficient solution to</context>
</contexts>
<marker>Li, Church, Hastie, 2008</marker>
<rawString>Ping Li, Kenneth Ward Church, and Trevor Hastie. 2008. One sketch for all: Theory and application of conditional random sampling. In Neural Information Processing Systems, pages 953–960.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G S Manku</author>
<author>R Motwani</author>
</authors>
<title>Approximate frequency counts over data streams.</title>
<date>2002</date>
<booktitle>In VLDB.</booktitle>
<contexts>
<context position="13112" citStr="Manku and Motwani, 2002" startWordPosition="2205" endWordPosition="2208">edure for COUNT-CU is the same as 1095 COUNT. The UPDATE procedure follows the same outline as CM-CU, but uses the current estimate c(x) from the COUNT sketch, i.e. c(x) = mediank sk[k, hk(x)] · A(x), b1 &lt; k &lt; d. Note, this heuristic is not as strong as for CM-CU and SBF-CU because COUNT can have both overestimate and under-estimate errors. Lossy counting with conservative update (LCUWS): LCU-WS (Goyal and Daum´e III, 2011b) was proposed to reduce the amount of over-estimation error for CM-CU sketch, without incurring too much under-estimation error. This scheme is inspired by lossy counting (Manku and Motwani, 2002). In this approach, the input sequence is conceptually divided into windows, each containing 1/-y items. The size of each window is equal to size of the sketch i.e. d x w. Note that there are -yN windows; let t denote the index of current window. At window boundaries, b 1 &lt; i &lt; d, 1 &lt; j &lt; w, if (sk[i, j] &gt; 0 and sk[i, j] &lt; t), then sk[i, j] +— sk[i, j]−1. The idea is to remove the contribution of small items colliding in the same entry, while not altering the count of frequent items. The current window index is used to draw this distinction. Here, all reported frequencies f have both under and</context>
</contexts>
<marker>Manku, Motwani, 2002</marker>
<rawString>G. S. Manku and R. Motwani. 2002. Approximate frequency counts over data streams. In VLDB.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G A Miller</author>
<author>W G Charles</author>
</authors>
<title>Contextual correlates of semantic similarity.</title>
<date>1991</date>
<booktitle>Language and Cognitive Processes,</booktitle>
<pages>6--1</pages>
<contexts>
<context position="33742" citStr="Miller and Charles, 1991" startWordPosition="5714" endWordPosition="5717">of the context to generate the context vector. We use the cosine similarity measure to find the distributional similarity between the context vectors for each of the terms. We use two test sets which consist of word pairs, and their corresponding human rankings. We generate the word pair rankings using distributional similarity. We report the Spearman’s rank correlation coefficient (p) between the human and distributional similarity rankings. We report results on two test sets: WS-203: A set of 203 word pairs marked according to similarity (Agirre et al., 2009). MC-30: A set of 30 noun pairs (Miller and Charles, 1991). We evaluate distributional similarity on Gigaword (GW) (9.8GB) (see Section 4.1). First, we store exact counts of all words and contexts in a hash table from GW. Next, we count all the word-context pairs and store them in CM-CU, CMM-CU, and LCU-WS sketches. That generates a stream of size 3.35 billion (3.35B) word-context pair tokens and 215 million unique word-context pair types (It takes 4.6 GB uncompressed disk space to store exact counts of all these unique word-context pair types.). For every target word in the test set, we maintain top-1000 approximate AS scores contexts using a priori</context>
</contexts>
<marker>Miller, Charles, 1991</marker>
<rawString>G.A. Miller and W.G. Charles. 1991. Contextual correlates of semantic similarity. Language and Cognitive Processes, 6(1):1–28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deepak Ravichandran</author>
<author>Patrick Pantel</author>
<author>Eduard Hovy</author>
</authors>
<title>Randomized algorithms and nlp: using locality sensitive hash function for high speed noun clustering.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="2086" citStr="Ravichandran et al., 2005" startWordPosition="321" endWordPosition="324">d streaming techniques for large-scale problems (Brants et al., 2007; Turney, 2008). Much of this work relies on tracking very many statistics. For example, storing approximate counts (Talbot and Osborne, 2007; Van Durme and Lall, 2009a; Goyal and Daum´e III, 2011a), computing approximate association scores like Pointwise Mutual Information (Li et al., 2008; Van Durme and Lall, 2009b; Goyal and Daum´e III, 2011a), finding frequent items (like n-grams) (Goyal et al., 2009), building streaming language models (Talbot and Brants, 2008; Levenberg and Osborne, 2009), and distributional similarity (Ravichandran et al., 2005; Van Durme and Lall, 2010). All these problems ultimately depend on approximate counts of items (such as n-grams, word pairs and word-context pairs). Thus we focus on solving this central problem in the context of NLP applications. Sketch algorithms (Charikar et al., 2004; Cormode, 2011) are a memory- and time-efficient solution to answering point queries. Recently in NLP, we (Goyal and Daum´e III, 2011a) demonstrated that a version of the Count-Min sketch (Cormode and Muthukrishnan, 2004) accurately solves three largescale NLP problems using small bounded memory footprint. However, there are</context>
<context position="24665" citStr="Ravichandran et al., 2005" startWordPosition="4163" endWordPosition="4166">solve these tasks. This allows us to have a fair comparison among different sketches, and to more directly see the impact of different choices of sketch on the task outcome. Of course, sketches are still broadly applicable to many NLP problems where we want to count (many) items or compute associations: e.g. language models, Statistical Machine Translation, paraphrasing, bootstrapping and label propagation for automatically creating a knowledge base and finding interesting patterns in social media. Data: We use Gigaword (Graff, 2003) and a 50% portion of a copy of news web (GWB50) crawled by (Ravichandran et al., 2005). The raw size of Gigaword (GW) and GWB50 is 9.8 GB and 49 GB with 56.78 million and 462.60 sentences respectively. For both the corpora, we split the text into sentences, tokenize and convert into lower-case. 4.2 Pseudo-Words Evaluation In NLP, it is difficult and time consuming to create annotated test sets. This problem has motivated the use of pseudo-words to automatically create the test sets without human annotation. The pseudo-words are a common way to evaluate selectional preferences models (Erk, 2007; Bergsma et al., 2008) that measure the strength of association between a predicate a</context>
</contexts>
<marker>Ravichandran, Pantel, Hovy, 2005</marker>
<rawString>Deepak Ravichandran, Patrick Pantel, and Eduard Hovy. 2005. Randomized algorithms and nlp: using locality sensitive hash function for high speed noun clustering. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Florin Rusu</author>
<author>Alin Dobra</author>
</authors>
<title>Statistical analysis of sketch estimators.</title>
<date>2007</date>
<booktitle>In SIGMOD ’07.</booktitle>
<publisher>ACM.</publisher>
<contexts>
<context position="3112" citStr="Rusu and Dobra, 2007" startWordPosition="482" endWordPosition="485">11a) demonstrated that a version of the Count-Min sketch (Cormode and Muthukrishnan, 2004) accurately solves three largescale NLP problems using small bounded memory footprint. However, there are several other sketch algorithms, and it is not clear why this instance should be preferred amongst these. In this work, we conduct a systematic study and compare many sketch techniques which answer point queries with focus on large-scale NLP tasks. While sketches have been evaluated within the database community for finding frequent items (Cormode and Hadjieleftheriou, 2008) and join-size estimation (Rusu and Dobra, 2007), this is the first comparative study for NLP problems. Our work includes three contributions: (1) We propose novel variants of existing sketches by extending the idea of conservative update to them. We propose Count sketch (Charikar et al., 2004) with conservative update (COUNT-CU) and Count-mean-min sketch with conservative update 1093 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1093–1103, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics (CMM-CU). The motiv</context>
</contexts>
<marker>Rusu, Dobra, 2007</marker>
<rawString>Florin Rusu and Alin Dobra. 2007. Statistical analysis of sketch estimators. In SIGMOD ’07. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart Schechter</author>
<author>Cormac Herley</author>
<author>Michael Mitzenmacher</author>
</authors>
<title>Popularity is everything: a new approach to protecting passwords from statisticalguessing attacks.</title>
<date>2010</date>
<booktitle>In Proceedings of the 5th USENIX conference on Hot topics in security, HotSec’10,</booktitle>
<pages>1--8</pages>
<publisher>USENIX Association.</publisher>
<location>Berkeley, CA, USA.</location>
<contexts>
<context position="6825" citStr="Schechter et al., 2010" startWordPosition="1081" endWordPosition="1084">nput is obtained by adding the individual sketches, entry-wise. The time to perform the COMBINE operation on sketches is O(d x w), independent of the data size. This property enables sketches to be implemented in distributed setting, where each machine computes the sketch over a small portion of the corpus and makes it scalable to large datasets. 2.1 Existing sketch algorithms This section describes sketches from the literature: Count-Min sketch (CM): The CM (Cormode and Muthukrishnan, 2004) sketch has been used effectively for many large scale problems across several areas, such as Security (Schechter et al., 2010), Machine Learning (Shi et al., 2009; Aggarwal and Yu, 2010), Privacy (Dwork et al., 2010), and NLP (Goyal and Daum´e III, 2011a). The sketch stores an array of size d x w counters, along with d hash functions (drawn from a pairwise-independent family), one for each row of the array. Given an input of N items (x1, x2 ... xN), each of the hash functions hk:U —* {1... w}, V1 &lt; k &lt; d, takes an item from the input and maps it into a counter indexed by the corresponding hash function. UPDATE: For each new item “x” with count c, the sketch is updated as: sk[k, hk(x)] +-- sk[k, hk(x)] + c, V1 &lt; k &lt; d</context>
</contexts>
<marker>Schechter, Herley, Mitzenmacher, 2010</marker>
<rawString>Stuart Schechter, Cormac Herley, and Michael Mitzenmacher. 2010. Popularity is everything: a new approach to protecting passwords from statisticalguessing attacks. In Proceedings of the 5th USENIX conference on Hot topics in security, HotSec’10, pages 1–8, Berkeley, CA, USA. USENIX Association.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qinfeng Shi</author>
<author>James Petterson</author>
<author>Gideon Dror</author>
<author>John Langford</author>
<author>Alex Smola</author>
<author>S V N Vishwanathan</author>
</authors>
<title>Hash kernels for structured data.</title>
<date>2009</date>
<journal>Journal Machine Learning Research,</journal>
<pages>10--2615</pages>
<contexts>
<context position="6861" citStr="Shi et al., 2009" startWordPosition="1087" endWordPosition="1090">ketches, entry-wise. The time to perform the COMBINE operation on sketches is O(d x w), independent of the data size. This property enables sketches to be implemented in distributed setting, where each machine computes the sketch over a small portion of the corpus and makes it scalable to large datasets. 2.1 Existing sketch algorithms This section describes sketches from the literature: Count-Min sketch (CM): The CM (Cormode and Muthukrishnan, 2004) sketch has been used effectively for many large scale problems across several areas, such as Security (Schechter et al., 2010), Machine Learning (Shi et al., 2009; Aggarwal and Yu, 2010), Privacy (Dwork et al., 2010), and NLP (Goyal and Daum´e III, 2011a). The sketch stores an array of size d x w counters, along with d hash functions (drawn from a pairwise-independent family), one for each row of the array. Given an input of N items (x1, x2 ... xN), each of the hash functions hk:U —* {1... w}, V1 &lt; k &lt; d, takes an item from the input and maps it into a counter indexed by the corresponding hash function. UPDATE: For each new item “x” with count c, the sketch is updated as: sk[k, hk(x)] +-- sk[k, hk(x)] + c, V1 &lt; k &lt; d. QUERY: Since multiple items are ha</context>
</contexts>
<marker>Shi, Petterson, Dror, Langford, Smola, Vishwanathan, 2009</marker>
<rawString>Qinfeng Shi, James Petterson, Gideon Dror, John Langford, Alex Smola, and S.V.N. Vishwanathan. 2009. Hash kernels for structured data. Journal Machine Learning Research, 10:2615–2637, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip J Stone</author>
<author>Dexter C Dunphy</author>
<author>Marshall S Smith</author>
<author>Daniel M Ogilvie</author>
</authors>
<title>The General Inquirer: A Computer Approach to Content Analysis.</title>
<date>1966</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="29699" citStr="Stone et al., 1966" startWordPosition="5011" endWordPosition="5014">words (bad, nasty, poor, negative, unfortunate, wrong, and inferior) from their framework as seeds. The SO of a given word is computed based on the strength of its association with the seven positive words and the seven negative words. Association scores are computed via Pointwise Mutual Information (PMI). We compute the SO of a word “w” as: SO(W) = EpEPos PMI(p, w)−EnENeg PMI(n, w) where, Pos and Neg denote the seven positive and negative seeds respectively. If this score is negative, we predict the word as negative; otherwise, we predict it as positive. We use the General Inquirer lexicon2 (Stone et al., 1966) as a benchmark to evaluate the semantic orientation similar to Turney and Littman’s (2003) work. Our test set consists of 1611 positive and 1987 negative words. Accuracy is used for evaluation and is defined as the percentage of number of correctly identified SO words. We evaluate SO of words on two different sized corpora (see Section 4.1): Gigaword (GW) (9.8GB), and GW with 50% news web corpus (GWB50) (49GB). We fix the size of all sketches to 2 billion (2B) counters with 5 hash functions. We store exact counts of all words in a hash table for both GW and GWB50. We count all word pairs (exc</context>
</contexts>
<marker>Stone, Dunphy, Smith, Ogilvie, 1966</marker>
<rawString>Philip J. Stone, Dexter C. Dunphy, Marshall S. Smith, and Daniel M. Ogilvie. 1966. The General Inquirer: A Computer Approach to Content Analysis. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Talbot</author>
<author>Thorsten Brants</author>
</authors>
<title>Randomized language models via perfect hash functions.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT.</booktitle>
<contexts>
<context position="1998" citStr="Talbot and Brants, 2008" startWordPosition="308" endWordPosition="311">has seen tremendous growth and interest in the use of approximation, randomization, and streaming techniques for large-scale problems (Brants et al., 2007; Turney, 2008). Much of this work relies on tracking very many statistics. For example, storing approximate counts (Talbot and Osborne, 2007; Van Durme and Lall, 2009a; Goyal and Daum´e III, 2011a), computing approximate association scores like Pointwise Mutual Information (Li et al., 2008; Van Durme and Lall, 2009b; Goyal and Daum´e III, 2011a), finding frequent items (like n-grams) (Goyal et al., 2009), building streaming language models (Talbot and Brants, 2008; Levenberg and Osborne, 2009), and distributional similarity (Ravichandran et al., 2005; Van Durme and Lall, 2010). All these problems ultimately depend on approximate counts of items (such as n-grams, word pairs and word-context pairs). Thus we focus on solving this central problem in the context of NLP applications. Sketch algorithms (Charikar et al., 2004; Cormode, 2011) are a memory- and time-efficient solution to answering point queries. Recently in NLP, we (Goyal and Daum´e III, 2011a) demonstrated that a version of the Count-Min sketch (Cormode and Muthukrishnan, 2004) accurately solve</context>
</contexts>
<marker>Talbot, Brants, 2008</marker>
<rawString>David Talbot and Thorsten Brants. 2008. Randomized language models via perfect hash functions. In Proceedings of ACL-08: HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Talbot</author>
<author>Miles Osborne</author>
</authors>
<title>Smoothed Bloom filter language models: Tera-scale LMs on the cheap.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EM NLP-CoNLL).</booktitle>
<contexts>
<context position="1670" citStr="Talbot and Osborne, 2007" startWordPosition="257" endWordPosition="261">However, this new view comes with its own challenges: principally, how to effectively represent such large data sets so that model parameters can be efficiently extracted? One answer is to adopt compact summaries of corpora in the form of probabilistic “sketches”. In recent years, the field of Natural Language Processing (NLP) has seen tremendous growth and interest in the use of approximation, randomization, and streaming techniques for large-scale problems (Brants et al., 2007; Turney, 2008). Much of this work relies on tracking very many statistics. For example, storing approximate counts (Talbot and Osborne, 2007; Van Durme and Lall, 2009a; Goyal and Daum´e III, 2011a), computing approximate association scores like Pointwise Mutual Information (Li et al., 2008; Van Durme and Lall, 2009b; Goyal and Daum´e III, 2011a), finding frequent items (like n-grams) (Goyal et al., 2009), building streaming language models (Talbot and Brants, 2008; Levenberg and Osborne, 2009), and distributional similarity (Ravichandran et al., 2005; Van Durme and Lall, 2010). All these problems ultimately depend on approximate counts of items (such as n-grams, word pairs and word-context pairs). Thus we focus on solving this cen</context>
</contexts>
<marker>Talbot, Osborne, 2007</marker>
<rawString>David Talbot and Miles Osborne. 2007. Smoothed Bloom filter language models: Tera-scale LMs on the cheap. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EM NLP-CoNLL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Talbot</author>
</authors>
<title>Succinct approximate counting of skewed data. In</title>
<date>2009</date>
<booktitle>IJCAI’09: Proceedings of the 21st international jont conference on Artifical intelligence.</booktitle>
<marker>Talbot, 2009</marker>
<rawString>David Talbot. 2009. Succinct approximate counting of skewed data. In IJCAI’09: Proceedings of the 21st international jont conference on Artifical intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
<author>Michael L Littman</author>
</authors>
<title>Measuring praise and criticism: Inference of semantic orientation from association.</title>
<date>2003</date>
<journal>ACM Trans. Inf. Syst.,</journal>
<volume>21</volume>
<pages>346</pages>
<contexts>
<context position="28770" citStr="Turney and Littman, 2003" startWordPosition="4850" endWordPosition="4853">to storing 94M unique word pairs using 200M integer (4 bytes each) counters (using 800 MB) &lt; 1.8 GB to maintain exact counts. Moreover, these results are comparable to Chambers and Jurafsky’s state-of-the-art framework. Data Exact CM-CU CMM-CU LCU-WS GW 74.2 74.0 65.3 72.9 GWB50 81.2 80.9 74.9 78.3 Table 2: Evaluating Semantic Orientation on accuracy metric using several sketches of 2 billion counters against exact. Bold and italic numbers denote no statistically significant difference. 4.3 Finding Semantic Orientation of a word Given a word, the task of finding its Semantic Orientation (SO) (Turney and Littman, 2003) is to determine if the word is more probable to be used in positive or negative connotation. We use Turney and Littman’s (2003) state-of-the-art framework to compute the SO of a word. We use same seven positive words (good, nice, excellent, positive, fortunate, correct, and superior) and same seven negative words (bad, nasty, poor, negative, unfortunate, wrong, and inferior) from their framework as seeds. The SO of a given word is computed based on the strength of its association with the seven positive words and the seven negative words. Association scores are computed via Pointwise Mutual I</context>
</contexts>
<marker>Turney, Littman, 2003</marker>
<rawString>Peter D. Turney and Michael L. Littman. 2003. Measuring praise and criticism: Inference of semantic orientation from association. ACM Trans. Inf. Syst., 21:315– 346, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>A uniform approach to analogies, synonyms, antonyms, and associations.</title>
<date>2008</date>
<booktitle>In Proceedings of COLING</booktitle>
<contexts>
<context position="1544" citStr="Turney, 2008" startWordPosition="239" endWordPosition="240">sks: the concept is that simple models trained on big data can outperform more complex models with fewer examples. However, this new view comes with its own challenges: principally, how to effectively represent such large data sets so that model parameters can be efficiently extracted? One answer is to adopt compact summaries of corpora in the form of probabilistic “sketches”. In recent years, the field of Natural Language Processing (NLP) has seen tremendous growth and interest in the use of approximation, randomization, and streaming techniques for large-scale problems (Brants et al., 2007; Turney, 2008). Much of this work relies on tracking very many statistics. For example, storing approximate counts (Talbot and Osborne, 2007; Van Durme and Lall, 2009a; Goyal and Daum´e III, 2011a), computing approximate association scores like Pointwise Mutual Information (Li et al., 2008; Van Durme and Lall, 2009b; Goyal and Daum´e III, 2011a), finding frequent items (like n-grams) (Goyal et al., 2009), building streaming language models (Talbot and Brants, 2008; Levenberg and Osborne, 2009), and distributional similarity (Ravichandran et al., 2005; Van Durme and Lall, 2010). All these problems ultimately</context>
</contexts>
<marker>Turney, 2008</marker>
<rawString>Peter D. Turney. 2008. A uniform approach to analogies, synonyms, antonyms, and associations. In Proceedings of COLING 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Van Durme</author>
<author>Ashwin Lall</author>
</authors>
<title>Probabilistic counting with randomized storage.</title>
<date>2009</date>
<booktitle>In IJCAI’09: Proceedings of the 21st international jont conference on Artifical intelligence.</booktitle>
<marker>Van Durme, Lall, 2009</marker>
<rawString>Benjamin Van Durme and Ashwin Lall. 2009a. Probabilistic counting with randomized storage. In IJCAI’09: Proceedings of the 21st international jont conference on Artifical intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Van Durme</author>
<author>Ashwin Lall</author>
</authors>
<title>Streaming pointwise mutual information.</title>
<date>2009</date>
<booktitle>In Advances in Neural Information Processing Systems 22.</booktitle>
<marker>Van Durme, Lall, 2009</marker>
<rawString>Benjamin Van Durme and Ashwin Lall. 2009b. Streaming pointwise mutual information. In Advances in Neural Information Processing Systems 22.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Van Durme</author>
<author>Ashwin Lall</author>
</authors>
<title>Online generation of locality sensitive hash signatures.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACL 2010 Conference Short Papers,</booktitle>
<pages>231--235</pages>
<marker>Van Durme, Lall, 2010</marker>
<rawString>Benjamin Van Durme and Ashwin Lall. 2010. Online generation of locality sensitive hash signatures. In Proceedings of the ACL 2010 Conference Short Papers, pages 231–235, July.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004621">
<title confidence="0.9978725">
Extending Machine Translation Evaluation Metrics with Lexical Cohesion
To Document Level
</title>
<author confidence="0.999308">
Billy T. M. Wong and Chunyu Kit
</author>
<affiliation confidence="0.999267">
Department of Chinese, Translation and Linguistics
City University of Hong Kong
</affiliation>
<address confidence="0.999518">
83 Tat Chee Avenue, Kowloon, Hong Kong SAR, P. R. China
</address>
<email confidence="0.999723">
{tmwong,ctckit}@cityu.edu.hk
</email>
<sectionHeader confidence="0.998605" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999074888888889">
This paper proposes the utilization of lexical
cohesion to facilitate evaluation of machine
translation at the document level. As a linguis-
tic means to achieve text coherence, lexical
cohesion ties sentences together into a mean-
ingfully interwoven structure through words
with the same or related meaning. A compar-
ison between machine and human translation
is conducted to illustrate one of their critical
distinctions that human translators tend to use
more cohesion devices than machine. Various
ways to apply this feature to evaluate machine-
translated documents are presented, including
one without reliance on reference translation.
Experimental results show that incorporating
this feature into sentence-level evaluation met-
rics can enhance their correlation with human
judgements.
</bodyText>
<sectionHeader confidence="0.999474" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999764375">
Machine translation (MT) has benefited a lot from
the advancement of automatic evaluation in the past
decade. To a certain degree, its progress is also con-
fined to the limitations of evaluation metrics in use.
Most efforts devoted to evaluate the quality of MT
output so far have still focused on the sentence level
without sufficient attention to how a larger text is
structured. This is notably reflected in the represen-
tative MT evaluation metrics, such as BLEU (Pap-
ineni et al., 2002), METEOR (Banerjee and Lavie,
2005) and TER (Snover et al., 2006), that adopt a
sentence-by-sentence fashion to score MT outputs.
The evaluation result for a document by any of them
is usually a simple average of its sentence scores. A
drawback of this kind of sentence-based evaluation
is the neglect of document structure. There is no
guarantee for the coherence of a text if it is produced
by simply putting together stand-alone sentences, no
matter how well-translated, without adequate inter-
sentential connection. As a consequence, MT sys-
tem optimized this way to any of these metrics can
only have a very dim chance of producing translated
document that reads as natural as human writing.
The accuracy of MT output at the document level
is particularly important to MT users, for they care
about the overall meaning of a text in question more
than the grammatical correctness of each sentence
(Visser and Fuji, 1996). Post-editors particularly
need to ensure the quality of a whole document of
MT output when revising its sentences. The con-
nectivity of sentences is surely a significant factor
contributing to the understandability of a text as a
whole.
This paper studies the inter-sentential linguistic
features of cohesion and coherence and presents
plausible ways to incorporate them into the
sentence-based metrics to support MT evaluation at
the document level. In the Framework for MT Eval-
uation in the International Standards of Language
Engineering (FEMTI) (King et al., 2003), coherence
is defined as “the degree to which the reader can de-
scribe the role of each individual sentence (or group
of sentences) with respect to the text as a whole”.
The measurement of coherence has to rely on cohe-
sion, referring to the “relations of meaning that exist
within the text” (Halliday and Hasan, 1976). Cohe-
sion is realized via the interlinkage of grammatical
and lexical elements across sentences. Grammatical
</bodyText>
<page confidence="0.872361">
1060
</page>
<note confidence="0.760793">
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1060–1068, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.999803444444444">
cohesion refers to the syntactic links between text
items, while lexical cohesion is achieved through the
word choices in a text. This paper focuses on the
latter. A quantitative comparison of lexical cohesion
devices between MT output and human translation
is first conducted, to examine the weakness of cur-
rent MT systems in handling this feature. Different
ways of exploiting lexical cohesion devices for MT
evaluation at the document level are then illustrated.
</bodyText>
<sectionHeader confidence="0.99978" genericHeader="introduction">
2 Related Works
</sectionHeader>
<bodyText confidence="0.999860057142857">
Cohesion and coherence are both necessary mono-
lingual features in a target text. They can hardly
be evaluated in isolation and have to be conjoined
with other quality criteria such as adequacy and flu-
ency. A survey of MT post-editing (Vasconcellos,
1989) suggests that cohesion and coherence serve
as higher level quality criteria beyond many others
such as syntactic well-formedness. Post-editors tend
to correct syntactic errors first before any amend-
ment for improving the cohesion and coherence of
an MT output. Also, as Wilks (1978)1 noted, it
is rather unlikely for a sufficiently large sample of
translations to be coherent and totally wrong at the
same time. Cohesion and coherence are appropri-
ate to serve as criteria for the overall quality of MT
output.
Previous researches in MT predominantly focus
on specific types of cohesion devices. For grammat-
ical cohesion, a series of works, including Nakaiwa
and Ikehara (1992), Nakaiwa et al. (1995), and
Nakaiwa and Shirai (1996), present approaches to
resolving Japanese zero pronouns and to integrat-
ing them into a Japanese-English transferred-based
MT system. Peral et al. (1999) propose an inter-
lingual mechanism for pronominal anaphora gen-
eration by exploiting a rich set of lexical, syntac-
tic, morphologic and semantic information. Mu-
rata and Nagao (1993) and Murata et al. (2001) de-
velop a rule base to identify the referential prop-
erties of Japanese noun phrases, so as to facilitate
anaphora resolution for Japanese and article gen-
eration for English during translation. A recent
COMTIS project (Cartoni et al., 2011) begins to ex-
ploit inter-sentential information for statistical MT.
A phase of its work is to have grammatical devices,
</bodyText>
<footnote confidence="0.729421">
1As cited in van Slype (1979).
</footnote>
<bodyText confidence="0.999583333333334">
such as verbal tense/aspect/mode, discourse connec-
tives and pronouns, manually annotated in multilin-
gual corpora, in hopes of laying a foundation for the
development of automatic labelers for them that can
be integrated into an MT model.
For lexical cohesion, it has been only partially and
indirectly addressed in terms of translation consis-
tency in MT output. Different approaches to main-
taining consistency in target word choices are pro-
posed (Itagaki et al., 2007; Gong et al., 2011; Xiao
et al., 2011). Carpuat (2009) also observes a general
tendency in human translation that a given sense is
usually lexicalized in a consistent manner through-
out the whole translation.
Nevertheless there are only a few evaluation
methods explicitly targeting on the quality of a docu-
ment. Miller and Vanni (2001) devise a human eval-
uation approach to measure the comprehensibility
of a text as a whole, based on the Rhetorical Struc-
ture Theory (Mann and Thompson, 1988), a theory
of text organization specifying coherence relations
in an authentic text. Snover et al. (2006) proposes
HTER to assess post-editing effort through human
annotation. Its automatic versions TER and TERp
(Snover et al., 2009), however, remain sentence-
based metrics. Comelles et al. (2010) present a
family of automatic MT evaluation measures, based
on the Discourse Representation Theory (Kamp and
Reyle, 1993), that generate semantic trees to put to-
gether different text entities for the same referent ac-
cording to their contexts and grammatical connec-
tions. Apart from MT evaluation, automated essay
scoring programs such as E-rater (Burstein, 2003)
also employ a rich set of discourse features for as-
sessment. However, the parsing process needed for
these linguistic-heavy approaches may suffer seri-
ously from grammatical errors, which are unavoid-
able in MT output. Hence their accuracy and reli-
ability inevitably fluctuate in accord with different
evaluation data.
Lexical cohesion has far been neglected in both
MT and MT evaluation, even though it is the single
most important form of cohesion devices, account-
ing for nearly half of the cohesion devices in En-
glish (Halliday and Hasan, 1976). It is also a signif-
icant feature contributing to translation equivalence
of texts by preserving their texture (Lotfipour-Saedi,
1997). The lexical cohesion devices in a text can be
</bodyText>
<page confidence="0.979571">
1061
</page>
<bodyText confidence="0.999991307692308">
represented as lexical chains conjoining related en-
tities. There are many methods of computing lexical
chains for various purposes, e.g., Morris and Hirst
(1991), Barzilay and Elhadad (1997), Chan (2004),
Li et al. (2007), among many others. Contrary to
grammatical cohesion highly depending on syntac-
tic well-formedness of a text, lexical cohesion is less
affected by grammatical errors. Its computation has
to rely on a thesaurus, which is usually available for
almost every language. In this research, a number
of formulations of lexical cohesion, with or without
reliance on external language resource, will be ex-
plored for the purpose of MT evaluation.
</bodyText>
<sectionHeader confidence="0.9069945" genericHeader="method">
3 Lexical Cohesion in Machine and
Human Translation
</sectionHeader>
<bodyText confidence="0.9998784">
This section presents a comparative study of MT and
human translation (HT) in terms of the use of lexi-
cal cohesion devices. It is an intuition that more co-
hesion devices are used by humans than machines
in translation, as part of the superior quality of HT.
Two different datasets are used to ensure the relia-
bility and generality of the comparison. The results
confirm the incapability of MT in handling this fea-
ture and the necessity of using lexical cohesion in
MT evaluation.
</bodyText>
<subsectionHeader confidence="0.995389">
3.1 Data
</subsectionHeader>
<bodyText confidence="0.999991833333333">
The MetricsMATR 2008 development set (Przy-
bocki et al., 2009) and the Multiple-Translation Chi-
nese (MTC) part 4 (Ma, 2006) are used for this
study. They consist of MT outputs of different
source languages in company with reference trans-
lations. The data of MetricsMATR is selected from
the NIST Open MT 2006 evaluation, while MTC4 is
from the TIDES 2003 MT evaluation. Both datasets
include human assessments of MT output, from
which the part of adequacy assessment is selected
for this study. Table 1 provides overall statistics of
the datasets.
</bodyText>
<subsectionHeader confidence="0.999929">
3.2 Identification of Lexical Cohesion Devices
</subsectionHeader>
<bodyText confidence="0.9999722">
Lexical cohesion is achieved through word choices
of two major types: reiteration and collocation. Re-
iteration can be realized in a continuum or a cline of
specificity, with repetition of the same lexical item at
one end and the use of a general noun to point to the
</bodyText>
<equation confidence="0.481822">
MetricsMATR MTC4
</equation>
<bodyText confidence="0.860489333333333">
Number of systems 8 6
Number of documents 25 100
Number of segments 249 919
Number of references 4 4
Source language Arabic Chinese
Genre Newswire Newswire
</bodyText>
<tableCaption confidence="0.998485">
Table 1: Information about the datasets in use
</tableCaption>
<bodyText confidence="0.999993483870968">
same referent at the other. In between the two ends
is to use a synonym (or near-synonym) and superor-
dinate. Collocation refers to those lexical items that
share the same or similar semantic relations, includ-
ing complementarity, antonym, converse, coordinate
term, meronym, troponym, and so on.
In this study, lexical cohesion devices are defined
as content words (i.e., tokens after stopword having
been removed) that reiterate once or more times in
a document, including synonym, near-synonym and
superordinate, besides those repetition and colloca-
tion. Repetition refers to the same words or stems
in a document. Stems are identified with the aid of
Porter stemmer (1980).
To classify the semantic relationships of words,
WordNet (Fellbaum, 1998) is used as a lexical re-
source, which clusters words of the same sense (i.e.,
synonyms) into a semantic group, namely a synset.
Synsets are interlinked in WordNet according to
their semantic relationships. Superordinate and col-
location are formed by words in a proximate se-
mantic relationship, such as bicycle and vehicle (hy-
pernym), bicycle and wheel (meronym), bicycle and
car (coordinate term), and so on. They are defined
as synset pairs with a distance of 1 in WordNet.
The measure of semantic distance (Wu and Palmer,
1994) is also applied to identify near-synonyms, i.e.,
words that are synonyms in a broad sense but not
grouped in the same synset. It quantifies the seman-
tic similarity of word pairs as a real number in be-
tween 0 and 1 (the higher the more similar) as
</bodyText>
<equation confidence="0.711381">
sim(c1, c2) = d(c1) + d(c2)
</equation>
<bodyText confidence="0.999983666666667">
where c1 and c2 are the concepts (synsets) that the
two words in question belong to, d is the distance
in terms of the shortest path from a concept to the
</bodyText>
<equation confidence="0.525414">
2 d(lcs(c1,c2))
</equation>
<page confidence="0.840732">
1062
</page>
<table confidence="0.998552">
Word type MT MetricsMATR MT MTC4
HT Difference (%) HT Difference (%)
Content word 4428 4636 208 (4.7) 16162 16982 830 (5.1)
- Not lexical cohesion device 2403 2381 -22 (-1.0) 8657 8814 157 (1.8)
- Lexical cohesion device 2025 2255 230 (11.4) 7505 8168 663 (8.9)
- Repetition 1297 1445 148 (11.4) 4888 5509 621 (12.7)
- Synonym and near-synonym 318 350 32 (10.1) 1323 1311 -12 (-0.9)
- Superordinate and collocation 410 460 50 (12.4) 1294 1348 54 (4.2)
</table>
<tableCaption confidence="0.9320715">
Table 2: Statistics of lexical cohesion devices in machine versus human translation (average frequencies per version
of MT/HT)
</tableCaption>
<figure confidence="0.999476642857143">
RC (MT)
RC (HT)
LC (MT)
LC (HT)
0.55 0.55
0.50 0.50
0.45 0.45
0.40 0.40
Values of RC &amp; LC
0.35 0.35
0.30 0.30
0.25 0.25
0.20 0.20
MetricsMATR MTC4
</figure>
<figureCaption confidence="0.99999">
Figure 1: Use of lexical cohesion devices in machine versus human translation
</figureCaption>
<bodyText confidence="0.9996115">
global root node in WordNet, and lcs is the least
common subsumer (i.e., the most specific ancestor
concept) of c1 and c2. A threshold is set to 0.96
for words to be considered near-synonyms of each
other, based on the empirical observation in a previ-
ous study (Wong, 2010).
</bodyText>
<subsectionHeader confidence="0.841543">
3.3 Results
</subsectionHeader>
<bodyText confidence="0.995370647058823">
The difference between MT and HT (reference
translation) in terms of the frequencies of lexical co-
hesion devices in MetricsMATR and MTC4 datasets
is presented in Table 2. The frequencies are aver-
aged by the number of MT/HT versions. A further
categorization breaks down content words into lex-
ical cohesion devices and those that are not. The
count of each type of lexical cohesion device is also
provided. In general the two datasets provide highly
similar statistics. There are 4.7–5.1% more content
words in HT than in MT. The numbers of ordinary
content words (i.e., not lexical cohesion devices) are
close in MT and HT. The difference of content words
in HT and MT is mostly due to that of lexical co-
hesion devices, which are mostly repetition. 8.9–
11.4% more lexical cohesion devices are found in
HT than in MT in the datasets.
A further analysis is carried out to investigate into
the use of lexical cohesion devices in each version
of MT and HT in terms of the following two ratios,
LC = lexical cohesion devices / content words,
RC = repetition / content words.
A higher LC or RC ratio means that a greater pro-
portion of content words are used as lexical cohesion
devices.
Figure 1 illustrates the RC and LC ratios in the
two datasets. The ratios of different MT systems
are presented in an ascending order in each graph
from left to right, according to their human assess-
ment results. The distributions of these values show
a strong similarity between the two datasets. First,
most of the RC and LC ratios are within an observ-
able range, i.e., 0.25–0.35 for the former and 0.40–
0.50 for the latter, except a particularly low LC for
</bodyText>
<page confidence="0.706428">
1063
</page>
<table confidence="0.85744219047619">
MT 1
1 Chine scrambled research on 16 key technical
2 These techniques are from within headline everyones boosting science and technology and achiev-
ing goals and contend of delivered on time bound through achieving breakthroughs in essential
technology and complimentarity resources . national
BLEU: 0.224 (1-gram:7, 2-gram:0, 3-gram:2, 4-gram:1)
LC: 0.107 (number of lexical cohesion devices: 5)
Human assessment: 2.67
MT 2
1 China is accelerating research 16 main technologies
2 These technologies are within the important realm to promote sciences and technology and
achieve national goals and must be completed in a timely manner through achieving main dis-
coveries in technology and integration of resources .
BLEU: 0.213 (1-gram:5, 2-gram:3, 3-gram:2, 4-gram:1)
LC: 0.231 (number of lexical cohesion devices: 9)
Human assessment: 4.33
Reference
1 China Accelerates Research on 16 Main Technologies
2 These technologies represent a significant part in the development of science and technology and
the achievement of national goals. They must be accomplished within a fixed period of time by
realizing breakthroughs in essential technologies and integration of resources.
</table>
<tableCaption confidence="0.999657">
Table 3: An example of MT outputs of different quality (underlined: matched n-grams; italic: lexical cohesion devices)
</tableCaption>
<bodyText confidence="0.9998585">
one MT system. Second, the ratios in those differ-
ent HT versions are very stable in comparison with
those of MT. Especially, all four HT versions in the
MetricsMATR dataset share the same RC ratio 0.31.
This shows a typical level of the use of lexical cohe-
sion device. Third, the ratios in MT are lower than or
at most equal to those in HT, suggesting their corre-
lation with translation quality: the closer their RC
and LC ratios to those in HT, the better the MT.
These results verify our assumption that lexical co-
hesion can serve as an effective proxy of the level of
translation quality.
</bodyText>
<sectionHeader confidence="0.990368" genericHeader="method">
4 MT Evaluation at Document Level
</sectionHeader>
<bodyText confidence="0.999949571428571">
As a feature at the discourse level, lexical cohesion
is a good complement to current evaluation met-
rics focusing on features at the sentence level. Ta-
ble 3 illustrates an example selected from the Met-
ricsMATR dataset, consisting two versions of MT
output for a short document of two segments only.
The n-grams matched with the reference are under-
lined, while the lexical cohesion devices are itali-
cized. The two MT outputs have a similar num-
ber of matched n-grams and hence receive similar
BLEU scores. These scores, however, do not reflect
their real difference in quality: the second version is
better, according to human assessment of adequacy.
Instead, their LC ratios seem to represent such a
variation more accurately. The theme of the second
output is also highlighted through the lexical chains,
including main/important, technology/technologies
and achieve/achieving, which create a tight texture
between the two sentences, a crucial factor of text
quality.
To perform MT evaluation at the document level,
the LC and RC ratios can be used alone or in-
tegrated into a sentence-level metric. The former
way has an advantage that it does not have to rely
on any reference translation. LC mainly requires
a thesaurus for computing semantic relation, while
RC only needs a morphological processor such as
stemmer, both of which are available for most lan-
</bodyText>
<page confidence="0.985391">
1064
</page>
<bodyText confidence="0.999971294117647">
guages. Its drawback, however, lies in the risk of
relying on a single discourse feature. Although lex-
ical cohesion gives a strong indication of text co-
herence, it is not indispensable, because a text can
be coherent without any surface cohesive clue. Fur-
thermore, the quality of a document is also reflected
in that of its sentences. A coherent translation may
be mistranslated, and on the other hand, a text con-
taining lots of sentence-level errors would make it
difficult to determine its document-level quality. A
previous study comparing MT evaluation at the sen-
tence versus document level (Wong et al., 2011) re-
ports a poor consistency in the evaluation results at
these two levels when the sentence-level scores of
MT output are low. In regard of these, how to inte-
grate these two levels of MT evaluation is particu-
larly worth studying.
</bodyText>
<sectionHeader confidence="0.999793" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999982260869566">
We examine, through experiments, the effectiveness
of using LC and RC ratios alone and integrating
them into other evaluation metrics for MT evalua-
tion at the document and system levels. Three evalu-
ation metrics, namely BLEU, TER and METEOR,2
are selected for testing. They represent three dis-
tinctive types of evaluation metrics: n-gram, edit-
distance, and unigram with external language re-
sources, respectively. These metrics are evaluated in
terms of their correlation with human assessments,
using Pearson’s r correlation coefficient. The Met-
ricsMATR and MTC4 datasets and their adequacy
assessments are used as evaluation data. Note that
the adequacy assessment is in fact an evaluation
method for the sentence level. We have to rely on
an assumption that this evaluation data may emulate
document-level quality, since its MT outputs were
assessed sentence by sentence in sequence as in a
document. All experiments are performed under a
setting of multiple reference translations.
The integration of the two ratios into an evaluation
metric follows a simple weighted average approach.
A hybrid metric H is formulated as
</bodyText>
<equation confidence="0.932284">
H = α mdoe + (1 − α) m3eg
</equation>
<bodyText confidence="0.996268">
where mdoe refers to the document-level feature in
</bodyText>
<footnote confidence="0.837706">
2METEOR 1.0 with default parameters optimized over the
adequacy assessments.
</footnote>
<bodyText confidence="0.998278714285714">
use (i.e., LC or RC), m3eg to a sentence-level met-
ric, and α to a weight controlling their proportion.
The MetricsMATR dataset is used as training data to
optimize the values of α for different metrics, while
the MTC4 is used as evaluation data. Table 4 shows
the optimized weights for the metrics for evaluation
at the document level.
</bodyText>
<table confidence="0.9977775">
Metrics RC LC
BLEU 0.28 0.29
TER 0.40 0.38
METEOR 0.19 0.18
</table>
<tableCaption confidence="0.9959815">
Table 4: Optimized weights for the integration of dis-
course feature into sentence-level metrics
</tableCaption>
<bodyText confidence="0.9984088125">
Table 5 presents the correlation rates of evalua-
tion metrics obtained in our experiments under dif-
ferent settings, with their 95% conference intervals
(CI) provided. The LC and RC ratios are found to
have strong correlations with human assessments at
the system level even when used alone, highly com-
parable to BLEU and TER. At the document level,
however, they are not as good as the others. They
show their advantages when integrated into other
metrics, especially BLEU and TER. LC raises the
correlation of BLEU from 0.447 to 0.472 and from
0.861 to 0.905 at the document and system levels,
respectively. It improves TER even more signifi-
cantly, in that the correlation rates are boosted up
from -0.326 to -0.390 at the document level, and
even from -0.601 to -0.763 at the system level. Since
there are only six systems in the MTC4 data, such a
dramatic change may not be as meaningful as the
smooth improvement at the document level. ME-
TEOR is a special case in this experiment. Its corre-
lation cannot be improved by integrating LC or RC,
and is even slightly dropped at the document level.
The cause for this is yet to be identified. Neverthe-
less, these results confirm the close relationship of
an MT system’s capability to appropriately generate
lexical cohesion devices with the quality of its out-
put.
Table 6 presents the Pearson correlations between
evaluation results at the document level using dif-
ferent evaluation metrics in the MTC4 data. It il-
lustrates the homogeneity/heterogeneity of different
metrics and helps explain the performance change
</bodyText>
<page confidence="0.930254">
1065
</page>
<table confidence="0.999849769230769">
Metrics Document System
Correlation 95% CI Correlation 95% CI
RC 0.243 (0.167, 0.316) 0.873 (0.211, 0.985)
LC 0.267 (0.192, 0.339) 0.818 (0.020, 0.979)
BLEU 0.447 (0.381, 0.508) 0.861 (0.165, 0.984)
BLEU+RC 0.463 (0.398, 0.523) 0.890 (0.283, 0.987)
BLEU+LC 0.472 (0.408, 0.531) 0.905 (0.352, 0.989)
TER -0.326 (-0.253, -0.395) -0.601 (-0.411, -0.949)
TER+RC -0.370 (-0.299, -0.437) -0.740 (-0.179, -0.969)
TER+LC -0.390 (-0.320, -0.455) -0.763 (-0.127, -0.972)
METEOR 0.557 (0.500, 0.609) 0.961 (0.679, 0.995)
METEOR+RC 0.555 (0.498, 0.608) 0.960 (0.672, 0.995)
METEOR+LC 0.556 (0.499, 0.609) 0.962 (0.687, 0.995)
</table>
<tableCaption confidence="0.995929">
Table 5: Correlation of different metrics with adequacy assessment in MTC4 data
</tableCaption>
<table confidence="0.999149833333333">
BLEU 1
TER -0.699 1
METEOR 0.834 -0.510 1
RC 0.287 -0.204 0.405 1
LC 0.263 -0.097 0.437 0.736 1
BLEU TER METEOR RC LC
</table>
<tableCaption confidence="0.999782">
Table 6: Correlation between the evaluation results of different metrics
</tableCaption>
<bodyText confidence="0.99996732">
by combining sentence- and document-level met-
rics. The table shows that the two ratios LC and
RC highly correlate with each other, as if they are
two variants of quantifying lexical cohesion devices.
The three sentence-level metrics, BLEU, TER and
METEOR, also show strong correlations with each
other, especially between BLEU and METEOR. The
correlations are generally weaker between sentence-
and document-level metrics, for instance, 0.263 be-
tween BLEU and LC and only -0.097 between TER
and LC, showing that they are quite heterogeneous
in nature. This accounts for the significant perfor-
mance gain from their combination: their difference
allows them to complement each other. It is also
worth noting that between METEOR and LC the
correlation of 0.437 is mildly strong, explaining the
negative result of their integration. On the one hand,
lexical cohesion is word choice oriented, which is
only sensitive to the reiteration and semantic relat-
edness of words in MT output. On the other hand,
METEOR is strong in unigram matching, with mul-
tiple strategies to maximize the matching rate be-
tween MT output and reference translation. In this
sense they are homogeneous to a certain extent, ex-
plaining the null effect of their combination.
</bodyText>
<sectionHeader confidence="0.993448" genericHeader="discussions">
6 Discussion and Conclusion
</sectionHeader>
<bodyText confidence="0.999987882352941">
In this study we have attempted to address the prob-
lem that most existing MT evaluation metrics dis-
regard the connectivity of sentences in a document.
By focusing on a typical type of cohesion, i.e., lexi-
cal cohesion, we have shown that its use frequency is
a significant factor to differentiate HT from MT and
MT outputs of different quality from each other. The
high correlation rate of its use with translation ade-
quacy also suggests that the more lexical cohesion
devices in use, the better the quality of MT output.
Accordingly we have used two ratios, LC and RC,
to capture such correlativity. Our experimental re-
sults have confirmed the effectiveness of this feature
in accounting for the document-level quality of MT
output. The performance of two evaluation metrics,
BLEU and TER, is highly improved through incor-
porating this document-level feature, in terms of the
</bodyText>
<page confidence="0.976255">
1066
</page>
<bodyText confidence="0.999776032258064">
change of their correlation with human assessments.
This finding is positive and sheds light on a region
of MT research that is still severely under-explored.
Our approach to extending the granularity of MT
evaluation from sentence to document through lex-
ical cohesion is highly applicable to different lan-
guages. It has a relatively weak demand for lan-
guage resource in comparison with the processing of
other discourse features like grammatical cohesion.
It is also much unaffected by grammatical problems
or errors commonly seen in natural languages and,
in particular, MT outputs.
Our future work will continue to explore the re-
lationship of lexical cohesion to translation quality,
so as to identify, apart from its use frequency, other
significant aspects for MT evaluation at the docu-
ment level. A frequent use of cohesion devices in
a text is not necessarily appropriate, because an ex-
cess of them may decrease the quality and readabil-
ity of a text. Human writers can strategically change
the ways of expression to achieve appropriate coher-
ence and also avoid overuse of the same lexical item.
To a certain extent, this is one of the causes for the
unnaturalness of MT output: it may contain a large
number of lexical cohesion devices which are sim-
ply direct translation of those in a source text that
do not fit in the target context. How to use lexical
cohesion devices appropriately instead of frequently
is thus an important issue to tackle before we can
adopt them in MT and MT evaluation by a suitable
means.
</bodyText>
<sectionHeader confidence="0.99863" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9997675">
The research described in this paper was substan-
tially supported by the Research Grants Council
(RGC) of Hong Kong SAR, P. R. China through
GRF grant 144410.
</bodyText>
<sectionHeader confidence="0.998737" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.993843180327869">
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An automatic metric for MT evaluation with improved
correlation with human judgments. In Proceedings of
the ACL Workshop on Intrinsic and Extrinsic Evalu-
ation Measures for Machine Translation and/or Sum-
marization, pages 65–72, Ann Arbor, Michigan.
Regina Barzilay and Michael Elhadad. 1997. Using lex-
ical chains for text summarization. In Proceedings of
the ACL Workshop on Intelligent Scalable Text Sum-
marization, pages 10–17.
Jill Burstein. 2003. The E-rater scoring engine: Auto-
mated essay scoring with natural language processing.
In Mark D. Shermis and Jill Burstein, editors, Auto-
mated Essay Scoring: A Cross-Disciplinary Perspec-
tive, chapter 7, pages 113–122. Lawrence Erlbaum As-
sociates.
Marine Carpuat. 2009. One translation per discourse.
In Proceedings of the NAACL HLT Workshop on Se-
mantic Evaluations: Recent Achievements and Future
Directions, pages 19–27, Boulder, Colorado.
Bruno Cartoni, Andrea Gesmundo, James Henderson,
Cristina Grisot, Paola Merlo, Thomas Meyer, Jacques
Moeschler, Sandrine Zufferey, and Andrei Popescu-
Belis. 2011. Improving MT coherence through text-
level processing of input texts: The COMTIS project.
In Tralogy, Paris.
Samuel W. K. Chan. 2004. Extraction of sailent tex-
tual patterns: Synergy between lexical cohesion and
contextual coherence. IEEE Transactions on Systems,
Man and Cybernetics, Part A: Systems and Humans,
34(2):205–218.
Elisabet Comelles, Jesus Gim´enez, Lluis M´arquez, Irene
Castell`on, and Victoria Arranz. 2010. Document-
level automatic MT evaluation based on discourse rep-
resentations. In Proceedings of the Joint Fifth Work-
shop on Statistical Machine Translation and Metrics-
MATR, pages 333–338, Uppsala.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press, Cambridge, MA.
Zhengxian Gong, Min Zhang, and Guodong Zhou. 2011.
Cache-based document-level statistical machine trans-
lation. In EMNLP 2011, pages 909–919, Edinburgh,
Scotland.
M. A. K. Halliday and Ruqaiya Hasan. 1976. Cohesion
in English. London: Longman.
Masaki Itagaki, Takako Aikawa, and Xiaodong He.
2007. Automatic validation of terminology translation
consistency with statistical method. In MT Summit XI,
pages 269–274.
Hans Kamp and Uwe Reyle. 1993. From Discourse to
Logic: An Introduction to Modeltheoretic Semantics of
Natural Language, Formal Logic and Discourse Rep-
resentation Theory. Dordrecht: Kluwer.
Margaret King, Andrei Popescu-Belis, and Eduard Hovy.
2003. FEMTI: Creating and using a framework for
MT evaluation. In MT Summit IX, pages 224–231,
New Orleans.
Jing Li, Le Sun, Chunyu Kit, and Jonathan Webster.
2007. A query-focused multi-document summarizer
based on lexical chains. In DUC 2007, Rochester,
New York.
</reference>
<page confidence="0.72738">
1067
</page>
<reference confidence="0.999665114583333">
Kazem Lotfipour-Saedi. 1997. Lexical cohesion and
translation equivalence. Meta, 42(1):185–192.
Xiaoyi Ma. 2006. Multiple-Translation Chinese (MTC)
part 4. Linguistic Data Consortium.
William C. Mann and Sandra A. Thompson. 1988.
Rhetorical structure theory: Toward a functional the-
ory of text organization. Text, 8(3):243–281.
Keith J. Miller and Michelle Vanni. 2001. Scaling
the ISLE taxonomy: Development of metrics for the
multi-dimensional characterisation of machine trans-
lation quality. In MT Summit VIII, pages 229–238.
Jane Morris and Graeme Hirst. 1991. Lexical cohe-
sion computed by thesaural relations as an indicator
of the structure of text. Computational Linguistics,
17(1):21–48.
Masaki Murata and Makoto Nagao. 1993. Determina-
tion of referential property and number of nouns in
Japanese sentences for machine translation into En-
glish. In TMI 1993, pages 218–225, Kyoto.
Masaki Murata, Kiyotaka Uchimoto, Qing Ma, and Hi-
toshi Isahara. 2001. A machine-learning approach to
estimating the referential properties of Japanese noun
phrases. In CICLING 2001, pages 142–153, Mexico-
City.
Hiromi Nakaiwa and Satoru Ikehara. 1992. Zero pro-
noun resolution in a machine translation system by us-
ing Japanese to English verbal semantic attributes. In
ANLP 1992, pages 201–208.
Hiromi Nakaiwa and Satoshi Shirai. 1996. Anaphora
resolution of Japanese zero pronouns with deictic ref-
erence. In COLING 1996, pages 812–817, Copen-
hagen.
Hiromi Nakaiwa, Satoshi Shirai, Satoru Ikehara, and
Tsukasa Kawaok. 1995. Extrasentential resolution
of Japanese zero pronouns using semantic and prag-
matic constraints. In Proceedings of the AAAI 1995
Spring Symposium Series: Empirical Methods in Dis-
course Interpretation and Generation, pages 99–105,
Stanford.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic eval-
uation of machine translation. In ACL 2002, pages
311–318.
Jes´us Peral, Manuel Palomar, and Antonio Ferr`andez.
1999. Coreference-oriented interlingual slot structure
and machine translation. In Proceedings of the ACL
Workshop on Coreference and its Applications, pages
69–76, College Park, MD.
Martin F. Porter. 1980. An algorithm for suffix stripping.
Program, 14(3):130–137.
Mark Przybocki, Kay Peterson, and S´ebastien Bronsart.
2009. 2008 NIST metrics for machine translation
(MetricsMATR08) development data. Linguistic Data
Consortium.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In AMTA 2006, pages 223–231.
Matthew Snover, Nitin Madnani, Bonnie J. Dorr, and
Richard Schwartz. 2009. Fluency, adequacy, or
HTER? Exploring different human judgments with a
tunable MT metric. In Proceedings of the 4th Work-
shop on Statistical Machine Translation, pages 259–
268, Athens.
Georges van Slype. 1979. Critical Study of Methods for
Evaluating the Quality of Machine Translation. Tech-
nical report, Bureau Marcel van Dijk / European Com-
mission, Brussels.
Muriel Vasconcellos. 1989. Cohesion and coherence in
the presentation of machine translation products. In
James E. Alatis, editor, Georgetown University Round
Table on Languages and Linguistics 1989: Language
Teaching, Testing, and Technology: Lessons from the
Past with a View Toward the Future, pages 89–105.
Georgetown University Press.
Eric M. Visser and Masaru Fuji. 1996. Using sentence
connectors for evaluating MT output. In COLING
1996, pages 1066–1069.
Yorick Wilks. 1978. The Value of the Monolingual
Component in MT Evaluation and its Role in the Bat-
telle. Report on Systran, Luxembourg CEC Memoran-
dum.
Billy T. M. Wong, Cecilia F. K. Pun, Chunyu Kit, and
Jonathan J. Webster. 2011. Lexical cohesion for eval-
uation of machine translation at document level. In
NLP-KE 2011, pages 238–242, Tokushima.
Billy Tak-Ming Wong. 2010. Semantic evaluation of ma-
chine translation. In LREC 2010, pages 2884–2888,
Valletta.
Zhibiao Wu and Martha Palmer. 1994. Verb semantics
and lexical selection. In ACL 1994, pages 133–138,
Las Cruces.
Tong Xiao, Jingbo Zhu, Shujie Yao, and Hao Zhang.
2011. Document-level consistency verification in ma-
chine translation. In MT summit XIII, pages 131–138,
Xiamen.
</reference>
<page confidence="0.976544">
1068
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.512885">
<title confidence="0.8178065">Extending Machine Translation Evaluation Metrics with Lexical To Document Level</title>
<author confidence="0.999919">T M Wong</author>
<affiliation confidence="0.9879095">Department of Chinese, Translation and City University of Hong</affiliation>
<address confidence="0.897285">83 Tat Chee Avenue, Kowloon, Hong Kong SAR, P. R.</address>
<abstract confidence="0.994716">This paper proposes the utilization of lexical cohesion to facilitate evaluation of machine translation at the document level. As a linguistic means to achieve text coherence, lexical cohesion ties sentences together into a meaningfully interwoven structure through words with the same or related meaning. A comparison between machine and human translation is conducted to illustrate one of their critical distinctions that human translators tend to use more cohesion devices than machine. Various ways to apply this feature to evaluate machinetranslated documents are presented, including one without reliance on reference translation. Experimental results show that incorporating this feature into sentence-level evaluation metrics can enhance their correlation with human judgements.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Satanjeev Banerjee</author>
<author>Alon Lavie</author>
</authors>
<title>METEOR: An automatic metric for MT evaluation with improved correlation with human judgments.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization,</booktitle>
<pages>65--72</pages>
<location>Ann Arbor, Michigan.</location>
<contexts>
<context position="1620" citStr="Banerjee and Lavie, 2005" startWordPosition="241" endWordPosition="244">sentence-level evaluation metrics can enhance their correlation with human judgements. 1 Introduction Machine translation (MT) has benefited a lot from the advancement of automatic evaluation in the past decade. To a certain degree, its progress is also confined to the limitations of evaluation metrics in use. Most efforts devoted to evaluate the quality of MT output so far have still focused on the sentence level without sufficient attention to how a larger text is structured. This is notably reflected in the representative MT evaluation metrics, such as BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005) and TER (Snover et al., 2006), that adopt a sentence-by-sentence fashion to score MT outputs. The evaluation result for a document by any of them is usually a simple average of its sentence scores. A drawback of this kind of sentence-based evaluation is the neglect of document structure. There is no guarantee for the coherence of a text if it is produced by simply putting together stand-alone sentences, no matter how well-translated, without adequate intersentential connection. As a consequence, MT system optimized this way to any of these metrics can only have a very dim chance of producing </context>
</contexts>
<marker>Banerjee, Lavie, 2005</marker>
<rawString>Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, pages 65–72, Ann Arbor, Michigan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Michael Elhadad</author>
</authors>
<title>Using lexical chains for text summarization.</title>
<date>1997</date>
<booktitle>In Proceedings of the ACL Workshop on Intelligent Scalable Text Summarization,</booktitle>
<pages>10--17</pages>
<contexts>
<context position="8507" citStr="Barzilay and Elhadad (1997)" startWordPosition="1332" endWordPosition="1335">ent evaluation data. Lexical cohesion has far been neglected in both MT and MT evaluation, even though it is the single most important form of cohesion devices, accounting for nearly half of the cohesion devices in English (Halliday and Hasan, 1976). It is also a significant feature contributing to translation equivalence of texts by preserving their texture (Lotfipour-Saedi, 1997). The lexical cohesion devices in a text can be 1061 represented as lexical chains conjoining related entities. There are many methods of computing lexical chains for various purposes, e.g., Morris and Hirst (1991), Barzilay and Elhadad (1997), Chan (2004), Li et al. (2007), among many others. Contrary to grammatical cohesion highly depending on syntactic well-formedness of a text, lexical cohesion is less affected by grammatical errors. Its computation has to rely on a thesaurus, which is usually available for almost every language. In this research, a number of formulations of lexical cohesion, with or without reliance on external language resource, will be explored for the purpose of MT evaluation. 3 Lexical Cohesion in Machine and Human Translation This section presents a comparative study of MT and human translation (HT) in te</context>
</contexts>
<marker>Barzilay, Elhadad, 1997</marker>
<rawString>Regina Barzilay and Michael Elhadad. 1997. Using lexical chains for text summarization. In Proceedings of the ACL Workshop on Intelligent Scalable Text Summarization, pages 10–17.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jill Burstein</author>
</authors>
<title>The E-rater scoring engine: Automated essay scoring with natural language processing.</title>
<date>2003</date>
<booktitle>Automated Essay Scoring: A Cross-Disciplinary Perspective, chapter 7,</booktitle>
<pages>113--122</pages>
<editor>In Mark D. Shermis and Jill Burstein, editors,</editor>
<contexts>
<context position="7583" citStr="Burstein, 2003" startWordPosition="1189" endWordPosition="1190">oherence relations in an authentic text. Snover et al. (2006) proposes HTER to assess post-editing effort through human annotation. Its automatic versions TER and TERp (Snover et al., 2009), however, remain sentencebased metrics. Comelles et al. (2010) present a family of automatic MT evaluation measures, based on the Discourse Representation Theory (Kamp and Reyle, 1993), that generate semantic trees to put together different text entities for the same referent according to their contexts and grammatical connections. Apart from MT evaluation, automated essay scoring programs such as E-rater (Burstein, 2003) also employ a rich set of discourse features for assessment. However, the parsing process needed for these linguistic-heavy approaches may suffer seriously from grammatical errors, which are unavoidable in MT output. Hence their accuracy and reliability inevitably fluctuate in accord with different evaluation data. Lexical cohesion has far been neglected in both MT and MT evaluation, even though it is the single most important form of cohesion devices, accounting for nearly half of the cohesion devices in English (Halliday and Hasan, 1976). It is also a significant feature contributing to tra</context>
</contexts>
<marker>Burstein, 2003</marker>
<rawString>Jill Burstein. 2003. The E-rater scoring engine: Automated essay scoring with natural language processing. In Mark D. Shermis and Jill Burstein, editors, Automated Essay Scoring: A Cross-Disciplinary Perspective, chapter 7, pages 113–122. Lawrence Erlbaum Associates.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marine Carpuat</author>
</authors>
<title>One translation per discourse.</title>
<date>2009</date>
<booktitle>In Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions,</booktitle>
<pages>pages</pages>
<location>Boulder, Colorado.</location>
<contexts>
<context position="6486" citStr="Carpuat (2009)" startWordPosition="1018" endWordPosition="1019">ical MT. A phase of its work is to have grammatical devices, 1As cited in van Slype (1979). such as verbal tense/aspect/mode, discourse connectives and pronouns, manually annotated in multilingual corpora, in hopes of laying a foundation for the development of automatic labelers for them that can be integrated into an MT model. For lexical cohesion, it has been only partially and indirectly addressed in terms of translation consistency in MT output. Different approaches to maintaining consistency in target word choices are proposed (Itagaki et al., 2007; Gong et al., 2011; Xiao et al., 2011). Carpuat (2009) also observes a general tendency in human translation that a given sense is usually lexicalized in a consistent manner throughout the whole translation. Nevertheless there are only a few evaluation methods explicitly targeting on the quality of a document. Miller and Vanni (2001) devise a human evaluation approach to measure the comprehensibility of a text as a whole, based on the Rhetorical Structure Theory (Mann and Thompson, 1988), a theory of text organization specifying coherence relations in an authentic text. Snover et al. (2006) proposes HTER to assess post-editing effort through huma</context>
</contexts>
<marker>Carpuat, 2009</marker>
<rawString>Marine Carpuat. 2009. One translation per discourse. In Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 19–27, Boulder, Colorado.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bruno Cartoni</author>
<author>Andrea Gesmundo</author>
<author>James Henderson</author>
<author>Cristina Grisot</author>
<author>Paola Merlo</author>
<author>Thomas Meyer</author>
<author>Jacques Moeschler</author>
<author>Sandrine Zufferey</author>
<author>Andrei PopescuBelis</author>
</authors>
<title>Improving MT coherence through textlevel processing of input texts: The COMTIS project. In Tralogy,</title>
<date>2011</date>
<location>Paris.</location>
<contexts>
<context position="5813" citStr="Cartoni et al., 2011" startWordPosition="907" endWordPosition="910">akaiwa and Shirai (1996), present approaches to resolving Japanese zero pronouns and to integrating them into a Japanese-English transferred-based MT system. Peral et al. (1999) propose an interlingual mechanism for pronominal anaphora generation by exploiting a rich set of lexical, syntactic, morphologic and semantic information. Murata and Nagao (1993) and Murata et al. (2001) develop a rule base to identify the referential properties of Japanese noun phrases, so as to facilitate anaphora resolution for Japanese and article generation for English during translation. A recent COMTIS project (Cartoni et al., 2011) begins to exploit inter-sentential information for statistical MT. A phase of its work is to have grammatical devices, 1As cited in van Slype (1979). such as verbal tense/aspect/mode, discourse connectives and pronouns, manually annotated in multilingual corpora, in hopes of laying a foundation for the development of automatic labelers for them that can be integrated into an MT model. For lexical cohesion, it has been only partially and indirectly addressed in terms of translation consistency in MT output. Different approaches to maintaining consistency in target word choices are proposed (It</context>
</contexts>
<marker>Cartoni, Gesmundo, Henderson, Grisot, Merlo, Meyer, Moeschler, Zufferey, PopescuBelis, 2011</marker>
<rawString>Bruno Cartoni, Andrea Gesmundo, James Henderson, Cristina Grisot, Paola Merlo, Thomas Meyer, Jacques Moeschler, Sandrine Zufferey, and Andrei PopescuBelis. 2011. Improving MT coherence through textlevel processing of input texts: The COMTIS project. In Tralogy, Paris.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Samuel W K Chan</author>
</authors>
<title>Extraction of sailent textual patterns: Synergy between lexical cohesion and contextual coherence.</title>
<date>2004</date>
<journal>IEEE Transactions on Systems, Man and Cybernetics, Part A: Systems and Humans,</journal>
<volume>34</volume>
<issue>2</issue>
<contexts>
<context position="8520" citStr="Chan (2004)" startWordPosition="1336" endWordPosition="1337">cohesion has far been neglected in both MT and MT evaluation, even though it is the single most important form of cohesion devices, accounting for nearly half of the cohesion devices in English (Halliday and Hasan, 1976). It is also a significant feature contributing to translation equivalence of texts by preserving their texture (Lotfipour-Saedi, 1997). The lexical cohesion devices in a text can be 1061 represented as lexical chains conjoining related entities. There are many methods of computing lexical chains for various purposes, e.g., Morris and Hirst (1991), Barzilay and Elhadad (1997), Chan (2004), Li et al. (2007), among many others. Contrary to grammatical cohesion highly depending on syntactic well-formedness of a text, lexical cohesion is less affected by grammatical errors. Its computation has to rely on a thesaurus, which is usually available for almost every language. In this research, a number of formulations of lexical cohesion, with or without reliance on external language resource, will be explored for the purpose of MT evaluation. 3 Lexical Cohesion in Machine and Human Translation This section presents a comparative study of MT and human translation (HT) in terms of the us</context>
</contexts>
<marker>Chan, 2004</marker>
<rawString>Samuel W. K. Chan. 2004. Extraction of sailent textual patterns: Synergy between lexical cohesion and contextual coherence. IEEE Transactions on Systems, Man and Cybernetics, Part A: Systems and Humans, 34(2):205–218.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elisabet Comelles</author>
<author>Jesus Gim´enez</author>
<author>Lluis M´arquez</author>
<author>Irene Castell`on</author>
<author>Victoria Arranz</author>
</authors>
<title>Documentlevel automatic MT evaluation based on discourse representations.</title>
<date>2010</date>
<booktitle>In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR,</booktitle>
<pages>333--338</pages>
<location>Uppsala.</location>
<marker>Comelles, Gim´enez, M´arquez, Castell`on, Arranz, 2010</marker>
<rawString>Elisabet Comelles, Jesus Gim´enez, Lluis M´arquez, Irene Castell`on, and Victoria Arranz. 2010. Documentlevel automatic MT evaluation based on discourse representations. In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR, pages 333–338, Uppsala.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="11346" citStr="Fellbaum, 1998" startWordPosition="1798" endWordPosition="1799">se lexical items that share the same or similar semantic relations, including complementarity, antonym, converse, coordinate term, meronym, troponym, and so on. In this study, lexical cohesion devices are defined as content words (i.e., tokens after stopword having been removed) that reiterate once or more times in a document, including synonym, near-synonym and superordinate, besides those repetition and collocation. Repetition refers to the same words or stems in a document. Stems are identified with the aid of Porter stemmer (1980). To classify the semantic relationships of words, WordNet (Fellbaum, 1998) is used as a lexical resource, which clusters words of the same sense (i.e., synonyms) into a semantic group, namely a synset. Synsets are interlinked in WordNet according to their semantic relationships. Superordinate and collocation are formed by words in a proximate semantic relationship, such as bicycle and vehicle (hypernym), bicycle and wheel (meronym), bicycle and car (coordinate term), and so on. They are defined as synset pairs with a distance of 1 in WordNet. The measure of semantic distance (Wu and Palmer, 1994) is also applied to identify near-synonyms, i.e., words that are synony</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Christiane Fellbaum. 1998. WordNet: An Electronic Lexical Database. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhengxian Gong</author>
<author>Min Zhang</author>
<author>Guodong Zhou</author>
</authors>
<title>Cache-based document-level statistical machine translation.</title>
<date>2011</date>
<booktitle>In EMNLP 2011,</booktitle>
<pages>909--919</pages>
<location>Edinburgh, Scotland.</location>
<contexts>
<context position="6450" citStr="Gong et al., 2011" startWordPosition="1010" endWordPosition="1013">nter-sentential information for statistical MT. A phase of its work is to have grammatical devices, 1As cited in van Slype (1979). such as verbal tense/aspect/mode, discourse connectives and pronouns, manually annotated in multilingual corpora, in hopes of laying a foundation for the development of automatic labelers for them that can be integrated into an MT model. For lexical cohesion, it has been only partially and indirectly addressed in terms of translation consistency in MT output. Different approaches to maintaining consistency in target word choices are proposed (Itagaki et al., 2007; Gong et al., 2011; Xiao et al., 2011). Carpuat (2009) also observes a general tendency in human translation that a given sense is usually lexicalized in a consistent manner throughout the whole translation. Nevertheless there are only a few evaluation methods explicitly targeting on the quality of a document. Miller and Vanni (2001) devise a human evaluation approach to measure the comprehensibility of a text as a whole, based on the Rhetorical Structure Theory (Mann and Thompson, 1988), a theory of text organization specifying coherence relations in an authentic text. Snover et al. (2006) proposes HTER to ass</context>
</contexts>
<marker>Gong, Zhang, Zhou, 2011</marker>
<rawString>Zhengxian Gong, Min Zhang, and Guodong Zhou. 2011. Cache-based document-level statistical machine translation. In EMNLP 2011, pages 909–919, Edinburgh, Scotland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A K Halliday</author>
<author>Ruqaiya Hasan</author>
</authors>
<date>1976</date>
<booktitle>Cohesion in English.</booktitle>
<publisher>Longman.</publisher>
<location>London:</location>
<contexts>
<context position="3393" citStr="Halliday and Hasan, 1976" startWordPosition="532" endWordPosition="535">inter-sentential linguistic features of cohesion and coherence and presents plausible ways to incorporate them into the sentence-based metrics to support MT evaluation at the document level. In the Framework for MT Evaluation in the International Standards of Language Engineering (FEMTI) (King et al., 2003), coherence is defined as “the degree to which the reader can describe the role of each individual sentence (or group of sentences) with respect to the text as a whole”. The measurement of coherence has to rely on cohesion, referring to the “relations of meaning that exist within the text” (Halliday and Hasan, 1976). Cohesion is realized via the interlinkage of grammatical and lexical elements across sentences. Grammatical 1060 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1060–1068, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics cohesion refers to the syntactic links between text items, while lexical cohesion is achieved through the word choices in a text. This paper focuses on the latter. A quantitative comparison of lexical cohesion devices between MT output and huma</context>
<context position="8129" citStr="Halliday and Hasan, 1976" startWordPosition="1275" endWordPosition="1278">T evaluation, automated essay scoring programs such as E-rater (Burstein, 2003) also employ a rich set of discourse features for assessment. However, the parsing process needed for these linguistic-heavy approaches may suffer seriously from grammatical errors, which are unavoidable in MT output. Hence their accuracy and reliability inevitably fluctuate in accord with different evaluation data. Lexical cohesion has far been neglected in both MT and MT evaluation, even though it is the single most important form of cohesion devices, accounting for nearly half of the cohesion devices in English (Halliday and Hasan, 1976). It is also a significant feature contributing to translation equivalence of texts by preserving their texture (Lotfipour-Saedi, 1997). The lexical cohesion devices in a text can be 1061 represented as lexical chains conjoining related entities. There are many methods of computing lexical chains for various purposes, e.g., Morris and Hirst (1991), Barzilay and Elhadad (1997), Chan (2004), Li et al. (2007), among many others. Contrary to grammatical cohesion highly depending on syntactic well-formedness of a text, lexical cohesion is less affected by grammatical errors. Its computation has to </context>
</contexts>
<marker>Halliday, Hasan, 1976</marker>
<rawString>M. A. K. Halliday and Ruqaiya Hasan. 1976. Cohesion in English. London: Longman.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masaki Itagaki</author>
</authors>
<title>Takako Aikawa, and Xiaodong He.</title>
<date>2007</date>
<booktitle>In MT Summit XI,</booktitle>
<pages>269--274</pages>
<marker>Itagaki, 2007</marker>
<rawString>Masaki Itagaki, Takako Aikawa, and Xiaodong He. 2007. Automatic validation of terminology translation consistency with statistical method. In MT Summit XI, pages 269–274.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hans Kamp</author>
<author>Uwe Reyle</author>
</authors>
<title>From Discourse to Logic: An Introduction to Modeltheoretic Semantics of Natural Language, Formal Logic and Discourse Representation Theory.</title>
<date>1993</date>
<publisher>Kluwer.</publisher>
<location>Dordrecht:</location>
<contexts>
<context position="7342" citStr="Kamp and Reyle, 1993" startWordPosition="1150" endWordPosition="1153">quality of a document. Miller and Vanni (2001) devise a human evaluation approach to measure the comprehensibility of a text as a whole, based on the Rhetorical Structure Theory (Mann and Thompson, 1988), a theory of text organization specifying coherence relations in an authentic text. Snover et al. (2006) proposes HTER to assess post-editing effort through human annotation. Its automatic versions TER and TERp (Snover et al., 2009), however, remain sentencebased metrics. Comelles et al. (2010) present a family of automatic MT evaluation measures, based on the Discourse Representation Theory (Kamp and Reyle, 1993), that generate semantic trees to put together different text entities for the same referent according to their contexts and grammatical connections. Apart from MT evaluation, automated essay scoring programs such as E-rater (Burstein, 2003) also employ a rich set of discourse features for assessment. However, the parsing process needed for these linguistic-heavy approaches may suffer seriously from grammatical errors, which are unavoidable in MT output. Hence their accuracy and reliability inevitably fluctuate in accord with different evaluation data. Lexical cohesion has far been neglected i</context>
</contexts>
<marker>Kamp, Reyle, 1993</marker>
<rawString>Hans Kamp and Uwe Reyle. 1993. From Discourse to Logic: An Introduction to Modeltheoretic Semantics of Natural Language, Formal Logic and Discourse Representation Theory. Dordrecht: Kluwer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Margaret King</author>
<author>Andrei Popescu-Belis</author>
<author>Eduard Hovy</author>
</authors>
<title>FEMTI: Creating and using a framework for MT evaluation.</title>
<date>2003</date>
<booktitle>In MT Summit IX,</booktitle>
<pages>224--231</pages>
<location>New Orleans.</location>
<contexts>
<context position="3076" citStr="King et al., 2003" startWordPosition="476" endWordPosition="479">rectness of each sentence (Visser and Fuji, 1996). Post-editors particularly need to ensure the quality of a whole document of MT output when revising its sentences. The connectivity of sentences is surely a significant factor contributing to the understandability of a text as a whole. This paper studies the inter-sentential linguistic features of cohesion and coherence and presents plausible ways to incorporate them into the sentence-based metrics to support MT evaluation at the document level. In the Framework for MT Evaluation in the International Standards of Language Engineering (FEMTI) (King et al., 2003), coherence is defined as “the degree to which the reader can describe the role of each individual sentence (or group of sentences) with respect to the text as a whole”. The measurement of coherence has to rely on cohesion, referring to the “relations of meaning that exist within the text” (Halliday and Hasan, 1976). Cohesion is realized via the interlinkage of grammatical and lexical elements across sentences. Grammatical 1060 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1060–1068, Jeju Island, </context>
</contexts>
<marker>King, Popescu-Belis, Hovy, 2003</marker>
<rawString>Margaret King, Andrei Popescu-Belis, and Eduard Hovy. 2003. FEMTI: Creating and using a framework for MT evaluation. In MT Summit IX, pages 224–231, New Orleans.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jing Li</author>
<author>Chunyu Kit Le Sun</author>
<author>Jonathan Webster</author>
</authors>
<title>A query-focused multi-document summarizer based on lexical chains.</title>
<date>2007</date>
<booktitle>In DUC 2007,</booktitle>
<location>Rochester, New York.</location>
<marker>Li, Le Sun, Webster, 2007</marker>
<rawString>Jing Li, Le Sun, Chunyu Kit, and Jonathan Webster. 2007. A query-focused multi-document summarizer based on lexical chains. In DUC 2007, Rochester, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kazem Lotfipour-Saedi</author>
</authors>
<title>Lexical cohesion and translation equivalence.</title>
<date>1997</date>
<journal>Meta,</journal>
<volume>42</volume>
<issue>1</issue>
<contexts>
<context position="8264" citStr="Lotfipour-Saedi, 1997" startWordPosition="1296" endWordPosition="1297">t. However, the parsing process needed for these linguistic-heavy approaches may suffer seriously from grammatical errors, which are unavoidable in MT output. Hence their accuracy and reliability inevitably fluctuate in accord with different evaluation data. Lexical cohesion has far been neglected in both MT and MT evaluation, even though it is the single most important form of cohesion devices, accounting for nearly half of the cohesion devices in English (Halliday and Hasan, 1976). It is also a significant feature contributing to translation equivalence of texts by preserving their texture (Lotfipour-Saedi, 1997). The lexical cohesion devices in a text can be 1061 represented as lexical chains conjoining related entities. There are many methods of computing lexical chains for various purposes, e.g., Morris and Hirst (1991), Barzilay and Elhadad (1997), Chan (2004), Li et al. (2007), among many others. Contrary to grammatical cohesion highly depending on syntactic well-formedness of a text, lexical cohesion is less affected by grammatical errors. Its computation has to rely on a thesaurus, which is usually available for almost every language. In this research, a number of formulations of lexical cohesi</context>
</contexts>
<marker>Lotfipour-Saedi, 1997</marker>
<rawString>Kazem Lotfipour-Saedi. 1997. Lexical cohesion and translation equivalence. Meta, 42(1):185–192.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaoyi Ma</author>
</authors>
<title>Multiple-Translation Chinese (MTC) part 4. Linguistic Data Consortium.</title>
<date>2006</date>
<contexts>
<context position="9638" citStr="Ma, 2006" startWordPosition="1522" endWordPosition="1523">is section presents a comparative study of MT and human translation (HT) in terms of the use of lexical cohesion devices. It is an intuition that more cohesion devices are used by humans than machines in translation, as part of the superior quality of HT. Two different datasets are used to ensure the reliability and generality of the comparison. The results confirm the incapability of MT in handling this feature and the necessity of using lexical cohesion in MT evaluation. 3.1 Data The MetricsMATR 2008 development set (Przybocki et al., 2009) and the Multiple-Translation Chinese (MTC) part 4 (Ma, 2006) are used for this study. They consist of MT outputs of different source languages in company with reference translations. The data of MetricsMATR is selected from the NIST Open MT 2006 evaluation, while MTC4 is from the TIDES 2003 MT evaluation. Both datasets include human assessments of MT output, from which the part of adequacy assessment is selected for this study. Table 1 provides overall statistics of the datasets. 3.2 Identification of Lexical Cohesion Devices Lexical cohesion is achieved through word choices of two major types: reiteration and collocation. Reiteration can be realized i</context>
</contexts>
<marker>Ma, 2006</marker>
<rawString>Xiaoyi Ma. 2006. Multiple-Translation Chinese (MTC) part 4. Linguistic Data Consortium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William C Mann</author>
<author>Sandra A Thompson</author>
</authors>
<title>Rhetorical structure theory: Toward a functional theory of text organization.</title>
<date>1988</date>
<tech>Text, 8(3):243–281.</tech>
<contexts>
<context position="6924" citStr="Mann and Thompson, 1988" startWordPosition="1088" endWordPosition="1091">consistency in MT output. Different approaches to maintaining consistency in target word choices are proposed (Itagaki et al., 2007; Gong et al., 2011; Xiao et al., 2011). Carpuat (2009) also observes a general tendency in human translation that a given sense is usually lexicalized in a consistent manner throughout the whole translation. Nevertheless there are only a few evaluation methods explicitly targeting on the quality of a document. Miller and Vanni (2001) devise a human evaluation approach to measure the comprehensibility of a text as a whole, based on the Rhetorical Structure Theory (Mann and Thompson, 1988), a theory of text organization specifying coherence relations in an authentic text. Snover et al. (2006) proposes HTER to assess post-editing effort through human annotation. Its automatic versions TER and TERp (Snover et al., 2009), however, remain sentencebased metrics. Comelles et al. (2010) present a family of automatic MT evaluation measures, based on the Discourse Representation Theory (Kamp and Reyle, 1993), that generate semantic trees to put together different text entities for the same referent according to their contexts and grammatical connections. Apart from MT evaluation, automa</context>
</contexts>
<marker>Mann, Thompson, 1988</marker>
<rawString>William C. Mann and Sandra A. Thompson. 1988. Rhetorical structure theory: Toward a functional theory of text organization. Text, 8(3):243–281.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Keith J Miller</author>
<author>Michelle Vanni</author>
</authors>
<title>Scaling the ISLE taxonomy: Development of metrics for the multi-dimensional characterisation of machine translation quality.</title>
<date>2001</date>
<booktitle>In MT Summit VIII,</booktitle>
<pages>229--238</pages>
<contexts>
<context position="6767" citStr="Miller and Vanni (2001)" startWordPosition="1061" endWordPosition="1064">abelers for them that can be integrated into an MT model. For lexical cohesion, it has been only partially and indirectly addressed in terms of translation consistency in MT output. Different approaches to maintaining consistency in target word choices are proposed (Itagaki et al., 2007; Gong et al., 2011; Xiao et al., 2011). Carpuat (2009) also observes a general tendency in human translation that a given sense is usually lexicalized in a consistent manner throughout the whole translation. Nevertheless there are only a few evaluation methods explicitly targeting on the quality of a document. Miller and Vanni (2001) devise a human evaluation approach to measure the comprehensibility of a text as a whole, based on the Rhetorical Structure Theory (Mann and Thompson, 1988), a theory of text organization specifying coherence relations in an authentic text. Snover et al. (2006) proposes HTER to assess post-editing effort through human annotation. Its automatic versions TER and TERp (Snover et al., 2009), however, remain sentencebased metrics. Comelles et al. (2010) present a family of automatic MT evaluation measures, based on the Discourse Representation Theory (Kamp and Reyle, 1993), that generate semantic </context>
</contexts>
<marker>Miller, Vanni, 2001</marker>
<rawString>Keith J. Miller and Michelle Vanni. 2001. Scaling the ISLE taxonomy: Development of metrics for the multi-dimensional characterisation of machine translation quality. In MT Summit VIII, pages 229–238.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jane Morris</author>
<author>Graeme Hirst</author>
</authors>
<title>Lexical cohesion computed by thesaural relations as an indicator of the structure of text.</title>
<date>1991</date>
<journal>Computational Linguistics,</journal>
<volume>17</volume>
<issue>1</issue>
<contexts>
<context position="8478" citStr="Morris and Hirst (1991)" startWordPosition="1328" endWordPosition="1331">ate in accord with different evaluation data. Lexical cohesion has far been neglected in both MT and MT evaluation, even though it is the single most important form of cohesion devices, accounting for nearly half of the cohesion devices in English (Halliday and Hasan, 1976). It is also a significant feature contributing to translation equivalence of texts by preserving their texture (Lotfipour-Saedi, 1997). The lexical cohesion devices in a text can be 1061 represented as lexical chains conjoining related entities. There are many methods of computing lexical chains for various purposes, e.g., Morris and Hirst (1991), Barzilay and Elhadad (1997), Chan (2004), Li et al. (2007), among many others. Contrary to grammatical cohesion highly depending on syntactic well-formedness of a text, lexical cohesion is less affected by grammatical errors. Its computation has to rely on a thesaurus, which is usually available for almost every language. In this research, a number of formulations of lexical cohesion, with or without reliance on external language resource, will be explored for the purpose of MT evaluation. 3 Lexical Cohesion in Machine and Human Translation This section presents a comparative study of MT and</context>
</contexts>
<marker>Morris, Hirst, 1991</marker>
<rawString>Jane Morris and Graeme Hirst. 1991. Lexical cohesion computed by thesaural relations as an indicator of the structure of text. Computational Linguistics, 17(1):21–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masaki Murata</author>
<author>Makoto Nagao</author>
</authors>
<title>Determination of referential property and number of nouns in Japanese sentences for machine translation into English. In TMI</title>
<date>1993</date>
<pages>218--225</pages>
<contexts>
<context position="5548" citStr="Murata and Nagao (1993)" startWordPosition="862" endWordPosition="866">appropriate to serve as criteria for the overall quality of MT output. Previous researches in MT predominantly focus on specific types of cohesion devices. For grammatical cohesion, a series of works, including Nakaiwa and Ikehara (1992), Nakaiwa et al. (1995), and Nakaiwa and Shirai (1996), present approaches to resolving Japanese zero pronouns and to integrating them into a Japanese-English transferred-based MT system. Peral et al. (1999) propose an interlingual mechanism for pronominal anaphora generation by exploiting a rich set of lexical, syntactic, morphologic and semantic information. Murata and Nagao (1993) and Murata et al. (2001) develop a rule base to identify the referential properties of Japanese noun phrases, so as to facilitate anaphora resolution for Japanese and article generation for English during translation. A recent COMTIS project (Cartoni et al., 2011) begins to exploit inter-sentential information for statistical MT. A phase of its work is to have grammatical devices, 1As cited in van Slype (1979). such as verbal tense/aspect/mode, discourse connectives and pronouns, manually annotated in multilingual corpora, in hopes of laying a foundation for the development of automatic label</context>
</contexts>
<marker>Murata, Nagao, 1993</marker>
<rawString>Masaki Murata and Makoto Nagao. 1993. Determination of referential property and number of nouns in Japanese sentences for machine translation into English. In TMI 1993, pages 218–225, Kyoto.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masaki Murata</author>
<author>Kiyotaka Uchimoto</author>
<author>Qing Ma</author>
<author>Hitoshi Isahara</author>
</authors>
<title>A machine-learning approach to estimating the referential properties of Japanese noun phrases. In CICLING</title>
<date>2001</date>
<pages>142--153</pages>
<contexts>
<context position="5573" citStr="Murata et al. (2001)" startWordPosition="868" endWordPosition="871">eria for the overall quality of MT output. Previous researches in MT predominantly focus on specific types of cohesion devices. For grammatical cohesion, a series of works, including Nakaiwa and Ikehara (1992), Nakaiwa et al. (1995), and Nakaiwa and Shirai (1996), present approaches to resolving Japanese zero pronouns and to integrating them into a Japanese-English transferred-based MT system. Peral et al. (1999) propose an interlingual mechanism for pronominal anaphora generation by exploiting a rich set of lexical, syntactic, morphologic and semantic information. Murata and Nagao (1993) and Murata et al. (2001) develop a rule base to identify the referential properties of Japanese noun phrases, so as to facilitate anaphora resolution for Japanese and article generation for English during translation. A recent COMTIS project (Cartoni et al., 2011) begins to exploit inter-sentential information for statistical MT. A phase of its work is to have grammatical devices, 1As cited in van Slype (1979). such as verbal tense/aspect/mode, discourse connectives and pronouns, manually annotated in multilingual corpora, in hopes of laying a foundation for the development of automatic labelers for them that can be </context>
</contexts>
<marker>Murata, Uchimoto, Ma, Isahara, 2001</marker>
<rawString>Masaki Murata, Kiyotaka Uchimoto, Qing Ma, and Hitoshi Isahara. 2001. A machine-learning approach to estimating the referential properties of Japanese noun phrases. In CICLING 2001, pages 142–153, MexicoCity.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiromi Nakaiwa</author>
<author>Satoru Ikehara</author>
</authors>
<title>Zero pronoun resolution in a machine translation system by using Japanese to English verbal semantic attributes. In ANLP</title>
<date>1992</date>
<pages>201--208</pages>
<contexts>
<context position="5162" citStr="Nakaiwa and Ikehara (1992)" startWordPosition="804" endWordPosition="807">er level quality criteria beyond many others such as syntactic well-formedness. Post-editors tend to correct syntactic errors first before any amendment for improving the cohesion and coherence of an MT output. Also, as Wilks (1978)1 noted, it is rather unlikely for a sufficiently large sample of translations to be coherent and totally wrong at the same time. Cohesion and coherence are appropriate to serve as criteria for the overall quality of MT output. Previous researches in MT predominantly focus on specific types of cohesion devices. For grammatical cohesion, a series of works, including Nakaiwa and Ikehara (1992), Nakaiwa et al. (1995), and Nakaiwa and Shirai (1996), present approaches to resolving Japanese zero pronouns and to integrating them into a Japanese-English transferred-based MT system. Peral et al. (1999) propose an interlingual mechanism for pronominal anaphora generation by exploiting a rich set of lexical, syntactic, morphologic and semantic information. Murata and Nagao (1993) and Murata et al. (2001) develop a rule base to identify the referential properties of Japanese noun phrases, so as to facilitate anaphora resolution for Japanese and article generation for English during translat</context>
</contexts>
<marker>Nakaiwa, Ikehara, 1992</marker>
<rawString>Hiromi Nakaiwa and Satoru Ikehara. 1992. Zero pronoun resolution in a machine translation system by using Japanese to English verbal semantic attributes. In ANLP 1992, pages 201–208.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiromi Nakaiwa</author>
<author>Satoshi Shirai</author>
</authors>
<title>Anaphora resolution of Japanese zero pronouns with deictic reference.</title>
<date>1996</date>
<booktitle>In COLING</booktitle>
<pages>812--817</pages>
<location>Copenhagen.</location>
<contexts>
<context position="5216" citStr="Nakaiwa and Shirai (1996)" startWordPosition="813" endWordPosition="816">ntactic well-formedness. Post-editors tend to correct syntactic errors first before any amendment for improving the cohesion and coherence of an MT output. Also, as Wilks (1978)1 noted, it is rather unlikely for a sufficiently large sample of translations to be coherent and totally wrong at the same time. Cohesion and coherence are appropriate to serve as criteria for the overall quality of MT output. Previous researches in MT predominantly focus on specific types of cohesion devices. For grammatical cohesion, a series of works, including Nakaiwa and Ikehara (1992), Nakaiwa et al. (1995), and Nakaiwa and Shirai (1996), present approaches to resolving Japanese zero pronouns and to integrating them into a Japanese-English transferred-based MT system. Peral et al. (1999) propose an interlingual mechanism for pronominal anaphora generation by exploiting a rich set of lexical, syntactic, morphologic and semantic information. Murata and Nagao (1993) and Murata et al. (2001) develop a rule base to identify the referential properties of Japanese noun phrases, so as to facilitate anaphora resolution for Japanese and article generation for English during translation. A recent COMTIS project (Cartoni et al., 2011) be</context>
</contexts>
<marker>Nakaiwa, Shirai, 1996</marker>
<rawString>Hiromi Nakaiwa and Satoshi Shirai. 1996. Anaphora resolution of Japanese zero pronouns with deictic reference. In COLING 1996, pages 812–817, Copenhagen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiromi Nakaiwa</author>
<author>Satoshi Shirai</author>
<author>Satoru Ikehara</author>
<author>Tsukasa Kawaok</author>
</authors>
<title>Extrasentential resolution of Japanese zero pronouns using semantic and pragmatic constraints.</title>
<date>1995</date>
<booktitle>In Proceedings of the AAAI 1995 Spring Symposium Series: Empirical Methods in Discourse Interpretation and Generation,</booktitle>
<pages>99--105</pages>
<location>Stanford.</location>
<contexts>
<context position="5185" citStr="Nakaiwa et al. (1995)" startWordPosition="808" endWordPosition="811">yond many others such as syntactic well-formedness. Post-editors tend to correct syntactic errors first before any amendment for improving the cohesion and coherence of an MT output. Also, as Wilks (1978)1 noted, it is rather unlikely for a sufficiently large sample of translations to be coherent and totally wrong at the same time. Cohesion and coherence are appropriate to serve as criteria for the overall quality of MT output. Previous researches in MT predominantly focus on specific types of cohesion devices. For grammatical cohesion, a series of works, including Nakaiwa and Ikehara (1992), Nakaiwa et al. (1995), and Nakaiwa and Shirai (1996), present approaches to resolving Japanese zero pronouns and to integrating them into a Japanese-English transferred-based MT system. Peral et al. (1999) propose an interlingual mechanism for pronominal anaphora generation by exploiting a rich set of lexical, syntactic, morphologic and semantic information. Murata and Nagao (1993) and Murata et al. (2001) develop a rule base to identify the referential properties of Japanese noun phrases, so as to facilitate anaphora resolution for Japanese and article generation for English during translation. A recent COMTIS pr</context>
</contexts>
<marker>Nakaiwa, Shirai, Ikehara, Kawaok, 1995</marker>
<rawString>Hiromi Nakaiwa, Satoshi Shirai, Satoru Ikehara, and Tsukasa Kawaok. 1995. Extrasentential resolution of Japanese zero pronouns using semantic and pragmatic constraints. In Proceedings of the AAAI 1995 Spring Symposium Series: Empirical Methods in Discourse Interpretation and Generation, pages 99–105, Stanford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: A method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In ACL</booktitle>
<pages>311--318</pages>
<contexts>
<context position="1585" citStr="Papineni et al., 2002" startWordPosition="235" endWordPosition="239">incorporating this feature into sentence-level evaluation metrics can enhance their correlation with human judgements. 1 Introduction Machine translation (MT) has benefited a lot from the advancement of automatic evaluation in the past decade. To a certain degree, its progress is also confined to the limitations of evaluation metrics in use. Most efforts devoted to evaluate the quality of MT output so far have still focused on the sentence level without sufficient attention to how a larger text is structured. This is notably reflected in the representative MT evaluation metrics, such as BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005) and TER (Snover et al., 2006), that adopt a sentence-by-sentence fashion to score MT outputs. The evaluation result for a document by any of them is usually a simple average of its sentence scores. A drawback of this kind of sentence-based evaluation is the neglect of document structure. There is no guarantee for the coherence of a text if it is produced by simply putting together stand-alone sentences, no matter how well-translated, without adequate intersentential connection. As a consequence, MT system optimized this way to any of these metrics can only h</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: A method for automatic evaluation of machine translation. In ACL 2002, pages 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jes´us Peral</author>
<author>Manuel Palomar</author>
<author>Antonio Ferr`andez</author>
</authors>
<title>Coreference-oriented interlingual slot structure and machine translation.</title>
<date>1999</date>
<booktitle>In Proceedings of the ACL Workshop on Coreference and its Applications,</booktitle>
<pages>69--76</pages>
<location>College Park, MD.</location>
<marker>Peral, Palomar, Ferr`andez, 1999</marker>
<rawString>Jes´us Peral, Manuel Palomar, and Antonio Ferr`andez. 1999. Coreference-oriented interlingual slot structure and machine translation. In Proceedings of the ACL Workshop on Coreference and its Applications, pages 69–76, College Park, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin F Porter</author>
</authors>
<title>An algorithm for suffix stripping.</title>
<date>1980</date>
<journal>Program,</journal>
<volume>14</volume>
<issue>3</issue>
<marker>Porter, 1980</marker>
<rawString>Martin F. Porter. 1980. An algorithm for suffix stripping. Program, 14(3):130–137.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Przybocki</author>
<author>Kay Peterson</author>
<author>S´ebastien Bronsart</author>
</authors>
<title>NIST metrics for machine translation (MetricsMATR08) development data. Linguistic Data Consortium.</title>
<date>2009</date>
<contexts>
<context position="9577" citStr="Przybocki et al., 2009" startWordPosition="1509" endWordPosition="1513">se of MT evaluation. 3 Lexical Cohesion in Machine and Human Translation This section presents a comparative study of MT and human translation (HT) in terms of the use of lexical cohesion devices. It is an intuition that more cohesion devices are used by humans than machines in translation, as part of the superior quality of HT. Two different datasets are used to ensure the reliability and generality of the comparison. The results confirm the incapability of MT in handling this feature and the necessity of using lexical cohesion in MT evaluation. 3.1 Data The MetricsMATR 2008 development set (Przybocki et al., 2009) and the Multiple-Translation Chinese (MTC) part 4 (Ma, 2006) are used for this study. They consist of MT outputs of different source languages in company with reference translations. The data of MetricsMATR is selected from the NIST Open MT 2006 evaluation, while MTC4 is from the TIDES 2003 MT evaluation. Both datasets include human assessments of MT output, from which the part of adequacy assessment is selected for this study. Table 1 provides overall statistics of the datasets. 3.2 Identification of Lexical Cohesion Devices Lexical cohesion is achieved through word choices of two major type</context>
</contexts>
<marker>Przybocki, Peterson, Bronsart, 2009</marker>
<rawString>Mark Przybocki, Kay Peterson, and S´ebastien Bronsart. 2009. 2008 NIST metrics for machine translation (MetricsMATR08) development data. Linguistic Data Consortium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
<author>Linnea Micciulla</author>
<author>John Makhoul</author>
</authors>
<title>A study of translation edit rate with targeted human annotation. In AMTA</title>
<date>2006</date>
<pages>223--231</pages>
<contexts>
<context position="1650" citStr="Snover et al., 2006" startWordPosition="247" endWordPosition="250">an enhance their correlation with human judgements. 1 Introduction Machine translation (MT) has benefited a lot from the advancement of automatic evaluation in the past decade. To a certain degree, its progress is also confined to the limitations of evaluation metrics in use. Most efforts devoted to evaluate the quality of MT output so far have still focused on the sentence level without sufficient attention to how a larger text is structured. This is notably reflected in the representative MT evaluation metrics, such as BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005) and TER (Snover et al., 2006), that adopt a sentence-by-sentence fashion to score MT outputs. The evaluation result for a document by any of them is usually a simple average of its sentence scores. A drawback of this kind of sentence-based evaluation is the neglect of document structure. There is no guarantee for the coherence of a text if it is produced by simply putting together stand-alone sentences, no matter how well-translated, without adequate intersentential connection. As a consequence, MT system optimized this way to any of these metrics can only have a very dim chance of producing translated document that reads</context>
<context position="7029" citStr="Snover et al. (2006)" startWordPosition="1104" endWordPosition="1107"> (Itagaki et al., 2007; Gong et al., 2011; Xiao et al., 2011). Carpuat (2009) also observes a general tendency in human translation that a given sense is usually lexicalized in a consistent manner throughout the whole translation. Nevertheless there are only a few evaluation methods explicitly targeting on the quality of a document. Miller and Vanni (2001) devise a human evaluation approach to measure the comprehensibility of a text as a whole, based on the Rhetorical Structure Theory (Mann and Thompson, 1988), a theory of text organization specifying coherence relations in an authentic text. Snover et al. (2006) proposes HTER to assess post-editing effort through human annotation. Its automatic versions TER and TERp (Snover et al., 2009), however, remain sentencebased metrics. Comelles et al. (2010) present a family of automatic MT evaluation measures, based on the Discourse Representation Theory (Kamp and Reyle, 1993), that generate semantic trees to put together different text entities for the same referent according to their contexts and grammatical connections. Apart from MT evaluation, automated essay scoring programs such as E-rater (Burstein, 2003) also employ a rich set of discourse features </context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A study of translation edit rate with targeted human annotation. In AMTA 2006, pages 223–231.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Nitin Madnani</author>
<author>Bonnie J Dorr</author>
<author>Richard Schwartz</author>
</authors>
<title>Fluency, adequacy, or HTER? Exploring different human judgments with a tunable MT metric.</title>
<date>2009</date>
<booktitle>In Proceedings of the 4th Workshop on Statistical Machine Translation,</booktitle>
<pages>259--268</pages>
<location>Athens.</location>
<contexts>
<context position="7157" citStr="Snover et al., 2009" startWordPosition="1123" endWordPosition="1126">ion that a given sense is usually lexicalized in a consistent manner throughout the whole translation. Nevertheless there are only a few evaluation methods explicitly targeting on the quality of a document. Miller and Vanni (2001) devise a human evaluation approach to measure the comprehensibility of a text as a whole, based on the Rhetorical Structure Theory (Mann and Thompson, 1988), a theory of text organization specifying coherence relations in an authentic text. Snover et al. (2006) proposes HTER to assess post-editing effort through human annotation. Its automatic versions TER and TERp (Snover et al., 2009), however, remain sentencebased metrics. Comelles et al. (2010) present a family of automatic MT evaluation measures, based on the Discourse Representation Theory (Kamp and Reyle, 1993), that generate semantic trees to put together different text entities for the same referent according to their contexts and grammatical connections. Apart from MT evaluation, automated essay scoring programs such as E-rater (Burstein, 2003) also employ a rich set of discourse features for assessment. However, the parsing process needed for these linguistic-heavy approaches may suffer seriously from grammatical </context>
</contexts>
<marker>Snover, Madnani, Dorr, Schwartz, 2009</marker>
<rawString>Matthew Snover, Nitin Madnani, Bonnie J. Dorr, and Richard Schwartz. 2009. Fluency, adequacy, or HTER? Exploring different human judgments with a tunable MT metric. In Proceedings of the 4th Workshop on Statistical Machine Translation, pages 259– 268, Athens.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Georges van Slype</author>
</authors>
<title>Critical Study of Methods for Evaluating the Quality of Machine Translation.</title>
<date>1979</date>
<tech>Technical report,</tech>
<institution>Bureau Marcel van Dijk / European Commission,</institution>
<location>Brussels.</location>
<marker>van Slype, 1979</marker>
<rawString>Georges van Slype. 1979. Critical Study of Methods for Evaluating the Quality of Machine Translation. Technical report, Bureau Marcel van Dijk / European Commission, Brussels.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Muriel Vasconcellos</author>
</authors>
<title>Cohesion and coherence in the presentation of machine translation products.</title>
<date>1989</date>
<booktitle>Georgetown University Round Table on Languages and Linguistics 1989: Language Teaching, Testing, and Technology: Lessons from the Past with a View Toward the Future,</booktitle>
<pages>89--105</pages>
<editor>In James E. Alatis, editor,</editor>
<publisher>Georgetown University Press.</publisher>
<contexts>
<context position="4485" citStr="Vasconcellos, 1989" startWordPosition="699" endWordPosition="700">s in a text. This paper focuses on the latter. A quantitative comparison of lexical cohesion devices between MT output and human translation is first conducted, to examine the weakness of current MT systems in handling this feature. Different ways of exploiting lexical cohesion devices for MT evaluation at the document level are then illustrated. 2 Related Works Cohesion and coherence are both necessary monolingual features in a target text. They can hardly be evaluated in isolation and have to be conjoined with other quality criteria such as adequacy and fluency. A survey of MT post-editing (Vasconcellos, 1989) suggests that cohesion and coherence serve as higher level quality criteria beyond many others such as syntactic well-formedness. Post-editors tend to correct syntactic errors first before any amendment for improving the cohesion and coherence of an MT output. Also, as Wilks (1978)1 noted, it is rather unlikely for a sufficiently large sample of translations to be coherent and totally wrong at the same time. Cohesion and coherence are appropriate to serve as criteria for the overall quality of MT output. Previous researches in MT predominantly focus on specific types of cohesion devices. For </context>
</contexts>
<marker>Vasconcellos, 1989</marker>
<rawString>Muriel Vasconcellos. 1989. Cohesion and coherence in the presentation of machine translation products. In James E. Alatis, editor, Georgetown University Round Table on Languages and Linguistics 1989: Language Teaching, Testing, and Technology: Lessons from the Past with a View Toward the Future, pages 89–105. Georgetown University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric M Visser</author>
<author>Masaru Fuji</author>
</authors>
<title>Using sentence connectors for evaluating MT output.</title>
<date>1996</date>
<booktitle>In COLING</booktitle>
<pages>1066--1069</pages>
<contexts>
<context position="2507" citStr="Visser and Fuji, 1996" startWordPosition="389" endWordPosition="392">t of document structure. There is no guarantee for the coherence of a text if it is produced by simply putting together stand-alone sentences, no matter how well-translated, without adequate intersentential connection. As a consequence, MT system optimized this way to any of these metrics can only have a very dim chance of producing translated document that reads as natural as human writing. The accuracy of MT output at the document level is particularly important to MT users, for they care about the overall meaning of a text in question more than the grammatical correctness of each sentence (Visser and Fuji, 1996). Post-editors particularly need to ensure the quality of a whole document of MT output when revising its sentences. The connectivity of sentences is surely a significant factor contributing to the understandability of a text as a whole. This paper studies the inter-sentential linguistic features of cohesion and coherence and presents plausible ways to incorporate them into the sentence-based metrics to support MT evaluation at the document level. In the Framework for MT Evaluation in the International Standards of Language Engineering (FEMTI) (King et al., 2003), coherence is defined as “the </context>
</contexts>
<marker>Visser, Fuji, 1996</marker>
<rawString>Eric M. Visser and Masaru Fuji. 1996. Using sentence connectors for evaluating MT output. In COLING 1996, pages 1066–1069.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yorick Wilks</author>
</authors>
<title>The Value of the Monolingual Component</title>
<date>1978</date>
<booktitle>in MT Evaluation and its Role in the Battelle. Report on Systran, Luxembourg CEC Memorandum.</booktitle>
<contexts>
<context position="4768" citStr="Wilks (1978)" startWordPosition="742" endWordPosition="743"> MT evaluation at the document level are then illustrated. 2 Related Works Cohesion and coherence are both necessary monolingual features in a target text. They can hardly be evaluated in isolation and have to be conjoined with other quality criteria such as adequacy and fluency. A survey of MT post-editing (Vasconcellos, 1989) suggests that cohesion and coherence serve as higher level quality criteria beyond many others such as syntactic well-formedness. Post-editors tend to correct syntactic errors first before any amendment for improving the cohesion and coherence of an MT output. Also, as Wilks (1978)1 noted, it is rather unlikely for a sufficiently large sample of translations to be coherent and totally wrong at the same time. Cohesion and coherence are appropriate to serve as criteria for the overall quality of MT output. Previous researches in MT predominantly focus on specific types of cohesion devices. For grammatical cohesion, a series of works, including Nakaiwa and Ikehara (1992), Nakaiwa et al. (1995), and Nakaiwa and Shirai (1996), present approaches to resolving Japanese zero pronouns and to integrating them into a Japanese-English transferred-based MT system. Peral et al. (1999</context>
</contexts>
<marker>Wilks, 1978</marker>
<rawString>Yorick Wilks. 1978. The Value of the Monolingual Component in MT Evaluation and its Role in the Battelle. Report on Systran, Luxembourg CEC Memorandum.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Billy T M Wong</author>
<author>Cecilia F K Pun</author>
<author>Chunyu Kit</author>
<author>Jonathan J Webster</author>
</authors>
<title>Lexical cohesion for evaluation of machine translation at document level.</title>
<date>2011</date>
<booktitle>In NLP-KE 2011,</booktitle>
<pages>238--242</pages>
<location>Tokushima.</location>
<contexts>
<context position="18964" citStr="Wong et al., 2011" startWordPosition="3084" endWordPosition="3087">064 guages. Its drawback, however, lies in the risk of relying on a single discourse feature. Although lexical cohesion gives a strong indication of text coherence, it is not indispensable, because a text can be coherent without any surface cohesive clue. Furthermore, the quality of a document is also reflected in that of its sentences. A coherent translation may be mistranslated, and on the other hand, a text containing lots of sentence-level errors would make it difficult to determine its document-level quality. A previous study comparing MT evaluation at the sentence versus document level (Wong et al., 2011) reports a poor consistency in the evaluation results at these two levels when the sentence-level scores of MT output are low. In regard of these, how to integrate these two levels of MT evaluation is particularly worth studying. 5 Experiments We examine, through experiments, the effectiveness of using LC and RC ratios alone and integrating them into other evaluation metrics for MT evaluation at the document and system levels. Three evaluation metrics, namely BLEU, TER and METEOR,2 are selected for testing. They represent three distinctive types of evaluation metrics: n-gram, editdistance, and</context>
</contexts>
<marker>Wong, Pun, Kit, Webster, 2011</marker>
<rawString>Billy T. M. Wong, Cecilia F. K. Pun, Chunyu Kit, and Jonathan J. Webster. 2011. Lexical cohesion for evaluation of machine translation at document level. In NLP-KE 2011, pages 238–242, Tokushima.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Billy Tak-Ming Wong</author>
</authors>
<title>Semantic evaluation of machine translation.</title>
<date>2010</date>
<booktitle>In LREC 2010,</booktitle>
<pages>2884--2888</pages>
<location>Valletta.</location>
<contexts>
<context position="13404" citStr="Wong, 2010" startWordPosition="2164" endWordPosition="2165">stics of lexical cohesion devices in machine versus human translation (average frequencies per version of MT/HT) RC (MT) RC (HT) LC (MT) LC (HT) 0.55 0.55 0.50 0.50 0.45 0.45 0.40 0.40 Values of RC &amp; LC 0.35 0.35 0.30 0.30 0.25 0.25 0.20 0.20 MetricsMATR MTC4 Figure 1: Use of lexical cohesion devices in machine versus human translation global root node in WordNet, and lcs is the least common subsumer (i.e., the most specific ancestor concept) of c1 and c2. A threshold is set to 0.96 for words to be considered near-synonyms of each other, based on the empirical observation in a previous study (Wong, 2010). 3.3 Results The difference between MT and HT (reference translation) in terms of the frequencies of lexical cohesion devices in MetricsMATR and MTC4 datasets is presented in Table 2. The frequencies are averaged by the number of MT/HT versions. A further categorization breaks down content words into lexical cohesion devices and those that are not. The count of each type of lexical cohesion device is also provided. In general the two datasets provide highly similar statistics. There are 4.7–5.1% more content words in HT than in MT. The numbers of ordinary content words (i.e., not lexical cohe</context>
</contexts>
<marker>Wong, 2010</marker>
<rawString>Billy Tak-Ming Wong. 2010. Semantic evaluation of machine translation. In LREC 2010, pages 2884–2888, Valletta.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhibiao Wu</author>
<author>Martha Palmer</author>
</authors>
<title>Verb semantics and lexical selection.</title>
<date>1994</date>
<booktitle>In ACL</booktitle>
<pages>133--138</pages>
<location>Las Cruces.</location>
<contexts>
<context position="11875" citStr="Wu and Palmer, 1994" startWordPosition="1884" endWordPosition="1887">r stemmer (1980). To classify the semantic relationships of words, WordNet (Fellbaum, 1998) is used as a lexical resource, which clusters words of the same sense (i.e., synonyms) into a semantic group, namely a synset. Synsets are interlinked in WordNet according to their semantic relationships. Superordinate and collocation are formed by words in a proximate semantic relationship, such as bicycle and vehicle (hypernym), bicycle and wheel (meronym), bicycle and car (coordinate term), and so on. They are defined as synset pairs with a distance of 1 in WordNet. The measure of semantic distance (Wu and Palmer, 1994) is also applied to identify near-synonyms, i.e., words that are synonyms in a broad sense but not grouped in the same synset. It quantifies the semantic similarity of word pairs as a real number in between 0 and 1 (the higher the more similar) as sim(c1, c2) = d(c1) + d(c2) where c1 and c2 are the concepts (synsets) that the two words in question belong to, d is the distance in terms of the shortest path from a concept to the 2 d(lcs(c1,c2)) 1062 Word type MT MetricsMATR MT MTC4 HT Difference (%) HT Difference (%) Content word 4428 4636 208 (4.7) 16162 16982 830 (5.1) - Not lexical cohesion d</context>
</contexts>
<marker>Wu, Palmer, 1994</marker>
<rawString>Zhibiao Wu and Martha Palmer. 1994. Verb semantics and lexical selection. In ACL 1994, pages 133–138, Las Cruces.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tong Xiao</author>
<author>Jingbo Zhu</author>
<author>Shujie Yao</author>
<author>Hao Zhang</author>
</authors>
<title>Document-level consistency verification in machine translation.</title>
<date>2011</date>
<booktitle>In MT summit XIII,</booktitle>
<pages>131--138</pages>
<location>Xiamen.</location>
<contexts>
<context position="6470" citStr="Xiao et al., 2011" startWordPosition="1014" endWordPosition="1017">ormation for statistical MT. A phase of its work is to have grammatical devices, 1As cited in van Slype (1979). such as verbal tense/aspect/mode, discourse connectives and pronouns, manually annotated in multilingual corpora, in hopes of laying a foundation for the development of automatic labelers for them that can be integrated into an MT model. For lexical cohesion, it has been only partially and indirectly addressed in terms of translation consistency in MT output. Different approaches to maintaining consistency in target word choices are proposed (Itagaki et al., 2007; Gong et al., 2011; Xiao et al., 2011). Carpuat (2009) also observes a general tendency in human translation that a given sense is usually lexicalized in a consistent manner throughout the whole translation. Nevertheless there are only a few evaluation methods explicitly targeting on the quality of a document. Miller and Vanni (2001) devise a human evaluation approach to measure the comprehensibility of a text as a whole, based on the Rhetorical Structure Theory (Mann and Thompson, 1988), a theory of text organization specifying coherence relations in an authentic text. Snover et al. (2006) proposes HTER to assess post-editing eff</context>
</contexts>
<marker>Xiao, Zhu, Yao, Zhang, 2011</marker>
<rawString>Tong Xiao, Jingbo Zhu, Shujie Yao, and Hao Zhang. 2011. Document-level consistency verification in machine translation. In MT summit XIII, pages 131–138, Xiamen.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
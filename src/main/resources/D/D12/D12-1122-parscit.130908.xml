<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000008">
<title confidence="0.9988525">
Extracting Opinion Expressions with
semi-Markov Conditional Random Fields
</title>
<author confidence="0.997924">
Bishan Yang
</author>
<affiliation confidence="0.9937755">
Department of Computer Science
Cornell University
</affiliation>
<email confidence="0.997755">
bishan@cs.cornell.edu
</email>
<author confidence="0.992527">
Claire Cardie
</author>
<affiliation confidence="0.9937685">
Department of Computer Science
Cornell University
</affiliation>
<email confidence="0.998325">
cardie@cs.cornell.edu
</email>
<sectionHeader confidence="0.995636" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999562714285714">
Extracting opinion expressions from text is
usually formulated as a token-level sequence
labeling task tackled using Conditional Ran-
dom Fields (CRFs). CRFs, however, do not
readily model potentially useful segment-level
information like syntactic constituent struc-
ture. Thus, we propose a semi-CRF-based ap-
proach to the task that can perform sequence
labeling at the segment level. We extend the
original semi-CRF model (Sarawagi and Co-
hen, 2004) to allow the modeling of arbitrar-
ily long expressions while accounting for their
likely syntactic structure when modeling seg-
ment boundaries. We evaluate performance on
two opinion extraction tasks, and, in contrast
to previous sequence labeling approaches to
the task, explore the usefulness of segment-
level syntactic parse features. Experimental
results demonstrate that our approach outper-
forms state-of-the-art methods for both opin-
ion expression tasks.
</bodyText>
<sectionHeader confidence="0.999135" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9998863">
Accurate opinion expression identification is crucial
for tasks that benefit from fine-grained opinion anal-
ysis (Wiebe et al., 2005): e.g., it is a first step
in characterizing the sentiment and intensity of the
opinion; it provides a textual anchor for identifying
the opinion holder and the target or topic of an opin-
ion; and these, in turn, form the basis of opinion-
oriented question answering and opinion summa-
rization systems. In this paper, we focus on opin-
ion expressions as defined in Wiebe et al. (2005) —
subjective expressions that denote emotions, senti-
ment, beliefs, opinions, judgments, or other private
states (Quirk et al., 1985) in text. These include
direct subjective expressions (DSEs): explicit men-
tions of private states or speech events expressing
private states; and expressive subjective expressions
(ESEs): expressions that indicate sentiment, emo-
tion, etc. without explicitly conveying them. Follow-
ing are two example sentences labeled with DSEs
and ESEs.
</bodyText>
<listItem confidence="0.752638333333333">
(1) The International Committee of the
Red Cross, [as usual][ESE], [has refused to
make any statements][DSE].
(2) The Chief Minister [said][DSE] that [the
demon they have reared will eat up their
own vitals][ESE].
</listItem>
<bodyText confidence="0.9999405">
As a type of information extraction task, opinion
expression extraction has been successfully tackled
in the past via sequence tagging methods: Choi et
al. (2006) and Breck et al. (2007), for example, ap-
ply conditional random fields (CRFs) (Lafferty et
al., 2001) using sophisticated token-level features.
In token-level sequence labeling, labels are assigned
to single tokens, and the label of each token depends
on the current token and the label of the previous to-
ken (we consider the usual first-order assumption).
Segment-based features — features that describe a
set of related contiguous tokens, e.g., a phrase or
constituent — might provide critical information for
identifying opinion expressions; they cannot, how-
ever, be readily and naturally represented in the CRF
model.
</bodyText>
<page confidence="0.922365">
1335
</page>
<note confidence="0.761033">
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1335–1345, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.999914393939394">
Our goal in this work is to extract opinion ex-
pressions at the segment level with semi-Markov
conditional random fields (semi-CRFs). Semi-CRFs
(Sarawagi and Cohen, 2004) are more powerful than
CRFs in that they allow one to construct features
to capture characteristics of the subsequences of a
sentence. They are defined on semi-Markov chains
where labels are attached to segments instead of
tokens and label dependencies are modeled at the
segment-level. Previous work has shown that semi-
CRFs outperform CRFs on named entity recog-
nition (NER) tasks (Sarawagi and Cohen, 2004;
Okanohara et al., 2006). However, to the best of
our knowledge, semi-CRF techniques have not been
investigated for opinion expression extraction.
The contribution of this paper is a semi-CRF-
based approach for opinion expression extraction
that leverages parsing information to provide better
modeling of opinion expressions. Specifically, pos-
sible segmentations are generated by taking into ac-
count likely syntactic structure during learning and
inference. As a result, arbitrarily long expressions
can be modeled and their boundaries can be influ-
enced by probable syntactic structure. We also ex-
plore the impact of syntactic features for extracting
opinion expressions.
We evaluate our model on two opinion extrac-
tion tasks: identifying direct subjective expres-
sions (DSEs) and expressive subjective expressions
(ESEs). Experimental results show that our ap-
proach outperforms the state-of-the-art approach for
the task by a large margin. We also identify useful
syntactic features for the task.
</bodyText>
<sectionHeader confidence="0.999833" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999961235294118">
Previous research to extract direct subjective ex-
pressions exists, but is mainly focused on single-
word expressions (Wiebe et al., 2005; Wilson et
al., 2005; Munson et al., 2005). More recent stud-
ies tackle opinion expression extraction at the ex-
pression level. Breck et al. (2007) formulate the
problem as a token-level sequence labeling prob-
lem; their CRF-based approach was shown to sig-
nificantly outperform two subjectivity-clue-based
baselines. Others extend the token-level approach
to jointly identify opinion holders (Choi et al.,
2006), and to determine the polarity and inten-
sity of the opinion expressions (Choi and Cardie,
2010). Reranking the output of a simple sequence
labeler has been shown to further improve the ex-
traction of opinion expressions (Johansson and Mos-
chitti, 2010; Johansson and Moschitti, 2011); impor-
tantly, their reranking approach relied on features
that encoded syntactic structure. All of the above
approaches, however, are based on token-level se-
quence labeling, which ignores potentially useful
phrase-level information.
Semi-CRFs (Sarawagi and Cohen, 2004) are gen-
eral CRFs that relax the Markovian assumptions to
allow sequence labeling at the segment level. Pre-
vious work has shown that semi-CRFs are supe-
rior to CRFs for NER and Chinese word segmen-
tation (Sarawagi and Cohen, 2004; Okanohara et al.,
2006; Andrew, 2006). The task of opinion expres-
sion extraction is known to be harder than traditional
NER since subjective expressions exhibit substantial
lexical variation and their recognition requires more
attention to linguistic structure.
Parsing has been leveraged to improve perfor-
mance for numerous natural language tasks. In opin-
ion mining, numerous studies have shown that syn-
tactic parsing features are very helpful for opinion
analysis. A lot of work uses syntactic features to
identify opinion holders and opinion topics (Bethard
et al., 2005; Kim and Hovy, 2006; Kobayashi et al.,
2007; Joshi and Carolyn, 2009; Wu et al., 2009;
Choi et al., 2005). Jakob et al. (2010) recently
employed dependency path features for the extrac-
tion of opinion targets. Johansson and Moschitti
(2010; Johansson and Moschitti (2011) also success-
fully employed syntactic features that indicate de-
pendency relations between opinion expressions for
the task of opinion expression extraction. However,
as their approach is based on the output of a se-
quence labeler, these features cannot be encoded to
help the learning of the sequence labeler.
</bodyText>
<sectionHeader confidence="0.996289" genericHeader="method">
3 Approach
</sectionHeader>
<bodyText confidence="0.999961666666667">
We formulate the extraction of opinion expres-
sions as a sequence labeling problem. Unlike
previous sequence-labeling approaches to the task
(e.g., Breck et al. (2007)), however, we aim to model
segment-level, rather than token-level, information.
As a result, we explore the use of semi-CRFs, which
</bodyText>
<page confidence="0.986776">
1336
</page>
<bodyText confidence="0.999935571428571">
can assign labels to segments instead of tokens;
hence, features can be defined at the segment level.
For example, features like QX is a verb phrase� can
be easily encoded in the model. In the following
subsections, we first introduce standard semi-CRFs
and then describe our semi-CRF-based approach for
opinion expression extraction.
</bodyText>
<subsectionHeader confidence="0.999887">
3.1 Semi-CRFs
</subsectionHeader>
<bodyText confidence="0.997491764705882">
In semi-CRFs, each observed sentence x is repre-
sented as a sequence of consecutive segments s =
(s1, ..., sr,,), where si is a triple si = (ti, ui, yi), ti
denotes the start position of segment si, ui denotes
the end position, and yi denotes the label of the seg-
ment. Segments are restricted to have positive length
less than or equal to a maximum length of L that has
been seen in the corpus (1 G ui − ti + 1 G L).
Features in semi-CRFs are defined at the seg-
ment level rather than the word level. The fea-
ture function g(i, x, s) is a function of x, the cur-
rent segment si, and the label yi−1 of the previ-
ous segment si−1 (we consider the usual first-order
Markovian assumption). It can also be written as
g(x, ti, ui, yi, yi−1). The conditional probability of
a segmentation s given a sequence x is defined as
where
</bodyText>
<equation confidence="0.9350932">
exp
(X )
X
Akgk(i, x, s�)
i k
</equation>
<bodyText confidence="0.9783485">
and the set 5 contains all possible segmentations ob-
tained from segment candidates with length ranging
from 1 to the maximum length L.
The correct segmentation s of a sentence
is defined as a sequence of entity segments
(i.e., the entities to be extracted) and non-
entity segments. For example, the correct
segmentation of sentence (2) in Section 1 is
(said,DSE),(that,NONE),(the demon they have
reared will
((The,NONE),(Chief,NONE),(Minister,NONE),
eat up their own vitals,ESE),(.,NONE)).
Here, non-entity segments are represented as
unit-length segments.
</bodyText>
<subsectionHeader confidence="0.9919275">
3.2 Semi-CRF-based Approach for Opinion
Expression Extraction
</subsectionHeader>
<bodyText confidence="0.99953945">
In this section, we present an extended version of
semi-CRFs in which we can make use of parsing in-
formation in learning entity boundaries and labels
for opinion expression extraction.
As discussed in Section 3.1, the maximum entity
length L is fixed during training to generate segment
candidates in the standard semi-CRFs. In opinion
expression extraction, L is unbounded since opin-
ion expressions may be clauses or whole sentences,
which can be arbitrarily long. Thus, fixing an upper
bound on segment length based on the observed en-
tities may lead to an incorrect removal of segments
during inference. Also note that possible segment
candidates are generated based on the length con-
straint, which means any span of the text consisting
of no more than L words would be considered as
a possible segment. This would lead to the consid-
eration of implausible segments, e.g.,
in sentence (2) is an incorrect segment within the
multi-word expression
</bodyText>
<subsectionHeader confidence="0.650619">
Chief
</subsectionHeader>
<bodyText confidence="0.95585425">
To address these problems, we propose tech-
niques to incorporate parsing information into the
modeling of segments in semi-CRFs. More specifi-
cally, we construct segment units from the parse tree
of each
and then build up possible seg-
ment candidates based on those units. In the parse
tree, each leaf phrase or leaf word is considered to be
a segment unit. Each segment unit performs as the
smallest unit in the model (words within a segment
unit will be automatically assigned the same label).
The segment units are highlighted in rectangles in
the parse tree example in Figure 1. As the segment
units are not separable, we avoid implausible seg-
ments, which truncate multi-word expressions. For
example,
ridiculous
would not be con-
sidered apossible segment in our model.
To generate segment candidates for the model,
we consider meaningful combinations of consecu-
tive segment units. Intuitively, a sentence is made
“TheChief”
“The
</bodyText>
<equation confidence="0.695339">
Minister”.
sentence1,
</equation>
<bodyText confidence="0.568785">
“both
and”,
y,
</bodyText>
<page confidence="0.626214">
1337
</page>
<bodyText confidence="0.973688">
up of several parts, and each has its own grammati-
cal role or meaning. We define the boundary of these
parts based on the parse tree structure. Specificall
</bodyText>
<subsectionHeader confidence="0.362658">
use the Stanford Parser
</subsectionHeader>
<bodyText confidence="0.791932">
edu/software/lex-parser.shtml to generate the
parse tre
</bodyText>
<equation confidence="0.891102222222222">
1We
http://nlp.stanford.
es.
p(s|x) = Z1x) exp(XXAkgk(i, x, s) (1)
i k
X
Z(x) =
s&apos;ES
root
</equation>
<figureCaption confidence="0.996196">
Figure 1: A parse tree example. There are seven segment units in the sentence. The shaded regions correspond to
segment groups, where Gi represents the segment group starting from segment unit Ui.
</figureCaption>
<figure confidence="0.992702894736842">
Ga
PRP
NP VP .
I found
VBD SBAR
that
the statements
DT NNS
IN S
NP VP
G1
S
VBP ADJP
are DT JJ CC JJ
riddiculous and odd
both ridiculous and odd
G4
.
Gs
</figure>
<bodyText confidence="0.969024333333333">
we consider each segment unit to belong to a mean-
ingful group defined by the span of its parent node.
Two consecutive segment units are considered to be-
long to the same group if the subtrees rooted in their
parent nodes have the same rightmost child. For ex-
ample, in Figure 1, segment units “are” and “both
ridiculous and odd” belong to the same group, while
“I” and “found” belong to different groups.
Algorithm 1 Construction of segment candidates
</bodyText>
<listItem confidence="0.9543314">
Input: A training sentence x
Output: A set of segment candidates 5
1: Obtain the segment units U = (U1, ..., Um) by
preorder traversal of the parse tree T, each Ui
corresponds to a node in T
2: for i = 1 to m do
3: j &lt;-- i - 1
4: while j &lt; m — 1 and
commonGroup(Ui, ..., Uj+1) do
5: j &lt;-- j + 1
6: fork=itojdo
7: for t=0toj—kdo
8: s &lt;-- segment(Uk, ..., Uk+t)
9: 5 &lt; --5 U s
10: Return 5
</listItem>
<bodyText confidence="0.964935146341464">
Following this idea, we generate possible seg-
ment candidates by Algorithm 1. Starting from
each segment unit Ui, we first find the rightmost
segment unit Uj that belongs to the same group
as Ui. Function commonGroup(Ui, ..., Uj) re-
turns True if Ui, ..., Uj are within the same group
(the parent nodes of Ui,...,Uj have the same right-
most child in their subtrees), otherwise it returns
False. Then we enumerate all possible combina-
tions of segment units Ui, ..., Uk where i &lt; k &lt;
j. segment(Ui, ..., Uj) denotes the segment ob-
tained by concatenating words in the consecutive
segment units Ui,...,Uj. This way, segment can-
didates are generated without constraints on length
and are meaningful for learning entity boundaries.
Based on the generated segment candidates, the
correct segmentation for each training sentence can
be obtained as follows. For opinion expressions
that do not match any segment candidate, we break
them down into smaller segments using a greedy
matching process. Starting from the start position
of the expression, we search for the longest candi-
date that is contained in the expression, add it to
the correct segmentation for the sentence, set the
start position to be the next position, and repeat the
process. Using this process, the correct segmen-
tation of sentence (2) would be s = ((The Chief
Minister,NONE),(said,DSE),(that,NONE),(the de-
mon they have reared,ESE), (will eat up their own
vitals,ESE),(.)). Note that here non-entities corre-
spond to segment units instead of single-word seg-
ments in the original semi-CRF model.2
After obtaining the set of possible segment candi-
dates and the correct segmentation s for each train-
ing sentence, the semi-CRF model can be trained.
The goal of learning is to find the optimal parameter
A by maximizing log-likelihood. We use the limited-
2There are cases where words within a segment unit have
different labels. This may be due to errors by the human anno-
tators or the errors in the parser. In such cases, we consider each
word within the segment unit as a segment.
</bodyText>
<page confidence="0.977153">
1338
</page>
<bodyText confidence="0.9995475">
memory BFGS algorithm (Liu and Nocedal, 1989)
for optimization in our implementation, where the
gradient of the log-likelihood L (corresponding to
one instance x) is computed:
</bodyText>
<equation confidence="0.97488375">
gk(x, ti, ui, yi, yi−1)
Xgk(x, t0j, u0j, y, y0)p(y, y0|x)
j
(2)
</equation>
<bodyText confidence="0.975809533333333">
where S is all possible segmentations consisting of
the generated segment candidates, p(y, y0|x) is the
probability of having label y for the current segment
s0j (with boundary (t0j, u0j)) and label y0 for the pre-
vious segment s0j−1.
We use a forward-backward algorithm to com-
pute the marginal distribution p(y, y0|x) and the nor-
malization factor Z(x) efficiently. For inference we
seek the best segmentation s∗ = arg maxs p(s|x),
where p(s|x) is defined by Equation 1. We im-
plement efficient inference using an extension of
Viterbi algorithm to segments. In particular, define
V (j,y) as the largest unnormalized probability of
p(s1:j|x) with label y at the ending position j. Then
we have
</bodyText>
<equation confidence="0.995826">
V (j, y) = max max φ(x, i, j, y, y0)V (i − 1, y0)
(i,j)∈s:�� y/
</equation>
<bodyText confidence="0.824606">
where
</bodyText>
<equation confidence="0.979058333333333">
(X
φ(x, i, j, y, y0) = exp λkgk(x, i, j, y, y0)
k
</equation>
<bodyText confidence="0.99865225">
and s:,j denotes the set of the generated segment
candidates ending at position j. The best segmen-
tation can be obtained from tracing the path of
maxy V (n, y).
</bodyText>
<subsectionHeader confidence="0.952095">
3.3 Features
</subsectionHeader>
<bodyText confidence="0.999979882352941">
Here we described the features used in our model.
Very generally, we include CRF-style features that
are segment-level extensions of the token-level fea-
tures. We also include new segment-level features
that can be naturally represented in semi-CRFs but
not CRFs.
For CRF-style features, we consider the string
representation of the current word, its part-of-
speech, and a dictionary-derived feature, which is
based on a subjectivity lexicon provided by Wilson
et al. (2005). The lexicon consists of a set of words
that can act as strong or weak cues to subjectivity.
If the current word appears as an entry in the lexi-
con, then a feature strong or weak will be fired if the
entry is of that strength. These features have been
successfully employed in previous work (Breck et
al., 2007). To employ them in our model, we sim-
ply extend the feature definition to the segment level.
For example, a token-level feature Qx is great � will
be extended to a segment-level feature Qs contains
great �.
Previous work on semi-CRFs has explored fea-
tures such as the length of the segment, the position
of the segment in the current segmentation (at the be-
ginning or at the end), indicators for the start word
and end word within the segment, and indicators for
words before and after the segment. These features
have been shown useful for the task of NE recogni-
tion (Sarawagi and Cohen, 2004; Okanohara et al.,
2006). However, we only found the position of the
segment to be helpful for the extraction of opinion
expressions, probably due to the lack of patterns in
the length distribution and word choices of opinion
expressions.
Besides the above features, we design new
segment-level syntactic features to capture the syn-
tactic patterns of opinion expressions. Syntactic pat-
terns are often used to identify useful information in
information extraction tasks. In our task, we found
that the majority of opinion expressions involve verb
phrases.3 For example, “was encouraged”, “ex-
pressed goodwill”, “cannot accept” are all within a
VP constituent. To capture such structural prefer-
ences, we define several syntax-based parse features
for VP-related constituents.4
Let VPROOT denote a VP constituent whose par-
ent node is not VP, and let VPLEAF denote a VP
constituent whose children nodes are non-VP. De-
note the head of VPLEAF as the predicate, and its
next segment unit as the argument. If a segment con-
sists of words in the VP nodes visited by the preorder
</bodyText>
<footnote confidence="0.984536">
3The percentages of opinion expressions involving
VP/NP/PP are 64.13%/18.43%/5.92% for DSEs and
43.22%/24.99%/11.77% for ESEs in the data set we used.
4We also conducted experiments with NP and PP-related
features, and could not find any performance improvement for
the tasks.
</footnote>
<equation confidence="0.997705">
X=
i
∂L
∂λk
X− X
s/∈S y,y,
</equation>
<page confidence="0.94563">
1339
</page>
<bodyText confidence="0.999668785714286">
traversal from a VPROOT to a VPLEAF, then we re-
fer to it as a verb-cluster segment. If a segment con-
sists of a verb cluster and the argument in VPLEAF,
we consider it as a VP segment. The following fea-
tures are defined for verb-cluster segments and VP
segments.
VPcluster: Indicates whether or not the segment
matches the verb-cluster structure.
VPpred: A feature of the syntactic category and
the word of the head of VPLEAF. The head of
VPLEAF is the predicate of the verb phrase, which
may encode some intention of opinions in the verb
phrase. For example, if “warned” is the head of
VPLEAF rather than “informed”, the chance of the
segment being an opinion expression increases.
VParg: A feature of the syntactic category and
the head word of the argument in VPLEAF. For ex-
ample, the noun phrase “a negative stand” is the ar-
gument of the predicate “take” in the verb phrase
“take a negative stand”. The argument in the verb
phrase (could be a noun phrase, adjectival phrase or
prepositional phrase) may convey some relevant in-
formation for identifying opinion expressions.
VPsubj: Whether the verb clusters or the argu-
ment in the segment contains an entry from the sub-
jectivity lexicon. For example, the word “negative”
is in the lexicon, so the segment “take a negative
stand” has a feature ISVPSUBJ.
</bodyText>
<sectionHeader confidence="0.999651" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999851357142857">
For evaluation, we use the MPQA 1.2 corpus (Wiebe
et al., 2005)5, a widely used data set for fine-grained
opinion analysis. It contains 535 news articles, a to-
tal of 11,114 sentences with subjectivity-related an-
notations at the phrase level. We focus on the task
of extracting two types of opinion expressions: di-
rect subjective expressions (DSEs) and expressive
subjective expressions (ESEs). Table 1 shows some
statistics of the corpus. As in prior research that
uses the corpus, we set aside the standard 135 docu-
ments as a development set and use 400 documents
as the evaluation set. All experiments employ 10-
fold cross validation on the evaluation set, and the
average over all runs is reported.
</bodyText>
<footnote confidence="0.995626">
5Available at http://www.cs.pitt.edu/mpqa/.
</footnote>
<table confidence="0.999191166666667">
DSEs ESEs
Sentences with opinions(%) 55.89 57.93
TotalNum 9746 11730
MaxLength 15 40
Length &gt; 1 (%) 43.38 71.65
Length &gt; 4 (%) 9.44 35.01
</table>
<tableCaption confidence="0.991079">
Table 1: Statistics of opinion expressions in the MPQA
Corpus.
</tableCaption>
<subsectionHeader confidence="0.983835">
4.1 Evaluation Metrics
</subsectionHeader>
<bodyText confidence="0.96277788">
We use precision, recall, and F-measure to evalu-
ate the quality of the model. Precision is defined
as |C∩P|
|P  |and recall, as|C∩P|
|C |, where C and P are
the sets of correct and predicted expression spans,
respectively. F-measure is computed as �P R
P �R. Be-
cause the boundaries of opinion expressions are hard
to define even for human annotators (Wiebe et al.,
2005), previous research mainly focused on soft pre-
cision and recall measures for performance evalu-
ation. Breck et al. (2007) introduced an overlap
measure, which considers a predicted expression to
be correct if it overlaps with a correct expression.
We refer to this metric as Binary Overlap. Johans-
son and Moschitti (2010) provides a stricter measure
that computes the proportion of overlapping spans:
if a correct expression s overlaps with a predicted
expression s0, the overlap contributes value |s∩s�|
|s� |to
|C n P |instead of value 1. We refer to this metric
as Proportional Overlap. To compare with previous
work, we present our results according to both met-
rics.
</bodyText>
<subsectionHeader confidence="0.508287">
4.2 Baseline Methods
</subsectionHeader>
<bodyText confidence="0.999964428571429">
As a baseline, we use the token-level CRF-based ap-
proach of Breck et al. (2007) applied to the MPQA
dataset. We employ a very similar, but not iden-
tical set of features: indicators for specific words
at the current location and neighboring words in a
[−4, +4] window, part-of-speech features, and opin-
ion lexicon features for tokens that are contained in
the subjectivity lexicon (see Section 3.3). We do not
include WordNet, Levin’s verb categorization, and
FrameNet features.
We also include two variants of standard CRFs as
baselines: segment-CRF and syntactic-CRF. They
incorporate segmentation information into standard
CRFs without modifying the Markovian assump-
</bodyText>
<page confidence="0.970571">
1340
</page>
<table confidence="0.999917444444444">
DSE Extraction ESE Extraction
Method Precision Recall F-measure Precision Recall F-measure
CRF 82.83 49.38 61.87 78.56 43.57 56.05
segment-CRF 82.52 51.48 63.41 78.90 44.46 56.88
syntactic-CRF 82.48 49.09 61.55 78.41 43.39 55.95
semi-CRF 66.67 74.13 70.20 71.21 57.41 63.57
new-semi-CRF 67.72** 74.33 70.87* 73.57*** 57.63 64.74**
semi-CRF(w/ syn) 64.86 74.10 69.17 70.68 56.61 62.87
new-semi-CRF(w/ syn) 70.12*** 74.74* 72.36*** 73.61*** 59.27*** 65.67***
</table>
<tableCaption confidence="0.981666">
Table 2: Results for extracting opinion expressions with Binary-Overlap metric. (w/ syn) indicates the inclusion of
syntactic parse features VPpre, VParg and VPsubj. Results of new-semi-CRF that are statistically significantly greater
than semi-CRF according to a two-tailed t-test are indicated with *(p &lt; 0.1), **(p &lt; 0.05), ***(p &lt; 0.005). T-test
results are also shown for new-semi-CRF(w/ syn) versus semi-CRF(w/ syn).
</tableCaption>
<table confidence="0.999949666666667">
DSE Extraction ESE Extraction
Method Precision Recall F-measure Precision Recall F-measure
CRF 77.91 46.45 58.20 67.72 37.55 48.31
segment-CRF 77.86 48.58 59.83 68.03 38.34 49.04
syntactic-CRF 77.73 46.27 58.01 67.80 37.60 48.37
semi-CRF 60.38 68.34 64.11 57.30 46.20 51.16
new-semi-CRF 62.50** 68.59* 65.41* 61.69*** 47.44** 53.63***
semi-CRF(w/ syn) 58.69 67.80 62.92 57.09 45.63 50.72
new-semi-CRF(w/ syn) 65.52*** 68.91*** 67.17*** 61.66*** 48.77*** 54.47***
</table>
<tableCaption confidence="0.997323">
Table 3: Results for extracting opinion expressions with Proportional-Overlap metric. Notation is the same as above.
</tableCaption>
<figureCaption confidence="0.820881333333333">
tion. Segment-CRF treats segment units obtained
from the parser as word tokens. For example, in
Figure 1, the segment units the statement and both
</figureCaption>
<bodyText confidence="0.979377631578948">
ridiculous and odd will be treated as word tokens.
Syntactic-CRF encodes segment-level syntactic in-
formation in a standard token-level CRF as input
features. We consider the VP-related segment fea-
tures introduced in Section 3.3. VPPRE and VPARG
are added to the head word of the corresponding verb
phrase, and VPSUBJ and VPCLUSTER are added to
each token within the corresponding segment.
Another baseline method is the original semi-
CRF model (Sarawagi and Cohen, 2004). To the
best of our knowledge, our work is the first to ex-
plore the use of semi-CRFs on the extraction of
opinion expressions. They are considered to be more
powerful than CRFs since they allow information to
be represented at the expression level. The model
requires an input of the maximum entity length. We
set it to 15 for DSE and 40 for ESE. For segment fea-
tures, we used the same features as in our approach
(see Section 3.3).
</bodyText>
<subsectionHeader confidence="0.956513">
4.3 Results
</subsectionHeader>
<bodyText confidence="0.999770739130435">
Table 2 and Table 3 show the results of DSE and
ESE extraction using two different metrics. The
standard token-based CRF baseline of Breck et al.
(2007) is labeled CRF; the original semi-CRF base-
line is labeled semi-CRF; and our extended semi-
CRF approach is labeled new-semi-CRF. For semi-
CRF and new-semi-CRF, the results were obtained
using two different settings of features: the basic
feature set includes features described in Section 3.3
excluding the segment-level syntactic features. In
the second feature setting (labeled as w/ syn in the
tables), we further augment the basic features with
the syntactic parse features.
Using the basic features, we observe that
semi-CRF-based approaches significantly outper-
form CRF and its two variants segment-CRF and
syntactic-CRF in F-Measure on both DSE and ESE
extraction, and new-semi-CRF achieves the best re-
sults. By simply incorporating the segmentation
prior into the standard CRF, segment-CRF achieves
a slight improvement over standard CRF, but the
results are still worse than those of semi-CRF
and new-semi-CRF. However, adding segment-level
</bodyText>
<page confidence="0.961728">
1341
</page>
<table confidence="0.999962636363636">
DSE Extraction ESE Extraction
Feature set Precision Recall F-measure Precision Recall F-measure
Basic 67.72 74.33 70.87 73.57 57.63 64.74
Basic+VPpre 70.88 71.44 71.16 73.20 58.20 64.85
Basic+VParg 70.12 74.03 72.02 73.05 58.20 64.79
Basic+VPcluster 70.08 72.94 71.48 73.06 58.45 64.94
Basic+VPsubj 70.04 72.34 71.17 73.31 58.53 65.09
Basic+VPpre+VPsubj 70.91 72.54 71.72 73.61 58.29 65.07
Basic+VParg+VPsubj 70.45 73.53 71.96 74.45 57.80 65.07
Basic+VPpre+VParg+VPsubj 70.12 74.74 72.36 73.61 59.27 65.67
Basic+VPcluster+VPpre+VParg+VPsubj 70.91 72.54 71.72 72.84 58.45 64.86
</table>
<tableCaption confidence="0.999875">
Table 4: Effect of syntactic features on extracting opinion expressions with Binary-Overlap metric
</tableCaption>
<bodyText confidence="0.999947602941177">
syntactic features into standard CRF yields slightly
reduced performance. This is not surprising as en-
coding segment-level information into the token-
level CRF is not natural. These experiments in-
dicate that simply encoding segmentation informa-
tion into standard CRF cannot result in large per-
formance gains. The promising F-measure results
obtained by semi-CRF and new-semi-CRF confirm
that relaxing the Markovian assumption on segments
leads to better modeling of opinion expressions. We
can also see that new-semi-CRF consistently outper-
forms the original semi-CRF model. This further
confirms the benefit of taking into account syntactic
parsing information in modeling segments. In Ta-
ble 3, we observe the same general results trend as
in Table 2. The scores are generally lower since the
metric Proportional Overlap is stricter than Binary
Overlap.
We also study the impact of syntactic parse fea-
tures on the semi-Markov CRF models. Here we
consider the combination of VPPRE, VPARG and
VPSUBJ since they turned out to be the most help-
ful features for our tasks. Interestingly, we found
that after incorporating the syntactic parse features,
performance decreases on semi-CRF. This indicates
that syntactic information does not help if learning
and inference take place on segment candidates gen-
erated without accounting for parse information. In
contrast, our approach incorporates syntactic pars-
ing information in modeling segments and meaning-
ful segmentations. We can see in Tables 2 and 3
that adding syntactic features successfully boosts the
performance of our approach.
To further explore the effect of the syntactic fea-
tures, we include the results of our model with dif-
ferent configurations of syntactic features in Table 4
(here we focus on the Binary Overlap metric as
the results with Proportional Overlap demonstrate
a similar conclusion). We can see that using the ba-
sic features and the combination of VPPRE, VPARG
and VPSUBJ yields the best results for both DSE
and ESE extraction. For DSE extraction, combin-
ing these three features improves the precision no-
ticeably from 67.72% to 70.12% while the recall
slightly improves. This indicates that VP-related
structural information is very helpful for modeling
segments as DSEs. However, this trend is not so
clear for ESE extraction. This may be due to the fact
that DSEs often involve verb phrases while ESEs are
represented via a variety of syntactic structures.
Comparison with previous work. In Table 5, we
compare our results to the previous work on opinion
expression extraction (here we also focus on the Bi-
nary Overlap metric due to the similar trend demon-
strated by the Proportional Overlap metric). Breck
et al. (2007) presents the state-of-the-art sequence
labeling approach on the tasks of DSE and ESE ex-
traction. Their best results are shown as Breck et
al. Baseline in the table. Johansson and Mos-
chitti (2010) use a reranking technique on the best
k outputs of a sequence labeler to further improve
their sequence labeling results on the task of ex-
tracting DSEs, ESEs and OSEs (Objective Speech
Events) (we don’t consider OSEs here). Results
using our re-implementation of their approach us-
ing SVM&amp;quot;&amp;quot;&apos; (Tsochantaridis et al., 2004) on the
output of CRF are labeled CRF+Reranking Base-
line in the table. We use the same features and
</bodyText>
<page confidence="0.983536">
1342
</page>
<bodyText confidence="0.998375857142857">
parameter settings as in their approach. Our ap-
proach+Reranking are results obtained by apply-
ing the reranking step on the output of our new-
semi-CRF approach.
We can see that our approach outperforms the
Breck et al. Baseline on both DSE extraction and
ESE extraction in spite of the fact that we do not
use their WordNet, Levin’s verb categorization, and
FrameNet features. The CRF+Reranking Baseline
does provide a performance increase over the the
baseline CRF results, but overall it cannot beat the
other methods since the CRF baseline is very low.
As one might expect, reranking also succeeds in
boosting the performance of new-semi-CRF, achiev-
ing the best performance on F-measure for both DSE
and ESE extraction. Note that the interannotator
agreement results for these two tasks are 75% for
DSE and 72% for ESE using a similar metric to Bi-
nary Overlap. Our results are much closer to these
interannotator scores than previous systems espe-
cially for DSEs.
</bodyText>
<table confidence="0.998932777777778">
Task Method F-measure
Breck et al. Baseline 70.65
DSE Extraction CRF+Reranking Baseline 63.87
Our approach 72.36
Our approach+Reranking 73.12
Breck et al. Baseline 63.43
ESE Extraction CRF+Reranking Baseline 58.21
Our approach 65.67
Our approach+Reranking 67.01
</table>
<tableCaption confidence="0.986085333333333">
Table 5: Comparison of our work with previous work on
opinion expression extraction using the Binary-Overlap
metric
</tableCaption>
<subsectionHeader confidence="0.907338">
4.4 Discussion
</subsectionHeader>
<bodyText confidence="0.99998706779661">
We note that our new-semi-CRF approach outper-
forms the original semi-CRF w.r.t. both precision
and recall, but compared to CRF, our approach
yields a clear improvement on recall but not on pre-
cision. An error analysis helps explain why. We
found that our semi-CRF approach predicted almost
the same number of DSEs as the gold standard la-
bels while CRF only predicted half of them (for ESE
extraction, the trend is similar). With more pre-
dicted entities, the precision is sacrificed but recall is
boosted substantially, and overall we see an increase
in F-measure.
Looking further into the errors, we found sev-
eral mistakes that could potentially be fixed to yield
better a precision score. Some errors were due to
the false prediction of speech events like “said” or
“told” as DSEs in cases where they actually just in-
troduced statements of fact without expressing any
private state. Adding features to distinguish such
cases should help improve performance. Other er-
rors were due to inadequate modeling of the context
surrounding the expressions. For example, “enjoy a
relative advantage” was falsely predicted as an ESE.
If incorporating information about the subject of this
verb phrase which is “products”, this mistake could
be avoided since “products” cannot hold or express
private state. We also noticed some errors caused
by inaccurate parsing and hope to study ways to ac-
count for these in our approach as future work.
By comparing the extraction results across differ-
ent methods, we see that full parsing provides many
benefits for modeling segment boundaries and im-
proving the prediction precision for opinion expres-
sion extraction. For example, given the sentence, “...
who are living [a lot better][ESE] ...”, both CRF and
the original semi-CRF extract “lot better” as an ESE,
while our approach correctly extracts “a lot better”
as an ESE. And we also found many cases where
the original semi-CRF cannot extract the opinion ex-
pressions while our approach can. Another benefit
of utilizing parsing is to speed up learning and infer-
ence. Although in theory, the computational cost of
parsing is O(g x n3) where g is the grammar size
and n is the sentence length while the cost of semi-
CRFs is O(K2 x L x n) where K is the number of
labels and L is the maximum entity length, feature
extraction overhead and the potentially large num-
ber of learning iterations in parameter optimization
may lead to a long training time for semi-CRFs. In
our experiments on the MPQA data set, our machine
with Intel Core 2 Duo CPU and 4GB RAM took 2
hours to fully parse 11,114 sentences using the Stan-
ford Parser, and also 2 hours to train the standard
semi-CRF. With the parsing information, our semi-
CRF-based approach is able to finish training in 15
minutes. As full parsing would be expensive when
the average sentence length is very large, it would be
interesting to study how to utilize parsing with less
cost in our task.
</bodyText>
<page confidence="0.984098">
1343
</page>
<sectionHeader confidence="0.998549" genericHeader="evaluation">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.99977175">
In this paper we propose a semi-CRF-based ap-
proach for extracting opinion expressions that takes
into account during learning and inference the struc-
tural information available from syntactic parsing.
Our approach allows opinion expressions to be iden-
tified at the segment level and their boundaries to
be influenced by their probable syntactic structure.
Experimental evaluations show that our model out-
performs the best existing approaches on two opin-
ion extraction tasks. In addition, we identify useful
syntactic parse features for these tasks that have not
been explored in previous work. Our error analysis
indicates that adding additional features that account
for subjectivity cues in the local context might fur-
ther improve the performance. In future work, we
hope to explore better ways of utilizing parsing in-
formation with less cost. Also, we will apply our
model to additional opinion analysis tasks such as
fine-grained opinion summarization and relation ex-
traction.
</bodyText>
<sectionHeader confidence="0.999058" genericHeader="conclusions">
6 Acknowledgement
</sectionHeader>
<bodyText confidence="0.999693333333333">
This work was supported in part by National Science
Foundation Grants IIS-1111176 and IIS-0968450,
and by a gift from Google. We thank Nikos Karam-
patziakis, Igor Labutov, Veselin Stoyanov, Ainur
Yessenalina and Jason Yosinski for their helpful
comments.
</bodyText>
<sectionHeader confidence="0.999235" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999943352941177">
Galen Andrew. 2006. A hybrid Markov/semi-Markov
conditional random field for sequence segmentation.
In Proceedings of EMNLP ’06.
Steven Bethard, Hong Yu, Ashley Thornton, Vasileios
Hatzivassiloglou, and Dan Jurafsky. 2005. Extracting
opinion propositions and opinion holders using syn-
tactic and lexical cues. In Shanahan, James G., Yan
Qu, and Janyce Wiebe, editors, Computing Attitude
and Affect in Text: Theory and Applications.
Eric Breck, Yejin Choi, and Claire Cardie. 2007. Identi-
fying expressions of opinion in context. IJCAI’07.
Yejin Choi, Claire Cardie, Ellen Riloff, and Siddharth
Patwardhan. 2005. Identifying sources of opinions
with conditional random fields and extraction patterns.
In Proceedings of HLT ’05.
Yejin Choi, Eric Breck, and Claire Cardie. 2006. Joint
extraction of entities and relations for opinion recog-
nition. In Proceedings of EMNLP ’06.
Yejin Choi and Claire Cardie. 2010. Hierarchical se-
quential learning for extracting opinions and their at-
tributes. In Proceedings of ACL 2010, Short Papers.
Richard Johansson and Alessandro Moschitti. 2010. Syn-
tactic and semantic structure for opinion expression
detection. In Proceedings of CoNLL ’10.
Niklas Jakob and Iryna Gurevych. Extracting opinion tar-
gets in a single- and cross-domain setting with condi-
tional random fields. In Proceedings of EMNLP’ 10.
Mahesh Joshi and Penstein-Ros’e Carolyn. 2009. Gen-
eralizing dependency features for opinion mining.
In Proceedings of ACL/IJCNLP 2009, Short Papers
Track.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of ACL ’03.
Soo-Min Kim and Eduard Hovy. 2006. Extracting opin-
ions, opinion holders, and topics expressed in online
news media text. In Proceedings of the ACL Workshop
on Sentiment and Subjectivity in Text.
Nozomi Kobayashi, Kentaro Inui, and Yuji Matsumoto.
2007. Extracting aspect-evaluation and aspect-of rela-
tions in opinion mining. In Proceedings of EMNLP-
CoNLL-2007.
John D. Lafferty, Andrew McCallum, and Fernando C.
N. Pereira. 2001. Conditional Random Fields: Proba-
bilistic Models for Segmenting and Labeling Sequence
Data. In Proceedings of ICML ’01.
Dong C. Liu and Jorge Nocedal. 1989. On the limited
memory BFGS method for large scale optimization.
Mathematical Programming B 45(3): 503-528.
M Arthur Munson, Claire Cardie, and Rich Caruana. Op-
timizing to arbitrary NLP metrics using ensemble se-
lection. In HLT-EMNLP05, 2005.
Daisuke Okanohara, Yusuke Miyao, Yoshimasa Tsu-
ruoka, and Jun’ichi Tsujii. Improving the scalability
of semi-Markov conditional random fields for named
entity recognition. In Proceedings of ACL’06.
Randolph Quirk, Sidney Greenbaum, Geoffrey Leech,
and Jan Svartvik. A comprehensive grammar of the
English language. New York: Longman, 1985.
Richard Johansson and Alessandro Moschitti. Extract-
ing Opinion Expressions and Their Polarities - Explo-
ration of Pipelines and Joint Models. In Proceedings
of ACL ’11, Short Paper.
Ellen Riloff and Janyce M Wiebe. Learning extraction
patterns for subjective expressions. In Proceedings of
EMNLP 2003.
Sunita Sarawagi and William W. Cohen. 2004. Semi-
Markov Conditional Random Fields for Information
Extraction. In Proceedings of NIPS 2004.
</reference>
<page confidence="0.867217">
1344
</page>
<reference confidence="0.999800909090909">
Charles Sutton and Andrew McCallum. An Introduc-
tion to Conditional Random Fields. Foundations and
Trends in Machine Learning (FnT ML), 2010.
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005.
Annotating expressions of opinions and emotions in
language. Language Resources and Evaluation, vol-
ume 39, issue 2-3, pp. 165-210.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In Proceedings of HLT ’05.
Theresa Wilson, Paul Hoffmann, Swapna Somasun-
daran, Jason Kessler, Janyce Wiebe, Yejin Choi, Claire
Cardie, Ellen Riloff, and Siddharth Patwardhan. Opin-
ionFinder: A system for subjectivity analysis. EMNLP
2005. Demo abstract.
Yuanbin Wu, Qi Zhang, Xuanjing Huang, and Lide Wu.
Phrase dependency parsing for opinion mining. In Pro-
ceedings of EMNLP 2009.
Ioannis Tsochantaridis, Thomas Hofmann, Thorsten
Joachims, and Yasemi Altun. Support Vector Learn-
ing for Interdependent and Structured Output Spaces.
In Proceedings of ICML 2004.
</reference>
<page confidence="0.992643">
1345
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.686324">
<title confidence="0.979757">Extracting Opinion Expressions semi-Markov Conditional Random Fields</title>
<author confidence="0.973243">Bishan</author>
<affiliation confidence="0.9299315">Department of Computer Cornell</affiliation>
<email confidence="0.998567">bishan@cs.cornell.edu</email>
<author confidence="0.976921">Claire</author>
<affiliation confidence="0.9352705">Department of Computer Cornell</affiliation>
<email confidence="0.998819">cardie@cs.cornell.edu</email>
<abstract confidence="0.9993225">Extracting opinion expressions from text is usually formulated as a token-level sequence labeling task tackled using Conditional Random Fields (CRFs). CRFs, however, do not readily model potentially useful segment-level information like syntactic constituent structure. Thus, we propose a semi-CRF-based approach to the task that can perform sequence labeling at the segment level. We extend the original semi-CRF model (Sarawagi and Cohen, 2004) to allow the modeling of arbitrarily long expressions while accounting for their likely syntactic structure when modeling segment boundaries. We evaluate performance on two opinion extraction tasks, and, in contrast to previous sequence labeling approaches to the task, explore the usefulness of segmentlevel syntactic parse features. Experimental results demonstrate that our approach outperforms state-of-the-art methods for both opinion expression tasks.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Galen Andrew</author>
</authors>
<title>A hybrid Markov/semi-Markov conditional random field for sequence segmentation.</title>
<date>2006</date>
<booktitle>In Proceedings of EMNLP ’06.</booktitle>
<contexts>
<context position="6350" citStr="Andrew, 2006" startWordPosition="943" endWordPosition="944">on expressions (Johansson and Moschitti, 2010; Johansson and Moschitti, 2011); importantly, their reranking approach relied on features that encoded syntactic structure. All of the above approaches, however, are based on token-level sequence labeling, which ignores potentially useful phrase-level information. Semi-CRFs (Sarawagi and Cohen, 2004) are general CRFs that relax the Markovian assumptions to allow sequence labeling at the segment level. Previous work has shown that semi-CRFs are superior to CRFs for NER and Chinese word segmentation (Sarawagi and Cohen, 2004; Okanohara et al., 2006; Andrew, 2006). The task of opinion expression extraction is known to be harder than traditional NER since subjective expressions exhibit substantial lexical variation and their recognition requires more attention to linguistic structure. Parsing has been leveraged to improve performance for numerous natural language tasks. In opinion mining, numerous studies have shown that syntactic parsing features are very helpful for opinion analysis. A lot of work uses syntactic features to identify opinion holders and opinion topics (Bethard et al., 2005; Kim and Hovy, 2006; Kobayashi et al., 2007; Joshi and Carolyn,</context>
</contexts>
<marker>Andrew, 2006</marker>
<rawString>Galen Andrew. 2006. A hybrid Markov/semi-Markov conditional random field for sequence segmentation. In Proceedings of EMNLP ’06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven Bethard</author>
<author>Hong Yu</author>
<author>Ashley Thornton</author>
<author>Vasileios Hatzivassiloglou</author>
<author>Dan Jurafsky</author>
</authors>
<title>Extracting opinion propositions and opinion holders using syntactic and lexical cues.</title>
<date>2005</date>
<booktitle>Computing Attitude and Affect in Text: Theory and Applications.</booktitle>
<editor>In Shanahan, James G., Yan Qu, and Janyce Wiebe, editors,</editor>
<contexts>
<context position="6886" citStr="Bethard et al., 2005" startWordPosition="1022" endWordPosition="1025">e word segmentation (Sarawagi and Cohen, 2004; Okanohara et al., 2006; Andrew, 2006). The task of opinion expression extraction is known to be harder than traditional NER since subjective expressions exhibit substantial lexical variation and their recognition requires more attention to linguistic structure. Parsing has been leveraged to improve performance for numerous natural language tasks. In opinion mining, numerous studies have shown that syntactic parsing features are very helpful for opinion analysis. A lot of work uses syntactic features to identify opinion holders and opinion topics (Bethard et al., 2005; Kim and Hovy, 2006; Kobayashi et al., 2007; Joshi and Carolyn, 2009; Wu et al., 2009; Choi et al., 2005). Jakob et al. (2010) recently employed dependency path features for the extraction of opinion targets. Johansson and Moschitti (2010; Johansson and Moschitti (2011) also successfully employed syntactic features that indicate dependency relations between opinion expressions for the task of opinion expression extraction. However, as their approach is based on the output of a sequence labeler, these features cannot be encoded to help the learning of the sequence labeler. 3 Approach We formul</context>
</contexts>
<marker>Bethard, Yu, Thornton, Hatzivassiloglou, Jurafsky, 2005</marker>
<rawString>Steven Bethard, Hong Yu, Ashley Thornton, Vasileios Hatzivassiloglou, and Dan Jurafsky. 2005. Extracting opinion propositions and opinion holders using syntactic and lexical cues. In Shanahan, James G., Yan Qu, and Janyce Wiebe, editors, Computing Attitude and Affect in Text: Theory and Applications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Breck</author>
<author>Yejin Choi</author>
<author>Claire Cardie</author>
</authors>
<title>Identifying expressions of opinion in context.</title>
<date>2007</date>
<pages>07</pages>
<contexts>
<context position="2556" citStr="Breck et al. (2007)" startWordPosition="371" endWordPosition="374">essing private states; and expressive subjective expressions (ESEs): expressions that indicate sentiment, emotion, etc. without explicitly conveying them. Following are two example sentences labeled with DSEs and ESEs. (1) The International Committee of the Red Cross, [as usual][ESE], [has refused to make any statements][DSE]. (2) The Chief Minister [said][DSE] that [the demon they have reared will eat up their own vitals][ESE]. As a type of information extraction task, opinion expression extraction has been successfully tackled in the past via sequence tagging methods: Choi et al. (2006) and Breck et al. (2007), for example, apply conditional random fields (CRFs) (Lafferty et al., 2001) using sophisticated token-level features. In token-level sequence labeling, labels are assigned to single tokens, and the label of each token depends on the current token and the label of the previous token (we consider the usual first-order assumption). Segment-based features — features that describe a set of related contiguous tokens, e.g., a phrase or constituent — might provide critical information for identifying opinion expressions; they cannot, however, be readily and naturally represented in the CRF model. 13</context>
<context position="5269" citStr="Breck et al. (2007)" startWordPosition="778" endWordPosition="781">e evaluate our model on two opinion extraction tasks: identifying direct subjective expressions (DSEs) and expressive subjective expressions (ESEs). Experimental results show that our approach outperforms the state-of-the-art approach for the task by a large margin. We also identify useful syntactic features for the task. 2 Related Work Previous research to extract direct subjective expressions exists, but is mainly focused on singleword expressions (Wiebe et al., 2005; Wilson et al., 2005; Munson et al., 2005). More recent studies tackle opinion expression extraction at the expression level. Breck et al. (2007) formulate the problem as a token-level sequence labeling problem; their CRF-based approach was shown to significantly outperform two subjectivity-clue-based baselines. Others extend the token-level approach to jointly identify opinion holders (Choi et al., 2006), and to determine the polarity and intensity of the opinion expressions (Choi and Cardie, 2010). Reranking the output of a simple sequence labeler has been shown to further improve the extraction of opinion expressions (Johansson and Moschitti, 2010; Johansson and Moschitti, 2011); importantly, their reranking approach relied on featu</context>
<context position="7643" citStr="Breck et al. (2007)" startWordPosition="1141" endWordPosition="1144">mployed dependency path features for the extraction of opinion targets. Johansson and Moschitti (2010; Johansson and Moschitti (2011) also successfully employed syntactic features that indicate dependency relations between opinion expressions for the task of opinion expression extraction. However, as their approach is based on the output of a sequence labeler, these features cannot be encoded to help the learning of the sequence labeler. 3 Approach We formulate the extraction of opinion expressions as a sequence labeling problem. Unlike previous sequence-labeling approaches to the task (e.g., Breck et al. (2007)), however, we aim to model segment-level, rather than token-level, information. As a result, we explore the use of semi-CRFs, which 1336 can assign labels to segments instead of tokens; hence, features can be defined at the segment level. For example, features like QX is a verb phrase� can be easily encoded in the model. In the following subsections, we first introduce standard semi-CRFs and then describe our semi-CRF-based approach for opinion expression extraction. 3.1 Semi-CRFs In semi-CRFs, each observed sentence x is represented as a sequence of consecutive segments s = (s1, ..., sr,,), </context>
<context position="17084" citStr="Breck et al., 2007" startWordPosition="2765" endWordPosition="2768">o include new segment-level features that can be naturally represented in semi-CRFs but not CRFs. For CRF-style features, we consider the string representation of the current word, its part-ofspeech, and a dictionary-derived feature, which is based on a subjectivity lexicon provided by Wilson et al. (2005). The lexicon consists of a set of words that can act as strong or weak cues to subjectivity. If the current word appears as an entry in the lexicon, then a feature strong or weak will be fired if the entry is of that strength. These features have been successfully employed in previous work (Breck et al., 2007). To employ them in our model, we simply extend the feature definition to the segment level. For example, a token-level feature Qx is great � will be extended to a segment-level feature Qs contains great �. Previous work on semi-CRFs has explored features such as the length of the segment, the position of the segment in the current segmentation (at the beginning or at the end), indicators for the start word and end word within the segment, and indicators for words before and after the segment. These features have been shown useful for the task of NE recognition (Sarawagi and Cohen, 2004; Okano</context>
<context position="21834" citStr="Breck et al. (2007)" startWordPosition="3565" endWordPosition="3568">1 (%) 43.38 71.65 Length &gt; 4 (%) 9.44 35.01 Table 1: Statistics of opinion expressions in the MPQA Corpus. 4.1 Evaluation Metrics We use precision, recall, and F-measure to evaluate the quality of the model. Precision is defined as |C∩P| |P |and recall, as|C∩P| |C |, where C and P are the sets of correct and predicted expression spans, respectively. F-measure is computed as �P R P �R. Because the boundaries of opinion expressions are hard to define even for human annotators (Wiebe et al., 2005), previous research mainly focused on soft precision and recall measures for performance evaluation. Breck et al. (2007) introduced an overlap measure, which considers a predicted expression to be correct if it overlaps with a correct expression. We refer to this metric as Binary Overlap. Johansson and Moschitti (2010) provides a stricter measure that computes the proportion of overlapping spans: if a correct expression s overlaps with a predicted expression s0, the overlap contributes value |s∩s�| |s� |to |C n P |instead of value 1. We refer to this metric as Proportional Overlap. To compare with previous work, we present our results according to both metrics. 4.2 Baseline Methods As a baseline, we use the tok</context>
<context position="25752" citStr="Breck et al. (2007)" startWordPosition="4171" endWordPosition="4174">wagi and Cohen, 2004). To the best of our knowledge, our work is the first to explore the use of semi-CRFs on the extraction of opinion expressions. They are considered to be more powerful than CRFs since they allow information to be represented at the expression level. The model requires an input of the maximum entity length. We set it to 15 for DSE and 40 for ESE. For segment features, we used the same features as in our approach (see Section 3.3). 4.3 Results Table 2 and Table 3 show the results of DSE and ESE extraction using two different metrics. The standard token-based CRF baseline of Breck et al. (2007) is labeled CRF; the original semi-CRF baseline is labeled semi-CRF; and our extended semiCRF approach is labeled new-semi-CRF. For semiCRF and new-semi-CRF, the results were obtained using two different settings of features: the basic feature set includes features described in Section 3.3 excluding the segment-level syntactic features. In the second feature setting (labeled as w/ syn in the tables), we further augment the basic features with the syntactic parse features. Using the basic features, we observe that semi-CRF-based approaches significantly outperform CRF and its two variants segme</context>
<context position="30077" citStr="Breck et al. (2007)" startWordPosition="4827" endWordPosition="4830">oticeably from 67.72% to 70.12% while the recall slightly improves. This indicates that VP-related structural information is very helpful for modeling segments as DSEs. However, this trend is not so clear for ESE extraction. This may be due to the fact that DSEs often involve verb phrases while ESEs are represented via a variety of syntactic structures. Comparison with previous work. In Table 5, we compare our results to the previous work on opinion expression extraction (here we also focus on the Binary Overlap metric due to the similar trend demonstrated by the Proportional Overlap metric). Breck et al. (2007) presents the state-of-the-art sequence labeling approach on the tasks of DSE and ESE extraction. Their best results are shown as Breck et al. Baseline in the table. Johansson and Moschitti (2010) use a reranking technique on the best k outputs of a sequence labeler to further improve their sequence labeling results on the task of extracting DSEs, ESEs and OSEs (Objective Speech Events) (we don’t consider OSEs here). Results using our re-implementation of their approach using SVM&amp;quot;&amp;quot;&apos; (Tsochantaridis et al., 2004) on the output of CRF are labeled CRF+Reranking Baseline in the table. We use the s</context>
</contexts>
<marker>Breck, Choi, Cardie, 2007</marker>
<rawString>Eric Breck, Yejin Choi, and Claire Cardie. 2007. Identifying expressions of opinion in context. IJCAI’07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yejin Choi</author>
<author>Claire Cardie</author>
<author>Ellen Riloff</author>
<author>Siddharth Patwardhan</author>
</authors>
<title>Identifying sources of opinions with conditional random fields and extraction patterns.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT ’05.</booktitle>
<contexts>
<context position="6992" citStr="Choi et al., 2005" startWordPosition="1042" endWordPosition="1045">pression extraction is known to be harder than traditional NER since subjective expressions exhibit substantial lexical variation and their recognition requires more attention to linguistic structure. Parsing has been leveraged to improve performance for numerous natural language tasks. In opinion mining, numerous studies have shown that syntactic parsing features are very helpful for opinion analysis. A lot of work uses syntactic features to identify opinion holders and opinion topics (Bethard et al., 2005; Kim and Hovy, 2006; Kobayashi et al., 2007; Joshi and Carolyn, 2009; Wu et al., 2009; Choi et al., 2005). Jakob et al. (2010) recently employed dependency path features for the extraction of opinion targets. Johansson and Moschitti (2010; Johansson and Moschitti (2011) also successfully employed syntactic features that indicate dependency relations between opinion expressions for the task of opinion expression extraction. However, as their approach is based on the output of a sequence labeler, these features cannot be encoded to help the learning of the sequence labeler. 3 Approach We formulate the extraction of opinion expressions as a sequence labeling problem. Unlike previous sequence-labelin</context>
</contexts>
<marker>Choi, Cardie, Riloff, Patwardhan, 2005</marker>
<rawString>Yejin Choi, Claire Cardie, Ellen Riloff, and Siddharth Patwardhan. 2005. Identifying sources of opinions with conditional random fields and extraction patterns. In Proceedings of HLT ’05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yejin Choi</author>
<author>Eric Breck</author>
<author>Claire Cardie</author>
</authors>
<title>Joint extraction of entities and relations for opinion recognition.</title>
<date>2006</date>
<booktitle>In Proceedings of EMNLP ’06.</booktitle>
<contexts>
<context position="2532" citStr="Choi et al. (2006)" startWordPosition="366" endWordPosition="369">s or speech events expressing private states; and expressive subjective expressions (ESEs): expressions that indicate sentiment, emotion, etc. without explicitly conveying them. Following are two example sentences labeled with DSEs and ESEs. (1) The International Committee of the Red Cross, [as usual][ESE], [has refused to make any statements][DSE]. (2) The Chief Minister [said][DSE] that [the demon they have reared will eat up their own vitals][ESE]. As a type of information extraction task, opinion expression extraction has been successfully tackled in the past via sequence tagging methods: Choi et al. (2006) and Breck et al. (2007), for example, apply conditional random fields (CRFs) (Lafferty et al., 2001) using sophisticated token-level features. In token-level sequence labeling, labels are assigned to single tokens, and the label of each token depends on the current token and the label of the previous token (we consider the usual first-order assumption). Segment-based features — features that describe a set of related contiguous tokens, e.g., a phrase or constituent — might provide critical information for identifying opinion expressions; they cannot, however, be readily and naturally represen</context>
<context position="5532" citStr="Choi et al., 2006" startWordPosition="814" endWordPosition="817">n. We also identify useful syntactic features for the task. 2 Related Work Previous research to extract direct subjective expressions exists, but is mainly focused on singleword expressions (Wiebe et al., 2005; Wilson et al., 2005; Munson et al., 2005). More recent studies tackle opinion expression extraction at the expression level. Breck et al. (2007) formulate the problem as a token-level sequence labeling problem; their CRF-based approach was shown to significantly outperform two subjectivity-clue-based baselines. Others extend the token-level approach to jointly identify opinion holders (Choi et al., 2006), and to determine the polarity and intensity of the opinion expressions (Choi and Cardie, 2010). Reranking the output of a simple sequence labeler has been shown to further improve the extraction of opinion expressions (Johansson and Moschitti, 2010; Johansson and Moschitti, 2011); importantly, their reranking approach relied on features that encoded syntactic structure. All of the above approaches, however, are based on token-level sequence labeling, which ignores potentially useful phrase-level information. Semi-CRFs (Sarawagi and Cohen, 2004) are general CRFs that relax the Markovian assum</context>
</contexts>
<marker>Choi, Breck, Cardie, 2006</marker>
<rawString>Yejin Choi, Eric Breck, and Claire Cardie. 2006. Joint extraction of entities and relations for opinion recognition. In Proceedings of EMNLP ’06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yejin Choi</author>
<author>Claire Cardie</author>
</authors>
<title>Hierarchical sequential learning for extracting opinions and their attributes.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL</booktitle>
<institution>Short Papers.</institution>
<contexts>
<context position="5628" citStr="Choi and Cardie, 2010" startWordPosition="830" endWordPosition="833"> to extract direct subjective expressions exists, but is mainly focused on singleword expressions (Wiebe et al., 2005; Wilson et al., 2005; Munson et al., 2005). More recent studies tackle opinion expression extraction at the expression level. Breck et al. (2007) formulate the problem as a token-level sequence labeling problem; their CRF-based approach was shown to significantly outperform two subjectivity-clue-based baselines. Others extend the token-level approach to jointly identify opinion holders (Choi et al., 2006), and to determine the polarity and intensity of the opinion expressions (Choi and Cardie, 2010). Reranking the output of a simple sequence labeler has been shown to further improve the extraction of opinion expressions (Johansson and Moschitti, 2010; Johansson and Moschitti, 2011); importantly, their reranking approach relied on features that encoded syntactic structure. All of the above approaches, however, are based on token-level sequence labeling, which ignores potentially useful phrase-level information. Semi-CRFs (Sarawagi and Cohen, 2004) are general CRFs that relax the Markovian assumptions to allow sequence labeling at the segment level. Previous work has shown that semi-CRFs a</context>
</contexts>
<marker>Choi, Cardie, 2010</marker>
<rawString>Yejin Choi and Claire Cardie. 2010. Hierarchical sequential learning for extracting opinions and their attributes. In Proceedings of ACL 2010, Short Papers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Johansson</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Syntactic and semantic structure for opinion expression detection.</title>
<date>2010</date>
<booktitle>In Proceedings of CoNLL ’10.</booktitle>
<contexts>
<context position="5782" citStr="Johansson and Moschitti, 2010" startWordPosition="854" endWordPosition="858">et al., 2005). More recent studies tackle opinion expression extraction at the expression level. Breck et al. (2007) formulate the problem as a token-level sequence labeling problem; their CRF-based approach was shown to significantly outperform two subjectivity-clue-based baselines. Others extend the token-level approach to jointly identify opinion holders (Choi et al., 2006), and to determine the polarity and intensity of the opinion expressions (Choi and Cardie, 2010). Reranking the output of a simple sequence labeler has been shown to further improve the extraction of opinion expressions (Johansson and Moschitti, 2010; Johansson and Moschitti, 2011); importantly, their reranking approach relied on features that encoded syntactic structure. All of the above approaches, however, are based on token-level sequence labeling, which ignores potentially useful phrase-level information. Semi-CRFs (Sarawagi and Cohen, 2004) are general CRFs that relax the Markovian assumptions to allow sequence labeling at the segment level. Previous work has shown that semi-CRFs are superior to CRFs for NER and Chinese word segmentation (Sarawagi and Cohen, 2004; Okanohara et al., 2006; Andrew, 2006). The task of opinion expression</context>
<context position="7125" citStr="Johansson and Moschitti (2010" startWordPosition="1062" endWordPosition="1065">riation and their recognition requires more attention to linguistic structure. Parsing has been leveraged to improve performance for numerous natural language tasks. In opinion mining, numerous studies have shown that syntactic parsing features are very helpful for opinion analysis. A lot of work uses syntactic features to identify opinion holders and opinion topics (Bethard et al., 2005; Kim and Hovy, 2006; Kobayashi et al., 2007; Joshi and Carolyn, 2009; Wu et al., 2009; Choi et al., 2005). Jakob et al. (2010) recently employed dependency path features for the extraction of opinion targets. Johansson and Moschitti (2010; Johansson and Moschitti (2011) also successfully employed syntactic features that indicate dependency relations between opinion expressions for the task of opinion expression extraction. However, as their approach is based on the output of a sequence labeler, these features cannot be encoded to help the learning of the sequence labeler. 3 Approach We formulate the extraction of opinion expressions as a sequence labeling problem. Unlike previous sequence-labeling approaches to the task (e.g., Breck et al. (2007)), however, we aim to model segment-level, rather than token-level, information. A</context>
<context position="22034" citStr="Johansson and Moschitti (2010)" startWordPosition="3596" endWordPosition="3600">lity of the model. Precision is defined as |C∩P| |P |and recall, as|C∩P| |C |, where C and P are the sets of correct and predicted expression spans, respectively. F-measure is computed as �P R P �R. Because the boundaries of opinion expressions are hard to define even for human annotators (Wiebe et al., 2005), previous research mainly focused on soft precision and recall measures for performance evaluation. Breck et al. (2007) introduced an overlap measure, which considers a predicted expression to be correct if it overlaps with a correct expression. We refer to this metric as Binary Overlap. Johansson and Moschitti (2010) provides a stricter measure that computes the proportion of overlapping spans: if a correct expression s overlaps with a predicted expression s0, the overlap contributes value |s∩s�| |s� |to |C n P |instead of value 1. We refer to this metric as Proportional Overlap. To compare with previous work, we present our results according to both metrics. 4.2 Baseline Methods As a baseline, we use the token-level CRF-based approach of Breck et al. (2007) applied to the MPQA dataset. We employ a very similar, but not identical set of features: indicators for specific words at the current location and n</context>
<context position="30273" citStr="Johansson and Moschitti (2010)" startWordPosition="4859" endWordPosition="4863">end is not so clear for ESE extraction. This may be due to the fact that DSEs often involve verb phrases while ESEs are represented via a variety of syntactic structures. Comparison with previous work. In Table 5, we compare our results to the previous work on opinion expression extraction (here we also focus on the Binary Overlap metric due to the similar trend demonstrated by the Proportional Overlap metric). Breck et al. (2007) presents the state-of-the-art sequence labeling approach on the tasks of DSE and ESE extraction. Their best results are shown as Breck et al. Baseline in the table. Johansson and Moschitti (2010) use a reranking technique on the best k outputs of a sequence labeler to further improve their sequence labeling results on the task of extracting DSEs, ESEs and OSEs (Objective Speech Events) (we don’t consider OSEs here). Results using our re-implementation of their approach using SVM&amp;quot;&amp;quot;&apos; (Tsochantaridis et al., 2004) on the output of CRF are labeled CRF+Reranking Baseline in the table. We use the same features and 1342 parameter settings as in their approach. Our approach+Reranking are results obtained by applying the reranking step on the output of our newsemi-CRF approach. We can see that</context>
</contexts>
<marker>Johansson, Moschitti, 2010</marker>
<rawString>Richard Johansson and Alessandro Moschitti. 2010. Syntactic and semantic structure for opinion expression detection. In Proceedings of CoNLL ’10.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Niklas Jakob</author>
<author>Iryna Gurevych</author>
</authors>
<title>Extracting opinion targets in a single- and cross-domain setting with conditional random fields.</title>
<booktitle>In Proceedings of EMNLP’</booktitle>
<pages>10</pages>
<marker>Jakob, Gurevych, </marker>
<rawString>Niklas Jakob and Iryna Gurevych. Extracting opinion targets in a single- and cross-domain setting with conditional random fields. In Proceedings of EMNLP’ 10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mahesh Joshi</author>
<author>Penstein-Ros’e Carolyn</author>
</authors>
<title>Generalizing dependency features for opinion mining.</title>
<date>2009</date>
<booktitle>In Proceedings of ACL/IJCNLP 2009, Short Papers Track.</booktitle>
<contexts>
<context position="6955" citStr="Joshi and Carolyn, 2009" startWordPosition="1034" endWordPosition="1037">006; Andrew, 2006). The task of opinion expression extraction is known to be harder than traditional NER since subjective expressions exhibit substantial lexical variation and their recognition requires more attention to linguistic structure. Parsing has been leveraged to improve performance for numerous natural language tasks. In opinion mining, numerous studies have shown that syntactic parsing features are very helpful for opinion analysis. A lot of work uses syntactic features to identify opinion holders and opinion topics (Bethard et al., 2005; Kim and Hovy, 2006; Kobayashi et al., 2007; Joshi and Carolyn, 2009; Wu et al., 2009; Choi et al., 2005). Jakob et al. (2010) recently employed dependency path features for the extraction of opinion targets. Johansson and Moschitti (2010; Johansson and Moschitti (2011) also successfully employed syntactic features that indicate dependency relations between opinion expressions for the task of opinion expression extraction. However, as their approach is based on the output of a sequence labeler, these features cannot be encoded to help the learning of the sequence labeler. 3 Approach We formulate the extraction of opinion expressions as a sequence labeling prob</context>
</contexts>
<marker>Joshi, Carolyn, 2009</marker>
<rawString>Mahesh Joshi and Penstein-Ros’e Carolyn. 2009. Generalizing dependency features for opinion mining. In Proceedings of ACL/IJCNLP 2009, Short Papers Track.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL ’03.</booktitle>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Accurate unlexicalized parsing. In Proceedings of ACL ’03.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Soo-Min Kim</author>
<author>Eduard Hovy</author>
</authors>
<title>Extracting opinions, opinion holders, and topics expressed in online news media text.</title>
<date>2006</date>
<booktitle>In Proceedings of the ACL Workshop on Sentiment and Subjectivity in Text.</booktitle>
<contexts>
<context position="6906" citStr="Kim and Hovy, 2006" startWordPosition="1026" endWordPosition="1029">arawagi and Cohen, 2004; Okanohara et al., 2006; Andrew, 2006). The task of opinion expression extraction is known to be harder than traditional NER since subjective expressions exhibit substantial lexical variation and their recognition requires more attention to linguistic structure. Parsing has been leveraged to improve performance for numerous natural language tasks. In opinion mining, numerous studies have shown that syntactic parsing features are very helpful for opinion analysis. A lot of work uses syntactic features to identify opinion holders and opinion topics (Bethard et al., 2005; Kim and Hovy, 2006; Kobayashi et al., 2007; Joshi and Carolyn, 2009; Wu et al., 2009; Choi et al., 2005). Jakob et al. (2010) recently employed dependency path features for the extraction of opinion targets. Johansson and Moschitti (2010; Johansson and Moschitti (2011) also successfully employed syntactic features that indicate dependency relations between opinion expressions for the task of opinion expression extraction. However, as their approach is based on the output of a sequence labeler, these features cannot be encoded to help the learning of the sequence labeler. 3 Approach We formulate the extraction o</context>
</contexts>
<marker>Kim, Hovy, 2006</marker>
<rawString>Soo-Min Kim and Eduard Hovy. 2006. Extracting opinions, opinion holders, and topics expressed in online news media text. In Proceedings of the ACL Workshop on Sentiment and Subjectivity in Text.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nozomi Kobayashi</author>
<author>Kentaro Inui</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Extracting aspect-evaluation and aspect-of relations in opinion mining.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLPCoNLL-2007.</booktitle>
<contexts>
<context position="6930" citStr="Kobayashi et al., 2007" startWordPosition="1030" endWordPosition="1033">004; Okanohara et al., 2006; Andrew, 2006). The task of opinion expression extraction is known to be harder than traditional NER since subjective expressions exhibit substantial lexical variation and their recognition requires more attention to linguistic structure. Parsing has been leveraged to improve performance for numerous natural language tasks. In opinion mining, numerous studies have shown that syntactic parsing features are very helpful for opinion analysis. A lot of work uses syntactic features to identify opinion holders and opinion topics (Bethard et al., 2005; Kim and Hovy, 2006; Kobayashi et al., 2007; Joshi and Carolyn, 2009; Wu et al., 2009; Choi et al., 2005). Jakob et al. (2010) recently employed dependency path features for the extraction of opinion targets. Johansson and Moschitti (2010; Johansson and Moschitti (2011) also successfully employed syntactic features that indicate dependency relations between opinion expressions for the task of opinion expression extraction. However, as their approach is based on the output of a sequence labeler, these features cannot be encoded to help the learning of the sequence labeler. 3 Approach We formulate the extraction of opinion expressions as</context>
</contexts>
<marker>Kobayashi, Inui, Matsumoto, 2007</marker>
<rawString>Nozomi Kobayashi, Kentaro Inui, and Yuji Matsumoto. 2007. Extracting aspect-evaluation and aspect-of relations in opinion mining. In Proceedings of EMNLPCoNLL-2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John D Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando C N Pereira</author>
</authors>
<title>Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data.</title>
<date>2001</date>
<booktitle>In Proceedings of ICML ’01.</booktitle>
<contexts>
<context position="2633" citStr="Lafferty et al., 2001" startWordPosition="383" endWordPosition="386">ssions that indicate sentiment, emotion, etc. without explicitly conveying them. Following are two example sentences labeled with DSEs and ESEs. (1) The International Committee of the Red Cross, [as usual][ESE], [has refused to make any statements][DSE]. (2) The Chief Minister [said][DSE] that [the demon they have reared will eat up their own vitals][ESE]. As a type of information extraction task, opinion expression extraction has been successfully tackled in the past via sequence tagging methods: Choi et al. (2006) and Breck et al. (2007), for example, apply conditional random fields (CRFs) (Lafferty et al., 2001) using sophisticated token-level features. In token-level sequence labeling, labels are assigned to single tokens, and the label of each token depends on the current token and the label of the previous token (we consider the usual first-order assumption). Segment-based features — features that describe a set of related contiguous tokens, e.g., a phrase or constituent — might provide critical information for identifying opinion expressions; they cannot, however, be readily and naturally represented in the CRF model. 1335 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural L</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John D. Lafferty, Andrew McCallum, and Fernando C. N. Pereira. 2001. Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data. In Proceedings of ICML ’01.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dong C Liu</author>
<author>Jorge Nocedal</author>
</authors>
<title>On the limited memory BFGS method for large scale optimization.</title>
<date>1989</date>
<journal>Mathematical Programming B</journal>
<volume>45</volume>
<issue>3</issue>
<pages>503--528</pages>
<contexts>
<context position="15120" citStr="Liu and Nocedal, 1989" startWordPosition="2424" endWordPosition="2427">rrespond to segment units instead of single-word segments in the original semi-CRF model.2 After obtaining the set of possible segment candidates and the correct segmentation s for each training sentence, the semi-CRF model can be trained. The goal of learning is to find the optimal parameter A by maximizing log-likelihood. We use the limited2There are cases where words within a segment unit have different labels. This may be due to errors by the human annotators or the errors in the parser. In such cases, we consider each word within the segment unit as a segment. 1338 memory BFGS algorithm (Liu and Nocedal, 1989) for optimization in our implementation, where the gradient of the log-likelihood L (corresponding to one instance x) is computed: gk(x, ti, ui, yi, yi−1) Xgk(x, t0j, u0j, y, y0)p(y, y0|x) j (2) where S is all possible segmentations consisting of the generated segment candidates, p(y, y0|x) is the probability of having label y for the current segment s0j (with boundary (t0j, u0j)) and label y0 for the previous segment s0j−1. We use a forward-backward algorithm to compute the marginal distribution p(y, y0|x) and the normalization factor Z(x) efficiently. For inference we seek the best segmentat</context>
</contexts>
<marker>Liu, Nocedal, 1989</marker>
<rawString>Dong C. Liu and Jorge Nocedal. 1989. On the limited memory BFGS method for large scale optimization. Mathematical Programming B 45(3): 503-528.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Arthur Munson</author>
<author>Claire Cardie</author>
<author>Rich Caruana</author>
</authors>
<title>Optimizing to arbitrary NLP metrics using ensemble selection.</title>
<date>2005</date>
<booktitle>In HLT-EMNLP05,</booktitle>
<contexts>
<context position="5166" citStr="Munson et al., 2005" startWordPosition="761" endWordPosition="764">tactic structure. We also explore the impact of syntactic features for extracting opinion expressions. We evaluate our model on two opinion extraction tasks: identifying direct subjective expressions (DSEs) and expressive subjective expressions (ESEs). Experimental results show that our approach outperforms the state-of-the-art approach for the task by a large margin. We also identify useful syntactic features for the task. 2 Related Work Previous research to extract direct subjective expressions exists, but is mainly focused on singleword expressions (Wiebe et al., 2005; Wilson et al., 2005; Munson et al., 2005). More recent studies tackle opinion expression extraction at the expression level. Breck et al. (2007) formulate the problem as a token-level sequence labeling problem; their CRF-based approach was shown to significantly outperform two subjectivity-clue-based baselines. Others extend the token-level approach to jointly identify opinion holders (Choi et al., 2006), and to determine the polarity and intensity of the opinion expressions (Choi and Cardie, 2010). Reranking the output of a simple sequence labeler has been shown to further improve the extraction of opinion expressions (Johansson and</context>
</contexts>
<marker>Munson, Cardie, Caruana, 2005</marker>
<rawString>M Arthur Munson, Claire Cardie, and Rich Caruana. Optimizing to arbitrary NLP metrics using ensemble selection. In HLT-EMNLP05, 2005.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Daisuke Okanohara</author>
<author>Yusuke Miyao</author>
<author>Yoshimasa Tsuruoka</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Improving the scalability of semi-Markov conditional random fields for named entity recognition.</title>
<booktitle>In Proceedings of ACL’06.</booktitle>
<marker>Okanohara, Miyao, Tsuruoka, Tsujii, </marker>
<rawString>Daisuke Okanohara, Yusuke Miyao, Yoshimasa Tsuruoka, and Jun’ichi Tsujii. Improving the scalability of semi-Markov conditional random fields for named entity recognition. In Proceedings of ACL’06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Randolph Quirk</author>
<author>Sidney Greenbaum</author>
<author>Geoffrey Leech</author>
<author>Jan Svartvik</author>
</authors>
<title>A comprehensive grammar of the English language.</title>
<date>1985</date>
<location>New York: Longman,</location>
<contexts>
<context position="1818" citStr="Quirk et al., 1985" startWordPosition="261" endWordPosition="264">n identification is crucial for tasks that benefit from fine-grained opinion analysis (Wiebe et al., 2005): e.g., it is a first step in characterizing the sentiment and intensity of the opinion; it provides a textual anchor for identifying the opinion holder and the target or topic of an opinion; and these, in turn, form the basis of opinionoriented question answering and opinion summarization systems. In this paper, we focus on opinion expressions as defined in Wiebe et al. (2005) — subjective expressions that denote emotions, sentiment, beliefs, opinions, judgments, or other private states (Quirk et al., 1985) in text. These include direct subjective expressions (DSEs): explicit mentions of private states or speech events expressing private states; and expressive subjective expressions (ESEs): expressions that indicate sentiment, emotion, etc. without explicitly conveying them. Following are two example sentences labeled with DSEs and ESEs. (1) The International Committee of the Red Cross, [as usual][ESE], [has refused to make any statements][DSE]. (2) The Chief Minister [said][DSE] that [the demon they have reared will eat up their own vitals][ESE]. As a type of information extraction task, opinio</context>
</contexts>
<marker>Quirk, Greenbaum, Leech, Svartvik, 1985</marker>
<rawString>Randolph Quirk, Sidney Greenbaum, Geoffrey Leech, and Jan Svartvik. A comprehensive grammar of the English language. New York: Longman, 1985.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Richard Johansson</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Extracting Opinion Expressions and Their Polarities - Exploration of Pipelines and Joint Models.</title>
<booktitle>In Proceedings of ACL ’11, Short Paper.</booktitle>
<marker>Johansson, Moschitti, </marker>
<rawString>Richard Johansson and Alessandro Moschitti. Extracting Opinion Expressions and Their Polarities - Exploration of Pipelines and Joint Models. In Proceedings of ACL ’11, Short Paper.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Riloff</author>
<author>Janyce M Wiebe</author>
</authors>
<title>Learning extraction patterns for subjective expressions.</title>
<date>2003</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<marker>Riloff, Wiebe, 2003</marker>
<rawString>Ellen Riloff and Janyce M Wiebe. Learning extraction patterns for subjective expressions. In Proceedings of EMNLP 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sunita Sarawagi</author>
<author>William W Cohen</author>
</authors>
<title>SemiMarkov Conditional Random Fields for Information Extraction.</title>
<date>2004</date>
<booktitle>In Proceedings of NIPS</booktitle>
<contexts>
<context position="699" citStr="Sarawagi and Cohen, 2004" startWordPosition="88" endWordPosition="92">Bishan Yang Department of Computer Science Cornell University bishan@cs.cornell.edu Claire Cardie Department of Computer Science Cornell University cardie@cs.cornell.edu Abstract Extracting opinion expressions from text is usually formulated as a token-level sequence labeling task tackled using Conditional Random Fields (CRFs). CRFs, however, do not readily model potentially useful segment-level information like syntactic constituent structure. Thus, we propose a semi-CRF-based approach to the task that can perform sequence labeling at the segment level. We extend the original semi-CRF model (Sarawagi and Cohen, 2004) to allow the modeling of arbitrarily long expressions while accounting for their likely syntactic structure when modeling segment boundaries. We evaluate performance on two opinion extraction tasks, and, in contrast to previous sequence labeling approaches to the task, explore the usefulness of segmentlevel syntactic parse features. Experimental results demonstrate that our approach outperforms state-of-the-art methods for both opinion expression tasks. 1 Introduction Accurate opinion expression identification is crucial for tasks that benefit from fine-grained opinion analysis (Wiebe et al.,</context>
<context position="3569" citStr="Sarawagi and Cohen, 2004" startWordPosition="519" endWordPosition="522">f related contiguous tokens, e.g., a phrase or constituent — might provide critical information for identifying opinion expressions; they cannot, however, be readily and naturally represented in the CRF model. 1335 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1335–1345, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics Our goal in this work is to extract opinion expressions at the segment level with semi-Markov conditional random fields (semi-CRFs). Semi-CRFs (Sarawagi and Cohen, 2004) are more powerful than CRFs in that they allow one to construct features to capture characteristics of the subsequences of a sentence. They are defined on semi-Markov chains where labels are attached to segments instead of tokens and label dependencies are modeled at the segment-level. Previous work has shown that semiCRFs outperform CRFs on named entity recognition (NER) tasks (Sarawagi and Cohen, 2004; Okanohara et al., 2006). However, to the best of our knowledge, semi-CRF techniques have not been investigated for opinion expression extraction. The contribution of this paper is a semi-CRFb</context>
<context position="6084" citStr="Sarawagi and Cohen, 2004" startWordPosition="895" endWordPosition="898">en-level approach to jointly identify opinion holders (Choi et al., 2006), and to determine the polarity and intensity of the opinion expressions (Choi and Cardie, 2010). Reranking the output of a simple sequence labeler has been shown to further improve the extraction of opinion expressions (Johansson and Moschitti, 2010; Johansson and Moschitti, 2011); importantly, their reranking approach relied on features that encoded syntactic structure. All of the above approaches, however, are based on token-level sequence labeling, which ignores potentially useful phrase-level information. Semi-CRFs (Sarawagi and Cohen, 2004) are general CRFs that relax the Markovian assumptions to allow sequence labeling at the segment level. Previous work has shown that semi-CRFs are superior to CRFs for NER and Chinese word segmentation (Sarawagi and Cohen, 2004; Okanohara et al., 2006; Andrew, 2006). The task of opinion expression extraction is known to be harder than traditional NER since subjective expressions exhibit substantial lexical variation and their recognition requires more attention to linguistic structure. Parsing has been leveraged to improve performance for numerous natural language tasks. In opinion mining, num</context>
<context position="17677" citStr="Sarawagi and Cohen, 2004" startWordPosition="2871" endWordPosition="2874">vious work (Breck et al., 2007). To employ them in our model, we simply extend the feature definition to the segment level. For example, a token-level feature Qx is great � will be extended to a segment-level feature Qs contains great �. Previous work on semi-CRFs has explored features such as the length of the segment, the position of the segment in the current segmentation (at the beginning or at the end), indicators for the start word and end word within the segment, and indicators for words before and after the segment. These features have been shown useful for the task of NE recognition (Sarawagi and Cohen, 2004; Okanohara et al., 2006). However, we only found the position of the segment to be helpful for the extraction of opinion expressions, probably due to the lack of patterns in the length distribution and word choices of opinion expressions. Besides the above features, we design new segment-level syntactic features to capture the syntactic patterns of opinion expressions. Syntactic patterns are often used to identify useful information in information extraction tasks. In our task, we found that the majority of opinion expressions involve verb phrases.3 For example, “was encouraged”, “expressed g</context>
<context position="25154" citStr="Sarawagi and Cohen, 2004" startWordPosition="4061" endWordPosition="4064">s above. tion. Segment-CRF treats segment units obtained from the parser as word tokens. For example, in Figure 1, the segment units the statement and both ridiculous and odd will be treated as word tokens. Syntactic-CRF encodes segment-level syntactic information in a standard token-level CRF as input features. We consider the VP-related segment features introduced in Section 3.3. VPPRE and VPARG are added to the head word of the corresponding verb phrase, and VPSUBJ and VPCLUSTER are added to each token within the corresponding segment. Another baseline method is the original semiCRF model (Sarawagi and Cohen, 2004). To the best of our knowledge, our work is the first to explore the use of semi-CRFs on the extraction of opinion expressions. They are considered to be more powerful than CRFs since they allow information to be represented at the expression level. The model requires an input of the maximum entity length. We set it to 15 for DSE and 40 for ESE. For segment features, we used the same features as in our approach (see Section 3.3). 4.3 Results Table 2 and Table 3 show the results of DSE and ESE extraction using two different metrics. The standard token-based CRF baseline of Breck et al. (2007) i</context>
</contexts>
<marker>Sarawagi, Cohen, 2004</marker>
<rawString>Sunita Sarawagi and William W. Cohen. 2004. SemiMarkov Conditional Random Fields for Information Extraction. In Proceedings of NIPS 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Sutton</author>
<author>Andrew McCallum</author>
</authors>
<title>An Introduction to Conditional Random Fields. Foundations and Trends</title>
<date>2010</date>
<booktitle>in Machine Learning (FnT ML),</booktitle>
<marker>Sutton, McCallum, 2010</marker>
<rawString>Charles Sutton and Andrew McCallum. An Introduction to Conditional Random Fields. Foundations and Trends in Machine Learning (FnT ML), 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce Wiebe</author>
<author>Theresa Wilson</author>
<author>Claire Cardie</author>
</authors>
<title>Annotating expressions of opinions and emotions in language.</title>
<date>2005</date>
<journal>Language Resources and Evaluation,</journal>
<volume>39</volume>
<pages>2--3</pages>
<contexts>
<context position="1305" citStr="Wiebe et al., 2005" startWordPosition="175" endWordPosition="178"> Cohen, 2004) to allow the modeling of arbitrarily long expressions while accounting for their likely syntactic structure when modeling segment boundaries. We evaluate performance on two opinion extraction tasks, and, in contrast to previous sequence labeling approaches to the task, explore the usefulness of segmentlevel syntactic parse features. Experimental results demonstrate that our approach outperforms state-of-the-art methods for both opinion expression tasks. 1 Introduction Accurate opinion expression identification is crucial for tasks that benefit from fine-grained opinion analysis (Wiebe et al., 2005): e.g., it is a first step in characterizing the sentiment and intensity of the opinion; it provides a textual anchor for identifying the opinion holder and the target or topic of an opinion; and these, in turn, form the basis of opinionoriented question answering and opinion summarization systems. In this paper, we focus on opinion expressions as defined in Wiebe et al. (2005) — subjective expressions that denote emotions, sentiment, beliefs, opinions, judgments, or other private states (Quirk et al., 1985) in text. These include direct subjective expressions (DSEs): explicit mentions of priv</context>
<context position="5123" citStr="Wiebe et al., 2005" startWordPosition="753" endWordPosition="756">ndaries can be influenced by probable syntactic structure. We also explore the impact of syntactic features for extracting opinion expressions. We evaluate our model on two opinion extraction tasks: identifying direct subjective expressions (DSEs) and expressive subjective expressions (ESEs). Experimental results show that our approach outperforms the state-of-the-art approach for the task by a large margin. We also identify useful syntactic features for the task. 2 Related Work Previous research to extract direct subjective expressions exists, but is mainly focused on singleword expressions (Wiebe et al., 2005; Wilson et al., 2005; Munson et al., 2005). More recent studies tackle opinion expression extraction at the expression level. Breck et al. (2007) formulate the problem as a token-level sequence labeling problem; their CRF-based approach was shown to significantly outperform two subjectivity-clue-based baselines. Others extend the token-level approach to jointly identify opinion holders (Choi et al., 2006), and to determine the polarity and intensity of the opinion expressions (Choi and Cardie, 2010). Reranking the output of a simple sequence labeler has been shown to further improve the extra</context>
<context position="20437" citStr="Wiebe et al., 2005" startWordPosition="3334" endWordPosition="3337">PLEAF. For example, the noun phrase “a negative stand” is the argument of the predicate “take” in the verb phrase “take a negative stand”. The argument in the verb phrase (could be a noun phrase, adjectival phrase or prepositional phrase) may convey some relevant information for identifying opinion expressions. VPsubj: Whether the verb clusters or the argument in the segment contains an entry from the subjectivity lexicon. For example, the word “negative” is in the lexicon, so the segment “take a negative stand” has a feature ISVPSUBJ. 4 Experiments For evaluation, we use the MPQA 1.2 corpus (Wiebe et al., 2005)5, a widely used data set for fine-grained opinion analysis. It contains 535 news articles, a total of 11,114 sentences with subjectivity-related annotations at the phrase level. We focus on the task of extracting two types of opinion expressions: direct subjective expressions (DSEs) and expressive subjective expressions (ESEs). Table 1 shows some statistics of the corpus. As in prior research that uses the corpus, we set aside the standard 135 documents as a development set and use 400 documents as the evaluation set. All experiments employ 10- fold cross validation on the evaluation set, and</context>
<context position="21714" citStr="Wiebe et al., 2005" startWordPosition="3546" endWordPosition="3549">://www.cs.pitt.edu/mpqa/. DSEs ESEs Sentences with opinions(%) 55.89 57.93 TotalNum 9746 11730 MaxLength 15 40 Length &gt; 1 (%) 43.38 71.65 Length &gt; 4 (%) 9.44 35.01 Table 1: Statistics of opinion expressions in the MPQA Corpus. 4.1 Evaluation Metrics We use precision, recall, and F-measure to evaluate the quality of the model. Precision is defined as |C∩P| |P |and recall, as|C∩P| |C |, where C and P are the sets of correct and predicted expression spans, respectively. F-measure is computed as �P R P �R. Because the boundaries of opinion expressions are hard to define even for human annotators (Wiebe et al., 2005), previous research mainly focused on soft precision and recall measures for performance evaluation. Breck et al. (2007) introduced an overlap measure, which considers a predicted expression to be correct if it overlaps with a correct expression. We refer to this metric as Binary Overlap. Johansson and Moschitti (2010) provides a stricter measure that computes the proportion of overlapping spans: if a correct expression s overlaps with a predicted expression s0, the overlap contributes value |s∩s�| |s� |to |C n P |instead of value 1. We refer to this metric as Proportional Overlap. To compare </context>
</contexts>
<marker>Wiebe, Wilson, Cardie, 2005</marker>
<rawString>Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005. Annotating expressions of opinions and emotions in language. Language Resources and Evaluation, volume 39, issue 2-3, pp. 165-210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Paul Hoffmann</author>
</authors>
<title>Recognizing contextual polarity in phrase-level sentiment analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT ’05.</booktitle>
<contexts>
<context position="5144" citStr="Wilson et al., 2005" startWordPosition="757" endWordPosition="760">enced by probable syntactic structure. We also explore the impact of syntactic features for extracting opinion expressions. We evaluate our model on two opinion extraction tasks: identifying direct subjective expressions (DSEs) and expressive subjective expressions (ESEs). Experimental results show that our approach outperforms the state-of-the-art approach for the task by a large margin. We also identify useful syntactic features for the task. 2 Related Work Previous research to extract direct subjective expressions exists, but is mainly focused on singleword expressions (Wiebe et al., 2005; Wilson et al., 2005; Munson et al., 2005). More recent studies tackle opinion expression extraction at the expression level. Breck et al. (2007) formulate the problem as a token-level sequence labeling problem; their CRF-based approach was shown to significantly outperform two subjectivity-clue-based baselines. Others extend the token-level approach to jointly identify opinion holders (Choi et al., 2006), and to determine the polarity and intensity of the opinion expressions (Choi and Cardie, 2010). Reranking the output of a simple sequence labeler has been shown to further improve the extraction of opinion expr</context>
<context position="16772" citStr="Wilson et al. (2005)" startWordPosition="2706" endWordPosition="2709">f the generated segment candidates ending at position j. The best segmentation can be obtained from tracing the path of maxy V (n, y). 3.3 Features Here we described the features used in our model. Very generally, we include CRF-style features that are segment-level extensions of the token-level features. We also include new segment-level features that can be naturally represented in semi-CRFs but not CRFs. For CRF-style features, we consider the string representation of the current word, its part-ofspeech, and a dictionary-derived feature, which is based on a subjectivity lexicon provided by Wilson et al. (2005). The lexicon consists of a set of words that can act as strong or weak cues to subjectivity. If the current word appears as an entry in the lexicon, then a feature strong or weak will be fired if the entry is of that strength. These features have been successfully employed in previous work (Breck et al., 2007). To employ them in our model, we simply extend the feature definition to the segment level. For example, a token-level feature Qx is great � will be extended to a segment-level feature Qs contains great �. Previous work on semi-CRFs has explored features such as the length of the segmen</context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2005</marker>
<rawString>Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005. Recognizing contextual polarity in phrase-level sentiment analysis. In Proceedings of HLT ’05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Paul Hoffmann</author>
<author>Swapna Somasundaran</author>
<author>Jason Kessler</author>
<author>Janyce Wiebe</author>
<author>Yejin Choi</author>
<author>Claire Cardie</author>
<author>Ellen Riloff</author>
<author>Siddharth Patwardhan</author>
</authors>
<title>OpinionFinder: A system for subjectivity analysis. EMNLP</title>
<date>2005</date>
<note>Demo abstract.</note>
<contexts>
<context position="5144" citStr="Wilson et al., 2005" startWordPosition="757" endWordPosition="760">enced by probable syntactic structure. We also explore the impact of syntactic features for extracting opinion expressions. We evaluate our model on two opinion extraction tasks: identifying direct subjective expressions (DSEs) and expressive subjective expressions (ESEs). Experimental results show that our approach outperforms the state-of-the-art approach for the task by a large margin. We also identify useful syntactic features for the task. 2 Related Work Previous research to extract direct subjective expressions exists, but is mainly focused on singleword expressions (Wiebe et al., 2005; Wilson et al., 2005; Munson et al., 2005). More recent studies tackle opinion expression extraction at the expression level. Breck et al. (2007) formulate the problem as a token-level sequence labeling problem; their CRF-based approach was shown to significantly outperform two subjectivity-clue-based baselines. Others extend the token-level approach to jointly identify opinion holders (Choi et al., 2006), and to determine the polarity and intensity of the opinion expressions (Choi and Cardie, 2010). Reranking the output of a simple sequence labeler has been shown to further improve the extraction of opinion expr</context>
<context position="16772" citStr="Wilson et al. (2005)" startWordPosition="2706" endWordPosition="2709">f the generated segment candidates ending at position j. The best segmentation can be obtained from tracing the path of maxy V (n, y). 3.3 Features Here we described the features used in our model. Very generally, we include CRF-style features that are segment-level extensions of the token-level features. We also include new segment-level features that can be naturally represented in semi-CRFs but not CRFs. For CRF-style features, we consider the string representation of the current word, its part-ofspeech, and a dictionary-derived feature, which is based on a subjectivity lexicon provided by Wilson et al. (2005). The lexicon consists of a set of words that can act as strong or weak cues to subjectivity. If the current word appears as an entry in the lexicon, then a feature strong or weak will be fired if the entry is of that strength. These features have been successfully employed in previous work (Breck et al., 2007). To employ them in our model, we simply extend the feature definition to the segment level. For example, a token-level feature Qx is great � will be extended to a segment-level feature Qs contains great �. Previous work on semi-CRFs has explored features such as the length of the segmen</context>
</contexts>
<marker>Wilson, Hoffmann, Somasundaran, Kessler, Wiebe, Choi, Cardie, Riloff, Patwardhan, 2005</marker>
<rawString>Theresa Wilson, Paul Hoffmann, Swapna Somasundaran, Jason Kessler, Janyce Wiebe, Yejin Choi, Claire Cardie, Ellen Riloff, and Siddharth Patwardhan. OpinionFinder: A system for subjectivity analysis. EMNLP 2005. Demo abstract.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuanbin Wu</author>
<author>Qi Zhang</author>
<author>Xuanjing Huang</author>
<author>Lide Wu</author>
</authors>
<title>Phrase dependency parsing for opinion mining.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<contexts>
<context position="6972" citStr="Wu et al., 2009" startWordPosition="1038" endWordPosition="1041">ask of opinion expression extraction is known to be harder than traditional NER since subjective expressions exhibit substantial lexical variation and their recognition requires more attention to linguistic structure. Parsing has been leveraged to improve performance for numerous natural language tasks. In opinion mining, numerous studies have shown that syntactic parsing features are very helpful for opinion analysis. A lot of work uses syntactic features to identify opinion holders and opinion topics (Bethard et al., 2005; Kim and Hovy, 2006; Kobayashi et al., 2007; Joshi and Carolyn, 2009; Wu et al., 2009; Choi et al., 2005). Jakob et al. (2010) recently employed dependency path features for the extraction of opinion targets. Johansson and Moschitti (2010; Johansson and Moschitti (2011) also successfully employed syntactic features that indicate dependency relations between opinion expressions for the task of opinion expression extraction. However, as their approach is based on the output of a sequence labeler, these features cannot be encoded to help the learning of the sequence labeler. 3 Approach We formulate the extraction of opinion expressions as a sequence labeling problem. Unlike previ</context>
</contexts>
<marker>Wu, Zhang, Huang, Wu, 2009</marker>
<rawString>Yuanbin Wu, Qi Zhang, Xuanjing Huang, and Lide Wu. Phrase dependency parsing for opinion mining. In Proceedings of EMNLP 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ioannis Tsochantaridis</author>
<author>Thomas Hofmann</author>
<author>Thorsten Joachims</author>
<author>Yasemi Altun</author>
</authors>
<title>Support Vector Learning for Interdependent and Structured Output Spaces.</title>
<date>2004</date>
<booktitle>In Proceedings of ICML</booktitle>
<contexts>
<context position="30594" citStr="Tsochantaridis et al., 2004" startWordPosition="4912" endWordPosition="4915">y Overlap metric due to the similar trend demonstrated by the Proportional Overlap metric). Breck et al. (2007) presents the state-of-the-art sequence labeling approach on the tasks of DSE and ESE extraction. Their best results are shown as Breck et al. Baseline in the table. Johansson and Moschitti (2010) use a reranking technique on the best k outputs of a sequence labeler to further improve their sequence labeling results on the task of extracting DSEs, ESEs and OSEs (Objective Speech Events) (we don’t consider OSEs here). Results using our re-implementation of their approach using SVM&amp;quot;&amp;quot;&apos; (Tsochantaridis et al., 2004) on the output of CRF are labeled CRF+Reranking Baseline in the table. We use the same features and 1342 parameter settings as in their approach. Our approach+Reranking are results obtained by applying the reranking step on the output of our newsemi-CRF approach. We can see that our approach outperforms the Breck et al. Baseline on both DSE extraction and ESE extraction in spite of the fact that we do not use their WordNet, Levin’s verb categorization, and FrameNet features. The CRF+Reranking Baseline does provide a performance increase over the the baseline CRF results, but overall it cannot </context>
</contexts>
<marker>Tsochantaridis, Hofmann, Joachims, Altun, 2004</marker>
<rawString>Ioannis Tsochantaridis, Thomas Hofmann, Thorsten Joachims, and Yasemi Altun. Support Vector Learning for Interdependent and Structured Output Spaces. In Proceedings of ICML 2004.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
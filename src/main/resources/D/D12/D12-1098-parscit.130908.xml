<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000034">
<title confidence="0.980725">
Fast Large-Scale Approximate Graph Construction for NLP
</title>
<author confidence="0.992923">
Amit Goyal and Hal Daum´e III
</author>
<affiliation confidence="0.997285">
Dept. of Computer Science
University of Maryland
</affiliation>
<address confidence="0.922167">
College Park, MD
</address>
<email confidence="0.998837">
{amit,hal}@umiacs.umd.edu
</email>
<author confidence="0.9958">
Raul Guerra
</author>
<affiliation confidence="0.9973465">
Dept. of Computer Science
University of Maryland
</affiliation>
<address confidence="0.922591">
College Park, MD
</address>
<email confidence="0.999572">
rguerra@cs.umd.edu
</email>
<sectionHeader confidence="0.993917" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999870882352941">
Many natural language processing problems
involve constructing large nearest-neighbor
graphs. We propose a system called FLAG
to construct such graphs approximately from
large data sets. To handle the large amount
of data, our algorithm maintains approximate
counts based on sketching algorithms. To
find the approximate nearest neighbors, our
algorithm pairs a new distributed online-PMI
algorithm with novel fast approximate near-
est neighbor search algorithms (variants of
PLEB). These algorithms return the approxi-
mate nearest neighbors quickly. We show our
system’s efficiency in both intrinsic and ex-
trinsic experiments. We further evaluate our
fast search algorithms both quantitatively and
qualitatively on two NLP applications.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999980365384616">
Many natural language processing (NLP) prob-
lems involve graph construction. Examples in-
clude constructing polarity lexicons based on lexi-
cal graphs from WordNet (Rao and Ravichandran,
2009), constructing polarity lexicons from web data
(Velikovich et al., 2010) and unsupervised part-of-
speech tagging using label propagation (Das and
Petrov, 2011). The later two approaches con-
struct nearest-neighbor graphs between word pairs
by computing nearest neighbors between word pairs
from large corpora. These nearest neighbors form
the edges of the graph, with weights given by the
distributional similarity (Turney and Pantel, 2010)
between terms. Unfortunately, computing the distri-
butional similarity between all words in a large vo-
cabulary is computationally and memory intensive
when working with large amounts of data (Pantel et
al., 2009). This bottleneck is typically addressed by
means of commodity clusters. For example, Pantel
et al. (2009) compute distributional similarity be-
tween 500 million terms over a 200 billion words in
50 hours using 100 quad-core nodes, explicitly stor-
ing a similarity matrix between 500 million terms.
In this work, we propose Fast Large-Scale Ap-
proximate Graph (FLAG) construction, a sys-
tem that constructs a fast large-scale approximate
nearest-neighbor graph from a large text corpus. To
build this system, we exploit recent developments
in the area of approximation, randomization and
streaming for large-scale NLP problems (Ravichan-
dran et al., 2005; Goyal et al., 2009; Levenberg et
al., 2010). More specifically we exploit work on Lo-
cality Sensitive Hashing (LSH) (Charikar, 2002) for
computing word-pair similarities from large text col-
lections (Ravichandran et al., 2005; Van Durme and
Lall, 2010). However, Ravichandran et al. (2005)
approach stored an enormous matrix of all unique
words and their contexts in main memory, which is
infeasible for very large data sets. A more efficient
online framework to locality sensitive hashing (Van
Durme and Lall, 2010; Van Durme and Lall, 2011)
computes distributional similarity in a streaming set-
ting. Unfortunately, their approach can handle only
additive features like raw-counts, and not non-linear
association scores like pointwise mutual information
(PMI), which generates better context vectors for
distributional similarity (Ravichandran et al., 2005;
Pantel et al., 2009; Turney and Pantel, 2010).
In FLAG, we first propose a novel distributed
online-PMI algorithm (Section 3.1). It is a stream-
ing method that processes large data sets in one pass
while distributing the data over commodity clusters
</bodyText>
<page confidence="0.943051">
1069
</page>
<note confidence="0.767097">
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1069–1080, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.999945789473684">
and returns context vectors weighted by pointwise
mutual information (PMI) for all the words. Our
distributed online-PMI algorithm makes use of the
Count-Min (CM) sketch algorithm (Cormode and
Muthukrishnan, 2004) (previously shown effective
for computing distributional similarity in our ear-
lier work (Goyal and Daum´e III, 2011)) to store the
counts of all words, contexts and word-context pairs
using only 8GB of main memory. The main motiva-
tion for using the CM sketch comes from its linear-
ity property (see last paragraph of Section 2) which
makes CM sketch to be implemented in distributed
setting for large data sets. In our implementation,
FLAG scaled up to 110 GB of web data with 866
million sentences in less than 2 days using 100 quad-
core nodes. Our intrinsic and extrinsic experiments
demonstrate the effectiveness of distributed online-
PMI.
After generating context vectors from distributed
online-PMI algorithm, our goal is to use them to find
fast approximate nearest neighbors for all words. To
achieve this goal, we exploit recent developments in
the area of existing randomized algorithms for ran-
dom projections (Achlioptas, 2003; Li et al., 2006),
Locality Sensitive Hashing (LSH) (Charikar, 2002)
and improve on previous work done on PLEB (Point
Location in Equal Balls) (Indyk and Motwani, 1998;
Charikar, 2002). We propose novel variants of PLEB
to address the issue of reducing the pre-processing
time for PLEB. One of the variants of PLEB (FAST-
PLEB) with considerably less pre-processing time
has effectiveness comparable to PLEB. We evaluate
these variants of PLEB both quantitatively and qual-
itatively on large data sets. Finally, we show the ap-
plicability of large-scale graphs built from FLAG on
two applications: the Google-Sets problem (Ghahra-
mani and Heller, 2005), and learning concrete and
abstract words (Turney et al., 2011).
</bodyText>
<sectionHeader confidence="0.989743" genericHeader="method">
2 Count-Min sketch
</sectionHeader>
<bodyText confidence="0.999835869565218">
The Count-Min (CM) sketch (Cormode and
Muthukrishnan, 2004) belongs to a class of ‘sketch’
algorithms that represents a large data set with a
compact summary, typically much smaller than the
full size of the input by processing the data in
one pass. The following surveys comprehensively
review the streaming literature (Rusu and Dobra,
2007; Cormode and Hadjieleftheriou, 2008) and
sketch techniques (Charikar et al., 2004; Li et al.,
2008; Cormode and Muthukrishnan, 2004; Rusu
and Dobra, 2007). In our another recent paper
(Goyal et al., 2012), we conducted a systematic
study and compare many sketch techniques which
answer point queries with focus on large-scale NLP
tasks. In that paper, we empirically demonstrated
that CM sketch performs the best among all the
sketches on three large-scale NLP tasks.
CM sketch uses hashing to store the approximate
frequencies of all items from the large data set onto a
small sketch vector that can be updated and queried
in constant time. CM has two parameters E and 6: E
controls the amount of tolerable error in the returned
count and 6 controls the probability with which the
error exceeds the bound E.
CM sketch with parameters (E,6) is represented
as a two-dimensional array with width w and depth
d; where w and d depends on E and 6 respectively.
We set w=2�and d=log(1� ). The depth d denotes
the number of pairwise-independent hash functions
employed by the CM sketch; and the width w de-
notes the range of the hash functions. Given an
input stream of items of length N (x1, x2 ... xN),
each of the hash functions hk:{x1, x2 ... xN} —*
{1... w}, V1 &lt; k &lt; d, takes an item from the in-
put stream and maps it into a position indexed by the
corresponding hash function.
UPDATE: For each new item “x” with count c, the
sketch is updated as:
sketch[k, hk(x)] &lt;-- sketch[k, hk(x)]+c, V1 &lt; k &lt; d.
QUERY: Since multiple items can be hashed to the
same index for each row of the array, hence the
stored frequency in each row is guaranteed to over-
estimate the true count, which makes it a biased esti-
mator. Therefore, to answer the point query (QUERY
(x)), CM returns the minimum over all the d posi-
tions indexed by the hash functions.
</bodyText>
<equation confidence="0.967239">
c(x) = mink sketch[k, hk(x)], V1 &lt; k &lt; d.
</equation>
<bodyText confidence="0.978231142857143">
All reported frequencies by CM exceed the true
frequencies by at most EN with probability of at
least 1 − 6. The space used by the algorithm is
O(1� log �1). Constant time of O(log(1� )) per each
update and query operation.
CM sketch has a linearity property which states
that: Given two sketches s1 and s2 computed (us-
</bodyText>
<page confidence="0.990363">
1070
</page>
<bodyText confidence="0.999635806451613">
ing the same parameters w and d, and the same set
of d hash functions) over different input streams; the
sketch of the combined data stream can be easily ob-
tained by adding the individual sketches in O(dxw)
time which is independent of the stream size. This
property enables sketches to be implemented in dis-
tributed setting, where each machine computes the
sketch over a small portion of the corpus and makes
it scalable to large datasets.
The idea of conservative update (Estan and Vargh-
ese, 2002) is to only increase counts in the sketch
by the minimum amount needed to ensure that the
estimate remains accurate. We (Goyal and Daum´e
III, 2011) used CM sketch with conservative update
(CM-CU sketch) to show that the update reduces
the amount of over-estimation error by a factor of
at least 1.5 on NLP data and showed the effective-
ness of CM-CU on three important NLP tasks. The
QUERY procedure for CM-CU is identical to Count-
Min. However, to UPDATE an item “x” with fre-
quency c, first we compute the frequency c(x) of this
item from the existing data structure:
(b1 G k G d, c(x) = mink sketch[k, hk(x)])
and the counts are updated according to:
sketch[k, hk(x)] &lt;-- max{sketch[k, hk(x)], c(x) + c}.
The intuition is that, since the point query returns
the minimum of all the d values, we will update
a counter only if it is necessary as indicated by
the above equation. This heuristic avoids the
unnecessary updating of counter values to reduce
the over-estimation error.
</bodyText>
<sectionHeader confidence="0.995964" genericHeader="method">
3 FLAG: Fast Large-Scale Approximate
</sectionHeader>
<subsectionHeader confidence="0.687316">
Graph Construction
</subsectionHeader>
<bodyText confidence="0.999970076923077">
We describe a system, FLAG, for generating a near-
est neighbor graph from a large corpus. For ev-
ery node (word), our system returns top l approxi-
mate nearest neighbors, which implicitly defines the
graph. Our system operates in four steps. First, for
every word “z”, our system generates a sparse con-
text vector (((c1, v1); (c2, v2) ... ; (cd, vd))) of size
d where cd denotes the context and vd denotes the
PMI (strength of association) between the context
cd and the word “z”. The context can be lexical,
semantic, syntactic, and/or dependency units that
co-occur with the word “z”. We compute this ef-
ficiently using a new distributed online Pointwise
Mutual Information algorithm (Section 3.1). Sec-
ond, we project all the words with context vector
size d onto k random vectors and then binarize these
random projection vectors (Section 3.2). Third, we
propose novel variants of PLEB (Section 3.3) with
less pre-processing time to represent data for fast
query retrieval. Fourth, using the output of vari-
ants of PLEB, we generate a small set of potential
nearest neighbors for every word “z” (Section 3.4).
From this small set, we can compute the Hamming
distance between every word “z” and its potential
nearest neighbors to return the l nearest-neighbors
for all unique words.
</bodyText>
<subsectionHeader confidence="0.979489">
3.1 Distributed online-PMI
</subsectionHeader>
<bodyText confidence="0.999763666666667">
We propose a new distributed online Pointwise Mu-
tual Information (PMI) algorithm motivated by the
online-PMI algorithm (Van Durme and Lall, 2009b)
(page 5). This is a streaming algorithm which pro-
cesses the input corpus in one pass. After one
pass over the data set, it returns the context vec-
tors for all query words. The original online-PMI
algorithm was used to find the top-d verbs for a
query verb using the highest approximate online-
PMI values using a Talbot-Osborne-Morris-Bloom1
(TOMB) Counter (Van Durme and Lall, 2009a).
Unfortunately, this algorithm is prohibitively slow
when computing contexts for all words, rather than
just a small query set. This motivates us to propose
a distributed variant that enables us to scale to large
data and large vocabularies.2
We make three modifications to the original
online-PMI algorithm and refer to it as the “modified
online-PMI algorithm” shown in Algorithm 1. First,
we use Count-Min with conservative update (CM-
CU) sketch (Goyal and Daum´e III, 2011) instead of
TOMB. We prefer CM because it enables distribu-
tion due to its linearity property (Section 2) and foot-
note #1. Distribution using TOMB is not known in
literature and we will like to explore that direction in
future. Second, we store the counts of words (“z”),
contexts (“y”) and word-context pairs all together in
</bodyText>
<footnote confidence="0.9996158">
1TOMB is a variant of CM sketch which focuses on reduc-
ing the bit size of each counter (in addition to the number of
counters) at the cost of incurring more error in the counts.
2The serialized online-PMI algorithm took a week to gener-
ate context vectors for all the words from GW (Section 4.1).
</footnote>
<page confidence="0.992962">
1071
</page>
<figure confidence="0.347325">
Algorithm 1 Modified online-PMI
Require: Data set D, buffer size B
Ensure: context vectors V , mapping word z to d-best
contexts in priority queue ( y,PMI(z, y))
</figure>
<listItem confidence="0.976574642857143">
1: initialize CM-CU sketch to store approximate counts
of words, context and word-context pairs
2: for each buffer B in the data set D do
3: initialize S to store (z,y) observed in B
4: for (z,y) in B do
5: set S ((z,y)) =1
6: insert z, y and pair (z,y) in sketch
7: end for
8: for x in set S do
9: recompute vectors V(x) using current contexts
in priority queue and {yjS((z,y))=1}
10: end for
11: end for
12: return context vectors V
</listItem>
<bodyText confidence="0.999600051724138">
the CM-CU sketch (in the original online-PMI al-
gorithm, exact counts of words and contexts were
stored in a hash table; only the pairs were stored in
the TOMB data structure). Third, in the original al-
gorithm, for each “z” a vector of top-d contexts are
modified at the end of each buffer (refer Algorithm
1). However, in our algorithm, we only modify the
list of those “z”’s which appeared in the recent buffer
rather than modifying for all the “z”’s (Note, if “z”
does not appear in the recent buffer, then its top-d
contexts cannot be changed. Hence, we only modify
those “z”s which appear in the recent buffer).
In our distributed online-PMI algorithm, first we
split the data into chunks of 10 million sentences.
Second, we run the modified online-PMI algorithm
on each chunk in distributed setting. This stores
counts of all words (“z”), contexts (“y”) and word-
context pairs in the CM-CU sketch, and store top-d
contexts for each word in priority queues. In third
step, we merge all the sketches using linearity prop-
erty to sum the counts of the words, contexts and
word-context pairs. Additionally we merge the lists
of top-d contexts for each word. In the last step, we
use the single merged sketch and merged top-d con-
texts list to generate the final distributed online-PMI
top-d contexts list.
It takes around one day to compute context vec-
tors for all the words from a chunk of 10 million
sentences using first step of distributed online-PMI.
We generated context vectors for all the 87 chunks
(110 GB data with 866 million sentences: see Table
1) in one day by running one process per chunk over
a cluster. The first step of the algorithm involves
traversing the data set and is the most time intensive
step. For the second step, the merging of sketches is
fast, since sketches are two dimensional array data
structures (we used the sketch of size 2 billion coun-
ters with 3 hash functions). Merging the lists of top-
d contexts for each word is embarrassingly parallel
and fast. The last step to generate the final top-d
contexts list is again embarrassingly parallel and fast
and takes couple of hours to generate the top-d con-
texts for all the words from all the chunks. If im-
plemented serially the “modified online-PMI algo-
rithm” on 110 GB data with 866 million sentences
would take approximately 3 months.
The downside of the distributed online-PMI is that
it splits the data into small chunks and loses infor-
mation about the global best contexts for a word
over all the chunks. The algorithm locally computes
the best contexts for each chunk, that can be bad if
the algorithm misses out globally good contexts and
that can affect the accuracy of downstream applica-
tion. We will demonstrate in our experiments (Sec-
tion 4.2) by using distributed online-PMI, we do not
lose any significant information about global con-
texts and perform comparable to offline-PMI over
an intrinsic and extrinsic evaluation.
</bodyText>
<subsectionHeader confidence="0.994813">
3.2 Dimensionality Reduction from RD to Rk
</subsectionHeader>
<bodyText confidence="0.999881055555556">
We are given context vectors for Z words, our goal
is to use k random projections to project the con-
text vectors from RD to Rk. There are total D
unique contexts (D &gt;&gt; k) for all Z words. Let
(((c1, v1); (c2, v2) ... ; (cd, vd))) be sparse context
vectors of size d for Z words. For each word, we use
hashing to project the context vectors onto k direc-
tions. We use k pairwise independent hash functions
that maps each of the d context (cd) dimensions onto
Qd,k E I− 1, +1}; and compute inner product be-
tween Qd,k and vd. Next, bk, Ed Qd,k.vd returns the
k random projections for each word “z”. We store
the k random projections for all words (mapped to
integers) as a matrix A of size of k x Z.
The mechanism described above generates ran-
dom projections by implicitly creating a random
projection matrix from a set of 1−1,+1}. This
idea of creating implicit random projection matrix
</bodyText>
<page confidence="0.983009">
1072
</page>
<figure confidence="0.971089833333333">
⎞
⎠⎟⎟⎟
Sort
=�
⎛
⎜ ⎜ ⎜ ⎝
⎞
⎠⎟⎟⎟
�
⎛
⎜ ⎜ ⎜ ⎝
⎞
⎠⎟⎟⎟
�
⎛
⎜ ⎜ ⎝
⎞
⎠⎟⎟
</figure>
<equation confidence="0.947024769230769">
1 2 ··· Z
⎛k1 (z1, 26) (z2,80)··· (zZ, 3)
k2(z1, −28) (z2, 6) · · · (zZ, 111)
⎜ ⎜
.⎜ ...
. .⎝ . .. ....
. .
kK (z1, 78) (z2,69) ··· (zZ, 92)
(a) Matrix A
Smallest to Largest
(zZ, 3) (z1, 26) ··· (zm, 700)
(zr, −50) (z2, 6) · · · (zZ,111)
(z1, 78) (zZ, 92) ··· (zu, 432)
</equation>
<figure confidence="0.493721235294118">
(b) Matrix A
1 2 ··· Z
zZ z1 ··· zm
zr z2 ··· zZ
...
. .. ....
. .
z1 zZ ··· zu
(c) Matrix A
z1 z2 ··· zZ
2 60 ··· 1
55 2 ··· Z
...
. .. ....
. .
1 90 ··· 2
(d) Matrix C
</figure>
<figureCaption confidence="0.968659333333333">
Figure 1: First matrix pairs the words 1 · · · Z and their random projection values. Second matrix sorts each row by the random
projection values from smallest to largest. Third matrix throws away the projection values leaving only the words. Fourth matrix
maps the words 1 · · · Z to their sorted position in the third matrix for each k. This allows constant query time for all the words.
</figureCaption>
<bodyText confidence="0.999734590909091">
is motivated by the work on stable random projec-
tions (Li et al., 2006; Li et al., 2008), Count sketch
(Charikar et al., 2004), feature hashing (Weinberger
et al., 2009) and online Locality Sensitive Hashing
(LSH) (Van Durme and Lall, 2010). The idea of gen-
erating random projections from the set 1−1, +11
was originally proposed by Achlioptas (2003).
Next we create a binary matrix B using matrix
A by taking sign of each of the entries of the ma-
trix A. If A(i, j) &gt; 0, then B(i, j) = 1; else
B(i, j) = 0. This binarization creates Locality Sen-
sitive Hash (LSH) function that preserves the cosine
similarity between every pair of word vectors. This
idea was first proposed by Charikar (2002) and used
in NLP for large-scale noun clustering (Ravichan-
dran et al., 2005). However, in large-scale noun
clustering work, their approach had to store the ran-
dom projection matrix of size D x k; where D de-
notes the number of all unique contexts (which is
generally large and D &gt;&gt; Z) and in this paper, we
do not explicitly require storing a random projection
matrix.
</bodyText>
<subsectionHeader confidence="0.980853">
3.3 Representation for Fast-Search
</subsectionHeader>
<bodyText confidence="0.999980363636364">
We describe three approaches to represent the data
(matrix A and B from Section 3.2) in such a manner
that finding nearest neighbors is fast. These three
approaches differ in amount of pre-processing time.
First, we propose a naive baseline approach using
random projections independently with the best pre-
processing time. Second, we describe PLEB (Point
Location in Equal Balls) (Indyk and Motwani, 1998;
Charikar, 2002) with the worst pre-processing time.
Third, we propose a variant of PLEB to reduce its
pre-processing time.
</bodyText>
<subsectionHeader confidence="0.743997">
3.3.1 Independent Random Projections (IRP)
</subsectionHeader>
<bodyText confidence="0.995191105263158">
Here, we describe a naive baseline approach to
arrange nearest neighbors next to each other by us-
ing Independent Random Projections (IRP). In this
approach, we pre-process the matrix A. First for
matrix A, we pair the words z1 · · · zZ and their ran-
dom projection values as shown in Fig. 1(a). Sec-
ond, we sort the elements of each row of matrix A
by their random projection values from smallest to
largest (shown in Fig. 1(b)). The sorting step takes
O(ZlogZ) time (We can assume k to be a constant).
The sorting operation puts all the nearest neighbor
words (for each k independent projections) next to
each other. After sorting the matrix A, we throw
away the projection values leaving only the words
(see Fig. 1(c)). To search for a word in matrix A
in constant time, we create another matrix C of size
(k x Z) (see Fig. 1(d)). Matrix C maps the words
z1 · · · zZ to their sorted position in the matrix A (see
Fig. 1(c)) for each k.
</bodyText>
<sectionHeader confidence="0.618044" genericHeader="method">
3.3.2 PLEB
</sectionHeader>
<bodyText confidence="0.982685947368421">
PLEB (Point Location in Equal Balls) was first
proposed by Indyk and Motwani (1998) and further
improved by Charikar (2002). The improved PLEB
algorithm puts in operation all k random projections
together. It randomly permutes the ordering of k bi-
nary LSH bits (stored in matrix B) for all the words
p times. For each permutation it sorts all the words
lexicographically based on their permuted LSH rep-
resentation of size k. The sorting operation puts all
the nearest neighbor words (using k projections to-
gether) next to each other for all the permutations.
In practice p is generally large, Ravichandran et al.
(2005) used p = 1000 in their work.
In our implementation of PLEB, we have a matrix
A of size (p x Z) similar to the first matrix in Fig.
1(a). The main difference to the first matrix in Fig.
1(a) is that bit vectors of size k are used for sorting
rather than using scalar projection values. Similar to
Fig. 1(c) after sorting, bit vectors are discarded and
</bodyText>
<page confidence="0.955821">
1073
</page>
<bodyText confidence="0.91334325">
a matrix C of size (p x Z) is used to map the words
1 · · · Z to their sorted position in the matrix A. Note,
in IRP approach, the size of A and C matrix is (k x
Z). In PLEB generating random permutations and
sorting the bit vectors of size k involves worse pre-
processing time than using IRP. However, spending
more time in pre-processing leads to finding better
approximate nearest neighbors.
</bodyText>
<subsectionHeader confidence="0.762277">
3.3.3 FAST-PLEB
</subsectionHeader>
<bodyText confidence="0.999971">
To reduce the pre-processing time for PLEB, we
propose a variant of PLEB (FAST-PLEB). In PLEB,
while generating random permutations, it uses all
the k bits. In this variant, for each random permu-
tation we randomly sample without replacement q
(q &lt;&lt; k) bits out of k. We use q bits to repre-
sent each permutation and sort based on these q bits.
This makes pre-processing faster for PLEB. Section
4.3 shows that FAST-PLEB only needs q = 10 to
perform comparable to PLEB with q = 3000 (that
makes FAST-PLEB 300 times faster than PLEB).
Here, again we store matrices A and C of size
(p x Z).
</bodyText>
<subsectionHeader confidence="0.989957">
3.4 Finding Approximate Nearest Neighbors
</subsectionHeader>
<bodyText confidence="0.999957818181818">
The goal here is to exploit three representations dis-
cussed in Section 3.3 to find approximate nearest
neighbors quickly. For all the three methods (IRP,
PLEB, FAST-PLEB), we can use the same fast ap-
proximate search which is simple and fast. To search
a word “z”, first, we can look up matrix C to locate
the k positions where “z” is stored in matrix A. This
can be done in constant time (Again assuming k (for
IRP) and p (for PLEB and FAST-PLEB) to be a con-
stant.). Once, we find “z” in each row, we can select
b (beam parameter) neighbors (b/2 neighbors from
left and b/2 neighbors from right of the query word.)
for all the k or p rows. This can be done in constant
time (Assuming k, p and b to be constants.). This
search procedure produces a set of bk (IRP) or bp
(PLEB and FAST-PLEB) potential nearest neighbors
for a query word “z”. Next, we compute Hamming
distance between query word “z” and the set of po-
tential nearest neighbors from matrix B to return l
closest nearest neighbors. For computing hamming
distance, all the approaches discussed in Section 3.3
require all k random projection bits.
</bodyText>
<sectionHeader confidence="0.999385" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.99989175">
We evaluate our system FLAG for fast large-scale
approximate graph construction. First, we show that
using distributed online-PMI algorithm is as effec-
tive as offline-PMI. Second, we compare the approx-
imate nearest neighbors lists generated by FLAG
against the exact nearest neighbor lists. Finally, we
show the quality of our approximate similarity lists
generated by FLAG from the web corpus.
</bodyText>
<subsectionHeader confidence="0.968472">
4.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.999678461538462">
Data sets: We use two data sets: Gigaword (Graff,
2003) and a copy of news web (Ravichandran et
al., 2005). For both the corpora, we split the text
into sentences, tokenize and convert into lower-case.
To evaluate our approximate graph construction, we
evaluate on three data sets: Gigaword (GW), Giga-
word + 50% of web data (GWB50) and Gigaword
+ 100% ((GWB100)) of web data. Corpus statistics
are shown in Table 1. We define the context for a
given word “z” as the surrounding words appearing
in a window of 2 words to the left and 2 words to
the right. The context words are concatenated along
with their positions -2, -1, +1, and +2.
</bodyText>
<table confidence="0.999053">
Corpus GW GWB50 GWB100
Unzipped 12 60 110
Size (GB)
# of sentences 57 463 866
(Million)
# of tokens 2.1 10.9 20.0
(Billion)
</table>
<tableCaption confidence="0.999251">
Table 1: Corpus Description
</tableCaption>
<subsectionHeader confidence="0.995746">
4.2 Evaluating Distributed online-PMI
</subsectionHeader>
<bodyText confidence="0.999595083333333">
Experimental Setup: First we do an intrinsic
evaluation to quantitatively evaluate the distributed
online-PMI vectors against the offline-PMI vectors
computed from Gigaword (GW). Offline-PMI com-
puted from the sketches have been shown as effec-
tive as exact PMI by Goyal and Daum´e III (2011).
To compute offline-PMI vectors, we do two passes
over the corpus. In the first pass, we store the counts
of words, contexts and word-context pairs computed
from GW in the Count-Min with conservative up-
date (CM-CU) sketch. We use the CM-CU sketch
of size 2 billion counters (bounded 8 GB memory)
</bodyText>
<page confidence="0.994185">
1074
</page>
<bodyText confidence="0.998942811320755">
with 3 hash functions. In second pass, using the
aggregated counts from the sketch, we generate the
offline-PMI vectors of size d = 1000 for every word.
For rest of this paper for distributed online-PMI, we
set d = 1000 and the size of the buffer=10, 000 and
we split the data sets into small chunks of 10 million
sentences.
Intrinsic Evaluation: We use four kinds of mea-
sures: precision (P), recall (R), f-measure (F1) and
Pearson’s correlation (p) to measure the overlap in
the context vectors obtained using online and offline
PMI. p is computed between contexts that are found
in offline and online context vectors. We do this
evaluation on 447 words selected from the concate-
nation of four test-sets mentioned in the next para-
graph. On these 447 words, we achieve an average P
of .97, average R of .96 and average F1 of .97 and a
perfect average p of 1. This evaluation show that the
vectors obtained using online-PMI are as effective
as offline-PMI.
Extrinsic Evaluation: We also compare online-
PMI effectiveness on four test sets which consist of
word pairs, and their corresponding human rank-
ings. We generate the word pair rankings using
online-PMI and offline-PMI strategies. We report
the Pearson’s correlation (p) between the human and
system generated similarity rankings. The four test
sets are: WS-353 (Finkelstein et al., 2002) is a set
of 353 word pairs. WS-203: A subset of WS-353
with 203 word pairs (Agirre et al., 2009). RG-65:
(Rubenstein and Goodenough, 1965) has 65 word
pairs. MC-30: A subset of RG-65 dataset with 30
word pairs (Miller and Charles, 1991).
The results in Table 2 shows that by using dis-
tributed online-PMI (by making a single pass over
the corpus) is comparable to offline-PMI (which is
computed by making two passes over the corpus).
For generating context vectors from GW, for both
offline-PMI and online-PMI, we use a frequency
cutoff of 5 for word-context pairs to throw away the
rare terms as they are sensitive to PMI (Church and
Hanks, 1989). Next, FLAG generates online-PMI
vectors from GWB50 and GWB100 and uses fre-
quency cutoffs of 15 and 25. The higher frequency
cutoffs are selected based on the intuition that, with
more data, we get more noise, and hence not con-
sidering word-context pairs with frequency less than
25 will be better for the system. As FLAG is go-
ing to use the context vectors to find nearest neigh-
bors, we also throw away all those words which have
&lt; 50 contexts associated with them. This generates
context vectors for 57,930 words from GW; 95, 626
from GWB50 and 106,733 from GWB100.
</bodyText>
<table confidence="0.998846666666667">
Test Set WS-353 WS-203 RG-65 MC-30
Offline-PMI .41 .55 .40 .52
Online-PMI .41 .56 .39 .51
</table>
<tableCaption confidence="0.997718">
Table 2: Evaluating word pairs ranking with online and offline
PMI. Scores are evaluated using p metric.
</tableCaption>
<table confidence="0.999784125">
10 25 50 100
R p R p R p R p
IRP .40 .53 .38 .51 .35 .54 .34 .51
q=1 .24 .62 .20 .63 .18 .59 .17 .54
q=5 .47 .60 .43 .57 .40 .57 .37 .53
q=10 .53 .58 .49 .56 .45 .55 .42 .53
q=100 .53 .60 .50 .59 .46 .56 .43 .53
q=3000 .54 .58 .50 .59 .46 .56 .43 .54
</table>
<tableCaption confidence="0.9884435">
Table 4: Varying parameter q for FAST-PLEB with fixed p =
1000, k = 3000 and b = 40. Results reported on recall and p.
</tableCaption>
<subsectionHeader confidence="0.997967">
4.3 Evaluating Approximate Nearest Neighbor
</subsectionHeader>
<bodyText confidence="0.999508916666667">
Experimental Setup: To evaluate approximate
nearest neighbor similarity lists generated by
FLAG, we conduct three experiments. We evaluate
all the three experiments on 447 words (test set) as
used in Section 4.2. For each word, both exact and
approximate methods return l = 100 nearest neigh-
bors. The exact similarity lists for 447 test words is
computed by calculating cosine similarity between
447 test words with respect to all other words. We
also compare the LSH (computed using Hamming
distance between all words and test set.) approxi-
mate nearest neighbor similarity lists against the ex-
act similarity lists. LSH provides an upper bound
on the performance of our approximate search rep-
resentations (IRP, PLEB, and FAST-PLEB) for fast-
search from Section 3.3) . We set the number of
projections k = 3000 for all three methods and for
PLEB and FAST-PLEB, we set number of permuta-
tions p = 1000 as used in large-scale noun cluster-
ing work (Ravichandran et al., 2005).
Evaluation Metric: We use two kinds of mea-
sures, recall and Pearson’s correlation to measure
the overlap in the approximate and exact similarity
lists. Intuitively, recall (R) captures the number of
</bodyText>
<page confidence="0.972846">
1075
</page>
<figure confidence="0.9722144375">
10 10 10
IRP PLEB FAST-PLEB
25 50 100 25 50 100 25 50 100
R p R p R p R p R p R p R p R p R p R p R p R p
.55 .57 .52 .56 .49 .54 .46 .52 .55 .57 .52 .56 .49 .54 .46 .52 .55 .57 .52 .56 .49 .54 .46 .52
.29 .50 .26 .55 .25 .54 .24 .50 .50 .59 .45 .60 .41 .57 .37 .55 .48 .58 .42 .58 .38 .58 .35 .55
.36 .55 .33 .56 .31 .55 .30 .52 .53 .59 .48 .59 .44 .56 .41 .54 .51 .57 .47 .57 .42 .56 .40 .54
.40 .53 .38 .51 .35 .54 .34 .51 .54 .58 .50 .59 .46 .56 .43 .54 .53 .58 .49 .56 .45 .55 .42 .53
.44 .56 .42 .54 .39 .54 .37 .52 .54 .58 .51 .57 .47 .56 .44 .53 .54 .58 .50 .56 .46 .55 .44 .53
.53 .59 .49 .54 .46 .55 .43 .53 .55 .56 .52 .56 .48 .54 .46 .53 .55 .57 .52 .56 .48 .54 .46 .53
LSH
20
30
40
50
100
</figure>
<tableCaption confidence="0.809590666666667">
Table 3: Evaluation results on comparing LSH, IRP, PLEB, and FAST-PLEB with k = 3000 and b = {20, 30, 40, 50, 100} with
exact nearest neighbors over GW data set. For PLEB and FAST-PLEB, we set p = 1000 and for FAST-PLEB, we set q = 10. We
report results on recall (R) and p metric. For IRP, we sample first p rows and only use p rows rather than k.
</tableCaption>
<table confidence="0.994000333333333">
LSH
IRP
PLEB
FAST-PLEB
10 GWB50 10 25
GW 10 25 50 100 GWB100
25 50 100 50 100
R p R p R p R p R p R p R p R p R p R p R p R p
.55 .57 .52 .56 .49 .54 .46 .52 .51 .55 .46 .54 .44 .52 .42 .48 .48 .58 .45 .52 .42 .49 .40 .47
.40 .53 .37 .53 .35 .54 .34 .51 .29 .50 .27 .51 .25 .51 .24 .47 .26 .57 .24 .49 .23 .48 .22 .45
.54 .58 .50 .59 .46 .56 .43 .54 .46 .58 .42 .56 .38 .53 .36 .51 .44 .57 .40 .56 .36 .52 .33 .49
.53 .58 .49 .56 .45 .55 .42 .53 .46 .56 .41 .56 .37 .54 .35 .51 .43 .57 .38 .55 .35 .52 .32 .50
</table>
<tableCaption confidence="0.852766333333333">
Table 5: Evaluation results on comparing LSH, IRP, PLEB, and FAST-PLEB with k = 3000, b = 40, p = 1000 and q = 10 with
exact nearest neighbors across three different data sets: GW, GWB50, and GWB100. We report results on recall (R) and p metric.
The gray color row is the system that we use for further evaluations.
</tableCaption>
<bodyText confidence="0.996740076923077">
nearest neighbors that are found in both the lists and
then Pearson’s (p) correlation captures if the rela-
tive order of these lists is preserved in both the sim-
ilarity lists. We also compute R and p at various
l = {10, 25, 50, 100}.
Results: For the first experiment, we evaluate
IRP, PLEB, and FAST-PLEB against the exact near-
est neighbor similarity lists. For IRP, we sample
first p rows and only use p rather than k, this en-
sures that all the three methods (IRP, PLEB, and
FAST-PLEB) take the same query time. We vary
the approximate nearest neighbor beam parameter
b = {20,30,40,50, 100} that controls the number
of closest neighbors for a word with respect to each
independent random projection. Note, with increas-
ing b, our algorithm approaches towards LSH (com-
puting Hamming distance with respect to all the
words). For FAST-PLEB, we set q = 10 (q &lt;&lt; k)
that is the number of random bits selected out of k to
generate p permuted bit vectors of size q. The results
are reported in Table 3, where the first row com-
pares the LSH approach against the exact similar-
ity list for test set words. Across three columns we
compare IRP, PLEB, and FAST-PLEB. For all meth-
ods, increasing b means better recall. If we move
down the table, with b = 100, IRP, PLEB, and FAST-
PLEB get results comparable to LSH (reaches an up-
per bound). However, using large b implies gener-
ating a long potential nearest neighbor list close to
the size of the unique context vectors. If we focus
on the gray color row with b = 40 (This will have
comparatively small potential list and return nearest
neighbors in less time), IRP has worse recall with
best pre-processing time. FAST-PLEB (q = 10) is
comparable to PLEB (using all bits q = 3000) with
pre-processing time 300 times faster than PLEB. For
rest of this work, FLAG will use FAST-PLEB as it
has best recall and pre-processing time with fixed
b = 40.
For the second experiment, we vary parameter
q = {1, 5,10,100, 3000} for FAST-PLEB in Table
4. Table 4 demonstrates using q = {1, 5} result in
worse recall, however using q = 5 for FAST-PLEB
is better than IRP. q = 10 has comparable recall
to q = {100, 3000}. For rest of this work, we fix
q = 10 as it has best recall and pre-processing time.
For the third experiment, we increase the size of
the data set across the Table 5. With the increase
in size of the data set, LSH, IRP, PLEB, and FAST-
PLEB (q = 10) have worse recall. The reason for
such a behavior is that the number of unique context
vectors is greater for big data sets. Across all the
</bodyText>
<page confidence="0.918945">
1076
</page>
<bodyText confidence="0.988760636363636">
jazz yale soccer physics wednesday
reggae harvard basketball chemistry tuesday
rockabilly cornell hockey mathematics thursday
rock fordham lacrosse biology monday
bluegrass rutgers handball biochemistry friday
indie dartmouth badminton science saturday
baroque nyu softball microbiology sunday
ska ucla football geophysics yesterday
funk princeton tennis economics tues
banjo stanford wrestling psychology october
blues loyola rugby neuroscience week
</bodyText>
<tableCaption confidence="0.586535">
Table 6: Sample Top 10 similarity lists returned by FAST-PLEB
with k = 3000, p = 1000, b = 40 and q = 10 from GWB100.
</tableCaption>
<bodyText confidence="0.999450228571429">
three data sets, FAST-PLEB has recall comparable to
PLEB with best pre-processing time. Hence, for the
next evaluation to show the quality of final lists we
use FAST-PLEB with q = 10 for GWB100 data set.
In Table 6, we list the top 10 most similar words
for some words found by our system FLAG using
GWB100 data set. Even though FLAG’s approxi-
mate nearest neighbor algorithm has less recall with
respect to exact but still the quality of these nearest
neighbor lists is excellent.
For the final experiment, we demonstrate the pre-
processing and query time results comparing LSH,
IRP, PLEB, and FAST-PLEB with k = 3000, p =
1000, b = 40 and q = 10 parameter settings. For
pre-processing timing results, we perform all the ex-
periments (averaged over 5 runs) on GWB100 data
set with 106, 733 words. The second pre-processing
step of the system FLAG (Section 3.2) that is di-
mensionality reduction from RD to Rk took 8.8
hours. The pre-processing time differences among
IRP, PLEB, and FAST-PLEB from third step (Section
3.3) are shown in second column of Table 7. Ex-
perimental results show that the naive baseline IRP
is the fastest and FAST-PLEB has 120 times faster
pre-processing time compared to PLEB.
For comparing query time among several meth-
ods, we evaluate over 447 words (Section 4.2). We
report average timing results (averaged over 10 runs
and 447 words) to find top 100 nearest neighbors for
single query word. The results are shown in third
column of Table 7. Comparing first and second rows
show that LSH is 87 times faster than computing
exact top-100 (cosine similarity) nearest neighbors.
Comparing second, third, fourth and fifth rows of the
table demonstrate that IRP, PLEB and FAST-PLEB
</bodyText>
<table confidence="0.999382166666667">
Methods Preprocessing Query (seconds)
Exact n/a 87
LSH 8.8 hours 0.59
IRP 7.5 minutes 0.28
PLEB 1.8 days 0.28
FAST-PLEB 22 minutes 0.26
</table>
<tableCaption confidence="0.977766666666667">
Table 7: Preprocessing and query time results compar-
ing exact, LSH, IRP, PLEB, and FAST-PLEB methods on
GWB100 data set.
</tableCaption>
<table confidence="0.9994236">
Language english chinese japanese spanish russian
Place africa america washington london pacific
Nationality american european french british western
Date january may december october june
Organization ford microsoft sony disneyland google
</table>
<tableCaption confidence="0.999325">
Table 8: Query terms for Google Sets Problem evaluation
</tableCaption>
<bodyText confidence="0.5833">
methods are twice as fast as LSH.
</bodyText>
<sectionHeader confidence="0.998074" genericHeader="method">
5 Applications
</sectionHeader>
<bodyText confidence="0.9997716875">
We use the graph constructed by FLAG from
GWB100 data set (110 GB) by applying FAST-
PLEB with parameters k = 3000, p = 1000, q = 10
and b = 40. The graph has 106, 733 nodes (words),
with each node having 100 edges that denote the top
l = 100 approximate nearest neighbors associated
with each node. However, FLAG applied FAST-
PLEB (approximate search) to find these neighbors.
Therefore many of these edges can be noisy for our
applications. Hence for each node, we only consider
top 10 edges. In general for graph-based NLP prob-
lems; for example, constructing web-derived polar-
ity lexicons (Velikovich et al., 2010), top 25 edges
were used, and for unsupervised part-of-speech tag-
ging using label propagation (Das and Petrov, 2011),
top 5 edges were used.
</bodyText>
<subsectionHeader confidence="0.990892">
5.1 Google Sets Problem
</subsectionHeader>
<bodyText confidence="0.9998921">
Google Sets problem (Ghahramani and Heller,
2005) can be defined as: given a set of query words,
return top t similar words with respect to query
words. To evaluate the quality of our approximate
large-scale graph, we return top 25 words which
have best aggregated similarity scores with respect
to query words. We take 5 classes and their query
terms (McIntosh and Curran, 2008) shown in Table
8 and our goal is to learn 25 new words which are
similar with these 5 query words.
</bodyText>
<page confidence="0.992272">
1077
</page>
<note confidence="0.9819742">
Language: german, french, estonian, hungarian, bulgarian
Place: scandinavia, mongolia, mozambique, zambia, namibia
Nationality: german, hungarian, estonian, latvian, lithuanian
Date: september, february, august, july, november
Organization: looksmart, hotbot, lycos, webcrawler, alltheweb
</note>
<tableCaption confidence="0.99677">
Table 9: Learned terms for Google Sets Problem
</tableCaption>
<table confidence="0.71382225">
Concrete car, house, tree, horse, animal
seeds man, table, bottle, woman, computer
Abstract idea, bravery, deceit, trust, dedication
seeds anger, humour, luck, inflation, honesty
</table>
<tableCaption confidence="0.997603">
Table 10: Example seeds for bootstrapping.
</tableCaption>
<bodyText confidence="0.999989363636364">
We conduct a manual evaluation to directly mea-
sure the quality of returned words. We recruited 1
annotator and developed annotation guidelines that
instructed each recruiter to judge whether learned
values are similar to query words or not. Overall the
annotator found almost all the learned words to be
similar to the query words. However, the algorithm
can not differentiate between different senses of the
word. For example, “French” can be a language and
a nationality. Table 9 shows the top ranked words
with respect to query words.
</bodyText>
<subsectionHeader confidence="0.999879">
5.2 Learning Concrete and Abstract Words
</subsectionHeader>
<bodyText confidence="0.992442933333333">
Our goal is to automatically learn concrete and ab-
stract words (Turney et al., 2011). We apply boot-
strapping (Kozareva et al., 2008) on the word graphs
by manually selecting 10 seeds for concrete and ab-
stract words (see Table 10). We use in-degree (sum
of weights of incoming edges) to compute the score
for each node which has connections with known
(seeds) or automatically labeled nodes, previously
exploited to learn hyponymy relations from the web
(Kozareva et al., 2008). We learn concrete and ab-
stract words together (known as mutual exclusion
principle in bootstrapping (Thelen and Riloff, 2002;
McIntosh and Curran, 2008)), and each word is as-
signed to only one class. Moreover, after each it-
eration, we harmonically decrease the weight of the
in-degree associated with instances learned in later
iterations. We add 25 new instances at each itera-
tion and ran 100 iterations of bootstrapping, yielding
2506 concrete nouns and 2498 abstract nouns. To
evaluate our learned words, we searched in WordNet
whether they had ‘abstraction’ or ’physical’ as their
hypernym. Out of 2506 learned concrete nouns,
Concrete: girl, person, bottles, wife, gentleman, mi-
crocomputer, neighbor, boy, foreigner, housewives,
texan, granny, bartender, tables, policeman, chubby,
mature, trees, mainframe, backbone, truck
Abstract: perseverance, tenacity, sincerity, profes-
sionalism, generosity, heroism, compassion, commit-
ment, openness, resentment, treachery, deception, no-
tion, jealousy, loathing, hurry, valour
</bodyText>
<tableCaption confidence="0.966698">
Table 11: Learned concrete/abstract words.
</tableCaption>
<bodyText confidence="0.999687833333333">
1655 were found in WordNet. According to Word-
Net, 74% of those are concrete and 26% are ab-
stract. Out of 2498 learned abstract nouns, 942 were
found in WordNet. According to WordNet, 5% of
those are concrete and 95% are abstract. Table 11
shows the top ranked concrete and abstract words.
</bodyText>
<sectionHeader confidence="0.999561" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999976761904762">
We proposed a system, FLAG which constructs
fast large-scale approximate graphs from large data
sets. To build this system we proposed a distributed
online-PMI algorithm that scaled up to 110 GB of
web data with 866 million sentences in less than 2
days using 100 quad-core nodes. Our both intrinsic
and extrinsic experiments demonstrated that online-
PMI algorithm not at all loses globally good con-
texts and perform comparable to offline-PMI. Next,
we proposed FAST-PLEB (a variant of PLEB) and
empirically demonstrated that it has recall compa-
rable to PLEB with 120 times faster pre-processing
time. Finally, we show the applicability of FLAG on
two applications: Google-Sets problem and learning
concrete and abstract words.
In future, we will apply FLAG to construct graphs
using several kinds of contexts like lexical, seman-
tic, syntactic and dependency relations or a combi-
nation of them. Moreover, we will apply graph theo-
retic models on graphs constructed using FLAG for
solving a large variety of NLP applications.
</bodyText>
<sectionHeader confidence="0.998296" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9995096">
This work was partially supported by NSF Award
IIS-1139909. Thanks to Graham Cormode and
Suresh Venkatasubramanian for useful discussions
and the anonymous reviewers for many helpful com-
ments.
</bodyText>
<page confidence="0.995444">
1078
</page>
<sectionHeader confidence="0.989785" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999834466666667">
Dimitris Achlioptas. 2003. Database-friendly random
projections: Johnson-lindenstrauss with binary coins.
J. Comput. Syst. Sci., 66(4):671–687.
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana
Kravalova, Marius Pas¸ca, and Aitor Soroa. 2009. A
study on similarity and relatedness using distributional
and wordnet-based approaches. In NAACL ’09: Pro-
ceedings of HLT-NAACL.
Moses Charikar, Kevin Chen, and Martin Farach-Colton.
2004. Finding frequent items in data streams. Theor.
Comput. Sci., 312:3–15, January.
Moses S. Charikar. 2002. Similarity estimation tech-
niques from rounding algorithms. In In Proc. of 34th
STOC, pages 380–388. ACM.
K. Church and P. Hanks. 1989. Word Associa-
tion Norms, Mutual Information and Lexicography.
In Proceedings of ACL, pages 76–83, Vancouver,
Canada, June.
Graham Cormode and Marios Hadjieleftheriou. 2008.
Finding frequent items in data streams. In VLDB.
Graham Cormode and S. Muthukrishnan. 2004. An im-
proved data stream summary: The count-min sketch
and its applications. J. Algorithms.
Dipanjan Das and Slav Petrov. 2011. Unsupervised
part-of-speech tagging with bilingual graph-based pro-
jections. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies, pages 600–609, Port-
land, Oregon, USA, June. Association for Computa-
tional Linguistics.
Cristian Estan and George Varghese. 2002. New di-
rections in traffic measurement and accounting. SIG-
COMM Comput. Commun. Rev., 32(4).
L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin,
Z. Solan, G. Wolfman, and E. Ruppin. 2002. Plac-
ing search in context: The concept revisited. In ACM
Transactions on Information Systems.
Zoubin Ghahramani and Katherine A. Heller. 2005.
Bayesian Sets. In in Advances in Neural Information
Processing Systems, volume 18.
Amit Goyal and Hal Daum´e III. 2011. Approximate
scalable bounded space sketch for large data NLP. In
Empirical Methods in Natural Language Processing
(EMNLP).
Amit Goyal, Hal Daum´e III, and Suresh Venkatasubra-
manian. 2009. Streaming for large scale NLP: Lan-
guage modeling. In NAACL.
Amit Goyal, Graham Cormode, and Hal Daum´e III.
2012. Sketch algorithms for estimating point queries
in NLP. In Empirical Methods in Natural Language
Processing (EMNLP).
D. Graff. 2003. English Gigaword. Linguistic Data Con-
sortium, Philadelphia, PA, January.
Piotr Indyk and Rajeev Motwani. 1998. Approximate
nearest neighbors: towards removing the curse of di-
mensionality. In Proceedings of the thirtieth annual
ACM symposium on Theory of computing, STOC ’98,
pages 604–613. ACM.
Zornitsa Kozareva, Ellen Riloff, and Eduard Hovy. 2008.
Semantic class learning from the web with hyponym
pattern linkage graphs. In Proceedings of ACL-08:
HLT, pages 1048–1056, Columbus, Ohio, June. As-
sociation for Computational Linguistics.
Abby Levenberg, Chris Callison-Burch, and Miles Os-
borne. 2010. Stream-based translation models for
statistical machine translation. In Human Language
Technologies: The 2010 Annual Conference of the
North American Chapter of the Association for Com-
putational Linguistics, HLT ’10, pages 394–402. As-
sociation for Computational Linguistics.
Ping Li, Trevor J. Hastie, and Kenneth W. Church. 2006.
Very sparse random projections. In Proceedings of
the 12th ACM SIGKDD international conference on
Knowledge discovery and data mining, KDD ’06,
pages 287–296. ACM.
Ping Li, Kenneth Ward Church, and Trevor Hastie. 2008.
One sketch for all: Theory and application of condi-
tional random sampling. In Neural Information Pro-
cessing Systems, pages 953–960.
Tara McIntosh and James R Curran. 2008. Weighted
mutual exclusion bootstrapping for domain indepen-
dent lexicon and template acquisition. In Proceedings
of the Australasian Language Technology Association
Workshop 2008, pages 97–105, December.
G.A. Miller and W.G. Charles. 1991. Contextual corre-
lates of semantic similarity. Language and Cognitive
Processes, 6(1):1–28.
Patrick Pantel, Eric Crestan, Arkady Borkovsky, Ana-
Maria Popescu, and Vishnu Vyas. 2009. Web-scale
distributional similarity and entity set expansion. In
Proceedings of EMNLP.
Delip Rao and Deepak Ravichandran. 2009. Semi-
supervised polarity lexicon induction. In Proceedings
of the 12th Conference of the European Chapter of the
ACL (EACL 2009), pages 675–682, Athens, Greece,
March. Association for Computational Linguistics.
Deepak Ravichandran, Patrick Pantel, and Eduard Hovy.
2005. Randomized algorithms and nlp: using locality
sensitive hash function for high speed noun clustering.
In Proceedings of ACL.
H. Rubenstein and J.B. Goodenough. 1965. Contextual
correlates of synonymy. Computational Linguistics,
8:627–633.
Florin Rusu and Alin Dobra. 2007. Statistical analysis of
sketch estimators. In SIGMOD ’07. ACM.
</reference>
<page confidence="0.917469">
1079
</page>
<reference confidence="0.999424380952381">
M. Thelen and E. Riloff. 2002. A Bootstrapping Method
for Learning Semantic Lexicons Using Extraction Pat-
tern Contexts. In Proceedings of the Empirical Meth-
ods in Natural Language Processing, pages 214–221.
Peter D. Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of seman-
tics. JOURNAL OF ARTIFICIAL INTELLIGENCE
RESEARCH, 37:141.
Peter Turney, Yair Neuman, Dan Assaf, and Yohai Co-
hen. 2011. Literal and metaphorical sense identifi-
cation through concrete and abstract context. In Pro-
ceedings of the 2011 Conference on Empirical Meth-
ods in Natural Language Processing, pages 680–690.
Association for Computational Linguistics.
Benjamin Van Durme and Ashwin Lall. 2009a. Prob-
abilistic counting with randomized storage. In IJ-
CAI’09: Proceedings of the 21st international jont
conference on Artifical intelligence.
Benjamin Van Durme and Ashwin Lall. 2009b. Stream-
ing pointwise mutual information. In Advances in
Neural Information Processing Systems 22.
Benjamin Van Durme and Ashwin Lall. 2010. Online
generation of locality sensitive hash signatures. In
Proceedings of the ACL 2010 Conference Short Pa-
pers, pages 231–235, July.
Benjamin Van Durme and Ashwin Lall. 2011. Efficient
online locality sensitive hashing via reservoir count-
ing. In Proceedings of the ACL 2011 Conference Short
Papers, June.
Leonid Velikovich, Sasha Blair-Goldensohn, Kerry Han-
nan, and Ryan McDonald. 2010. The viability of web-
derived polarity lexicons. In Human Language Tech-
nologies: The 2010 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 777–785, Los Angeles, Cal-
ifornia, June. Association for Computational Linguis-
tics.
Kilian Weinberger, Anirban Dasgupta, John Langford,
Alex Smola, and Josh Attenberg. 2009. Feature hash-
ing for large scale multitask learning. In Proceedings
of the 26th Annual International Conference on Ma-
chine Learning, ICML ’09, pages 1113–1120. ACM.
</reference>
<page confidence="0.993722">
1080
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.952725">
<title confidence="0.999973">Fast Large-Scale Approximate Graph Construction for NLP</title>
<author confidence="0.996755">Amit Goyal</author>
<author confidence="0.996755">Hal Daum´e</author>
<affiliation confidence="0.9999785">Dept. of Computer University of Maryland</affiliation>
<address confidence="0.992046">College Park, MD</address>
<author confidence="0.997317">Raul Guerra</author>
<affiliation confidence="0.9999725">Dept. of Computer University of Maryland</affiliation>
<address confidence="0.993583">College Park, MD</address>
<email confidence="0.999449">rguerra@cs.umd.edu</email>
<abstract confidence="0.9984635">Many natural language processing problems involve constructing large nearest-neighbor We propose a system called to construct such graphs approximately from large data sets. To handle the large amount of data, our algorithm maintains approximate counts based on sketching algorithms. To find the approximate nearest neighbors, our algorithm pairs a new distributed online-PMI algorithm with novel fast approximate nearest neighbor search algorithms (variants of These algorithms return the approximate nearest neighbors quickly. We show our system’s efficiency in both intrinsic and extrinsic experiments. We further evaluate our fast search algorithms both quantitatively and qualitatively on two NLP applications.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Dimitris Achlioptas</author>
</authors>
<title>Database-friendly random projections: Johnson-lindenstrauss with binary coins.</title>
<date>2003</date>
<journal>J. Comput. Syst. Sci.,</journal>
<volume>66</volume>
<issue>4</issue>
<contexts>
<context position="5024" citStr="Achlioptas, 2003" startWordPosition="742" endWordPosition="743">ection 2) which makes CM sketch to be implemented in distributed setting for large data sets. In our implementation, FLAG scaled up to 110 GB of web data with 866 million sentences in less than 2 days using 100 quadcore nodes. Our intrinsic and extrinsic experiments demonstrate the effectiveness of distributed onlinePMI. After generating context vectors from distributed online-PMI algorithm, our goal is to use them to find fast approximate nearest neighbors for all words. To achieve this goal, we exploit recent developments in the area of existing randomized algorithms for random projections (Achlioptas, 2003; Li et al., 2006), Locality Sensitive Hashing (LSH) (Charikar, 2002) and improve on previous work done on PLEB (Point Location in Equal Balls) (Indyk and Motwani, 1998; Charikar, 2002). We propose novel variants of PLEB to address the issue of reducing the pre-processing time for PLEB. One of the variants of PLEB (FASTPLEB) with considerably less pre-processing time has effectiveness comparable to PLEB. We evaluate these variants of PLEB both quantitatively and qualitatively on large data sets. Finally, we show the applicability of large-scale graphs built from FLAG on two applications: the G</context>
<context position="18467" citStr="Achlioptas (2003)" startWordPosition="3137" endWordPosition="3138">ndom projection values from smallest to largest. Third matrix throws away the projection values leaving only the words. Fourth matrix maps the words 1 · · · Z to their sorted position in the third matrix for each k. This allows constant query time for all the words. is motivated by the work on stable random projections (Li et al., 2006; Li et al., 2008), Count sketch (Charikar et al., 2004), feature hashing (Weinberger et al., 2009) and online Locality Sensitive Hashing (LSH) (Van Durme and Lall, 2010). The idea of generating random projections from the set 1−1, +11 was originally proposed by Achlioptas (2003). Next we create a binary matrix B using matrix A by taking sign of each of the entries of the matrix A. If A(i, j) &gt; 0, then B(i, j) = 1; else B(i, j) = 0. This binarization creates Locality Sensitive Hash (LSH) function that preserves the cosine similarity between every pair of word vectors. This idea was first proposed by Charikar (2002) and used in NLP for large-scale noun clustering (Ravichandran et al., 2005). However, in large-scale noun clustering work, their approach had to store the random projection matrix of size D x k; where D denotes the number of all unique contexts (which is ge</context>
</contexts>
<marker>Achlioptas, 2003</marker>
<rawString>Dimitris Achlioptas. 2003. Database-friendly random projections: Johnson-lindenstrauss with binary coins. J. Comput. Syst. Sci., 66(4):671–687.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Enrique Alfonseca</author>
<author>Keith Hall</author>
<author>Jana Kravalova</author>
<author>Marius Pas¸ca</author>
<author>Aitor Soroa</author>
</authors>
<title>A study on similarity and relatedness using distributional and wordnet-based approaches.</title>
<date>2009</date>
<booktitle>In NAACL ’09: Proceedings of HLT-NAACL.</booktitle>
<marker>Agirre, Alfonseca, Hall, Kravalova, Pas¸ca, Soroa, 2009</marker>
<rawString>Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana Kravalova, Marius Pas¸ca, and Aitor Soroa. 2009. A study on similarity and relatedness using distributional and wordnet-based approaches. In NAACL ’09: Proceedings of HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Moses Charikar</author>
<author>Kevin Chen</author>
<author>Martin Farach-Colton</author>
</authors>
<title>Finding frequent items in data streams.</title>
<date>2004</date>
<journal>Theor. Comput. Sci.,</journal>
<pages>312--3</pages>
<contexts>
<context position="6180" citStr="Charikar et al., 2004" startWordPosition="920" endWordPosition="923"> of large-scale graphs built from FLAG on two applications: the Google-Sets problem (Ghahramani and Heller, 2005), and learning concrete and abstract words (Turney et al., 2011). 2 Count-Min sketch The Count-Min (CM) sketch (Cormode and Muthukrishnan, 2004) belongs to a class of ‘sketch’ algorithms that represents a large data set with a compact summary, typically much smaller than the full size of the input by processing the data in one pass. The following surveys comprehensively review the streaming literature (Rusu and Dobra, 2007; Cormode and Hadjieleftheriou, 2008) and sketch techniques (Charikar et al., 2004; Li et al., 2008; Cormode and Muthukrishnan, 2004; Rusu and Dobra, 2007). In our another recent paper (Goyal et al., 2012), we conducted a systematic study and compare many sketch techniques which answer point queries with focus on large-scale NLP tasks. In that paper, we empirically demonstrated that CM sketch performs the best among all the sketches on three large-scale NLP tasks. CM sketch uses hashing to store the approximate frequencies of all items from the large data set onto a small sketch vector that can be updated and queried in constant time. CM has two parameters E and 6: E contro</context>
<context position="18243" citStr="Charikar et al., 2004" startWordPosition="3100" endWordPosition="3103"> . z1 zZ ··· zu (c) Matrix A z1 z2 ··· zZ 2 60 ··· 1 55 2 ··· Z ... . .. .... . . 1 90 ··· 2 (d) Matrix C Figure 1: First matrix pairs the words 1 · · · Z and their random projection values. Second matrix sorts each row by the random projection values from smallest to largest. Third matrix throws away the projection values leaving only the words. Fourth matrix maps the words 1 · · · Z to their sorted position in the third matrix for each k. This allows constant query time for all the words. is motivated by the work on stable random projections (Li et al., 2006; Li et al., 2008), Count sketch (Charikar et al., 2004), feature hashing (Weinberger et al., 2009) and online Locality Sensitive Hashing (LSH) (Van Durme and Lall, 2010). The idea of generating random projections from the set 1−1, +11 was originally proposed by Achlioptas (2003). Next we create a binary matrix B using matrix A by taking sign of each of the entries of the matrix A. If A(i, j) &gt; 0, then B(i, j) = 1; else B(i, j) = 0. This binarization creates Locality Sensitive Hash (LSH) function that preserves the cosine similarity between every pair of word vectors. This idea was first proposed by Charikar (2002) and used in NLP for large-scale n</context>
</contexts>
<marker>Charikar, Chen, Farach-Colton, 2004</marker>
<rawString>Moses Charikar, Kevin Chen, and Martin Farach-Colton. 2004. Finding frequent items in data streams. Theor. Comput. Sci., 312:3–15, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Moses S Charikar</author>
</authors>
<title>Similarity estimation techniques from rounding algorithms. In</title>
<date>2002</date>
<booktitle>In Proc. of 34th STOC,</booktitle>
<pages>380--388</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="2656" citStr="Charikar, 2002" startWordPosition="383" endWordPosition="384">over a 200 billion words in 50 hours using 100 quad-core nodes, explicitly storing a similarity matrix between 500 million terms. In this work, we propose Fast Large-Scale Approximate Graph (FLAG) construction, a system that constructs a fast large-scale approximate nearest-neighbor graph from a large text corpus. To build this system, we exploit recent developments in the area of approximation, randomization and streaming for large-scale NLP problems (Ravichandran et al., 2005; Goyal et al., 2009; Levenberg et al., 2010). More specifically we exploit work on Locality Sensitive Hashing (LSH) (Charikar, 2002) for computing word-pair similarities from large text collections (Ravichandran et al., 2005; Van Durme and Lall, 2010). However, Ravichandran et al. (2005) approach stored an enormous matrix of all unique words and their contexts in main memory, which is infeasible for very large data sets. A more efficient online framework to locality sensitive hashing (Van Durme and Lall, 2010; Van Durme and Lall, 2011) computes distributional similarity in a streaming setting. Unfortunately, their approach can handle only additive features like raw-counts, and not non-linear association scores like pointwi</context>
<context position="5093" citStr="Charikar, 2002" startWordPosition="752" endWordPosition="753">ng for large data sets. In our implementation, FLAG scaled up to 110 GB of web data with 866 million sentences in less than 2 days using 100 quadcore nodes. Our intrinsic and extrinsic experiments demonstrate the effectiveness of distributed onlinePMI. After generating context vectors from distributed online-PMI algorithm, our goal is to use them to find fast approximate nearest neighbors for all words. To achieve this goal, we exploit recent developments in the area of existing randomized algorithms for random projections (Achlioptas, 2003; Li et al., 2006), Locality Sensitive Hashing (LSH) (Charikar, 2002) and improve on previous work done on PLEB (Point Location in Equal Balls) (Indyk and Motwani, 1998; Charikar, 2002). We propose novel variants of PLEB to address the issue of reducing the pre-processing time for PLEB. One of the variants of PLEB (FASTPLEB) with considerably less pre-processing time has effectiveness comparable to PLEB. We evaluate these variants of PLEB both quantitatively and qualitatively on large data sets. Finally, we show the applicability of large-scale graphs built from FLAG on two applications: the Google-Sets problem (Ghahramani and Heller, 2005), and learning concre</context>
<context position="18809" citStr="Charikar (2002)" startWordPosition="3203" endWordPosition="3204"> al., 2008), Count sketch (Charikar et al., 2004), feature hashing (Weinberger et al., 2009) and online Locality Sensitive Hashing (LSH) (Van Durme and Lall, 2010). The idea of generating random projections from the set 1−1, +11 was originally proposed by Achlioptas (2003). Next we create a binary matrix B using matrix A by taking sign of each of the entries of the matrix A. If A(i, j) &gt; 0, then B(i, j) = 1; else B(i, j) = 0. This binarization creates Locality Sensitive Hash (LSH) function that preserves the cosine similarity between every pair of word vectors. This idea was first proposed by Charikar (2002) and used in NLP for large-scale noun clustering (Ravichandran et al., 2005). However, in large-scale noun clustering work, their approach had to store the random projection matrix of size D x k; where D denotes the number of all unique contexts (which is generally large and D &gt;&gt; Z) and in this paper, we do not explicitly require storing a random projection matrix. 3.3 Representation for Fast-Search We describe three approaches to represent the data (matrix A and B from Section 3.2) in such a manner that finding nearest neighbors is fast. These three approaches differ in amount of pre-processi</context>
<context position="20854" citStr="Charikar (2002)" startWordPosition="3560" endWordPosition="3561">ogZ) time (We can assume k to be a constant). The sorting operation puts all the nearest neighbor words (for each k independent projections) next to each other. After sorting the matrix A, we throw away the projection values leaving only the words (see Fig. 1(c)). To search for a word in matrix A in constant time, we create another matrix C of size (k x Z) (see Fig. 1(d)). Matrix C maps the words z1 · · · zZ to their sorted position in the matrix A (see Fig. 1(c)) for each k. 3.3.2 PLEB PLEB (Point Location in Equal Balls) was first proposed by Indyk and Motwani (1998) and further improved by Charikar (2002). The improved PLEB algorithm puts in operation all k random projections together. It randomly permutes the ordering of k binary LSH bits (stored in matrix B) for all the words p times. For each permutation it sorts all the words lexicographically based on their permuted LSH representation of size k. The sorting operation puts all the nearest neighbor words (using k projections together) next to each other for all the permutations. In practice p is generally large, Ravichandran et al. (2005) used p = 1000 in their work. In our implementation of PLEB, we have a matrix A of size (p x Z) similar </context>
</contexts>
<marker>Charikar, 2002</marker>
<rawString>Moses S. Charikar. 2002. Similarity estimation techniques from rounding algorithms. In In Proc. of 34th STOC, pages 380–388. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Church</author>
<author>P Hanks</author>
</authors>
<title>Word Association Norms, Mutual Information and Lexicography.</title>
<date>1989</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>76--83</pages>
<location>Vancouver, Canada,</location>
<contexts>
<context position="27693" citStr="Church and Hanks, 1989" startWordPosition="4754" endWordPosition="4757">irs. WS-203: A subset of WS-353 with 203 word pairs (Agirre et al., 2009). RG-65: (Rubenstein and Goodenough, 1965) has 65 word pairs. MC-30: A subset of RG-65 dataset with 30 word pairs (Miller and Charles, 1991). The results in Table 2 shows that by using distributed online-PMI (by making a single pass over the corpus) is comparable to offline-PMI (which is computed by making two passes over the corpus). For generating context vectors from GW, for both offline-PMI and online-PMI, we use a frequency cutoff of 5 for word-context pairs to throw away the rare terms as they are sensitive to PMI (Church and Hanks, 1989). Next, FLAG generates online-PMI vectors from GWB50 and GWB100 and uses frequency cutoffs of 15 and 25. The higher frequency cutoffs are selected based on the intuition that, with more data, we get more noise, and hence not considering word-context pairs with frequency less than 25 will be better for the system. As FLAG is going to use the context vectors to find nearest neighbors, we also throw away all those words which have &lt; 50 contexts associated with them. This generates context vectors for 57,930 words from GW; 95, 626 from GWB50 and 106,733 from GWB100. Test Set WS-353 WS-203 RG-65 MC</context>
</contexts>
<marker>Church, Hanks, 1989</marker>
<rawString>K. Church and P. Hanks. 1989. Word Association Norms, Mutual Information and Lexicography. In Proceedings of ACL, pages 76–83, Vancouver, Canada, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graham Cormode</author>
<author>Marios Hadjieleftheriou</author>
</authors>
<title>Finding frequent items in data streams.</title>
<date>2008</date>
<booktitle>In VLDB.</booktitle>
<contexts>
<context position="6135" citStr="Cormode and Hadjieleftheriou, 2008" startWordPosition="913" endWordPosition="916">vely on large data sets. Finally, we show the applicability of large-scale graphs built from FLAG on two applications: the Google-Sets problem (Ghahramani and Heller, 2005), and learning concrete and abstract words (Turney et al., 2011). 2 Count-Min sketch The Count-Min (CM) sketch (Cormode and Muthukrishnan, 2004) belongs to a class of ‘sketch’ algorithms that represents a large data set with a compact summary, typically much smaller than the full size of the input by processing the data in one pass. The following surveys comprehensively review the streaming literature (Rusu and Dobra, 2007; Cormode and Hadjieleftheriou, 2008) and sketch techniques (Charikar et al., 2004; Li et al., 2008; Cormode and Muthukrishnan, 2004; Rusu and Dobra, 2007). In our another recent paper (Goyal et al., 2012), we conducted a systematic study and compare many sketch techniques which answer point queries with focus on large-scale NLP tasks. In that paper, we empirically demonstrated that CM sketch performs the best among all the sketches on three large-scale NLP tasks. CM sketch uses hashing to store the approximate frequencies of all items from the large data set onto a small sketch vector that can be updated and queried in constant </context>
</contexts>
<marker>Cormode, Hadjieleftheriou, 2008</marker>
<rawString>Graham Cormode and Marios Hadjieleftheriou. 2008. Finding frequent items in data streams. In VLDB.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graham Cormode</author>
<author>S Muthukrishnan</author>
</authors>
<title>An improved data stream summary: The count-min sketch and its applications.</title>
<date>2004</date>
<journal>J. Algorithms.</journal>
<contexts>
<context position="4090" citStr="Cormode and Muthukrishnan, 2004" startWordPosition="589" endWordPosition="592">ovel distributed online-PMI algorithm (Section 3.1). It is a streaming method that processes large data sets in one pass while distributing the data over commodity clusters 1069 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1069–1080, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics and returns context vectors weighted by pointwise mutual information (PMI) for all the words. Our distributed online-PMI algorithm makes use of the Count-Min (CM) sketch algorithm (Cormode and Muthukrishnan, 2004) (previously shown effective for computing distributional similarity in our earlier work (Goyal and Daum´e III, 2011)) to store the counts of all words, contexts and word-context pairs using only 8GB of main memory. The main motivation for using the CM sketch comes from its linearity property (see last paragraph of Section 2) which makes CM sketch to be implemented in distributed setting for large data sets. In our implementation, FLAG scaled up to 110 GB of web data with 866 million sentences in less than 2 days using 100 quadcore nodes. Our intrinsic and extrinsic experiments demonstrate the</context>
<context position="5816" citStr="Cormode and Muthukrishnan, 2004" startWordPosition="863" endWordPosition="866">ni, 1998; Charikar, 2002). We propose novel variants of PLEB to address the issue of reducing the pre-processing time for PLEB. One of the variants of PLEB (FASTPLEB) with considerably less pre-processing time has effectiveness comparable to PLEB. We evaluate these variants of PLEB both quantitatively and qualitatively on large data sets. Finally, we show the applicability of large-scale graphs built from FLAG on two applications: the Google-Sets problem (Ghahramani and Heller, 2005), and learning concrete and abstract words (Turney et al., 2011). 2 Count-Min sketch The Count-Min (CM) sketch (Cormode and Muthukrishnan, 2004) belongs to a class of ‘sketch’ algorithms that represents a large data set with a compact summary, typically much smaller than the full size of the input by processing the data in one pass. The following surveys comprehensively review the streaming literature (Rusu and Dobra, 2007; Cormode and Hadjieleftheriou, 2008) and sketch techniques (Charikar et al., 2004; Li et al., 2008; Cormode and Muthukrishnan, 2004; Rusu and Dobra, 2007). In our another recent paper (Goyal et al., 2012), we conducted a systematic study and compare many sketch techniques which answer point queries with focus on lar</context>
</contexts>
<marker>Cormode, Muthukrishnan, 2004</marker>
<rawString>Graham Cormode and S. Muthukrishnan. 2004. An improved data stream summary: The count-min sketch and its applications. J. Algorithms.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dipanjan Das</author>
<author>Slav Petrov</author>
</authors>
<title>Unsupervised part-of-speech tagging with bilingual graph-based projections.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>600--609</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="1382" citStr="Das and Petrov, 2011" startWordPosition="188" endWordPosition="191">EB). These algorithms return the approximate nearest neighbors quickly. We show our system’s efficiency in both intrinsic and extrinsic experiments. We further evaluate our fast search algorithms both quantitatively and qualitatively on two NLP applications. 1 Introduction Many natural language processing (NLP) problems involve graph construction. Examples include constructing polarity lexicons based on lexical graphs from WordNet (Rao and Ravichandran, 2009), constructing polarity lexicons from web data (Velikovich et al., 2010) and unsupervised part-ofspeech tagging using label propagation (Das and Petrov, 2011). The later two approaches construct nearest-neighbor graphs between word pairs by computing nearest neighbors between word pairs from large corpora. These nearest neighbors form the edges of the graph, with weights given by the distributional similarity (Turney and Pantel, 2010) between terms. Unfortunately, computing the distributional similarity between all words in a large vocabulary is computationally and memory intensive when working with large amounts of data (Pantel et al., 2009). This bottleneck is typically addressed by means of commodity clusters. For example, Pantel et al. (2009) c</context>
<context position="38060" citStr="Das and Petrov, 2011" startWordPosition="6702" endWordPosition="6705">000, p = 1000, q = 10 and b = 40. The graph has 106, 733 nodes (words), with each node having 100 edges that denote the top l = 100 approximate nearest neighbors associated with each node. However, FLAG applied FASTPLEB (approximate search) to find these neighbors. Therefore many of these edges can be noisy for our applications. Hence for each node, we only consider top 10 edges. In general for graph-based NLP problems; for example, constructing web-derived polarity lexicons (Velikovich et al., 2010), top 25 edges were used, and for unsupervised part-of-speech tagging using label propagation (Das and Petrov, 2011), top 5 edges were used. 5.1 Google Sets Problem Google Sets problem (Ghahramani and Heller, 2005) can be defined as: given a set of query words, return top t similar words with respect to query words. To evaluate the quality of our approximate large-scale graph, we return top 25 words which have best aggregated similarity scores with respect to query words. We take 5 classes and their query terms (McIntosh and Curran, 2008) shown in Table 8 and our goal is to learn 25 new words which are similar with these 5 query words. 1077 Language: german, french, estonian, hungarian, bulgarian Place: sca</context>
</contexts>
<marker>Das, Petrov, 2011</marker>
<rawString>Dipanjan Das and Slav Petrov. 2011. Unsupervised part-of-speech tagging with bilingual graph-based projections. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 600–609, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cristian Estan</author>
<author>George Varghese</author>
</authors>
<title>New directions in traffic measurement and accounting.</title>
<date>2002</date>
<journal>SIGCOMM Comput. Commun. Rev.,</journal>
<volume>32</volume>
<issue>4</issue>
<contexts>
<context position="8799" citStr="Estan and Varghese, 2002" startWordPosition="1395" endWordPosition="1399">date and query operation. CM sketch has a linearity property which states that: Given two sketches s1 and s2 computed (us1070 ing the same parameters w and d, and the same set of d hash functions) over different input streams; the sketch of the combined data stream can be easily obtained by adding the individual sketches in O(dxw) time which is independent of the stream size. This property enables sketches to be implemented in distributed setting, where each machine computes the sketch over a small portion of the corpus and makes it scalable to large datasets. The idea of conservative update (Estan and Varghese, 2002) is to only increase counts in the sketch by the minimum amount needed to ensure that the estimate remains accurate. We (Goyal and Daum´e III, 2011) used CM sketch with conservative update (CM-CU sketch) to show that the update reduces the amount of over-estimation error by a factor of at least 1.5 on NLP data and showed the effectiveness of CM-CU on three important NLP tasks. The QUERY procedure for CM-CU is identical to CountMin. However, to UPDATE an item “x” with frequency c, first we compute the frequency c(x) of this item from the existing data structure: (b1 G k G d, c(x) = mink sketch[</context>
</contexts>
<marker>Estan, Varghese, 2002</marker>
<rawString>Cristian Estan and George Varghese. 2002. New directions in traffic measurement and accounting. SIGCOMM Comput. Commun. Rev., 32(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Finkelstein</author>
<author>E Gabrilovich</author>
<author>Y Matias</author>
<author>E Rivlin</author>
<author>Z Solan</author>
<author>G Wolfman</author>
<author>E Ruppin</author>
</authors>
<title>Placing search in context: The concept revisited. In</title>
<date>2002</date>
<journal>ACM Transactions on Information Systems.</journal>
<contexts>
<context position="27046" citStr="Finkelstein et al., 2002" startWordPosition="4640" endWordPosition="4643"> the next paragraph. On these 447 words, we achieve an average P of .97, average R of .96 and average F1 of .97 and a perfect average p of 1. This evaluation show that the vectors obtained using online-PMI are as effective as offline-PMI. Extrinsic Evaluation: We also compare onlinePMI effectiveness on four test sets which consist of word pairs, and their corresponding human rankings. We generate the word pair rankings using online-PMI and offline-PMI strategies. We report the Pearson’s correlation (p) between the human and system generated similarity rankings. The four test sets are: WS-353 (Finkelstein et al., 2002) is a set of 353 word pairs. WS-203: A subset of WS-353 with 203 word pairs (Agirre et al., 2009). RG-65: (Rubenstein and Goodenough, 1965) has 65 word pairs. MC-30: A subset of RG-65 dataset with 30 word pairs (Miller and Charles, 1991). The results in Table 2 shows that by using distributed online-PMI (by making a single pass over the corpus) is comparable to offline-PMI (which is computed by making two passes over the corpus). For generating context vectors from GW, for both offline-PMI and online-PMI, we use a frequency cutoff of 5 for word-context pairs to throw away the rare terms as the</context>
</contexts>
<marker>Finkelstein, Gabrilovich, Matias, Rivlin, Solan, Wolfman, Ruppin, 2002</marker>
<rawString>L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin, Z. Solan, G. Wolfman, and E. Ruppin. 2002. Placing search in context: The concept revisited. In ACM Transactions on Information Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zoubin Ghahramani</author>
<author>Katherine A Heller</author>
</authors>
<title>Bayesian Sets. In</title>
<date>2005</date>
<booktitle>in Advances in Neural Information Processing Systems,</booktitle>
<volume>18</volume>
<contexts>
<context position="5672" citStr="Ghahramani and Heller, 2005" startWordPosition="841" endWordPosition="845">Locality Sensitive Hashing (LSH) (Charikar, 2002) and improve on previous work done on PLEB (Point Location in Equal Balls) (Indyk and Motwani, 1998; Charikar, 2002). We propose novel variants of PLEB to address the issue of reducing the pre-processing time for PLEB. One of the variants of PLEB (FASTPLEB) with considerably less pre-processing time has effectiveness comparable to PLEB. We evaluate these variants of PLEB both quantitatively and qualitatively on large data sets. Finally, we show the applicability of large-scale graphs built from FLAG on two applications: the Google-Sets problem (Ghahramani and Heller, 2005), and learning concrete and abstract words (Turney et al., 2011). 2 Count-Min sketch The Count-Min (CM) sketch (Cormode and Muthukrishnan, 2004) belongs to a class of ‘sketch’ algorithms that represents a large data set with a compact summary, typically much smaller than the full size of the input by processing the data in one pass. The following surveys comprehensively review the streaming literature (Rusu and Dobra, 2007; Cormode and Hadjieleftheriou, 2008) and sketch techniques (Charikar et al., 2004; Li et al., 2008; Cormode and Muthukrishnan, 2004; Rusu and Dobra, 2007). In our another re</context>
<context position="38158" citStr="Ghahramani and Heller, 2005" startWordPosition="6718" endWordPosition="6721">ng 100 edges that denote the top l = 100 approximate nearest neighbors associated with each node. However, FLAG applied FASTPLEB (approximate search) to find these neighbors. Therefore many of these edges can be noisy for our applications. Hence for each node, we only consider top 10 edges. In general for graph-based NLP problems; for example, constructing web-derived polarity lexicons (Velikovich et al., 2010), top 25 edges were used, and for unsupervised part-of-speech tagging using label propagation (Das and Petrov, 2011), top 5 edges were used. 5.1 Google Sets Problem Google Sets problem (Ghahramani and Heller, 2005) can be defined as: given a set of query words, return top t similar words with respect to query words. To evaluate the quality of our approximate large-scale graph, we return top 25 words which have best aggregated similarity scores with respect to query words. We take 5 classes and their query terms (McIntosh and Curran, 2008) shown in Table 8 and our goal is to learn 25 new words which are similar with these 5 query words. 1077 Language: german, french, estonian, hungarian, bulgarian Place: scandinavia, mongolia, mozambique, zambia, namibia Nationality: german, hungarian, estonian, latvian,</context>
</contexts>
<marker>Ghahramani, Heller, 2005</marker>
<rawString>Zoubin Ghahramani and Katherine A. Heller. 2005. Bayesian Sets. In in Advances in Neural Information Processing Systems, volume 18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amit Goyal</author>
<author>Hal Daum´e</author>
</authors>
<title>Approximate scalable bounded space sketch for large data NLP.</title>
<date>2011</date>
<booktitle>In Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<marker>Goyal, Daum´e, 2011</marker>
<rawString>Amit Goyal and Hal Daum´e III. 2011. Approximate scalable bounded space sketch for large data NLP. In Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amit Goyal</author>
<author>Hal Daum´e</author>
<author>Suresh Venkatasubramanian</author>
</authors>
<title>Streaming for large scale NLP: Language modeling.</title>
<date>2009</date>
<booktitle>In NAACL.</booktitle>
<marker>Goyal, Daum´e, Venkatasubramanian, 2009</marker>
<rawString>Amit Goyal, Hal Daum´e III, and Suresh Venkatasubramanian. 2009. Streaming for large scale NLP: Language modeling. In NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amit Goyal</author>
<author>Graham Cormode</author>
<author>Hal Daum´e</author>
</authors>
<title>Sketch algorithms for estimating point queries in NLP.</title>
<date>2012</date>
<booktitle>In Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<marker>Goyal, Cormode, Daum´e, 2012</marker>
<rawString>Amit Goyal, Graham Cormode, and Hal Daum´e III. 2012. Sketch algorithms for estimating point queries in NLP. In Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Graff</author>
</authors>
<title>English Gigaword. Linguistic Data Consortium,</title>
<date>2003</date>
<location>Philadelphia, PA,</location>
<contexts>
<context position="24340" citStr="Graff, 2003" startWordPosition="4182" endWordPosition="4183">neighbors. For computing hamming distance, all the approaches discussed in Section 3.3 require all k random projection bits. 4 Experiments We evaluate our system FLAG for fast large-scale approximate graph construction. First, we show that using distributed online-PMI algorithm is as effective as offline-PMI. Second, we compare the approximate nearest neighbors lists generated by FLAG against the exact nearest neighbor lists. Finally, we show the quality of our approximate similarity lists generated by FLAG from the web corpus. 4.1 Experimental Setup Data sets: We use two data sets: Gigaword (Graff, 2003) and a copy of news web (Ravichandran et al., 2005). For both the corpora, we split the text into sentences, tokenize and convert into lower-case. To evaluate our approximate graph construction, we evaluate on three data sets: Gigaword (GW), Gigaword + 50% of web data (GWB50) and Gigaword + 100% ((GWB100)) of web data. Corpus statistics are shown in Table 1. We define the context for a given word “z” as the surrounding words appearing in a window of 2 words to the left and 2 words to the right. The context words are concatenated along with their positions -2, -1, +1, and +2. Corpus GW GWB50 GW</context>
</contexts>
<marker>Graff, 2003</marker>
<rawString>D. Graff. 2003. English Gigaword. Linguistic Data Consortium, Philadelphia, PA, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Piotr Indyk</author>
<author>Rajeev Motwani</author>
</authors>
<title>Approximate nearest neighbors: towards removing the curse of dimensionality.</title>
<date>1998</date>
<booktitle>In Proceedings of the thirtieth annual ACM symposium on Theory of computing, STOC ’98,</booktitle>
<pages>604--613</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="5192" citStr="Indyk and Motwani, 1998" startWordPosition="767" endWordPosition="770">6 million sentences in less than 2 days using 100 quadcore nodes. Our intrinsic and extrinsic experiments demonstrate the effectiveness of distributed onlinePMI. After generating context vectors from distributed online-PMI algorithm, our goal is to use them to find fast approximate nearest neighbors for all words. To achieve this goal, we exploit recent developments in the area of existing randomized algorithms for random projections (Achlioptas, 2003; Li et al., 2006), Locality Sensitive Hashing (LSH) (Charikar, 2002) and improve on previous work done on PLEB (Point Location in Equal Balls) (Indyk and Motwani, 1998; Charikar, 2002). We propose novel variants of PLEB to address the issue of reducing the pre-processing time for PLEB. One of the variants of PLEB (FASTPLEB) with considerably less pre-processing time has effectiveness comparable to PLEB. We evaluate these variants of PLEB both quantitatively and qualitatively on large data sets. Finally, we show the applicability of large-scale graphs built from FLAG on two applications: the Google-Sets problem (Ghahramani and Heller, 2005), and learning concrete and abstract words (Turney et al., 2011). 2 Count-Min sketch The Count-Min (CM) sketch (Cormode </context>
<context position="19616" citStr="Indyk and Motwani, 1998" startWordPosition="3335" endWordPosition="3338">size D x k; where D denotes the number of all unique contexts (which is generally large and D &gt;&gt; Z) and in this paper, we do not explicitly require storing a random projection matrix. 3.3 Representation for Fast-Search We describe three approaches to represent the data (matrix A and B from Section 3.2) in such a manner that finding nearest neighbors is fast. These three approaches differ in amount of pre-processing time. First, we propose a naive baseline approach using random projections independently with the best preprocessing time. Second, we describe PLEB (Point Location in Equal Balls) (Indyk and Motwani, 1998; Charikar, 2002) with the worst pre-processing time. Third, we propose a variant of PLEB to reduce its pre-processing time. 3.3.1 Independent Random Projections (IRP) Here, we describe a naive baseline approach to arrange nearest neighbors next to each other by using Independent Random Projections (IRP). In this approach, we pre-process the matrix A. First for matrix A, we pair the words z1 · · · zZ and their random projection values as shown in Fig. 1(a). Second, we sort the elements of each row of matrix A by their random projection values from smallest to largest (shown in Fig. 1(b)). The </context>
</contexts>
<marker>Indyk, Motwani, 1998</marker>
<rawString>Piotr Indyk and Rajeev Motwani. 1998. Approximate nearest neighbors: towards removing the curse of dimensionality. In Proceedings of the thirtieth annual ACM symposium on Theory of computing, STOC ’98, pages 604–613. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zornitsa Kozareva</author>
<author>Ellen Riloff</author>
<author>Eduard Hovy</author>
</authors>
<title>Semantic class learning from the web with hyponym pattern linkage graphs.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>1048--1056</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<contexts>
<context position="39862" citStr="Kozareva et al., 2008" startWordPosition="6982" endWordPosition="6985">uited 1 annotator and developed annotation guidelines that instructed each recruiter to judge whether learned values are similar to query words or not. Overall the annotator found almost all the learned words to be similar to the query words. However, the algorithm can not differentiate between different senses of the word. For example, “French” can be a language and a nationality. Table 9 shows the top ranked words with respect to query words. 5.2 Learning Concrete and Abstract Words Our goal is to automatically learn concrete and abstract words (Turney et al., 2011). We apply bootstrapping (Kozareva et al., 2008) on the word graphs by manually selecting 10 seeds for concrete and abstract words (see Table 10). We use in-degree (sum of weights of incoming edges) to compute the score for each node which has connections with known (seeds) or automatically labeled nodes, previously exploited to learn hyponymy relations from the web (Kozareva et al., 2008). We learn concrete and abstract words together (known as mutual exclusion principle in bootstrapping (Thelen and Riloff, 2002; McIntosh and Curran, 2008)), and each word is assigned to only one class. Moreover, after each iteration, we harmonically decrea</context>
</contexts>
<marker>Kozareva, Riloff, Hovy, 2008</marker>
<rawString>Zornitsa Kozareva, Ellen Riloff, and Eduard Hovy. 2008. Semantic class learning from the web with hyponym pattern linkage graphs. In Proceedings of ACL-08: HLT, pages 1048–1056, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Abby Levenberg</author>
<author>Chris Callison-Burch</author>
<author>Miles Osborne</author>
</authors>
<title>Stream-based translation models for statistical machine translation.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT ’10,</booktitle>
<pages>394--402</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2568" citStr="Levenberg et al., 2010" startWordPosition="368" endWordPosition="371">. For example, Pantel et al. (2009) compute distributional similarity between 500 million terms over a 200 billion words in 50 hours using 100 quad-core nodes, explicitly storing a similarity matrix between 500 million terms. In this work, we propose Fast Large-Scale Approximate Graph (FLAG) construction, a system that constructs a fast large-scale approximate nearest-neighbor graph from a large text corpus. To build this system, we exploit recent developments in the area of approximation, randomization and streaming for large-scale NLP problems (Ravichandran et al., 2005; Goyal et al., 2009; Levenberg et al., 2010). More specifically we exploit work on Locality Sensitive Hashing (LSH) (Charikar, 2002) for computing word-pair similarities from large text collections (Ravichandran et al., 2005; Van Durme and Lall, 2010). However, Ravichandran et al. (2005) approach stored an enormous matrix of all unique words and their contexts in main memory, which is infeasible for very large data sets. A more efficient online framework to locality sensitive hashing (Van Durme and Lall, 2010; Van Durme and Lall, 2011) computes distributional similarity in a streaming setting. Unfortunately, their approach can handle on</context>
</contexts>
<marker>Levenberg, Callison-Burch, Osborne, 2010</marker>
<rawString>Abby Levenberg, Chris Callison-Burch, and Miles Osborne. 2010. Stream-based translation models for statistical machine translation. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT ’10, pages 394–402. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ping Li</author>
<author>Trevor J Hastie</author>
<author>Kenneth W Church</author>
</authors>
<title>Very sparse random projections.</title>
<date>2006</date>
<booktitle>In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, KDD ’06,</booktitle>
<pages>287--296</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="5042" citStr="Li et al., 2006" startWordPosition="744" endWordPosition="747">kes CM sketch to be implemented in distributed setting for large data sets. In our implementation, FLAG scaled up to 110 GB of web data with 866 million sentences in less than 2 days using 100 quadcore nodes. Our intrinsic and extrinsic experiments demonstrate the effectiveness of distributed onlinePMI. After generating context vectors from distributed online-PMI algorithm, our goal is to use them to find fast approximate nearest neighbors for all words. To achieve this goal, we exploit recent developments in the area of existing randomized algorithms for random projections (Achlioptas, 2003; Li et al., 2006), Locality Sensitive Hashing (LSH) (Charikar, 2002) and improve on previous work done on PLEB (Point Location in Equal Balls) (Indyk and Motwani, 1998; Charikar, 2002). We propose novel variants of PLEB to address the issue of reducing the pre-processing time for PLEB. One of the variants of PLEB (FASTPLEB) with considerably less pre-processing time has effectiveness comparable to PLEB. We evaluate these variants of PLEB both quantitatively and qualitatively on large data sets. Finally, we show the applicability of large-scale graphs built from FLAG on two applications: the Google-Sets problem</context>
<context position="18187" citStr="Li et al., 2006" startWordPosition="3090" endWordPosition="3093">2 ··· Z zZ z1 ··· zm zr z2 ··· zZ ... . .. .... . . z1 zZ ··· zu (c) Matrix A z1 z2 ··· zZ 2 60 ··· 1 55 2 ··· Z ... . .. .... . . 1 90 ··· 2 (d) Matrix C Figure 1: First matrix pairs the words 1 · · · Z and their random projection values. Second matrix sorts each row by the random projection values from smallest to largest. Third matrix throws away the projection values leaving only the words. Fourth matrix maps the words 1 · · · Z to their sorted position in the third matrix for each k. This allows constant query time for all the words. is motivated by the work on stable random projections (Li et al., 2006; Li et al., 2008), Count sketch (Charikar et al., 2004), feature hashing (Weinberger et al., 2009) and online Locality Sensitive Hashing (LSH) (Van Durme and Lall, 2010). The idea of generating random projections from the set 1−1, +11 was originally proposed by Achlioptas (2003). Next we create a binary matrix B using matrix A by taking sign of each of the entries of the matrix A. If A(i, j) &gt; 0, then B(i, j) = 1; else B(i, j) = 0. This binarization creates Locality Sensitive Hash (LSH) function that preserves the cosine similarity between every pair of word vectors. This idea was first propo</context>
</contexts>
<marker>Li, Hastie, Church, 2006</marker>
<rawString>Ping Li, Trevor J. Hastie, and Kenneth W. Church. 2006. Very sparse random projections. In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, KDD ’06, pages 287–296. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ping Li</author>
<author>Kenneth Ward Church</author>
<author>Trevor Hastie</author>
</authors>
<title>One sketch for all: Theory and application of conditional random sampling.</title>
<date>2008</date>
<booktitle>In Neural Information Processing Systems,</booktitle>
<pages>953--960</pages>
<contexts>
<context position="6197" citStr="Li et al., 2008" startWordPosition="924" endWordPosition="927">built from FLAG on two applications: the Google-Sets problem (Ghahramani and Heller, 2005), and learning concrete and abstract words (Turney et al., 2011). 2 Count-Min sketch The Count-Min (CM) sketch (Cormode and Muthukrishnan, 2004) belongs to a class of ‘sketch’ algorithms that represents a large data set with a compact summary, typically much smaller than the full size of the input by processing the data in one pass. The following surveys comprehensively review the streaming literature (Rusu and Dobra, 2007; Cormode and Hadjieleftheriou, 2008) and sketch techniques (Charikar et al., 2004; Li et al., 2008; Cormode and Muthukrishnan, 2004; Rusu and Dobra, 2007). In our another recent paper (Goyal et al., 2012), we conducted a systematic study and compare many sketch techniques which answer point queries with focus on large-scale NLP tasks. In that paper, we empirically demonstrated that CM sketch performs the best among all the sketches on three large-scale NLP tasks. CM sketch uses hashing to store the approximate frequencies of all items from the large data set onto a small sketch vector that can be updated and queried in constant time. CM has two parameters E and 6: E controls the amount of </context>
<context position="18205" citStr="Li et al., 2008" startWordPosition="3094" endWordPosition="3097"> zm zr z2 ··· zZ ... . .. .... . . z1 zZ ··· zu (c) Matrix A z1 z2 ··· zZ 2 60 ··· 1 55 2 ··· Z ... . .. .... . . 1 90 ··· 2 (d) Matrix C Figure 1: First matrix pairs the words 1 · · · Z and their random projection values. Second matrix sorts each row by the random projection values from smallest to largest. Third matrix throws away the projection values leaving only the words. Fourth matrix maps the words 1 · · · Z to their sorted position in the third matrix for each k. This allows constant query time for all the words. is motivated by the work on stable random projections (Li et al., 2006; Li et al., 2008), Count sketch (Charikar et al., 2004), feature hashing (Weinberger et al., 2009) and online Locality Sensitive Hashing (LSH) (Van Durme and Lall, 2010). The idea of generating random projections from the set 1−1, +11 was originally proposed by Achlioptas (2003). Next we create a binary matrix B using matrix A by taking sign of each of the entries of the matrix A. If A(i, j) &gt; 0, then B(i, j) = 1; else B(i, j) = 0. This binarization creates Locality Sensitive Hash (LSH) function that preserves the cosine similarity between every pair of word vectors. This idea was first proposed by Charikar (2</context>
</contexts>
<marker>Li, Church, Hastie, 2008</marker>
<rawString>Ping Li, Kenneth Ward Church, and Trevor Hastie. 2008. One sketch for all: Theory and application of conditional random sampling. In Neural Information Processing Systems, pages 953–960.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tara McIntosh</author>
<author>James R Curran</author>
</authors>
<title>Weighted mutual exclusion bootstrapping for domain independent lexicon and template acquisition.</title>
<date>2008</date>
<booktitle>In Proceedings of the Australasian Language Technology Association Workshop</booktitle>
<pages>97--105</pages>
<contexts>
<context position="38488" citStr="McIntosh and Curran, 2008" startWordPosition="6775" endWordPosition="6778"> for example, constructing web-derived polarity lexicons (Velikovich et al., 2010), top 25 edges were used, and for unsupervised part-of-speech tagging using label propagation (Das and Petrov, 2011), top 5 edges were used. 5.1 Google Sets Problem Google Sets problem (Ghahramani and Heller, 2005) can be defined as: given a set of query words, return top t similar words with respect to query words. To evaluate the quality of our approximate large-scale graph, we return top 25 words which have best aggregated similarity scores with respect to query words. We take 5 classes and their query terms (McIntosh and Curran, 2008) shown in Table 8 and our goal is to learn 25 new words which are similar with these 5 query words. 1077 Language: german, french, estonian, hungarian, bulgarian Place: scandinavia, mongolia, mozambique, zambia, namibia Nationality: german, hungarian, estonian, latvian, lithuanian Date: september, february, august, july, november Organization: looksmart, hotbot, lycos, webcrawler, alltheweb Table 9: Learned terms for Google Sets Problem Concrete car, house, tree, horse, animal seeds man, table, bottle, woman, computer Abstract idea, bravery, deceit, trust, dedication seeds anger, humour, luck,</context>
<context position="40360" citStr="McIntosh and Curran, 2008" startWordPosition="7062" endWordPosition="7065">goal is to automatically learn concrete and abstract words (Turney et al., 2011). We apply bootstrapping (Kozareva et al., 2008) on the word graphs by manually selecting 10 seeds for concrete and abstract words (see Table 10). We use in-degree (sum of weights of incoming edges) to compute the score for each node which has connections with known (seeds) or automatically labeled nodes, previously exploited to learn hyponymy relations from the web (Kozareva et al., 2008). We learn concrete and abstract words together (known as mutual exclusion principle in bootstrapping (Thelen and Riloff, 2002; McIntosh and Curran, 2008)), and each word is assigned to only one class. Moreover, after each iteration, we harmonically decrease the weight of the in-degree associated with instances learned in later iterations. We add 25 new instances at each iteration and ran 100 iterations of bootstrapping, yielding 2506 concrete nouns and 2498 abstract nouns. To evaluate our learned words, we searched in WordNet whether they had ‘abstraction’ or ’physical’ as their hypernym. Out of 2506 learned concrete nouns, Concrete: girl, person, bottles, wife, gentleman, microcomputer, neighbor, boy, foreigner, housewives, texan, granny, bar</context>
</contexts>
<marker>McIntosh, Curran, 2008</marker>
<rawString>Tara McIntosh and James R Curran. 2008. Weighted mutual exclusion bootstrapping for domain independent lexicon and template acquisition. In Proceedings of the Australasian Language Technology Association Workshop 2008, pages 97–105, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G A Miller</author>
<author>W G Charles</author>
</authors>
<title>Contextual correlates of semantic similarity.</title>
<date>1991</date>
<booktitle>Language and Cognitive Processes,</booktitle>
<pages>6--1</pages>
<contexts>
<context position="27283" citStr="Miller and Charles, 1991" startWordPosition="4683" endWordPosition="4686">. Extrinsic Evaluation: We also compare onlinePMI effectiveness on four test sets which consist of word pairs, and their corresponding human rankings. We generate the word pair rankings using online-PMI and offline-PMI strategies. We report the Pearson’s correlation (p) between the human and system generated similarity rankings. The four test sets are: WS-353 (Finkelstein et al., 2002) is a set of 353 word pairs. WS-203: A subset of WS-353 with 203 word pairs (Agirre et al., 2009). RG-65: (Rubenstein and Goodenough, 1965) has 65 word pairs. MC-30: A subset of RG-65 dataset with 30 word pairs (Miller and Charles, 1991). The results in Table 2 shows that by using distributed online-PMI (by making a single pass over the corpus) is comparable to offline-PMI (which is computed by making two passes over the corpus). For generating context vectors from GW, for both offline-PMI and online-PMI, we use a frequency cutoff of 5 for word-context pairs to throw away the rare terms as they are sensitive to PMI (Church and Hanks, 1989). Next, FLAG generates online-PMI vectors from GWB50 and GWB100 and uses frequency cutoffs of 15 and 25. The higher frequency cutoffs are selected based on the intuition that, with more data</context>
</contexts>
<marker>Miller, Charles, 1991</marker>
<rawString>G.A. Miller and W.G. Charles. 1991. Contextual correlates of semantic similarity. Language and Cognitive Processes, 6(1):1–28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Pantel</author>
</authors>
<title>Eric Crestan, Arkady Borkovsky, AnaMaria Popescu, and Vishnu Vyas.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<marker>Pantel, 2009</marker>
<rawString>Patrick Pantel, Eric Crestan, Arkady Borkovsky, AnaMaria Popescu, and Vishnu Vyas. 2009. Web-scale distributional similarity and entity set expansion. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Delip Rao</author>
<author>Deepak Ravichandran</author>
</authors>
<title>Semisupervised polarity lexicon induction.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the European Chapter of the ACL (EACL</booktitle>
<pages>675--682</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Athens, Greece,</location>
<contexts>
<context position="1224" citStr="Rao and Ravichandran, 2009" startWordPosition="166" endWordPosition="169">oximate nearest neighbors, our algorithm pairs a new distributed online-PMI algorithm with novel fast approximate nearest neighbor search algorithms (variants of PLEB). These algorithms return the approximate nearest neighbors quickly. We show our system’s efficiency in both intrinsic and extrinsic experiments. We further evaluate our fast search algorithms both quantitatively and qualitatively on two NLP applications. 1 Introduction Many natural language processing (NLP) problems involve graph construction. Examples include constructing polarity lexicons based on lexical graphs from WordNet (Rao and Ravichandran, 2009), constructing polarity lexicons from web data (Velikovich et al., 2010) and unsupervised part-ofspeech tagging using label propagation (Das and Petrov, 2011). The later two approaches construct nearest-neighbor graphs between word pairs by computing nearest neighbors between word pairs from large corpora. These nearest neighbors form the edges of the graph, with weights given by the distributional similarity (Turney and Pantel, 2010) between terms. Unfortunately, computing the distributional similarity between all words in a large vocabulary is computationally and memory intensive when workin</context>
</contexts>
<marker>Rao, Ravichandran, 2009</marker>
<rawString>Delip Rao and Deepak Ravichandran. 2009. Semisupervised polarity lexicon induction. In Proceedings of the 12th Conference of the European Chapter of the ACL (EACL 2009), pages 675–682, Athens, Greece, March. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deepak Ravichandran</author>
<author>Patrick Pantel</author>
<author>Eduard Hovy</author>
</authors>
<title>Randomized algorithms and nlp: using locality sensitive hash function for high speed noun clustering.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="2523" citStr="Ravichandran et al., 2005" startWordPosition="359" endWordPosition="363">ically addressed by means of commodity clusters. For example, Pantel et al. (2009) compute distributional similarity between 500 million terms over a 200 billion words in 50 hours using 100 quad-core nodes, explicitly storing a similarity matrix between 500 million terms. In this work, we propose Fast Large-Scale Approximate Graph (FLAG) construction, a system that constructs a fast large-scale approximate nearest-neighbor graph from a large text corpus. To build this system, we exploit recent developments in the area of approximation, randomization and streaming for large-scale NLP problems (Ravichandran et al., 2005; Goyal et al., 2009; Levenberg et al., 2010). More specifically we exploit work on Locality Sensitive Hashing (LSH) (Charikar, 2002) for computing word-pair similarities from large text collections (Ravichandran et al., 2005; Van Durme and Lall, 2010). However, Ravichandran et al. (2005) approach stored an enormous matrix of all unique words and their contexts in main memory, which is infeasible for very large data sets. A more efficient online framework to locality sensitive hashing (Van Durme and Lall, 2010; Van Durme and Lall, 2011) computes distributional similarity in a streaming setting</context>
<context position="18885" citStr="Ravichandran et al., 2005" startWordPosition="3213" endWordPosition="3217">g (Weinberger et al., 2009) and online Locality Sensitive Hashing (LSH) (Van Durme and Lall, 2010). The idea of generating random projections from the set 1−1, +11 was originally proposed by Achlioptas (2003). Next we create a binary matrix B using matrix A by taking sign of each of the entries of the matrix A. If A(i, j) &gt; 0, then B(i, j) = 1; else B(i, j) = 0. This binarization creates Locality Sensitive Hash (LSH) function that preserves the cosine similarity between every pair of word vectors. This idea was first proposed by Charikar (2002) and used in NLP for large-scale noun clustering (Ravichandran et al., 2005). However, in large-scale noun clustering work, their approach had to store the random projection matrix of size D x k; where D denotes the number of all unique contexts (which is generally large and D &gt;&gt; Z) and in this paper, we do not explicitly require storing a random projection matrix. 3.3 Representation for Fast-Search We describe three approaches to represent the data (matrix A and B from Section 3.2) in such a manner that finding nearest neighbors is fast. These three approaches differ in amount of pre-processing time. First, we propose a naive baseline approach using random projection</context>
<context position="21350" citStr="Ravichandran et al. (2005)" startWordPosition="3642" endWordPosition="3645">2 PLEB PLEB (Point Location in Equal Balls) was first proposed by Indyk and Motwani (1998) and further improved by Charikar (2002). The improved PLEB algorithm puts in operation all k random projections together. It randomly permutes the ordering of k binary LSH bits (stored in matrix B) for all the words p times. For each permutation it sorts all the words lexicographically based on their permuted LSH representation of size k. The sorting operation puts all the nearest neighbor words (using k projections together) next to each other for all the permutations. In practice p is generally large, Ravichandran et al. (2005) used p = 1000 in their work. In our implementation of PLEB, we have a matrix A of size (p x Z) similar to the first matrix in Fig. 1(a). The main difference to the first matrix in Fig. 1(a) is that bit vectors of size k are used for sorting rather than using scalar projection values. Similar to Fig. 1(c) after sorting, bit vectors are discarded and 1073 a matrix C of size (p x Z) is used to map the words 1 · · · Z to their sorted position in the matrix A. Note, in IRP approach, the size of A and C matrix is (k x Z). In PLEB generating random permutations and sorting the bit vectors of size k </context>
<context position="24391" citStr="Ravichandran et al., 2005" startWordPosition="4190" endWordPosition="4193">ance, all the approaches discussed in Section 3.3 require all k random projection bits. 4 Experiments We evaluate our system FLAG for fast large-scale approximate graph construction. First, we show that using distributed online-PMI algorithm is as effective as offline-PMI. Second, we compare the approximate nearest neighbors lists generated by FLAG against the exact nearest neighbor lists. Finally, we show the quality of our approximate similarity lists generated by FLAG from the web corpus. 4.1 Experimental Setup Data sets: We use two data sets: Gigaword (Graff, 2003) and a copy of news web (Ravichandran et al., 2005). For both the corpora, we split the text into sentences, tokenize and convert into lower-case. To evaluate our approximate graph construction, we evaluate on three data sets: Gigaword (GW), Gigaword + 50% of web data (GWB50) and Gigaword + 100% ((GWB100)) of web data. Corpus statistics are shown in Table 1. We define the context for a given word “z” as the surrounding words appearing in a window of 2 words to the left and 2 words to the right. The context words are concatenated along with their positions -2, -1, +1, and +2. Corpus GW GWB50 GWB100 Unzipped 12 60 110 Size (GB) # of sentences 57</context>
<context position="29840" citStr="Ravichandran et al., 2005" startWordPosition="5146" endWordPosition="5149">is computed by calculating cosine similarity between 447 test words with respect to all other words. We also compare the LSH (computed using Hamming distance between all words and test set.) approximate nearest neighbor similarity lists against the exact similarity lists. LSH provides an upper bound on the performance of our approximate search representations (IRP, PLEB, and FAST-PLEB) for fastsearch from Section 3.3) . We set the number of projections k = 3000 for all three methods and for PLEB and FAST-PLEB, we set number of permutations p = 1000 as used in large-scale noun clustering work (Ravichandran et al., 2005). Evaluation Metric: We use two kinds of measures, recall and Pearson’s correlation to measure the overlap in the approximate and exact similarity lists. Intuitively, recall (R) captures the number of 1075 10 10 10 IRP PLEB FAST-PLEB 25 50 100 25 50 100 25 50 100 R p R p R p R p R p R p R p R p R p R p R p R p .55 .57 .52 .56 .49 .54 .46 .52 .55 .57 .52 .56 .49 .54 .46 .52 .55 .57 .52 .56 .49 .54 .46 .52 .29 .50 .26 .55 .25 .54 .24 .50 .50 .59 .45 .60 .41 .57 .37 .55 .48 .58 .42 .58 .38 .58 .35 .55 .36 .55 .33 .56 .31 .55 .30 .52 .53 .59 .48 .59 .44 .56 .41 .54 .51 .57 .47 .57 .42 .56 .40 .54 </context>
</contexts>
<marker>Ravichandran, Pantel, Hovy, 2005</marker>
<rawString>Deepak Ravichandran, Patrick Pantel, and Eduard Hovy. 2005. Randomized algorithms and nlp: using locality sensitive hash function for high speed noun clustering. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Rubenstein</author>
<author>J B Goodenough</author>
</authors>
<title>Contextual correlates of synonymy.</title>
<date>1965</date>
<journal>Computational Linguistics,</journal>
<pages>8--627</pages>
<contexts>
<context position="27185" citStr="Rubenstein and Goodenough, 1965" startWordPosition="4665" endWordPosition="4668">e p of 1. This evaluation show that the vectors obtained using online-PMI are as effective as offline-PMI. Extrinsic Evaluation: We also compare onlinePMI effectiveness on four test sets which consist of word pairs, and their corresponding human rankings. We generate the word pair rankings using online-PMI and offline-PMI strategies. We report the Pearson’s correlation (p) between the human and system generated similarity rankings. The four test sets are: WS-353 (Finkelstein et al., 2002) is a set of 353 word pairs. WS-203: A subset of WS-353 with 203 word pairs (Agirre et al., 2009). RG-65: (Rubenstein and Goodenough, 1965) has 65 word pairs. MC-30: A subset of RG-65 dataset with 30 word pairs (Miller and Charles, 1991). The results in Table 2 shows that by using distributed online-PMI (by making a single pass over the corpus) is comparable to offline-PMI (which is computed by making two passes over the corpus). For generating context vectors from GW, for both offline-PMI and online-PMI, we use a frequency cutoff of 5 for word-context pairs to throw away the rare terms as they are sensitive to PMI (Church and Hanks, 1989). Next, FLAG generates online-PMI vectors from GWB50 and GWB100 and uses frequency cutoffs o</context>
</contexts>
<marker>Rubenstein, Goodenough, 1965</marker>
<rawString>H. Rubenstein and J.B. Goodenough. 1965. Contextual correlates of synonymy. Computational Linguistics, 8:627–633.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Florin Rusu</author>
<author>Alin Dobra</author>
</authors>
<title>Statistical analysis of sketch estimators.</title>
<date>2007</date>
<booktitle>In SIGMOD ’07.</booktitle>
<publisher>ACM.</publisher>
<contexts>
<context position="6098" citStr="Rusu and Dobra, 2007" startWordPosition="909" endWordPosition="912">tatively and qualitatively on large data sets. Finally, we show the applicability of large-scale graphs built from FLAG on two applications: the Google-Sets problem (Ghahramani and Heller, 2005), and learning concrete and abstract words (Turney et al., 2011). 2 Count-Min sketch The Count-Min (CM) sketch (Cormode and Muthukrishnan, 2004) belongs to a class of ‘sketch’ algorithms that represents a large data set with a compact summary, typically much smaller than the full size of the input by processing the data in one pass. The following surveys comprehensively review the streaming literature (Rusu and Dobra, 2007; Cormode and Hadjieleftheriou, 2008) and sketch techniques (Charikar et al., 2004; Li et al., 2008; Cormode and Muthukrishnan, 2004; Rusu and Dobra, 2007). In our another recent paper (Goyal et al., 2012), we conducted a systematic study and compare many sketch techniques which answer point queries with focus on large-scale NLP tasks. In that paper, we empirically demonstrated that CM sketch performs the best among all the sketches on three large-scale NLP tasks. CM sketch uses hashing to store the approximate frequencies of all items from the large data set onto a small sketch vector that ca</context>
</contexts>
<marker>Rusu, Dobra, 2007</marker>
<rawString>Florin Rusu and Alin Dobra. 2007. Statistical analysis of sketch estimators. In SIGMOD ’07. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Thelen</author>
<author>E Riloff</author>
</authors>
<title>A Bootstrapping Method for Learning Semantic Lexicons Using Extraction Pattern Contexts.</title>
<date>2002</date>
<booktitle>In Proceedings of the Empirical Methods in Natural Language Processing,</booktitle>
<pages>214--221</pages>
<contexts>
<context position="40332" citStr="Thelen and Riloff, 2002" startWordPosition="7058" endWordPosition="7061">e and Abstract Words Our goal is to automatically learn concrete and abstract words (Turney et al., 2011). We apply bootstrapping (Kozareva et al., 2008) on the word graphs by manually selecting 10 seeds for concrete and abstract words (see Table 10). We use in-degree (sum of weights of incoming edges) to compute the score for each node which has connections with known (seeds) or automatically labeled nodes, previously exploited to learn hyponymy relations from the web (Kozareva et al., 2008). We learn concrete and abstract words together (known as mutual exclusion principle in bootstrapping (Thelen and Riloff, 2002; McIntosh and Curran, 2008)), and each word is assigned to only one class. Moreover, after each iteration, we harmonically decrease the weight of the in-degree associated with instances learned in later iterations. We add 25 new instances at each iteration and ran 100 iterations of bootstrapping, yielding 2506 concrete nouns and 2498 abstract nouns. To evaluate our learned words, we searched in WordNet whether they had ‘abstraction’ or ’physical’ as their hypernym. Out of 2506 learned concrete nouns, Concrete: girl, person, bottles, wife, gentleman, microcomputer, neighbor, boy, foreigner, ho</context>
</contexts>
<marker>Thelen, Riloff, 2002</marker>
<rawString>M. Thelen and E. Riloff. 2002. A Bootstrapping Method for Learning Semantic Lexicons Using Extraction Pattern Contexts. In Proceedings of the Empirical Methods in Natural Language Processing, pages 214–221.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
<author>Patrick Pantel</author>
</authors>
<title>From frequency to meaning: Vector space models of semantics.</title>
<date>2010</date>
<journal>JOURNAL OF ARTIFICIAL INTELLIGENCE RESEARCH,</journal>
<pages>37--141</pages>
<contexts>
<context position="1662" citStr="Turney and Pantel, 2010" startWordPosition="229" endWordPosition="232">y natural language processing (NLP) problems involve graph construction. Examples include constructing polarity lexicons based on lexical graphs from WordNet (Rao and Ravichandran, 2009), constructing polarity lexicons from web data (Velikovich et al., 2010) and unsupervised part-ofspeech tagging using label propagation (Das and Petrov, 2011). The later two approaches construct nearest-neighbor graphs between word pairs by computing nearest neighbors between word pairs from large corpora. These nearest neighbors form the edges of the graph, with weights given by the distributional similarity (Turney and Pantel, 2010) between terms. Unfortunately, computing the distributional similarity between all words in a large vocabulary is computationally and memory intensive when working with large amounts of data (Pantel et al., 2009). This bottleneck is typically addressed by means of commodity clusters. For example, Pantel et al. (2009) compute distributional similarity between 500 million terms over a 200 billion words in 50 hours using 100 quad-core nodes, explicitly storing a similarity matrix between 500 million terms. In this work, we propose Fast Large-Scale Approximate Graph (FLAG) construction, a system t</context>
<context position="3427" citStr="Turney and Pantel, 2010" startWordPosition="494" endWordPosition="497">l. (2005) approach stored an enormous matrix of all unique words and their contexts in main memory, which is infeasible for very large data sets. A more efficient online framework to locality sensitive hashing (Van Durme and Lall, 2010; Van Durme and Lall, 2011) computes distributional similarity in a streaming setting. Unfortunately, their approach can handle only additive features like raw-counts, and not non-linear association scores like pointwise mutual information (PMI), which generates better context vectors for distributional similarity (Ravichandran et al., 2005; Pantel et al., 2009; Turney and Pantel, 2010). In FLAG, we first propose a novel distributed online-PMI algorithm (Section 3.1). It is a streaming method that processes large data sets in one pass while distributing the data over commodity clusters 1069 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1069–1080, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics and returns context vectors weighted by pointwise mutual information (PMI) for all the words. Our distributed online-PMI algorithm makes use of the Co</context>
</contexts>
<marker>Turney, Pantel, 2010</marker>
<rawString>Peter D. Turney and Patrick Pantel. 2010. From frequency to meaning: Vector space models of semantics. JOURNAL OF ARTIFICIAL INTELLIGENCE RESEARCH, 37:141.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Turney</author>
<author>Yair Neuman</author>
<author>Dan Assaf</author>
<author>Yohai Cohen</author>
</authors>
<title>Literal and metaphorical sense identification through concrete and abstract context.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>680--690</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="5736" citStr="Turney et al., 2011" startWordPosition="852" endWordPosition="855">s work done on PLEB (Point Location in Equal Balls) (Indyk and Motwani, 1998; Charikar, 2002). We propose novel variants of PLEB to address the issue of reducing the pre-processing time for PLEB. One of the variants of PLEB (FASTPLEB) with considerably less pre-processing time has effectiveness comparable to PLEB. We evaluate these variants of PLEB both quantitatively and qualitatively on large data sets. Finally, we show the applicability of large-scale graphs built from FLAG on two applications: the Google-Sets problem (Ghahramani and Heller, 2005), and learning concrete and abstract words (Turney et al., 2011). 2 Count-Min sketch The Count-Min (CM) sketch (Cormode and Muthukrishnan, 2004) belongs to a class of ‘sketch’ algorithms that represents a large data set with a compact summary, typically much smaller than the full size of the input by processing the data in one pass. The following surveys comprehensively review the streaming literature (Rusu and Dobra, 2007; Cormode and Hadjieleftheriou, 2008) and sketch techniques (Charikar et al., 2004; Li et al., 2008; Cormode and Muthukrishnan, 2004; Rusu and Dobra, 2007). In our another recent paper (Goyal et al., 2012), we conducted a systematic study</context>
<context position="39814" citStr="Turney et al., 2011" startWordPosition="6974" endWordPosition="6977">measure the quality of returned words. We recruited 1 annotator and developed annotation guidelines that instructed each recruiter to judge whether learned values are similar to query words or not. Overall the annotator found almost all the learned words to be similar to the query words. However, the algorithm can not differentiate between different senses of the word. For example, “French” can be a language and a nationality. Table 9 shows the top ranked words with respect to query words. 5.2 Learning Concrete and Abstract Words Our goal is to automatically learn concrete and abstract words (Turney et al., 2011). We apply bootstrapping (Kozareva et al., 2008) on the word graphs by manually selecting 10 seeds for concrete and abstract words (see Table 10). We use in-degree (sum of weights of incoming edges) to compute the score for each node which has connections with known (seeds) or automatically labeled nodes, previously exploited to learn hyponymy relations from the web (Kozareva et al., 2008). We learn concrete and abstract words together (known as mutual exclusion principle in bootstrapping (Thelen and Riloff, 2002; McIntosh and Curran, 2008)), and each word is assigned to only one class. Moreov</context>
</contexts>
<marker>Turney, Neuman, Assaf, Cohen, 2011</marker>
<rawString>Peter Turney, Yair Neuman, Dan Assaf, and Yohai Cohen. 2011. Literal and metaphorical sense identification through concrete and abstract context. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 680–690. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Van Durme</author>
<author>Ashwin Lall</author>
</authors>
<title>Probabilistic counting with randomized storage.</title>
<date>2009</date>
<booktitle>In IJCAI’09: Proceedings of the 21st international jont conference on Artifical intelligence.</booktitle>
<marker>Van Durme, Lall, 2009</marker>
<rawString>Benjamin Van Durme and Ashwin Lall. 2009a. Probabilistic counting with randomized storage. In IJCAI’09: Proceedings of the 21st international jont conference on Artifical intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Van Durme</author>
<author>Ashwin Lall</author>
</authors>
<title>Streaming pointwise mutual information.</title>
<date>2009</date>
<booktitle>In Advances in Neural Information Processing Systems 22.</booktitle>
<marker>Van Durme, Lall, 2009</marker>
<rawString>Benjamin Van Durme and Ashwin Lall. 2009b. Streaming pointwise mutual information. In Advances in Neural Information Processing Systems 22.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Van Durme</author>
<author>Ashwin Lall</author>
</authors>
<title>Online generation of locality sensitive hash signatures.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACL 2010 Conference Short Papers,</booktitle>
<pages>231--235</pages>
<marker>Van Durme, Lall, 2010</marker>
<rawString>Benjamin Van Durme and Ashwin Lall. 2010. Online generation of locality sensitive hash signatures. In Proceedings of the ACL 2010 Conference Short Papers, pages 231–235, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Van Durme</author>
<author>Ashwin Lall</author>
</authors>
<title>Efficient online locality sensitive hashing via reservoir counting.</title>
<date>2011</date>
<booktitle>In Proceedings of the ACL 2011 Conference Short Papers,</booktitle>
<marker>Van Durme, Lall, 2011</marker>
<rawString>Benjamin Van Durme and Ashwin Lall. 2011. Efficient online locality sensitive hashing via reservoir counting. In Proceedings of the ACL 2011 Conference Short Papers, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leonid Velikovich</author>
<author>Sasha Blair-Goldensohn</author>
<author>Kerry Hannan</author>
<author>Ryan McDonald</author>
</authors>
<title>The viability of webderived polarity lexicons.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>777--785</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Los Angeles, California,</location>
<contexts>
<context position="1296" citStr="Velikovich et al., 2010" startWordPosition="176" endWordPosition="179"> algorithm with novel fast approximate nearest neighbor search algorithms (variants of PLEB). These algorithms return the approximate nearest neighbors quickly. We show our system’s efficiency in both intrinsic and extrinsic experiments. We further evaluate our fast search algorithms both quantitatively and qualitatively on two NLP applications. 1 Introduction Many natural language processing (NLP) problems involve graph construction. Examples include constructing polarity lexicons based on lexical graphs from WordNet (Rao and Ravichandran, 2009), constructing polarity lexicons from web data (Velikovich et al., 2010) and unsupervised part-ofspeech tagging using label propagation (Das and Petrov, 2011). The later two approaches construct nearest-neighbor graphs between word pairs by computing nearest neighbors between word pairs from large corpora. These nearest neighbors form the edges of the graph, with weights given by the distributional similarity (Turney and Pantel, 2010) between terms. Unfortunately, computing the distributional similarity between all words in a large vocabulary is computationally and memory intensive when working with large amounts of data (Pantel et al., 2009). This bottleneck is t</context>
<context position="37944" citStr="Velikovich et al., 2010" startWordPosition="6684" endWordPosition="6687">lications We use the graph constructed by FLAG from GWB100 data set (110 GB) by applying FASTPLEB with parameters k = 3000, p = 1000, q = 10 and b = 40. The graph has 106, 733 nodes (words), with each node having 100 edges that denote the top l = 100 approximate nearest neighbors associated with each node. However, FLAG applied FASTPLEB (approximate search) to find these neighbors. Therefore many of these edges can be noisy for our applications. Hence for each node, we only consider top 10 edges. In general for graph-based NLP problems; for example, constructing web-derived polarity lexicons (Velikovich et al., 2010), top 25 edges were used, and for unsupervised part-of-speech tagging using label propagation (Das and Petrov, 2011), top 5 edges were used. 5.1 Google Sets Problem Google Sets problem (Ghahramani and Heller, 2005) can be defined as: given a set of query words, return top t similar words with respect to query words. To evaluate the quality of our approximate large-scale graph, we return top 25 words which have best aggregated similarity scores with respect to query words. We take 5 classes and their query terms (McIntosh and Curran, 2008) shown in Table 8 and our goal is to learn 25 new words </context>
</contexts>
<marker>Velikovich, Blair-Goldensohn, Hannan, McDonald, 2010</marker>
<rawString>Leonid Velikovich, Sasha Blair-Goldensohn, Kerry Hannan, and Ryan McDonald. 2010. The viability of webderived polarity lexicons. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 777–785, Los Angeles, California, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kilian Weinberger</author>
<author>Anirban Dasgupta</author>
<author>John Langford</author>
<author>Alex Smola</author>
<author>Josh Attenberg</author>
</authors>
<title>Feature hashing for large scale multitask learning.</title>
<date>2009</date>
<booktitle>In Proceedings of the 26th Annual International Conference on Machine Learning, ICML ’09,</booktitle>
<pages>1113--1120</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="18286" citStr="Weinberger et al., 2009" startWordPosition="3106" endWordPosition="3109"> 2 60 ··· 1 55 2 ··· Z ... . .. .... . . 1 90 ··· 2 (d) Matrix C Figure 1: First matrix pairs the words 1 · · · Z and their random projection values. Second matrix sorts each row by the random projection values from smallest to largest. Third matrix throws away the projection values leaving only the words. Fourth matrix maps the words 1 · · · Z to their sorted position in the third matrix for each k. This allows constant query time for all the words. is motivated by the work on stable random projections (Li et al., 2006; Li et al., 2008), Count sketch (Charikar et al., 2004), feature hashing (Weinberger et al., 2009) and online Locality Sensitive Hashing (LSH) (Van Durme and Lall, 2010). The idea of generating random projections from the set 1−1, +11 was originally proposed by Achlioptas (2003). Next we create a binary matrix B using matrix A by taking sign of each of the entries of the matrix A. If A(i, j) &gt; 0, then B(i, j) = 1; else B(i, j) = 0. This binarization creates Locality Sensitive Hash (LSH) function that preserves the cosine similarity between every pair of word vectors. This idea was first proposed by Charikar (2002) and used in NLP for large-scale noun clustering (Ravichandran et al., 2005).</context>
</contexts>
<marker>Weinberger, Dasgupta, Langford, Smola, Attenberg, 2009</marker>
<rawString>Kilian Weinberger, Anirban Dasgupta, John Langford, Alex Smola, and Josh Attenberg. 2009. Feature hashing for large scale multitask learning. In Proceedings of the 26th Annual International Conference on Machine Learning, ICML ’09, pages 1113–1120. ACM.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
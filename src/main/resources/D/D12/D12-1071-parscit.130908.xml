<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000133">
<title confidence="0.9989695">
Resolving Complex Cases of Definite Pronouns:
The Winograd Schema Challenge
</title>
<author confidence="0.974955">
Altaf Rahman and Vincent Ng
</author>
<affiliation confidence="0.9885385">
Human Language Technology Research Institute
University of Texas at Dallas
</affiliation>
<address confidence="0.918222">
Richardson, TX 75083-0688
</address>
<email confidence="0.999727">
{altaf,vince}@hlt.utdallas.edu
</email>
<sectionHeader confidence="0.998606" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998367411764706">
We examine the task of resolving complex
cases of definite pronouns, specifically those
for which traditional linguistic constraints
on coreference (e.g., Binding Constraints,
gender and number agreement) as well as
commonly-used resolution heuristics (e.g.,
string-matching facilities, syntactic salience)
are not useful. Being able to solve this task has
broader implications in artificial intelligence:
a restricted version of it, sometimes referred
to as the Winograd Schema Challenge, has
been suggested as a conceptually and practi-
cally appealing alternative to the Turing Test.
We employ a knowledge-rich approach to this
task, which yields a pronoun resolver that out-
performs state-of-the-art resolvers by nearly
18 points in accuracy on our dataset.
</bodyText>
<sectionHeader confidence="0.999506" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999954875">
Despite the significant amount of work on pronoun
resolution in the natural language processing com-
munity in the past forty years, the problem is still
far from being solved. Its difficulty stems in part
from its reliance on sophisticated knowledge sources
and inference mechanisms. The sentence pair below,
which we will subsequently refer to as the shout ex-
ample, illustrates how difficult the problem can be:
</bodyText>
<listItem confidence="0.81364">
(1a) Ed shouted at Tim because he crashed the car.
(1b) Ed shouted at Tim because he was angry.
</listItem>
<bodyText confidence="0.999987447368421">
The pronoun he refers to Tim in 1a and Ed in 1b.
Humans can resolve the pronoun easily, but state-
of-the-art coreference resolvers cannot. The reason
is that humans have the kind of world knowledge
needed to resolve the pronouns that machines do not.
Our world knowledge tells us that if someone is an-
gry, he may shout at other people. Since Ed shouted,
he should be the one who was angry. Our world
knowledge also tells us that we may shout at some-
one who made a mistake and that crashing a car is
a mistake. Combining these two pieces of evidence,
we can easily infer that Tim crashed the car.
Our goal in this paper is to examine the resolu-
tion of complex cases of definite pronouns that ap-
pear in sentences exemplified by the shout example.
Specifically, each sentence (1) has two clauses sepa-
rated by a discourse connective (i.e., the connective
appears between the two clauses, just like because
in the shout example), where the first clause con-
tains two or more candidate antecedents (e.g., Ed
and Tim), and the second clause contains the tar-
get pronoun (e.g., he); and (2) the target pronoun
agrees in gender, number, and semantic class with
each candidate antecedent, but does not have any
overlap in content words with any of them. For con-
venience, we will refer to the target pronoun that ap-
pears in this kind of sentences as a difficult pronoun.
Note that many traditional linguistic constraints
on coreference are no longer useful for resolving dif-
ficult pronouns. For instance, syntactic constraints
such as the Binding Constraints will not be useful,
since the pronoun and the candidate antecedents ap-
pear in different clauses separated by a discourse
connective; and constraints concerning agreement in
gender, number, and semantic class will not be use-
ful, since the pronoun and the candidate antecedents
are compatible with respect to all these attributes.
Traditionally important clues provided by various
</bodyText>
<page confidence="0.927694">
777
</page>
<note confidence="0.959091">
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 777–789, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics
</note>
<figure confidence="0.944281">
I(a) The city councilmen refused the demonstrators a permit because they feared violence.
I(b) The city councilmen refused the demonstrators a permit because they advocated violence.
II(a) James asked Robert for a favor, but he refused.
II(b) James asked Robert for a favor, but he was refused.
III(a) Keith fired Blaine but he did not regret.
III(b) Keith fired Blaine although he is diligent.
IV(a) Emma did not pass the ball to Janie, although she was open.
IV(b) Emma did not pass the ball to Janie, although she should have.
V(a) Medvedev will cede the presidency to Putin because he is more popular.
V(b) Medvedev will cede the presidency to Putin because he is less popular.
</figure>
<tableCaption confidence="0.994522">
Table 1: Sample twin sentences. The target pronoun in each sentence is italicized, and its antecedent is boldfaced.
</tableCaption>
<bodyText confidence="0.999244458333334">
string-matching facilities will not be useful either,
since the pronoun and its candidate antecedents do
not have any words in common.
As in the shout example, we ensure that each sen-
tence has a twin. Twin sentences were used ex-
tensively by researchers in the 1970s to illustrate
the difficulty of pronoun resolution (Hirst, 1981).
We consider two sentences as twins if (1) they
are identical up to and possibly including the dis-
course connective; and (2) the difficult pronouns in
them are lexically identical but have different an-
tecedents. The presence of twins implies that syn-
tactic salience, a commonly-used heuristic in pro-
noun resolution that prefers the selection of syntac-
tically salient candidate antecedents, may no longer
be useful, since the candidate in the subject position
is not more likely to be the correct antecedent than
the other candidates.
To enable the reader to get a sense of how hard it is
to resolve difficult pronouns, Table 1 shows sample
twin sentences from our dataset. Note that state-of-
the-art pronoun resolvers (e.g., JavaRAP (Qiu et al.,
2004), GuiTaR (Poesio and Kabadjov, 2004), as well
as those designed by Mitkov (2002) and Charniak
and Elsner (2009)) and coreference resolvers (e.g.,
BART (Versley et al., 2008), CherryPicker (Rahman
and Ng, 2009), Reconcile (Stoyanov et al., 2010),
the Stanford resolver (Raghunathan et al., 2010; Lee
et al., 2011)) cannot accurately resolve the difficult
pronouns in these structurally simple sentences, as
they do not have the mechanism to capture the fine
distinctions between twin sentences. In other words,
when given these sentences, the best that the existing
resolvers can do to resolve the pronouns is guess-
ing. This could be surprising to a non-coreference
researcher, but it is indeed the state of the art.
A natural question is: why do existing resolvers
not attempt to handle difficult pronouns? One rea-
son could be that these difficult pronouns do not
appear frequently in standard evaluation corpora
such as MUC, ACE, and OntoNotes (Bagga, 1998;
Haghighi and Klein, 2009). In fact, the Stanford
coreference resolver (Lee et al., 2011), which won
the CoNLL-2011 shared task on coreference resolu-
tion, adopts the once-popular rule-based approach,
resolving pronouns simply with rules that encode
the aforementioned traditional linguistic constraints
on coreference, such as the Binding constraints and
gender and number agreement.
The infrequency of occurrences of difficult pro-
nouns in these standard evaluation corpora by no
means undermines their significance, however. In
fact, being able to automatically resolve difficult
pronouns has broader implications in artificial intel-
ligence. Recently, Levesque (2011) has argued that
the problem of resolving the difficult pronouns in
a carefully chosen set of twin sentences, which he
refers to as the Winograd Schema Challenge1, could
serve as a conceptually and practically appealing
alternative to the well-known Turing Test (Turing,
1Levesque (2011) defines a Winograd Schema as a small
reading comprehension test involving the question of which of
the two candidate antecedents for the definite pronoun in a given
sentence is its correct antecedent. Levesque names this chal-
lenge after Winograd because of his pioneering attempt to use a
well-known pair of twin sentences — specifically the first pair
in Table 1 — to illustrate the difficulty of natural language un-
derstanding (Winograd, 1972). Strictly speaking, we are ad-
dressing a relaxed version of the Challenge: while Levesque
focuses solely on definite pronouns whose resolution requires
background knowledge not expressed in the words of a sen-
tence, we do not impose such a condition on a sentence.
</bodyText>
<page confidence="0.996125">
778
</page>
<bodyText confidence="0.999952394736842">
1950). The reason should perhaps be clear given the
above discussion: this is an easy task for a subject
who can “understand” natural language but a chal-
lenging task for one who can only make intelligent
guesses. Levesque believes that “with a very high
probability”, anything that can resolve correctly a
series of difficult pronouns “is thinking in the full-
bodied sense we usually reserve for people”. Hence,
being able to make progress on this task enables us
to move one step closer to building an intelligent ma-
chine that can truly understand natural language.
To sum up, an important contribution of our work
is that it opens up a new line of research involving
a problem whose solution requires a deeper under-
standing of a text. With recent advances in knowl-
edge extraction from text, we believe that time is ripe
to tackle this problem. It is worth noting that some
researchers have focused on other kinds of anaphors
that are hard to resolve, including bridging anaphors
(e.g., Poesio et al. (2004)) and anaphors referring
to abstract entities, such as those realized by verb
phrases in dialogs (e.g., Byron (2002), Strube and
M¨uller (2003), M¨uller (2007)). Nevertheless, to our
knowledge, there has been little work that specifi-
cally targets difficult pronouns.
Given the complexity of our task, we investigate
a variety of sophisticated knowledge sources for re-
solving difficult pronouns, and combine them via a
machine learning approach. Note that there has been
a recent surge of interest in extracting world knowl-
edge from online encyclopedias such as Wikipedia
(e.g., Ponzetto and Strube (2006, 2007), Poesio et
al. (2007)), YAGO (e.g., Bryl et al. (2010), Rahman
and Ng (2011), Uryupina et al. (2011)), and Free-
base (e.g., Lee et al. (2011)). However, the resulting
extractions are primarily IS-A relations (e.g., Barack
Obama IS-A U. S. president), which would not be
useful for resolving definite pronouns.
</bodyText>
<sectionHeader confidence="0.993776" genericHeader="method">
2 Dataset Creation
</sectionHeader>
<bodyText confidence="0.999994142857143">
We asked 30 undergraduate students who are not af-
filiated with this research to compose sentence pairs
(i.e., twin sentences) that conform to the constraints
specified in the introduction. Each student was also
asked to annotate the candidate antecedents, the tar-
get pronoun, and the correct antecedent for each
sentence she composed. Note that a sentence may
contain multiple pronouns, but exactly one of them
— the one explicitly annotated by its author — is
the target pronoun. Each sentence pair was cross-
checked by one other student to ensure that it (1)
conforms to the desired constraints and (2) does not
contain pronouns with ambiguous antecedents (in
other words, a human should not be confused as
to which candidate antecedent is the correct one).
At the end of the process, 941 sentence pairs were
considered acceptable, and they formed our dataset.
These sentences cover a variety of topics, ranging
from real events (e.g., Iran’s plan to attack the Saudi
ambassador to the U.S.), to events and characters in
movies (e.g., Batman and Robin), to purely imagi-
nary situations (e.g., the shout example). We parti-
tion these sentence pairs into a training set and a test
set following a 70/30 ratio.
While not requested by us, the students annotated
exactly two candidate antecedents for each sentence.
For ease of exposition, we will assume below that
there are two candidate antecedents per sentence.
</bodyText>
<sectionHeader confidence="0.995047" genericHeader="method">
3 Machine Learning Framework
</sectionHeader>
<bodyText confidence="0.999948875">
Since our goal is to determine which of the two can-
didate antecedents is the correct antecedent for the
target pronoun in each sentence, our system assumes
as input the sentence, the target pronoun, and the two
candidate antecedents.
We employ machine learning to combine the
features derived from different knowledge sources.
Specifically, we employ a ranking-based approach.
Ranking-based approaches have been shown to out-
perform their classification-based counterparts (De-
nis and Baldridge, 2007, 2008; Iida et al., 2003;
Yang et al., 2003). Given a pronoun and two can-
didate antecedents, we aim to train a ranking model
that ranks the two candidates such that the correct
antecedent is assigned a higher rank.
More formally, given training sentence Sk con-
taining target pronoun Ak, correct antecedent Ck
and incorrect antecedent Ik, we create two feature
vectors, xCAk and xIAk, where xCAk is generated
from Ak and Ck, and xIAk is generated from Ak
and Ik. The training set consists of ordered pairs
of feature vectors (xCAk, xIAk), and the goal of the
training procedure is to acquire a ranker that mini-
mizes the number of violations of pairwise rankings
</bodyText>
<page confidence="0.997279">
779
</page>
<bodyText confidence="0.999779">
provided in the training set. We train this ranker us-
ing Joachims’ (2002) SVMUght package. It is worth
noting that we do not exploit the fact that each sen-
tence has a twin in training or testing.
After training, the ranker can be applied to the test
instances, which are created in the same way as the
training instances. For each test instance, the target
pronoun is resolved to the higher-ranked candidate
antecedent.
</bodyText>
<sectionHeader confidence="0.988221" genericHeader="method">
4 Linguistic Features
</sectionHeader>
<bodyText confidence="0.99987975">
We derive linguistic features for resolving difficult
pronouns from eight components, as described be-
low. To enable the reader to keep track of these fea-
tures more easily, we summarize them in Table 2.
</bodyText>
<subsectionHeader confidence="0.975785">
4.1 Narrative Chains
</subsectionHeader>
<bodyText confidence="0.984702586206896">
Consider the following sentence:
(2) Ed punished Tim because he tried to escape.
Humans resolve he to Tim by exploiting the world
knowledge that someone who tried to escape is bad
and therefore should be punished. Such kind of
knowledge can be extracted from narrative chains.
Narrative chains are partially ordered sets of
events centered around a common protagonist, aim-
ing to encode the kind of knowledge provided by
scripts (Schank and Abelson, 1977). While scripts
are hand-written, narrative chains can be learned
from unannotated text. Below is a chain learned by
Chambers and Jurafsky (2008):
borrow-s invest-s spend-s pay-s raise-s lend-s
As we can see, a narrative chain is composed of a
sequence of events (verbs) together with the roles of
the protagonist. Here, “s” denotes the subject role,
even though a chain can contain a mix of “s” and “o”
(the object role). From this chain, we know that the
person who borrows something (probably money)
may invest, spend, pay, or lend it.
We employ narrative chains to heuristically pre-
dict the antecedent for the target pronoun, and en-
code the prediction as a feature. The heuristic de-
cision procedure operates as follows. Given a sen-
tence, we first determine the event the target pro-
noun participates in and its role in the event. As
an example, we determine that in sentence (2) he
participates in the try event and the escape event
</bodyText>
<table confidence="0.999909090909091">
Component # Features Features
Narrative Chains 1 NC
Google 4 G1, G2, G3, G4
FrameNet 4 FN1, FN2, FN3, FN4
Heuristic Polarity 3 HPOL1, HPOL2, HPOL3
Learned Polarity 3 LPOL1, LPOL2, LPOL3
Connective-Based 1 CBR
Relation
Semantic Compat. 3 SC1, SC2, SC3
Lexical Features 68,331 antecedent- independent
and dependent features
</table>
<tableCaption confidence="0.99963">
Table 2: Summary of the features described in Section 4.
</tableCaption>
<bodyText confidence="0.999984321428571">
as a subject.2 Second, we determine the event(s)
that the candidate antecedents participate in. In (2),
both candidate antecedents participate in the pun-
ish event. Third, we pair each event participated
by each candidate antecedent with each event par-
ticipated by the pronoun. In our example, we would
create two pairs, (punish, try-s) and (punish, escape-
s). Note that try and escape are associated with the
role of the pronoun that we extracted in the first step.
Fourth, for each such pair, we extract all the narra-
tive chains containing both elements in the pair from
Chambers and Jurafsky’s output.3 This step results
in one chain being extracted, which contains punish-
o and escape-s. In other words, the protagonist in
this chain is the subject of an escape event and the
object of a punish event. Fifth, from the extracted
chain, we obtain the role played by the pronoun (i.e.,
the protagonist) in the event in which the candidate
antecedents participate. In our example, the pronoun
plays an object role in the punish event. Finally, we
extract the candidate antecedent that plays the ex-
tracted role, which in our example is the second an-
tecedent, Tim.4
We create a binary feature, NC, which encodes
this heuristic decision, and compute its value as fol-
lows. Assume in the rest of the paper that i1 and
i2 are the feature vectors corresponding to the first
candidate antecedent and the second candidate an-
</bodyText>
<footnote confidence="0.871186444444444">
2Throughout the paper, the subject/object of an event refers
to its deep rather than surface subject/object. We determine
the grammatical role of an NP using the Stanford dependency
parser (de Marneffe et al., 2006) and a set of simple heuristics.
3We employ narrative chains of length 12, which are
available from http://cs.stanford.edu/people/
nc/schemas/schemas-size12.
4For an alternative way of using narrative chains for coref-
erence resolution, see Irwin et al. (2011).
</footnote>
<page confidence="0.994491">
780
</page>
<bodyText confidence="0.999982">
tecedent, respectively.5 For our running example,
since Tim is predicted to be the antecedent of he,
the value of NC in i2 is 1, and its value in i1 is 0.
For notational convenience, we write NC(i1)=0 and
NC(i2)=1, and will follow this convention when de-
scribing the features in the rest of the paper.
Finally, we note that NC(i1) and NC(i2) will
both be set to zero if (1) the pronoun and the an-
tecedents do not participate in events, or (2) no nar-
rative chains can be extracted in step 4 above, or (3)
step 4 enables us to extract more than one chain and
these chains indicate that the candidate antecedent
can have both a subject role and an object role.
</bodyText>
<subsectionHeader confidence="0.951235">
4.2 Google
</subsectionHeader>
<bodyText confidence="0.979234142857143">
Consider the following sentences:
(3a) Lions eat zebras because they are predators.
(3b) The knife sliced through the flesh because
it was sharp.
Humans resolve they to Lions in (3a) by exploiting
the world knowledge that predators attack and eat
other animals. Similarly, humans resolve it to the
knife in (3b) by exploiting the world knowledge that
the word sharp can be used to describe a knife but
not flesh. To acquire this kind of world knowledge,
we learn patterns of word usage from the Web by
issuing search queries. To facilitate our discussion,
let us first introduce some notation. Let a sentence
5 be denoted by a triple (Z1, Conn, Z2), where Z1
and Z2 are the clauses preceding and following the
discourse connective Conn, respectively; A E Z2
be the pronoun governed by the verb V ; W be the
sequence of words following V in 5; and C1, C2 E
Z1 be the candidate antecedents.
Given a sentence, we generate four queries: (Q1)
C1V ; (Q2) C2V ; (Q3) C1V W; and (Q4) C2V W. If
v is a verb-to-be followed by an adjective J, we gen-
erate two more queries: (Q5) JC1 and (Q6) JC2.
To exemplify, six queries are generated for (3b):
(Q1) “knife was”; (Q2) “flesh was”; (Q3) “knife was
sharp”; (Q4) “flesh was sharp”; (Q5) “sharp knife”;
and (Q6) “sharp flesh”. On the other hand, only four
queries are generated for (3a): (Q1) “lions are”; (Q2)
</bodyText>
<footnote confidence="0.5597305">
5The nth candidate antecedent in a sentence is the nth an-
notated NP encountered when processing the sentence in a left-
to-right manner. In sentence (2), Ed is the first candidate an-
tecedent and Tim is the second.
</footnote>
<bodyText confidence="0.768869369565217">
“zebras are”; (Q3) “lions are predators”; and (Q4)
“zebras are predators”.
Using the counts returned by Google for these
queries, we create four features, G1, G2, G3, and
G4, whose values are determined by Rules 1, 2, 3,
and 4, respectively, as described below.
Rule 1: if count(Q1) &gt; count(Q2) by at
least x% then G1(i1)=1 and G1(i2)=0;
else if count(Q2) &gt; count(Q1) by at least
x% then G1(i2)=1 and G1(i1)=0; else
G1(i1)=G1(i2)=0.
Rule 2: if count(Q3) &gt; count(Q4) by at
least x% then G2(i1)=1 and G2(i2)=0;
else if count(Q4) &gt; count(Q3) by at least
x% then G2(i2)=1 and G2(i1)=0; else
G2(i1)=G2(i2)=0.
Rule 3: if count(Q5) &gt; count(Q6) by at
least x% then G3(i1)=1 and G3(i2)=0;
else if count(Q6) &gt; count(Q5) by at least
x% then G3(i2)=1 and G3(i1)=0; else
G3(i1)=G3(i2)=0.
Rule 4: if one of G1(i1) and G1(i2) is 1,
then G4(i1)=G1(i1) and G4(i2)=G1(i2);
else if one of G2(i1) and G2(i2) is 1,
then G4(i1)=G2(i1) and G4(i2)=G2(i2);
else if one of G3(i1) and G3(i2) is 1,
then G4(i1)=G3(i1) and G4(i2)=G3(i2);
else G4(i1)=G4(i2)=0.
The role of the threshold x should be obvious: it
ensures that a heuristic decision is made only if the
difference between the counts for the two queries are
sufficiently large, because otherwise there is no rea-
son for us to prefer one candidate antecedent to the
other. In all of our experiments, we set x to 20.
Note that other researchers have also used lexico-
syntactic patterns to generate search queries for
bridging anaphora resolution (e.g., Poesio et al.
(2004)), other-anaphora resolution (e.g., Modjeska
et al. (2003)), and learning selectional preferences
for pronoun resolution (e.g., Yang et al. (2005)).
However, in each of these three cases, the target re-
lations (e.g., the part-whole relation in the case of
bridging anaphora resolution, and the subject-verb
and verb-object relations in the case of selectional
preferences) are specific enough that they can be ef-
fectively captured by specific patterns. For example,
</bodyText>
<page confidence="0.986619">
781
</page>
<bodyText confidence="0.999983454545455">
to determine whether the wheel is part of the car in
bridging anaphora resolution, Poesio et al. employ
queries of the form “X of Y”, where X and Y would
be replaced with the wheel and the car, respectively.
On the other hand, we are not targeting a particular
type of relation. Rather, we intend to capture world
knowledge like lions rather than zebras are preda-
tors. Such knowledge may not be expressed as a
relation and hence may not be easily captured using
specific patterns. For this reason, we need to employ
patterns as general as those such as Q3 and Q4.
</bodyText>
<subsectionHeader confidence="0.997449">
4.3 FrameIet
</subsectionHeader>
<bodyText confidence="0.999958666666667">
If we generate search queries as described in the pre-
vious subsection for the shout example, it is unlikely
that Google will return meaningful counts to us. The
reason is that both candidate antecedents in the sen-
tence are proper names belonging to the same type
(which in this case is PERSON).
However, in some cases, we may be able to gener-
ate more meaningful queries from such kind of sen-
tences. Consider the following sentence:
</bodyText>
<listItem confidence="0.614583">
(4) John killed Jim, so he was arrested.
</listItem>
<bodyText confidence="0.993832304347826">
To generate meaningful queries, we make one ob-
servation: John and Jim played different roles in a
kill event. Hence, we can replace these proper names
with their roles. We propose to obtain these roles
from FrameNet (Baker et al., 1998). More gener-
ally, for each proper name e in a given sentence, we
(1) determine the event in which e is involved (using
the Stanford dependency parser); (2) search for the
FrameNet frame corresponding to the event as well
as e’s role in the event; and (3) replace the name
with its FrameNet role. In our example, since both
names are involved in the kill event, we retrieve the
FrameNet frame for kill. Given that John and Jim are
the subject and object of kill, we can extract their se-
mantic roles directly from the frame, which are killer
and victim, respectively.6 Consequently, we replace
the two names with their extracted semantic roles,
and generate the search queries from the resulting
sentence in the same way as before.
Note that if no frames can be found for the verb in
the first clause, no search queries will be generated.
After obtaining the query counts, we generate four
binary features, FN1, FN2, FN3, FN4, whose values
</bodyText>
<footnote confidence="0.597844">
6We heuristically map grammatical roles to semantic roles.
</footnote>
<bodyText confidence="0.9773195">
are computed based on the same four heuristic rules
that were discussed in the previous subsection.
</bodyText>
<subsectionHeader confidence="0.995533">
4.4 Heuristic Polarity
</subsectionHeader>
<bodyText confidence="0.976259641025641">
Some sentences involve comparing the two candi-
date antecedents. Consider the following sentences:
(5a) John was defeated by Jim in the election
even though he is more popular.
(5b) John was defeated by Jim in the election
because he is more popular.
The pronoun he refers to John in (5a) and Jim in
(5b). To see how we can design an algorithm for re-
solving these pronouns, it would be useful to under-
stand how humans resolve them. The phrase more
popular has a positive sentiment. In (5a), the use
of even though yields a clause of concession, which
flips the polarity of more popular (from positive to
negative), whereas in (5b), the use of because yields
a clause of cause, which does not change the po-
larity of more popular (i.e., more popular remains
positive). Since more popular is used to describe he,
he is “better” in (5b) but “worse” in (5a). Now, the
word defeat has a positive sentiment, and since Jim
is the deep subject of defeat, Jim is “better” and John
is “worse”. Finally, in (5b), he and Jim are “better”,
so he is resolved to Jim; on the other hand, in (5a),
he and John are “worse”, so he is resolved to John.
We automate this (human) method for resolv-
ing pronouns as follows. We begin by determin-
ing whether we can assign a rank value (i.e., “bet-
ter” or “worse”) to the pronoun and the two can-
didate antecedents. For instance, to determine the
rank value of the pronoun A, we first determine the
polarity value pA of its anchor word wA, which is
either the verb v for which A serves as the deep sub-
ject, or the adjective modifying A if v does not ex-
ist,7 using Wilson et al.’s (2005b) subjectivity lex-
icon.8 If pA is not NEUTRAL, we check whether
it can be flipped by the context of wA. We con-
sider three kinds of polarity-reversing context: nega-
tion, comparative adverb, and discourse connective.
Specifically, we determine whether wA is negated
using the Stanford dependency parser, which explic-
</bodyText>
<footnote confidence="0.99927775">
7In the sentiment analysis and opinion mining literature,
(WA, PA) is known as an opinion-target pair.
8The lexicon contains 8221 words, each of which is hand
labeled with a polarity of POSITIVE, NEGATIVE, or NEUTRAL.
</footnote>
<page confidence="0.991033">
782
</page>
<bodyText confidence="0.999988125">
itly annotates instances of negation; we determine
the existence of a comparative adverb (e.g., “more”,
“less”) using the POS tag “RBR”; and we determine
whether A exists in a clause headed by a polarity-
reversing connective, such as although. After flip-
ping pA by context, we can infer A’s rank value from
it. Specifically, A’s rank value is “better” if pA is
positive; “worse” if pA is negative; and “cannot be
determined” if pA is neutral. The polarity values of
the two candidate antecedents can be determined in
a similar fashion. Note that sometimes we may need
to infer rank values. For example, given the sentence
“Jane is prettier than Jill”, prettier has a positive po-
larity, so its modifying NP, Jane, has a “better” rank,
and we can infer that Jill’s rank is “worse”.
We create three features, HPOL1, HPOL2, and
HPOL3, based on our heuristic polarity determina-
tion component. Specifically, if the rank value of
the pronoun or the rank value of one or both of the
candidate antecedents cannot be determined, the val-
ues of all three binary features will be set to zero
for both i1 and i2. Otherwise, we compute the val-
ues of the three features as follows. To compute
HPOL1, which is a binary feature, we (1) employ
a heuristic resolution procedure, which resolves the
pronoun to the candidate antecedent with the same
rank value, and then (2) encode the outcome of this
heuristic procedure as the value of HPOL1. For ex-
ample, since the first candidate antecedent, John, is
predicted to be the antecedent in (5a), HPOL1(i1)=1
and HPOL1(i2)=0. The value of HPOL2 is the
concatenation of the polarity values determined
for the pronoun and the candidate antecedent.
Referring again to (5a), HPOL2(i1)=positive-
positive and HPOL2(i2)=positive-negative. To
compute HPOL3 for a given instance, we sim-
ply take its HPOL2 value and append the
connective to it. Using (5a) as an exam-
ple, HPOL3(i1)=positive-positive-even-though and
HPOL3(i1)=positive-negative-even-though.
</bodyText>
<subsectionHeader confidence="0.923413">
4.5 Machine-Learned Polarity
</subsectionHeader>
<bodyText confidence="0.999811666666667">
In the previous subsection, we compute the polarity
of a word by updating its prior polarity heuristically
with contextual information. We hypothesized that
polarity could be computed more accurately by em-
ploying a sentiment analyzer that can capture richer
contextual information. For this reason, we employ
OpinionFinder (Wilson et al., 2005a), which has a
pre-trained classifier for annotating the phrases in a
sentence with their contextual polarity values.
Given a sentence and the polarity values of the
phrases annotated by OpinionFinder, we determine
the rank values of the pronoun and the two candi-
date antecedents by mapping them to the polarized
phrases using the dependency relations provided by
the Stanford dependency parser. We create three bi-
nary features, LPOL1, LPOL2, and LPOL3, whose
values are computed in the same way as HPOL1,
HPOL2, and HPOL3, respectively, except that the
computation here is based on the machine-learned
polarity values rather than the heuristically deter-
mined polarity values.
</bodyText>
<subsectionHeader confidence="0.523778">
4.6 Connective-Based Relations
</subsectionHeader>
<bodyText confidence="0.632628">
Consider the following sentences:
</bodyText>
<listItem confidence="0.8997865">
(6a) Google bought Motorola because they
want its customer base.
(6b) Google bought Motorola because they
are rich.
</listItem>
<bodyText confidence="0.9998762">
Humans resolve they to Google in (6a) by exploit-
ing the world knowledge that there is a causal rela-
tion (signaled by the discourse connective because)
between the want event and the buy event. A simi-
lar mechanism is used to resolve they to Google in
(6b): from world knowledge we know that there is a
causal relation between rich and buy.
We automate this (human) method for resolving
pronouns as follows. First, we gather connective-
based relations of this kind from a large, unanno-
tated corpus. In our experiments, we use as our
unannotated corpus the documents in three text cor-
pora (namely, BLLIP, Reuters, and English Giga-
word), but retain only those sentences that con-
tain a single discourse connective and do not be-
gin with the connective. From these sentences,
we collect triples and their frequencies of occur-
rences in the corpus. Each triple is of the form
(V,Conn,X), where Conn is a discourse connec-
tive, V is a stemmed verb in the clause preceding
Conn, and X is a stemmed verb or an adjective in
the clause following Conn. Each triple essentially
denotes a relation between V and X expressed by
Conn. Conceivably, the strength of the relation in a
triple increases with its frequency count.
</bodyText>
<page confidence="0.996965">
783
</page>
<bodyText confidence="0.99999368">
We use the frequency counts of these triples to
heuristically predict the correct antecedent for a tar-
get pronoun. Given a sentence where Conn is the
discourse connective, X is the stemmed verb gov-
erning the target pronoun A or the adjective modify-
ing A (if X is a to be verb), and V is the stemmed
verb governing the candidate antecedents, we re-
trieve the frequency count of the triple (V ,Conn,X).
If the count is at least 100, we employ a procedure
for heuristically selecting the antecedent for the tar-
get anaphor. Specifically, if X is a verb, then it re-
solves the target pronoun to the candidate antecedent
that has the same grammatical role as the pronoun.
However, if X is an adjective and the sentence does
not involve comparison, then it resolves the target
pronoun to the candidate antecedent serving as the
subject of V .
We create a binary feature, CBR, that encodes
this heuristic decision. In our running example, the
triple (buy, because, want) occurs 860 times in our
corpus, so the pronoun they is resolved to the can-
didate antecedent that occurs as the subject of buy.
Hence, CBR(i1)=1 and CBR(i2)=0. However, had
the triple occurred less than 100 times, both of these
features would have been set to zero.
</bodyText>
<subsectionHeader confidence="0.994294">
4.7 Semantic Compatibility
</subsectionHeader>
<bodyText confidence="0.998512018867925">
Some of the queries generated by the Google com-
ponent, such as Q1 and Q2, aim to capture the
semantic compatibility between a candidate an-
tecedent, C, and the verb governing the target pro-
noun, V . However, using web search queries to esti-
mate semantic compatibility has potential problems,
including (1) a precision problem: the fact that C
and V appear next to each other in a query does
not necessarily imply that a subject-verb relation ex-
ists between them; and (2) a recall problem: these
queries fail to capture subject-verb relations where
C and V are not immediately adjacent to each other.
To address these potential problems, we com-
pute knowledge of selectional preferences from a
large, unannotated corpus. As before, we cre-
ate our unannotated corpus using the documents in
BLLIP, Reuters, and English Gigaword. Specifi-
cally, we first parse each sentence in the corpus us-
ing the Stanford dependency parser. Then, for each
stemmed verb v and each stemmed noun n in the
corpus, we collect the following statistics: (1) the
number of times n is the subject of v; (2) the num-
ber of times n is the direct object of v; (3) the mutual
information (MI) of v and n (with n as the subject
of v); and (4) the MI of v and n (with n as the direct
object of v).9
To understand how we use these statistics to gen-
erate features for resolving pronouns, consider the
following sentence:
(7) The man stole the neighbor’s bike because
he needed one.
Assuming that the target pronoun and its govern-
ing verb V has grammatical relation GR, we create
three features, SC1, SC2, and SC3, based on our se-
mantic compatibility component. SC1 encodes the
MI value of the head noun of a candidate antecedent
and V (and GR). SC2 is a binary feature whose
value indicates which of the candidate antecedents
has a larger MI value with V (and GR). SC3 is the
same as SC2, except that MI is replaced with corpus
frequency. In other words, SC2 and SC3 employ
different measures to heuristically predict the cor-
rect antecedent for the target pronoun. If the target
pronoun is governed by a to be verb, the values of
these three features will all be set to zero.
Given our running example, we first retrieve
the following corpus-based statistics: MI(need:subj,
man)=0.6322; MI(need:subj, neighbor)=0.3975;
count(need:subj, man)=474; and count(need:subj,
neighbor)=68. Using these statistics, we can then
compute the aforementioned features for our exam-
ple. Specifically, SC1(i1)=0.6322, SC1(i2)=0.3975,
SC2(i1)=1, SC2(i2)=0, SC3(i1)=1, and SC3(i2)=0.
</bodyText>
<subsectionHeader confidence="0.994678">
4.8 Lexical Features
</subsectionHeader>
<bodyText confidence="0.999970727272727">
We exploit the coreference-annotated training docu-
ments by creating lexical features from them. These
lexical features can be divided into two categories,
depending on whether they are computed based on
the candidate antecedents.
Let us begin with the antecedent-independent fea-
tures. Assuming that W is an arbitrary word in a
sentence 5 that is not part of a candidate antecedent
and Conn is the connective in 5, we create three
types of binary-valued antecedent-independent fea-
tures, namely (1) unigrams, where we create one
</bodyText>
<footnote confidence="0.4520375">
9We use the same formula as described in Section 4.2 of
Bergsma and Lin (2006) to compute MI values.
</footnote>
<page confidence="0.997506">
784
</page>
<bodyText confidence="0.999976851851852">
feature for each W; (2) word pairs, where we cre-
ate features by pairing each W appearing before
Conn with each W appearing after Conn, exclud-
ing adjective-noun and noun-adjective pairs10; and
(3) word triples, where we augment each word pair
in (2) with Conn. The value of each feature f indi-
cates the presence or absence of f in S.
Next, we compute the antecedent-dependent fea-
tures. Let (1) HC1 and HC2 be the head words of
candidate antecedents C1 and C2, respectively; (2)
VC1, VC2, and VA be the verbs governing C1, C2,
and the target pronoun A, respectively; and (3) JC1,
JC2, and JA be the adjectives modifying C1, C2, and
A, respectively.11 We create from each candidate an-
tecedent four features, each of which is a word pair.
From C1, we create (HC1, VC1), (HC1, JC1), (HC1,
VA), and (HC1, JA), all of which will appear in the
feature vector corresponding to C1. A similar set of
four features are created from C2. These antecedent-
dependent features are all binary-valued.
It is worth mentioning that while we also consid-
ered word triples in the connective-based relations
component and word pairs in the semantic compat-
ibility component, in those components we deter-
mine their usefulness in an unsupervised manner,
whereas by employing them as lexical features we
determine their usefulness in a supervised manner.
</bodyText>
<sectionHeader confidence="0.999812" genericHeader="evaluation">
5 Evaluation
</sectionHeader>
<subsectionHeader confidence="0.993037">
5.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.999881875">
Dataset. We report results on the test set, which
comprises 30% of our hand-annotated sentence pairs
(see Section 2 for details).
Evaluation metrics. Results are expressed in
terms of accuracy, which is the percentage of cor-
rectly resolved target pronouns. We also report the
percentages of these pronouns that are (1) not re-
solved and (2) incorrectly resolved.
</bodyText>
<subsectionHeader confidence="0.970692">
5.2 Results and Discussion
</subsectionHeader>
<bodyText confidence="0.998885">
The Random baseline. Our first baseline is a re-
solver that randomly guesses the antecedent for the
</bodyText>
<footnote confidence="0.522149">
10Pairing an adjective A in one clause with a noun N in an-
other clause may mislead the learner into thinking that N is
modified by A, and hence we do not create such pairs.
11If C1, C2, and A are not modified by adjectives, no
adjective-based features will be created.
</footnote>
<bodyText confidence="0.999957553191489">
target pronoun in each sentence. Since there are
two candidate antecedents per sentence, the Random
baseline should achieve an accuracy of 50%.
The Stanford resolver. Our second baseline is the
Stanford resolver (Lee et al., 2011), which achieves
the best performance in the CoNLL 2011 shared task
(Pradhan et al., 2011). As a rule-based resolver, it
does not exploit any coreference-annotated data.
Recall from Section 3 that our system assumes as
input not only a sentence containing a target pronoun
but also the two candidate antecedents. To ensure a
fair comparison, the same input is provided to this
and other baselines. Hence, if the Stanford resolver
decides to resolve the target pronoun, it will resolve
it to one of the two candidate antecedents. However,
if it does not have enough confidence about resolv-
ing it, it will leave it unresolved. Its performance on
the test set is shown in the “Unadjusted Scores” col-
umn in row 1 of Table 3. As we can see, it correctly
resolves 40.1% of the pronouns, incorrectly resolves
29.8% of them, and does not make any decision on
the remaining 30.1%.
Given that the Random baseline correctly resolves
50% of pronouns and the Stanford resolver correctly
resolves only 40.1% of the pronouns, it is tempting
to conclude that Stanford does not perform as well
as Random. However, recall that Stanford leaves
30.1% of the pronouns unresolved. Hence, to ensure
a fairer comparison, we produce “adjusted” scores
for the Stanford resolver, where we “force” it to re-
solve all of the unresolved target pronouns by as-
suming that probabilistically half of them will be re-
solved correctly. This adjusted score is shown in the
“Adjusted Scores” column in row 1 of Table 3. As
we can see, Stanford achieves an accuracy of 55.1%,
which is 5.1 points higher than that of Random.
The Baseline Ranker. To understand whether the
somewhat unsatisfactory Stanford results can be at-
tributed to its inability to exploit the training data,
we employ as our third baseline a mention ranker
that is trained in the same way as our system (see
Section 3), except that it employs 39 commonly-
used linguistic features for learning-based corefer-
ence resolution (see Table 1 of Rahman and Ng
(2009) for a description of these features). Hence,
the performance difference between this Baseline
Ranker and our system can be attributed entirely
</bodyText>
<page confidence="0.994929">
785
</page>
<table confidence="0.999806">
Unadjusted Scores Adjusted Scores
Coreference System Correct Wrong No Decision Correct Wrong No Decision
Stanford 40.07% 29.79% 30.14% 55.14% 44.86% 0.00%
Baseline Ranker 47.70% 47.16% 5.14% 50.27% 49.73% 0.00%
Stanford+Baseline Ranker 53.49% 43.12% 3.39% 55.19% 44.77% 0.00%
Our system 73.05% 26.95% 0.00% 73.05% 26.95% 0.00%
</table>
<tableCaption confidence="0.99966">
Table 3: Results of the Stanford resolver, the Baseline Ranker, the Combined resolver, and our system.
</tableCaption>
<figure confidence="0.82060875">
1
2
3
4
</figure>
<bodyText confidence="0.99812702631579">
to the difference between the two linguistic feature
sets. Results of the Baseline Ranker are shown in
row 2 of Table 3. Before score adjustment, it cor-
rectly resolves 47.7% of the target pronouns, incor-
rectly resolves 47.2% of them, and leaves the re-
maining 5.1% unresolved. (Note that we output “no
decision” if the ranker assigns the same rank value
to both candidate antecedents.) After score adjust-
ment, its accuracy is 50.3%, which is 0.3 points
higher than that of Random but statistically indis-
tinguishable from it.12 On the other hand, its accu-
racy is 4.9 points lower than that of Stanford, and
the difference between their performance is signifi-
cant. While it seems somewhat surprising that a su-
pervised resolver does not perform as well as a rule-
based resolver, neither of them employs knowledge
sources that are particularly useful for our dataset. In
other words, despite given access to annotated data,
the Baseline Ranker may not be able to make effec-
tive use of it due to the lack of useful features.
The Combined resolver. We create a fourth base-
line by combining the Stanford resolver and the
Baseline Ranker. The motivation is that the former
can provide better precision and the latter can pro-
vide better recall by handling “no decision” cases
not covered by the former. Note that the Baseline
Ranker will be applied to resolve only those pro-
nouns that are left unresolved by Stanford. Results
in row 3 of Table 3 show that the adjusted accuracy
of this Combined resolver is 55.2%, which is sta-
tistically indistinguishable from Stanford’s adjusted
accuracy. Hence, these results show that the addi-
tion of the Baseline Ranker does not help improve
Stanford’s resolution accuracy.
Our system. Results of our system, which is
trained using the features described in Section 4 in
combination with a ranking model, are shown in
row 4 of Table 3. As we can see, our system achieves
</bodyText>
<footnote confidence="0.6812775">
12All statistical significance test results in this paper are ob-
tained using the paired t-test, with p &lt; 0.05.
</footnote>
<table confidence="0.9998718">
Feature Type Correct Wrong No Decision
All features 73.05% 26.95% 0.00%
−Narrative Chains 68.97% 31.03% 0.00%
−Google 65.96% 34.04% 0.00%
−FrameNet 72.16% 27.84% 0.00%
−Heuristic Polarity 71.45% 28.55% 0.00%
−Learned Polarity 72.70% 27.30% 0.00%
−Connective-Based Rel. 71.28% 28.72% 0.00%
−Semantic Compat. 71.81% 28.19% 0.00%
−Lexical Features 60.11% 25.35% 14.54%
</table>
<tableCaption confidence="0.999898">
Table 4: Results of feature ablation experiments.
</tableCaption>
<bodyText confidence="0.9956116">
an accuracy of 73.1%, significantly outperforming
the Combined resolver by 17.9 points in accuracy.
These results suggest that our features are more use-
ful for resolving difficult pronouns than those com-
monly used for coreference resolution.
</bodyText>
<subsectionHeader confidence="0.999275">
5.3 Feature Analysis
</subsectionHeader>
<bodyText confidence="0.999984434782609">
In an attempt to gain additional insight into the per-
formance contribution of each of the eight types of
features used in our system, we conduct feature ab-
lation experiments. The unadjusted scores of these
experiments are shown in Table 4, where each row
shows the performance of the model trained on all
types of features except for the one shown in that
row. For easy reference, the performance of the
model trained on all types of features is shown in
row 1 of the table.
A few points deserve mention. First, perfor-
mance drops significantly whichever feature type is
removed. This suggests that all eight feature types
are contributing positively to overall accuracy. Sec-
ond, the Google-based features and the Lexical Fea-
tures are the most useful, and those generated via
FrameNet and Learned Polarity are the least use-
ful in the presence of other feature types. While it
is somewhat surprising that Learned Polarity is not
more useful than Heuristic Polarity, we speculate
the reason can be attributed to the fact that the cor-
pus on which OpinionFinder was trained was quite
different from ours. Finally, even without using the
</bodyText>
<page confidence="0.994676">
786
</page>
<table confidence="0.999924333333333">
Feature Type Correct Wrong No Decision
Narrative Chains 30.67% 24.47% 44.86%
Google 33.16% 7.09% 59.75%
FrameNet 7.27% 4.08% 88.65%
Learned Polarity 4.79% 2.66% 92.55%
Heuristic Polarity 7.27% 1.77% 90.96%
Connective-Based Rel. 14.01% 8.69% 77.30%
Semantic Compat. 23.58% 13.12% 63.30%
Lexical Features 56.91% 43.09% 0.00%
</table>
<tableCaption confidence="0.999857">
Table 5: Results of single-feature coreference models.
</tableCaption>
<bodyText confidence="0.999791133333333">
Lexical Features, our system still outperforms all the
baseline resolvers: as can been implied from the last
row of Table 4, in the absence of the Lexical Fea-
tures, our resolver achieves an adjusted accuracy of
67.4%, which is only 5.7 points less than that ob-
tained when the full feature set is employed. Hence,
while the Lexical Features are useful, their impor-
tance should not be over-emphasized.
To get a better idea of the utility of each feature
type, we conduct another experiment in which we
train eight models, each of which employs exactly
one type of features. Their unadjusted scores are
shown in Table 5. As we can see, Learned Polarity
has the smallest contribution, whereas the Lexical
Features have the largest contribution.
</bodyText>
<subsectionHeader confidence="0.777052">
5.4 Error Analysis
</subsectionHeader>
<bodyText confidence="0.99998984">
While our resolver significantly outperforms state-
of-the-art resolvers, there is a lot of room for im-
provement. To help direct future research on the res-
olution of difficult pronouns, we analyze the major
sources of errors made by our resolver.
Our analysis reveals that many of the errors cor-
respond to cases that cannot be handled by any of
the eight components of our resolver. To understand
these cases, consider first the strengths and weak-
nesses of Narrative Chains and Google, the two
components that contribute the most to overall per-
formance after Lexical Features.
Google is especially good at capturing facts, such
as lions are predators and zebras are not predators,
helping us correctly resolve sentences such as (5a)
and (5b), as well as those in sentence pair (I) in Ta-
ble 1. However, it may not be good at handling pro-
nouns whose resolution requires an understanding of
the connection between the facts or events described
in the two clauses of a sentence. The reason is that
establishing such a connection requires that we con-
struct a search query composed of information ex-
tracted from both clauses, and the resulting, possi-
bly long, query is likely to receive no hit count due
to data sparseness. Investigating how to construct
such queries while avoiding data sparseness would
be an interesting line of future work.
Narrative chains, on the other hand, are useful
for capturing the relationship between the events de-
scribed in the two clauses. However, they are com-
puted over verbs, and therefore cannot capture such
a relationship when one or both of the events in-
volved are not described by verbs. For example,
narrative chains fail to capture the causal relation
between the event expressed by angry and shout in
sentence (1b). It is also worth mentioning that some
pronouns that could have been resolved using nar-
rative chains are not owing to the coverage and ac-
curacy of Chambers and Jurafsky’s (2008) chains,
but we believe that these recall and precision prob-
lems could be addressed by (1) inducing chains from
a larger corpus and (2) using semantic roles rather
than grammatical roles in the induction process.
Some resolution errors arise from errors in polar-
ity analysis. This can be attributed to the simplicity
of our Heuristic Polarity component: determining
the polarity of a word based on its prior polarity is
too naive. Fine-grained polarity analysis would be
a promising solution to this problem (see Pang and
Lee (2008) and Liu (2012) for related work).
</bodyText>
<sectionHeader confidence="0.999681" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999990125">
We investigated the resolution of complex cases of
definite pronouns, a problem that was under exten-
sive discussion by coreference researchers in the
1970s but has received revived interest owing in part
to its relevance to the Turing Test. Our experimental
results indicate that it is a challenge for state-of-the-
art resolvers, and while we proposed new knowledge
sources for addressing this challenge, our resolver
still has a lot of room for improvement. In partic-
ular, our error analysis indicates that further gains
could be achieved via more accurate sentiment anal-
ysis and induction of world knowledge from corpora
or the Web. In addition, we plan to integrate our
resolver into a general-purpose coreference system
and evaluate the resulting resolver on standard eval-
uation corpora such as MUC, ACE, and OntoNotes.
</bodyText>
<page confidence="0.994803">
787
</page>
<sectionHeader confidence="0.999209" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99972675">
We thank the three anonymous reviewers for their
detailed and insightful comments on an earlier draft
of the paper. This work was supported in part by
NSF Grants IIS-0812261 and IIS-1147644.
</bodyText>
<sectionHeader confidence="0.998906" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999223848484849">
Amit Bagga. 1998. Coreference, Cross-Document
Coreference, and Information Extraction Methodolo-
gies. Ph.D. thesis, Duke University.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet project. In Proceed-
ings of the 36th Annual Meeting of the Association for
Computational Linguistics and the 17th International
Conference on Computational Linguistics, pages 86–
90.
Shane Bergsma and Dekang Lin. 2006. Bootstrapping
path-based pronoun resolution. In Proceedings of the
21st International Conference on Computational Lin-
guistics and 44th Annual Meeting of the Association
for Computational Linguistics, pages 33–40.
Volha Bryl, Claudio Guiliano, Luciano Serafini, and
Kateryna Tymoshenko. 2010. Using background
knowledge to support coreference resolution. In Pro-
ceedings ofthe 19th European Conference on Artificial
Intelligence, pages 759–764.
Donna K. Byron. 2002. Resolving pronominal reference
to abstract entities. In Proceedings of the 40th Annual
Meeting ofthe Association for Computational Linguis-
tics, pages 80–87.
Nathanael Chambers and Dan Jurafsky. 2008. Unsu-
pervised learning of narrative event chains. In Pro-
ceedings of the 46th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Language
Technologies, pages 787–797.
Eugene Charniak and Micha Elsner. 2009. EM works for
pronoun anaphora resolution. In Proceedings of the
12th Conference of the European Chapter of the Asso-
ciation for Computational Linguistics, pages 148–156.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed de-
pendency parses from phrase structure parses. In Pro-
ceedings of the 5th International Conference on Lan-
guage Resources and Evaluation, pages 449–454.
Pascal Denis and Jason Baldridge. 2007. A ranking ap-
proach to pronoun resolution. In Proceedings of the
Twentieth International Conference on Artificial Intel-
ligence, pages 1588–1593.
Pascal Denis and Jason Baldridge. 2008. Specialized
models and ranking for coreference resolution. In Pro-
ceedings of the 2008 Conference on Empirical Meth-
ods in Natural Language Processing, pages 660–669.
Aria Haghighi and Dan Klein. 2009. Simple coreference
resolution with rich syntactic and semantic features.
In Proceedings of the 2009 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1152–1161.
Graeme Hirst. 1981. Anaphora in Natural Language
Understanding. Springer Verlag.
Ryu Iida, Kentaro Inui, Hiroya Takamura, and Yuji Mat-
sumoto. 2003. Incorporating contextual cues in train-
able models for coreference resolution. In Proceed-
ings of the EACL Workshop on The Computational
Treatment ofAnaphora.
Joseph Irwin, Mamoru Komachi, and Yuji Matsumoto.
2011. Narrative schema as world knowledge for
coreference resolution. In Proceedings of the Fif-
teenth Conference on Computational Natural Lan-
guage Learning: Shared Task, pages 86–92.
Thorsten Joachims. 2002. Optimizing search engines us-
ing clickthrough data. In Proceedings of the Eighth
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, pages 133–142.
Heeyoung Lee, Yves Peirsman, Angel Chang, Nathanael
Chambers, Mihai Surdeanu, and Dan Jurafsky. 2011.
Stanford’s multi-pass sieve coreference resolution sys-
tem at the CoNLL-2011 shared task. In Proceedings
of the Fifteenth Conference on Computational Natural
Language Learning: Shared Task, pages 28–34.
Hector J. Levesque. 2011. The Winograd Schema Chal-
lenge. In AAAI Spring Symposium: Logical Formal-
izations of Commonsense Reasoning.
Bing Liu. 2012. Sentiment Analysis and Opinion Min-
ing. Morgan &amp; Claypool Publishers.
Ruslan Mitkov, Richard Evans, and Constantin Orasan.
2002. A new, fully automatic version of Mitkov’s
knowledge-poor pronoun resolution method. In Al.
Gelbukh, editor, Computational Linguistics and Intel-
ligent Text Processing, pages 169–187. Springer.
Natalia N. Modjeska, Katja Markert, and Malvina Nis-
sim. 2003. Using the web in machine learning for
other-anaphora resolution. In Proceedings of the 2003
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 176–183.
Christoph M¨uller. 2007. Resolving it, this, and that in
unrestricted multi-party dialog. In Proceedings of the
45th Annual Meeting of the Association of Computa-
tional Linguistics, pages 816–823.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and Trends in Infor-
mation Retrieval 2(1–2):1–135.
Massimo Poesio and Mijail A. Kabadjov. 2004.
A general-purpose, off-the-shelf anaphora resolution
module: Implementation and preliminary evaluation.
In Proceedings of the 4th International Conference on
Language Resources and Evaluation, pages 663–666.
</reference>
<page confidence="0.975075">
788
</page>
<reference confidence="0.999863103773585">
Massimo Poesio, Rahul Mehta, Axel Maroudas, and
Janet Hitzeman. 2004. Learning to resolve bridging
references. In Proceedings of the 42nd Annual Meet-
ing of the Association for Computational Linguistics,
pages 143–150.
Massimo Poesio, David Day, Ron Artstein, Jason Dun-
can, Vladimir Eidelman, Claudio Giuliano, Rob Hall,
Janet Hitzeman, Alan Jern, Mijail Kabadjov, Stanley
Yong Wai Keong, Gideon Mann, Alessandro Mos-
chitti, Simone Ponzetto, Jason Smith, Josef Stein-
berger, Michael Strube, Jian Su, Yannick Versley,
Xiaofeng Yang, and Michael Wick. 2007. EL-
ERFED: Final report of the research group on Exploit-
ing Lexical and Encyclopedic Resources For Entity
Disambiguation. Technical report, Summer Workshop
on Language Engineering, Center for Language and
Speech Processing, Johns Hopkins University, Balti-
more, MD.
Simone Paolo Ponzetto and Michael Strube. 2006.
Exploiting semantic role labeling, WordNet and
Wikipedia for coreference resolution. In Proceedings
of the Human Language Technology Conference and
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, pages 192–
199.
Simone Paolo Ponzetto and Michael Strube. 2007.
Knowledge derived from Wikipedia for computing se-
mantic relatedness. Journal of Artificial Intelligence
Research, 30:181–212.
Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,
Martha Palmer, Ralph Weischedel, and Nianwen Xue.
2011. CoNLL-2011 Shared Task: Modeling unre-
stricted coreference in OntoNotes. In Proceedings of
the Fifteenth Conference on Computational Natural
Language Learning: Shared Task, pages 1–27.
Long Qiu, Min-Yen Kan, and Tat-Seng Chua. 2004. A
public reference implementation of the RAP anaphora
resolution algorithm. In Proceedings of the 4th In-
ternational Conference on Language Resources and
Evaluation, pages 291–294.
Karthik Raghunathan, Heeyoung Lee, Sudarshan Ran-
garajan, Nate Chambers, Mihai Surdeanu, Dan Juraf-
sky, and Christopher Manning. 2010. A multi-pass
sieve for coreference resolution. In Proceedings of
the 2010 Conference on Empirical Methods in Natu-
ral Language Processing, pages 492–501.
Altaf Rahman and Vincent Ng. 2009. Supervised mod-
els for coreference resolution. In Proceedings of the
2009 Conference on Empirical Methods in Natural
Language Processing, pages 968–977.
Altaf Rahman and Vincent Ng. 2011. Coreference res-
olution with world knowledge. In Proceedings of the
49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies,
pages 814–824.
Roger C. Schank and Robert P. Abelson. 1977. Scripts,
Plans, Goals, and Understanding. Lawrence Erl-
baum.
Veselin Stoyanov, Claire Cardie, Nathan Gilbert, Ellen
Riloff, David Buttler, and David Hysom. 2010. REC-
ONCILE: A coreference resolution research platform.
In Proceedings of the ACL 2010 Conference Short Pa-
pers, pages 156–161.
Michael Strube and Christoph M¨uller. 2003. A machine
learning approach to pronoun resolution in spoken di-
alogue. In Proceedings of the 41st Annual Meeting of
the Association for Computational Linguistics, pages
168–175.
Alan M. Turing. 1950. Computing machinery and intel-
ligence. Mind, 59:433–460.
Olga Uryupina, Massimo Poesio, Claudio Giuliano, and
Kateryna Tymoshenko. 2011. Disambiguation and
filtering methods in using Web knowledge for coref-
erence resolution. In Proceedings of the 24th Interna-
tional Florida Artificial Intelligence Research Society
Conference, pages 317–322.
Yannick Versley, Simone Paolo Ponzetto, Massimo Poe-
sio, Vladimir Eidelman, Alan Jern, Jason Smith,
Xiaofeng Yang, and Alessandro Moschitti. 2008.
BART: A modular toolkit for coreference resolution.
In Proceedings of the ACL-08: HLT Demo Session,
pages 9–12.
Theresa Wilson, Paul Hoffmann, Swapna Somasun-
daran, Jason Kessler, Janyce Wiebe, Yejin Choi,
Claire Cardie, Ellen Riloff, and Siddharth Patwardhan.
2005a. Opinionfinder: A system for subjectivity anal-
ysis. In Proceedings ofHLT/EMNLP 2005 Interactive
Demonstrations, pages 34–35.
Theresa Wilson, Janyce M. Wiebe, and Paul Hoffmann.
2005b. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of the Joint
Human Language Technology Conference and Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 347–354.
Terry Winograd. 1972. Understanding Natural Lan-
guage. Academic Press, Inc., New York.
Xiaofeng Yang, Guodong Zhou, Jian Su, and Chew Lim
Tan. 2003. Coreference resolution using competitive
learning approach. In Proceedings of the 41st Annual
Meeting ofthe Association for Computational Linguis-
tics, pages 176–183.
Xiaofeng Yang, Jian Su, and Chew Lim Tan. 2005. Im-
proving pronoun resolution using statistics-based se-
mantic compatibility information. In Proceedings of
the 43rd Annual Meeting of the Association for Com-
putational Linguistics, pages 165–172.
</reference>
<page confidence="0.99861">
789
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.475128">
<title confidence="0.9943335">Resolving Complex Cases of Definite The Winograd Schema Challenge</title>
<author confidence="0.974204">Rahman</author>
<affiliation confidence="0.987755">Human Language Technology Research University of Texas at</affiliation>
<address confidence="0.548903">Richardson, TX</address>
<abstract confidence="0.993603444444445">We examine the task of resolving complex cases of definite pronouns, specifically those for which traditional linguistic constraints on coreference (e.g., Binding Constraints, gender and number agreement) as well as commonly-used resolution heuristics (e.g., string-matching facilities, syntactic salience) are not useful. Being able to solve this task has broader implications in artificial intelligence: a restricted version of it, sometimes referred to as the Winograd Schema Challenge, has been suggested as a conceptually and practically appealing alternative to the Turing Test. We employ a knowledge-rich approach to this task, which yields a pronoun resolver that outperforms state-of-the-art resolvers by nearly 18 points in accuracy on our dataset.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Amit Bagga</author>
</authors>
<title>Coreference, Cross-Document Coreference, and Information Extraction Methodologies.</title>
<date>1998</date>
<tech>Ph.D. thesis,</tech>
<institution>Duke University.</institution>
<contexts>
<context position="6530" citStr="Bagga, 1998" startWordPosition="1047" endWordPosition="1048">fficult pronouns in these structurally simple sentences, as they do not have the mechanism to capture the fine distinctions between twin sentences. In other words, when given these sentences, the best that the existing resolvers can do to resolve the pronouns is guessing. This could be surprising to a non-coreference researcher, but it is indeed the state of the art. A natural question is: why do existing resolvers not attempt to handle difficult pronouns? One reason could be that these difficult pronouns do not appear frequently in standard evaluation corpora such as MUC, ACE, and OntoNotes (Bagga, 1998; Haghighi and Klein, 2009). In fact, the Stanford coreference resolver (Lee et al., 2011), which won the CoNLL-2011 shared task on coreference resolution, adopts the once-popular rule-based approach, resolving pronouns simply with rules that encode the aforementioned traditional linguistic constraints on coreference, such as the Binding constraints and gender and number agreement. The infrequency of occurrences of difficult pronouns in these standard evaluation corpora by no means undermines their significance, however. In fact, being able to automatically resolve difficult pronouns has broad</context>
</contexts>
<marker>Bagga, 1998</marker>
<rawString>Amit Bagga. 1998. Coreference, Cross-Document Coreference, and Information Extraction Methodologies. Ph.D. thesis, Duke University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Collin F Baker</author>
<author>Charles J Fillmore</author>
<author>John B Lowe</author>
</authors>
<title>The Berkeley FrameNet project.</title>
<date>1998</date>
<booktitle>In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and the 17th International Conference on Computational Linguistics,</booktitle>
<pages>86--90</pages>
<contexts>
<context position="22558" citStr="Baker et al., 1998" startWordPosition="3721" endWordPosition="3724">e, it is unlikely that Google will return meaningful counts to us. The reason is that both candidate antecedents in the sentence are proper names belonging to the same type (which in this case is PERSON). However, in some cases, we may be able to generate more meaningful queries from such kind of sentences. Consider the following sentence: (4) John killed Jim, so he was arrested. To generate meaningful queries, we make one observation: John and Jim played different roles in a kill event. Hence, we can replace these proper names with their roles. We propose to obtain these roles from FrameNet (Baker et al., 1998). More generally, for each proper name e in a given sentence, we (1) determine the event in which e is involved (using the Stanford dependency parser); (2) search for the FrameNet frame corresponding to the event as well as e’s role in the event; and (3) replace the name with its FrameNet role. In our example, since both names are involved in the kill event, we retrieve the FrameNet frame for kill. Given that John and Jim are the subject and object of kill, we can extract their semantic roles directly from the frame, which are killer and victim, respectively.6 Consequently, we replace the two </context>
</contexts>
<marker>Baker, Fillmore, Lowe, 1998</marker>
<rawString>Collin F. Baker, Charles J. Fillmore, and John B. Lowe. 1998. The Berkeley FrameNet project. In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and the 17th International Conference on Computational Linguistics, pages 86– 90.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shane Bergsma</author>
<author>Dekang Lin</author>
</authors>
<title>Bootstrapping path-based pronoun resolution.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>33--40</pages>
<contexts>
<context position="34630" citStr="Bergsma and Lin (2006)" startWordPosition="5762" endWordPosition="5765">Lexical Features We exploit the coreference-annotated training documents by creating lexical features from them. These lexical features can be divided into two categories, depending on whether they are computed based on the candidate antecedents. Let us begin with the antecedent-independent features. Assuming that W is an arbitrary word in a sentence 5 that is not part of a candidate antecedent and Conn is the connective in 5, we create three types of binary-valued antecedent-independent features, namely (1) unigrams, where we create one 9We use the same formula as described in Section 4.2 of Bergsma and Lin (2006) to compute MI values. 784 feature for each W; (2) word pairs, where we create features by pairing each W appearing before Conn with each W appearing after Conn, excluding adjective-noun and noun-adjective pairs10; and (3) word triples, where we augment each word pair in (2) with Conn. The value of each feature f indicates the presence or absence of f in S. Next, we compute the antecedent-dependent features. Let (1) HC1 and HC2 be the head words of candidate antecedents C1 and C2, respectively; (2) VC1, VC2, and VA be the verbs governing C1, C2, and the target pronoun A, respectively; and (3) </context>
</contexts>
<marker>Bergsma, Lin, 2006</marker>
<rawString>Shane Bergsma and Dekang Lin. 2006. Bootstrapping path-based pronoun resolution. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 33–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Volha Bryl</author>
<author>Claudio Guiliano</author>
<author>Luciano Serafini</author>
<author>Kateryna Tymoshenko</author>
</authors>
<title>Using background knowledge to support coreference resolution.</title>
<date>2010</date>
<booktitle>In Proceedings ofthe 19th European Conference on Artificial Intelligence,</booktitle>
<pages>759--764</pages>
<contexts>
<context position="9872" citStr="Bryl et al. (2010)" startWordPosition="1573" endWordPosition="1576">such as those realized by verb phrases in dialogs (e.g., Byron (2002), Strube and M¨uller (2003), M¨uller (2007)). Nevertheless, to our knowledge, there has been little work that specifically targets difficult pronouns. Given the complexity of our task, we investigate a variety of sophisticated knowledge sources for resolving difficult pronouns, and combine them via a machine learning approach. Note that there has been a recent surge of interest in extracting world knowledge from online encyclopedias such as Wikipedia (e.g., Ponzetto and Strube (2006, 2007), Poesio et al. (2007)), YAGO (e.g., Bryl et al. (2010), Rahman and Ng (2011), Uryupina et al. (2011)), and Freebase (e.g., Lee et al. (2011)). However, the resulting extractions are primarily IS-A relations (e.g., Barack Obama IS-A U. S. president), which would not be useful for resolving definite pronouns. 2 Dataset Creation We asked 30 undergraduate students who are not affiliated with this research to compose sentence pairs (i.e., twin sentences) that conform to the constraints specified in the introduction. Each student was also asked to annotate the candidate antecedents, the target pronoun, and the correct antecedent for each sentence she c</context>
</contexts>
<marker>Bryl, Guiliano, Serafini, Tymoshenko, 2010</marker>
<rawString>Volha Bryl, Claudio Guiliano, Luciano Serafini, and Kateryna Tymoshenko. 2010. Using background knowledge to support coreference resolution. In Proceedings ofthe 19th European Conference on Artificial Intelligence, pages 759–764.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donna K Byron</author>
</authors>
<title>Resolving pronominal reference to abstract entities.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting ofthe Association for Computational Linguistics,</booktitle>
<pages>80--87</pages>
<contexts>
<context position="9323" citStr="Byron (2002)" startWordPosition="1490" endWordPosition="1491">nt machine that can truly understand natural language. To sum up, an important contribution of our work is that it opens up a new line of research involving a problem whose solution requires a deeper understanding of a text. With recent advances in knowledge extraction from text, we believe that time is ripe to tackle this problem. It is worth noting that some researchers have focused on other kinds of anaphors that are hard to resolve, including bridging anaphors (e.g., Poesio et al. (2004)) and anaphors referring to abstract entities, such as those realized by verb phrases in dialogs (e.g., Byron (2002), Strube and M¨uller (2003), M¨uller (2007)). Nevertheless, to our knowledge, there has been little work that specifically targets difficult pronouns. Given the complexity of our task, we investigate a variety of sophisticated knowledge sources for resolving difficult pronouns, and combine them via a machine learning approach. Note that there has been a recent surge of interest in extracting world knowledge from online encyclopedias such as Wikipedia (e.g., Ponzetto and Strube (2006, 2007), Poesio et al. (2007)), YAGO (e.g., Bryl et al. (2010), Rahman and Ng (2011), Uryupina et al. (2011)), an</context>
</contexts>
<marker>Byron, 2002</marker>
<rawString>Donna K. Byron. 2002. Resolving pronominal reference to abstract entities. In Proceedings of the 40th Annual Meeting ofthe Association for Computational Linguistics, pages 80–87.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nathanael Chambers</author>
<author>Dan Jurafsky</author>
</authors>
<title>Unsupervised learning of narrative event chains.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>787--797</pages>
<contexts>
<context position="14013" citStr="Chambers and Jurafsky (2008)" startWordPosition="2251" endWordPosition="2254">able 2. 4.1 Narrative Chains Consider the following sentence: (2) Ed punished Tim because he tried to escape. Humans resolve he to Tim by exploiting the world knowledge that someone who tried to escape is bad and therefore should be punished. Such kind of knowledge can be extracted from narrative chains. Narrative chains are partially ordered sets of events centered around a common protagonist, aiming to encode the kind of knowledge provided by scripts (Schank and Abelson, 1977). While scripts are hand-written, narrative chains can be learned from unannotated text. Below is a chain learned by Chambers and Jurafsky (2008): borrow-s invest-s spend-s pay-s raise-s lend-s As we can see, a narrative chain is composed of a sequence of events (verbs) together with the roles of the protagonist. Here, “s” denotes the subject role, even though a chain can contain a mix of “s” and “o” (the object role). From this chain, we know that the person who borrows something (probably money) may invest, spend, pay, or lend it. We employ narrative chains to heuristically predict the antecedent for the target pronoun, and encode the prediction as a feature. The heuristic decision procedure operates as follows. Given a sentence, we </context>
</contexts>
<marker>Chambers, Jurafsky, 2008</marker>
<rawString>Nathanael Chambers and Dan Jurafsky. 2008. Unsupervised learning of narrative event chains. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 787–797.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Micha Elsner</author>
</authors>
<title>EM works for pronoun anaphora resolution.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>148--156</pages>
<contexts>
<context position="5684" citStr="Charniak and Elsner (2009)" startWordPosition="911" endWordPosition="914">mplies that syntactic salience, a commonly-used heuristic in pronoun resolution that prefers the selection of syntactically salient candidate antecedents, may no longer be useful, since the candidate in the subject position is not more likely to be the correct antecedent than the other candidates. To enable the reader to get a sense of how hard it is to resolve difficult pronouns, Table 1 shows sample twin sentences from our dataset. Note that state-ofthe-art pronoun resolvers (e.g., JavaRAP (Qiu et al., 2004), GuiTaR (Poesio and Kabadjov, 2004), as well as those designed by Mitkov (2002) and Charniak and Elsner (2009)) and coreference resolvers (e.g., BART (Versley et al., 2008), CherryPicker (Rahman and Ng, 2009), Reconcile (Stoyanov et al., 2010), the Stanford resolver (Raghunathan et al., 2010; Lee et al., 2011)) cannot accurately resolve the difficult pronouns in these structurally simple sentences, as they do not have the mechanism to capture the fine distinctions between twin sentences. In other words, when given these sentences, the best that the existing resolvers can do to resolve the pronouns is guessing. This could be surprising to a non-coreference researcher, but it is indeed the state of the </context>
</contexts>
<marker>Charniak, Elsner, 2009</marker>
<rawString>Eugene Charniak and Micha Elsner. 2009. EM works for pronoun anaphora resolution. In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics, pages 148–156.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Bill MacCartney</author>
<author>Christopher D Manning</author>
</authors>
<title>Generating typed dependency parses from phrase structure parses.</title>
<date>2006</date>
<booktitle>In Proceedings of the 5th International Conference on Language Resources and Evaluation,</booktitle>
<pages>449--454</pages>
<marker>de Marneffe, MacCartney, Manning, 2006</marker>
<rawString>Marie-Catherine de Marneffe, Bill MacCartney, and Christopher D. Manning. 2006. Generating typed dependency parses from phrase structure parses. In Proceedings of the 5th International Conference on Language Resources and Evaluation, pages 449–454.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascal Denis</author>
<author>Jason Baldridge</author>
</authors>
<title>A ranking approach to pronoun resolution.</title>
<date>2007</date>
<booktitle>In Proceedings of the Twentieth International Conference on Artificial Intelligence,</booktitle>
<pages>1588--1593</pages>
<contexts>
<context position="12083" citStr="Denis and Baldridge, 2007" startWordPosition="1925" endWordPosition="1929">r ease of exposition, we will assume below that there are two candidate antecedents per sentence. 3 Machine Learning Framework Since our goal is to determine which of the two candidate antecedents is the correct antecedent for the target pronoun in each sentence, our system assumes as input the sentence, the target pronoun, and the two candidate antecedents. We employ machine learning to combine the features derived from different knowledge sources. Specifically, we employ a ranking-based approach. Ranking-based approaches have been shown to outperform their classification-based counterparts (Denis and Baldridge, 2007, 2008; Iida et al., 2003; Yang et al., 2003). Given a pronoun and two candidate antecedents, we aim to train a ranking model that ranks the two candidates such that the correct antecedent is assigned a higher rank. More formally, given training sentence Sk containing target pronoun Ak, correct antecedent Ck and incorrect antecedent Ik, we create two feature vectors, xCAk and xIAk, where xCAk is generated from Ak and Ck, and xIAk is generated from Ak and Ik. The training set consists of ordered pairs of feature vectors (xCAk, xIAk), and the goal of the training procedure is to acquire a ranker</context>
</contexts>
<marker>Denis, Baldridge, 2007</marker>
<rawString>Pascal Denis and Jason Baldridge. 2007. A ranking approach to pronoun resolution. In Proceedings of the Twentieth International Conference on Artificial Intelligence, pages 1588–1593.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascal Denis</author>
<author>Jason Baldridge</author>
</authors>
<title>Specialized models and ranking for coreference resolution.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>660--669</pages>
<marker>Denis, Baldridge, 2008</marker>
<rawString>Pascal Denis and Jason Baldridge. 2008. Specialized models and ranking for coreference resolution. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 660–669.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Dan Klein</author>
</authors>
<title>Simple coreference resolution with rich syntactic and semantic features.</title>
<date>2009</date>
<contexts>
<context position="6557" citStr="Haghighi and Klein, 2009" startWordPosition="1049" endWordPosition="1052">uns in these structurally simple sentences, as they do not have the mechanism to capture the fine distinctions between twin sentences. In other words, when given these sentences, the best that the existing resolvers can do to resolve the pronouns is guessing. This could be surprising to a non-coreference researcher, but it is indeed the state of the art. A natural question is: why do existing resolvers not attempt to handle difficult pronouns? One reason could be that these difficult pronouns do not appear frequently in standard evaluation corpora such as MUC, ACE, and OntoNotes (Bagga, 1998; Haghighi and Klein, 2009). In fact, the Stanford coreference resolver (Lee et al., 2011), which won the CoNLL-2011 shared task on coreference resolution, adopts the once-popular rule-based approach, resolving pronouns simply with rules that encode the aforementioned traditional linguistic constraints on coreference, such as the Binding constraints and gender and number agreement. The infrequency of occurrences of difficult pronouns in these standard evaluation corpora by no means undermines their significance, however. In fact, being able to automatically resolve difficult pronouns has broader implications in artifici</context>
</contexts>
<marker>Haghighi, Klein, 2009</marker>
<rawString>Aria Haghighi and Dan Klein. 2009. Simple coreference resolution with rich syntactic and semantic features.</rawString>
</citation>
<citation valid="false">
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1152--1161</pages>
<marker></marker>
<rawString>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1152–1161.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graeme Hirst</author>
</authors>
<title>Anaphora in Natural Language Understanding.</title>
<date>1981</date>
<publisher>Springer Verlag.</publisher>
<contexts>
<context position="4822" citStr="Hirst, 1981" startWordPosition="771" endWordPosition="772"> should have. V(a) Medvedev will cede the presidency to Putin because he is more popular. V(b) Medvedev will cede the presidency to Putin because he is less popular. Table 1: Sample twin sentences. The target pronoun in each sentence is italicized, and its antecedent is boldfaced. string-matching facilities will not be useful either, since the pronoun and its candidate antecedents do not have any words in common. As in the shout example, we ensure that each sentence has a twin. Twin sentences were used extensively by researchers in the 1970s to illustrate the difficulty of pronoun resolution (Hirst, 1981). We consider two sentences as twins if (1) they are identical up to and possibly including the discourse connective; and (2) the difficult pronouns in them are lexically identical but have different antecedents. The presence of twins implies that syntactic salience, a commonly-used heuristic in pronoun resolution that prefers the selection of syntactically salient candidate antecedents, may no longer be useful, since the candidate in the subject position is not more likely to be the correct antecedent than the other candidates. To enable the reader to get a sense of how hard it is to resolve </context>
</contexts>
<marker>Hirst, 1981</marker>
<rawString>Graeme Hirst. 1981. Anaphora in Natural Language Understanding. Springer Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryu Iida</author>
<author>Kentaro Inui</author>
<author>Hiroya Takamura</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Incorporating contextual cues in trainable models for coreference resolution.</title>
<date>2003</date>
<booktitle>In Proceedings of the EACL Workshop on The Computational Treatment ofAnaphora.</booktitle>
<contexts>
<context position="12108" citStr="Iida et al., 2003" startWordPosition="1931" endWordPosition="1934">ume below that there are two candidate antecedents per sentence. 3 Machine Learning Framework Since our goal is to determine which of the two candidate antecedents is the correct antecedent for the target pronoun in each sentence, our system assumes as input the sentence, the target pronoun, and the two candidate antecedents. We employ machine learning to combine the features derived from different knowledge sources. Specifically, we employ a ranking-based approach. Ranking-based approaches have been shown to outperform their classification-based counterparts (Denis and Baldridge, 2007, 2008; Iida et al., 2003; Yang et al., 2003). Given a pronoun and two candidate antecedents, we aim to train a ranking model that ranks the two candidates such that the correct antecedent is assigned a higher rank. More formally, given training sentence Sk containing target pronoun Ak, correct antecedent Ck and incorrect antecedent Ik, we create two feature vectors, xCAk and xIAk, where xCAk is generated from Ak and Ck, and xIAk is generated from Ak and Ik. The training set consists of ordered pairs of feature vectors (xCAk, xIAk), and the goal of the training procedure is to acquire a ranker that minimizes the numbe</context>
</contexts>
<marker>Iida, Inui, Takamura, Matsumoto, 2003</marker>
<rawString>Ryu Iida, Kentaro Inui, Hiroya Takamura, and Yuji Matsumoto. 2003. Incorporating contextual cues in trainable models for coreference resolution. In Proceedings of the EACL Workshop on The Computational Treatment ofAnaphora.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Irwin</author>
<author>Mamoru Komachi</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Narrative schema as world knowledge for coreference resolution.</title>
<date>2011</date>
<booktitle>In Proceedings of the Fifteenth Conference on Computational Natural Language Learning: Shared Task,</booktitle>
<pages>86--92</pages>
<contexts>
<context position="17069" citStr="Irwin et al. (2011)" startWordPosition="2760" endWordPosition="2763">ssume in the rest of the paper that i1 and i2 are the feature vectors corresponding to the first candidate antecedent and the second candidate an2Throughout the paper, the subject/object of an event refers to its deep rather than surface subject/object. We determine the grammatical role of an NP using the Stanford dependency parser (de Marneffe et al., 2006) and a set of simple heuristics. 3We employ narrative chains of length 12, which are available from http://cs.stanford.edu/people/ nc/schemas/schemas-size12. 4For an alternative way of using narrative chains for coreference resolution, see Irwin et al. (2011). 780 tecedent, respectively.5 For our running example, since Tim is predicted to be the antecedent of he, the value of NC in i2 is 1, and its value in i1 is 0. For notational convenience, we write NC(i1)=0 and NC(i2)=1, and will follow this convention when describing the features in the rest of the paper. Finally, we note that NC(i1) and NC(i2) will both be set to zero if (1) the pronoun and the antecedents do not participate in events, or (2) no narrative chains can be extracted in step 4 above, or (3) step 4 enables us to extract more than one chain and these chains indicate that the candid</context>
</contexts>
<marker>Irwin, Komachi, Matsumoto, 2011</marker>
<rawString>Joseph Irwin, Mamoru Komachi, and Yuji Matsumoto. 2011. Narrative schema as world knowledge for coreference resolution. In Proceedings of the Fifteenth Conference on Computational Natural Language Learning: Shared Task, pages 86–92.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Optimizing search engines using clickthrough data.</title>
<date>2002</date>
<booktitle>In Proceedings of the Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,</booktitle>
<pages>133--142</pages>
<marker>Joachims, 2002</marker>
<rawString>Thorsten Joachims. 2002. Optimizing search engines using clickthrough data. In Proceedings of the Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 133–142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heeyoung Lee</author>
<author>Yves Peirsman</author>
<author>Angel Chang</author>
<author>Nathanael Chambers</author>
<author>Mihai Surdeanu</author>
<author>Dan Jurafsky</author>
</authors>
<date>2011</date>
<contexts>
<context position="5885" citStr="Lee et al., 2011" startWordPosition="941" endWordPosition="944">ject position is not more likely to be the correct antecedent than the other candidates. To enable the reader to get a sense of how hard it is to resolve difficult pronouns, Table 1 shows sample twin sentences from our dataset. Note that state-ofthe-art pronoun resolvers (e.g., JavaRAP (Qiu et al., 2004), GuiTaR (Poesio and Kabadjov, 2004), as well as those designed by Mitkov (2002) and Charniak and Elsner (2009)) and coreference resolvers (e.g., BART (Versley et al., 2008), CherryPicker (Rahman and Ng, 2009), Reconcile (Stoyanov et al., 2010), the Stanford resolver (Raghunathan et al., 2010; Lee et al., 2011)) cannot accurately resolve the difficult pronouns in these structurally simple sentences, as they do not have the mechanism to capture the fine distinctions between twin sentences. In other words, when given these sentences, the best that the existing resolvers can do to resolve the pronouns is guessing. This could be surprising to a non-coreference researcher, but it is indeed the state of the art. A natural question is: why do existing resolvers not attempt to handle difficult pronouns? One reason could be that these difficult pronouns do not appear frequently in standard evaluation corpora</context>
<context position="9958" citStr="Lee et al. (2011)" startWordPosition="1589" endWordPosition="1592">er (2003), M¨uller (2007)). Nevertheless, to our knowledge, there has been little work that specifically targets difficult pronouns. Given the complexity of our task, we investigate a variety of sophisticated knowledge sources for resolving difficult pronouns, and combine them via a machine learning approach. Note that there has been a recent surge of interest in extracting world knowledge from online encyclopedias such as Wikipedia (e.g., Ponzetto and Strube (2006, 2007), Poesio et al. (2007)), YAGO (e.g., Bryl et al. (2010), Rahman and Ng (2011), Uryupina et al. (2011)), and Freebase (e.g., Lee et al. (2011)). However, the resulting extractions are primarily IS-A relations (e.g., Barack Obama IS-A U. S. president), which would not be useful for resolving definite pronouns. 2 Dataset Creation We asked 30 undergraduate students who are not affiliated with this research to compose sentence pairs (i.e., twin sentences) that conform to the constraints specified in the introduction. Each student was also asked to annotate the candidate antecedents, the target pronoun, and the correct antecedent for each sentence she composed. Note that a sentence may contain multiple pronouns, but exactly one of them —</context>
<context position="37005" citStr="Lee et al., 2011" startWordPosition="6165" endWordPosition="6168">lts and Discussion The Random baseline. Our first baseline is a resolver that randomly guesses the antecedent for the 10Pairing an adjective A in one clause with a noun N in another clause may mislead the learner into thinking that N is modified by A, and hence we do not create such pairs. 11If C1, C2, and A are not modified by adjectives, no adjective-based features will be created. target pronoun in each sentence. Since there are two candidate antecedents per sentence, the Random baseline should achieve an accuracy of 50%. The Stanford resolver. Our second baseline is the Stanford resolver (Lee et al., 2011), which achieves the best performance in the CoNLL 2011 shared task (Pradhan et al., 2011). As a rule-based resolver, it does not exploit any coreference-annotated data. Recall from Section 3 that our system assumes as input not only a sentence containing a target pronoun but also the two candidate antecedents. To ensure a fair comparison, the same input is provided to this and other baselines. Hence, if the Stanford resolver decides to resolve the target pronoun, it will resolve it to one of the two candidate antecedents. However, if it does not have enough confidence about resolving it, it w</context>
</contexts>
<marker>Lee, Peirsman, Chang, Chambers, Surdeanu, Jurafsky, 2011</marker>
<rawString>Heeyoung Lee, Yves Peirsman, Angel Chang, Nathanael Chambers, Mihai Surdeanu, and Dan Jurafsky. 2011.</rawString>
</citation>
<citation valid="false">
<title>Stanford’s multi-pass sieve coreference resolution system at the CoNLL-2011 shared task.</title>
<booktitle>In Proceedings of the Fifteenth Conference on Computational Natural Language Learning: Shared Task,</booktitle>
<pages>28--34</pages>
<marker></marker>
<rawString>Stanford’s multi-pass sieve coreference resolution system at the CoNLL-2011 shared task. In Proceedings of the Fifteenth Conference on Computational Natural Language Learning: Shared Task, pages 28–34.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hector J Levesque</author>
</authors>
<title>The Winograd Schema Challenge. In AAAI Spring Symposium: Logical Formalizations of Commonsense Reasoning.</title>
<date>2011</date>
<contexts>
<context position="7199" citStr="Levesque (2011)" startWordPosition="1139" endWordPosition="1140">eference resolver (Lee et al., 2011), which won the CoNLL-2011 shared task on coreference resolution, adopts the once-popular rule-based approach, resolving pronouns simply with rules that encode the aforementioned traditional linguistic constraints on coreference, such as the Binding constraints and gender and number agreement. The infrequency of occurrences of difficult pronouns in these standard evaluation corpora by no means undermines their significance, however. In fact, being able to automatically resolve difficult pronouns has broader implications in artificial intelligence. Recently, Levesque (2011) has argued that the problem of resolving the difficult pronouns in a carefully chosen set of twin sentences, which he refers to as the Winograd Schema Challenge1, could serve as a conceptually and practically appealing alternative to the well-known Turing Test (Turing, 1Levesque (2011) defines a Winograd Schema as a small reading comprehension test involving the question of which of the two candidate antecedents for the definite pronoun in a given sentence is its correct antecedent. Levesque names this challenge after Winograd because of his pioneering attempt to use a well-known pair of twin</context>
</contexts>
<marker>Levesque, 2011</marker>
<rawString>Hector J. Levesque. 2011. The Winograd Schema Challenge. In AAAI Spring Symposium: Logical Formalizations of Commonsense Reasoning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing Liu</author>
</authors>
<title>Sentiment Analysis and Opinion Mining.</title>
<date>2012</date>
<publisher>Morgan &amp; Claypool Publishers.</publisher>
<contexts>
<context position="47011" citStr="Liu (2012)" startWordPosition="7817" endWordPosition="7818"> to the coverage and accuracy of Chambers and Jurafsky’s (2008) chains, but we believe that these recall and precision problems could be addressed by (1) inducing chains from a larger corpus and (2) using semantic roles rather than grammatical roles in the induction process. Some resolution errors arise from errors in polarity analysis. This can be attributed to the simplicity of our Heuristic Polarity component: determining the polarity of a word based on its prior polarity is too naive. Fine-grained polarity analysis would be a promising solution to this problem (see Pang and Lee (2008) and Liu (2012) for related work). 6 Conclusions We investigated the resolution of complex cases of definite pronouns, a problem that was under extensive discussion by coreference researchers in the 1970s but has received revived interest owing in part to its relevance to the Turing Test. Our experimental results indicate that it is a challenge for state-of-theart resolvers, and while we proposed new knowledge sources for addressing this challenge, our resolver still has a lot of room for improvement. In particular, our error analysis indicates that further gains could be achieved via more accurate sentiment</context>
</contexts>
<marker>Liu, 2012</marker>
<rawString>Bing Liu. 2012. Sentiment Analysis and Opinion Mining. Morgan &amp; Claypool Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruslan Mitkov</author>
<author>Richard Evans</author>
<author>Constantin Orasan</author>
</authors>
<title>A new, fully automatic version of Mitkov’s knowledge-poor pronoun resolution method.</title>
<date>2002</date>
<booktitle>Computational Linguistics and Intelligent Text Processing,</booktitle>
<pages>169--187</pages>
<editor>In Al. Gelbukh, editor,</editor>
<publisher>Springer.</publisher>
<marker>Mitkov, Evans, Orasan, 2002</marker>
<rawString>Ruslan Mitkov, Richard Evans, and Constantin Orasan. 2002. A new, fully automatic version of Mitkov’s knowledge-poor pronoun resolution method. In Al. Gelbukh, editor, Computational Linguistics and Intelligent Text Processing, pages 169–187. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Natalia N Modjeska</author>
<author>Katja Markert</author>
<author>Malvina Nissim</author>
</authors>
<title>Using the web in machine learning for other-anaphora resolution.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>176--183</pages>
<contexts>
<context position="20861" citStr="Modjeska et al. (2003)" startWordPosition="3429" endWordPosition="3432"> if one of G3(i1) and G3(i2) is 1, then G4(i1)=G3(i1) and G4(i2)=G3(i2); else G4(i1)=G4(i2)=0. The role of the threshold x should be obvious: it ensures that a heuristic decision is made only if the difference between the counts for the two queries are sufficiently large, because otherwise there is no reason for us to prefer one candidate antecedent to the other. In all of our experiments, we set x to 20. Note that other researchers have also used lexicosyntactic patterns to generate search queries for bridging anaphora resolution (e.g., Poesio et al. (2004)), other-anaphora resolution (e.g., Modjeska et al. (2003)), and learning selectional preferences for pronoun resolution (e.g., Yang et al. (2005)). However, in each of these three cases, the target relations (e.g., the part-whole relation in the case of bridging anaphora resolution, and the subject-verb and verb-object relations in the case of selectional preferences) are specific enough that they can be effectively captured by specific patterns. For example, 781 to determine whether the wheel is part of the car in bridging anaphora resolution, Poesio et al. employ queries of the form “X of Y”, where X and Y would be replaced with the wheel and the </context>
</contexts>
<marker>Modjeska, Markert, Nissim, 2003</marker>
<rawString>Natalia N. Modjeska, Katja Markert, and Malvina Nissim. 2003. Using the web in machine learning for other-anaphora resolution. In Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing, pages 176–183.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph M¨uller</author>
</authors>
<title>Resolving it, this, and that in unrestricted multi-party dialog.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>816--823</pages>
<marker>M¨uller, 2007</marker>
<rawString>Christoph M¨uller. 2007. Resolving it, this, and that in unrestricted multi-party dialog. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 816–823.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Opinion mining and sentiment analysis.</title>
<date>2008</date>
<booktitle>Foundations and Trends in Information Retrieval</booktitle>
<pages>2--1</pages>
<contexts>
<context position="46996" citStr="Pang and Lee (2008)" startWordPosition="7812" endWordPosition="7815">ive chains are not owing to the coverage and accuracy of Chambers and Jurafsky’s (2008) chains, but we believe that these recall and precision problems could be addressed by (1) inducing chains from a larger corpus and (2) using semantic roles rather than grammatical roles in the induction process. Some resolution errors arise from errors in polarity analysis. This can be attributed to the simplicity of our Heuristic Polarity component: determining the polarity of a word based on its prior polarity is too naive. Fine-grained polarity analysis would be a promising solution to this problem (see Pang and Lee (2008) and Liu (2012) for related work). 6 Conclusions We investigated the resolution of complex cases of definite pronouns, a problem that was under extensive discussion by coreference researchers in the 1970s but has received revived interest owing in part to its relevance to the Turing Test. Our experimental results indicate that it is a challenge for state-of-theart resolvers, and while we proposed new knowledge sources for addressing this challenge, our resolver still has a lot of room for improvement. In particular, our error analysis indicates that further gains could be achieved via more acc</context>
</contexts>
<marker>Pang, Lee, 2008</marker>
<rawString>Bo Pang and Lillian Lee. 2008. Opinion mining and sentiment analysis. Foundations and Trends in Information Retrieval 2(1–2):1–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Massimo Poesio</author>
<author>Mijail A Kabadjov</author>
</authors>
<title>A general-purpose, off-the-shelf anaphora resolution module: Implementation and preliminary evaluation.</title>
<date>2004</date>
<booktitle>In Proceedings of the 4th International Conference on Language Resources and Evaluation,</booktitle>
<pages>663--666</pages>
<contexts>
<context position="5609" citStr="Poesio and Kabadjov, 2004" startWordPosition="898" endWordPosition="901">lexically identical but have different antecedents. The presence of twins implies that syntactic salience, a commonly-used heuristic in pronoun resolution that prefers the selection of syntactically salient candidate antecedents, may no longer be useful, since the candidate in the subject position is not more likely to be the correct antecedent than the other candidates. To enable the reader to get a sense of how hard it is to resolve difficult pronouns, Table 1 shows sample twin sentences from our dataset. Note that state-ofthe-art pronoun resolvers (e.g., JavaRAP (Qiu et al., 2004), GuiTaR (Poesio and Kabadjov, 2004), as well as those designed by Mitkov (2002) and Charniak and Elsner (2009)) and coreference resolvers (e.g., BART (Versley et al., 2008), CherryPicker (Rahman and Ng, 2009), Reconcile (Stoyanov et al., 2010), the Stanford resolver (Raghunathan et al., 2010; Lee et al., 2011)) cannot accurately resolve the difficult pronouns in these structurally simple sentences, as they do not have the mechanism to capture the fine distinctions between twin sentences. In other words, when given these sentences, the best that the existing resolvers can do to resolve the pronouns is guessing. This could be sur</context>
</contexts>
<marker>Poesio, Kabadjov, 2004</marker>
<rawString>Massimo Poesio and Mijail A. Kabadjov. 2004. A general-purpose, off-the-shelf anaphora resolution module: Implementation and preliminary evaluation. In Proceedings of the 4th International Conference on Language Resources and Evaluation, pages 663–666.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Massimo Poesio</author>
<author>Rahul Mehta</author>
<author>Axel Maroudas</author>
<author>Janet Hitzeman</author>
</authors>
<title>Learning to resolve bridging references.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>143--150</pages>
<contexts>
<context position="9207" citStr="Poesio et al. (2004)" startWordPosition="1470" endWordPosition="1473">rve for people”. Hence, being able to make progress on this task enables us to move one step closer to building an intelligent machine that can truly understand natural language. To sum up, an important contribution of our work is that it opens up a new line of research involving a problem whose solution requires a deeper understanding of a text. With recent advances in knowledge extraction from text, we believe that time is ripe to tackle this problem. It is worth noting that some researchers have focused on other kinds of anaphors that are hard to resolve, including bridging anaphors (e.g., Poesio et al. (2004)) and anaphors referring to abstract entities, such as those realized by verb phrases in dialogs (e.g., Byron (2002), Strube and M¨uller (2003), M¨uller (2007)). Nevertheless, to our knowledge, there has been little work that specifically targets difficult pronouns. Given the complexity of our task, we investigate a variety of sophisticated knowledge sources for resolving difficult pronouns, and combine them via a machine learning approach. Note that there has been a recent surge of interest in extracting world knowledge from online encyclopedias such as Wikipedia (e.g., Ponzetto and Strube (2</context>
<context position="20803" citStr="Poesio et al. (2004)" startWordPosition="3422" endWordPosition="3425"> G2(i2) is 1, then G4(i1)=G2(i1) and G4(i2)=G2(i2); else if one of G3(i1) and G3(i2) is 1, then G4(i1)=G3(i1) and G4(i2)=G3(i2); else G4(i1)=G4(i2)=0. The role of the threshold x should be obvious: it ensures that a heuristic decision is made only if the difference between the counts for the two queries are sufficiently large, because otherwise there is no reason for us to prefer one candidate antecedent to the other. In all of our experiments, we set x to 20. Note that other researchers have also used lexicosyntactic patterns to generate search queries for bridging anaphora resolution (e.g., Poesio et al. (2004)), other-anaphora resolution (e.g., Modjeska et al. (2003)), and learning selectional preferences for pronoun resolution (e.g., Yang et al. (2005)). However, in each of these three cases, the target relations (e.g., the part-whole relation in the case of bridging anaphora resolution, and the subject-verb and verb-object relations in the case of selectional preferences) are specific enough that they can be effectively captured by specific patterns. For example, 781 to determine whether the wheel is part of the car in bridging anaphora resolution, Poesio et al. employ queries of the form “X of Y</context>
</contexts>
<marker>Poesio, Mehta, Maroudas, Hitzeman, 2004</marker>
<rawString>Massimo Poesio, Rahul Mehta, Axel Maroudas, and Janet Hitzeman. 2004. Learning to resolve bridging references. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics, pages 143–150.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Massimo Poesio</author>
<author>David Day</author>
<author>Ron Artstein</author>
<author>Jason Duncan</author>
<author>Vladimir Eidelman</author>
<author>Claudio Giuliano</author>
<author>Rob Hall</author>
<author>Janet Hitzeman</author>
<author>Alan Jern</author>
<author>Mijail Kabadjov</author>
<author>Stanley Yong Wai Keong</author>
<author>Gideon Mann</author>
<author>Alessandro Moschitti</author>
<author>Simone Ponzetto</author>
<author>Jason Smith</author>
<author>Josef Steinberger</author>
<author>Michael Strube</author>
<author>Jian Su</author>
<author>Yannick Versley</author>
<author>Xiaofeng Yang</author>
<author>Michael Wick</author>
</authors>
<date>2007</date>
<booktitle>ELERFED: Final report of the research group on Exploiting Lexical and Encyclopedic Resources For Entity Disambiguation. Technical report, Summer Workshop on Language Engineering,</booktitle>
<institution>Center for Language and Speech Processing, Johns Hopkins University,</institution>
<location>Baltimore, MD.</location>
<contexts>
<context position="9839" citStr="Poesio et al. (2007)" startWordPosition="1567" endWordPosition="1570">rs referring to abstract entities, such as those realized by verb phrases in dialogs (e.g., Byron (2002), Strube and M¨uller (2003), M¨uller (2007)). Nevertheless, to our knowledge, there has been little work that specifically targets difficult pronouns. Given the complexity of our task, we investigate a variety of sophisticated knowledge sources for resolving difficult pronouns, and combine them via a machine learning approach. Note that there has been a recent surge of interest in extracting world knowledge from online encyclopedias such as Wikipedia (e.g., Ponzetto and Strube (2006, 2007), Poesio et al. (2007)), YAGO (e.g., Bryl et al. (2010), Rahman and Ng (2011), Uryupina et al. (2011)), and Freebase (e.g., Lee et al. (2011)). However, the resulting extractions are primarily IS-A relations (e.g., Barack Obama IS-A U. S. president), which would not be useful for resolving definite pronouns. 2 Dataset Creation We asked 30 undergraduate students who are not affiliated with this research to compose sentence pairs (i.e., twin sentences) that conform to the constraints specified in the introduction. Each student was also asked to annotate the candidate antecedents, the target pronoun, and the correct a</context>
</contexts>
<marker>Poesio, Day, Artstein, Duncan, Eidelman, Giuliano, Hall, Hitzeman, Jern, Kabadjov, Keong, Mann, Moschitti, Ponzetto, Smith, Steinberger, Strube, Su, Versley, Yang, Wick, 2007</marker>
<rawString>Massimo Poesio, David Day, Ron Artstein, Jason Duncan, Vladimir Eidelman, Claudio Giuliano, Rob Hall, Janet Hitzeman, Alan Jern, Mijail Kabadjov, Stanley Yong Wai Keong, Gideon Mann, Alessandro Moschitti, Simone Ponzetto, Jason Smith, Josef Steinberger, Michael Strube, Jian Su, Yannick Versley, Xiaofeng Yang, and Michael Wick. 2007. ELERFED: Final report of the research group on Exploiting Lexical and Encyclopedic Resources For Entity Disambiguation. Technical report, Summer Workshop on Language Engineering, Center for Language and Speech Processing, Johns Hopkins University, Baltimore, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simone Paolo Ponzetto</author>
<author>Michael Strube</author>
</authors>
<title>Exploiting semantic role labeling, WordNet and Wikipedia for coreference resolution.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference and Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>192--199</pages>
<contexts>
<context position="9810" citStr="Ponzetto and Strube (2006" startWordPosition="1562" endWordPosition="1565">, Poesio et al. (2004)) and anaphors referring to abstract entities, such as those realized by verb phrases in dialogs (e.g., Byron (2002), Strube and M¨uller (2003), M¨uller (2007)). Nevertheless, to our knowledge, there has been little work that specifically targets difficult pronouns. Given the complexity of our task, we investigate a variety of sophisticated knowledge sources for resolving difficult pronouns, and combine them via a machine learning approach. Note that there has been a recent surge of interest in extracting world knowledge from online encyclopedias such as Wikipedia (e.g., Ponzetto and Strube (2006, 2007), Poesio et al. (2007)), YAGO (e.g., Bryl et al. (2010), Rahman and Ng (2011), Uryupina et al. (2011)), and Freebase (e.g., Lee et al. (2011)). However, the resulting extractions are primarily IS-A relations (e.g., Barack Obama IS-A U. S. president), which would not be useful for resolving definite pronouns. 2 Dataset Creation We asked 30 undergraduate students who are not affiliated with this research to compose sentence pairs (i.e., twin sentences) that conform to the constraints specified in the introduction. Each student was also asked to annotate the candidate antecedents, the targ</context>
</contexts>
<marker>Ponzetto, Strube, 2006</marker>
<rawString>Simone Paolo Ponzetto and Michael Strube. 2006. Exploiting semantic role labeling, WordNet and Wikipedia for coreference resolution. In Proceedings of the Human Language Technology Conference and Conference of the North American Chapter of the Association for Computational Linguistics, pages 192– 199.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simone Paolo Ponzetto</author>
<author>Michael Strube</author>
</authors>
<title>Knowledge derived from Wikipedia for computing semantic relatedness.</title>
<date>2007</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>30--181</pages>
<marker>Ponzetto, Strube, 2007</marker>
<rawString>Simone Paolo Ponzetto and Michael Strube. 2007. Knowledge derived from Wikipedia for computing semantic relatedness. Journal of Artificial Intelligence Research, 30:181–212.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Pradhan</author>
<author>Lance Ramshaw</author>
<author>Mitchell Marcus</author>
<author>Martha Palmer</author>
<author>Ralph Weischedel</author>
<author>Nianwen Xue</author>
</authors>
<title>CoNLL-2011 Shared Task: Modeling unrestricted coreference in OntoNotes.</title>
<date>2011</date>
<booktitle>In Proceedings of the Fifteenth Conference on Computational Natural Language Learning: Shared Task,</booktitle>
<pages>1--27</pages>
<contexts>
<context position="37095" citStr="Pradhan et al., 2011" startWordPosition="6180" endWordPosition="6183"> guesses the antecedent for the 10Pairing an adjective A in one clause with a noun N in another clause may mislead the learner into thinking that N is modified by A, and hence we do not create such pairs. 11If C1, C2, and A are not modified by adjectives, no adjective-based features will be created. target pronoun in each sentence. Since there are two candidate antecedents per sentence, the Random baseline should achieve an accuracy of 50%. The Stanford resolver. Our second baseline is the Stanford resolver (Lee et al., 2011), which achieves the best performance in the CoNLL 2011 shared task (Pradhan et al., 2011). As a rule-based resolver, it does not exploit any coreference-annotated data. Recall from Section 3 that our system assumes as input not only a sentence containing a target pronoun but also the two candidate antecedents. To ensure a fair comparison, the same input is provided to this and other baselines. Hence, if the Stanford resolver decides to resolve the target pronoun, it will resolve it to one of the two candidate antecedents. However, if it does not have enough confidence about resolving it, it will leave it unresolved. Its performance on the test set is shown in the “Unadjusted Score</context>
</contexts>
<marker>Pradhan, Ramshaw, Marcus, Palmer, Weischedel, Xue, 2011</marker>
<rawString>Sameer Pradhan, Lance Ramshaw, Mitchell Marcus, Martha Palmer, Ralph Weischedel, and Nianwen Xue. 2011. CoNLL-2011 Shared Task: Modeling unrestricted coreference in OntoNotes. In Proceedings of the Fifteenth Conference on Computational Natural Language Learning: Shared Task, pages 1–27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Long Qiu</author>
<author>Min-Yen Kan</author>
<author>Tat-Seng Chua</author>
</authors>
<title>A public reference implementation of the RAP anaphora resolution algorithm.</title>
<date>2004</date>
<booktitle>In Proceedings of the 4th International Conference on Language Resources and Evaluation,</booktitle>
<pages>291--294</pages>
<contexts>
<context position="5573" citStr="Qiu et al., 2004" startWordPosition="893" endWordPosition="896">icult pronouns in them are lexically identical but have different antecedents. The presence of twins implies that syntactic salience, a commonly-used heuristic in pronoun resolution that prefers the selection of syntactically salient candidate antecedents, may no longer be useful, since the candidate in the subject position is not more likely to be the correct antecedent than the other candidates. To enable the reader to get a sense of how hard it is to resolve difficult pronouns, Table 1 shows sample twin sentences from our dataset. Note that state-ofthe-art pronoun resolvers (e.g., JavaRAP (Qiu et al., 2004), GuiTaR (Poesio and Kabadjov, 2004), as well as those designed by Mitkov (2002) and Charniak and Elsner (2009)) and coreference resolvers (e.g., BART (Versley et al., 2008), CherryPicker (Rahman and Ng, 2009), Reconcile (Stoyanov et al., 2010), the Stanford resolver (Raghunathan et al., 2010; Lee et al., 2011)) cannot accurately resolve the difficult pronouns in these structurally simple sentences, as they do not have the mechanism to capture the fine distinctions between twin sentences. In other words, when given these sentences, the best that the existing resolvers can do to resolve the pro</context>
</contexts>
<marker>Qiu, Kan, Chua, 2004</marker>
<rawString>Long Qiu, Min-Yen Kan, and Tat-Seng Chua. 2004. A public reference implementation of the RAP anaphora resolution algorithm. In Proceedings of the 4th International Conference on Language Resources and Evaluation, pages 291–294.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karthik Raghunathan</author>
<author>Heeyoung Lee</author>
<author>Sudarshan Rangarajan</author>
<author>Nate Chambers</author>
<author>Mihai Surdeanu</author>
<author>Dan Jurafsky</author>
<author>Christopher Manning</author>
</authors>
<title>A multi-pass sieve for coreference resolution.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>492--501</pages>
<contexts>
<context position="5866" citStr="Raghunathan et al., 2010" startWordPosition="937" endWordPosition="940">e the candidate in the subject position is not more likely to be the correct antecedent than the other candidates. To enable the reader to get a sense of how hard it is to resolve difficult pronouns, Table 1 shows sample twin sentences from our dataset. Note that state-ofthe-art pronoun resolvers (e.g., JavaRAP (Qiu et al., 2004), GuiTaR (Poesio and Kabadjov, 2004), as well as those designed by Mitkov (2002) and Charniak and Elsner (2009)) and coreference resolvers (e.g., BART (Versley et al., 2008), CherryPicker (Rahman and Ng, 2009), Reconcile (Stoyanov et al., 2010), the Stanford resolver (Raghunathan et al., 2010; Lee et al., 2011)) cannot accurately resolve the difficult pronouns in these structurally simple sentences, as they do not have the mechanism to capture the fine distinctions between twin sentences. In other words, when given these sentences, the best that the existing resolvers can do to resolve the pronouns is guessing. This could be surprising to a non-coreference researcher, but it is indeed the state of the art. A natural question is: why do existing resolvers not attempt to handle difficult pronouns? One reason could be that these difficult pronouns do not appear frequently in standard</context>
</contexts>
<marker>Raghunathan, Lee, Rangarajan, Chambers, Surdeanu, Jurafsky, Manning, 2010</marker>
<rawString>Karthik Raghunathan, Heeyoung Lee, Sudarshan Rangarajan, Nate Chambers, Mihai Surdeanu, Dan Jurafsky, and Christopher Manning. 2010. A multi-pass sieve for coreference resolution. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 492–501.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Altaf Rahman</author>
<author>Vincent Ng</author>
</authors>
<title>Supervised models for coreference resolution.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>968--977</pages>
<contexts>
<context position="5782" citStr="Rahman and Ng, 2009" startWordPosition="925" endWordPosition="928">on of syntactically salient candidate antecedents, may no longer be useful, since the candidate in the subject position is not more likely to be the correct antecedent than the other candidates. To enable the reader to get a sense of how hard it is to resolve difficult pronouns, Table 1 shows sample twin sentences from our dataset. Note that state-ofthe-art pronoun resolvers (e.g., JavaRAP (Qiu et al., 2004), GuiTaR (Poesio and Kabadjov, 2004), as well as those designed by Mitkov (2002) and Charniak and Elsner (2009)) and coreference resolvers (e.g., BART (Versley et al., 2008), CherryPicker (Rahman and Ng, 2009), Reconcile (Stoyanov et al., 2010), the Stanford resolver (Raghunathan et al., 2010; Lee et al., 2011)) cannot accurately resolve the difficult pronouns in these structurally simple sentences, as they do not have the mechanism to capture the fine distinctions between twin sentences. In other words, when given these sentences, the best that the existing resolvers can do to resolve the pronouns is guessing. This could be surprising to a non-coreference researcher, but it is indeed the state of the art. A natural question is: why do existing resolvers not attempt to handle difficult pronouns? On</context>
<context position="38990" citStr="Rahman and Ng (2009)" startWordPosition="6502" endWordPosition="6505">them will be resolved correctly. This adjusted score is shown in the “Adjusted Scores” column in row 1 of Table 3. As we can see, Stanford achieves an accuracy of 55.1%, which is 5.1 points higher than that of Random. The Baseline Ranker. To understand whether the somewhat unsatisfactory Stanford results can be attributed to its inability to exploit the training data, we employ as our third baseline a mention ranker that is trained in the same way as our system (see Section 3), except that it employs 39 commonlyused linguistic features for learning-based coreference resolution (see Table 1 of Rahman and Ng (2009) for a description of these features). Hence, the performance difference between this Baseline Ranker and our system can be attributed entirely 785 Unadjusted Scores Adjusted Scores Coreference System Correct Wrong No Decision Correct Wrong No Decision Stanford 40.07% 29.79% 30.14% 55.14% 44.86% 0.00% Baseline Ranker 47.70% 47.16% 5.14% 50.27% 49.73% 0.00% Stanford+Baseline Ranker 53.49% 43.12% 3.39% 55.19% 44.77% 0.00% Our system 73.05% 26.95% 0.00% 73.05% 26.95% 0.00% Table 3: Results of the Stanford resolver, the Baseline Ranker, the Combined resolver, and our system. 1 2 3 4 to the differe</context>
</contexts>
<marker>Rahman, Ng, 2009</marker>
<rawString>Altaf Rahman and Vincent Ng. 2009. Supervised models for coreference resolution. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 968–977.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Altaf Rahman</author>
<author>Vincent Ng</author>
</authors>
<title>Coreference resolution with world knowledge.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>814--824</pages>
<contexts>
<context position="9894" citStr="Rahman and Ng (2011)" startWordPosition="1577" endWordPosition="1580">ed by verb phrases in dialogs (e.g., Byron (2002), Strube and M¨uller (2003), M¨uller (2007)). Nevertheless, to our knowledge, there has been little work that specifically targets difficult pronouns. Given the complexity of our task, we investigate a variety of sophisticated knowledge sources for resolving difficult pronouns, and combine them via a machine learning approach. Note that there has been a recent surge of interest in extracting world knowledge from online encyclopedias such as Wikipedia (e.g., Ponzetto and Strube (2006, 2007), Poesio et al. (2007)), YAGO (e.g., Bryl et al. (2010), Rahman and Ng (2011), Uryupina et al. (2011)), and Freebase (e.g., Lee et al. (2011)). However, the resulting extractions are primarily IS-A relations (e.g., Barack Obama IS-A U. S. president), which would not be useful for resolving definite pronouns. 2 Dataset Creation We asked 30 undergraduate students who are not affiliated with this research to compose sentence pairs (i.e., twin sentences) that conform to the constraints specified in the introduction. Each student was also asked to annotate the candidate antecedents, the target pronoun, and the correct antecedent for each sentence she composed. Note that a s</context>
</contexts>
<marker>Rahman, Ng, 2011</marker>
<rawString>Altaf Rahman and Vincent Ng. 2011. Coreference resolution with world knowledge. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 814–824.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger C Schank</author>
<author>Robert P Abelson</author>
</authors>
<title>Scripts, Plans, Goals, and Understanding. Lawrence Erlbaum.</title>
<date>1977</date>
<contexts>
<context position="13868" citStr="Schank and Abelson, 1977" startWordPosition="2229" endWordPosition="2232">t pronouns from eight components, as described below. To enable the reader to keep track of these features more easily, we summarize them in Table 2. 4.1 Narrative Chains Consider the following sentence: (2) Ed punished Tim because he tried to escape. Humans resolve he to Tim by exploiting the world knowledge that someone who tried to escape is bad and therefore should be punished. Such kind of knowledge can be extracted from narrative chains. Narrative chains are partially ordered sets of events centered around a common protagonist, aiming to encode the kind of knowledge provided by scripts (Schank and Abelson, 1977). While scripts are hand-written, narrative chains can be learned from unannotated text. Below is a chain learned by Chambers and Jurafsky (2008): borrow-s invest-s spend-s pay-s raise-s lend-s As we can see, a narrative chain is composed of a sequence of events (verbs) together with the roles of the protagonist. Here, “s” denotes the subject role, even though a chain can contain a mix of “s” and “o” (the object role). From this chain, we know that the person who borrows something (probably money) may invest, spend, pay, or lend it. We employ narrative chains to heuristically predict the antec</context>
</contexts>
<marker>Schank, Abelson, 1977</marker>
<rawString>Roger C. Schank and Robert P. Abelson. 1977. Scripts, Plans, Goals, and Understanding. Lawrence Erlbaum.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Veselin Stoyanov</author>
<author>Claire Cardie</author>
<author>Nathan Gilbert</author>
<author>Ellen Riloff</author>
<author>David Buttler</author>
<author>David Hysom</author>
</authors>
<title>RECONCILE: A coreference resolution research platform.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACL 2010 Conference Short Papers,</booktitle>
<pages>156--161</pages>
<contexts>
<context position="5817" citStr="Stoyanov et al., 2010" startWordPosition="930" endWordPosition="933">date antecedents, may no longer be useful, since the candidate in the subject position is not more likely to be the correct antecedent than the other candidates. To enable the reader to get a sense of how hard it is to resolve difficult pronouns, Table 1 shows sample twin sentences from our dataset. Note that state-ofthe-art pronoun resolvers (e.g., JavaRAP (Qiu et al., 2004), GuiTaR (Poesio and Kabadjov, 2004), as well as those designed by Mitkov (2002) and Charniak and Elsner (2009)) and coreference resolvers (e.g., BART (Versley et al., 2008), CherryPicker (Rahman and Ng, 2009), Reconcile (Stoyanov et al., 2010), the Stanford resolver (Raghunathan et al., 2010; Lee et al., 2011)) cannot accurately resolve the difficult pronouns in these structurally simple sentences, as they do not have the mechanism to capture the fine distinctions between twin sentences. In other words, when given these sentences, the best that the existing resolvers can do to resolve the pronouns is guessing. This could be surprising to a non-coreference researcher, but it is indeed the state of the art. A natural question is: why do existing resolvers not attempt to handle difficult pronouns? One reason could be that these diffic</context>
</contexts>
<marker>Stoyanov, Cardie, Gilbert, Riloff, Buttler, Hysom, 2010</marker>
<rawString>Veselin Stoyanov, Claire Cardie, Nathan Gilbert, Ellen Riloff, David Buttler, and David Hysom. 2010. RECONCILE: A coreference resolution research platform. In Proceedings of the ACL 2010 Conference Short Papers, pages 156–161.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Strube</author>
<author>Christoph M¨uller</author>
</authors>
<title>A machine learning approach to pronoun resolution in spoken dialogue.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>168--175</pages>
<marker>Strube, M¨uller, 2003</marker>
<rawString>Michael Strube and Christoph M¨uller. 2003. A machine learning approach to pronoun resolution in spoken dialogue. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 168–175.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan M Turing</author>
</authors>
<title>Computing machinery and intelligence.</title>
<date>1950</date>
<journal>Mind,</journal>
<pages>59--433</pages>
<marker>Turing, 1950</marker>
<rawString>Alan M. Turing. 1950. Computing machinery and intelligence. Mind, 59:433–460.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olga Uryupina</author>
<author>Massimo Poesio</author>
<author>Claudio Giuliano</author>
<author>Kateryna Tymoshenko</author>
</authors>
<title>Disambiguation and filtering methods in using Web knowledge for coreference resolution.</title>
<date>2011</date>
<booktitle>In Proceedings of the 24th International Florida Artificial Intelligence Research Society Conference,</booktitle>
<pages>317--322</pages>
<contexts>
<context position="9918" citStr="Uryupina et al. (2011)" startWordPosition="1581" endWordPosition="1584">dialogs (e.g., Byron (2002), Strube and M¨uller (2003), M¨uller (2007)). Nevertheless, to our knowledge, there has been little work that specifically targets difficult pronouns. Given the complexity of our task, we investigate a variety of sophisticated knowledge sources for resolving difficult pronouns, and combine them via a machine learning approach. Note that there has been a recent surge of interest in extracting world knowledge from online encyclopedias such as Wikipedia (e.g., Ponzetto and Strube (2006, 2007), Poesio et al. (2007)), YAGO (e.g., Bryl et al. (2010), Rahman and Ng (2011), Uryupina et al. (2011)), and Freebase (e.g., Lee et al. (2011)). However, the resulting extractions are primarily IS-A relations (e.g., Barack Obama IS-A U. S. president), which would not be useful for resolving definite pronouns. 2 Dataset Creation We asked 30 undergraduate students who are not affiliated with this research to compose sentence pairs (i.e., twin sentences) that conform to the constraints specified in the introduction. Each student was also asked to annotate the candidate antecedents, the target pronoun, and the correct antecedent for each sentence she composed. Note that a sentence may contain mult</context>
</contexts>
<marker>Uryupina, Poesio, Giuliano, Tymoshenko, 2011</marker>
<rawString>Olga Uryupina, Massimo Poesio, Claudio Giuliano, and Kateryna Tymoshenko. 2011. Disambiguation and filtering methods in using Web knowledge for coreference resolution. In Proceedings of the 24th International Florida Artificial Intelligence Research Society Conference, pages 317–322.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Yannick Versley</author>
</authors>
<title>Simone Paolo Ponzetto, Massimo Poesio,</title>
<location>Vladimir Eidelman, Alan Jern, Jason Smith,</location>
<marker>Versley, </marker>
<rawString>Yannick Versley, Simone Paolo Ponzetto, Massimo Poesio, Vladimir Eidelman, Alan Jern, Jason Smith,</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaofeng Yang</author>
<author>Alessandro Moschitti</author>
</authors>
<title>BART: A modular toolkit for coreference resolution.</title>
<date>2008</date>
<booktitle>In Proceedings of the ACL-08: HLT Demo Session,</booktitle>
<pages>9--12</pages>
<marker>Yang, Moschitti, 2008</marker>
<rawString>Xiaofeng Yang, and Alessandro Moschitti. 2008. BART: A modular toolkit for coreference resolution. In Proceedings of the ACL-08: HLT Demo Session, pages 9–12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Paul Hoffmann</author>
<author>Swapna Somasundaran</author>
<author>Jason Kessler</author>
<author>Janyce Wiebe</author>
<author>Yejin Choi</author>
<author>Claire Cardie</author>
<author>Ellen Riloff</author>
<author>Siddharth Patwardhan</author>
</authors>
<title>Opinionfinder: A system for subjectivity analysis.</title>
<date>2005</date>
<booktitle>In Proceedings ofHLT/EMNLP 2005 Interactive Demonstrations,</booktitle>
<pages>34--35</pages>
<contexts>
<context position="28161" citStr="Wilson et al., 2005" startWordPosition="4674" endWordPosition="4677">and HPOL2(i2)=positive-negative. To compute HPOL3 for a given instance, we simply take its HPOL2 value and append the connective to it. Using (5a) as an example, HPOL3(i1)=positive-positive-even-though and HPOL3(i1)=positive-negative-even-though. 4.5 Machine-Learned Polarity In the previous subsection, we compute the polarity of a word by updating its prior polarity heuristically with contextual information. We hypothesized that polarity could be computed more accurately by employing a sentiment analyzer that can capture richer contextual information. For this reason, we employ OpinionFinder (Wilson et al., 2005a), which has a pre-trained classifier for annotating the phrases in a sentence with their contextual polarity values. Given a sentence and the polarity values of the phrases annotated by OpinionFinder, we determine the rank values of the pronoun and the two candidate antecedents by mapping them to the polarized phrases using the dependency relations provided by the Stanford dependency parser. We create three binary features, LPOL1, LPOL2, and LPOL3, whose values are computed in the same way as HPOL1, HPOL2, and HPOL3, respectively, except that the computation here is based on the machine-lear</context>
</contexts>
<marker>Wilson, Hoffmann, Somasundaran, Kessler, Wiebe, Choi, Cardie, Riloff, Patwardhan, 2005</marker>
<rawString>Theresa Wilson, Paul Hoffmann, Swapna Somasundaran, Jason Kessler, Janyce Wiebe, Yejin Choi, Claire Cardie, Ellen Riloff, and Siddharth Patwardhan. 2005a. Opinionfinder: A system for subjectivity analysis. In Proceedings ofHLT/EMNLP 2005 Interactive Demonstrations, pages 34–35.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce M Wiebe</author>
<author>Paul Hoffmann</author>
</authors>
<title>Recognizing contextual polarity in phraselevel sentiment analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of the Joint Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>347--354</pages>
<contexts>
<context position="28161" citStr="Wilson et al., 2005" startWordPosition="4674" endWordPosition="4677">and HPOL2(i2)=positive-negative. To compute HPOL3 for a given instance, we simply take its HPOL2 value and append the connective to it. Using (5a) as an example, HPOL3(i1)=positive-positive-even-though and HPOL3(i1)=positive-negative-even-though. 4.5 Machine-Learned Polarity In the previous subsection, we compute the polarity of a word by updating its prior polarity heuristically with contextual information. We hypothesized that polarity could be computed more accurately by employing a sentiment analyzer that can capture richer contextual information. For this reason, we employ OpinionFinder (Wilson et al., 2005a), which has a pre-trained classifier for annotating the phrases in a sentence with their contextual polarity values. Given a sentence and the polarity values of the phrases annotated by OpinionFinder, we determine the rank values of the pronoun and the two candidate antecedents by mapping them to the polarized phrases using the dependency relations provided by the Stanford dependency parser. We create three binary features, LPOL1, LPOL2, and LPOL3, whose values are computed in the same way as HPOL1, HPOL2, and HPOL3, respectively, except that the computation here is based on the machine-lear</context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2005</marker>
<rawString>Theresa Wilson, Janyce M. Wiebe, and Paul Hoffmann. 2005b. Recognizing contextual polarity in phraselevel sentiment analysis. In Proceedings of the Joint Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages 347–354.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Winograd</author>
</authors>
<title>Understanding Natural Language.</title>
<date>1972</date>
<publisher>Academic Press, Inc.,</publisher>
<location>New York.</location>
<contexts>
<context position="7932" citStr="Winograd, 1972" startWordPosition="1255" endWordPosition="1256"> refers to as the Winograd Schema Challenge1, could serve as a conceptually and practically appealing alternative to the well-known Turing Test (Turing, 1Levesque (2011) defines a Winograd Schema as a small reading comprehension test involving the question of which of the two candidate antecedents for the definite pronoun in a given sentence is its correct antecedent. Levesque names this challenge after Winograd because of his pioneering attempt to use a well-known pair of twin sentences — specifically the first pair in Table 1 — to illustrate the difficulty of natural language understanding (Winograd, 1972). Strictly speaking, we are addressing a relaxed version of the Challenge: while Levesque focuses solely on definite pronouns whose resolution requires background knowledge not expressed in the words of a sentence, we do not impose such a condition on a sentence. 778 1950). The reason should perhaps be clear given the above discussion: this is an easy task for a subject who can “understand” natural language but a challenging task for one who can only make intelligent guesses. Levesque believes that “with a very high probability”, anything that can resolve correctly a series of difficult pronou</context>
</contexts>
<marker>Winograd, 1972</marker>
<rawString>Terry Winograd. 1972. Understanding Natural Language. Academic Press, Inc., New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaofeng Yang</author>
<author>Guodong Zhou</author>
<author>Jian Su</author>
<author>Chew Lim Tan</author>
</authors>
<title>Coreference resolution using competitive learning approach.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting ofthe Association for Computational Linguistics,</booktitle>
<pages>176--183</pages>
<contexts>
<context position="12128" citStr="Yang et al., 2003" startWordPosition="1935" endWordPosition="1938">e are two candidate antecedents per sentence. 3 Machine Learning Framework Since our goal is to determine which of the two candidate antecedents is the correct antecedent for the target pronoun in each sentence, our system assumes as input the sentence, the target pronoun, and the two candidate antecedents. We employ machine learning to combine the features derived from different knowledge sources. Specifically, we employ a ranking-based approach. Ranking-based approaches have been shown to outperform their classification-based counterparts (Denis and Baldridge, 2007, 2008; Iida et al., 2003; Yang et al., 2003). Given a pronoun and two candidate antecedents, we aim to train a ranking model that ranks the two candidates such that the correct antecedent is assigned a higher rank. More formally, given training sentence Sk containing target pronoun Ak, correct antecedent Ck and incorrect antecedent Ik, we create two feature vectors, xCAk and xIAk, where xCAk is generated from Ak and Ck, and xIAk is generated from Ak and Ik. The training set consists of ordered pairs of feature vectors (xCAk, xIAk), and the goal of the training procedure is to acquire a ranker that minimizes the number of violations of p</context>
</contexts>
<marker>Yang, Zhou, Su, Tan, 2003</marker>
<rawString>Xiaofeng Yang, Guodong Zhou, Jian Su, and Chew Lim Tan. 2003. Coreference resolution using competitive learning approach. In Proceedings of the 41st Annual Meeting ofthe Association for Computational Linguistics, pages 176–183.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaofeng Yang</author>
<author>Jian Su</author>
<author>Chew Lim Tan</author>
</authors>
<title>Improving pronoun resolution using statistics-based semantic compatibility information.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>165--172</pages>
<contexts>
<context position="20949" citStr="Yang et al. (2005)" startWordPosition="3441" endWordPosition="3444">0. The role of the threshold x should be obvious: it ensures that a heuristic decision is made only if the difference between the counts for the two queries are sufficiently large, because otherwise there is no reason for us to prefer one candidate antecedent to the other. In all of our experiments, we set x to 20. Note that other researchers have also used lexicosyntactic patterns to generate search queries for bridging anaphora resolution (e.g., Poesio et al. (2004)), other-anaphora resolution (e.g., Modjeska et al. (2003)), and learning selectional preferences for pronoun resolution (e.g., Yang et al. (2005)). However, in each of these three cases, the target relations (e.g., the part-whole relation in the case of bridging anaphora resolution, and the subject-verb and verb-object relations in the case of selectional preferences) are specific enough that they can be effectively captured by specific patterns. For example, 781 to determine whether the wheel is part of the car in bridging anaphora resolution, Poesio et al. employ queries of the form “X of Y”, where X and Y would be replaced with the wheel and the car, respectively. On the other hand, we are not targeting a particular type of relation</context>
</contexts>
<marker>Yang, Su, Tan, 2005</marker>
<rawString>Xiaofeng Yang, Jian Su, and Chew Lim Tan. 2005. Improving pronoun resolution using statistics-based semantic compatibility information. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, pages 165–172.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001207">
<title confidence="0.941276">
Training Factored PCFGs with Expectation Propagation
</title>
<author confidence="0.991235">
David Hall and Dan Klein
</author>
<affiliation confidence="0.997986">
Computer Science Division
University of California, Berkeley
</affiliation>
<email confidence="0.997561">
{dlwh,klein}@cs.berkeley.edu
</email>
<sectionHeader confidence="0.99473" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999425583333333">
PCFGs can grow exponentially as additional
annotations are added to an initially simple
base grammar. We present an approach where
multiple annotations coexist, but in a factored
manner that avoids this combinatorial explo-
sion. Our method works with linguistically-
motivated annotations, induced latent struc-
ture, lexicalization, or any mix of the three.
We use a structured expectation propagation
algorithm that makes use of the factored struc-
ture in two ways. First, by partitioning the fac-
tors, it speeds up parsing exponentially over
the unfactored approach. Second, it minimizes
the redundancy of the factors during training,
improving accuracy over an independent ap-
proach. Using purely latent variable annota-
tions, we can efficiently train and parse with
up to 8 latent bits per symbol, achieving F1
scores up to 88.4 on the Penn Treebank while
using two orders of magnitudes fewer parame-
ters compared to the naive approach. Combin-
ing latent, lexicalized, and unlexicalized anno-
tations, our best parser gets 89.4 F1 on all sen-
tences from section 23 of the Penn Treebank.
</bodyText>
<sectionHeader confidence="0.998882" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99994498">
Many high-performance PCFG parsers take an ini-
tially simple base grammar over treebank labels like
NP and enrich it with deeper syntactic features to
improve accuracy. This broad characterization in-
cludes lexicalized parsers (Collins, 1997), unlexical-
ized parsers (Klein and Manning, 2003), and latent
variable parsers (Matsuzaki et al., 2005). Figures
1(a), 1(b), and 1(c) show small examples of context-
free trees that have been annotated in these ways.
When multi-part annotations are used in the same
grammar, systems have generally multiplied these
annotations together, in the sense that an NP that
was definite, possessive, and VP-dominated would
have a single unstructured PCFG symbol that en-
coded all three facts. In addition, modulo backoff
or smoothing, that unstructured symbol would of-
ten have rewrite parameters entirely distinct from,
say, the indefinite but otherwise similar variant of
the symbol (Klein and Manning, 2003). Therefore,
when designing a grammar, one would have to care-
fully weigh new contextual annotations. Should a
definiteness annotation be included, doubling the
number of NPs in the grammar and perhaps overly
fragmenting statistics? Or should it be excluded,
thereby losing important distinctions? Klein and
Manning (2003) discuss exactly such trade-offs and
omit annotations that were helpful on their own be-
cause they were not worth the combinatorial or sta-
tistical cost when combined with other annotations.
In this paper, we argue for grammars with fac-
tored annotations, that is, grammars with annota-
tions that have structured component parts that are
partially decoupled. Our annotated grammars can
include both latent and explicit annotations, as illus-
trated in Figure 1(d), and we demonstrate that these
factored grammars outperform parsers with unstruc-
tured annotations.
After discussing the factored representation, we
describe a method for parsing with factored anno-
tations, using an approximate inference technique
called expectation propagation (Minka, 2001). Our
algorithm has runtime linear in the number of an-
notation factors in the grammar, improving on the
naive algorithm, which has runtime exponential in
the number of annotations. Our method, the Ex-
pectation Propagation for Inferring Constituency
(EPIC) parser, jointly trains a model over factored
annotations, where each factor naturally leverages
information from other annotation factors and im-
proves on their mistakes.
</bodyText>
<page confidence="0.938496">
1146
</page>
<note confidence="0.966157">
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1146–1156, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics
</note>
<figure confidence="0.9988538">
(a) NP[agenda]
NN[agenda]
The president’s agenda
(c) NP[1]
NP[1] NN[0]
The president’s agenda
NP[’s]
(b) NP[&amp;quot;S]
NP[&amp;quot;NP-Poss-Det]
The president’s
NN[&amp;quot;NP]
agenda
(d) NP[agenda,&amp;quot;S,1]
NP[’s,&amp;quot;NP-Poss-Det,1] NN[agenda,&amp;quot;NP,0]
The president’s agenda
</figure>
<figureCaption confidence="0.997721">
Figure 1: Parse trees using four different annotation schemes: (a) Lexicalized annotation like that in Collins (1997);
(b) Unlexicalized annotation like that in Klein and Manning (2003); (c) Latent annotation like that in Matsuzaki et al.
(2005); and (d) the factored, mixed annotations we argue for in our paper.
</figureCaption>
<bodyText confidence="0.999801083333333">
We demonstrate the empirical effectiveness of our
approach in two ways. First, we efficiently train
a latent-variable grammar with 8 disjoint one-bit
latent annotation factors, with scores as high as
89.7 F1 on length ≤40 sentences from the Penn
Treebank (Marcus et al., 1993). This latent vari-
able parser outscores the best of Petrov and Klein
(2008a)’s comparable parsers while using two or-
ders of magnitude fewer parameters. Second, we
combine our latent variable factors with lexicalized
and unlexicalized annotations, resulting in our best
F1 score of 89.4 on all sentences.
</bodyText>
<sectionHeader confidence="0.998491" genericHeader="introduction">
2 Intuitions
</sectionHeader>
<bodyText confidence="0.999947866666666">
Modern theories of grammar such as HPSG (Pollard
and Sag, 1994) and Minimalism (Chomsky, 1992)
do not ascribe unstructured conjunctions of anno-
tations to phrasal categories. Rather, phrasal cat-
egories are associated with sequences of metadata
that control their function. For instance, an NP
might have annotations to the effect that it is sin-
gular, masculine, and nominative, with perhaps fur-
ther information about its animacy or other aspects
of the head noun. Thus, it is appealing for a gram-
mar to be able to model these (somewhat) orthog-
onal notions, but most models have no mechanism
to encourage this. As a notable exception, Dreyer
and Eisner (2006) tried to capture this kind of insight
by allowing factored annotations to pass unchanged
from parent label to child label, though they were not
able to demonstrate substantial gains in accuracy.
Moreover, there has been to our knowledge no at-
tempt to employ both latent and non-latent annota-
tions at the same time. There is good reason for this:
lexicalized or highly annotated grammars like those
of Collins (1997) or Klein and Manning (2003) have
a very large number of states and an even larger
number of rules. Further annotating these rules with
latent annotations would produce an infeasibly large
grammar. Nevertheless, it is a shame to sacrifice ex-
pert annotation just to get latent annotations. Thus,
it makes sense to combine these annotation methods
in a way that does not lead to an explosion of the
state space or a fragmentation of statistics.
</bodyText>
<sectionHeader confidence="0.720046" genericHeader="method">
3 Parsing with Annotations
</sectionHeader>
<bodyText confidence="0.999535928571429">
Suppose we have a raw (binarized) treebank gram-
mar, with productions of the form A —* B C.
The typical process is to then annotate these rules
with additional information, giving rules of the form
A[x] —* B[y] C[z]. In the case of explicit annota-
tions, an x might include information about the par-
ent category, or a head word, or a combination of
things. In the case of latent annotations, x will be
an integer that may or may not correspond to some
linguistic notion. We are interested in the specific
case where each x is actually factored into M dis-
joint parts: A[x1, x2, ... , xM]. (See Figure 1(d).)
We call each component of x an annotation factor
or an annotation component.
</bodyText>
<page confidence="0.989282">
1147
</page>
<subsectionHeader confidence="0.999396">
3.1 Annotation Classes
</subsectionHeader>
<bodyText confidence="0.980457804347826">
In this paper, we consider three kinds of annotation
models, representing three of the major traditions in
constituency parsing. Individually, none of our mod-
els are state-of-the-art, instead achieving F1 scores
in the mid-80’s on the Penn Treebank.
The first model is a relatively simple lexicalized
parser. We are not aware of a prior discriminative
lexicalized constituency parser, and it is quite dif-
ferent from the generative models of Collins (1997).
Broadly, it considers features over a binary rule an-
notated with head words: A[h] —* B[h] C[d] and
A[h] —* B[d] C[h], focusing on monolexical rule
features and bilexical dependency features. It is our
best individual model, scoring 87.3 F1 on the devel-
opment set.
The second is similar to the unlexicalized model
of Klein and Manning (2003). This parser starts
from a grammar with labels annotated with sibling
and parent information, and then adds specific an-
notations, such as whether an NP is possessive or
whether a symbol rewrites as a unary. This parser
gets 86.3, tying the original generative version of
Klein and Manning (2003).
Finally, we use a straightforward discriminative
latent variable model much like that of Petrov and
Klein (2008a). Here, each symbol is given a la-
tent annotation, referred to as a substate. Typically,
these substates correlate at least loosely with linguis-
tic phenomena. For instance, NP-1 might be associ-
ated with possessive NPs, while NP-3 might be for
adjuncts. Often, these latent integers are considered
as bit strings, with each bit indicating one latent an-
notation. Prior work in this area has considered the
effect of splitting and merging these states (Petrov et
al., 2006; Petrov and Klein, 2007), as well as “mul-
tiscale” grammars (Petrov and Klein, 2008b). With
two states (or one bit of annotation), our version of
this parser gets 81.7 F1, edging out the compara-
ble parser of Petrov and Klein (2008a). On the other
hand, our parser gets 83.2 with four states (two bits),
short of the performance of prior work.1
1Much of the difference stems from the different binariza-
tion scheme we employ. We use head-outward binarization,
rather than the left-branching binarization they employed. This
change was to enable integrating lexicalization with our other
models.
</bodyText>
<subsectionHeader confidence="0.99986">
3.2 Model Representation
</subsectionHeader>
<bodyText confidence="0.999848529411765">
We employ a general exponential family representa-
tion of our grammar. This representation is fairly
general, and—in its generic form—by no means
new, save for the focus on annotation components.
Formally, we begin with a parse tree T over base
symbols for some sentence w, and we decorate the
tree with annotations X, giving a parse tree T[X].
We focus on the case when X partitions into disjoint
components X = [X1, X2,..., XM]. These com-
ponents are decoupled in the sense that, conditioned
on the coarse tree T, each column of the annota-
tion is independent of every other column. How-
ever, they are crucially not independent conditioned
only on the sentence w. This model is represented
schematically in Figure 2(a).
The conditional probability P(T [X]|w, θ) of an
annotated tree given words is:
</bodyText>
<equation confidence="0.9860815">
P(T [X]|w, θ)
H
m fm(T [Xm]; w, θm)
=
� H
mfm(T&apos;[X&apos;m]; w, θm)
T 1
Z(w, θ) rl fm(T [Xm]; w, θm)
</equation>
<bodyText confidence="0.984867190476191">
where the factors fm for each model take the form:
fm(T [Xm]; w, θm) = exp (θT
mϕm(T, Xm, w))
Here, Xm is the annotation associated with a partic-
ular model m. ϕ is a feature function that projects
the raw tree, annotations, and words into a feature
vector. The features ϕ need to decompose into fea-
tures for each factor fm; we do not allow features
that take into account the annotation from two dif-
ferent components.
We further add a pruning filter that assigns zero
weight to any tree with a constituent that a baseline
unannotated grammar finds sufficiently unlikely, and
a weight of one to any other tree. This filter is similar
to that used in Petrov and Klein (2008a) and allows
for much more efficient training and inference.
Because our model is discriminative, training
takes the form of maximizing the probability of the
training trees given the words. This objective is con-
vex for deterministic annotations, but non-convex
for latent annotations. We (locally) optimize the
</bodyText>
<equation confidence="0.703059">
(1)
m
</equation>
<page confidence="0.710115">
1148
</page>
<figure confidence="0.989763">
FLEX FLAT fUNL
fUNL
Full product model Approximate model
P(T [X]|w; ✓) q(T Iw)
(a) (b) (c)
</figure>
<figureCaption confidence="0.998332">
Figure 2: Schematic representation of our model, its approximation, and expectation propagation. (a) The full joint
</figureCaption>
<bodyText confidence="0.918195142857143">
distribution consists of a product of three grammars with different annotations, here lexicalized, latent, and unlexi-
calized. This model is described in Section 3.2. (b) The core approximation is an anchored PCFG with one factor
corresponding to each annotation component, described in Section 5.1. (c) Fitting the approximation with expectation
propagation, as described in Section 5.3. At the center is the core approximation. During each step, an “augmented”
distribution q,.. is created by taking one annotation factor from the full grammar and the rest from the approximate
grammar. For instance, in upper left hand corner the full fLEX is substituted for �fLEX. This new augmented distribution
is projected back to the core approximation. This process is repeated for each factor until convergence.
</bodyText>
<equation confidence="0.951395777777778">
(non-convex) log conditional likelihood of the ob-
served training data (T(d), w(d)):
log P(T(d)|w(d), 0)
1: (2)
log P(T (d)[X]|w(d), 0)
X
Using standard results, the derivative takes the form:
V�(0) = 1: E[(P(T, X, w)|T(d), w(d)]
_1:
</equation>
<bodyText confidence="0.952574894736842">
d E[(P(T, X, w)|w(d)] (3)
d
The first half of this derivative can be obtained by the
forward/backward-like computation defined by Mat-
suzaki et al. (2005), while the second half requires
an inside/outside computation (Petrov and Klein,
2008a). The partition function Z(w, 0) is computed
as a byproduct of the latter computation. Finally,
this objective is regularized, using the L2 norm of 0
as a penalty.
We note that we omit from our parser one major
feature class found in other discriminative parsers,
namely those that use features over the words in the
span (Finkel et al., 2008; Petrov and Klein, 2008b).
These features might condition on words on either
side of the split point of a binary rule or take into
account the length of the span. While such features
have proven useful in previous work, they are not the
focus of our current work and so we omit them.
</bodyText>
<sectionHeader confidence="0.951764" genericHeader="method">
4 The Complexity of Annotated
Grammars
</sectionHeader>
<bodyText confidence="0.999975">
Note that the first term of Equation 3—which is
conditioned on the coarse tree T—factors into M
pieces, one for each of the annotation components.
However, the second term does not factor because it
is conditioned on just the words w. Indeed, naively
computing this term requires parsing with the fully
articulated grammar, meaning that inference would
be no more efficient than parsing with non-factored
annotations.
Standard algorithms for parsing run in time
O(G|w|3), where |w |is the length of the sentence,
and G is the size of the grammar, measured in the
number of (binary) rules. Let G0 be the number
of binary rules in the unannotated “base” grammar.
</bodyText>
<equation confidence="0.694310666666667">
�(0) = 1:
d
1: =
</equation>
<bodyText confidence="0.399998">
d
</bodyText>
<page confidence="0.964246">
1149
</page>
<bodyText confidence="0.999982647058824">
Suppose that we have M annotation components.
Each annotation component can have up to A primi-
tive annotations per rule. For instance, a latent vari-
able grammar will have A = 8b where b is the num-
ber of bits of annotation. If we compile all annota-
tion components into unstructured annotations, we
can end up with a total grammar size of O(AmG0),
and so in general parsing time scales exponentially
with the number of annotation components. Thus, if
we use latent annotations and the hierarchical split-
ting approach of Petrov et al. (2006), then the gram-
mar has size O(8SG0), where 5 is the number of
times the grammar was split in two. Therefore, the
size of annotated grammars can reach intractable
levels very quickly, particularly in the case of latent
annotations, where all combinations of annotations
are possible.
Petrov (2010) considered an approach to slowing
this growth down by using a set of M independently
trained parsers Pm, and parsed using the product
of the scores from each parser as the score for the
tree. This approach worked largely because train-
ing was intractable: if the training algorithm could
reach the global optimum, then this approach might
have yielded no gain. However, because the opti-
mization technique is local, the same algorithm pro-
duced multiple grammars.
In what follows, we propose another solution that
exploits the factored structure of our grammar with
expectation propagation. Crucially, we are able to
jointly train and parse with all annotation factors,
minimizing redundancy across the models. While
not exact, we will see that expectation propagation
is indeed effective.
</bodyText>
<sectionHeader confidence="0.987737" genericHeader="method">
5 Factored Inference
</sectionHeader>
<bodyText confidence="0.999630217391304">
The key insight behind the approximate inference
methods we consider here is that the full model is
a product of complex factors that interact in compli-
cated ways, and we will approximate it with a prod-
uct of corresponding simple factors that interact in
simple ways. Since each annotation factor is a rea-
sonable model in both power and complexity on its
own, we can consider them one at a time, replac-
ing all others with their approximations, as shown in
Figure 2(c).
The way we will build these approximations is
with expectation propagation (Minka, 2001). Ex-
pectation propagation (EP) is a general method for
approximate inference that generalizes belief propa-
gation. We describe it here, but we first try to pro-
vide an intuition for how it functions in our system.
We also describe a simplified version of EP, called
assumed density filtering (Boyen and Koller, 1998),
which is somewhat easier to understand and rhetori-
cally convenient. For a more detailed introduction to
EP in general, we direct the reader to either Minka
(2001) or Wainwright and Jordan (2008). Our treat-
ment most resembles the former.
</bodyText>
<subsectionHeader confidence="0.970092">
5.1 Factored Approximations
</subsectionHeader>
<bodyText confidence="0.999770235294118">
Our goal is to build an approximation that takes in-
formation from all components into account. To be-
gin, we note that each of these components captures
different phenomena: an unlexicalized grammar is
good at capturing structural relationships in a parse
tree (e.g. subject noun phrases have different dis-
tributions than object noun phrases), while a lexi-
calized grammar captures preferred attachments for
different verbs. At the same time, each of these com-
ponent grammars can be thought of as a refinement
of the raw unannotated treebank grammar. By itself,
each of these grammars induces a different poste-
rior distribution over unannotated trees for each sen-
tence. If we can approximate each model’s contri-
bution by using only unannotated symbols, we can
define an algorithm that avoids the exponential over-
head of parsing with the full grammar, and instead
works with each factor in turn.
To do so, we define a sentence specific core
approximation over unannotated trees q(T|w) =
lm �fm(T, w). Figure 2(b) illustrates this approx-
imation. Here, q(T) is a product of M structurally
identical factors, one for each of the annotated com-
ponents. We will approximate each model fm by
its corresponding �fm. Thus, there is one color-
coordinated approximate factor for each component
of the model in Figure 2(a).
There are multiple choices for the structure of
these factors, but we focus on anchored PCFGs. An-
chored PCFGs have productions of the form iAj —*
iBk k�j, where i, k, and j are indexes into the sen-
tence. Here, iAj is a symbol representing building
the base symbol A over the span [i, j].
Billott and Lang (1989) introduced anchored
</bodyText>
<page confidence="0.961316">
1150
</page>
<bodyText confidence="0.99998105">
CFGs as “shared forests,” and Matsuzaki et al.
(2005) have previously used these grammars for
finding an approximate one-best tree in a latent vari-
able parser. Note that, even though an anchored
grammar is unannotated, because it is sentence spe-
cific it can represent many complex properties of the
full grammar’s posterior distribution for a given sen-
tence. For example, it might express a preference
for whether a PP token attaches to a particular verb
or to that verb’s object noun phrase in a particular
sentence.
Before continuing, note that a pointwise product
of anchored grammars is still an anchored gram-
mar. The complexity of parsing with a product of
these grammars is therefore no more expensive than
parsing with just one. Indeed, anchoring adds no
inferential cost at all over parsing with an unanno-
tated grammar: the anchored indices i, j, k have to
be computed just to parse the sentence at all. This
property is crucial to EP’s efficiency in our setting.
</bodyText>
<subsectionHeader confidence="0.995906">
5.2 Assumed Density Filtering
</subsectionHeader>
<bodyText confidence="0.999682857142857">
We now describe a simplified version of EP: parsing
with assumed density filtering (Boyen and Koller,
1998). We would like to train a sequence of M mod-
els, where each model is trained with knowledge
of the posterior distribution induced by the previous
models. Much as boosting algorithms (Freund and
Schapire, 1995) work by focusing learning on as-
yet-unexplained data points, this approach will en-
courage each model to improve on earlier models,
albeit in a different formal way.
At a high level, assumed density filtering (ADF)
proceeds as follows. First, we have an initially un-
informative q: it assigns the same probability to all
unpruned trees for a given sentence. Then, we fac-
tor in one of the annotated grammars and parse with
this new augmented grammar. This gives us a new
posterior distribution for this sentence over trees an-
notated with just that annotation component. Then,
we can marginalize out the annotations, giving us a
new q that approximates the annotated grammar as
closely as possible without using any annotations.
Once we have incorporated the current model’s com-
ponent, we move on to the next annotated grammar,
augmenting it with the new q, and repeating. In
this way, information from all grammars is incor-
porated into a final posterior distribution over trees
using only unannotated symbols. The algorithm is
then as follows:
</bodyText>
<listItem confidence="0.9221521">
• Initialize q(T) uniformly.
• For each m in sequence:
1. Create the augmented distribution
qm(T[Xm]) a q(T) · fm(T[Xm]) and
compute inside and outside scores.
( )
2. Minimize DKL qm(T) ||�fm(T)q(T) by
fitting an anchored grammar �fm.
3. Set q(T) = Hm �fm&apos;(T).
m&apos;=1
</listItem>
<bodyText confidence="0.976973771428571">
Step 1 of the inner loop forms an approximate pos-
terior distribution using fm, which is the parsing
model associated with component m, and q, which
is the anchored core approximation to the poste-
rior induced by the first m − 1 models. Then, the
marginals are computed, and the new posterior dis-
tribution is projected to an anchored grammar, cre-
ating fm. More intuitively, we create an anchored
PCFG that makes the approximation “as close as
possible” to the augmented grammar. (We describe
this procedure more precisely in Section 5.4.) Thus,
each term fm is approximated in the context of the
terms that come before it. This contextual approx-
imation is essential: without it, ADF would ap-
proximate the terms independently, meaning that no
information would be shared between the models.
This method would be, in effect, a simple method
for parser combination, not all that dissimilar to the
method proposed by Petrov (2010). Finally, note
that the same inside and outside scores computed in
the loop can be used to compute the expected counts
needed in Equation 3.
Now we consider the runtime complexity of this
algorithm. If the maximum number of annotations
per rule for any factor is A, ADF has complex-
ity O (MAG0 |W|3 ) when using M factors. In
contrast, parsing with the fully annotated grammar
(AMG0|W|3)
would have complexity O . Critically,
for a latent variable parser with M annotation bits,
the exact algorithm takes time exponential in M,
while this approximate algorithm takes time linear
in M.
It is worth pausing to consider what this algo-
rithm does during training. At each step, we have
</bodyText>
<page confidence="0.984099">
1151
</page>
<bodyText confidence="0.999913444444445">
in q an approximation to what the posterior distribu-
tion looks like with the first m — 1 models. In some
places, q will assign high probabilities to spans in the
gold tree, and in some places it will not be so accu-
rate. Bm will be particularly motivated to correct the
latter, because they are less like the gold tree. On the
other hand, Bm will ignore the other “correct” seg-
ments, because q has already sufficiently captured
them.
</bodyText>
<subsectionHeader confidence="0.987396">
5.3 Expectation Propagation
</subsectionHeader>
<bodyText confidence="0.998916952380953">
While this sequential algorithm gives us a way to ef-
ficiently combine many kinds of annotations, it is
not a fully joint algorithm: there is no backward
propagation of information from later models to ear-
lier models. Ideally, no model should be privileged
over any other. To correct that, we use EP, which is
essentially the iterative generalization of ADF.
Intuitively, EP cycles among the models, updat-
ing the approximation for that model in turn so that
it closely resembles the predictions made by fm in
the context of all other approximations, as in Fig-
ure 2(c). Thus, each approximate term �fm is cre-
ated using information from all other �fm,, meaning
that the different annotation factors can still “talk”
to each other. The product of these approximations
q will therefore come to act as an approximation to
the true posterior: it takes into account joint infor-
mation about all annotation components, all within
one tractable anchored grammar.
With that intuition in mind, EP is defined as fol-
lows:
</bodyText>
<listItem confidence="0.9998191">
• Initialize contributions fm to the approximate
posterior q.
• At each step, choose m.
1. Include approximations to all factors other
than m: q\m(T) = Qm,,�m fm,(T).
2. Create the augmented distribution by in-
cluding the actual factor for component m
qm(T[Xm]) a fm(T[Xm])q\m(T)
and compute inside and outside scores.
3. Create a new fm(T) that minimizes
</listItem>
<equation confidence="0.9032438">
� �
DKL qm(T)JJ �fm(T)q\m(T) .
•
Finally, set q(T) a Q �fm(T).
m
</equation>
<bodyText confidence="0.9993959">
Step 2 creates the augmented distribution qm, which
includes fm along with the approximate factors for
all models except the current model. Step 3 creates
a new anchored �fm that has the same marginal dis-
tribution as the true model fm in the context of the
other approximations, just as we did in ADF.
In practice, it is usually better to not recompute
the product of all �fm each time, but instead to main-
tain the full product q(T) a Qm fm and to remove
the appropriate fm by division. This optimization is
analogous to belief propagation, where messages are
removed from beliefs by division, instead of recom-
puting beliefs on the fly by multiplying all messages.
Schematically, the whole process is illustrated in
Figure 2(c). At each step, one piece of the core
approximation is replaced with the corresponding
component from the full model. This augmented
model is then reapproximated by a new core approx-
imation q after updating the corresponding �fm. This
process repeats until convergence.
</bodyText>
<subsectionHeader confidence="0.920789">
5.4 EPIC Parsing
</subsectionHeader>
<bodyText confidence="0.999987692307692">
In our parser, EP is implemented as follows. q
and each of the fm are anchored grammars that as-
sign weights to unannotated rules. The product of
anchored grammars with the annotated factor fm
need not be carried out explicitly. Instead, note
that an anchored grammar is just a function q(A —*
B C, i, k, j) E R+ that returns a score for every an-
chored binary rule. This function can be easily in-
tegrated into the CKY algorithm for a single anno-
tated grammar by simply multiplying in the value
of q whenever computing the score of the respective
production over some span. The modified inside re-
currence takes the form:
</bodyText>
<equation confidence="0.997502428571429">
INSIDE(A[x], i, j)
X= BT�o(A[x] —* B[y] C[z], w)
B,y,C,z
X· INSIDE(B[y], i, k) · INSIDE(C[z], k, j)
i&lt;k&lt;j
· q(A —* B C, i, k, j)
(4)
</equation>
<bodyText confidence="0.9997655">
Thus, parsing with a pointwise product of an an-
chored grammar and an annotated grammar has no
increased combinatorial cost over parsing with just
the annotated grammar.
</bodyText>
<page confidence="0.990288">
1152
</page>
<bodyText confidence="0.991297866666667">
To actually perform the projection in step 3 of EP,
we create an anchored grammar from inside and out-
side probabilities. First, we compute the expected
number of times the rule iAj —* iBk kCj occurs,
and then then we locally normalize for each sym-
bol iAj. This actually creates the new q distribution,
and so we have to divide out q\m This process mini-
mizes KL divergence subject to the local normaliza-
tion constraints.
All in all, this gives an algorithm that takes time
(IMAG0|w|3)
O , where I is the maximum num-
ber of iterations, M is the number of models, and
A is the maximum number of annotations for any
given rule.
</bodyText>
<subsectionHeader confidence="0.959924">
5.5 Other Inference Algorithms
</subsectionHeader>
<bodyText confidence="0.999969976190476">
To our knowledge, expectation propagation has been
used only once in the NLP community; Daum´e III
and Marcu (2006) employed an unstructured ver-
sion in a Bayesian model of extractive summariza-
tion. Therefore, it is worth describing how EP dif-
fers from more familiar techniques.
EP can be thought of as a more flexible gen-
eralization of belief propagation, which has been
used several times in NLP (Smith and Eisner, 2008;
Niehues and Vogel, 2008; Cromi`eres and Kurohashi,
2009; Burkett and Klein, 2012). In particular, EP al-
lows for the arbitrary choice of messages (the �fm),
meaning that we can use structured messages like
anchored PCFGs.
Mean field (Saul and Jordan, 1996) is another ap-
proximate inference technique that allows for struc-
tured approximations (Xing et al., 2003; Burkett et
al., 2010), but here the natural version of mean field
for our model would still be intractable. However,
it is possible to adapt mean field into allowing for
tractable updates that are similar to the ones we pro-
posed. We do not pursue that approach here.
Dual decomposition (Dantzig and Wolfe, 1960;
Komodakis et al., 2007) has recently become pop-
ular in the community (Rush et al., 2010; Koo et
al., 2010). In fact, EP can be seen as a particular
kind of dual decomposition of the log normalization
constant log Z* 0) that is optimized with message
passing rather than (sub-)gradient descent or LP re-
laxations. Indeed, Minka (2001) argues that the EP
objective is more efficiently optimized with message
passing than with gradient updates. This assertion
should be examined for the structured models com-
mon in NLP, but that is beyond the scope of this pa-
per.
Finally, note that EP, like belief propagation but
unlike mean field, is not guaranteed to converge,
though in practice it usually seems to. In our exper-
iments, typically three or four iterations are enough
for almost all sentences to reach convergence, and
we found no loss in cutting off the number of itera-
tions to four.
</bodyText>
<sectionHeader confidence="0.998215" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<bodyText confidence="0.99998175">
In what follows, we describe three experiments.
First, in a small experiment, we examine how effec-
tive the different inference algorithms are for both
training and testing. Second, we scale up our latent
variable model into successively larger products. Fi-
nally, we present a selection of the many possible
model combinations, showing that combining latent
and expert annotation can be quite effective.
</bodyText>
<subsectionHeader confidence="0.99613">
6.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.999759583333333">
For our experiments, we trained and tested on the
Penn Treebank using the standard splits: sections 2-
21 were training, 22 development, and 23 testing.
In preliminary experiments, we report development
set F1 on sentences up to length 40. For our final
test set experiment, we report F1 on sentences from
section 23 up to length 40, as well as all sentences
from that section. Scores reported are computed us-
ing EVALB (Sekine and Collins, 1997). We binarize
trees using Collins’ head rules (Collins, 1997).
Each discriminative parser was trained using the
Adaptive Gradient variant of Stochastic Gradient
Descent (Duchi et al., 2010). Smaller models were
seeded from larger models. That is, before training
a grammar of 5 models with 1 latent bit each, we
started with weights from a parser with 4 factored
bits. Initial experiments suggested this step did not
affect final performance, but greatly decreased to-
tal training time, especially for the latent variable
parsers. For extracting a one-best tree, we use a
version of the Max-Recall algorithm of Goodman
(1996). When using EP or ADF, we initialized
the core approximation q to the uniform distribution
over unpruned trees.
</bodyText>
<page confidence="0.933692">
1153
</page>
<table confidence="0.999458666666667">
Parsing
Training ADF EP Exact Petrov
ADF 84.3 84.5 84.5 82.5
EP 84.1 84.6 84.5 78.7
Exact 83.8 84.5 84.9 81.5
Indep. 82.3 82.1 82.2 82.6
</table>
<tableCaption confidence="0.954138">
Table 1: The effect of algorithm choice for training and
parsing on a product of two 2-state parsers on F1. Petrov
is the product parser of Petrov (2010), and Indep. refers
to independently trained models. For comparison, a four-
state parser achieves a score of 83.2.
</tableCaption>
<bodyText confidence="0.9989588">
When counting parameters, we consider the num-
ber of parameters per binary rule. Hence, a single
four-state latent model would have 64 (= 43) param-
eters per rule, while a product of 5 two-state models
would have just 40 (= 5 · 23).
</bodyText>
<subsectionHeader confidence="0.999955">
6.2 Comparison of Inference Algorithms
</subsectionHeader>
<bodyText confidence="0.9999822">
In our first experiment, we test the relative perfor-
mance of the various approximate inference meth-
ods at both train and test time. In order to include
exact inference, we necessarily need to look at a
smaller scale example for which exact inference is
still feasible. We examined development perfor-
mance for training and inference on a small product
of two parsers, each with two latent states per sym-
bol.
During training, we have several options. We can
use exact training by parsing with the fully articu-
lated product of both grammars, or, we can instead
use EP, ADF, or independent training. At test time,
we can parse using the full product of both gram-
mars, or, we can instead use EP, ADF, or we can use
the method of Petrov (2010) wherein we multiply
the parsers together in an ad hoc fashion.
The results are in Table 1. The best reported score,
unsurprisingly, is for using exact training and pars-
ing, but using EP for training and parsing results in
a relatively small loss of 0.3 F1. ADF, however, suf-
fers a loss of 0.6 F1 over Exact when used for train-
ing and parsing. Otherwise, Exact and EP seem to
perform fairly similarly at parse time for all training
conditions.
In general, there seems to be a gain for using the
same method for training and testing. Each test-
ing method performs at its best when using models
trained with the same method. Moreover, except for
ADF, the converse holds true: the grammars trained
</bodyText>
<figure confidence="0.9733105">
1 2 3 4 5 6 7 8
Number of Models
</figure>
<figureCaption confidence="0.999523666666667">
Figure 3: Development F1 plotted against the number M
of one-bit latent annotation components. The best gram-
mar has 6 one-bit annotations, with 89.7 F1.
</figureCaption>
<bodyText confidence="0.99998275">
with a given parsing method are best decoded using
the same method.
Oddly, using Petrov (2010)’s method does not
seem to work well at all for jointly trained models,
except for ADF. Similarly, joint parsing underper-
forms Petrov (2010)’s method when using indepen-
dently trained models. Likely, the joint parsing al-
gorithms are miscalibrating the redundant informa-
tion present in the two independently-trained mod-
els, while the two jointly-trained components come
to depend on each other. In fact, the F1 scores for
the two separate models of the EP parser are in the
60’s.
As expected, ADF does not perform as well as
EP. Therefore, we exclude it from our subsequent
experiments, focusing exclusively on EP.
</bodyText>
<subsectionHeader confidence="0.997516">
6.3 Latent Variable Experiments
</subsectionHeader>
<bodyText confidence="0.9999755">
Most of the previous work in latent variable parsing
has focused on splitting smaller unstructured anno-
tations into larger unstructured annotations. Here,
we consider training a joint model consisting of a
large number of disjoint one-bit (i.e. two-state) la-
tent variable annotations. Specifically, we consider
the performance of products of up to 8 one-bit anno-
tations.
In Figure 3, we show development F1 as a func-
tion of the number of latent bits. Improvement is
roughly linear up to 3 components. Performance
levels off afterwards, with the top performing sys-
tem scoring 89.7 F1. Nevertheless, these parsers
outperform the comparable parsers of Petrov and
Klein (2008a) (89.3), even though our six-bit parser
has many fewer effective parameters per binary rule:
</bodyText>
<figure confidence="0.9789535">
F1 90
88
86
84
82
80
</figure>
<page confidence="0.973608">
1154
</page>
<table confidence="0.999610875">
Models F1, &lt; 40 F1, All
Lexicalized 87.3 86.5
Unlexicalized 86.3 85.4
3xLatent 88.6 87.6
Lex+Unlex 90.2 89.5
Lex+Lat 90.0 89.4
Unlex+Lat 90.0 89.4
Lex+Unlex+Lat 90.2 89.7
</table>
<tableCaption confidence="0.99581575">
Table 2: Development F1 score for various model com-
binations for sentences less than length 40 and all sen-
tences. 3xLatent refers to a latent annotation model with
3 factored latent bits.
</tableCaption>
<bodyText confidence="0.998887">
48 instead of the 4096 in their best parser. We also
ran our best system on Section 23, where it gets 89.1
and 88.4 on sentences less than length 40 and on all
sentences, respectively. This result compares favor-
ably to the 88.8/88.3 of Petrov and Klein (2008a).
</bodyText>
<subsectionHeader confidence="0.917953">
6.4 Heterogeneous Models
</subsectionHeader>
<bodyText confidence="0.99999775">
We now consider factored models with different
kinds of annotations. Specifically, we tested gram-
mars comprising all subsets of fLexicalized, Unlex-
icalized, Latent}. We used a model with 3 factored
bits as our representative of the latent variable class,
because it was closest in performance to the other
models. Of course, other smaller and larger combi-
nations are possible, but we found this selection to
be representative.
The development results are in Table 2. Unsur-
prisingly, adding more kinds of annotations helps for
the most part, though the combination of all three
components is not much better than a combination
of just the lexicalized and unlexicalized models. In-
deed, our best systems involved combining the lexi-
calized model with some other model. This is proba-
bly because the lexicalized model can represent very
different syntactic relationships than the latent and
unlexicalized models, meaning there is more diver-
sity in the joint model’s capacity when using combi-
nations involving the lexicalized annotations.
Finally, we ran our best system (the fully com-
bined one) on Section 23 of the Penn Treebank. It
scored 90.1/89.4 F1 on length 40 and all sentences
respectively, slightly edging out the 90.0/89.3 F1
of Petrov and Klein (2008a). However, it is not
quite as good at exact match: 37.7/35.3 vs 40.1/37.7.
Note, though, that their parser makes use of span
features, which deliver a gain of +0.3/0.2F1 respec-
tively, while ours does not. We suspect that similar
gains could be had by incorporating these features,
but we leave that for future work.
</bodyText>
<sectionHeader confidence="0.996433" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.9999694">
Factored representations capture a fundamental lin-
guistic insight: grammatical categories are not
monolithic, unanalyzable entities. Instead, they are
composed of numerous facets that together govern
how categories combine into parse trees.
We have developed a new model for grammars
with factored annotations and presented two meth-
ods for parsing with these grammars. Our ex-
periments have demonstrated that our approach
produces higher performance parsers with many
fewer parameters. Moreover, our model works
with both latent and explicit annotations, allowing
us to combine linguistic knowledge with machine
learning. Finally, our source code is available at
http://nlp.cs.berkeley.edu/Software.shtml.
</bodyText>
<sectionHeader confidence="0.997311" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99994775">
We would like to thank Slav Petrov, David Burkett,
Adam Pauls, Greg Durrett and the anonymous re-
viewers for helpful comments. We would also like
to thank Daphne Koller for originally suggesting the
assumed density filtering approach. This work was
partially supported by BBN under DARPA contract
HR0011-12-C-0014, and by an NSF fellowship to
the first author.
</bodyText>
<sectionHeader confidence="0.998772" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997104125">
Sylvie Billott and Bernard Lang. 1989. The structure
of shared forests in ambiguous parsing. In Proceed-
ings of the 27th Annual Meeting of the Association for
Computational Linguistics, pages 143–151, Vancou-
ver, British Columbia, Canada, June.
Xavier Boyen and Daphne Koller. 1998. Tractable in-
ference for complex stochastic processes. In Proceed-
ings of the 14th Conference on Uncertainty in Artificial
Intelligence—UAI 1998, pages 33–42. San Francisco:
Morgan Kaufmann.
David Burkett and Dan Klein. 2012. Fast inference in
phrase extraction models with belief propagation. In
NAACL.
David Burkett, John Blitzer, and Dan Klein. 2010.
Joint parsing and alignment with weakly synchronized
grammars. In NAACL.
</reference>
<page confidence="0.856129">
1155
</page>
<reference confidence="0.999932649484536">
Noam Chomsky. 1992. A minimalist program for lin-
guistic theory, volume 1. MIT Working Papers in Lin-
guistics, MIT, Cambridge Massachusetts.
Michael Collins. 1997. Three generative, lexicalised
models for statistical parsing. In ACL, pages 16–23.
Fabien Cromi`eres and Sadao Kurohashi. 2009. An
alignment algorithm using belief propagation and a
structure-based distortion model. In EACL.
G. B. Dantzig and P. Wolfe. 1960. Decomposition
principle for linear programs. Operations Research,
8:101–111.
Hal Daum´e III and Daniel Marcu. 2006. Bayesian query-
focused summarization. In Proceedings of the Confer-
ence of the Association for Computational Linguistics
(ACL), Sydney, Australia.
Markus Dreyer and Jason Eisner. 2006. Better informed
training of latent syntactic features. In EMNLP, pages
317–326, July.
John Duchi, Elad Hazan, and Yoram Singer. 2010.
Adaptive Subgradient Methods for Online Learning
and Stochastic Optimization. COLT.
Jenny Rose Finkel, Alex Kleeman, and Christopher D.
Manning. 2008. Efficient, feature-based, conditional
random field parsing. In ACL 2008, pages 959–967.
Yoav Freund and Robert E. Schapire. 1995. A decision-
theoretic generalization of on-line learning and an ap-
plication to boosting.
Joshua Goodman. 1996. Parsing algorithms and metrics.
In ACL, pages 177–183.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In ACL, pages 423–430.
Nikos Komodakis, Nikos Paragios, and Georgios Tziri-
tas. 2007. Mrf optimization via dual decomposition:
Message-passing revisited. In ICCV, pages 1–8.
Terry Koo, Alexander M. Rush, Michael Collins, Tommi
Jaakkola, and David Sontag. 2010. Dual decompo-
sition for parsing with non-projective head automata.
In Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing (EMNLP).
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19(2):313–330.
Takuya Matsuzaki, Yusuke Miyao, and Jun’ichi Tsujii.
2005. Probabilistic CFG with latent annotations. In
ACL, pages 75–82, Morristown, NJ, USA.
Thomas P. Minka. 2001. Expectation propagation for
approximate Bayesian inference. In UAI, pages 362–
369.
Jan Niehues and Stephan Vogel. 2008. Discriminative
word alignment via alignment matrix modeling. In
Proceedings of the Third Workshop on Statistical Ma-
chine Translation, pages 18–25, June.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In NAACL-HLT, April.
Slav Petrov and Dan Klein. 2008a. Discriminative log-
linear grammars with latent variables. In NIPS, pages
1153–1160.
Slav Petrov and Dan Klein. 2008b. Sparse multi-scale
grammars for discriminative latent variable parsing. In
EMNLP, pages 867–876, Honolulu, Hawaii, October.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics
and 44th Annual Meeting of the Association for Com-
putational Linguistics, pages 433–440, Sydney, Aus-
tralia, July.
Slav Petrov. 2010. Products of random latent variable
grammars. In Human Language Technologies: The
2010 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 19–27, Los Angeles, California, June.
Carl Pollard and Ivan A. Sag. 1994. Head-Driven Phrase
Structure Grammar. University of Chicago Press,
Chicago.
Alexander M Rush, David Sontag, Michael Collins, and
Tommi Jaakkola. 2010. On dual decomposition and
linear programming relaxations for natural language
processing. In EMNLP, pages 1–11, Cambridge, MA,
October.
Lawrence Saul and Michael Jordan. 1996. Exploit-
ing tractable substructures in intractable networks. In
NIPS 1995.
Satoshi Sekine and Michael J. Collins. 1997. Evalb –
bracket scoring program.
David A. Smith and Jason Eisner. 2008. Dependency
parsing by belief propagation. In EMNLP, pages 145–
156, Honolulu, October.
Martin J Wainwright and Michael I Jordan. 2008.
Graphical Models, Exponential Families, and Varia-
tional Inference. Now Publishers Inc., Hanover, MA,
USA.
Eric P. Xing, Michael I. Jordan, and Stuart J. Russell.
2003. A generalized mean field algorithm for varia-
tional inference in exponential families. In UAI, pages
583–591.
</reference>
<page confidence="0.994429">
1156
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.216028">
<title confidence="0.998584">Training Factored PCFGs with Expectation Propagation</title>
<author confidence="0.561589">Hall</author>
<affiliation confidence="0.9998575">Computer Science University of California,</affiliation>
<abstract confidence="0.994391708333334">PCFGs can grow exponentially as additional annotations are added to an initially simple base grammar. We present an approach where multiple annotations coexist, but in a factored manner that avoids this combinatorial explosion. Our method works with linguisticallymotivated annotations, induced latent structure, lexicalization, or any mix of the three. We use a structured expectation propagation algorithm that makes use of the factored structure in two ways. First, by partitioning the factors, it speeds up parsing exponentially over the unfactored approach. Second, it minimizes the redundancy of the factors during training, improving accuracy over an independent approach. Using purely latent variable annotations, we can efficiently train and parse with up to 8 latent bits per symbol, achieving F1 scores up to 88.4 on the Penn Treebank while using two orders of magnitudes fewer parameters compared to the naive approach. Combining latent, lexicalized, and unlexicalized annotations, our best parser gets 89.4 F1 on all sen-</abstract>
<note confidence="0.460895">tences from section 23 of the Penn Treebank.</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Sylvie Billott</author>
<author>Bernard Lang</author>
</authors>
<title>The structure of shared forests in ambiguous parsing.</title>
<date>1989</date>
<booktitle>In Proceedings of the 27th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>143--151</pages>
<location>Vancouver, British Columbia, Canada,</location>
<contexts>
<context position="18768" citStr="Billott and Lang (1989)" startWordPosition="3040" endWordPosition="3043">. Figure 2(b) illustrates this approximation. Here, q(T) is a product of M structurally identical factors, one for each of the annotated components. We will approximate each model fm by its corresponding �fm. Thus, there is one colorcoordinated approximate factor for each component of the model in Figure 2(a). There are multiple choices for the structure of these factors, but we focus on anchored PCFGs. Anchored PCFGs have productions of the form iAj —* iBk k�j, where i, k, and j are indexes into the sentence. Here, iAj is a symbol representing building the base symbol A over the span [i, j]. Billott and Lang (1989) introduced anchored 1150 CFGs as “shared forests,” and Matsuzaki et al. (2005) have previously used these grammars for finding an approximate one-best tree in a latent variable parser. Note that, even though an anchored grammar is unannotated, because it is sentence specific it can represent many complex properties of the full grammar’s posterior distribution for a given sentence. For example, it might express a preference for whether a PP token attaches to a particular verb or to that verb’s object noun phrase in a particular sentence. Before continuing, note that a pointwise product of anch</context>
</contexts>
<marker>Billott, Lang, 1989</marker>
<rawString>Sylvie Billott and Bernard Lang. 1989. The structure of shared forests in ambiguous parsing. In Proceedings of the 27th Annual Meeting of the Association for Computational Linguistics, pages 143–151, Vancouver, British Columbia, Canada, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Boyen</author>
<author>Daphne Koller</author>
</authors>
<title>Tractable inference for complex stochastic processes.</title>
<date>1998</date>
<booktitle>In Proceedings of the 14th Conference on Uncertainty in Artificial Intelligence—UAI</booktitle>
<pages>33--42</pages>
<publisher>Morgan Kaufmann.</publisher>
<location>San Francisco:</location>
<contexts>
<context position="16882" citStr="Boyen and Koller, 1998" startWordPosition="2725" endWordPosition="2728">act in simple ways. Since each annotation factor is a reasonable model in both power and complexity on its own, we can consider them one at a time, replacing all others with their approximations, as shown in Figure 2(c). The way we will build these approximations is with expectation propagation (Minka, 2001). Expectation propagation (EP) is a general method for approximate inference that generalizes belief propagation. We describe it here, but we first try to provide an intuition for how it functions in our system. We also describe a simplified version of EP, called assumed density filtering (Boyen and Koller, 1998), which is somewhat easier to understand and rhetorically convenient. For a more detailed introduction to EP in general, we direct the reader to either Minka (2001) or Wainwright and Jordan (2008). Our treatment most resembles the former. 5.1 Factored Approximations Our goal is to build an approximation that takes information from all components into account. To begin, we note that each of these components captures different phenomena: an unlexicalized grammar is good at capturing structural relationships in a parse tree (e.g. subject noun phrases have different distributions than object noun </context>
<context position="19903" citStr="Boyen and Koller, 1998" startWordPosition="3228" endWordPosition="3231">hrase in a particular sentence. Before continuing, note that a pointwise product of anchored grammars is still an anchored grammar. The complexity of parsing with a product of these grammars is therefore no more expensive than parsing with just one. Indeed, anchoring adds no inferential cost at all over parsing with an unannotated grammar: the anchored indices i, j, k have to be computed just to parse the sentence at all. This property is crucial to EP’s efficiency in our setting. 5.2 Assumed Density Filtering We now describe a simplified version of EP: parsing with assumed density filtering (Boyen and Koller, 1998). We would like to train a sequence of M models, where each model is trained with knowledge of the posterior distribution induced by the previous models. Much as boosting algorithms (Freund and Schapire, 1995) work by focusing learning on asyet-unexplained data points, this approach will encourage each model to improve on earlier models, albeit in a different formal way. At a high level, assumed density filtering (ADF) proceeds as follows. First, we have an initially uninformative q: it assigns the same probability to all unpruned trees for a given sentence. Then, we factor in one of the annot</context>
</contexts>
<marker>Boyen, Koller, 1998</marker>
<rawString>Xavier Boyen and Daphne Koller. 1998. Tractable inference for complex stochastic processes. In Proceedings of the 14th Conference on Uncertainty in Artificial Intelligence—UAI 1998, pages 33–42. San Francisco: Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Burkett</author>
<author>Dan Klein</author>
</authors>
<title>Fast inference in phrase extraction models with belief propagation.</title>
<date>2012</date>
<booktitle>In NAACL.</booktitle>
<contexts>
<context position="28014" citStr="Burkett and Klein, 2012" startWordPosition="4621" endWordPosition="4624">M is the number of models, and A is the maximum number of annotations for any given rule. 5.5 Other Inference Algorithms To our knowledge, expectation propagation has been used only once in the NLP community; Daum´e III and Marcu (2006) employed an unstructured version in a Bayesian model of extractive summarization. Therefore, it is worth describing how EP differs from more familiar techniques. EP can be thought of as a more flexible generalization of belief propagation, which has been used several times in NLP (Smith and Eisner, 2008; Niehues and Vogel, 2008; Cromi`eres and Kurohashi, 2009; Burkett and Klein, 2012). In particular, EP allows for the arbitrary choice of messages (the �fm), meaning that we can use structured messages like anchored PCFGs. Mean field (Saul and Jordan, 1996) is another approximate inference technique that allows for structured approximations (Xing et al., 2003; Burkett et al., 2010), but here the natural version of mean field for our model would still be intractable. However, it is possible to adapt mean field into allowing for tractable updates that are similar to the ones we proposed. We do not pursue that approach here. Dual decomposition (Dantzig and Wolfe, 1960; Komodaki</context>
</contexts>
<marker>Burkett, Klein, 2012</marker>
<rawString>David Burkett and Dan Klein. 2012. Fast inference in phrase extraction models with belief propagation. In NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Burkett</author>
<author>John Blitzer</author>
<author>Dan Klein</author>
</authors>
<title>Joint parsing and alignment with weakly synchronized grammars.</title>
<date>2010</date>
<booktitle>In NAACL.</booktitle>
<contexts>
<context position="28315" citStr="Burkett et al., 2010" startWordPosition="4670" endWordPosition="4673">summarization. Therefore, it is worth describing how EP differs from more familiar techniques. EP can be thought of as a more flexible generalization of belief propagation, which has been used several times in NLP (Smith and Eisner, 2008; Niehues and Vogel, 2008; Cromi`eres and Kurohashi, 2009; Burkett and Klein, 2012). In particular, EP allows for the arbitrary choice of messages (the �fm), meaning that we can use structured messages like anchored PCFGs. Mean field (Saul and Jordan, 1996) is another approximate inference technique that allows for structured approximations (Xing et al., 2003; Burkett et al., 2010), but here the natural version of mean field for our model would still be intractable. However, it is possible to adapt mean field into allowing for tractable updates that are similar to the ones we proposed. We do not pursue that approach here. Dual decomposition (Dantzig and Wolfe, 1960; Komodakis et al., 2007) has recently become popular in the community (Rush et al., 2010; Koo et al., 2010). In fact, EP can be seen as a particular kind of dual decomposition of the log normalization constant log Z* 0) that is optimized with message passing rather than (sub-)gradient descent or LP relaxation</context>
</contexts>
<marker>Burkett, Blitzer, Klein, 2010</marker>
<rawString>David Burkett, John Blitzer, and Dan Klein. 2010. Joint parsing and alignment with weakly synchronized grammars. In NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noam Chomsky</author>
</authors>
<title>A minimalist program for linguistic theory, volume 1. MIT Working Papers in Linguistics, MIT,</title>
<date>1992</date>
<location>Cambridge Massachusetts.</location>
<contexts>
<context position="5191" citStr="Chomsky, 1992" startWordPosition="770" endWordPosition="771">irst, we efficiently train a latent-variable grammar with 8 disjoint one-bit latent annotation factors, with scores as high as 89.7 F1 on length ≤40 sentences from the Penn Treebank (Marcus et al., 1993). This latent variable parser outscores the best of Petrov and Klein (2008a)’s comparable parsers while using two orders of magnitude fewer parameters. Second, we combine our latent variable factors with lexicalized and unlexicalized annotations, resulting in our best F1 score of 89.4 on all sentences. 2 Intuitions Modern theories of grammar such as HPSG (Pollard and Sag, 1994) and Minimalism (Chomsky, 1992) do not ascribe unstructured conjunctions of annotations to phrasal categories. Rather, phrasal categories are associated with sequences of metadata that control their function. For instance, an NP might have annotations to the effect that it is singular, masculine, and nominative, with perhaps further information about its animacy or other aspects of the head noun. Thus, it is appealing for a grammar to be able to model these (somewhat) orthogonal notions, but most models have no mechanism to encourage this. As a notable exception, Dreyer and Eisner (2006) tried to capture this kind of insigh</context>
</contexts>
<marker>Chomsky, 1992</marker>
<rawString>Noam Chomsky. 1992. A minimalist program for linguistic theory, volume 1. MIT Working Papers in Linguistics, MIT, Cambridge Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Three generative, lexicalised models for statistical parsing. In</title>
<date>1997</date>
<booktitle>ACL,</booktitle>
<pages>16--23</pages>
<contexts>
<context position="1510" citStr="Collins, 1997" startWordPosition="226" endWordPosition="227">s, we can efficiently train and parse with up to 8 latent bits per symbol, achieving F1 scores up to 88.4 on the Penn Treebank while using two orders of magnitudes fewer parameters compared to the naive approach. Combining latent, lexicalized, and unlexicalized annotations, our best parser gets 89.4 F1 on all sentences from section 23 of the Penn Treebank. 1 Introduction Many high-performance PCFG parsers take an initially simple base grammar over treebank labels like NP and enrich it with deeper syntactic features to improve accuracy. This broad characterization includes lexicalized parsers (Collins, 1997), unlexicalized parsers (Klein and Manning, 2003), and latent variable parsers (Matsuzaki et al., 2005). Figures 1(a), 1(b), and 1(c) show small examples of contextfree trees that have been annotated in these ways. When multi-part annotations are used in the same grammar, systems have generally multiplied these annotations together, in the sense that an NP that was definite, possessive, and VP-dominated would have a single unstructured PCFG symbol that encoded all three facts. In addition, modulo backoff or smoothing, that unstructured symbol would often have rewrite parameters entirely distin</context>
<context position="4307" citStr="Collins (1997)" startWordPosition="630" endWordPosition="631">istakes. 1146 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1146–1156, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics (a) NP[agenda] NN[agenda] The president’s agenda (c) NP[1] NP[1] NN[0] The president’s agenda NP[’s] (b) NP[&amp;quot;S] NP[&amp;quot;NP-Poss-Det] The president’s NN[&amp;quot;NP] agenda (d) NP[agenda,&amp;quot;S,1] NP[’s,&amp;quot;NP-Poss-Det,1] NN[agenda,&amp;quot;NP,0] The president’s agenda Figure 1: Parse trees using four different annotation schemes: (a) Lexicalized annotation like that in Collins (1997); (b) Unlexicalized annotation like that in Klein and Manning (2003); (c) Latent annotation like that in Matsuzaki et al. (2005); and (d) the factored, mixed annotations we argue for in our paper. We demonstrate the empirical effectiveness of our approach in two ways. First, we efficiently train a latent-variable grammar with 8 disjoint one-bit latent annotation factors, with scores as high as 89.7 F1 on length ≤40 sentences from the Penn Treebank (Marcus et al., 1993). This latent variable parser outscores the best of Petrov and Klein (2008a)’s comparable parsers while using two orders of mag</context>
<context position="6170" citStr="Collins (1997)" startWordPosition="932" endWordPosition="933">un. Thus, it is appealing for a grammar to be able to model these (somewhat) orthogonal notions, but most models have no mechanism to encourage this. As a notable exception, Dreyer and Eisner (2006) tried to capture this kind of insight by allowing factored annotations to pass unchanged from parent label to child label, though they were not able to demonstrate substantial gains in accuracy. Moreover, there has been to our knowledge no attempt to employ both latent and non-latent annotations at the same time. There is good reason for this: lexicalized or highly annotated grammars like those of Collins (1997) or Klein and Manning (2003) have a very large number of states and an even larger number of rules. Further annotating these rules with latent annotations would produce an infeasibly large grammar. Nevertheless, it is a shame to sacrifice expert annotation just to get latent annotations. Thus, it makes sense to combine these annotation methods in a way that does not lead to an explosion of the state space or a fragmentation of statistics. 3 Parsing with Annotations Suppose we have a raw (binarized) treebank grammar, with productions of the form A —* B C. The typical process is to then annotate</context>
<context position="7805" citStr="Collins (1997)" startWordPosition="1211" endWordPosition="1212">sjoint parts: A[x1, x2, ... , xM]. (See Figure 1(d).) We call each component of x an annotation factor or an annotation component. 1147 3.1 Annotation Classes In this paper, we consider three kinds of annotation models, representing three of the major traditions in constituency parsing. Individually, none of our models are state-of-the-art, instead achieving F1 scores in the mid-80’s on the Penn Treebank. The first model is a relatively simple lexicalized parser. We are not aware of a prior discriminative lexicalized constituency parser, and it is quite different from the generative models of Collins (1997). Broadly, it considers features over a binary rule annotated with head words: A[h] —* B[h] C[d] and A[h] —* B[d] C[h], focusing on monolexical rule features and bilexical dependency features. It is our best individual model, scoring 87.3 F1 on the development set. The second is similar to the unlexicalized model of Klein and Manning (2003). This parser starts from a grammar with labels annotated with sibling and parent information, and then adds specific annotations, such as whether an NP is possessive or whether a symbol rewrites as a unary. This parser gets 86.3, tying the original generati</context>
<context position="30375" citStr="Collins, 1997" startWordPosition="5018" endWordPosition="5019">ally, we present a selection of the many possible model combinations, showing that combining latent and expert annotation can be quite effective. 6.1 Experimental Setup For our experiments, we trained and tested on the Penn Treebank using the standard splits: sections 2- 21 were training, 22 development, and 23 testing. In preliminary experiments, we report development set F1 on sentences up to length 40. For our final test set experiment, we report F1 on sentences from section 23 up to length 40, as well as all sentences from that section. Scores reported are computed using EVALB (Sekine and Collins, 1997). We binarize trees using Collins’ head rules (Collins, 1997). Each discriminative parser was trained using the Adaptive Gradient variant of Stochastic Gradient Descent (Duchi et al., 2010). Smaller models were seeded from larger models. That is, before training a grammar of 5 models with 1 latent bit each, we started with weights from a parser with 4 factored bits. Initial experiments suggested this step did not affect final performance, but greatly decreased total training time, especially for the latent variable parsers. For extracting a one-best tree, we use a version of the Max-Recall alg</context>
</contexts>
<marker>Collins, 1997</marker>
<rawString>Michael Collins. 1997. Three generative, lexicalised models for statistical parsing. In ACL, pages 16–23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabien Cromi`eres</author>
<author>Sadao Kurohashi</author>
</authors>
<title>An alignment algorithm using belief propagation and a structure-based distortion model.</title>
<date>2009</date>
<booktitle>In EACL.</booktitle>
<marker>Cromi`eres, Kurohashi, 2009</marker>
<rawString>Fabien Cromi`eres and Sadao Kurohashi. 2009. An alignment algorithm using belief propagation and a structure-based distortion model. In EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G B Dantzig</author>
<author>P Wolfe</author>
</authors>
<title>Decomposition principle for linear programs.</title>
<date>1960</date>
<journal>Operations Research,</journal>
<pages>8--101</pages>
<contexts>
<context position="28604" citStr="Dantzig and Wolfe, 1960" startWordPosition="4720" endWordPosition="4723">i, 2009; Burkett and Klein, 2012). In particular, EP allows for the arbitrary choice of messages (the �fm), meaning that we can use structured messages like anchored PCFGs. Mean field (Saul and Jordan, 1996) is another approximate inference technique that allows for structured approximations (Xing et al., 2003; Burkett et al., 2010), but here the natural version of mean field for our model would still be intractable. However, it is possible to adapt mean field into allowing for tractable updates that are similar to the ones we proposed. We do not pursue that approach here. Dual decomposition (Dantzig and Wolfe, 1960; Komodakis et al., 2007) has recently become popular in the community (Rush et al., 2010; Koo et al., 2010). In fact, EP can be seen as a particular kind of dual decomposition of the log normalization constant log Z* 0) that is optimized with message passing rather than (sub-)gradient descent or LP relaxations. Indeed, Minka (2001) argues that the EP objective is more efficiently optimized with message passing than with gradient updates. This assertion should be examined for the structured models common in NLP, but that is beyond the scope of this paper. Finally, note that EP, like belief pro</context>
</contexts>
<marker>Dantzig, Wolfe, 1960</marker>
<rawString>G. B. Dantzig and P. Wolfe. 1960. Decomposition principle for linear programs. Operations Research, 8:101–111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
<author>Daniel Marcu</author>
</authors>
<title>Bayesian queryfocused summarization.</title>
<date>2006</date>
<booktitle>In Proceedings of the Conference of the Association for Computational Linguistics (ACL),</booktitle>
<location>Sydney, Australia.</location>
<marker>Daum´e, Marcu, 2006</marker>
<rawString>Hal Daum´e III and Daniel Marcu. 2006. Bayesian queryfocused summarization. In Proceedings of the Conference of the Association for Computational Linguistics (ACL), Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markus Dreyer</author>
<author>Jason Eisner</author>
</authors>
<title>Better informed training of latent syntactic features.</title>
<date>2006</date>
<booktitle>In EMNLP,</booktitle>
<pages>317--326</pages>
<contexts>
<context position="5754" citStr="Dreyer and Eisner (2006)" startWordPosition="861" endWordPosition="864">s HPSG (Pollard and Sag, 1994) and Minimalism (Chomsky, 1992) do not ascribe unstructured conjunctions of annotations to phrasal categories. Rather, phrasal categories are associated with sequences of metadata that control their function. For instance, an NP might have annotations to the effect that it is singular, masculine, and nominative, with perhaps further information about its animacy or other aspects of the head noun. Thus, it is appealing for a grammar to be able to model these (somewhat) orthogonal notions, but most models have no mechanism to encourage this. As a notable exception, Dreyer and Eisner (2006) tried to capture this kind of insight by allowing factored annotations to pass unchanged from parent label to child label, though they were not able to demonstrate substantial gains in accuracy. Moreover, there has been to our knowledge no attempt to employ both latent and non-latent annotations at the same time. There is good reason for this: lexicalized or highly annotated grammars like those of Collins (1997) or Klein and Manning (2003) have a very large number of states and an even larger number of rules. Further annotating these rules with latent annotations would produce an infeasibly l</context>
</contexts>
<marker>Dreyer, Eisner, 2006</marker>
<rawString>Markus Dreyer and Jason Eisner. 2006. Better informed training of latent syntactic features. In EMNLP, pages 317–326, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Duchi</author>
<author>Elad Hazan</author>
<author>Yoram Singer</author>
</authors>
<title>Adaptive Subgradient Methods for Online Learning and Stochastic Optimization.</title>
<date>2010</date>
<publisher>COLT.</publisher>
<contexts>
<context position="30564" citStr="Duchi et al., 2010" startWordPosition="5043" endWordPosition="5046">ents, we trained and tested on the Penn Treebank using the standard splits: sections 2- 21 were training, 22 development, and 23 testing. In preliminary experiments, we report development set F1 on sentences up to length 40. For our final test set experiment, we report F1 on sentences from section 23 up to length 40, as well as all sentences from that section. Scores reported are computed using EVALB (Sekine and Collins, 1997). We binarize trees using Collins’ head rules (Collins, 1997). Each discriminative parser was trained using the Adaptive Gradient variant of Stochastic Gradient Descent (Duchi et al., 2010). Smaller models were seeded from larger models. That is, before training a grammar of 5 models with 1 latent bit each, we started with weights from a parser with 4 factored bits. Initial experiments suggested this step did not affect final performance, but greatly decreased total training time, especially for the latent variable parsers. For extracting a one-best tree, we use a version of the Max-Recall algorithm of Goodman (1996). When using EP or ADF, we initialized the core approximation q to the uniform distribution over unpruned trees. 1153 Parsing Training ADF EP Exact Petrov ADF 84.3 8</context>
</contexts>
<marker>Duchi, Hazan, Singer, 2010</marker>
<rawString>John Duchi, Elad Hazan, and Yoram Singer. 2010. Adaptive Subgradient Methods for Online Learning and Stochastic Optimization. COLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Alex Kleeman</author>
<author>Christopher D Manning</author>
</authors>
<title>Efficient, feature-based, conditional random field parsing.</title>
<date>2008</date>
<booktitle>In ACL</booktitle>
<pages>959--967</pages>
<contexts>
<context position="13359" citStr="Finkel et al., 2008" startWordPosition="2131" endWordPosition="2134"> = 1: E[(P(T, X, w)|T(d), w(d)] _1: d E[(P(T, X, w)|w(d)] (3) d The first half of this derivative can be obtained by the forward/backward-like computation defined by Matsuzaki et al. (2005), while the second half requires an inside/outside computation (Petrov and Klein, 2008a). The partition function Z(w, 0) is computed as a byproduct of the latter computation. Finally, this objective is regularized, using the L2 norm of 0 as a penalty. We note that we omit from our parser one major feature class found in other discriminative parsers, namely those that use features over the words in the span (Finkel et al., 2008; Petrov and Klein, 2008b). These features might condition on words on either side of the split point of a binary rule or take into account the length of the span. While such features have proven useful in previous work, they are not the focus of our current work and so we omit them. 4 The Complexity of Annotated Grammars Note that the first term of Equation 3—which is conditioned on the coarse tree T—factors into M pieces, one for each of the annotation components. However, the second term does not factor because it is conditioned on just the words w. Indeed, naively computing this term requi</context>
</contexts>
<marker>Finkel, Kleeman, Manning, 2008</marker>
<rawString>Jenny Rose Finkel, Alex Kleeman, and Christopher D. Manning. 2008. Efficient, feature-based, conditional random field parsing. In ACL 2008, pages 959–967.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Freund</author>
<author>Robert E Schapire</author>
</authors>
<title>A decisiontheoretic generalization of on-line learning and an application to boosting.</title>
<date>1995</date>
<contexts>
<context position="20112" citStr="Freund and Schapire, 1995" startWordPosition="3263" endWordPosition="3266"> more expensive than parsing with just one. Indeed, anchoring adds no inferential cost at all over parsing with an unannotated grammar: the anchored indices i, j, k have to be computed just to parse the sentence at all. This property is crucial to EP’s efficiency in our setting. 5.2 Assumed Density Filtering We now describe a simplified version of EP: parsing with assumed density filtering (Boyen and Koller, 1998). We would like to train a sequence of M models, where each model is trained with knowledge of the posterior distribution induced by the previous models. Much as boosting algorithms (Freund and Schapire, 1995) work by focusing learning on asyet-unexplained data points, this approach will encourage each model to improve on earlier models, albeit in a different formal way. At a high level, assumed density filtering (ADF) proceeds as follows. First, we have an initially uninformative q: it assigns the same probability to all unpruned trees for a given sentence. Then, we factor in one of the annotated grammars and parse with this new augmented grammar. This gives us a new posterior distribution for this sentence over trees annotated with just that annotation component. Then, we can marginalize out the </context>
</contexts>
<marker>Freund, Schapire, 1995</marker>
<rawString>Yoav Freund and Robert E. Schapire. 1995. A decisiontheoretic generalization of on-line learning and an application to boosting.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua Goodman</author>
</authors>
<title>Parsing algorithms and metrics.</title>
<date>1996</date>
<booktitle>In ACL,</booktitle>
<pages>177--183</pages>
<contexts>
<context position="30999" citStr="Goodman (1996)" startWordPosition="5116" endWordPosition="5117">ize trees using Collins’ head rules (Collins, 1997). Each discriminative parser was trained using the Adaptive Gradient variant of Stochastic Gradient Descent (Duchi et al., 2010). Smaller models were seeded from larger models. That is, before training a grammar of 5 models with 1 latent bit each, we started with weights from a parser with 4 factored bits. Initial experiments suggested this step did not affect final performance, but greatly decreased total training time, especially for the latent variable parsers. For extracting a one-best tree, we use a version of the Max-Recall algorithm of Goodman (1996). When using EP or ADF, we initialized the core approximation q to the uniform distribution over unpruned trees. 1153 Parsing Training ADF EP Exact Petrov ADF 84.3 84.5 84.5 82.5 EP 84.1 84.6 84.5 78.7 Exact 83.8 84.5 84.9 81.5 Indep. 82.3 82.1 82.2 82.6 Table 1: The effect of algorithm choice for training and parsing on a product of two 2-state parsers on F1. Petrov is the product parser of Petrov (2010), and Indep. refers to independently trained models. For comparison, a fourstate parser achieves a score of 83.2. When counting parameters, we consider the number of parameters per binary rule</context>
</contexts>
<marker>Goodman, 1996</marker>
<rawString>Joshua Goodman. 1996. Parsing algorithms and metrics. In ACL, pages 177–183.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In ACL,</booktitle>
<pages>423--430</pages>
<contexts>
<context position="1559" citStr="Klein and Manning, 2003" startWordPosition="231" endWordPosition="234">ith up to 8 latent bits per symbol, achieving F1 scores up to 88.4 on the Penn Treebank while using two orders of magnitudes fewer parameters compared to the naive approach. Combining latent, lexicalized, and unlexicalized annotations, our best parser gets 89.4 F1 on all sentences from section 23 of the Penn Treebank. 1 Introduction Many high-performance PCFG parsers take an initially simple base grammar over treebank labels like NP and enrich it with deeper syntactic features to improve accuracy. This broad characterization includes lexicalized parsers (Collins, 1997), unlexicalized parsers (Klein and Manning, 2003), and latent variable parsers (Matsuzaki et al., 2005). Figures 1(a), 1(b), and 1(c) show small examples of contextfree trees that have been annotated in these ways. When multi-part annotations are used in the same grammar, systems have generally multiplied these annotations together, in the sense that an NP that was definite, possessive, and VP-dominated would have a single unstructured PCFG symbol that encoded all three facts. In addition, modulo backoff or smoothing, that unstructured symbol would often have rewrite parameters entirely distinct from, say, the indefinite but otherwise simila</context>
<context position="4375" citStr="Klein and Manning (2003)" startWordPosition="638" endWordPosition="641">Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1146–1156, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics (a) NP[agenda] NN[agenda] The president’s agenda (c) NP[1] NP[1] NN[0] The president’s agenda NP[’s] (b) NP[&amp;quot;S] NP[&amp;quot;NP-Poss-Det] The president’s NN[&amp;quot;NP] agenda (d) NP[agenda,&amp;quot;S,1] NP[’s,&amp;quot;NP-Poss-Det,1] NN[agenda,&amp;quot;NP,0] The president’s agenda Figure 1: Parse trees using four different annotation schemes: (a) Lexicalized annotation like that in Collins (1997); (b) Unlexicalized annotation like that in Klein and Manning (2003); (c) Latent annotation like that in Matsuzaki et al. (2005); and (d) the factored, mixed annotations we argue for in our paper. We demonstrate the empirical effectiveness of our approach in two ways. First, we efficiently train a latent-variable grammar with 8 disjoint one-bit latent annotation factors, with scores as high as 89.7 F1 on length ≤40 sentences from the Penn Treebank (Marcus et al., 1993). This latent variable parser outscores the best of Petrov and Klein (2008a)’s comparable parsers while using two orders of magnitude fewer parameters. Second, we combine our latent variable fact</context>
<context position="6198" citStr="Klein and Manning (2003)" startWordPosition="935" endWordPosition="938">pealing for a grammar to be able to model these (somewhat) orthogonal notions, but most models have no mechanism to encourage this. As a notable exception, Dreyer and Eisner (2006) tried to capture this kind of insight by allowing factored annotations to pass unchanged from parent label to child label, though they were not able to demonstrate substantial gains in accuracy. Moreover, there has been to our knowledge no attempt to employ both latent and non-latent annotations at the same time. There is good reason for this: lexicalized or highly annotated grammars like those of Collins (1997) or Klein and Manning (2003) have a very large number of states and an even larger number of rules. Further annotating these rules with latent annotations would produce an infeasibly large grammar. Nevertheless, it is a shame to sacrifice expert annotation just to get latent annotations. Thus, it makes sense to combine these annotation methods in a way that does not lead to an explosion of the state space or a fragmentation of statistics. 3 Parsing with Annotations Suppose we have a raw (binarized) treebank grammar, with productions of the form A —* B C. The typical process is to then annotate these rules with additional</context>
<context position="8147" citStr="Klein and Manning (2003)" startWordPosition="1267" endWordPosition="1270">f-the-art, instead achieving F1 scores in the mid-80’s on the Penn Treebank. The first model is a relatively simple lexicalized parser. We are not aware of a prior discriminative lexicalized constituency parser, and it is quite different from the generative models of Collins (1997). Broadly, it considers features over a binary rule annotated with head words: A[h] —* B[h] C[d] and A[h] —* B[d] C[h], focusing on monolexical rule features and bilexical dependency features. It is our best individual model, scoring 87.3 F1 on the development set. The second is similar to the unlexicalized model of Klein and Manning (2003). This parser starts from a grammar with labels annotated with sibling and parent information, and then adds specific annotations, such as whether an NP is possessive or whether a symbol rewrites as a unary. This parser gets 86.3, tying the original generative version of Klein and Manning (2003). Finally, we use a straightforward discriminative latent variable model much like that of Petrov and Klein (2008a). Here, each symbol is given a latent annotation, referred to as a substate. Typically, these substates correlate at least loosely with linguistic phenomena. For instance, NP-1 might be ass</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Accurate unlexicalized parsing. In ACL, pages 423–430.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nikos Komodakis</author>
<author>Nikos Paragios</author>
<author>Georgios Tziritas</author>
</authors>
<title>Mrf optimization via dual decomposition: Message-passing revisited.</title>
<date>2007</date>
<booktitle>In ICCV,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="28629" citStr="Komodakis et al., 2007" startWordPosition="4724" endWordPosition="4727">n, 2012). In particular, EP allows for the arbitrary choice of messages (the �fm), meaning that we can use structured messages like anchored PCFGs. Mean field (Saul and Jordan, 1996) is another approximate inference technique that allows for structured approximations (Xing et al., 2003; Burkett et al., 2010), but here the natural version of mean field for our model would still be intractable. However, it is possible to adapt mean field into allowing for tractable updates that are similar to the ones we proposed. We do not pursue that approach here. Dual decomposition (Dantzig and Wolfe, 1960; Komodakis et al., 2007) has recently become popular in the community (Rush et al., 2010; Koo et al., 2010). In fact, EP can be seen as a particular kind of dual decomposition of the log normalization constant log Z* 0) that is optimized with message passing rather than (sub-)gradient descent or LP relaxations. Indeed, Minka (2001) argues that the EP objective is more efficiently optimized with message passing than with gradient updates. This assertion should be examined for the structured models common in NLP, but that is beyond the scope of this paper. Finally, note that EP, like belief propagation but unlike mean </context>
</contexts>
<marker>Komodakis, Paragios, Tziritas, 2007</marker>
<rawString>Nikos Komodakis, Nikos Paragios, and Georgios Tziritas. 2007. Mrf optimization via dual decomposition: Message-passing revisited. In ICCV, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Koo</author>
<author>Alexander M Rush</author>
<author>Michael Collins</author>
<author>Tommi Jaakkola</author>
<author>David Sontag</author>
</authors>
<title>Dual decomposition for parsing with non-projective head automata.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="28712" citStr="Koo et al., 2010" startWordPosition="4740" endWordPosition="4743">g that we can use structured messages like anchored PCFGs. Mean field (Saul and Jordan, 1996) is another approximate inference technique that allows for structured approximations (Xing et al., 2003; Burkett et al., 2010), but here the natural version of mean field for our model would still be intractable. However, it is possible to adapt mean field into allowing for tractable updates that are similar to the ones we proposed. We do not pursue that approach here. Dual decomposition (Dantzig and Wolfe, 1960; Komodakis et al., 2007) has recently become popular in the community (Rush et al., 2010; Koo et al., 2010). In fact, EP can be seen as a particular kind of dual decomposition of the log normalization constant log Z* 0) that is optimized with message passing rather than (sub-)gradient descent or LP relaxations. Indeed, Minka (2001) argues that the EP objective is more efficiently optimized with message passing than with gradient updates. This assertion should be examined for the structured models common in NLP, but that is beyond the scope of this paper. Finally, note that EP, like belief propagation but unlike mean field, is not guaranteed to converge, though in practice it usually seems to. In ou</context>
</contexts>
<marker>Koo, Rush, Collins, Jaakkola, Sontag, 2010</marker>
<rawString>Terry Koo, Alexander M. Rush, Michael Collins, Tommi Jaakkola, and David Sontag. 2010. Dual decomposition for parsing with non-projective head automata. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="4780" citStr="Marcus et al., 1993" startWordPosition="704" endWordPosition="707">&amp;quot;NP,0] The president’s agenda Figure 1: Parse trees using four different annotation schemes: (a) Lexicalized annotation like that in Collins (1997); (b) Unlexicalized annotation like that in Klein and Manning (2003); (c) Latent annotation like that in Matsuzaki et al. (2005); and (d) the factored, mixed annotations we argue for in our paper. We demonstrate the empirical effectiveness of our approach in two ways. First, we efficiently train a latent-variable grammar with 8 disjoint one-bit latent annotation factors, with scores as high as 89.7 F1 on length ≤40 sentences from the Penn Treebank (Marcus et al., 1993). This latent variable parser outscores the best of Petrov and Klein (2008a)’s comparable parsers while using two orders of magnitude fewer parameters. Second, we combine our latent variable factors with lexicalized and unlexicalized annotations, resulting in our best F1 score of 89.4 on all sentences. 2 Intuitions Modern theories of grammar such as HPSG (Pollard and Sag, 1994) and Minimalism (Chomsky, 1992) do not ascribe unstructured conjunctions of annotations to phrasal categories. Rather, phrasal categories are associated with sequences of metadata that control their function. For instanc</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takuya Matsuzaki</author>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Probabilistic CFG with latent annotations.</title>
<date>2005</date>
<booktitle>In ACL,</booktitle>
<pages>75--82</pages>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="1613" citStr="Matsuzaki et al., 2005" startWordPosition="239" endWordPosition="242"> up to 88.4 on the Penn Treebank while using two orders of magnitudes fewer parameters compared to the naive approach. Combining latent, lexicalized, and unlexicalized annotations, our best parser gets 89.4 F1 on all sentences from section 23 of the Penn Treebank. 1 Introduction Many high-performance PCFG parsers take an initially simple base grammar over treebank labels like NP and enrich it with deeper syntactic features to improve accuracy. This broad characterization includes lexicalized parsers (Collins, 1997), unlexicalized parsers (Klein and Manning, 2003), and latent variable parsers (Matsuzaki et al., 2005). Figures 1(a), 1(b), and 1(c) show small examples of contextfree trees that have been annotated in these ways. When multi-part annotations are used in the same grammar, systems have generally multiplied these annotations together, in the sense that an NP that was definite, possessive, and VP-dominated would have a single unstructured PCFG symbol that encoded all three facts. In addition, modulo backoff or smoothing, that unstructured symbol would often have rewrite parameters entirely distinct from, say, the indefinite but otherwise similar variant of the symbol (Klein and Manning, 2003). The</context>
<context position="4435" citStr="Matsuzaki et al. (2005)" startWordPosition="648" endWordPosition="651">ional Natural Language Learning, pages 1146–1156, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics (a) NP[agenda] NN[agenda] The president’s agenda (c) NP[1] NP[1] NN[0] The president’s agenda NP[’s] (b) NP[&amp;quot;S] NP[&amp;quot;NP-Poss-Det] The president’s NN[&amp;quot;NP] agenda (d) NP[agenda,&amp;quot;S,1] NP[’s,&amp;quot;NP-Poss-Det,1] NN[agenda,&amp;quot;NP,0] The president’s agenda Figure 1: Parse trees using four different annotation schemes: (a) Lexicalized annotation like that in Collins (1997); (b) Unlexicalized annotation like that in Klein and Manning (2003); (c) Latent annotation like that in Matsuzaki et al. (2005); and (d) the factored, mixed annotations we argue for in our paper. We demonstrate the empirical effectiveness of our approach in two ways. First, we efficiently train a latent-variable grammar with 8 disjoint one-bit latent annotation factors, with scores as high as 89.7 F1 on length ≤40 sentences from the Penn Treebank (Marcus et al., 1993). This latent variable parser outscores the best of Petrov and Klein (2008a)’s comparable parsers while using two orders of magnitude fewer parameters. Second, we combine our latent variable factors with lexicalized and unlexicalized annotations, resultin</context>
<context position="12929" citStr="Matsuzaki et al. (2005)" startWordPosition="2058" endWordPosition="2062">pproximate grammar. For instance, in upper left hand corner the full fLEX is substituted for �fLEX. This new augmented distribution is projected back to the core approximation. This process is repeated for each factor until convergence. (non-convex) log conditional likelihood of the observed training data (T(d), w(d)): log P(T(d)|w(d), 0) 1: (2) log P(T (d)[X]|w(d), 0) X Using standard results, the derivative takes the form: V�(0) = 1: E[(P(T, X, w)|T(d), w(d)] _1: d E[(P(T, X, w)|w(d)] (3) d The first half of this derivative can be obtained by the forward/backward-like computation defined by Matsuzaki et al. (2005), while the second half requires an inside/outside computation (Petrov and Klein, 2008a). The partition function Z(w, 0) is computed as a byproduct of the latter computation. Finally, this objective is regularized, using the L2 norm of 0 as a penalty. We note that we omit from our parser one major feature class found in other discriminative parsers, namely those that use features over the words in the span (Finkel et al., 2008; Petrov and Klein, 2008b). These features might condition on words on either side of the split point of a binary rule or take into account the length of the span. While </context>
<context position="18847" citStr="Matsuzaki et al. (2005)" startWordPosition="3052" endWordPosition="3055">cturally identical factors, one for each of the annotated components. We will approximate each model fm by its corresponding �fm. Thus, there is one colorcoordinated approximate factor for each component of the model in Figure 2(a). There are multiple choices for the structure of these factors, but we focus on anchored PCFGs. Anchored PCFGs have productions of the form iAj —* iBk k�j, where i, k, and j are indexes into the sentence. Here, iAj is a symbol representing building the base symbol A over the span [i, j]. Billott and Lang (1989) introduced anchored 1150 CFGs as “shared forests,” and Matsuzaki et al. (2005) have previously used these grammars for finding an approximate one-best tree in a latent variable parser. Note that, even though an anchored grammar is unannotated, because it is sentence specific it can represent many complex properties of the full grammar’s posterior distribution for a given sentence. For example, it might express a preference for whether a PP token attaches to a particular verb or to that verb’s object noun phrase in a particular sentence. Before continuing, note that a pointwise product of anchored grammars is still an anchored grammar. The complexity of parsing with a pr</context>
</contexts>
<marker>Matsuzaki, Miyao, Tsujii, 2005</marker>
<rawString>Takuya Matsuzaki, Yusuke Miyao, and Jun’ichi Tsujii. 2005. Probabilistic CFG with latent annotations. In ACL, pages 75–82, Morristown, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas P Minka</author>
</authors>
<title>Expectation propagation for approximate Bayesian inference.</title>
<date>2001</date>
<booktitle>In UAI,</booktitle>
<pages>362--369</pages>
<contexts>
<context position="3277" citStr="Minka, 2001" startWordPosition="490" endWordPosition="491">atistical cost when combined with other annotations. In this paper, we argue for grammars with factored annotations, that is, grammars with annotations that have structured component parts that are partially decoupled. Our annotated grammars can include both latent and explicit annotations, as illustrated in Figure 1(d), and we demonstrate that these factored grammars outperform parsers with unstructured annotations. After discussing the factored representation, we describe a method for parsing with factored annotations, using an approximate inference technique called expectation propagation (Minka, 2001). Our algorithm has runtime linear in the number of annotation factors in the grammar, improving on the naive algorithm, which has runtime exponential in the number of annotations. Our method, the Expectation Propagation for Inferring Constituency (EPIC) parser, jointly trains a model over factored annotations, where each factor naturally leverages information from other annotation factors and improves on their mistakes. 1146 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1146–1156, Jeju Island, Ko</context>
<context position="16568" citStr="Minka, 2001" startWordPosition="2675" endWordPosition="2676"> propagation is indeed effective. 5 Factored Inference The key insight behind the approximate inference methods we consider here is that the full model is a product of complex factors that interact in complicated ways, and we will approximate it with a product of corresponding simple factors that interact in simple ways. Since each annotation factor is a reasonable model in both power and complexity on its own, we can consider them one at a time, replacing all others with their approximations, as shown in Figure 2(c). The way we will build these approximations is with expectation propagation (Minka, 2001). Expectation propagation (EP) is a general method for approximate inference that generalizes belief propagation. We describe it here, but we first try to provide an intuition for how it functions in our system. We also describe a simplified version of EP, called assumed density filtering (Boyen and Koller, 1998), which is somewhat easier to understand and rhetorically convenient. For a more detailed introduction to EP in general, we direct the reader to either Minka (2001) or Wainwright and Jordan (2008). Our treatment most resembles the former. 5.1 Factored Approximations Our goal is to buil</context>
<context position="28938" citStr="Minka (2001)" startWordPosition="4780" endWordPosition="4781">the natural version of mean field for our model would still be intractable. However, it is possible to adapt mean field into allowing for tractable updates that are similar to the ones we proposed. We do not pursue that approach here. Dual decomposition (Dantzig and Wolfe, 1960; Komodakis et al., 2007) has recently become popular in the community (Rush et al., 2010; Koo et al., 2010). In fact, EP can be seen as a particular kind of dual decomposition of the log normalization constant log Z* 0) that is optimized with message passing rather than (sub-)gradient descent or LP relaxations. Indeed, Minka (2001) argues that the EP objective is more efficiently optimized with message passing than with gradient updates. This assertion should be examined for the structured models common in NLP, but that is beyond the scope of this paper. Finally, note that EP, like belief propagation but unlike mean field, is not guaranteed to converge, though in practice it usually seems to. In our experiments, typically three or four iterations are enough for almost all sentences to reach convergence, and we found no loss in cutting off the number of iterations to four. 6 Experiments In what follows, we describe three</context>
</contexts>
<marker>Minka, 2001</marker>
<rawString>Thomas P. Minka. 2001. Expectation propagation for approximate Bayesian inference. In UAI, pages 362– 369.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Niehues</author>
<author>Stephan Vogel</author>
</authors>
<title>Discriminative word alignment via alignment matrix modeling.</title>
<date>2008</date>
<booktitle>In Proceedings of the Third Workshop on Statistical Machine Translation,</booktitle>
<pages>18--25</pages>
<contexts>
<context position="27956" citStr="Niehues and Vogel, 2008" startWordPosition="4613" endWordPosition="4616">G0|w|3) O , where I is the maximum number of iterations, M is the number of models, and A is the maximum number of annotations for any given rule. 5.5 Other Inference Algorithms To our knowledge, expectation propagation has been used only once in the NLP community; Daum´e III and Marcu (2006) employed an unstructured version in a Bayesian model of extractive summarization. Therefore, it is worth describing how EP differs from more familiar techniques. EP can be thought of as a more flexible generalization of belief propagation, which has been used several times in NLP (Smith and Eisner, 2008; Niehues and Vogel, 2008; Cromi`eres and Kurohashi, 2009; Burkett and Klein, 2012). In particular, EP allows for the arbitrary choice of messages (the �fm), meaning that we can use structured messages like anchored PCFGs. Mean field (Saul and Jordan, 1996) is another approximate inference technique that allows for structured approximations (Xing et al., 2003; Burkett et al., 2010), but here the natural version of mean field for our model would still be intractable. However, it is possible to adapt mean field into allowing for tractable updates that are similar to the ones we proposed. We do not pursue that approach h</context>
</contexts>
<marker>Niehues, Vogel, 2008</marker>
<rawString>Jan Niehues and Stephan Vogel. 2008. Discriminative word alignment via alignment matrix modeling. In Proceedings of the Third Workshop on Statistical Machine Translation, pages 18–25, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dan Klein</author>
</authors>
<title>Improved inference for unlexicalized parsing.</title>
<date>2007</date>
<booktitle>In NAACL-HLT,</booktitle>
<contexts>
<context position="9051" citStr="Petrov and Klein, 2007" startWordPosition="1414" endWordPosition="1417"> Manning (2003). Finally, we use a straightforward discriminative latent variable model much like that of Petrov and Klein (2008a). Here, each symbol is given a latent annotation, referred to as a substate. Typically, these substates correlate at least loosely with linguistic phenomena. For instance, NP-1 might be associated with possessive NPs, while NP-3 might be for adjuncts. Often, these latent integers are considered as bit strings, with each bit indicating one latent annotation. Prior work in this area has considered the effect of splitting and merging these states (Petrov et al., 2006; Petrov and Klein, 2007), as well as “multiscale” grammars (Petrov and Klein, 2008b). With two states (or one bit of annotation), our version of this parser gets 81.7 F1, edging out the comparable parser of Petrov and Klein (2008a). On the other hand, our parser gets 83.2 with four states (two bits), short of the performance of prior work.1 1Much of the difference stems from the different binarization scheme we employ. We use head-outward binarization, rather than the left-branching binarization they employed. This change was to enable integrating lexicalization with our other models. 3.2 Model Representation We empl</context>
</contexts>
<marker>Petrov, Klein, 2007</marker>
<rawString>Slav Petrov and Dan Klein. 2007. Improved inference for unlexicalized parsing. In NAACL-HLT, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dan Klein</author>
</authors>
<title>Discriminative loglinear grammars with latent variables.</title>
<date>2008</date>
<booktitle>In NIPS,</booktitle>
<pages>1153--1160</pages>
<contexts>
<context position="4854" citStr="Petrov and Klein (2008" startWordPosition="717" endWordPosition="720"> annotation schemes: (a) Lexicalized annotation like that in Collins (1997); (b) Unlexicalized annotation like that in Klein and Manning (2003); (c) Latent annotation like that in Matsuzaki et al. (2005); and (d) the factored, mixed annotations we argue for in our paper. We demonstrate the empirical effectiveness of our approach in two ways. First, we efficiently train a latent-variable grammar with 8 disjoint one-bit latent annotation factors, with scores as high as 89.7 F1 on length ≤40 sentences from the Penn Treebank (Marcus et al., 1993). This latent variable parser outscores the best of Petrov and Klein (2008a)’s comparable parsers while using two orders of magnitude fewer parameters. Second, we combine our latent variable factors with lexicalized and unlexicalized annotations, resulting in our best F1 score of 89.4 on all sentences. 2 Intuitions Modern theories of grammar such as HPSG (Pollard and Sag, 1994) and Minimalism (Chomsky, 1992) do not ascribe unstructured conjunctions of annotations to phrasal categories. Rather, phrasal categories are associated with sequences of metadata that control their function. For instance, an NP might have annotations to the effect that it is singular, masculi</context>
<context position="8556" citStr="Petrov and Klein (2008" startWordPosition="1333" endWordPosition="1336">on monolexical rule features and bilexical dependency features. It is our best individual model, scoring 87.3 F1 on the development set. The second is similar to the unlexicalized model of Klein and Manning (2003). This parser starts from a grammar with labels annotated with sibling and parent information, and then adds specific annotations, such as whether an NP is possessive or whether a symbol rewrites as a unary. This parser gets 86.3, tying the original generative version of Klein and Manning (2003). Finally, we use a straightforward discriminative latent variable model much like that of Petrov and Klein (2008a). Here, each symbol is given a latent annotation, referred to as a substate. Typically, these substates correlate at least loosely with linguistic phenomena. For instance, NP-1 might be associated with possessive NPs, while NP-3 might be for adjuncts. Often, these latent integers are considered as bit strings, with each bit indicating one latent annotation. Prior work in this area has considered the effect of splitting and merging these states (Petrov et al., 2006; Petrov and Klein, 2007), as well as “multiscale” grammars (Petrov and Klein, 2008b). With two states (or one bit of annotation),</context>
<context position="11207" citStr="Petrov and Klein (2008" startWordPosition="1791" endWordPosition="1794">: fm(T [Xm]; w, θm) = exp (θT mϕm(T, Xm, w)) Here, Xm is the annotation associated with a particular model m. ϕ is a feature function that projects the raw tree, annotations, and words into a feature vector. The features ϕ need to decompose into features for each factor fm; we do not allow features that take into account the annotation from two different components. We further add a pruning filter that assigns zero weight to any tree with a constituent that a baseline unannotated grammar finds sufficiently unlikely, and a weight of one to any other tree. This filter is similar to that used in Petrov and Klein (2008a) and allows for much more efficient training and inference. Because our model is discriminative, training takes the form of maximizing the probability of the training trees given the words. This objective is convex for deterministic annotations, but non-convex for latent annotations. We (locally) optimize the (1) m 1148 FLEX FLAT fUNL fUNL Full product model Approximate model P(T [X]|w; ✓) q(T Iw) (a) (b) (c) Figure 2: Schematic representation of our model, its approximation, and expectation propagation. (a) The full joint distribution consists of a product of three grammars with different a</context>
<context position="13015" citStr="Petrov and Klein, 2008" startWordPosition="2071" endWordPosition="2074">d for �fLEX. This new augmented distribution is projected back to the core approximation. This process is repeated for each factor until convergence. (non-convex) log conditional likelihood of the observed training data (T(d), w(d)): log P(T(d)|w(d), 0) 1: (2) log P(T (d)[X]|w(d), 0) X Using standard results, the derivative takes the form: V�(0) = 1: E[(P(T, X, w)|T(d), w(d)] _1: d E[(P(T, X, w)|w(d)] (3) d The first half of this derivative can be obtained by the forward/backward-like computation defined by Matsuzaki et al. (2005), while the second half requires an inside/outside computation (Petrov and Klein, 2008a). The partition function Z(w, 0) is computed as a byproduct of the latter computation. Finally, this objective is regularized, using the L2 norm of 0 as a penalty. We note that we omit from our parser one major feature class found in other discriminative parsers, namely those that use features over the words in the span (Finkel et al., 2008; Petrov and Klein, 2008b). These features might condition on words on either side of the split point of a binary rule or take into account the length of the span. While such features have proven useful in previous work, they are not the focus of our curre</context>
<context position="34818" citStr="Petrov and Klein (2008" startWordPosition="5771" endWordPosition="5774">ing has focused on splitting smaller unstructured annotations into larger unstructured annotations. Here, we consider training a joint model consisting of a large number of disjoint one-bit (i.e. two-state) latent variable annotations. Specifically, we consider the performance of products of up to 8 one-bit annotations. In Figure 3, we show development F1 as a function of the number of latent bits. Improvement is roughly linear up to 3 components. Performance levels off afterwards, with the top performing system scoring 89.7 F1. Nevertheless, these parsers outperform the comparable parsers of Petrov and Klein (2008a) (89.3), even though our six-bit parser has many fewer effective parameters per binary rule: F1 90 88 86 84 82 80 1154 Models F1, &lt; 40 F1, All Lexicalized 87.3 86.5 Unlexicalized 86.3 85.4 3xLatent 88.6 87.6 Lex+Unlex 90.2 89.5 Lex+Lat 90.0 89.4 Unlex+Lat 90.0 89.4 Lex+Unlex+Lat 90.2 89.7 Table 2: Development F1 score for various model combinations for sentences less than length 40 and all sentences. 3xLatent refers to a latent annotation model with 3 factored latent bits. 48 instead of the 4096 in their best parser. We also ran our best system on Section 23, where it gets 89.1 and 88.4 on s</context>
<context position="36840" citStr="Petrov and Klein (2008" startWordPosition="6104" endWordPosition="6107"> the lexicalized and unlexicalized models. Indeed, our best systems involved combining the lexicalized model with some other model. This is probably because the lexicalized model can represent very different syntactic relationships than the latent and unlexicalized models, meaning there is more diversity in the joint model’s capacity when using combinations involving the lexicalized annotations. Finally, we ran our best system (the fully combined one) on Section 23 of the Penn Treebank. It scored 90.1/89.4 F1 on length 40 and all sentences respectively, slightly edging out the 90.0/89.3 F1 of Petrov and Klein (2008a). However, it is not quite as good at exact match: 37.7/35.3 vs 40.1/37.7. Note, though, that their parser makes use of span features, which deliver a gain of +0.3/0.2F1 respectively, while ours does not. We suspect that similar gains could be had by incorporating these features, but we leave that for future work. 7 Conclusion Factored representations capture a fundamental linguistic insight: grammatical categories are not monolithic, unanalyzable entities. Instead, they are composed of numerous facets that together govern how categories combine into parse trees. We have developed a new mode</context>
</contexts>
<marker>Petrov, Klein, 2008</marker>
<rawString>Slav Petrov and Dan Klein. 2008a. Discriminative loglinear grammars with latent variables. In NIPS, pages 1153–1160.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dan Klein</author>
</authors>
<title>Sparse multi-scale grammars for discriminative latent variable parsing.</title>
<date>2008</date>
<booktitle>In EMNLP,</booktitle>
<pages>867--876</pages>
<location>Honolulu, Hawaii,</location>
<contexts>
<context position="4854" citStr="Petrov and Klein (2008" startWordPosition="717" endWordPosition="720"> annotation schemes: (a) Lexicalized annotation like that in Collins (1997); (b) Unlexicalized annotation like that in Klein and Manning (2003); (c) Latent annotation like that in Matsuzaki et al. (2005); and (d) the factored, mixed annotations we argue for in our paper. We demonstrate the empirical effectiveness of our approach in two ways. First, we efficiently train a latent-variable grammar with 8 disjoint one-bit latent annotation factors, with scores as high as 89.7 F1 on length ≤40 sentences from the Penn Treebank (Marcus et al., 1993). This latent variable parser outscores the best of Petrov and Klein (2008a)’s comparable parsers while using two orders of magnitude fewer parameters. Second, we combine our latent variable factors with lexicalized and unlexicalized annotations, resulting in our best F1 score of 89.4 on all sentences. 2 Intuitions Modern theories of grammar such as HPSG (Pollard and Sag, 1994) and Minimalism (Chomsky, 1992) do not ascribe unstructured conjunctions of annotations to phrasal categories. Rather, phrasal categories are associated with sequences of metadata that control their function. For instance, an NP might have annotations to the effect that it is singular, masculi</context>
<context position="8556" citStr="Petrov and Klein (2008" startWordPosition="1333" endWordPosition="1336">on monolexical rule features and bilexical dependency features. It is our best individual model, scoring 87.3 F1 on the development set. The second is similar to the unlexicalized model of Klein and Manning (2003). This parser starts from a grammar with labels annotated with sibling and parent information, and then adds specific annotations, such as whether an NP is possessive or whether a symbol rewrites as a unary. This parser gets 86.3, tying the original generative version of Klein and Manning (2003). Finally, we use a straightforward discriminative latent variable model much like that of Petrov and Klein (2008a). Here, each symbol is given a latent annotation, referred to as a substate. Typically, these substates correlate at least loosely with linguistic phenomena. For instance, NP-1 might be associated with possessive NPs, while NP-3 might be for adjuncts. Often, these latent integers are considered as bit strings, with each bit indicating one latent annotation. Prior work in this area has considered the effect of splitting and merging these states (Petrov et al., 2006; Petrov and Klein, 2007), as well as “multiscale” grammars (Petrov and Klein, 2008b). With two states (or one bit of annotation),</context>
<context position="11207" citStr="Petrov and Klein (2008" startWordPosition="1791" endWordPosition="1794">: fm(T [Xm]; w, θm) = exp (θT mϕm(T, Xm, w)) Here, Xm is the annotation associated with a particular model m. ϕ is a feature function that projects the raw tree, annotations, and words into a feature vector. The features ϕ need to decompose into features for each factor fm; we do not allow features that take into account the annotation from two different components. We further add a pruning filter that assigns zero weight to any tree with a constituent that a baseline unannotated grammar finds sufficiently unlikely, and a weight of one to any other tree. This filter is similar to that used in Petrov and Klein (2008a) and allows for much more efficient training and inference. Because our model is discriminative, training takes the form of maximizing the probability of the training trees given the words. This objective is convex for deterministic annotations, but non-convex for latent annotations. We (locally) optimize the (1) m 1148 FLEX FLAT fUNL fUNL Full product model Approximate model P(T [X]|w; ✓) q(T Iw) (a) (b) (c) Figure 2: Schematic representation of our model, its approximation, and expectation propagation. (a) The full joint distribution consists of a product of three grammars with different a</context>
<context position="13015" citStr="Petrov and Klein, 2008" startWordPosition="2071" endWordPosition="2074">d for �fLEX. This new augmented distribution is projected back to the core approximation. This process is repeated for each factor until convergence. (non-convex) log conditional likelihood of the observed training data (T(d), w(d)): log P(T(d)|w(d), 0) 1: (2) log P(T (d)[X]|w(d), 0) X Using standard results, the derivative takes the form: V�(0) = 1: E[(P(T, X, w)|T(d), w(d)] _1: d E[(P(T, X, w)|w(d)] (3) d The first half of this derivative can be obtained by the forward/backward-like computation defined by Matsuzaki et al. (2005), while the second half requires an inside/outside computation (Petrov and Klein, 2008a). The partition function Z(w, 0) is computed as a byproduct of the latter computation. Finally, this objective is regularized, using the L2 norm of 0 as a penalty. We note that we omit from our parser one major feature class found in other discriminative parsers, namely those that use features over the words in the span (Finkel et al., 2008; Petrov and Klein, 2008b). These features might condition on words on either side of the split point of a binary rule or take into account the length of the span. While such features have proven useful in previous work, they are not the focus of our curre</context>
<context position="34818" citStr="Petrov and Klein (2008" startWordPosition="5771" endWordPosition="5774">ing has focused on splitting smaller unstructured annotations into larger unstructured annotations. Here, we consider training a joint model consisting of a large number of disjoint one-bit (i.e. two-state) latent variable annotations. Specifically, we consider the performance of products of up to 8 one-bit annotations. In Figure 3, we show development F1 as a function of the number of latent bits. Improvement is roughly linear up to 3 components. Performance levels off afterwards, with the top performing system scoring 89.7 F1. Nevertheless, these parsers outperform the comparable parsers of Petrov and Klein (2008a) (89.3), even though our six-bit parser has many fewer effective parameters per binary rule: F1 90 88 86 84 82 80 1154 Models F1, &lt; 40 F1, All Lexicalized 87.3 86.5 Unlexicalized 86.3 85.4 3xLatent 88.6 87.6 Lex+Unlex 90.2 89.5 Lex+Lat 90.0 89.4 Unlex+Lat 90.0 89.4 Lex+Unlex+Lat 90.2 89.7 Table 2: Development F1 score for various model combinations for sentences less than length 40 and all sentences. 3xLatent refers to a latent annotation model with 3 factored latent bits. 48 instead of the 4096 in their best parser. We also ran our best system on Section 23, where it gets 89.1 and 88.4 on s</context>
<context position="36840" citStr="Petrov and Klein (2008" startWordPosition="6104" endWordPosition="6107"> the lexicalized and unlexicalized models. Indeed, our best systems involved combining the lexicalized model with some other model. This is probably because the lexicalized model can represent very different syntactic relationships than the latent and unlexicalized models, meaning there is more diversity in the joint model’s capacity when using combinations involving the lexicalized annotations. Finally, we ran our best system (the fully combined one) on Section 23 of the Penn Treebank. It scored 90.1/89.4 F1 on length 40 and all sentences respectively, slightly edging out the 90.0/89.3 F1 of Petrov and Klein (2008a). However, it is not quite as good at exact match: 37.7/35.3 vs 40.1/37.7. Note, though, that their parser makes use of span features, which deliver a gain of +0.3/0.2F1 respectively, while ours does not. We suspect that similar gains could be had by incorporating these features, but we leave that for future work. 7 Conclusion Factored representations capture a fundamental linguistic insight: grammatical categories are not monolithic, unanalyzable entities. Instead, they are composed of numerous facets that together govern how categories combine into parse trees. We have developed a new mode</context>
</contexts>
<marker>Petrov, Klein, 2008</marker>
<rawString>Slav Petrov and Dan Klein. 2008b. Sparse multi-scale grammars for discriminative latent variable parsing. In EMNLP, pages 867–876, Honolulu, Hawaii, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Leon Barrett</author>
<author>Romain Thibaux</author>
<author>Dan Klein</author>
</authors>
<title>Learning accurate, compact, and interpretable tree annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>433--440</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="9026" citStr="Petrov et al., 2006" startWordPosition="1410" endWordPosition="1413"> version of Klein and Manning (2003). Finally, we use a straightforward discriminative latent variable model much like that of Petrov and Klein (2008a). Here, each symbol is given a latent annotation, referred to as a substate. Typically, these substates correlate at least loosely with linguistic phenomena. For instance, NP-1 might be associated with possessive NPs, while NP-3 might be for adjuncts. Often, these latent integers are considered as bit strings, with each bit indicating one latent annotation. Prior work in this area has considered the effect of splitting and merging these states (Petrov et al., 2006; Petrov and Klein, 2007), as well as “multiscale” grammars (Petrov and Klein, 2008b). With two states (or one bit of annotation), our version of this parser gets 81.7 F1, edging out the comparable parser of Petrov and Klein (2008a). On the other hand, our parser gets 83.2 with four states (two bits), short of the performance of prior work.1 1Much of the difference stems from the different binarization scheme we employ. We use head-outward binarization, rather than the left-branching binarization they employed. This change was to enable integrating lexicalization with our other models. 3.2 Mod</context>
<context position="14906" citStr="Petrov et al. (2006)" startWordPosition="2402" endWordPosition="2405">number of binary rules in the unannotated “base” grammar. �(0) = 1: d 1: = d 1149 Suppose that we have M annotation components. Each annotation component can have up to A primitive annotations per rule. For instance, a latent variable grammar will have A = 8b where b is the number of bits of annotation. If we compile all annotation components into unstructured annotations, we can end up with a total grammar size of O(AmG0), and so in general parsing time scales exponentially with the number of annotation components. Thus, if we use latent annotations and the hierarchical splitting approach of Petrov et al. (2006), then the grammar has size O(8SG0), where 5 is the number of times the grammar was split in two. Therefore, the size of annotated grammars can reach intractable levels very quickly, particularly in the case of latent annotations, where all combinations of annotations are possible. Petrov (2010) considered an approach to slowing this growth down by using a set of M independently trained parsers Pm, and parsed using the product of the scores from each parser as the score for the tree. This approach worked largely because training was intractable: if the training algorithm could reach the global</context>
</contexts>
<marker>Petrov, Barrett, Thibaux, Klein, 2006</marker>
<rawString>Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. 2006. Learning accurate, compact, and interpretable tree annotation. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 433–440, Sydney, Australia, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
</authors>
<title>Products of random latent variable grammars.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>pages</pages>
<location>Los Angeles, California,</location>
<contexts>
<context position="15202" citStr="Petrov (2010)" startWordPosition="2452" endWordPosition="2453">. If we compile all annotation components into unstructured annotations, we can end up with a total grammar size of O(AmG0), and so in general parsing time scales exponentially with the number of annotation components. Thus, if we use latent annotations and the hierarchical splitting approach of Petrov et al. (2006), then the grammar has size O(8SG0), where 5 is the number of times the grammar was split in two. Therefore, the size of annotated grammars can reach intractable levels very quickly, particularly in the case of latent annotations, where all combinations of annotations are possible. Petrov (2010) considered an approach to slowing this growth down by using a set of M independently trained parsers Pm, and parsed using the product of the scores from each parser as the score for the tree. This approach worked largely because training was intractable: if the training algorithm could reach the global optimum, then this approach might have yielded no gain. However, because the optimization technique is local, the same algorithm produced multiple grammars. In what follows, we propose another solution that exploits the factored structure of our grammar with expectation propagation. Crucially, </context>
<context position="22343" citStr="Petrov (2010)" startWordPosition="3638" endWordPosition="3639">ected to an anchored grammar, creating fm. More intuitively, we create an anchored PCFG that makes the approximation “as close as possible” to the augmented grammar. (We describe this procedure more precisely in Section 5.4.) Thus, each term fm is approximated in the context of the terms that come before it. This contextual approximation is essential: without it, ADF would approximate the terms independently, meaning that no information would be shared between the models. This method would be, in effect, a simple method for parser combination, not all that dissimilar to the method proposed by Petrov (2010). Finally, note that the same inside and outside scores computed in the loop can be used to compute the expected counts needed in Equation 3. Now we consider the runtime complexity of this algorithm. If the maximum number of annotations per rule for any factor is A, ADF has complexity O (MAG0 |W|3 ) when using M factors. In contrast, parsing with the fully annotated grammar (AMG0|W|3) would have complexity O . Critically, for a latent variable parser with M annotation bits, the exact algorithm takes time exponential in M, while this approximate algorithm takes time linear in M. It is worth pau</context>
<context position="31407" citStr="Petrov (2010)" startWordPosition="5189" endWordPosition="5190">fect final performance, but greatly decreased total training time, especially for the latent variable parsers. For extracting a one-best tree, we use a version of the Max-Recall algorithm of Goodman (1996). When using EP or ADF, we initialized the core approximation q to the uniform distribution over unpruned trees. 1153 Parsing Training ADF EP Exact Petrov ADF 84.3 84.5 84.5 82.5 EP 84.1 84.6 84.5 78.7 Exact 83.8 84.5 84.9 81.5 Indep. 82.3 82.1 82.2 82.6 Table 1: The effect of algorithm choice for training and parsing on a product of two 2-state parsers on F1. Petrov is the product parser of Petrov (2010), and Indep. refers to independently trained models. For comparison, a fourstate parser achieves a score of 83.2. When counting parameters, we consider the number of parameters per binary rule. Hence, a single four-state latent model would have 64 (= 43) parameters per rule, while a product of 5 two-state models would have just 40 (= 5 · 23). 6.2 Comparison of Inference Algorithms In our first experiment, we test the relative performance of the various approximate inference methods at both train and test time. In order to include exact inference, we necessarily need to look at a smaller scale </context>
<context position="33502" citStr="Petrov (2010)" startWordPosition="5563" endWordPosition="5564">and EP seem to perform fairly similarly at parse time for all training conditions. In general, there seems to be a gain for using the same method for training and testing. Each testing method performs at its best when using models trained with the same method. Moreover, except for ADF, the converse holds true: the grammars trained 1 2 3 4 5 6 7 8 Number of Models Figure 3: Development F1 plotted against the number M of one-bit latent annotation components. The best grammar has 6 one-bit annotations, with 89.7 F1. with a given parsing method are best decoded using the same method. Oddly, using Petrov (2010)’s method does not seem to work well at all for jointly trained models, except for ADF. Similarly, joint parsing underperforms Petrov (2010)’s method when using independently trained models. Likely, the joint parsing algorithms are miscalibrating the redundant information present in the two independently-trained models, while the two jointly-trained components come to depend on each other. In fact, the F1 scores for the two separate models of the EP parser are in the 60’s. As expected, ADF does not perform as well as EP. Therefore, we exclude it from our subsequent experiments, focusing exclus</context>
</contexts>
<marker>Petrov, 2010</marker>
<rawString>Slav Petrov. 2010. Products of random latent variable grammars. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 19–27, Los Angeles, California, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl Pollard</author>
<author>Ivan A Sag</author>
</authors>
<title>Head-Driven Phrase Structure Grammar.</title>
<date>1994</date>
<publisher>University of Chicago Press,</publisher>
<location>Chicago.</location>
<contexts>
<context position="5160" citStr="Pollard and Sag, 1994" startWordPosition="764" endWordPosition="767">tiveness of our approach in two ways. First, we efficiently train a latent-variable grammar with 8 disjoint one-bit latent annotation factors, with scores as high as 89.7 F1 on length ≤40 sentences from the Penn Treebank (Marcus et al., 1993). This latent variable parser outscores the best of Petrov and Klein (2008a)’s comparable parsers while using two orders of magnitude fewer parameters. Second, we combine our latent variable factors with lexicalized and unlexicalized annotations, resulting in our best F1 score of 89.4 on all sentences. 2 Intuitions Modern theories of grammar such as HPSG (Pollard and Sag, 1994) and Minimalism (Chomsky, 1992) do not ascribe unstructured conjunctions of annotations to phrasal categories. Rather, phrasal categories are associated with sequences of metadata that control their function. For instance, an NP might have annotations to the effect that it is singular, masculine, and nominative, with perhaps further information about its animacy or other aspects of the head noun. Thus, it is appealing for a grammar to be able to model these (somewhat) orthogonal notions, but most models have no mechanism to encourage this. As a notable exception, Dreyer and Eisner (2006) tried</context>
</contexts>
<marker>Pollard, Sag, 1994</marker>
<rawString>Carl Pollard and Ivan A. Sag. 1994. Head-Driven Phrase Structure Grammar. University of Chicago Press, Chicago.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander M Rush</author>
<author>David Sontag</author>
<author>Michael Collins</author>
<author>Tommi Jaakkola</author>
</authors>
<title>On dual decomposition and linear programming relaxations for natural language processing.</title>
<date>2010</date>
<booktitle>In EMNLP,</booktitle>
<pages>1--11</pages>
<location>Cambridge, MA,</location>
<contexts>
<context position="28693" citStr="Rush et al., 2010" startWordPosition="4736" endWordPosition="4739">s (the �fm), meaning that we can use structured messages like anchored PCFGs. Mean field (Saul and Jordan, 1996) is another approximate inference technique that allows for structured approximations (Xing et al., 2003; Burkett et al., 2010), but here the natural version of mean field for our model would still be intractable. However, it is possible to adapt mean field into allowing for tractable updates that are similar to the ones we proposed. We do not pursue that approach here. Dual decomposition (Dantzig and Wolfe, 1960; Komodakis et al., 2007) has recently become popular in the community (Rush et al., 2010; Koo et al., 2010). In fact, EP can be seen as a particular kind of dual decomposition of the log normalization constant log Z* 0) that is optimized with message passing rather than (sub-)gradient descent or LP relaxations. Indeed, Minka (2001) argues that the EP objective is more efficiently optimized with message passing than with gradient updates. This assertion should be examined for the structured models common in NLP, but that is beyond the scope of this paper. Finally, note that EP, like belief propagation but unlike mean field, is not guaranteed to converge, though in practice it usua</context>
</contexts>
<marker>Rush, Sontag, Collins, Jaakkola, 2010</marker>
<rawString>Alexander M Rush, David Sontag, Michael Collins, and Tommi Jaakkola. 2010. On dual decomposition and linear programming relaxations for natural language processing. In EMNLP, pages 1–11, Cambridge, MA, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lawrence Saul</author>
<author>Michael Jordan</author>
</authors>
<title>Exploiting tractable substructures in intractable networks.</title>
<date>1996</date>
<booktitle>In NIPS</booktitle>
<contexts>
<context position="28188" citStr="Saul and Jordan, 1996" startWordPosition="4650" endWordPosition="4653"> only once in the NLP community; Daum´e III and Marcu (2006) employed an unstructured version in a Bayesian model of extractive summarization. Therefore, it is worth describing how EP differs from more familiar techniques. EP can be thought of as a more flexible generalization of belief propagation, which has been used several times in NLP (Smith and Eisner, 2008; Niehues and Vogel, 2008; Cromi`eres and Kurohashi, 2009; Burkett and Klein, 2012). In particular, EP allows for the arbitrary choice of messages (the �fm), meaning that we can use structured messages like anchored PCFGs. Mean field (Saul and Jordan, 1996) is another approximate inference technique that allows for structured approximations (Xing et al., 2003; Burkett et al., 2010), but here the natural version of mean field for our model would still be intractable. However, it is possible to adapt mean field into allowing for tractable updates that are similar to the ones we proposed. We do not pursue that approach here. Dual decomposition (Dantzig and Wolfe, 1960; Komodakis et al., 2007) has recently become popular in the community (Rush et al., 2010; Koo et al., 2010). In fact, EP can be seen as a particular kind of dual decomposition of the </context>
</contexts>
<marker>Saul, Jordan, 1996</marker>
<rawString>Lawrence Saul and Michael Jordan. 1996. Exploiting tractable substructures in intractable networks. In NIPS 1995.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satoshi Sekine</author>
<author>Michael J Collins</author>
</authors>
<date>1997</date>
<note>Evalb – bracket scoring program.</note>
<contexts>
<context position="30375" citStr="Sekine and Collins, 1997" startWordPosition="5016" endWordPosition="5019">oducts. Finally, we present a selection of the many possible model combinations, showing that combining latent and expert annotation can be quite effective. 6.1 Experimental Setup For our experiments, we trained and tested on the Penn Treebank using the standard splits: sections 2- 21 were training, 22 development, and 23 testing. In preliminary experiments, we report development set F1 on sentences up to length 40. For our final test set experiment, we report F1 on sentences from section 23 up to length 40, as well as all sentences from that section. Scores reported are computed using EVALB (Sekine and Collins, 1997). We binarize trees using Collins’ head rules (Collins, 1997). Each discriminative parser was trained using the Adaptive Gradient variant of Stochastic Gradient Descent (Duchi et al., 2010). Smaller models were seeded from larger models. That is, before training a grammar of 5 models with 1 latent bit each, we started with weights from a parser with 4 factored bits. Initial experiments suggested this step did not affect final performance, but greatly decreased total training time, especially for the latent variable parsers. For extracting a one-best tree, we use a version of the Max-Recall alg</context>
</contexts>
<marker>Sekine, Collins, 1997</marker>
<rawString>Satoshi Sekine and Michael J. Collins. 1997. Evalb – bracket scoring program.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David A Smith</author>
<author>Jason Eisner</author>
</authors>
<title>Dependency parsing by belief propagation.</title>
<date>2008</date>
<booktitle>In EMNLP,</booktitle>
<pages>145--156</pages>
<location>Honolulu,</location>
<contexts>
<context position="27931" citStr="Smith and Eisner, 2008" startWordPosition="4609" endWordPosition="4612">thm that takes time (IMAG0|w|3) O , where I is the maximum number of iterations, M is the number of models, and A is the maximum number of annotations for any given rule. 5.5 Other Inference Algorithms To our knowledge, expectation propagation has been used only once in the NLP community; Daum´e III and Marcu (2006) employed an unstructured version in a Bayesian model of extractive summarization. Therefore, it is worth describing how EP differs from more familiar techniques. EP can be thought of as a more flexible generalization of belief propagation, which has been used several times in NLP (Smith and Eisner, 2008; Niehues and Vogel, 2008; Cromi`eres and Kurohashi, 2009; Burkett and Klein, 2012). In particular, EP allows for the arbitrary choice of messages (the �fm), meaning that we can use structured messages like anchored PCFGs. Mean field (Saul and Jordan, 1996) is another approximate inference technique that allows for structured approximations (Xing et al., 2003; Burkett et al., 2010), but here the natural version of mean field for our model would still be intractable. However, it is possible to adapt mean field into allowing for tractable updates that are similar to the ones we proposed. We do n</context>
</contexts>
<marker>Smith, Eisner, 2008</marker>
<rawString>David A. Smith and Jason Eisner. 2008. Dependency parsing by belief propagation. In EMNLP, pages 145– 156, Honolulu, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin J Wainwright</author>
<author>Michael I Jordan</author>
</authors>
<title>Graphical Models, Exponential Families, and Variational Inference.</title>
<date>2008</date>
<publisher>Now Publishers Inc.,</publisher>
<location>Hanover, MA, USA.</location>
<contexts>
<context position="17078" citStr="Wainwright and Jordan (2008)" startWordPosition="2757" endWordPosition="2760">ons, as shown in Figure 2(c). The way we will build these approximations is with expectation propagation (Minka, 2001). Expectation propagation (EP) is a general method for approximate inference that generalizes belief propagation. We describe it here, but we first try to provide an intuition for how it functions in our system. We also describe a simplified version of EP, called assumed density filtering (Boyen and Koller, 1998), which is somewhat easier to understand and rhetorically convenient. For a more detailed introduction to EP in general, we direct the reader to either Minka (2001) or Wainwright and Jordan (2008). Our treatment most resembles the former. 5.1 Factored Approximations Our goal is to build an approximation that takes information from all components into account. To begin, we note that each of these components captures different phenomena: an unlexicalized grammar is good at capturing structural relationships in a parse tree (e.g. subject noun phrases have different distributions than object noun phrases), while a lexicalized grammar captures preferred attachments for different verbs. At the same time, each of these component grammars can be thought of as a refinement of the raw unannotate</context>
</contexts>
<marker>Wainwright, Jordan, 2008</marker>
<rawString>Martin J Wainwright and Michael I Jordan. 2008. Graphical Models, Exponential Families, and Variational Inference. Now Publishers Inc., Hanover, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric P Xing</author>
<author>Michael I Jordan</author>
<author>Stuart J Russell</author>
</authors>
<title>A generalized mean field algorithm for variational inference in exponential families.</title>
<date>2003</date>
<booktitle>In UAI,</booktitle>
<pages>583--591</pages>
<contexts>
<context position="28292" citStr="Xing et al., 2003" startWordPosition="4666" endWordPosition="4669">odel of extractive summarization. Therefore, it is worth describing how EP differs from more familiar techniques. EP can be thought of as a more flexible generalization of belief propagation, which has been used several times in NLP (Smith and Eisner, 2008; Niehues and Vogel, 2008; Cromi`eres and Kurohashi, 2009; Burkett and Klein, 2012). In particular, EP allows for the arbitrary choice of messages (the �fm), meaning that we can use structured messages like anchored PCFGs. Mean field (Saul and Jordan, 1996) is another approximate inference technique that allows for structured approximations (Xing et al., 2003; Burkett et al., 2010), but here the natural version of mean field for our model would still be intractable. However, it is possible to adapt mean field into allowing for tractable updates that are similar to the ones we proposed. We do not pursue that approach here. Dual decomposition (Dantzig and Wolfe, 1960; Komodakis et al., 2007) has recently become popular in the community (Rush et al., 2010; Koo et al., 2010). In fact, EP can be seen as a particular kind of dual decomposition of the log normalization constant log Z* 0) that is optimized with message passing rather than (sub-)gradient d</context>
</contexts>
<marker>Xing, Jordan, Russell, 2003</marker>
<rawString>Eric P. Xing, Michael I. Jordan, and Stuart J. Russell. 2003. A generalized mean field algorithm for variational inference in exponential families. In UAI, pages 583–591.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
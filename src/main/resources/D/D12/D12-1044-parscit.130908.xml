<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.997453">
Dynamic Programming for Higher Order Parsing of Gap-Minding Trees
</title>
<author confidence="0.999103">
Emily Pitler, Sampath Kannan, Mitchell Marcus
</author>
<affiliation confidence="0.9983815">
Computer and Information Science
University of Pennsylvania
</affiliation>
<address confidence="0.774455">
Philadelphia, PA 19104
</address>
<email confidence="0.998825">
epitler,kannan,mitch@seas.upenn.edu
</email>
<sectionHeader confidence="0.997389" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999292727272727">
We introduce gap inheritance, a new struc-
tural property on trees, which provides a way
to quantify the degree to which intervals of de-
scendants can be nested. Based on this prop-
erty, two new classes of trees are derived that
provide a closer approximation to the set of
plausible natural language dependency trees
than some alternative classes of trees: unlike
projective trees, a word can have descendants
in more than one interval; unlike spanning
trees, these intervals cannot be nested in ar-
bitrary ways. The 1-Inherit class of trees has
exactly the same empirical coverage of natural
language sentences as the class of mildly non-
projective trees, yet the optimal scoring tree
can be found in an order of magnitude less
time. Gap-minding trees (the second class)
have the property that all edges into an interval
of descendants come from the same node, and
thus an algorithm which uses only single in-
tervals can produce trees in which a node has
descendants in multiple intervals.
</bodyText>
<sectionHeader confidence="0.99947" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998932354166667">
Dependency parsers vary in what space of possi-
ble tree structures they search over when parsing
a sentence. One commonly used space is the set
of projective trees, in which every node’s descen-
dants form a contiguous interval in the input sen-
tence. Finding the optimal tree in the set of projec-
tive trees can be done efficiently (Eisner, 2000), even
when the score of a tree depends on higher order fac-
tors (McDonald and Pereira, 2006; Carreras, 2007;
Koo and Collins, 2010). However, the projectivity
assumption is too strict for all natural language de-
pendency trees; for example, only 63.6% of Dutch
sentences from the CoNLL-X training set are pro-
jective (Table 1).
At the other end of the spectrum, some parsers
search over all spanning trees, a class of structures
much larger than the set of plausible linguistic struc-
tures. The maximum scoring directed spanning tree
can be found efficiently when the score of a tree de-
pends only on edge-based factors (McDonald et al.,
2005b). However, it is NP-hard to extend MST to in-
clude sibling or grandparent factors (McDonald and
Pereira, 2006; McDonald and Satta, 2007). MST-
based non-projective parsers that use higher order
factors (Martins et al., 2009; Koo et al., 2010), uti-
lize different techniques than the basic MST algo-
rithm. In addition, learning is done over a relaxation
of the problem, so the inference procedures at train-
ing and at test time are not identical.
We propose two new classes of trees between pro-
jective trees and the set of all spanning trees. These
two classes provide a closer approximation to the set
of plausible natural language dependency trees: un-
like projective trees, a word can have descendants in
more than one interval; unlike spanning trees, these
intervals cannot be nested in arbitrary ways. We in-
troduce gap inheritance, a new structural property
on trees, which provides a way to quantify the de-
gree to which these intervals can be nested. Differ-
ent levels of gap inheritance define each of these two
classes (Section 3).
The 1-Inherit class of trees (Section 4) has exactly
the same empirical coverage (Table 1) of natural lan-
guage sentences as the class of mildly non-projective
trees (Bodirsky et al., 2005), yet the optimal scoring
tree can be found in an order of magnitude less time
(Section 4.1).
Gap-minding trees (the second class) have the
</bodyText>
<page confidence="0.976072">
478
</page>
<note confidence="0.838037">
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 478–488, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.999899105263158">
property that all edges into an interval of descen-
dants come from the same node. Non-contiguous
intervals are therefore decoupled given this single
node, and thus an algorithm which uses only single
intervals (as in projective parsing) can produce trees
in which a node has descendants in multiple inter-
vals (as in mildly non-projective parsing (G´omez-
Rodr´ıguez et al., 2011)). A procedure for finding
the optimal scoring tree in this space is given in Sec-
tion 5, which can be searched in yet another order of
magnitude faster than the 1-Inherit class.
Unlike the class of spanning trees, it is still
tractable to find the optimal tree in these new spaces
when higher order factors are included. An exten-
sion which finds the optimal scoring gap-minding
tree with scores over pairs of adjacent edges (grand-
parent scoring) is given in Section 6. These gap-
minding algorithms have been implemented in prac-
tice and empirical results are presented in Section 7.
</bodyText>
<sectionHeader confidence="0.996928" genericHeader="introduction">
2 Preliminaries
</sectionHeader>
<bodyText confidence="0.971426571428571">
In this section, we review some relevant defini-
tions from previous work that characterize degrees
of non-projectivity. We also review how well
these definitions cover empirical data from six lan-
guages: Arabic, Czech, Danish, Dutch, Portuguese,
and Swedish. These are the six languages whose
CoNLL-X shared task data are either available open
source1 or from the LDC2.
A dependency tree is a rooted, directed spanning
tree that represents a set of dependencies between
words in a sentence.3 The tree has one artificial root
node and vertices that correspond to the words in an
input sentence w1, w2,...,wr,,. There is an edge from
h to m if m depends on (or modifies) h.
Definition 1. The projection of a node is the set of
words in the subtree rooted at it (including itself).
A tree is projective if, for every node in the tree,
that node’s projection forms a contiguous interval in
the input sentence order.
A tree is non-projective if the above does not hold,
i.e., there exists at least one word whose descendants
</bodyText>
<footnote confidence="0.7764566">
1http://ilk.uvt.nl/conll/free_data.html
2LDC catalogue numbers LDC2006E01 and LDC2006E02
3Trees are a reasonable assumption for most, but not all,
linguistic structures. Parasitic gaps are an example in which
a word perhaps should have multiple parents.
</footnote>
<construct confidence="0.481459">
do not form a contiguous interval.
Definition 2. A gap of a node v is a non-empty, max-
imal interval that does not contain any words in the
projection of v but lies between words that are in
</construct>
<bodyText confidence="0.929670857142857">
the projection of v. The gap degree of a node is
the number of gaps it has. The gap degree of a tree
is the maximum of the gap degrees of its vertices.
(Bodirsky et al., 2005)
Note that a projective tree will have gap degree 0.
Two subtrees interleave if there are vertices l1, r1
from one subtree and l2, r2 from the other such that
</bodyText>
<figure confidence="0.436302666666667">
l1 &lt; l2 &lt; r1 &lt; r2.
Definition 3. A tree is well-nested if no two disjoint
subtrees interleave (Bodirsky et al., 2005).
</figure>
<figureCaption confidence="0.808349">
Definition 4. A mildly non-projective tree has gap
degree at most one and is well-nested.
</figureCaption>
<bodyText confidence="0.998817285714286">
Mildly non-projective trees are of both theoret-
ical and practical interest, as they correspond to
derivations in Lexicalized Tree Adjoining Grammar
(Bodirsky et al., 2005) and cover the overwhelming
majority of sentences found in treebanks for Czech
and Danish (Kuhlmann and Nivre, 2006).
Table 1 shows the proportion of mildly non-
projective sentences for Arabic, Czech, Danish,
Dutch, Portuguese, and Swedish, ranging from
95.4% of Portuguese sentences to 99.9% of Ara-
bic sentences.4 This definition covers a substan-
tially larger set of sentences than projectivity does
— an assumption of projectivity covers only 63.6%
(Dutch) to 90.2% (Swedish) of examples (Table 1).
</bodyText>
<sectionHeader confidence="0.982655" genericHeader="method">
3 Gap Inheritance
</sectionHeader>
<bodyText confidence="0.999882555555556">
Empirically, natural language sentences seem to be
mostly mildly non-projective trees, but mildly non-
projective trees are quite expensive to parse (O(W)
(G´omez-Rodr´ıguez et al., 2011)). The parsing com-
plexity comes from the fact that the definition al-
lows two non-contiguous intervals of a projection to
be tightly coupled, with an unbounded number of
edges passing back and forth between the two inter-
vals; however, this type of structure seems unusual
</bodyText>
<footnote confidence="0.995849">
4While some of the treebank structures are ill-nested or have
a larger gap degree because of annotation decisions, some lin-
guistic constructions in German and Czech are ill-nested or
require at least two gaps under any reasonable representation
(Chen-Main and Joshi, 2010; Chen-Main and Joshi, 2012).
</footnote>
<page confidence="0.996709">
479
</page>
<table confidence="0.998954666666667">
Arabic Czech Danish Dutch Portuguese Swedish Parsing
Mildly non-proj 1458 (99.9) 72321 (99.5) 5175 (99.7) 12896 (96.6) 8650 (95.4) 10955 (99.2) O(n7)
Mild+1-Inherit 1458 (99.9) 72321 (99.5) 5175 (99.7) 12896 (96.6) 8650 (95.4) 10955 (99.2) O(ns)
Mild+0-Inherit 1394 (95.5) 70695 (97.2) 4985 (96.1) 12068 (90.4) 8481 (93.5) 10787 (97.7) O(n5)
Projective 1297 (88.8) 55872 (76.8) 4379 (84.4) 8484 (63.6) 7353 (81.1) 9963 (90.2) O(n3)
# Sentences 1460 72703 5190 13349 9071 11042
</table>
<tableCaption confidence="0.984092">
Table 1: The number of sentences from the CoNLL-X training sets whose parse trees fall into each of the above
classes. The two new classes of structures, Mild+0-Inherit and Mild+1-Inherit, have more coverage of empirical data
than projective structures, yet can be parsed faster than mildly non-projective structures. Parsing times assume an edge-
based factorization with no pruning of edges. The corresponding algorithms for Mild+1-Inherit and Mild+0-Inherit
are in Sections 4 and 5.
</tableCaption>
<bodyText confidence="0.999679869565218">
for natural language. We therefore investigate if we
can define further structural properties that are both
appropriate for describing natural language trees and
which admit more efficient parsing algorithms.
Let us first consider an example of a tree which
both has gap degree at most one and satisfies well-
nestedness, yet appears to be an unrealistic struc-
ture for a natural language syntactic tree. Consider
a tree which is rooted at node xn+2, which has one
child, node xn+1, whose projection is [x1, xn+1] U
[xn+3, x2n+2], with n children (x1, ..., xn), and each
child xi has a child at x2n_i+3. This tree is well-
nested, has gap degree 1, but all n of xn+1’s children
have edges into the other projection interval.
We introduce a further structural restriction in this
section, and show that trees satisfying our new prop-
erty can be parsed more efficiently with no drop in
empirical coverage.
Definition 5. A child is gap inheriting if its parent
has gap degree 1 and it has descendants on both
sides of its parent’s gap. The inheritance degree of
a node is the number of its children which inherit its
gap. The inheritance degree of a tree is the maximum
inheritance degree over all its nodes.
Figure 1 gives examples of trees with varying de-
grees of gap inheritance. Each projection of a node
with a gap is shown with two matching rectangles. If
a child has a projection rectangle nested inside each
of the parent’s projection rectangles, then that child
inherits the parent’s gap. Figure 1(a) shows a mildly
projective tree (with inheritance degree 2), with both
node 2 and node 11 inheriting their parent (node 3)’s
gap (note that both the dashed and dotted rectangles
each show up inside both of the solid rectangles).
Figure 1(b) shows a tree with inheritance degree 1:
there is now only one pair of rectangles (the dot-
ted ones) which show up in both of the solid ones.
Figure 1(c) shows a tree with inheritance degree 0:
while there are gaps, each set of matching rectangles
is contained within a single rectangle (projection in-
terval) of its parent, i.e., the two dashed rectangles
of node 2’s projection are contained within the left
interval of node 3; the two dotted rectangles of node
12’s projection are contained within the right inter-
val of node 3, etc.
We now ask:
</bodyText>
<listItem confidence="0.983129166666667">
1. How often does gap inheritance occur in the
parses of natural language sentences found in
treebanks?
2. Furthermore, how often are there multiple gap
inheriting children of the same node (inheri-
tance degree at least two)?
</listItem>
<bodyText confidence="0.996950125">
Table 1 shows what proportion of mildly non-
projective trees have the added property of gap in-
heritance degree 0 (Mild+0-Inherit) or have gap in-
heritance degree 1 (Mild+1-Inherit). Over all six
languages, there are no examples of multiple gap
inheritance — Mild+1-Inherit has exactly the same
empirical coverage as the unrestricted set of mildly
non-projective trees.
</bodyText>
<sectionHeader confidence="0.997634" genericHeader="method">
4 Mild+1-Inherit Trees
</sectionHeader>
<bodyText confidence="0.999991333333333">
There are some reasons from syntactic theory why
we might expect at most one child to inherit its par-
ent’s gap. Traditional Government and Binding the-
ories of syntax (Chomsky, 1981) assume that there
is an underlying projective (phrase structure) tree,
and that gaps primarily arise through movement of
</bodyText>
<page confidence="0.984151">
480
</page>
<figure confidence="0.991256774193548">
6
6
6
3
3
3
2
4
2
4
2
4
1 5
1 5
1 5
7
8
8
10
10
11 12
7
9 13
11 12
7
9 13
11 12
9 13
8
10
(a) Mildly Non-Projective: The projec-
</figure>
<figureCaption confidence="0.745594631578947">
tions (set of descendants) of both node 2
(the dashed red rectangles) and node 11
(dotted magenta) appear in both of node
3’s intervals (the solid blue rectangles).
(b) Mild+1-Inherit: Only node 2 inherits
node 3’s gap: the dashed red rectangles
appear in each of the two solid blue rect-
angles.
(c) Mild+0-Inherit: Even though node 3
has children with gaps (node 2 and node
12), neither of them inherit node 3’s gap.
There are several nodes with gaps, but
every node with a gap is properly con-
tained within just one of its parent’s in-
tervals.
Figure 1: Rectangles that match in color and style indicate the two projection intervals of a node, separated by a gap.
In all three trees, node 3’s two projection intervals are shown in the two solid blue rectangles. The number of children
which inherit its gap vary, however; in 1(a), two children have descendants within both sides; in 1(b) only one child
has descendants on both sides; in 1(c), none of its children do.
</figureCaption>
<bodyText confidence="0.931199724137931">
subtrees (constituents). One of the fundamental as-
sumptions of syntactic theory is that movement is
upward in the phrase structure tree.5
Consider one movement operation and its effect
on the gap degree of all other nodes in the tree: (a) it
should have no effect on the gap degree of the nodes
in the subtree itself, (b) it can create a gap for an an-
cestor node if it moves out of its projection interval,
and (c) it can create a gap for a non-ancestor node
if it moves in to its projection interval. Now con-
sider which cases can lead to gap inheritance: in case
(b), there is a single path from the ancestor to the
root of the subtree, so the parent of the subtree will
have no gap inheritance and any higher ancestors
will have a single child inherit the gap created by this
movement. In case (c), it is possible for there to be
multiple children that inherit this newly created gap
if multiple children had descendents on both sides.
However, the assumption of upward movement in
the phrase structure tree should rule out movement
into the projection interval of a non-ancestor. There-
fore, under these syntactic assumptions, we would
expect at most one child to inherit a parent’s gap.
5The Proper Binding Condition (Fiengo,1977) asserts that a
moved element leaves behind a trace (unpronounced element),
which must be c-commanded (Reinhart, 1976) by the corre-
sponding pronounced material in its final location. Informally,
c-commanded means that the first node is descended from the
lowest ancestor of the other that has more than one child.
</bodyText>
<subsectionHeader confidence="0.999003">
4.1 Parsing Mild+1-Inherit Trees
</subsectionHeader>
<bodyText confidence="0.999979793103448">
Finding the optimal Mild+1-Inherit tree can be done
by bottom-up constructing the tree for each node and
its descendants. We can maintain subtrees with two
intervals (two endpoints each) and one root (O(n5)
space). Consider the most complicated possible
case: a parent that has a gap, a (single) child which
inherits the gap, and additional children. An exam-
ple of this is seen with the parent node 3 in Figure
1(b).
This subtree can be constructed by first starting
with the child spanning the gap, updating its root
index to be the parent, and then expanding the inter-
val indices to the left and right to include the other
children. In each case, only one index needs to be
updated at a time, so the optimal tree can be found
in O(n6) time. In the Figure 1(b) example, the sub-
tree rooted at 3 would be built by starting with the
intervals [1, 2] U [12,13] rooted at 2, first adding the
edge from 2 to 3 (so the root is updated to 3), then
adding an edge from 3 to 4 to extend the left inter-
val to [1, 5], and then adding an edge from 3 to 11 to
extend the right interval to [8,13]. The subtree cor-
responds to the completed item [1, 5]U[8,13] rooted
at 3.
This procedure corresponds to G´omez-Rodriguez
et al. (2011)’s O(n7) algorithm for parsing mildly
non-projective structures if the most expensive step
(Combine Shrinking Gap Centre) is dropped; this
step would only ever be needed if a parent node has
</bodyText>
<page confidence="0.993905">
481
</page>
<bodyText confidence="0.999919111111111">
more than one child inheriting its gap.
This is also similar in spirit to the algorithm de-
scribed in Satta and Schuler (1998) for parsing a
restricted version of TAG, in which there are some
limitations on adjunction operations into the spines
of trees.6 That algorithm has similar steps and items,
with the root portion of the item replaced with a
node in a phrase structure tree (which may be a non-
terminal).
</bodyText>
<sectionHeader confidence="0.990734" genericHeader="method">
5 Gap-minding Trees
</sectionHeader>
<bodyText confidence="0.90651608">
The algorithm in the previous section used O(n5)
space and O(n6) time. While more efficient than
parsing in the space of mildly projective trees, this
is still probably not practically implementable. Part
of the difficulty lies in the fact that gap inheritance
causes the two non-contiguous projection intervals
to be coupled.
Definition 6. A tree is called gap-minding7 if it has
gap degree at most one, is well-nested, and has gap
inheritance degree 0.
Gap-minding trees still have good empirical cov-
erage (between 90.4% for Dutch and 97.7% for
Swedish). We now turn to the parsing of gap-
minding trees and show how a few consequences of
its definition allow us to use items ranging over only
one interval.
In Figure 1(c), notice how each rectangle has
edges incoming from exactly one node. This is not
unique to this example; all projection intervals in a
gap-minding tree have incoming edges from exactly
one node outside the interval.
Claim 1. Within a gap-minding tree, consider any
node n with a gap (i.e., n’s projection forms two
non-contiguous intervals [xi, xj] U [xk, xl]). Let p
be the parent of n.
</bodyText>
<listItem confidence="0.369231333333333">
1. For each of the intervals of n’s projection:
(a) If the interval contains n, the only edge
incoming to that interval is from p to n.
</listItem>
<footnote confidence="0.997196666666667">
6That algorithm has a running time of O(Gn5), where as
written G would likely add a factor of n2 with bilexical selec-
tional preferences; this can be lowered to n using the same tech-
nique as in Eisner and Satta (2000) for non-restricted TAG.
7The terminology is a nod to the London Underground but
imagines parents admonishing children to mind the gap.
</footnote>
<listItem confidence="0.860545">
(b) If the interval does not contain n, all edges
incoming to that interval come from n.
2. For the gap interval ([xj+1, xk_1]):
(a) If the interval contains p, then the only
edge incoming is from p’s parent top
(b) If the interval does not contain p, then all
edges incoming to that interval come from
p.
</listItem>
<bodyText confidence="0.949495111111111">
As a consequence of the above, [xi, xj] U {n} forms
a gap-minding tree rooted at n, [xk, xl] U {n}
also forms a gap-minding tree rooted at n, and
[xj+1, xk_1] U {p} forms a gap-minding tree rooted
at p.
Proof. (Part 1): Assume there was a directed edge
(x, y) such that y is inside a projection interval of n
and x is not inside the same interval, and x 7� y 7� n.
y is a descendant of n since it is contained in n’s pro-
jection. Since there is a directed edge from x to y,
x is y’s parent, and thus x must also be a descen-
dant of n and therefore in another of n’s projection
intervals. Since x and y are in different intervals,
then whichever child of n that x and y are descended
from would have inherited n’s gap, leading to a con-
tradiction.
(Part 2): First, suppose there existed a set of nodes
in n’s gap which were not descended from p. Then
p has a gap over these nodes. (p clearly has descen-
dants on each side of the gap, because all descen-
dants of n are also descendants of p). n, p’s child,
would then have descendants on both sides of p’s
gap, which would violate the property of no gap in-
heritance. It is also not possible for there to be edges
incoming from other descendants of p outside the
gap, as that would imply another child of p being
ill-nested with respect to n.
From the above, we can build gap-minding trees
using only single intervals, potentially with a sin-
gle node outside of the interval. Our objective is
to find the maximum scoring gap-minding tree, in
which the score of a tree is the sum of the scores of
its edges. Let Score(p, x) indicate the score of the
directed edge from p to x.
Therefore, the main type of sub-problems we will
use are:
</bodyText>
<page confidence="0.989045">
482
</page>
<listItem confidence="0.498902333333333">
1. C[i, j, p]: The maximum score of any gap- We can exhaustively enumerate all possibilities for
minding tree, rooted at p, with vertices [i, j] U T by considering all valid combinations of the fol-
{p} (p may or may not be within [i, j]). lowing binary cases:
</listItem>
<bodyText confidence="0.991678416666667">
This improves our space requirement, but not nec-
essarily the time requirement. For example, if we
built up the subtree in Figure 1(c) by concatenating
the three intervals [1, 5] rooted at 3, [6, 7] rooted at 6,
and [8,13] rooted at 3, and add the edge 6 —* 3, we
would still need 6 indices to describe this operation
(the four interval endpoints and the two roots), and
so we have not yet improved the running time over
the Inherit-1 case.
By part 2, we can concatenate one interval of a
child with its gap, knowing that the gap is entirely
descended from the child’s parent, and forget the
concatenation split point between the parent’s other
descendants and this side of the child. This allows us
to substitute all operations involving 6 indices with
two operations involving just 5 indices. For exam-
ple, in Figure 1(c), we could first merge [6, 7] rooted
at 6 with [8,13] rooted at 3 to create an interval
[6,13] and say that it is descended from 6, with the
rightmost side descended from its child 3. That step
required 5 indices. The following step would merge
this concatenated interval ([6,13] rooted at 6 and 3)
with [1, 5] rooted at 3. This step also requires only 5
indices.
Our helper subtype we make use of is then:
2. D[i, j, p, x, b]: The maximum score of any set
of two gap-minding trees, one rooted at p, one
rooted at x, with vertices [i, j] U {p, x} (x E�
[i, j], p may or may not be in [i, j]), such that
for some k, vertices [i, k] are in the tree rooted
at p if b = true (and at x if b = false), and
vertices [k + 1, j] are in the tree rooted at x (p).
Consider an optimum scoring gap-minding tree T
rooted at p with vertices V = [i, j] U {p} and edges
E, where E =� 0. The form of the dynamic program
may depend on whether:
</bodyText>
<listItem confidence="0.927932625">
• p is within (i, j) (I) or external to [i, j] (E)8
8In the discussion we will assume that p =� i and p =� j,
since any optimum solution with V = [i, j] U {i} and a root
at i will be equivalent to V = [i + 1, j] U {i} rooted at i (and
similarly for p = j).
• p has a single child (S) or multiple children (M)
• i and j are descended from the same child of p
(C) or different children of p (D)
</listItem>
<bodyText confidence="0.9456616">
Note that case (S/D) is not possible: i and j cannot
be descended from different children of p if p has
only a single child. We therefore need to find the
maximum scoring tree over the three cases of S/C,
M/C, and M/D.
</bodyText>
<figureCaption confidence="0.62594925">
Claim 2. Let T be the optimum scoring gap-
minding tree rooted at p with vertices V = [i, j] U
{p}. Then T and its score are derived from one of
the following:
</figureCaption>
<bodyText confidence="0.848584214285714">
S/C If p has a single child x in T, then if p E (i, j)
(I), T’s score is Score(p, x)+C[i, p−1, x]+
C[p + 1, j, x]; if p E� [i, j] (E), T’s score is
Score(p, x) + C[i, j, x].
M/C If p has multiple children in T and i and j
are descended from the same child x in T, then
there is a split point k such that T’s score is:
Score(p, x)+C[i, k, x]+D[k + 1, j, p, x, T]
if x is on the left side of its own gap, and
T’s score is: Score(p, x) + C[k, j, x] +
D[i, k − 1, p, x, F] if x is on the right side.
M/D If p has multiple children in T and i and j
are descended from different children in T, then
there is a split point k such that T’s score is
C[i, k, p] + C[k + 1, j, p].
T has the maximum score over each of the above
cases, for all valid choices of x and k.
Proof. Case S/C: If p has exactly one child x,
then the tree can be decomposed into the edge
from p to x and the subtree rooted at x. If p
is outside the interval, then the maximum scor-
ing such tree is clearly Score(p, x) + C[i, j, x].
If p is inside, then x has a gap across p, and
so using Claim 1, the maximum scoring tree
rooted at p with a single child x has score of
Score(p, x) + C[i, p − 1,x] + C[p + 1, j, x].
Case M/C: If there are multiple children and the
endpoints are descended from the same child x, then
</bodyText>
<page confidence="0.99869">
483
</page>
<bodyText confidence="0.998631463414634">
the child x has to have gap degree 1. x itself is on
either the left or right side of its gap. For the mo-
ment, assume x is in the left interval. By Claim 1,
we can split up the score of the tree as the score of
the edge from p to x (Score(p, x)), the score of the
subtree corresponding to the projection of x to the
left of its gap (C[i, k, x]), and the score of the sub-
trees rooted at p with its remaining children and the
subtree rooted at x corresponding to the right side
of x’s projection (D[k + 1, j, p, x, T]). The case in
which x is on the right side of its gap is symmetric.
Case M/D: If there are multiple children and the
endpoints are descended from different children of
p, then there must exist a split point k that parti-
tions the children of p into two non-empty sets, such
that each child’s projection is either entirely on the
left or entirely on the right of the split point. We
show one such split point to demonstrate that there
always exists at least one. Let x be the child of p
that i is descended from, and let xl and xr be x’s
leftmost and right descendants, respectively.9 Con-
sider all the children of p (whose projections taken
together partition [i, j] − {p}). No child can have
descendants both to the left of xr and to the right
of xr, because otherwise that child and x would be
ill-nested. Therefore we can split up the interval at
xr to have two gap-minding trees, both rooted at p.
The score of T is then the sum of the scores of the
best subtree rooted at p over [i, k] (C[i, k, p]) and
the score of the best subtree rooted at p over [k+1, j]
(C[k + 1,j, p]).
The above cases cover all non-empty gap-
minding trees, so the maximum will be found.
Using Claim 2 to Devise an Algorithm The above
claim showed that any problem of type C can be
decomposed into subproblems of types C and D.
From the definition of D, any problem of type D can
clearly be decomposed into two problems of type C
— simply split the interval at the split point known
to exist and assign p or x as the roots for each side
of the interval, as prescribed by the boolean b:
</bodyText>
<construct confidence="0.749159">
D(i, j, p, x, T) = maxkC[i, k, p] + C[k + 1,j, x]
D(i, j, p, x, F) = maxkC[i, k, x] + C[k + 1, j, p]
</construct>
<bodyText confidence="0.991741466666667">
9Note that x, = i by construction, and x,. =� j (because the
endpoints are descended from different children).
Algorithm 1 makes direct use of the above claims.
Note that in every gap-minding tree referred to
in the cases above, all vertices that were not the
root formed a single interval. Algorithm 1 builds
up trees in increasing sizes of [i, j] U {p}. The
tree in C[i, j, p] corresponds to the maximum of
four subroutines: SingleChild (S/C), EndpointsDiff
(M/D), EndsFromLeftChild (M/C), and EndsFrom-
RightChild (M/C). The D subproblems are filled in
with the subroutine Max2Subtrees, which uses the
above discussion. The maximum score of any gap-
minding tree is then found in C[1, n, 0], and the tree
itself can be found using backpointers.
</bodyText>
<subsectionHeader confidence="0.998683">
5.1 Runtime analysis
</subsectionHeader>
<bodyText confidence="0.999747416666667">
If the input is assumed to be the complete graph (any
word can have any other word as its parent), then
the above algorithm takes O(n5) time. The most
expensive steps are M/C, which take O(n2) time to
fill in each of the O(n3) C cells. and solving a D
subproblem, which takes O(n) time on each of the
O(n4) possible such problems.
Pruning: In practice, the set of edges considered
(m) is not necessarily O(n2). Many edges can be
ruled out beforehand, either based on the distance
in the sentence between the two words (Eisner and
Smith, 2010), the predictions of a local ranker (Mar-
tins et al., 2009), or the marginals computed from a
simpler parsing model (Carreras et al., 2008).
If we choose a pruning strategy such that each
word has at most k potential parents (incoming
edges), then the running time drops to O(kn4). The
five indices in an M/C step were: i, j, k, p, and x.
As there must be an edge from p to x, and x only has
k possible parents, there are now only O(kn4) valid
such combinations. Similarly, each D subproblem
(which ranges over i, j, k, p, x) may only come into
existence because of an edge from p to x, so again
the runtime of these such steps drops to O(kn4).
</bodyText>
<sectionHeader confidence="0.9553915" genericHeader="method">
6 Extension to Grandparent
Factorizations
</sectionHeader>
<bodyText confidence="0.999731">
The ability to define slightly non-local features has
been shown to improve parsing performance. In this
section, we assume a grandparent-factored model,
where the score of a tree is now the sum over scores
of (g, p, c) triples, where (g, p) and (p, c) are both
</bodyText>
<page confidence="0.997857">
484
</page>
<bodyText confidence="0.998340344827586">
directed edges in the tree. Let Score(g, p, c) indi-
cate the score of this grandparent-parent-child triple.
We now show how to extend the above algorithm
to find the maximum scoring gap-minding tree with
grandparent scoring.
Our two subproblems are now C[i, j, p, g] and
D[i, j, p, x, b, g]; each subproblem has been aug-
mented with an additional grandparent index g,
which has the meaning that g is p’s parent. Note that
g must be outside of the interval [i, j] (if it were not,
a cycle would be introduced). Edge scores are now
computed over (g, p, x) triples. In particular, claim
2 is modified:
Claim 3. Let T be the optimum scoring gap-
minding tree rooted at p with vertices V = [i, j] ∪
{p}, where p ∈ (i, j) (I), with a grandparent index
g (g ∈� V ). Then T and its score are derived from
one of the following:
S/C If p has a single child x in T, then if
p ∈ (i, j) (I), T’s score is Score(g, p, x) +
C[i,p−1, x, p]+C[p + 1, j, x, p]; ifp ∈� [i, j]
(E), T’s score is Score(g, p, x)+C[i, j, x, p].
M/C If p has multiple children in T and i
and j are descended from the same child
x in T, then there is a split point k
such that T’s score is: Score(g, p, x) +
C[i, k, x, p] + D[k + 1, j, p, x, T, g] if x is
on the left side of its own gap, and T’s
score is: Score(g, p, x) + C[k, j, x, p] +
D[i, k − 1, p, x, F, g] if x is on the right side.
M/D If p has multiple children in T and i and j
are descended from different children in T, then
there is a split point k such that T’s score is
C[i, k, p, g] + C[k + 1, j, p, g].
T has the maximum score over each of the above
cases, for all valid choices of x and k.
Note that for subproblems rooted at p, g is the
grandparent index, while for subproblems rooted at
x, p is the updated grandparent index. The D sub-
problems with the grandparent index are shown be-
low:
D(i, j, p, x, T, g) = maxkC[i, k, p, g] + C[k + 1,j, x, p]
D(i,j, p, x, F, g) = maxkC[i, k, x, p] + C[k + 1,j, p, g]
We have added another index which ranges over
n, so without pruning, we have now increased the
running time to O(n6). However, every step now in-
cludes both a g and a p (and often an x), so there is
at least one implied edge in every step. If pruning
is done in such a way that each word has at most k
parents, then each word’s set of grandparent and par-
ent possibilities is at most k2. To run all of the S/C
steps, we therefore need O(k2n3) time; for all of the
M/C steps, O(k2n4) time; for all of the M/D steps,
O(kn4); for all of the D subproblems, O(k2n4). The
overall running time is therefore O(k2n4), and we
have shown that when edges are sufficiently pruned,
grandparent factors add only an extra factor of k, and
not a full extra factor of n.
</bodyText>
<sectionHeader confidence="0.999155" genericHeader="evaluation">
7 Experiments
</sectionHeader>
<bodyText confidence="0.999988137931034">
The space of projective trees is strictly contained
within the space of gap-minding trees which is
strictly contained within spanning trees. Which
space is most appropriate for natural language pars-
ing may depend on the particular language and the
type and frequencies of non-projective structures
found in it. In this section we compare the parsing
accuracy across languages for a parser which uses
either the Eisner algorithm (projective), MST (span-
ning trees), or MaxGapMindingTree (gap-minding
trees) as its decoder for both training and inference.
We implemented both the basic gap-minding al-
gorithm and the gap-minding algorithm with grand-
parent scoring as extensions to MSTParser10. MST-
Parser (McDonald et al., 2005b; McDonald et al.,
2005a) uses the Margin Infused Relaxed Algo-
rithm (Crammer and Singer, 2003) for discrimina-
tive training. Training requires a decoder which
produces the highest scoring tree (in the space of
valid trees) under the current model weights. This
same decoder is then used to produce parses at test
time. MSTParser comes packaged with the Eis-
ner algorithm (for projective trees) and MST (for
spanning trees). MSTParser also includes two sec-
ond order models: one of which is a projective de-
coder that also scores siblings (Proj+Sib) and the
other of which produces non-projective trees by re-
arranging edges after producing a projective tree
(Proj+Sib+Rearr). We add a further decoder with
</bodyText>
<footnote confidence="0.887791">
10http://sourceforge.net/projects/mstparser/
</footnote>
<page confidence="0.998382">
485
</page>
<bodyText confidence="0.987491734693878">
the algorithm presented here for gap minding trees,
and plan to make the extension publicly available.
The gap-minding decoder has both an edge-factored
implementation and a version which scores grand-
parents as well.11
The gap-minding algorithm is much more effi-
cient when edges have been pruned so that each
word has at most k potential parents. We use the
weights from the trained MST models combined
with the Matrix Tree Theorem (Smith and Smith,
2007; Koo et al., 2007; McDonald and Satta, 2007)
to produce marginal probabilities of each edge. We
wanted to be able to both achieve the running time
bound and yet take advantage of the fact that the
size of the set of reasonable parent choices is vari-
able. We therefore use a hybrid pruning strategy:
each word’s set of potential parents is the smaller of
a) the top k parents (we chose k = 10) or b) the set
of parents whose probabilities are above a thresh-
old (we chose th = .001). The running time for
the gap-minding algorithm is then O(kn4); with the
grandparent features the gap-minding running time
is O(k2n4).
The training and test sets for the six languages
come from the CoNLL-X shared task.12 We train
the gap-minding algorithm on sentences of length
at most 10013 (the vast majority of sentences). The
projective and MST models are trained on all sen-
tences and are run without any pruning. The Czech
training set is much larger than the others and so for
Czech only the first 10,000 training sentences were
used. Testing is on the full test set, with no length
restrictions.
The results are shown in Table 2. The first three
lines show the first order gap-minding decoder com-
pared with the first order projective and MST de-
11The grandparent features used were identical to the fea-
tures provided within MSTParser for the second-order sibling
parsers, with one exception — many features are conjoined with
a direction indicator, which in the projective case has only two
possibilities. We replaced this two-way distinction with a six-
way distinction of the six possible orders of the grandparent,
parent, and child.
12MSTParser produces labeled dependencies on CoNLL for-
matted input. We replace all labels in the training set with a
single dummy label to produce unlabeled dependency trees.
13Because of long training times, the gap-minding with
grandparent models for Portuguese and Swedish were trained
on only sentences up to 50 words.
</bodyText>
<table confidence="0.989592428571428">
Ar Cz Da Du Pt Sw
Proj. 78.0 80.0 88.2 79.8 87.4 86.9
MST 78.0 80.4 88.1 84.6 86.7 86.2
Gap-Mind 77.6 80.8 88.6 83.9 86.8 86.0
Proj+Sib 78.2 80.0 88.9 81.1 87.5 88.1
+Rearr 78.5 81.3 89.3 85.4 88.2 87.7
GM+Grand 78.3 82.1 89.1 84.6 87.7 88.5
</table>
<tableCaption confidence="0.998697">
Table 2: Unlabeled Attachment Scores on the CoNLL-X
shared task test set.
</tableCaption>
<bodyText confidence="0.999972428571429">
coders. The gap-minding decoder does better than
the projective decoder on Czech, Danish, and Dutch,
the three languages with the most non-projectivity,
even though it was at a competitive disadvantage in
terms of both pruning and (on languages with very
long sentences) training data. The gap-minding de-
coder with grandparent features is better than the
projective decoder with sibling features on all six
of the languages. On some languages, the local
search decoder with siblings has the absolute high-
est accuracy in Table 2; on other languages (Czech
and Swedish) the gap-minding+grandparents has the
highest accuracy. While not directly comparable be-
cause of the difference in features, the promising
performance of the gap-minding+grandparents de-
coder shows that the space of gap-minding trees is
larger than the space of projective trees, yet unlike
spanning trees, it is tractable to find the best tree with
higher order features. It would be interesting to ex-
tend the gap-minding algorithm to include siblings
as well.
</bodyText>
<sectionHeader confidence="0.99885" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.99998">
Gap inheritance, a structural property on trees, has
implications both for natural language syntax and
for natural language parsing. We have shown that
the mildly non-projective trees present in natural
language treebanks all have zero or one children in-
herit each parent’s gap. We also showed that the as-
sumption of 1 gap inheritance removes a factor of
n from parsing time, and the further assumption of
0 gap inheritance removes yet another factor of n.
The space of gap-minding trees provides a closer fit
to naturally occurring linguistic structures than the
space of projective trees, and unlike spanning trees,
the inclusion of higher order factors does not sub-
stantially increase the difficulty of finding the maxi-
mum scoring tree in that space.
</bodyText>
<page confidence="0.998627">
486
</page>
<sectionHeader confidence="0.999214" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99645325">
We would like to thank Aravind Joshi for comments
on an earlier draft. This material is based upon
work supported under a National Science Founda-
tion Graduate Research Fellowship.
</bodyText>
<figure confidence="0.800338826086957">
Algorithm 1: MaxGapMindingTree
Init: diE[1,�]C[i, i, i] = 0
for size = 0 to n − 1 do
for i = 1 to n − size do
j = i + size
/* Endpoint parents */
if size &gt; 0 then
C[i, j, i] = C[i + 1, j, i]
C[i, j, j] = C[i, j − 1, j]
/* Interior parents */
for p = i + 1 to j − 1 do
C[i, j, p] = max (SingleChild(i,j,p),
EndpointsDiff(i,j,p),
EndsFromLeftChild(i,j,p),
EndsFromRightChild(i,j,p))
/* Exterior parents */
forall the p E [0, i − 1] U [j + 1, n] do
C[i, j, p] = max (SingleChild(i,j,p),
EndpointsDiff(i,j,p),
EndsFromLeftChild(i,j,p),
EndsFromRightChild(i,j,p))
/* Helper subproblems */
for p E [0, n] do
</figure>
<construct confidence="0.929577833333333">
forall the x E PosChild[p] n x E� [i, j] do
if p =� j then
D[i, j, p, x, T] = Max2Subtrees(i, j, p, x, T)
if p =� i then
D[i, j, p, x, F] = Max2Subtrees(i, j, p, x, F)
Final answer: C[1, n, 0]
</construct>
<equation confidence="0.936942163265306">
Function SingleChild(i,j,p)
X = PosChild[p] n [i, j]
/* Interior p */
ifp&gt;inp&lt;jthen
return maxxEX C[i, p − 1, x]
+C[p + 1, j, x] + Score(p, x)
/* Exterior p */
else
return maxxEX C[i, j, x] + Score(p, x)
Function EndpointsDiff(i,j,p)
return maxkE[i,j−1] C[i, k, p] + C[k + 1, j, p]
Function EndsFromLeftChild(i,j,p)
/* Interior p */
if p &gt; i A p &lt; j then
X = PosChild[p] n [i, p − 1]
forall the x E X n x &lt; p do
K[x] = [x, p − 1]
/* Exterior p */
else
X = PosChild[p] n [i, j]
forall the x E X do
K = [x, j − 2]
return maxxEX,kEK[x] C[i, k, x]
+Score(p, x) + D[k + 1, j, p, x, T]
Function EndsFromRightChild(i,j,p)
/* Interior p */
if p &gt; i A p &lt; j then
X = PosChild[p] n [p + 1, j]
forall the x E X n x &gt; p do
K[x] = [p + 1, x]
/* Exterior p */
else
X = PosChild[p] n [i, j]
forall the x E X do
K[x] = [i + 2, x]
return maxxEX,kEK[x] C[k, j, x]
+Score(p, x) + D[i, k − 1, p, x, F]
Function Max2Subtrees(i,j,p,x,pOnLeft)
/* Interior p */
if p &gt; i A p &lt; j then
if pOnLeft then
K = [p, j − 1]
return maxkEK C[i, k, p] + C[k + 1, j, x]
else
K = [i, p − 1]
return maxkEK C[i, k, x] + C[k + 1, j, p]
/* Exterior p */
else
K = [i, j − 1]}
</equation>
<reference confidence="0.5590185">
if pOnLeft then
return maxkEK C[i, k, p] + C[k + 1, j, x]
else
return maxkEK C[i, k, x] + C[k + 1, j, p]
</reference>
<page confidence="0.99913">
487
</page>
<sectionHeader confidence="0.998335" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999796459770115">
M. Bodirsky, M. Kuhlmann, and M. M¨ohl. 2005. Well-
nested drawings as models of syntactic structure. In
In Tenth Conference on Formal Grammar and Ninth
Meeting on Mathematics of Language, pages 88–1.
University Press.
X. Carreras, M. Collins, and T. Koo. 2008. Tag, dynamic
programming, and the perceptron for efficient, feature-
rich parsing. In Proceedings of CoNLL, pages 9–16.
Association for Computational Linguistics.
X. Carreras. 2007. Experiments with a higher-order
projective dependency parser. In Proceedings of the
CoNLL Shared Task Session of EMNLP-CoNLL, vol-
ume 7, pages 957–961.
J. Chen-Main and A. Joshi. 2010. Unavoidable ill-
nestedness in natural language and the adequacy of
tree local-mctag induced dependency structures. In
Proceedings of the Tenth International Workshop on
Tree Adjoining Grammar and Related Formalisms
(TAG+ 10).
J. Chen-Main and A.K. Joshi. 2012. A depen-
dency perspective on the adequacy of tree local multi-
component tree adjoining grammar. In Journal of
Logic and Computation. (to appear).
N. Chomsky. 1981. Lectures on Government and Bind-
ing. Dordrecht: Foris.
K. Crammer and Y. Singer. 2003. Ultraconservative on-
line algorithms for multiclass problems. Journal of
Machine Learning Research, 3:951–991, March.
J. Eisner and G. Satta. 2000. A faster parsing algorithm
for lexicalized tree-adjoining grammars. In Proceed-
ings of the 5th Workshop on Tree-Adjoining Grammars
and Related Formalisms (TAG+5), pages 14–19.
J. Eisner and N.A. Smith. 2010. Favor short dependen-
cies: Parsing with soft and hard constraints on depen-
dency length. In Harry Bunt, Paola Merlo, and Joakim
Nivre, editors, Trends in Parsing Technology: Depen-
dency Parsing, Domain Adaptation, and Deep Parsing,
chapter 8, pages 121–150. Springer.
J. Eisner. 2000. Bilexical grammars and their cubic-
time parsing algorithms. In Harry Bunt and Anton
Nijholt, editors, Advances in Probabilistic and Other
Parsing Technologies, pages 29–62. Kluwer Academic
Publishers, October.
R. Fiengo. 1977. On trace theory. Linguistic Inquiry,
8(1):35–61.
C. G´omez-Rodr´ıguez, J. Carroll, and D. Weir. 2011. De-
pendency parsing schemata and mildly non-projective
dependency parsing. Computational Linguistics,
37(3):541–586.
T. Koo and M. Collins. 2010. Efficient third-order de-
pendency parsers. In Proceedings ofACL, pages 1–11.
T. Koo, A. Globerson, X. Carreras, and M. Collins. 2007.
Structured prediction models via the matrix-tree theo-
rem. In Proceedings of EMNLP-CoNLL.
T. Koo, A.M. Rush, M. Collins, T. Jaakkola, and D. Son-
tag. 2010. Dual decomposition for parsing with non-
projective head automata. In Proceedings of EMNLP,
pages 1288–1298.
M. Kuhlmann and J. Nivre. 2006. Mildly non-
projective dependency structures. In Proceedings of
COLING/ACL, pages 507–514.
A.F.T. Martins, N.A. Smith, and E.P. Xing. 2009. Con-
cise integer linear programming formulations for de-
pendency parsing. In Proceedings ofACL, pages 342–
350.
R. McDonald and F. Pereira. 2006. Online learning of
approximate dependency parsing algorithms. In Pro-
ceedings of EACL, pages 81–88.
R. McDonald and G. Satta. 2007. On the complexity
of non-projective data-driven dependency parsing. In
Proceedings of the 10th International Conference on
Parsing Technologies, pages 121–132.
R. McDonald, K. Crammer, and F. Pereira. 2005a. On-
line large-margin training of dependency parsers. In
Proceedings of ACL, pages 91–98.
R. McDonald, F. Pereira, K. Ribarov, and J. Hajiˇc. 2005b.
Non-projective dependency parsing using spanning
tree algorithms. In Proceedings of HLT-EMNLP,
pages 523–530.
T. Reinhart. 1976. The Syntactic Domain of Anaphora.
Ph.D. thesis, Massachusetts Institute of Technology.
G. Satta and W. Schuler. 1998. Restrictions on tree ad-
joining languages. In Proceedings of COLING-ACL,
pages 1176–1182.
D.A. Smith and N.A. Smith. 2007. Probabilistic models
of nonprojective dependency trees. In Proceedings of
EMNLP-CoNLL.
</reference>
<page confidence="0.99798">
488
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.789930">
<title confidence="0.999966">Dynamic Programming for Higher Order Parsing of Gap-Minding Trees</title>
<author confidence="0.99901">Emily Pitler</author>
<author confidence="0.99901">Sampath Kannan</author>
<author confidence="0.99901">Mitchell</author>
<affiliation confidence="0.996054">Computer and Information University of</affiliation>
<address confidence="0.811345">Philadelphia, PA</address>
<email confidence="0.999687">epitler,kannan,mitch@seas.upenn.edu</email>
<abstract confidence="0.999215739130435">introduce a new structural property on trees, which provides a way to quantify the degree to which intervals of descendants can be nested. Based on this property, two new classes of trees are derived that provide a closer approximation to the set of plausible natural language dependency trees than some alternative classes of trees: unlike projective trees, a word can have descendants in more than one interval; unlike spanning trees, these intervals cannot be nested in arways. The of trees has same empirical coverage of natural language sentences as the class of mildly nonprojective trees, yet the optimal scoring tree can be found in an order of magnitude less trees second class) have the property that all edges into an interval of descendants come from the same node, and thus an algorithm which uses only single intervals can produce trees in which a node has descendants in multiple intervals.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<title>if pOnLeft then return maxkEK C[i, k,</title>
<journal>p] + C[k +</journal>
<volume>1</volume>
<note>j, x] else return maxkEK C[i, k, x] + C[k + 1, j, p]</note>
<marker></marker>
<rawString>if pOnLeft then return maxkEK C[i, k, p] + C[k + 1, j, x] else return maxkEK C[i, k, x] + C[k + 1, j, p]</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Bodirsky</author>
<author>M Kuhlmann</author>
<author>M M¨ohl</author>
</authors>
<title>Wellnested drawings as models of syntactic structure.</title>
<date>2005</date>
<booktitle>In In Tenth Conference on Formal Grammar and Ninth Meeting on Mathematics of Language,</booktitle>
<pages>88--1</pages>
<publisher>University Press.</publisher>
<marker>Bodirsky, Kuhlmann, M¨ohl, 2005</marker>
<rawString>M. Bodirsky, M. Kuhlmann, and M. M¨ohl. 2005. Wellnested drawings as models of syntactic structure. In In Tenth Conference on Formal Grammar and Ninth Meeting on Mathematics of Language, pages 88–1. University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Carreras</author>
<author>M Collins</author>
<author>T Koo</author>
</authors>
<title>Tag, dynamic programming, and the perceptron for efficient, featurerich parsing.</title>
<date>2008</date>
<booktitle>In Proceedings of CoNLL,</booktitle>
<pages>9--16</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="28202" citStr="Carreras et al., 2008" startWordPosition="5091" endWordPosition="5094"> can have any other word as its parent), then the above algorithm takes O(n5) time. The most expensive steps are M/C, which take O(n2) time to fill in each of the O(n3) C cells. and solving a D subproblem, which takes O(n) time on each of the O(n4) possible such problems. Pruning: In practice, the set of edges considered (m) is not necessarily O(n2). Many edges can be ruled out beforehand, either based on the distance in the sentence between the two words (Eisner and Smith, 2010), the predictions of a local ranker (Martins et al., 2009), or the marginals computed from a simpler parsing model (Carreras et al., 2008). If we choose a pruning strategy such that each word has at most k potential parents (incoming edges), then the running time drops to O(kn4). The five indices in an M/C step were: i, j, k, p, and x. As there must be an edge from p to x, and x only has k possible parents, there are now only O(kn4) valid such combinations. Similarly, each D subproblem (which ranges over i, j, k, p, x) may only come into existence because of an edge from p to x, so again the runtime of these such steps drops to O(kn4). 6 Extension to Grandparent Factorizations The ability to define slightly non-local features ha</context>
</contexts>
<marker>Carreras, Collins, Koo, 2008</marker>
<rawString>X. Carreras, M. Collins, and T. Koo. 2008. Tag, dynamic programming, and the perceptron for efficient, featurerich parsing. In Proceedings of CoNLL, pages 9–16. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Carreras</author>
</authors>
<title>Experiments with a higher-order projective dependency parser.</title>
<date>2007</date>
<booktitle>In Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL,</booktitle>
<volume>7</volume>
<pages>957--961</pages>
<contexts>
<context position="1689" citStr="Carreras, 2007" startWordPosition="272" endWordPosition="273">e from the same node, and thus an algorithm which uses only single intervals can produce trees in which a node has descendants in multiple intervals. 1 Introduction Dependency parsers vary in what space of possible tree structures they search over when parsing a sentence. One commonly used space is the set of projective trees, in which every node’s descendants form a contiguous interval in the input sentence. Finding the optimal tree in the set of projective trees can be done efficiently (Eisner, 2000), even when the score of a tree depends on higher order factors (McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010). However, the projectivity assumption is too strict for all natural language dependency trees; for example, only 63.6% of Dutch sentences from the CoNLL-X training set are projective (Table 1). At the other end of the spectrum, some parsers search over all spanning trees, a class of structures much larger than the set of plausible linguistic structures. The maximum scoring directed spanning tree can be found efficiently when the score of a tree depends only on edge-based factors (McDonald et al., 2005b). However, it is NP-hard to extend MST to include sibling or grandp</context>
</contexts>
<marker>Carreras, 2007</marker>
<rawString>X. Carreras. 2007. Experiments with a higher-order projective dependency parser. In Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL, volume 7, pages 957–961.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Chen-Main</author>
<author>A Joshi</author>
</authors>
<title>Unavoidable illnestedness in natural language and the adequacy of tree local-mctag induced dependency structures.</title>
<date>2010</date>
<booktitle>In Proceedings of the Tenth International Workshop on Tree Adjoining Grammar and Related Formalisms (TAG+ 10).</booktitle>
<contexts>
<context position="8218" citStr="Chen-Main and Joshi, 2010" startWordPosition="1350" endWordPosition="1353">rojective trees are quite expensive to parse (O(W) (G´omez-Rodr´ıguez et al., 2011)). The parsing complexity comes from the fact that the definition allows two non-contiguous intervals of a projection to be tightly coupled, with an unbounded number of edges passing back and forth between the two intervals; however, this type of structure seems unusual 4While some of the treebank structures are ill-nested or have a larger gap degree because of annotation decisions, some linguistic constructions in German and Czech are ill-nested or require at least two gaps under any reasonable representation (Chen-Main and Joshi, 2010; Chen-Main and Joshi, 2012). 479 Arabic Czech Danish Dutch Portuguese Swedish Parsing Mildly non-proj 1458 (99.9) 72321 (99.5) 5175 (99.7) 12896 (96.6) 8650 (95.4) 10955 (99.2) O(n7) Mild+1-Inherit 1458 (99.9) 72321 (99.5) 5175 (99.7) 12896 (96.6) 8650 (95.4) 10955 (99.2) O(ns) Mild+0-Inherit 1394 (95.5) 70695 (97.2) 4985 (96.1) 12068 (90.4) 8481 (93.5) 10787 (97.7) O(n5) Projective 1297 (88.8) 55872 (76.8) 4379 (84.4) 8484 (63.6) 7353 (81.1) 9963 (90.2) O(n3) # Sentences 1460 72703 5190 13349 9071 11042 Table 1: The number of sentences from the CoNLL-X training sets whose parse trees fall in</context>
</contexts>
<marker>Chen-Main, Joshi, 2010</marker>
<rawString>J. Chen-Main and A. Joshi. 2010. Unavoidable illnestedness in natural language and the adequacy of tree local-mctag induced dependency structures. In Proceedings of the Tenth International Workshop on Tree Adjoining Grammar and Related Formalisms (TAG+ 10).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Chen-Main</author>
<author>A K Joshi</author>
</authors>
<title>A dependency perspective on the adequacy of tree local multicomponent tree adjoining grammar.</title>
<date>2012</date>
<booktitle>In Journal of Logic and Computation.</booktitle>
<note>(to appear).</note>
<contexts>
<context position="8246" citStr="Chen-Main and Joshi, 2012" startWordPosition="1354" endWordPosition="1357">xpensive to parse (O(W) (G´omez-Rodr´ıguez et al., 2011)). The parsing complexity comes from the fact that the definition allows two non-contiguous intervals of a projection to be tightly coupled, with an unbounded number of edges passing back and forth between the two intervals; however, this type of structure seems unusual 4While some of the treebank structures are ill-nested or have a larger gap degree because of annotation decisions, some linguistic constructions in German and Czech are ill-nested or require at least two gaps under any reasonable representation (Chen-Main and Joshi, 2010; Chen-Main and Joshi, 2012). 479 Arabic Czech Danish Dutch Portuguese Swedish Parsing Mildly non-proj 1458 (99.9) 72321 (99.5) 5175 (99.7) 12896 (96.6) 8650 (95.4) 10955 (99.2) O(n7) Mild+1-Inherit 1458 (99.9) 72321 (99.5) 5175 (99.7) 12896 (96.6) 8650 (95.4) 10955 (99.2) O(ns) Mild+0-Inherit 1394 (95.5) 70695 (97.2) 4985 (96.1) 12068 (90.4) 8481 (93.5) 10787 (97.7) O(n5) Projective 1297 (88.8) 55872 (76.8) 4379 (84.4) 8484 (63.6) 7353 (81.1) 9963 (90.2) O(n3) # Sentences 1460 72703 5190 13349 9071 11042 Table 1: The number of sentences from the CoNLL-X training sets whose parse trees fall into each of the above classes</context>
</contexts>
<marker>Chen-Main, Joshi, 2012</marker>
<rawString>J. Chen-Main and A.K. Joshi. 2012. A dependency perspective on the adequacy of tree local multicomponent tree adjoining grammar. In Journal of Logic and Computation. (to appear).</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Chomsky</author>
</authors>
<date>1981</date>
<booktitle>Lectures on Government and Binding.</booktitle>
<location>Dordrecht: Foris.</location>
<contexts>
<context position="12295" citStr="Chomsky, 1981" startWordPosition="2029" endWordPosition="2030">me node (inheritance degree at least two)? Table 1 shows what proportion of mildly nonprojective trees have the added property of gap inheritance degree 0 (Mild+0-Inherit) or have gap inheritance degree 1 (Mild+1-Inherit). Over all six languages, there are no examples of multiple gap inheritance — Mild+1-Inherit has exactly the same empirical coverage as the unrestricted set of mildly non-projective trees. 4 Mild+1-Inherit Trees There are some reasons from syntactic theory why we might expect at most one child to inherit its parent’s gap. Traditional Government and Binding theories of syntax (Chomsky, 1981) assume that there is an underlying projective (phrase structure) tree, and that gaps primarily arise through movement of 480 6 6 6 3 3 3 2 4 2 4 2 4 1 5 1 5 1 5 7 8 8 10 10 11 12 7 9 13 11 12 7 9 13 11 12 9 13 8 10 (a) Mildly Non-Projective: The projections (set of descendants) of both node 2 (the dashed red rectangles) and node 11 (dotted magenta) appear in both of node 3’s intervals (the solid blue rectangles). (b) Mild+1-Inherit: Only node 2 inherits node 3’s gap: the dashed red rectangles appear in each of the two solid blue rectangles. (c) Mild+0-Inherit: Even though node 3 has children </context>
</contexts>
<marker>Chomsky, 1981</marker>
<rawString>N. Chomsky. 1981. Lectures on Government and Binding. Dordrecht: Foris.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Crammer</author>
<author>Y Singer</author>
</authors>
<title>Ultraconservative online algorithms for multiclass problems.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>3</volume>
<contexts>
<context position="32528" citStr="Crammer and Singer, 2003" startWordPosition="5940" endWordPosition="5943">e parsing may depend on the particular language and the type and frequencies of non-projective structures found in it. In this section we compare the parsing accuracy across languages for a parser which uses either the Eisner algorithm (projective), MST (spanning trees), or MaxGapMindingTree (gap-minding trees) as its decoder for both training and inference. We implemented both the basic gap-minding algorithm and the gap-minding algorithm with grandparent scoring as extensions to MSTParser10. MSTParser (McDonald et al., 2005b; McDonald et al., 2005a) uses the Margin Infused Relaxed Algorithm (Crammer and Singer, 2003) for discriminative training. Training requires a decoder which produces the highest scoring tree (in the space of valid trees) under the current model weights. This same decoder is then used to produce parses at test time. MSTParser comes packaged with the Eisner algorithm (for projective trees) and MST (for spanning trees). MSTParser also includes two second order models: one of which is a projective decoder that also scores siblings (Proj+Sib) and the other of which produces non-projective trees by rearranging edges after producing a projective tree (Proj+Sib+Rearr). We add a further decode</context>
</contexts>
<marker>Crammer, Singer, 2003</marker>
<rawString>K. Crammer and Y. Singer. 2003. Ultraconservative online algorithms for multiclass problems. Journal of Machine Learning Research, 3:951–991, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Eisner</author>
<author>G Satta</author>
</authors>
<title>A faster parsing algorithm for lexicalized tree-adjoining grammars.</title>
<date>2000</date>
<booktitle>In Proceedings of the 5th Workshop on Tree-Adjoining Grammars and Related Formalisms (TAG+5),</booktitle>
<pages>14--19</pages>
<contexts>
<context position="18401" citStr="Eisner and Satta (2000)" startWordPosition="3125" endWordPosition="3128">on intervals in a gap-minding tree have incoming edges from exactly one node outside the interval. Claim 1. Within a gap-minding tree, consider any node n with a gap (i.e., n’s projection forms two non-contiguous intervals [xi, xj] U [xk, xl]). Let p be the parent of n. 1. For each of the intervals of n’s projection: (a) If the interval contains n, the only edge incoming to that interval is from p to n. 6That algorithm has a running time of O(Gn5), where as written G would likely add a factor of n2 with bilexical selectional preferences; this can be lowered to n using the same technique as in Eisner and Satta (2000) for non-restricted TAG. 7The terminology is a nod to the London Underground but imagines parents admonishing children to mind the gap. (b) If the interval does not contain n, all edges incoming to that interval come from n. 2. For the gap interval ([xj+1, xk_1]): (a) If the interval contains p, then the only edge incoming is from p’s parent top (b) If the interval does not contain p, then all edges incoming to that interval come from p. As a consequence of the above, [xi, xj] U {n} forms a gap-minding tree rooted at n, [xk, xl] U {n} also forms a gap-minding tree rooted at n, and [xj+1, xk_1]</context>
</contexts>
<marker>Eisner, Satta, 2000</marker>
<rawString>J. Eisner and G. Satta. 2000. A faster parsing algorithm for lexicalized tree-adjoining grammars. In Proceedings of the 5th Workshop on Tree-Adjoining Grammars and Related Formalisms (TAG+5), pages 14–19.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Eisner</author>
<author>N A Smith</author>
</authors>
<title>Favor short dependencies: Parsing with soft and hard constraints on dependency length.</title>
<date>2010</date>
<booktitle>Trends in Parsing Technology: Dependency Parsing, Domain Adaptation, and Deep Parsing, chapter 8,</booktitle>
<pages>121--150</pages>
<editor>In Harry Bunt, Paola Merlo, and Joakim Nivre, editors,</editor>
<publisher>Springer.</publisher>
<contexts>
<context position="28064" citStr="Eisner and Smith, 2010" startWordPosition="5067" endWordPosition="5070">n, 0], and the tree itself can be found using backpointers. 5.1 Runtime analysis If the input is assumed to be the complete graph (any word can have any other word as its parent), then the above algorithm takes O(n5) time. The most expensive steps are M/C, which take O(n2) time to fill in each of the O(n3) C cells. and solving a D subproblem, which takes O(n) time on each of the O(n4) possible such problems. Pruning: In practice, the set of edges considered (m) is not necessarily O(n2). Many edges can be ruled out beforehand, either based on the distance in the sentence between the two words (Eisner and Smith, 2010), the predictions of a local ranker (Martins et al., 2009), or the marginals computed from a simpler parsing model (Carreras et al., 2008). If we choose a pruning strategy such that each word has at most k potential parents (incoming edges), then the running time drops to O(kn4). The five indices in an M/C step were: i, j, k, p, and x. As there must be an edge from p to x, and x only has k possible parents, there are now only O(kn4) valid such combinations. Similarly, each D subproblem (which ranges over i, j, k, p, x) may only come into existence because of an edge from p to x, so again the r</context>
</contexts>
<marker>Eisner, Smith, 2010</marker>
<rawString>J. Eisner and N.A. Smith. 2010. Favor short dependencies: Parsing with soft and hard constraints on dependency length. In Harry Bunt, Paola Merlo, and Joakim Nivre, editors, Trends in Parsing Technology: Dependency Parsing, Domain Adaptation, and Deep Parsing, chapter 8, pages 121–150. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Eisner</author>
</authors>
<title>Bilexical grammars and their cubictime parsing algorithms.</title>
<date>2000</date>
<booktitle>Advances in Probabilistic and Other Parsing Technologies,</booktitle>
<pages>29--62</pages>
<editor>In Harry Bunt and Anton Nijholt, editors,</editor>
<publisher>Kluwer Academic Publishers,</publisher>
<contexts>
<context position="1582" citStr="Eisner, 2000" startWordPosition="253" endWordPosition="254"> Gap-minding trees (the second class) have the property that all edges into an interval of descendants come from the same node, and thus an algorithm which uses only single intervals can produce trees in which a node has descendants in multiple intervals. 1 Introduction Dependency parsers vary in what space of possible tree structures they search over when parsing a sentence. One commonly used space is the set of projective trees, in which every node’s descendants form a contiguous interval in the input sentence. Finding the optimal tree in the set of projective trees can be done efficiently (Eisner, 2000), even when the score of a tree depends on higher order factors (McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010). However, the projectivity assumption is too strict for all natural language dependency trees; for example, only 63.6% of Dutch sentences from the CoNLL-X training set are projective (Table 1). At the other end of the spectrum, some parsers search over all spanning trees, a class of structures much larger than the set of plausible linguistic structures. The maximum scoring directed spanning tree can be found efficiently when the score of a tree depends only on edg</context>
</contexts>
<marker>Eisner, 2000</marker>
<rawString>J. Eisner. 2000. Bilexical grammars and their cubictime parsing algorithms. In Harry Bunt and Anton Nijholt, editors, Advances in Probabilistic and Other Parsing Technologies, pages 29–62. Kluwer Academic Publishers, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Fiengo</author>
</authors>
<title>On trace theory.</title>
<date>1977</date>
<journal>Linguistic Inquiry,</journal>
<volume>8</volume>
<issue>1</issue>
<marker>Fiengo, 1977</marker>
<rawString>R. Fiengo. 1977. On trace theory. Linguistic Inquiry, 8(1):35–61.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C G´omez-Rodr´ıguez</author>
<author>J Carroll</author>
<author>D Weir</author>
</authors>
<title>Dependency parsing schemata and mildly non-projective dependency parsing.</title>
<date>2011</date>
<journal>Computational Linguistics,</journal>
<volume>37</volume>
<issue>3</issue>
<marker>G´omez-Rodr´ıguez, Carroll, Weir, 2011</marker>
<rawString>C. G´omez-Rodr´ıguez, J. Carroll, and D. Weir. 2011. Dependency parsing schemata and mildly non-projective dependency parsing. Computational Linguistics, 37(3):541–586.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Koo</author>
<author>M Collins</author>
</authors>
<title>Efficient third-order dependency parsers.</title>
<date>2010</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>1--11</pages>
<contexts>
<context position="1713" citStr="Koo and Collins, 2010" startWordPosition="274" endWordPosition="277">node, and thus an algorithm which uses only single intervals can produce trees in which a node has descendants in multiple intervals. 1 Introduction Dependency parsers vary in what space of possible tree structures they search over when parsing a sentence. One commonly used space is the set of projective trees, in which every node’s descendants form a contiguous interval in the input sentence. Finding the optimal tree in the set of projective trees can be done efficiently (Eisner, 2000), even when the score of a tree depends on higher order factors (McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010). However, the projectivity assumption is too strict for all natural language dependency trees; for example, only 63.6% of Dutch sentences from the CoNLL-X training set are projective (Table 1). At the other end of the spectrum, some parsers search over all spanning trees, a class of structures much larger than the set of plausible linguistic structures. The maximum scoring directed spanning tree can be found efficiently when the score of a tree depends only on edge-based factors (McDonald et al., 2005b). However, it is NP-hard to extend MST to include sibling or grandparent factors (McDonald </context>
</contexts>
<marker>Koo, Collins, 2010</marker>
<rawString>T. Koo and M. Collins. 2010. Efficient third-order dependency parsers. In Proceedings ofACL, pages 1–11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Koo</author>
<author>A Globerson</author>
<author>X Carreras</author>
<author>M Collins</author>
</authors>
<title>Structured prediction models via the matrix-tree theorem.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP-CoNLL.</booktitle>
<contexts>
<context position="33656" citStr="Koo et al., 2007" startWordPosition="6119" endWordPosition="6122">anging edges after producing a projective tree (Proj+Sib+Rearr). We add a further decoder with 10http://sourceforge.net/projects/mstparser/ 485 the algorithm presented here for gap minding trees, and plan to make the extension publicly available. The gap-minding decoder has both an edge-factored implementation and a version which scores grandparents as well.11 The gap-minding algorithm is much more efficient when edges have been pruned so that each word has at most k potential parents. We use the weights from the trained MST models combined with the Matrix Tree Theorem (Smith and Smith, 2007; Koo et al., 2007; McDonald and Satta, 2007) to produce marginal probabilities of each edge. We wanted to be able to both achieve the running time bound and yet take advantage of the fact that the size of the set of reasonable parent choices is variable. We therefore use a hybrid pruning strategy: each word’s set of potential parents is the smaller of a) the top k parents (we chose k = 10) or b) the set of parents whose probabilities are above a threshold (we chose th = .001). The running time for the gap-minding algorithm is then O(kn4); with the grandparent features the gap-minding running time is O(k2n4). T</context>
</contexts>
<marker>Koo, Globerson, Carreras, Collins, 2007</marker>
<rawString>T. Koo, A. Globerson, X. Carreras, and M. Collins. 2007. Structured prediction models via the matrix-tree theorem. In Proceedings of EMNLP-CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Koo</author>
<author>A M Rush</author>
<author>M Collins</author>
<author>T Jaakkola</author>
<author>D Sontag</author>
</authors>
<title>Dual decomposition for parsing with nonprojective head automata.</title>
<date>2010</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>1288--1298</pages>
<contexts>
<context position="2461" citStr="Koo et al., 2010" startWordPosition="398" endWordPosition="401">ntences from the CoNLL-X training set are projective (Table 1). At the other end of the spectrum, some parsers search over all spanning trees, a class of structures much larger than the set of plausible linguistic structures. The maximum scoring directed spanning tree can be found efficiently when the score of a tree depends only on edge-based factors (McDonald et al., 2005b). However, it is NP-hard to extend MST to include sibling or grandparent factors (McDonald and Pereira, 2006; McDonald and Satta, 2007). MSTbased non-projective parsers that use higher order factors (Martins et al., 2009; Koo et al., 2010), utilize different techniques than the basic MST algorithm. In addition, learning is done over a relaxation of the problem, so the inference procedures at training and at test time are not identical. We propose two new classes of trees between projective trees and the set of all spanning trees. These two classes provide a closer approximation to the set of plausible natural language dependency trees: unlike projective trees, a word can have descendants in more than one interval; unlike spanning trees, these intervals cannot be nested in arbitrary ways. We introduce gap inheritance, a new stru</context>
</contexts>
<marker>Koo, Rush, Collins, Jaakkola, Sontag, 2010</marker>
<rawString>T. Koo, A.M. Rush, M. Collins, T. Jaakkola, and D. Sontag. 2010. Dual decomposition for parsing with nonprojective head automata. In Proceedings of EMNLP, pages 1288–1298.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kuhlmann</author>
<author>J Nivre</author>
</authors>
<title>Mildly nonprojective dependency structures.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING/ACL,</booktitle>
<pages>507--514</pages>
<contexts>
<context position="7089" citStr="Kuhlmann and Nivre, 2006" startWordPosition="1175" endWordPosition="1178">ill have gap degree 0. Two subtrees interleave if there are vertices l1, r1 from one subtree and l2, r2 from the other such that l1 &lt; l2 &lt; r1 &lt; r2. Definition 3. A tree is well-nested if no two disjoint subtrees interleave (Bodirsky et al., 2005). Definition 4. A mildly non-projective tree has gap degree at most one and is well-nested. Mildly non-projective trees are of both theoretical and practical interest, as they correspond to derivations in Lexicalized Tree Adjoining Grammar (Bodirsky et al., 2005) and cover the overwhelming majority of sentences found in treebanks for Czech and Danish (Kuhlmann and Nivre, 2006). Table 1 shows the proportion of mildly nonprojective sentences for Arabic, Czech, Danish, Dutch, Portuguese, and Swedish, ranging from 95.4% of Portuguese sentences to 99.9% of Arabic sentences.4 This definition covers a substantially larger set of sentences than projectivity does — an assumption of projectivity covers only 63.6% (Dutch) to 90.2% (Swedish) of examples (Table 1). 3 Gap Inheritance Empirically, natural language sentences seem to be mostly mildly non-projective trees, but mildly nonprojective trees are quite expensive to parse (O(W) (G´omez-Rodr´ıguez et al., 2011)). The parsin</context>
</contexts>
<marker>Kuhlmann, Nivre, 2006</marker>
<rawString>M. Kuhlmann and J. Nivre. 2006. Mildly nonprojective dependency structures. In Proceedings of COLING/ACL, pages 507–514.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A F T Martins</author>
<author>N A Smith</author>
<author>E P Xing</author>
</authors>
<title>Concise integer linear programming formulations for dependency parsing.</title>
<date>2009</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>342--350</pages>
<contexts>
<context position="2442" citStr="Martins et al., 2009" startWordPosition="394" endWordPosition="397">only 63.6% of Dutch sentences from the CoNLL-X training set are projective (Table 1). At the other end of the spectrum, some parsers search over all spanning trees, a class of structures much larger than the set of plausible linguistic structures. The maximum scoring directed spanning tree can be found efficiently when the score of a tree depends only on edge-based factors (McDonald et al., 2005b). However, it is NP-hard to extend MST to include sibling or grandparent factors (McDonald and Pereira, 2006; McDonald and Satta, 2007). MSTbased non-projective parsers that use higher order factors (Martins et al., 2009; Koo et al., 2010), utilize different techniques than the basic MST algorithm. In addition, learning is done over a relaxation of the problem, so the inference procedures at training and at test time are not identical. We propose two new classes of trees between projective trees and the set of all spanning trees. These two classes provide a closer approximation to the set of plausible natural language dependency trees: unlike projective trees, a word can have descendants in more than one interval; unlike spanning trees, these intervals cannot be nested in arbitrary ways. We introduce gap inhe</context>
<context position="28122" citStr="Martins et al., 2009" startWordPosition="5077" endWordPosition="5081">5.1 Runtime analysis If the input is assumed to be the complete graph (any word can have any other word as its parent), then the above algorithm takes O(n5) time. The most expensive steps are M/C, which take O(n2) time to fill in each of the O(n3) C cells. and solving a D subproblem, which takes O(n) time on each of the O(n4) possible such problems. Pruning: In practice, the set of edges considered (m) is not necessarily O(n2). Many edges can be ruled out beforehand, either based on the distance in the sentence between the two words (Eisner and Smith, 2010), the predictions of a local ranker (Martins et al., 2009), or the marginals computed from a simpler parsing model (Carreras et al., 2008). If we choose a pruning strategy such that each word has at most k potential parents (incoming edges), then the running time drops to O(kn4). The five indices in an M/C step were: i, j, k, p, and x. As there must be an edge from p to x, and x only has k possible parents, there are now only O(kn4) valid such combinations. Similarly, each D subproblem (which ranges over i, j, k, p, x) may only come into existence because of an edge from p to x, so again the runtime of these such steps drops to O(kn4). 6 Extension to</context>
</contexts>
<marker>Martins, Smith, Xing, 2009</marker>
<rawString>A.F.T. Martins, N.A. Smith, and E.P. Xing. 2009. Concise integer linear programming formulations for dependency parsing. In Proceedings ofACL, pages 342– 350.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>F Pereira</author>
</authors>
<title>Online learning of approximate dependency parsing algorithms.</title>
<date>2006</date>
<booktitle>In Proceedings of EACL,</booktitle>
<pages>81--88</pages>
<contexts>
<context position="1673" citStr="McDonald and Pereira, 2006" startWordPosition="268" endWordPosition="271"> interval of descendants come from the same node, and thus an algorithm which uses only single intervals can produce trees in which a node has descendants in multiple intervals. 1 Introduction Dependency parsers vary in what space of possible tree structures they search over when parsing a sentence. One commonly used space is the set of projective trees, in which every node’s descendants form a contiguous interval in the input sentence. Finding the optimal tree in the set of projective trees can be done efficiently (Eisner, 2000), even when the score of a tree depends on higher order factors (McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010). However, the projectivity assumption is too strict for all natural language dependency trees; for example, only 63.6% of Dutch sentences from the CoNLL-X training set are projective (Table 1). At the other end of the spectrum, some parsers search over all spanning trees, a class of structures much larger than the set of plausible linguistic structures. The maximum scoring directed spanning tree can be found efficiently when the score of a tree depends only on edge-based factors (McDonald et al., 2005b). However, it is NP-hard to extend MST to include s</context>
</contexts>
<marker>McDonald, Pereira, 2006</marker>
<rawString>R. McDonald and F. Pereira. 2006. Online learning of approximate dependency parsing algorithms. In Proceedings of EACL, pages 81–88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>G Satta</author>
</authors>
<title>On the complexity of non-projective data-driven dependency parsing.</title>
<date>2007</date>
<booktitle>In Proceedings of the 10th International Conference on Parsing Technologies,</booktitle>
<pages>121--132</pages>
<contexts>
<context position="2357" citStr="McDonald and Satta, 2007" startWordPosition="381" endWordPosition="384">ectivity assumption is too strict for all natural language dependency trees; for example, only 63.6% of Dutch sentences from the CoNLL-X training set are projective (Table 1). At the other end of the spectrum, some parsers search over all spanning trees, a class of structures much larger than the set of plausible linguistic structures. The maximum scoring directed spanning tree can be found efficiently when the score of a tree depends only on edge-based factors (McDonald et al., 2005b). However, it is NP-hard to extend MST to include sibling or grandparent factors (McDonald and Pereira, 2006; McDonald and Satta, 2007). MSTbased non-projective parsers that use higher order factors (Martins et al., 2009; Koo et al., 2010), utilize different techniques than the basic MST algorithm. In addition, learning is done over a relaxation of the problem, so the inference procedures at training and at test time are not identical. We propose two new classes of trees between projective trees and the set of all spanning trees. These two classes provide a closer approximation to the set of plausible natural language dependency trees: unlike projective trees, a word can have descendants in more than one interval; unlike span</context>
<context position="33683" citStr="McDonald and Satta, 2007" startWordPosition="6123" endWordPosition="6126"> producing a projective tree (Proj+Sib+Rearr). We add a further decoder with 10http://sourceforge.net/projects/mstparser/ 485 the algorithm presented here for gap minding trees, and plan to make the extension publicly available. The gap-minding decoder has both an edge-factored implementation and a version which scores grandparents as well.11 The gap-minding algorithm is much more efficient when edges have been pruned so that each word has at most k potential parents. We use the weights from the trained MST models combined with the Matrix Tree Theorem (Smith and Smith, 2007; Koo et al., 2007; McDonald and Satta, 2007) to produce marginal probabilities of each edge. We wanted to be able to both achieve the running time bound and yet take advantage of the fact that the size of the set of reasonable parent choices is variable. We therefore use a hybrid pruning strategy: each word’s set of potential parents is the smaller of a) the top k parents (we chose k = 10) or b) the set of parents whose probabilities are above a threshold (we chose th = .001). The running time for the gap-minding algorithm is then O(kn4); with the grandparent features the gap-minding running time is O(k2n4). The training and test sets f</context>
</contexts>
<marker>McDonald, Satta, 2007</marker>
<rawString>R. McDonald and G. Satta. 2007. On the complexity of non-projective data-driven dependency parsing. In Proceedings of the 10th International Conference on Parsing Technologies, pages 121–132.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>K Crammer</author>
<author>F Pereira</author>
</authors>
<title>Online large-margin training of dependency parsers.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>91--98</pages>
<contexts>
<context position="2220" citStr="McDonald et al., 2005" startWordPosition="359" endWordPosition="362">core of a tree depends on higher order factors (McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010). However, the projectivity assumption is too strict for all natural language dependency trees; for example, only 63.6% of Dutch sentences from the CoNLL-X training set are projective (Table 1). At the other end of the spectrum, some parsers search over all spanning trees, a class of structures much larger than the set of plausible linguistic structures. The maximum scoring directed spanning tree can be found efficiently when the score of a tree depends only on edge-based factors (McDonald et al., 2005b). However, it is NP-hard to extend MST to include sibling or grandparent factors (McDonald and Pereira, 2006; McDonald and Satta, 2007). MSTbased non-projective parsers that use higher order factors (Martins et al., 2009; Koo et al., 2010), utilize different techniques than the basic MST algorithm. In addition, learning is done over a relaxation of the problem, so the inference procedures at training and at test time are not identical. We propose two new classes of trees between projective trees and the set of all spanning trees. These two classes provide a closer approximation to the set of</context>
<context position="32433" citStr="McDonald et al., 2005" startWordPosition="5925" endWordPosition="5928">rictly contained within spanning trees. Which space is most appropriate for natural language parsing may depend on the particular language and the type and frequencies of non-projective structures found in it. In this section we compare the parsing accuracy across languages for a parser which uses either the Eisner algorithm (projective), MST (spanning trees), or MaxGapMindingTree (gap-minding trees) as its decoder for both training and inference. We implemented both the basic gap-minding algorithm and the gap-minding algorithm with grandparent scoring as extensions to MSTParser10. MSTParser (McDonald et al., 2005b; McDonald et al., 2005a) uses the Margin Infused Relaxed Algorithm (Crammer and Singer, 2003) for discriminative training. Training requires a decoder which produces the highest scoring tree (in the space of valid trees) under the current model weights. This same decoder is then used to produce parses at test time. MSTParser comes packaged with the Eisner algorithm (for projective trees) and MST (for spanning trees). MSTParser also includes two second order models: one of which is a projective decoder that also scores siblings (Proj+Sib) and the other of which produces non-projective trees b</context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>R. McDonald, K. Crammer, and F. Pereira. 2005a. Online large-margin training of dependency parsers. In Proceedings of ACL, pages 91–98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>F Pereira</author>
<author>K Ribarov</author>
<author>J Hajiˇc</author>
</authors>
<title>Non-projective dependency parsing using spanning tree algorithms.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT-EMNLP,</booktitle>
<pages>523--530</pages>
<marker>McDonald, Pereira, Ribarov, Hajiˇc, 2005</marker>
<rawString>R. McDonald, F. Pereira, K. Ribarov, and J. Hajiˇc. 2005b. Non-projective dependency parsing using spanning tree algorithms. In Proceedings of HLT-EMNLP, pages 523–530.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Reinhart</author>
</authors>
<title>The Syntactic Domain of Anaphora.</title>
<date>1976</date>
<tech>Ph.D. thesis,</tech>
<institution>Massachusetts Institute of Technology.</institution>
<contexts>
<context position="14866" citStr="Reinhart, 1976" startWordPosition="2499" endWordPosition="2500">gle child inherit the gap created by this movement. In case (c), it is possible for there to be multiple children that inherit this newly created gap if multiple children had descendents on both sides. However, the assumption of upward movement in the phrase structure tree should rule out movement into the projection interval of a non-ancestor. Therefore, under these syntactic assumptions, we would expect at most one child to inherit a parent’s gap. 5The Proper Binding Condition (Fiengo,1977) asserts that a moved element leaves behind a trace (unpronounced element), which must be c-commanded (Reinhart, 1976) by the corresponding pronounced material in its final location. Informally, c-commanded means that the first node is descended from the lowest ancestor of the other that has more than one child. 4.1 Parsing Mild+1-Inherit Trees Finding the optimal Mild+1-Inherit tree can be done by bottom-up constructing the tree for each node and its descendants. We can maintain subtrees with two intervals (two endpoints each) and one root (O(n5) space). Consider the most complicated possible case: a parent that has a gap, a (single) child which inherits the gap, and additional children. An example of this i</context>
</contexts>
<marker>Reinhart, 1976</marker>
<rawString>T. Reinhart. 1976. The Syntactic Domain of Anaphora. Ph.D. thesis, Massachusetts Institute of Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Satta</author>
<author>W Schuler</author>
</authors>
<title>Restrictions on tree adjoining languages.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING-ACL,</booktitle>
<pages>1176--1182</pages>
<contexts>
<context position="16632" citStr="Satta and Schuler (1998)" startWordPosition="2815" endWordPosition="2818">root is updated to 3), then adding an edge from 3 to 4 to extend the left interval to [1, 5], and then adding an edge from 3 to 11 to extend the right interval to [8,13]. The subtree corresponds to the completed item [1, 5]U[8,13] rooted at 3. This procedure corresponds to G´omez-Rodriguez et al. (2011)’s O(n7) algorithm for parsing mildly non-projective structures if the most expensive step (Combine Shrinking Gap Centre) is dropped; this step would only ever be needed if a parent node has 481 more than one child inheriting its gap. This is also similar in spirit to the algorithm described in Satta and Schuler (1998) for parsing a restricted version of TAG, in which there are some limitations on adjunction operations into the spines of trees.6 That algorithm has similar steps and items, with the root portion of the item replaced with a node in a phrase structure tree (which may be a nonterminal). 5 Gap-minding Trees The algorithm in the previous section used O(n5) space and O(n6) time. While more efficient than parsing in the space of mildly projective trees, this is still probably not practically implementable. Part of the difficulty lies in the fact that gap inheritance causes the two non-contiguous pro</context>
</contexts>
<marker>Satta, Schuler, 1998</marker>
<rawString>G. Satta and W. Schuler. 1998. Restrictions on tree adjoining languages. In Proceedings of COLING-ACL, pages 1176–1182.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D A Smith</author>
<author>N A Smith</author>
</authors>
<title>Probabilistic models of nonprojective dependency trees.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP-CoNLL.</booktitle>
<contexts>
<context position="33638" citStr="Smith and Smith, 2007" startWordPosition="6115" endWordPosition="6118">ojective trees by rearranging edges after producing a projective tree (Proj+Sib+Rearr). We add a further decoder with 10http://sourceforge.net/projects/mstparser/ 485 the algorithm presented here for gap minding trees, and plan to make the extension publicly available. The gap-minding decoder has both an edge-factored implementation and a version which scores grandparents as well.11 The gap-minding algorithm is much more efficient when edges have been pruned so that each word has at most k potential parents. We use the weights from the trained MST models combined with the Matrix Tree Theorem (Smith and Smith, 2007; Koo et al., 2007; McDonald and Satta, 2007) to produce marginal probabilities of each edge. We wanted to be able to both achieve the running time bound and yet take advantage of the fact that the size of the set of reasonable parent choices is variable. We therefore use a hybrid pruning strategy: each word’s set of potential parents is the smaller of a) the top k parents (we chose k = 10) or b) the set of parents whose probabilities are above a threshold (we chose th = .001). The running time for the gap-minding algorithm is then O(kn4); with the grandparent features the gap-minding running </context>
</contexts>
<marker>Smith, Smith, 2007</marker>
<rawString>D.A. Smith and N.A. Smith. 2007. Probabilistic models of nonprojective dependency trees. In Proceedings of EMNLP-CoNLL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000009">
<title confidence="0.971176">
Framework of Automatic Text Summarization
Using Reinforcement Learning
</title>
<author confidence="0.742508">
Seonggi Ryang Takeshi Abekawa
</author>
<affiliation confidence="0.785452">
Graduate School of Information National Institute of Informatics
Science and Technology abekawa@nii.ac.jp
University of Tokyo
</affiliation>
<email confidence="0.99422">
sryang@is.s.u-tokyo.ac.jp
</email>
<sectionHeader confidence="0.996603" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999890772727273">
We present a new approach to the problem
of automatic text summarization called Au-
tomatic Summarization using Reinforcement
Learning (ASRL) in this paper, which models
the process of constructing a summary within
the framework of reinforcement learning and
attempts to optimize the given score function
with the given feature representation of a sum-
mary. We demonstrate that the method of re-
inforcement learning can be adapted to auto-
matic summarization problems naturally and
simply, and other summarizing techniques,
such as sentence compression, can be easily
adapted as actions of the framework.
The experimental results indicated ASRL was
superior to the best performing method in
DUC2004 and comparable to the state of the
art ILP-style method, in terms of ROUGE
scores. The results also revealed ASRL can
search for sub-optimal solutions efficiently
under conditions for effectively selecting fea-
tures and the score function.
</bodyText>
<sectionHeader confidence="0.998882" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999949681818182">
Automatic text summarization aims to automatically
produce a short and well-organized summary of sin-
gle or multiple documents (Mani, 2001). Automatic
summarization, especially multi-document summa-
rization, has been an increasingly important task in
recent years, because of the exponential explosion
of available information. The brief summary that
the summarization system produces allows readers
to quickly and easily understand the content of orig-
inal documents without having to read each individ-
ual document, and it should be helpful for dealing
with enormous amounts of information.
The extractive approach to automatic summariza-
tion is a popular and well-known approach in this
field, which creates a summary by directly selecting
some textual units (e.g., words and sentences) from
the original documents, because it is difficult to gen-
uinely evaluate and guarantee the linguistic quality
of the produced summary.
One of the most well-known extractive ap-
proaches is maximal marginal relevance (MMR),
which scores each textual unit and extracts the unit
that has the highest score in terms of the MMR cri-
teria (Goldstein et al., 2000). Greedy MMR-style
algorithms are widely used; however, they cannot
take into account the whole quality of the sum-
mary due to their greediness, although a summary
should convey all the information in given docu-
ments. Global inference algorithms for the extrac-
tive approach have been researched widely in recent
years (Filatova and Hatzivassiloglou, 2004; McDon-
ald, 2007; Takamura and Okumura, 2009) to con-
sider whether the summary is “good” as a whole.
These algorithms formulate the problem as integer
linear programming (ILP) to optimize the score:
however, as ILP is non-deterministic polynomial-
time hard (NP-hard), the time complexity is very
large. Consequently, we need some more efficient
algorithm for calculations.
We present a new approach to the problem of au-
tomatic text summarization called Automatic Sum-
marization using Reinforcement Learning (ASRL),
which models the process of construction of a sum-
mary within the framework of reinforcement learn-
</bodyText>
<page confidence="0.971903">
256
</page>
<note confidence="0.868989">
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 256–265, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.999851033333333">
ing and attempts to optimize the given score function
with the given feature representation of a summary.
We demonstrate that the method of reinforcement
learning can be adapted to problems with automatic
summarization naturally and simply, and other sum-
marizing techniques, such as sentence compression,
can be easily adapted as actions of the framework,
which should be helpful to enhance the quality of
the summary that is produced. This is the first paper
utilizing reinforcement learning for problems with
automatic summarization of text.
We evaluated ASRL with the DUC2004 summa-
rization task 2, and the experimental results revealed
ASRL is superior to the best method of performance
in DUC2004 and comparable with the state of the
art ILP-style method, based on maximum coverage
with the knapsack constraint problem, in terms of
ROUGE scores with experimental settings. We also
evaluated ASRL in terms of optimality and execu-
tion time. The experimental results indicated ASRL
can search the state space efficiently for some sub-
optimal solutions under the condition of effectively
selecting features and the score function, and pro-
duce a summary whose score denotes the expecta-
tion of the score of the same features’ states. The
evaluation of the quality of a produced summary
only depends on the given score function, and there-
fore it is easy to adapt the new method of evaluation
without having to modify the structure of the frame-
work.
</bodyText>
<sectionHeader confidence="0.919442" genericHeader="introduction">
2 Formulation of Extractive Approach
</sectionHeader>
<bodyText confidence="0.999868258064516">
We first focus on the extractive approach, which is
directly used to produce a summary by extracting
some textual units, by avoiding the difficulty of hav-
ing to consider the genuine linguistic quality of a
summary.
The given document (or documents) in extractive
summarization approaches is reduced to the set of
textual units: D = {x1, x2, · · · , xn}, where n is
the size of the set, and xi denotes individual textual
units. Note that any textual unit is permitted, such
as character, word, sentence, phrase, and concep-
tual unit. If we determine a sentence is a textual unit
to be extracted, the formulated problem is a problem
of extracting sentences from the source document,
which is one of the most popular settings for sum-
marization tasks.
Next, we define the score function, score(S), for
any subset of the document: S C D. Subset S is one
of the summaries of the given document. The aim of
this summarization problem is to find the summary
that maximizes this function when the score function
is given. The score function is typically defined by
taking into consideration the tradeoff between rele-
vance and redundancy.
Then, we define length function L(S), which in-
dicates the length of summary S. The length is
also arbitrary, which can be based on the character,
word, and sentence. We assume the limitation of
summary length K is given in summarization tasks.
Finally, we define the extractive approach of the
automatic summarization problem as:
</bodyText>
<equation confidence="0.992693">
S∗ = arg max score(S) (1)
S⊂D
s.t. L(S) G K.
</equation>
<sectionHeader confidence="0.983075" genericHeader="method">
3 Motivation
</sectionHeader>
<bodyText confidence="0.999985538461538">
We can regard the extractive approach as a search
problem. It is extremely difficult to solve this search
problem because the final result of evaluation given
by the given score function is not available until it
finishes, and we therefore need to try all combina-
tions of textual units. Consequently, the score func-
tion, which denotes some criterion for the quality
of a summary, tends to be determined so that the
function can be decomposed to components and it
is solved with global inference algorithms, such as
ILP. However, both decomposing the score func-
tion properly and utilizing the evaluation of half-way
process of searches are generally difficult. For ex-
ample, let us assume that we design the score func-
tion by using some complex semantic considerations
to take into account the readability of a summary,
and the score is efficiently calculated if the whole
summary is given. Then, formulating the problem
as a global inference problem and solving it with
methods of integer linear programming might gen-
erally be difficult, because of the complex compo-
sition of the score function, despite the ease with
which the whole summary is evaluated. The read-
ability score might be based on extremely complex
calculations of dependency relations, or a great deal
of external knowledge the summarizer cannot know
</bodyText>
<page confidence="0.993134">
257
</page>
<bodyText confidence="0.999730333333333">
merely from the source documents. In fact, it is ideal
that we can only directly utilize the score function,
in the sense that we do not have to consider the de-
composed form of the given score function.
We need to consider the problem with automatic
summarization to be the same as that with reinforce-
ment learning to handle these problems. Reinforce-
ment learning is one of the solutions to three prob-
lems.
</bodyText>
<listItem confidence="0.811713444444444">
• The learning of the agent only depends on the
reward provided by the environment.
• Furthermore, the reward is delayed, in the sense
that the agent cannot immediately know the ac-
tual evaluation of the executed action.
• The agent only estimates the value of the
state with the information on rewards, without
knowledge of the actual form of the score func-
tion, to maximize future rewards.
</listItem>
<bodyText confidence="0.9989885">
We suggest the formulation of the problem as we
have just described will enable us to freely design
the score function without limitations and expand
the capabilities of automatic summarization.
</bodyText>
<sectionHeader confidence="0.7122825" genericHeader="method">
4 Models of Extractive Approach for
Reinforcement Learning
</sectionHeader>
<subsectionHeader confidence="0.999849">
4.1 Reinforcement Learning
</subsectionHeader>
<bodyText confidence="0.999795333333333">
Reinforcement learning is a powerful method of
solving planning problems, especially problems for-
mulated as Markov decision processes (MDPs) (Sut-
ton and Barto, 1998). The agent of reinforcement
learning repeats three steps until terminated at each
episode in the learning process.
</bodyText>
<listItem confidence="0.998934363636364">
1. The agent observes current state s from the en-
vironment, contained in state space S.
2. Next, it determines and executes next action a
according to current policy 7r. Action a is con-
tained in the action space limited by the current
state: A(s), which is a subset of whole action
space A = U,cS A(s). Policy 7r is the strat-
egy for selecting action, represented as a con-
ditional distribution of actions: p(a|s).
3. It then observes next state s′ and receives re-
ward r from the environment.
</listItem>
<bodyText confidence="0.998811333333333">
The aim of reinforcement learning is to find optimal
policy 7r* only with information on sample trajecto-
ries and to reward the experienced agent.
We describe how to adapt the extractive approach
to the problem of reinforcement learning in the sec-
tions that follow.
</bodyText>
<subsectionHeader confidence="0.996196">
4.2 State
</subsectionHeader>
<bodyText confidence="0.999976363636364">
A state denotes a summary. We represent state s
as a tuple of summary S (a set of textual units) and
additional state variables: s = (S, A, f). We assume
s has the history of actions A that the agent executed
to achieve this state. Additionally, s has the binary
state variable, f E 10, 11, which denotes whether s
is a terminal state or not. Initial state s0 is (0, 0, 0).
We assume the d-dimensional feature representa-
tion of state s: O(s) E Rd, which only depends on
the feature of summary O′(S) E Rd−1. Given O′(S),
we define the features as:
</bodyText>
<equation confidence="0.998309">
O(s) = S (O′(S), 0)T (L(S) &lt; K) 2)
l (0, 1)T (K &lt; L(S))
</equation>
<bodyText confidence="0.999979714285714">
This definition denotes that summaries that violate
the length limitation are shrunk to a single feature,
(0,1)T, which means it is not a summary.
Note the features of the state only depend on the
features of the summary, not on the executed actions
to achieve the state. Unlike naive search methods,
this property has the potential for different states to
be represented as the same vector, which has the
same features. The agent, however, should search as
many possible states as it can. Therefore, the gen-
eralization function of the feature representation is
of utmost importance. The accurate selection of fea-
tures contributes to reducing the search space and
provides efficient learning as will be discussed later.
</bodyText>
<subsectionHeader confidence="0.999312">
4.3 Action
</subsectionHeader>
<bodyText confidence="0.999976666666667">
An action denotes a transition operation that pro-
duces a new state from a current state. We assumed
all actions were deterministic in this study. We de-
fine inserti(1 &lt; i &lt; n) actions, each of which
inserts textual unit xi to the current state unless the
state is terminated, as described in the following di-
</bodyText>
<page confidence="0.924436">
258
</page>
<equation confidence="0.8194562">
agram:
st at st+1
�
−−−−→
inserti �
</equation>
<bodyText confidence="0.984105">
In addition to insertion actions, we define finish
that terminates the current episode in reinforcement
learning:
</bodyText>
<equation confidence="0.9149965">
st at st+1
�
−−−−→
finish �
</equation>
<bodyText confidence="0.988617666666667">
Note that ft = 1 means state st is a terminal state.
Then, the whole action set, A, is defined by
inserti and finish:
A = {insert1, insert2, · · · , insertn, finish}. (5)
We can calculate the available actions limited by
state st:
</bodyText>
<equation confidence="0.98806875">
� A\At (L(St) &lt; K)
L(St))
{finish} (K &lt;
.(6)
</equation>
<bodyText confidence="0.999913">
This definition means that the agent may execute one
of the actions that have not yet been executed in this
episode, and it has no choice but to finish if the sum-
mary of the current state already violates length lim-
itations.
</bodyText>
<subsectionHeader confidence="0.989763">
4.4 Reward
</subsectionHeader>
<bodyText confidence="0.95526">
The agent receives a reward from the environment
as some kind of criterion of how good the action the
agent executed was. If the current state is st, the
agent executes at, and the state makes a transition
into st+1; then, the agent receives the reward, rt+1:
rt+1= { score(St) (at = finish, L(St) &lt; K)
−Rpenalty (at = finish, K &lt; L(St))
0 (otherwise)
where Rpenalty &gt; 0.
The agent can receive the score awarded by the
given score function if and only if the executed ac-
tion is finish and the summary length is appropri-
ate. If the summary length is inappropriate but the
executed action is finish, the environment awards
a penalty to the agent. The most important point of
this definition is that the agent receives nothing un-
der the condition where the next state is not termi-
nated. In this sense, the reward is delayed. Due to
this definition, maximizing the expectation of future
rewards is equivalent to maximizing the given score
function, and we do not need to consider the decom-
posed form of the score function, i.e., we only need
to consider the final score of the whole summary.
</bodyText>
<subsectionHeader confidence="0.996868">
4.5 Value Function Approximation
</subsectionHeader>
<bodyText confidence="0.999502625">
Our aim is to find the optimal policy. This is
achieved by obtaining the optimal state value func-
tion, V *(s), because if we obtain this, the greedy
policy is optimal, which determines the action so as
to maximize the state value after the transition oc-
curred. Therefore, our aim is equivalent to finding
V *(s). Let us try to estimate the state value func-
tion with parameter θ E Rd:
</bodyText>
<equation confidence="0.999006">
V (s) = θTϕ(s). (8)
</equation>
<bodyText confidence="0.6550085">
We can also represent and estimate the action value
function, Q(s, a), by using V (s):
</bodyText>
<equation confidence="0.909691">
Q(s, a) = r + γV (s′), (9)
</equation>
<bodyText confidence="0.98795225">
where the execution of a causes the state transition
from s to s′ and the agent receives reward r, and
γ(0 &lt; γ &lt; 1) is the discount rate. Note that all
actions are deterministic in this study.
By using these value functions, we define the
policy as the conditional distribution, p(a|s; θ, τ),
which is parameterized by θ and a temperature pa-
rameter τ:
</bodyText>
<equation confidence="0.953021666666667">
eQ(s,a)/τ
p(a|s; θ, τ) = r eQ�s,a&apos;�I r . (10)
a,
</equation>
<bodyText confidence="0.999948">
Temperature τ decreases as learning progresses,
which causes the policy to be greedier. This softmax
selection strategy is called Boltzmann selection.
</bodyText>
<subsectionHeader confidence="0.993973">
4.6 Learning Algorithm
</subsectionHeader>
<bodyText confidence="0.9995302">
The goal of learning is to estimate θ. We use the
TD (λ) algorithm with function approximation (Sut-
ton and Barto, 1998). Algorithm 1 represents the
whole system of our method, called Automatic Sum-
marization using Reinforcement Learning (ASRL)
</bodyText>
<figure confidence="0.957347653846154">
�
�
St
At
0
�
�
St U {xi} ) . (3)
At U {inserti}
0
�
�
St
At
0
�
�
St �
At U {finish} � (4)
1
A(st) =
, (7)
259
Algorithm 1 ASRL
Input: document D = {x1, x2, · · · , xn},
score function score(S)
</figure>
<listItem confidence="0.976724607142857">
1: initialize θ = 0
2: for k = 1 to N do
3: s ← (∅, ∅, 0)
// initial state
4: e = 0
5: while s is not terminated do
6: a ∼ p(a|s; θ,τk)
// selects action with current policy
7: (s′, r) ← execute(s, a)
// observes next state and receive reward
8: δ ← r + γθTϕ(s′) − θTϕ(s)
// calculates TD-error
9: e ← γλe + ϕ(s)
// updates the eligibility trace
10: θ ← θ + αkδe
// learning with current learning rate
11: s ← s′
12: end while
13: end for
14: s ← (∅, ∅, 0)
15: while s is not terminated do
16: a ← maxa Q(s, a)
// selects action greedily
with the learned policy
17: (s′, r) ← execute(s, a)
18: s ← s′
19: end while
20: return the summary of s
</listItem>
<bodyText confidence="0.9982309375">
in this paper. N is the number of learning episodes,
and e(∈ Rd) and λ(0 ≤ λ ≤ 1) correspond to the el-
igibility trace and the trace decay parameter. The el-
igibility trace, e, conveys all information on the fea-
tures of states that the agent previously experienced,
with previously decaying influences of features due
to decay parameter λ and discount rate γ (Line 9).
Line 1 initializes parameter θ to start up its learn-
ing. The following procedures from Lines 2 to 13
learn θ with the TD (λ) algorithm, by using infor-
mation on actual interactions with the environment.
Learning rate αk and temperature parameter τk de-
cay as the learning episode progresses. The best
summary with the obtained policy is calculated in
steps from Lines 14 to 19. If the agent can estimate
θ properly, greedy output is the optimal solution.
</bodyText>
<sectionHeader confidence="0.460562" genericHeader="method">
5 Models of Combined Approach for
Reinforcement Learning
</sectionHeader>
<bodyText confidence="0.999924">
We formulated the extractive approach as a problem
with reinforcement learning in the previous section.
In fact, we can also formulate a more general model
of summarization, since evaluation only depends on
the final state and it is not actually very important to
regard the given documents as a set of textual units
contained in the original documents.
We explain how to take into account other meth-
ods within the ASRL framework by modifying the
models in this section, with an example of sentence
compression. We assume that we have a method of
sentence compression, comp(x), and that a textual
unit to be extracted is a sentence. What we have to
do is to only simply modify the definitions of the
state and action. Note that this is just one example
of the combined method. Even other summarization
systems can be similarly adapted to ASRL.
</bodyText>
<subsectionHeader confidence="0.968104">
5.1 State
</subsectionHeader>
<bodyText confidence="0.9999628">
We do not want to execute sentence compression
twice, so we have to modify the state variables to
convey the information: s = (S, A, c, f), where
c ∈ {0, 1}, and S, A, and f are the same definitions
as previously described.
</bodyText>
<subsectionHeader confidence="0.993739">
5.2 Action
</subsectionHeader>
<bodyText confidence="0.999886">
We add deterministic action comp to A, which pro-
duces the new summary constructed by compressing
the last inserted sentence of the current summary:
</bodyText>
<equation confidence="0.9905642">
st at st+1
St\{x,-} ∪ {comp(x&apos;)}
At ∪ {comp}
1
0
</equation>
<bodyText confidence="0.9972515">
where x, is the last sentence that is inserted into St.
Next, we modify inserti and finish:
</bodyText>
<equation confidence="0.62687125">
st at st+1
� St � −−−−→ � St ∪ {xi} 1 ,(12)
� � � At � � � inserti � � � At ∪ {inserti}
ct 0
0 0
�
� � �
COMP
−−−−→
St
At
0
0
�
� � �
�
� � �
�
� � �
,(11)
</equation>
<page confidence="0.793639">
260
</page>
<figure confidence="0.6740235">
st at st+1 where
� St � −−−−→ � St
� � � At � � � finish � � � At U {finish}
ct ct
0 1
Note comp E A(st) may be executed if and only if
</figure>
<bodyText confidence="0.717138">
ct = 0. inserti resets c to 0.
</bodyText>
<sectionHeader confidence="0.977895" genericHeader="method">
6 Experiments
</sectionHeader>
<bodyText confidence="0.999993222222222">
We conducted three experiments in this study. First,
we evaluated our method with ROUGE metrics
(Lin, 2004), in terms of ROUGE-1, ROUGE-2, and
ROUGE-L. Second, we conducted an experiment on
measuring the optimization capabilities of ASRL,
with the scores we obtained and the execution time.
Third, we evaluated ASRL taking into consideration
sentence compression by using a very naive method,
in terms of ROUGE-1, ROUGE-2, and ROUGE-3.
</bodyText>
<subsectionHeader confidence="0.992129">
6.1 Experimental Settings
</subsectionHeader>
<bodyText confidence="0.999857142857143">
We used sentences as textual units for the extrac-
tive approach in this research. Each sentence and
document were represented as a bag-of-words vec-
tor with tf*idf values, with stopwords removed. All
tokens were stemmed by using Porter’s stemmer
(Porter, 1980).
We experimented with our proposed method on
the dataset of DUC2004 task2. This is a multi-
document summarization task that contains 50 docu-
ment clusters, each of which has 10 documents. We
set up the length limitation to 665 bytes, used in the
evaluation of DUC2004.
We set up the parameters of ASRL where the
number of episodes N = 300, the training rate
αk = 0.001 101/(100 + k1-1), and the tempera-
ture τk = 1.0 0.987k−1 where k was the number of
episodes that decayed as learning progressed. Both
discount rate γ and trace decay parameter λ were
fixed to 1 for episodic tasks. The penalty, Rpmalty,
was fixed to 1.
We used the following score function in this
study:
relevance and redundancy, Sim(xi, D) and
Sim(xi, xj) correspond to the cosine similarities
between sentence xi and the sentence set of the
given original documents D, and between sentence
xi and sentence xj. Pos(xi) is the position of
the occurrence of xi when we index sentences in
each document from top to bottom with one origin.
This score function was determined by reference
to McDonald (2007). We set λs = 0.9 in this
experiment.
We designed ϕ′(S), i.e., the vector representation
of a summary, to adapt it to the summarization prob-
lem as follows.
</bodyText>
<listItem confidence="0.990816923076923">
• Coverage of important words: The elements
are the top 100 words in terms of the tf*idf of
the given document with binary representation.
• Coverage ratio: This is calculated by counting
up the number of top 100 elements included in
the summary.
• Redundancy ratio: This is calculated by
counting up the number of elements that exces-
sively cover the top 100 elements.
• Length ratio: This is the ratio between the
length of the summary and length limitation K.
• Position: This feature takes into consideration
the position of sentence occurrences. It is cal-
</listItem>
<equation confidence="0.72396">
culated with Ex∈S Pos(x)−1.
Consequently, ϕ′(S) is a 104-dimensional vector.
</equation>
<bodyText confidence="0.99998">
We executed ASRL 10 times with the settings pre-
viously described and used all the results for evalu-
ation.
We used the dataset of DUC2003, which is a simi-
lar task that contains 30 document clusters and each
cluster had 10 documents, to determine τk and λs.
We determined the parameters so that they would
converge properly and become close to the opti-
mal solutions calculated by ILP, under the condi-
tions that the described feature representation and
the score function were given.
</bodyText>
<equation confidence="0.994553333333333">
score(S) = λsRel (xi)
xi∈S
− (1 − λs)Red(xi, xj), (14)
xi,xj ∈S,i&lt;j
�Rel(xi) = Sim(xi, D) + Pos(xi)−1 (15)
� �.(13) �Red(xi, xj) = Sim(xi, xj). (16)
λ
is the arameter for the trade-off between
g p
</equation>
<page confidence="0.992064">
261
</page>
<table confidence="0.9993575">
ROUGE-1 ROUGE-2 ROUGE-L
ASRL 0.39013 0.09479 0.33769
MCKP 0.39033 0.09613 0.34225
PEER65 0.38279 0.09217 0.33099
ILP 0.34712 0.07528 0.31241
GREEDY 0.30618 0.06400 0.27507
</table>
<tableCaption confidence="0.96953">
Table 1: Results of ROUGE evaluation compared with
other peers in DUC2004. Scores for ILP and GREEDY
have statistically significant differences from scores of
ASRL.
</tableCaption>
<subsectionHeader confidence="0.997967">
6.2 Evaluation
</subsectionHeader>
<bodyText confidence="0.9980365">
We compared ASRL with four other conventional
methods.
</bodyText>
<listItem confidence="0.6764392">
• GREEDY: This method is a simple greedy al-
gorithm, which repeats the selection of the sen-
tence with the highest score of the remaining
sentences by using an MMR-like method of
scoring as follows:
</listItem>
<equation confidence="0.99952325">
x = arg max [asRel(x)
x∈D\S
−(1 − as) max
xi∈S Red(x, xi)], (17)
</equation>
<bodyText confidence="0.916373">
where S is the current summary.
</bodyText>
<listItem confidence="0.947490666666667">
• ILP: This indicates the method proposed by
McDonald (2007) for maximizing the score
function (14) with integer linear programming.
• PEER65: This is the best performing system in
task 2 of the DUC2004 competition in terms of
ROUGE-1 proposed by Conroy et al. (2004).
• MCKP: This method was proposed by Taka-
mura and Okamura (2009). MCKP defines an
automatic summarization problem as a maxi-
mum coverage problem with a knapsack con-
straint, which uses conceptual units (Filatova
and Hatzivassiloglou, 2004), and composes the
meaning of sentences, as textual units and at-
tempts to cover as many units as possible under
the knapsack constraint.
</listItem>
<sectionHeader confidence="0.999627" genericHeader="evaluation">
7 Results
</sectionHeader>
<subsectionHeader confidence="0.99977">
7.1 Evaluation with ROUGE
</subsectionHeader>
<bodyText confidence="0.999446">
We evaluated our method of ASRL with ROUGE,
in terms of ROUGE-1, ROUGE-2, and ROUGE-L.
</bodyText>
<table confidence="0.999187363636364">
ROUGE-1 ROUGE-2 ROUGE-L
ASRL.0 0.39274 0.09537 0.34010
ASRL.1 0.39243 0.09683 0.33855
ASRL.2 0.39241 0.09597 0.34070
ASRL.3 0.39190 0.09580 0.33898
ASRL.4 0.39054 0.09579 0.33663
ASRL.5 0.38911 0.09395 0.33551
ASRL.6 0.38866 0.09392 0.33701
ASRL.7 0.38854 0.09338 0.33661
ASRL.8 0.38821 0.09363 0.33833
ASRL.9 0.38532 0.09281 0.33321
</table>
<tableCaption confidence="0.8377">
Table 2: Results of ROGUE evaluation for each ASRL
peer of 10 results in DUC2004. ASRL did not converge
with stable solution with these experimental settings be-
cause of property of randomness.
</tableCaption>
<bodyText confidence="0.999904391304348">
The experimental results are summarized in Tables
1 and 2. Table 1 lists the results for the comparison
and Table 2 lists all the results for ASRL peers.
The results imply ASRL is superior to PEER65,
ILP, and GREEDY, and comparable to MCKP with
these experimental settings in terms of ROUGE met-
rics. Note that ASRL is a kind of approximate
method, because actions are selected probabilisti-
cally and the method of reinforcement learning oc-
casionally converges with some sub-optimal solu-
tion. This can be expected from Table 2, which in-
dicates the results vary although each ASRL solu-
tion converged with some solution. However, in this
experiment, ASRL achieved higher ROUGE scores
than ILP, which achieved optimal solutions. This
seems to have been caused by the properties of the
features, which we will discuss later. It seems this
feature representation is useful for efficiently search-
ing the feature space. The method of mapping a state
to features is, however, approximate in the sense that
some states will shrink to the same feature vector,
and ASRL therefore has no tendency to converge
with some stable solution.
</bodyText>
<subsectionHeader confidence="0.99969">
7.2 Evaluation of Optimization Capabilities
</subsectionHeader>
<bodyText confidence="0.999969666666667">
Since we proposed our method as an approach to ap-
proximate optimization, there was the possibility of
convergence with some sub-optimal solution as pre-
viously discussed. We also evaluated our approach
from the point of view of the obtained scores and the
execution time to confirm whether our method had
</bodyText>
<page confidence="0.987183">
262
</page>
<figure confidence="0.997083583333334">
3.0
2.5
2.0
1.5
1.0
0.5
0.0
−0.50 50 100 150 200 250 300
Episode
ASRL
GREEDY
ILP
6000
5000
4000
3000
2000
1000
0 100 200 300 400 500 600 700
ASRL
ASRL(fit)
ILP
ILP(fit)
The number of textual units
</figure>
<figureCaption confidence="0.999698666666667">
Figure 1: Average score for each episode in ASRL in
DUC2004. Horizontal lines indicate scores of summaries
obtained with ILP and GREEDY.
</figureCaption>
<bodyText confidence="0.994456275862069">
optimization capabilities.
The experimental results are plotted in Figures
1 and 2. Figure 1 plots the average for the re-
wards (i.e., scores) that the agent obtained for each
episode. The horizontal line for ILP is the average
for the optimal scores of (14). The score in ASRL
increases as the number of episodes increases, and
overtakes the score of GREEDY at some episode.
The agent attempts to come close to the optimal
score line of ILP but seems to fail, and finally con-
verges to some local optimal solution. We should
increase the number of episodes, adjust parameters
α and τ, and select more appropriate features for
the state to improve the optimization capabilities of
ASRL.
Figure 2 plots the execution time for each peer.
The horizontal axis is the number of textual units,
i.e., the number of sentences in this experiment. The
vertical axis is the execution time taken by the task.
The plots of ASRL and ILP fit a linear function for
the former and an exponential function for the lat-
ter. The experimental results indicate that while the
execution time for ILP tends to increase exponen-
tially, that for ASRL increases linearly. The time
complexity of ASRL is linear with respect to the
number of actions because the agent has to select
the next action from the available actions for each
episode, whose time complexity is naively O(|A|).
As insertz actions are dominant in the extractive
</bodyText>
<figureCaption confidence="0.94369625">
Figure 2: Execution time on number of textual units for
each problem in DUC2004. Plot of ASRL is fitted to lin-
ear function and that of ILP is fitted to exponential func-
tion.
</figureCaption>
<bodyText confidence="0.999794777777778">
approach, the execution time increases linearly with
respect to the number of textual units. However, ILP
has to take into account the combinations of textual
units, whose number increases exponentially.
In conclusion, both the experimental results in-
dicate that ASRL efficiently calculated a summary
that was sub-optimal, but that was of relatively high-
quality in terms of ROUGE metrics, with the exper-
imental settings we used.
</bodyText>
<subsectionHeader confidence="0.6453985">
7.3 Evaluation of Effects of Sentence
Compression
</subsectionHeader>
<bodyText confidence="0.9986693125">
We also evaluated the combined approach with sen-
tence compression. We evaluated the method de-
scribed in Section 5 called ASRLC in this experi-
ment for the sake of convenience. We used a very
naive method of sentence compression for this ex-
periment, which compressed a sentence to only im-
portant words, i.e., selecting word order by using
the tf*idf score to compress the length to about
half. This method of compression did not take into
consideration either readability or linguistic quality.
Note we wanted to confirm what effect the other
methods would have, and we expected this to im-
prove the ROUGE-1 score. We used the ROUGE-3
score in this evaluation instead of ROUGE-L, to con-
firm whether naive sentence compression occurred.
The experimental results are summarized in Ta-
</bodyText>
<page confidence="0.996326">
263
</page>
<table confidence="0.974279">
ROUGE-1 ROUGE-2 ROUGE-3
ASRL 0.39013 0.09479 0.03435
ASRLC 0.39141 0.09259 0.03239
</table>
<tableCaption confidence="0.999948">
Table 3: Evaluation of combined methods.
</tableCaption>
<bodyText confidence="0.999849125">
ble 3, which indicates ROUGE-1 increases but
ROUGE-2 and ROUGE-3 decrease as expected. The
variations, however, are small. This phenomenon
was reported by Lin (2003) in that the effectiveness
of sentence compression by local optimization at the
sentence level was insufficient. Therefore, we would
have to consider the range of applications with the
combined method.
</bodyText>
<sectionHeader confidence="0.999876" genericHeader="conclusions">
8 Discussion
</sectionHeader>
<subsectionHeader confidence="0.997488">
8.1 Local Optimality of ASRL
</subsectionHeader>
<bodyText confidence="0.999988054054054">
We will discuss why ASRL seems to converge with
some “good” local optimum with the described ex-
perimental settings in this section.
Since our model of the state value function was
simply linear and our parameter estimation was im-
plemented by TD (λ), which is a simple method
in RL, it seems simply employing more efficient
or state-of-the-art reinforcement learning methods
may improve the performance of ASRL, such as
GTD and GTD2 (Sutton et al., 2009b; Sutton et al.,
2009a). These methods basically only contribute to
faster convergence, and the score that they will con-
verge to might not differ significantly. As a result, it
would not matter much which method was used for
optimization.
The main point of this problem is modeling the
feature representation of states, and this causes
sub-optimality. The vector representation of states
shrinks the different states to a single representation,
i.e., the agent regards states whose features are simi-
lar to be similar states. Due to this property, the pol-
icy of reinforcement learning is learned to maximize
the expected score of each feature vector, which
includes many states. Such sub-optimality aver-
agely balanced by the feature representation raises
the possibility of achieving states that have a high-
quality summary with a low score, since we do not
have a genuine score function.
Thus, the most important thing in our method
is to intentionally design the features of states and
the score function, so that the agent can generalize
states, while taking into consideration truly-essential
features for the required summarization. It would be
useful if the forms of features and the score function
could be arbitrarily designed by the user because
there is the capability of obtaining a high-quality
summaries.
</bodyText>
<subsectionHeader confidence="0.996985">
8.2 Potential of Combined Method
</subsectionHeader>
<bodyText confidence="0.9999698">
Other useful methods, even other summarization
systems, can easily be adapted to ASRL as was de-
scribed in Section 5. The experimental results re-
vealed that sentence compression has some effect.
In fact, all operations that produce a new summary
from an old summary can be used, i.e., even other
summarizing methods can be employed for an ac-
tion. We assumed a general combined method may
have a great deal of potential to enhance the quality
of summaries.
</bodyText>
<subsectionHeader confidence="0.99716">
8.3 Can We Obtain “a Global Policy”?
</subsectionHeader>
<bodyText confidence="0.999993555555556">
We formulated each summarization task as a rein-
forcement learning task in this paper, i.e., where
each learned policy differs. As this may be a little
unnatural, we wanted to obtain a single learned pol-
icy, i.e., a global policy.
However, we assessed that we cannot achieve a
global policy with these feature and score function
settings because the best vector, which is the fea-
ture representation of the summary that achieves an
optimal score under the current settings, seems to
vary for each cluster, even if the domain of the clus-
ters is the same (e.g., a news domain). Having said
that, we simultaneously surmised that we could ob-
tain a global policy if we could obtain a highly gen-
eral, crucial, and efficient feature representation of a
summary. We also think a global policy is essential
in terms of reinforcement learning and we intend to
attempt to achieve this in future work.
</bodyText>
<sectionHeader confidence="0.997916" genericHeader="acknowledgments">
9 Conclusion
</sectionHeader>
<bodyText confidence="0.999893333333333">
We presented a new approach to the problem of
automatic text summarization called ASRL in this
paper, which models the process of constructing
a summary with the framework of reinforcement
learning and attempts to optimize the given score
function with the given feature representation.
</bodyText>
<page confidence="0.989816">
264
</page>
<bodyText confidence="0.999976066666667">
The experimental results demonstrated ASRL
tends to converge sub-optimally, and excessively de-
pends on the formulation of features and the score
function. Although it is difficult, we believe this for-
mulation would enable us to improve the quality of
summaries by designing them freely.
We intend to employ the ROUGE score as the
score function in future work, and obtain the param-
eters of the state value function. Using these results,
we will attempt to obtain a single learned policy by
employing the ROUGE score or human evaluations
as rewards. We also intend to consider efficient fea-
tures and a score to achieve stable convergence. In
addition, we plan to use other methods of function
approximation, such as RBF networks.
</bodyText>
<sectionHeader confidence="0.999209" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999932392156863">
J.M. Conroy, J.D. Schlesinger, J. Goldstein, and D.P. O ’
leary. 2004. Left-brain/right-brain multi-document
summarization. In Proceedings of the Document Un-
derstanding Conference (DUC 2004).
E. Filatova and V. Hatzivassiloglou. 2004. A formal
model for information selection in multi-sentence text
extraction. In Proceedings of the 20th international
conference on Computational Linguistics, page 397.
Association for Computational Linguistics.
J. Goldstein, V. Mittal, J. Carbonell, and M. Kantrowitz.
2000. Multi-document summarization by sentence
extraction. In Proceedings of the 2000 NAACL-
ANLPWorkshop on Automatic summarization-Volume
4, pages 40–48. Association for Computational Lin-
guistics.
C.Y. Lin. 2003. Improving summarization performance
by sentence compression: a pilot study. In Proceed-
ings of the sixth international workshop on Informa-
tion retrieval with Asian languages-Volume 11, pages
1–8. Association for Computational Linguistics.
C.Y. Lin. 2004. Rouge: A package for automatic eval-
uation of summaries. In Proceedings of the workshop
on text summarization branches out (WAS 2004), vol-
ume 16.
I. Mani. 2001. Automatic summarization, volume 3.
John Benjamins Pub Co.
R. McDonald. 2007. A study of global inference algo-
rithms in multi-document summarization. Advances
in Information Retrieval, pages 557–564.
MF Porter. 1980. An algorithm for suffix stripping.
Program: electronic library and information systems,
14(3):130–137.
R.S. Sutton and A.G. Barto. 1998. Reinforcement learn-
ing: An introduction, volume 1. Cambridge Univ
Press.
R.S. Sutton, H.R. Maei, D. Precup, S. Bhatnagar,
D. Silver, C. Szepesv´ari, and E. Wiewiora. 2009a.
Fast gradient-descent methods for temporal-difference
learning with linear function approximation. In Pro-
ceedings of the 26th Annual International Conference
on Machine Learning, pages 993–1000. ACM.
R.S. Sutton, C. Szepesv´ari, and H.R. Maei. 2009b. A
convergent o (n) algorithm for off-policy temporal-
difference learning with linear function approxima-
tion.
H. Takamura and M. Okumura. 2009. Text summariza-
tion model based on maximum coverage problem and
its variant. In Proceedings of the 12th Conference of
the European Chapter of the Association for Compu-
tational Linguistics, pages 781–789. Association for
Computational Linguistics.
</reference>
<page confidence="0.998453">
265
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.645761">
<title confidence="0.9975175">Framework of Automatic Text Using Reinforcement Learning</title>
<author confidence="0.903736">Seonggi Takeshi</author>
<affiliation confidence="0.924305666666667">Graduate School of National Institute of Science and abekawa@nii.ac.jp University of</affiliation>
<email confidence="0.828295">sryang@is.s.u-tokyo.ac.jp</email>
<abstract confidence="0.99637352173913">We present a new approach to the problem of automatic text summarization called Automatic Summarization using Reinforcement Learning (ASRL) in this paper, which models the process of constructing a summary within the framework of reinforcement learning and attempts to optimize the given score function with the given feature representation of a summary. We demonstrate that the method of reinforcement learning can be adapted to automatic summarization problems naturally and simply, and other summarizing techniques, such as sentence compression, can be easily adapted as actions of the framework. The experimental results indicated ASRL was superior to the best performing method in DUC2004 and comparable to the state of the art ILP-style method, in terms of ROUGE scores. The results also revealed ASRL can search for sub-optimal solutions efficiently under conditions for effectively selecting features and the score function.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J M Conroy</author>
<author>J D Schlesinger</author>
<author>J Goldstein</author>
<author>D P O</author>
</authors>
<title>Left-brain/right-brain multi-document summarization.</title>
<date>2004</date>
<booktitle>In Proceedings of the Document Understanding Conference (DUC</booktitle>
<contexts>
<context position="22458" citStr="Conroy et al. (2004)" startWordPosition="3878" endWordPosition="3881">ion We compared ASRL with four other conventional methods. • GREEDY: This method is a simple greedy algorithm, which repeats the selection of the sentence with the highest score of the remaining sentences by using an MMR-like method of scoring as follows: x = arg max [asRel(x) x∈D\S −(1 − as) max xi∈S Red(x, xi)], (17) where S is the current summary. • ILP: This indicates the method proposed by McDonald (2007) for maximizing the score function (14) with integer linear programming. • PEER65: This is the best performing system in task 2 of the DUC2004 competition in terms of ROUGE-1 proposed by Conroy et al. (2004). • MCKP: This method was proposed by Takamura and Okamura (2009). MCKP defines an automatic summarization problem as a maximum coverage problem with a knapsack constraint, which uses conceptual units (Filatova and Hatzivassiloglou, 2004), and composes the meaning of sentences, as textual units and attempts to cover as many units as possible under the knapsack constraint. 7 Results 7.1 Evaluation with ROUGE We evaluated our method of ASRL with ROUGE, in terms of ROUGE-1, ROUGE-2, and ROUGE-L. ROUGE-1 ROUGE-2 ROUGE-L ASRL.0 0.39274 0.09537 0.34010 ASRL.1 0.39243 0.09683 0.33855 ASRL.2 0.39241 0</context>
</contexts>
<marker>Conroy, Schlesinger, Goldstein, O, 2004</marker>
<rawString>J.M. Conroy, J.D. Schlesinger, J. Goldstein, and D.P. O ’ leary. 2004. Left-brain/right-brain multi-document summarization. In Proceedings of the Document Understanding Conference (DUC 2004).</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Filatova</author>
<author>V Hatzivassiloglou</author>
</authors>
<title>A formal model for information selection in multi-sentence text extraction.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th international conference on Computational Linguistics,</booktitle>
<pages>397</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2704" citStr="Filatova and Hatzivassiloglou, 2004" startWordPosition="396" endWordPosition="399"> and guarantee the linguistic quality of the produced summary. One of the most well-known extractive approaches is maximal marginal relevance (MMR), which scores each textual unit and extracts the unit that has the highest score in terms of the MMR criteria (Goldstein et al., 2000). Greedy MMR-style algorithms are widely used; however, they cannot take into account the whole quality of the summary due to their greediness, although a summary should convey all the information in given documents. Global inference algorithms for the extractive approach have been researched widely in recent years (Filatova and Hatzivassiloglou, 2004; McDonald, 2007; Takamura and Okumura, 2009) to consider whether the summary is “good” as a whole. These algorithms formulate the problem as integer linear programming (ILP) to optimize the score: however, as ILP is non-deterministic polynomialtime hard (NP-hard), the time complexity is very large. Consequently, we need some more efficient algorithm for calculations. We present a new approach to the problem of automatic text summarization called Automatic Summarization using Reinforcement Learning (ASRL), which models the process of construction of a summary within the framework of reinforcem</context>
<context position="22696" citStr="Filatova and Hatzivassiloglou, 2004" startWordPosition="3915" endWordPosition="3918">ike method of scoring as follows: x = arg max [asRel(x) x∈D\S −(1 − as) max xi∈S Red(x, xi)], (17) where S is the current summary. • ILP: This indicates the method proposed by McDonald (2007) for maximizing the score function (14) with integer linear programming. • PEER65: This is the best performing system in task 2 of the DUC2004 competition in terms of ROUGE-1 proposed by Conroy et al. (2004). • MCKP: This method was proposed by Takamura and Okamura (2009). MCKP defines an automatic summarization problem as a maximum coverage problem with a knapsack constraint, which uses conceptual units (Filatova and Hatzivassiloglou, 2004), and composes the meaning of sentences, as textual units and attempts to cover as many units as possible under the knapsack constraint. 7 Results 7.1 Evaluation with ROUGE We evaluated our method of ASRL with ROUGE, in terms of ROUGE-1, ROUGE-2, and ROUGE-L. ROUGE-1 ROUGE-2 ROUGE-L ASRL.0 0.39274 0.09537 0.34010 ASRL.1 0.39243 0.09683 0.33855 ASRL.2 0.39241 0.09597 0.34070 ASRL.3 0.39190 0.09580 0.33898 ASRL.4 0.39054 0.09579 0.33663 ASRL.5 0.38911 0.09395 0.33551 ASRL.6 0.38866 0.09392 0.33701 ASRL.7 0.38854 0.09338 0.33661 ASRL.8 0.38821 0.09363 0.33833 ASRL.9 0.38532 0.09281 0.33321 Table </context>
</contexts>
<marker>Filatova, Hatzivassiloglou, 2004</marker>
<rawString>E. Filatova and V. Hatzivassiloglou. 2004. A formal model for information selection in multi-sentence text extraction. In Proceedings of the 20th international conference on Computational Linguistics, page 397. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Goldstein</author>
<author>V Mittal</author>
<author>J Carbonell</author>
<author>M Kantrowitz</author>
</authors>
<title>Multi-document summarization by sentence extraction.</title>
<date>2000</date>
<booktitle>In Proceedings of the 2000 NAACLANLPWorkshop on Automatic summarization-Volume 4,</booktitle>
<pages>40--48</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2351" citStr="Goldstein et al., 2000" startWordPosition="342" endWordPosition="345">it should be helpful for dealing with enormous amounts of information. The extractive approach to automatic summarization is a popular and well-known approach in this field, which creates a summary by directly selecting some textual units (e.g., words and sentences) from the original documents, because it is difficult to genuinely evaluate and guarantee the linguistic quality of the produced summary. One of the most well-known extractive approaches is maximal marginal relevance (MMR), which scores each textual unit and extracts the unit that has the highest score in terms of the MMR criteria (Goldstein et al., 2000). Greedy MMR-style algorithms are widely used; however, they cannot take into account the whole quality of the summary due to their greediness, although a summary should convey all the information in given documents. Global inference algorithms for the extractive approach have been researched widely in recent years (Filatova and Hatzivassiloglou, 2004; McDonald, 2007; Takamura and Okumura, 2009) to consider whether the summary is “good” as a whole. These algorithms formulate the problem as integer linear programming (ILP) to optimize the score: however, as ILP is non-deterministic polynomialti</context>
</contexts>
<marker>Goldstein, Mittal, Carbonell, Kantrowitz, 2000</marker>
<rawString>J. Goldstein, V. Mittal, J. Carbonell, and M. Kantrowitz. 2000. Multi-document summarization by sentence extraction. In Proceedings of the 2000 NAACLANLPWorkshop on Automatic summarization-Volume 4, pages 40–48. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Y Lin</author>
</authors>
<title>Improving summarization performance by sentence compression: a pilot study.</title>
<date>2003</date>
<booktitle>In Proceedings of the sixth international workshop on Information retrieval with Asian languages-Volume 11,</booktitle>
<pages>1--8</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="28416" citStr="Lin (2003)" startWordPosition="4860" endWordPosition="4861">dability or linguistic quality. Note we wanted to confirm what effect the other methods would have, and we expected this to improve the ROUGE-1 score. We used the ROUGE-3 score in this evaluation instead of ROUGE-L, to confirm whether naive sentence compression occurred. The experimental results are summarized in Ta263 ROUGE-1 ROUGE-2 ROUGE-3 ASRL 0.39013 0.09479 0.03435 ASRLC 0.39141 0.09259 0.03239 Table 3: Evaluation of combined methods. ble 3, which indicates ROUGE-1 increases but ROUGE-2 and ROUGE-3 decrease as expected. The variations, however, are small. This phenomenon was reported by Lin (2003) in that the effectiveness of sentence compression by local optimization at the sentence level was insufficient. Therefore, we would have to consider the range of applications with the combined method. 8 Discussion 8.1 Local Optimality of ASRL We will discuss why ASRL seems to converge with some “good” local optimum with the described experimental settings in this section. Since our model of the state value function was simply linear and our parameter estimation was implemented by TD (λ), which is a simple method in RL, it seems simply employing more efficient or state-of-the-art reinforcement</context>
</contexts>
<marker>Lin, 2003</marker>
<rawString>C.Y. Lin. 2003. Improving summarization performance by sentence compression: a pilot study. In Proceedings of the sixth international workshop on Information retrieval with Asian languages-Volume 11, pages 1–8. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Y Lin</author>
</authors>
<title>Rouge: A package for automatic evaluation of summaries.</title>
<date>2004</date>
<booktitle>In Proceedings of the workshop on text summarization branches out (WAS</booktitle>
<volume>16</volume>
<contexts>
<context position="18333" citStr="Lin, 2004" startWordPosition="3184" endWordPosition="3185">the current summary: st at st+1 St\{x,-} ∪ {comp(x&apos;)} At ∪ {comp} 1 0 where x, is the last sentence that is inserted into St. Next, we modify inserti and finish: st at st+1 � St � −−−−→ � St ∪ {xi} 1 ,(12) � � � At � � � inserti � � � At ∪ {inserti} ct 0 0 0 � � � � COMP −−−−→ St At 0 0 � � � � � � � � � � � � ,(11) 260 st at st+1 where � St � −−−−→ � St � � � At � � � finish � � � At U {finish} ct ct 0 1 Note comp E A(st) may be executed if and only if ct = 0. inserti resets c to 0. 6 Experiments We conducted three experiments in this study. First, we evaluated our method with ROUGE metrics (Lin, 2004), in terms of ROUGE-1, ROUGE-2, and ROUGE-L. Second, we conducted an experiment on measuring the optimization capabilities of ASRL, with the scores we obtained and the execution time. Third, we evaluated ASRL taking into consideration sentence compression by using a very naive method, in terms of ROUGE-1, ROUGE-2, and ROUGE-3. 6.1 Experimental Settings We used sentences as textual units for the extractive approach in this research. Each sentence and document were represented as a bag-of-words vector with tf*idf values, with stopwords removed. All tokens were stemmed by using Porter’s stemmer (</context>
</contexts>
<marker>Lin, 2004</marker>
<rawString>C.Y. Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Proceedings of the workshop on text summarization branches out (WAS 2004), volume 16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Mani</author>
</authors>
<title>Automatic summarization,</title>
<date>2001</date>
<volume>3</volume>
<institution>John Benjamins Pub Co.</institution>
<contexts>
<context position="1348" citStr="Mani, 2001" startWordPosition="191" endWordPosition="192">d simply, and other summarizing techniques, such as sentence compression, can be easily adapted as actions of the framework. The experimental results indicated ASRL was superior to the best performing method in DUC2004 and comparable to the state of the art ILP-style method, in terms of ROUGE scores. The results also revealed ASRL can search for sub-optimal solutions efficiently under conditions for effectively selecting features and the score function. 1 Introduction Automatic text summarization aims to automatically produce a short and well-organized summary of single or multiple documents (Mani, 2001). Automatic summarization, especially multi-document summarization, has been an increasingly important task in recent years, because of the exponential explosion of available information. The brief summary that the summarization system produces allows readers to quickly and easily understand the content of original documents without having to read each individual document, and it should be helpful for dealing with enormous amounts of information. The extractive approach to automatic summarization is a popular and well-known approach in this field, which creates a summary by directly selecting </context>
</contexts>
<marker>Mani, 2001</marker>
<rawString>I. Mani. 2001. Automatic summarization, volume 3. John Benjamins Pub Co.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
</authors>
<title>A study of global inference algorithms in multi-document summarization.</title>
<date>2007</date>
<booktitle>Advances in Information Retrieval,</booktitle>
<pages>557--564</pages>
<contexts>
<context position="2720" citStr="McDonald, 2007" startWordPosition="400" endWordPosition="402"> of the produced summary. One of the most well-known extractive approaches is maximal marginal relevance (MMR), which scores each textual unit and extracts the unit that has the highest score in terms of the MMR criteria (Goldstein et al., 2000). Greedy MMR-style algorithms are widely used; however, they cannot take into account the whole quality of the summary due to their greediness, although a summary should convey all the information in given documents. Global inference algorithms for the extractive approach have been researched widely in recent years (Filatova and Hatzivassiloglou, 2004; McDonald, 2007; Takamura and Okumura, 2009) to consider whether the summary is “good” as a whole. These algorithms formulate the problem as integer linear programming (ILP) to optimize the score: however, as ILP is non-deterministic polynomialtime hard (NP-hard), the time complexity is very large. Consequently, we need some more efficient algorithm for calculations. We present a new approach to the problem of automatic text summarization called Automatic Summarization using Reinforcement Learning (ASRL), which models the process of construction of a summary within the framework of reinforcement learn256 Pro</context>
<context position="20016" citStr="McDonald (2007)" startWordPosition="3466" endWordPosition="3467">des that decayed as learning progressed. Both discount rate γ and trace decay parameter λ were fixed to 1 for episodic tasks. The penalty, Rpmalty, was fixed to 1. We used the following score function in this study: relevance and redundancy, Sim(xi, D) and Sim(xi, xj) correspond to the cosine similarities between sentence xi and the sentence set of the given original documents D, and between sentence xi and sentence xj. Pos(xi) is the position of the occurrence of xi when we index sentences in each document from top to bottom with one origin. This score function was determined by reference to McDonald (2007). We set λs = 0.9 in this experiment. We designed ϕ′(S), i.e., the vector representation of a summary, to adapt it to the summarization problem as follows. • Coverage of important words: The elements are the top 100 words in terms of the tf*idf of the given document with binary representation. • Coverage ratio: This is calculated by counting up the number of top 100 elements included in the summary. • Redundancy ratio: This is calculated by counting up the number of elements that excessively cover the top 100 elements. • Length ratio: This is the ratio between the length of the summary and len</context>
<context position="22251" citStr="McDonald (2007)" startWordPosition="3845" endWordPosition="3846">Y 0.30618 0.06400 0.27507 Table 1: Results of ROUGE evaluation compared with other peers in DUC2004. Scores for ILP and GREEDY have statistically significant differences from scores of ASRL. 6.2 Evaluation We compared ASRL with four other conventional methods. • GREEDY: This method is a simple greedy algorithm, which repeats the selection of the sentence with the highest score of the remaining sentences by using an MMR-like method of scoring as follows: x = arg max [asRel(x) x∈D\S −(1 − as) max xi∈S Red(x, xi)], (17) where S is the current summary. • ILP: This indicates the method proposed by McDonald (2007) for maximizing the score function (14) with integer linear programming. • PEER65: This is the best performing system in task 2 of the DUC2004 competition in terms of ROUGE-1 proposed by Conroy et al. (2004). • MCKP: This method was proposed by Takamura and Okamura (2009). MCKP defines an automatic summarization problem as a maximum coverage problem with a knapsack constraint, which uses conceptual units (Filatova and Hatzivassiloglou, 2004), and composes the meaning of sentences, as textual units and attempts to cover as many units as possible under the knapsack constraint. 7 Results 7.1 Eval</context>
</contexts>
<marker>McDonald, 2007</marker>
<rawString>R. McDonald. 2007. A study of global inference algorithms in multi-document summarization. Advances in Information Retrieval, pages 557–564.</rawString>
</citation>
<citation valid="true">
<authors>
<author>MF Porter</author>
</authors>
<title>An algorithm for suffix stripping. Program: electronic library and information systems,</title>
<date>1980</date>
<pages>14--3</pages>
<contexts>
<context position="18946" citStr="Porter, 1980" startWordPosition="3278" endWordPosition="3279">, in terms of ROUGE-1, ROUGE-2, and ROUGE-L. Second, we conducted an experiment on measuring the optimization capabilities of ASRL, with the scores we obtained and the execution time. Third, we evaluated ASRL taking into consideration sentence compression by using a very naive method, in terms of ROUGE-1, ROUGE-2, and ROUGE-3. 6.1 Experimental Settings We used sentences as textual units for the extractive approach in this research. Each sentence and document were represented as a bag-of-words vector with tf*idf values, with stopwords removed. All tokens were stemmed by using Porter’s stemmer (Porter, 1980). We experimented with our proposed method on the dataset of DUC2004 task2. This is a multidocument summarization task that contains 50 document clusters, each of which has 10 documents. We set up the length limitation to 665 bytes, used in the evaluation of DUC2004. We set up the parameters of ASRL where the number of episodes N = 300, the training rate αk = 0.001 101/(100 + k1-1), and the temperature τk = 1.0 0.987k−1 where k was the number of episodes that decayed as learning progressed. Both discount rate γ and trace decay parameter λ were fixed to 1 for episodic tasks. The penalty, Rpmalt</context>
</contexts>
<marker>Porter, 1980</marker>
<rawString>MF Porter. 1980. An algorithm for suffix stripping. Program: electronic library and information systems, 14(3):130–137.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R S Sutton</author>
<author>A G Barto</author>
</authors>
<title>Reinforcement learning: An introduction, volume 1.</title>
<date>1998</date>
<publisher>Cambridge Univ Press.</publisher>
<contexts>
<context position="9118" citStr="Sutton and Barto, 1998" startWordPosition="1443" endWordPosition="1447">tion. • The agent only estimates the value of the state with the information on rewards, without knowledge of the actual form of the score function, to maximize future rewards. We suggest the formulation of the problem as we have just described will enable us to freely design the score function without limitations and expand the capabilities of automatic summarization. 4 Models of Extractive Approach for Reinforcement Learning 4.1 Reinforcement Learning Reinforcement learning is a powerful method of solving planning problems, especially problems formulated as Markov decision processes (MDPs) (Sutton and Barto, 1998). The agent of reinforcement learning repeats three steps until terminated at each episode in the learning process. 1. The agent observes current state s from the environment, contained in state space S. 2. Next, it determines and executes next action a according to current policy 7r. Action a is contained in the action space limited by the current state: A(s), which is a subset of whole action space A = U,cS A(s). Policy 7r is the strategy for selecting action, represented as a conditional distribution of actions: p(a|s). 3. It then observes next state s′ and receives reward r from the enviro</context>
<context position="14668" citStr="Sutton and Barto, 1998" startWordPosition="2444" endWordPosition="2448">he agent receives reward r, and γ(0 &lt; γ &lt; 1) is the discount rate. Note that all actions are deterministic in this study. By using these value functions, we define the policy as the conditional distribution, p(a|s; θ, τ), which is parameterized by θ and a temperature parameter τ: eQ(s,a)/τ p(a|s; θ, τ) = r eQ�s,a&apos;�I r . (10) a, Temperature τ decreases as learning progresses, which causes the policy to be greedier. This softmax selection strategy is called Boltzmann selection. 4.6 Learning Algorithm The goal of learning is to estimate θ. We use the TD (λ) algorithm with function approximation (Sutton and Barto, 1998). Algorithm 1 represents the whole system of our method, called Automatic Summarization using Reinforcement Learning (ASRL) � � St At 0 � � St U {xi} ) . (3) At U {inserti} 0 � � St At 0 � � St � At U {finish} � (4) 1 A(st) = , (7) 259 Algorithm 1 ASRL Input: document D = {x1, x2, · · · , xn}, score function score(S) 1: initialize θ = 0 2: for k = 1 to N do 3: s ← (∅, ∅, 0) // initial state 4: e = 0 5: while s is not terminated do 6: a ∼ p(a|s; θ,τk) // selects action with current policy 7: (s′, r) ← execute(s, a) // observes next state and receive reward 8: δ ← r + γθTϕ(s′) − θTϕ(s) // calcul</context>
</contexts>
<marker>Sutton, Barto, 1998</marker>
<rawString>R.S. Sutton and A.G. Barto. 1998. Reinforcement learning: An introduction, volume 1. Cambridge Univ Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R S Sutton</author>
<author>H R Maei</author>
<author>D Precup</author>
<author>S Bhatnagar</author>
<author>D Silver</author>
<author>C Szepesv´ari</author>
<author>E Wiewiora</author>
</authors>
<title>Fast gradient-descent methods for temporal-difference learning with linear function approximation.</title>
<date>2009</date>
<booktitle>In Proceedings of the 26th Annual International Conference on Machine Learning,</booktitle>
<pages>993--1000</pages>
<publisher>ACM.</publisher>
<marker>Sutton, Maei, Precup, Bhatnagar, Silver, Szepesv´ari, Wiewiora, 2009</marker>
<rawString>R.S. Sutton, H.R. Maei, D. Precup, S. Bhatnagar, D. Silver, C. Szepesv´ari, and E. Wiewiora. 2009a. Fast gradient-descent methods for temporal-difference learning with linear function approximation. In Proceedings of the 26th Annual International Conference on Machine Learning, pages 993–1000. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R S Sutton</author>
<author>C Szepesv´ari</author>
<author>H R Maei</author>
</authors>
<title>A convergent o (n) algorithm for off-policy temporaldifference learning with linear function approximation.</title>
<date>2009</date>
<marker>Sutton, Szepesv´ari, Maei, 2009</marker>
<rawString>R.S. Sutton, C. Szepesv´ari, and H.R. Maei. 2009b. A convergent o (n) algorithm for off-policy temporaldifference learning with linear function approximation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Takamura</author>
<author>M Okumura</author>
</authors>
<title>Text summarization model based on maximum coverage problem and its variant.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>781--789</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2749" citStr="Takamura and Okumura, 2009" startWordPosition="403" endWordPosition="406"> summary. One of the most well-known extractive approaches is maximal marginal relevance (MMR), which scores each textual unit and extracts the unit that has the highest score in terms of the MMR criteria (Goldstein et al., 2000). Greedy MMR-style algorithms are widely used; however, they cannot take into account the whole quality of the summary due to their greediness, although a summary should convey all the information in given documents. Global inference algorithms for the extractive approach have been researched widely in recent years (Filatova and Hatzivassiloglou, 2004; McDonald, 2007; Takamura and Okumura, 2009) to consider whether the summary is “good” as a whole. These algorithms formulate the problem as integer linear programming (ILP) to optimize the score: however, as ILP is non-deterministic polynomialtime hard (NP-hard), the time complexity is very large. Consequently, we need some more efficient algorithm for calculations. We present a new approach to the problem of automatic text summarization called Automatic Summarization using Reinforcement Learning (ASRL), which models the process of construction of a summary within the framework of reinforcement learn256 Proceedings of the 2012 Joint Co</context>
</contexts>
<marker>Takamura, Okumura, 2009</marker>
<rawString>H. Takamura and M. Okumura. 2009. Text summarization model based on maximum coverage problem and its variant. In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics, pages 781–789. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
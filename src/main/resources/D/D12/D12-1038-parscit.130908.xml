<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000154">
<title confidence="0.9990575">
Iterative Annotation Transformation with Predict-Self Reestimation
for Chinese Word Segmentation
</title>
<author confidence="0.997268">
Wenbin Jiang and Fandong Meng and Qun Liu and Yajuan L¨u
</author>
<affiliation confidence="0.991856666666667">
Key Laboratory of Intelligent Information Processing
Institute of Computing Technology
Chinese Academy of Sciences
</affiliation>
<email confidence="0.993044">
{jiangwenbin, mengfandong, liuqun, lvyajuan}@ict.ac.cn
</email>
<sectionHeader confidence="0.995778" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.996383454545455">
In this paper we first describe the technol-
ogy of automatic annotation transformation,
which is based on the annotation adaptation
algorithm (Jiang et al., 2009). It can auto-
matically transform a human-annotated cor-
pus from one annotation guideline to another.
We then propose two optimization strategies,
iterative training and predict-self reestimation,
to further improve the accuracy of annota-
tion guideline transformation. Experiments on
Chinese word segmentation show that, the it-
erative training strategy together with predict-
self reestimation brings significant improve-
ment over the simple annotation transforma-
tion baseline, and leads to classifiers with sig-
nificantly higher accuracy and several times
faster processing than annotation adaptation
does. On the Penn Chinese Treebank 5.0,
it achieves an F-measure of 98.43%, signif-
icantly outperforms previous works although
using a single classifier with only local fea-
tures.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999970431818182">
Annotation guideline adaptation depicts a general
pipeline to integrate the knowledge of corpora with
different underling annotation guidelines (Jiang et
al., 2009). In annotation adaptation two classifiers
are cascaded together, where the classification re-
sults of the lower classifier are used as guiding fea-
tures of the upper classifier, in order to achieve more
accurate classification. This method can automat-
ically adapt the divergence between different an-
notation guidelines and bring improvement to Chi-
nese word segmentation. However, the need of cas-
caded classification decisions makes it less practical
for tasks of high computational complexity such as
parsing, and less efficient to incorporate more than
two annotated corpora.
In this paper, we first describe the algorithm of
automatic annotation transformation. It is based on
the annotation adaptation algorithm, and it focuses
on the automatic transformation (rather than adapta-
tion) of a human-annotated corpus from one annota-
tion guideline to another. First, a classifier is trained
on the corpus with an annotation guideline not de-
sired, it is used to classify the corpus with the an-
notation guideline we want, so as to obtain a corpus
with parallel annotation guidelines. Then a second
classifier is trained on the parallelly annotated cor-
pus to learn the statistical regularity of annotation
transformation, and it is used to process the previous
corpus to transform its annotation guideline to that
of the target corpus. Instead of the online knowl-
edge integration methodology of annotation adapta-
tion, annotation transformation can lead to improved
classification accuracy in an offline manner by using
the transformed corpora as additional training data
for the classifier. This method leads to an enhanced
classifier with much faster processing than the cas-
caded classifiers in annotation adaptation.
We then propose two optimization strategies, iter-
ative training and predict-self reestimation, to fur-
ther improve the accuracy of annotation transfor-
mation. Although the transformation classifiers
can only be trained on corpora with autogenerated
(rather than gold) parallel annotations, an iterative
training procedure can gradually improve the trans-
</bodyText>
<page confidence="0.972375">
412
</page>
<note confidence="0.7973265">
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 412–420, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.99990302631579">
formation accuracy by iteratively optimizing the par-
allelly annotated corpora. Both source-to-target and
target-to-source annotation transformations are per-
formed in each training iteration, and the trans-
formed corpora are used to provide better annota-
tions for the parallelly annotated corpora of the next
iteration; then the better parallelly annotated corpora
will result in more accurate transformation classi-
fiers, which will generate better transformed corpora
in the new iteration. The predict-self reestimation
is based on the following hypothesis, a better trans-
formation result should be easier to be transformed
back to the original form. The predict-self heuristic
is also validated by Daum´e III (2009) in unsuper-
vised dependency parsing.
Experiments in Chinese word segmentation show
that, the iterative training strategy together with
predict-self reestimation brings significant improve-
ment over the simple annotation transformation
baseline. We perform optimized annotation trans-
formation from the People’s Daily (Yu et al., 2001)
to the Penn Chinese Treebank 5.0 (CTB) (Xue et
al., 2005), in order to improve the word segmenter
with CTB annotation guideline. Compared to anno-
tation adaptation, the optimized annotation transfor-
mation strategy leads to classifiers with significantly
higher accuracy and several times faster processing
on the same data sets. On CTB 5.0, it achieves an F-
measure of 98.43%, significantly outperforms pre-
vious works although using a single classifier with
only local features.
The rest of the paper is organized as follows.
Section 2 describes the classification-based Chinese
word segmentation method. Section 3 details the
simple annotation transformation algorithm and the
two optimization methods. After the introduction of
related works in section 4, we give the experimental
results on Chinese word segmentation in section 5.
</bodyText>
<sectionHeader confidence="0.953424" genericHeader="introduction">
2 Classification-Based Chinese Word
Segmentation
</sectionHeader>
<bodyText confidence="0.99957">
Chinese word segmentation can be formalized as
the problem of sequence labeling (Xue and Shen,
2003), where each character in the sentence is given
a boundary tag denoting its position in a word. Fol-
lowing Ng and Low (2004), joint word segmenta-
tion and part-of-speech (POS) tagging can also be
</bodyText>
<listItem confidence="0.939467333333333">
Algorithm 1 Perceptron training algorithm.
1: Input: Training examples (xi, yi)
2: C�+— 0
3: fort +— 1 .. T do
4: for i +— 1 .. N do
5: zi +— argmaxzEGEN(xi) 4)(xi, z) · C�
6: if zi =� yi then
7: C� +— C� + 4)(xi, yi) − 4)(xi, zi)
8: Output: Parameters C�
</listItem>
<bodyText confidence="0.999503321428571">
solved in a character classification approach by ex-
tending the boundary tags to include POS informa-
tion. For word segmentation we adopt the 4 bound-
ary tags of Ng and Low (2004), b, m, e and s, where
b, m and e mean the beginning, the middle and the
end of a word, and s indicates a single-character
word. The word segmentation result can be gen-
erated by splitting the labeled character sequence
into subsequences of pattern s or bm*e, indicating
single-character words or multi-character words, re-
spectively.
We choose the perceptron algorithm (Collins,
2002) to train the character classifier. It is an online
training algorithm and has been successfully used in
many NLP tasks, including POS tagging (Collins,
2002), parsing (Collins and Roark, 2004) and word
segmentation (Zhang and Clark, 2007; Jiang et al.,
2008; Zhang and Clark, 2010).
The training procedure learns a discriminative
model mapping from the inputs x E X to the outputs
y E Y , where X is the set of sentences in the train-
ing corpus and Y is the set of corresponding labeled
results. We use the function GEN(x) to enumerate
the candidate results of an input x, and the function
4) to map a training example (x, y) E X x Y to a
feature vector 4)(x, y) E Rd. Given the character
sequence x, the decoder finds the output F(x) that
maximizes the score function:
</bodyText>
<equation confidence="0.9980695">
F(x) = argmax
yEGEN(x)
= argmax
yEGEN(x)
</equation>
<bodyText confidence="0.991158">
Where C� E Rd is the parameter vector (that is, the
discriminative model) and 4)(x, y) · C� is the inner
product of 4)(x, y) and a.
Algorithm 1 shows the perceptron algorithm for
tuning the parameter a. The “averaged parameters”
</bodyText>
<equation confidence="0.998627">
S(y|a, 4), x)
4)(x, y) · C� (1)
</equation>
<page confidence="0.998655">
413
</page>
<table confidence="0.999581142857143">
Type Feature Templates
Unigram C−2 C−1 C0
C1 C2
Bigram C−2C−1 C−1C0 C0C1
C1C2 C−1C1
Property Pu(C0)
T (C−2)T (C−1)T (C0)T (C1)T (C2)
</table>
<tableCaption confidence="0.988768">
Table 1: Feature templates for classification-based Chi-
nese segmentation model.
</tableCaption>
<bodyText confidence="0.999515375">
technology (Collins, 2002) is used for better per-
formance. The feature templates for the classifier
is shown in Table 1. Co denotes the current char-
acter, while C_i/Ci denote the ith character to the
left/right of Co. The function Pu(·) returns true
for a punctuation character and false for others, the
function T(·) classifies a character into four types:
number, date, English letter and others.
</bodyText>
<sectionHeader confidence="0.9795135" genericHeader="method">
3 Iterative and Predict-Self Annotation
Transformation
</sectionHeader>
<bodyText confidence="0.999924692307692">
This section first describes the technology of au-
tomatic annotation transformation, then introduces
the two optimization strategies, iterative training and
predict-self reestimation. Iterative training takes
a global view, it conducts several rounds of bidi-
rectional annotation transformations, and improve
the transformation performance round by round.
Predict-self reestimation takes a local view instead,
it considers each training sentence, and improves the
transformation performance by taking into account
the predication result of the reverse transformation.
The two strategies can be adopted jointly to obtain
better transformation performance.
</bodyText>
<subsectionHeader confidence="0.998505">
3.1 Automatic Annotation Transformation
</subsectionHeader>
<bodyText confidence="0.999984555555555">
Annotation adaptation can integrate the knowledge
from two corpora with different underling annota-
tion guidelines. First, a classifier (source classi-
fier) is trained on the corpus (source corpus) with
an annotation standard (source annotation) not de-
sired, it is then used to classify the corpus (target
corpus) with the annotation standard (target annota-
tion) we want. Then a second classifier (transforma-
tion classifier 1) is trained on the target corpus with
</bodyText>
<footnote confidence="0.900498">
1It is called target classifier in (Jiang et al., 2009). We
think that transformation classifier better reflects its role, the
</footnote>
<subsectionHeader confidence="0.62868">
Type Feature Templates
</subsectionHeader>
<equation confidence="0.983920461538462">
Baseline C−2 C−1 C0
C1 C2
C−2C−1 C−1C0 C0C1
C1C2 C−1C1
Pu(C0)
T (C−2)T (C−1)T (C0)T (C1)T (C2)
Guiding α
C−2 ◦ α C−1 ◦ α C0 ◦ α
C1 ◦ α C2 ◦ α
C−2C−1 ◦ α C−1C0 ◦ α C0C1 ◦ α
C1C2 ◦ α C−1C1 ◦ α
Pu(C0) ◦ α
T(C−2)T(C−1)T(C0)T(C1)T(C2) ◦ α
</equation>
<tableCaption confidence="0.681647">
Table 2: Feature templates for annotation transformation,
</tableCaption>
<bodyText confidence="0.98584546875">
where α is short for α(C0), representing the source an-
notation of C0.
the source classifier’s classification result as guid-
ing features. In decoding, a raw sentence is first de-
coded by the source classifier, and then inputted into
the transformation classifier together with the anno-
tations given by the source classifier, so as to obtain
an improved classification result.
However, annotation adaptation has a drawback,
it has to cascade two classifiers in decoding to inte-
grate the knowledge in two corpora, thus seriously
degrades the processing speed. This paper describes
a variant of annotation adaptation, name annotation
transformation, aiming at automatic transformation
(rather than adaptation) between annotation stan-
dards of human-annotated corpora. In annotation
transformation, a source classifier and a transforma-
tion classifier are trained in the same way as in an-
notation adaptation. The transformation classifier is
used to process the source corpus, with the classi-
fication label derived from the segmented sentences
as the guiding features, so as to relabel the source
corpus with the target annotation guideline. By inte-
grating the target corpus and the transformed source
corpus for the training of the character classifier, im-
proved classification accuracy can be achieved.
Both the source classifier and the transforma-
tion classifier are trained with the perceptron algo-
rithm. The feature templates used for the source
classifier are the same with those for the baseline
renaming also avoids name confusion in the optimized annota-
tion transformation.
</bodyText>
<page confidence="0.994182">
414
</page>
<construct confidence="0.362959">
Algorithm 2 Baseline annotation transformation.
</construct>
<listItem confidence="0.992948222222222">
1: function ANNOTRANS(Cs, Ct)
2: Ms ← TRAIN(Cs)
3: Cst ← ANNOTATE(Ms, Ct)
4: Ms→t ← TRANSTRAIN(Cst , Ct)
5: Ct s ← TRANSANNOTATE(Ms→t,Cs)
6: Ct∗ ← Ct s ∪ Ct
7: return Ct∗
8: function DECODE(M, Φ, x)
9: return argmaxy∈GEN(x) S(y|M, Φ, x)
</listItem>
<bodyText confidence="0.999603645161291">
character classifier. The feature templates for the
transformation classifier are the same with those in
annotation adaptation, as listed in Table 2. Al-
gorithm 2 shows the overall training algorithm
for annotation transformation. C3 and Ct denote
the source corpus and the target corpus; M3 and
M3,t denote the source classifier and the trans-
formation classifier; CP denotes the p corpus re-
labeled in q annotation guideline, for example Ct3
is the source corpus transformed to target annota-
tion guideline; Functions TRAIN and TRANSTRAIN
both invoke the perceptron algorithm, yet with
different feature sets; Functions ANNOTATE and
TRANSANNOTATE call the function DECODE with
different models (source/transformation classifiers),
feature functions (without/with guiding features),
and inputs (raw/source-annotated sentences).
The best training iterations for the functions
TRAIN and TRANSTRAIN are determined on the de-
veloping sets of the source corpus and the target
corpus, respectively. In the algorithm the param-
eters corresponding to developing sets are omitted
for simplicity. Compared to the online knowledge
integration methodology of annotation adaptation,
annotation transformation leads to improved perfor-
mance in an offline manner by integrating corpora
before the training procedure. This manner could
achieve processing several times as fast as the cas-
caded classifiers in annotation adaptation. In the fol-
lowing we will describe the two optimization strate-
gies in details.
</bodyText>
<subsectionHeader confidence="0.974557">
3.2 Iterative Training for Annotation
Transformation
</subsectionHeader>
<bodyText confidence="0.9205795">
The training of annotation transformation is based
on an auto-generated (rather than gold) parallelly an-
notated corpus, where the source annotation is pro-
Algorithm 3 Iterative annotation transformation.
</bodyText>
<listItem confidence="0.838293263157895">
1: function ITERANNOTRANS(Cs, Ct)
2: Ms ← TRAIN(Cs)
3: Cst ← ANNOTATE(Ms, Ct)
4: Mt ← TRAIN(Ct)
5: Ct s ← ANNOTATE(Mt, Cs)
6: repeat
7: Ms→t ← TRANSTRAIN(Cst , Ct)
8: Mt→s ← TRANSTRAIN(Cts,Cs)
9: Ct s ← TRANSANNOTATE(Ms→t, Cs)
10: Cst ← TRANSANNOTATE(Mt→s, Ct)
11: Ct∗ ← Ct s ∪ Ct
12: M∗ ← TRAIN(Ct∗)
13: until EVAL(M∗) converges
14: return Ct∗
15: function DECODE(M, Φ, x)
16: return argmaxy∈GEN(x) S(y|M, Φ, x)
vided by the source classifier. Therefore, the perfor-
mance of transformation training is correspondingly
determined by the accuracy of the source classifier.
</listItem>
<bodyText confidence="0.999956148148148">
We propose an iterative training procedure to
gradually improve the transformation accuracy by
iteratively optimizing the parallelly annotated cor-
pora. In each training iteration, both source-to-target
and target-to-source annotation transformations are
performed, and the transformed corpora are used to
provide better annotations for the parallelly anno-
tated corpora of the next iteration. Then in the new
iteration, the better parallelly annotated corpora will
result in more accurate transformation classifiers, so
as to generate better transformed corpora.
Algorithm 3 shows the overall procedure of the
iterative training method. The loop of lines 6-13
iteratively performs source-to-target and target-to-
source annotation transformations. The source an-
notations of the parallelly annotated corpora, C3t and
Ct3, are initialized by applying the source and tar-
get classifiers respectively on the target and source
corpora (lines 2-5). In each training iteration, the
transformation classifiers are trained on the current
parallelly annotated corpora (lines 7-8), they are
used to produce the transformed corpora (lines 9-10)
which provide better annotations for the parallelly
annotated corpora of the next iteration. The itera-
tive training terminates when the performance of the
classifier trained on the merged corpus Ct 3 ∪ Ct con-
verges.
</bodyText>
<page confidence="0.996355">
415
</page>
<bodyText confidence="0.999904461538461">
The discriminative training of TRANSTRAIN pre-
dicts the target annotations with the guidance of
source annotations. In the first iteration, the trans-
formed corpora generated by the transformation
classifiers are better than the initialized ones gener-
ated by the source and target classifiers, due to the
assistance of the guiding features. In the follow-
ing iterations, the transformed corpora provide bet-
ter annotations for the parallelly annotated corpora
of the subsequent iteration, the transformation ac-
curacy will improve gradually along with optimiza-
tion of the parallelly annotated corpora until conver-
gence.
</bodyText>
<subsectionHeader confidence="0.8006475">
3.3 Predict-Self Reestimation for Annotation
Transformation
</subsectionHeader>
<bodyText confidence="0.998514947368421">
The predict-self hypothesis is implicit in many unsu-
pervised learning approaches, such as Markov ran-
dom field. This methodology has also been success-
fully used by Daum´e III (2009) in unsupervised de-
pendency parsing. The basic idea of predict-self is
that, if a prediction is a better candidate for an input,
it can be easier converted back to the original input
by a reverse procedure. If applied to the task of an-
notation transformation, predict-self indicates that a
better transformation candidate following the target
annotation guideline can be easier transformed back
to the original form following the source annotation
guideline.
The most intuitionistic strategy to introduce the
predict-self methodology into annotation transfor-
mation is using a reversed annotation transforma-
tion procedure to filter out unreliable predictions of
the previous transformation. In detail, a source-to-
target annotation transformation is performed on the
source annotated sentence to obtain a prediction that
follows the target annotation guideline, then a sec-
ond, target-to-source transformation is performed
on this prediction result to check whether it can
be transformed back to the previous source annota-
tion. Transformation results failing in this reversal
verification are discarded, so this strategy is named
predict-self filtration.
A more precious strategy can be called predict-
self reestimation. Instead of using the reversed
transformation procedure for filtration, the rees-
timation strategy integrates the scores given by
the source-to-target and target-to-source annotation
transformation models when evaluating the transfor-
mation candidates. By properly tuning the relative
weights of the two transformation directions, bet-
ter transformation performance would be achieved.
The scores of the two transformation models are
weighted integrated in a log-linear manner:
</bodyText>
<equation confidence="0.953262666666667">
S+(y|Ms→t, Mt→s, Φ, x)
_ (1 − λ) x S(y|Ms→t, Φ, x) (2)
+ λ x S(x|Mt→s, Φ, y)
</equation>
<bodyText confidence="0.999559571428571">
The weight parameter λ is tuned on the develop-
ing set. To integrating the predict-self reestima-
tion into the iterative transformation training, a re-
versed transformation model is introduced and the
enhanced scoring function above is used when the
function TRANSANNOTATE invokes the function
DECODE.
</bodyText>
<sectionHeader confidence="0.999894" genericHeader="method">
4 Related Works
</sectionHeader>
<bodyText confidence="0.999962928571429">
Researches focused on the automatic adaptation
between different corpora can be roughly clas-
sified into two kinds, adaptation between differ-
ent domains (with different statistical distribution)
(Blitzer et al., 2006; Daum´e III, 2007), and adapta-
tion between different annotation guidelines (Jiang
et al., 2009; Zhu et al., 2011). There are also
some efforts that totally or partially resort to man-
ual transformation rules, to conduct treebank con-
version (Cahill and Mccarthy, 2002; Hockenmaier
and Steedman, 2007; Clark and Curran, 2009), and
word segmentation guideline transformation (Gao
et al., 2004; Mi et al., 2008). This work focuses
on the automatic transformation between annotation
guidelines, and proposes better annotation transfor-
mation technologies to improve the transformation
accuracy and the utilization rate of human-annotated
knowledge.
The iterative training procedure proposed in this
work shares some similarity with the co-training al-
gorithm in parsing (Sarkar, 2001), where the train-
ing procedure lets two different models learn from
each other during parsing the raw text. The key
idea of co-training is utilize the complementarity of
different parsing models to mine additional training
data from raw text, while iterative training for an-
notation transformation emphasizes the iterative op-
timization of the parellelly annotated corpora used
</bodyText>
<page confidence="0.997085">
416
</page>
<figure confidence="0.963089958333333">
Test on (F1%)
CTB
SPD
Train on
97.35
91.23(t 3.02)
86.65(t 10.70)
94.25
CTB
SPD
Table 4: Performance of the perceptron classifiers for
Chinese word segmentation.
Model
Baseline
Time (s)
1.33
4.39
1.33
1.21
Accuracy (F1%)
93.79
97.67
97.69
97.35
</figure>
<table confidence="0.889227769230769">
Merging
Anno. Adapt.
Anno. Trans.
Partition Sections # of word
CTB
Training 1 − 270 0.47M
400 − 931
1001 − 1151
Developing 301 − 325 6.66K
Test 271 − 300 7.82K
PD
Training 02 − 06 5.86M
Test 01 1.07M
</table>
<tableCaption confidence="0.999904">
Table 3: Data partitioning for CTB and PD.
</tableCaption>
<bodyText confidence="0.999965652173913">
to train the transformation models. The predict-
self methodology is implicit in many unsupervised
learning approaches, it has been successfully used
by (Daum´e III, 2009) in unsupervised dependency
parsing. We adapt this idea to the scenario of anno-
tation transformation to improve transformation ac-
curacy.
In recent years many works have been devoted to
the word segmentation task. For example, the in-
troduction of global training or complicated features
(Zhang and Clark, 2007; Zhang and Clark, 2010);
the investigation of word structures (Li, 2011);
the strategies of hybrid, joint or stacked modeling
(Nakagawa and Uchimoto, 2007; Kruengkrai et al.,
2009; Wang et al., 2010; Sun, 2011), and the semi-
supervised and unsupervised technologies utilizing
raw text (Zhao and Kit, 2008; Johnson and Gold-
water, 2009; Mochihashi et al., 2009; Hewlett and
Cohen, 2011). We estimate that the annotation trans-
formation technologies can be adopted jointly with
complicated features, system combination and semi-
supervised/unsupervised technologies to further im-
prove segmentation performance.
</bodyText>
<sectionHeader confidence="0.993659" genericHeader="evaluation">
5 Experiments and Analysis
</sectionHeader>
<bodyText confidence="0.9992413">
We perform annotation transformation from Peo-
ple’s Daily (PD) (Yu et al., 2001) to Penn Chi-
nese Treebank 5.0 (CTB) (Xue et al., 2005), follow-
ing the same experimental setting as the annotation
adaptation work (Jiang et al., 2009) for convenience
of comparison. The two corpora are segmented fol-
lowing different segmentation guidelines and differ
largely in quantity of data. CTB is smaller in size
with about 0.5M words, while PD is much larger,
containing nearly 6M words.
</bodyText>
<tableCaption confidence="0.979947">
Table 5: Comparison of the baseline annotation transfor-
mation, annotation adaptation and a simple corpus merg-
ing strategy.
</tableCaption>
<bodyText confidence="0.9999744">
To approximate more general scenarios of anno-
tation adaptation problems, we extract from PD a
subset which is comparable to CTB in size. We ran-
domly select 20, 000 sentences (0.45M words) from
the PD training data as the new training set, and
1000/1000 sentences from the PD test data as the
new test/developing set. 2 We name the smaller ver-
sion of PD as SPD. The balanced source corpus and
target corpus also facilitate the investigation of an-
notation transformation.
</bodyText>
<subsectionHeader confidence="0.988319">
5.1 Baseline Classifiers for Word Segmentation
</subsectionHeader>
<bodyText confidence="0.999923235294118">
We train the baseline perceptron classifiers de-
scribed in section 2 on the training sets of SPD
and CTB, using the developing sets to determine the
best training iterations. The performance measure-
ment indicators for word segmentation is balanced
F-measure, F = 2PR/(P + R), a function of Pre-
cision P and Recall R. where P is the percentage
of words in segmentation result that are segmented
correctly, and R is the percentage of correctly seg-
mented words in the gold standard words.
Accuracies of the baseline classifiers are listed in
Table 4. We also report the performance of the clas-
sifiers on the test sets of the opposite corpora. Ex-
perimental results are in line with our expectations.
A classifier performs better in its corresponding test
set, and performs significantly worse on a test set
following a different annotation guideline.
</bodyText>
<footnote confidence="0.918720666666667">
2There are many extremely long sentences in original PD
corpus, we split them into normal sentences according to period
punctuations.
</footnote>
<page confidence="0.995693">
417
</page>
<figure confidence="0.9790995">
0 1 2 3 4 5 6 7 8 9 10
Training iterations
</figure>
<figureCaption confidence="0.9975015">
Figure 1: Learning curve of iterative training for annota-
tion transformation.
</figureCaption>
<figure confidence="0.985728">
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Predict-self ratio
</figure>
<figureCaption confidence="0.99144">
Figure 2: Performance of predict-self filtration and
predict-self reestimation.
</figureCaption>
<figure confidence="0.997183894736842">
96.4
96.2
95.8
95.6
95.4
96
Iterative training
Baseline annotation transformation
96.4
96.2
95.8
95.6
95.4
96
Predict-self reestimation
Predict-self filtration
Baseline annotation transformation
Accuracy (F%)
Accuracy (F%)
</figure>
<subsectionHeader confidence="0.900646">
5.2 Annotation Transformation vs. Annotation
Adaptation
</subsectionHeader>
<bodyText confidence="0.999930285714286">
Experiments of annotation transformation are con-
ducted on the direction of SPD-to-CTB. The trans-
formed corpus can be merged into the regular cor-
pus, so as to train an enhanced classifier. As com-
parison, the cascaded model of annotation adapta-
tion (Jiang et al., 2009) is faithfully implemented
(yet using our feature representation) and tested on
the same adaptation direction.
Table 5 shows the performances of the classi-
fiers resulted by the baseline annotation transforma-
tion and annotation adaptation, as well as the clas-
sifier trained on the directly merged corpus. The
time costs for decoding are also listed to facilitate
the comparison of practicality. We find that the sim-
ple corpus merging strategy leads to dramatic de-
crease in accuracy, due to the different and incom-
patible annotation guidelines. The baseline annota-
tion transformation method leads to a classifier with
accuracy increment comparable to that of the anno-
tation adaptation strategy, while consuming only one
third of the decoding time.
</bodyText>
<subsectionHeader confidence="0.8435945">
5.3 Iterative Training with Predict-Self
Reestimation
</subsectionHeader>
<bodyText confidence="0.999996428571428">
We adopt the iterative training strategy to the base-
line annotation transformation model. The CTB de-
veloping set is used to determine the best training
iteration for annotation transformation from SPD to
CTB. After each iteration, we test the performance
of the classifier trained on the merged corpus. Fig-
ure 1 shows the performance curve, with iterations
</bodyText>
<figure confidence="0.974251">
0 1 2 3 4 5 6 7 8 9 10
Training iterations
</figure>
<figureCaption confidence="0.996786">
Figure 3: Learning curve of iterative training with
predict-self reestimation for annotation transformation.
</figureCaption>
<bodyText confidence="0.999946631578947">
ranging from 1 to 10. The performance of the base-
line annotation transformation model is naturally in-
cluded in the curve (located at iteration 1). The
curve shows that the performance of the classifier
trained on the merged corpus consistently improves
from iteration 2 to iteration 5.
Experimental results of predict-self filtration and
predict-self reestimation are shown in Figure 2.
The curve shows the performance of the predict-self
reestimation according to a series of weight param-
eters, ranging from 0 to 1 with step 0.05. The point
at λ = 0 shows the performance of the baseline
annotation transformation strategy. The upper hor-
izontal line shows the performance of predict-self
filtration. We find that predict-self filtration brings
slight improvement over the baseline, and predict-
self reestimation outperforms the filtration strategy
when λ falls in a proper range. An initial analysis
on the experimental results of predict-self filtration
</bodyText>
<figure confidence="0.997515666666666">
Accuracy (F%)
96.4
96.2
95.8
95.6
95.4
96
Iterative training with predict-self reestimation
Iterative training
</figure>
<page confidence="0.987015">
418
</page>
<table confidence="0.996826333333333">
Model Time (s) Accuracy (F1%)
SPD → CTB
Anno. Adapt. 4.39 97.67
Opt. Trans. 1.33 97.97
PD → CTB
Anno. Adapt. 4.76 98.15
Opt. Trans. 1.37 98.43
Previous Works
(Jiang et al., 2008) 97.85
(Kruengkrai et al., 2009) 97.87
(Zhang and Clark, 2010) 97.79
(Sun, 2011) 98.17
</table>
<tableCaption confidence="0.972555666666667">
Table 6: The performance of the iterative annotation
transformation with predict-self reestimation compared
with annotation adaptation.
</tableCaption>
<bodyText confidence="0.999805833333333">
shows that, the filtration discards 5% of the train-
ing sentences and these discarded sentences contain
nearly 10% of training words. It can be confirmed
that the sentences discarded by predict-self filtra-
tion are much longer and more complicated. With a
properly tuned weight, predict-self reestimation can
make better use of the training data. The best F-
measure improvement achieved over the annotation
transformation baseline is 0.3 points, a little worse
than that brought by iterative training.
Figure 3 shows the performance curve of iterative
annotation transformation with predict-self reesti-
mation. We find that the predict-self reestimation
brings improvement to the iterative training at each
iteration. The maximum performance is achieved
at iteration 4. The corresponding model is evalu-
ated on the test set of CTB, table 6 shows the ex-
perimental results. Compared to annotation adapta-
tion, the optimized annotation transformation strat-
egy leads to a classifier with significantly higher ac-
curacy and several times faster processing. When
using the whole PD as the source corpus, the final
classifier 3 achieves an F-measure of 98.43%, sig-
nificantly outperforms previous works although us-
ing a single classifier with only local features. Of
course, the comparison between our system and pre-
vious works without using additional training data
is unfair. This work aim to find another way to im-
prove Chinese word segmentation, which focuses on
the collection of more training data instead of mak-
</bodyText>
<footnote confidence="0.8401525">
3The predict-self reestimation ratio A is fixed after the first
training iteration for efficiency.
</footnote>
<bodyText confidence="0.99993975">
ing full use of a certain corpus. We believe that the
performance can be further improved by adopting
the advanced technologies of previous works, such
as complicated features and model combination.
Considering the fact that today some corpora for
word segmentation are really large (usually tens
of thousands of sentences), it is necessary to ob-
tain the latest CTB and investigate whether and
how much does annotation transformation bring im-
provement to a much higher baseline. On the other
hand, it is valuable to conduct experiments with
more source-annotated training data, such as the
SIGHAN dataset, to investigate the trend of im-
provement along with the increment of the addi-
tional annotated sentences. It is also valuable to
evaluate the improved word segmenter on the out-
of-domain datasets. However, currently most cor-
pora for Chinese word segmentation do not explic-
itly distinguish the domains of their data sections, it
makes such evaluations difficult to conduct.
</bodyText>
<sectionHeader confidence="0.996168" genericHeader="conclusions">
6 Conclusion and Future Works
</sectionHeader>
<bodyText confidence="0.999966142857143">
In this paper, we first describe an annotation trans-
formation algorithm to automatically transform a
human-annotated corpus from one annotation guide-
line to another. Then we propose two optimization
strategies, iterative training and predict-self reesti-
mation, to further improve the accuracy of anno-
tation guideline transformation. On Chinese word
segmentation, the optimized annotation transforma-
tion strategy leads to classifiers with obviously bet-
ter performance and several times faster processing
on the same datasets, compared to annotation adap-
tation. When adopting the whole PD as the source
corpus, the final classifier significantly outperforms
previous works on CTB 5.0, although using a single
classifier with only local features.
As future works, we will investigate the accel-
eration of the iterative training and the weight pa-
rameter tuning, and extend the optimized annotation
transformation strategy to joint Chinese word seg-
mentation and POS tagging, parsing and other NLP
tasks.
</bodyText>
<sectionHeader confidence="0.998316" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9595645">
The authors were supported by National Natural
Science Foundation of China, Contracts 90920004
</bodyText>
<page confidence="0.997131">
419
</page>
<bodyText confidence="0.999911">
and 61100082, and 863 State Key Project No.
2011AA01A207. We are grateful to the anonymous
reviewers for their thorough reviewing and valuable
suggestions.
</bodyText>
<sectionHeader confidence="0.996388" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99983446875">
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In Proceedings ofEMNLP.
Aoife Cahill and Mairead Mccarthy. 2002. Automatic
annotation of the penn treebank with lfg f-structure in-
formation. In in Proceedings of the LREC Workshop.
Stephen Clark and James R. Curran. 2009. Comparing
the accuracy of ccg and penn treebank parsers. In Pro-
ceedings ofACL-IJCNLP.
Michael Collins and Brian Roark. 2004. Incremental
parsing with the perceptron algorithm. In Proceedings
ofACL 2004.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings of
EMNLP, pages 1–8, Philadelphia, USA.
Hal Daum´e III. 2007. Frustratingly easy domain adapta-
tion. In Proceedings ofACL.
Hal Daum´e III. 2009. Unsupervised search-based struc-
tured prediction. In Proceedings ofICML.
Jianfeng Gao, Andi Wu, Mu Li, Chang-Ning Huang,
Hongqiao Li, Xinsong Xia, and Haowei Qin. 2004.
Adaptive chinese word segmentation. In Proceedings
ofACL.
Daniel Hewlett and Paul Cohen. 2011. Fully unsuper-
vised word segmentation with bve and mdl. In Pro-
ceedings ofACL.
Julia Hockenmaier and Mark Steedman. 2007. Ccgbank:
a corpus of ccg derivations and dependency structures
extracted from the penn treebank. In Computational
Linguistics, volume 33(3), pages 355–396.
Wenbin Jiang, Liang Huang, Yajuan Lv, and Qun Liu.
2008. A cascaded linear model for joint chinese word
segmentation and part-of-speech tagging. In Proceed-
ings ofACL.
Wenbin Jiang, Liang Huang, and Qun Liu. 2009. Au-
tomatic adaptation of annotation standards: Chinese
word segmentation and pos tagging–a case study. In
Proceedings of the 47th ACL.
Mark Johnson and Sharon Goldwater. 2009. Improving
nonparameteric bayesian inference: experiments on
unsupervised word segmentation with adaptor gram-
mars. In Proceedings ofNAACL.
Canasai Kruengkrai, Kiyotaka Uchimoto, Junichi
Kazama, Yiou Wang, Kentaro Torisawa, and Hitoshi
Isahara. 2009. An error-driven word-character hybrid
model for joint chinese word segmentation and pos
tagging. In Proceedings ofACL-IJCNLP.
Zhongguo Li. 2011. Parsing the internal structure of
words: A new paradigm for chineseword segmenta-
tion. In Proceedings ofACL.
Haitao Mi, Deyi Xiong, and Qun Liu. 2008. Research
on strategy of integrating chinese lexical analysis and
parser. In Journal of Chinese Information Processing.
Daichi Mochihashi, Takeshi Yamada, and Naonori Ueda.
2009. Bayesian unsupervised word segmentation with
nested pitman-yor language modeling. In Proceedings
ofACL-IJCNLP.
Tetsuji Nakagawa and Kiyotaka Uchimoto. 2007. A hy-
brid approach to word segmentation and pos tagging.
In Proceedings ofACL.
Hwee Tou Ng and Jin Kiat Low. 2004. Chinese part-of-
speech tagging: One-at-a-time or all-at-once? word-
based or character-based? In Proceedings ofEMNLP.
Anoop Sarkar. 2001. Applying co-training methods to
statistical parsing. In Proceedings ofNAACL.
Weiwei Sun. 2011. A stacked sub-word model for
joint chinese word segmentation and part-of-speech
tagging. In Proceedings ofACL.
Kun Wang, Chengqing Zong, and Keh-Yih Su. 2010. A
character-based joint model for chinese word segmen-
tation. In Proceedings of COLING.
Nianwen Xue and Libin Shen. 2003. Chinese word seg-
mentation as lmr tagging. In Proceedings of SIGHAN
Workshop.
Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha
Palmer. 2005. The penn chinese treebank: Phrase
structure annotation of a large corpus. In Natural Lan-
guage Engineering.
Shiwen Yu, Jianming Lu, Xuefeng Zhu, Huiming Duan,
Shiyong Kang, Honglin Sun, Hui Wang, Qiang Zhao,
and Weidong Zhan. 2001. Processing norms of mod-
ern chinese corpus. Technical report.
Yue Zhang and Stephen Clark. 2007. Chinese segmenta-
tion with a word-based perceptron algorithm. In Pro-
ceedings ofACL 2007.
Yue Zhang and Stephen Clark. 2010. A fast decoder for
joint word segmentation and pos-tagging using a sin-
gle discriminative model. In Proceedings ofEMNLP.
Hai Zhao and Chunyu Kit. 2008. Unsupervised segmen-
tation helps supervised learning of character tagging
for word segmentation and named entity recognition.
In Proceedings of SIGHAN Workshop.
Muhua Zhu, Jingbo Zhu, and Minghan Hu. 2011. Better
automatic treebank conversion using a feature-based
approach. In Proceedings ofACL.
</reference>
<page confidence="0.998375">
420
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.374094">
<title confidence="0.999388">Iterative Annotation Transformation with Predict-Self for Chinese Word Segmentation</title>
<author confidence="0.992204">Jiang Meng Liu L¨u</author>
<affiliation confidence="0.965758">Key Laboratory of Intelligent Information Institute of Computing</affiliation>
<address confidence="0.46378">Chinese Academy of</address>
<email confidence="0.8494">mengfandong,liuqun,</email>
<abstract confidence="0.995551608695652">In this paper we first describe the technology of automatic annotation transformation, which is based on the annotation adaptation algorithm (Jiang et al., 2009). It can automatically transform a human-annotated corpus from one annotation guideline to another. We then propose two optimization strategies, iterative training and predict-self reestimation, to further improve the accuracy of annotation guideline transformation. Experiments on Chinese word segmentation show that, the iterative training strategy together with predictself reestimation brings significant improvement over the simple annotation transformation baseline, and leads to classifiers with significantly higher accuracy and several times faster processing than annotation adaptation does. On the Penn Chinese Treebank 5.0, achieves an F-measure of significantly outperforms previous works although using a single classifier with only local features.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>John Blitzer</author>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
</authors>
<title>Domain adaptation with structural correspondence learning.</title>
<date>2006</date>
<booktitle>In Proceedings ofEMNLP.</booktitle>
<contexts>
<context position="18811" citStr="Blitzer et al., 2006" startWordPosition="2856" endWordPosition="2859">near manner: S+(y|Ms→t, Mt→s, Φ, x) _ (1 − λ) x S(y|Ms→t, Φ, x) (2) + λ x S(x|Mt→s, Φ, y) The weight parameter λ is tuned on the developing set. To integrating the predict-self reestimation into the iterative transformation training, a reversed transformation model is introduced and the enhanced scoring function above is used when the function TRANSANNOTATE invokes the function DECODE. 4 Related Works Researches focused on the automatic adaptation between different corpora can be roughly classified into two kinds, adaptation between different domains (with different statistical distribution) (Blitzer et al., 2006; Daum´e III, 2007), and adaptation between different annotation guidelines (Jiang et al., 2009; Zhu et al., 2011). There are also some efforts that totally or partially resort to manual transformation rules, to conduct treebank conversion (Cahill and Mccarthy, 2002; Hockenmaier and Steedman, 2007; Clark and Curran, 2009), and word segmentation guideline transformation (Gao et al., 2004; Mi et al., 2008). This work focuses on the automatic transformation between annotation guidelines, and proposes better annotation transformation technologies to improve the transformation accuracy and the util</context>
</contexts>
<marker>Blitzer, McDonald, Pereira, 2006</marker>
<rawString>John Blitzer, Ryan McDonald, and Fernando Pereira. 2006. Domain adaptation with structural correspondence learning. In Proceedings ofEMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aoife Cahill</author>
<author>Mairead Mccarthy</author>
</authors>
<title>Automatic annotation of the penn treebank with lfg f-structure information.</title>
<date>2002</date>
<booktitle>In in Proceedings of the LREC Workshop.</booktitle>
<contexts>
<context position="19077" citStr="Cahill and Mccarthy, 2002" startWordPosition="2898" endWordPosition="2901">odel is introduced and the enhanced scoring function above is used when the function TRANSANNOTATE invokes the function DECODE. 4 Related Works Researches focused on the automatic adaptation between different corpora can be roughly classified into two kinds, adaptation between different domains (with different statistical distribution) (Blitzer et al., 2006; Daum´e III, 2007), and adaptation between different annotation guidelines (Jiang et al., 2009; Zhu et al., 2011). There are also some efforts that totally or partially resort to manual transformation rules, to conduct treebank conversion (Cahill and Mccarthy, 2002; Hockenmaier and Steedman, 2007; Clark and Curran, 2009), and word segmentation guideline transformation (Gao et al., 2004; Mi et al., 2008). This work focuses on the automatic transformation between annotation guidelines, and proposes better annotation transformation technologies to improve the transformation accuracy and the utilization rate of human-annotated knowledge. The iterative training procedure proposed in this work shares some similarity with the co-training algorithm in parsing (Sarkar, 2001), where the training procedure lets two different models learn from each other during par</context>
</contexts>
<marker>Cahill, Mccarthy, 2002</marker>
<rawString>Aoife Cahill and Mairead Mccarthy. 2002. Automatic annotation of the penn treebank with lfg f-structure information. In in Proceedings of the LREC Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>James R Curran</author>
</authors>
<title>Comparing the accuracy of ccg and penn treebank parsers.</title>
<date>2009</date>
<booktitle>In Proceedings ofACL-IJCNLP.</booktitle>
<contexts>
<context position="19134" citStr="Clark and Curran, 2009" startWordPosition="2906" endWordPosition="2909">is used when the function TRANSANNOTATE invokes the function DECODE. 4 Related Works Researches focused on the automatic adaptation between different corpora can be roughly classified into two kinds, adaptation between different domains (with different statistical distribution) (Blitzer et al., 2006; Daum´e III, 2007), and adaptation between different annotation guidelines (Jiang et al., 2009; Zhu et al., 2011). There are also some efforts that totally or partially resort to manual transformation rules, to conduct treebank conversion (Cahill and Mccarthy, 2002; Hockenmaier and Steedman, 2007; Clark and Curran, 2009), and word segmentation guideline transformation (Gao et al., 2004; Mi et al., 2008). This work focuses on the automatic transformation between annotation guidelines, and proposes better annotation transformation technologies to improve the transformation accuracy and the utilization rate of human-annotated knowledge. The iterative training procedure proposed in this work shares some similarity with the co-training algorithm in parsing (Sarkar, 2001), where the training procedure lets two different models learn from each other during parsing the raw text. The key idea of co-training is utilize</context>
</contexts>
<marker>Clark, Curran, 2009</marker>
<rawString>Stephen Clark and James R. Curran. 2009. Comparing the accuracy of ccg and penn treebank parsers. In Proceedings ofACL-IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Brian Roark</author>
</authors>
<title>Incremental parsing with the perceptron algorithm.</title>
<date>2004</date>
<booktitle>In Proceedings ofACL</booktitle>
<contexts>
<context position="6985" citStr="Collins and Roark, 2004" startWordPosition="1046" endWordPosition="1049">e adopt the 4 boundary tags of Ng and Low (2004), b, m, e and s, where b, m and e mean the beginning, the middle and the end of a word, and s indicates a single-character word. The word segmentation result can be generated by splitting the labeled character sequence into subsequences of pattern s or bm*e, indicating single-character words or multi-character words, respectively. We choose the perceptron algorithm (Collins, 2002) to train the character classifier. It is an online training algorithm and has been successfully used in many NLP tasks, including POS tagging (Collins, 2002), parsing (Collins and Roark, 2004) and word segmentation (Zhang and Clark, 2007; Jiang et al., 2008; Zhang and Clark, 2010). The training procedure learns a discriminative model mapping from the inputs x E X to the outputs y E Y , where X is the set of sentences in the training corpus and Y is the set of corresponding labeled results. We use the function GEN(x) to enumerate the candidate results of an input x, and the function 4) to map a training example (x, y) E X x Y to a feature vector 4)(x, y) E Rd. Given the character sequence x, the decoder finds the output F(x) that maximizes the score function: F(x) = argmax yEGEN(x) </context>
</contexts>
<marker>Collins, Roark, 2004</marker>
<rawString>Michael Collins and Brian Roark. 2004. Incremental parsing with the perceptron algorithm. In Proceedings ofACL 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>1--8</pages>
<location>Philadelphia, USA.</location>
<contexts>
<context position="6792" citStr="Collins, 2002" startWordPosition="1018" endWordPosition="1019"> C� + 4)(xi, yi) − 4)(xi, zi) 8: Output: Parameters C� solved in a character classification approach by extending the boundary tags to include POS information. For word segmentation we adopt the 4 boundary tags of Ng and Low (2004), b, m, e and s, where b, m and e mean the beginning, the middle and the end of a word, and s indicates a single-character word. The word segmentation result can be generated by splitting the labeled character sequence into subsequences of pattern s or bm*e, indicating single-character words or multi-character words, respectively. We choose the perceptron algorithm (Collins, 2002) to train the character classifier. It is an online training algorithm and has been successfully used in many NLP tasks, including POS tagging (Collins, 2002), parsing (Collins and Roark, 2004) and word segmentation (Zhang and Clark, 2007; Jiang et al., 2008; Zhang and Clark, 2010). The training procedure learns a discriminative model mapping from the inputs x E X to the outputs y E Y , where X is the set of sentences in the training corpus and Y is the set of corresponding labeled results. We use the function GEN(x) to enumerate the candidate results of an input x, and the function 4) to map </context>
<context position="8107" citStr="Collins, 2002" startWordPosition="1251" endWordPosition="1252">the decoder finds the output F(x) that maximizes the score function: F(x) = argmax yEGEN(x) = argmax yEGEN(x) Where C� E Rd is the parameter vector (that is, the discriminative model) and 4)(x, y) · C� is the inner product of 4)(x, y) and a. Algorithm 1 shows the perceptron algorithm for tuning the parameter a. The “averaged parameters” S(y|a, 4), x) 4)(x, y) · C� (1) 413 Type Feature Templates Unigram C−2 C−1 C0 C1 C2 Bigram C−2C−1 C−1C0 C0C1 C1C2 C−1C1 Property Pu(C0) T (C−2)T (C−1)T (C0)T (C1)T (C2) Table 1: Feature templates for classification-based Chinese segmentation model. technology (Collins, 2002) is used for better performance. The feature templates for the classifier is shown in Table 1. Co denotes the current character, while C_i/Ci denote the ith character to the left/right of Co. The function Pu(·) returns true for a punctuation character and false for others, the function T(·) classifies a character into four types: number, date, English letter and others. 3 Iterative and Predict-Self Annotation Transformation This section first describes the technology of automatic annotation transformation, then introduces the two optimization strategies, iterative training and predict-self ree</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms. In Proceedings of EMNLP, pages 1–8, Philadelphia, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
</authors>
<title>Frustratingly easy domain adaptation.</title>
<date>2007</date>
<booktitle>In Proceedings ofACL.</booktitle>
<marker>Daum´e, 2007</marker>
<rawString>Hal Daum´e III. 2007. Frustratingly easy domain adaptation. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
</authors>
<title>Unsupervised search-based structured prediction.</title>
<date>2009</date>
<booktitle>In Proceedings ofICML.</booktitle>
<marker>Daum´e, 2009</marker>
<rawString>Hal Daum´e III. 2009. Unsupervised search-based structured prediction. In Proceedings ofICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianfeng Gao</author>
<author>Andi Wu</author>
<author>Mu Li</author>
</authors>
<title>Chang-Ning Huang, Hongqiao Li, Xinsong Xia, and Haowei Qin.</title>
<date>2004</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="19200" citStr="Gao et al., 2004" startWordPosition="2915" endWordPosition="2918">lated Works Researches focused on the automatic adaptation between different corpora can be roughly classified into two kinds, adaptation between different domains (with different statistical distribution) (Blitzer et al., 2006; Daum´e III, 2007), and adaptation between different annotation guidelines (Jiang et al., 2009; Zhu et al., 2011). There are also some efforts that totally or partially resort to manual transformation rules, to conduct treebank conversion (Cahill and Mccarthy, 2002; Hockenmaier and Steedman, 2007; Clark and Curran, 2009), and word segmentation guideline transformation (Gao et al., 2004; Mi et al., 2008). This work focuses on the automatic transformation between annotation guidelines, and proposes better annotation transformation technologies to improve the transformation accuracy and the utilization rate of human-annotated knowledge. The iterative training procedure proposed in this work shares some similarity with the co-training algorithm in parsing (Sarkar, 2001), where the training procedure lets two different models learn from each other during parsing the raw text. The key idea of co-training is utilize the complementarity of different parsing models to mine additiona</context>
</contexts>
<marker>Gao, Wu, Li, 2004</marker>
<rawString>Jianfeng Gao, Andi Wu, Mu Li, Chang-Ning Huang, Hongqiao Li, Xinsong Xia, and Haowei Qin. 2004. Adaptive chinese word segmentation. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Hewlett</author>
<author>Paul Cohen</author>
</authors>
<title>Fully unsupervised word segmentation with bve and mdl.</title>
<date>2011</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="21318" citStr="Hewlett and Cohen, 2011" startWordPosition="3248" endWordPosition="3251">rio of annotation transformation to improve transformation accuracy. In recent years many works have been devoted to the word segmentation task. For example, the introduction of global training or complicated features (Zhang and Clark, 2007; Zhang and Clark, 2010); the investigation of word structures (Li, 2011); the strategies of hybrid, joint or stacked modeling (Nakagawa and Uchimoto, 2007; Kruengkrai et al., 2009; Wang et al., 2010; Sun, 2011), and the semisupervised and unsupervised technologies utilizing raw text (Zhao and Kit, 2008; Johnson and Goldwater, 2009; Mochihashi et al., 2009; Hewlett and Cohen, 2011). We estimate that the annotation transformation technologies can be adopted jointly with complicated features, system combination and semisupervised/unsupervised technologies to further improve segmentation performance. 5 Experiments and Analysis We perform annotation transformation from People’s Daily (PD) (Yu et al., 2001) to Penn Chinese Treebank 5.0 (CTB) (Xue et al., 2005), following the same experimental setting as the annotation adaptation work (Jiang et al., 2009) for convenience of comparison. The two corpora are segmented following different segmentation guidelines and differ largel</context>
</contexts>
<marker>Hewlett, Cohen, 2011</marker>
<rawString>Daniel Hewlett and Paul Cohen. 2011. Fully unsupervised word segmentation with bve and mdl. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hockenmaier</author>
<author>Mark Steedman</author>
</authors>
<title>Ccgbank: a corpus of ccg derivations and dependency structures extracted from the penn treebank.</title>
<date>2007</date>
<booktitle>In Computational Linguistics,</booktitle>
<volume>33</volume>
<issue>3</issue>
<pages>355--396</pages>
<contexts>
<context position="19109" citStr="Hockenmaier and Steedman, 2007" startWordPosition="2902" endWordPosition="2905">enhanced scoring function above is used when the function TRANSANNOTATE invokes the function DECODE. 4 Related Works Researches focused on the automatic adaptation between different corpora can be roughly classified into two kinds, adaptation between different domains (with different statistical distribution) (Blitzer et al., 2006; Daum´e III, 2007), and adaptation between different annotation guidelines (Jiang et al., 2009; Zhu et al., 2011). There are also some efforts that totally or partially resort to manual transformation rules, to conduct treebank conversion (Cahill and Mccarthy, 2002; Hockenmaier and Steedman, 2007; Clark and Curran, 2009), and word segmentation guideline transformation (Gao et al., 2004; Mi et al., 2008). This work focuses on the automatic transformation between annotation guidelines, and proposes better annotation transformation technologies to improve the transformation accuracy and the utilization rate of human-annotated knowledge. The iterative training procedure proposed in this work shares some similarity with the co-training algorithm in parsing (Sarkar, 2001), where the training procedure lets two different models learn from each other during parsing the raw text. The key idea </context>
</contexts>
<marker>Hockenmaier, Steedman, 2007</marker>
<rawString>Julia Hockenmaier and Mark Steedman. 2007. Ccgbank: a corpus of ccg derivations and dependency structures extracted from the penn treebank. In Computational Linguistics, volume 33(3), pages 355–396.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenbin Jiang</author>
<author>Liang Huang</author>
<author>Yajuan Lv</author>
<author>Qun Liu</author>
</authors>
<title>A cascaded linear model for joint chinese word segmentation and part-of-speech tagging.</title>
<date>2008</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="7050" citStr="Jiang et al., 2008" startWordPosition="1057" endWordPosition="1060">b, m and e mean the beginning, the middle and the end of a word, and s indicates a single-character word. The word segmentation result can be generated by splitting the labeled character sequence into subsequences of pattern s or bm*e, indicating single-character words or multi-character words, respectively. We choose the perceptron algorithm (Collins, 2002) to train the character classifier. It is an online training algorithm and has been successfully used in many NLP tasks, including POS tagging (Collins, 2002), parsing (Collins and Roark, 2004) and word segmentation (Zhang and Clark, 2007; Jiang et al., 2008; Zhang and Clark, 2010). The training procedure learns a discriminative model mapping from the inputs x E X to the outputs y E Y , where X is the set of sentences in the training corpus and Y is the set of corresponding labeled results. We use the function GEN(x) to enumerate the candidate results of an input x, and the function 4) to map a training example (x, y) E X x Y to a feature vector 4)(x, y) E Rd. Given the character sequence x, the decoder finds the output F(x) that maximizes the score function: F(x) = argmax yEGEN(x) = argmax yEGEN(x) Where C� E Rd is the parameter vector (that is,</context>
<context position="27026" citStr="Jiang et al., 2008" startWordPosition="4150" endWordPosition="4153">upper horizontal line shows the performance of predict-self filtration. We find that predict-self filtration brings slight improvement over the baseline, and predictself reestimation outperforms the filtration strategy when λ falls in a proper range. An initial analysis on the experimental results of predict-self filtration Accuracy (F%) 96.4 96.2 95.8 95.6 95.4 96 Iterative training with predict-self reestimation Iterative training 418 Model Time (s) Accuracy (F1%) SPD → CTB Anno. Adapt. 4.39 97.67 Opt. Trans. 1.33 97.97 PD → CTB Anno. Adapt. 4.76 98.15 Opt. Trans. 1.37 98.43 Previous Works (Jiang et al., 2008) 97.85 (Kruengkrai et al., 2009) 97.87 (Zhang and Clark, 2010) 97.79 (Sun, 2011) 98.17 Table 6: The performance of the iterative annotation transformation with predict-self reestimation compared with annotation adaptation. shows that, the filtration discards 5% of the training sentences and these discarded sentences contain nearly 10% of training words. It can be confirmed that the sentences discarded by predict-self filtration are much longer and more complicated. With a properly tuned weight, predict-self reestimation can make better use of the training data. The best Fmeasure improvement ac</context>
</contexts>
<marker>Jiang, Huang, Lv, Liu, 2008</marker>
<rawString>Wenbin Jiang, Liang Huang, Yajuan Lv, and Qun Liu. 2008. A cascaded linear model for joint chinese word segmentation and part-of-speech tagging. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenbin Jiang</author>
<author>Liang Huang</author>
<author>Qun Liu</author>
</authors>
<title>Automatic adaptation of annotation standards: Chinese word segmentation and pos tagging–a case study.</title>
<date>2009</date>
<booktitle>In Proceedings of the 47th ACL.</booktitle>
<contexts>
<context position="1447" citStr="Jiang et al., 2009" startWordPosition="194" endWordPosition="197">raining strategy together with predictself reestimation brings significant improvement over the simple annotation transformation baseline, and leads to classifiers with significantly higher accuracy and several times faster processing than annotation adaptation does. On the Penn Chinese Treebank 5.0, it achieves an F-measure of 98.43%, significantly outperforms previous works although using a single classifier with only local features. 1 Introduction Annotation guideline adaptation depicts a general pipeline to integrate the knowledge of corpora with different underling annotation guidelines (Jiang et al., 2009). In annotation adaptation two classifiers are cascaded together, where the classification results of the lower classifier are used as guiding features of the upper classifier, in order to achieve more accurate classification. This method can automatically adapt the divergence between different annotation guidelines and bring improvement to Chinese word segmentation. However, the need of cascaded classification decisions makes it less practical for tasks of high computational complexity such as parsing, and less efficient to incorporate more than two annotated corpora. In this paper, we first </context>
<context position="9745" citStr="Jiang et al., 2009" startWordPosition="1487" endWordPosition="1490"> adopted jointly to obtain better transformation performance. 3.1 Automatic Annotation Transformation Annotation adaptation can integrate the knowledge from two corpora with different underling annotation guidelines. First, a classifier (source classifier) is trained on the corpus (source corpus) with an annotation standard (source annotation) not desired, it is then used to classify the corpus (target corpus) with the annotation standard (target annotation) we want. Then a second classifier (transformation classifier 1) is trained on the target corpus with 1It is called target classifier in (Jiang et al., 2009). We think that transformation classifier better reflects its role, the Type Feature Templates Baseline C−2 C−1 C0 C1 C2 C−2C−1 C−1C0 C0C1 C1C2 C−1C1 Pu(C0) T (C−2)T (C−1)T (C0)T (C1)T (C2) Guiding α C−2 ◦ α C−1 ◦ α C0 ◦ α C1 ◦ α C2 ◦ α C−2C−1 ◦ α C−1C0 ◦ α C0C1 ◦ α C1C2 ◦ α C−1C1 ◦ α Pu(C0) ◦ α T(C−2)T(C−1)T(C0)T(C1)T(C2) ◦ α Table 2: Feature templates for annotation transformation, where α is short for α(C0), representing the source annotation of C0. the source classifier’s classification result as guiding features. In decoding, a raw sentence is first decoded by the source classifier, and t</context>
<context position="18906" citStr="Jiang et al., 2009" startWordPosition="2870" endWordPosition="2873">ght parameter λ is tuned on the developing set. To integrating the predict-self reestimation into the iterative transformation training, a reversed transformation model is introduced and the enhanced scoring function above is used when the function TRANSANNOTATE invokes the function DECODE. 4 Related Works Researches focused on the automatic adaptation between different corpora can be roughly classified into two kinds, adaptation between different domains (with different statistical distribution) (Blitzer et al., 2006; Daum´e III, 2007), and adaptation between different annotation guidelines (Jiang et al., 2009; Zhu et al., 2011). There are also some efforts that totally or partially resort to manual transformation rules, to conduct treebank conversion (Cahill and Mccarthy, 2002; Hockenmaier and Steedman, 2007; Clark and Curran, 2009), and word segmentation guideline transformation (Gao et al., 2004; Mi et al., 2008). This work focuses on the automatic transformation between annotation guidelines, and proposes better annotation transformation technologies to improve the transformation accuracy and the utilization rate of human-annotated knowledge. The iterative training procedure proposed in this wo</context>
<context position="21795" citStr="Jiang et al., 2009" startWordPosition="3318" endWordPosition="3321">supervised technologies utilizing raw text (Zhao and Kit, 2008; Johnson and Goldwater, 2009; Mochihashi et al., 2009; Hewlett and Cohen, 2011). We estimate that the annotation transformation technologies can be adopted jointly with complicated features, system combination and semisupervised/unsupervised technologies to further improve segmentation performance. 5 Experiments and Analysis We perform annotation transformation from People’s Daily (PD) (Yu et al., 2001) to Penn Chinese Treebank 5.0 (CTB) (Xue et al., 2005), following the same experimental setting as the annotation adaptation work (Jiang et al., 2009) for convenience of comparison. The two corpora are segmented following different segmentation guidelines and differ largely in quantity of data. CTB is smaller in size with about 0.5M words, while PD is much larger, containing nearly 6M words. Table 5: Comparison of the baseline annotation transformation, annotation adaptation and a simple corpus merging strategy. To approximate more general scenarios of annotation adaptation problems, we extract from PD a subset which is comparable to CTB in size. We randomly select 20, 000 sentences (0.45M words) from the PD training data as the new trainin</context>
<context position="24469" citStr="Jiang et al., 2009" startWordPosition="3749" endWordPosition="3752">2: Performance of predict-self filtration and predict-self reestimation. 96.4 96.2 95.8 95.6 95.4 96 Iterative training Baseline annotation transformation 96.4 96.2 95.8 95.6 95.4 96 Predict-self reestimation Predict-self filtration Baseline annotation transformation Accuracy (F%) Accuracy (F%) 5.2 Annotation Transformation vs. Annotation Adaptation Experiments of annotation transformation are conducted on the direction of SPD-to-CTB. The transformed corpus can be merged into the regular corpus, so as to train an enhanced classifier. As comparison, the cascaded model of annotation adaptation (Jiang et al., 2009) is faithfully implemented (yet using our feature representation) and tested on the same adaptation direction. Table 5 shows the performances of the classifiers resulted by the baseline annotation transformation and annotation adaptation, as well as the classifier trained on the directly merged corpus. The time costs for decoding are also listed to facilitate the comparison of practicality. We find that the simple corpus merging strategy leads to dramatic decrease in accuracy, due to the different and incompatible annotation guidelines. The baseline annotation transformation method leads to a </context>
</contexts>
<marker>Jiang, Huang, Liu, 2009</marker>
<rawString>Wenbin Jiang, Liang Huang, and Qun Liu. 2009. Automatic adaptation of annotation standards: Chinese word segmentation and pos tagging–a case study. In Proceedings of the 47th ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
<author>Sharon Goldwater</author>
</authors>
<title>Improving nonparameteric bayesian inference: experiments on unsupervised word segmentation with adaptor grammars.</title>
<date>2009</date>
<booktitle>In Proceedings ofNAACL.</booktitle>
<contexts>
<context position="21267" citStr="Johnson and Goldwater, 2009" startWordPosition="3239" endWordPosition="3243">ed dependency parsing. We adapt this idea to the scenario of annotation transformation to improve transformation accuracy. In recent years many works have been devoted to the word segmentation task. For example, the introduction of global training or complicated features (Zhang and Clark, 2007; Zhang and Clark, 2010); the investigation of word structures (Li, 2011); the strategies of hybrid, joint or stacked modeling (Nakagawa and Uchimoto, 2007; Kruengkrai et al., 2009; Wang et al., 2010; Sun, 2011), and the semisupervised and unsupervised technologies utilizing raw text (Zhao and Kit, 2008; Johnson and Goldwater, 2009; Mochihashi et al., 2009; Hewlett and Cohen, 2011). We estimate that the annotation transformation technologies can be adopted jointly with complicated features, system combination and semisupervised/unsupervised technologies to further improve segmentation performance. 5 Experiments and Analysis We perform annotation transformation from People’s Daily (PD) (Yu et al., 2001) to Penn Chinese Treebank 5.0 (CTB) (Xue et al., 2005), following the same experimental setting as the annotation adaptation work (Jiang et al., 2009) for convenience of comparison. The two corpora are segmented following </context>
</contexts>
<marker>Johnson, Goldwater, 2009</marker>
<rawString>Mark Johnson and Sharon Goldwater. 2009. Improving nonparameteric bayesian inference: experiments on unsupervised word segmentation with adaptor grammars. In Proceedings ofNAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Canasai Kruengkrai</author>
<author>Kiyotaka Uchimoto</author>
<author>Junichi Kazama</author>
<author>Yiou Wang</author>
<author>Kentaro Torisawa</author>
<author>Hitoshi Isahara</author>
</authors>
<title>An error-driven word-character hybrid model for joint chinese word segmentation and pos tagging.</title>
<date>2009</date>
<booktitle>In Proceedings ofACL-IJCNLP.</booktitle>
<contexts>
<context position="21114" citStr="Kruengkrai et al., 2009" startWordPosition="3215" endWordPosition="3218">. The predictself methodology is implicit in many unsupervised learning approaches, it has been successfully used by (Daum´e III, 2009) in unsupervised dependency parsing. We adapt this idea to the scenario of annotation transformation to improve transformation accuracy. In recent years many works have been devoted to the word segmentation task. For example, the introduction of global training or complicated features (Zhang and Clark, 2007; Zhang and Clark, 2010); the investigation of word structures (Li, 2011); the strategies of hybrid, joint or stacked modeling (Nakagawa and Uchimoto, 2007; Kruengkrai et al., 2009; Wang et al., 2010; Sun, 2011), and the semisupervised and unsupervised technologies utilizing raw text (Zhao and Kit, 2008; Johnson and Goldwater, 2009; Mochihashi et al., 2009; Hewlett and Cohen, 2011). We estimate that the annotation transformation technologies can be adopted jointly with complicated features, system combination and semisupervised/unsupervised technologies to further improve segmentation performance. 5 Experiments and Analysis We perform annotation transformation from People’s Daily (PD) (Yu et al., 2001) to Penn Chinese Treebank 5.0 (CTB) (Xue et al., 2005), following the</context>
<context position="27058" citStr="Kruengkrai et al., 2009" startWordPosition="4155" endWordPosition="4158"> the performance of predict-self filtration. We find that predict-self filtration brings slight improvement over the baseline, and predictself reestimation outperforms the filtration strategy when λ falls in a proper range. An initial analysis on the experimental results of predict-self filtration Accuracy (F%) 96.4 96.2 95.8 95.6 95.4 96 Iterative training with predict-self reestimation Iterative training 418 Model Time (s) Accuracy (F1%) SPD → CTB Anno. Adapt. 4.39 97.67 Opt. Trans. 1.33 97.97 PD → CTB Anno. Adapt. 4.76 98.15 Opt. Trans. 1.37 98.43 Previous Works (Jiang et al., 2008) 97.85 (Kruengkrai et al., 2009) 97.87 (Zhang and Clark, 2010) 97.79 (Sun, 2011) 98.17 Table 6: The performance of the iterative annotation transformation with predict-self reestimation compared with annotation adaptation. shows that, the filtration discards 5% of the training sentences and these discarded sentences contain nearly 10% of training words. It can be confirmed that the sentences discarded by predict-self filtration are much longer and more complicated. With a properly tuned weight, predict-self reestimation can make better use of the training data. The best Fmeasure improvement achieved over the annotation trans</context>
</contexts>
<marker>Kruengkrai, Uchimoto, Kazama, Wang, Torisawa, Isahara, 2009</marker>
<rawString>Canasai Kruengkrai, Kiyotaka Uchimoto, Junichi Kazama, Yiou Wang, Kentaro Torisawa, and Hitoshi Isahara. 2009. An error-driven word-character hybrid model for joint chinese word segmentation and pos tagging. In Proceedings ofACL-IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhongguo Li</author>
</authors>
<title>Parsing the internal structure of words: A new paradigm for chineseword segmentation.</title>
<date>2011</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="21007" citStr="Li, 2011" startWordPosition="3201" endWordPosition="3202">M Test 01 1.07M Table 3: Data partitioning for CTB and PD. to train the transformation models. The predictself methodology is implicit in many unsupervised learning approaches, it has been successfully used by (Daum´e III, 2009) in unsupervised dependency parsing. We adapt this idea to the scenario of annotation transformation to improve transformation accuracy. In recent years many works have been devoted to the word segmentation task. For example, the introduction of global training or complicated features (Zhang and Clark, 2007; Zhang and Clark, 2010); the investigation of word structures (Li, 2011); the strategies of hybrid, joint or stacked modeling (Nakagawa and Uchimoto, 2007; Kruengkrai et al., 2009; Wang et al., 2010; Sun, 2011), and the semisupervised and unsupervised technologies utilizing raw text (Zhao and Kit, 2008; Johnson and Goldwater, 2009; Mochihashi et al., 2009; Hewlett and Cohen, 2011). We estimate that the annotation transformation technologies can be adopted jointly with complicated features, system combination and semisupervised/unsupervised technologies to further improve segmentation performance. 5 Experiments and Analysis We perform annotation transformation from</context>
</contexts>
<marker>Li, 2011</marker>
<rawString>Zhongguo Li. 2011. Parsing the internal structure of words: A new paradigm for chineseword segmentation. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haitao Mi</author>
<author>Deyi Xiong</author>
<author>Qun Liu</author>
</authors>
<title>Research on strategy of integrating chinese lexical analysis and parser.</title>
<date>2008</date>
<journal>In Journal of Chinese Information Processing.</journal>
<contexts>
<context position="19218" citStr="Mi et al., 2008" startWordPosition="2919" endWordPosition="2922">ches focused on the automatic adaptation between different corpora can be roughly classified into two kinds, adaptation between different domains (with different statistical distribution) (Blitzer et al., 2006; Daum´e III, 2007), and adaptation between different annotation guidelines (Jiang et al., 2009; Zhu et al., 2011). There are also some efforts that totally or partially resort to manual transformation rules, to conduct treebank conversion (Cahill and Mccarthy, 2002; Hockenmaier and Steedman, 2007; Clark and Curran, 2009), and word segmentation guideline transformation (Gao et al., 2004; Mi et al., 2008). This work focuses on the automatic transformation between annotation guidelines, and proposes better annotation transformation technologies to improve the transformation accuracy and the utilization rate of human-annotated knowledge. The iterative training procedure proposed in this work shares some similarity with the co-training algorithm in parsing (Sarkar, 2001), where the training procedure lets two different models learn from each other during parsing the raw text. The key idea of co-training is utilize the complementarity of different parsing models to mine additional training data fr</context>
</contexts>
<marker>Mi, Xiong, Liu, 2008</marker>
<rawString>Haitao Mi, Deyi Xiong, and Qun Liu. 2008. Research on strategy of integrating chinese lexical analysis and parser. In Journal of Chinese Information Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daichi Mochihashi</author>
<author>Takeshi Yamada</author>
<author>Naonori Ueda</author>
</authors>
<title>Bayesian unsupervised word segmentation with nested pitman-yor language modeling.</title>
<date>2009</date>
<booktitle>In Proceedings ofACL-IJCNLP.</booktitle>
<contexts>
<context position="21292" citStr="Mochihashi et al., 2009" startWordPosition="3244" endWordPosition="3247">pt this idea to the scenario of annotation transformation to improve transformation accuracy. In recent years many works have been devoted to the word segmentation task. For example, the introduction of global training or complicated features (Zhang and Clark, 2007; Zhang and Clark, 2010); the investigation of word structures (Li, 2011); the strategies of hybrid, joint or stacked modeling (Nakagawa and Uchimoto, 2007; Kruengkrai et al., 2009; Wang et al., 2010; Sun, 2011), and the semisupervised and unsupervised technologies utilizing raw text (Zhao and Kit, 2008; Johnson and Goldwater, 2009; Mochihashi et al., 2009; Hewlett and Cohen, 2011). We estimate that the annotation transformation technologies can be adopted jointly with complicated features, system combination and semisupervised/unsupervised technologies to further improve segmentation performance. 5 Experiments and Analysis We perform annotation transformation from People’s Daily (PD) (Yu et al., 2001) to Penn Chinese Treebank 5.0 (CTB) (Xue et al., 2005), following the same experimental setting as the annotation adaptation work (Jiang et al., 2009) for convenience of comparison. The two corpora are segmented following different segmentation gu</context>
</contexts>
<marker>Mochihashi, Yamada, Ueda, 2009</marker>
<rawString>Daichi Mochihashi, Takeshi Yamada, and Naonori Ueda. 2009. Bayesian unsupervised word segmentation with nested pitman-yor language modeling. In Proceedings ofACL-IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tetsuji Nakagawa</author>
<author>Kiyotaka Uchimoto</author>
</authors>
<title>A hybrid approach to word segmentation and pos tagging.</title>
<date>2007</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="21089" citStr="Nakagawa and Uchimoto, 2007" startWordPosition="3211" endWordPosition="3214">ain the transformation models. The predictself methodology is implicit in many unsupervised learning approaches, it has been successfully used by (Daum´e III, 2009) in unsupervised dependency parsing. We adapt this idea to the scenario of annotation transformation to improve transformation accuracy. In recent years many works have been devoted to the word segmentation task. For example, the introduction of global training or complicated features (Zhang and Clark, 2007; Zhang and Clark, 2010); the investigation of word structures (Li, 2011); the strategies of hybrid, joint or stacked modeling (Nakagawa and Uchimoto, 2007; Kruengkrai et al., 2009; Wang et al., 2010; Sun, 2011), and the semisupervised and unsupervised technologies utilizing raw text (Zhao and Kit, 2008; Johnson and Goldwater, 2009; Mochihashi et al., 2009; Hewlett and Cohen, 2011). We estimate that the annotation transformation technologies can be adopted jointly with complicated features, system combination and semisupervised/unsupervised technologies to further improve segmentation performance. 5 Experiments and Analysis We perform annotation transformation from People’s Daily (PD) (Yu et al., 2001) to Penn Chinese Treebank 5.0 (CTB) (Xue et </context>
</contexts>
<marker>Nakagawa, Uchimoto, 2007</marker>
<rawString>Tetsuji Nakagawa and Kiyotaka Uchimoto. 2007. A hybrid approach to word segmentation and pos tagging. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hwee Tou Ng</author>
<author>Jin Kiat Low</author>
</authors>
<title>Chinese part-ofspeech tagging: One-at-a-time or all-at-once? wordbased or character-based?</title>
<date>2004</date>
<booktitle>In Proceedings ofEMNLP.</booktitle>
<contexts>
<context position="5906" citStr="Ng and Low (2004)" startWordPosition="853" endWordPosition="856">f the paper is organized as follows. Section 2 describes the classification-based Chinese word segmentation method. Section 3 details the simple annotation transformation algorithm and the two optimization methods. After the introduction of related works in section 4, we give the experimental results on Chinese word segmentation in section 5. 2 Classification-Based Chinese Word Segmentation Chinese word segmentation can be formalized as the problem of sequence labeling (Xue and Shen, 2003), where each character in the sentence is given a boundary tag denoting its position in a word. Following Ng and Low (2004), joint word segmentation and part-of-speech (POS) tagging can also be Algorithm 1 Perceptron training algorithm. 1: Input: Training examples (xi, yi) 2: C�+— 0 3: fort +— 1 .. T do 4: for i +— 1 .. N do 5: zi +— argmaxzEGEN(xi) 4)(xi, z) · C� 6: if zi =� yi then 7: C� +— C� + 4)(xi, yi) − 4)(xi, zi) 8: Output: Parameters C� solved in a character classification approach by extending the boundary tags to include POS information. For word segmentation we adopt the 4 boundary tags of Ng and Low (2004), b, m, e and s, where b, m and e mean the beginning, the middle and the end of a word, and s ind</context>
</contexts>
<marker>Ng, Low, 2004</marker>
<rawString>Hwee Tou Ng and Jin Kiat Low. 2004. Chinese part-ofspeech tagging: One-at-a-time or all-at-once? wordbased or character-based? In Proceedings ofEMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anoop Sarkar</author>
</authors>
<title>Applying co-training methods to statistical parsing.</title>
<date>2001</date>
<booktitle>In Proceedings ofNAACL.</booktitle>
<contexts>
<context position="19588" citStr="Sarkar, 2001" startWordPosition="2970" endWordPosition="2971">ially resort to manual transformation rules, to conduct treebank conversion (Cahill and Mccarthy, 2002; Hockenmaier and Steedman, 2007; Clark and Curran, 2009), and word segmentation guideline transformation (Gao et al., 2004; Mi et al., 2008). This work focuses on the automatic transformation between annotation guidelines, and proposes better annotation transformation technologies to improve the transformation accuracy and the utilization rate of human-annotated knowledge. The iterative training procedure proposed in this work shares some similarity with the co-training algorithm in parsing (Sarkar, 2001), where the training procedure lets two different models learn from each other during parsing the raw text. The key idea of co-training is utilize the complementarity of different parsing models to mine additional training data from raw text, while iterative training for annotation transformation emphasizes the iterative optimization of the parellelly annotated corpora used 416 Test on (F1%) CTB SPD Train on 97.35 91.23(t 3.02) 86.65(t 10.70) 94.25 CTB SPD Table 4: Performance of the perceptron classifiers for Chinese word segmentation. Model Baseline Time (s) 1.33 4.39 1.33 1.21 Accuracy (F1%</context>
</contexts>
<marker>Sarkar, 2001</marker>
<rawString>Anoop Sarkar. 2001. Applying co-training methods to statistical parsing. In Proceedings ofNAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Weiwei Sun</author>
</authors>
<title>A stacked sub-word model for joint chinese word segmentation and part-of-speech tagging.</title>
<date>2011</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="21145" citStr="Sun, 2011" startWordPosition="3223" endWordPosition="3224"> many unsupervised learning approaches, it has been successfully used by (Daum´e III, 2009) in unsupervised dependency parsing. We adapt this idea to the scenario of annotation transformation to improve transformation accuracy. In recent years many works have been devoted to the word segmentation task. For example, the introduction of global training or complicated features (Zhang and Clark, 2007; Zhang and Clark, 2010); the investigation of word structures (Li, 2011); the strategies of hybrid, joint or stacked modeling (Nakagawa and Uchimoto, 2007; Kruengkrai et al., 2009; Wang et al., 2010; Sun, 2011), and the semisupervised and unsupervised technologies utilizing raw text (Zhao and Kit, 2008; Johnson and Goldwater, 2009; Mochihashi et al., 2009; Hewlett and Cohen, 2011). We estimate that the annotation transformation technologies can be adopted jointly with complicated features, system combination and semisupervised/unsupervised technologies to further improve segmentation performance. 5 Experiments and Analysis We perform annotation transformation from People’s Daily (PD) (Yu et al., 2001) to Penn Chinese Treebank 5.0 (CTB) (Xue et al., 2005), following the same experimental setting as t</context>
<context position="27106" citStr="Sun, 2011" startWordPosition="4165" endWordPosition="4166">ict-self filtration brings slight improvement over the baseline, and predictself reestimation outperforms the filtration strategy when λ falls in a proper range. An initial analysis on the experimental results of predict-self filtration Accuracy (F%) 96.4 96.2 95.8 95.6 95.4 96 Iterative training with predict-self reestimation Iterative training 418 Model Time (s) Accuracy (F1%) SPD → CTB Anno. Adapt. 4.39 97.67 Opt. Trans. 1.33 97.97 PD → CTB Anno. Adapt. 4.76 98.15 Opt. Trans. 1.37 98.43 Previous Works (Jiang et al., 2008) 97.85 (Kruengkrai et al., 2009) 97.87 (Zhang and Clark, 2010) 97.79 (Sun, 2011) 98.17 Table 6: The performance of the iterative annotation transformation with predict-self reestimation compared with annotation adaptation. shows that, the filtration discards 5% of the training sentences and these discarded sentences contain nearly 10% of training words. It can be confirmed that the sentences discarded by predict-self filtration are much longer and more complicated. With a properly tuned weight, predict-self reestimation can make better use of the training data. The best Fmeasure improvement achieved over the annotation transformation baseline is 0.3 points, a little worse</context>
</contexts>
<marker>Sun, 2011</marker>
<rawString>Weiwei Sun. 2011. A stacked sub-word model for joint chinese word segmentation and part-of-speech tagging. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kun Wang</author>
<author>Chengqing Zong</author>
<author>Keh-Yih Su</author>
</authors>
<title>A character-based joint model for chinese word segmentation.</title>
<date>2010</date>
<booktitle>In Proceedings of COLING.</booktitle>
<contexts>
<context position="21133" citStr="Wang et al., 2010" startWordPosition="3219" endWordPosition="3222">logy is implicit in many unsupervised learning approaches, it has been successfully used by (Daum´e III, 2009) in unsupervised dependency parsing. We adapt this idea to the scenario of annotation transformation to improve transformation accuracy. In recent years many works have been devoted to the word segmentation task. For example, the introduction of global training or complicated features (Zhang and Clark, 2007; Zhang and Clark, 2010); the investigation of word structures (Li, 2011); the strategies of hybrid, joint or stacked modeling (Nakagawa and Uchimoto, 2007; Kruengkrai et al., 2009; Wang et al., 2010; Sun, 2011), and the semisupervised and unsupervised technologies utilizing raw text (Zhao and Kit, 2008; Johnson and Goldwater, 2009; Mochihashi et al., 2009; Hewlett and Cohen, 2011). We estimate that the annotation transformation technologies can be adopted jointly with complicated features, system combination and semisupervised/unsupervised technologies to further improve segmentation performance. 5 Experiments and Analysis We perform annotation transformation from People’s Daily (PD) (Yu et al., 2001) to Penn Chinese Treebank 5.0 (CTB) (Xue et al., 2005), following the same experimental </context>
</contexts>
<marker>Wang, Zong, Su, 2010</marker>
<rawString>Kun Wang, Chengqing Zong, and Keh-Yih Su. 2010. A character-based joint model for chinese word segmentation. In Proceedings of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
<author>Libin Shen</author>
</authors>
<title>Chinese word segmentation as lmr tagging.</title>
<date>2003</date>
<booktitle>In Proceedings of SIGHAN Workshop.</booktitle>
<contexts>
<context position="5783" citStr="Xue and Shen, 2003" startWordPosition="830" endWordPosition="833">e of 98.43%, significantly outperforms previous works although using a single classifier with only local features. The rest of the paper is organized as follows. Section 2 describes the classification-based Chinese word segmentation method. Section 3 details the simple annotation transformation algorithm and the two optimization methods. After the introduction of related works in section 4, we give the experimental results on Chinese word segmentation in section 5. 2 Classification-Based Chinese Word Segmentation Chinese word segmentation can be formalized as the problem of sequence labeling (Xue and Shen, 2003), where each character in the sentence is given a boundary tag denoting its position in a word. Following Ng and Low (2004), joint word segmentation and part-of-speech (POS) tagging can also be Algorithm 1 Perceptron training algorithm. 1: Input: Training examples (xi, yi) 2: C�+— 0 3: fort +— 1 .. T do 4: for i +— 1 .. N do 5: zi +— argmaxzEGEN(xi) 4)(xi, z) · C� 6: if zi =� yi then 7: C� +— C� + 4)(xi, yi) − 4)(xi, zi) 8: Output: Parameters C� solved in a character classification approach by extending the boundary tags to include POS information. For word segmentation we adopt the 4 boundary</context>
</contexts>
<marker>Xue, Shen, 2003</marker>
<rawString>Nianwen Xue and Libin Shen. 2003. Chinese word segmentation as lmr tagging. In Proceedings of SIGHAN Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
<author>Fei Xia</author>
<author>Fu-Dong Chiou</author>
<author>Martha Palmer</author>
</authors>
<title>The penn chinese treebank: Phrase structure annotation of a large corpus.</title>
<date>2005</date>
<journal>In Natural Language Engineering.</journal>
<contexts>
<context position="4859" citStr="Xue et al., 2005" startWordPosition="694" endWordPosition="697"> The predict-self reestimation is based on the following hypothesis, a better transformation result should be easier to be transformed back to the original form. The predict-self heuristic is also validated by Daum´e III (2009) in unsupervised dependency parsing. Experiments in Chinese word segmentation show that, the iterative training strategy together with predict-self reestimation brings significant improvement over the simple annotation transformation baseline. We perform optimized annotation transformation from the People’s Daily (Yu et al., 2001) to the Penn Chinese Treebank 5.0 (CTB) (Xue et al., 2005), in order to improve the word segmenter with CTB annotation guideline. Compared to annotation adaptation, the optimized annotation transformation strategy leads to classifiers with significantly higher accuracy and several times faster processing on the same data sets. On CTB 5.0, it achieves an Fmeasure of 98.43%, significantly outperforms previous works although using a single classifier with only local features. The rest of the paper is organized as follows. Section 2 describes the classification-based Chinese word segmentation method. Section 3 details the simple annotation transformation</context>
<context position="21699" citStr="Xue et al., 2005" startWordPosition="3303" endWordPosition="3306">o, 2007; Kruengkrai et al., 2009; Wang et al., 2010; Sun, 2011), and the semisupervised and unsupervised technologies utilizing raw text (Zhao and Kit, 2008; Johnson and Goldwater, 2009; Mochihashi et al., 2009; Hewlett and Cohen, 2011). We estimate that the annotation transformation technologies can be adopted jointly with complicated features, system combination and semisupervised/unsupervised technologies to further improve segmentation performance. 5 Experiments and Analysis We perform annotation transformation from People’s Daily (PD) (Yu et al., 2001) to Penn Chinese Treebank 5.0 (CTB) (Xue et al., 2005), following the same experimental setting as the annotation adaptation work (Jiang et al., 2009) for convenience of comparison. The two corpora are segmented following different segmentation guidelines and differ largely in quantity of data. CTB is smaller in size with about 0.5M words, while PD is much larger, containing nearly 6M words. Table 5: Comparison of the baseline annotation transformation, annotation adaptation and a simple corpus merging strategy. To approximate more general scenarios of annotation adaptation problems, we extract from PD a subset which is comparable to CTB in size.</context>
</contexts>
<marker>Xue, Xia, Chiou, Palmer, 2005</marker>
<rawString>Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha Palmer. 2005. The penn chinese treebank: Phrase structure annotation of a large corpus. In Natural Language Engineering.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shiwen Yu</author>
</authors>
<title>Jianming Lu, Xuefeng Zhu, Huiming Duan, Shiyong Kang, Honglin Sun, Hui Wang, Qiang Zhao, and Weidong Zhan.</title>
<date>2001</date>
<tech>Technical report.</tech>
<marker>Yu, 2001</marker>
<rawString>Shiwen Yu, Jianming Lu, Xuefeng Zhu, Huiming Duan, Shiyong Kang, Honglin Sun, Hui Wang, Qiang Zhao, and Weidong Zhan. 2001. Processing norms of modern chinese corpus. Technical report.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>Chinese segmentation with a word-based perceptron algorithm.</title>
<date>2007</date>
<booktitle>In Proceedings ofACL</booktitle>
<contexts>
<context position="7030" citStr="Zhang and Clark, 2007" startWordPosition="1053" endWordPosition="1056">, b, m, e and s, where b, m and e mean the beginning, the middle and the end of a word, and s indicates a single-character word. The word segmentation result can be generated by splitting the labeled character sequence into subsequences of pattern s or bm*e, indicating single-character words or multi-character words, respectively. We choose the perceptron algorithm (Collins, 2002) to train the character classifier. It is an online training algorithm and has been successfully used in many NLP tasks, including POS tagging (Collins, 2002), parsing (Collins and Roark, 2004) and word segmentation (Zhang and Clark, 2007; Jiang et al., 2008; Zhang and Clark, 2010). The training procedure learns a discriminative model mapping from the inputs x E X to the outputs y E Y , where X is the set of sentences in the training corpus and Y is the set of corresponding labeled results. We use the function GEN(x) to enumerate the candidate results of an input x, and the function 4) to map a training example (x, y) E X x Y to a feature vector 4)(x, y) E Rd. Given the character sequence x, the decoder finds the output F(x) that maximizes the score function: F(x) = argmax yEGEN(x) = argmax yEGEN(x) Where C� E Rd is the parame</context>
<context position="20934" citStr="Zhang and Clark, 2007" startWordPosition="3188" endWordPosition="3191"> 1001 − 1151 Developing 301 − 325 6.66K Test 271 − 300 7.82K PD Training 02 − 06 5.86M Test 01 1.07M Table 3: Data partitioning for CTB and PD. to train the transformation models. The predictself methodology is implicit in many unsupervised learning approaches, it has been successfully used by (Daum´e III, 2009) in unsupervised dependency parsing. We adapt this idea to the scenario of annotation transformation to improve transformation accuracy. In recent years many works have been devoted to the word segmentation task. For example, the introduction of global training or complicated features (Zhang and Clark, 2007; Zhang and Clark, 2010); the investigation of word structures (Li, 2011); the strategies of hybrid, joint or stacked modeling (Nakagawa and Uchimoto, 2007; Kruengkrai et al., 2009; Wang et al., 2010; Sun, 2011), and the semisupervised and unsupervised technologies utilizing raw text (Zhao and Kit, 2008; Johnson and Goldwater, 2009; Mochihashi et al., 2009; Hewlett and Cohen, 2011). We estimate that the annotation transformation technologies can be adopted jointly with complicated features, system combination and semisupervised/unsupervised technologies to further improve segmentation performa</context>
</contexts>
<marker>Zhang, Clark, 2007</marker>
<rawString>Yue Zhang and Stephen Clark. 2007. Chinese segmentation with a word-based perceptron algorithm. In Proceedings ofACL 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>A fast decoder for joint word segmentation and pos-tagging using a single discriminative model.</title>
<date>2010</date>
<booktitle>In Proceedings ofEMNLP.</booktitle>
<contexts>
<context position="7074" citStr="Zhang and Clark, 2010" startWordPosition="1061" endWordPosition="1064">beginning, the middle and the end of a word, and s indicates a single-character word. The word segmentation result can be generated by splitting the labeled character sequence into subsequences of pattern s or bm*e, indicating single-character words or multi-character words, respectively. We choose the perceptron algorithm (Collins, 2002) to train the character classifier. It is an online training algorithm and has been successfully used in many NLP tasks, including POS tagging (Collins, 2002), parsing (Collins and Roark, 2004) and word segmentation (Zhang and Clark, 2007; Jiang et al., 2008; Zhang and Clark, 2010). The training procedure learns a discriminative model mapping from the inputs x E X to the outputs y E Y , where X is the set of sentences in the training corpus and Y is the set of corresponding labeled results. We use the function GEN(x) to enumerate the candidate results of an input x, and the function 4) to map a training example (x, y) E X x Y to a feature vector 4)(x, y) E Rd. Given the character sequence x, the decoder finds the output F(x) that maximizes the score function: F(x) = argmax yEGEN(x) = argmax yEGEN(x) Where C� E Rd is the parameter vector (that is, the discriminative mode</context>
<context position="20958" citStr="Zhang and Clark, 2010" startWordPosition="3192" endWordPosition="3195"> 301 − 325 6.66K Test 271 − 300 7.82K PD Training 02 − 06 5.86M Test 01 1.07M Table 3: Data partitioning for CTB and PD. to train the transformation models. The predictself methodology is implicit in many unsupervised learning approaches, it has been successfully used by (Daum´e III, 2009) in unsupervised dependency parsing. We adapt this idea to the scenario of annotation transformation to improve transformation accuracy. In recent years many works have been devoted to the word segmentation task. For example, the introduction of global training or complicated features (Zhang and Clark, 2007; Zhang and Clark, 2010); the investigation of word structures (Li, 2011); the strategies of hybrid, joint or stacked modeling (Nakagawa and Uchimoto, 2007; Kruengkrai et al., 2009; Wang et al., 2010; Sun, 2011), and the semisupervised and unsupervised technologies utilizing raw text (Zhao and Kit, 2008; Johnson and Goldwater, 2009; Mochihashi et al., 2009; Hewlett and Cohen, 2011). We estimate that the annotation transformation technologies can be adopted jointly with complicated features, system combination and semisupervised/unsupervised technologies to further improve segmentation performance. 5 Experiments and A</context>
<context position="27088" citStr="Zhang and Clark, 2010" startWordPosition="4160" endWordPosition="4163"> filtration. We find that predict-self filtration brings slight improvement over the baseline, and predictself reestimation outperforms the filtration strategy when λ falls in a proper range. An initial analysis on the experimental results of predict-self filtration Accuracy (F%) 96.4 96.2 95.8 95.6 95.4 96 Iterative training with predict-self reestimation Iterative training 418 Model Time (s) Accuracy (F1%) SPD → CTB Anno. Adapt. 4.39 97.67 Opt. Trans. 1.33 97.97 PD → CTB Anno. Adapt. 4.76 98.15 Opt. Trans. 1.37 98.43 Previous Works (Jiang et al., 2008) 97.85 (Kruengkrai et al., 2009) 97.87 (Zhang and Clark, 2010) 97.79 (Sun, 2011) 98.17 Table 6: The performance of the iterative annotation transformation with predict-self reestimation compared with annotation adaptation. shows that, the filtration discards 5% of the training sentences and these discarded sentences contain nearly 10% of training words. It can be confirmed that the sentences discarded by predict-self filtration are much longer and more complicated. With a properly tuned weight, predict-self reestimation can make better use of the training data. The best Fmeasure improvement achieved over the annotation transformation baseline is 0.3 poin</context>
</contexts>
<marker>Zhang, Clark, 2010</marker>
<rawString>Yue Zhang and Stephen Clark. 2010. A fast decoder for joint word segmentation and pos-tagging using a single discriminative model. In Proceedings ofEMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai Zhao</author>
<author>Chunyu Kit</author>
</authors>
<title>Unsupervised segmentation helps supervised learning of character tagging for word segmentation and named entity recognition.</title>
<date>2008</date>
<booktitle>In Proceedings of SIGHAN Workshop.</booktitle>
<contexts>
<context position="21238" citStr="Zhao and Kit, 2008" startWordPosition="3235" endWordPosition="3238"> 2009) in unsupervised dependency parsing. We adapt this idea to the scenario of annotation transformation to improve transformation accuracy. In recent years many works have been devoted to the word segmentation task. For example, the introduction of global training or complicated features (Zhang and Clark, 2007; Zhang and Clark, 2010); the investigation of word structures (Li, 2011); the strategies of hybrid, joint or stacked modeling (Nakagawa and Uchimoto, 2007; Kruengkrai et al., 2009; Wang et al., 2010; Sun, 2011), and the semisupervised and unsupervised technologies utilizing raw text (Zhao and Kit, 2008; Johnson and Goldwater, 2009; Mochihashi et al., 2009; Hewlett and Cohen, 2011). We estimate that the annotation transformation technologies can be adopted jointly with complicated features, system combination and semisupervised/unsupervised technologies to further improve segmentation performance. 5 Experiments and Analysis We perform annotation transformation from People’s Daily (PD) (Yu et al., 2001) to Penn Chinese Treebank 5.0 (CTB) (Xue et al., 2005), following the same experimental setting as the annotation adaptation work (Jiang et al., 2009) for convenience of comparison. The two cor</context>
</contexts>
<marker>Zhao, Kit, 2008</marker>
<rawString>Hai Zhao and Chunyu Kit. 2008. Unsupervised segmentation helps supervised learning of character tagging for word segmentation and named entity recognition. In Proceedings of SIGHAN Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Muhua Zhu</author>
<author>Jingbo Zhu</author>
<author>Minghan Hu</author>
</authors>
<title>Better automatic treebank conversion using a feature-based approach.</title>
<date>2011</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="18925" citStr="Zhu et al., 2011" startWordPosition="2874" endWordPosition="2877">uned on the developing set. To integrating the predict-self reestimation into the iterative transformation training, a reversed transformation model is introduced and the enhanced scoring function above is used when the function TRANSANNOTATE invokes the function DECODE. 4 Related Works Researches focused on the automatic adaptation between different corpora can be roughly classified into two kinds, adaptation between different domains (with different statistical distribution) (Blitzer et al., 2006; Daum´e III, 2007), and adaptation between different annotation guidelines (Jiang et al., 2009; Zhu et al., 2011). There are also some efforts that totally or partially resort to manual transformation rules, to conduct treebank conversion (Cahill and Mccarthy, 2002; Hockenmaier and Steedman, 2007; Clark and Curran, 2009), and word segmentation guideline transformation (Gao et al., 2004; Mi et al., 2008). This work focuses on the automatic transformation between annotation guidelines, and proposes better annotation transformation technologies to improve the transformation accuracy and the utilization rate of human-annotated knowledge. The iterative training procedure proposed in this work shares some simi</context>
</contexts>
<marker>Zhu, Zhu, Hu, 2011</marker>
<rawString>Muhua Zhu, Jingbo Zhu, and Minghan Hu. 2011. Better automatic treebank conversion using a feature-based approach. In Proceedings ofACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
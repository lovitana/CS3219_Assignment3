<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002064">
<title confidence="0.992738">
Multiple Aspect Summarization Using Integer Linear Programming
</title>
<author confidence="0.994825">
Kristian Woodsend and Mirella Lapata
</author>
<affiliation confidence="0.9995085">
Institute for Language, Cognition and Computation
School of Informatics, University of Edinburgh
</affiliation>
<address confidence="0.992646">
10 Crichton Street, Edinburgh EH8 9AB
</address>
<email confidence="0.999211">
k.woodsend@ed.ac.uk,mlap@inf.ed.ac.uk
</email>
<sectionHeader confidence="0.998625" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.996818777777778">
Multi-document summarization involves
many aspects of content selection and sur-
face realization. The summaries must be
informative, succinct, grammatical, and obey
stylistic writing conventions. We present a
method where such individual aspects are
learned separately from data (without any
hand-engineering) but optimized jointly
using an integer linear programme. The
ILP framework allows us to combine the
decisions of the expert learners and to select
and rewrite source content through a mixture
of objective setting, soft and hard constraints.
Experimental results on the TAC-08 data set
show that our model achieves state-of-the-art
performance using ROUGE and signifi-
cantly improves the informativeness of the
summaries.
</bodyText>
<sectionHeader confidence="0.999472" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999933674418605">
Automatic summarization has enjoyed wide popu-
larity in natural language processing (see the pro-
ceedings of the Document Understanding and Text
Analysis conferences) due to its potential for prac-
tical applications but also because it incorporates
many important aspects of both natural language un-
derstanding and generation. Of the many summa-
rization paradigms that have been identified over the
years (see Sparck Jones (1999) and Mani (2001) for
comprehensive overviews), multi-document sum-
marization — the task of producing summaries from
clusters of thematically related documents — has
consistently attracted attention.
Despite considerable research effort, the auto-
matic generation of multi-document summaries that
resemble those written by humans remains chal-
lenging. This is primarily due to the task itself
which is complex and subject to several constraints:
the summary must be maximally informative and
minimally redundant, grammatical, coherent, adhere
to a pre-specified length and stylistic conventions.
An ideal model would learn to output summaries
that simultaneously meet all these constraints from
data (i.e., document clusters and their correspond-
ing summaries). This global inference problem is,
however, hard — the solution space is large and the
lack of easily accessible datasets an obstacle to joint
learning. It is thus no surprise that previous work has
focused on specific aspects of joint learning.
Initial global formulations of the multi-document
summarization task focused on extractive summa-
rization and used approximate greedy algorithms for
finding the sentences of the summary. Goldstein et
al. (2000) search for the set of sentences that are
both relevant and non-redundant, whereas Filatova
and Hatzivassiloglou (2004) model multi-document
summarization as an instance of the maximum cov-
erage set problem.1 More recent work improves on
the search problem by considering exact solutions
and permits a limited amount of rewriting. McDon-
ald (2007) proposes an integer linear programming
formulation that maximizes the sum of relevance
scores of the selected sentences penalized by the
</bodyText>
<footnote confidence="0.8776555">
1Given C, a finite set of weighted elements, a collection T of
subsets of C, and an integer k, find those k sets that maximize the
total number of elements in the union of T’s members (Hochba,
1997).
</footnote>
<page confidence="0.962463">
233
</page>
<note confidence="0.8897805">
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 233–243, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.998245">
sum of redundancy scores of all pairs of selected
sentences. Gillick et al. (2008) develop an exact so-
lution for a model similar to Filatova and Hatzivas-
siloglou (2004) under the assumption that the value
of a summary is the sum of values of the unique con-
cepts (approximated by bigrams) it contains. Subse-
quent work (Gillick et al., 2009; Berg-Kirkpatrick et
al., 2011) extends this model to allow sentence com-
pression in the form of word or constituent deletion.
In this paper we propose a model for multi-
document summarization that attempts to cover
many different aspects of the task such as content se-
lection, surface realization, paraphrasing, and stylis-
tic conventions. These aspects are learned separately
using specific “expert” predictors, but are optimized
jointly using an integer linear programming model
(ILP) to generate the output summary.2 All experts
are learned from data without requiring additional
annotation over and above the summaries written
for each document cluster. Our predictors include
the use of unique bigram information to model con-
tent and avoid redundancy, positional information to
model important and poor locations of content, and
language modeling to capture stylistic conventions.
Learning each predictor separately gives better gen-
eralization, while the ILP framework allows us to
combine the decisions of the expert learners through
the use of objectives, hard and soft constraints.
The experts work collaboratively to rewrite the
content using rules extracted from document clusters
and model summaries. We adopt the synchronous
tree substitution grammar (STSG) formalism (Eis-
ner, 2003) which can model non-isomorphic tree
structures (the grammar rules can comprise trees of
arbitrary depth) and is thus suited to text-rewriting
tasks which typically involve a number of local mod-
ifications to the input text. Specifically, we pro-
pose quasi-synchronous tree substitution grammar
(QTSG) as a flexible formalism to learn general tree-
edits from loosely-aligned phrase structure trees.
We evaluate our model on the 100-word “non-
</bodyText>
<footnote confidence="0.769142428571429">
2Our task is standard multi-document summarization and
should not be confused with “guided” summarization where
system and human summarizers are given a list of important
aspects to cover in the summary. Our usage of the term aspects
broadly refers to the different types of constraints (e.g., relating
to content or style) a summary must meet, but these are learned
rather than specified in advance.
</footnote>
<bodyText confidence="0.999919909090909">
update” summarization task as defined in the the
Text Analysis Conference (TAC 2008). Experimen-
tal results show that our method obtains perfor-
mance comparable and in some cases superior to
state-of-the-art, in terms of ROUGE and human rat-
ings of summary grammaticality and informative-
ness. Importantly, there is nothing inherent in our
model that is specific to this particular summariza-
tion task. As all of the different experts are learned
from data, it could easily adapt to other summariza-
tion styles or conventions as needed.
</bodyText>
<sectionHeader confidence="0.999879" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.995057823529412">
Recent years have seen increased interest in global
inference methods for summarization. ILP-based
models have been developed for several subtasks
ranging from sentence compression (Clarke and La-
pata, 2008), to single- and multi-document sum-
marization (McDonald, 2007; Martins and Smith,
2009; Gillick and Favre, 2009; Woodsend and Lap-
ata, 2010; Berg-Kirkpatrick et al., 2011), and head-
line generation (Deshpande et al., 2007; Wood-
send et al., 2010). Most of these approaches are ei-
ther purely extractive or implement a single rewrite
operation, namely word deletion. Although it is
well-known that hand-written summaries often ex-
hibit additional edits and sentence recombinations
(Jing, 2002), the challenges involved in acquiring
the rewrite rules, interfacing them with inference,
and ensuring grammatical output make the develop-
ment of abstractive models non-trivial.
Our work is closest to Gillick et al. (2008) who
also develop an ILP model for multi-document sum-
marization. A key assumption in their model which
we also follow is that input documents contain a
variety of concepts, each of which are allocated a
value, and the goal of a good summary is to max-
imize the sum of these values subject to the length
constraint. The authors use bigrams as concepts and
their frequency in the input documents as a proxy
for their value. This model can also perform sen-
tence compression (see also Gillick et al. (2009)),
however, the deletion rules are hand-coded. Berg-
Kirkpatrick et al. (2011) build on this work by re-
casting it as a structured prediction problem. They
essentially combine the same bigram content scor-
ing system with features relating to the parse tree
</bodyText>
<page confidence="0.996047">
234
</page>
<bodyText confidence="0.999594">
which they learn using a maximum-margin SVM
trained on annotated gold-standard compressions.
Our multi-document summarization model jointly
optimizes different aspects of the task involving both
content selection and surface realization. Each indi-
vidual aspect has its own dedicated expert, which we
argue is advantageous as it renders inference simpler
and affords flexibility (e.g., additional aspects can be
incorporated into the model or trained separately on
different datasets). Our work differs from Gillick et
al. (2009) and Berg-Kirkpatrick et al. (2011) in three
important respects. Firstly, we develop a genuinely
abstractive model that is not limited to deletion.
Our rewrite rules are encoded in quasi-synchronous
tree substitution grammar and learned automatically
from source documents and their summaries. Un-
like previous applications of STSG to sentence com-
pression (Cohn and Lapata, 2009; Cohn and Lap-
ata, 2008) our quasi-synchronous TSG does not at-
tempt to learn the complete translation from source
to target sentence; it only loosely links the syntactic
structure of the two (Smith and Eisner, 2006), and
is therefore well suited to describing the relation-
ship between documents and their abstracts. Sec-
ondly, our content selection component extends to
features beyond the bigram horizon, as we learn to
identify important concepts based on syntactic and
positional information. We also learn which words
are unlikely to appear in a summary. Thirdly, unlike
Berg-Kirkpatrick et al. (2011) our model does not
try to learn all the parameters (e.g., content, rewrite
rules, style) of the summarization problem jointly;
although decoupling learning from inference is per-
haps less elegant from a modeling perspective, the
learning process is more robust and reliable.
</bodyText>
<sectionHeader confidence="0.984212" genericHeader="method">
3 Modeling
</sectionHeader>
<bodyText confidence="0.999887303030303">
There are many aspects to producing a good sum-
mary of multiple documents. The important con-
tent needs to be captured, typically key facts in
each individual document, and information seen
across the cluster. Stylistic features may be differ-
ent in the summary from original documents. For
instance, summaries tend to use more concise lan-
guage, sources are not attributed as they are in news
articles, and relative dates are not included. In addi-
tion, the summary must be fluent, coherent, and re-
spect a pre-specified maximum length requirement.
We present an approach where elements of all the
above considerations are learned from training data
by separate dedicated components, and then com-
bined in an integer linear programme. Content se-
lection is performed partly through identifying the
most salient topics (bigrams); an additional compo-
nent learns to identify which information from the
source documents should be in the summary based
on positional information. Meanwhile, in terms of
surface realization, a language model identifies the
words that should not be in the output summaries,
whereas a separate component learns to exclude
sentences that are poor candidates for summaries.
QTSG rules, learned from the training corpus, are
used to generate alternative compressions and para-
phrases of the source sentences, in the style suit-
able for the summaries. Finally, an ILP model com-
bines the output of these components into a sum-
mary, jointly optimizing content selection and sur-
face realization preferences, and providing the flexi-
bility to treat some components as soft while others
as hard constraints.
</bodyText>
<subsectionHeader confidence="0.999338">
3.1 Document Representation
</subsectionHeader>
<bodyText confidence="0.999813583333333">
Given an input sentence, our approach deconstructs
it into component phrases and clauses, typical of a
phrase structure parser. In our experiments, we ob-
tain this representation from the output of the Stan-
ford parser (Klein and Manning, 2003) but any other
broadly similar parser could be used instead. Nodes
in the parse tree represent points where QTSG rules
can be applied (and paraphrases generated), and they
also represent decision points for the ILP. In the fol-
lowing, we will refer to these decision nodes as the
set N , and decisions for each node using the binary
variable zi, i EN.
</bodyText>
<subsectionHeader confidence="0.999792">
3.2 Content Selection Using Bigrams
</subsectionHeader>
<bodyText confidence="0.9999855">
We follow Gillick et al. (2008) in modeling the infor-
mation content of the summary as the weighted sum
of the individual information units it contains. We
represent information units as the set of bigrams B
seen in the source documents. The weight w of each
bigram is calculated from the number of source doc-
uments where the bigram was seen. The summary is
thus given the score fB(z), i.e., the weighted sum of
</bodyText>
<page confidence="0.974861">
235
</page>
<bodyText confidence="0.783412">
its information units:
</bodyText>
<equation confidence="0.995511">
fB(z) _ ∑ wjbj (1)
jEB
</equation>
<bodyText confidence="0.999952">
where wj is the weight of concept j, bj a binary vari-
able to indicate if concept j is present in the sum-
mary, and j E B.
Importantly, each information unit is counted only
once; this encourages wide coverage of the source
documents, and removes any drive towards redun-
dant information without actively discouraging it,
contrary to other global formulations where redun-
dancy measures form part of the objective (McDon-
ald, 2007). The counting mechanism is achieved by
linking the variables z indicating nodes in the parse
tree and b indicating bigrams:
</bodyText>
<equation confidence="0.8167425">
bj &lt; ∑ zi Vj E B (2)
iEN: jEBi
</equation>
<bodyText confidence="0.999951">
where Bi C B is the subset of bigrams that are con-
tained in node i. A drawback of the global nature
of this counting mechanism, however, is that it can-
not be integrated with local features such as those
described below; our approach takes local features
into account but these are weighted by other compo-
nents.
</bodyText>
<subsectionHeader confidence="0.99929">
3.3 Content Selection Using Salience
</subsectionHeader>
<bodyText confidence="0.999940789473684">
The bigram approach is a powerful method for
identifying important concepts within the document
cluster. It works particularly well in the sentence ex-
traction paradigm. However, additional elements are
known to be good predictors of important informa-
tion. Examples include the position of a sentence
in the document (e.g., first sentences often con-
tain salient information), whether it contains proper
nouns, numbers, pronouns, mentions of money, and
so on. We decided to learn which of these elements
(represented as nodes in the parse tree) are infor-
mative from training data. Specifically, sentences
in the cluster documents were aligned to sentences
from corresponding human summaries. Alignment
was based rather simply on identifying the sentence
pairs with the highest number of overlapping bi-
grams, without compensating for sentence length, or
matching the sequence of information in the sum-
maries and source documents (Nelken and Schieber,
</bodyText>
<table confidence="0.963478">
Weight Feature
1.21 From first sentence in document
0.73 Contains proper nouns
0.68 Contains nouns
0.57 From first paragraph
0.53 From first three sentences
0.51 Contains numbers
-0.50 Contains pronouns
0.32 Contains money
</table>
<tableCaption confidence="0.985054333333333">
Table 1: Weights and features of SVM that predicts the
salience of summary content. Negative weights indicate
information that should not be included in the summary.
</tableCaption>
<bodyText confidence="0.999915230769231">
2006). Matched sentences in the source documents
were given positive labels, while unaligned sen-
tences were given negative labels. These labels were
then propagated to phrase structure nodes.
We trained an SVM on this data (tree nodes and
their labels) using surface features that do not over-
lap with bigram information: sentence and para-
graph position, POS-tag information. Table 1 shows
the most important features learned by the model as
predictors of salient content.
The summary can be given a salience score fS(z)
using the raw SVM prediction scores of the individ-
ual parse tree nodes:
</bodyText>
<equation confidence="0.990588">
fS(z) _ ∑ (Φ(i) - θ)zi (3)
iEN
</equation>
<bodyText confidence="0.999925">
where Φ(i) is the feature vector for node i, and θ the
weights learned by the SVM.
</bodyText>
<subsectionHeader confidence="0.98697">
3.4 Surface Realization Using Style
</subsectionHeader>
<bodyText confidence="0.9999775">
Some sentences in the source documents will make
poor summary sentences, despite the information
they contain, and therefore contrary to the predic-
tions of the content selection indicators described
above. This may be because the source sentence is
very short, or is expressed as a quotation, or con-
tains many pronouns that will not be resolved when
the sentence is extracted.
Our idea is to learn which sentences are poor from
a stylistic perspective using again aligned training
data. We train a second SVM on the aligned sen-
tences and their labels using surface features at the
sentence level, such as sentence length and POS-tag
information. The most important features learned by
</bodyText>
<page confidence="0.985737">
236
</page>
<table confidence="0.9993965">
Weight Feature
-1.04 Word count less than 10
-0.83 Word count less than 20
-0.30 Question
-0.30 Quotation
-0.14 Personal pronouns
</table>
<tableCaption confidence="0.998056">
Table 2: Weights and features of SVM that predicts poor
candidate sentences.
</tableCaption>
<bodyText confidence="0.928928857142857">
the model as predictors of poor sentences, and the
weights assigned to them, are shown in Table 2.
The predictions of the SVM are incorporated into
the ILP as a hard constraint, by forcing all parse tree
nodes within those sentences predicted as poor (the
set N −) to be zero:
zi = 0 Vi E N −. (4)
</bodyText>
<subsectionHeader confidence="0.7107505">
3.5 Surface Realization Using Lexical
Preferences
</subsectionHeader>
<bodyText confidence="0.999972185185185">
Human-written summaries differ from the source
news articles in a number of ways. They delete ex-
traneous information, merge material from several
sentences, employ paraphrases and syntactic trans-
formations, change the order of the source sentences
and replace phrases or clauses with more general
or specific descriptions. We could attempt to learn
the “language of summaries” with a language model
which we could then use to guide the generation
process (e.g., by producing maximally probable out-
put). Aside from the logistics of gathering training
data large enough to provide robust estimates, we
believe that a more compelling approach is to focus
on the words that are unlikely to appear in the sum-
mary despite appearing in the source documents.
A comparison of the language models generated
from the source documents and model summaries,
even at the unigram level, is revealing. Table 3 shows
lexemes that appear in both source and summary
documents, but where the likelihood of the lexeme
appearing in the summary is much less than that
of it appearing the document, taking into account
that the summary is much shorter anyway. The fi-
nal column shows the log10-ratio (L(w)) between
the two probabilities. We can see that least prob-
able words are those that correspond to attribut-
ing information sources (e.g., said, told, according
</bodyText>
<table confidence="0.999892461538462">
Lexeme w Source Summary L(w)
count count
say 5670 88 -1.63
go 638 11 -1.52
last 616 9 -1.69
get 543 15 -1.05
tell 512 8 -1.62
come 488 12 -1.17
know 404 9 -1.27
monday 391 8 -1.35
think 382 7 -1.46
next 239 7 -0.99
spokesman 197 4 -1.36
</table>
<tableCaption confidence="0.977904">
Table 3: Counts of lexemes in the source news articles and
summaries, and measure of the ratio of their probabilities
(for most common lexemes with ratio &lt; −0.95).
</tableCaption>
<bodyText confidence="0.999515833333333">
to, spokesman), dates described relatively (e.g., last
Monday), and events that are in the process of hap-
pening (e.g., coming, going).
As the amount of training data tends to be lim-
ited — there are usually only a few human-written
summaries available per document cluster — we
use a unigram language model, but conceivably a
longer-range n-gram could be employed in the same
vein. We incorporate preferences about summary
language into the model as a soft constraint. The
log-ratio values fLR(z) are included in the objective
and defined at the tree node level:
</bodyText>
<equation confidence="0.958633">
fLR (z) = Y, E L(w)zi (5)
iEN wEWi
</equation>
<bodyText confidence="0.999456">
where L(w), w E Wi is the log-ratio value for an in-
dividual word w:
</bodyText>
<equation confidence="0.998529">
Psrc(w)
L(w) = log10 Psum(w),
</equation>
<bodyText confidence="0.9755345">
Psrc(w) and Psum(w) are the probabilities of word w
appearing in the source and summary documents re-
spectively, and Wi is the set of words at parse tree
node i. Importantly, we include only those those lex-
emes with negative L(w) values. This guides the
model away from the kind of phrases described
above, but not towards any particular language pref-
erences.
</bodyText>
<page confidence="0.979619">
237
</page>
<subsectionHeader confidence="0.9065895">
3.6 Quasi-synchronous Tree Substitution
Grammar
</subsectionHeader>
<bodyText confidence="0.999756222222222">
Rewrite rules involving substitutions, deletions and
reorderings are captured in our model using a quasi-
synchronous tree substitution grammar. Given an in-
put (source) sentence S1 or its parse tree T1, the
QTSG contains rules for generating possible trans-
lation trees T2. A grammar node in the target tree T2
is modeled on a subset of nodes in the source tree,
with a rather loose alignment between the trees.
We extract QTSG rules from aligned source
and summary sentence pairs represented by their
phrase structure trees. Our algorithm builds up a
list of leaf node alignments based on lexical iden-
tity. Direct parent nodes are aligned where more
than one child node aligns. This quasi-synchronous
“bottom-up” process gives us better ability to match
non-isomorphic structures. We do not assume an
alignment between source and target root nodes, nor
do we require a surjective alignment of all target
nodes to the source tree. QTSG rules are then cre-
ated from aligned nodes above the leaf node level if
all the nodes in the target tree can be explained us-
ing nodes from the source. Individual rewrite rules
describe the mapping of source tree fragments into
target tree fragments, and so the grammar represents
the space of valid target trees that can be produced
from a given source tree (Eisner, 2003; Cohn and
Lapata, 2009).
Examples of the most frequent QTSG rules
learned by the above process are shown in Figure 1.
Many of the rules relate to the compression of noun
phrases through deletion, and examples are shown
in the upper box. Others capture the compression of
verb phrases (middle box). An important rewrite op-
eration is the abstraction of a sentence from a more
complex source sentence, adding final punctuation if
necessary (lower box).
At generation, paraphrases are created from
source sentence parse trees by identifying and ap-
plying QTSG rules with matching structure. The
transduction process starts at the root node of the
parse tree, applying QTSG rules to sub-trees un-
til leaf nodes are reached. Note that we do not use
the Bayesian probability model normally associated
with quasi-synchronous grammars (Smith and Eis-
ner, 2006); instead, we ask the QTSG to provide
</bodyText>
<table confidence="0.681022285714286">
(NP, NP) , ([NP1 PP], [NP1 ])
(NP, NP) , ([NP1 VP], [NP1 ])
(NP, NP) , ([NP1 SBAR], [NP1 ])
(NP, NP) , ([NP1 , NP ,], [NP1 ])
(NP, NP) , ([NP1 CC NP], [NP1 ])
(NP, NP) , ([NNP NNP1 ], [NNP1 ])
(NP, NP) , ([DT1 JJ NN2], [DT1 NN2])
</table>
<figureCaption confidence="0.9972575">
Figure 1: Examples of most frequently learned QTSG
rules. Boxed subscripts show aligned nodes.
</figureCaption>
<bodyText confidence="0.9629037">
paraphrases that are acceptable rather than probable,
and generate all paraphrases licensed by the QTSG.
The alternative paraphrases are incorporated into
the target phrase structure tree as choices that the
ILP can make. We use the set C C N to be the
set of nodes where a choice of paraphrases is avail-
able, and Ci C N ,i E C to be the actual paraphrases
of i. Where there are alternatives, it makes sense of
course to select only one, which we implement using
the constraint:
</bodyText>
<equation confidence="0.9265035">
E zj=zi Vi E C, j E Ci (6)
jECi
</equation>
<bodyText confidence="0.998562444444444">
More generally, we need to constrain the output to
ensure that a parse tree structure is maintained. For
each node i E N, the set Di C N contains the list of
dependent nodes (both ancestors and descendants)
of node i, so that each set Di contains the nodes that
depend on the presence of i. We introduce a con-
straint to force node i to be present if any of its de-
pendent nodes are chosen:
zj , zi ViEN ,j E Di (7)
</bodyText>
<subsectionHeader confidence="0.94218">
3.7 The ILP Objective
</subsectionHeader>
<bodyText confidence="0.99993425">
The model we propose for generating a multi-
document summary is expressed as an integer linear
programme and incorporates the content selection
and surface realization preferences, as well as the
</bodyText>
<figure confidence="0.4509966">
(VP, VP) , ([VP1 CC VP], [VP1 ])
(VP, VP) , ([VP CC VP1 ], [VP1 ])
(VP, VP) , ([VP1 , CC VP], [VP1 ])
(S, S) , ([NP1 VP2 ], [NP1 VP2 .])
(S, S) , ([ADVP , NP1 VP2 .], [NP1 VP2 .])
</figure>
<page confidence="0.993751">
238
</page>
<bodyText confidence="0.9982086">
soft and hard constraints described in the preceding
sections. The objective of the optimization problem
is to maximize the score contributed by the various
elements of content selection (fB(z) and fS(z)) and
soft surface realization constraints (fLR(z)) :
</bodyText>
<equation confidence="0.791510333333333">
max fB(z)+ fS(z) + fLR (z) (8)
z
This objective is subject to the constraints (2), (4),
</equation>
<bodyText confidence="0.49664">
(6), and (7) that represent hard constraint decisions,
or maintain the logical integrity of the model. An
overall length constraint completes the model:
</bodyText>
<equation confidence="0.861688">
E lizi G lmax (9)
iEN
</equation>
<bodyText confidence="0.999515130434783">
where li is the number of words generated by choos-
ing node i, and lmax is the global word length limit.
Note that the scores in the objective are for each
tree node and not each sentence. This affords the
model flexibility: the content selection elements are
generally not competing with each other to give a
decision on a sentence (see McDonald (2007)). In-
stead, components are marking positive and nega-
tive nodes. The ILP is implicitly searching the gram-
mar rules for ways to rewrite the sentence, with the
aim of including the salient nodes while removing
negative-scoring nodes (deleting them increases the
score of the node to zero). Figure 2 shows an exam-
ple of a source sentence where the bigram, salience
and language preference components of the ILP
work together to score nodes in the parse tree. The
nodes NP1, VP3 and VP4 all have positive scores,
while “said Tuesday” is negative. As a rewrite pos-
sibility, the rewrite rule shown bottom left is avail-
able, which will remove the negative node. Further
rewrite rules allow VP2 to be compressed. The out-
put actually generated by the model used sub-trees
(b) and (d) — the final text is included in Table 6.
</bodyText>
<sectionHeader confidence="0.998634" genericHeader="method">
4 Experimental Set-up
</sectionHeader>
<bodyText confidence="0.999930269230769">
Data Our model was evaluated on the TAC non-
update multi-document summarization task which
involves generating a 100-word-limited summary
from a cluster of 10 related input documents; ad-
ditionally, TAC provides a set of four model sum-
maries for each cluster, written by human experts.
We used the 44 document clusters from TAC-2009
as training data, to learn the different elements of
the model. The 48 document clusters of TAC-2008
were reserved for the generation of test summaries.3
Training The two components described in Sec-
tions 3.3 and 3.4 were trained using binary SVM
classifiers, with labels inferred automatically via
alignment. The salience classifier was trained on
102,754 node instances (16,042 positive and 86,712
negative). The style classifier was trained on 20,443
sentence instances (2,083 positive and 18,360 neg-
ative). We learned the feature weights with a linear
SVM, using the software SVM-OOPS (Woodsend
and Gondzio, 2009). Because of the high compres-
sion rate in this task, sentence alignment leads to an
unbalanced data set. We compensated for this by us-
ing different SVM hyper-parameters C+ and C− as
the loss multiplier for misclassification of positive
and negative training samples respectively. SVM
hyper-parameters were chosen that gave the high-
est F1 values using 10-fold cross-validation. The
salience SVM obtained a precision of 0.28 and re-
call of 0.43. Precision for the style SVM was 0.20
and recall 0.63, respectively. The classifiers on their
own would thus not be great predictors of salience
or style, but in practice they were useful for break-
ing ties in bigram scores.
Aligned sentences from the training data were
also used to learn the quasi-synchronous tree sub-
stitution grammar, using the process described in
Section 3.6. Rules seen fewer than 3 times were re-
moved, resulting in a total of 339 QTSG rules. Two
unigram language models (see Section 3.5) were
trained on the source articles and summaries, respec-
tively. Their probabilities were compared to give the
word list shown in Table 3. We removed words with
a source count less than 50, providing a list of 60 lex-
emes. The resulting integer linear programmes were
solved using SCIP,4 and it took 55 seconds on aver-
age to read in and solve a document cluster problem.
Evaluation We compared our model against two
systems. As a baseline, we used the ICSI-1 extrac-
tive system (Gillick et al., 2008) which is also based
on ILP and was highly ranked in the TAC-2008
evaluation. We also compared against the “learned
phrase compression” system of Berg-Kirkpatrick et
</bodyText>
<footnote confidence="0.9992355">
3This split follows Berg-Kirkpatrick et al. (2011).
4http://scip.zib.de/
</footnote>
<page confidence="0.996481">
239
</page>
<figure confidence="0.996796166666667">
.
.
VP
said
(a) S
Tue
</figure>
<page confidence="0.979602">
240
</page>
<table confidence="0.9921172">
Models 2-R ROUGE Ins TER (%) Shift Count Sentences Mod (%)
SU4-R Del Sub CR (%)
ICSI-1 11.03 13.96 — — — — 200 — —
B-K 11.71 14.47 0.2 26.2 2.3 0.4 216 74.0 63.9
MA-ILP 11.37 14.47 0.7 11.6 5.3 0.6 191 89.1 61.8
ILP w/o bigrams 9.24 12.66 0.8 15.4 11.8 1.2 205 85.4 80.0
ILP w/o salience 11.38 14.71 1.1 19.1 12.0 1.3 233 82.1 92.3
ILP w/o style 11.83 15.09 1.4 17.4 18.9 1.7 271 84.1 86.3
ILP w/o log-ratio 11.41 14.70 1.2 16.9 12.5 1.5 223 84.3 90.1
ILP w/o QTSG 10.32 13.68 0 0 0 0 163 100.0 0
</table>
<tableCaption confidence="0.98948475">
Table 4: Performance of the multiple-aspect ILP model against comparison systems using ROUGE and the four com-
ponents of TER (insertion, deletion, substitution, shifts). In the lower section, performance of our model without (w/o)
each component in turn. The final columns show the number of source sentences, the average compression ratio, and
the proportion of sentences modified.
</tableCaption>
<bodyText confidence="0.99854209375">
from the upper section of Table 4, the systems incor-
porating some form of rewriting gain slightly higher
ROUGE scores than ICSI-1. The multiple aspects
ILP system (MA-ILP) yields ROUGE scores simi-
lar to B-K, despite performing rewriting operations
which increase the scope for error and without re-
quiring any hand-crafted compression rules or man-
ually annotated training data. Indeed, the outputs of
the two systems are not significantly different under
ROUGE (using a paired t-test, p &gt; 0.5).
In the lower section of Table 4, we show the per-
formance of our model when each of the contribut-
ing components described in Section 3 are removed.
Clearly the bigram content indicators are an impor-
tant element for the ROUGE scores, as their removal
yields a reduction of 2.46 points (see the row ILP
w/o bigrams in Table 4). The model without QTSG
rules (ILP w/o QTSG) is effectively limited to sen-
tence extraction, and removing rewrite rules also
lowers ROUGE scores to levels similar to ICSI-1.
ROUGE scores are increased by allowing the model
to select “poor quality” sentences (ILP w/o style),
higher indeed than those of the B-K system. The
inclusion of non-summary language (ILP w/o log-
ratio) does not affect ROUGE scores to the same ex-
tent that bigrams and QTSG do.
Table 4 includes a break-down of the systems’
rewrite operations as measured by TER. We also
show the number of source sentences (Count), the
average compression ratio (CR %) and the propor-
tion of sentences modified (Mod %) by each system.
As can be seen, MA-ILP draws on fewer sentences,
</bodyText>
<table confidence="0.996828166666667">
Models Grammar Inform
ICSI-1 4.68 2.55
B-K 4.40 2.70
MA-ILP 4.68 3.90
ILP w/o style 3.30 2.67
Gold 4.90 4.75
</table>
<tableCaption confidence="0.999653">
Table 5: Mean ratings on system output output.
</tableCaption>
<bodyText confidence="0.999907173913043">
performs less deletion and more rewriting than B-K.
The number of deletions increases when individual
ILP components are removed and so does the num-
ber of substitutions. All the subsystems are more ag-
gressive in their rewriting than when used in com-
bination (higher TER, higher compression rate and
a larger number of sentences are modified). Expect-
edly, when removing the QTSG rules, the ILP is lim-
ited to a pure extractive system (last row in Table 4).
The results of our human evaluation study are
shown in Table 5. We elicited grammaticality and in-
formativeness ratings for a randomly selected model
summary, ICSI-1, B-K, the multiple aspect ILP
(MA-ILP), and the ILP w/o style which we in-
cluded in this study as it performed best under
ROUGE. ICSI-1, B-K, and MA-ILP are rated highly
on the grammaticality dimension. MA-ILP is in-
distinguishable from the sentence extraction sys-
tem (ICSI-1). Both systems are significantly more
grammatical than B-K (a &lt; 0.05, using a Post-hoc
Tukey test). Notice that summaries created by the
ILP w/o style are rated poorly by humans, contrary
to ROUGE. The style component stops very short
</bodyText>
<page confidence="0.991834">
241
</page>
<bodyText confidence="0.999739461538462">
Florida’s Governor Jeb Bush asked the US
Supreme Court to intervene to keep a comatose
woman alive, over the wishes of her husband,
who wants to disconnect the feeding tube that
has sustained her for 14 years. Her husband,
Michael Schiavo, and her parents, Robert and
Mary Schindler, have conflicts of interest that pre-
vent them from fairly deciding whether to keep
her alive. Some doctors have testified that Terri
Schiavo is in a persistent vegetative state with
no hope for recovery. The state House in Florida
passed a bill Thursday to extend life support for a
brain-damaged woman.
The space agencies of India and France signed an
agreement to cooperate in launching a satellite in
four years that will help make climate predictions
more accurate. The Indian Space Research Orga-
nization (ISRO) has short-listed experiments from
five nations including the United States, Britain
and Germany, for a slot on India’s unmanned
moon mission Chandrayaan-1 to be undertaken
by 2006-2007, the Press Trust of India (PTI) re-
ported Monday. India will launch more missions
to the moon if its maiden unmanned spacecraft
Chandrayaan-1, slated to be launched by 2008, is
successful in mapping the lunar surface.
</bodyText>
<tableCaption confidence="0.977701">
Table 6: Example summaries generated by the multiple
aspects model (MA-ILP).
</tableCaption>
<bodyText confidence="0.9997047">
sentences and quotations from being included in the
summary even if they have quite high bigram or
content scores. Without it, the model tends to gen-
erate summaries that are fragmentary and lacking
proper context, resulting in lower grammaticality
(and informativeness) when judged by humans. The
MA-ILP system obtains the highest rating with re-
spect to information content. It is significantly better
(a &lt; 0.05) than ICSI-1 and B-K. This is not entirely
surprising as our model includes additional content
selection elements over and above the bigram units.
There is still a significant gap from all systems to the
gold-standard human-authored summaries. Example
output summaries of the full ILP model are shown in
Table 6.
Overall, we obtain best results when considering
the contributions from the individual model experts
collectively. This suggests that additional improve-
ments could be obtained with more experts. It is also
possible that optimizing the relative weightings of
experts in the ILP objective would improve output.
The TER analysis shows that the experts have a tem-
pering effect on each other, resulting in less aggres-
sive, but qualitatively better, rewriting than when
used individually. Generally, experts work together
to shape an output sentence, but they can also com-
pete. In the future, we also plan to test the ability
of the model to adapt to other multi-document sum-
marization tasks, where the location of summary in-
formation is not as regular as it is in news articles.
We would also like interface our model with sen-
tence ordering and more generally with some notion
of the coherence of the generated summary.
Acknowledgments We are grateful to Micha El-
sner for his input on earlier versions of this work.
We would also like to thank members of the ILCC
at the School of Informatics for valuable discus-
sions and comments. We acknowledge the support
of EPSRC through project grants EP/I032916/1 and
EP/I017127/1.
</bodyText>
<sectionHeader confidence="0.999131" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.993768363636363">
Taylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein.
2011. Jointly learning to extract and compress. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 481–490, Portland, Ore-
gon.
James Clarke and Mirella Lapata. 2008. Global infer-
ence for sentence compression: An integer linear pro-
gramming approach. Journal of Artificial Intelligence
Research, 31:273–381.
Trevor Cohn and Mirella Lapata. 2008. Sentence com-
pression beyond word deletion. In Proceedings of the
22nd International Conference on Computational Lin-
guistics, pages 137–144, Manchester, UK.
Trevor Cohn and Mirella Lapata. 2009. Sentence com-
pression as tree transduction. Journal of Artificial In-
telligence Research, 34:637–674.
Pawan Deshpande, Regina Barzilay, and David Karger.
2007. Randomized decoding for selection-and-
ordering problems. In Human Language Technologies
2007: The Conference of the North American Chap-
ter of the Association for Computational Linguistics;
</reference>
<page confidence="0.964556">
242
</page>
<reference confidence="0.999846103448276">
Proceedings of the Main Conference, pages 444–451,
Rochester, New York.
Jason Eisner. 2003. Learning non-isomorphic tree map-
pings for machine translation. In Proceedings of the
ACL Interactive Poster/Demonstration Sessions, pages
205–208, Sapporo, Japan.
Elena Filatova and Vasileios Hatzivassiloglou. 2004.
A formal model for information selection in multi-
sentence text extraction. In Proceedings of the 20th
International Conference on Computational Linguis-
tics, pages 397–403, Geneva, Switzerland.
Dan Gillick and Benoit Favre. 2009. A scalable global
model for summarization. In Proceedings of the Work-
shop on Integer Linear Programming for Natural Lan-
guage Processing, pages 10–18, Boulder, Colorado.
Dan Gillick, Benoit Favre, and Dilek Hakkani-t¨ur. 2008.
The ICSI summarization system at TAC 2008. In Pro-
ceedings of the Text Analysis Conference.
Dan Gillick, Benoit Favre, Dilek Hakkani-t¨ur, Berndt
Bohnet, Yang Liu, and Shasha Xie. 2009. The
ICSI/UTD summarization system at TAC 2009. In
Proceedings of the Text Analysis Conference.
Jade Goldstein, Vibhu Mittal, Jaime Carbonell, and Mark
Kantrowitz. 2000. Multi-document summarization
by sentence extraction. In Proceedings of the 2000
NAACL–ANLP Workshop on Automatic Summariza-
tion, pages 40–48, Seattle, Washington.
Dorit S. Hochba. 1997. Approximating covering
and packing problems: Set cover, vertex cover, in-
dependent set, and related problems. In Dorit S.
Hochba, editor, Approximation Algorithms for NP-
Hard Problems, pages 94–143. PWS Publishing Com-
pany, Boston, MA.
Honyang Jing. 2002. Using Hidden Markov modeling
to decompose human-written summaries. Computa-
tional Linguistics, 28(4):527–544.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of the 41st As-
sociation for Computational Linguistics, pages 423–
430, Sapporo, Japan.
Chin-Yew Lin and Eduard H. Hovy. 2003. Automatic
evaluation of summaries using n-gram co-occurrence
statistics. In Proceedings of HLT–NAACL, pages 71–
78, Edmonton, Canada.
Inderjeet Mani. 2001. Automatic Summarization. John
Benjamins Pub Co.
Andr´e Martins and Noah A. Smith. 2009. Summariza-
tion with a joint model for sentence extraction and
compression. In Proceedings of the Workshop on In-
teger Linear Programming for Natural Language Pro-
cessing, pages 1–9, Boulder, Colorado.
Ryan McDonald. 2007. A study of global inference algo-
rithms in multi-document summarization. In Proceed-
ings of the 29th European conference on IR Research,
pages 557–564, Rome, Italy.
Rani Nelken and Stuart Schieber. 2006. Towards ro-
bust context-sensitive sentence alignment for monolin-
gual corpora. In Proceedings of the 11th Conference
of the European Chapter of the Association for Com-
putational Linguistics, pages 161–168, Trento, Italy.
David Smith and Jason Eisner. 2006. Quasi-synchronous
grammars: Alignment by soft projection of syntactic
dependencies. In Proceedings of Workshop on Statis-
tical Machine Translation, pages 23–30, NYC.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of the 7th Conference of the Associa-
tion for Machine Translation in the Americas, pages
223–231, Cambridge.
Karen Sparck Jones. 1999. Automatic summarizing:
Factors and directions. In Inderjeet Mani and Mark T.
Maybury, editors, Advances in Automatic Text Summa-
rization, pages 1–33. MIT Press, Cambridge.
Kristian Woodsend and Jacek Gondzio. 2009. Exploiting
separability in large-scale linear support vector ma-
chine training. Computational Optimization and Ap-
plications.
Kristian Woodsend and Mirella Lapata. 2010. Automatic
generation of story highlights. In Proceedings of the
48th Annual Meeting of the Association for Computa-
tional Linguistics, pages 565–574, Uppsala, Sweden.
Kristian Woodsend, Yansong Feng, and Mirella Lapata.
2010. Title generation with quasi-synchronous gram-
mar. In Proceedings of the 2010 Conference on Empir-
ical Methods in Natural Language Processing, pages
513–523, Cambridge, MA.
</reference>
<page confidence="0.999053">
243
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.635933">
<title confidence="0.999036">Multiple Aspect Summarization Using Integer Linear Programming</title>
<author confidence="0.696627">Woodsend</author>
<affiliation confidence="0.9366865">Institute for Language, Cognition and School of Informatics, University of</affiliation>
<address confidence="0.890238">10 Crichton Street, Edinburgh EH8</address>
<abstract confidence="0.998812842105263">Multi-document summarization involves many aspects of content selection and surface realization. The summaries must be informative, succinct, grammatical, and obey stylistic writing conventions. We present a method where such individual aspects are learned separately from data (without any hand-engineering) but optimized jointly using an integer linear programme. The ILP framework allows us to combine the decisions of the expert learners and to select and rewrite source content through a mixture of objective setting, soft and hard constraints. Experimental results on the TAC-08 data set show that our model achieves state-of-the-art performance using ROUGE and significantly improves the informativeness of the summaries.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Taylor Berg-Kirkpatrick</author>
<author>Dan Gillick</author>
<author>Dan Klein</author>
</authors>
<title>Jointly learning to extract and compress.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>481--490</pages>
<location>Portland, Oregon.</location>
<contexts>
<context position="3958" citStr="Berg-Kirkpatrick et al., 2011" startWordPosition="578" endWordPosition="581"> (Hochba, 1997). 233 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 233–243, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics sum of redundancy scores of all pairs of selected sentences. Gillick et al. (2008) develop an exact solution for a model similar to Filatova and Hatzivassiloglou (2004) under the assumption that the value of a summary is the sum of values of the unique concepts (approximated by bigrams) it contains. Subsequent work (Gillick et al., 2009; Berg-Kirkpatrick et al., 2011) extends this model to allow sentence compression in the form of word or constituent deletion. In this paper we propose a model for multidocument summarization that attempts to cover many different aspects of the task such as content selection, surface realization, paraphrasing, and stylistic conventions. These aspects are learned separately using specific “expert” predictors, but are optimized jointly using an integer linear programming model (ILP) to generate the output summary.2 All experts are learned from data without requiring additional annotation over and above the summaries written fo</context>
<context position="6980" citStr="Berg-Kirkpatrick et al., 2011" startWordPosition="1037" endWordPosition="1040">portantly, there is nothing inherent in our model that is specific to this particular summarization task. As all of the different experts are learned from data, it could easily adapt to other summarization styles or conventions as needed. 2 Related work Recent years have seen increased interest in global inference methods for summarization. ILP-based models have been developed for several subtasks ranging from sentence compression (Clarke and Lapata, 2008), to single- and multi-document summarization (McDonald, 2007; Martins and Smith, 2009; Gillick and Favre, 2009; Woodsend and Lapata, 2010; Berg-Kirkpatrick et al., 2011), and headline generation (Deshpande et al., 2007; Woodsend et al., 2010). Most of these approaches are either purely extractive or implement a single rewrite operation, namely word deletion. Although it is well-known that hand-written summaries often exhibit additional edits and sentence recombinations (Jing, 2002), the challenges involved in acquiring the rewrite rules, interfacing them with inference, and ensuring grammatical output make the development of abstractive models non-trivial. Our work is closest to Gillick et al. (2008) who also develop an ILP model for multi-document summarizat</context>
<context position="8841" citStr="Berg-Kirkpatrick et al. (2011)" startWordPosition="1330" endWordPosition="1333">ontent scoring system with features relating to the parse tree 234 which they learn using a maximum-margin SVM trained on annotated gold-standard compressions. Our multi-document summarization model jointly optimizes different aspects of the task involving both content selection and surface realization. Each individual aspect has its own dedicated expert, which we argue is advantageous as it renders inference simpler and affords flexibility (e.g., additional aspects can be incorporated into the model or trained separately on different datasets). Our work differs from Gillick et al. (2009) and Berg-Kirkpatrick et al. (2011) in three important respects. Firstly, we develop a genuinely abstractive model that is not limited to deletion. Our rewrite rules are encoded in quasi-synchronous tree substitution grammar and learned automatically from source documents and their summaries. Unlike previous applications of STSG to sentence compression (Cohn and Lapata, 2009; Cohn and Lapata, 2008) our quasi-synchronous TSG does not attempt to learn the complete translation from source to target sentence; it only loosely links the syntactic structure of the two (Smith and Eisner, 2006), and is therefore well suited to describin</context>
<context position="28068" citStr="Berg-Kirkpatrick et al. (2011)" startWordPosition="4570" endWordPosition="4573"> were compared to give the word list shown in Table 3. We removed words with a source count less than 50, providing a list of 60 lexemes. The resulting integer linear programmes were solved using SCIP,4 and it took 55 seconds on average to read in and solve a document cluster problem. Evaluation We compared our model against two systems. As a baseline, we used the ICSI-1 extractive system (Gillick et al., 2008) which is also based on ILP and was highly ranked in the TAC-2008 evaluation. We also compared against the “learned phrase compression” system of Berg-Kirkpatrick et 3This split follows Berg-Kirkpatrick et al. (2011). 4http://scip.zib.de/ 239 . . VP said (a) S Tue 240 Models 2-R ROUGE Ins TER (%) Shift Count Sentences Mod (%) SU4-R Del Sub CR (%) ICSI-1 11.03 13.96 — — — — 200 — — B-K 11.71 14.47 0.2 26.2 2.3 0.4 216 74.0 63.9 MA-ILP 11.37 14.47 0.7 11.6 5.3 0.6 191 89.1 61.8 ILP w/o bigrams 9.24 12.66 0.8 15.4 11.8 1.2 205 85.4 80.0 ILP w/o salience 11.38 14.71 1.1 19.1 12.0 1.3 233 82.1 92.3 ILP w/o style 11.83 15.09 1.4 17.4 18.9 1.7 271 84.1 86.3 ILP w/o log-ratio 11.41 14.70 1.2 16.9 12.5 1.5 223 84.3 90.1 ILP w/o QTSG 10.32 13.68 0 0 0 0 163 100.0 0 Table 4: Performance of the multiple-aspect ILP mo</context>
</contexts>
<marker>Berg-Kirkpatrick, Gillick, Klein, 2011</marker>
<rawString>Taylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein. 2011. Jointly learning to extract and compress. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 481–490, Portland, Oregon.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Clarke</author>
<author>Mirella Lapata</author>
</authors>
<title>Global inference for sentence compression: An integer linear programming approach.</title>
<date>2008</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>31--273</pages>
<contexts>
<context position="6810" citStr="Clarke and Lapata, 2008" startWordPosition="1011" endWordPosition="1015">obtains performance comparable and in some cases superior to state-of-the-art, in terms of ROUGE and human ratings of summary grammaticality and informativeness. Importantly, there is nothing inherent in our model that is specific to this particular summarization task. As all of the different experts are learned from data, it could easily adapt to other summarization styles or conventions as needed. 2 Related work Recent years have seen increased interest in global inference methods for summarization. ILP-based models have been developed for several subtasks ranging from sentence compression (Clarke and Lapata, 2008), to single- and multi-document summarization (McDonald, 2007; Martins and Smith, 2009; Gillick and Favre, 2009; Woodsend and Lapata, 2010; Berg-Kirkpatrick et al., 2011), and headline generation (Deshpande et al., 2007; Woodsend et al., 2010). Most of these approaches are either purely extractive or implement a single rewrite operation, namely word deletion. Although it is well-known that hand-written summaries often exhibit additional edits and sentence recombinations (Jing, 2002), the challenges involved in acquiring the rewrite rules, interfacing them with inference, and ensuring grammatic</context>
</contexts>
<marker>Clarke, Lapata, 2008</marker>
<rawString>James Clarke and Mirella Lapata. 2008. Global inference for sentence compression: An integer linear programming approach. Journal of Artificial Intelligence Research, 31:273–381.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Cohn</author>
<author>Mirella Lapata</author>
</authors>
<title>Sentence compression beyond word deletion.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics,</booktitle>
<pages>137--144</pages>
<location>Manchester, UK.</location>
<contexts>
<context position="9207" citStr="Cohn and Lapata, 2008" startWordPosition="1384" endWordPosition="1388">s advantageous as it renders inference simpler and affords flexibility (e.g., additional aspects can be incorporated into the model or trained separately on different datasets). Our work differs from Gillick et al. (2009) and Berg-Kirkpatrick et al. (2011) in three important respects. Firstly, we develop a genuinely abstractive model that is not limited to deletion. Our rewrite rules are encoded in quasi-synchronous tree substitution grammar and learned automatically from source documents and their summaries. Unlike previous applications of STSG to sentence compression (Cohn and Lapata, 2009; Cohn and Lapata, 2008) our quasi-synchronous TSG does not attempt to learn the complete translation from source to target sentence; it only loosely links the syntactic structure of the two (Smith and Eisner, 2006), and is therefore well suited to describing the relationship between documents and their abstracts. Secondly, our content selection component extends to features beyond the bigram horizon, as we learn to identify important concepts based on syntactic and positional information. We also learn which words are unlikely to appear in a summary. Thirdly, unlike Berg-Kirkpatrick et al. (2011) our model does not </context>
</contexts>
<marker>Cohn, Lapata, 2008</marker>
<rawString>Trevor Cohn and Mirella Lapata. 2008. Sentence compression beyond word deletion. In Proceedings of the 22nd International Conference on Computational Linguistics, pages 137–144, Manchester, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Cohn</author>
<author>Mirella Lapata</author>
</authors>
<title>Sentence compression as tree transduction.</title>
<date>2009</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>34--637</pages>
<contexts>
<context position="9183" citStr="Cohn and Lapata, 2009" startWordPosition="1380" endWordPosition="1383">xpert, which we argue is advantageous as it renders inference simpler and affords flexibility (e.g., additional aspects can be incorporated into the model or trained separately on different datasets). Our work differs from Gillick et al. (2009) and Berg-Kirkpatrick et al. (2011) in three important respects. Firstly, we develop a genuinely abstractive model that is not limited to deletion. Our rewrite rules are encoded in quasi-synchronous tree substitution grammar and learned automatically from source documents and their summaries. Unlike previous applications of STSG to sentence compression (Cohn and Lapata, 2009; Cohn and Lapata, 2008) our quasi-synchronous TSG does not attempt to learn the complete translation from source to target sentence; it only loosely links the syntactic structure of the two (Smith and Eisner, 2006), and is therefore well suited to describing the relationship between documents and their abstracts. Secondly, our content selection component extends to features beyond the bigram horizon, as we learn to identify important concepts based on syntactic and positional information. We also learn which words are unlikely to appear in a summary. Thirdly, unlike Berg-Kirkpatrick et al. (2</context>
<context position="21230" citStr="Cohn and Lapata, 2009" startWordPosition="3380" endWordPosition="3383">m-up” process gives us better ability to match non-isomorphic structures. We do not assume an alignment between source and target root nodes, nor do we require a surjective alignment of all target nodes to the source tree. QTSG rules are then created from aligned nodes above the leaf node level if all the nodes in the target tree can be explained using nodes from the source. Individual rewrite rules describe the mapping of source tree fragments into target tree fragments, and so the grammar represents the space of valid target trees that can be produced from a given source tree (Eisner, 2003; Cohn and Lapata, 2009). Examples of the most frequent QTSG rules learned by the above process are shown in Figure 1. Many of the rules relate to the compression of noun phrases through deletion, and examples are shown in the upper box. Others capture the compression of verb phrases (middle box). An important rewrite operation is the abstraction of a sentence from a more complex source sentence, adding final punctuation if necessary (lower box). At generation, paraphrases are created from source sentence parse trees by identifying and applying QTSG rules with matching structure. The transduction process starts at th</context>
</contexts>
<marker>Cohn, Lapata, 2009</marker>
<rawString>Trevor Cohn and Mirella Lapata. 2009. Sentence compression as tree transduction. Journal of Artificial Intelligence Research, 34:637–674.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pawan Deshpande</author>
<author>Regina Barzilay</author>
<author>David Karger</author>
</authors>
<title>Randomized decoding for selection-andordering problems.</title>
<date>2007</date>
<booktitle>In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference,</booktitle>
<pages>444--451</pages>
<location>Rochester, New York.</location>
<contexts>
<context position="7029" citStr="Deshpande et al., 2007" startWordPosition="1045" endWordPosition="1048"> specific to this particular summarization task. As all of the different experts are learned from data, it could easily adapt to other summarization styles or conventions as needed. 2 Related work Recent years have seen increased interest in global inference methods for summarization. ILP-based models have been developed for several subtasks ranging from sentence compression (Clarke and Lapata, 2008), to single- and multi-document summarization (McDonald, 2007; Martins and Smith, 2009; Gillick and Favre, 2009; Woodsend and Lapata, 2010; Berg-Kirkpatrick et al., 2011), and headline generation (Deshpande et al., 2007; Woodsend et al., 2010). Most of these approaches are either purely extractive or implement a single rewrite operation, namely word deletion. Although it is well-known that hand-written summaries often exhibit additional edits and sentence recombinations (Jing, 2002), the challenges involved in acquiring the rewrite rules, interfacing them with inference, and ensuring grammatical output make the development of abstractive models non-trivial. Our work is closest to Gillick et al. (2008) who also develop an ILP model for multi-document summarization. A key assumption in their model which we als</context>
</contexts>
<marker>Deshpande, Barzilay, Karger, 2007</marker>
<rawString>Pawan Deshpande, Regina Barzilay, and David Karger. 2007. Randomized decoding for selection-andordering problems. In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference, pages 444–451, Rochester, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
</authors>
<title>Learning non-isomorphic tree mappings for machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the ACL Interactive Poster/Demonstration Sessions,</booktitle>
<pages>205--208</pages>
<location>Sapporo, Japan.</location>
<contexts>
<context position="5219" citStr="Eisner, 2003" startWordPosition="766" endWordPosition="768">the use of unique bigram information to model content and avoid redundancy, positional information to model important and poor locations of content, and language modeling to capture stylistic conventions. Learning each predictor separately gives better generalization, while the ILP framework allows us to combine the decisions of the expert learners through the use of objectives, hard and soft constraints. The experts work collaboratively to rewrite the content using rules extracted from document clusters and model summaries. We adopt the synchronous tree substitution grammar (STSG) formalism (Eisner, 2003) which can model non-isomorphic tree structures (the grammar rules can comprise trees of arbitrary depth) and is thus suited to text-rewriting tasks which typically involve a number of local modifications to the input text. Specifically, we propose quasi-synchronous tree substitution grammar (QTSG) as a flexible formalism to learn general treeedits from loosely-aligned phrase structure trees. We evaluate our model on the 100-word “non2Our task is standard multi-document summarization and should not be confused with “guided” summarization where system and human summarizers are given a list of i</context>
<context position="21206" citStr="Eisner, 2003" startWordPosition="3378" endWordPosition="3379">hronous “bottom-up” process gives us better ability to match non-isomorphic structures. We do not assume an alignment between source and target root nodes, nor do we require a surjective alignment of all target nodes to the source tree. QTSG rules are then created from aligned nodes above the leaf node level if all the nodes in the target tree can be explained using nodes from the source. Individual rewrite rules describe the mapping of source tree fragments into target tree fragments, and so the grammar represents the space of valid target trees that can be produced from a given source tree (Eisner, 2003; Cohn and Lapata, 2009). Examples of the most frequent QTSG rules learned by the above process are shown in Figure 1. Many of the rules relate to the compression of noun phrases through deletion, and examples are shown in the upper box. Others capture the compression of verb phrases (middle box). An important rewrite operation is the abstraction of a sentence from a more complex source sentence, adding final punctuation if necessary (lower box). At generation, paraphrases are created from source sentence parse trees by identifying and applying QTSG rules with matching structure. The transduct</context>
</contexts>
<marker>Eisner, 2003</marker>
<rawString>Jason Eisner. 2003. Learning non-isomorphic tree mappings for machine translation. In Proceedings of the ACL Interactive Poster/Demonstration Sessions, pages 205–208, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elena Filatova</author>
<author>Vasileios Hatzivassiloglou</author>
</authors>
<title>A formal model for information selection in multisentence text extraction.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th International Conference on Computational Linguistics,</booktitle>
<pages>397--403</pages>
<location>Geneva, Switzerland.</location>
<contexts>
<context position="2781" citStr="Filatova and Hatzivassiloglou (2004)" startWordPosition="389" endWordPosition="392">a (i.e., document clusters and their corresponding summaries). This global inference problem is, however, hard — the solution space is large and the lack of easily accessible datasets an obstacle to joint learning. It is thus no surprise that previous work has focused on specific aspects of joint learning. Initial global formulations of the multi-document summarization task focused on extractive summarization and used approximate greedy algorithms for finding the sentences of the summary. Goldstein et al. (2000) search for the set of sentences that are both relevant and non-redundant, whereas Filatova and Hatzivassiloglou (2004) model multi-document summarization as an instance of the maximum coverage set problem.1 More recent work improves on the search problem by considering exact solutions and permits a limited amount of rewriting. McDonald (2007) proposes an integer linear programming formulation that maximizes the sum of relevance scores of the selected sentences penalized by the 1Given C, a finite set of weighted elements, a collection T of subsets of C, and an integer k, find those k sets that maximize the total number of elements in the union of T’s members (Hochba, 1997). 233 Proceedings of the 2012 Joint Co</context>
</contexts>
<marker>Filatova, Hatzivassiloglou, 2004</marker>
<rawString>Elena Filatova and Vasileios Hatzivassiloglou. 2004. A formal model for information selection in multisentence text extraction. In Proceedings of the 20th International Conference on Computational Linguistics, pages 397–403, Geneva, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Gillick</author>
<author>Benoit Favre</author>
</authors>
<title>A scalable global model for summarization.</title>
<date>2009</date>
<booktitle>In Proceedings of the Workshop on Integer Linear Programming for Natural Language Processing,</booktitle>
<pages>10--18</pages>
<location>Boulder, Colorado.</location>
<contexts>
<context position="6921" citStr="Gillick and Favre, 2009" startWordPosition="1028" endWordPosition="1031">gs of summary grammaticality and informativeness. Importantly, there is nothing inherent in our model that is specific to this particular summarization task. As all of the different experts are learned from data, it could easily adapt to other summarization styles or conventions as needed. 2 Related work Recent years have seen increased interest in global inference methods for summarization. ILP-based models have been developed for several subtasks ranging from sentence compression (Clarke and Lapata, 2008), to single- and multi-document summarization (McDonald, 2007; Martins and Smith, 2009; Gillick and Favre, 2009; Woodsend and Lapata, 2010; Berg-Kirkpatrick et al., 2011), and headline generation (Deshpande et al., 2007; Woodsend et al., 2010). Most of these approaches are either purely extractive or implement a single rewrite operation, namely word deletion. Although it is well-known that hand-written summaries often exhibit additional edits and sentence recombinations (Jing, 2002), the challenges involved in acquiring the rewrite rules, interfacing them with inference, and ensuring grammatical output make the development of abstractive models non-trivial. Our work is closest to Gillick et al. (2008) </context>
</contexts>
<marker>Gillick, Favre, 2009</marker>
<rawString>Dan Gillick and Benoit Favre. 2009. A scalable global model for summarization. In Proceedings of the Workshop on Integer Linear Programming for Natural Language Processing, pages 10–18, Boulder, Colorado.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Gillick</author>
<author>Benoit Favre</author>
<author>Dilek Hakkani-t¨ur</author>
</authors>
<title>The ICSI summarization system at TAC</title>
<date>2008</date>
<booktitle>In Proceedings of the Text Analysis Conference.</booktitle>
<marker>Gillick, Favre, Hakkani-t¨ur, 2008</marker>
<rawString>Dan Gillick, Benoit Favre, and Dilek Hakkani-t¨ur. 2008. The ICSI summarization system at TAC 2008. In Proceedings of the Text Analysis Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Gillick</author>
<author>Benoit Favre</author>
<author>Dilek Hakkani-t¨ur</author>
<author>Berndt Bohnet</author>
<author>Yang Liu</author>
<author>Shasha Xie</author>
</authors>
<title>The ICSI/UTD summarization system at TAC</title>
<date>2009</date>
<booktitle>In Proceedings of the Text Analysis Conference.</booktitle>
<marker>Gillick, Favre, Hakkani-t¨ur, Bohnet, Liu, Xie, 2009</marker>
<rawString>Dan Gillick, Benoit Favre, Dilek Hakkani-t¨ur, Berndt Bohnet, Yang Liu, and Shasha Xie. 2009. The ICSI/UTD summarization system at TAC 2009. In Proceedings of the Text Analysis Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jade Goldstein</author>
<author>Vibhu Mittal</author>
<author>Jaime Carbonell</author>
<author>Mark Kantrowitz</author>
</authors>
<title>Multi-document summarization by sentence extraction.</title>
<date>2000</date>
<booktitle>In Proceedings of the 2000 NAACL–ANLP Workshop on Automatic Summarization,</booktitle>
<pages>40--48</pages>
<location>Seattle, Washington.</location>
<contexts>
<context position="2662" citStr="Goldstein et al. (2000)" startWordPosition="372" endWordPosition="375">ns. An ideal model would learn to output summaries that simultaneously meet all these constraints from data (i.e., document clusters and their corresponding summaries). This global inference problem is, however, hard — the solution space is large and the lack of easily accessible datasets an obstacle to joint learning. It is thus no surprise that previous work has focused on specific aspects of joint learning. Initial global formulations of the multi-document summarization task focused on extractive summarization and used approximate greedy algorithms for finding the sentences of the summary. Goldstein et al. (2000) search for the set of sentences that are both relevant and non-redundant, whereas Filatova and Hatzivassiloglou (2004) model multi-document summarization as an instance of the maximum coverage set problem.1 More recent work improves on the search problem by considering exact solutions and permits a limited amount of rewriting. McDonald (2007) proposes an integer linear programming formulation that maximizes the sum of relevance scores of the selected sentences penalized by the 1Given C, a finite set of weighted elements, a collection T of subsets of C, and an integer k, find those k sets that</context>
</contexts>
<marker>Goldstein, Mittal, Carbonell, Kantrowitz, 2000</marker>
<rawString>Jade Goldstein, Vibhu Mittal, Jaime Carbonell, and Mark Kantrowitz. 2000. Multi-document summarization by sentence extraction. In Proceedings of the 2000 NAACL–ANLP Workshop on Automatic Summarization, pages 40–48, Seattle, Washington.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dorit S Hochba</author>
</authors>
<title>Approximating covering and packing problems: Set cover, vertex cover, independent set, and related problems.</title>
<date>1997</date>
<booktitle>Approximation Algorithms for NPHard Problems,</booktitle>
<pages>94--143</pages>
<editor>In Dorit S. Hochba, editor,</editor>
<publisher>PWS Publishing Company,</publisher>
<location>Boston, MA.</location>
<contexts>
<context position="3343" citStr="Hochba, 1997" startWordPosition="484" endWordPosition="485">ndant, whereas Filatova and Hatzivassiloglou (2004) model multi-document summarization as an instance of the maximum coverage set problem.1 More recent work improves on the search problem by considering exact solutions and permits a limited amount of rewriting. McDonald (2007) proposes an integer linear programming formulation that maximizes the sum of relevance scores of the selected sentences penalized by the 1Given C, a finite set of weighted elements, a collection T of subsets of C, and an integer k, find those k sets that maximize the total number of elements in the union of T’s members (Hochba, 1997). 233 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 233–243, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics sum of redundancy scores of all pairs of selected sentences. Gillick et al. (2008) develop an exact solution for a model similar to Filatova and Hatzivassiloglou (2004) under the assumption that the value of a summary is the sum of values of the unique concepts (approximated by bigrams) it contains. Subsequent work (Gillick et al., 2009; Berg-Kirkpatric</context>
</contexts>
<marker>Hochba, 1997</marker>
<rawString>Dorit S. Hochba. 1997. Approximating covering and packing problems: Set cover, vertex cover, independent set, and related problems. In Dorit S. Hochba, editor, Approximation Algorithms for NPHard Problems, pages 94–143. PWS Publishing Company, Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Honyang Jing</author>
</authors>
<title>Using Hidden Markov modeling to decompose human-written summaries.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>4</issue>
<contexts>
<context position="7297" citStr="Jing, 2002" startWordPosition="1087" endWordPosition="1088">ation. ILP-based models have been developed for several subtasks ranging from sentence compression (Clarke and Lapata, 2008), to single- and multi-document summarization (McDonald, 2007; Martins and Smith, 2009; Gillick and Favre, 2009; Woodsend and Lapata, 2010; Berg-Kirkpatrick et al., 2011), and headline generation (Deshpande et al., 2007; Woodsend et al., 2010). Most of these approaches are either purely extractive or implement a single rewrite operation, namely word deletion. Although it is well-known that hand-written summaries often exhibit additional edits and sentence recombinations (Jing, 2002), the challenges involved in acquiring the rewrite rules, interfacing them with inference, and ensuring grammatical output make the development of abstractive models non-trivial. Our work is closest to Gillick et al. (2008) who also develop an ILP model for multi-document summarization. A key assumption in their model which we also follow is that input documents contain a variety of concepts, each of which are allocated a value, and the goal of a good summary is to maximize the sum of these values subject to the length constraint. The authors use bigrams as concepts and their frequency in the </context>
</contexts>
<marker>Jing, 2002</marker>
<rawString>Honyang Jing. 2002. Using Hidden Markov modeling to decompose human-written summaries. Computational Linguistics, 28(4):527–544.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Association for Computational Linguistics,</booktitle>
<pages>423--430</pages>
<location>Sapporo, Japan.</location>
<contexts>
<context position="11956" citStr="Klein and Manning, 2003" startWordPosition="1816" endWordPosition="1819">ternative compressions and paraphrases of the source sentences, in the style suitable for the summaries. Finally, an ILP model combines the output of these components into a summary, jointly optimizing content selection and surface realization preferences, and providing the flexibility to treat some components as soft while others as hard constraints. 3.1 Document Representation Given an input sentence, our approach deconstructs it into component phrases and clauses, typical of a phrase structure parser. In our experiments, we obtain this representation from the output of the Stanford parser (Klein and Manning, 2003) but any other broadly similar parser could be used instead. Nodes in the parse tree represent points where QTSG rules can be applied (and paraphrases generated), and they also represent decision points for the ILP. In the following, we will refer to these decision nodes as the set N , and decisions for each node using the binary variable zi, i EN. 3.2 Content Selection Using Bigrams We follow Gillick et al. (2008) in modeling the information content of the summary as the weighted sum of the individual information units it contains. We represent information units as the set of bigrams B seen i</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Accurate unlexicalized parsing. In Proceedings of the 41st Association for Computational Linguistics, pages 423– 430, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
<author>Eduard H Hovy</author>
</authors>
<title>Automatic evaluation of summaries using n-gram co-occurrence statistics.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT–NAACL,</booktitle>
<pages>71--78</pages>
<location>Edmonton, Canada.</location>
<marker>Lin, Hovy, 2003</marker>
<rawString>Chin-Yew Lin and Eduard H. Hovy. 2003. Automatic evaluation of summaries using n-gram co-occurrence statistics. In Proceedings of HLT–NAACL, pages 71– 78, Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Inderjeet Mani</author>
</authors>
<title>Automatic Summarization.</title>
<date>2001</date>
<publisher>John Benjamins Pub Co.</publisher>
<contexts>
<context position="1467" citStr="Mani (2001)" startWordPosition="202" endWordPosition="203">the TAC-08 data set show that our model achieves state-of-the-art performance using ROUGE and significantly improves the informativeness of the summaries. 1 Introduction Automatic summarization has enjoyed wide popularity in natural language processing (see the proceedings of the Document Understanding and Text Analysis conferences) due to its potential for practical applications but also because it incorporates many important aspects of both natural language understanding and generation. Of the many summarization paradigms that have been identified over the years (see Sparck Jones (1999) and Mani (2001) for comprehensive overviews), multi-document summarization — the task of producing summaries from clusters of thematically related documents — has consistently attracted attention. Despite considerable research effort, the automatic generation of multi-document summaries that resemble those written by humans remains challenging. This is primarily due to the task itself which is complex and subject to several constraints: the summary must be maximally informative and minimally redundant, grammatical, coherent, adhere to a pre-specified length and stylistic conventions. An ideal model would lea</context>
</contexts>
<marker>Mani, 2001</marker>
<rawString>Inderjeet Mani. 2001. Automatic Summarization. John Benjamins Pub Co.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andr´e Martins</author>
<author>Noah A Smith</author>
</authors>
<title>Summarization with a joint model for sentence extraction and compression.</title>
<date>2009</date>
<booktitle>In Proceedings of the Workshop on Integer Linear Programming for Natural Language Processing,</booktitle>
<pages>1--9</pages>
<location>Boulder, Colorado.</location>
<contexts>
<context position="6896" citStr="Martins and Smith, 2009" startWordPosition="1024" endWordPosition="1027"> of ROUGE and human ratings of summary grammaticality and informativeness. Importantly, there is nothing inherent in our model that is specific to this particular summarization task. As all of the different experts are learned from data, it could easily adapt to other summarization styles or conventions as needed. 2 Related work Recent years have seen increased interest in global inference methods for summarization. ILP-based models have been developed for several subtasks ranging from sentence compression (Clarke and Lapata, 2008), to single- and multi-document summarization (McDonald, 2007; Martins and Smith, 2009; Gillick and Favre, 2009; Woodsend and Lapata, 2010; Berg-Kirkpatrick et al., 2011), and headline generation (Deshpande et al., 2007; Woodsend et al., 2010). Most of these approaches are either purely extractive or implement a single rewrite operation, namely word deletion. Although it is well-known that hand-written summaries often exhibit additional edits and sentence recombinations (Jing, 2002), the challenges involved in acquiring the rewrite rules, interfacing them with inference, and ensuring grammatical output make the development of abstractive models non-trivial. Our work is closest </context>
</contexts>
<marker>Martins, Smith, 2009</marker>
<rawString>Andr´e Martins and Noah A. Smith. 2009. Summarization with a joint model for sentence extraction and compression. In Proceedings of the Workshop on Integer Linear Programming for Natural Language Processing, pages 1–9, Boulder, Colorado.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
</authors>
<title>A study of global inference algorithms in multi-document summarization.</title>
<date>2007</date>
<booktitle>In Proceedings of the 29th European conference on IR Research,</booktitle>
<pages>557--564</pages>
<location>Rome, Italy.</location>
<contexts>
<context position="3007" citStr="McDonald (2007)" startWordPosition="425" endWordPosition="427">ious work has focused on specific aspects of joint learning. Initial global formulations of the multi-document summarization task focused on extractive summarization and used approximate greedy algorithms for finding the sentences of the summary. Goldstein et al. (2000) search for the set of sentences that are both relevant and non-redundant, whereas Filatova and Hatzivassiloglou (2004) model multi-document summarization as an instance of the maximum coverage set problem.1 More recent work improves on the search problem by considering exact solutions and permits a limited amount of rewriting. McDonald (2007) proposes an integer linear programming formulation that maximizes the sum of relevance scores of the selected sentences penalized by the 1Given C, a finite set of weighted elements, a collection T of subsets of C, and an integer k, find those k sets that maximize the total number of elements in the union of T’s members (Hochba, 1997). 233 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 233–243, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics sum of redundancy s</context>
<context position="6871" citStr="McDonald, 2007" startWordPosition="1022" endWordPosition="1023">he-art, in terms of ROUGE and human ratings of summary grammaticality and informativeness. Importantly, there is nothing inherent in our model that is specific to this particular summarization task. As all of the different experts are learned from data, it could easily adapt to other summarization styles or conventions as needed. 2 Related work Recent years have seen increased interest in global inference methods for summarization. ILP-based models have been developed for several subtasks ranging from sentence compression (Clarke and Lapata, 2008), to single- and multi-document summarization (McDonald, 2007; Martins and Smith, 2009; Gillick and Favre, 2009; Woodsend and Lapata, 2010; Berg-Kirkpatrick et al., 2011), and headline generation (Deshpande et al., 2007; Woodsend et al., 2010). Most of these approaches are either purely extractive or implement a single rewrite operation, namely word deletion. Although it is well-known that hand-written summaries often exhibit additional edits and sentence recombinations (Jing, 2002), the challenges involved in acquiring the rewrite rules, interfacing them with inference, and ensuring grammatical output make the development of abstractive models non-triv</context>
<context position="13230" citStr="McDonald, 2007" startWordPosition="2042" endWordPosition="2044">ulated from the number of source documents where the bigram was seen. The summary is thus given the score fB(z), i.e., the weighted sum of 235 its information units: fB(z) _ ∑ wjbj (1) jEB where wj is the weight of concept j, bj a binary variable to indicate if concept j is present in the summary, and j E B. Importantly, each information unit is counted only once; this encourages wide coverage of the source documents, and removes any drive towards redundant information without actively discouraging it, contrary to other global formulations where redundancy measures form part of the objective (McDonald, 2007). The counting mechanism is achieved by linking the variables z indicating nodes in the parse tree and b indicating bigrams: bj &lt; ∑ zi Vj E B (2) iEN: jEBi where Bi C B is the subset of bigrams that are contained in node i. A drawback of the global nature of this counting mechanism, however, is that it cannot be integrated with local features such as those described below; our approach takes local features into account but these are weighted by other components. 3.3 Content Selection Using Salience The bigram approach is a powerful method for identifying important concepts within the document </context>
<context position="24616" citStr="McDonald (2007)" startWordPosition="4001" endWordPosition="4002">max fB(z)+ fS(z) + fLR (z) (8) z This objective is subject to the constraints (2), (4), (6), and (7) that represent hard constraint decisions, or maintain the logical integrity of the model. An overall length constraint completes the model: E lizi G lmax (9) iEN where li is the number of words generated by choosing node i, and lmax is the global word length limit. Note that the scores in the objective are for each tree node and not each sentence. This affords the model flexibility: the content selection elements are generally not competing with each other to give a decision on a sentence (see McDonald (2007)). Instead, components are marking positive and negative nodes. The ILP is implicitly searching the grammar rules for ways to rewrite the sentence, with the aim of including the salient nodes while removing negative-scoring nodes (deleting them increases the score of the node to zero). Figure 2 shows an example of a source sentence where the bigram, salience and language preference components of the ILP work together to score nodes in the parse tree. The nodes NP1, VP3 and VP4 all have positive scores, while “said Tuesday” is negative. As a rewrite possibility, the rewrite rule shown bottom le</context>
</contexts>
<marker>McDonald, 2007</marker>
<rawString>Ryan McDonald. 2007. A study of global inference algorithms in multi-document summarization. In Proceedings of the 29th European conference on IR Research, pages 557–564, Rome, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rani Nelken</author>
<author>Stuart Schieber</author>
</authors>
<title>Towards robust context-sensitive sentence alignment for monolingual corpora.</title>
<date>2006</date>
<booktitle>In Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>161--168</pages>
<location>Trento, Italy.</location>
<marker>Nelken, Schieber, 2006</marker>
<rawString>Rani Nelken and Stuart Schieber. 2006. Towards robust context-sensitive sentence alignment for monolingual corpora. In Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics, pages 161–168, Trento, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Smith</author>
<author>Jason Eisner</author>
</authors>
<title>Quasi-synchronous grammars: Alignment by soft projection of syntactic dependencies.</title>
<date>2006</date>
<booktitle>In Proceedings of Workshop on Statistical Machine Translation,</booktitle>
<pages>23--30</pages>
<contexts>
<context position="9398" citStr="Smith and Eisner, 2006" startWordPosition="1416" endWordPosition="1419">fers from Gillick et al. (2009) and Berg-Kirkpatrick et al. (2011) in three important respects. Firstly, we develop a genuinely abstractive model that is not limited to deletion. Our rewrite rules are encoded in quasi-synchronous tree substitution grammar and learned automatically from source documents and their summaries. Unlike previous applications of STSG to sentence compression (Cohn and Lapata, 2009; Cohn and Lapata, 2008) our quasi-synchronous TSG does not attempt to learn the complete translation from source to target sentence; it only loosely links the syntactic structure of the two (Smith and Eisner, 2006), and is therefore well suited to describing the relationship between documents and their abstracts. Secondly, our content selection component extends to features beyond the bigram horizon, as we learn to identify important concepts based on syntactic and positional information. We also learn which words are unlikely to appear in a summary. Thirdly, unlike Berg-Kirkpatrick et al. (2011) our model does not try to learn all the parameters (e.g., content, rewrite rules, style) of the summarization problem jointly; although decoupling learning from inference is perhaps less elegant from a modeling</context>
<context position="22055" citStr="Smith and Eisner, 2006" startWordPosition="3513" endWordPosition="3517"> upper box. Others capture the compression of verb phrases (middle box). An important rewrite operation is the abstraction of a sentence from a more complex source sentence, adding final punctuation if necessary (lower box). At generation, paraphrases are created from source sentence parse trees by identifying and applying QTSG rules with matching structure. The transduction process starts at the root node of the parse tree, applying QTSG rules to sub-trees until leaf nodes are reached. Note that we do not use the Bayesian probability model normally associated with quasi-synchronous grammars (Smith and Eisner, 2006); instead, we ask the QTSG to provide (NP, NP) , ([NP1 PP], [NP1 ]) (NP, NP) , ([NP1 VP], [NP1 ]) (NP, NP) , ([NP1 SBAR], [NP1 ]) (NP, NP) , ([NP1 , NP ,], [NP1 ]) (NP, NP) , ([NP1 CC NP], [NP1 ]) (NP, NP) , ([NNP NNP1 ], [NNP1 ]) (NP, NP) , ([DT1 JJ NN2], [DT1 NN2]) Figure 1: Examples of most frequently learned QTSG rules. Boxed subscripts show aligned nodes. paraphrases that are acceptable rather than probable, and generate all paraphrases licensed by the QTSG. The alternative paraphrases are incorporated into the target phrase structure tree as choices that the ILP can make. We use the set </context>
</contexts>
<marker>Smith, Eisner, 2006</marker>
<rawString>David Smith and Jason Eisner. 2006. Quasi-synchronous grammars: Alignment by soft projection of syntactic dependencies. In Proceedings of Workshop on Statistical Machine Translation, pages 23–30, NYC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
<author>Linnea Micciulla</author>
<author>John Makhoul</author>
</authors>
<title>A study of translation edit rate with targeted human annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 7th Conference of the Association for Machine Translation in the Americas,</booktitle>
<pages>223--231</pages>
<location>Cambridge.</location>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A study of translation edit rate with targeted human annotation. In Proceedings of the 7th Conference of the Association for Machine Translation in the Americas, pages 223–231, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karen Sparck Jones</author>
</authors>
<title>Automatic summarizing: Factors and directions.</title>
<date>1999</date>
<booktitle>In Inderjeet Mani and</booktitle>
<pages>1--33</pages>
<editor>Mark T. Maybury, editors,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge.</location>
<contexts>
<context position="1451" citStr="Jones (1999)" startWordPosition="199" endWordPosition="200">ental results on the TAC-08 data set show that our model achieves state-of-the-art performance using ROUGE and significantly improves the informativeness of the summaries. 1 Introduction Automatic summarization has enjoyed wide popularity in natural language processing (see the proceedings of the Document Understanding and Text Analysis conferences) due to its potential for practical applications but also because it incorporates many important aspects of both natural language understanding and generation. Of the many summarization paradigms that have been identified over the years (see Sparck Jones (1999) and Mani (2001) for comprehensive overviews), multi-document summarization — the task of producing summaries from clusters of thematically related documents — has consistently attracted attention. Despite considerable research effort, the automatic generation of multi-document summaries that resemble those written by humans remains challenging. This is primarily due to the task itself which is complex and subject to several constraints: the summary must be maximally informative and minimally redundant, grammatical, coherent, adhere to a pre-specified length and stylistic conventions. An ideal</context>
</contexts>
<marker>Jones, 1999</marker>
<rawString>Karen Sparck Jones. 1999. Automatic summarizing: Factors and directions. In Inderjeet Mani and Mark T. Maybury, editors, Advances in Automatic Text Summarization, pages 1–33. MIT Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristian Woodsend</author>
<author>Jacek Gondzio</author>
</authors>
<title>Exploiting separability in large-scale linear support vector machine training. Computational Optimization and Applications.</title>
<date>2009</date>
<contexts>
<context position="26400" citStr="Woodsend and Gondzio, 2009" startWordPosition="4291" endWordPosition="4294">m TAC-2009 as training data, to learn the different elements of the model. The 48 document clusters of TAC-2008 were reserved for the generation of test summaries.3 Training The two components described in Sections 3.3 and 3.4 were trained using binary SVM classifiers, with labels inferred automatically via alignment. The salience classifier was trained on 102,754 node instances (16,042 positive and 86,712 negative). The style classifier was trained on 20,443 sentence instances (2,083 positive and 18,360 negative). We learned the feature weights with a linear SVM, using the software SVM-OOPS (Woodsend and Gondzio, 2009). Because of the high compression rate in this task, sentence alignment leads to an unbalanced data set. We compensated for this by using different SVM hyper-parameters C+ and C− as the loss multiplier for misclassification of positive and negative training samples respectively. SVM hyper-parameters were chosen that gave the highest F1 values using 10-fold cross-validation. The salience SVM obtained a precision of 0.28 and recall of 0.43. Precision for the style SVM was 0.20 and recall 0.63, respectively. The classifiers on their own would thus not be great predictors of salience or style, but</context>
</contexts>
<marker>Woodsend, Gondzio, 2009</marker>
<rawString>Kristian Woodsend and Jacek Gondzio. 2009. Exploiting separability in large-scale linear support vector machine training. Computational Optimization and Applications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristian Woodsend</author>
<author>Mirella Lapata</author>
</authors>
<title>Automatic generation of story highlights.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>565--574</pages>
<location>Uppsala,</location>
<contexts>
<context position="6948" citStr="Woodsend and Lapata, 2010" startWordPosition="1032" endWordPosition="1036">ity and informativeness. Importantly, there is nothing inherent in our model that is specific to this particular summarization task. As all of the different experts are learned from data, it could easily adapt to other summarization styles or conventions as needed. 2 Related work Recent years have seen increased interest in global inference methods for summarization. ILP-based models have been developed for several subtasks ranging from sentence compression (Clarke and Lapata, 2008), to single- and multi-document summarization (McDonald, 2007; Martins and Smith, 2009; Gillick and Favre, 2009; Woodsend and Lapata, 2010; Berg-Kirkpatrick et al., 2011), and headline generation (Deshpande et al., 2007; Woodsend et al., 2010). Most of these approaches are either purely extractive or implement a single rewrite operation, namely word deletion. Although it is well-known that hand-written summaries often exhibit additional edits and sentence recombinations (Jing, 2002), the challenges involved in acquiring the rewrite rules, interfacing them with inference, and ensuring grammatical output make the development of abstractive models non-trivial. Our work is closest to Gillick et al. (2008) who also develop an ILP mod</context>
</contexts>
<marker>Woodsend, Lapata, 2010</marker>
<rawString>Kristian Woodsend and Mirella Lapata. 2010. Automatic generation of story highlights. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 565–574, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristian Woodsend</author>
<author>Yansong Feng</author>
<author>Mirella Lapata</author>
</authors>
<title>Title generation with quasi-synchronous grammar.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>513--523</pages>
<location>Cambridge, MA.</location>
<contexts>
<context position="7053" citStr="Woodsend et al., 2010" startWordPosition="1049" endWordPosition="1053">ular summarization task. As all of the different experts are learned from data, it could easily adapt to other summarization styles or conventions as needed. 2 Related work Recent years have seen increased interest in global inference methods for summarization. ILP-based models have been developed for several subtasks ranging from sentence compression (Clarke and Lapata, 2008), to single- and multi-document summarization (McDonald, 2007; Martins and Smith, 2009; Gillick and Favre, 2009; Woodsend and Lapata, 2010; Berg-Kirkpatrick et al., 2011), and headline generation (Deshpande et al., 2007; Woodsend et al., 2010). Most of these approaches are either purely extractive or implement a single rewrite operation, namely word deletion. Although it is well-known that hand-written summaries often exhibit additional edits and sentence recombinations (Jing, 2002), the challenges involved in acquiring the rewrite rules, interfacing them with inference, and ensuring grammatical output make the development of abstractive models non-trivial. Our work is closest to Gillick et al. (2008) who also develop an ILP model for multi-document summarization. A key assumption in their model which we also follow is that input d</context>
</contexts>
<marker>Woodsend, Feng, Lapata, 2010</marker>
<rawString>Kristian Woodsend, Yansong Feng, and Mirella Lapata. 2010. Title generation with quasi-synchronous grammar. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 513–523, Cambridge, MA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
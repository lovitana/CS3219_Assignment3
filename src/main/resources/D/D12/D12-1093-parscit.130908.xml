<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.887952">
Reading The Web with Learned Syntactic-Semantic Inference Rules
</title>
<author confidence="0.818009">
Ni Lao&apos;; Amarnag Subramanya&apos;, Fernando Pereira&apos;, William W. Cohen&apos;
</author>
<affiliation confidence="0.75918">
&apos;Carnegie Mellon University, 5000 Forbes Avenue, Pittsburgh, PA 15213, USA
</affiliation>
<address confidence="0.339055">
&apos;Google Research, 1600 Amphitheatre Parkway, Mountain View, CA 94043, USA
</address>
<email confidence="0.937137">
nlao@cs.cmu.edu, {asubram, pereira}@google.com, wcohen@cs.cmu.edu
</email>
<sectionHeader confidence="0.983188" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998785647058823">
We study how to extend a large knowledge
base (Freebase) by reading relational informa-
tion from a large Web text corpus. Previous
studies on extracting relational knowledge
from text show the potential of syntactic
patterns for extraction, but they do not exploit
background knowledge of other relations
in the knowledge base. We describe a
distributed, Web-scale implementation of a
path-constrained random walk model that
learns syntactic-semantic inference rules for
binary relations from a graph representation
of the parsed text and the knowledge base.
Experiments show significant accuracy im-
provements in binary relation prediction over
methods that consider only text, or only the
existing knowledge base.
</bodyText>
<sectionHeader confidence="0.992349" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.969396319148936">
Manually-created knowledge bases (KBs) often lack
basic information about some entities and their
relationships, either because the information was
missing in the initial sources used to create the
KB, or because human curators were not confident
about the status of some putative fact, and so they
excluded it from the KB. For instance, as we will
see in more detail later, many person entries in
Freebase (Bollacker et al., 2008) lack nationality
information. To fill those KB gaps, we might use
general rules, ideally automatically learned, such as
“if person was born in town and town is in country
*This research was carried out during an internship at
Google Research
then the person is a national of the country.” Of
course, rules like this may be defeasible, in this case
for example because of naturalization or political
changes. Nevertheless, many such imperfect rules
can be learned and combined to yield useful KB
completions, as demonstrated in particular with the
Path-Ranking Algorithm (PRA) (Lao and Cohen,
2010; Lao et al., 2011), which learns such rules on
heterogenous graphs for link prediction tasks.
Alternatively, we may attempt to fill KB gaps by
applying relation extraction rules to free text. For
instance, Snow et al. (2005) and Suchanek et al.
(2006) showed the value of syntactic patterns in
extracting specific relations. In those approaches,
KB tuples of the relation to be extracted serve as
positive training examples to the extraction rule
induction algorithm. However, the KB contains
much more knowledge about other relations that
could potentially be helpful in improving relation
extraction accuracy and coverage, but that is not
used in such purely text-based approaches.
In this work, we use PRA to learn weighted
rules (represented as graph path patterns) that
combine both semantic (KB) and syntactic infor-
mation encoded respectively as edges in a graph-
structured KB, and as syntactic dependency edges
in dependency-parsed Web text. Our approach can
easily incorporate existing knowledge in extraction
tasks, and its distributed implementation scales to
the whole of the Freebase KB and 60 million parsed
documents. To the best of our knowledge, this is the
first successful attempt to apply relational learning
methods to heterogeneous data with this scale.
</bodyText>
<page confidence="0.356654">
1017
</page>
<note confidence="0.967131">
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1017–1026, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics
</note>
<subsectionHeader confidence="0.946743">
1.1 Terminology and Notation
</subsectionHeader>
<bodyText confidence="0.99986828">
In this study, we use a simplified KB consisting of a
set C of concepts and a set R of labels. Each label r
denotes some binary relation partially represented in
the KB. The concrete KB is a directed, edge-labeled
graph G = (C, T) where T C_ C x R x C is the
set of labeled edges (also known as triples) (c, r, c0).
Each triple represents an instance r(c, c0) of the
relation r E R. The KB may be incomplete, that
is, r(c, c0) holds in the real world but (c, r, c0) E� T.
Our method will attempt to learn rules to infer such
missing relation instances by combining the KB
with parsed text.
We denote by r−1 the inverse relation of r:
r(c, c0) &lt;---&gt; r−1(c0, c). For instance Parent−1 is
equivalent to Children. It is convenient to take G
as containing triple (c0, r−1, c) whenever it contains
triple (c, r, c0).
A path type in G is a sequence 7r = (r1, ... , rm).
An instance of the path type is a sequence of nodes
c0, ... , cm such that ri(ci−1, ci). For instance, “the
persons who were born in the same town as the
query person”, and “the nationalities of persons who
were born in the same town as the query person” can
be reached respectively through paths matching the
following types
</bodyText>
<equation confidence="0.877474">
7r1 : BornIn, BornIn−1~
72 : BornIn, BornIn−1, Nationality)
</equation>
<subsectionHeader confidence="0.945472">
1.2 Learning Syntactic-Semantic Rules with
Path-Constrained Random Walks
</subsectionHeader>
<bodyText confidence="0.998170533333333">
Given a query concept s E C and a relation
r E R, PRA begins by enumerating a large set of
bounded-length path types. These path types are
treated as ranking “experts,” each generating some
random instance of the path type starting from s, and
ranking end nodes t by their weights in the resulting
distribution. Finally, PRA combines the weights
contributed by different “experts” by using logistic
regression to predict the probability that the relation
r(s, t) holds.
In this study, we test the hypothesis that PRA can
be used to find useful “syntactic-semantic patterns”
– that is, patterns that exploit both semantic
and syntactic relationships, thereby using semantic
knowledge as background in interpreting syntactic
</bodyText>
<subsectionHeader confidence="0.490745">
News Corpus
</subsectionHeader>
<figureCaption confidence="0.9049645">
Figure 1: Knowledge base and parsed text as a labeled
graph. For clarity, some word nodes are omitted.
</figureCaption>
<bodyText confidence="0.999928629629629">
relationships. As shown in Figure 1, we extend the
KB graph G with nodes and edges from text that
has been syntactically analyzed with a dependency
parser1 and where pronouns and other anaphoric
referring expressions have been clustered with their
antecedents. The text nodes are word/phrase
instances, and the edges are syntactic dependencies
labeled by the corresponding dependency type.
Mentions of entities in the text are linked to KB
concepts by mention edges created by an entity
resolution process.
Given for instance the query
Profession(CharlotteBronte, ?), PRA produces
a ranked list of answers that may have the relation
Profession with the query node CharlotteBronte.
The features used to score answers are the
random walk probabilities of reaching a certain
profession node from the query node by paths
with particular path types. PRA can learn path
types that combine background knowledge in
the database with syntactic patterns in the text
corpus. We now exemplify some path types
involving relations described in Table 3. Type
�M, conj, M−1, Profession) is active (matches
paths) for professions of persons who are mentioned
in conjunction with the query person as in
“collaboration between McDougall and Simon
</bodyText>
<figure confidence="0.989858481481482">
1Stanford dependencies (de Marneffe and Manning, 2008).
Freebase
Writer
Charlotte
Bronte
Write
HasFather
Jane Eyre
Patrick Brontë
Entity
Resolution
Mention
Mention
Mention
was
wrote
nsubj
nsubj
dobj
Charlotte
She
Jane Eyre
Dependency Trees
Coreference Resolution
?
Profession
1018
</figure>
<bodyText confidence="0.9803224375">
Philips”. For a somewhat subtler example, type
(M, TW, CW−1, Profession−1, Profession) is active
for persons who are mentioned by their titles as in
“President Barack Obama”. The type subsequence
(Profession−1, Profession) ensures that only
profession concepts are activated. The features
generated from these path types combine syntactic
dependency relations (conj) and textual information
relations (TW and CW) with semantic relations in
the KB (Profession).
Experiments on three Freebase relations (profes-
sion, nationality and parents) show that exploiting
existing background knowledge as path features
can significantly improve the quality of extraction
compared with using either Freebase or the text
corpus alone.
</bodyText>
<sectionHeader confidence="0.5791" genericHeader="related work">
1.3 Related Work
</sectionHeader>
<bodyText confidence="0.999969716666667">
Information extraction from varied unstructured and
structured sources involves both complex relational
structure and uncertainty at all levels of the extrac-
tion process. Statistical Relational Learning (SRL)
seeks to combine statistical and relational learning
methods to address such tasks. However, most SRL
approaches (Friedman et al., 1999; Richardson and
Domingos, 2006) suffer the complexity of inference
and learning when applied to large scale problems.
Recently, Lao and Cohen (2010) introduced Path
Ranking algorithm, which is applicable to larger
scale problems such as literature recommendation
(Lao and Cohen, 2010) and inference on a large
knowledge base (Lao et al., 2011).
Much of the previous work on automatic relation
extraction was based on certain lexico-syntactic
patterns. Hearst (1992) first noticed that patterns
such as “NP and other NP” and “NP such as NP”
often imply hyponym relations (NP here refers to
a noun phrase). However, such approaches to
relation extraction are limited by the availability of
domain knowledge. Later systems for extracting
arbitrary relations from text mostly use shallow
surface text patterns (Etzioni et al., 2004; Agichtein
and Gravano, 2000; Ravichandran and Hovy, 2002).
The idea of using sequences of dependency edges
as features for relation extraction was explored by
Snow et al. (2005) and Suchanek et al. (2006). They
define features to be shortest paths on dependency
trees which connect pairs of NP candidates.
This study is most closely related to work of
Mintz et al. (2009), who also study the problem of
extending Freebase with extraction from parsed text.
As in our work, they use a logistic regression model
with path features. However, their approach does not
exploit existing knowledge in the KB. Furthermore,
their path patterns are used as binary-values features.
We show experimentally that fractional-valued
features generated by random walks provide much
higher accuracy than binary-valued ones.
Culotta et al. (2006)’s work is similar to our
approach in the sense of relation extraction by
discovering relational patterns. However while
they focus on identifying relation mentions in text
(microreading),this work attempts to infer new
tuples by gathering path evidence over the whole
corpus (macroreading). In addition, their work
involves a few thousand examples, while we aim for
Web-scale extraction.
Do and Roth (2010) use a KB (YAGO) to
aid the generation of features from free text.
However their method is designed specifically for
extracting hierarchical taxonomic structures, while
our algorithm can be used to discover relations for
general general graph-based KBs.
In this paper we extend the PRA algorithm along
two dimensions: combining syntactic and semantic
cues in text with existing knowledge in the KB;
and a distributed implementation of the learning and
inference algorithms that works at Web scale.
</bodyText>
<sectionHeader confidence="0.786723" genericHeader="method">
2 Path Ranking Algorithm
</sectionHeader>
<bodyText confidence="0.9991962">
We briefly review the Path Ranking algorithm
(PRA), described in more detail by Lao and Cohen
(2010). Each path type 7r = (r1, r2, ..., r`) specifies
a real-valued feature. For a given query-answer node
pair (s, t), the value of the feature 7r is P(s —* t; 7r),
the probability of reaching t from s by a random
walk that instantiates the type. More specifically,
suppose that the random walk has just reached vi by
traversing edges labeled r1, ... , ri with s=v0. Then
vi+1 is drawn at random from all nodes reachable
from vi by edges labeled ri+1. A path type 7r is
active for pair (s, t) if P(s —* t; 7r) &gt; 0.
Let B = {L, 7r1, ..., 7rnJ be the set of all path
types of length no greater than E that occur in
the graph together with the dummy type L, which
</bodyText>
<page confidence="0.496135">
1019
</page>
<bodyText confidence="0.98357125">
represents the bias feature. For convenience, we set
P(s —* t;1) = 1 for any nodes s, t. The score for
whether query node s is related to another node t by
relation r is given by
</bodyText>
<equation confidence="0.982106">
�score(s,t) = P(s —* t; 7r)BI ,
IEB
</equation>
<bodyText confidence="0.999935076923077">
where 0I is the weight of feature 7r. The model
parameters to be learned are the vector θ =
(0I)IEB. The procedures used to discover B and
estimate θ are described in the following. Finally,
note that we train a separate PRA model for each
relation r.
Path Discovery: Given a graph and a target
relation r, the total number of path types is an
exponential function of the maximum path length
E and considering all possible paths would be
computationally very expensive. As a result, B is
constructed using only path types that satisfy the
following two constraints:
</bodyText>
<listItem confidence="0.6390454">
1. the path type is active for more than K training
query nodes, and
2. the probability of reaching any correct target
node t is larger than a threshold α on average
for the training query nodes s.
</listItem>
<bodyText confidence="0.99900247368421">
We will discuss how K, α and the training queries
are chosen in Section 5. In addition to making the
training more efficient, these constraints are also
helpful in removing low quality path types.
Training Examples: For each relation r of inter-
est, we start with a set of node pairs 5r = {(si, ti)}.
From 5r, we create the training set Dr = {(xi, yi)},
where xi = (P(si —* ti;7r))IEB is the vector
of path feature values for the pair (si, ti), and yi
indicates whether r(si, ti) holds.
Following previous work (Lao and Cohen, 2010;
Mintz et al., 2009), node pairs that are in r in
the KB are legitimate positive training examples2.
One can generate negative training examples by
considering all possible pairs of concepts whose
type is compatible with r (as given by the schema)
and are not present in the KB. However this
2In our experiments we subsample the positive examples.
See section 3.2 for more details.
procedure leads to a very large number of negative
examples (e.g., for the parents relation, any pair of
person concepts which are related by this relation
would be valid negative examples) which not only
makes training very expensive but also introduces
an incorrect bias in the training set. Following
Lao and Cohen (2010) we use a simple biased
sampling procedure to generate negative examples:
first, the path types discovered in the previous (path
discovery) step are used to construct an initial PRA
model (all feature weights are set to 1.0); then, for
each query node si, this model is used to retrieve
candidate answer nodes, which are then sorted in
descending order by their scores; finally, nodes at
the k(k + 1)/2-th positions are selected as negative
samples, where k = 0, 1, 2,....
Logistic Regression Training: Given a training
set D, we estimate parameters θ by maximizing the
following objective
</bodyText>
<equation confidence="0.99619475">
1 F(θ) = |D |f( 10 1
x, y; θ) − A111θ111 − A2112
2
(x,y)ED
</equation>
<bodyText confidence="0.9995074">
where A1 and A2 control the strength of the L1-
regularization which helps with structure selection
and L22-regularization which prevents overfitting.
The log-likelihood f(x, y; θ) of example (x, y) is
given by
</bodyText>
<equation confidence="0.9951018">
f(x, y, θ) = y lnp(x, θ) + (1 − y) ln(1 − p(x, θ))
p()
x θ _ exp(θ xT
1 + exp(B x
) .
</equation>
<bodyText confidence="0.9994218">
Inference: After a model is trained for a relation
r in the knowledge base, it can be used to produce
new instances of r. We first generate unlabeled
queries s which belong to the domain of r. Queries
which appear in the training set are excluded. For
each unlabeled query node s, we apply the trained
PRA model to generate a list of candidate t nodes
together with their scores. We then sort all the
predictions (s, t) by their scores in descending order,
and evaluate the top ones.
</bodyText>
<sectionHeader confidence="0.994449" genericHeader="method">
3 Extending PRA
</sectionHeader>
<bodyText confidence="0.970990789473684">
As described in the previous section, the PRA model
is trained on positive and negative queries generated
from the KB. As Freebase contains millions of
1020
concepts and edges, training on all the generated
queries is computationally challenging. Further,
we extend the Freebase graph with parse paths of
mentions of concepts in Freebase in millions of Web
pages. Yet another issue is that the training queries
generated using Freebase are inherently biased
towards the distribution of concepts in Freebase
and may not reflect the distribution of mentions of
these concepts in text data. As one of the goals of
our approach is to learn relation instances that are
missing in Freebase, training on such a set biased
towards the distribution of concepts in Freebase may
not lead to good performance. In this section we
explain how we modified the PRA algorithm to
address those issues.
</bodyText>
<subsectionHeader confidence="0.999618">
3.1 Scaling Up
</subsectionHeader>
<bodyText confidence="0.999977186046512">
Most relations in Freebase have a large set of
existing triples. For example, for the profession
relation, there are around 2 million persons in
Freebase, and about 0.3 million of them have known
professions. This results in more than 0.3 million
training queries (persons), each with one or more
positive answers (professions), and many negative
answers, which make training computationally
challenging. Generating all the paths for millions
of queries over a graph with millions of concepts
and edges further complicates the computational
issues. Incorporating the parse path features from
the text only exacerbates the matter. Finally once we
have trained a PRA model for a given relation, say
profession, we would like to infer the professions for
all the 1.7 million persons whose professions are not
known to Freebase (and possibly predict changes to
the profession information of the 0.3 million people
whose professions were given).
We use distributed computing to deal with the
large number of training and prediction queries
over a large graph. A key observation is that the
different stages of the PRA algorithm are based
on independent computations involving individual
queries. Therefore, we can use the MapReduce
framework to distribute the computation (Dean and
Ghemawat, 2008). For path discovery, we modify
Lao et al.’s path finding (2011) approach to decouple
the queries: instead of using one depth-first search
that involves all the queries, we first find all paths
up to certain length for each query node in the
map stage, and then collect statistics for each path
from all the query nodes in the reduce stage. We
used a 500-machine, 8GB/machine cluster for these
computations.
Another challenge associated with applying PRA
to a graph constructed using a large amounts of
text is that we cannot load the entire graph on a
single machine. To circumvent this problem, we first
index all parsed sentences by the concepts that they
mention. Therefore, to perform a random walk for a
query concept s, we only load the sentences which
mention s.
</bodyText>
<subsectionHeader confidence="0.999927">
3.2 Sampling Training Data
</subsectionHeader>
<bodyText confidence="0.99999275">
Using the r-edges in the KB as positive examples
distorts the training set. For example, for the
profession relation, there are 0.3 million persons
for whom Freebase has profession information, and
amongst these 0.24 million are either politicians
or actors. This may not reflect the distribution
of professions of persons mentioned in Web data.
Using all of these as training queries will most
certainly bias the trained model towards these
professions as PRA is trained discriminatively. In
other words, training directly with this data would
lead to a model that is more likely to predict
professions that are popular in Freebase. To avoid
this distortion, we use stratified sampling. For each
relation r and concept t E C, we count the number
of r edges pointing to t
</bodyText>
<equation confidence="0.706149">
Nr,t = |{(s,r,t) E T} |.
</equation>
<bodyText confidence="0.985336">
Given a training query (s, r, t) we sample it
according to
</bodyText>
<equation confidence="0.548553">
Pr ,t = min 1, V/m + Nr,t
Nr,t
</equation>
<bodyText confidence="0.999912166666667">
We fix m = 100 in our experiments. If we take the
profession relation as an example, the above implies
that for popular professions, we only sample about
VINr,t out of the Nr,t possible queries that end in t,
whereas for the less popular professions we would
accept all the training queries.
</bodyText>
<subsectionHeader confidence="0.999108">
3.3 Text Graph Construction
</subsectionHeader>
<bodyText confidence="0.9999615">
As we are processing Web text data (see following
section for more detail), the number of mentions
</bodyText>
<page confidence="0.569154">
1021
</page>
<bodyText confidence="0.999956555555556">
of a concept follows a somewhat heavy-tailed
distribution: there are a small number of very
popular concepts (head) and a large number of not
so popular concepts (tail). For instance the concept
BarackObama is mentioned about 8.9 million times
in our text corpus. To prevent the text graph from
being dominated by the head concepts, for each
sentence that mentions concept c ∈ C, we accept
it as part of the text graph with probability:
</bodyText>
<equation confidence="0.934857">
√k + Sc
, 1
&apos;_ )
</equation>
<bodyText confidence="0.994341666666667">
where Sc is the number of sentences in which c is
mentioned in the whole corpus. In our experiments
we use k = 105. This means that if Sc » k, then we
only sample about √Sc of the sentences that contain
a mention of the concept, while if Sc « k, then all
mentions of that concept will likely be included.
</bodyText>
<tableCaption confidence="0.6654785">
Table 1: Size of training and test sets for each relation.
Task Training Set Test Set
</tableCaption>
<table confidence="0.510395333333333">
Profession 22,829 15,219
Nationality 14,431 9,620
Parents 21,232 14,155
</table>
<bodyText confidence="0.999571166666667">
the mention. If a cluster has multiple possible
matching Freebase concepts, we choose a single
sense based on the following simple model. For
each Freebase concept c ∈ C, we compute N(c, m),
the number of times the concept c is referred by
mention m by using both the alias information
in Freebase and the anchors of the corresponding
Wikipedia page for that concept. Based on N(c, m)
we can calculate the empirical probability p(c|m) =
N(c, m)/ Ec� N(c&apos;, m). If u is a cluster with
mention set M(u) in the document, and C(m) the
set of concepts in KB with name or alias m, we
</bodyText>
<equation confidence="0.980515333333333">
(Pc = min 1
4 Datasets assign u to concept c* = argmax p(c|m),
cEC(m),mEM(u)
</equation>
<bodyText confidence="0.99995044">
We use Freebase as our knowledge base. Freebase
data is harvested from many sources, including
Wikipedia, AMG, and IMDB.3 As of this writing,
it contains more than 21 million concepts and 70
million labeled edges. For a large majority of con-
cepts that appear both in Freebase and Wikipedia,
Freebase maintains a link to the Wikipedia page of
that concept.
We also collect a large Web corpus and identify
60 million pages that mention concepts relevant
to this study. The free text on those pages
are POS-tagged and dependency parsed with an
accuracy comparable to that of the current Stanford
dependency parser (Klein and Manning, 2003). The
parser produces a dependency tree for each sentence
with each edge labeled with a standard dependency
tag (see Figure 1).
In each of the parsed documents, we use POS tags
and dependency edges to identify potential referring
noun phrases (NPs). We then use a within-document
coreference resolver comparable to that of Haghighi
and Klein (2009) to group referring NPs into
co-referring clusters. For each cluster that contains a
proper-name mention, we find the Freebase concept
or concepts, if any, with a name or alias that matches
</bodyText>
<equation confidence="0.461986">
3www.wikipedia.org,www.allmusic.com,www.
imdb.com.
</equation>
<bodyText confidence="0.999930666666667">
provided that there exists at least one c ∈ C(m) and
m ∈ M(u) such that p(c|m) &gt; 0. Note that M(c)
only contains the proper-name mentions in cluster c.
</bodyText>
<sectionHeader confidence="0.99982" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.999995454545455">
We use three relations profession, nationality and
parents for our experiments. For each relation, we
select its current set of triples in Freebase, and apply
the stratified sampling (Section 3.2) to each of the
three triple sets. The resulting triple sets are then
randomly split into training (60% of the triples) and
test (the remaining triples). However, the parents
relation yields 350k triples after stratified sampling,
so to reduce experimental effort we further randomly
sub-sample 10% of that as input to the train-test
split. Table 1 shows the sizes of the training and
test sets for each relation.
To encourage PRA to find paths involving the
text corpus, we do not count relation M (which
connects concepts to their mentions) or M−1 when
calculating path lengths. We use L1/L22-regularized
logistic regression to learn feature weights. The
PRA hyperparameters (α and K as defined in
Section 2) and regularizer hyperparameters are
tuned by threefold cross validation (CV) on the
training set. We average the models across all
the folds and choose the model that gives the best
</bodyText>
<page confidence="0.908497">
1022
</page>
<tableCaption confidence="0.8193648">
Table 2: Mean Reciprocal Rank (MRR) for different approaches under closed-world assumption. Here KB, Text and
KB+Text columns represent results obtained by training a PRA model with only the KB, only text, and both KB and
text. KB+Text[b] is the binarized PRA approach trained on both KB and text. The best performing system (results
shown in bold font) is significant at 0.0001 level over its nearest competitor according to a difference of proportions
significance test.
</tableCaption>
<table confidence="0.9879855">
Task KB Text KB+Text KB+Text[b]
Profession 0.532 0.516 0.583 0.453
Nationality 0.734 0.729 0.812 0.693
Parents 0.329 0.332 0.392 0.319
</table>
<bodyText confidence="0.995224714285714">
performance on the training set for each relation.
We report results of two evaluations. First, we
evaluate the performance of the PRA algorithm
when trained on a subset of existing Freebase facts
and tested on the rest. Second, we had human
annotators verify facts proposed by PRA that are not
in Freebase.
</bodyText>
<subsectionHeader confidence="0.998635">
5.1 Evaluation with Existing Knowledge
</subsectionHeader>
<bodyText confidence="0.999975428571429">
Previous work in relation extraction from parsed
text (Mintz et al., 2009) has mostly used binary
features to indicate whether a pattern is present in
the sentences where two concepts are mentioned.
To investigate the benefit of having fractional valued
features generated by random walks (as in PRA), we
also evaluate a binarized PRA approach, for which
we use the same syntactic-semantic pattern features
as PRA does, but binarize the feature values from
PRA: if the original fractional feature value was
zero, the feature value is set to zero (equivalent to
not having the feature in that example), otherwise it
is set to 1.
Table 2 shows a comparison of the results
obtained using the PRA algorithm trained using
only Freebase (KB), using only the text corpus
graph (Text), trained with both Freebase and the
text corpus (KB+Text) and the binarized PRA
algorithm using both Freebase and the text corpus
(KB+Text[b]). We report Mean Reciprocal Rank
(MRR) where, given a set of queries Q,
</bodyText>
<equation confidence="0.990517">
MRR = IQI qEQ
1
</equation>
<bodyText confidence="0.993758952380952">
rank of q’s first correct answer
Comparing the results of first three columns we
see that combining Freebase and text achieves
significantly better results than using either Freebase
or text alone. Further comparing the results of last
two columns we also observe a significant drop in
MRR for the binarized version of PRA. This clearly
shows the importance of using the random walk
probabilities. It can also be seen that the MRR for
the parents relation is lower than those for other
relations. This is mainly because there are larger
number of potential answers for each query node of
Parent relation than for each query node of the other
two relations – all persons in Freebase versus all
professions or nationalities. Finally, it is important
to point out that our evaluations are actually lower
bounds of actual performance, because, for instance,
a person might have a profession besides the ones in
Freebase and in such cases, this evaluation does not
give any credit for predicting those professions —
they are treated as errors. We try to address this issue
with the manual evaluations in the next section.
Table 2 only reports results for the maximum path
length E = 4 case. We found that shorter maximum
path lengths give worse results: for instance, with
E = 3 for the profession relation, MRR drops to
0.542, from 0.583 for E = 4 when using both
Freebase and text. This difference is significant
at the 0.0001 level according to a difference of
proportions test. Further we find that using longer
path length takes much longer time to train and test,
but does not lead to significant improvements over
the E = 4 case. For example, for profession, E = 5
gives a MRR of 0.589.
Table 3 shows the top weighted features that
involve text edges for PRA models trained on both
Freebase and the text corpus. To make them
easier to understand, we group them based on their
functionality. For the profession and nationality
tasks, the conjunction dependency relation (in group
1,4) plays an important role: these features first find
persons mentioned in conjunction with the query
</bodyText>
<table confidence="0.617663333333333">
1
.
1023
</table>
<tableCaption confidence="0.659348">
Table 3: Top weighted path types involving text edges for each task grouped according to functionality. M relations
connect each concept in knowledge base to its mentions in the corpus. TW relations connect each token in a sentence to
the words in the text representation of this token. CW relations connect each concept in knowledge base to the words
in the text representation of this concept. We use lower case names to denote dependency edges, word capitalized
names to denote KB edges, and “−1 ” to denote the inverse of a relation.
</tableCaption>
<table confidence="0.708622125">
Profession Top Weighted Features Comments
(M, conj, M−1, Profession)
1 Professions of persons mentioned in conjunction
(M, conj−1, M−1, Profession) with the query person: “McDougall and Simon
Phillips collaborated ...”
(M, TW, CW−1, Profession−1, Profession)
2 Active if a person is mentioned by his profession:
“The president said ...”
</table>
<equation confidence="0.755549">
(M, TW, TW−1, M−1, Children, Profession)
3
1, M− 1, Parents, Profession) First find persons with similar names or
(M, TW, TW−
</equation>
<bodyText confidence="0.8230915">
mentioned in similar ways, then aggregate the
(M, TW, TW−1, M−1,Advisors, Profession)
professions of their children/parents/advisors:
starting from the concept BarackObama, words
such as “Obama”, “leader”, “president”, and
“he” are reachable through path (M, TW)
</bodyText>
<table confidence="0.697830829268293">
Nationality Top Weighted Features Comments
The nationali
(M, conj, TW, CW−1,Nationality)
4
ties of persons mentioned in
M, conj−1, TW, CW−1,Nationality) conjunction with the query person: “McDougall
collaborated ...”
The nationali
(M, nc−1, TW, CW−1,Nationality)
5
ties of persons mentioned close to
(M, the query person through other dependency
TW, relations.
tmod−1,
CW−1,Nationality)
M, nn, TW, CW−1,Nationality)
M, poss,poss−1, M−1, PlaceOfBirth, ContainedBy) The birth/death places of the query person with
M, title, title−1, M−1, PlaceOfDeath, ContainedBy) restrictions to different syntactic constructions.
The parents of persons with similar names or
mentioned in similar ways: starting from the
concept CharlotteBronte words such as
an
(M, TW, CW−1,Parents)
7
“Bronte”,“Charlotte”,“Patrick’’,
d “she” are
reachable through path (M, TW).
(M, nsubj, (
CW, and Simon Phillips
nc, TW, (
(M, 6 (
(M, TW, (
nsubj−1,M−1, Parents Top Weighted Features Comments
CW−1) (M, nsubj, nsubj−1, TW, CW−1)
nc−1, 8 Persons with similar names or mentioned in
CW−1) (M,
TW,CW−1) −1
TW−1, TW, CW−1) (nc−1,
(TW−1,
ds further by word similarities.
1024
</table>
<bodyText confidence="0.993920472222222">
similar ways to the query person with various
restrictions or expansions. nsubj, nsubj
) and
nc) require the query to be subject and
noun compound respectively.
TW)
expan
person, and then find their professions or nation-
alities. The features in group 2 capture the fact
that sometimes people are mentioned by their pro-
fessions. The subpath (Profession−1, Profession)
ensures that only profession related concepts are
activated. Features in group 3 first find persons
with similar names or mentioned in similar ways
to the query person, and then aggregate the
professions of their children, parents, or advisors.
Features in group 6 can be seen as special
versions of feature (PlaceOfBirth, ContainedBy)
and (PlaceOfDeath, ContainedBy). The subpaths
(M, poss, poss−1, M−1) and (M, title, title−1, M−1)
return the random walks back to the query node only
if the mentions of the query node have poss (stands
for possessive modifier, e.g. “Bill’s clothes”) or title
(stands for person’s title, e.g. “President Obama”)
edges in text; otherwise these features are inactive.
Therefore, these features are active only for specific
subsets of queries. Features in group 8 generally find
persons with similar names or mentioned in similar
ways to the query person. However, they further
expand or restrict this person set in various ways.
Typically, each trained model includes hundreds
of paths with non-zero weights, so the bulk of
classifications are not based on a few high-precision-
recall patterns, but rather on the combination of
a large number of lower-precision high-recall or
high-precision lower-recall rules.
</bodyText>
<subsectionHeader confidence="0.996578">
5.2 Manual Evaluation
</subsectionHeader>
<bodyText confidence="0.999968352941177">
We performed two sets of manual evaluations. In
each case, an annotator is presented with the triples
predicted by PRA, and asked if they are correct. The
annotator has access to the Freebase and Wikipedia
pages for the concepts (and is able to issue search
queries about the concepts).
In the first evaluation, we compared the perfor-
mance of two PRA models, one trained using the
stratified sampled queries and another trained using
a randomly sampled set of queries for the profession
relation. For each model, we randomly sample 100
predictions from the top 1000 predictions (sorted by
the scores returned by the model). We found that the
PRA model trained with stratified sampled queries
has 0.92 precision, while the other model has only
0.84 precision (significant at the 0.02 level). This
shows that stratified sampling leads to improved
</bodyText>
<tableCaption confidence="0.998907">
Table 4: Human judgement for predicted new beliefs.
</tableCaption>
<table confidence="0.998821">
Task p@100 p@1k p@10k
Profession 0.97 0.92 0.84
Nationality 0.98 0.97 0.90
Parents 0.86 0.81 0.79
</table>
<bodyText confidence="0.978835315789474">
performance.
We also evaluated the new beliefs proposed by
the models trained for all the three relations using
stratified sampled queries. We estimated precision
for the top 100 predictions and randomly sampled
100 predictions each from the top 1,000 and 10,000
predictions. Here we use the PRA model trained
using both KB and text. The results of this
evaluation are shown in Table 4. It can be seen
that the PRA model is able to produce very high
precision predications even when one considers the
top 10,000 predictions.
Finally, note that our model is inductive. For
instance, for the profession relation, we are able to
predict professions for the around 2 million persons
in Freebase. The top 1000 profession facts extracted
by our system involve 970 distinct people, the top
10,000 facts involve 8,726 distinct people, and the
top 100,000 facts involve 79,885 people.
</bodyText>
<sectionHeader confidence="0.99821" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999976888888889">
We have shown that path constrained random walk
models can effectively infer new beliefs from a
large scale parsed text corpus with background
knowledge. Evaluation by human annotators shows
that by combining syntactic patterns in parsed
text with semantic patterns in the background
knowledge, our model can propose new beliefs
with high accuracy. Thus, the proposed random
walk model can be an effective way to automate
knowledge acquisition from the web.
There are several interesting directions to con-
tinue this line of work. First, bidirectional search
from both query and target nodes can be an efficient
way to discover long paths. This would especially
useful for parsed text. Second, relation paths that
contain constant nodes (lexicalized features) and
conjunction of random walk features are potentially
very useful for extraction tasks.
</bodyText>
<page confidence="0.68367">
1025
</page>
<sectionHeader confidence="0.998223" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9992055">
We thank Rahul Gupta, Michael Ringgaard, John
Blitzer and the anonymous reviewers for helpful
comments. The first author was supported by a
Google Research grant.
</bodyText>
<sectionHeader confidence="0.993969" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999006846938776">
Eugene Agichtein and Luis Gravano. 2000. Snowball:
extracting relations from large plain-text collections.
In Proceedings of the fifth ACM conference on Digital
libraries, DL ’00, pages 85–94, New York, NY, USA.
ACM.
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: a
collaboratively created graph database for structuring
human knowledge. In Proceedings of the 2008 ACM
SIGMOD international conference on Management of
data, SIGMOD ’08, pages 1247–1250, New York, NY,
USA. ACM.
Aron Culotta, Andrew McCallum, and Jonathan Betz.
2006. Integrating probabilistic extraction models and
data mining to discover relations and patterns in text.
In Proceedings of the Human Language Technology
Conference of the NAACL, Main Conference, pages
296–303, New York City, USA, June. Association for
Computational Linguistics.
Marie-Catherine de Marneffe and Chris Manning.
2008. Stanford dependencies. http:
//www.tex.ac.uk/cgi-bin/texfaq2html?
label=citeURL.
Jeffrey Dean and Sanjay Ghemawat. 2008. Mapreduce:
simplified data processing on large clusters. Commun.
ACM, 51(1):107–113, January.
Quang Do and Dan Roth. 2010. Constraints based
taxonomic relation classification. In Proceedings of
the 2010 Conference on Empirical Methods in Natural
Language Processing, pages 1099–1109, Cambridge,
MA, October. Association for Computational Linguis-
tics.
Oren Etzioni, Michael Cafarella, Doug Downey, Stanley
Kok, Ana-Maria Popescu, Tal Shaked, Stephen
Soderland, Daniel S. Weld, and Alexander Yates.
2004. Web-scale information extraction in knowitall:
(preliminary results). In Proceedings of the 13th
international conference on World Wide Web, WWW
’04, pages 100–110, New York, NY, USA. ACM.
Nir Friedman, Lise Getoor, Daphne Koller, and Avi
Pfeffer. 1999. Learning Probabilistic Relational
Models. In IJCAI, volume 16, pages 1300–1309.
Aria Haghighi and Dan Klein. 2009. Simple coref-
erence resolution with rich syntactic and semantic
features. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing,
pages 1152–1161, Singapore, August. Association for
Computational Linguistics.
Marti A. Hearst. 1992. Automatic acquisition of
hyponyms from large text corpora. In Proceedings
of COLING-92, pages 539–545. Association for
Computational Linguistics, August.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Erhard Hinrichs and Dan
Roth, editors, Proceedings of the 41st Annual Meeting
on Association for Computational Linguistics, pages
423–430. Association for Computational Linguistics,
July.
Ni Lao and William Cohen. 2010. Relational retrieval
using a combination of path-constrained random
walks. Machine Learning, 81:53–67.
Ni Lao, Tom Mitchell, and William W. Cohen. 2011.
Random walk inference and learning in a large
scale knowledge base. In Proceedings of the
2011 Conference on Empirical Methods in Natural
Language Processing, pages 529–539, Edinburgh,
Scotland, UK., July. Association for Computational
Linguistics.
Mike Mintz, Steven Bills, Rion Snow, and Daniel
Jurafsky. 2009. Distant supervision for relation
extraction without labeled data. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP, pages
1003–1011, Suntec, Singapore, August. Association
for Computational Linguistics.
Deepak Ravichandran and Eduard Hovy. 2002.
Learning surface text patterns for a question answering
system. In Proceedings of 40th Annual Meeting
of the Association for Computational Linguistics,
pages 41–47, Philadelphia, Pennsylvania, USA, July.
Association for Computational Linguistics.
Matthew Richardson and Pedro Domingos. 2006.
Markov logic networks. Machine Learning, 62:107–
136.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2005.
Learning syntactic patterns for automatic hypernym
discovery. In Lawrence K. Saul, Yair Weiss, and
L´eon Bottou, editors, Advances in Neural Information
Processing Systems 17, pages 1297–1304, Cambridge,
MA. NIPS Foundation, MIT Press.
Fabian M. Suchanek, Georgiana Ifrim, and Gerhard
Weikum. 2006. Combining linguistic and statistical
analysis to extract relations from web documents. In
Proceedings of the 12th ACM SIGKDD international
conference on Knowledge discovery and data mining,
KDD ’06, pages 712–717, New York, NY, USA.
ACM.
</reference>
<page confidence="0.786941">
1026
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.593689">
<title confidence="0.999066">Reading The Web with Learned Syntactic-Semantic Inference Rules</title>
<author confidence="0.990898">Fernando William W</author>
<affiliation confidence="0.602051">Mellon University, 5000 Forbes Avenue, Pittsburgh, PA 15213,</affiliation>
<address confidence="0.993258">Research, 1600 Amphitheatre Parkway, Mountain View, CA 94043,</address>
<email confidence="0.999744">wcohen@cs.cmu.edu</email>
<abstract confidence="0.999603777777778">We study how to extend a large knowledge base (Freebase) by reading relational information from a large Web text corpus. Previous studies on extracting relational knowledge from text show the potential of syntactic patterns for extraction, but they do not exploit background knowledge of other relations in the knowledge base. We describe a distributed, Web-scale implementation of a path-constrained random walk model that learns syntactic-semantic inference rules for binary relations from a graph representation of the parsed text and the knowledge base. Experiments show significant accuracy improvements in binary relation prediction over methods that consider only text, or only the existing knowledge base.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eugene Agichtein</author>
<author>Luis Gravano</author>
</authors>
<title>Snowball: extracting relations from large plain-text collections.</title>
<date>2000</date>
<booktitle>In Proceedings of the fifth ACM conference on Digital libraries, DL ’00,</booktitle>
<pages>85--94</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="9270" citStr="Agichtein and Gravano, 2000" startWordPosition="1442" endWordPosition="1445">such as literature recommendation (Lao and Cohen, 2010) and inference on a large knowledge base (Lao et al., 2011). Much of the previous work on automatic relation extraction was based on certain lexico-syntactic patterns. Hearst (1992) first noticed that patterns such as “NP and other NP” and “NP such as NP” often imply hyponym relations (NP here refers to a noun phrase). However, such approaches to relation extraction are limited by the availability of domain knowledge. Later systems for extracting arbitrary relations from text mostly use shallow surface text patterns (Etzioni et al., 2004; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002). The idea of using sequences of dependency edges as features for relation extraction was explored by Snow et al. (2005) and Suchanek et al. (2006). They define features to be shortest paths on dependency trees which connect pairs of NP candidates. This study is most closely related to work of Mintz et al. (2009), who also study the problem of extending Freebase with extraction from parsed text. As in our work, they use a logistic regression model with path features. However, their approach does not exploit existing knowledge in the KB. Furthermore, their path pat</context>
</contexts>
<marker>Agichtein, Gravano, 2000</marker>
<rawString>Eugene Agichtein and Luis Gravano. 2000. Snowball: extracting relations from large plain-text collections. In Proceedings of the fifth ACM conference on Digital libraries, DL ’00, pages 85–94, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kurt Bollacker</author>
<author>Colin Evans</author>
<author>Praveen Paritosh</author>
<author>Tim Sturge</author>
<author>Jamie Taylor</author>
</authors>
<title>Freebase: a collaboratively created graph database for structuring human knowledge.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 ACM SIGMOD international conference on Management of data, SIGMOD ’08,</booktitle>
<pages>1247--1250</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="1515" citStr="Bollacker et al., 2008" startWordPosition="215" endWordPosition="218"> the knowledge base. Experiments show significant accuracy improvements in binary relation prediction over methods that consider only text, or only the existing knowledge base. 1 Introduction Manually-created knowledge bases (KBs) often lack basic information about some entities and their relationships, either because the information was missing in the initial sources used to create the KB, or because human curators were not confident about the status of some putative fact, and so they excluded it from the KB. For instance, as we will see in more detail later, many person entries in Freebase (Bollacker et al., 2008) lack nationality information. To fill those KB gaps, we might use general rules, ideally automatically learned, such as “if person was born in town and town is in country *This research was carried out during an internship at Google Research then the person is a national of the country.” Of course, rules like this may be defeasible, in this case for example because of naturalization or political changes. Nevertheless, many such imperfect rules can be learned and combined to yield useful KB completions, as demonstrated in particular with the Path-Ranking Algorithm (PRA) (Lao and Cohen, 2010; L</context>
</contexts>
<marker>Bollacker, Evans, Paritosh, Sturge, Taylor, 2008</marker>
<rawString>Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. 2008. Freebase: a collaboratively created graph database for structuring human knowledge. In Proceedings of the 2008 ACM SIGMOD international conference on Management of data, SIGMOD ’08, pages 1247–1250, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aron Culotta</author>
<author>Andrew McCallum</author>
<author>Jonathan Betz</author>
</authors>
<title>Integrating probabilistic extraction models and data mining to discover relations and patterns in text.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference,</booktitle>
<pages>296--303</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>New York City, USA,</location>
<contexts>
<context position="10068" citStr="Culotta et al. (2006)" startWordPosition="1567" endWordPosition="1570">. They define features to be shortest paths on dependency trees which connect pairs of NP candidates. This study is most closely related to work of Mintz et al. (2009), who also study the problem of extending Freebase with extraction from parsed text. As in our work, they use a logistic regression model with path features. However, their approach does not exploit existing knowledge in the KB. Furthermore, their path patterns are used as binary-values features. We show experimentally that fractional-valued features generated by random walks provide much higher accuracy than binary-valued ones. Culotta et al. (2006)’s work is similar to our approach in the sense of relation extraction by discovering relational patterns. However while they focus on identifying relation mentions in text (microreading),this work attempts to infer new tuples by gathering path evidence over the whole corpus (macroreading). In addition, their work involves a few thousand examples, while we aim for Web-scale extraction. Do and Roth (2010) use a KB (YAGO) to aid the generation of features from free text. However their method is designed specifically for extracting hierarchical taxonomic structures, while our algorithm can be use</context>
</contexts>
<marker>Culotta, McCallum, Betz, 2006</marker>
<rawString>Aron Culotta, Andrew McCallum, and Jonathan Betz. 2006. Integrating probabilistic extraction models and data mining to discover relations and patterns in text. In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference, pages 296–303, New York City, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Chris Manning</author>
</authors>
<date>2008</date>
<note>Stanford dependencies. http: //www.tex.ac.uk/cgi-bin/texfaq2html? label=citeURL.</note>
<marker>de Marneffe, Manning, 2008</marker>
<rawString>Marie-Catherine de Marneffe and Chris Manning. 2008. Stanford dependencies. http: //www.tex.ac.uk/cgi-bin/texfaq2html? label=citeURL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey Dean</author>
<author>Sanjay Ghemawat</author>
</authors>
<title>Mapreduce: simplified data processing on large clusters.</title>
<date>2008</date>
<journal>Commun. ACM,</journal>
<volume>51</volume>
<issue>1</issue>
<contexts>
<context position="17614" citStr="Dean and Ghemawat, 2008" startWordPosition="2872" endWordPosition="2875">el for a given relation, say profession, we would like to infer the professions for all the 1.7 million persons whose professions are not known to Freebase (and possibly predict changes to the profession information of the 0.3 million people whose professions were given). We use distributed computing to deal with the large number of training and prediction queries over a large graph. A key observation is that the different stages of the PRA algorithm are based on independent computations involving individual queries. Therefore, we can use the MapReduce framework to distribute the computation (Dean and Ghemawat, 2008). For path discovery, we modify Lao et al.’s path finding (2011) approach to decouple the queries: instead of using one depth-first search that involves all the queries, we first find all paths up to certain length for each query node in the map stage, and then collect statistics for each path from all the query nodes in the reduce stage. We used a 500-machine, 8GB/machine cluster for these computations. Another challenge associated with applying PRA to a graph constructed using a large amounts of text is that we cannot load the entire graph on a single machine. To circumvent this problem, we </context>
</contexts>
<marker>Dean, Ghemawat, 2008</marker>
<rawString>Jeffrey Dean and Sanjay Ghemawat. 2008. Mapreduce: simplified data processing on large clusters. Commun. ACM, 51(1):107–113, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Quang Do</author>
<author>Dan Roth</author>
</authors>
<title>Constraints based taxonomic relation classification.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1099--1109</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Cambridge, MA,</location>
<contexts>
<context position="10475" citStr="Do and Roth (2010)" startWordPosition="1628" endWordPosition="1631">heir path patterns are used as binary-values features. We show experimentally that fractional-valued features generated by random walks provide much higher accuracy than binary-valued ones. Culotta et al. (2006)’s work is similar to our approach in the sense of relation extraction by discovering relational patterns. However while they focus on identifying relation mentions in text (microreading),this work attempts to infer new tuples by gathering path evidence over the whole corpus (macroreading). In addition, their work involves a few thousand examples, while we aim for Web-scale extraction. Do and Roth (2010) use a KB (YAGO) to aid the generation of features from free text. However their method is designed specifically for extracting hierarchical taxonomic structures, while our algorithm can be used to discover relations for general general graph-based KBs. In this paper we extend the PRA algorithm along two dimensions: combining syntactic and semantic cues in text with existing knowledge in the KB; and a distributed implementation of the learning and inference algorithms that works at Web scale. 2 Path Ranking Algorithm We briefly review the Path Ranking algorithm (PRA), described in more detail </context>
</contexts>
<marker>Do, Roth, 2010</marker>
<rawString>Quang Do and Dan Roth. 2010. Constraints based taxonomic relation classification. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1099–1109, Cambridge, MA, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oren Etzioni</author>
<author>Michael Cafarella</author>
<author>Doug Downey</author>
<author>Stanley Kok</author>
<author>Ana-Maria Popescu</author>
<author>Tal Shaked</author>
<author>Stephen Soderland</author>
<author>Daniel S Weld</author>
<author>Alexander Yates</author>
</authors>
<title>Web-scale information extraction in knowitall: (preliminary results).</title>
<date>2004</date>
<booktitle>In Proceedings of the 13th international conference on World Wide Web, WWW ’04,</booktitle>
<pages>100--110</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="9241" citStr="Etzioni et al., 2004" startWordPosition="1438" endWordPosition="1441">larger scale problems such as literature recommendation (Lao and Cohen, 2010) and inference on a large knowledge base (Lao et al., 2011). Much of the previous work on automatic relation extraction was based on certain lexico-syntactic patterns. Hearst (1992) first noticed that patterns such as “NP and other NP” and “NP such as NP” often imply hyponym relations (NP here refers to a noun phrase). However, such approaches to relation extraction are limited by the availability of domain knowledge. Later systems for extracting arbitrary relations from text mostly use shallow surface text patterns (Etzioni et al., 2004; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002). The idea of using sequences of dependency edges as features for relation extraction was explored by Snow et al. (2005) and Suchanek et al. (2006). They define features to be shortest paths on dependency trees which connect pairs of NP candidates. This study is most closely related to work of Mintz et al. (2009), who also study the problem of extending Freebase with extraction from parsed text. As in our work, they use a logistic regression model with path features. However, their approach does not exploit existing knowledge in the KB</context>
</contexts>
<marker>Etzioni, Cafarella, Downey, Kok, Popescu, Shaked, Soderland, Weld, Yates, 2004</marker>
<rawString>Oren Etzioni, Michael Cafarella, Doug Downey, Stanley Kok, Ana-Maria Popescu, Tal Shaked, Stephen Soderland, Daniel S. Weld, and Alexander Yates. 2004. Web-scale information extraction in knowitall: (preliminary results). In Proceedings of the 13th international conference on World Wide Web, WWW ’04, pages 100–110, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nir Friedman</author>
<author>Lise Getoor</author>
<author>Daphne Koller</author>
<author>Avi Pfeffer</author>
</authors>
<title>Learning Probabilistic Relational Models.</title>
<date>1999</date>
<booktitle>In IJCAI,</booktitle>
<volume>16</volume>
<pages>1300--1309</pages>
<contexts>
<context position="8413" citStr="Friedman et al., 1999" startWordPosition="1311" endWordPosition="1314">Experiments on three Freebase relations (profession, nationality and parents) show that exploiting existing background knowledge as path features can significantly improve the quality of extraction compared with using either Freebase or the text corpus alone. 1.3 Related Work Information extraction from varied unstructured and structured sources involves both complex relational structure and uncertainty at all levels of the extraction process. Statistical Relational Learning (SRL) seeks to combine statistical and relational learning methods to address such tasks. However, most SRL approaches (Friedman et al., 1999; Richardson and Domingos, 2006) suffer the complexity of inference and learning when applied to large scale problems. Recently, Lao and Cohen (2010) introduced Path Ranking algorithm, which is applicable to larger scale problems such as literature recommendation (Lao and Cohen, 2010) and inference on a large knowledge base (Lao et al., 2011). Much of the previous work on automatic relation extraction was based on certain lexico-syntactic patterns. Hearst (1992) first noticed that patterns such as “NP and other NP” and “NP such as NP” often imply hyponym relations (NP here refers to a noun phr</context>
</contexts>
<marker>Friedman, Getoor, Koller, Pfeffer, 1999</marker>
<rawString>Nir Friedman, Lise Getoor, Daphne Koller, and Avi Pfeffer. 1999. Learning Probabilistic Relational Models. In IJCAI, volume 16, pages 1300–1309.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Dan Klein</author>
</authors>
<title>Simple coreference resolution with rich syntactic and semantic features.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1152--1161</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="22278" citStr="Haghighi and Klein (2009)" startWordPosition="3690" endWordPosition="3693">also collect a large Web corpus and identify 60 million pages that mention concepts relevant to this study. The free text on those pages are POS-tagged and dependency parsed with an accuracy comparable to that of the current Stanford dependency parser (Klein and Manning, 2003). The parser produces a dependency tree for each sentence with each edge labeled with a standard dependency tag (see Figure 1). In each of the parsed documents, we use POS tags and dependency edges to identify potential referring noun phrases (NPs). We then use a within-document coreference resolver comparable to that of Haghighi and Klein (2009) to group referring NPs into co-referring clusters. For each cluster that contains a proper-name mention, we find the Freebase concept or concepts, if any, with a name or alias that matches 3www.wikipedia.org,www.allmusic.com,www. imdb.com. provided that there exists at least one c ∈ C(m) and m ∈ M(u) such that p(c|m) &gt; 0. Note that M(c) only contains the proper-name mentions in cluster c. 5 Results We use three relations profession, nationality and parents for our experiments. For each relation, we select its current set of triples in Freebase, and apply the stratified sampling (Section 3.2) </context>
</contexts>
<marker>Haghighi, Klein, 2009</marker>
<rawString>Aria Haghighi and Dan Klein. 2009. Simple coreference resolution with rich syntactic and semantic features. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1152–1161, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti A Hearst</author>
</authors>
<title>Automatic acquisition of hyponyms from large text corpora.</title>
<date>1992</date>
<booktitle>In Proceedings of COLING-92,</booktitle>
<pages>539--545</pages>
<contexts>
<context position="8879" citStr="Hearst (1992)" startWordPosition="1382" endWordPosition="1383">rning (SRL) seeks to combine statistical and relational learning methods to address such tasks. However, most SRL approaches (Friedman et al., 1999; Richardson and Domingos, 2006) suffer the complexity of inference and learning when applied to large scale problems. Recently, Lao and Cohen (2010) introduced Path Ranking algorithm, which is applicable to larger scale problems such as literature recommendation (Lao and Cohen, 2010) and inference on a large knowledge base (Lao et al., 2011). Much of the previous work on automatic relation extraction was based on certain lexico-syntactic patterns. Hearst (1992) first noticed that patterns such as “NP and other NP” and “NP such as NP” often imply hyponym relations (NP here refers to a noun phrase). However, such approaches to relation extraction are limited by the availability of domain knowledge. Later systems for extracting arbitrary relations from text mostly use shallow surface text patterns (Etzioni et al., 2004; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002). The idea of using sequences of dependency edges as features for relation extraction was explored by Snow et al. (2005) and Suchanek et al. (2006). They define features to be sho</context>
</contexts>
<marker>Hearst, 1992</marker>
<rawString>Marti A. Hearst. 1992. Automatic acquisition of hyponyms from large text corpora. In Proceedings of COLING-92, pages 539–545. Association for Computational Linguistics, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>Proceedings of the 41st Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>423--430</pages>
<editor>In Erhard Hinrichs and Dan Roth, editors,</editor>
<contexts>
<context position="21930" citStr="Klein and Manning, 2003" startWordPosition="3634" endWordPosition="3637">ase as our knowledge base. Freebase data is harvested from many sources, including Wikipedia, AMG, and IMDB.3 As of this writing, it contains more than 21 million concepts and 70 million labeled edges. For a large majority of concepts that appear both in Freebase and Wikipedia, Freebase maintains a link to the Wikipedia page of that concept. We also collect a large Web corpus and identify 60 million pages that mention concepts relevant to this study. The free text on those pages are POS-tagged and dependency parsed with an accuracy comparable to that of the current Stanford dependency parser (Klein and Manning, 2003). The parser produces a dependency tree for each sentence with each edge labeled with a standard dependency tag (see Figure 1). In each of the parsed documents, we use POS tags and dependency edges to identify potential referring noun phrases (NPs). We then use a within-document coreference resolver comparable to that of Haghighi and Klein (2009) to group referring NPs into co-referring clusters. For each cluster that contains a proper-name mention, we find the Freebase concept or concepts, if any, with a name or alias that matches 3www.wikipedia.org,www.allmusic.com,www. imdb.com. provided th</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Accurate unlexicalized parsing. In Erhard Hinrichs and Dan Roth, editors, Proceedings of the 41st Annual Meeting on Association for Computational Linguistics, pages 423–430. Association for Computational Linguistics, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ni Lao</author>
<author>William Cohen</author>
</authors>
<title>Relational retrieval using a combination of path-constrained random walks.</title>
<date>2010</date>
<booktitle>Machine Learning,</booktitle>
<pages>81--53</pages>
<contexts>
<context position="2112" citStr="Lao and Cohen, 2010" startWordPosition="311" endWordPosition="314">ollacker et al., 2008) lack nationality information. To fill those KB gaps, we might use general rules, ideally automatically learned, such as “if person was born in town and town is in country *This research was carried out during an internship at Google Research then the person is a national of the country.” Of course, rules like this may be defeasible, in this case for example because of naturalization or political changes. Nevertheless, many such imperfect rules can be learned and combined to yield useful KB completions, as demonstrated in particular with the Path-Ranking Algorithm (PRA) (Lao and Cohen, 2010; Lao et al., 2011), which learns such rules on heterogenous graphs for link prediction tasks. Alternatively, we may attempt to fill KB gaps by applying relation extraction rules to free text. For instance, Snow et al. (2005) and Suchanek et al. (2006) showed the value of syntactic patterns in extracting specific relations. In those approaches, KB tuples of the relation to be extracted serve as positive training examples to the extraction rule induction algorithm. However, the KB contains much more knowledge about other relations that could potentially be helpful in improving relation extracti</context>
<context position="8562" citStr="Lao and Cohen (2010)" startWordPosition="1333" endWordPosition="1336">significantly improve the quality of extraction compared with using either Freebase or the text corpus alone. 1.3 Related Work Information extraction from varied unstructured and structured sources involves both complex relational structure and uncertainty at all levels of the extraction process. Statistical Relational Learning (SRL) seeks to combine statistical and relational learning methods to address such tasks. However, most SRL approaches (Friedman et al., 1999; Richardson and Domingos, 2006) suffer the complexity of inference and learning when applied to large scale problems. Recently, Lao and Cohen (2010) introduced Path Ranking algorithm, which is applicable to larger scale problems such as literature recommendation (Lao and Cohen, 2010) and inference on a large knowledge base (Lao et al., 2011). Much of the previous work on automatic relation extraction was based on certain lexico-syntactic patterns. Hearst (1992) first noticed that patterns such as “NP and other NP” and “NP such as NP” often imply hyponym relations (NP here refers to a noun phrase). However, such approaches to relation extraction are limited by the availability of domain knowledge. Later systems for extracting arbitrary rel</context>
<context position="11098" citStr="Lao and Cohen (2010)" startWordPosition="1726" endWordPosition="1729">e a KB (YAGO) to aid the generation of features from free text. However their method is designed specifically for extracting hierarchical taxonomic structures, while our algorithm can be used to discover relations for general general graph-based KBs. In this paper we extend the PRA algorithm along two dimensions: combining syntactic and semantic cues in text with existing knowledge in the KB; and a distributed implementation of the learning and inference algorithms that works at Web scale. 2 Path Ranking Algorithm We briefly review the Path Ranking algorithm (PRA), described in more detail by Lao and Cohen (2010). Each path type 7r = (r1, r2, ..., r`) specifies a real-valued feature. For a given query-answer node pair (s, t), the value of the feature 7r is P(s —* t; 7r), the probability of reaching t from s by a random walk that instantiates the type. More specifically, suppose that the random walk has just reached vi by traversing edges labeled r1, ... , ri with s=v0. Then vi+1 is drawn at random from all nodes reachable from vi by edges labeled ri+1. A path type 7r is active for pair (s, t) if P(s —* t; 7r) &gt; 0. Let B = {L, 7r1, ..., 7rnJ be the set of all path types of length no greater than E that</context>
<context position="13270" citStr="Lao and Cohen, 2010" startWordPosition="2140" endWordPosition="2143">ct target node t is larger than a threshold α on average for the training query nodes s. We will discuss how K, α and the training queries are chosen in Section 5. In addition to making the training more efficient, these constraints are also helpful in removing low quality path types. Training Examples: For each relation r of interest, we start with a set of node pairs 5r = {(si, ti)}. From 5r, we create the training set Dr = {(xi, yi)}, where xi = (P(si —* ti;7r))IEB is the vector of path feature values for the pair (si, ti), and yi indicates whether r(si, ti) holds. Following previous work (Lao and Cohen, 2010; Mintz et al., 2009), node pairs that are in r in the KB are legitimate positive training examples2. One can generate negative training examples by considering all possible pairs of concepts whose type is compatible with r (as given by the schema) and are not present in the KB. However this 2In our experiments we subsample the positive examples. See section 3.2 for more details. procedure leads to a very large number of negative examples (e.g., for the parents relation, any pair of person concepts which are related by this relation would be valid negative examples) which not only makes traini</context>
</contexts>
<marker>Lao, Cohen, 2010</marker>
<rawString>Ni Lao and William Cohen. 2010. Relational retrieval using a combination of path-constrained random walks. Machine Learning, 81:53–67.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ni Lao</author>
<author>Tom Mitchell</author>
<author>William W Cohen</author>
</authors>
<title>Random walk inference and learning in a large scale knowledge base.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>529--539</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Edinburgh, Scotland, UK.,</location>
<contexts>
<context position="2131" citStr="Lao et al., 2011" startWordPosition="315" endWordPosition="318">) lack nationality information. To fill those KB gaps, we might use general rules, ideally automatically learned, such as “if person was born in town and town is in country *This research was carried out during an internship at Google Research then the person is a national of the country.” Of course, rules like this may be defeasible, in this case for example because of naturalization or political changes. Nevertheless, many such imperfect rules can be learned and combined to yield useful KB completions, as demonstrated in particular with the Path-Ranking Algorithm (PRA) (Lao and Cohen, 2010; Lao et al., 2011), which learns such rules on heterogenous graphs for link prediction tasks. Alternatively, we may attempt to fill KB gaps by applying relation extraction rules to free text. For instance, Snow et al. (2005) and Suchanek et al. (2006) showed the value of syntactic patterns in extracting specific relations. In those approaches, KB tuples of the relation to be extracted serve as positive training examples to the extraction rule induction algorithm. However, the KB contains much more knowledge about other relations that could potentially be helpful in improving relation extraction accuracy and cov</context>
<context position="8757" citStr="Lao et al., 2011" startWordPosition="1363" endWordPosition="1366">involves both complex relational structure and uncertainty at all levels of the extraction process. Statistical Relational Learning (SRL) seeks to combine statistical and relational learning methods to address such tasks. However, most SRL approaches (Friedman et al., 1999; Richardson and Domingos, 2006) suffer the complexity of inference and learning when applied to large scale problems. Recently, Lao and Cohen (2010) introduced Path Ranking algorithm, which is applicable to larger scale problems such as literature recommendation (Lao and Cohen, 2010) and inference on a large knowledge base (Lao et al., 2011). Much of the previous work on automatic relation extraction was based on certain lexico-syntactic patterns. Hearst (1992) first noticed that patterns such as “NP and other NP” and “NP such as NP” often imply hyponym relations (NP here refers to a noun phrase). However, such approaches to relation extraction are limited by the availability of domain knowledge. Later systems for extracting arbitrary relations from text mostly use shallow surface text patterns (Etzioni et al., 2004; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002). The idea of using sequences of dependency edges as feat</context>
</contexts>
<marker>Lao, Mitchell, Cohen, 2011</marker>
<rawString>Ni Lao, Tom Mitchell, and William W. Cohen. 2011. Random walk inference and learning in a large scale knowledge base. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 529–539, Edinburgh, Scotland, UK., July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mike Mintz</author>
<author>Steven Bills</author>
<author>Rion Snow</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Distant supervision for relation extraction without labeled data.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>1003--1011</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Suntec, Singapore,</location>
<contexts>
<context position="9614" citStr="Mintz et al. (2009)" startWordPosition="1500" endWordPosition="1503">fers to a noun phrase). However, such approaches to relation extraction are limited by the availability of domain knowledge. Later systems for extracting arbitrary relations from text mostly use shallow surface text patterns (Etzioni et al., 2004; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002). The idea of using sequences of dependency edges as features for relation extraction was explored by Snow et al. (2005) and Suchanek et al. (2006). They define features to be shortest paths on dependency trees which connect pairs of NP candidates. This study is most closely related to work of Mintz et al. (2009), who also study the problem of extending Freebase with extraction from parsed text. As in our work, they use a logistic regression model with path features. However, their approach does not exploit existing knowledge in the KB. Furthermore, their path patterns are used as binary-values features. We show experimentally that fractional-valued features generated by random walks provide much higher accuracy than binary-valued ones. Culotta et al. (2006)’s work is similar to our approach in the sense of relation extraction by discovering relational patterns. However while they focus on identifying</context>
<context position="13291" citStr="Mintz et al., 2009" startWordPosition="2144" endWordPosition="2147">arger than a threshold α on average for the training query nodes s. We will discuss how K, α and the training queries are chosen in Section 5. In addition to making the training more efficient, these constraints are also helpful in removing low quality path types. Training Examples: For each relation r of interest, we start with a set of node pairs 5r = {(si, ti)}. From 5r, we create the training set Dr = {(xi, yi)}, where xi = (P(si —* ti;7r))IEB is the vector of path feature values for the pair (si, ti), and yi indicates whether r(si, ti) holds. Following previous work (Lao and Cohen, 2010; Mintz et al., 2009), node pairs that are in r in the KB are legitimate positive training examples2. One can generate negative training examples by considering all possible pairs of concepts whose type is compatible with r (as given by the schema) and are not present in the KB. However this 2In our experiments we subsample the positive examples. See section 3.2 for more details. procedure leads to a very large number of negative examples (e.g., for the parents relation, any pair of person concepts which are related by this relation would be valid negative examples) which not only makes training very expensive but</context>
<context position="24804" citStr="Mintz et al., 2009" startWordPosition="4096" endWordPosition="4099">or according to a difference of proportions significance test. Task KB Text KB+Text KB+Text[b] Profession 0.532 0.516 0.583 0.453 Nationality 0.734 0.729 0.812 0.693 Parents 0.329 0.332 0.392 0.319 performance on the training set for each relation. We report results of two evaluations. First, we evaluate the performance of the PRA algorithm when trained on a subset of existing Freebase facts and tested on the rest. Second, we had human annotators verify facts proposed by PRA that are not in Freebase. 5.1 Evaluation with Existing Knowledge Previous work in relation extraction from parsed text (Mintz et al., 2009) has mostly used binary features to indicate whether a pattern is present in the sentences where two concepts are mentioned. To investigate the benefit of having fractional valued features generated by random walks (as in PRA), we also evaluate a binarized PRA approach, for which we use the same syntactic-semantic pattern features as PRA does, but binarize the feature values from PRA: if the original fractional feature value was zero, the feature value is set to zero (equivalent to not having the feature in that example), otherwise it is set to 1. Table 2 shows a comparison of the results obta</context>
</contexts>
<marker>Mintz, Bills, Snow, Jurafsky, 2009</marker>
<rawString>Mike Mintz, Steven Bills, Rion Snow, and Daniel Jurafsky. 2009. Distant supervision for relation extraction without labeled data. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 1003–1011, Suntec, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deepak Ravichandran</author>
<author>Eduard Hovy</author>
</authors>
<title>Learning surface text patterns for a question answering system.</title>
<date>2002</date>
<booktitle>In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>41--47</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Philadelphia, Pennsylvania, USA,</location>
<contexts>
<context position="9300" citStr="Ravichandran and Hovy, 2002" startWordPosition="1446" endWordPosition="1449">tion (Lao and Cohen, 2010) and inference on a large knowledge base (Lao et al., 2011). Much of the previous work on automatic relation extraction was based on certain lexico-syntactic patterns. Hearst (1992) first noticed that patterns such as “NP and other NP” and “NP such as NP” often imply hyponym relations (NP here refers to a noun phrase). However, such approaches to relation extraction are limited by the availability of domain knowledge. Later systems for extracting arbitrary relations from text mostly use shallow surface text patterns (Etzioni et al., 2004; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002). The idea of using sequences of dependency edges as features for relation extraction was explored by Snow et al. (2005) and Suchanek et al. (2006). They define features to be shortest paths on dependency trees which connect pairs of NP candidates. This study is most closely related to work of Mintz et al. (2009), who also study the problem of extending Freebase with extraction from parsed text. As in our work, they use a logistic regression model with path features. However, their approach does not exploit existing knowledge in the KB. Furthermore, their path patterns are used as binary-value</context>
</contexts>
<marker>Ravichandran, Hovy, 2002</marker>
<rawString>Deepak Ravichandran and Eduard Hovy. 2002. Learning surface text patterns for a question answering system. In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics, pages 41–47, Philadelphia, Pennsylvania, USA, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Richardson</author>
<author>Pedro Domingos</author>
</authors>
<title>Markov logic networks.</title>
<date>2006</date>
<booktitle>Machine Learning,</booktitle>
<pages>62--107</pages>
<contexts>
<context position="8445" citStr="Richardson and Domingos, 2006" startWordPosition="1315" endWordPosition="1318">eebase relations (profession, nationality and parents) show that exploiting existing background knowledge as path features can significantly improve the quality of extraction compared with using either Freebase or the text corpus alone. 1.3 Related Work Information extraction from varied unstructured and structured sources involves both complex relational structure and uncertainty at all levels of the extraction process. Statistical Relational Learning (SRL) seeks to combine statistical and relational learning methods to address such tasks. However, most SRL approaches (Friedman et al., 1999; Richardson and Domingos, 2006) suffer the complexity of inference and learning when applied to large scale problems. Recently, Lao and Cohen (2010) introduced Path Ranking algorithm, which is applicable to larger scale problems such as literature recommendation (Lao and Cohen, 2010) and inference on a large knowledge base (Lao et al., 2011). Much of the previous work on automatic relation extraction was based on certain lexico-syntactic patterns. Hearst (1992) first noticed that patterns such as “NP and other NP” and “NP such as NP” often imply hyponym relations (NP here refers to a noun phrase). However, such approaches t</context>
</contexts>
<marker>Richardson, Domingos, 2006</marker>
<rawString>Matthew Richardson and Pedro Domingos. 2006. Markov logic networks. Machine Learning, 62:107– 136.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rion Snow</author>
<author>Daniel Jurafsky</author>
<author>Andrew Y Ng</author>
</authors>
<title>Learning syntactic patterns for automatic hypernym discovery. In</title>
<date>2005</date>
<booktitle>Advances in Neural Information Processing Systems 17,</booktitle>
<pages>1297--1304</pages>
<editor>Lawrence K. Saul, Yair Weiss, and L´eon Bottou, editors,</editor>
<publisher>NIPS Foundation, MIT Press.</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="2337" citStr="Snow et al. (2005)" startWordPosition="348" endWordPosition="351">uring an internship at Google Research then the person is a national of the country.” Of course, rules like this may be defeasible, in this case for example because of naturalization or political changes. Nevertheless, many such imperfect rules can be learned and combined to yield useful KB completions, as demonstrated in particular with the Path-Ranking Algorithm (PRA) (Lao and Cohen, 2010; Lao et al., 2011), which learns such rules on heterogenous graphs for link prediction tasks. Alternatively, we may attempt to fill KB gaps by applying relation extraction rules to free text. For instance, Snow et al. (2005) and Suchanek et al. (2006) showed the value of syntactic patterns in extracting specific relations. In those approaches, KB tuples of the relation to be extracted serve as positive training examples to the extraction rule induction algorithm. However, the KB contains much more knowledge about other relations that could potentially be helpful in improving relation extraction accuracy and coverage, but that is not used in such purely text-based approaches. In this work, we use PRA to learn weighted rules (represented as graph path patterns) that combine both semantic (KB) and syntactic informat</context>
<context position="9420" citStr="Snow et al. (2005)" startWordPosition="1466" endWordPosition="1469">tion extraction was based on certain lexico-syntactic patterns. Hearst (1992) first noticed that patterns such as “NP and other NP” and “NP such as NP” often imply hyponym relations (NP here refers to a noun phrase). However, such approaches to relation extraction are limited by the availability of domain knowledge. Later systems for extracting arbitrary relations from text mostly use shallow surface text patterns (Etzioni et al., 2004; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002). The idea of using sequences of dependency edges as features for relation extraction was explored by Snow et al. (2005) and Suchanek et al. (2006). They define features to be shortest paths on dependency trees which connect pairs of NP candidates. This study is most closely related to work of Mintz et al. (2009), who also study the problem of extending Freebase with extraction from parsed text. As in our work, they use a logistic regression model with path features. However, their approach does not exploit existing knowledge in the KB. Furthermore, their path patterns are used as binary-values features. We show experimentally that fractional-valued features generated by random walks provide much higher accurac</context>
</contexts>
<marker>Snow, Jurafsky, Ng, 2005</marker>
<rawString>Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2005. Learning syntactic patterns for automatic hypernym discovery. In Lawrence K. Saul, Yair Weiss, and L´eon Bottou, editors, Advances in Neural Information Processing Systems 17, pages 1297–1304, Cambridge, MA. NIPS Foundation, MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabian M Suchanek</author>
<author>Georgiana Ifrim</author>
<author>Gerhard Weikum</author>
</authors>
<title>Combining linguistic and statistical analysis to extract relations from web documents.</title>
<date>2006</date>
<booktitle>In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, KDD ’06,</booktitle>
<pages>712--717</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="2364" citStr="Suchanek et al. (2006)" startWordPosition="353" endWordPosition="356">Google Research then the person is a national of the country.” Of course, rules like this may be defeasible, in this case for example because of naturalization or political changes. Nevertheless, many such imperfect rules can be learned and combined to yield useful KB completions, as demonstrated in particular with the Path-Ranking Algorithm (PRA) (Lao and Cohen, 2010; Lao et al., 2011), which learns such rules on heterogenous graphs for link prediction tasks. Alternatively, we may attempt to fill KB gaps by applying relation extraction rules to free text. For instance, Snow et al. (2005) and Suchanek et al. (2006) showed the value of syntactic patterns in extracting specific relations. In those approaches, KB tuples of the relation to be extracted serve as positive training examples to the extraction rule induction algorithm. However, the KB contains much more knowledge about other relations that could potentially be helpful in improving relation extraction accuracy and coverage, but that is not used in such purely text-based approaches. In this work, we use PRA to learn weighted rules (represented as graph path patterns) that combine both semantic (KB) and syntactic information encoded respectively as</context>
<context position="9447" citStr="Suchanek et al. (2006)" startWordPosition="1471" endWordPosition="1474">ed on certain lexico-syntactic patterns. Hearst (1992) first noticed that patterns such as “NP and other NP” and “NP such as NP” often imply hyponym relations (NP here refers to a noun phrase). However, such approaches to relation extraction are limited by the availability of domain knowledge. Later systems for extracting arbitrary relations from text mostly use shallow surface text patterns (Etzioni et al., 2004; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002). The idea of using sequences of dependency edges as features for relation extraction was explored by Snow et al. (2005) and Suchanek et al. (2006). They define features to be shortest paths on dependency trees which connect pairs of NP candidates. This study is most closely related to work of Mintz et al. (2009), who also study the problem of extending Freebase with extraction from parsed text. As in our work, they use a logistic regression model with path features. However, their approach does not exploit existing knowledge in the KB. Furthermore, their path patterns are used as binary-values features. We show experimentally that fractional-valued features generated by random walks provide much higher accuracy than binary-valued ones. </context>
</contexts>
<marker>Suchanek, Ifrim, Weikum, 2006</marker>
<rawString>Fabian M. Suchanek, Georgiana Ifrim, and Gerhard Weikum. 2006. Combining linguistic and statistical analysis to extract relations from web documents. In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, KDD ’06, pages 712–717, New York, NY, USA. ACM.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
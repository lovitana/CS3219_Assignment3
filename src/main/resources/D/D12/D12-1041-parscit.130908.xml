<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000228">
<title confidence="0.99889">
Forced Derivation Tree based Model Training to
Statistical Machine Translation
</title>
<author confidence="0.975172">
Nan Duan
</author>
<affiliation confidence="0.952092">
Microsoft Research Asia
</affiliation>
<author confidence="0.834956">
Mu Li
</author>
<affiliation confidence="0.824914">
Microsoft Research Asia
</affiliation>
<author confidence="0.948049">
Ming Zhou
</author>
<affiliation confidence="0.956929">
Microsoft Research Asia
</affiliation>
<email confidence="0.996607">
nanduan@microsoft.com muli@microsoft.com mingzhou@microsoft.com
</email>
<sectionHeader confidence="0.996607" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998517526315789">
A forced derivation tree (FDT) of a sentence
pair {f, e} denotes a derivation tree that can
translate f into its accurate target translation
e. In this paper, we present an approach that
leverages structured knowledge contained in
FDTs to train component models for statistical
machine translation (SMT) systems. We first
describe how to generate different FDTs for
each sentence pair in training corpus, and then
present how to infer the optimal FDTs based
on their derivation and alignment qualities. As
the first step in this line of research, we verify
the effectiveness of our approach in a BTG-
based phrasal system, and propose four FDT-
based component models. Experiments are
carried out on large scale English-to-Japanese
and Chinese-to-English translation tasks, and
significant improvements are reported on both
translation quality and alignment quality.
</bodyText>
<sectionHeader confidence="0.998882" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998847808510639">
Most of today’s SMT systems depends heavily on
parallel corpora aligned at the word-level to train
their different component models. However, such
annotations do have their drawbacks in training.
On one hand, word links predicted by automatic
aligners such as GIZA++ (Och and Ney, 2004) often
contain errors. This problem gets even worse on lan-
guage pairs that differ substantially in word orders,
such as English and Japanese/Korean/German. The
descent of the word alignment quality will lead to
inaccurate component models straightforwardly.
On the other hand, several component models
are designed to supervise the decoding procedures,
which usually rely on training examples extracted
from word-aligned sentence pairs, such as distortion
models (Tillman, 2004; Xiong et al., 2006; Galley
and Manning, 2008) and sequence models (Banchs
et al., 2005; Quirk and Menezes, 2006; Vaswani et
al., 2011). Ideally, training examples of models are
expected to match most of the situations that could
be met in decoding procedures. But actually, plain
structures of word alignments are too coarse to pro-
vide enough knowledge to ensure this expectation.
This paper presents an FDT-based model training
approach to SMT systems by leveraging structured
knowledge contained in FDTs. An FDT of a sen-
tence pair {f, e} denotes a derivation tree that can
translate f into its accurate target translation e. The
principle advantage of this work is two-fold. First,
using alignments induced from the 1-best FDTs of
all sentence pairs, the overall alignment quality of
training corpus can be improved. Second, compar-
ing to word alignments, FDTs can provide richer
structured knowledge for various component models
to extract training instances. Our FDT-based mod-
el training approach performs via three steps: (1)
generation, where an FDT space composed of dif-
ferent FDTs is generated for each sentence pair in
training corpus by the forced decoding technique;
(2) inference, where the optimal FDTs are extract-
ed from the FDT space of each sentence pair based
on both derivation and alignment qualities measured
by a memory-based re-ranking model; (3) training,
where various component models are trained based
on the optimal FDTs extracted in the inference step.
Our FDT-based model training approach can be
adapted to SMT systems with arbitrary paradigms.
</bodyText>
<page confidence="0.987546">
445
</page>
<note confidence="0.808244">
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 445–454, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.9997105">
As the first step in this line of research, our approach
is verified in a phrase-based SMT system on both
English-to-Japanese and Chinese-to-English transla-
tion tasks . Significant improvements are reported
on both translation quality (up to 1.31 BLEU) and
word alignment quality (up to 3.15 F-score).
</bodyText>
<sectionHeader confidence="0.97628" genericHeader="method">
2 Forced Derivation Tree for SMT
</sectionHeader>
<bodyText confidence="0.9863255">
A forced derivation tree (FDT) of a sentence pair
{f, e} can be defined as a pair G =&lt; D, A &gt;:
</bodyText>
<listItem confidence="0.990515">
• D denotes a derivation that can translate f into
e accurately, using a set of translation rules.
• A denotes a set of word links (i, j) indicating
that ei ∈ e aligns to fj ∈ f.
</listItem>
<bodyText confidence="0.999914142857143">
In this section, we first describe how to gener-
ate FDTs for each sentence pair in training corpus,
which is denoted as the generation step, and then
present how to select the optimal FDT for each sen-
tence pair, which is denoted as the inference step.
We leave a real application of FDTs to the model
training in a phrase-based SMT system in Section 3.
</bodyText>
<subsectionHeader confidence="0.793504">
2.1 Generation
</subsectionHeader>
<bodyText confidence="0.99952175">
We first describe how to generate multiple FDTs for
each sentence pair in training corpus C based on the
forced decoding (FD) technique, which performs via
the following four steps:
</bodyText>
<listItem confidence="0.9802008">
1. Train component models needed for a specific
SMT paradigm M based on training corpus C;
2. Perform MERT on the development data set to
obtain a set of optimized feature weights;
3. For each {f, e} ∈ C, translate f into accurate e
based on M, component models trained in step
1, and feature weights optimized in step 2;
4. For each {f, e} ∈ C, output the hypergraph
(Huang and Chiang, 2005) H(f, e) generated
in step 3 as its FDT space.
</listItem>
<bodyText confidence="0.999807857142857">
In step 3: (1) all partial hypotheses that do not match
any sequence in e will be discarded; (2) derivations
covering identical source and target words but with
different alignments will be kept as different partial
candidates, as they can produce different FDTs for
the same sentence pair. For each {f, e}, the proba-
bility of each G ∈ H(f, e) is computed as:
</bodyText>
<equation confidence="0.979948333333333">
exp{o(G)}
p(G|H(f, e)) = (1)
∑G′∈H(f,e) exp{o(G′)}
</equation>
<bodyText confidence="0.998297375">
where o(G) is the FD model score assigned to G.
For each sentence pair, different alignment candi-
dates can be induced from its different forced deriva-
tion trees generated in the generation step, because
FD can use phrase pairs with different internal word
links extracted from other sentence pairs to recon-
struct the given sentence pair, which could lead to
better word alignment candidates.
</bodyText>
<subsectionHeader confidence="0.883415">
2.2 Inference
</subsectionHeader>
<bodyText confidence="0.996419333333333">
Given an FDT space H(f, e), we propose a memory-
based re-ranking model (MRM), which selects the
best FDT Gˆ as follows:
</bodyText>
<equation confidence="0.9964306">
exp{∑i Aihi(G)}
∑G′∈H(f,e) exp{∑i Aihi(G′)}
∑
= argmax Aihi(G) (2)
G∈H(f,e) i
</equation>
<bodyText confidence="0.9954215">
where hi(G) is feature function and Ai is its feature
weight. Here, memory means the whole translation
history that happened in the generation step will be
used as the evidence to help us compute features.
From the definition we can see that the quality of
an FDT directly relates to two aspects: its derivation
D and alignments A. So two kinds of features are
used to measure the overall quality of each FDT.
(I) The features in the first category measure the
derivation quality of each FDT, including:
</bodyText>
<listItem confidence="0.85295525">
• h(¯e |¯f), source-to-target translation probability
of a translation rule r = {¯f, ¯e}.
fracH(f,e)( f, ¯e) denotes the fractional count of
r used in generating H(f, e):
</listItem>
<equation confidence="0.994087">
∑fracH(f,e)( ¯f, ¯e) = 1r(G)p(G|H(f, e))
G∈H(f,e)
</equation>
<bodyText confidence="0.698604">
1r(G) is an indicator function that equals 1
when r is used in G and 0 otherwise. In prac-
tice, we use pH(f,e)(r) of r to approximate
</bodyText>
<equation confidence="0.999790428571429">
Gˆ = argmax
G∈H(f,e)
f) = ∑{f,e}∈C fracH(f e) (
f, ¯e)
¯f, ¯e′) (3)
h(¯e|
∑{f,e}∈C ∑¯e′ fracH(f e) (
</equation>
<page confidence="0.978944">
446
</page>
<bodyText confidence="0.949541">
fracH(f,e)( f, ¯e) when the size of H(f, e) is too
large to enumerate all FDTs:
</bodyText>
<equation confidence="0.9976445">
ω(r)O(head(r)) HvEtail(r) I(v)
Z(f)
</equation>
<bodyText confidence="0.989707">
where ω(r) is the weight of translation rule r
in the FDT space H(f, e), Z is a normalization
factor that equals to the inside probability of
the root node in H(f, e), I(v) and O(v) are
the standard inside and outside probabilities of
anode v in H(f, e), head(r) and tail(r) are the
head node and a set of tail nodes of a translation
rule r in H(f, e) respectively.
</bodyText>
<listItem confidence="0.98654925">
• h(¯f|¯e), target-to-source translation probability
of a translation rule r = { ¯f, ¯e}.
• h#(r), smoothed usage count for translation
rule r = { ¯f, ¯e} in the whole generation step.
</listItem>
<equation confidence="0.967686">
¯f,¯e)1 (5)
</equation>
<bodyText confidence="0.999118666666667">
In this paper, the sigmoid function is used to
make sure that the feature values of different
translation rules are in a proper value range.
</bodyText>
<listItem confidence="0.994717714285714">
• hr(G), number of translation rules used in G.
• hd(G), structure-based score of G. For FDTs
generated by phrase-based paradigms, it can be
computed by distortion models; while for FDTs
generated by syntax-based paradigms, it can be
computed by either parsing models or syntactic
LMs (Charniak et al., 2003).
</listItem>
<bodyText confidence="0.994888272727273">
The overfitting issue in the generation step can be
alleviated by leveraging memory-based features in
the inference step. h#(r) is used to penalize those
long translation rules which tend to occur in only a
few training sentences and are used few times in FD,
hr(G) adjust our MRM to prefer FDTs consisting of
more translation rules, hd(G) is used to select FDTs
with better parse tree-like structures, which can be
induced from their derivations directly.
(II) The features in the second category measure
the alignment quality of each FDT, including:
</bodyText>
<listItem confidence="0.999848833333333">
• word pair translation probabilities trained from
IBM models (Brown et al., 1993);
• log-likelihood ratio (Moore, 2005);
• conditional link probability (Moore, 2005);
• count of unlinked words;
• counts of inversion and concatenation.
</listItem>
<bodyText confidence="0.995257631578948">
Many alignment-inspired features can be used in
MRM. This paper only uses those commonly-used
ones that have already been proved useful in many
previous work (Moore, 2005; Moore et al., 2006;
Fraser and Marcu, 2006; Liu et al., 2010).
Following the common practice in SMT research,
the MERT algorithm (Och, 2003) is used to tune fea-
ture weights in MRM. Due to the fact that all FDTs
of each sentence pair share identical translation, we
cannot use BLEU as the error criterion any more.
Instead, alignment F-score is used as the alterna-
tive. We will show in Section 5 that after the in-
ference step, alignment quality can be improved by
replacing original alignments of each sentence pair
with alignments induced from its 1-best FDT. Future
work could experiment with other error criterions,
such as reordering-based loss functions (Birch et al.,
2010; Talbot et al., 2011; Birch and Osborne, 2011)
or span F1 (DeNero and Uszkoreit, 2011).
</bodyText>
<sectionHeader confidence="0.957988" genericHeader="method">
3 Training in Phrase-based SMT
</sectionHeader>
<bodyText confidence="0.999994909090909">
As the first step in this line of research, we explore
the usage of FDT-based model training method in
a phrase-based SMT system (Xiong et al., 2006),
which employs Bracketing Transduction Grammar
(BTG) (Wu, 1997) to parse parallel sentences. The
reason of choosing this system is due to the promi-
nent advantages of BTG, such as the simplicity of
the grammar and the good coverage of syntactic di-
versities between different language pairs. We first
describe more details of FDTs under BTG. Then,
four FDT-based component models are presented.
</bodyText>
<subsectionHeader confidence="0.995395">
3.1 BTG-based FDT
</subsectionHeader>
<bodyText confidence="0.9996646">
Given a sentence pair f = {f0, ..., fJ} and e =
{e0, ..., eI} in training corpus, its FDT G generat-
ed based on BTG is a binary tree, which is presented
by a set of terminal translation states T and a set of
non-terminal translation states N, where:
</bodyText>
<equation confidence="0.999566625">
pH(f,e)(r) =
W |¯e) = E{f,e1EC fracH(f,e)(
E{f,e1EC E¯f′ fracH(f,e)(
f, ¯e)
¯f′, ¯e) (4)
1
h#(r) =
1 + e{− ∑{f,e}∈C fracH(f,e�(
</equation>
<page confidence="0.993474">
447
</page>
<figureCaption confidence="0.989551">
Figure 1: S = { ✓[i,j), ¯e[i′,j′), A, m, m′, R} is denoted
</figureCaption>
<bodyText confidence="0.893344">
by the dark-shaded rectangle pair. It can be split into two
child translation states, Sl, which is denoted by the light-
shaded rectangle pair, and ST, which is denoted by the
white rectangle pair. Dash lines within rectangle pairs
denote their internal alignments and solid lines with rows
denote BTG rules. (a) uses [·] to combine two translation
states, while (b) uses ⟨·⟩. Both Sl and ST belong to TUN.
</bodyText>
<listItem confidence="0.9816042">
• each terminal translation state S ∈ T is a 3-
tuple { f[i,j), ¯e[i′,j′), A}, in which f[i,j) denotes
the word sequence that covers the source span
[i, j) of f, ¯e [i′ j ′) denotes the target translation
of f [i j), which is the word sequence that covers
the target span [i′, j′) of e at the same time, A¯ is
a set of word links that aligns ¯f[i,j) and ¯e[i′,j′).
• each non-terminal translation state S ∈ N is a
5-tuple { f[i,j), ¯e[i′,j′), ¯A, m, m′, R}1. The first
3 elements have the same meanings as in T ,
while m and m′ denote two split points that di-
vide S into two child translation states, Sl and
Sr, R denotes a BTG rule, which is either a [·]
operation or a ⟨·⟩ operation2. The relationship
between Sl, Sr and S is illustrated in Figure 1.
</listItem>
<bodyText confidence="0.9247322">
All terminal translation states of the sentence pair
{f, e} are disjoint but cover f[0,J+1) and e[0,I+1) at
the same time, where J = |f |and I = |e|, and
all non-terminal translation states correspond to the
partial decoding states generated during decoding.
</bodyText>
<subsectionHeader confidence="0.998749">
3.2 FDT-based Translation Model
</subsectionHeader>
<bodyText confidence="0.992508">
First, an FDT-based translation model (FDT-TM) is
presented for our BTG-based system.
</bodyText>
<tableCaption confidence="0.57635775">
&apos;We sometimes omit m, m′ and R for a simplicity reason.
2A [·] operation combines the translations of two consecu-
tive source spans [i, m) and [m, j) in a monotonic way; while
a ⟨·⟩ operation combines them in an inverted way.
</tableCaption>
<bodyText confidence="0.9996072">
Given sentence pairs in training corpus with their
corresponding FDT spaces, we train FDT-TM in t-
wo different ways: (1) The first only uses the 1-best
FDT of each sentence pair. Based on each align-
ment A induced from each 1-best FDT G, all possi-
ble bilingual phrases are extracted. Then, the max-
imum likelihood estimation (MLE) is used to com-
pute probabilities and generate an FDT-TM. (2) The
second uses the n-best FDTs of each sentence pair,
which is motivated by several studies (Venugopal et
al., 2008; Liu et al., 2009). For each sentence pair
{f, e}, we first induce n alignments {A1, ..., An}
from the top n FDTs Q = {G1, ..., Gn} ⊂ H(f, e).
Each Ak is annotated with the posterior probability
of its corresponding FDT Gk as follows:
</bodyText>
<equation confidence="0.992709">
exp{Ei Aihi(Gk)}
p(Ak|Gk) = EGk′∈Ω exp{Ei Aihi(Gk′)} (6)
</equation>
<bodyText confidence="0.99995075">
where Ei Aihi(Gk) is the model score assigned to
Gk by MRM. Then, all possible bilingual phrases
are extracted from the expanded training corpus built
using n-best alignments for each sentence pair. The
count of each phrase pair is now computed as the
sum of posterior probabilities, instead of the sum of
absolute frequencies. Last, MLE is used to compute
probabilities and generate an FDT-TM.
</bodyText>
<subsectionHeader confidence="0.987992">
3.3 FDT-based Distortion Model
</subsectionHeader>
<bodyText confidence="0.9998501">
In Xiong’s BTG system, training instances of the
distortion model (DM) are pruned based on heuris-
tic rules, aiming to keep the training size acceptable.
But this will cause the examples remained cannot
cover all reordering cases that could be met in real
decoding procedures. To overcome this drawback,
we propose an FDT-based DM (FDT-DM).
Given the 1-best FDT G of a sentence pair {f, e},
all non-terminal translation states {S1, ..., SK} are
first extracted. For each Sk, we split it into two
child translation states Skl and Skr. A training in-
stance can be then obtained, using the BTG opera-
tion R ∈ Sk as its class label and boundary words of
two translation blocks ( fSkl, ¯eSkl) and ( fSkr, ¯eSkr)
contained in Skl and Skr as its features. Last, the
FDT-DM is trained based on all training instances
by a MaxEnt toolkit, which can cover both local and
global reordering situations due to its training in-
stance extraction mechanism. Figure 2 shows an ex-
ample of extracting training instances from an FDT.
</bodyText>
<page confidence="0.996201">
448
</page>
<figureCaption confidence="0.831648125">
Figure 2: An example of extracting training instances
from an FDT, where solid lines with rows denote BTG
operations and dash lines denote alignments. Two in-
stances can be extracted from this FDT, where 0 and 1
denote a [·] operation and a ⟨·⟩ operation respectively. In
DM training, the number (0 or 1) in each instance is used
as a label, while boundary words are extracted from each
instance’s two phrase pairs and used as lexical features.
</figureCaption>
<subsectionHeader confidence="0.900953">
3.4 FDT-based Source Language Model
</subsectionHeader>
<bodyText confidence="0.999900076923077">
We next propose an FDT-based source language
model (FDT-SLM).
Given the 1-best FDT G of a sentence pair {f, e},
we first extract a reordered source word sequence
f′ = {f′0, ..., f′J} from G, based on the order of ter-
minal translation states in G which covers the target
translation e from left to right. This procedure can
be illustrated by Algorithm 1. Then, all reordered
source sentences of training corpus are used to train
a source LM. During decoding, each time when a
new hypothesis is generated, we obtain its reordered
source word sequence as well, compute a LM score
based on FDT-SLM and use it as a new feature:
</bodyText>
<equation confidence="0.991388">
J
hSLM(f′) = p(f′k|f′k−n+1, ..., f′k−1) (7)
k=1
</equation>
<subsectionHeader confidence="0.72317">
3.5 FDT-based Rule Sequence Model
</subsectionHeader>
<bodyText confidence="0.9980312">
The last contribution in this section is an FDT-based
rule sequence model (FDT-RSM).
Given the 1-best FDT G of a sentence pair {f, e},
we first extract a sequence of translation rule appli-
cations {r1, ..., rK} based on Algorithm 2, where
</bodyText>
<listItem confidence="0.340046454545455">
Algorithm 1: Sequence Extraction in FDT-SLM
1 let f′ = ∅;
2 let S¯ = {S1′, ..., SK′} represents an ordered
sequence of terminal translation states whose
target phrases cover e from left to right orderly;
3 foreach S ∈ S¯ in the left-to-right order do
4 extract f[i,j) from S;
5 append f[i,j) to f′;
6 append a blank space to f′;
7 end
8 return f′ as a reordered source word sequence.
</listItem>
<bodyText confidence="0.995490428571428">
rk = ( ¯f[i,j), ¯e[i′,j′)) denotes the kth phrase pair. Fig-
ure 3 gives an example of extracting a rule sequence
from an FDT. An FDT-RSM is trained based on all
rule sequences extracted from training corpus. Dur-
ing decoding, each time when a new hypothesis is
generated, we compute an FDT-RSM score based on
its rule sequence and use it as a new feature:
</bodyText>
<equation confidence="0.9890954">
K
hRSM(f, e) = H p(rk|rk−n+1, ..., rk−1) (8)
k=1
Algorithm 2: Sequence Extraction in FDT-RSM
1 let r′ = ∅;
</equation>
<bodyText confidence="0.956126">
2 let S¯ = {S1′, ..., SK′} represents an ordered
sequence of terminal translation states whose
target phrases cover e from left to right orderly;
</bodyText>
<listItem confidence="0.6990864">
3 foreach S ∈ S¯ in the left-to-right order do
extract a phrase pair ( f[i,j), ¯e[i′,j′)) from S;
add rk = ( f[i,j), ¯e[i′,j′)) to r′;
6 end
7 return r′ as a rule sequence.
</listItem>
<bodyText confidence="0.999651818181818">
The main difference between FDT-SLM and
FDT-RSM is that the former is trained based on
monolingual n-grams; while the latter is trained
based on bilingual phrases. Although these two
models are trained and computed in an LM style,
they are used as reordering features, because they
help SMT decoder find better decoding sequences.
Of course, the usage of FDTs need not be limit-
ed to the BTG-based system, and we consider using
FDTs generated by SCFG-based systems or tradi-
tional left-to-right phrase-based systems in future.
</bodyText>
<figure confidence="0.56256">
4
5
</figure>
<page confidence="0.969019">
449
</page>
<figureCaption confidence="0.992817333333333">
Figure 3: An example of extracting a rule sequence from
an FDT. In order to generate the correct target translation,
the desired rule sequence should be r2 ⇒ r3 ⇒ r1.
</figureCaption>
<sectionHeader confidence="0.999887" genericHeader="method">
4 Related Work
</sectionHeader>
<subsectionHeader confidence="0.98693">
4.1 Forced Decoding/Alignment
</subsectionHeader>
<bodyText confidence="0.9999673">
Schwartz (2008) used forced decoding to leverage
multilingual corpus to improve translation quality;
Shen et al. (2008) used forced alignment to train
a better phrase segmentation model; Wuebker et al.
(2010) used forced alignment to re-estimate trans-
lation probabilities using a leaving-one-out strategy.
We consider the usage of FD in Section 2.1 to be
a direct extension of these approaches, but one that
generates FDTs for parallel data rather than focusing
on phrase segmentation or probability estimation.
</bodyText>
<subsectionHeader confidence="0.999346">
4.2 Pre-reordering
</subsectionHeader>
<bodyText confidence="0.999917578947368">
Pre-reordering (PRO) techniques (Collins et al.,
2005; Xu et al., 2009; Genzel et al., 2010; Lee et
al., 2010) used features from syntactic parse trees
to reorder source sentences at training and transla-
tion time. A parser is often indispensable to provide
syntactic information for such methods. Recently,
DeNero and Uszkoreit (2011) proposed an approach
that induced parse trees automatically from word-
aligned training corpus to perform PRO for a phrase-
based SMT system, instead of relying on treebanks.
First, binary parse trees are induced from word-
aligned training corpus. Based on them, a monolin-
gual parsing model and a tree reordering model are
trained to pre-reorder source words into the target-
language-like order. Their work is distinct from ours
because it focused on inducing sentence structures
for the PRO task, but mirrors ours in demonstrating
that there is a potential role for structure-based train-
ing corpus in SMT model training.
</bodyText>
<subsectionHeader confidence="0.997605">
4.3 Distortion Models
</subsectionHeader>
<bodyText confidence="0.999985272727273">
Lexicalized distortion models (Tillman, 2004; Zens
and Ney, 2006; Xiong et al., 2006; Galley and Man-
ning, 2008;) are widely used in phrase-based SMT
systems. Training instances of these models are ex-
tracted from word-aligned sentence pairs. Due to ef-
ficiency reasons, only parts of all instances are kept
and used in DM training, which cannot cover all pos-
sible reordering situations that could be met in de-
coding. In FDT-DM, by contrast, training instances
are extracted from FDTs. Such instances take both
local and global reordering cases into consideration.
</bodyText>
<subsectionHeader confidence="0.996589">
4.4 Sequence Models
</subsectionHeader>
<bodyText confidence="0.999984">
Feng et al. (2010) proposed an SLM in a phrase-
based SMT system. They used it as a reordering
feature in the sense that it helped the decoder to find
correct decoding sequences. The difference between
their model and our FDT-SLM is that, in their work,
the reordered source sequences are extracted based
on word alignments only; while in our FDT-SLM,
such sequences are obtained based on FDTs.
Quirk and Menezes (2006) proposed a Minimal
Translation Unit (MTU) -based sequence model and
used it in their treelet system; Vaswani et al. (2011)
proposed a rule Markov model to capture dependen-
cies between minimal rules for a top-down tree-to-
string system. The key difference between FDT-
RSM and previous work is that the rule sequences
are extracted from FDTs, and no parser is needed.
</bodyText>
<sectionHeader confidence="0.999864" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.999602">
5.1 Data and Metric
</subsectionHeader>
<bodyText confidence="0.999939333333333">
Experiments are carried out on English-to-Japanese
(E-J) and Chinese-to-English (C-E) MT tasks.
For E-J task, bilingual data used contains 13.3M
sentence pairs after pre-processing. The Japanese
side of bilingual data is used to train a 4-gram LM.
The development set (dev) which contains 2,000
sentences is used to optimize the log-linear SMT
model. Two test sets are used for evaluation, which
contain 5,000 sentences (test-1) and 999 sentences
</bodyText>
<page confidence="0.993729">
450
</page>
<bodyText confidence="0.9955845">
(test-2) respectively. In all evaluation data sets, each
source sentence has only one reference translation.
For C-E task, bilingual data used contains 0.5M
sentence pairs with high translation quality, includ-
ing LDC2003E07, LDC2003E14, LDC2005T06,
LDC2005T10, LDC2005E83, LDC2006E26, LD-
C2006E34, LDC2006E85 and LDC2006E92. A 5-
gram LM is trained on the Xinhua portion of LDC
English Gigaword Version 3.0. NIST 2004 (MT04)
data set is used as dev set, and evaluation results
are measured on NIST 2005 (MT05) and NIST 2008
(MT08) data sets. In all evaluation data sets, each
source sentence has four reference translations.
Default word alignments for both SMT tasks are
performed by GIZA++ with the intersect-diag-grow
refinement. Translation quality is measured in terms
of case-insensitive BLEU (Papineni et al., 2002) and
reported in percentage numbers.
</bodyText>
<subsectionHeader confidence="0.999597">
5.2 Baseline System
</subsectionHeader>
<bodyText confidence="0.999988285714286">
The phrase-based SMT system proposed by Xiong
et al. (2006) is used as the baseline system, with a
MaxEnt principle-based lexicalized reordering mod-
el integrated, which is used to handle reorderings in
decoding. The maximum lengths for the source and
target phrases are 5 and 7 on E-J task, and 3 and 5
on C-E task. The beam size is set to 20.
</bodyText>
<subsectionHeader confidence="0.997846">
5.3 Translation Quality on E-J Task
</subsectionHeader>
<bodyText confidence="0.999572872340425">
We first evaluate the effectiveness of our FDT-based
model training approach on E-J translation task, and
present evaluation results in Table 1, in which BTG
denotes the performance of the baseline system.
FDT-TM denotes the improved system that uses
FDT-TM proposed in Section 3.2 instead of original
phrase table. As described in Section 3.2, we tried
different sizes of n-best FDTs to induce alignments
for phrase extraction and found the optimal choice
is 5. Besides, in order to make full use of the train-
ing corpus, for those sentence pairs that are failed in
FD, we just use their original word alignments to ex-
tract bilingual phrases. We can see from Table 1 that
FDT-TM outperforms the BTG system significantly.
FDT-DM denotes the improved system that us-
es FDT-DM proposed in Section 3.3 instead of o-
riginal distortion model. Comparing to baseline D-
M which has length limitation on training instances,
training examples of FDT-DM are extracted from 1-
best FDTs without any restriction. This makes our
new DM can cover both local and global reordering
situations that might be met in decoding procedures.
We can see from Table 1 that using FDT-DM, sig-
nificant improvements can be achieved.
FDT-SLM denotes the improved system that uses
FDT-SLM proposed in Section 3.4 as an addition-
al feature, in which the maximum n-gram order is
4. However, from Table 1 we notice that with FDT-
SLM integrated, only 0.2 BLEU improvements can
be obtained. We analyze decoding-logs and find that
the reordered source sequences of n-best translation-
s are very similar, which, we think, can explain why
improvements of using this model are so limited.
FDT-RSM denotes the improved system that us-
es FDT-RSM proposed in Section 3.5 as an addi-
tional feature. The maximum order of this model
is 3. From Table 1 we can see that FDT-RSM out-
performs BTG significantly, with up to 0.48 BLEU
improvements. Comparing to FDT-SLM, FDT-RSM
performs slightly better as well. We think it is due
to the fact that bilingual phrases can provide more
discriminative power than monolingual n-grams do.
Last, all these four FDT-based models (FDT-TM,
FDT-DM, FDT-SLM and FDT-RSM) are put togeth-
er to form an improved system that is denoted as
FDT-ALL. It can provide an averaged 1.2 BLEU im-
provements on these three evaluation data sets.
</bodyText>
<table confidence="0.99978925">
BLEU dev test-1 test-2
BTG 20.60 20.27 13.15
FDT-TM 21.21 20.71(+0.44) 13.98(+0.83)
FDT-DM 21.13 20.79(+0.52) 14.25(+1.10)
FDT-SLM 20.84 20.50(+0.23) 13.36(+0.21)
FDT-RSM 21.07 20.75(+0.48) 13.59(+0.44)
FDT-ALL 21.83 21.34(+1.07) 14.46(+1.31)
PRO 21.89 21.81 14.69
</table>
<tableCaption confidence="0.999986">
Table 1: FDT-based model training on E-J task.
</tableCaption>
<bodyText confidence="0.998501714285714">
Pre-reordering (PRO) is often used on language
pairs, e.g. English and Japanese, with very different
word orders. So we compare our method with PRO
as well. We re-implement the PRO method proposed
by Genzel (2010) and show its results in Table 1. On
dev and test-2, FDT-ALL performs comparable to
PRO, with no syntactic information needed at all.
</bodyText>
<page confidence="0.996865">
451
</page>
<subsectionHeader confidence="0.953752">
5.4 Translation Quality on C-E Task
</subsectionHeader>
<bodyText confidence="0.99984275">
We then evaluate the effectiveness of our FDT-based
model training approach on C-E translation task, and
present evaluation results in Table 2, from which we
can see significant improvements as well.
</bodyText>
<tableCaption confidence="0.996738">
Table 2: FDT-based model training on C-E task
</tableCaption>
<bodyText confidence="0.9984438">
Comparing to numbers in Table 1, the gains com-
ing from the first two FDT-based models become s-
mall on C-E task. This might be due to the fact that
the word alignment quality in C-E task is more reli-
able than that in E-J task for TM and DM training.
</bodyText>
<subsectionHeader confidence="0.994746">
5.5 Effect on Alignment Quality
</subsectionHeader>
<bodyText confidence="0.999931466666667">
We compare the qualities of alignments predicted by
GIZA++ and alignments induced from 1-best FDTs.
For E-J task, 575 English-Japanese sentence pairs
are manually annotated with word alignments. 382
sentence pairs are used as the dev set, and the other
193 sentence pairs are used as the test set. For C-E
task, 491 Chinese-English sentence pairs are manu-
ally annotated with word alignments. 250 sentence
pairs are used as the dev set, and the other 241 sen-
tence pairs are used as the test set. Both Japanese
and Chinese sentences are adapted to our own word
segmentation standards respectively. Table 3 shows
the comparison results. Comparing to C-E language
pair (S-V-O), E-J language pair (S-O-V) has much
lower F-scores, due to its very different word order.
</bodyText>
<table confidence="0.9816412">
F-score from GIZA++ from 1-best FDTs
devEJ 54.75% 57.93%(+3.18%)
testEJ 55.32% 58.47%(+3.15%)
devCE 81.32% 83.37%(+2.05%)
testCE 80.61% 82.51%(+1.90%)
</table>
<tableCaption confidence="0.9947775">
Table 3: Comparison of alignment qualities predicted by
GIZA++ and induced from 1-best FDTs.
</tableCaption>
<bodyText confidence="0.999067333333333">
From Table 3 we can see that the F-score im-
proves on all language pairs when using alignments
induced from 1-best FDTs, rather than GIZA++.
</bodyText>
<subsectionHeader confidence="0.990043">
5.6 Effect on Classification Accuracy
</subsectionHeader>
<bodyText confidence="0.997731294117647">
In the BTG system, the MaxEnt model is used as a
binary classifier to predict reordering operations of
neighbor translation blocks. As the baseline DM and
our FDT-DM have different mechanisms on training
instance extraction procedures, we compare the clas-
sification accuracies of these two DMs in Table 4 to
show the effect of different training instances. The
MaxEnt toolkit (Zhang, 2004) is used to optimize
feature weights using the l-BFGS method (Byrd et
al., 1995). We set the iteration number to 200 and
Gaussian prior to 1 for avoiding overfitting. Table
4 shows that when using training instances extract-
ed from FDTs, classification accuracy of reorderings
improves on both E-J and C-E tasks. This is because
FDTs can provide more deterministic and structured
knowledge for training instance extraction, which
can cover both local and global reordering cases.
</bodyText>
<table confidence="0.992841">
baseline DM FDT-based DM
E-J 93.67% 95.60%(+1.93%)
C-E 95.85% 97.52%(+1.67%)
</table>
<tableCaption confidence="0.9929465">
Table 4: Comparison of classification accuracies of DMs
based on instances extracted by different mechanisms.
</tableCaption>
<sectionHeader confidence="0.998824" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999941375">
In this paper, we have presented an FDT-based mod-
el training approach to SMT. As the first step in this
research direction, we have verified our method on a
phrase-based SMT system, and proposed four FDT-
based component models. Experiments on both E-J
and C-E tasks have demonstrated the effectiveness
of our approach. Summing up, comparing to plain
word alignments, FDTs provide richer structured
knowledge for more accurate SMT model training.
Several potential research topics can be explored in
future. For example, FDTs can be used in a pre-
reordering framework. This is feasible in the sense
that FDTs can provide both tree-like structures and
reordering information. We also plan to adapt our
FDT-based model training approach to SCFG-based
and traditional left-to-right phrase-based systems.
</bodyText>
<table confidence="0.998524714285714">
BLEU MT03 MT05 MT08
BTG 38.73 38.01 23.78
FDT-TM 39.14 38.31(+0.30) 24.30(+0.52)
FDT-DM 39.27 38.56(+0.55) 24.50(+0.72)
FDT-SLM 38.97 38.22(+0.21) 24.04(+0.26)
FDT-RSM 39.06 38.33(+0.32) 24.13(+0.35)
FDT-ALL 39.59 38.72(+0.71) 24.67(+0.89)
</table>
<page confidence="0.99591">
452
</page>
<sectionHeader confidence="0.977713" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999639019047619">
Peter Brown, Stephen Pietra, Vincent Pietra, and Robert
Mercer. 1993. The Mathematics of Statistical Ma-
chine Translation: Parameter Estimation, Computa-
tional Linguistics.
Rafael Banchs, Josep Crego, Adri‘a Gispert, Patrik Lam-
bert, and Jos Mario. 2005. Statistical Machine Trans-
lation of Euparl Data by using Bilingual N-grams, In
Proceedings of the ACL Workshop on Building and
Using Parallel Texts.
Alexandra Birch, Phil Blunsom, and Miles Osborne.
2010. Metrics for MT evaluation: Evaluating reorder-
ing, Machine Translation.
Alexandra Birch and Miles Osborne. 2011. Reordering
metrics for MT, In Proceedings of the Association for
Computational Linguistics.
Richard Byrd, Peihuang Lu, Jorge Nocedal, and Ciyou
Zhu. 1995. A limited memory algorithm for bound
constrained optimization, SIAM Journal of Science
and Statistical Computing.
Eugene Charniak, Kevin Knight, and Kenji Yamada.
2003. Syntax-based Language Models for Statistical
Machine Translation, MT Summit.
Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005. Clause Restructuring for Statistical Machine
Translation, In Proceedings of the Association for
Computational Linguistics.
John DeNero and Jakob Uszkoreit. 2011. Inducing Sen-
tence Structure from Parallel Corpora for Reordering,
In Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing.
Minwei Feng, Arne Mauser, and Hermann Ney. 2010. A
Source-side Decoding Sequence Model for Statistical
Machine Translation, In Proceedings of the Confer-
ence of the Association for Machine Translation.
Alexander Fraser and Daniel Marcu. 2006. Semi-
Supervised Training for Statistical Word Alignment, In
Proceedings of the International Conference on Com-
putational Linguistics and Annual Meeting of the As-
sociation for Computational Linguistics.
Dmitriy Genzel. 2010. Automatically learning source-
side reordering rules for large scale machine transla-
tion, In Proceedings of the Conference on Computa-
tional Linguistics.
Liang Huang and David Chiang. 2005. Better k-best
Parsing, In Proceedings of International Conference
on Parsing Technologies,.
Young-Suk Lee, Bing Zhao, and Xiaoqiang Luo.
2010. Constituent Reordering and Syntax Models for
English-to-Japanese Statistical Machine Translation,
In Proceedings of the Conference on Computational
Linguistics.
Yang Liu, Tian Xia, Xinyan Xiao, and Qun Liu. 2009.
Weighted Alignment Matrices for Statistical Ma-chine
Translation, In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing.
Yang Liu, Qun Liu, and Shouxun Lin. 2010. Discrimi-
native Word Alignment by Linear Modeling, Compu-
tational Linguistics.
Robert Moore. 2005. A Discriminative Framework for
Bilingual Word Alignment, In Proceedings of Human
Language Technology Conference and Conference on
Empirical Methods in Natural Language Processing.
Robert Moore, Wen-tau Yih, and Andreas Bode. 2006.
Improved Discriminative Bilingual Word Alignmen-
t, In Proceedings of the International Conference on
Computational Linguistics and Annual Meeting of the
Association for Computational Linguistics.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based Translation, In Proceedings of the Association
for Computational Linguistics.
Galley Michel and Christopher D. Manning. 2008. A
Simple and Effective Hierarchical Phrase Reordering
Model, In Proceedings of the Conference on Empirical
Methods in Natural Language Processing.
Franz Och. 2003. Minimum Error Rate Training in S-
tatistical Machine Translation, In Proceedings of the
Association for Computational Linguistics.
Franz Och and Hermann Ney. 2004. The Alignment
Template Approach to Statistical Machine Translation,
Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation, In Proceedings of the
Association for Computational Linguistics.
Chris Quirk and Arul Menezes. 2006. Do we need phras-
es? Challenging the conventional wisdom in Statisti-
cal Machine Translation, In Proceedings of the North
American Chapter of the Association for Computa-
tional Linguistics.
Lane Schwartz. 2008. Multi-Source Translation Method-
s, In Proceedings of the Conference of the Association
for Machine Translation.
Wade Shen, Brian Delaney, Tim Anderson, and Ray Sly-
h. 2008. The MIT-LL/AFRL IWSLT-2008 MT System,
International Workshop on Spoken Language Transla-
tion.
David Talbot, Hideto Kazawa, Hiroshi Ichikawa, Ja-
son Katz-Brown, Masakazu Seno, and Franz Och.
2011. A lightweight evaluation framework for ma-
chine translation reordering, In Proceedings of the
Sixth Workshop on Statistical Machine Translation.
Ashish Vaswani, Haitao Mi, Liang Huang, and David
Chiang. 2011. Rule Markov Models for Fast Tree-to-
String Translation, In Proceedings of the Association
for Computational Linguistics.
</reference>
<page confidence="0.991201">
453
</page>
<reference confidence="0.999394444444444">
Ashish Venugopal, Andreas Zollmann, Noah Smith, and
Stephan Vogel. 2008. Wider Pipelines: N-best Align-
ments and Parses in MT Training, In Proceedings of
the Conference of the Association for Machine Trans-
lation.
Joern Wuebker, Arne Mauser, and Hermann Ney. 2010.
Training Phrase Translation Models with Leaving-
One-Out, In Proceedings of the Association for Com-
putational Linguistics.
Dekai Wu. 1997. Stochastic Inversion Transduction
Grammars and Bilingual Parsing of Parallel Corpora,
Computational Linguistics.
Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Max-
imum entropy based phrase reordering model for s-
tatistical machine translation, In Proceedings of the
Association for Computational Linguistics.
Peng Xu, Jaeho Kang, Michael Ringgaard, and Franz
Och. 2009. Using a Dependency Parser to Improve
SMTfor Subject-Object-Verb Languages, In Proceed-
ings of the North American Chapter of the Association
for Computational Linguistics.
Richard Zens and Hermann Ney. 2006. Discriminative
Reordering Models for Statistical Machine Transla-
tion, In Proceedings of the Workshop on Statistical
Machine Translation.
Le Zhang. 2004. Maximum Entropy Modeling Toolkit
for Python and C++.
</reference>
<page confidence="0.999173">
454
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.967746">
<title confidence="0.9995635">Forced Derivation Tree based Model Training Statistical Machine Translation</title>
<author confidence="0.999891">Nan Duan</author>
<affiliation confidence="0.997326">Microsoft Research Asia</affiliation>
<author confidence="0.992845">Mu Li</author>
<affiliation confidence="0.99792">Microsoft Research Asia</affiliation>
<author confidence="0.995419">Ming Zhou</author>
<affiliation confidence="0.999445">Microsoft Research Asia</affiliation>
<email confidence="0.998042">nanduan@microsoft.commuli@microsoft.commingzhou@microsoft.com</email>
<abstract confidence="0.9993576">derivation tree of a sentence a derivation tree that can its accurate target translation In this paper, we present an approach that leverages structured knowledge contained in FDTs to train component models for statistical machine translation (SMT) systems. We first describe how to generate different FDTs for each sentence pair in training corpus, and then present how to infer the optimal FDTs based on their derivation and alignment qualities. As the first step in this line of research, we verify the effectiveness of our approach in a BTGbased phrasal system, and propose four FDTbased component models. Experiments are carried out on large scale English-to-Japanese and Chinese-to-English translation tasks, and significant improvements are reported on both translation quality and alignment quality.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Peter Brown</author>
<author>Stephen Pietra</author>
<author>Vincent Pietra</author>
<author>Robert Mercer</author>
</authors>
<date>1993</date>
<journal>The Mathematics of Statistical Machine Translation: Parameter Estimation, Computational Linguistics.</journal>
<contexts>
<context position="9047" citStr="Brown et al., 1993" startWordPosition="1495" endWordPosition="1498"> issue in the generation step can be alleviated by leveraging memory-based features in the inference step. h#(r) is used to penalize those long translation rules which tend to occur in only a few training sentences and are used few times in FD, hr(G) adjust our MRM to prefer FDTs consisting of more translation rules, hd(G) is used to select FDTs with better parse tree-like structures, which can be induced from their derivations directly. (II) The features in the second category measure the alignment quality of each FDT, including: • word pair translation probabilities trained from IBM models (Brown et al., 1993); • log-likelihood ratio (Moore, 2005); • conditional link probability (Moore, 2005); • count of unlinked words; • counts of inversion and concatenation. Many alignment-inspired features can be used in MRM. This paper only uses those commonly-used ones that have already been proved useful in many previous work (Moore, 2005; Moore et al., 2006; Fraser and Marcu, 2006; Liu et al., 2010). Following the common practice in SMT research, the MERT algorithm (Och, 2003) is used to tune feature weights in MRM. Due to the fact that all FDTs of each sentence pair share identical translation, we cannot us</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter Brown, Stephen Pietra, Vincent Pietra, and Robert Mercer. 1993. The Mathematics of Statistical Machine Translation: Parameter Estimation, Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rafael Banchs</author>
<author>Josep Crego</author>
<author>Adri‘a Gispert</author>
<author>Patrik Lambert</author>
<author>Jos Mario</author>
</authors>
<title>Statistical Machine Translation of Euparl Data by using Bilingual N-grams,</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL Workshop on Building and Using Parallel Texts.</booktitle>
<contexts>
<context position="1978" citStr="Banchs et al., 2005" startWordPosition="288" endWordPosition="291"> automatic aligners such as GIZA++ (Och and Ney, 2004) often contain errors. This problem gets even worse on language pairs that differ substantially in word orders, such as English and Japanese/Korean/German. The descent of the word alignment quality will lead to inaccurate component models straightforwardly. On the other hand, several component models are designed to supervise the decoding procedures, which usually rely on training examples extracted from word-aligned sentence pairs, such as distortion models (Tillman, 2004; Xiong et al., 2006; Galley and Manning, 2008) and sequence models (Banchs et al., 2005; Quirk and Menezes, 2006; Vaswani et al., 2011). Ideally, training examples of models are expected to match most of the situations that could be met in decoding procedures. But actually, plain structures of word alignments are too coarse to provide enough knowledge to ensure this expectation. This paper presents an FDT-based model training approach to SMT systems by leveraging structured knowledge contained in FDTs. An FDT of a sentence pair {f, e} denotes a derivation tree that can translate f into its accurate target translation e. The principle advantage of this work is two-fold. First, us</context>
</contexts>
<marker>Banchs, Crego, Gispert, Lambert, Mario, 2005</marker>
<rawString>Rafael Banchs, Josep Crego, Adri‘a Gispert, Patrik Lambert, and Jos Mario. 2005. Statistical Machine Translation of Euparl Data by using Bilingual N-grams, In Proceedings of the ACL Workshop on Building and Using Parallel Texts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexandra Birch</author>
<author>Phil Blunsom</author>
<author>Miles Osborne</author>
</authors>
<title>Metrics for MT evaluation: Evaluating reordering, Machine Translation.</title>
<date>2010</date>
<contexts>
<context position="10050" citStr="Birch et al., 2010" startWordPosition="1659" endWordPosition="1662">0). Following the common practice in SMT research, the MERT algorithm (Och, 2003) is used to tune feature weights in MRM. Due to the fact that all FDTs of each sentence pair share identical translation, we cannot use BLEU as the error criterion any more. Instead, alignment F-score is used as the alternative. We will show in Section 5 that after the inference step, alignment quality can be improved by replacing original alignments of each sentence pair with alignments induced from its 1-best FDT. Future work could experiment with other error criterions, such as reordering-based loss functions (Birch et al., 2010; Talbot et al., 2011; Birch and Osborne, 2011) or span F1 (DeNero and Uszkoreit, 2011). 3 Training in Phrase-based SMT As the first step in this line of research, we explore the usage of FDT-based model training method in a phrase-based SMT system (Xiong et al., 2006), which employs Bracketing Transduction Grammar (BTG) (Wu, 1997) to parse parallel sentences. The reason of choosing this system is due to the prominent advantages of BTG, such as the simplicity of the grammar and the good coverage of syntactic diversities between different language pairs. We first describe more details of FDTs u</context>
</contexts>
<marker>Birch, Blunsom, Osborne, 2010</marker>
<rawString>Alexandra Birch, Phil Blunsom, and Miles Osborne. 2010. Metrics for MT evaluation: Evaluating reordering, Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexandra Birch</author>
<author>Miles Osborne</author>
</authors>
<title>Reordering metrics for MT,</title>
<date>2011</date>
<booktitle>In Proceedings of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="10097" citStr="Birch and Osborne, 2011" startWordPosition="1667" endWordPosition="1670">research, the MERT algorithm (Och, 2003) is used to tune feature weights in MRM. Due to the fact that all FDTs of each sentence pair share identical translation, we cannot use BLEU as the error criterion any more. Instead, alignment F-score is used as the alternative. We will show in Section 5 that after the inference step, alignment quality can be improved by replacing original alignments of each sentence pair with alignments induced from its 1-best FDT. Future work could experiment with other error criterions, such as reordering-based loss functions (Birch et al., 2010; Talbot et al., 2011; Birch and Osborne, 2011) or span F1 (DeNero and Uszkoreit, 2011). 3 Training in Phrase-based SMT As the first step in this line of research, we explore the usage of FDT-based model training method in a phrase-based SMT system (Xiong et al., 2006), which employs Bracketing Transduction Grammar (BTG) (Wu, 1997) to parse parallel sentences. The reason of choosing this system is due to the prominent advantages of BTG, such as the simplicity of the grammar and the good coverage of syntactic diversities between different language pairs. We first describe more details of FDTs under BTG. Then, four FDT-based component models</context>
</contexts>
<marker>Birch, Osborne, 2011</marker>
<rawString>Alexandra Birch and Miles Osborne. 2011. Reordering metrics for MT, In Proceedings of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Byrd</author>
<author>Peihuang Lu</author>
<author>Jorge Nocedal</author>
<author>Ciyou Zhu</author>
</authors>
<title>A limited memory algorithm for bound constrained optimization,</title>
<date>1995</date>
<journal>SIAM Journal of Science and Statistical Computing.</journal>
<contexts>
<context position="28297" citStr="Byrd et al., 1995" startWordPosition="4748" endWordPosition="4751">the F-score improves on all language pairs when using alignments induced from 1-best FDTs, rather than GIZA++. 5.6 Effect on Classification Accuracy In the BTG system, the MaxEnt model is used as a binary classifier to predict reordering operations of neighbor translation blocks. As the baseline DM and our FDT-DM have different mechanisms on training instance extraction procedures, we compare the classification accuracies of these two DMs in Table 4 to show the effect of different training instances. The MaxEnt toolkit (Zhang, 2004) is used to optimize feature weights using the l-BFGS method (Byrd et al., 1995). We set the iteration number to 200 and Gaussian prior to 1 for avoiding overfitting. Table 4 shows that when using training instances extracted from FDTs, classification accuracy of reorderings improves on both E-J and C-E tasks. This is because FDTs can provide more deterministic and structured knowledge for training instance extraction, which can cover both local and global reordering cases. baseline DM FDT-based DM E-J 93.67% 95.60%(+1.93%) C-E 95.85% 97.52%(+1.67%) Table 4: Comparison of classification accuracies of DMs based on instances extracted by different mechanisms. 6 Conclusions </context>
</contexts>
<marker>Byrd, Lu, Nocedal, Zhu, 1995</marker>
<rawString>Richard Byrd, Peihuang Lu, Jorge Nocedal, and Ciyou Zhu. 1995. A limited memory algorithm for bound constrained optimization, SIAM Journal of Science and Statistical Computing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Kevin Knight</author>
<author>Kenji Yamada</author>
</authors>
<title>Syntax-based Language Models for Statistical Machine Translation,</title>
<date>2003</date>
<location>MT Summit.</location>
<contexts>
<context position="8411" citStr="Charniak et al., 2003" startWordPosition="1392" endWordPosition="1395">urce translation probability of a translation rule r = { ¯f, ¯e}. • h#(r), smoothed usage count for translation rule r = { ¯f, ¯e} in the whole generation step. ¯f,¯e)1 (5) In this paper, the sigmoid function is used to make sure that the feature values of different translation rules are in a proper value range. • hr(G), number of translation rules used in G. • hd(G), structure-based score of G. For FDTs generated by phrase-based paradigms, it can be computed by distortion models; while for FDTs generated by syntax-based paradigms, it can be computed by either parsing models or syntactic LMs (Charniak et al., 2003). The overfitting issue in the generation step can be alleviated by leveraging memory-based features in the inference step. h#(r) is used to penalize those long translation rules which tend to occur in only a few training sentences and are used few times in FD, hr(G) adjust our MRM to prefer FDTs consisting of more translation rules, hd(G) is used to select FDTs with better parse tree-like structures, which can be induced from their derivations directly. (II) The features in the second category measure the alignment quality of each FDT, including: • word pair translation probabilities trained </context>
</contexts>
<marker>Charniak, Knight, Yamada, 2003</marker>
<rawString>Eugene Charniak, Kevin Knight, and Kenji Yamada. 2003. Syntax-based Language Models for Statistical Machine Translation, MT Summit.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Philipp Koehn</author>
<author>Ivona Kucerova</author>
</authors>
<title>Clause Restructuring for Statistical Machine Translation,</title>
<date>2005</date>
<booktitle>In Proceedings of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="19085" citStr="Collins et al., 2005" startWordPosition="3250" endWordPosition="3253"> Work 4.1 Forced Decoding/Alignment Schwartz (2008) used forced decoding to leverage multilingual corpus to improve translation quality; Shen et al. (2008) used forced alignment to train a better phrase segmentation model; Wuebker et al. (2010) used forced alignment to re-estimate translation probabilities using a leaving-one-out strategy. We consider the usage of FD in Section 2.1 to be a direct extension of these approaches, but one that generates FDTs for parallel data rather than focusing on phrase segmentation or probability estimation. 4.2 Pre-reordering Pre-reordering (PRO) techniques (Collins et al., 2005; Xu et al., 2009; Genzel et al., 2010; Lee et al., 2010) used features from syntactic parse trees to reorder source sentences at training and translation time. A parser is often indispensable to provide syntactic information for such methods. Recently, DeNero and Uszkoreit (2011) proposed an approach that induced parse trees automatically from wordaligned training corpus to perform PRO for a phrasebased SMT system, instead of relying on treebanks. First, binary parse trees are induced from wordaligned training corpus. Based on them, a monolingual parsing model and a tree reordering model are </context>
</contexts>
<marker>Collins, Koehn, Kucerova, 2005</marker>
<rawString>Michael Collins, Philipp Koehn, and Ivona Kucerova. 2005. Clause Restructuring for Statistical Machine Translation, In Proceedings of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John DeNero</author>
<author>Jakob Uszkoreit</author>
</authors>
<title>Inducing Sentence Structure from Parallel Corpora for Reordering,</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="10137" citStr="DeNero and Uszkoreit, 2011" startWordPosition="1674" endWordPosition="1677">03) is used to tune feature weights in MRM. Due to the fact that all FDTs of each sentence pair share identical translation, we cannot use BLEU as the error criterion any more. Instead, alignment F-score is used as the alternative. We will show in Section 5 that after the inference step, alignment quality can be improved by replacing original alignments of each sentence pair with alignments induced from its 1-best FDT. Future work could experiment with other error criterions, such as reordering-based loss functions (Birch et al., 2010; Talbot et al., 2011; Birch and Osborne, 2011) or span F1 (DeNero and Uszkoreit, 2011). 3 Training in Phrase-based SMT As the first step in this line of research, we explore the usage of FDT-based model training method in a phrase-based SMT system (Xiong et al., 2006), which employs Bracketing Transduction Grammar (BTG) (Wu, 1997) to parse parallel sentences. The reason of choosing this system is due to the prominent advantages of BTG, such as the simplicity of the grammar and the good coverage of syntactic diversities between different language pairs. We first describe more details of FDTs under BTG. Then, four FDT-based component models are presented. 3.1 BTG-based FDT Given </context>
<context position="19366" citStr="DeNero and Uszkoreit (2011)" startWordPosition="3295" endWordPosition="3298">stimate translation probabilities using a leaving-one-out strategy. We consider the usage of FD in Section 2.1 to be a direct extension of these approaches, but one that generates FDTs for parallel data rather than focusing on phrase segmentation or probability estimation. 4.2 Pre-reordering Pre-reordering (PRO) techniques (Collins et al., 2005; Xu et al., 2009; Genzel et al., 2010; Lee et al., 2010) used features from syntactic parse trees to reorder source sentences at training and translation time. A parser is often indispensable to provide syntactic information for such methods. Recently, DeNero and Uszkoreit (2011) proposed an approach that induced parse trees automatically from wordaligned training corpus to perform PRO for a phrasebased SMT system, instead of relying on treebanks. First, binary parse trees are induced from wordaligned training corpus. Based on them, a monolingual parsing model and a tree reordering model are trained to pre-reorder source words into the targetlanguage-like order. Their work is distinct from ours because it focused on inducing sentence structures for the PRO task, but mirrors ours in demonstrating that there is a potential role for structure-based training corpus in SMT</context>
</contexts>
<marker>DeNero, Uszkoreit, 2011</marker>
<rawString>John DeNero and Jakob Uszkoreit. 2011. Inducing Sentence Structure from Parallel Corpora for Reordering, In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minwei Feng</author>
<author>Arne Mauser</author>
<author>Hermann Ney</author>
</authors>
<title>A Source-side Decoding Sequence Model for Statistical Machine Translation,</title>
<date>2010</date>
<booktitle>In Proceedings of the Conference of the Association for Machine Translation.</booktitle>
<contexts>
<context position="20605" citStr="Feng et al. (2010)" startWordPosition="3494" endWordPosition="3497">Distortion Models Lexicalized distortion models (Tillman, 2004; Zens and Ney, 2006; Xiong et al., 2006; Galley and Manning, 2008;) are widely used in phrase-based SMT systems. Training instances of these models are extracted from word-aligned sentence pairs. Due to efficiency reasons, only parts of all instances are kept and used in DM training, which cannot cover all possible reordering situations that could be met in decoding. In FDT-DM, by contrast, training instances are extracted from FDTs. Such instances take both local and global reordering cases into consideration. 4.4 Sequence Models Feng et al. (2010) proposed an SLM in a phrasebased SMT system. They used it as a reordering feature in the sense that it helped the decoder to find correct decoding sequences. The difference between their model and our FDT-SLM is that, in their work, the reordered source sequences are extracted based on word alignments only; while in our FDT-SLM, such sequences are obtained based on FDTs. Quirk and Menezes (2006) proposed a Minimal Translation Unit (MTU) -based sequence model and used it in their treelet system; Vaswani et al. (2011) proposed a rule Markov model to capture dependencies between minimal rules fo</context>
</contexts>
<marker>Feng, Mauser, Ney, 2010</marker>
<rawString>Minwei Feng, Arne Mauser, and Hermann Ney. 2010. A Source-side Decoding Sequence Model for Statistical Machine Translation, In Proceedings of the Conference of the Association for Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Fraser</author>
<author>Daniel Marcu</author>
</authors>
<title>SemiSupervised Training for Statistical Word Alignment,</title>
<date>2006</date>
<booktitle>In Proceedings of the International Conference on Computational Linguistics and Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="9415" citStr="Fraser and Marcu, 2006" startWordPosition="1552" endWordPosition="1555"> tree-like structures, which can be induced from their derivations directly. (II) The features in the second category measure the alignment quality of each FDT, including: • word pair translation probabilities trained from IBM models (Brown et al., 1993); • log-likelihood ratio (Moore, 2005); • conditional link probability (Moore, 2005); • count of unlinked words; • counts of inversion and concatenation. Many alignment-inspired features can be used in MRM. This paper only uses those commonly-used ones that have already been proved useful in many previous work (Moore, 2005; Moore et al., 2006; Fraser and Marcu, 2006; Liu et al., 2010). Following the common practice in SMT research, the MERT algorithm (Och, 2003) is used to tune feature weights in MRM. Due to the fact that all FDTs of each sentence pair share identical translation, we cannot use BLEU as the error criterion any more. Instead, alignment F-score is used as the alternative. We will show in Section 5 that after the inference step, alignment quality can be improved by replacing original alignments of each sentence pair with alignments induced from its 1-best FDT. Future work could experiment with other error criterions, such as reordering-based</context>
</contexts>
<marker>Fraser, Marcu, 2006</marker>
<rawString>Alexander Fraser and Daniel Marcu. 2006. SemiSupervised Training for Statistical Word Alignment, In Proceedings of the International Conference on Computational Linguistics and Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitriy Genzel</author>
</authors>
<title>Automatically learning sourceside reordering rules for large scale machine translation,</title>
<date>2010</date>
<booktitle>In Proceedings of the Conference on Computational Linguistics.</booktitle>
<contexts>
<context position="25942" citStr="Genzel (2010)" startWordPosition="4361" endWordPosition="4362">. It can provide an averaged 1.2 BLEU improvements on these three evaluation data sets. BLEU dev test-1 test-2 BTG 20.60 20.27 13.15 FDT-TM 21.21 20.71(+0.44) 13.98(+0.83) FDT-DM 21.13 20.79(+0.52) 14.25(+1.10) FDT-SLM 20.84 20.50(+0.23) 13.36(+0.21) FDT-RSM 21.07 20.75(+0.48) 13.59(+0.44) FDT-ALL 21.83 21.34(+1.07) 14.46(+1.31) PRO 21.89 21.81 14.69 Table 1: FDT-based model training on E-J task. Pre-reordering (PRO) is often used on language pairs, e.g. English and Japanese, with very different word orders. So we compare our method with PRO as well. We re-implement the PRO method proposed by Genzel (2010) and show its results in Table 1. On dev and test-2, FDT-ALL performs comparable to PRO, with no syntactic information needed at all. 451 5.4 Translation Quality on C-E Task We then evaluate the effectiveness of our FDT-based model training approach on C-E translation task, and present evaluation results in Table 2, from which we can see significant improvements as well. Table 2: FDT-based model training on C-E task Comparing to numbers in Table 1, the gains coming from the first two FDT-based models become small on C-E task. This might be due to the fact that the word alignment quality in C-E</context>
</contexts>
<marker>Genzel, 2010</marker>
<rawString>Dmitriy Genzel. 2010. Automatically learning sourceside reordering rules for large scale machine translation, In Proceedings of the Conference on Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>David Chiang</author>
</authors>
<title>Better k-best Parsing,</title>
<date>2005</date>
<booktitle>In Proceedings of International Conference on Parsing Technologies,.</booktitle>
<contexts>
<context position="5259" citStr="Huang and Chiang, 2005" startWordPosition="839" endWordPosition="842">se-based SMT system in Section 3. 2.1 Generation We first describe how to generate multiple FDTs for each sentence pair in training corpus C based on the forced decoding (FD) technique, which performs via the following four steps: 1. Train component models needed for a specific SMT paradigm M based on training corpus C; 2. Perform MERT on the development data set to obtain a set of optimized feature weights; 3. For each {f, e} ∈ C, translate f into accurate e based on M, component models trained in step 1, and feature weights optimized in step 2; 4. For each {f, e} ∈ C, output the hypergraph (Huang and Chiang, 2005) H(f, e) generated in step 3 as its FDT space. In step 3: (1) all partial hypotheses that do not match any sequence in e will be discarded; (2) derivations covering identical source and target words but with different alignments will be kept as different partial candidates, as they can produce different FDTs for the same sentence pair. For each {f, e}, the probability of each G ∈ H(f, e) is computed as: exp{o(G)} p(G|H(f, e)) = (1) ∑G′∈H(f,e) exp{o(G′)} where o(G) is the FD model score assigned to G. For each sentence pair, different alignment candidates can be induced from its different force</context>
</contexts>
<marker>Huang, Chiang, 2005</marker>
<rawString>Liang Huang and David Chiang. 2005. Better k-best Parsing, In Proceedings of International Conference on Parsing Technologies,.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Young-Suk Lee</author>
<author>Bing Zhao</author>
<author>Xiaoqiang Luo</author>
</authors>
<title>Constituent Reordering and Syntax Models for English-to-Japanese Statistical Machine Translation,</title>
<date>2010</date>
<booktitle>In Proceedings of the Conference on Computational Linguistics.</booktitle>
<contexts>
<context position="19142" citStr="Lee et al., 2010" startWordPosition="3262" endWordPosition="3265">ced decoding to leverage multilingual corpus to improve translation quality; Shen et al. (2008) used forced alignment to train a better phrase segmentation model; Wuebker et al. (2010) used forced alignment to re-estimate translation probabilities using a leaving-one-out strategy. We consider the usage of FD in Section 2.1 to be a direct extension of these approaches, but one that generates FDTs for parallel data rather than focusing on phrase segmentation or probability estimation. 4.2 Pre-reordering Pre-reordering (PRO) techniques (Collins et al., 2005; Xu et al., 2009; Genzel et al., 2010; Lee et al., 2010) used features from syntactic parse trees to reorder source sentences at training and translation time. A parser is often indispensable to provide syntactic information for such methods. Recently, DeNero and Uszkoreit (2011) proposed an approach that induced parse trees automatically from wordaligned training corpus to perform PRO for a phrasebased SMT system, instead of relying on treebanks. First, binary parse trees are induced from wordaligned training corpus. Based on them, a monolingual parsing model and a tree reordering model are trained to pre-reorder source words into the targetlangua</context>
</contexts>
<marker>Lee, Zhao, Luo, 2010</marker>
<rawString>Young-Suk Lee, Bing Zhao, and Xiaoqiang Luo. 2010. Constituent Reordering and Syntax Models for English-to-Japanese Statistical Machine Translation, In Proceedings of the Conference on Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Tian Xia</author>
<author>Xinyan Xiao</author>
<author>Qun Liu</author>
</authors>
<title>Weighted Alignment Matrices for Statistical Ma-chine Translation,</title>
<date>2009</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="13455" citStr="Liu et al., 2009" startWordPosition="2272" endWordPosition="2275">i, m) and [m, j) in a monotonic way; while a ⟨·⟩ operation combines them in an inverted way. Given sentence pairs in training corpus with their corresponding FDT spaces, we train FDT-TM in two different ways: (1) The first only uses the 1-best FDT of each sentence pair. Based on each alignment A induced from each 1-best FDT G, all possible bilingual phrases are extracted. Then, the maximum likelihood estimation (MLE) is used to compute probabilities and generate an FDT-TM. (2) The second uses the n-best FDTs of each sentence pair, which is motivated by several studies (Venugopal et al., 2008; Liu et al., 2009). For each sentence pair {f, e}, we first induce n alignments {A1, ..., An} from the top n FDTs Q = {G1, ..., Gn} ⊂ H(f, e). Each Ak is annotated with the posterior probability of its corresponding FDT Gk as follows: exp{Ei Aihi(Gk)} p(Ak|Gk) = EGk′∈Ω exp{Ei Aihi(Gk′)} (6) where Ei Aihi(Gk) is the model score assigned to Gk by MRM. Then, all possible bilingual phrases are extracted from the expanded training corpus built using n-best alignments for each sentence pair. The count of each phrase pair is now computed as the sum of posterior probabilities, instead of the sum of absolute frequencies</context>
</contexts>
<marker>Liu, Xia, Xiao, Liu, 2009</marker>
<rawString>Yang Liu, Tian Xia, Xinyan Xiao, and Qun Liu. 2009. Weighted Alignment Matrices for Statistical Ma-chine Translation, In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Discriminative Word Alignment by Linear Modeling, Computational Linguistics.</title>
<date>2010</date>
<contexts>
<context position="9434" citStr="Liu et al., 2010" startWordPosition="1556" endWordPosition="1559">hich can be induced from their derivations directly. (II) The features in the second category measure the alignment quality of each FDT, including: • word pair translation probabilities trained from IBM models (Brown et al., 1993); • log-likelihood ratio (Moore, 2005); • conditional link probability (Moore, 2005); • count of unlinked words; • counts of inversion and concatenation. Many alignment-inspired features can be used in MRM. This paper only uses those commonly-used ones that have already been proved useful in many previous work (Moore, 2005; Moore et al., 2006; Fraser and Marcu, 2006; Liu et al., 2010). Following the common practice in SMT research, the MERT algorithm (Och, 2003) is used to tune feature weights in MRM. Due to the fact that all FDTs of each sentence pair share identical translation, we cannot use BLEU as the error criterion any more. Instead, alignment F-score is used as the alternative. We will show in Section 5 that after the inference step, alignment quality can be improved by replacing original alignments of each sentence pair with alignments induced from its 1-best FDT. Future work could experiment with other error criterions, such as reordering-based loss functions (Bi</context>
</contexts>
<marker>Liu, Liu, Lin, 2010</marker>
<rawString>Yang Liu, Qun Liu, and Shouxun Lin. 2010. Discriminative Word Alignment by Linear Modeling, Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Moore</author>
</authors>
<title>A Discriminative Framework for Bilingual Word Alignment,</title>
<date>2005</date>
<booktitle>In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="9085" citStr="Moore, 2005" startWordPosition="1502" endWordPosition="1503">ed by leveraging memory-based features in the inference step. h#(r) is used to penalize those long translation rules which tend to occur in only a few training sentences and are used few times in FD, hr(G) adjust our MRM to prefer FDTs consisting of more translation rules, hd(G) is used to select FDTs with better parse tree-like structures, which can be induced from their derivations directly. (II) The features in the second category measure the alignment quality of each FDT, including: • word pair translation probabilities trained from IBM models (Brown et al., 1993); • log-likelihood ratio (Moore, 2005); • conditional link probability (Moore, 2005); • count of unlinked words; • counts of inversion and concatenation. Many alignment-inspired features can be used in MRM. This paper only uses those commonly-used ones that have already been proved useful in many previous work (Moore, 2005; Moore et al., 2006; Fraser and Marcu, 2006; Liu et al., 2010). Following the common practice in SMT research, the MERT algorithm (Och, 2003) is used to tune feature weights in MRM. Due to the fact that all FDTs of each sentence pair share identical translation, we cannot use BLEU as the error criterion any more</context>
</contexts>
<marker>Moore, 2005</marker>
<rawString>Robert Moore. 2005. A Discriminative Framework for Bilingual Word Alignment, In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Moore</author>
<author>Wen-tau Yih</author>
<author>Andreas Bode</author>
</authors>
<title>Improved Discriminative Bilingual Word Alignment,</title>
<date>2006</date>
<booktitle>In Proceedings of the International Conference on Computational Linguistics and Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="9391" citStr="Moore et al., 2006" startWordPosition="1548" endWordPosition="1551">Ts with better parse tree-like structures, which can be induced from their derivations directly. (II) The features in the second category measure the alignment quality of each FDT, including: • word pair translation probabilities trained from IBM models (Brown et al., 1993); • log-likelihood ratio (Moore, 2005); • conditional link probability (Moore, 2005); • count of unlinked words; • counts of inversion and concatenation. Many alignment-inspired features can be used in MRM. This paper only uses those commonly-used ones that have already been proved useful in many previous work (Moore, 2005; Moore et al., 2006; Fraser and Marcu, 2006; Liu et al., 2010). Following the common practice in SMT research, the MERT algorithm (Och, 2003) is used to tune feature weights in MRM. Due to the fact that all FDTs of each sentence pair share identical translation, we cannot use BLEU as the error criterion any more. Instead, alignment F-score is used as the alternative. We will show in Section 5 that after the inference step, alignment quality can be improved by replacing original alignments of each sentence pair with alignments induced from its 1-best FDT. Future work could experiment with other error criterions, </context>
</contexts>
<marker>Moore, Yih, Bode, 2006</marker>
<rawString>Robert Moore, Wen-tau Yih, and Andreas Bode. 2006. Improved Discriminative Bilingual Word Alignment, In Proceedings of the International Conference on Computational Linguistics and Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haitao Mi</author>
<author>Liang Huang</author>
<author>Qun Liu</author>
</authors>
<title>Forestbased Translation,</title>
<date>2008</date>
<booktitle>In Proceedings of the Association for Computational Linguistics.</booktitle>
<marker>Mi, Huang, Liu, 2008</marker>
<rawString>Haitao Mi, Liang Huang, and Qun Liu. 2008. Forestbased Translation, In Proceedings of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Galley Michel</author>
<author>Christopher D Manning</author>
</authors>
<title>A Simple and Effective Hierarchical Phrase Reordering Model,</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<marker>Michel, Manning, 2008</marker>
<rawString>Galley Michel and Christopher D. Manning. 2008. A Simple and Effective Hierarchical Phrase Reordering Model, In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Och</author>
</authors>
<title>Minimum Error Rate Training in Statistical Machine Translation,</title>
<date>2003</date>
<booktitle>In Proceedings of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="9513" citStr="Och, 2003" startWordPosition="1570" endWordPosition="1571">ategory measure the alignment quality of each FDT, including: • word pair translation probabilities trained from IBM models (Brown et al., 1993); • log-likelihood ratio (Moore, 2005); • conditional link probability (Moore, 2005); • count of unlinked words; • counts of inversion and concatenation. Many alignment-inspired features can be used in MRM. This paper only uses those commonly-used ones that have already been proved useful in many previous work (Moore, 2005; Moore et al., 2006; Fraser and Marcu, 2006; Liu et al., 2010). Following the common practice in SMT research, the MERT algorithm (Och, 2003) is used to tune feature weights in MRM. Due to the fact that all FDTs of each sentence pair share identical translation, we cannot use BLEU as the error criterion any more. Instead, alignment F-score is used as the alternative. We will show in Section 5 that after the inference step, alignment quality can be improved by replacing original alignments of each sentence pair with alignments induced from its 1-best FDT. Future work could experiment with other error criterions, such as reordering-based loss functions (Birch et al., 2010; Talbot et al., 2011; Birch and Osborne, 2011) or span F1 (DeN</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Och. 2003. Minimum Error Rate Training in Statistical Machine Translation, In Proceedings of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Och</author>
<author>Hermann Ney</author>
</authors>
<title>The Alignment Template Approach to Statistical Machine Translation, Computational Linguistics.</title>
<date>2004</date>
<contexts>
<context position="1413" citStr="Och and Ney, 2004" startWordPosition="205" endWordPosition="208"> we verify the effectiveness of our approach in a BTGbased phrasal system, and propose four FDTbased component models. Experiments are carried out on large scale English-to-Japanese and Chinese-to-English translation tasks, and significant improvements are reported on both translation quality and alignment quality. 1 Introduction Most of today’s SMT systems depends heavily on parallel corpora aligned at the word-level to train their different component models. However, such annotations do have their drawbacks in training. On one hand, word links predicted by automatic aligners such as GIZA++ (Och and Ney, 2004) often contain errors. This problem gets even worse on language pairs that differ substantially in word orders, such as English and Japanese/Korean/German. The descent of the word alignment quality will lead to inaccurate component models straightforwardly. On the other hand, several component models are designed to supervise the decoding procedures, which usually rely on training examples extracted from word-aligned sentence pairs, such as distortion models (Tillman, 2004; Xiong et al., 2006; Galley and Manning, 2008) and sequence models (Banchs et al., 2005; Quirk and Menezes, 2006; Vaswani </context>
</contexts>
<marker>Och, Ney, 2004</marker>
<rawString>Franz Och and Hermann Ney. 2004. The Alignment Template Approach to Statistical Machine Translation, Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>Weijing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation,</title>
<date>2002</date>
<booktitle>In Proceedings of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="22675" citStr="Papineni et al., 2002" startWordPosition="3821" endWordPosition="3824">ty, including LDC2003E07, LDC2003E14, LDC2005T06, LDC2005T10, LDC2005E83, LDC2006E26, LDC2006E34, LDC2006E85 and LDC2006E92. A 5- gram LM is trained on the Xinhua portion of LDC English Gigaword Version 3.0. NIST 2004 (MT04) data set is used as dev set, and evaluation results are measured on NIST 2005 (MT05) and NIST 2008 (MT08) data sets. In all evaluation data sets, each source sentence has four reference translations. Default word alignments for both SMT tasks are performed by GIZA++ with the intersect-diag-grow refinement. Translation quality is measured in terms of case-insensitive BLEU (Papineni et al., 2002) and reported in percentage numbers. 5.2 Baseline System The phrase-based SMT system proposed by Xiong et al. (2006) is used as the baseline system, with a MaxEnt principle-based lexicalized reordering model integrated, which is used to handle reorderings in decoding. The maximum lengths for the source and target phrases are 5 and 7 on E-J task, and 3 and 5 on C-E task. The beam size is set to 20. 5.3 Translation Quality on E-J Task We first evaluate the effectiveness of our FDT-based model training approach on E-J translation task, and present evaluation results in Table 1, in which BTG denot</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and Weijing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation, In Proceedings of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Quirk</author>
<author>Arul Menezes</author>
</authors>
<title>Do we need phrases? Challenging the conventional wisdom in Statistical Machine Translation,</title>
<date>2006</date>
<booktitle>In Proceedings of the North American Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="2003" citStr="Quirk and Menezes, 2006" startWordPosition="292" endWordPosition="295">uch as GIZA++ (Och and Ney, 2004) often contain errors. This problem gets even worse on language pairs that differ substantially in word orders, such as English and Japanese/Korean/German. The descent of the word alignment quality will lead to inaccurate component models straightforwardly. On the other hand, several component models are designed to supervise the decoding procedures, which usually rely on training examples extracted from word-aligned sentence pairs, such as distortion models (Tillman, 2004; Xiong et al., 2006; Galley and Manning, 2008) and sequence models (Banchs et al., 2005; Quirk and Menezes, 2006; Vaswani et al., 2011). Ideally, training examples of models are expected to match most of the situations that could be met in decoding procedures. But actually, plain structures of word alignments are too coarse to provide enough knowledge to ensure this expectation. This paper presents an FDT-based model training approach to SMT systems by leveraging structured knowledge contained in FDTs. An FDT of a sentence pair {f, e} denotes a derivation tree that can translate f into its accurate target translation e. The principle advantage of this work is two-fold. First, using alignments induced fr</context>
<context position="21004" citStr="Quirk and Menezes (2006)" startWordPosition="3562" endWordPosition="3565">tuations that could be met in decoding. In FDT-DM, by contrast, training instances are extracted from FDTs. Such instances take both local and global reordering cases into consideration. 4.4 Sequence Models Feng et al. (2010) proposed an SLM in a phrasebased SMT system. They used it as a reordering feature in the sense that it helped the decoder to find correct decoding sequences. The difference between their model and our FDT-SLM is that, in their work, the reordered source sequences are extracted based on word alignments only; while in our FDT-SLM, such sequences are obtained based on FDTs. Quirk and Menezes (2006) proposed a Minimal Translation Unit (MTU) -based sequence model and used it in their treelet system; Vaswani et al. (2011) proposed a rule Markov model to capture dependencies between minimal rules for a top-down tree-tostring system. The key difference between FDTRSM and previous work is that the rule sequences are extracted from FDTs, and no parser is needed. 5 Experiments 5.1 Data and Metric Experiments are carried out on English-to-Japanese (E-J) and Chinese-to-English (C-E) MT tasks. For E-J task, bilingual data used contains 13.3M sentence pairs after pre-processing. The Japanese side o</context>
</contexts>
<marker>Quirk, Menezes, 2006</marker>
<rawString>Chris Quirk and Arul Menezes. 2006. Do we need phrases? Challenging the conventional wisdom in Statistical Machine Translation, In Proceedings of the North American Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lane Schwartz</author>
</authors>
<title>Multi-Source Translation Methods,</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference of the Association for Machine Translation.</booktitle>
<contexts>
<context position="18516" citStr="Schwartz (2008)" startWordPosition="3168" endWordPosition="3169">bilingual phrases. Although these two models are trained and computed in an LM style, they are used as reordering features, because they help SMT decoder find better decoding sequences. Of course, the usage of FDTs need not be limited to the BTG-based system, and we consider using FDTs generated by SCFG-based systems or traditional left-to-right phrase-based systems in future. 4 5 449 Figure 3: An example of extracting a rule sequence from an FDT. In order to generate the correct target translation, the desired rule sequence should be r2 ⇒ r3 ⇒ r1. 4 Related Work 4.1 Forced Decoding/Alignment Schwartz (2008) used forced decoding to leverage multilingual corpus to improve translation quality; Shen et al. (2008) used forced alignment to train a better phrase segmentation model; Wuebker et al. (2010) used forced alignment to re-estimate translation probabilities using a leaving-one-out strategy. We consider the usage of FD in Section 2.1 to be a direct extension of these approaches, but one that generates FDTs for parallel data rather than focusing on phrase segmentation or probability estimation. 4.2 Pre-reordering Pre-reordering (PRO) techniques (Collins et al., 2005; Xu et al., 2009; Genzel et al</context>
</contexts>
<marker>Schwartz, 2008</marker>
<rawString>Lane Schwartz. 2008. Multi-Source Translation Methods, In Proceedings of the Conference of the Association for Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wade Shen</author>
<author>Brian Delaney</author>
<author>Tim Anderson</author>
<author>Ray Slyh</author>
</authors>
<date>2008</date>
<booktitle>The MIT-LL/AFRL IWSLT-2008 MT System, International Workshop on Spoken Language Translation.</booktitle>
<contexts>
<context position="18620" citStr="Shen et al. (2008)" startWordPosition="3181" endWordPosition="3184">s reordering features, because they help SMT decoder find better decoding sequences. Of course, the usage of FDTs need not be limited to the BTG-based system, and we consider using FDTs generated by SCFG-based systems or traditional left-to-right phrase-based systems in future. 4 5 449 Figure 3: An example of extracting a rule sequence from an FDT. In order to generate the correct target translation, the desired rule sequence should be r2 ⇒ r3 ⇒ r1. 4 Related Work 4.1 Forced Decoding/Alignment Schwartz (2008) used forced decoding to leverage multilingual corpus to improve translation quality; Shen et al. (2008) used forced alignment to train a better phrase segmentation model; Wuebker et al. (2010) used forced alignment to re-estimate translation probabilities using a leaving-one-out strategy. We consider the usage of FD in Section 2.1 to be a direct extension of these approaches, but one that generates FDTs for parallel data rather than focusing on phrase segmentation or probability estimation. 4.2 Pre-reordering Pre-reordering (PRO) techniques (Collins et al., 2005; Xu et al., 2009; Genzel et al., 2010; Lee et al., 2010) used features from syntactic parse trees to reorder source sentences at train</context>
</contexts>
<marker>Shen, Delaney, Anderson, Slyh, 2008</marker>
<rawString>Wade Shen, Brian Delaney, Tim Anderson, and Ray Slyh. 2008. The MIT-LL/AFRL IWSLT-2008 MT System, International Workshop on Spoken Language Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Talbot</author>
<author>Hideto Kazawa</author>
<author>Hiroshi Ichikawa</author>
<author>Jason Katz-Brown</author>
<author>Masakazu Seno</author>
<author>Franz Och</author>
</authors>
<title>A lightweight evaluation framework for machine translation reordering,</title>
<date>2011</date>
<booktitle>In Proceedings of the Sixth Workshop on Statistical Machine Translation.</booktitle>
<contexts>
<context position="10071" citStr="Talbot et al., 2011" startWordPosition="1663" endWordPosition="1666">mmon practice in SMT research, the MERT algorithm (Och, 2003) is used to tune feature weights in MRM. Due to the fact that all FDTs of each sentence pair share identical translation, we cannot use BLEU as the error criterion any more. Instead, alignment F-score is used as the alternative. We will show in Section 5 that after the inference step, alignment quality can be improved by replacing original alignments of each sentence pair with alignments induced from its 1-best FDT. Future work could experiment with other error criterions, such as reordering-based loss functions (Birch et al., 2010; Talbot et al., 2011; Birch and Osborne, 2011) or span F1 (DeNero and Uszkoreit, 2011). 3 Training in Phrase-based SMT As the first step in this line of research, we explore the usage of FDT-based model training method in a phrase-based SMT system (Xiong et al., 2006), which employs Bracketing Transduction Grammar (BTG) (Wu, 1997) to parse parallel sentences. The reason of choosing this system is due to the prominent advantages of BTG, such as the simplicity of the grammar and the good coverage of syntactic diversities between different language pairs. We first describe more details of FDTs under BTG. Then, four </context>
</contexts>
<marker>Talbot, Kazawa, Ichikawa, Katz-Brown, Seno, Och, 2011</marker>
<rawString>David Talbot, Hideto Kazawa, Hiroshi Ichikawa, Jason Katz-Brown, Masakazu Seno, and Franz Och. 2011. A lightweight evaluation framework for machine translation reordering, In Proceedings of the Sixth Workshop on Statistical Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ashish Vaswani</author>
<author>Haitao Mi</author>
<author>Liang Huang</author>
<author>David Chiang</author>
</authors>
<title>Rule Markov Models for Fast Tree-toString Translation,</title>
<date>2011</date>
<booktitle>In Proceedings of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="2026" citStr="Vaswani et al., 2011" startWordPosition="296" endWordPosition="299">y, 2004) often contain errors. This problem gets even worse on language pairs that differ substantially in word orders, such as English and Japanese/Korean/German. The descent of the word alignment quality will lead to inaccurate component models straightforwardly. On the other hand, several component models are designed to supervise the decoding procedures, which usually rely on training examples extracted from word-aligned sentence pairs, such as distortion models (Tillman, 2004; Xiong et al., 2006; Galley and Manning, 2008) and sequence models (Banchs et al., 2005; Quirk and Menezes, 2006; Vaswani et al., 2011). Ideally, training examples of models are expected to match most of the situations that could be met in decoding procedures. But actually, plain structures of word alignments are too coarse to provide enough knowledge to ensure this expectation. This paper presents an FDT-based model training approach to SMT systems by leveraging structured knowledge contained in FDTs. An FDT of a sentence pair {f, e} denotes a derivation tree that can translate f into its accurate target translation e. The principle advantage of this work is two-fold. First, using alignments induced from the 1-best FDTs of a</context>
<context position="21127" citStr="Vaswani et al. (2011)" startWordPosition="3582" endWordPosition="3585">e both local and global reordering cases into consideration. 4.4 Sequence Models Feng et al. (2010) proposed an SLM in a phrasebased SMT system. They used it as a reordering feature in the sense that it helped the decoder to find correct decoding sequences. The difference between their model and our FDT-SLM is that, in their work, the reordered source sequences are extracted based on word alignments only; while in our FDT-SLM, such sequences are obtained based on FDTs. Quirk and Menezes (2006) proposed a Minimal Translation Unit (MTU) -based sequence model and used it in their treelet system; Vaswani et al. (2011) proposed a rule Markov model to capture dependencies between minimal rules for a top-down tree-tostring system. The key difference between FDTRSM and previous work is that the rule sequences are extracted from FDTs, and no parser is needed. 5 Experiments 5.1 Data and Metric Experiments are carried out on English-to-Japanese (E-J) and Chinese-to-English (C-E) MT tasks. For E-J task, bilingual data used contains 13.3M sentence pairs after pre-processing. The Japanese side of bilingual data is used to train a 4-gram LM. The development set (dev) which contains 2,000 sentences is used to optimize</context>
</contexts>
<marker>Vaswani, Mi, Huang, Chiang, 2011</marker>
<rawString>Ashish Vaswani, Haitao Mi, Liang Huang, and David Chiang. 2011. Rule Markov Models for Fast Tree-toString Translation, In Proceedings of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ashish Venugopal</author>
<author>Andreas Zollmann</author>
<author>Noah Smith</author>
<author>Stephan Vogel</author>
</authors>
<title>Wider Pipelines: N-best Alignments and Parses in MT Training,</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference of the Association for Machine Translation.</booktitle>
<contexts>
<context position="13436" citStr="Venugopal et al., 2008" startWordPosition="2268" endWordPosition="2271">nsecutive source spans [i, m) and [m, j) in a monotonic way; while a ⟨·⟩ operation combines them in an inverted way. Given sentence pairs in training corpus with their corresponding FDT spaces, we train FDT-TM in two different ways: (1) The first only uses the 1-best FDT of each sentence pair. Based on each alignment A induced from each 1-best FDT G, all possible bilingual phrases are extracted. Then, the maximum likelihood estimation (MLE) is used to compute probabilities and generate an FDT-TM. (2) The second uses the n-best FDTs of each sentence pair, which is motivated by several studies (Venugopal et al., 2008; Liu et al., 2009). For each sentence pair {f, e}, we first induce n alignments {A1, ..., An} from the top n FDTs Q = {G1, ..., Gn} ⊂ H(f, e). Each Ak is annotated with the posterior probability of its corresponding FDT Gk as follows: exp{Ei Aihi(Gk)} p(Ak|Gk) = EGk′∈Ω exp{Ei Aihi(Gk′)} (6) where Ei Aihi(Gk) is the model score assigned to Gk by MRM. Then, all possible bilingual phrases are extracted from the expanded training corpus built using n-best alignments for each sentence pair. The count of each phrase pair is now computed as the sum of posterior probabilities, instead of the sum of a</context>
</contexts>
<marker>Venugopal, Zollmann, Smith, Vogel, 2008</marker>
<rawString>Ashish Venugopal, Andreas Zollmann, Noah Smith, and Stephan Vogel. 2008. Wider Pipelines: N-best Alignments and Parses in MT Training, In Proceedings of the Conference of the Association for Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joern Wuebker</author>
<author>Arne Mauser</author>
<author>Hermann Ney</author>
</authors>
<title>Training Phrase Translation Models with LeavingOne-Out,</title>
<date>2010</date>
<booktitle>In Proceedings of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="18709" citStr="Wuebker et al. (2010)" startWordPosition="3195" endWordPosition="3198">f course, the usage of FDTs need not be limited to the BTG-based system, and we consider using FDTs generated by SCFG-based systems or traditional left-to-right phrase-based systems in future. 4 5 449 Figure 3: An example of extracting a rule sequence from an FDT. In order to generate the correct target translation, the desired rule sequence should be r2 ⇒ r3 ⇒ r1. 4 Related Work 4.1 Forced Decoding/Alignment Schwartz (2008) used forced decoding to leverage multilingual corpus to improve translation quality; Shen et al. (2008) used forced alignment to train a better phrase segmentation model; Wuebker et al. (2010) used forced alignment to re-estimate translation probabilities using a leaving-one-out strategy. We consider the usage of FD in Section 2.1 to be a direct extension of these approaches, but one that generates FDTs for parallel data rather than focusing on phrase segmentation or probability estimation. 4.2 Pre-reordering Pre-reordering (PRO) techniques (Collins et al., 2005; Xu et al., 2009; Genzel et al., 2010; Lee et al., 2010) used features from syntactic parse trees to reorder source sentences at training and translation time. A parser is often indispensable to provide syntactic informatio</context>
</contexts>
<marker>Wuebker, Mauser, Ney, 2010</marker>
<rawString>Joern Wuebker, Arne Mauser, and Hermann Ney. 2010. Training Phrase Translation Models with LeavingOne-Out, In Proceedings of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic Inversion Transduction Grammars and Bilingual Parsing of Parallel Corpora, Computational Linguistics.</title>
<date>1997</date>
<contexts>
<context position="10383" citStr="Wu, 1997" startWordPosition="1716" endWordPosition="1717">fter the inference step, alignment quality can be improved by replacing original alignments of each sentence pair with alignments induced from its 1-best FDT. Future work could experiment with other error criterions, such as reordering-based loss functions (Birch et al., 2010; Talbot et al., 2011; Birch and Osborne, 2011) or span F1 (DeNero and Uszkoreit, 2011). 3 Training in Phrase-based SMT As the first step in this line of research, we explore the usage of FDT-based model training method in a phrase-based SMT system (Xiong et al., 2006), which employs Bracketing Transduction Grammar (BTG) (Wu, 1997) to parse parallel sentences. The reason of choosing this system is due to the prominent advantages of BTG, such as the simplicity of the grammar and the good coverage of syntactic diversities between different language pairs. We first describe more details of FDTs under BTG. Then, four FDT-based component models are presented. 3.1 BTG-based FDT Given a sentence pair f = {f0, ..., fJ} and e = {e0, ..., eI} in training corpus, its FDT G generated based on BTG is a binary tree, which is presented by a set of terminal translation states T and a set of non-terminal translation states N, where: pH(</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>Dekai Wu. 1997. Stochastic Inversion Transduction Grammars and Bilingual Parsing of Parallel Corpora, Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deyi Xiong</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Maximum entropy based phrase reordering model for statistical machine translation,</title>
<date>2006</date>
<booktitle>In Proceedings of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1910" citStr="Xiong et al., 2006" startWordPosition="277" endWordPosition="280">e their drawbacks in training. On one hand, word links predicted by automatic aligners such as GIZA++ (Och and Ney, 2004) often contain errors. This problem gets even worse on language pairs that differ substantially in word orders, such as English and Japanese/Korean/German. The descent of the word alignment quality will lead to inaccurate component models straightforwardly. On the other hand, several component models are designed to supervise the decoding procedures, which usually rely on training examples extracted from word-aligned sentence pairs, such as distortion models (Tillman, 2004; Xiong et al., 2006; Galley and Manning, 2008) and sequence models (Banchs et al., 2005; Quirk and Menezes, 2006; Vaswani et al., 2011). Ideally, training examples of models are expected to match most of the situations that could be met in decoding procedures. But actually, plain structures of word alignments are too coarse to provide enough knowledge to ensure this expectation. This paper presents an FDT-based model training approach to SMT systems by leveraging structured knowledge contained in FDTs. An FDT of a sentence pair {f, e} denotes a derivation tree that can translate f into its accurate target transl</context>
<context position="10319" citStr="Xiong et al., 2006" startWordPosition="1706" endWordPosition="1709">nment F-score is used as the alternative. We will show in Section 5 that after the inference step, alignment quality can be improved by replacing original alignments of each sentence pair with alignments induced from its 1-best FDT. Future work could experiment with other error criterions, such as reordering-based loss functions (Birch et al., 2010; Talbot et al., 2011; Birch and Osborne, 2011) or span F1 (DeNero and Uszkoreit, 2011). 3 Training in Phrase-based SMT As the first step in this line of research, we explore the usage of FDT-based model training method in a phrase-based SMT system (Xiong et al., 2006), which employs Bracketing Transduction Grammar (BTG) (Wu, 1997) to parse parallel sentences. The reason of choosing this system is due to the prominent advantages of BTG, such as the simplicity of the grammar and the good coverage of syntactic diversities between different language pairs. We first describe more details of FDTs under BTG. Then, four FDT-based component models are presented. 3.1 BTG-based FDT Given a sentence pair f = {f0, ..., fJ} and e = {e0, ..., eI} in training corpus, its FDT G generated based on BTG is a binary tree, which is presented by a set of terminal translation sta</context>
<context position="20089" citStr="Xiong et al., 2006" startWordPosition="3410" endWordPosition="3413">PRO for a phrasebased SMT system, instead of relying on treebanks. First, binary parse trees are induced from wordaligned training corpus. Based on them, a monolingual parsing model and a tree reordering model are trained to pre-reorder source words into the targetlanguage-like order. Their work is distinct from ours because it focused on inducing sentence structures for the PRO task, but mirrors ours in demonstrating that there is a potential role for structure-based training corpus in SMT model training. 4.3 Distortion Models Lexicalized distortion models (Tillman, 2004; Zens and Ney, 2006; Xiong et al., 2006; Galley and Manning, 2008;) are widely used in phrase-based SMT systems. Training instances of these models are extracted from word-aligned sentence pairs. Due to efficiency reasons, only parts of all instances are kept and used in DM training, which cannot cover all possible reordering situations that could be met in decoding. In FDT-DM, by contrast, training instances are extracted from FDTs. Such instances take both local and global reordering cases into consideration. 4.4 Sequence Models Feng et al. (2010) proposed an SLM in a phrasebased SMT system. They used it as a reordering feature i</context>
<context position="22791" citStr="Xiong et al. (2006)" startWordPosition="3839" endWordPosition="3842">6E92. A 5- gram LM is trained on the Xinhua portion of LDC English Gigaword Version 3.0. NIST 2004 (MT04) data set is used as dev set, and evaluation results are measured on NIST 2005 (MT05) and NIST 2008 (MT08) data sets. In all evaluation data sets, each source sentence has four reference translations. Default word alignments for both SMT tasks are performed by GIZA++ with the intersect-diag-grow refinement. Translation quality is measured in terms of case-insensitive BLEU (Papineni et al., 2002) and reported in percentage numbers. 5.2 Baseline System The phrase-based SMT system proposed by Xiong et al. (2006) is used as the baseline system, with a MaxEnt principle-based lexicalized reordering model integrated, which is used to handle reorderings in decoding. The maximum lengths for the source and target phrases are 5 and 7 on E-J task, and 3 and 5 on C-E task. The beam size is set to 20. 5.3 Translation Quality on E-J Task We first evaluate the effectiveness of our FDT-based model training approach on E-J translation task, and present evaluation results in Table 1, in which BTG denotes the performance of the baseline system. FDT-TM denotes the improved system that uses FDT-TM proposed in Section 3</context>
</contexts>
<marker>Xiong, Liu, Lin, 2006</marker>
<rawString>Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Maximum entropy based phrase reordering model for statistical machine translation, In Proceedings of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peng Xu</author>
<author>Jaeho Kang</author>
<author>Michael Ringgaard</author>
<author>Franz Och</author>
</authors>
<title>Using a Dependency Parser to Improve SMTfor Subject-Object-Verb Languages,</title>
<date>2009</date>
<booktitle>In Proceedings of the North American Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="19102" citStr="Xu et al., 2009" startWordPosition="3254" endWordPosition="3257">ing/Alignment Schwartz (2008) used forced decoding to leverage multilingual corpus to improve translation quality; Shen et al. (2008) used forced alignment to train a better phrase segmentation model; Wuebker et al. (2010) used forced alignment to re-estimate translation probabilities using a leaving-one-out strategy. We consider the usage of FD in Section 2.1 to be a direct extension of these approaches, but one that generates FDTs for parallel data rather than focusing on phrase segmentation or probability estimation. 4.2 Pre-reordering Pre-reordering (PRO) techniques (Collins et al., 2005; Xu et al., 2009; Genzel et al., 2010; Lee et al., 2010) used features from syntactic parse trees to reorder source sentences at training and translation time. A parser is often indispensable to provide syntactic information for such methods. Recently, DeNero and Uszkoreit (2011) proposed an approach that induced parse trees automatically from wordaligned training corpus to perform PRO for a phrasebased SMT system, instead of relying on treebanks. First, binary parse trees are induced from wordaligned training corpus. Based on them, a monolingual parsing model and a tree reordering model are trained to pre-re</context>
</contexts>
<marker>Xu, Kang, Ringgaard, Och, 2009</marker>
<rawString>Peng Xu, Jaeho Kang, Michael Ringgaard, and Franz Och. 2009. Using a Dependency Parser to Improve SMTfor Subject-Object-Verb Languages, In Proceedings of the North American Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Zens</author>
<author>Hermann Ney</author>
</authors>
<title>Discriminative Reordering Models for Statistical Machine Translation,</title>
<date>2006</date>
<booktitle>In Proceedings of the Workshop on Statistical Machine Translation.</booktitle>
<contexts>
<context position="20069" citStr="Zens and Ney, 2006" startWordPosition="3406" endWordPosition="3409">g corpus to perform PRO for a phrasebased SMT system, instead of relying on treebanks. First, binary parse trees are induced from wordaligned training corpus. Based on them, a monolingual parsing model and a tree reordering model are trained to pre-reorder source words into the targetlanguage-like order. Their work is distinct from ours because it focused on inducing sentence structures for the PRO task, but mirrors ours in demonstrating that there is a potential role for structure-based training corpus in SMT model training. 4.3 Distortion Models Lexicalized distortion models (Tillman, 2004; Zens and Ney, 2006; Xiong et al., 2006; Galley and Manning, 2008;) are widely used in phrase-based SMT systems. Training instances of these models are extracted from word-aligned sentence pairs. Due to efficiency reasons, only parts of all instances are kept and used in DM training, which cannot cover all possible reordering situations that could be met in decoding. In FDT-DM, by contrast, training instances are extracted from FDTs. Such instances take both local and global reordering cases into consideration. 4.4 Sequence Models Feng et al. (2010) proposed an SLM in a phrasebased SMT system. They used it as a </context>
</contexts>
<marker>Zens, Ney, 2006</marker>
<rawString>Richard Zens and Hermann Ney. 2006. Discriminative Reordering Models for Statistical Machine Translation, In Proceedings of the Workshop on Statistical Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Le Zhang</author>
</authors>
<title>Maximum Entropy Modeling Toolkit for Python and C++.</title>
<date>2004</date>
<marker>Le Zhang, 2004</marker>
<rawString>Le Zhang. 2004. Maximum Entropy Modeling Toolkit for Python and C++.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
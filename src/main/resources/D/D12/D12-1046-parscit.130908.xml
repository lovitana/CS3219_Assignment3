<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.934932">
Joint Chinese Word Segmentation, POS Tagging and Parsing
</title>
<author confidence="0.997715">
Xian Qian Yang Liu
</author>
<affiliation confidence="0.999158">
Computer Science Department
The University of Texas at Dallas
</affiliation>
<email confidence="0.996898">
qx,yangl@hlt.utdallas.edu
</email>
<sectionHeader confidence="0.996649" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99948215">
In this paper, we propose a novel decoding al-
gorithm for discriminative joint Chinese word
segmentation, part-of-speech (POS) tagging,
and parsing. Previous work often used a
pipeline method – Chinese word segmentation
followed by POS tagging and parsing, which
suffers from error propagation and is unable
to leverage information in later modules for
earlier components. In our approach, we train
the three individual models separately during
training, and incorporate them together in a u-
nified framework during decoding. We extend
the CYK parsing algorithm so that it can deal
with word segmentation and POS tagging fea-
tures. As far as we know, this is the first work
on joint Chinese word segmentation, POS tag-
ging and parsing. Our experimental result-
s on Chinese Tree Bank 5 corpus show that
our approach outperforms the state-of-the-art
pipeline system.
</bodyText>
<sectionHeader confidence="0.998884" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999969666666667">
For Asian languages such as Japanese and Chi-
nese that do not contain explicitly marked word
boundaries, word segmentation is an important first
step for many subsequent language processing tasks,
such as POS tagging, parsing, semantic role label-
ing, and various applications. Previous studies for
POS tagging and syntax parsing on these languages
sometimes assume that gold standard word segmen-
tation information is provided, which is not the re-
al scenario. In a fully automatic system, a pipeline
approach is often adopted, where raw sentences are
first segmented into word sequences, then POS tag-
ging and parsing are performed. This kind of ap-
proach suffers from error propagation. For exam-
ple, word segmentation errors will result in tagging
and parsing errors. Additionally, early modules can-
not use information from subsequent modules. In-
tuitively a joint model that performs the three tasks
together should help the system make the best deci-
sions.
In this paper, we propose a unified model for joint
Chinese word segmentation, POS tagging, and pars-
ing. Three sub-models are independently trained
using the state-of-the-art methods. We do not use
the joint inference algorithm for training because of
the high complexity caused by the large amount of
parameters. We use linear chain Conditional Ran-
dom Fields (CRFs) (Lafferty et al., 2001) to train the
word segmentation model and POS tagging model,
and averaged perceptron (Collins, 2002) to learn the
parsing model. During decoding, parameters of each
sub-model are scaled to represent its importance in
the joint model. Our decoding algorithm is an exten-
sion of CYK parsing. Initially, weights of all possi-
ble words together with their POS tags are calcu-
lated. When searching the parse tree, the word and
POS tagging features are dynamically generated and
the transition information of POS tagging is consid-
ered in the span merge operation.
Experiments are conducted on Chinese Tree Bank
(CTB) 5 dataset, which is widely used for Chinese
word segmentation, POS tagging and parsing. We
compare our proposed joint model with the pipeline
system, both built using the state-of-the-art sub-
models. We also propose an evaluation metric to
</bodyText>
<page confidence="0.965416">
501
</page>
<note confidence="0.825619">
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 501–511, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.999391">
calculate the bracket scores for parsing in the face of
word segmentation errors. Our experimental results
show that the joint model significantly outperform-
s the pipeline method based on the state-of-the-art
sub-models.
</bodyText>
<sectionHeader confidence="0.999324" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999956033333333">
There is very limited previous work on joint Chinese
word segmentation, POS tagging, and parsing. Pre-
vious joint models mainly focus on word segmenta-
tion and POS tagging task, such as the virtual nodes
method (Qian et al., 2010), cascaded linear model
(Jiang et al., 2008a), perceptron (Zhang and Clark,
2008), sub-word based stacked learning (Sun, 2011),
reranking (Jiang et al., 2008b). These joint models
showed about 0.2 − 1% F-score improvement over
the pipeline method. Recently, joint tagging and de-
pendency parsing has been studied as well (Li et al.,
2011; Lee et al., 2011).
Previous research has showed that word segmen-
tation has a great impact on parsing accuracy in
the pipeline method (Harper and Huang, 2009). In
(Jiang et al., 2009), additional data was used to im-
prove Chinese word segmentation, which resulted
in significant improvement on the parsing task us-
ing the pipeline framework. Joint segmentation and
parsing was also investigated for Arabic (Green and
Manning, 2010). A study that is closely related to
ours is (Goldberg and Tsarfaty, 2008), where a s-
ingle generative model was proposed for joint mor-
phological segmentation and syntactic parsing for
Hebrew. Different from that work, we use a discrim-
inative model, which benefits from large amounts of
features and is easier to deal with unknown words.
Another main difference is that, besides segmenta-
tion and parsing, we also incorporate the POS tag-
ging model into the CYK parsing framework.
</bodyText>
<sectionHeader confidence="0.99924" genericHeader="method">
3 Methods
</sectionHeader>
<bodyText confidence="0.999955583333333">
For a given Chinese sentence, our task is to gener-
ate the word sequence, its POS tag sequence, and
the parse tree (constituent parsing). A joint model
is expected to make more optimal decisions than a
pipeline approach; however, such a model will be
too complex and it is difficult to estimate model pa-
rameters. Therefore we do not perform joint infer-
ence for training. Instead, we develop three individ-
ual models independently during training and per-
form joint decoding using them. In this section, we
first describe the three sub-models and then the joint
decoding algorithm.
</bodyText>
<subsectionHeader confidence="0.998001">
3.1 Word Segmentation Model
</subsectionHeader>
<bodyText confidence="0.999702047619048">
Methods for Chinese word segmentation can be
broadly categorized into character based and word
based models. Previous studies showed that
character-based models are more effective to detect
out-of-vocabulary words while word-based model-
s are more accurate to predict in-vocabulary words
(Zhang et al., 2006). Here, we use order-0 semi-
Markov model (Sarawagi and Cohen, 2004) to take
advantages of both approaches.
More specifically, given a sentence x =
c1, c2, ... , cl (where cz is the ith Chinese character,
l is the sentence length), the character-based mod-
el assigns each character with a word boundary tag.
Here we use the BCDIES tag set, which achieved
the best official performance (Zhao and Kit, 2008):
B, C, D, E denote the first, second, third, and last
character of a multi-character word respectively, I
denotes the other characters, and S denotes the s-
ingle character word. We use the same character-
based feature templates as in the best official system,
shown in Table 1 (1.1-1.3), including character un-
igram and bigram features, and transition features.
Linear chain CRFs are used for training.
Feature templates in the word-based model are
shown in Table 1 (1.4-1.6), including word features,
sub-word features, and character bigrams within
words. The word feature is activated if a predicted
word w is in the vocabulary (i.e., appears in train-
ing data). Subword(w) is the longest in-vocabulary
word within w. To use word features, we adopt a K-
best reranking approach. The top K candidate seg-
mentation results for each training sample are gen-
erated using the character-based model, and the gold
segmentation is added if it is not in the candidate set.
We use the Maximum Entropy (ME) model to learn
the weights of word features such that the probabil-
ity of the gold candidate is maximal.
A problem arises when combining the two mod-
els and using it in joint segmentation and parsing,
since the linear chain used in the character-based
model is incompatible with CYK parsing model and
the word-based model due to the transition informa-
</bodyText>
<page confidence="0.995799">
502
</page>
<table confidence="0.69981">
Character Level Feature Templates
(1.1) ci−2yi, ci−1yi, ciyi, ci+1yi, ci+2yi
(1.2) ci−1ciyi, cici+1yi, ci−1ci+1yi
(1.3) yi−1yi
Word Level Feature Templates
(1.4) word w
(1.5) subword(w)
(1.6) character bigrams within w
</table>
<tableCaption confidence="0.968276666666667">
Table 1: Feature templates for word segmentation. ci is
the ith character in the sentence, yi is its label, w is a
predicted word.
</tableCaption>
<bodyText confidence="0.990046125">
tion. Thus, we slightly modify the linear chain CRF-
s by fixing the weights of transition features during
training and testing. That is, weights of impossible
transition features (e.g., B—*B) are set to −oc, and
weights of the other transition features (e.g., E—*B)
are set to 0. In this way, the transition feature could
be neglected in testing for two reasons. First, all ille-
gal label assignments are prohibited in prediction, s-
ince their weights are −oc; second, because weights
of legal transition features are 0, they do not affec-
t the prediction at all. In the following, transition
features are excluded.
Now we can use order-0 semi Markov model as
the hybrid model. We define the score of a word as
the sum of the weights of all the features within the
word. Formally, the score of a multi-character word
</bodyText>
<equation confidence="0.99936425">
w = ci, ... , cj is defined as:
scoreseg(x, i, i) = BCRF - fCRF(x, yi = B) + .. .
+BCRF - fCRF (x, yj = E) + BME - fME(x, i, �)
= Bsegfseg(x, i, A (1)
</equation>
<bodyText confidence="0.999714777777778">
where fCRF and fME are the feature vectors in the
character and word based models respectively, and
BCRF, BME are their corresponding weight vectors.
For simplicity, we denote Bseg = BCRF®ME, fseg =
fCRF®ME, where BCRF®ME means the concatena-
tion of BCRF and BME. Scores for single character
words are defined similarly. These word scores will
be used in the joint segmentation and parsing task
Section 3.4.
</bodyText>
<subsectionHeader confidence="0.995679">
3.2 POS Tagging Model
</subsectionHeader>
<bodyText confidence="0.9993015">
Though syntax parsing model can directly predict
the POS tag itself, we choose not to use this, but use
an independent POS tagger for two reasons. First,
there is a large amount of data with labeled POS tags
but no syntax annotations, such as the People’s Daily
corpus and SIGHAN bakeoff corpora (Jin and Chen,
2008). Such data can only be used to train POS tag-
gers, but not for training the parsing model. Often
using a larger training set will result in a better POS
tagger. Second, the state-of-the-art POS tagging sys-
tems are often trained by sequence labeling models,
not parsing models.
</bodyText>
<figure confidence="0.7788035">
(2.1) wi−2ti, wi−1ti, witi, wi+1ti, wi+2ti
(2.2) wi−2wi−1ti, wi−1witi, wiwi+1ti,
wi+1wi+2ti wi−1wi+1ti
(2.3) c1(wi)ti, c2(wi)ti, c3(wi)ti, c−2(wi)ti
c−1(wi)ti
(2.4) c1(wi)c2(wi)ti, c−2(wi)c−1(wi)ti
(2.5) l(wi)ti
(2.5) ti−1ti
</figure>
<figureCaption confidence="0.18187075">
Table 2: Feature templates for POS tagging. wi is the
ith word in the sentence, ti is its POS tag. For a word w,
cj(w) is its jth character, c−j(w) is the last jth character,
and l(w) is its length.
</figureCaption>
<bodyText confidence="0.9999592">
The POS tagging problem is to assign a POS tag
t E T to each word in a sentence. We also use lin-
ear chain CRFs for POS tagging. Feature templates
shown in Table 2 are the same as those in (Qian
et al., 2010), which have been shown effective on
CTB corpus. Three feature sets are considered: (i)
word level features, including surrounding word uni-
grams, bigrams, and word length; (ii) character level
features, such as the first and last characters in the
words; (iii) transition features.
</bodyText>
<subsectionHeader confidence="0.997738">
3.3 Parsing Model
</subsectionHeader>
<bodyText confidence="0.994055846153846">
We choose discriminative models for parsing since it
is easy to handle unknown words by simply adding
character level features. Online structured learn-
ing algorithms were demonstrated to be effective for
training, such as stochastic optimization (Finkel et
al., 2008). In this study, we use averaged perceptron
algorithm for parameter estimation since it is easier
to implement and has competitive performance.
A Context Free Grammar (CFG) consists of (i) a
set of terminals; (ii) a set of nonterminals {Nk}; (i-
ii) a designated start symbol ROOT; and (iv) a set of
rules, {r = Ni —* (j}, where (j is a sequence of
terminals and nonterminals. In the parsing task, ter-
</bodyText>
<page confidence="0.990504">
503
</page>
<figure confidence="0.984705">
NP CP top state CP
NR NN NR NN
!&amp;quot; &amp;quot;# $% &amp;&apos;(
Shanghai customs Chongming office
NP
NR NN_NR
!&amp;quot;
IP
</figure>
<figureCaption confidence="0.987273">
Figure 2: Unary rule normalization. Nonterminal-yield
unary chains are collapsed to single unary rules. Identity
unary rules are added to spans that have no unary rule.
</figureCaption>
<figure confidence="0.999533764705883">
-Aif 2z-A
Last year realized
-Aif 2z-A
Last year realized
VP
bottom state
VP
NP VV
NT VV
NP
NT
VV
Shanghai NN NR_NN
&amp;quot;# NR NN
customs
$% &amp;&apos;(
Chongming office
</figure>
<figureCaption confidence="0.999975">
Figure 1: Parse tree binarization
</figureCaption>
<bodyText confidence="0.992558909090909">
minals are the words, and nonterminals are the POS
tags and phrase types. In this paper, nonterminal is
named state for short. A parse tree T of sentence
x can be factorized into several one-level subtrees,
each corresponding to a rule r.
In practice, binarization of rules is necessary to
obtain cubic parsing time. That is, the right hand
side of each rule should contain no more than 2 s-
tates. We used right branching binarization, as il-
lustrated in Figure 1. We did not use parent anno-
tation, since we found it degraded the performance
in our experiments (shown in Section 4). We used
the same preprocessing step as (Harper and Huang,
2009), collapsing all the allowed nonterminal-yield
unary chains to single unary rules. Therefore, all s-
pans in the binarized trees contain no more than one
unary rules. To facilitate decoding, we unify the for-
m of spans so that each span contains exactly one u-
nary rule. This is done by adding identity unary rules
(N —* N ) to spans that have no unary rule. These
identity unary rules will be removed in evaluation.
Hence, there are two states of a span: the top state
N and the bottom state N that correspond to the left
and right hand of the unary rule runary = N —* N
respectively, as shown in Figure 2.
Table 3 lists the feature templates we use for pars-
ing. There are 4 feature sets: (i) bottom state fea-
tures fbottom(i, j, x, Ni,j), which depend on the bot-
tom states; (ii) top state features ftop(i, j, x, Ni,j);
(iii) unary rule features funary(i, j, x, runary
i,j ), which
extract the transition information from bottom s-
tates to top states; (iv) binary rule features
</bodyText>
<equation confidence="0.9845845">
f (i j k, rbinary = N -� NiN)
binary &gt; &gt; &gt; , z j k z j z,k−1 + k,r
</equation>
<bodyText confidence="0.9852215">
where Ni,k−1, Nk,r are the top states of the left and
right children.
The score function for a sentence x with parse tree
T is defined as:
</bodyText>
<equation confidence="0.99501">
score(x, T) =
∑ θbottom &apos; fbottom(i, j, x, Ni,j)
Ni,jET
+ ∑ θtop &apos; ftop(i, j, x, Ni,j)
Ni,jET
unary
θunary &apos; funary(i, j, x, ri,j )
binary
θbinary &apos; fbinary(i, j, x, ri,j,k )
bina
i,j,kry ET
</equation>
<bodyText confidence="0.998915625">
where θbottom, θtop, θunary, θbinary are the weight
vectors of the four feature sets.
Given the training corpus {(xi, ˜Ti)}, the learning
task is to estimate the weight vectors so that for each
sentence xi, the gold standard tree ˜Ti achieves the
maximal score among all the possible trees. The per-
ceptron algorithm is guaranteed to find the solution
if it exists.
</bodyText>
<subsectionHeader confidence="0.986832">
3.4 Joint Decoding
</subsectionHeader>
<bodyText confidence="0.998575">
The three models described above are separately
trained to make parameter estimation feasible as
well as optimize each individual component. In test-
</bodyText>
<table confidence="0.932798375">
∑
+
runar y ET
i,j
∑
+
r
504
(3.1) Binary rule templates Xl Xm Xr lenl lenr
N --+ Nl + Nr Xl + Xm Xr wordm(ROOT)
Xl Xm−1Xr lenllenr
Xl Xm−1 Xr wordm−1(ROOT)
(3.2) Unary rule templates
N --+ N
(3.3) Bottom state templates Xl−1 Xr+1 Xr+2len
Xllen Xrlen XlXrwlllen XlXrwlrlen
Xl−2Xl−1 Xr+1len Xlwordl(LEAF) Xlwll(LEAF)
wllwlrXllen wllwlrXrlen −1 &lt; a, b &lt; 1
wordlwordrXlXrlen wordlwordrXlXr
Xl−1Xl(LEAF) Xl+1Xl(LEAF)
Xl+aXr+blen wordl+awordr+b
(3.3) Top state templates Xlwordl(LEAF) Xlwll(LEAF)
Xl−1Xl(LEAF) Xl+1Xl(LEAF) −1 &lt; a, b &lt; 1
Xl+aXr+blen wordl+awordr+b
</table>
<tableCaption confidence="0.998222">
Table 3: Feature templates for parsing, where X can be word, first and last character of word, first and last character
</tableCaption>
<bodyText confidence="0.94131675">
bigram of word, POS tag. Xl+a/Xr_a denotes the first/last ath X in the span, while Xl_a/Xr+a denotes the ath X
left/right to span. Xm is the first X of right child, and Xm_1 is the last X of the left child. len, lenl, lenr denote the
length of the span, left child and right child respectively. wl is the length of word. ROOT/LEAF means the template
can only generate the features for the root/initial span.
ing, we perform joint decoding to combine informa-
tion from the three models. Parameters of word seg-
mentation (θseg), POS tagging (θpos), and parsing
models (θparse = θbottom®top® unary®bianry) are s-
caled by three positive hyper-parameters α, β, and
γ respectively, which control their contribution in
the joint model. If α &gt;&gt; β &gt;&gt; γ, then the join-
t model is equivalent to a pipeline model, in which
there is no feedback from downstream models to up-
stream ones. For well tuned hyper-parameters, we
expect that segmentation and POS tagging results
can be improved by parsing information. The hyper-
parameters are tuned on development data. In the
following sections, for simplicity we drop α, β, γ,
and just use θseg, θpos, θparse to represent the scaled
parameters.
The basic idea of our decoding algorithm is to ex-
tend the CYK parsing algorithm so that it can deal
with transition features in POS tagging and segmen-
tation scores in word segmentation.
</bodyText>
<subsectionHeader confidence="0.691847">
3.4.1 Algorithm
</subsectionHeader>
<bodyText confidence="0.999867428571429">
The joint decoding algorithm is shown in Algo-
rithm 1. Given a sentence x = c1, ... , cl, Line 0
calculates the scores of all possible words in the sen-
tence using Eq(1). There are l(l + 1)/2 word candi-
dates in total.
Surrounding words are important features for
POS tagging and parsing; however, they are un-
available because segmentation is incomplete before
parsing. Therefore, we adopt pseudo surrounding
features by simply fixing the context words as the s-
ingle most likely ones. Given a word candidate wi,j
from ci to cj, its previous word s′ is the rightmost
one in the best word sequence of c1, ... , ci−1, which
can be obtained by dynamic programming. Recur-
sively, the second word left to wi,j is the previous
word of s′. The next word of wi,j is defined similar-
ly. In Line 1, we use bidirectional Viterbi decoding
to obtain all the surrounding words. In the forward
direction, the algorithm starts from the first charac-
ter boundary to the last, and finds the best previous
word for the ith character boundary bi. In the back-
ward direction, the algorithm starts from right to left,
and finds the best next word of each bi.
In Line 2, for each word candidate, we can calcu-
late the score of each POS tag using state features in
the POS tagging model, since the context words are
available now. The score function of word wi,j with
POS tag t is:
</bodyText>
<equation confidence="0.996245">
scoreseg®pos(x, i, j, t) =
scoreseg(x, i, j) + θpos · fpos(x, wi,j, t) (2)
</equation>
<bodyText confidence="0.9283595">
In Line 3, POS tags of surrounding words can
be obtained similarly using bidirectional decoding.
</bodyText>
<page confidence="0.989885">
505
</page>
<table confidence="0.8025405">
Algorithm 1 Joint Word Segmentation, POS tagging, and Parsing Algorithm
Input: Sentence x = c1, ... , cl, beam size B, scaled word segmentation model, POS tagging model and
parsing model.
Output: Word sequence, POS tag sequence, and parse tree
0: d0 &lt; i &lt; j &lt; l − 1, calculate scoreseg(x, i, j) using Equation (1)
1: For each character boundary bi, 0 &lt; i &lt; l, get the best previous and next words of bi using bidirectional
Viterbi decoding
2: d0 &lt; i &lt; j &lt; l − 1, t E T , calculate scoreseg®pos(x, i, j, t) using Equation (2)
</table>
<listItem confidence="0.855514">
3: dbi, 0 &lt; i &lt; l, t E T , get the best POS tags of words left/right to bi using bidirectional viterbi
decoding.
4: For each word candidate wi,j, 0 &lt; i &lt; j &lt; l − 1
5: For each bottom state N, POS tag t E T &lt; step 1 (Line 5-7): get bottom states
6: scorebottom(x, i, j, wi,j, t, N) = scoreseg®pos(x, i, j, t) + Bbottom &apos; fbottom(x, i, j, wi,j, t, N)
7: Keep B best scorebottom.
8: For each top state N &lt; step 2 (Line 8-9): get top states
9: scoretop(x, i, j, wi,j, t, N) = maxN {scorebottom(x, i, j, wi,j, t, N) + Btop &apos; ftop(x, i, j, wi,j, t, N)
+Bunary &apos; funary(x, i, j, wi,j, t, N , N)}
10: for i = 0,...,l − 1 do
11: for width = 1,...,l − 1 do
12: j = i + width
13: for k = i + 1,...,j do
14: scorebottom(x, i, j, w, t, N) = maxl,r {scoretop(x, i, k − 1, wl, tl, Nl) + scoretop(x, k, j, wr, tr, Nr)
+Bbinary &apos; fbinary(x, i, j, k, w, t, N , Nr + Nr) + Bpos &apos; fpos(tlast
</listItem>
<equation confidence="0.902711666666667">
l ,tfirst
r )
+Bbottomfbottom(x, i, j, w, t, N)}
</equation>
<listItem confidence="0.682309285714286">
15: Keep B best scorebottom &lt; step 1 (Line 14-15): get bottom states
16: For each top state N &lt; step 2 (Line 16-17): get top states
17: scoretop (x, i, j, w, t, N) = maxN {scorebottom (x, i, j, w, t, N)
+Bunary &apos; funary (x, i, j, w, t, N , N) }
18: end for
19: end for
20: end for
</listItem>
<table confidence="0.6302365">
Line 0 1 2 3 6 9 14 15 Total Bound(w.r.t. l)
Complexity l2 l2 |T |l2 |T |2l2 |T |Ml2 BMl2 l3MB2 BMl2 l3MB2
</table>
<tableCaption confidence="0.994712">
Table 4: Complexity Analysis of Algorithm 1.
</tableCaption>
<bodyText confidence="0.999783230769231">
That is, for wiJ with POS tag t, we use Viterbi algo-
rithm to search the optimal POS tags of its left and
right words.
In Lines 4-9, each word was initialized as a basic
span. A span structure in the joint model is a 6-tuple:
5(i, j, w, t, N, N), where i, j are the boundary in-
dices, w, t are the word sequence and POS sequence
within the span respectively, and N, N are the bot-
tom and top states. There are two types of surround-
ing n-grams: one is inside the span, for example, the
first word of a span, which can be obtained from w;
the other is outside the span, for example, the pre-
vious word of a span, which is obtained from the
pseudo context information. The score of a basic s-
pan depends on its corresponding word and POS pair
score, and the weights of the active state and unary
features.
To avoid enumerating the combination of the bot-
tom and top states, initialization for each span is di-
vided into 2 steps. In the first step, the score of ev-
ery bottom state is calculated using bottom state fea-
tures, and only the B best states are maintained (see
Line 6-7). In the second step, top state features and
unary rule features are used to get the score of each
top state (Line 9), and only the top B states are pre-
served.
</bodyText>
<page confidence="0.996013">
506
</page>
<bodyText confidence="0.819456">
Similarly, there are two steps in the merge opera-
tion: S(i, j, w, t, N, N) = Sl(i, k, wl, tl, Nl, Nl) +
Sr(k + 1, j, wr, tr, Nr, Nr). The score of the bot-
tom state N is calculated using binary features
fbinary(x, i, j, k, w, t, N —* Nr+Nr), bottom state
features fbottom(x, i, j, w, t, N), and POS tag transi-
tion features that depend on the boundary POS tags
of Sl and Sr. See Line 14 of Algorithm 1, where
tlast
l and tfirst
r are the POS tags of the last word in
the left child span and the first word in the right child
span respectively.
</bodyText>
<subsectionHeader confidence="0.988745">
3.4.2 Complexity analysis
</subsectionHeader>
<bodyText confidence="0.99977525">
Given a sentence of length l, the complexity for
each line of Algorithm 1 is listed in Table 4, where
|T  |is the size of POS tag set, M is the number of
states, and B is the beam size.
</bodyText>
<sectionHeader confidence="0.999891" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.996515">
4.1 Data
</subsectionHeader>
<bodyText confidence="0.9908156">
For comparison with other systems, we use the CT-
B5 corpus, which has been studied for Chinese word
segmentation, POS tagging and parsing. We use the
standard train/develop/test split of the data. Details
are shown in Table 5.
</bodyText>
<table confidence="0.9993998">
CTB files # sent. # words
Training 1-270 18089 493,939
400-1151
Develop 301-325 350 6,821
Test 271-300 348 8,008
</table>
<tableCaption confidence="0.999925">
Table 5: Training, development, and test data of CTB 5.
</tableCaption>
<bodyText confidence="0.99976355">
For joint word segmentation and POS tagging, a
word is correctly predicted if both the boundaries
and the POS tag are correctly identified. For joint
segmentation, POS tagging, and parsing task, when
calculating the bracket scores using existing parseval
tools, we need to consider possible word segmenta-
tion errors. To do this, we add the word boundary
information in states – a bracket is correct only if
its boundaries, label and word segmentation are all
correct. One example is shown in Figure 3. Notice
that identity unary rules are removed during evalua-
tion. The basic spans are characters, not words, be-
cause the number of words in reference and predic-
tion may be different. POS tags are removed since
they do not affect the bracket scores. If the segmen-
tation is perfect, then the bracket scores of the mod-
ified tree are exactly the same as the original tree.
This is similar to evaluating parsing performance on
speech transcripts with automatic sentence segmen-
tation (Roark et al., 2006).
</bodyText>
<equation confidence="0.67221475">
NP(0,2,5)
NR NN - - - - -
!&amp;quot; #$% ! &amp;quot; # $ %
Shanghai office Shanghai office
</equation>
<figureCaption confidence="0.9921522">
Figure 3: Boundary information is added to states to cal-
culate the bracket scores in the face of word segmentation
errors. Left: the original parse tree, Right: the converted
parse tree. The numbers in the brackets are the indices of
the character boundaries based on word segmentation.
</figureCaption>
<figure confidence="0.381745">
NP
</figure>
<subsectionHeader confidence="0.990255">
4.2 Evaluation Metric
</subsectionHeader>
<bodyText confidence="0.993341615384615">
We evaluate system performance on the individual
tasks, as well as the joint tasks.&apos; For word segmen-
tation, three metrics are used for evaluation: pre-
cision (P), recall (R), and F-score (F) defined by
2PR/(P+R). Precision is the percentage of correct
words in the system output. Recall is the percent-
age of words in gold standard annotations that are
correctly predicted. For parsing, we use the stan-
dard parseval evaluation metrics: bracketing preci-
sion, recall and F-score.
&apos;Note that the joint task refers to automatic segmentation
and tagging/parsing. It can be achieved using a pipeline system
or our joint decoding method.
</bodyText>
<subsectionHeader confidence="0.99925">
4.3 Parameter Estimation
</subsectionHeader>
<bodyText confidence="0.999969166666667">
We train three submodels using the gold features,
that is, POS tagger is trained using the perfect seg-
mentation, and parser is trained using perfect seg-
mentation and POS tags. Some studies reported
that better performance may be achieved by train-
ing subsequent models using representative output
of the preceding models (Che et al., 2009). Hence
for comparison we trained another parser using auto-
matically generated POS tags obtained from 10-fold
cross validation, but did not find significant differ-
ence between these two parsers when testing on the
perfectly segmented development dataset. Therefore
</bodyText>
<page confidence="0.991634">
507
</page>
<bodyText confidence="0.989072615384615">
we use the parser trained with perfect POS tags for
the joint task.
Three hyper-parameters, α, 0, and -y, are tuned
on development data using a heuristic search. Pa-
rameters that achieved the best joint parsing result
are selected. In the search, we fixed -y = 1 and
varied α, 0. First, we set 0 = 1, and enumerate
α = 4, 2, 1, 2, ... , and choose the best α*. Then,
we setα = α* and vary 0 = 4, 2, 1, 2, ... , and
select the best 0*.
Table 6 lists the parameters we used for training
the submodels, as well as the hyper-parameters for
joint decoding.
</bodyText>
<table confidence="0.999583076923077">
Model Parameter Value
Character based Gaussian prior 0.01
word segmentor # Feature 3,875,802
Word based Gaussian prior 0.01
word segmentor # Feature 312,533
POS tagger Gaussian prior 0.1
# Feature 48,608,802
Parser Iteration Number 10
# Feature 49,369,843
Hyper-parameter α 4
Hyper-parameter 0 0.5
Joint Hyper-parameter y 1
Beam Size B 20
</table>
<tableCaption confidence="0.999447">
Table 6: Parameters used in our system.
</tableCaption>
<subsectionHeader confidence="0.998458">
4.4 Experimental Results
</subsectionHeader>
<bodyText confidence="0.999829">
In this section we first show that our sub-models are
better than or comparable to state-of-the-art systems,
and then the joint model is superior to the pipeline
approach.
</bodyText>
<subsectionHeader confidence="0.871277">
4.4.1 Evaluating Sub-models
</subsectionHeader>
<bodyText confidence="0.982866166666667">
Table 7 shows word segmentation results using
our word segmentation submodel, in comparison to
a few state-of-the-art systems. For our segmentor,
we show results for two variants: one removes tran-
sition features as described in Section 3.1, the other
uses CRFs to learn the weights of transition features.
We can see that our system is competitive with al-
l the others except Sun’s that used additional idiom
resources. Our two word segmentors have similar
performance. Since the one without transition fea-
tures can be naturally integrated into the joint sys-
tem, we use it in the following joint tasks.
</bodyText>
<table confidence="0.994523444444444">
System P R F
(Jiang et al., 2008b) - - 97.74
(Jiang et al., 2008a) - 97.85
(Kruengkrai et al., 2009) 97.46 98.29 97.87
(Zhang and Clark, 2010) - - 97.78
(Zhang and Clark, 2011) - - 97.78
(Sun, 2011) - - 98.17
Ours (w/o transition features) 97.45 98.24 97.85
Ours (with transition features) 97.44 98.23 97.84
</table>
<tableCaption confidence="0.998458">
Table 7: Word segmentation results.
</tableCaption>
<bodyText confidence="0.9998575">
For the POS tagging only task that takes gold s-
tandard word segmentation as input, we have two
systems. One uses the linear chain CRFs as de-
scribed in Section 3.2, the other is obtained using the
parser described in Section 3.3 – the parser gener-
ates POS tag hypotheses when POS tag features are
not used. The POS tagging accuracy is 95.53% and
95.10% using these two methods respectively. The
better performance from the former system may be
because the local label dependency is more helpful
for POS tagging than the long distance dependencies
that might be noisy. This result also confirms our
choice of using an independent POS tagger for the
sub-model, rather than relying on a parser for POS
tagging. However, since there are no reported results
for this setup, we demonstrate the competence of our
POS tagger using the joint word segmentation and
POS tagging task. Table 8 shows the performance of
a few systems along with ours, all using the pipeline
approach where automatic segmentation is followed
by POS tagging. We can see that our POS tagger is
comparable to the others.
</bodyText>
<table confidence="0.9998595">
System P R F
(Jiang et al., 2008b)
(Jiang et al., 2008a) - - 93.41
(Kruengkrai et al., 2009) 93.28 94.07 93.67
(Zhang and Clark, 2010) - - 93.67
(Zhang and Clark, 2011) - - 93.67
(Sun, 2011) - - 94.02
Ours (pipeline) 93.10 93.96 93.53
</table>
<tableCaption confidence="0.991028">
Table 8: Results for the joint word segmentation and POS
tagging task.
</tableCaption>
<bodyText confidence="0.992180333333333">
For parsing, Table 9 presents the parsing result
on gold standard segmented sentence. Notice that
the result of (Harper and Huang, 2009; Zhang and
</bodyText>
<page confidence="0.991699">
508
</page>
<bodyText confidence="0.987705333333333">
Clark, 2011) are not directly comparable to ours, as
they used a different data split. The best published
system result on CTB5 is Petrov and Klein’s, which
used PCFG with latent Variables. Our system per-
forms better mainly because it benefits from a large
amount of features.
</bodyText>
<table confidence="0.99938375">
System LP LR F
(Petrov and Klein, 2007) 84.8 81.9 83.3
(Jiang et al., 2009)
(Harper and Huang, 2009)* 83.22 82.84 83.03
(Zhang and Clark, 2011)* 78.6 78.0 78.3
Ours 84.57 83.68 84.13
Ours (w/ parent annotation) 83.35 82.73 83.04
Ours (no POS tag feature) 83.49 82.97 83.23
</table>
<tableCaption confidence="0.9869455">
Table 9: Parsing results using gold standard word seg-
mentation.
</tableCaption>
<bodyText confidence="0.999857814814815">
For our parser, besides the model described in
Section 3.3, we tried two variations: one does not
use the automatic POS tag features, the other one is
learned on the parent annotated training data. The
results in Table 9 show that there is a performance
degradation when using parent annotation. This may
be due to the introduction of a large number of s-
tates, resulting in sparse features. We also notice
that with the help of the POS tag information, even
automatically generated, the parser gained 0.9% im-
provement in F-score. This demonstrates the advan-
tage of using a better independent POS tagger and
incorporating it in parsing.
Finally Table 10 shows the results for the three
tasks using our joint decoding method in compari-
son to the pipeline method. We can see that the joint
model outperforms the pipeline one. This is mainly
because of a better parsing module as well as join-
t decoding. In the table we also include results of
(Jiang et al., 2009), which is the only reported join-
t parsing result we found using the same data split
on CTB5. They achieved 80.28% parsing F-score
using automatic word segmentation. Their adapted
system Jiang09+ leveraged additional corpus to im-
prove Chinese word segmentation, resulting in an F-
score of 81.07%. Our system has better performance
than these.
</bodyText>
<table confidence="0.999295777777778">
System Task P R F
Jiang09 Parse - - 80.28
Jiang09+ Parse - - 81.07
Ours Seg. 97.45 98.24 97.85
Pipeline POS 93.10 93.96 93.53
Parse 81.87 81.65 81.76
Ours Seg. 97.56 98.36 97.96
Joint POS 93.43 94.20 93.81
Parse 83.03 82.66 82.85
</table>
<tableCaption confidence="0.996556">
Table 10: Results for the joint segmentation, tagging, and
parsing task using pipeline and joint models.
</tableCaption>
<subsectionHeader confidence="0.816333">
4.5 Error Analysis
</subsectionHeader>
<bodyText confidence="0.999413">
We compared the results from the pipeline and our
joint decoding systems in order to understand the
impact of the joint model on word segmentation and
POS tagging. We notice that the joint model tend to
generate more words than the pipeline model. For
example, “巴尔一行” is one word in the pipeline
model, but correctly segmented as two words “巴
尔/一行” in the joint model. This tendency of seg-
mentation also makes it fail to recognize some long
words, especially OOV words. For example, “事
实上” is segmented as “事实/上”. In the data set,
we find that, the joint model corrected 10 missing
boundaries over the pipeline method, and introduced
3 false positive segmentation errors.
For the analysis of POS tags, we only examined
the words that are correctly segmented by both the
pipeline and the joint models. Table 11 shows the
increase and decrease of error patterns of the joint
model over the pipeline POS tagger. An error pat-
tern “X -* Y” means that the word whose true tag is
‘X’ is assigned a tag ‘Y’. All the patterns are ranked
in descending order of the reduction/increase of the
error number. We can see that the joint model has a
clear advantage in the disambiguation of {VV, NN}
and {DEG, DEC}, which results in the overall im-
proved performance. In contrast, the joint method
performs worse on ambiguous POS pairs such as
{NN, NR}. This observation is similar to those re-
ported by (Li et al., 2011; Hatori et al., 2011).
</bodyText>
<sectionHeader confidence="0.999178" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.937126333333333">
In this paper, we proposed a new algorithm for joint
Chinese word segmentation, POS tagging, and pars-
ing. Our algorithm is an extension of the CYK
</bodyText>
<page confidence="0.992838">
509
</page>
<table confidence="0.999778">
error pattern # ↓ error pattern # ↑
NN→ VV 47 19 NN→ NR 15 12
VV→ NN 42 13 NR→ NN 7 5
DEG→ DEC 23 10 JJ→ P 1 4
NN→ JJ 29 8 NN→ DT 2 4
DEC→ DEG 11 4 P→ VV 3 2
JJ→ NN 12 4 AD→ NN 1 2
</table>
<tableCaption confidence="0.981855">
Table 11: POS tagging error patterns. # means the error
</tableCaption>
<bodyText confidence="0.978502105263158">
number of the corresponding pattern made by the pipeline
tagging model. ↓ and ↑ mean the error number reduced
or increased by the joint model.
parsing method. The sub-models are independently
trained for the three tasks to reduce model complex-
ity and optimize individual sub-models. Our exper-
iments demonstrate the advantage of the joint mod-
els. In the future work, we will compare this joint
model to the pipeline approach that uses multiple
candidates or soft decisions in the early modules.
We will also investigate methods for joint learning
as well as ways to speed up the joint decoding algo-
rithm.
Acknowledgments The authors thank Zhongqiang
Huang for his help with experiments. This work
is partly supported by DARPA under Contrac-
t No. HR0011-12-C-0016. Any opinions expressed
in this material are those of the authors and do not
necessarily reflect the views of DARPA.
</bodyText>
<sectionHeader confidence="0.998984" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999927208333334">
Wanxiang Che, Zhenghua Li, Yongqiang Li, Yuhang
Guo, Bing Qin, and Ting Liu. 2009. Multilingual
dependency-based syntactic and semantic parsing. In
Proceedings of CoNLL 09, pages 49–54.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings of
EMNLP 2002, pages 1–8.
Jenny Rose Finkel, Alex Kleeman, and Christopher D.
Manning. 2008. Efficient, feature-based, condition-
al random field parsing. In Proceedings of ACL-08:
HLT, pages 959–967.
Yoav Goldberg and Reut Tsarfaty. 2008. A single gener-
ative model for joint morphological segmentation and
syntactic parsing. In Proceedings of ACL 2008: HLT,
pages 371–379.
Spence Green and Christopher D. Manning. 2010. Better
arbic parsing: Baselines, evaluations, and analysis. In
Proceedings of Coling 2010, pages 394–402.
Mary Harper and Zhongqiang Huang. 2009. Chinese
statistical parsing. In Gale Book.
Jun Hatori, Takuya Matsuzaki, Yusuke Miyao, and
Jun’ichi Tsujii. 2011. Incremental joint pos tagging
and dependency parsing in chinese. In Proceedings of
IJCNLP 2011, pages 1216–1224.
Wenbin Jiang, Liang Huang, Qun Liu, and Yajuan L¨u.
2008a. A cascaded linear model for joint chinese word
segmentation and part-of-speech tagging. In Proceed-
ings of ACL 2008: HLT, pages 897–904.
Wenbin Jiang, Haitao Mi, and Qun Liu. 2008b. Word lat-
tice reranking for chinese word segmentation and part-
of-speech tagging. In Proceedings of Coling 2008,
pages 385–392.
Wenbin Jiang, Liang Huang, and Qun Liu. 2009. Au-
tomatic adaptation of annotation standards: Chinese
word segmentation and pos tagging – a case study. In
Proceedings of ACL-IJCNLP 2009, pages 522–530.
Guangjin Jin and Xiao Chen. 2008. The fourth interna-
tional chinese language processing bakeoff: Chinese
word segmentation, named entity recognition and chi-
nese pos tagging. In Proceedings of Sixth SIGHAN
Workshop on Chinese Language Processing.
Canasai Kruengkrai, Kiyotaka Uchimoto, Jun’ichi Kaza-
ma, Yiou Wang, Kentaro Torisawa, and Hitoshi Isa-
hara. 2009. An error-driven word-character hybrid
model for joint chinese word segmentation and pos
tagging. In Proceedings ofACL 2009, pages 513–521.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic mod-
els for segmenting and labeling sequence data. In Pro-
ceedings of ICML 2001, pages 282–289.
John Lee, Jason Naradowsky, and David A. Smith. 2011.
A discriminative model for joint morphological disam-
biguation and dependency parsing. In Proceedings A-
CL 2011: HLT, pages 885–894.
Zhenghua Li, Min Zhang, Wanxiang Che, Ting Liu, Wen-
liang Chen, and Haizhou Li. 2011. Joint models for
chinese pos tagging and dependency parsing. In Pro-
ceedings of EMNLP 2011, pages 1180–1191.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings of NAACL
2007, pages 404–411.
Xian Qian, Qi Zhang, Yaqian Zhou, Xuanjing Huang, and
Lide Wu. 2010. Joint training and decoding using
virtual nodes for cascaded segmentation and tagging
tasks. In Proceedings of EMNLP 2010, pages 187–
195.
Brian Roark, Mary Harper, Eugene Charniak, Bonnie
Dorr, Mark Johnson, Jeremy G. Kahn, Yang Liu, Mari
Ostendorf, John Hale, Anna Krasnyanskaya, Matthew
Lease, Izhak Shafran, Matthew Snover, Robin Stew-
art, Lisa Yung, and Lisa Yung. 2006. Sparseval: E-
</reference>
<page confidence="0.964539">
510
</page>
<reference confidence="0.993299666666667">
valuation metrics for parsing speech. In Proceedings
Language Resources and Evaluation (LREC).
Sunita Sarawagi and William W. Cohen. 2004. Semi-
markov conditional random fields for information ex-
traction. In Proceedings of NIPS 2004.
Weiwei Sun. 2011. A stacked sub-word model for join-
t chinese word segmentation and part-of-speech tag-
ging. In Proceedings of ACL 2011, pages 1385–1394.
Yue Zhang and Stephen Clark. 2008. Joint word seg-
mentation and POS tagging using a single perceptron.
In Proceedings of ACL 2008: HLT, pages 888–896.
Yue Zhang and Stephen Clark. 2010. A fast decoder for
joint word segmentation and POS-tagging using a sin-
gle discriminative model. In Proceedings of EMNLP
2010, pages 843–852.
Yue Zhang and Stephen Clark. 2011. Syntactic process-
ing using the generalized perceptron and beam search.
Comput. Linguist., 37(1):105–151.
Ruiqiang Zhang, Genichiro Kikui, and Eiichiro Sumi-
ta. 2006. Subword-based tagging for confidence-
dependent chinese word segmentation. In Proceedings
of the COLING/ACL 2006, pages 961–968.
Hai Zhao and Chunyu Kit. 2008. Unsupervised segmen-
tation helps supervised learning of character tagging
forword segmentation and named entity recognition.
In Proceedings of Sixth SIGHAN Workshop on Chinese
Language Processing, pages 106–111.
</reference>
<page confidence="0.997691">
511
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.975611">
<title confidence="0.999081">Joint Chinese Word Segmentation, POS Tagging and Parsing</title>
<author confidence="0.993985">Xian Qian Yang Liu</author>
<affiliation confidence="0.999914">Computer Science Department The University of Texas at Dallas</affiliation>
<email confidence="0.99969">qx,yangl@hlt.utdallas.edu</email>
<abstract confidence="0.999157428571429">In this paper, we propose a novel decoding algorithm for discriminative joint Chinese word segmentation, part-of-speech (POS) tagging, and parsing. Previous work often used a pipeline method – Chinese word segmentation followed by POS tagging and parsing, which suffers from error propagation and is unable to leverage information in later modules for earlier components. In our approach, we train the three individual models separately during training, and incorporate them together in a unified framework during decoding. We extend the CYK parsing algorithm so that it can deal with word segmentation and POS tagging features. As far as we know, this is the first work on joint Chinese word segmentation, POS tagging and parsing. Our experimental results on Chinese Tree Bank 5 corpus show that our approach outperforms the state-of-the-art pipeline system.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Wanxiang Che</author>
<author>Zhenghua Li</author>
<author>Yongqiang Li</author>
<author>Yuhang Guo</author>
<author>Bing Qin</author>
<author>Ting Liu</author>
</authors>
<title>Multilingual dependency-based syntactic and semantic parsing.</title>
<date>2009</date>
<booktitle>In Proceedings of CoNLL 09,</booktitle>
<pages>49--54</pages>
<contexts>
<context position="25039" citStr="Che et al., 2009" startWordPosition="4395" endWordPosition="4398"> parsing, we use the standard parseval evaluation metrics: bracketing precision, recall and F-score. &apos;Note that the joint task refers to automatic segmentation and tagging/parsing. It can be achieved using a pipeline system or our joint decoding method. 4.3 Parameter Estimation We train three submodels using the gold features, that is, POS tagger is trained using the perfect segmentation, and parser is trained using perfect segmentation and POS tags. Some studies reported that better performance may be achieved by training subsequent models using representative output of the preceding models (Che et al., 2009). Hence for comparison we trained another parser using automatically generated POS tags obtained from 10-fold cross validation, but did not find significant difference between these two parsers when testing on the perfectly segmented development dataset. Therefore 507 we use the parser trained with perfect POS tags for the joint task. Three hyper-parameters, α, 0, and -y, are tuned on development data using a heuristic search. Parameters that achieved the best joint parsing result are selected. In the search, we fixed -y = 1 and varied α, 0. First, we set 0 = 1, and enumerate α = 4, 2, 1, 2, .</context>
</contexts>
<marker>Che, Li, Li, Guo, Qin, Liu, 2009</marker>
<rawString>Wanxiang Che, Zhenghua Li, Yongqiang Li, Yuhang Guo, Bing Qin, and Ting Liu. 2009. Multilingual dependency-based syntactic and semantic parsing. In Proceedings of CoNLL 09, pages 49–54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<pages>1--8</pages>
<contexts>
<context position="2492" citStr="Collins, 2002" startWordPosition="387" endWordPosition="388">t modules. Intuitively a joint model that performs the three tasks together should help the system make the best decisions. In this paper, we propose a unified model for joint Chinese word segmentation, POS tagging, and parsing. Three sub-models are independently trained using the state-of-the-art methods. We do not use the joint inference algorithm for training because of the high complexity caused by the large amount of parameters. We use linear chain Conditional Random Fields (CRFs) (Lafferty et al., 2001) to train the word segmentation model and POS tagging model, and averaged perceptron (Collins, 2002) to learn the parsing model. During decoding, parameters of each sub-model are scaled to represent its importance in the joint model. Our decoding algorithm is an extension of CYK parsing. Initially, weights of all possible words together with their POS tags are calculated. When searching the parse tree, the word and POS tagging features are dynamically generated and the transition information of POS tagging is considered in the span merge operation. Experiments are conducted on Chinese Tree Bank (CTB) 5 dataset, which is widely used for Chinese word segmentation, POS tagging and parsing. We c</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms. In Proceedings of EMNLP 2002, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Alex Kleeman</author>
<author>Christopher D Manning</author>
</authors>
<title>Efficient, feature-based, conditional random field parsing.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>959--967</pages>
<contexts>
<context position="11394" citStr="Finkel et al., 2008" startWordPosition="1862" endWordPosition="1865"> Table 2 are the same as those in (Qian et al., 2010), which have been shown effective on CTB corpus. Three feature sets are considered: (i) word level features, including surrounding word unigrams, bigrams, and word length; (ii) character level features, such as the first and last characters in the words; (iii) transition features. 3.3 Parsing Model We choose discriminative models for parsing since it is easy to handle unknown words by simply adding character level features. Online structured learning algorithms were demonstrated to be effective for training, such as stochastic optimization (Finkel et al., 2008). In this study, we use averaged perceptron algorithm for parameter estimation since it is easier to implement and has competitive performance. A Context Free Grammar (CFG) consists of (i) a set of terminals; (ii) a set of nonterminals {Nk}; (iii) a designated start symbol ROOT; and (iv) a set of rules, {r = Ni —* (j}, where (j is a sequence of terminals and nonterminals. In the parsing task, ter503 NP CP top state CP NR NN NR NN !&amp;quot; &amp;quot;# $% &amp;&apos;( Shanghai customs Chongming office NP NR NN_NR !&amp;quot; IP Figure 2: Unary rule normalization. Nonterminal-yield unary chains are collapsed to single unary rule</context>
</contexts>
<marker>Finkel, Kleeman, Manning, 2008</marker>
<rawString>Jenny Rose Finkel, Alex Kleeman, and Christopher D. Manning. 2008. Efficient, feature-based, conditional random field parsing. In Proceedings of ACL-08: HLT, pages 959–967.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Goldberg</author>
<author>Reut Tsarfaty</author>
</authors>
<title>A single generative model for joint morphological segmentation and syntactic parsing.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL 2008: HLT,</booktitle>
<pages>371--379</pages>
<contexts>
<context position="4786" citStr="Goldberg and Tsarfaty, 2008" startWordPosition="747" endWordPosition="750">provement over the pipeline method. Recently, joint tagging and dependency parsing has been studied as well (Li et al., 2011; Lee et al., 2011). Previous research has showed that word segmentation has a great impact on parsing accuracy in the pipeline method (Harper and Huang, 2009). In (Jiang et al., 2009), additional data was used to improve Chinese word segmentation, which resulted in significant improvement on the parsing task using the pipeline framework. Joint segmentation and parsing was also investigated for Arabic (Green and Manning, 2010). A study that is closely related to ours is (Goldberg and Tsarfaty, 2008), where a single generative model was proposed for joint morphological segmentation and syntactic parsing for Hebrew. Different from that work, we use a discriminative model, which benefits from large amounts of features and is easier to deal with unknown words. Another main difference is that, besides segmentation and parsing, we also incorporate the POS tagging model into the CYK parsing framework. 3 Methods For a given Chinese sentence, our task is to generate the word sequence, its POS tag sequence, and the parse tree (constituent parsing). A joint model is expected to make more optimal de</context>
</contexts>
<marker>Goldberg, Tsarfaty, 2008</marker>
<rawString>Yoav Goldberg and Reut Tsarfaty. 2008. A single generative model for joint morphological segmentation and syntactic parsing. In Proceedings of ACL 2008: HLT, pages 371–379.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Spence Green</author>
<author>Christopher D Manning</author>
</authors>
<title>Better arbic parsing: Baselines, evaluations, and analysis.</title>
<date>2010</date>
<booktitle>In Proceedings of Coling</booktitle>
<pages>394--402</pages>
<contexts>
<context position="4712" citStr="Green and Manning, 2010" startWordPosition="734" endWordPosition="737">ng et al., 2008b). These joint models showed about 0.2 − 1% F-score improvement over the pipeline method. Recently, joint tagging and dependency parsing has been studied as well (Li et al., 2011; Lee et al., 2011). Previous research has showed that word segmentation has a great impact on parsing accuracy in the pipeline method (Harper and Huang, 2009). In (Jiang et al., 2009), additional data was used to improve Chinese word segmentation, which resulted in significant improvement on the parsing task using the pipeline framework. Joint segmentation and parsing was also investigated for Arabic (Green and Manning, 2010). A study that is closely related to ours is (Goldberg and Tsarfaty, 2008), where a single generative model was proposed for joint morphological segmentation and syntactic parsing for Hebrew. Different from that work, we use a discriminative model, which benefits from large amounts of features and is easier to deal with unknown words. Another main difference is that, besides segmentation and parsing, we also incorporate the POS tagging model into the CYK parsing framework. 3 Methods For a given Chinese sentence, our task is to generate the word sequence, its POS tag sequence, and the parse tre</context>
</contexts>
<marker>Green, Manning, 2010</marker>
<rawString>Spence Green and Christopher D. Manning. 2010. Better arbic parsing: Baselines, evaluations, and analysis. In Proceedings of Coling 2010, pages 394–402.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mary Harper</author>
<author>Zhongqiang Huang</author>
</authors>
<title>Chinese statistical parsing.</title>
<date>2009</date>
<booktitle>In Gale Book.</booktitle>
<contexts>
<context position="4441" citStr="Harper and Huang, 2009" startWordPosition="692" endWordPosition="695">evious joint models mainly focus on word segmentation and POS tagging task, such as the virtual nodes method (Qian et al., 2010), cascaded linear model (Jiang et al., 2008a), perceptron (Zhang and Clark, 2008), sub-word based stacked learning (Sun, 2011), reranking (Jiang et al., 2008b). These joint models showed about 0.2 − 1% F-score improvement over the pipeline method. Recently, joint tagging and dependency parsing has been studied as well (Li et al., 2011; Lee et al., 2011). Previous research has showed that word segmentation has a great impact on parsing accuracy in the pipeline method (Harper and Huang, 2009). In (Jiang et al., 2009), additional data was used to improve Chinese word segmentation, which resulted in significant improvement on the parsing task using the pipeline framework. Joint segmentation and parsing was also investigated for Arabic (Green and Manning, 2010). A study that is closely related to ours is (Goldberg and Tsarfaty, 2008), where a single generative model was proposed for joint morphological segmentation and syntactic parsing for Hebrew. Different from that work, we use a discriminative model, which benefits from large amounts of features and is easier to deal with unknown</context>
<context position="12897" citStr="Harper and Huang, 2009" startWordPosition="2133" endWordPosition="2136"> and nonterminals are the POS tags and phrase types. In this paper, nonterminal is named state for short. A parse tree T of sentence x can be factorized into several one-level subtrees, each corresponding to a rule r. In practice, binarization of rules is necessary to obtain cubic parsing time. That is, the right hand side of each rule should contain no more than 2 states. We used right branching binarization, as illustrated in Figure 1. We did not use parent annotation, since we found it degraded the performance in our experiments (shown in Section 4). We used the same preprocessing step as (Harper and Huang, 2009), collapsing all the allowed nonterminal-yield unary chains to single unary rules. Therefore, all spans in the binarized trees contain no more than one unary rules. To facilitate decoding, we unify the form of spans so that each span contains exactly one unary rule. This is done by adding identity unary rules (N —* N ) to spans that have no unary rule. These identity unary rules will be removed in evaluation. Hence, there are two states of a span: the top state N and the bottom state N that correspond to the left and right hand of the unary rule runary = N —* N respectively, as shown in Figure</context>
<context position="28935" citStr="Harper and Huang, 2009" startWordPosition="5066" endWordPosition="5069"> a few systems along with ours, all using the pipeline approach where automatic segmentation is followed by POS tagging. We can see that our POS tagger is comparable to the others. System P R F (Jiang et al., 2008b) (Jiang et al., 2008a) - - 93.41 (Kruengkrai et al., 2009) 93.28 94.07 93.67 (Zhang and Clark, 2010) - - 93.67 (Zhang and Clark, 2011) - - 93.67 (Sun, 2011) - - 94.02 Ours (pipeline) 93.10 93.96 93.53 Table 8: Results for the joint word segmentation and POS tagging task. For parsing, Table 9 presents the parsing result on gold standard segmented sentence. Notice that the result of (Harper and Huang, 2009; Zhang and 508 Clark, 2011) are not directly comparable to ours, as they used a different data split. The best published system result on CTB5 is Petrov and Klein’s, which used PCFG with latent Variables. Our system performs better mainly because it benefits from a large amount of features. System LP LR F (Petrov and Klein, 2007) 84.8 81.9 83.3 (Jiang et al., 2009) (Harper and Huang, 2009)* 83.22 82.84 83.03 (Zhang and Clark, 2011)* 78.6 78.0 78.3 Ours 84.57 83.68 84.13 Ours (w/ parent annotation) 83.35 82.73 83.04 Ours (no POS tag feature) 83.49 82.97 83.23 Table 9: Parsing results using gol</context>
</contexts>
<marker>Harper, Huang, 2009</marker>
<rawString>Mary Harper and Zhongqiang Huang. 2009. Chinese statistical parsing. In Gale Book.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun Hatori</author>
<author>Takuya Matsuzaki</author>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Incremental joint pos tagging and dependency parsing in chinese.</title>
<date>2011</date>
<booktitle>In Proceedings of IJCNLP 2011,</booktitle>
<pages>1216--1224</pages>
<contexts>
<context position="32644" citStr="Hatori et al., 2011" startWordPosition="5711" endWordPosition="5714">ble 11 shows the increase and decrease of error patterns of the joint model over the pipeline POS tagger. An error pattern “X -* Y” means that the word whose true tag is ‘X’ is assigned a tag ‘Y’. All the patterns are ranked in descending order of the reduction/increase of the error number. We can see that the joint model has a clear advantage in the disambiguation of {VV, NN} and {DEG, DEC}, which results in the overall improved performance. In contrast, the joint method performs worse on ambiguous POS pairs such as {NN, NR}. This observation is similar to those reported by (Li et al., 2011; Hatori et al., 2011). 5 Conclusion In this paper, we proposed a new algorithm for joint Chinese word segmentation, POS tagging, and parsing. Our algorithm is an extension of the CYK 509 error pattern # ↓ error pattern # ↑ NN→ VV 47 19 NN→ NR 15 12 VV→ NN 42 13 NR→ NN 7 5 DEG→ DEC 23 10 JJ→ P 1 4 NN→ JJ 29 8 NN→ DT 2 4 DEC→ DEG 11 4 P→ VV 3 2 JJ→ NN 12 4 AD→ NN 1 2 Table 11: POS tagging error patterns. # means the error number of the corresponding pattern made by the pipeline tagging model. ↓ and ↑ mean the error number reduced or increased by the joint model. parsing method. The sub-models are independently train</context>
</contexts>
<marker>Hatori, Matsuzaki, Miyao, Tsujii, 2011</marker>
<rawString>Jun Hatori, Takuya Matsuzaki, Yusuke Miyao, and Jun’ichi Tsujii. 2011. Incremental joint pos tagging and dependency parsing in chinese. In Proceedings of IJCNLP 2011, pages 1216–1224.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenbin Jiang</author>
<author>Liang Huang</author>
<author>Qun Liu</author>
<author>Yajuan L¨u</author>
</authors>
<title>A cascaded linear model for joint chinese word segmentation and part-of-speech tagging.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL 2008: HLT,</booktitle>
<pages>897--904</pages>
<marker>Jiang, Huang, Liu, L¨u, 2008</marker>
<rawString>Wenbin Jiang, Liang Huang, Qun Liu, and Yajuan L¨u. 2008a. A cascaded linear model for joint chinese word segmentation and part-of-speech tagging. In Proceedings of ACL 2008: HLT, pages 897–904.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenbin Jiang</author>
<author>Haitao Mi</author>
<author>Qun Liu</author>
</authors>
<title>Word lattice reranking for chinese word segmentation and partof-speech tagging.</title>
<date>2008</date>
<booktitle>In Proceedings of Coling</booktitle>
<pages>385--392</pages>
<contexts>
<context position="3989" citStr="Jiang et al., 2008" startWordPosition="619" endWordPosition="622">Learning, pages 501–511, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics calculate the bracket scores for parsing in the face of word segmentation errors. Our experimental results show that the joint model significantly outperforms the pipeline method based on the state-of-the-art sub-models. 2 Related Work There is very limited previous work on joint Chinese word segmentation, POS tagging, and parsing. Previous joint models mainly focus on word segmentation and POS tagging task, such as the virtual nodes method (Qian et al., 2010), cascaded linear model (Jiang et al., 2008a), perceptron (Zhang and Clark, 2008), sub-word based stacked learning (Sun, 2011), reranking (Jiang et al., 2008b). These joint models showed about 0.2 − 1% F-score improvement over the pipeline method. Recently, joint tagging and dependency parsing has been studied as well (Li et al., 2011; Lee et al., 2011). Previous research has showed that word segmentation has a great impact on parsing accuracy in the pipeline method (Harper and Huang, 2009). In (Jiang et al., 2009), additional data was used to improve Chinese word segmentation, which resulted in significant improvement on the parsing t</context>
<context position="27097" citStr="Jiang et al., 2008" startWordPosition="4745" endWordPosition="4748">rd segmentation results using our word segmentation submodel, in comparison to a few state-of-the-art systems. For our segmentor, we show results for two variants: one removes transition features as described in Section 3.1, the other uses CRFs to learn the weights of transition features. We can see that our system is competitive with all the others except Sun’s that used additional idiom resources. Our two word segmentors have similar performance. Since the one without transition features can be naturally integrated into the joint system, we use it in the following joint tasks. System P R F (Jiang et al., 2008b) - - 97.74 (Jiang et al., 2008a) - 97.85 (Kruengkrai et al., 2009) 97.46 98.29 97.87 (Zhang and Clark, 2010) - - 97.78 (Zhang and Clark, 2011) - - 97.78 (Sun, 2011) - - 98.17 Ours (w/o transition features) 97.45 98.24 97.85 Ours (with transition features) 97.44 98.23 97.84 Table 7: Word segmentation results. For the POS tagging only task that takes gold standard word segmentation as input, we have two systems. One uses the linear chain CRFs as described in Section 3.2, the other is obtained using the parser described in Section 3.3 – the parser generates POS tag hypotheses when POS tag featu</context>
<context position="28526" citStr="Jiang et al., 2008" startWordPosition="4994" endWordPosition="4997">OS tagging than the long distance dependencies that might be noisy. This result also confirms our choice of using an independent POS tagger for the sub-model, rather than relying on a parser for POS tagging. However, since there are no reported results for this setup, we demonstrate the competence of our POS tagger using the joint word segmentation and POS tagging task. Table 8 shows the performance of a few systems along with ours, all using the pipeline approach where automatic segmentation is followed by POS tagging. We can see that our POS tagger is comparable to the others. System P R F (Jiang et al., 2008b) (Jiang et al., 2008a) - - 93.41 (Kruengkrai et al., 2009) 93.28 94.07 93.67 (Zhang and Clark, 2010) - - 93.67 (Zhang and Clark, 2011) - - 93.67 (Sun, 2011) - - 94.02 Ours (pipeline) 93.10 93.96 93.53 Table 8: Results for the joint word segmentation and POS tagging task. For parsing, Table 9 presents the parsing result on gold standard segmented sentence. Notice that the result of (Harper and Huang, 2009; Zhang and 508 Clark, 2011) are not directly comparable to ours, as they used a different data split. The best published system result on CTB5 is Petrov and Klein’s, which used PCFG with lat</context>
</contexts>
<marker>Jiang, Mi, Liu, 2008</marker>
<rawString>Wenbin Jiang, Haitao Mi, and Qun Liu. 2008b. Word lattice reranking for chinese word segmentation and partof-speech tagging. In Proceedings of Coling 2008, pages 385–392.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenbin Jiang</author>
<author>Liang Huang</author>
<author>Qun Liu</author>
</authors>
<title>Automatic adaptation of annotation standards: Chinese word segmentation and pos tagging – a case study.</title>
<date>2009</date>
<booktitle>In Proceedings of ACL-IJCNLP</booktitle>
<pages>522--530</pages>
<contexts>
<context position="4466" citStr="Jiang et al., 2009" startWordPosition="697" endWordPosition="700">cus on word segmentation and POS tagging task, such as the virtual nodes method (Qian et al., 2010), cascaded linear model (Jiang et al., 2008a), perceptron (Zhang and Clark, 2008), sub-word based stacked learning (Sun, 2011), reranking (Jiang et al., 2008b). These joint models showed about 0.2 − 1% F-score improvement over the pipeline method. Recently, joint tagging and dependency parsing has been studied as well (Li et al., 2011; Lee et al., 2011). Previous research has showed that word segmentation has a great impact on parsing accuracy in the pipeline method (Harper and Huang, 2009). In (Jiang et al., 2009), additional data was used to improve Chinese word segmentation, which resulted in significant improvement on the parsing task using the pipeline framework. Joint segmentation and parsing was also investigated for Arabic (Green and Manning, 2010). A study that is closely related to ours is (Goldberg and Tsarfaty, 2008), where a single generative model was proposed for joint morphological segmentation and syntactic parsing for Hebrew. Different from that work, we use a discriminative model, which benefits from large amounts of features and is easier to deal with unknown words. Another main diff</context>
<context position="29303" citStr="Jiang et al., 2009" startWordPosition="5131" endWordPosition="5134"> - - 94.02 Ours (pipeline) 93.10 93.96 93.53 Table 8: Results for the joint word segmentation and POS tagging task. For parsing, Table 9 presents the parsing result on gold standard segmented sentence. Notice that the result of (Harper and Huang, 2009; Zhang and 508 Clark, 2011) are not directly comparable to ours, as they used a different data split. The best published system result on CTB5 is Petrov and Klein’s, which used PCFG with latent Variables. Our system performs better mainly because it benefits from a large amount of features. System LP LR F (Petrov and Klein, 2007) 84.8 81.9 83.3 (Jiang et al., 2009) (Harper and Huang, 2009)* 83.22 82.84 83.03 (Zhang and Clark, 2011)* 78.6 78.0 78.3 Ours 84.57 83.68 84.13 Ours (w/ parent annotation) 83.35 82.73 83.04 Ours (no POS tag feature) 83.49 82.97 83.23 Table 9: Parsing results using gold standard word segmentation. For our parser, besides the model described in Section 3.3, we tried two variations: one does not use the automatic POS tag features, the other one is learned on the parent annotated training data. The results in Table 9 show that there is a performance degradation when using parent annotation. This may be due to the introduction of a l</context>
<context position="30525" citStr="Jiang et al., 2009" startWordPosition="5342" endWordPosition="5345">e number of states, resulting in sparse features. We also notice that with the help of the POS tag information, even automatically generated, the parser gained 0.9% improvement in F-score. This demonstrates the advantage of using a better independent POS tagger and incorporating it in parsing. Finally Table 10 shows the results for the three tasks using our joint decoding method in comparison to the pipeline method. We can see that the joint model outperforms the pipeline one. This is mainly because of a better parsing module as well as joint decoding. In the table we also include results of (Jiang et al., 2009), which is the only reported joint parsing result we found using the same data split on CTB5. They achieved 80.28% parsing F-score using automatic word segmentation. Their adapted system Jiang09+ leveraged additional corpus to improve Chinese word segmentation, resulting in an Fscore of 81.07%. Our system has better performance than these. System Task P R F Jiang09 Parse - - 80.28 Jiang09+ Parse - - 81.07 Ours Seg. 97.45 98.24 97.85 Pipeline POS 93.10 93.96 93.53 Parse 81.87 81.65 81.76 Ours Seg. 97.56 98.36 97.96 Joint POS 93.43 94.20 93.81 Parse 83.03 82.66 82.85 Table 10: Results for the jo</context>
</contexts>
<marker>Jiang, Huang, Liu, 2009</marker>
<rawString>Wenbin Jiang, Liang Huang, and Qun Liu. 2009. Automatic adaptation of annotation standards: Chinese word segmentation and pos tagging – a case study. In Proceedings of ACL-IJCNLP 2009, pages 522–530.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guangjin Jin</author>
<author>Xiao Chen</author>
</authors>
<title>The fourth international chinese language processing bakeoff: Chinese word segmentation, named entity recognition and chinese pos tagging.</title>
<date>2008</date>
<booktitle>In Proceedings of Sixth SIGHAN Workshop on Chinese Language Processing.</booktitle>
<contexts>
<context position="9919" citStr="Jin and Chen, 2008" startWordPosition="1618" endWordPosition="1621">ponding weight vectors. For simplicity, we denote Bseg = BCRF®ME, fseg = fCRF®ME, where BCRF®ME means the concatenation of BCRF and BME. Scores for single character words are defined similarly. These word scores will be used in the joint segmentation and parsing task Section 3.4. 3.2 POS Tagging Model Though syntax parsing model can directly predict the POS tag itself, we choose not to use this, but use an independent POS tagger for two reasons. First, there is a large amount of data with labeled POS tags but no syntax annotations, such as the People’s Daily corpus and SIGHAN bakeoff corpora (Jin and Chen, 2008). Such data can only be used to train POS taggers, but not for training the parsing model. Often using a larger training set will result in a better POS tagger. Second, the state-of-the-art POS tagging systems are often trained by sequence labeling models, not parsing models. (2.1) wi−2ti, wi−1ti, witi, wi+1ti, wi+2ti (2.2) wi−2wi−1ti, wi−1witi, wiwi+1ti, wi+1wi+2ti wi−1wi+1ti (2.3) c1(wi)ti, c2(wi)ti, c3(wi)ti, c−2(wi)ti c−1(wi)ti (2.4) c1(wi)c2(wi)ti, c−2(wi)c−1(wi)ti (2.5) l(wi)ti (2.5) ti−1ti Table 2: Feature templates for POS tagging. wi is the ith word in the sentence, ti is its POS tag.</context>
</contexts>
<marker>Jin, Chen, 2008</marker>
<rawString>Guangjin Jin and Xiao Chen. 2008. The fourth international chinese language processing bakeoff: Chinese word segmentation, named entity recognition and chinese pos tagging. In Proceedings of Sixth SIGHAN Workshop on Chinese Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Canasai Kruengkrai</author>
<author>Kiyotaka Uchimoto</author>
<author>Jun’ichi Kazama</author>
<author>Yiou Wang</author>
<author>Kentaro Torisawa</author>
<author>Hitoshi Isahara</author>
</authors>
<title>An error-driven word-character hybrid model for joint chinese word segmentation and pos tagging.</title>
<date>2009</date>
<booktitle>In Proceedings ofACL 2009,</booktitle>
<pages>513--521</pages>
<contexts>
<context position="27165" citStr="Kruengkrai et al., 2009" startWordPosition="4758" endWordPosition="4761">in comparison to a few state-of-the-art systems. For our segmentor, we show results for two variants: one removes transition features as described in Section 3.1, the other uses CRFs to learn the weights of transition features. We can see that our system is competitive with all the others except Sun’s that used additional idiom resources. Our two word segmentors have similar performance. Since the one without transition features can be naturally integrated into the joint system, we use it in the following joint tasks. System P R F (Jiang et al., 2008b) - - 97.74 (Jiang et al., 2008a) - 97.85 (Kruengkrai et al., 2009) 97.46 98.29 97.87 (Zhang and Clark, 2010) - - 97.78 (Zhang and Clark, 2011) - - 97.78 (Sun, 2011) - - 98.17 Ours (w/o transition features) 97.45 98.24 97.85 Ours (with transition features) 97.44 98.23 97.84 Table 7: Word segmentation results. For the POS tagging only task that takes gold standard word segmentation as input, we have two systems. One uses the linear chain CRFs as described in Section 3.2, the other is obtained using the parser described in Section 3.3 – the parser generates POS tag hypotheses when POS tag features are not used. The POS tagging accuracy is 95.53% and 95.10% usin</context>
<context position="28586" citStr="Kruengkrai et al., 2009" startWordPosition="5005" endWordPosition="5008">ght be noisy. This result also confirms our choice of using an independent POS tagger for the sub-model, rather than relying on a parser for POS tagging. However, since there are no reported results for this setup, we demonstrate the competence of our POS tagger using the joint word segmentation and POS tagging task. Table 8 shows the performance of a few systems along with ours, all using the pipeline approach where automatic segmentation is followed by POS tagging. We can see that our POS tagger is comparable to the others. System P R F (Jiang et al., 2008b) (Jiang et al., 2008a) - - 93.41 (Kruengkrai et al., 2009) 93.28 94.07 93.67 (Zhang and Clark, 2010) - - 93.67 (Zhang and Clark, 2011) - - 93.67 (Sun, 2011) - - 94.02 Ours (pipeline) 93.10 93.96 93.53 Table 8: Results for the joint word segmentation and POS tagging task. For parsing, Table 9 presents the parsing result on gold standard segmented sentence. Notice that the result of (Harper and Huang, 2009; Zhang and 508 Clark, 2011) are not directly comparable to ours, as they used a different data split. The best published system result on CTB5 is Petrov and Klein’s, which used PCFG with latent Variables. Our system performs better mainly because it </context>
</contexts>
<marker>Kruengkrai, Uchimoto, Kazama, Wang, Torisawa, Isahara, 2009</marker>
<rawString>Canasai Kruengkrai, Kiyotaka Uchimoto, Jun’ichi Kazama, Yiou Wang, Kentaro Torisawa, and Hitoshi Isahara. 2009. An error-driven word-character hybrid model for joint chinese word segmentation and pos tagging. In Proceedings ofACL 2009, pages 513–521.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings of ICML</booktitle>
<pages>282--289</pages>
<contexts>
<context position="2392" citStr="Lafferty et al., 2001" startWordPosition="370" endWordPosition="373">will result in tagging and parsing errors. Additionally, early modules cannot use information from subsequent modules. Intuitively a joint model that performs the three tasks together should help the system make the best decisions. In this paper, we propose a unified model for joint Chinese word segmentation, POS tagging, and parsing. Three sub-models are independently trained using the state-of-the-art methods. We do not use the joint inference algorithm for training because of the high complexity caused by the large amount of parameters. We use linear chain Conditional Random Fields (CRFs) (Lafferty et al., 2001) to train the word segmentation model and POS tagging model, and averaged perceptron (Collins, 2002) to learn the parsing model. During decoding, parameters of each sub-model are scaled to represent its importance in the joint model. Our decoding algorithm is an extension of CYK parsing. Initially, weights of all possible words together with their POS tags are calculated. When searching the parse tree, the word and POS tagging features are dynamically generated and the transition information of POS tagging is considered in the span merge operation. Experiments are conducted on Chinese Tree Ban</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of ICML 2001, pages 282–289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lee</author>
<author>Jason Naradowsky</author>
<author>David A Smith</author>
</authors>
<title>A discriminative model for joint morphological disambiguation and dependency parsing.</title>
<date>2011</date>
<booktitle>In Proceedings ACL 2011: HLT,</booktitle>
<pages>885--894</pages>
<contexts>
<context position="4301" citStr="Lee et al., 2011" startWordPosition="669" endWordPosition="672">he-art sub-models. 2 Related Work There is very limited previous work on joint Chinese word segmentation, POS tagging, and parsing. Previous joint models mainly focus on word segmentation and POS tagging task, such as the virtual nodes method (Qian et al., 2010), cascaded linear model (Jiang et al., 2008a), perceptron (Zhang and Clark, 2008), sub-word based stacked learning (Sun, 2011), reranking (Jiang et al., 2008b). These joint models showed about 0.2 − 1% F-score improvement over the pipeline method. Recently, joint tagging and dependency parsing has been studied as well (Li et al., 2011; Lee et al., 2011). Previous research has showed that word segmentation has a great impact on parsing accuracy in the pipeline method (Harper and Huang, 2009). In (Jiang et al., 2009), additional data was used to improve Chinese word segmentation, which resulted in significant improvement on the parsing task using the pipeline framework. Joint segmentation and parsing was also investigated for Arabic (Green and Manning, 2010). A study that is closely related to ours is (Goldberg and Tsarfaty, 2008), where a single generative model was proposed for joint morphological segmentation and syntactic parsing for Hebre</context>
</contexts>
<marker>Lee, Naradowsky, Smith, 2011</marker>
<rawString>John Lee, Jason Naradowsky, and David A. Smith. 2011. A discriminative model for joint morphological disambiguation and dependency parsing. In Proceedings ACL 2011: HLT, pages 885–894.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhenghua Li</author>
<author>Min Zhang</author>
<author>Wanxiang Che</author>
<author>Ting Liu</author>
<author>Wenliang Chen</author>
<author>Haizhou Li</author>
</authors>
<title>Joint models for chinese pos tagging and dependency parsing.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP 2011,</booktitle>
<pages>1180--1191</pages>
<contexts>
<context position="4282" citStr="Li et al., 2011" startWordPosition="665" endWordPosition="668">on the state-of-the-art sub-models. 2 Related Work There is very limited previous work on joint Chinese word segmentation, POS tagging, and parsing. Previous joint models mainly focus on word segmentation and POS tagging task, such as the virtual nodes method (Qian et al., 2010), cascaded linear model (Jiang et al., 2008a), perceptron (Zhang and Clark, 2008), sub-word based stacked learning (Sun, 2011), reranking (Jiang et al., 2008b). These joint models showed about 0.2 − 1% F-score improvement over the pipeline method. Recently, joint tagging and dependency parsing has been studied as well (Li et al., 2011; Lee et al., 2011). Previous research has showed that word segmentation has a great impact on parsing accuracy in the pipeline method (Harper and Huang, 2009). In (Jiang et al., 2009), additional data was used to improve Chinese word segmentation, which resulted in significant improvement on the parsing task using the pipeline framework. Joint segmentation and parsing was also investigated for Arabic (Green and Manning, 2010). A study that is closely related to ours is (Goldberg and Tsarfaty, 2008), where a single generative model was proposed for joint morphological segmentation and syntacti</context>
<context position="32622" citStr="Li et al., 2011" startWordPosition="5707" endWordPosition="5710"> joint models. Table 11 shows the increase and decrease of error patterns of the joint model over the pipeline POS tagger. An error pattern “X -* Y” means that the word whose true tag is ‘X’ is assigned a tag ‘Y’. All the patterns are ranked in descending order of the reduction/increase of the error number. We can see that the joint model has a clear advantage in the disambiguation of {VV, NN} and {DEG, DEC}, which results in the overall improved performance. In contrast, the joint method performs worse on ambiguous POS pairs such as {NN, NR}. This observation is similar to those reported by (Li et al., 2011; Hatori et al., 2011). 5 Conclusion In this paper, we proposed a new algorithm for joint Chinese word segmentation, POS tagging, and parsing. Our algorithm is an extension of the CYK 509 error pattern # ↓ error pattern # ↑ NN→ VV 47 19 NN→ NR 15 12 VV→ NN 42 13 NR→ NN 7 5 DEG→ DEC 23 10 JJ→ P 1 4 NN→ JJ 29 8 NN→ DT 2 4 DEC→ DEG 11 4 P→ VV 3 2 JJ→ NN 12 4 AD→ NN 1 2 Table 11: POS tagging error patterns. # means the error number of the corresponding pattern made by the pipeline tagging model. ↓ and ↑ mean the error number reduced or increased by the joint model. parsing method. The sub-models a</context>
</contexts>
<marker>Li, Zhang, Che, Liu, Chen, Li, 2011</marker>
<rawString>Zhenghua Li, Min Zhang, Wanxiang Che, Ting Liu, Wenliang Chen, and Haizhou Li. 2011. Joint models for chinese pos tagging and dependency parsing. In Proceedings of EMNLP 2011, pages 1180–1191.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dan Klein</author>
</authors>
<title>Improved inference for unlexicalized parsing.</title>
<date>2007</date>
<booktitle>In Proceedings of NAACL</booktitle>
<pages>404--411</pages>
<contexts>
<context position="29267" citStr="Petrov and Klein, 2007" startWordPosition="5124" endWordPosition="5127">g and Clark, 2011) - - 93.67 (Sun, 2011) - - 94.02 Ours (pipeline) 93.10 93.96 93.53 Table 8: Results for the joint word segmentation and POS tagging task. For parsing, Table 9 presents the parsing result on gold standard segmented sentence. Notice that the result of (Harper and Huang, 2009; Zhang and 508 Clark, 2011) are not directly comparable to ours, as they used a different data split. The best published system result on CTB5 is Petrov and Klein’s, which used PCFG with latent Variables. Our system performs better mainly because it benefits from a large amount of features. System LP LR F (Petrov and Klein, 2007) 84.8 81.9 83.3 (Jiang et al., 2009) (Harper and Huang, 2009)* 83.22 82.84 83.03 (Zhang and Clark, 2011)* 78.6 78.0 78.3 Ours 84.57 83.68 84.13 Ours (w/ parent annotation) 83.35 82.73 83.04 Ours (no POS tag feature) 83.49 82.97 83.23 Table 9: Parsing results using gold standard word segmentation. For our parser, besides the model described in Section 3.3, we tried two variations: one does not use the automatic POS tag features, the other one is learned on the parent annotated training data. The results in Table 9 show that there is a performance degradation when using parent annotation. This m</context>
</contexts>
<marker>Petrov, Klein, 2007</marker>
<rawString>Slav Petrov and Dan Klein. 2007. Improved inference for unlexicalized parsing. In Proceedings of NAACL 2007, pages 404–411.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xian Qian</author>
<author>Qi Zhang</author>
<author>Yaqian Zhou</author>
<author>Xuanjing Huang</author>
<author>Lide Wu</author>
</authors>
<title>Joint training and decoding using virtual nodes for cascaded segmentation and tagging tasks.</title>
<date>2010</date>
<booktitle>In Proceedings of EMNLP 2010,</booktitle>
<pages>187--195</pages>
<contexts>
<context position="3946" citStr="Qian et al., 2010" startWordPosition="612" endWordPosition="615">cessing and Computational Natural Language Learning, pages 501–511, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics calculate the bracket scores for parsing in the face of word segmentation errors. Our experimental results show that the joint model significantly outperforms the pipeline method based on the state-of-the-art sub-models. 2 Related Work There is very limited previous work on joint Chinese word segmentation, POS tagging, and parsing. Previous joint models mainly focus on word segmentation and POS tagging task, such as the virtual nodes method (Qian et al., 2010), cascaded linear model (Jiang et al., 2008a), perceptron (Zhang and Clark, 2008), sub-word based stacked learning (Sun, 2011), reranking (Jiang et al., 2008b). These joint models showed about 0.2 − 1% F-score improvement over the pipeline method. Recently, joint tagging and dependency parsing has been studied as well (Li et al., 2011; Lee et al., 2011). Previous research has showed that word segmentation has a great impact on parsing accuracy in the pipeline method (Harper and Huang, 2009). In (Jiang et al., 2009), additional data was used to improve Chinese word segmentation, which resulted </context>
<context position="10827" citStr="Qian et al., 2010" startWordPosition="1776" endWordPosition="1779"> wi−1ti, witi, wi+1ti, wi+2ti (2.2) wi−2wi−1ti, wi−1witi, wiwi+1ti, wi+1wi+2ti wi−1wi+1ti (2.3) c1(wi)ti, c2(wi)ti, c3(wi)ti, c−2(wi)ti c−1(wi)ti (2.4) c1(wi)c2(wi)ti, c−2(wi)c−1(wi)ti (2.5) l(wi)ti (2.5) ti−1ti Table 2: Feature templates for POS tagging. wi is the ith word in the sentence, ti is its POS tag. For a word w, cj(w) is its jth character, c−j(w) is the last jth character, and l(w) is its length. The POS tagging problem is to assign a POS tag t E T to each word in a sentence. We also use linear chain CRFs for POS tagging. Feature templates shown in Table 2 are the same as those in (Qian et al., 2010), which have been shown effective on CTB corpus. Three feature sets are considered: (i) word level features, including surrounding word unigrams, bigrams, and word length; (ii) character level features, such as the first and last characters in the words; (iii) transition features. 3.3 Parsing Model We choose discriminative models for parsing since it is easy to handle unknown words by simply adding character level features. Online structured learning algorithms were demonstrated to be effective for training, such as stochastic optimization (Finkel et al., 2008). In this study, we use averaged </context>
</contexts>
<marker>Qian, Zhang, Zhou, Huang, Wu, 2010</marker>
<rawString>Xian Qian, Qi Zhang, Yaqian Zhou, Xuanjing Huang, and Lide Wu. 2010. Joint training and decoding using virtual nodes for cascaded segmentation and tagging tasks. In Proceedings of EMNLP 2010, pages 187– 195.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Roark</author>
<author>Mary Harper</author>
<author>Eugene Charniak</author>
<author>Bonnie Dorr</author>
<author>Mark Johnson</author>
<author>Jeremy G Kahn</author>
</authors>
<title>Sparseval: Evaluation metrics for parsing speech.</title>
<date>2006</date>
<booktitle>In Proceedings Language Resources and Evaluation (LREC).</booktitle>
<location>Yang Liu, Mari Ostendorf, John Hale, Anna Krasnyanskaya, Matthew Lease, Izhak Shafran, Matthew Snover, Robin</location>
<contexts>
<context position="23658" citStr="Roark et al., 2006" startWordPosition="4166" endWordPosition="4169"> bracket is correct only if its boundaries, label and word segmentation are all correct. One example is shown in Figure 3. Notice that identity unary rules are removed during evaluation. The basic spans are characters, not words, because the number of words in reference and prediction may be different. POS tags are removed since they do not affect the bracket scores. If the segmentation is perfect, then the bracket scores of the modified tree are exactly the same as the original tree. This is similar to evaluating parsing performance on speech transcripts with automatic sentence segmentation (Roark et al., 2006). NP(0,2,5) NR NN - - - - - !&amp;quot; #$% ! &amp;quot; # $ % Shanghai office Shanghai office Figure 3: Boundary information is added to states to calculate the bracket scores in the face of word segmentation errors. Left: the original parse tree, Right: the converted parse tree. The numbers in the brackets are the indices of the character boundaries based on word segmentation. NP 4.2 Evaluation Metric We evaluate system performance on the individual tasks, as well as the joint tasks.&apos; For word segmentation, three metrics are used for evaluation: precision (P), recall (R), and F-score (F) defined by 2PR/(P+R).</context>
</contexts>
<marker>Roark, Harper, Charniak, Dorr, Johnson, Kahn, 2006</marker>
<rawString>Brian Roark, Mary Harper, Eugene Charniak, Bonnie Dorr, Mark Johnson, Jeremy G. Kahn, Yang Liu, Mari Ostendorf, John Hale, Anna Krasnyanskaya, Matthew Lease, Izhak Shafran, Matthew Snover, Robin Stewart, Lisa Yung, and Lisa Yung. 2006. Sparseval: Evaluation metrics for parsing speech. In Proceedings Language Resources and Evaluation (LREC).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sunita Sarawagi</author>
<author>William W Cohen</author>
</authors>
<title>Semimarkov conditional random fields for information extraction.</title>
<date>2004</date>
<booktitle>In Proceedings of NIPS</booktitle>
<contexts>
<context position="6179" citStr="Sarawagi and Cohen, 2004" startWordPosition="971" endWordPosition="974">e for training. Instead, we develop three individual models independently during training and perform joint decoding using them. In this section, we first describe the three sub-models and then the joint decoding algorithm. 3.1 Word Segmentation Model Methods for Chinese word segmentation can be broadly categorized into character based and word based models. Previous studies showed that character-based models are more effective to detect out-of-vocabulary words while word-based models are more accurate to predict in-vocabulary words (Zhang et al., 2006). Here, we use order-0 semiMarkov model (Sarawagi and Cohen, 2004) to take advantages of both approaches. More specifically, given a sentence x = c1, c2, ... , cl (where cz is the ith Chinese character, l is the sentence length), the character-based model assigns each character with a word boundary tag. Here we use the BCDIES tag set, which achieved the best official performance (Zhao and Kit, 2008): B, C, D, E denote the first, second, third, and last character of a multi-character word respectively, I denotes the other characters, and S denotes the single character word. We use the same characterbased feature templates as in the best official system, shown</context>
</contexts>
<marker>Sarawagi, Cohen, 2004</marker>
<rawString>Sunita Sarawagi and William W. Cohen. 2004. Semimarkov conditional random fields for information extraction. In Proceedings of NIPS 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Weiwei Sun</author>
</authors>
<title>A stacked sub-word model for joint chinese word segmentation and part-of-speech tagging.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL 2011,</booktitle>
<pages>1385--1394</pages>
<contexts>
<context position="4072" citStr="Sun, 2011" startWordPosition="632" endWordPosition="633">ational Linguistics calculate the bracket scores for parsing in the face of word segmentation errors. Our experimental results show that the joint model significantly outperforms the pipeline method based on the state-of-the-art sub-models. 2 Related Work There is very limited previous work on joint Chinese word segmentation, POS tagging, and parsing. Previous joint models mainly focus on word segmentation and POS tagging task, such as the virtual nodes method (Qian et al., 2010), cascaded linear model (Jiang et al., 2008a), perceptron (Zhang and Clark, 2008), sub-word based stacked learning (Sun, 2011), reranking (Jiang et al., 2008b). These joint models showed about 0.2 − 1% F-score improvement over the pipeline method. Recently, joint tagging and dependency parsing has been studied as well (Li et al., 2011; Lee et al., 2011). Previous research has showed that word segmentation has a great impact on parsing accuracy in the pipeline method (Harper and Huang, 2009). In (Jiang et al., 2009), additional data was used to improve Chinese word segmentation, which resulted in significant improvement on the parsing task using the pipeline framework. Joint segmentation and parsing was also investiga</context>
<context position="27263" citStr="Sun, 2011" startWordPosition="4779" endWordPosition="4780">s transition features as described in Section 3.1, the other uses CRFs to learn the weights of transition features. We can see that our system is competitive with all the others except Sun’s that used additional idiom resources. Our two word segmentors have similar performance. Since the one without transition features can be naturally integrated into the joint system, we use it in the following joint tasks. System P R F (Jiang et al., 2008b) - - 97.74 (Jiang et al., 2008a) - 97.85 (Kruengkrai et al., 2009) 97.46 98.29 97.87 (Zhang and Clark, 2010) - - 97.78 (Zhang and Clark, 2011) - - 97.78 (Sun, 2011) - - 98.17 Ours (w/o transition features) 97.45 98.24 97.85 Ours (with transition features) 97.44 98.23 97.84 Table 7: Word segmentation results. For the POS tagging only task that takes gold standard word segmentation as input, we have two systems. One uses the linear chain CRFs as described in Section 3.2, the other is obtained using the parser described in Section 3.3 – the parser generates POS tag hypotheses when POS tag features are not used. The POS tagging accuracy is 95.53% and 95.10% using these two methods respectively. The better performance from the former system may be because the</context>
<context position="28684" citStr="Sun, 2011" startWordPosition="5026" endWordPosition="5027">than relying on a parser for POS tagging. However, since there are no reported results for this setup, we demonstrate the competence of our POS tagger using the joint word segmentation and POS tagging task. Table 8 shows the performance of a few systems along with ours, all using the pipeline approach where automatic segmentation is followed by POS tagging. We can see that our POS tagger is comparable to the others. System P R F (Jiang et al., 2008b) (Jiang et al., 2008a) - - 93.41 (Kruengkrai et al., 2009) 93.28 94.07 93.67 (Zhang and Clark, 2010) - - 93.67 (Zhang and Clark, 2011) - - 93.67 (Sun, 2011) - - 94.02 Ours (pipeline) 93.10 93.96 93.53 Table 8: Results for the joint word segmentation and POS tagging task. For parsing, Table 9 presents the parsing result on gold standard segmented sentence. Notice that the result of (Harper and Huang, 2009; Zhang and 508 Clark, 2011) are not directly comparable to ours, as they used a different data split. The best published system result on CTB5 is Petrov and Klein’s, which used PCFG with latent Variables. Our system performs better mainly because it benefits from a large amount of features. System LP LR F (Petrov and Klein, 2007) 84.8 81.9 83.3 (</context>
</contexts>
<marker>Sun, 2011</marker>
<rawString>Weiwei Sun. 2011. A stacked sub-word model for joint chinese word segmentation and part-of-speech tagging. In Proceedings of ACL 2011, pages 1385–1394.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>Joint word segmentation and POS tagging using a single perceptron.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL 2008: HLT,</booktitle>
<pages>888--896</pages>
<contexts>
<context position="4027" citStr="Zhang and Clark, 2008" startWordPosition="624" endWordPosition="627">nd, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics calculate the bracket scores for parsing in the face of word segmentation errors. Our experimental results show that the joint model significantly outperforms the pipeline method based on the state-of-the-art sub-models. 2 Related Work There is very limited previous work on joint Chinese word segmentation, POS tagging, and parsing. Previous joint models mainly focus on word segmentation and POS tagging task, such as the virtual nodes method (Qian et al., 2010), cascaded linear model (Jiang et al., 2008a), perceptron (Zhang and Clark, 2008), sub-word based stacked learning (Sun, 2011), reranking (Jiang et al., 2008b). These joint models showed about 0.2 − 1% F-score improvement over the pipeline method. Recently, joint tagging and dependency parsing has been studied as well (Li et al., 2011; Lee et al., 2011). Previous research has showed that word segmentation has a great impact on parsing accuracy in the pipeline method (Harper and Huang, 2009). In (Jiang et al., 2009), additional data was used to improve Chinese word segmentation, which resulted in significant improvement on the parsing task using the pipeline framework. Join</context>
</contexts>
<marker>Zhang, Clark, 2008</marker>
<rawString>Yue Zhang and Stephen Clark. 2008. Joint word segmentation and POS tagging using a single perceptron. In Proceedings of ACL 2008: HLT, pages 888–896.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>A fast decoder for joint word segmentation and POS-tagging using a single discriminative model.</title>
<date>2010</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<pages>843--852</pages>
<contexts>
<context position="27207" citStr="Zhang and Clark, 2010" startWordPosition="4765" endWordPosition="4768">ems. For our segmentor, we show results for two variants: one removes transition features as described in Section 3.1, the other uses CRFs to learn the weights of transition features. We can see that our system is competitive with all the others except Sun’s that used additional idiom resources. Our two word segmentors have similar performance. Since the one without transition features can be naturally integrated into the joint system, we use it in the following joint tasks. System P R F (Jiang et al., 2008b) - - 97.74 (Jiang et al., 2008a) - 97.85 (Kruengkrai et al., 2009) 97.46 98.29 97.87 (Zhang and Clark, 2010) - - 97.78 (Zhang and Clark, 2011) - - 97.78 (Sun, 2011) - - 98.17 Ours (w/o transition features) 97.45 98.24 97.85 Ours (with transition features) 97.44 98.23 97.84 Table 7: Word segmentation results. For the POS tagging only task that takes gold standard word segmentation as input, we have two systems. One uses the linear chain CRFs as described in Section 3.2, the other is obtained using the parser described in Section 3.3 – the parser generates POS tag hypotheses when POS tag features are not used. The POS tagging accuracy is 95.53% and 95.10% using these two methods respectively. The bett</context>
<context position="28628" citStr="Zhang and Clark, 2010" startWordPosition="5012" endWordPosition="5015">choice of using an independent POS tagger for the sub-model, rather than relying on a parser for POS tagging. However, since there are no reported results for this setup, we demonstrate the competence of our POS tagger using the joint word segmentation and POS tagging task. Table 8 shows the performance of a few systems along with ours, all using the pipeline approach where automatic segmentation is followed by POS tagging. We can see that our POS tagger is comparable to the others. System P R F (Jiang et al., 2008b) (Jiang et al., 2008a) - - 93.41 (Kruengkrai et al., 2009) 93.28 94.07 93.67 (Zhang and Clark, 2010) - - 93.67 (Zhang and Clark, 2011) - - 93.67 (Sun, 2011) - - 94.02 Ours (pipeline) 93.10 93.96 93.53 Table 8: Results for the joint word segmentation and POS tagging task. For parsing, Table 9 presents the parsing result on gold standard segmented sentence. Notice that the result of (Harper and Huang, 2009; Zhang and 508 Clark, 2011) are not directly comparable to ours, as they used a different data split. The best published system result on CTB5 is Petrov and Klein’s, which used PCFG with latent Variables. Our system performs better mainly because it benefits from a large amount of features. </context>
</contexts>
<marker>Zhang, Clark, 2010</marker>
<rawString>Yue Zhang and Stephen Clark. 2010. A fast decoder for joint word segmentation and POS-tagging using a single discriminative model. In Proceedings of EMNLP 2010, pages 843–852.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>Syntactic processing using the generalized perceptron and beam search.</title>
<date>2011</date>
<journal>Comput. Linguist.,</journal>
<volume>37</volume>
<issue>1</issue>
<contexts>
<context position="27241" citStr="Zhang and Clark, 2011" startWordPosition="4772" endWordPosition="4775">sults for two variants: one removes transition features as described in Section 3.1, the other uses CRFs to learn the weights of transition features. We can see that our system is competitive with all the others except Sun’s that used additional idiom resources. Our two word segmentors have similar performance. Since the one without transition features can be naturally integrated into the joint system, we use it in the following joint tasks. System P R F (Jiang et al., 2008b) - - 97.74 (Jiang et al., 2008a) - 97.85 (Kruengkrai et al., 2009) 97.46 98.29 97.87 (Zhang and Clark, 2010) - - 97.78 (Zhang and Clark, 2011) - - 97.78 (Sun, 2011) - - 98.17 Ours (w/o transition features) 97.45 98.24 97.85 Ours (with transition features) 97.44 98.23 97.84 Table 7: Word segmentation results. For the POS tagging only task that takes gold standard word segmentation as input, we have two systems. One uses the linear chain CRFs as described in Section 3.2, the other is obtained using the parser described in Section 3.3 – the parser generates POS tag hypotheses when POS tag features are not used. The POS tagging accuracy is 95.53% and 95.10% using these two methods respectively. The better performance from the former sys</context>
<context position="28662" citStr="Zhang and Clark, 2011" startWordPosition="5019" endWordPosition="5022"> tagger for the sub-model, rather than relying on a parser for POS tagging. However, since there are no reported results for this setup, we demonstrate the competence of our POS tagger using the joint word segmentation and POS tagging task. Table 8 shows the performance of a few systems along with ours, all using the pipeline approach where automatic segmentation is followed by POS tagging. We can see that our POS tagger is comparable to the others. System P R F (Jiang et al., 2008b) (Jiang et al., 2008a) - - 93.41 (Kruengkrai et al., 2009) 93.28 94.07 93.67 (Zhang and Clark, 2010) - - 93.67 (Zhang and Clark, 2011) - - 93.67 (Sun, 2011) - - 94.02 Ours (pipeline) 93.10 93.96 93.53 Table 8: Results for the joint word segmentation and POS tagging task. For parsing, Table 9 presents the parsing result on gold standard segmented sentence. Notice that the result of (Harper and Huang, 2009; Zhang and 508 Clark, 2011) are not directly comparable to ours, as they used a different data split. The best published system result on CTB5 is Petrov and Klein’s, which used PCFG with latent Variables. Our system performs better mainly because it benefits from a large amount of features. System LP LR F (Petrov and Klein, </context>
</contexts>
<marker>Zhang, Clark, 2011</marker>
<rawString>Yue Zhang and Stephen Clark. 2011. Syntactic processing using the generalized perceptron and beam search. Comput. Linguist., 37(1):105–151.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruiqiang Zhang</author>
<author>Genichiro Kikui</author>
<author>Eiichiro Sumita</author>
</authors>
<title>Subword-based tagging for confidencedependent chinese word segmentation.</title>
<date>2006</date>
<booktitle>In Proceedings of the COLING/ACL</booktitle>
<pages>961--968</pages>
<contexts>
<context position="6113" citStr="Zhang et al., 2006" startWordPosition="960" endWordPosition="963">model parameters. Therefore we do not perform joint inference for training. Instead, we develop three individual models independently during training and perform joint decoding using them. In this section, we first describe the three sub-models and then the joint decoding algorithm. 3.1 Word Segmentation Model Methods for Chinese word segmentation can be broadly categorized into character based and word based models. Previous studies showed that character-based models are more effective to detect out-of-vocabulary words while word-based models are more accurate to predict in-vocabulary words (Zhang et al., 2006). Here, we use order-0 semiMarkov model (Sarawagi and Cohen, 2004) to take advantages of both approaches. More specifically, given a sentence x = c1, c2, ... , cl (where cz is the ith Chinese character, l is the sentence length), the character-based model assigns each character with a word boundary tag. Here we use the BCDIES tag set, which achieved the best official performance (Zhao and Kit, 2008): B, C, D, E denote the first, second, third, and last character of a multi-character word respectively, I denotes the other characters, and S denotes the single character word. We use the same char</context>
</contexts>
<marker>Zhang, Kikui, Sumita, 2006</marker>
<rawString>Ruiqiang Zhang, Genichiro Kikui, and Eiichiro Sumita. 2006. Subword-based tagging for confidencedependent chinese word segmentation. In Proceedings of the COLING/ACL 2006, pages 961–968.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai Zhao</author>
<author>Chunyu Kit</author>
</authors>
<title>Unsupervised segmentation helps supervised learning of character tagging forword segmentation and named entity recognition.</title>
<date>2008</date>
<booktitle>In Proceedings of Sixth SIGHAN Workshop on Chinese Language Processing,</booktitle>
<pages>106--111</pages>
<contexts>
<context position="6515" citStr="Zhao and Kit, 2008" startWordPosition="1030" endWordPosition="1033">word based models. Previous studies showed that character-based models are more effective to detect out-of-vocabulary words while word-based models are more accurate to predict in-vocabulary words (Zhang et al., 2006). Here, we use order-0 semiMarkov model (Sarawagi and Cohen, 2004) to take advantages of both approaches. More specifically, given a sentence x = c1, c2, ... , cl (where cz is the ith Chinese character, l is the sentence length), the character-based model assigns each character with a word boundary tag. Here we use the BCDIES tag set, which achieved the best official performance (Zhao and Kit, 2008): B, C, D, E denote the first, second, third, and last character of a multi-character word respectively, I denotes the other characters, and S denotes the single character word. We use the same characterbased feature templates as in the best official system, shown in Table 1 (1.1-1.3), including character unigram and bigram features, and transition features. Linear chain CRFs are used for training. Feature templates in the word-based model are shown in Table 1 (1.4-1.6), including word features, sub-word features, and character bigrams within words. The word feature is activated if a predicted</context>
</contexts>
<marker>Zhao, Kit, 2008</marker>
<rawString>Hai Zhao and Chunyu Kit. 2008. Unsupervised segmentation helps supervised learning of character tagging forword segmentation and named entity recognition. In Proceedings of Sixth SIGHAN Workshop on Chinese Language Processing, pages 106–111.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
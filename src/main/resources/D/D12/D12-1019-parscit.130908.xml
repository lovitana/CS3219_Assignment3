<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.754652">
Spectral Dependency Parsing with Latent Variables
</title>
<note confidence="0.777730666666667">
Paramveer S. Dhillon1, Jordan Rodu2, Michael Collins3, Dean P. Foster2
and Lyle H. Ungar1
1Computer &amp; Information Science/ 2Statistics, University of Pennsylvania, Philadelphia, PA, U.S.A
</note>
<sectionHeader confidence="0.262911" genericHeader="abstract">
3 Computer Science, Columbia University, New York, NY, U.S.A
</sectionHeader>
<email confidence="0.986281">
{dhillon|ungar@cis.upenn.edu}, {jrodu|foster@wharton.upenn.edu}
mcollins@cs.columbia.edu
</email>
<sectionHeader confidence="0.980696" genericHeader="keywords">
Abstract
</sectionHeader>
<figureCaption confidence="0.715815526315789">
Recently there has been substantial interest in
using spectral methods to learn generative se-
quence models like HMMs. Spectral meth-
ods are attractive as they provide globally con-
sistent estimates of the model parameters and
are very fast and scalable, unlike EM meth-
ods, which can get stuck in local minima. In
this paper, we present a novel extension of
this class of spectral methods to learn depen-
dency tree structures. We propose a simple
yet powerful latent variable generative model
for dependency parsing, and a spectral learn-
ing method to efficiently estimate it. As a pi-
lot experimental evaluation, we use the spec-
tral tree probabilities estimated by our model
to re-rank the outputs of a near state-of-the-
art parser. Our approach gives us a moderate
reduction in error of up to 4.6% over the base-
line re-ranker.
</figureCaption>
<sectionHeader confidence="0.997876" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.991131659574468">
Markov models have been for two decades a
workhorse of statistical pattern recognition with ap-
plications ranging from speech to vision to lan-
guage. Adding latent variables to these models gives
us additional modeling power and have shown suc-
cess in applications like POS tagging (Merialdo,
1994), speech recognition (Rabiner, 1989) and ob-
ject recognition (Quattoni et al., 2004). However,
this comes at the cost that the resulting parameter
estimation problem becomes non-convex and tech-
niques like EM (Dempster et al., 1977) which are
used to estimate the parameters can only lead to lo-
cally optimal solutions.
Recent work by Hsu et al. (2008) has shown that
globally consistent estimates of the parameters of
HMMs can be found by using spectral methods, par-
ticularly by singular value decomposition (SVD) of
appropriately defined linear systems. They avoid the
NP Hard problem of the global optimization prob-
lem of the HMM parameters (Terwijn, 2002), by
putting restrictions on the smallest singular value
of the HMM parameters. The main intuition be-
hind the model is that, although the observed data
(i.e. words) seem to live in a very high dimensional
space, but in reality they live in a very low dimen-
sional space (size k — 30 — 50) and an appropriate
eigen decomposition of the observed data will re-
veal the underlying low dimensional dynamics and
thereby revealing the parameters of the model. Be-
sides ducking the NP hard problem, the spectral
methods are very fast and scalable to train compared
to EM methods.
In this paper we generalize the approach of Hsu
et al. (2008) to learn dependency tree structures with
latent variables.1 Petrov et al. (2006) and Musillo
and Merlo (2008) have shown that learning PCFGs
and dependency grammars respectively with latent
variables can produce parsers with very good gen-
eralization performance. However, both these ap-
proaches rely on EM for parameter estimation and
can benefit from using spectral methods.
We propose a simple yet powerful latent vari-
able generative model for use with dependency pars-
1Actually, instead of using the model by Hsu et al. (2008)
we work with a related model proposed by Foster et al. (2012)
which addresses some of the shortcomings of the earlier model
which we detail below.
</bodyText>
<page confidence="0.980453">
205
</page>
<note confidence="0.9567325">
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 205–213, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.999377681818182">
ing which has one hidden node for each word in
the sentence, like the one shown in Figure 1 and
work out the details for the parameter estimation
of the corresponding spectral learning model. At
a very high level, the parameter estimation of our
model involves collecting unigram, bigram and tri-
gram counts sensitive to the underlying dependency
structure of the given sentence.
Recently, Luque et al. (2012) have also proposed
a spectral method for dependency parsing, however
they deal with horizontal markovization and use hid-
den states to model sequential dependencies within a
word’s sequence of children. In contrast with that, in
this paper, we propose a spectral learning algorithm
where latent states are not restricted to HMM-like
distributions of modifier sequences for a particular
head, but instead allow information to be propagated
through the entire tree.
More recently, Cohen et al. (2012) have proposed
a spectral method for learning PCFGs.
Its worth noting that recent work by Parikh et al.
(2011) also extends Hsu et al. (2008) to latent vari-
able dependency trees like us but under the restric-
tive conditions that model parameters are trained for
a specified, albeit arbitrary, tree topology.2 In other
words, all training sentences and test sentences must
have identical tree topologies. By doing this they al-
low for node-specific model parameters, but must re-
train the model entirely when a different tree topol-
ogy is encountered. Our model on the other hand al-
lows the flexibility and efficiency of processing sen-
tences with a variety of tree topologies from a single
training run.
Most of the current state-of-the-art dependency
parsers are discriminative parsers (Koo et al., 2008;
McDonald, 2006) due to the flexibility of represen-
tations which can be used as features leading to bet-
ter accuracies and the ease of reproducibility of re-
sults. However, unlike discriminative models, gen-
erative models can exploit unlabeled data. Also, as
is common in statistical parsing, re-ranking the out-
puts of a parser leads to significant reductions in er-
ror (Collins and Koo, 2005).
Since our spectral learning algorithm uses a gen-
</bodyText>
<footnote confidence="0.932425">
2This can be useful in modeling phylogeny trees for in-
stance, but precludes most NLP applications, since there is a
need to model the full set of different tree topologies possible
in parsing.
</footnote>
<figureCaption confidence="0.9910785">
Figure 1: Sample dependency parsing tree for “Kilroy
was here”
</figureCaption>
<bodyText confidence="0.991577785714286">
erative model of words given a tree structure, it can
score a tree structure i.e. its probability of genera-
tion. Thus, it can be used to re-rank the n-best out-
puts of a given parser.
The remainder of the paper is organized as fol-
lows. In the next section we introduce the notation
and give a brief overview of the spectral algorithm
for learning HMMs (Hsu et al., 2008; Foster et al.,
2012). In Section 3 we describe our proposed model
for dependency parsing in detail and work out the
theory behind it. Section 4 provides experimental
evaluation of our model on Penn Treebank data. We
conclude with a brief summary and future avenues
for research.
</bodyText>
<sectionHeader confidence="0.95532" genericHeader="method">
2 Spectral Algorithm For Learning HMMs
</sectionHeader>
<bodyText confidence="0.9974795">
In this section we describe the spectral algorithm for
learning HMMs.3
</bodyText>
<subsectionHeader confidence="0.971348">
2.1 Notation
</subsectionHeader>
<bodyText confidence="0.997639333333333">
The HMM that we consider in this section is a se-
quence of hidden states h E 11, ... , k} that follow
the Markov property:
</bodyText>
<equation confidence="0.754514">
p(ht|h1, ... , ht−1) = p(ht|ht−1)
</equation>
<bodyText confidence="0.846257">
and a sequence of observations x E 11, ... , n} such
that
</bodyText>
<equation confidence="0.35136">
p(xt|x1, ... , xt−1, h1, ... , ht) = p(xt|ht)
</equation>
<footnote confidence="0.9936345">
3As mentioned earlier, we use the model by Foster et al.
(2012) which is conceptually similar to the one by Hsu et al.
(2008), but does further dimensionality reduction and thus has
lower sample complexity. Also, critically, the fully reduced di-
mension model that we use generalizes much more cleanly to
trees.
</footnote>
<equation confidence="0.6183244">
here
Kilroy
h1 h2
was
h0
</equation>
<page confidence="0.993108">
206
</page>
<bodyText confidence="0.9487322">
The parameters of this HMM are:
However, Hsu et al. (2008) and Foster et al. (2012)
showed that under certain conditions there exists a
fully observable representation of the observable op-
erator model.
</bodyText>
<listItem confidence="0.646467">
• A vector π of length k where πi = p(h1 = i):
The probability of the start state in the sequence
being i.
• A matrix T of size k x k where
</listItem>
<bodyText confidence="0.886373">
Ti,j = p(ht+1 = i|ht = j): The probability of
transitioning to state i, given that the previous
state was j.
</bodyText>
<listItem confidence="0.996562">
• A matrix O of size n x k where
</listItem>
<bodyText confidence="0.993674444444445">
Oi,j = p(x = i|h = j): The probability of
state h emitting observation x.
Define δj to be the vector of length n with a 1 in
the jth entry and 0 everywhere else, and diag(v) to
be the matrix with the entries of v on the diagonal
and 0 everywhere else.
The joint distribution of a sequence of observa-
tions x1, ... , xm and a sequence of hidden states
h1, ... , hm is:
</bodyText>
<subsectionHeader confidence="0.998421">
2.2 Fully observable representation
</subsectionHeader>
<bodyText confidence="0.9995252">
Before presenting the model, we need to address a
few more points. First, let U be a “representation
matrix” (eigenfeature dictionary) which maps each
observation to a reduced dimension space (n —* k)
that satisfies the conditions:
</bodyText>
<listItem confidence="0.9993325">
• UTO is invertible
• |Uij |&lt; 1.
</listItem>
<bodyText confidence="0.922846166666667">
Hsu et al. (2008); Foster et al. (2012) discuss U
in more detail, but U can, for example, be obtained
by the SVD of the bigram probability matrix (where
Pij = p(xt+1 = i|xt = j)) or by doing CCA on
neighboring n-grams (Dhillon et al., 2011).
Letting yi = UTδxi, we have
</bodyText>
<equation confidence="0.998575333333333">
p(x1, ... , xm)
= cTooC(ym)C(ym−1) ... C(y1)c1 (1)
p(x1, ... ,xm, h1, ... , hm)
= πh1 m−1H Thj,hj_1 m Oxj,hj
j=2 H
j=1
</equation>
<bodyText confidence="0.9352483125">
Now, we can write the marginal probability of a
sequence of observations as
p(x1, ... xm)
�= p(x1, ... , xm, h1, ... , hm)
h1,...,hm
which can be expressed in matrix form4 as:
p(x1, ... , xm) = 1TAxmAxm_1 ··· Am1π
where Axm = Tdiag(OTδxm), and 1 is a k-
dimensional vector with every entry equal to 1.
A is called an “observation operator”, and is ef-
fectively a third order tensor, and Axm which is a
matrix, gives the distribution vector over states at
time m+1 as a function of the state distribution vec-
tor at the current time m and the current observation
δxm. Since Axm depends on the hidden state, it is not
observable, and hence cannot be directly estimated.
</bodyText>
<footnote confidence="0.9601195">
4This is essentially the matrix form of the standard dynamic
program (forward algorithm) used to estimate HMMs.
</footnote>
<bodyText confidence="0.978745">
where
</bodyText>
<equation confidence="0.999543666666667">
c1 = µ
coo = µTE−1
C(y) = K(y)E−1
</equation>
<bodyText confidence="0.9999105">
and µ, E and K, described in more detail below, are
quantities estimated by frequencies of unigrams, bi-
grams, and trigrams in the observed (training) data.
Under the assumption that data is generated by
an HMM, the distribution p� obtained by substituting
the estimated values c1, coo, and C(y) into equation
(1) converges to p sufficiently fast as the amount of
training data increases, giving us consistent param-
eter estimates. For details of the convergence proof,
please see Hsu et al. (2008) and Foster et al. (2012).
</bodyText>
<sectionHeader confidence="0.8805815" genericHeader="method">
3 Spectral Algorithm For Learning
Dependency Trees
</sectionHeader>
<bodyText confidence="0.999951666666667">
In this section, we first describe a simple latent vari-
able generative model for dependency parsing. We
then define some extra notation and finally present
</bodyText>
<page confidence="0.955121">
207
</page>
<figure confidence="0.78610625">
h2 h3
y1
y2 y3
h1
</figure>
<figureCaption confidence="0.999692">
Figure 2: Dependency parsing tree with observed vari-
ables y1, y2, and y3.
</figureCaption>
<bodyText confidence="0.999996181818182">
where pa(j) is the parent of node j and d(j) E
{L, R} indicates whether hj is a left or a right node
of hpa(j). For simplicity, the number of hidden and
observed nodes in our tree are the same, however
they are not required to be so.
As is the case with the conventional HMM, the
parameters used to calculate this joint probability
are unobservable, but it turns out that under suitable
conditions a fully observable model is also possible
for the dependency tree case with the parameteriza-
tion as described below.
</bodyText>
<subsectionHeader confidence="0.999913">
3.2 Model parameters
</subsectionHeader>
<bodyText confidence="0.9883839">
We will define both the theoretical representations
of our observable parameters, and the sampling ver-
sions of these parameters. Note that in all the cases,
the estimated versions are unbiased estimates of the
theoretical quantities.
Define Td and Tdu where d E {L, R} to be the
hidden state transition matrices from parent to left
or right child, and from left or right child to parent
(hence the u for ‘up’), respectively. In other words
(referring to Figure 2)
</bodyText>
<equation confidence="0.99924825">
TR = t(h3|h1)
TL = t(h2|h1)
TRu = t(h1|h3)
TLu = t(h1|h2)
</equation>
<bodyText confidence="0.999428">
Let Ux(i) be the ith entry of vector UTSx and G =
UTO. Further, recall the notation diag(v), which is
a matrix with elements of v on its diagonal, then:
</bodyText>
<listItem confidence="0.97917">
• Define the k-dimensional vector p (unigram
</listItem>
<bodyText confidence="0.9672734">
the details of the corresponding spectral learning al-
gorithm for dependency parsing, and prove that our
learning algorithm provides a consistent estimation
of the marginal probabilities.
It is worth mentioning that an alternate way of ap-
proaching the spectral estimation of latent states for
dependency parsing is by converting the dependency
trees into linear sequences from root-to-leaf and do-
ing a spectral estimation of latent states using Hsu
et al. (2008). However, this approach would not
give us the correct probability distribution over trees
as the probability calculations for different paths
through the trees are not independent. Thus, al-
though one could calculate the probability of a path
from the root to a leaf, one cannot generalize from
this probability to say anything about the neighbor-
ing nodes or words. Put another way, when a par-
ent has more than the one descendant, one has to be
careful to take into account that the hidden variables
at each child node are all conditioned on the hidden
variable of the parent.
3.1 A latent variable generative model for
dependency parsing
In the standard setting, we are given training exam-
ples where each training example consists of a se-
quence of words x1, ... , xm together with a depen-
dency structure over those words, and we want to
estimate the probability of the observed structure.
This marginal probability estimates can then be used
to build an actual generative dependency parser or,
since the marginal probability is conditioned on the
tree structure, it can be used re-rank the outputs of a
parser.
As in the conventional HMM described in the pre-
vious section, we can define a simple latent variable
first order dependency parsing model by introduc-
ing a hidden variable hi for each word xi. The
joint probability of a sequence of observed nodes
x1, ... , xm together with hidden nodes h1, ... , hm
can be written as
</bodyText>
<equation confidence="0.9527278">
p(x1,... ,xm, h1, ... , hm)
m m
= 7rhi ri td(j)(hj|hpa(j)) H o(xj|hj)
j=2 j=1
(2)
</equation>
<page confidence="0.780738">
208
</page>
<bodyText confidence="0.679649">
counts): left child)) 5:
</bodyText>
<equation confidence="0.998798">
µ = Gπ
[ˆµ]i = Xn ¯c(u)Uu(i)
u=1
</equation>
<bodyText confidence="0.770226">
where ¯c(u) = c(u), c(u) is the count of ob-
</bodyText>
<equation confidence="0.89374825">
N1
servation u in the training sample, and N1 =
P
uEn c(u).
</equation>
<listItem confidence="0.973891">
• Define the kxk matrices ΣL and ΣR (left child-
parent and right child-parent bigram counts):
</listItem>
<equation confidence="0.9182725">
[ˆΣL]i,j = Xn Xn ¯cL(u, v)Uu(j)Uv(i)
u=1 v=1
ΣL = GTuLdiag(π)GT
[ˆΣR]i,j = Xn Xn ¯cR(u, v)Uu(j)Uv(i)
u=1 v=1
ΣR = GTuRdiag(π)GT
where ¯cL(u, v) =cL(u,v)
N�L , cL(u, v) is the count
</equation>
<bodyText confidence="0.952858">
of bigram (u, v) where u is the left child and
v is the parent in the training sample, and
N2L = P(u,v)Enxn cL(u, v). Define ¯cR(u, v)
similarly.
</bodyText>
<listItem confidence="0.95435">
• Define k x k x k tensor K (left child-parent-
right child trigram counts):
</listItem>
<bodyText confidence="0.546588">
where ¯c(w, u, v) = c(w,u ,v), c(w, u, v) is
the count of bigram (w, u, v) where w is
the left child, u is the parent and v is the
right child in the training sample, and N3 =
P(w,u,v)Enxnxn c(w, u, v).
</bodyText>
<listItem confidence="0.969505">
• Define k xk matrices ΩL and ΩR (skip-bigram
counts (left child-right child) and (right child-
</listItem>
<equation confidence="0.9989105">
[ˆΩL]i,j = Xn Xn Xn ¯c(u, v, w)Uw(i)Uu(j)
u=1 v=1 w=1
ΩL = GTLTuRdiag(π)GT
[ˆΩR]i,j = Xn Xn Xn ¯c(u, v, w)Uw(j)Uu(i)
u=1 v=1 w=1
ΩR = GTRTuLdiag(π)GT
</equation>
<subsectionHeader confidence="0.994058">
3.3 Parameter estimation
</subsectionHeader>
<bodyText confidence="0.98962575">
Using the above definitions, we can estimate the pa-
rameters of the model, namely µ, ΣL, ΣR, ΩL, ΩR
and K, from the training data and define observables
useful for the dependency model as6
</bodyText>
<equation confidence="0.999651">
c1 = µ
cT. = µT Σ�1
R
EL = ΩLΣ�1
R
ER = ΩRΣ�1
L
D(y) = E�1
L K(y)Σ�1
R
</equation>
<bodyText confidence="0.9963108">
As we will see, these quantities allow us to recur-
sively compute the marginal probability of the de-
pendency tree, ˆp(x1, ... , xm), in a bottom up man-
ner by using belief propagation.
To see this, let hch(i) be the set of hidden chil-
dren of hidden node i (in Figure 2 for instance,
hch(1) = 12,31) and let och(i) be the set of ob-
served children of hidden node i (in the same figure
och(i) = 111). Then compute the marginal proba-
bility p(x1, ... , xm) from Equation 2 as
</bodyText>
<equation confidence="0.9982545">
Yri(h) = αj(h) Y o(xjjh) (3)
jEhch(i) jEoch(i)
</equation>
<bodyText confidence="0.913601">
where αi(h) is defined by summing over all
the hidden random variables i.e., αi(h) =
</bodyText>
<equation confidence="0.997806428571429">
P
h&apos; p(h&apos;jh)ri(h&apos;).
This can be written in a compact matrix form as
rT= 1T Y
Z
jEhch(i)
diag(OTδxj) (4)
</equation>
<footnote confidence="0.97046275">
5Note than OR = 52TL, which is not immediately obvious
from the matrix representations.
6The details of the derivation follow directly from the matrix
versions of the variables.
</footnote>
<equation confidence="0.99395">
ˆKi,j,l = Xn Xn Xn ¯c(u, v, w)Uw(i)Uu(j)Uv(l)
u=1 v=1 w=1
K(y) = GTLdiag(GTy)TuRdiag(π)GT
diag(T T dj��rj)
Y�
jEoch(i)
</equation>
<page confidence="0.992581">
209
</page>
<bodyText confidence="0.99993825">
where →−ri is a vector of size k (the dimensionality of
the hidden space) of values ri(h). Note that since in
Equation 2 we condition on whether xj is the left or
right child of its parent, we have separate transition
matrices for left and right transitions from a given
hidden node dj ∈ {L, R}.
The recursive computation can be written in terms
of observables as:
</bodyText>
<equation confidence="0.9926594">
→− ri T = cT
� rl D(ET dj−→ rj )
jEhch(i)
rl· D((UTU)_1UTδxj)
jEoch(i)
</equation>
<bodyText confidence="0.9957805">
The final calculation for the marginal probability
of a given sequence is
</bodyText>
<equation confidence="0.9802325">
ˆp(x1, . . . , xm) =→−T
r1 (5)
</equation>
<bodyText confidence="0.994386">
The spectral estimation procedure is described be-
low in Algorithm 1.
Algorithm 1 Spectral dependency parsing (Comput-
ing marginal probability of a tree.)
</bodyText>
<equation confidence="0.9984235">
T =cT rl
i T
co
jEhch(i)
rl· D((UTU)−1UTδx,)
jEoch(i)
</equation>
<footnote confidence="0.591686">
5: end for
6: Finally compute
p(/ →−T
x1,... , xmi) = r1 c1
#The marginal probability of an entire tree.
</footnote>
<subsectionHeader confidence="0.993665">
3.4 Sample complexity
</subsectionHeader>
<bodyText confidence="0.9863714">
Our main theoretical result states that the above
scheme for spectral estimation of marginal proba-
bilities provides a guaranteed consistent estimation
scheme for the marginal probabilities:
Theorem 3.1. Let the sequence {x1, ... , xm} be
generated by an k ≥ 2 state HMM. Suppose we are
given a U which has the property that UTO is in-
vertible, and |Uij |≤ 1. Suppose we use equation
(5) to estimate the probability based on N indepen-
dent triples. Then
</bodyText>
<equation confidence="0.941059285714286">
c (δ)
N ≥ Cm 2 2 log
(6)
where Cm is specified in the appendix, implies that
p(x1, ... , xm)
p(x1, ... , xm)
holds with probability at least 1 − δ.
</equation>
<bodyText confidence="0.9912135">
Proof. A sketch of the proof, in the case without di-
rectional transition parameters, can be found in the
appendix. The proof with directional transition pa-
rameters is almost identical.
</bodyText>
<sectionHeader confidence="0.99755" genericHeader="method">
4 Experimental Evaluation
</sectionHeader>
<bodyText confidence="0.99995875">
Since our algorithm can score any given tree struc-
ture by computing its marginal probability, a natu-
ral way to benchmark our parser is to generate n-
best dependency trees using some standard parser
and then use our algorithm to re-rank the candidate
dependency trees, e.g. using the log spectral prob-
ability as described in Algorithm 1 as a feature in a
discriminative re-ranker.
</bodyText>
<subsectionHeader confidence="0.968982">
4.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.994445769230769">
Our base parser was the discriminatively trained
MSTParser (McDonald, 2006), which implements
both first and second order parsers and is trained
using MIRA (Crammer et al., 2006) and used the
standard baseline features as described in McDon-
ald (2006).
We tested our methods on the English Penn Tree-
bank (Marcus et al., 1993). We use the standard
splits of Penn Treebank; i.e., we used sections 2-21
for training, section 22 for development and section
23 for testing. We used the PennConverter7 tool to
convert Penn Treebank from constituent to depen-
dency format. Following (McDonald, 2006; Koo
</bodyText>
<footnote confidence="0.9781615">
7http://nlp.cs.lth.se/software/treebank_
converter/
</footnote>
<listItem confidence="0.616691166666667">
1: Input: Training examples- x(i) for i ∈ {1, ... , M}
along with dependency structures where each se-
quence x(i) = x(i)
1 , ... , x(i)mi.
2: Compute the spectral parameters ˆµ,
ˆΩL, and Kˆ
</listItem>
<bodyText confidence="0.694513">
#Now, for a given sentence, we can recursively com-
pute the following:
</bodyText>
<equation confidence="0.795404285714286">
3: for x(i)
j for j ∈ {mi, ... ,1} do
4: Compute:
ˆΣR, ˆΣL, ˆΩR,
D(ETd�−→rj)
1 − E ≤ ����
i ≤ 1+E
</equation>
<page confidence="0.995224">
210
</page>
<bodyText confidence="0.999074285714286">
et al., 2008), we used the POS tagger by Ratnaparkhi
(1996) trained on the full training data to provide
POS tags for development and test sets and used 10-
way jackknifing to generate tags for the training set.
As is common practice we stripped our sentences of
all the punctuation. We evaluated our approach on
sentences of all lengths.
</bodyText>
<subsectionHeader confidence="0.999635">
4.2 Details of spectral learning
</subsectionHeader>
<bodyText confidence="0.9999926">
For the spectral learning phase, we need to just col-
lect word counts from the training data as described
above, so there are no tunable parameters as such.
However, we need to have access to an attribute dic-
tionary U which contains a k dimensional represen-
tation for each word in the corpus. A possible way
of generating U as suggested by Hsu et al. (2008) is
by performing SVD on bigrams P21 and using the
left eigenvectors as U. We instead used the eigen-
feature dictionary proposed by Dhillon et al. (2011)
(LR-MVL) which is obtained by performing CCA
on neighboring words and has provably better sam-
ple complexity for rare words compared to the SVD
alternative.
We induced the LR-MVL embeddings for words
using the Reuters RCV1 corpus which contains
about 63 million tokens in 3.3 million sentences and
used their context oblivious embeddings as our esti-
mate of U. We experimented with different choices
of k (the size of the low dimensional projection)
on the development set and found k = 10 to work
reasonably well and fast. Using k = 10 we were
able to estimate our spectral learning parameters
A, EL,R, QL,R, K from the entire training data in un-
der 2 minutes on a 64 bit Intel 2.4 Ghz processor.
</bodyText>
<subsectionHeader confidence="0.999789">
4.3 Re-ranking the outputs of MST parser
</subsectionHeader>
<bodyText confidence="0.999838583333333">
We could not find any previous work which de-
scribes features for discriminative re-ranking for de-
pendency parsing, which is due to the fact that un-
like constituency parsing, the base parsers for depen-
dency parsing are discriminative (e.g. MST parser)
which obviates the need for re-ranking as one could
add a variety of features to the baseline parser itself.
However, parse re-ranking is a good testbed for our
spectral dependency parser which can score a given
tree. So, we came up with a baseline set of features
to use in an averaged perceptron re-ranker (Collins,
2002). Our baseline features comprised of two main
</bodyText>
<table confidence="0.998797666666667">
Method Accuracy Complete
I Order
MST Parser (No RR) 90.8 37.2
RR w. Base. Features 91.3 37.5
RR w. Base. Features +loge 91.7 37.8
II Order
MST Parser (No RR) 91.8 40.6
RR w. Base. Features 92.4 41.0
RR w. Base. Features +loge 92.7 41.3
</table>
<tableCaption confidence="0.692884375">
Table 1: (Unlabeled) Dependency Parse re-ranking re-
sults for English test set (Section 23). Note: 1). RR =
Re-ranking 2). Accuracy is the number of words which
correctly identified their parent and Complete is the num-
ber of sentences for which the entire dependency tree was
correct. 3). Base. Features are the two re-ranking fea-
tures described in Section 4.3. 4). log is the spectral log
probability feature.
</tableCaption>
<bodyText confidence="0.999717555555556">
features which capture information that varies across
the different n-best parses and moreover were not
used as features by the baseline MST parser, (POS-
left-modifier ∧ POS-head ∧ POS-right-modifier)
and (POS-left/right-modifier ∧ POS-head ∧ POS-
grandparent)8. In addition to that we used the log of
spectral probability (p(x1, ... , xM) - as calculated
using Algorithm 1) as a feature.
We used the MST parser trained on entire training
data to generate a list of n-best parses for the devel-
opment and test sets. The n-best parses for the train-
ing set were generated by 3-fold cross validation,
where we train on 2 folds to get the parses for the
third fold. In all our experiments we used n = 50.
The results are shown in Table 1. As can be seen,
the best results give up to 4.6% reduction in error
over the re-ranker which uses just the baseline set of
features.
</bodyText>
<sectionHeader confidence="0.998136" genericHeader="discussions">
5 Discussion and Future Work
</sectionHeader>
<bodyText confidence="0.997572333333333">
Spectral learning of structured latent variable mod-
els in general is a promising direction as has been
shown by the recent interest in this area. It al-
lows us to circumvent the ubiquitous problem of get-
ting stuck in local minima when estimating the la-
tent variable models via EM. In this paper we ex-
</bodyText>
<footnote confidence="0.691721">
8One might be able to come up with better features for de-
pendency parse re-ranking. Our goal in this paper was just to
get a reasonable baseline.
</footnote>
<page confidence="0.996762">
211
</page>
<bodyText confidence="0.999546125">
tended the spectral learning ideas to learn a simple
yet powerful dependency parser. As future work, we
are working on building an end-to-end parser which
would involve coming up with a spectral version of
the inside-outside algorithm for our setting. We are
also working on extending it to learn more power-
ful grammars e.g. split head-automata grammars
(SHAG) (Eisner and Satta, 1999).
</bodyText>
<sectionHeader confidence="0.998918" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.998985923076923">
In this paper we proposed a novel spectral method
for dependency parsing. Unlike EM trained gen-
erative latent variable models, our method does not
get stuck in local optima, it gives consistent param-
eter estimates, and it is extremely fast to train. We
worked out the theory of a simple yet powerful gen-
erative model and showed how it can be learned us-
ing a spectral method. As a pilot experimental evalu-
ation we showed the efficacy of our approach by us-
ing the spectral probabilities output by our model for
re-ranking the outputs of MST parser. Our method
reduced the error of the baseline re-ranker by up to
a moderate 4.6%.
</bodyText>
<sectionHeader confidence="0.997822" genericHeader="acknowledgments">
7 Appendix
</sectionHeader>
<bodyText confidence="0.987281">
This appendix offers a sketch of the proof of The-
orem 1. The proof uses the following definitions,
which are slightly modified from those of Foster
et al. (2012).
Definition 1. Define A as the smallest element of µ,
E−1, Q−1, and K(). In other words,
</bodyText>
<equation confidence="0.802186">
A - min{min |µi|, min |E−1 ij|, min |Q−ij1 |,
i i,j i,j
min |Kijk|, min  |Eij |, min  |Qij |, }
i,j,k i,j i,j
</equation>
<bodyText confidence="0.998824862068966">
where Kijk = K(δj)ik are the elements of the ten-
sor K().
Definition 2. Define σk as the smallest singular
value of E and Q.
The proof relies on the fact that a row vector mul-
tiplied by a series of matrices, and finally multiplied
by a column vector amounts to a sum over all possi-
ble products of individual entries in the vectors and
matrices. With this in mind, if we bound the largest
relative error of any particular entry in the matrix by,
say, ω, and there are, say, s parameters (vectors and
matrices) being multiplied together, then by simple
algebra the total relative error of the sum over the
products is bounded by ωs.
The proof then follows from two basic steps.
First, one must bound the maximal relative error, ω
for any particular entry in the parameters, which can
be done using central limit-type theorems and the
quantity A described above. Then, to calculate the
exponent s one simply counts the number of param-
eters multiplied together when calculating the prob-
ability of a particular sequence of observations.
Since each hidden node is associated with exactly
one observed node, it follows that s = 12m + 2L,
where L is the number of levels (for instance in our
example “Kilroy was here” there are two levels). s
can be easily computed for arbitrary tree topologies.
It follows from Foster et al. (2012) that we achieve
a sample complexity
</bodyText>
<equation confidence="0.936775">
N ) 128k 2s2
log C2k/
E2 A2
σk \ δ
(�/1 + E − 1)2 (7)
</equation>
<bodyText confidence="0.992804285714286">
leading to the theorem stated above.
Lastly, note that in reality one does not see A and
σk but instead estimates of these quantities; Foster
et al. (2012) shows how to incorporate the accuracy
of the estimates into the sample complexity.
Acknowledgement: We would like to thank
Emily Pitler for valuable feedback on the paper.
</bodyText>
<sectionHeader confidence="0.997543" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.957145357142857">
Shay Cohen, Karl Stratos, Michael Collins, Dean
Foster, and Lyle Ungar. Spectral learning of
latent-variable pcfgs. In Association of Compu-
tational Linguistics (ACL), volume 50, 2012.
Michael Collins. Ranking algorithms for named-
entity extraction: boosting and the voted percep-
tron. In Proceedings of the 40th Annual Meet-
ing on Association for Computational Linguis-
tics, ACL ’02, pages 489–496, Stroudsburg, PA,
USA, 2002. Association for Computational Lin-
guistics. URL http://dx.doi.org/10.
3115/1073083.1073165.
Michael Collins and Terry Koo. Discriminative
reranking for natural language parsing. Comput.
</reference>
<equation confidence="0.699597">
�1
� �� �
E2/s2
</equation>
<page confidence="0.99554">
212
</page>
<reference confidence="0.999875987804878">
Linguist., 31(1):25–70, March 2005. ISSN 0891-
2017.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai
Shalev-Shwartz, and Yoram Singer. Online
passive-aggressive algorithms. Journal of Ma-
chine Learning Research, 7:551–585, 2006.
A. P. Dempster, N. M. Laird, and D. B. Rubin. Max-
imum likelihood from incomplete data via the em
algorithm. JRSS, SERIES B, 39(1):1–38, 1977.
Paramveer S. Dhillon, Dean Foster, and Lyle Un-
gar. Multi-view learning of word embeddings via
cca. In Advances in Neural Information Process-
ing Systems (NIPS), volume 24, 2011.
Jason Eisner and Giorgio Satta. Efficient pars-
ing for bilexical context-free grammars and head-
automaton grammars. In Proceedings of the 37th
Annual Meeting of the Association for Computa-
tional Linguistics (ACL), pages 457–464, Univer-
sity of Maryland, June 1999. URL http://cs.
jhu.edu/˜jason/papers/#acl99.
Dean Foster, Jordan Rodu, and Lyle Ungar. Spec-
tral dimensionality reduction for HMMs. ArXiV
http://arxiv.org/abs/1203.6130, 2012.
D Hsu, S M. Kakade, and Tong Zhang. A spec-
tral algorithm for learning hidden markov models.
arXiv:0811.4413v2, 2008.
Terry Koo, Xavier Carreras, and Michael Collins.
Simple semi-supervised dependency parsing. In
In Proc. ACL/HLT, 2008.
F. Luque, A. Quattoni, B. Balle, and X. Carreras.
Spectral learning for non-deterministic depen-
dency parsing. In EACL, 2012.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. Building a large annotated
corpus of english: the penn treebank. Comput.
Linguist., 19:313–330, June 1993. ISSN 0891-
2017.
Ryan McDonald. Discriminative learning and span-
ning tree algorithms for dependency parsing. PhD
thesis, University of Pennsylvania, Philadelphia,
PA, USA, 2006. AAI3225503.
Bernard Merialdo. Tagging english text with a prob-
abilistic model. Comput. Linguist., 20:155–171,
June 1994. ISSN 0891-2017.
Gabriele Antonio Musillo and Paola Merlo. Un-
lexicalised hidden variable models of split de-
pendency grammars. In Proceedings of the 46th
Annual Meeting of the Association for Computa-
tional Linguistics on Human Language Technolo-
gies: Short Papers, HLT-Short ’08, pages 213–
216, Stroudsburg, PA, USA, 2008. Association
for Computational Linguistics.
Ankur P. Parikh, Le Song, and Eric P. Xing. A spec-
tral algorithm for latent tree graphical models. In
ICML, pages 1065–1072, 2011.
Slav Petrov, Leon Barrett, Romain Thibaux, and
Dan Klein. Learning accurate, compact, and in-
terpretable tree annotation. In Proceedings of the
21st International Conference on Computational
Linguistics and the 44th annual meeting of the As-
sociation for Computational Linguistics, ACL-44,
pages 433–440, Stroudsburg, PA, USA, 2006. As-
sociation for Computational Linguistics.
Ariadna Quattoni, Michael Collins, and Trevor Dar-
rell. Conditional random fields for object recog-
nition. In In NIPS, pages 1097–1104. MIT Press,
2004.
Lawrence R. Rabiner. A tutorial on hidden markov
models and selected applications in speech recog-
nition. In Proceedings of the IEEE, pages 257–
286, 1989.
Adwait Ratnaparkhi. A Maximum Entropy Model
for Part-Of-Speech Tagging. In Eric Brill and
Kenneth Church, editors, Proceedings of the Em-
pirical Methods in Natural Language Processing,
pages 133–142, 1996.
Sebastiaan Terwijn. On the learnability of hidden
markov models. In Proceedings of the 6th Inter-
national Colloquium on Grammatical Inference:
Algorithms and Applications, ICGI ’02, pages
261–268, London, UK, UK, 2002. Springer-
Verlag. ISBN 3-540-44239-1.
</reference>
<page confidence="0.999396">
213
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.552251">
<title confidence="0.999722">Spectral Dependency Parsing with Latent Variables</title>
<author confidence="0.985484">Lyle</author>
<affiliation confidence="0.791357">amp; Information Science/ University of Pennsylvania, Philadelphia, PA, Science, Columbia University, New York, NY,</affiliation>
<email confidence="0.999684">mcollins@cs.columbia.edu</email>
<abstract confidence="0.9954925">Recently there has been substantial interest in using spectral methods to learn generative sequence models like HMMs. Spectral methods are attractive as they provide globally consistent estimates of the model parameters and are very fast and scalable, unlike EM methods, which can get stuck in local minima. In this paper, we present a novel extension of this class of spectral methods to learn dependency tree structures. We propose a simple yet powerful latent variable generative model for dependency parsing, and a spectral learning method to efficiently estimate it. As a pilot experimental evaluation, we use the spectral tree probabilities estimated by our model to re-rank the outputs of a near state-of-theart parser. Our approach gives us a moderate in error of up to the baseline re-ranker.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Shay Cohen</author>
<author>Karl Stratos</author>
<author>Michael Collins</author>
<author>Dean Foster</author>
<author>Lyle Ungar</author>
</authors>
<title>Spectral learning of latent-variable pcfgs.</title>
<date>2012</date>
<booktitle>In Association of Computational Linguistics (ACL),</booktitle>
<volume>50</volume>
<contexts>
<context position="4640" citStr="Cohen et al. (2012)" startWordPosition="730" endWordPosition="733"> and trigram counts sensitive to the underlying dependency structure of the given sentence. Recently, Luque et al. (2012) have also proposed a spectral method for dependency parsing, however they deal with horizontal markovization and use hidden states to model sequential dependencies within a word’s sequence of children. In contrast with that, in this paper, we propose a spectral learning algorithm where latent states are not restricted to HMM-like distributions of modifier sequences for a particular head, but instead allow information to be propagated through the entire tree. More recently, Cohen et al. (2012) have proposed a spectral method for learning PCFGs. Its worth noting that recent work by Parikh et al. (2011) also extends Hsu et al. (2008) to latent variable dependency trees like us but under the restrictive conditions that model parameters are trained for a specified, albeit arbitrary, tree topology.2 In other words, all training sentences and test sentences must have identical tree topologies. By doing this they allow for node-specific model parameters, but must retrain the model entirely when a different tree topology is encountered. Our model on the other hand allows the flexibility an</context>
</contexts>
<marker>Cohen, Stratos, Collins, Foster, Ungar, 2012</marker>
<rawString>Shay Cohen, Karl Stratos, Michael Collins, Dean Foster, and Lyle Ungar. Spectral learning of latent-variable pcfgs. In Association of Computational Linguistics (ACL), volume 50, 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Ranking algorithms for namedentity extraction: boosting and the voted perceptron.</title>
<date>2002</date>
<journal>URL</journal>
<booktitle>In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL ’02,</booktitle>
<volume>10</volume>
<pages>489--496</pages>
<location>Stroudsburg, PA, USA,</location>
<contexts>
<context position="21602" citStr="Collins, 2002" startWordPosition="3770" endWordPosition="3771">.3 Re-ranking the outputs of MST parser We could not find any previous work which describes features for discriminative re-ranking for dependency parsing, which is due to the fact that unlike constituency parsing, the base parsers for dependency parsing are discriminative (e.g. MST parser) which obviates the need for re-ranking as one could add a variety of features to the baseline parser itself. However, parse re-ranking is a good testbed for our spectral dependency parser which can score a given tree. So, we came up with a baseline set of features to use in an averaged perceptron re-ranker (Collins, 2002). Our baseline features comprised of two main Method Accuracy Complete I Order MST Parser (No RR) 90.8 37.2 RR w. Base. Features 91.3 37.5 RR w. Base. Features +loge 91.7 37.8 II Order MST Parser (No RR) 91.8 40.6 RR w. Base. Features 92.4 41.0 RR w. Base. Features +loge 92.7 41.3 Table 1: (Unlabeled) Dependency Parse re-ranking results for English test set (Section 23). Note: 1). RR = Re-ranking 2). Accuracy is the number of words which correctly identified their parent and Complete is the number of sentences for which the entire dependency tree was correct. 3). Base. Features are the two re-</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. Ranking algorithms for namedentity extraction: boosting and the voted perceptron. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL ’02, pages 489–496, Stroudsburg, PA, USA, 2002. Association for Computational Linguistics. URL http://dx.doi.org/10. 3115/1073083.1073165.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Terry Koo</author>
</authors>
<title>Discriminative reranking for natural language parsing.</title>
<date>2005</date>
<journal>Comput. Linguist.,</journal>
<volume>31</volume>
<issue>1</issue>
<pages>0891--2017</pages>
<contexts>
<context position="5829" citStr="Collins and Koo, 2005" startWordPosition="925" endWordPosition="928">er hand allows the flexibility and efficiency of processing sentences with a variety of tree topologies from a single training run. Most of the current state-of-the-art dependency parsers are discriminative parsers (Koo et al., 2008; McDonald, 2006) due to the flexibility of representations which can be used as features leading to better accuracies and the ease of reproducibility of results. However, unlike discriminative models, generative models can exploit unlabeled data. Also, as is common in statistical parsing, re-ranking the outputs of a parser leads to significant reductions in error (Collins and Koo, 2005). Since our spectral learning algorithm uses a gen2This can be useful in modeling phylogeny trees for instance, but precludes most NLP applications, since there is a need to model the full set of different tree topologies possible in parsing. Figure 1: Sample dependency parsing tree for “Kilroy was here” erative model of words given a tree structure, it can score a tree structure i.e. its probability of generation. Thus, it can be used to re-rank the n-best outputs of a given parser. The remainder of the paper is organized as follows. In the next section we introduce the notation and give a br</context>
</contexts>
<marker>Collins, Koo, 2005</marker>
<rawString>Michael Collins and Terry Koo. Discriminative reranking for natural language parsing. Comput. Linguist., 31(1):25–70, March 2005. ISSN 0891-2017.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koby Crammer</author>
<author>Ofer Dekel</author>
<author>Joseph Keshet</author>
<author>Shai Shalev-Shwartz</author>
<author>Yoram Singer</author>
</authors>
<title>Online passive-aggressive algorithms.</title>
<date>2006</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>7</volume>
<contexts>
<context position="18580" citStr="Crammer et al., 2006" startWordPosition="3238" endWordPosition="3241">ost identical. 4 Experimental Evaluation Since our algorithm can score any given tree structure by computing its marginal probability, a natural way to benchmark our parser is to generate nbest dependency trees using some standard parser and then use our algorithm to re-rank the candidate dependency trees, e.g. using the log spectral probability as described in Algorithm 1 as a feature in a discriminative re-ranker. 4.1 Experimental Setup Our base parser was the discriminatively trained MSTParser (McDonald, 2006), which implements both first and second order parsers and is trained using MIRA (Crammer et al., 2006) and used the standard baseline features as described in McDonald (2006). We tested our methods on the English Penn Treebank (Marcus et al., 1993). We use the standard splits of Penn Treebank; i.e., we used sections 2-21 for training, section 22 for development and section 23 for testing. We used the PennConverter7 tool to convert Penn Treebank from constituent to dependency format. Following (McDonald, 2006; Koo 7http://nlp.cs.lth.se/software/treebank_ converter/ 1: Input: Training examples- x(i) for i ∈ {1, ... , M} along with dependency structures where each sequence x(i) = x(i) 1 , ... , x</context>
</contexts>
<marker>Crammer, Dekel, Keshet, Shalev-Shwartz, Singer, 2006</marker>
<rawString>Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-Shwartz, and Yoram Singer. Online passive-aggressive algorithms. Journal of Machine Learning Research, 7:551–585, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A P Dempster</author>
<author>N M Laird</author>
<author>D B Rubin</author>
</authors>
<title>Maximum likelihood from incomplete data via the em algorithm.</title>
<date>1977</date>
<journal>JRSS, SERIES B,</journal>
<volume>39</volume>
<issue>1</issue>
<contexts>
<context position="1759" citStr="Dempster et al., 1977" startWordPosition="263" endWordPosition="266">gives us a moderate reduction in error of up to 4.6% over the baseline re-ranker. 1 Introduction Markov models have been for two decades a workhorse of statistical pattern recognition with applications ranging from speech to vision to language. Adding latent variables to these models gives us additional modeling power and have shown success in applications like POS tagging (Merialdo, 1994), speech recognition (Rabiner, 1989) and object recognition (Quattoni et al., 2004). However, this comes at the cost that the resulting parameter estimation problem becomes non-convex and techniques like EM (Dempster et al., 1977) which are used to estimate the parameters can only lead to locally optimal solutions. Recent work by Hsu et al. (2008) has shown that globally consistent estimates of the parameters of HMMs can be found by using spectral methods, particularly by singular value decomposition (SVD) of appropriately defined linear systems. They avoid the NP Hard problem of the global optimization problem of the HMM parameters (Terwijn, 2002), by putting restrictions on the smallest singular value of the HMM parameters. The main intuition behind the model is that, although the observed data (i.e. words) seem to l</context>
</contexts>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from incomplete data via the em algorithm. JRSS, SERIES B, 39(1):1–38, 1977.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paramveer S Dhillon</author>
<author>Dean Foster</author>
<author>Lyle Ungar</author>
</authors>
<title>Multi-view learning of word embeddings via cca.</title>
<date>2011</date>
<booktitle>In Advances in Neural Information Processing Systems (NIPS),</booktitle>
<volume>24</volume>
<contexts>
<context position="8899" citStr="Dhillon et al., 2011" startWordPosition="1496" endWordPosition="1499">e of observations x1, ... , xm and a sequence of hidden states h1, ... , hm is: 2.2 Fully observable representation Before presenting the model, we need to address a few more points. First, let U be a “representation matrix” (eigenfeature dictionary) which maps each observation to a reduced dimension space (n —* k) that satisfies the conditions: • UTO is invertible • |Uij |&lt; 1. Hsu et al. (2008); Foster et al. (2012) discuss U in more detail, but U can, for example, be obtained by the SVD of the bigram probability matrix (where Pij = p(xt+1 = i|xt = j)) or by doing CCA on neighboring n-grams (Dhillon et al., 2011). Letting yi = UTδxi, we have p(x1, ... , xm) = cTooC(ym)C(ym−1) ... C(y1)c1 (1) p(x1, ... ,xm, h1, ... , hm) = πh1 m−1H Thj,hj_1 m Oxj,hj j=2 H j=1 Now, we can write the marginal probability of a sequence of observations as p(x1, ... xm) �= p(x1, ... , xm, h1, ... , hm) h1,...,hm which can be expressed in matrix form4 as: p(x1, ... , xm) = 1TAxmAxm_1 ··· Am1π where Axm = Tdiag(OTδxm), and 1 is a kdimensional vector with every entry equal to 1. A is called an “observation operator”, and is effectively a third order tensor, and Axm which is a matrix, gives the distribution vector over states at</context>
<context position="20290" citStr="Dhillon et al. (2011)" startWordPosition="3541" endWordPosition="3544"> of all the punctuation. We evaluated our approach on sentences of all lengths. 4.2 Details of spectral learning For the spectral learning phase, we need to just collect word counts from the training data as described above, so there are no tunable parameters as such. However, we need to have access to an attribute dictionary U which contains a k dimensional representation for each word in the corpus. A possible way of generating U as suggested by Hsu et al. (2008) is by performing SVD on bigrams P21 and using the left eigenvectors as U. We instead used the eigenfeature dictionary proposed by Dhillon et al. (2011) (LR-MVL) which is obtained by performing CCA on neighboring words and has provably better sample complexity for rare words compared to the SVD alternative. We induced the LR-MVL embeddings for words using the Reuters RCV1 corpus which contains about 63 million tokens in 3.3 million sentences and used their context oblivious embeddings as our estimate of U. We experimented with different choices of k (the size of the low dimensional projection) on the development set and found k = 10 to work reasonably well and fast. Using k = 10 we were able to estimate our spectral learning parameters A, EL,</context>
</contexts>
<marker>Dhillon, Foster, Ungar, 2011</marker>
<rawString>Paramveer S. Dhillon, Dean Foster, and Lyle Ungar. Multi-view learning of word embeddings via cca. In Advances in Neural Information Processing Systems (NIPS), volume 24, 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
<author>Giorgio Satta</author>
</authors>
<title>Efficient parsing for bilexical context-free grammars and headautomaton grammars.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>457--464</pages>
<institution>University of Maryland,</institution>
<note>URL http://cs. jhu.edu/˜jason/papers/#acl99.</note>
<contexts>
<context position="24021" citStr="Eisner and Satta, 1999" startWordPosition="4191" endWordPosition="4194">k in local minima when estimating the latent variable models via EM. In this paper we ex8One might be able to come up with better features for dependency parse re-ranking. Our goal in this paper was just to get a reasonable baseline. 211 tended the spectral learning ideas to learn a simple yet powerful dependency parser. As future work, we are working on building an end-to-end parser which would involve coming up with a spectral version of the inside-outside algorithm for our setting. We are also working on extending it to learn more powerful grammars e.g. split head-automata grammars (SHAG) (Eisner and Satta, 1999). 6 Conclusion In this paper we proposed a novel spectral method for dependency parsing. Unlike EM trained generative latent variable models, our method does not get stuck in local optima, it gives consistent parameter estimates, and it is extremely fast to train. We worked out the theory of a simple yet powerful generative model and showed how it can be learned using a spectral method. As a pilot experimental evaluation we showed the efficacy of our approach by using the spectral probabilities output by our model for re-ranking the outputs of MST parser. Our method reduced the error of the ba</context>
</contexts>
<marker>Eisner, Satta, 1999</marker>
<rawString>Jason Eisner and Giorgio Satta. Efficient parsing for bilexical context-free grammars and headautomaton grammars. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics (ACL), pages 457–464, University of Maryland, June 1999. URL http://cs. jhu.edu/˜jason/papers/#acl99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dean Foster</author>
<author>Jordan Rodu</author>
<author>Lyle Ungar</author>
</authors>
<title>Spectral dimensionality reduction for HMMs. ArXiV</title>
<date>2012</date>
<location>http://arxiv.org/abs/1203.6130,</location>
<contexts>
<context position="3405" citStr="Foster et al. (2012)" startWordPosition="540" endWordPosition="543"> approach of Hsu et al. (2008) to learn dependency tree structures with latent variables.1 Petrov et al. (2006) and Musillo and Merlo (2008) have shown that learning PCFGs and dependency grammars respectively with latent variables can produce parsers with very good generalization performance. However, both these approaches rely on EM for parameter estimation and can benefit from using spectral methods. We propose a simple yet powerful latent variable generative model for use with dependency pars1Actually, instead of using the model by Hsu et al. (2008) we work with a related model proposed by Foster et al. (2012) which addresses some of the shortcomings of the earlier model which we detail below. 205 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 205–213, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics ing which has one hidden node for each word in the sentence, like the one shown in Figure 1 and work out the details for the parameter estimation of the corresponding spectral learning model. At a very high level, the parameter estimation of our model involves collecting</context>
<context position="6525" citStr="Foster et al., 2012" startWordPosition="1049" endWordPosition="1052">eling phylogeny trees for instance, but precludes most NLP applications, since there is a need to model the full set of different tree topologies possible in parsing. Figure 1: Sample dependency parsing tree for “Kilroy was here” erative model of words given a tree structure, it can score a tree structure i.e. its probability of generation. Thus, it can be used to re-rank the n-best outputs of a given parser. The remainder of the paper is organized as follows. In the next section we introduce the notation and give a brief overview of the spectral algorithm for learning HMMs (Hsu et al., 2008; Foster et al., 2012). In Section 3 we describe our proposed model for dependency parsing in detail and work out the theory behind it. Section 4 provides experimental evaluation of our model on Penn Treebank data. We conclude with a brief summary and future avenues for research. 2 Spectral Algorithm For Learning HMMs In this section we describe the spectral algorithm for learning HMMs.3 2.1 Notation The HMM that we consider in this section is a sequence of hidden states h E 11, ... , k} that follow the Markov property: p(ht|h1, ... , ht−1) = p(ht|ht−1) and a sequence of observations x E 11, ... , n} such that p(xt</context>
<context position="8698" citStr="Foster et al. (2012)" startWordPosition="1457" endWordPosition="1460">be the vector of length n with a 1 in the jth entry and 0 everywhere else, and diag(v) to be the matrix with the entries of v on the diagonal and 0 everywhere else. The joint distribution of a sequence of observations x1, ... , xm and a sequence of hidden states h1, ... , hm is: 2.2 Fully observable representation Before presenting the model, we need to address a few more points. First, let U be a “representation matrix” (eigenfeature dictionary) which maps each observation to a reduced dimension space (n —* k) that satisfies the conditions: • UTO is invertible • |Uij |&lt; 1. Hsu et al. (2008); Foster et al. (2012) discuss U in more detail, but U can, for example, be obtained by the SVD of the bigram probability matrix (where Pij = p(xt+1 = i|xt = j)) or by doing CCA on neighboring n-grams (Dhillon et al., 2011). Letting yi = UTδxi, we have p(x1, ... , xm) = cTooC(ym)C(ym−1) ... C(y1)c1 (1) p(x1, ... ,xm, h1, ... , hm) = πh1 m−1H Thj,hj_1 m Oxj,hj j=2 H j=1 Now, we can write the marginal probability of a sequence of observations as p(x1, ... xm) �= p(x1, ... , xm, h1, ... , hm) h1,...,hm which can be expressed in matrix form4 as: p(x1, ... , xm) = 1TAxmAxm_1 ··· Am1π where Axm = Tdiag(OTδxm), and 1 is a</context>
<context position="10385" citStr="Foster et al. (2012)" startWordPosition="1765" endWordPosition="1768">dynamic program (forward algorithm) used to estimate HMMs. where c1 = µ coo = µTE−1 C(y) = K(y)E−1 and µ, E and K, described in more detail below, are quantities estimated by frequencies of unigrams, bigrams, and trigrams in the observed (training) data. Under the assumption that data is generated by an HMM, the distribution p� obtained by substituting the estimated values c1, coo, and C(y) into equation (1) converges to p sufficiently fast as the amount of training data increases, giving us consistent parameter estimates. For details of the convergence proof, please see Hsu et al. (2008) and Foster et al. (2012). 3 Spectral Algorithm For Learning Dependency Trees In this section, we first describe a simple latent variable generative model for dependency parsing. We then define some extra notation and finally present 207 h2 h3 y1 y2 y3 h1 Figure 2: Dependency parsing tree with observed variables y1, y2, and y3. where pa(j) is the parent of node j and d(j) E {L, R} indicates whether hj is a left or a right node of hpa(j). For simplicity, the number of hidden and observed nodes in our tree are the same, however they are not required to be so. As is the case with the conventional HMM, the parameters used</context>
<context position="24836" citStr="Foster et al. (2012)" startWordPosition="4336" endWordPosition="4339">es consistent parameter estimates, and it is extremely fast to train. We worked out the theory of a simple yet powerful generative model and showed how it can be learned using a spectral method. As a pilot experimental evaluation we showed the efficacy of our approach by using the spectral probabilities output by our model for re-ranking the outputs of MST parser. Our method reduced the error of the baseline re-ranker by up to a moderate 4.6%. 7 Appendix This appendix offers a sketch of the proof of Theorem 1. The proof uses the following definitions, which are slightly modified from those of Foster et al. (2012). Definition 1. Define A as the smallest element of µ, E−1, Q−1, and K(). In other words, A - min{min |µi|, min |E−1 ij|, min |Q−ij1 |, i i,j i,j min |Kijk|, min |Eij |, min |Qij |, } i,j,k i,j i,j where Kijk = K(δj)ik are the elements of the tensor K(). Definition 2. Define σk as the smallest singular value of E and Q. The proof relies on the fact that a row vector multiplied by a series of matrices, and finally multiplied by a column vector amounts to a sum over all possible products of individual entries in the vectors and matrices. With this in mind, if we bound the largest relative error </context>
</contexts>
<marker>Foster, Rodu, Ungar, 2012</marker>
<rawString>Dean Foster, Jordan Rodu, and Lyle Ungar. Spectral dimensionality reduction for HMMs. ArXiV http://arxiv.org/abs/1203.6130, 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Hsu</author>
<author>S M Kakade</author>
<author>Tong Zhang</author>
</authors>
<title>A spectral algorithm for learning hidden markov models. arXiv:0811.4413v2,</title>
<date>2008</date>
<contexts>
<context position="1878" citStr="Hsu et al. (2008)" startWordPosition="285" endWordPosition="288"> two decades a workhorse of statistical pattern recognition with applications ranging from speech to vision to language. Adding latent variables to these models gives us additional modeling power and have shown success in applications like POS tagging (Merialdo, 1994), speech recognition (Rabiner, 1989) and object recognition (Quattoni et al., 2004). However, this comes at the cost that the resulting parameter estimation problem becomes non-convex and techniques like EM (Dempster et al., 1977) which are used to estimate the parameters can only lead to locally optimal solutions. Recent work by Hsu et al. (2008) has shown that globally consistent estimates of the parameters of HMMs can be found by using spectral methods, particularly by singular value decomposition (SVD) of appropriately defined linear systems. They avoid the NP Hard problem of the global optimization problem of the HMM parameters (Terwijn, 2002), by putting restrictions on the smallest singular value of the HMM parameters. The main intuition behind the model is that, although the observed data (i.e. words) seem to live in a very high dimensional space, but in reality they live in a very low dimensional space (size k — 30 — 50) and a</context>
<context position="3343" citStr="Hsu et al. (2008)" startWordPosition="528" endWordPosition="531">ain compared to EM methods. In this paper we generalize the approach of Hsu et al. (2008) to learn dependency tree structures with latent variables.1 Petrov et al. (2006) and Musillo and Merlo (2008) have shown that learning PCFGs and dependency grammars respectively with latent variables can produce parsers with very good generalization performance. However, both these approaches rely on EM for parameter estimation and can benefit from using spectral methods. We propose a simple yet powerful latent variable generative model for use with dependency pars1Actually, instead of using the model by Hsu et al. (2008) we work with a related model proposed by Foster et al. (2012) which addresses some of the shortcomings of the earlier model which we detail below. 205 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 205–213, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics ing which has one hidden node for each word in the sentence, like the one shown in Figure 1 and work out the details for the parameter estimation of the corresponding spectral learning model. At a very high le</context>
<context position="4781" citStr="Hsu et al. (2008)" startWordPosition="755" endWordPosition="758">spectral method for dependency parsing, however they deal with horizontal markovization and use hidden states to model sequential dependencies within a word’s sequence of children. In contrast with that, in this paper, we propose a spectral learning algorithm where latent states are not restricted to HMM-like distributions of modifier sequences for a particular head, but instead allow information to be propagated through the entire tree. More recently, Cohen et al. (2012) have proposed a spectral method for learning PCFGs. Its worth noting that recent work by Parikh et al. (2011) also extends Hsu et al. (2008) to latent variable dependency trees like us but under the restrictive conditions that model parameters are trained for a specified, albeit arbitrary, tree topology.2 In other words, all training sentences and test sentences must have identical tree topologies. By doing this they allow for node-specific model parameters, but must retrain the model entirely when a different tree topology is encountered. Our model on the other hand allows the flexibility and efficiency of processing sentences with a variety of tree topologies from a single training run. Most of the current state-of-the-art depen</context>
<context position="6503" citStr="Hsu et al., 2008" startWordPosition="1045" endWordPosition="1048">n be useful in modeling phylogeny trees for instance, but precludes most NLP applications, since there is a need to model the full set of different tree topologies possible in parsing. Figure 1: Sample dependency parsing tree for “Kilroy was here” erative model of words given a tree structure, it can score a tree structure i.e. its probability of generation. Thus, it can be used to re-rank the n-best outputs of a given parser. The remainder of the paper is organized as follows. In the next section we introduce the notation and give a brief overview of the spectral algorithm for learning HMMs (Hsu et al., 2008; Foster et al., 2012). In Section 3 we describe our proposed model for dependency parsing in detail and work out the theory behind it. Section 4 provides experimental evaluation of our model on Penn Treebank data. We conclude with a brief summary and future avenues for research. 2 Spectral Algorithm For Learning HMMs In this section we describe the spectral algorithm for learning HMMs.3 2.1 Notation The HMM that we consider in this section is a sequence of hidden states h E 11, ... , k} that follow the Markov property: p(ht|h1, ... , ht−1) = p(ht|ht−1) and a sequence of observations x E 11, .</context>
<context position="8676" citStr="Hsu et al. (2008)" startWordPosition="1453" endWordPosition="1456">on x. Define δj to be the vector of length n with a 1 in the jth entry and 0 everywhere else, and diag(v) to be the matrix with the entries of v on the diagonal and 0 everywhere else. The joint distribution of a sequence of observations x1, ... , xm and a sequence of hidden states h1, ... , hm is: 2.2 Fully observable representation Before presenting the model, we need to address a few more points. First, let U be a “representation matrix” (eigenfeature dictionary) which maps each observation to a reduced dimension space (n —* k) that satisfies the conditions: • UTO is invertible • |Uij |&lt; 1. Hsu et al. (2008); Foster et al. (2012) discuss U in more detail, but U can, for example, be obtained by the SVD of the bigram probability matrix (where Pij = p(xt+1 = i|xt = j)) or by doing CCA on neighboring n-grams (Dhillon et al., 2011). Letting yi = UTδxi, we have p(x1, ... , xm) = cTooC(ym)C(ym−1) ... C(y1)c1 (1) p(x1, ... ,xm, h1, ... , hm) = πh1 m−1H Thj,hj_1 m Oxj,hj j=2 H j=1 Now, we can write the marginal probability of a sequence of observations as p(x1, ... xm) �= p(x1, ... , xm, h1, ... , hm) h1,...,hm which can be expressed in matrix form4 as: p(x1, ... , xm) = 1TAxmAxm_1 ··· Am1π where Axm = Td</context>
<context position="10360" citStr="Hsu et al. (2008)" startWordPosition="1760" endWordPosition="1763"> form of the standard dynamic program (forward algorithm) used to estimate HMMs. where c1 = µ coo = µTE−1 C(y) = K(y)E−1 and µ, E and K, described in more detail below, are quantities estimated by frequencies of unigrams, bigrams, and trigrams in the observed (training) data. Under the assumption that data is generated by an HMM, the distribution p� obtained by substituting the estimated values c1, coo, and C(y) into equation (1) converges to p sufficiently fast as the amount of training data increases, giving us consistent parameter estimates. For details of the convergence proof, please see Hsu et al. (2008) and Foster et al. (2012). 3 Spectral Algorithm For Learning Dependency Trees In this section, we first describe a simple latent variable generative model for dependency parsing. We then define some extra notation and finally present 207 h2 h3 y1 y2 y3 h1 Figure 2: Dependency parsing tree with observed variables y1, y2, and y3. where pa(j) is the parent of node j and d(j) E {L, R} indicates whether hj is a left or a right node of hpa(j). For simplicity, the number of hidden and observed nodes in our tree are the same, however they are not required to be so. As is the case with the conventional</context>
<context position="12407" citStr="Hsu et al. (2008)" startWordPosition="2112" endWordPosition="2115">and G = UTO. Further, recall the notation diag(v), which is a matrix with elements of v on its diagonal, then: • Define the k-dimensional vector p (unigram the details of the corresponding spectral learning algorithm for dependency parsing, and prove that our learning algorithm provides a consistent estimation of the marginal probabilities. It is worth mentioning that an alternate way of approaching the spectral estimation of latent states for dependency parsing is by converting the dependency trees into linear sequences from root-to-leaf and doing a spectral estimation of latent states using Hsu et al. (2008). However, this approach would not give us the correct probability distribution over trees as the probability calculations for different paths through the trees are not independent. Thus, although one could calculate the probability of a path from the root to a leaf, one cannot generalize from this probability to say anything about the neighboring nodes or words. Put another way, when a parent has more than the one descendant, one has to be careful to take into account that the hidden variables at each child node are all conditioned on the hidden variable of the parent. 3.1 A latent variable g</context>
<context position="20138" citStr="Hsu et al. (2008)" startWordPosition="3514" endWordPosition="3517">gs for development and test sets and used 10- way jackknifing to generate tags for the training set. As is common practice we stripped our sentences of all the punctuation. We evaluated our approach on sentences of all lengths. 4.2 Details of spectral learning For the spectral learning phase, we need to just collect word counts from the training data as described above, so there are no tunable parameters as such. However, we need to have access to an attribute dictionary U which contains a k dimensional representation for each word in the corpus. A possible way of generating U as suggested by Hsu et al. (2008) is by performing SVD on bigrams P21 and using the left eigenvectors as U. We instead used the eigenfeature dictionary proposed by Dhillon et al. (2011) (LR-MVL) which is obtained by performing CCA on neighboring words and has provably better sample complexity for rare words compared to the SVD alternative. We induced the LR-MVL embeddings for words using the Reuters RCV1 corpus which contains about 63 million tokens in 3.3 million sentences and used their context oblivious embeddings as our estimate of U. We experimented with different choices of k (the size of the low dimensional projection)</context>
</contexts>
<marker>Hsu, Kakade, Zhang, 2008</marker>
<rawString>D Hsu, S M. Kakade, and Tong Zhang. A spectral algorithm for learning hidden markov models. arXiv:0811.4413v2, 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Koo</author>
<author>Xavier Carreras</author>
<author>Michael Collins</author>
</authors>
<title>Simple semi-supervised dependency parsing. In</title>
<date>2008</date>
<booktitle>In Proc. ACL/HLT,</booktitle>
<contexts>
<context position="5439" citStr="Koo et al., 2008" startWordPosition="861" endWordPosition="864">us but under the restrictive conditions that model parameters are trained for a specified, albeit arbitrary, tree topology.2 In other words, all training sentences and test sentences must have identical tree topologies. By doing this they allow for node-specific model parameters, but must retrain the model entirely when a different tree topology is encountered. Our model on the other hand allows the flexibility and efficiency of processing sentences with a variety of tree topologies from a single training run. Most of the current state-of-the-art dependency parsers are discriminative parsers (Koo et al., 2008; McDonald, 2006) due to the flexibility of representations which can be used as features leading to better accuracies and the ease of reproducibility of results. However, unlike discriminative models, generative models can exploit unlabeled data. Also, as is common in statistical parsing, re-ranking the outputs of a parser leads to significant reductions in error (Collins and Koo, 2005). Since our spectral learning algorithm uses a gen2This can be useful in modeling phylogeny trees for instance, but precludes most NLP applications, since there is a need to model the full set of different tree</context>
</contexts>
<marker>Koo, Carreras, Collins, 2008</marker>
<rawString>Terry Koo, Xavier Carreras, and Michael Collins. Simple semi-supervised dependency parsing. In In Proc. ACL/HLT, 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Luque</author>
<author>A Quattoni</author>
<author>B Balle</author>
<author>X Carreras</author>
</authors>
<title>Spectral learning for non-deterministic dependency parsing.</title>
<date>2012</date>
<booktitle>In EACL,</booktitle>
<contexts>
<context position="4142" citStr="Luque et al. (2012)" startWordPosition="654" endWordPosition="657">t Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 205–213, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics ing which has one hidden node for each word in the sentence, like the one shown in Figure 1 and work out the details for the parameter estimation of the corresponding spectral learning model. At a very high level, the parameter estimation of our model involves collecting unigram, bigram and trigram counts sensitive to the underlying dependency structure of the given sentence. Recently, Luque et al. (2012) have also proposed a spectral method for dependency parsing, however they deal with horizontal markovization and use hidden states to model sequential dependencies within a word’s sequence of children. In contrast with that, in this paper, we propose a spectral learning algorithm where latent states are not restricted to HMM-like distributions of modifier sequences for a particular head, but instead allow information to be propagated through the entire tree. More recently, Cohen et al. (2012) have proposed a spectral method for learning PCFGs. Its worth noting that recent work by Parikh et al</context>
</contexts>
<marker>Luque, Quattoni, Balle, Carreras, 2012</marker>
<rawString>F. Luque, A. Quattoni, B. Balle, and X. Carreras. Spectral learning for non-deterministic dependency parsing. In EACL, 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Mary Ann Marcinkiewicz</author>
<author>Beatrice Santorini</author>
</authors>
<title>Building a large annotated corpus of english: the penn treebank.</title>
<date>1993</date>
<journal>Comput. Linguist.,</journal>
<volume>19</volume>
<pages>0891--2017</pages>
<contexts>
<context position="18726" citStr="Marcus et al., 1993" startWordPosition="3264" endWordPosition="3267">y to benchmark our parser is to generate nbest dependency trees using some standard parser and then use our algorithm to re-rank the candidate dependency trees, e.g. using the log spectral probability as described in Algorithm 1 as a feature in a discriminative re-ranker. 4.1 Experimental Setup Our base parser was the discriminatively trained MSTParser (McDonald, 2006), which implements both first and second order parsers and is trained using MIRA (Crammer et al., 2006) and used the standard baseline features as described in McDonald (2006). We tested our methods on the English Penn Treebank (Marcus et al., 1993). We use the standard splits of Penn Treebank; i.e., we used sections 2-21 for training, section 22 for development and section 23 for testing. We used the PennConverter7 tool to convert Penn Treebank from constituent to dependency format. Following (McDonald, 2006; Koo 7http://nlp.cs.lth.se/software/treebank_ converter/ 1: Input: Training examples- x(i) for i ∈ {1, ... , M} along with dependency structures where each sequence x(i) = x(i) 1 , ... , x(i)mi. 2: Compute the spectral parameters ˆµ, ˆΩL, and Kˆ #Now, for a given sentence, we can recursively compute the following: 3: for x(i) j for </context>
</contexts>
<marker>Marcus, Marcinkiewicz, Santorini, 1993</marker>
<rawString>Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated corpus of english: the penn treebank. Comput. Linguist., 19:313–330, June 1993. ISSN 0891-2017.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
</authors>
<title>Discriminative learning and spanning tree algorithms for dependency parsing.</title>
<date>2006</date>
<tech>PhD thesis,</tech>
<pages>3225503</pages>
<institution>University of Pennsylvania,</institution>
<location>Philadelphia, PA, USA,</location>
<contexts>
<context position="5456" citStr="McDonald, 2006" startWordPosition="865" endWordPosition="866">estrictive conditions that model parameters are trained for a specified, albeit arbitrary, tree topology.2 In other words, all training sentences and test sentences must have identical tree topologies. By doing this they allow for node-specific model parameters, but must retrain the model entirely when a different tree topology is encountered. Our model on the other hand allows the flexibility and efficiency of processing sentences with a variety of tree topologies from a single training run. Most of the current state-of-the-art dependency parsers are discriminative parsers (Koo et al., 2008; McDonald, 2006) due to the flexibility of representations which can be used as features leading to better accuracies and the ease of reproducibility of results. However, unlike discriminative models, generative models can exploit unlabeled data. Also, as is common in statistical parsing, re-ranking the outputs of a parser leads to significant reductions in error (Collins and Koo, 2005). Since our spectral learning algorithm uses a gen2This can be useful in modeling phylogeny trees for instance, but precludes most NLP applications, since there is a need to model the full set of different tree topologies possi</context>
<context position="18477" citStr="McDonald, 2006" startWordPosition="3223" endWordPosition="3224">parameters, can be found in the appendix. The proof with directional transition parameters is almost identical. 4 Experimental Evaluation Since our algorithm can score any given tree structure by computing its marginal probability, a natural way to benchmark our parser is to generate nbest dependency trees using some standard parser and then use our algorithm to re-rank the candidate dependency trees, e.g. using the log spectral probability as described in Algorithm 1 as a feature in a discriminative re-ranker. 4.1 Experimental Setup Our base parser was the discriminatively trained MSTParser (McDonald, 2006), which implements both first and second order parsers and is trained using MIRA (Crammer et al., 2006) and used the standard baseline features as described in McDonald (2006). We tested our methods on the English Penn Treebank (Marcus et al., 1993). We use the standard splits of Penn Treebank; i.e., we used sections 2-21 for training, section 22 for development and section 23 for testing. We used the PennConverter7 tool to convert Penn Treebank from constituent to dependency format. Following (McDonald, 2006; Koo 7http://nlp.cs.lth.se/software/treebank_ converter/ 1: Input: Training examples-</context>
</contexts>
<marker>McDonald, 2006</marker>
<rawString>Ryan McDonald. Discriminative learning and spanning tree algorithms for dependency parsing. PhD thesis, University of Pennsylvania, Philadelphia, PA, USA, 2006. AAI3225503.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernard Merialdo</author>
</authors>
<title>Tagging english text with a probabilistic model.</title>
<date>1994</date>
<journal>Comput. Linguist.,</journal>
<volume>20</volume>
<pages>0891--2017</pages>
<contexts>
<context position="1529" citStr="Merialdo, 1994" startWordPosition="230" endWordPosition="231">a spectral learning method to efficiently estimate it. As a pilot experimental evaluation, we use the spectral tree probabilities estimated by our model to re-rank the outputs of a near state-of-theart parser. Our approach gives us a moderate reduction in error of up to 4.6% over the baseline re-ranker. 1 Introduction Markov models have been for two decades a workhorse of statistical pattern recognition with applications ranging from speech to vision to language. Adding latent variables to these models gives us additional modeling power and have shown success in applications like POS tagging (Merialdo, 1994), speech recognition (Rabiner, 1989) and object recognition (Quattoni et al., 2004). However, this comes at the cost that the resulting parameter estimation problem becomes non-convex and techniques like EM (Dempster et al., 1977) which are used to estimate the parameters can only lead to locally optimal solutions. Recent work by Hsu et al. (2008) has shown that globally consistent estimates of the parameters of HMMs can be found by using spectral methods, particularly by singular value decomposition (SVD) of appropriately defined linear systems. They avoid the NP Hard problem of the global op</context>
</contexts>
<marker>Merialdo, 1994</marker>
<rawString>Bernard Merialdo. Tagging english text with a probabilistic model. Comput. Linguist., 20:155–171, June 1994. ISSN 0891-2017.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gabriele Antonio Musillo</author>
<author>Paola Merlo</author>
</authors>
<title>Unlexicalised hidden variable models of split dependency grammars.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics on Human Language Technologies: Short Papers, HLT-Short ’08,</booktitle>
<pages>213--216</pages>
<location>Stroudsburg, PA, USA,</location>
<contexts>
<context position="2925" citStr="Musillo and Merlo (2008)" startWordPosition="462" endWordPosition="465">is that, although the observed data (i.e. words) seem to live in a very high dimensional space, but in reality they live in a very low dimensional space (size k — 30 — 50) and an appropriate eigen decomposition of the observed data will reveal the underlying low dimensional dynamics and thereby revealing the parameters of the model. Besides ducking the NP hard problem, the spectral methods are very fast and scalable to train compared to EM methods. In this paper we generalize the approach of Hsu et al. (2008) to learn dependency tree structures with latent variables.1 Petrov et al. (2006) and Musillo and Merlo (2008) have shown that learning PCFGs and dependency grammars respectively with latent variables can produce parsers with very good generalization performance. However, both these approaches rely on EM for parameter estimation and can benefit from using spectral methods. We propose a simple yet powerful latent variable generative model for use with dependency pars1Actually, instead of using the model by Hsu et al. (2008) we work with a related model proposed by Foster et al. (2012) which addresses some of the shortcomings of the earlier model which we detail below. 205 Proceedings of the 2012 Joint </context>
</contexts>
<marker>Musillo, Merlo, 2008</marker>
<rawString>Gabriele Antonio Musillo and Paola Merlo. Unlexicalised hidden variable models of split dependency grammars. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics on Human Language Technologies: Short Papers, HLT-Short ’08, pages 213– 216, Stroudsburg, PA, USA, 2008. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ankur P Parikh</author>
<author>Le Song</author>
<author>Eric P Xing</author>
</authors>
<title>A spectral algorithm for latent tree graphical models.</title>
<date>2011</date>
<booktitle>In ICML,</booktitle>
<pages>1065--1072</pages>
<marker>Parikh, Le Song, Xing, 2011</marker>
<rawString>Ankur P. Parikh, Le Song, and Eric P. Xing. A spectral algorithm for latent tree graphical models. In ICML, pages 1065–1072, 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Leon Barrett</author>
<author>Romain Thibaux</author>
<author>Dan Klein</author>
</authors>
<title>Learning accurate, compact, and interpretable tree annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, ACL-44,</booktitle>
<pages>433--440</pages>
<location>Stroudsburg, PA, USA,</location>
<contexts>
<context position="2896" citStr="Petrov et al. (2006)" startWordPosition="457" endWordPosition="460">tuition behind the model is that, although the observed data (i.e. words) seem to live in a very high dimensional space, but in reality they live in a very low dimensional space (size k — 30 — 50) and an appropriate eigen decomposition of the observed data will reveal the underlying low dimensional dynamics and thereby revealing the parameters of the model. Besides ducking the NP hard problem, the spectral methods are very fast and scalable to train compared to EM methods. In this paper we generalize the approach of Hsu et al. (2008) to learn dependency tree structures with latent variables.1 Petrov et al. (2006) and Musillo and Merlo (2008) have shown that learning PCFGs and dependency grammars respectively with latent variables can produce parsers with very good generalization performance. However, both these approaches rely on EM for parameter estimation and can benefit from using spectral methods. We propose a simple yet powerful latent variable generative model for use with dependency pars1Actually, instead of using the model by Hsu et al. (2008) we work with a related model proposed by Foster et al. (2012) which addresses some of the shortcomings of the earlier model which we detail below. 205 P</context>
</contexts>
<marker>Petrov, Barrett, Thibaux, Klein, 2006</marker>
<rawString>Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact, and interpretable tree annotation. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, ACL-44, pages 433–440, Stroudsburg, PA, USA, 2006. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ariadna Quattoni</author>
<author>Michael Collins</author>
<author>Trevor Darrell</author>
</authors>
<title>Conditional random fields for object recognition. In</title>
<date>2004</date>
<booktitle>In NIPS,</booktitle>
<pages>1097--1104</pages>
<publisher>MIT Press,</publisher>
<contexts>
<context position="1612" citStr="Quattoni et al., 2004" startWordPosition="240" endWordPosition="243">al evaluation, we use the spectral tree probabilities estimated by our model to re-rank the outputs of a near state-of-theart parser. Our approach gives us a moderate reduction in error of up to 4.6% over the baseline re-ranker. 1 Introduction Markov models have been for two decades a workhorse of statistical pattern recognition with applications ranging from speech to vision to language. Adding latent variables to these models gives us additional modeling power and have shown success in applications like POS tagging (Merialdo, 1994), speech recognition (Rabiner, 1989) and object recognition (Quattoni et al., 2004). However, this comes at the cost that the resulting parameter estimation problem becomes non-convex and techniques like EM (Dempster et al., 1977) which are used to estimate the parameters can only lead to locally optimal solutions. Recent work by Hsu et al. (2008) has shown that globally consistent estimates of the parameters of HMMs can be found by using spectral methods, particularly by singular value decomposition (SVD) of appropriately defined linear systems. They avoid the NP Hard problem of the global optimization problem of the HMM parameters (Terwijn, 2002), by putting restrictions o</context>
</contexts>
<marker>Quattoni, Collins, Darrell, 2004</marker>
<rawString>Ariadna Quattoni, Michael Collins, and Trevor Darrell. Conditional random fields for object recognition. In In NIPS, pages 1097–1104. MIT Press, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lawrence R Rabiner</author>
</authors>
<title>A tutorial on hidden markov models and selected applications in speech recognition.</title>
<date>1989</date>
<booktitle>In Proceedings of the IEEE,</booktitle>
<pages>257--286</pages>
<contexts>
<context position="1565" citStr="Rabiner, 1989" startWordPosition="234" endWordPosition="235">ntly estimate it. As a pilot experimental evaluation, we use the spectral tree probabilities estimated by our model to re-rank the outputs of a near state-of-theart parser. Our approach gives us a moderate reduction in error of up to 4.6% over the baseline re-ranker. 1 Introduction Markov models have been for two decades a workhorse of statistical pattern recognition with applications ranging from speech to vision to language. Adding latent variables to these models gives us additional modeling power and have shown success in applications like POS tagging (Merialdo, 1994), speech recognition (Rabiner, 1989) and object recognition (Quattoni et al., 2004). However, this comes at the cost that the resulting parameter estimation problem becomes non-convex and techniques like EM (Dempster et al., 1977) which are used to estimate the parameters can only lead to locally optimal solutions. Recent work by Hsu et al. (2008) has shown that globally consistent estimates of the parameters of HMMs can be found by using spectral methods, particularly by singular value decomposition (SVD) of appropriately defined linear systems. They avoid the NP Hard problem of the global optimization problem of the HMM parame</context>
</contexts>
<marker>Rabiner, 1989</marker>
<rawString>Lawrence R. Rabiner. A tutorial on hidden markov models and selected applications in speech recognition. In Proceedings of the IEEE, pages 257– 286, 1989.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>A Maximum Entropy Model for Part-Of-Speech Tagging.</title>
<date>1996</date>
<booktitle>Proceedings of the Empirical Methods in Natural Language Processing,</booktitle>
<pages>133--142</pages>
<editor>In Eric Brill and Kenneth Church, editors,</editor>
<contexts>
<context position="19469" citStr="Ratnaparkhi (1996)" startWordPosition="3397" endWordPosition="3398">on 23 for testing. We used the PennConverter7 tool to convert Penn Treebank from constituent to dependency format. Following (McDonald, 2006; Koo 7http://nlp.cs.lth.se/software/treebank_ converter/ 1: Input: Training examples- x(i) for i ∈ {1, ... , M} along with dependency structures where each sequence x(i) = x(i) 1 , ... , x(i)mi. 2: Compute the spectral parameters ˆµ, ˆΩL, and Kˆ #Now, for a given sentence, we can recursively compute the following: 3: for x(i) j for j ∈ {mi, ... ,1} do 4: Compute: ˆΣR, ˆΣL, ˆΩR, D(ETd�−→rj) 1 − E ≤ ���� i ≤ 1+E 210 et al., 2008), we used the POS tagger by Ratnaparkhi (1996) trained on the full training data to provide POS tags for development and test sets and used 10- way jackknifing to generate tags for the training set. As is common practice we stripped our sentences of all the punctuation. We evaluated our approach on sentences of all lengths. 4.2 Details of spectral learning For the spectral learning phase, we need to just collect word counts from the training data as described above, so there are no tunable parameters as such. However, we need to have access to an attribute dictionary U which contains a k dimensional representation for each word in the cor</context>
</contexts>
<marker>Ratnaparkhi, 1996</marker>
<rawString>Adwait Ratnaparkhi. A Maximum Entropy Model for Part-Of-Speech Tagging. In Eric Brill and Kenneth Church, editors, Proceedings of the Empirical Methods in Natural Language Processing, pages 133–142, 1996.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastiaan Terwijn</author>
</authors>
<title>On the learnability of hidden markov models.</title>
<date>2002</date>
<booktitle>In Proceedings of the 6th International Colloquium on Grammatical Inference: Algorithms and Applications, ICGI ’02,</booktitle>
<pages>261--268</pages>
<publisher>SpringerVerlag. ISBN</publisher>
<location>London, UK, UK,</location>
<contexts>
<context position="2185" citStr="Terwijn, 2002" startWordPosition="335" endWordPosition="336">bject recognition (Quattoni et al., 2004). However, this comes at the cost that the resulting parameter estimation problem becomes non-convex and techniques like EM (Dempster et al., 1977) which are used to estimate the parameters can only lead to locally optimal solutions. Recent work by Hsu et al. (2008) has shown that globally consistent estimates of the parameters of HMMs can be found by using spectral methods, particularly by singular value decomposition (SVD) of appropriately defined linear systems. They avoid the NP Hard problem of the global optimization problem of the HMM parameters (Terwijn, 2002), by putting restrictions on the smallest singular value of the HMM parameters. The main intuition behind the model is that, although the observed data (i.e. words) seem to live in a very high dimensional space, but in reality they live in a very low dimensional space (size k — 30 — 50) and an appropriate eigen decomposition of the observed data will reveal the underlying low dimensional dynamics and thereby revealing the parameters of the model. Besides ducking the NP hard problem, the spectral methods are very fast and scalable to train compared to EM methods. In this paper we generalize the</context>
</contexts>
<marker>Terwijn, 2002</marker>
<rawString>Sebastiaan Terwijn. On the learnability of hidden markov models. In Proceedings of the 6th International Colloquium on Grammatical Inference: Algorithms and Applications, ICGI ’02, pages 261–268, London, UK, UK, 2002. SpringerVerlag. ISBN 3-540-44239-1.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
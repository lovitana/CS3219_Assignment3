<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.997492">
Unambiguity Regularization for Unsupervised Learning of Probabilistic
Grammars
</title>
<author confidence="0.991177">
Kewei Tu∗
</author>
<affiliation confidence="0.995951">
Departments of Statistics and Computer Science
University of California, Los Angeles
</affiliation>
<address confidence="0.967422">
Los Angeles, CA 90095, USA
</address>
<email confidence="0.9993">
tukw@ucla.edu
</email>
<author confidence="0.989363">
Vasant Honavar
</author>
<affiliation confidence="0.995576">
Department of Computer Science
Iowa State University
</affiliation>
<address confidence="0.962822">
Ames, IA 50011, USA
</address>
<email confidence="0.999402">
honavar@cs.iastate.edu
</email>
<sectionHeader confidence="0.995651" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999968541666667">
We introduce a novel approach named unam-
biguity regularization for unsupervised learn-
ing of probabilistic natural language gram-
mars. The approach is based on the observa-
tion that natural language is remarkably unam-
biguous in the sense that only a tiny portion of
the large number of possible parses of a nat-
ural language sentence are syntactically valid.
We incorporate an inductive bias into gram-
mar learning in favor of grammars that lead
to unambiguous parses on natural language
sentences. The resulting family of algorithms
includes the expectation-maximization algo-
rithm (EM) and its variant, Viterbi EM, as well
as a so-called softmax-EM algorithm. The
softmax-EM algorithm can be implemented
with a simple and computationally efficient
extension to standard EM. In our experiments
of unsupervised dependency grammar learn-
ing, we show that unambiguity regularization
is beneficial to learning, and in combination
with annealing (of the regularization strength)
and sparsity priors it leads to improvement
over the current state of the art.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.994273619047619">
Machine learning offers a potentially powerful ap-
proach to learning probabilistic grammars from data.
Because of the high cost of manual sentence anno-
tation, there is substantial interest in unsupervised
grammar learning, i.e., the induction of a grammar
from a corpus of unannotated sentences. The sim-
plest such approaches attempt to maximize the like-
*Part of the work was done while at Iowa State University.
lihood of the grammar given the training data, typi-
cally using expectation-maximization (EM) (Baker,
1979; Lari and Young, 1990; Klein and Manning,
2004). More recent approaches incorporate addi-
tional prior information of the target grammar into
learning. For example, Kurihara and Sato (2004)
used Dirichlet priors over rule probabilities to obtain
smoothed estimates of the probabilities. Johnson et
al. (2007) used Dirichlet priors with hyperparame-
ters set to values less than 1 to encourage sparsity
of grammar rules. Finkel et al. (2007) and Liang et
al. (2007) proposed to use the hierarchical Dirichlet
process prior to bias learning towards concise gram-
mars without the need to pre-specify the number of
nonterminals. Cohen et al. (2008) and Cohen and
Smith (2009) employed the logistic normal prior to
model the correlations between grammar symbols.
Gillenwater et al. (2010) incorporated a sparsity bias
on grammar rules into learning by means of poste-
rior regularization.
More recently, Spitkovsky et al. (2010) and Poon
and Domingos (2011) observed that the use of
Viterbi EM (also called hard EM) in place of stan-
dard EM can lead to significantly improved results
in unsupervised learning of probabilistic grammars
from natural language and image data respectively,
even if no prior information is used. This finding is
surprising because Viterbi EM is a degenerate case
of standard EM and is therefore generally consid-
ered to be less effective in locating the optimum
of the objective function. Spitkovsky et al. (2010)
speculated that the observed advantage of Viterbi
EM over standard EM is due to standard EM reserv-
ing too much probability mass to spurious parses in
</bodyText>
<page confidence="0.95013">
1324
</page>
<note confidence="0.8063535">
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1324–1334, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.99464225925926">
the E-step. However, it is still unclear as to why
Viterbi EM can avoid this problem.
Against this background, we propose the use of
a novel type of prior information for unsupervised
learning of probabilistic natural language grammars,
namely the syntactic unambiguity of natural lan-
guage. Although it is often possible to correctly
parse a natural language sentence in more than one
way, natural language is remarkably unambiguous
in the sense that the number of plausible parses of a
natural language sentence is rather small in compar-
ison with the total number of possible parses. Thus,
we incorporate into learning an inductive bias in fa-
vor of grammars that lead to unambiguous parses on
natural language sentences, by using the posterior
regularization framework (Ganchev et al., 2010).
We name this approach unambiguity regularization.
The resulting family of algorithms includes standard
EM and Viterbi EM, as well as an algorithm that
falls between standard EM and Viterbi EM which
we call softmax-EM. The softmax-EM algorithm
can be implemented with a simple and computation-
ally efficient extension to standard EM. The fact that
Viterbi EM is a special case of our approach also
gives an explanation of the advantage of Viterbi EM
observed in previous work: it is because Viterbi EM
implicitly utilizes unambiguity regularization. In
our experiments of unsupervised dependency gram-
mar learning, we show that unambiguity regulariza-
tion is beneficial to learning, and in combination
with annealing (of the regularization strength) and
sparsity priors it leads to improvement over the cur-
rent state of the art.
It should be noted that our approach is closely
related to the deterministic annealing (DA) tech-
nique studied in the optimization literature (Rose,
1998). However, DA has a very different motiva-
tion than ours and differs from our approach in a few
important algorithmic details, as will be discussed
in section 5. When applied to unsupervised gram-
mar learning, DA has been shown to lead to worse
parsing accuracy than standard EM (Smith and Eis-
ner, 2004); in contrast, we show that our approach
leads to significantly higher parsing accuracy than
standard EM in unsupervised dependency grammar
learning.
The rest of the paper is organized as follows. Sec-
tion 2 analyzes the degree of unambiguity of natural
language grammars. Section 3 introduces the unam-
biguity regularization approach and shows that stan-
dard EM, Viterbi EM and softmax-EM are its special
cases. We show the experimental results in section
4, discuss related work in section 5 and conclude the
paper in section 6.
</bodyText>
<sectionHeader confidence="0.9323585" genericHeader="introduction">
2 The (Un)ambiguity of Natural Language
Grammars
</sectionHeader>
<bodyText confidence="0.989284974358974">
A grammar is said to be ambiguous on a sentence if
the sentence can be parsed in more than one way by
the grammar. It is widely acknowledged that natu-
ral language grammars are ambiguous on a signifi-
cant proportion of natural language sentences. For
example, Manning and Sch¨utze (1999) show that a
sentence randomly chosen from the Wall Street Jour-
nal — “The post office will hold out discounts and
service concessions as incentives” — has at least
five plausible syntactic parses. When we parse this
sentence using the Berkeley parser (Petrov et al.,
2006), one of the state-of-the-art English language
parsers, we find many alternative parses in addition
to the parses shown in (Manning and Sch¨utze, 1999).
Indeed, with a probabilistic context-free grammar
of only 26 nonterminals (as used in the Berke-
ley parser), the estimated total number of possible
parses&apos; of the example sentence is 2 × 1037. How-
ever, upon closer examination, we find that among
this very large number of possible parses, only a few
have significant probabilities. Figure 1 shows the
probabilities of the 100 best parses of the example
sentence. We can see that most of the parses have
probabilities that are negligible compared with the
probability of the best parse (i.e., the parse with the
largest probability). Quantitatively, we find that the
probabilities of the parses decrease roughly expo-
nentially as we go from the best parse to the less
likely parses. We confirmed this observation by ex-
amining the parses of many other natural language
sentences obtained using the Berkeley parser. This
observation suggests that natural language gram-
mars are indeed remarkably unambiguous on natu-
ral language sentences, in the sense that for a typical
&apos;Given a sentence of length m and a complete Chomsky nor-
mal form grammar with n nonterminals, the number of all pos-
sible parses is C,,,,−1 x n2,,,,−1, where C,,,,−1 is the (m − 1)-th
Catalan number. This number is further increased if there are
unary rules between nonterminals in the grammar.
</bodyText>
<page confidence="0.977383">
1325
</page>
<figure confidence="0.999267">
0.25
0.2
0.15
0.1
0.05
0
0 20 40 60 80 100
100 Best Parses
</figure>
<figureCaption confidence="0.9986435">
Figure 1: The probabilities of the 100 best parses of the
example sentence.
</figureCaption>
<bodyText confidence="0.999098882352941">
natural language sentence, the probability mass of
the parses is concentrated to a tiny portion of all pos-
sible parses. This is not surprising in light of the fact
that the main purpose of natural language is commu-
nication and in the course of language evolution the
selection pressure for more efficient communication
would favor unambiguous languages.
To highlight the unambiguity of natural language
grammars, here we compare the parse probabilities
shown in Figure 1 with the parse probabilities pro-
duced by two other probabilistic context-free gram-
mars. In figure 2(a) we show the probabilities of the
100 best parses of the example sentence produced
by a random grammar. The random grammar has a
similar number of nonterminals as in the Berkeley
parser, and its grammar rule probabilities are sam-
pled from a uniform distribution and then normal-
ized. It can be seen that unlike the natural language
grammar, the random grammar produces a very uni-
form probability distribution over parses. Figure
2(b) shows the probabilities of the 100 best parses
of the example sentence produced by a maximum-
likelihood grammar learned from the unannotated
Wall Street Journal corpus of the Penn Treebank us-
ing the EM algorithm. An exponential decrease can
be observed in the probabilities, but the probabil-
ity mass is still much less concentrated than in the
case of the natural language grammar. Again, we
confirmed this observation by repeating the exper-
iments on many other natural language sentences.
This suggests that both the random grammar and the
maximum-likelihood grammar are far more ambigu-
ous on natural language sentences than true natural
language grammars.
</bodyText>
<sectionHeader confidence="0.989577" genericHeader="method">
3 Learning with Unambiguity
Regularization
</sectionHeader>
<bodyText confidence="0.9996417">
Motivated by the preceding observation, we want to
incorporate into learning an inductive bias in favor
of grammars that are unambiguous on natural lan-
guage sentences. First of all, we need a precise defi-
nition of the ambiguity of a grammar on a sentence.
Assume a grammar with a fixed set of grammar rules
and let 0 be the rule probabilities. Let x represent a
sentence and let z represent the parse of x. One natu-
ral measurement of the ambiguity is the information
entropy of z conditioned on x and 0:
</bodyText>
<equation confidence="0.975186333333333">
�
H(z|x, 0) = − p�(z|x) log p�(z|x)
z
</equation>
<bodyText confidence="0.996690533333334">
The lower the entropy is, the less ambiguous the
grammar is on sentence x. When the entropy
reaches 0, the grammar is strictly unambiguous on
sentence x, i.e., sentence x has a unique parse ac-
cording to the grammar.
Now we need to modify the objective function
of grammar learning to favor low ambiguity of the
learned grammar in parsing natural langauge sen-
tences. One approach is to use a prior distribu-
tion that favors grammars with low ambiguity on
the sentences that they generate. Since the likeli-
hood term in the objective function would ensure
that the learned grammar will have high probability
of generating natural language sentences, combin-
ing the likelihood and the prior would lead to low
ambiguity of the learned grammar on natural lan-
guage sentences. Unfortunately, adding this prior
to the objective function makes learning intractable.
Hence, here we adopt an alternative approach using
the posterior regularization framework (Ganchev et
al., 2010). Posterior regularization biases learning
in favor of solutions with desired behavior by con-
straining the model posteriors on the unlabeled data.
In our case, we use the constraint that the probability
distributions on the parses of the training sentences
given the learned grammar must have low entropy,
which is equivalent to requiring the learned grammar
to have low ambiguity on the training sentences.
Let X = {x1, x2, ... , xn} denote the set of train-
ing sentences, Z = {z1, z2,. . . , zn} denote the set
</bodyText>
<figure confidence="0.963394115384616">
Probability
1326
Probability
Probability
x 1024
6
5
4
3
2
1
0
0 20 40 60 80 100
100 Best Parses
(a)
100 Best Parses
(b)
0 20 40 60 80 100
2.5
0.5
1.5
2
x 103
3
0
1
</figure>
<figureCaption confidence="0.9939125">
Figure 2: The probabilities of the 100 best parses of the example sentence produced by (a) a random grammar and (b)
a maximum-likelihood grammar learned by the EM algorithm.
</figureCaption>
<bodyText confidence="0.971675">
of parses of the training sentences, and θ denote the
rule probabilities of the grammar. We use the slack-
penalized version of the posterior regularization ob-
jective function:
</bodyText>
<equation confidence="0.9577955">
J(θ) = log p(θ|X)
( )
∑
− minKL(q(Z)||pθ(Z|X)) + σ ξi
q,ξ
i
∑s.t. ∀i, H(zi) = − q(zi) log q(zi) ≤ ξi
zi
</equation>
<bodyText confidence="0.9999954">
where σ is a nonnegative constant that controls the
strength of the regularization term; q is an auxil-
iary distribution such that q(Z) = ∏i q(zi). The
first term in the objective function is the log poste-
rior probability of the grammar parameters given the
training corpus, and the second term minimizes the
KL-divergence between the auxiliary distribution q
and the posterior distribution on Z while constrains
q to have low entropy. We can incorporate the con-
straint into the objective function, so we get
</bodyText>
<equation confidence="0.9413345">
J(θ) = log p(θ|X)
( )
∑
− min KL(q(Z)||pθ(Z|X)) + σ H(zi)
q
i
</equation>
<bodyText confidence="0.9995515">
To optimize this objective function, we can per-
form coordinate ascent on a two-variable function:
</bodyText>
<equation confidence="0.963125666666667">
F(θ, q) = log p(θ|X)
(KL (q (Z)   ||pθ(Z |X)) + σ∑H(zi)
i
</equation>
<bodyText confidence="0.9999654">
There are two steps in each coordinate ascent itera-
tion. In the first step, we fix q and optimize θ. It can
be shown that
This is equivalent to the M-step in the EM algorithm.
The second step fixes θ and optimizes q.
</bodyText>
<equation confidence="0.752587571428571">
q* = arg max F(θ, q)
q
( )
∑
= arg min KL(q(Z)||pθ(Z|X)) + σ H(zi)
q
i
</equation>
<bodyText confidence="0.9999498">
It is different from the E-step of the EM algorithm
in that it contains an additional regularization term
σ ∑i H(zi). Ganchev et al. (2010) propose to use
the projected subgradient method to solve this op-
timization problem in the general case of posterior
regularization. In our case, however, it is possible to
obtain an analytical solution as shown below.
First, note that the optimization objective of this
step can be rewritten as the sum over functions of
individual training sentences.
</bodyText>
<equation confidence="0.992812">
KL(q(Z)||pθ(Z|X)) + σ H(zi) =
∑ ∑ fi(q)
i i
</equation>
<bodyText confidence="0.84537">
where
</bodyText>
<equation confidence="0.9798214">
fi(q) = KL(q(zi)||pθ(zi|xi)) + σH(zi)
( )
∑ q(zi) log q(zi)1−σ
= pθ(zi|xi)
zi
θ* = arg max F (θ, q)
θ
= arg
m0 Eq
[log(pθ (X, Z)p(θ))]
</equation>
<page confidence="0.909584">
1327
</page>
<bodyText confidence="0.999916333333333">
So we can optimize fi(q) for each training sentence
xi. The optimum of fi(q) depends on the value of
the constant σ.
</bodyText>
<equation confidence="0.849443166666667">
Case 1: σ = 0.
fi(q) contains only the KL-divergence term, so the
second step in the coordinate ascent iteration be-
comes the standard E-step of the EM algorithm.
q*(zi) = pθ(zi|xi)
Case 2: 0 &lt; σ &lt; 1.
</equation>
<bodyText confidence="0.9738661">
The space of valid assignments of the distribution
q(zi) is a unit (m−1)-simplex, where m is the num-
ber of valid parses of sentence xi. Denote this space
by ∆.
Theorem 1. fi(q) is strictly convex on the unit sim-
plex ∆ when 0 &lt; σ &lt; 1.
Proof Sketch. Define g(x) = x log x, where g(0) is
defined to be 0. For any t ∈ (0, 1), for any two
points q1 and q2 in the unit simplex ∆, we can show
that
</bodyText>
<equation confidence="0.999969">
tfi(q1) + (1 − t)fi(q2) − fi(tq1 + (1 − t)q2)
[tg(q1(zi)) + (1 − t)g(q2(zi)) ]
− g(tq1(zi) + (1 − t)q2(zi))
</equation>
<bodyText confidence="0.9946963125">
It is easy to prove that g(x) is strictly convex on the
interval [0, 1]. Because ∀zi, 0 ≤ q1(zi), q2(zi) ≤ 1,
we have
Because fi(q) is strictly convex on the unit simplex
∆, this stationary point is the global minimum. Note
that because 1
1�σ &gt; 1, q*(zi) can be seen as the re-
sult of applying a variant of the softmax function to
pθ(zi|xi). To compute q*, note that pθ(zi|xi) is the
product of a set of grammar rule probabilities, so we
can raise all the rule probabilities of the grammar to
the power of 1
1�σ and then run the normal E-step of
the EM algorithm. The normalization of q* is in-
cluded in the normal E-step.
With q*, the objective function becomes
</bodyText>
<equation confidence="0.9316005">
∑ ∑ p(zi, xi|θ) 1
F(θ, q*) = (1 − σ) log �−o
i zi
+ log p(θ) − logp(X)
</equation>
<bodyText confidence="0.999259142857143">
The first term is proportional to the log “likelihood”
of the corpus computed with the exponentiated rule
probabilities. So we can use the parsing algorithm to
efficiently compute the value of the objective func-
tion (on the training corpus or on a separate devel-
opment set) to determine when the coordinate ascent
iteration shall be terminated.
</bodyText>
<equation confidence="0.947502">
Case 3: σ = 1
</equation>
<bodyText confidence="0.734738">
We need to minimize
</bodyText>
<equation confidence="0.998144692307692">
fi(q) = − ∑ (q(zi) log pθ(zi|xi))
zi
Because log pθ(zi|xi) ≤ 0 for any zi, the minimum
of fi(q) is reached at
∑
= (1 − σ)
zi
tg(q1(zi)) + (1 − t)g(q2(zi))
&gt; g(tq1(zi) + (1 − t)q2(zi))
q*(zi) ={ 1 if zi = arg maxzi pθ(zi|xi)
0 otherwise
Because 1 − σ &gt; 0, we have
tfi(q1) + (1 − t)fi(q2) − fi(tq1 + (1 − t)q2) &gt; 0
</equation>
<bodyText confidence="0.9999035">
By applying the Lagrange multiplier, we get the
stationary point of fi(q) on the unit simplex ∆:
</bodyText>
<equation confidence="0.952874076923077">
q*(zi) = αipθ(zi|xi)
where αi is the normalization factor
1
αi =
∑zi pθ(zi|xi)
Case 4:
&gt; 1
Theorem 2.
is strictly concave on the unit sim-
plex
when
&gt; 1.
The proof is the same as that of theorem 1, ex-
</equation>
<bodyText confidence="0.9116809">
cept that 1
is now negative which reverses the
direction of the last inequality in the proof.
Theorem 3. The minimum of fi(q) is attained at a
vertex of the unit simplex
Proof. Assume the minimum of fi(q) is attained
at q* that is not a vertex of the unit simplex
so there are at least two assignments of zi, say
and z2, such that
an
</bodyText>
<equation confidence="0.963078769230769">
σ
fi(q)
∆
σ
−σ
∆.
∆,
z1
q*(z1)
d q*(z2) are nonzero.
(1)
1
�−o
</equation>
<page confidence="0.932329">
1
�−o
1328
</page>
<bodyText confidence="0.9946446">
Let q′ be the same distribution as q∗ except that
q′(z1) = 0 and q′(z2) = q∗(z1) + q∗(z2). Let q′′
be the same distribution as q∗ except that q′′(z1) =
q∗(z1) + q∗(z2) and q′′(z2) = 0. Obviously, both q′
and q′′ are in the unit simplex ∆ and q′ =� q′′. Let
</bodyText>
<equation confidence="0.934599333333333">
t = q*(z2)2 ,
q*(z )+q*z
(
</equation>
<bodyText confidence="0.99681747368421">
and obviously we have 0 &lt; t &lt; 1.
So we get q∗ = tq′ + (1 − t)q′′. According to The-
orem 2, fi(q) is strictly concave on the unit simplex
∆, so we have fi(q∗) &gt; tfi(q′) + (1 − t)fi(q′′).
Without loss of generality, suppose fi(q′) &gt; fi(q′′).
So we have tfi(q′) + (1 − t)fi(q′′) &gt; fi(q′′) and
therefore fi(q∗) &gt; fi(q′′), which means fi(q) does
not attain the minimum at q∗. This contradicts the
assumption.
Now we need to find out at which of the vertices
of the unit simplex ∆ is the minimum of fi(q) at-
tained. At the vertex where the probability mass is
concentrated at the assignment z, the value of fi(q)
is − log pθ(z|xi). So the minimum is attained at
&gt; 1 is attained at the same point as in the case of
= 1, at which all the probability mass is assigned
to the best parse of the sentence. So
can be com-
puted using the E-step of the Viterbi EM algori
</bodyText>
<equation confidence="0.870581">
σ
σ
q∗
thm.
</equation>
<bodyText confidence="0.846583">
Denote the best parse by z∗i . With q∗, the objective
function becomes
</bodyText>
<equation confidence="0.988459428571429">
F(θ, q∗) = � log p(z∗i , xi|θ)
i
log p(θ) − logp(X)
of 0 &lt;
σ
(Eq.1)
σ
</equation>
<bodyText confidence="0.969800684210526">
thm.
1329 reduces to the Viterbi EM algorithm, which consid-
ers only the best parses of the training sentences in
the E-step. When 0 &lt;
&lt; 1, our approach falls
between standard EM and Viterbi EM: it applies a
softmax function
to the distributions of parses
of the training sentences in the E-step. The softmax
function can be computed by simply exponentiating
the grammar rule probabilities before the standard
E-step, which does not increase the time complexity
of the E-step. We refer to the algorithm in the case
&lt; 1 as the softmax-EM algori
(e.g., a random grammar). So we need to set
to a
value that is large enough to induce unambiguity. On
the other hand, natural language grammars do con-
tain some degree of ambiguity, so if the value of
is too large, then the learned grammar might be ex-
cessively unambiguous and thus not a good model
of natural languages. Hence, it is unclear how to
choose an optimal value of
One way to avoid choosing a fixed value of
is
to anneal its value. We start learning with a large
value of
(e.g.,
= 1) to strongly push the learner
away from the highly ambiguous initial grammar;
then we gradually reduce the value of
possibly
ending with
= 0, to avoid inducing excessive un-
ambiguity in the learned grammar. Note that if the
value of
is annealed to 0, then our approach can be
seen as providing an unambiguous initialization for
</bodyText>
<equation confidence="0.6276818">
σ
σ
σ.
σ
σ
σ
σ,
σ
σ
dard EM.
q∗(zi) = �1 if zi = argmaxzz pθ(zi|xi)
0 otherwise
nce
thm.
ative parameter
</equation>
<bodyText confidence="0.803147333333333">
A larger value of
corresponds
to a stronger bias in favor of an unambiguous gram-
mar. When
= 0, our approach reduces to the stan-
dard EM algori
</bodyText>
<equation confidence="0.9700165">
σ.
σ
σ
thm. When σ &gt; 1, our approach
It can be seen that the minimum in the case of
+
</equation>
<bodyText confidence="0.9999885">
The first term is the sum of the log probabilities of
the best parses of the corpus. So again we can use
the parsing algorithm to efficiently compute it to de-
cide when to terminate the iterative algori
</bodyText>
<sectionHeader confidence="0.886299" genericHeader="method">
Summary
</sectionHeader>
<bodyText confidence="0.996572666666667">
Our unambiguity regularization approach is an ex-
tension of the EM algorithm. The behavior of our
approach is controlled by the value of the nonneg-
</bodyText>
<subsectionHeader confidence="0.999725">
3.1 Annealing the Strength of Regularization
</subsectionHeader>
<bodyText confidence="0.960135666666667">
In unsupervised learning of probabilistic grammars,
the initial grammar is typically very ambiguous
stan
</bodyText>
<subsectionHeader confidence="0.997885">
3.2 Unambiguity Regularization with
Mean-field Variational Infere
</subsectionHeader>
<bodyText confidence="0.999713272727273">
Variational inference approximates the posterior of
the model given the data. It typically leads to more
accurate predictions than the maximum a posteriori
(MAP) estimation. In addition, for certain types of
prior distributions (e.g., a Dirichlet prior with hy-
perparameters set to values less than 1), variational
inference is able to find a solution when MAP esti-
mation fails. Here we incorporate unambiguity reg-
ularization into mean
-field variational inference.
The objective function with unambiguity regular-
</bodyText>
<equation confidence="0.90919">
ization for mean-field variational inference is:
F(q(θ), q(Z)) = logp(X)
(KL(q(θ)q(Z)  ||p(θ, Z|X)) + σ∑H(zi)
i
∑where ∀i, H(zi) = − q(zi) log q(zi)
</equation>
<figure confidence="0.71632725">
Value of u Testing Accuracy
&lt; 10 &lt; 20 All
0 (standard EM) 46.2 39.7 34.9
0.25 53.7 44.7 40.3
0.5 51.9 42.9 38.8
0.75 51.6 43.1 38.8
1 (Viterbi EM) 58.3 45.2 39.4
zi
</figure>
<bodyText confidence="0.9663824">
We can perform coordinate ascent that alternately
optimizes q(θ) and q(Z). Since the regularization
term does not contain q(θ), the optimization of q(θ)
is exactly the same as in the standard mean-field
variational inference. To optimize q(Z), we have
</bodyText>
<equation confidence="0.845302428571429">
q*(Z) =
( )
∑
arg minKL(q(Z)||ep(X, Z)) + σ H(zi)
q(Z) i
where ep(X, Z) is defined as
log ep(X, Z) = Eq(θ)[log p(θ, Z, X)] + const
</equation>
<bodyText confidence="0.999771823529412">
Now we can follow a derivation similar to that in the
setting of MAP estimation with unambiguity regu-
larization, and we can obtain a similar result but with
pθ(zi|xi) replaced with ep(xi, zi) in each of the four
cases.
Note that if Dirichlet priors are used over gram-
mar rule probabilities θ, then ep(xi, zi) can be rep-
resented as the product of a set of weights in
mean-field variational inference (Kurihara and Sato,
2004). Therefore in order to compute q*(zi), when
0 &lt; σ &lt; 1, we simply need to raise all the weights
to the power of 1
1−σ before running the normal step
of computing q*(zi) in standard mean-field varia-
tional inference; and when σ ≥ 1, we can simply
use the weights to find the best parse of the training
sentence and assign probability 1 to it.
</bodyText>
<sectionHeader confidence="0.999644" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<bodyText confidence="0.9998065">
We tested the effectiveness of unambiguity regular-
ization in unsupervised learning of a type of depen-
dency grammar called the dependency model with
valence (DMV) (Klein and Manning, 2004). We
report the results on the Wall Street Journal cor-
pus (with section 2-21 for training and section 23
for testing) in section 4.1–4.3, and the results on
the corpora of eight additional languages in section
</bodyText>
<tableCaption confidence="0.997254">
Table 1: The dependency accuracies of grammars learned
by our approach with different values of u.
</tableCaption>
<bodyText confidence="0.99907425">
4.4. On each corpus, we trained the learner on the
gold-standard part-of-speech tags of the sentences
of length ≤ 10 with punctuation stripped off. We
started our algorithm with the informed initialization
proposed in (Klein and Manning, 2004), and termi-
nated the algorithm when the increase in the value
of the objective function fell below a threshold of
0.001%. To evaluate a learned grammar, we used the
grammar to parse the testing corpus and computed
the dependency accuracy which is the percentage of
the dependencies that are correctly matched between
the parses generated by the grammar and the gold
standard parses. We report the dependency accu-
racy on subsets of the testing corpus corresponding
to sentences of length ≤ 10, length ≤ 20, and the
entire testing corpus.
</bodyText>
<subsectionHeader confidence="0.975722">
4.1 Results with Different Values of σ
</subsectionHeader>
<bodyText confidence="0.999922647058823">
We compared the performance of our approach with
five different values of the parameter σ: 0 (i.e., stan-
dard EM), 0.25, 0.5, 0.75, 1 (i.e., Viterbi EM). Table
1 shows the experimental results. It can be seen that
learning with unambiguity regularization (i.e., with
σ &gt; 0) consistently outperforms learning without
unambiguity regularization (i.e., σ = 0). The gram-
mar learned by Viterbi EM has significantly higher
dependency accuracy in parsing short sentences. We
speculate that this is because short sentences are less
ambiguous and therefore a strong unambiguity regu-
larization is especially helpful in learning the gram-
matical structures of short sentences. On the testing
sentences of all lengths, σ = 0.25 achieves the best
dependency accuracy, which suggests that control-
ling the strength of unambiguity regularization can
contribute to improved performance.
</bodyText>
<page confidence="0.95991">
1330
</page>
<table confidence="0.987565692307692">
Testing Accuracy
&lt; 10 &lt; 20 All
DMV Model
UR-Annealing 63.6 53.1 47.9
UR-Annealing&amp;Prior 66.6 57.7 52.3
PR-S (Gillenwater et al., 2010) 62.1 53.8 49.1
SLN TieV&amp;N (Cohen and Smith, 2009) 61.3 47.4 41.4
LN Families (Cohen et al., 2008) 59.3 45.1 39.0
Extended Models
UR-Annealing on E-DMV(2,2) 71.4 62.4 57.0
UR-Annealing on E-DMV(3,3) 71.2 61.5 56.0
L-EVG (Headden et al., 2009) 68.8 - -
LexTSG-DMV (Blunsom and Cohn, 2010) 67.7 - 55.7
</table>
<tableCaption confidence="0.961109333333333">
Table 2: The dependency accuracies of grammars learned
by our approach (denoted by “UR”) with annealing and
prior, compared with previous published results.
</tableCaption>
<subsectionHeader confidence="0.97511">
4.2 Results with Annealing and Prior
</subsectionHeader>
<bodyText confidence="0.999918675675675">
We annealed the value of a from 1 to 0 when run-
ning our approach. We reduced the value of a at
a constant speed such that it reaches 0 at iteration
100. The results of this experiment (shown as “UR-
Annealing” in Table 2) suggest that annealing the
value of a not only helps circumvent the problem of
choosing an optimal value of a, but may also lead to
substantial improvements over the results of learn-
ing using any fixed value of a.
Dirichlet priors with the hyperparameter a set to a
value less than 1 are often used to induce parameter
sparsity. We added Dirichlet priors over grammar
rule probabilities and ran the variational inference
version of our approach. The value of a was set to
0.25 as suggested by previous work (Cohen et al.,
2008; Gillenwater et al., 2010). When tested with
different values of a, adding Dirichlet priors with
a = 0.25 consistently boosted the dependency ac-
curacy of the learned grammar by 1–2%. When the
value of a was annealed during variational inference
with Dirichlet priors, the dependency accuracy was
further improved (shown as “UR-Annealing&amp;Prior”
in Table 2).
The first part of Table 2 also compares our re-
sults with the best results that have been published in
the literature for unsupervised learning of the DMV
model (with different priors or regularizations than
ours). It can be seen that our best result (unambigu-
ity regularization with annealing and prior) clearly
outperforms previous results. Furthermore, we ex-
pect our approach to be more computationally ef-
ficient than the other approaches, because our ap-
proach only inserts an additional parameter expo-
nentiation step into each iteration of standard EM or
variational inference, in contrast to the other three
approaches all of which involve additional gradient
descent optimization steps in each iteration.
</bodyText>
<subsectionHeader confidence="0.993686">
4.3 Results on Extended Models
</subsectionHeader>
<bodyText confidence="0.999931136363637">
It has been pointed out that the DMV model is very
simplistic and cannot capture many linguistic phe-
nomena; therefore a few extensions of DMV have
been proposed, which achieve significant improve-
ment over DMV in unsupervised grammar learn-
ing (Headden et al., 2009; Blunsom and Cohn,
2010). We examined the effect of unambiguity reg-
ularization on E-DMV, an extension of DMV (with
two different settings: (2,2) and (3,3)) (Headden et
al., 2009; Gillenwater et al., 2010). As shown in
the second part of Table 2, unambiguity regular-
ization with annealing on E-DMV achieves better
dependency accuracies than the state-of-the-art ap-
proaches to unsupervised parsing with extended de-
pendency models. Addition of Dirichlet priors, how-
ever, did not further improve the accuracies in this
setting. Note that E-DMV is an unlexicalized ex-
tension of DMV that is relatively simple. We spec-
ulate that the performance of unambiguity regular-
ization can be further improved if applied to more
advanced models like LexTSG-DMV (Blunsom and
Cohn, 2010).
</bodyText>
<subsectionHeader confidence="0.988658">
4.4 Results on More Languages
</subsectionHeader>
<bodyText confidence="0.999740454545455">
We examined the effect of unambiguity regulariza-
tion with the DMV model on the corpora of eight
additional languages2. The experimental results of
all the nine languages are summarized in Table 3. It
can be seen that learning with unambiguity regular-
ization (i.e., with a &gt; 0) outperforms learning with-
out unambiguity regularization (i.e., a = 0) on eight
out of the nine languages, but the optimal value of
a is very different across languages. Annealing the
value of a from 1 to 0 does not always lead to fur-
ther improvement over using the optimal value of a
</bodyText>
<footnote confidence="0.991024666666667">
2The corpora are from the PASCAL Challenge on
Grammar Induction (http://wiki.cs.ox.ac.uk/
InducingLinguisticStructure/SharedTask).
</footnote>
<page confidence="0.994134">
1331
</page>
<bodyText confidence="0.99985725">
for each language, but on average it has better per-
formance than using any fixed value of Q and hence
is useful when the optimal value of Q is hard to iden-
tify.
</bodyText>
<sectionHeader confidence="0.999898" genericHeader="related work">
5 Related Work
</sectionHeader>
<bodyText confidence="0.99997734920635">
Deterministic annealing (DA) (Rose, 1998; Smith
and Eisner, 2004) also extends the standard EM al-
gorithm by exponentiating the posterior probabili-
ties of the hidden variables in the E-step. However,
the goal of DA is to improve the optimization of a
non-concave objective function, which is achieved
by setting the exponent in the E-step to a value close
to 0, so that the distribution of the hidden variables
becomes nearly uniform and the objective function
becomes almost concave and therefore easy to opti-
mize; this exponent is then gradually increased to
1 to optimize the original objective function. In
contrast, the goal of unambiguity regularization is
to bias learning in favor of unambiguous grammars,
which is achieved by setting the exponent in the E-
step (i.e., −�in Eq.1) to a value larger than 1, so
that the distribution of the hidden variables becomes
less uniform (i.e., parses become less ambiguous); in
our annealing approach, the exponent is initialized
to a very large value (positive infinity in our experi-
ment) to push the learner away from the ambiguous
initial grammar, and then gradually decreased to 1 to
avoid inducing excessive unambiguity in the learned
grammar. The empirical results of Smith and Eisner
(2004) show that DA resulted in lower parsing ac-
curacy compared with standard EM in unsupervised
constituent parsing; and a “skew” posterior term had
to be inserted into the E-step formulation of DA to
boost its accuracy over that of standard EM. In con-
trast, the results of our experiments show that unam-
biguity regularization leads to significantly higher
parsing accuracy than standard EM.
Unambiguity regularization is also related to
the minimum entropy regularization framework for
semi-supervised learning (Grandvalet and Bengio,
2005; Smith and Eisner, 2007), which tries to min-
imize the entropy of the class label or hidden vari-
ables on unlabeled data in addition to maximizing
the likelihood of labeled data. However, entropy
regularization is either motivated by the theoreti-
cal result that unlabeled data samples are informa-
tive when classes are well separated (Grandvalet and
Bengio, 2005), or derived from the expected condi-
tional log-likelihood (Smith and Eisner, 2007). In
contrast, our approach is motivated by the observed
unambiguity of natural language grammars. One
implication of this difference is that if our approach
is applied to semi-supervised learning, the regular-
ization term would be applied to labeled sentences
as well (by ignoring the labels) because the target
grammar shall be unambiguous on all the training
sentences.
The sparsity bias, which favors a grammar with
fewer grammar rules, has been widely used in un-
supervised grammar learning (Chen, 1995; Johnson
et al., 2007; Gillenwater et al., 2010). Although a
more sparse grammar is often less ambiguous, in
general that is not always the case. We have shown
that unambiguity regularization could lead to better
performance than approaches utilizing the sparsity
bias, and that the two types of biases can be applied
together for further improvement in the learning per-
formance.
</bodyText>
<sectionHeader confidence="0.999233" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999976086956522">
We have introduced unambiguity regularization, a
novel approach to unsupervised learning of proba-
bilistic natural language grammars. It is based on
the observation that natural language grammars are
remarkably unambiguous in the sense that in parsing
natural language sentences they tend to concentrate
the probability mass to a tiny portion of all possi-
ble parses. By using posterior regularization, we
incorporate an inductive bias into learning in favor
of grammars that are unambiguous on natural lan-
guage sentences. The resulting family of algorithms
includes standard EM and Viterbi EM, as well as
the softmax-EM algorithm which falls between stan-
dard EM and Viterbi EM. The softmax-EM algo-
rithm can be implemented by adding a simple pa-
rameter exponentiation step into standard EM. In
our experiments of unsupervised dependency gram-
mar learning, we show that unambiguity regulariza-
tion is beneficial to learning, and by incorporating
regularization strength annealing and sparsity priors
our approach outperforms the current state-of-the-
art grammar learning algorithms. For future work,
we plan to combine unambiguity regularization with
</bodyText>
<page confidence="0.966256">
1332
</page>
<table confidence="0.999353">
Arabic Basque Czech Danish Dutch English Portuguese Slovene Swedish
or = 0 (standard EM) 27.4 32.1 27.8 35.6 29.4 34.9 23.7 30.6 31.9
or = 0.25 30.6 39.3 27.2 35.2 30.9 40.3 27.7 23.8 42.0
or = 0.5 32.6 40.6 33.0 37.4 32.7 38.8 27.5 15.3 29.3
or = 0.75 31.6 41.8 16.1 36.0 35.1 38.8 26.2 15.1 32.7
or = 1(Viterbi EM) 29.6 39.8 28.6 33.6 28.0 39.4 27.3 14.6 37.2
UR-Annealing 26.7 41.6 39.3 34.1 43.1 47.8 26.4 16.4 46.0
</table>
<tableCaption confidence="0.9358895">
Table 3: The dependency accuracies (on sentences of all lengths in the testing corpus) of grammars learned by our
approach from the corpora of the following languages: Arabic (Hajiˇc et al., 2004), Basque (Aduriz et al., 2003), Czech
(Hajiˇc et al., 2000), Danish (Buch-Kromann et al., 2007), Dutch (Beek et al., 2002), English, Portuguese (Afonso et
al., 2002), Slovene (Erjavec et al., 2010), Swedish (Nivre et al., 2006).
</tableCaption>
<bodyText confidence="0.99807">
other types of priors and regularizations for unsu-
pervised grammar learning, to apply it to more ad-
vanced grammar models, and to explore alternative
formulations of unambiguity regularization.
</bodyText>
<sectionHeader confidence="0.978694" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.999951083333333">
The work of Kewei Tu was supported at Iowa State
University in part by a research assistantship from
the Iowa State University Center for Computational
Intelligence, Learning, and Discovery, and at Uni-
versity of California, Los Angeles by the DARPA
grant FA 8650-11-1-7149. The work of Vasant
Honavar was supported by the National Science
Foundation, while working at the Foundation. Any
opinion, finding, and conclusions contained in this
article are those of the authors and do not necessar-
ily reflect the views of the National Science Founda-
tion.
</bodyText>
<sectionHeader confidence="0.998602" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999446368421052">
I. Aduriz, M. J. Aranzabe, J. M. Arriola, A. Atutxa,
A. Diaz de Ilarraza, A. Garmendia, , and M. Oronoz.
2003. Construction of a basque dependency treebank.
In Proc. of the 2nd Workshop on Treebanks and Lin-
guistic Theories (TLT).
Susana Afonso, Eckhard Bick, Renato Haber, and Diana
Santos. 2002. “floresta sint´a(c)tica”: a treebank for
Portuguese. In Proceedings of the 3rd Intern. Conf. on
Language Resources and Evaluation (LREC), pages
1968–1703.
J. K. Baker. 1979. Trainable grammars for speech recog-
nition. In Speech Communication Papers for the 97th
Meeting of the Acoustical Society of America.
Van Der Beek, G. Bouma, R. Malouf, G. Van Noord, and
Rijksuniversiteit Groningen. 2002. The alpino depen-
dency treebank. In In Computational Linguistics in the
Netherlands (CLIN, pages 1686–1691.
Phil Blunsom and Trevor Cohn. 2010. Unsupervised
induction of tree substitution grammars for depen-
dency parsing. In Proceedings of the 2010 Conference
on Empirical Methods in Natural Language Process-
ing, EMNLP ’10, pages 1204–1213, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Matthias Buch-Kromann, J¨urgen Wedekind, , and
Jakob Elming. 2007. The copenhagen danish-
english dependency treebank v. 2.0. http://www.buch-
kromann.dk/matthias/cdt2.0/.
Stanley F. Chen. 1995. Bayesian grammar induction for
language modeling. In Proceedings of the 33rd annual
meeting on Association for Computational Linguistics.
Shay B. Cohen and Noah A. Smith. 2009. Shared logis-
tic normal distributions for soft parameter tying in un-
supervised grammar induction. In HLT-NAACL, pages
74–82.
Shay B. Cohen, Kevin Gimpel, and Noah A. Smith.
2008. Logistic normal priors for unsupervised prob-
abilistic grammar induction. In NIPS, pages 321–328.
Tomaz Erjavec, Darja Fiser, Simon Krek, and Nina
Ledinek. 2010. The jos linguistically tagged corpus
of slovene. In LREC.
Jenny Rose Finkel, Trond Grenager, and Christopher D.
Manning. 2007. The infinite tree. In Proceedings of
the 45th Annual Meeting of the Association of Compu-
tational Linguistics, pages 272–279. Association for
Computational Linguistics, June.
Kuzman Ganchev, Jo˜ao Grac¸a, Jennifer Gillenwater, and
Ben Taskar. 2010. Posterior regularization for struc-
tured latent variable models. Journal of Machine
Learning Research, 11:2001–2049.
Jennifer Gillenwater, Kuzman Ganchev, Jo˜ao Grac¸a, Fer-
nando Pereira, and Ben Taskar. 2010. Sparsity in de-
pendency grammar induction. In ACL ’10: Proceed-
ings of the ACL 2010 Conference Short Papers, pages
194–199, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Yves Grandvalet and Yoshua Bengio. 2005. Semi-
supervised learning by entropy minimization. In
</reference>
<page confidence="0.586707">
1333
</page>
<reference confidence="0.999891164556962">
Lawrence K. Saul, Yair Weiss, and L´eon Bottou, ed-
itors, Advances in Neural Information Processing Sys-
tems 17, pages 529–536. MIT Press, Cambridge, MA.
Jan Hajiˇc, Alena B¨ohmov´a, Eva Hajiˇcov´a, and Barbora
Vidov´a-Hladk´a. 2000. The Prague Dependency
Treebank: A Three-Level Annotation Scenario. In
A. Abeill´e, editor, Treebanks: Building and Using
Parsed Corpora, pages 103–127. Amsterdam:Kluwer.
Jan Hajiˇc, Otakar Smrˇz, Petr Zem´anek, Jan ˇSnaidauf, and
Emanuel Beˇska. 2004. Prague arabic dependency
treebank: Development in data and tools. In In Proc.
of the NEMLAR Intern. Conf. on Arabic Language Re-
sources and Tools, pages 110–117.
William P. Headden, III, Mark Johnson, and David Mc-
Closky. 2009. Improving unsupervised dependency
parsing with richer contexts and smoothing. In HLT-
NAACL, pages 101–109.
Mark Johnson, Thomas L. Griffiths, and Sharon Goldwa-
ter. 2007. Bayesian inference for pcfgs via markov
chain monte carlo. In HLT-NAACL, pages 139–146.
Dan Klein and Christopher D. Manning. 2004. Corpus-
based induction of syntactic structure: Models of de-
pendency and constituency. In Proceedings of ACL.
Kenichi Kurihara and Taisuke Sato. 2004. An appli-
cation of the variational Bayesian approach to prob-
abilistic contextfree grammars. In IJCNLP-04 Work-
shop beyond shallow analyses.
K. Lari and S. Young. 1990. The estimation of stochas-
tic context-free grammars using the inside-outside al-
gorithm. Computer Speech and Language, 4:35–36.
Percy Liang, Slav Petrov, Michael I. Jordan, and Dan
Klein. 2007. The infinite pcfg using hierarchi-
cal Dirichlet processes. In Proceedings of EMNLP-
CoNLL, pages 688–697.
Christopher D. Manning and Hinrich Sch¨utze. 1999.
Foundations of statistical natural language process-
ing. MIT Press, Cambridge, MA, USA.
Joakim Nivre, Jens Nilsson, and Johan Hall. 2006. Tal-
banken05: A Swedish Treebank with Phrase Struc-
ture and Dependency Annotation. In Proceedings of
the fifth international conference on Language Re-
sources and Evaluation (LREC2006), May 24-26,
2006, Genoa, Italy, pages 1392–1395. European Lan-
guage Resource Association, Paris.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In ACL-44: Proceedings of
the 21st International Conference on Computational
Linguistics and the 44th annual meeting of the Associ-
ation for Computational Linguistics, pages 433–440,
Morristown, NJ, USA. Association for Computational
Linguistics.
Hoifung Poon and Pedro Domingos. 2011. Sum-product
networks : A new deep architecture. In Proceedings
of the Twenty-Seventh Conference on Uncertainty in
Artificial Intelligence (UAI).
Kenneth Rose. 1998. Deterministic annealing for clus-
tering, compression, classification, regression, and re-
lated optimization problems. In Proceedings of the
IEEE, pages 2210–2239.
Noah A. Smith and Jason Eisner. 2004. Annealing
techniques for unsupervised statistical language learn-
ing. In Proceedings of the 42nd Annual Meeting on
Association for Computational Linguistics, ACL ’04,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
David A. Smith and Jason Eisner. 2007. Bootstrapping
feature-rich dependency parsers with entropic priors.
In Proceedings of the 2007 Joint Conference on Em-
pirical Methods in Natural Language Processing and
Computational Natural Language Learning (EMNLP-
CoNLL), pages 667–677, Prague, June.
Valentin I. Spitkovsky, Hiyan Alshawi, Daniel Jurafsky,
and Christopher D. Manning. 2010. Viterbi training
improves unsupervised dependency parsing. In Pro-
ceedings of the Fourteenth Conference on Computa-
tional Natural Language Learning, CoNLL ’10, pages
9–17, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
</reference>
<page confidence="0.995005">
1334
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.717497">
<title confidence="0.872966">Unambiguity Regularization for Unsupervised Learning of Probabilistic Grammars</title>
<affiliation confidence="0.989178">Departments of Statistics and Computer University of California, Los</affiliation>
<address confidence="0.993518">Los Angeles, CA 90095,</address>
<email confidence="0.999736">tukw@ucla.edu</email>
<author confidence="0.98772">Vasant Honavar</author>
<affiliation confidence="0.999871">Department of Computer Science Iowa State University</affiliation>
<address confidence="0.99996">Ames, IA 50011, USA</address>
<email confidence="0.999686">honavar@cs.iastate.edu</email>
<abstract confidence="0.9992348">introduce a novel approach named unamregularization unsupervised learning of probabilistic natural language grammars. The approach is based on the observation that natural language is remarkably unambiguous in the sense that only a tiny portion of the large number of possible parses of a natural language sentence are syntactically valid. We incorporate an inductive bias into grammar learning in favor of grammars that lead to unambiguous parses on natural language sentences. The resulting family of algorithms includes the expectation-maximization algorithm (EM) and its variant, Viterbi EM, as well as a so-called softmax-EM algorithm. The softmax-EM algorithm can be implemented with a simple and computationally efficient extension to standard EM. In our experiments of unsupervised dependency grammar learning, we show that unambiguity regularization is beneficial to learning, and in combination with annealing (of the regularization strength) and sparsity priors it leads to improvement over the current state of the art.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>I Aduriz</author>
<author>M J Aranzabe</author>
<author>J M Arriola</author>
<author>A Atutxa</author>
<author>A Diaz de Ilarraza</author>
<author>A Garmendia</author>
</authors>
<title>Construction of a basque dependency treebank.</title>
<date>2003</date>
<booktitle>In Proc. of the 2nd Workshop on Treebanks and Linguistic Theories (TLT).</booktitle>
<marker>Aduriz, Aranzabe, Arriola, Atutxa, de Ilarraza, Garmendia, 2003</marker>
<rawString>I. Aduriz, M. J. Aranzabe, J. M. Arriola, A. Atutxa, A. Diaz de Ilarraza, A. Garmendia, , and M. Oronoz. 2003. Construction of a basque dependency treebank. In Proc. of the 2nd Workshop on Treebanks and Linguistic Theories (TLT).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Susana Afonso</author>
<author>Eckhard Bick</author>
<author>Renato Haber</author>
<author>Diana Santos</author>
</authors>
<title>floresta sint´a(c)tica”: a treebank for Portuguese.</title>
<date>2002</date>
<booktitle>In Proceedings of the 3rd Intern. Conf. on Language Resources and Evaluation (LREC),</booktitle>
<pages>1968--1703</pages>
<contexts>
<context position="34720" citStr="Afonso et al., 2002" startWordPosition="5918" endWordPosition="5921">.2 30.9 40.3 27.7 23.8 42.0 or = 0.5 32.6 40.6 33.0 37.4 32.7 38.8 27.5 15.3 29.3 or = 0.75 31.6 41.8 16.1 36.0 35.1 38.8 26.2 15.1 32.7 or = 1(Viterbi EM) 29.6 39.8 28.6 33.6 28.0 39.4 27.3 14.6 37.2 UR-Annealing 26.7 41.6 39.3 34.1 43.1 47.8 26.4 16.4 46.0 Table 3: The dependency accuracies (on sentences of all lengths in the testing corpus) of grammars learned by our approach from the corpora of the following languages: Arabic (Hajiˇc et al., 2004), Basque (Aduriz et al., 2003), Czech (Hajiˇc et al., 2000), Danish (Buch-Kromann et al., 2007), Dutch (Beek et al., 2002), English, Portuguese (Afonso et al., 2002), Slovene (Erjavec et al., 2010), Swedish (Nivre et al., 2006). other types of priors and regularizations for unsupervised grammar learning, to apply it to more advanced grammar models, and to explore alternative formulations of unambiguity regularization. Acknowledgement The work of Kewei Tu was supported at Iowa State University in part by a research assistantship from the Iowa State University Center for Computational Intelligence, Learning, and Discovery, and at University of California, Los Angeles by the DARPA grant FA 8650-11-1-7149. The work of Vasant Honavar was supported by the Natio</context>
</contexts>
<marker>Afonso, Bick, Haber, Santos, 2002</marker>
<rawString>Susana Afonso, Eckhard Bick, Renato Haber, and Diana Santos. 2002. “floresta sint´a(c)tica”: a treebank for Portuguese. In Proceedings of the 3rd Intern. Conf. on Language Resources and Evaluation (LREC), pages 1968–1703.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J K Baker</author>
</authors>
<title>Trainable grammars for speech recognition.</title>
<date>1979</date>
<booktitle>In Speech Communication Papers for the 97th Meeting of the Acoustical</booktitle>
<publisher>Society of America.</publisher>
<contexts>
<context position="1913" citStr="Baker, 1979" startWordPosition="282" endWordPosition="283"> and sparsity priors it leads to improvement over the current state of the art. 1 Introduction Machine learning offers a potentially powerful approach to learning probabilistic grammars from data. Because of the high cost of manual sentence annotation, there is substantial interest in unsupervised grammar learning, i.e., the induction of a grammar from a corpus of unannotated sentences. The simplest such approaches attempt to maximize the like*Part of the work was done while at Iowa State University. lihood of the grammar given the training data, typically using expectation-maximization (EM) (Baker, 1979; Lari and Young, 1990; Klein and Manning, 2004). More recent approaches incorporate additional prior information of the target grammar into learning. For example, Kurihara and Sato (2004) used Dirichlet priors over rule probabilities to obtain smoothed estimates of the probabilities. Johnson et al. (2007) used Dirichlet priors with hyperparameters set to values less than 1 to encourage sparsity of grammar rules. Finkel et al. (2007) and Liang et al. (2007) proposed to use the hierarchical Dirichlet process prior to bias learning towards concise grammars without the need to pre-specify the num</context>
</contexts>
<marker>Baker, 1979</marker>
<rawString>J. K. Baker. 1979. Trainable grammars for speech recognition. In Speech Communication Papers for the 97th Meeting of the Acoustical Society of America.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Bouma Van Der Beek</author>
<author>R Malouf</author>
<author>G Van Noord</author>
<author>Rijksuniversiteit Groningen</author>
</authors>
<title>The alpino dependency treebank.</title>
<date>2002</date>
<booktitle>In In Computational Linguistics in the Netherlands (CLIN,</booktitle>
<pages>1686--1691</pages>
<marker>Van Der Beek, Malouf, Van Noord, Groningen, 2002</marker>
<rawString>Van Der Beek, G. Bouma, R. Malouf, G. Van Noord, and Rijksuniversiteit Groningen. 2002. The alpino dependency treebank. In In Computational Linguistics in the Netherlands (CLIN, pages 1686–1691.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Phil Blunsom</author>
<author>Trevor Cohn</author>
</authors>
<title>Unsupervised induction of tree substitution grammars for dependency parsing.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP ’10,</booktitle>
<pages>1204--1213</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="25683" citStr="Blunsom and Cohn, 2010" startWordPosition="4453" endWordPosition="4456">testing sentences of all lengths, σ = 0.25 achieves the best dependency accuracy, which suggests that controlling the strength of unambiguity regularization can contribute to improved performance. 1330 Testing Accuracy &lt; 10 &lt; 20 All DMV Model UR-Annealing 63.6 53.1 47.9 UR-Annealing&amp;Prior 66.6 57.7 52.3 PR-S (Gillenwater et al., 2010) 62.1 53.8 49.1 SLN TieV&amp;N (Cohen and Smith, 2009) 61.3 47.4 41.4 LN Families (Cohen et al., 2008) 59.3 45.1 39.0 Extended Models UR-Annealing on E-DMV(2,2) 71.4 62.4 57.0 UR-Annealing on E-DMV(3,3) 71.2 61.5 56.0 L-EVG (Headden et al., 2009) 68.8 - - LexTSG-DMV (Blunsom and Cohn, 2010) 67.7 - 55.7 Table 2: The dependency accuracies of grammars learned by our approach (denoted by “UR”) with annealing and prior, compared with previous published results. 4.2 Results with Annealing and Prior We annealed the value of a from 1 to 0 when running our approach. We reduced the value of a at a constant speed such that it reaches 0 at iteration 100. The results of this experiment (shown as “URAnnealing” in Table 2) suggest that annealing the value of a not only helps circumvent the problem of choosing an optimal value of a, but may also lead to substantial improvements over the results</context>
<context position="28023" citStr="Blunsom and Cohn, 2010" startWordPosition="4842" endWordPosition="4845">fficient than the other approaches, because our approach only inserts an additional parameter exponentiation step into each iteration of standard EM or variational inference, in contrast to the other three approaches all of which involve additional gradient descent optimization steps in each iteration. 4.3 Results on Extended Models It has been pointed out that the DMV model is very simplistic and cannot capture many linguistic phenomena; therefore a few extensions of DMV have been proposed, which achieve significant improvement over DMV in unsupervised grammar learning (Headden et al., 2009; Blunsom and Cohn, 2010). We examined the effect of unambiguity regularization on E-DMV, an extension of DMV (with two different settings: (2,2) and (3,3)) (Headden et al., 2009; Gillenwater et al., 2010). As shown in the second part of Table 2, unambiguity regularization with annealing on E-DMV achieves better dependency accuracies than the state-of-the-art approaches to unsupervised parsing with extended dependency models. Addition of Dirichlet priors, however, did not further improve the accuracies in this setting. Note that E-DMV is an unlexicalized extension of DMV that is relatively simple. We speculate that th</context>
</contexts>
<marker>Blunsom, Cohn, 2010</marker>
<rawString>Phil Blunsom and Trevor Cohn. 2010. Unsupervised induction of tree substitution grammars for dependency parsing. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP ’10, pages 1204–1213, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthias Buch-Kromann</author>
<author>J¨urgen Wedekind</author>
</authors>
<title>The copenhagen danishenglish dependency treebank v.</title>
<date>2007</date>
<note>2.0. http://www.buchkromann.dk/matthias/cdt2.0/.</note>
<marker>Buch-Kromann, Wedekind, 2007</marker>
<rawString>Matthias Buch-Kromann, J¨urgen Wedekind, , and Jakob Elming. 2007. The copenhagen danishenglish dependency treebank v. 2.0. http://www.buchkromann.dk/matthias/cdt2.0/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
</authors>
<title>Bayesian grammar induction for language modeling.</title>
<date>1995</date>
<booktitle>In Proceedings of the 33rd annual meeting on Association for Computational Linguistics.</booktitle>
<contexts>
<context position="32396" citStr="Chen, 1995" startWordPosition="5546" endWordPosition="5547">valet and Bengio, 2005), or derived from the expected conditional log-likelihood (Smith and Eisner, 2007). In contrast, our approach is motivated by the observed unambiguity of natural language grammars. One implication of this difference is that if our approach is applied to semi-supervised learning, the regularization term would be applied to labeled sentences as well (by ignoring the labels) because the target grammar shall be unambiguous on all the training sentences. The sparsity bias, which favors a grammar with fewer grammar rules, has been widely used in unsupervised grammar learning (Chen, 1995; Johnson et al., 2007; Gillenwater et al., 2010). Although a more sparse grammar is often less ambiguous, in general that is not always the case. We have shown that unambiguity regularization could lead to better performance than approaches utilizing the sparsity bias, and that the two types of biases can be applied together for further improvement in the learning performance. 6 Conclusion We have introduced unambiguity regularization, a novel approach to unsupervised learning of probabilistic natural language grammars. It is based on the observation that natural language grammars are remarka</context>
</contexts>
<marker>Chen, 1995</marker>
<rawString>Stanley F. Chen. 1995. Bayesian grammar induction for language modeling. In Proceedings of the 33rd annual meeting on Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shay B Cohen</author>
<author>Noah A Smith</author>
</authors>
<title>Shared logistic normal distributions for soft parameter tying in unsupervised grammar induction.</title>
<date>2009</date>
<booktitle>In HLT-NAACL,</booktitle>
<pages>74--82</pages>
<contexts>
<context position="2580" citStr="Cohen and Smith (2009)" startWordPosition="385" endWordPosition="388">04). More recent approaches incorporate additional prior information of the target grammar into learning. For example, Kurihara and Sato (2004) used Dirichlet priors over rule probabilities to obtain smoothed estimates of the probabilities. Johnson et al. (2007) used Dirichlet priors with hyperparameters set to values less than 1 to encourage sparsity of grammar rules. Finkel et al. (2007) and Liang et al. (2007) proposed to use the hierarchical Dirichlet process prior to bias learning towards concise grammars without the need to pre-specify the number of nonterminals. Cohen et al. (2008) and Cohen and Smith (2009) employed the logistic normal prior to model the correlations between grammar symbols. Gillenwater et al. (2010) incorporated a sparsity bias on grammar rules into learning by means of posterior regularization. More recently, Spitkovsky et al. (2010) and Poon and Domingos (2011) observed that the use of Viterbi EM (also called hard EM) in place of standard EM can lead to significantly improved results in unsupervised learning of probabilistic grammars from natural language and image data respectively, even if no prior information is used. This finding is surprising because Viterbi EM is a dege</context>
<context position="25446" citStr="Cohen and Smith, 2009" startWordPosition="4414" endWordPosition="4417"> in parsing short sentences. We speculate that this is because short sentences are less ambiguous and therefore a strong unambiguity regularization is especially helpful in learning the grammatical structures of short sentences. On the testing sentences of all lengths, σ = 0.25 achieves the best dependency accuracy, which suggests that controlling the strength of unambiguity regularization can contribute to improved performance. 1330 Testing Accuracy &lt; 10 &lt; 20 All DMV Model UR-Annealing 63.6 53.1 47.9 UR-Annealing&amp;Prior 66.6 57.7 52.3 PR-S (Gillenwater et al., 2010) 62.1 53.8 49.1 SLN TieV&amp;N (Cohen and Smith, 2009) 61.3 47.4 41.4 LN Families (Cohen et al., 2008) 59.3 45.1 39.0 Extended Models UR-Annealing on E-DMV(2,2) 71.4 62.4 57.0 UR-Annealing on E-DMV(3,3) 71.2 61.5 56.0 L-EVG (Headden et al., 2009) 68.8 - - LexTSG-DMV (Blunsom and Cohn, 2010) 67.7 - 55.7 Table 2: The dependency accuracies of grammars learned by our approach (denoted by “UR”) with annealing and prior, compared with previous published results. 4.2 Results with Annealing and Prior We annealed the value of a from 1 to 0 when running our approach. We reduced the value of a at a constant speed such that it reaches 0 at iteration 100. The</context>
</contexts>
<marker>Cohen, Smith, 2009</marker>
<rawString>Shay B. Cohen and Noah A. Smith. 2009. Shared logistic normal distributions for soft parameter tying in unsupervised grammar induction. In HLT-NAACL, pages 74–82.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shay B Cohen</author>
<author>Kevin Gimpel</author>
<author>Noah A Smith</author>
</authors>
<title>Logistic normal priors for unsupervised probabilistic grammar induction.</title>
<date>2008</date>
<booktitle>In NIPS,</booktitle>
<pages>321--328</pages>
<contexts>
<context position="2553" citStr="Cohen et al. (2008)" startWordPosition="380" endWordPosition="383">0; Klein and Manning, 2004). More recent approaches incorporate additional prior information of the target grammar into learning. For example, Kurihara and Sato (2004) used Dirichlet priors over rule probabilities to obtain smoothed estimates of the probabilities. Johnson et al. (2007) used Dirichlet priors with hyperparameters set to values less than 1 to encourage sparsity of grammar rules. Finkel et al. (2007) and Liang et al. (2007) proposed to use the hierarchical Dirichlet process prior to bias learning towards concise grammars without the need to pre-specify the number of nonterminals. Cohen et al. (2008) and Cohen and Smith (2009) employed the logistic normal prior to model the correlations between grammar symbols. Gillenwater et al. (2010) incorporated a sparsity bias on grammar rules into learning by means of posterior regularization. More recently, Spitkovsky et al. (2010) and Poon and Domingos (2011) observed that the use of Viterbi EM (also called hard EM) in place of standard EM can lead to significantly improved results in unsupervised learning of probabilistic grammars from natural language and image data respectively, even if no prior information is used. This finding is surprising b</context>
<context position="25494" citStr="Cohen et al., 2008" startWordPosition="4423" endWordPosition="4426"> is because short sentences are less ambiguous and therefore a strong unambiguity regularization is especially helpful in learning the grammatical structures of short sentences. On the testing sentences of all lengths, σ = 0.25 achieves the best dependency accuracy, which suggests that controlling the strength of unambiguity regularization can contribute to improved performance. 1330 Testing Accuracy &lt; 10 &lt; 20 All DMV Model UR-Annealing 63.6 53.1 47.9 UR-Annealing&amp;Prior 66.6 57.7 52.3 PR-S (Gillenwater et al., 2010) 62.1 53.8 49.1 SLN TieV&amp;N (Cohen and Smith, 2009) 61.3 47.4 41.4 LN Families (Cohen et al., 2008) 59.3 45.1 39.0 Extended Models UR-Annealing on E-DMV(2,2) 71.4 62.4 57.0 UR-Annealing on E-DMV(3,3) 71.2 61.5 56.0 L-EVG (Headden et al., 2009) 68.8 - - LexTSG-DMV (Blunsom and Cohn, 2010) 67.7 - 55.7 Table 2: The dependency accuracies of grammars learned by our approach (denoted by “UR”) with annealing and prior, compared with previous published results. 4.2 Results with Annealing and Prior We annealed the value of a from 1 to 0 when running our approach. We reduced the value of a at a constant speed such that it reaches 0 at iteration 100. The results of this experiment (shown as “URAnneali</context>
</contexts>
<marker>Cohen, Gimpel, Smith, 2008</marker>
<rawString>Shay B. Cohen, Kevin Gimpel, and Noah A. Smith. 2008. Logistic normal priors for unsupervised probabilistic grammar induction. In NIPS, pages 321–328.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomaz Erjavec</author>
<author>Darja Fiser</author>
<author>Simon Krek</author>
<author>Nina Ledinek</author>
</authors>
<title>The jos linguistically tagged corpus of slovene.</title>
<date>2010</date>
<booktitle>In LREC.</booktitle>
<contexts>
<context position="34752" citStr="Erjavec et al., 2010" startWordPosition="5923" endWordPosition="5926">= 0.5 32.6 40.6 33.0 37.4 32.7 38.8 27.5 15.3 29.3 or = 0.75 31.6 41.8 16.1 36.0 35.1 38.8 26.2 15.1 32.7 or = 1(Viterbi EM) 29.6 39.8 28.6 33.6 28.0 39.4 27.3 14.6 37.2 UR-Annealing 26.7 41.6 39.3 34.1 43.1 47.8 26.4 16.4 46.0 Table 3: The dependency accuracies (on sentences of all lengths in the testing corpus) of grammars learned by our approach from the corpora of the following languages: Arabic (Hajiˇc et al., 2004), Basque (Aduriz et al., 2003), Czech (Hajiˇc et al., 2000), Danish (Buch-Kromann et al., 2007), Dutch (Beek et al., 2002), English, Portuguese (Afonso et al., 2002), Slovene (Erjavec et al., 2010), Swedish (Nivre et al., 2006). other types of priors and regularizations for unsupervised grammar learning, to apply it to more advanced grammar models, and to explore alternative formulations of unambiguity regularization. Acknowledgement The work of Kewei Tu was supported at Iowa State University in part by a research assistantship from the Iowa State University Center for Computational Intelligence, Learning, and Discovery, and at University of California, Los Angeles by the DARPA grant FA 8650-11-1-7149. The work of Vasant Honavar was supported by the National Science Foundation, while wo</context>
</contexts>
<marker>Erjavec, Fiser, Krek, Ledinek, 2010</marker>
<rawString>Tomaz Erjavec, Darja Fiser, Simon Krek, and Nina Ledinek. 2010. The jos linguistically tagged corpus of slovene. In LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Trond Grenager</author>
<author>Christopher D Manning</author>
</authors>
<title>The infinite tree.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>272--279</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics,</institution>
<contexts>
<context position="2350" citStr="Finkel et al. (2007)" startWordPosition="347" endWordPosition="350">o maximize the like*Part of the work was done while at Iowa State University. lihood of the grammar given the training data, typically using expectation-maximization (EM) (Baker, 1979; Lari and Young, 1990; Klein and Manning, 2004). More recent approaches incorporate additional prior information of the target grammar into learning. For example, Kurihara and Sato (2004) used Dirichlet priors over rule probabilities to obtain smoothed estimates of the probabilities. Johnson et al. (2007) used Dirichlet priors with hyperparameters set to values less than 1 to encourage sparsity of grammar rules. Finkel et al. (2007) and Liang et al. (2007) proposed to use the hierarchical Dirichlet process prior to bias learning towards concise grammars without the need to pre-specify the number of nonterminals. Cohen et al. (2008) and Cohen and Smith (2009) employed the logistic normal prior to model the correlations between grammar symbols. Gillenwater et al. (2010) incorporated a sparsity bias on grammar rules into learning by means of posterior regularization. More recently, Spitkovsky et al. (2010) and Poon and Domingos (2011) observed that the use of Viterbi EM (also called hard EM) in place of standard EM can lead</context>
</contexts>
<marker>Finkel, Grenager, Manning, 2007</marker>
<rawString>Jenny Rose Finkel, Trond Grenager, and Christopher D. Manning. 2007. The infinite tree. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 272–279. Association for Computational Linguistics, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kuzman Ganchev</author>
<author>Jo˜ao Grac¸a</author>
<author>Jennifer Gillenwater</author>
<author>Ben Taskar</author>
</authors>
<title>Posterior regularization for structured latent variable models.</title>
<date>2010</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>11--2001</pages>
<marker>Ganchev, Grac¸a, Gillenwater, Taskar, 2010</marker>
<rawString>Kuzman Ganchev, Jo˜ao Grac¸a, Jennifer Gillenwater, and Ben Taskar. 2010. Posterior regularization for structured latent variable models. Journal of Machine Learning Research, 11:2001–2049.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jennifer Gillenwater</author>
<author>Kuzman Ganchev</author>
<author>Jo˜ao Grac¸a</author>
<author>Fernando Pereira</author>
<author>Ben Taskar</author>
</authors>
<title>Sparsity in dependency grammar induction.</title>
<date>2010</date>
<booktitle>In ACL ’10: Proceedings of the ACL 2010 Conference Short Papers,</booktitle>
<pages>194--199</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<marker>Gillenwater, Ganchev, Grac¸a, Pereira, Taskar, 2010</marker>
<rawString>Jennifer Gillenwater, Kuzman Ganchev, Jo˜ao Grac¸a, Fernando Pereira, and Ben Taskar. 2010. Sparsity in dependency grammar induction. In ACL ’10: Proceedings of the ACL 2010 Conference Short Papers, pages 194–199, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yves Grandvalet</author>
<author>Yoshua Bengio</author>
</authors>
<title>Semisupervised learning by entropy minimization.</title>
<date>2005</date>
<booktitle>Advances in Neural Information Processing Systems 17,</booktitle>
<pages>529--536</pages>
<editor>In Lawrence K. Saul, Yair Weiss, and L´eon Bottou, editors,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="31448" citStr="Grandvalet and Bengio, 2005" startWordPosition="5395" endWordPosition="5398">cessive unambiguity in the learned grammar. The empirical results of Smith and Eisner (2004) show that DA resulted in lower parsing accuracy compared with standard EM in unsupervised constituent parsing; and a “skew” posterior term had to be inserted into the E-step formulation of DA to boost its accuracy over that of standard EM. In contrast, the results of our experiments show that unambiguity regularization leads to significantly higher parsing accuracy than standard EM. Unambiguity regularization is also related to the minimum entropy regularization framework for semi-supervised learning (Grandvalet and Bengio, 2005; Smith and Eisner, 2007), which tries to minimize the entropy of the class label or hidden variables on unlabeled data in addition to maximizing the likelihood of labeled data. However, entropy regularization is either motivated by the theoretical result that unlabeled data samples are informative when classes are well separated (Grandvalet and Bengio, 2005), or derived from the expected conditional log-likelihood (Smith and Eisner, 2007). In contrast, our approach is motivated by the observed unambiguity of natural language grammars. One implication of this difference is that if our approach</context>
</contexts>
<marker>Grandvalet, Bengio, 2005</marker>
<rawString>Yves Grandvalet and Yoshua Bengio. 2005. Semisupervised learning by entropy minimization. In Lawrence K. Saul, Yair Weiss, and L´eon Bottou, editors, Advances in Neural Information Processing Systems 17, pages 529–536. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Hajiˇc</author>
</authors>
<title>Alena B¨ohmov´a, Eva Hajiˇcov´a, and Barbora Vidov´a-Hladk´a.</title>
<date>2000</date>
<pages>103--127</pages>
<editor>In A. Abeill´e, editor,</editor>
<publisher>Amsterdam:Kluwer.</publisher>
<marker>Hajiˇc, 2000</marker>
<rawString>Jan Hajiˇc, Alena B¨ohmov´a, Eva Hajiˇcov´a, and Barbora Vidov´a-Hladk´a. 2000. The Prague Dependency Treebank: A Three-Level Annotation Scenario. In A. Abeill´e, editor, Treebanks: Building and Using Parsed Corpora, pages 103–127. Amsterdam:Kluwer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Hajiˇc</author>
<author>Otakar Smrˇz</author>
<author>Petr Zem´anek</author>
<author>Jan ˇSnaidauf</author>
<author>Emanuel Beˇska</author>
</authors>
<title>Prague arabic dependency treebank: Development in data and tools. In</title>
<date>2004</date>
<booktitle>In Proc. of the NEMLAR Intern. Conf. on Arabic Language Resources and Tools,</booktitle>
<pages>110--117</pages>
<marker>Hajiˇc, Smrˇz, Zem´anek, ˇSnaidauf, Beˇska, 2004</marker>
<rawString>Jan Hajiˇc, Otakar Smrˇz, Petr Zem´anek, Jan ˇSnaidauf, and Emanuel Beˇska. 2004. Prague arabic dependency treebank: Development in data and tools. In In Proc. of the NEMLAR Intern. Conf. on Arabic Language Resources and Tools, pages 110–117.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William P Headden</author>
<author>Mark Johnson</author>
<author>David McClosky</author>
</authors>
<title>Improving unsupervised dependency parsing with richer contexts and smoothing.</title>
<date>2009</date>
<booktitle>In HLTNAACL,</booktitle>
<pages>101--109</pages>
<contexts>
<context position="25638" citStr="Headden et al., 2009" startWordPosition="4445" endWordPosition="4448">ical structures of short sentences. On the testing sentences of all lengths, σ = 0.25 achieves the best dependency accuracy, which suggests that controlling the strength of unambiguity regularization can contribute to improved performance. 1330 Testing Accuracy &lt; 10 &lt; 20 All DMV Model UR-Annealing 63.6 53.1 47.9 UR-Annealing&amp;Prior 66.6 57.7 52.3 PR-S (Gillenwater et al., 2010) 62.1 53.8 49.1 SLN TieV&amp;N (Cohen and Smith, 2009) 61.3 47.4 41.4 LN Families (Cohen et al., 2008) 59.3 45.1 39.0 Extended Models UR-Annealing on E-DMV(2,2) 71.4 62.4 57.0 UR-Annealing on E-DMV(3,3) 71.2 61.5 56.0 L-EVG (Headden et al., 2009) 68.8 - - LexTSG-DMV (Blunsom and Cohn, 2010) 67.7 - 55.7 Table 2: The dependency accuracies of grammars learned by our approach (denoted by “UR”) with annealing and prior, compared with previous published results. 4.2 Results with Annealing and Prior We annealed the value of a from 1 to 0 when running our approach. We reduced the value of a at a constant speed such that it reaches 0 at iteration 100. The results of this experiment (shown as “URAnnealing” in Table 2) suggest that annealing the value of a not only helps circumvent the problem of choosing an optimal value of a, but may also lead</context>
<context position="27998" citStr="Headden et al., 2009" startWordPosition="4838" endWordPosition="4841">more computationally efficient than the other approaches, because our approach only inserts an additional parameter exponentiation step into each iteration of standard EM or variational inference, in contrast to the other three approaches all of which involve additional gradient descent optimization steps in each iteration. 4.3 Results on Extended Models It has been pointed out that the DMV model is very simplistic and cannot capture many linguistic phenomena; therefore a few extensions of DMV have been proposed, which achieve significant improvement over DMV in unsupervised grammar learning (Headden et al., 2009; Blunsom and Cohn, 2010). We examined the effect of unambiguity regularization on E-DMV, an extension of DMV (with two different settings: (2,2) and (3,3)) (Headden et al., 2009; Gillenwater et al., 2010). As shown in the second part of Table 2, unambiguity regularization with annealing on E-DMV achieves better dependency accuracies than the state-of-the-art approaches to unsupervised parsing with extended dependency models. Addition of Dirichlet priors, however, did not further improve the accuracies in this setting. Note that E-DMV is an unlexicalized extension of DMV that is relatively sim</context>
</contexts>
<marker>Headden, Johnson, McClosky, 2009</marker>
<rawString>William P. Headden, III, Mark Johnson, and David McClosky. 2009. Improving unsupervised dependency parsing with richer contexts and smoothing. In HLTNAACL, pages 101–109.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
<author>Thomas L Griffiths</author>
<author>Sharon Goldwater</author>
</authors>
<title>Bayesian inference for pcfgs via markov chain monte carlo.</title>
<date>2007</date>
<booktitle>In HLT-NAACL,</booktitle>
<pages>139--146</pages>
<contexts>
<context position="2220" citStr="Johnson et al. (2007)" startWordPosition="325" endWordPosition="328">d grammar learning, i.e., the induction of a grammar from a corpus of unannotated sentences. The simplest such approaches attempt to maximize the like*Part of the work was done while at Iowa State University. lihood of the grammar given the training data, typically using expectation-maximization (EM) (Baker, 1979; Lari and Young, 1990; Klein and Manning, 2004). More recent approaches incorporate additional prior information of the target grammar into learning. For example, Kurihara and Sato (2004) used Dirichlet priors over rule probabilities to obtain smoothed estimates of the probabilities. Johnson et al. (2007) used Dirichlet priors with hyperparameters set to values less than 1 to encourage sparsity of grammar rules. Finkel et al. (2007) and Liang et al. (2007) proposed to use the hierarchical Dirichlet process prior to bias learning towards concise grammars without the need to pre-specify the number of nonterminals. Cohen et al. (2008) and Cohen and Smith (2009) employed the logistic normal prior to model the correlations between grammar symbols. Gillenwater et al. (2010) incorporated a sparsity bias on grammar rules into learning by means of posterior regularization. More recently, Spitkovsky et </context>
<context position="32418" citStr="Johnson et al., 2007" startWordPosition="5548" endWordPosition="5551">ngio, 2005), or derived from the expected conditional log-likelihood (Smith and Eisner, 2007). In contrast, our approach is motivated by the observed unambiguity of natural language grammars. One implication of this difference is that if our approach is applied to semi-supervised learning, the regularization term would be applied to labeled sentences as well (by ignoring the labels) because the target grammar shall be unambiguous on all the training sentences. The sparsity bias, which favors a grammar with fewer grammar rules, has been widely used in unsupervised grammar learning (Chen, 1995; Johnson et al., 2007; Gillenwater et al., 2010). Although a more sparse grammar is often less ambiguous, in general that is not always the case. We have shown that unambiguity regularization could lead to better performance than approaches utilizing the sparsity bias, and that the two types of biases can be applied together for further improvement in the learning performance. 6 Conclusion We have introduced unambiguity regularization, a novel approach to unsupervised learning of probabilistic natural language grammars. It is based on the observation that natural language grammars are remarkably unambiguous in the</context>
</contexts>
<marker>Johnson, Griffiths, Goldwater, 2007</marker>
<rawString>Mark Johnson, Thomas L. Griffiths, and Sharon Goldwater. 2007. Bayesian inference for pcfgs via markov chain monte carlo. In HLT-NAACL, pages 139–146.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Corpusbased induction of syntactic structure: Models of dependency and constituency.</title>
<date>2004</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="1961" citStr="Klein and Manning, 2004" startWordPosition="288" endWordPosition="291">provement over the current state of the art. 1 Introduction Machine learning offers a potentially powerful approach to learning probabilistic grammars from data. Because of the high cost of manual sentence annotation, there is substantial interest in unsupervised grammar learning, i.e., the induction of a grammar from a corpus of unannotated sentences. The simplest such approaches attempt to maximize the like*Part of the work was done while at Iowa State University. lihood of the grammar given the training data, typically using expectation-maximization (EM) (Baker, 1979; Lari and Young, 1990; Klein and Manning, 2004). More recent approaches incorporate additional prior information of the target grammar into learning. For example, Kurihara and Sato (2004) used Dirichlet priors over rule probabilities to obtain smoothed estimates of the probabilities. Johnson et al. (2007) used Dirichlet priors with hyperparameters set to values less than 1 to encourage sparsity of grammar rules. Finkel et al. (2007) and Liang et al. (2007) proposed to use the hierarchical Dirichlet process prior to bias learning towards concise grammars without the need to pre-specify the number of nonterminals. Cohen et al. (2008) and Coh</context>
<context position="23261" citStr="Klein and Manning, 2004" startWordPosition="4062" endWordPosition="4065"> of a set of weights in mean-field variational inference (Kurihara and Sato, 2004). Therefore in order to compute q*(zi), when 0 &lt; σ &lt; 1, we simply need to raise all the weights to the power of 1 1−σ before running the normal step of computing q*(zi) in standard mean-field variational inference; and when σ ≥ 1, we can simply use the weights to find the best parse of the training sentence and assign probability 1 to it. 4 Experiments We tested the effectiveness of unambiguity regularization in unsupervised learning of a type of dependency grammar called the dependency model with valence (DMV) (Klein and Manning, 2004). We report the results on the Wall Street Journal corpus (with section 2-21 for training and section 23 for testing) in section 4.1–4.3, and the results on the corpora of eight additional languages in section Table 1: The dependency accuracies of grammars learned by our approach with different values of u. 4.4. On each corpus, we trained the learner on the gold-standard part-of-speech tags of the sentences of length ≤ 10 with punctuation stripped off. We started our algorithm with the informed initialization proposed in (Klein and Manning, 2004), and terminated the algorithm when the increase</context>
</contexts>
<marker>Klein, Manning, 2004</marker>
<rawString>Dan Klein and Christopher D. Manning. 2004. Corpusbased induction of syntactic structure: Models of dependency and constituency. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenichi Kurihara</author>
<author>Taisuke Sato</author>
</authors>
<title>An application of the variational Bayesian approach to probabilistic contextfree grammars.</title>
<date>2004</date>
<booktitle>In IJCNLP-04 Workshop beyond shallow analyses.</booktitle>
<contexts>
<context position="2101" citStr="Kurihara and Sato (2004)" startWordPosition="308" endWordPosition="311">c grammars from data. Because of the high cost of manual sentence annotation, there is substantial interest in unsupervised grammar learning, i.e., the induction of a grammar from a corpus of unannotated sentences. The simplest such approaches attempt to maximize the like*Part of the work was done while at Iowa State University. lihood of the grammar given the training data, typically using expectation-maximization (EM) (Baker, 1979; Lari and Young, 1990; Klein and Manning, 2004). More recent approaches incorporate additional prior information of the target grammar into learning. For example, Kurihara and Sato (2004) used Dirichlet priors over rule probabilities to obtain smoothed estimates of the probabilities. Johnson et al. (2007) used Dirichlet priors with hyperparameters set to values less than 1 to encourage sparsity of grammar rules. Finkel et al. (2007) and Liang et al. (2007) proposed to use the hierarchical Dirichlet process prior to bias learning towards concise grammars without the need to pre-specify the number of nonterminals. Cohen et al. (2008) and Cohen and Smith (2009) employed the logistic normal prior to model the correlations between grammar symbols. Gillenwater et al. (2010) incorpor</context>
<context position="22719" citStr="Kurihara and Sato, 2004" startWordPosition="3965" endWordPosition="3968">standard mean-field variational inference. To optimize q(Z), we have q*(Z) = ( ) ∑ arg minKL(q(Z)||ep(X, Z)) + σ H(zi) q(Z) i where ep(X, Z) is defined as log ep(X, Z) = Eq(θ)[log p(θ, Z, X)] + const Now we can follow a derivation similar to that in the setting of MAP estimation with unambiguity regularization, and we can obtain a similar result but with pθ(zi|xi) replaced with ep(xi, zi) in each of the four cases. Note that if Dirichlet priors are used over grammar rule probabilities θ, then ep(xi, zi) can be represented as the product of a set of weights in mean-field variational inference (Kurihara and Sato, 2004). Therefore in order to compute q*(zi), when 0 &lt; σ &lt; 1, we simply need to raise all the weights to the power of 1 1−σ before running the normal step of computing q*(zi) in standard mean-field variational inference; and when σ ≥ 1, we can simply use the weights to find the best parse of the training sentence and assign probability 1 to it. 4 Experiments We tested the effectiveness of unambiguity regularization in unsupervised learning of a type of dependency grammar called the dependency model with valence (DMV) (Klein and Manning, 2004). We report the results on the Wall Street Journal corpus </context>
</contexts>
<marker>Kurihara, Sato, 2004</marker>
<rawString>Kenichi Kurihara and Taisuke Sato. 2004. An application of the variational Bayesian approach to probabilistic contextfree grammars. In IJCNLP-04 Workshop beyond shallow analyses.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Lari</author>
<author>S Young</author>
</authors>
<title>The estimation of stochastic context-free grammars using the inside-outside algorithm. Computer Speech and Language,</title>
<date>1990</date>
<pages>4--35</pages>
<contexts>
<context position="1935" citStr="Lari and Young, 1990" startWordPosition="284" endWordPosition="287"> priors it leads to improvement over the current state of the art. 1 Introduction Machine learning offers a potentially powerful approach to learning probabilistic grammars from data. Because of the high cost of manual sentence annotation, there is substantial interest in unsupervised grammar learning, i.e., the induction of a grammar from a corpus of unannotated sentences. The simplest such approaches attempt to maximize the like*Part of the work was done while at Iowa State University. lihood of the grammar given the training data, typically using expectation-maximization (EM) (Baker, 1979; Lari and Young, 1990; Klein and Manning, 2004). More recent approaches incorporate additional prior information of the target grammar into learning. For example, Kurihara and Sato (2004) used Dirichlet priors over rule probabilities to obtain smoothed estimates of the probabilities. Johnson et al. (2007) used Dirichlet priors with hyperparameters set to values less than 1 to encourage sparsity of grammar rules. Finkel et al. (2007) and Liang et al. (2007) proposed to use the hierarchical Dirichlet process prior to bias learning towards concise grammars without the need to pre-specify the number of nonterminals. C</context>
</contexts>
<marker>Lari, Young, 1990</marker>
<rawString>K. Lari and S. Young. 1990. The estimation of stochastic context-free grammars using the inside-outside algorithm. Computer Speech and Language, 4:35–36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Slav Petrov</author>
<author>Michael I Jordan</author>
<author>Dan Klein</author>
</authors>
<title>The infinite pcfg using hierarchical Dirichlet processes.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLPCoNLL,</booktitle>
<pages>688--697</pages>
<contexts>
<context position="2374" citStr="Liang et al. (2007)" startWordPosition="352" endWordPosition="355">of the work was done while at Iowa State University. lihood of the grammar given the training data, typically using expectation-maximization (EM) (Baker, 1979; Lari and Young, 1990; Klein and Manning, 2004). More recent approaches incorporate additional prior information of the target grammar into learning. For example, Kurihara and Sato (2004) used Dirichlet priors over rule probabilities to obtain smoothed estimates of the probabilities. Johnson et al. (2007) used Dirichlet priors with hyperparameters set to values less than 1 to encourage sparsity of grammar rules. Finkel et al. (2007) and Liang et al. (2007) proposed to use the hierarchical Dirichlet process prior to bias learning towards concise grammars without the need to pre-specify the number of nonterminals. Cohen et al. (2008) and Cohen and Smith (2009) employed the logistic normal prior to model the correlations between grammar symbols. Gillenwater et al. (2010) incorporated a sparsity bias on grammar rules into learning by means of posterior regularization. More recently, Spitkovsky et al. (2010) and Poon and Domingos (2011) observed that the use of Viterbi EM (also called hard EM) in place of standard EM can lead to significantly improv</context>
</contexts>
<marker>Liang, Petrov, Jordan, Klein, 2007</marker>
<rawString>Percy Liang, Slav Petrov, Michael I. Jordan, and Dan Klein. 2007. The infinite pcfg using hierarchical Dirichlet processes. In Proceedings of EMNLPCoNLL, pages 688–697.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Foundations of statistical natural language processing.</title>
<date>1999</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA, USA.</location>
<marker>Manning, Sch¨utze, 1999</marker>
<rawString>Christopher D. Manning and Hinrich Sch¨utze. 1999. Foundations of statistical natural language processing. MIT Press, Cambridge, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Jens Nilsson</author>
<author>Johan Hall</author>
</authors>
<title>Talbanken05: A Swedish Treebank with Phrase Structure and Dependency Annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of the fifth international conference on Language Resources and Evaluation (LREC2006),</booktitle>
<pages>1392--1395</pages>
<location>Genoa, Italy,</location>
<contexts>
<context position="34782" citStr="Nivre et al., 2006" startWordPosition="5928" endWordPosition="5931">8.8 27.5 15.3 29.3 or = 0.75 31.6 41.8 16.1 36.0 35.1 38.8 26.2 15.1 32.7 or = 1(Viterbi EM) 29.6 39.8 28.6 33.6 28.0 39.4 27.3 14.6 37.2 UR-Annealing 26.7 41.6 39.3 34.1 43.1 47.8 26.4 16.4 46.0 Table 3: The dependency accuracies (on sentences of all lengths in the testing corpus) of grammars learned by our approach from the corpora of the following languages: Arabic (Hajiˇc et al., 2004), Basque (Aduriz et al., 2003), Czech (Hajiˇc et al., 2000), Danish (Buch-Kromann et al., 2007), Dutch (Beek et al., 2002), English, Portuguese (Afonso et al., 2002), Slovene (Erjavec et al., 2010), Swedish (Nivre et al., 2006). other types of priors and regularizations for unsupervised grammar learning, to apply it to more advanced grammar models, and to explore alternative formulations of unambiguity regularization. Acknowledgement The work of Kewei Tu was supported at Iowa State University in part by a research assistantship from the Iowa State University Center for Computational Intelligence, Learning, and Discovery, and at University of California, Los Angeles by the DARPA grant FA 8650-11-1-7149. The work of Vasant Honavar was supported by the National Science Foundation, while working at the Foundation. Any o</context>
</contexts>
<marker>Nivre, Nilsson, Hall, 2006</marker>
<rawString>Joakim Nivre, Jens Nilsson, and Johan Hall. 2006. Talbanken05: A Swedish Treebank with Phrase Structure and Dependency Annotation. In Proceedings of the fifth international conference on Language Resources and Evaluation (LREC2006), May 24-26, 2006, Genoa, Italy, pages 1392–1395. European Language Resource Association, Paris.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Leon Barrett</author>
<author>Romain Thibaux</author>
<author>Dan Klein</author>
</authors>
<title>Learning accurate, compact, and interpretable tree annotation.</title>
<date>2006</date>
<booktitle>In ACL-44: Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics,</booktitle>
<pages>433--440</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="6945" citStr="Petrov et al., 2006" startWordPosition="1085" endWordPosition="1088">in section 6. 2 The (Un)ambiguity of Natural Language Grammars A grammar is said to be ambiguous on a sentence if the sentence can be parsed in more than one way by the grammar. It is widely acknowledged that natural language grammars are ambiguous on a significant proportion of natural language sentences. For example, Manning and Sch¨utze (1999) show that a sentence randomly chosen from the Wall Street Journal — “The post office will hold out discounts and service concessions as incentives” — has at least five plausible syntactic parses. When we parse this sentence using the Berkeley parser (Petrov et al., 2006), one of the state-of-the-art English language parsers, we find many alternative parses in addition to the parses shown in (Manning and Sch¨utze, 1999). Indeed, with a probabilistic context-free grammar of only 26 nonterminals (as used in the Berkeley parser), the estimated total number of possible parses&apos; of the example sentence is 2 × 1037. However, upon closer examination, we find that among this very large number of possible parses, only a few have significant probabilities. Figure 1 shows the probabilities of the 100 best parses of the example sentence. We can see that most of the parses </context>
</contexts>
<marker>Petrov, Barrett, Thibaux, Klein, 2006</marker>
<rawString>Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. 2006. Learning accurate, compact, and interpretable tree annotation. In ACL-44: Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, pages 433–440, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hoifung Poon</author>
<author>Pedro Domingos</author>
</authors>
<title>Sum-product networks : A new deep architecture.</title>
<date>2011</date>
<booktitle>In Proceedings of the Twenty-Seventh Conference on Uncertainty in Artificial Intelligence (UAI).</booktitle>
<contexts>
<context position="2859" citStr="Poon and Domingos (2011)" startWordPosition="427" endWordPosition="430"> priors with hyperparameters set to values less than 1 to encourage sparsity of grammar rules. Finkel et al. (2007) and Liang et al. (2007) proposed to use the hierarchical Dirichlet process prior to bias learning towards concise grammars without the need to pre-specify the number of nonterminals. Cohen et al. (2008) and Cohen and Smith (2009) employed the logistic normal prior to model the correlations between grammar symbols. Gillenwater et al. (2010) incorporated a sparsity bias on grammar rules into learning by means of posterior regularization. More recently, Spitkovsky et al. (2010) and Poon and Domingos (2011) observed that the use of Viterbi EM (also called hard EM) in place of standard EM can lead to significantly improved results in unsupervised learning of probabilistic grammars from natural language and image data respectively, even if no prior information is used. This finding is surprising because Viterbi EM is a degenerate case of standard EM and is therefore generally considered to be less effective in locating the optimum of the objective function. Spitkovsky et al. (2010) speculated that the observed advantage of Viterbi EM over standard EM is due to standard EM reserving too much probab</context>
</contexts>
<marker>Poon, Domingos, 2011</marker>
<rawString>Hoifung Poon and Pedro Domingos. 2011. Sum-product networks : A new deep architecture. In Proceedings of the Twenty-Seventh Conference on Uncertainty in Artificial Intelligence (UAI).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Rose</author>
</authors>
<title>Deterministic annealing for clustering, compression, classification, regression, and related optimization problems.</title>
<date>1998</date>
<booktitle>In Proceedings of the IEEE,</booktitle>
<pages>2210--2239</pages>
<contexts>
<context position="5510" citStr="Rose, 1998" startWordPosition="846" endWordPosition="847">al case of our approach also gives an explanation of the advantage of Viterbi EM observed in previous work: it is because Viterbi EM implicitly utilizes unambiguity regularization. In our experiments of unsupervised dependency grammar learning, we show that unambiguity regularization is beneficial to learning, and in combination with annealing (of the regularization strength) and sparsity priors it leads to improvement over the current state of the art. It should be noted that our approach is closely related to the deterministic annealing (DA) technique studied in the optimization literature (Rose, 1998). However, DA has a very different motivation than ours and differs from our approach in a few important algorithmic details, as will be discussed in section 5. When applied to unsupervised grammar learning, DA has been shown to lead to worse parsing accuracy than standard EM (Smith and Eisner, 2004); in contrast, we show that our approach leads to significantly higher parsing accuracy than standard EM in unsupervised dependency grammar learning. The rest of the paper is organized as follows. Section 2 analyzes the degree of unambiguity of natural language grammars. Section 3 introduces the un</context>
<context position="29711" citStr="Rose, 1998" startWordPosition="5119" endWordPosition="5120">ut unambiguity regularization (i.e., a = 0) on eight out of the nine languages, but the optimal value of a is very different across languages. Annealing the value of a from 1 to 0 does not always lead to further improvement over using the optimal value of a 2The corpora are from the PASCAL Challenge on Grammar Induction (http://wiki.cs.ox.ac.uk/ InducingLinguisticStructure/SharedTask). 1331 for each language, but on average it has better performance than using any fixed value of Q and hence is useful when the optimal value of Q is hard to identify. 5 Related Work Deterministic annealing (DA) (Rose, 1998; Smith and Eisner, 2004) also extends the standard EM algorithm by exponentiating the posterior probabilities of the hidden variables in the E-step. However, the goal of DA is to improve the optimization of a non-concave objective function, which is achieved by setting the exponent in the E-step to a value close to 0, so that the distribution of the hidden variables becomes nearly uniform and the objective function becomes almost concave and therefore easy to optimize; this exponent is then gradually increased to 1 to optimize the original objective function. In contrast, the goal of unambigu</context>
</contexts>
<marker>Rose, 1998</marker>
<rawString>Kenneth Rose. 1998. Deterministic annealing for clustering, compression, classification, regression, and related optimization problems. In Proceedings of the IEEE, pages 2210–2239.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noah A Smith</author>
<author>Jason Eisner</author>
</authors>
<title>Annealing techniques for unsupervised statistical language learning.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, ACL ’04,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="5811" citStr="Smith and Eisner, 2004" startWordPosition="896" endWordPosition="900">eneficial to learning, and in combination with annealing (of the regularization strength) and sparsity priors it leads to improvement over the current state of the art. It should be noted that our approach is closely related to the deterministic annealing (DA) technique studied in the optimization literature (Rose, 1998). However, DA has a very different motivation than ours and differs from our approach in a few important algorithmic details, as will be discussed in section 5. When applied to unsupervised grammar learning, DA has been shown to lead to worse parsing accuracy than standard EM (Smith and Eisner, 2004); in contrast, we show that our approach leads to significantly higher parsing accuracy than standard EM in unsupervised dependency grammar learning. The rest of the paper is organized as follows. Section 2 analyzes the degree of unambiguity of natural language grammars. Section 3 introduces the unambiguity regularization approach and shows that standard EM, Viterbi EM and softmax-EM are its special cases. We show the experimental results in section 4, discuss related work in section 5 and conclude the paper in section 6. 2 The (Un)ambiguity of Natural Language Grammars A grammar is said to be</context>
<context position="29736" citStr="Smith and Eisner, 2004" startWordPosition="5121" endWordPosition="5124">ty regularization (i.e., a = 0) on eight out of the nine languages, but the optimal value of a is very different across languages. Annealing the value of a from 1 to 0 does not always lead to further improvement over using the optimal value of a 2The corpora are from the PASCAL Challenge on Grammar Induction (http://wiki.cs.ox.ac.uk/ InducingLinguisticStructure/SharedTask). 1331 for each language, but on average it has better performance than using any fixed value of Q and hence is useful when the optimal value of Q is hard to identify. 5 Related Work Deterministic annealing (DA) (Rose, 1998; Smith and Eisner, 2004) also extends the standard EM algorithm by exponentiating the posterior probabilities of the hidden variables in the E-step. However, the goal of DA is to improve the optimization of a non-concave objective function, which is achieved by setting the exponent in the E-step to a value close to 0, so that the distribution of the hidden variables becomes nearly uniform and the objective function becomes almost concave and therefore easy to optimize; this exponent is then gradually increased to 1 to optimize the original objective function. In contrast, the goal of unambiguity regularization is to </context>
</contexts>
<marker>Smith, Eisner, 2004</marker>
<rawString>Noah A. Smith and Jason Eisner. 2004. Annealing techniques for unsupervised statistical language learning. In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, ACL ’04, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David A Smith</author>
<author>Jason Eisner</author>
</authors>
<title>Bootstrapping feature-rich dependency parsers with entropic priors.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL),</booktitle>
<pages>667--677</pages>
<location>Prague,</location>
<contexts>
<context position="31473" citStr="Smith and Eisner, 2007" startWordPosition="5399" endWordPosition="5402">arned grammar. The empirical results of Smith and Eisner (2004) show that DA resulted in lower parsing accuracy compared with standard EM in unsupervised constituent parsing; and a “skew” posterior term had to be inserted into the E-step formulation of DA to boost its accuracy over that of standard EM. In contrast, the results of our experiments show that unambiguity regularization leads to significantly higher parsing accuracy than standard EM. Unambiguity regularization is also related to the minimum entropy regularization framework for semi-supervised learning (Grandvalet and Bengio, 2005; Smith and Eisner, 2007), which tries to minimize the entropy of the class label or hidden variables on unlabeled data in addition to maximizing the likelihood of labeled data. However, entropy regularization is either motivated by the theoretical result that unlabeled data samples are informative when classes are well separated (Grandvalet and Bengio, 2005), or derived from the expected conditional log-likelihood (Smith and Eisner, 2007). In contrast, our approach is motivated by the observed unambiguity of natural language grammars. One implication of this difference is that if our approach is applied to semi-super</context>
</contexts>
<marker>Smith, Eisner, 2007</marker>
<rawString>David A. Smith and Jason Eisner. 2007. Bootstrapping feature-rich dependency parsers with entropic priors. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL), pages 667–677, Prague, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Valentin I Spitkovsky</author>
<author>Hiyan Alshawi</author>
<author>Daniel Jurafsky</author>
<author>Christopher D Manning</author>
</authors>
<title>Viterbi training improves unsupervised dependency parsing.</title>
<date>2010</date>
<booktitle>In Proceedings of the Fourteenth Conference on Computational Natural Language Learning, CoNLL ’10,</booktitle>
<pages>9--17</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="2830" citStr="Spitkovsky et al. (2010)" startWordPosition="422" endWordPosition="425"> et al. (2007) used Dirichlet priors with hyperparameters set to values less than 1 to encourage sparsity of grammar rules. Finkel et al. (2007) and Liang et al. (2007) proposed to use the hierarchical Dirichlet process prior to bias learning towards concise grammars without the need to pre-specify the number of nonterminals. Cohen et al. (2008) and Cohen and Smith (2009) employed the logistic normal prior to model the correlations between grammar symbols. Gillenwater et al. (2010) incorporated a sparsity bias on grammar rules into learning by means of posterior regularization. More recently, Spitkovsky et al. (2010) and Poon and Domingos (2011) observed that the use of Viterbi EM (also called hard EM) in place of standard EM can lead to significantly improved results in unsupervised learning of probabilistic grammars from natural language and image data respectively, even if no prior information is used. This finding is surprising because Viterbi EM is a degenerate case of standard EM and is therefore generally considered to be less effective in locating the optimum of the objective function. Spitkovsky et al. (2010) speculated that the observed advantage of Viterbi EM over standard EM is due to standard</context>
</contexts>
<marker>Spitkovsky, Alshawi, Jurafsky, Manning, 2010</marker>
<rawString>Valentin I. Spitkovsky, Hiyan Alshawi, Daniel Jurafsky, and Christopher D. Manning. 2010. Viterbi training improves unsupervised dependency parsing. In Proceedings of the Fourteenth Conference on Computational Natural Language Learning, CoNLL ’10, pages 9–17, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
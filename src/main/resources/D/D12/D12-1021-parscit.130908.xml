<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.985205">
A Bayesian Model for Learning SCFGs with Discontiguous Rules
</title>
<author confidence="0.997789">
Abby Levenberg Chris Dyer Phil Blunsom
</author>
<affiliation confidence="0.997015">
Dept. of Computer Science School of Computer Science Dept. of Computer Science
University of Oxford Carnegie Mellon Univeristy University of Oxford
</affiliation>
<email confidence="0.993075">
ablev@cs.ox.ac.uk cdyer@cs.cmu.edu pblunsom@cs.ox.ac.uk
</email>
<sectionHeader confidence="0.994946" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998684">
We describe a nonparametric model
and corresponding inference algorithm
for learning Synchronous Context Free
Grammar derivations for parallel text. The
model employs a Pitman-Yor Process prior
which uses a novel base distribution over
synchronous grammar rules. Through both
synthetic grammar induction and statistical
machine translation experiments, we show
that our model learns complex translational
correspondences— including discontiguous,
many-to-many alignments—and produces
competitive translation results. Further,
inference is efficient and we present results on
significantly larger corpora than prior work.
</bodyText>
<sectionHeader confidence="0.998128" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99996436">
In the twenty years since Brown et al. (1992) pio-
neered the first word-based statistical machine trans-
lation (SMT) models substantially more expressive
models of translational equivalence have been devel-
oped. The prevalence of complex phrasal, discon-
tiguous, and non-monotonic translation phenomena
in real-world applications of machine translation has
driven the development of hierarchical and syntac-
tic models based on synchronous context-free gram-
mars (SCFGs). Such models are now widely used in
translation and represent the state-of-the-art in most
language pairs (Galley et al., 2004; Chiang, 2007).
However, while the models used for translation have
evolved, the way in which they are learnt has not:
naive word-based models are still used to infer trans-
lational correspondences from parallel corpora.
In this work we bring the learning of the minimal
units of translation in step with the representational
power of modern translation models. We present a
nonparametric Bayesian model of translation based
on SCFGs, and we use its posterior distribution to
infer synchronous derivations for a parallel corpus
using a novel Gibbs sampler. Our model is able
to: 1) directly model many-to-many alignments,
thereby capturing non-compositional and idiomatic
translations; 2) align discontiguous phrases in both
the source and target languages; 3) have no restric-
tions on the length of a rule, the number of nonter-
minal symbols per rule, or their configuration.
Learning synchronous grammars is hard due to
the high polynomial complexity of dynamic pro-
gramming and the exponential space of possible
rules. As such most prior work for learning SCFGs
has relied on inference algorithms that were heuristi-
cally constrained or biased by word-based alignment
models and small experiments (Wu, 1997; Zhang et
al., 2008; Blunsom et al., 2009; Neubig et al., 2011).
In contrast to these previous attempts, our SCFG
model scales to large datasets (over 1.3M sentence
pairs) without imposing restrictions on the form of
the grammar rules or otherwise constraining the set
of learnable rules (e.g., with a word alignment).
We validate our sampler by demonstrating its
ability to recover grammars used to generate
synthetic datasets. We then evaluate our model by
inducing word alignments for SMT experiments
in several typologically diverse language pairs and
across a range of corpora sizes. Our results attest to
our model’s ability to learn synchronous grammars
encoding complex translation phenomena.
</bodyText>
<page confidence="0.559129">
223
</page>
<note confidence="0.991617">
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 223–232, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.980012" genericHeader="introduction">
2 Prior Work
</sectionHeader>
<bodyText confidence="0.999979842105264">
The goal of directly inducing phrasal translation
models from parallel corpora has received a lot of
attention in the NLP and SMT literature. Marcu
and Wong (2002) presented an ambitious maximum
likelihood model and EM inference algorithm for
learning phrasal translation representations. The
first issue this model faced was a massive parameter
space and intractable inference. However a more
subtle issue is that likelihood based models of this
form suffer from a degenerate solution, resulting
in the model learning whole sentences as phrases
rather than minimal units of translation. DeNero
et al. (2008) recognised this problem and proposed
a nonparametric Bayesian prior for contiguous
phrases. This had the dual benefits of biasing the
model towards learning minimal translation units,
and integrating out the parameters such that a much
smaller set of statistics would suffice for inference
with a Gibbs sampler. However this work fell short
by not evaluating the model independently, instead
only presenting results in which it was combined
with a standard word-alignment initialisation, thus
leaving open the question of its efficacy.
The fact that flat phrasal models lack a structured
approach to reordering has led many researchers to
pursue SCFG induction instead (Wu, 1997; Cherry
and Lin, 2007; Zhang et al., 2008; Blunsom et
al., 2009). The asymptotic time complexity of
the inside algorithm for even the simplest SCFG
models is O(|s|3|t|3), too high to be practical for
most real translation data. A popular solution to
this problem is to heuristically restrict inference
to derivations which agree with an independent
alignment model (Cherry and Lin, 2007; Zhang et
al., 2008). However this may have the unintended
effect of biasing the model back towards the initial
alignments that they attempt to improve upon.
More recently Neubig et al. (2011) reported a
novel Bayesian model for phrasal alignment and
extraction that was able to model phrases of multiple
granularities via a synchronous Adaptor Grammar.
However this model suffered from the common
problem of intractable inference and results were
presented for a very small number of samples from
a heuristically pruned beam, making interpreting
the results difficult.
Blunsom et al. (2009) presented an approach
similar to ours that implemented a Gibbs sampler
for a nonparametric Bayesian model of ITG. While
that work managed to scale to a non-trivially sized
corpus, like other works it relied on a state-of-the-art
word alignment model for initialisation. Our model
goes further by allowing discontiguous phrasal
translation units. Surprisingly, the freedom
that this extra power affords allows the Gibbs
sampler we propose to mix more quickly, allowing
state-of-the-art results from a simple initialiser.
</bodyText>
<sectionHeader confidence="0.992002" genericHeader="method">
3 Model
</sectionHeader>
<bodyText confidence="0.999851636363636">
We use a nonparametric generative model based on
the 2-parameter Pitman-Yor process (PYP) (Pitman
and Yor, 1997), a generalisation of the Dirichlet Pro-
cess, which has been used for various NLP modeling
tasks with state-of-the-art results such as language
modeling, word segmentation, text compression and
part of speech induction (Teh, 2006; Goldwater et
al., 2006; Wood et al., 2011; Blunsom and Cohn,
2011). In this section we first provide a brief defi-
nition of the SCFG formalism and then describe our
PYP prior for them.
</bodyText>
<subsectionHeader confidence="0.999302">
3.1 Synchronous Context-Free Grammar
</subsectionHeader>
<bodyText confidence="0.986951181818182">
An synchronous context-free grammar (SCFG) is a
5-tuple hE, A, V, S, Ri that generalises context-free
grammar to generate strings concurrently in two lan-
guages (Lewis and Stearns, 1968). E is a finite set of
source language terminal symbols, A is a finite set
of target language terminal symbols, V is a set of
nonterminal symbols, with a designated start sym-
bol S, and R is a set of synchronous rewrite rules.
A string pair is generated by starting with the pair
hS1  |S1i and recursively applying rewrite rules of
the form X → hs, t, ai where the left hand side
</bodyText>
<equation confidence="0.888416333333333">
(LHS) X is a nonterminal in V , s is a string in
(E ∪ V )*, t is a string in (A ∪ V )* and a specifies
a one-to-one mapping (bijection) between nontermi-
nal symbols in s and t. The following are examples:1
VP → h schlage NP1 NP2 vor  |suggest NP2 to NP1 i
NP → h die Kommission  |the commission i
</equation>
<footnote confidence="0.5820305">
1The nonterminal alignment a is indicated through sub-
scripts on the nonterminals.
</footnote>
<page confidence="0.872378">
224
</page>
<bodyText confidence="0.999945">
In a probabilistic SCFG, rules are associated with
probabilities such that the probabilities of all
rewrites of a particular LHS category sum to 1.
Translation with SCFGs is carried out by parsing
the source language with the monolingual source
language projection of the grammar (using standard
monolingual parsing algorithms), which induces
a parallel tree structure and translation in the
target language (Chiang, 2007). Alignment or
synchronous parsing is the process of concurrently
parsing both the source and target sentences,
uncovering the derivation or derivations that give
rise to a string pair (Wu, 1997; Dyer, 2010).
Our goal is to infer the most probable SCFG
derivations that explain a corpus of parallel sen-
tences, given a nonparametric prior over probabilis-
tic SCFGs. In this work we will consider grammars
with a single nonterminal category X.
</bodyText>
<subsectionHeader confidence="0.99308">
3.2 Pitman-Yor Process SCFG
</subsectionHeader>
<bodyText confidence="0.999993130434783">
Before training we have no way of knowing how
many rules will be needed in our grammar to ade-
quately represent the data. By using the Pitman-
Yor process as a prior on the parameters of a syn-
chronous grammar we can formulate a model which
prefers smaller numbers of rules that are reused
often, thereby avoiding degenerate grammars con-
sisting of large, overly specific rules. However, as
the data being fit grows, the model can become more
complex. The PYP is parameterised by a discount
parameter d, a strength parameter B, and the base
distribution !90, which gives the prior probability
of an event (in our case, events are rules) before
any observations have occurred. The discount is
subtracted from each positive rule count and damp-
ens the rich get richer effect where frequent rules
are given higher probability compared to infrequent
ones. The strength parameter controls the variance,
or concentration, about the base distribution.
In our model, a draw from a PYP is a distribution
over SCFG rules with a particular LHS (in fact, it is
a distribution over all well-formed rules). From this
distribution we can in turn draw individual rules:
</bodyText>
<equation confidence="0.999314">
!9X — PY(d, B, !90),
X —* (s, t, a) — !9X.
</equation>
<bodyText confidence="0.930053">
Although the PYP has no known analytical form,
we can marginalise out the !9X’s and reason about
</bodyText>
<figureCaption confidence="0.782389">
Figure 1: Example generation of a synchronous
grammar rule in our !90.
</figureCaption>
<bodyText confidence="0.999464">
individual rules directly using the process described
by Teh (2006). In this process, at time n a rule rn
is generated by stochastically deciding whether to
make another copy of a previously generated rule
or to draw a new one from the base distribution, !90.
Let V = (cp1, cp2,...) be the sequence of draws from
!90; thus V is the total number of draws from !90. A
rule rn corresponds to a selection of a cpk. Let ck
be a counter indicating the number of times cpk has
been selected. In particular, we set rn to cpk with
probability
</bodyText>
<equation confidence="0.9048575">
ck — d
B + n ,
</equation>
<bodyText confidence="0.60043">
and increment ck, or with probability
</bodyText>
<equation confidence="0.9785205">
B + d · V
B + n ,
</equation>
<bodyText confidence="0.9970145">
we draw a new rule from !90, append it to V, and use
it for rn.
</bodyText>
<subsectionHeader confidence="0.999751">
3.3 Base Distribution
</subsectionHeader>
<bodyText confidence="0.999525333333333">
The base distribution !90 for the PYP assigns prob-
ability to a rule based our belief about what consti-
tutes a good rule independent of observing any of
</bodyText>
<table confidence="0.347522571428571">
Step 1: Generate source side length.
Step 2: Generate source side configuration of
terminals (and non-terminal placeholders).
Step 3: Generate target length.
Step 4. Generate target side configuration of
terminals (and non-terminal placeholders).
Step 5. Generate the words.
</table>
<page confidence="0.359404">
225
</page>
<bodyText confidence="0.995919375">
the data. We describe a novel generative process for
all rules X —* (s, t, a) that encodes these beliefs.
We describe the generative process generally here
in text, and readers may refer to the example in Fig-
ure 1. The process begins by generating the source
length (total number of terminal and nonterminal
symbols, written |s|) by drawing from a Poisson dis-
tribution with mean 1:
</bodyText>
<equation confidence="0.975575">
|s |— Poisson(1) .
</equation>
<bodyText confidence="0.980058727272727">
This assigns high probability to shorter rules,
but arbitrarily long rules are possible with a low
probability. Then, for every position in s, we decide
whether it will contain a terminal or nonterminal
symbol by repeated, independent draws from a
Bernoulli distribution. Since we believe that shorter
rules should be relatively more likely to contain
terminal symbols than longer rules, we define the
probability of a terminal symbol to be φ&apos;s&apos; where
0 &lt; φ &lt; 1 is a hyperparameter.
si — Bernoulli(φ&apos;s&apos;) b i E [1, |s|] .
We next generate the length of the target side of
the rule. Let #NT(s) denote the number of nonter-
minal symbols we generated in s, i.e., the arity of
the rule. Our intuition here is that source and target
lengths should be similar. However, to ensure that
the rule is well-formed, t must contain exactly as
many nonterminal symbols as the source does. We
therefore draw the number of target terminal sym-
bols from a Poisson whose mean is the number of
terminal symbols in the source, plus a small constant
λ0 to ensure that it is greater than zero:
</bodyText>
<equation confidence="0.997062">
|t |— #NT(s) — Poisson (|s |— #NT(s) + λ0) .
</equation>
<bodyText confidence="0.984087722222222">
We then determine whether each position in t is
a terminal or nonterminal symbol by drawing uni-
formly from the bag of #NT(s) source nontermi-
nals and |t |— #NT(s) terminal indicators, with-
out replacement. At this point we have created a
rule template which indicates how large the rule is,
whether each position contains a terminal or non-
terminal symbol, and the reordering of the source
nonterminals a. To conclude the process we must
select the terminal types from the source and target
vocabularies. To do so, we use the following distri-
bution:
Pterminals(s, t) = 2
where PM1_(s, t) (PM1 (s, t)) first generates the
source (target) terminals from uniform draws from
the vocabulary, then generates the string in the other
language according to IBM MODEL 1, marginaliz-
ing over the alignments (Brown et al., 1993).
</bodyText>
<sectionHeader confidence="0.938148" genericHeader="method">
4 Gibbs Sampler
</sectionHeader>
<bodyText confidence="0.995155028571428">
In this section we introduce a Gibbs sampler that
enables us to perform posterior inference given a
corpus of sentence pairs. Our innovation is to repre-
sent the synchronous derivation of a sentence pair in
a hierarchical 4-dimensional binary alignment grid,
with elements z[s,t,u,v] E {0, 11.
The settings of the grid variables completely
determine the SCFG rules in the current derivation.
A setting of a binary variable z[s,t,u,v] = 1 represents
a constituent linking the source span [s, t] and the
target span [u, v] in the current derivation; variables
with a value of 0 indicate no link between spans
[s, t] and [u, v].2 This relationship from our grid
representation is illustrated in Figure 2a.
Our Gibbs sampler operates over the space of all
the random variables z[s,t,u,v], resampling one at a
time. Changes to a single variable imply that at most
two additional rules must be generated, as illustrated
in Figure 2b. The probability of choosing a binary
setting of 0 or 1 for a variable is proportional to the
probability of generating the two derivations under
the model described in the previous section. Note
that for a given sentence, most of the bispan vari-
ables must be set to 0 otherwise they would violate
the strict nesting constraint required for valid SCFG
derivations. We discuss below how to exploit this
fact to limit the number of binary variables that must
be resampled for each sentence.
To be valid, a Gibbs sampler must be ergodic and
satisfy detailed balance. Ergodicity requires that
there is non-zero probability that any state in the
sampler be reachable from any other state. Clearly
2Our grid representation is the synchronous generalisation
of the well-known correspondence between CFG derivations
and Boolean matrices; see Lee (2002) for an overview.
</bodyText>
<figure confidence="0.96857">
PM1_(s, t) + PM1_(s, t)
226
AMNA
SUCCESSFUL
BE
WILL
Amna will succeed
AMNA
SUCCESSFUL
BE
WILL
Amna will succeed
{mna
kAmyAb
hw
gy
{mna
kAmyAb
hw
gy
(a) An example grid representation of a syn-
</figure>
<bodyText confidence="0.606663666666667">
chronous derivation. The SCFG rules (annotated
with their bispans) that correspond to this setting
of the grid are:
</bodyText>
<equation confidence="0.976585">
X[0,4,0,3] →
h X[0,1,0,1] X[1,4,1,3]  |X[0,1,0,1] X[1,4,1,3] i
X[0,1,0,1] → h {mna  |Amna i
X[1,4,1,3] → h kAmyAb hw gy  |will succeed i
</equation>
<bodyText confidence="0.986002">
(b) The toggle operator resamples a bispan vari-
able (here, x[1,3,2,3], shown in blue) to determine
whether it should be subtracted from the immedi-
ately dominating rule (bispan in red) and made into
a child rule in the derivation. This would require
the addition of the following two rules:
</bodyText>
<figure confidence="0.714740833333333">
X[1,4,1,3] → h X[1,3,2,3] gy  |will X[1,3,2,3]i
X[1,3,2,3] → h kAmyAb hw  |succeed i
Alternatively, the active bispan variable can be set
so it is not a constituent, which would require the
single rule:
X[1,4,1,3] → h kAmyAb hw gy  |will succeed i
</figure>
<figureCaption confidence="0.998889">
Figure 2: A single operation of the Gibbs sampler for a binary alignment grid.
</figureCaption>
<bodyText confidence="0.999878625">
our operator satisfies this since given any configu-
ration of the alignment grid we can use the toggle
operator to flatten the derivation to a single rule and
then break it back down to reach any derivation.
Detailed balance requires that the probability of
transitioning between two possible adjacent sampler
states respects their joint probabilities in the station-
ary distribution. One way to ensure this is to make
the order in which bispan variables are visited deter-
ministic and independent of the variables’ current
settings. Then, the probability of the sampler tar-
geting any bispan in the grid is equal regardless of
the current configuration of the alignment grid.
A naive instantiation of this strategy is to visit all
JsJ2JtJ2 bispans in some order. However, since we
wish to be able to draw many samples, this is not
computationally feasible. A much more efficient
approach avoids resampling variables that would
result in violations without visiting each of them
individually. However, to ensure detailed balanced
is maintained, the order that we resample bispans
has to match the order we would sample them using
any exhaustive approach. We achieve this by always
checking a derivation top-down, from largest to
smallest bispan. Under this ordering, whether or not
a smaller bispan is visited will be independent of
how the larger ones were resampled. Furthermore,
the set of variables that may be resampled is fixed
given this ordering. Therefore, the probability of
sampling any possible bispan in the sentence pair is
still uniform (ensuring detailed balance), while our
sampler remains fast.
</bodyText>
<sectionHeader confidence="0.998747" genericHeader="evaluation">
5 Evaluation
</sectionHeader>
<bodyText confidence="0.999918">
The preceding sections have introduced a model,
and accompanying inference technique, designed to
induce a posterior distribution over SCFG deriva-
tions containing discontiguous and phrasal transla-
tion rules. The evaluation that follows aims to deter-
mine our models ability to meet these design goals,
and to do so in a range of translation scenarios.
In order to validate both the model and the sam-
pler’s ability to learn an SCFG we first conduct a
synthetic experiment in which the true grammar is
</bodyText>
<page confidence="0.530606">
227
</page>
<bodyText confidence="0.999787">
known. Subsequently we conduct a series of experi-
ments on real parallel corpora of increasing sizes to
explore the empirical properties of our model.
</bodyText>
<subsectionHeader confidence="0.988345">
5.1 Synthetic Data Experiments
</subsectionHeader>
<bodyText confidence="0.999983777777778">
Prior work on SCFG induction for SMT has val-
idated modeling claims by reporting BLEU scores
on real translation tasks. However, the combination
of noisy data and the complexity of SMT pipelines
conspire to obscure whether models actually achieve
their design goals, normally stated in terms of an
ability to induce SCFGs with particular properties.
Here we include a small synthetic data experiment
to clearly validate our models ability to learn an
SCFG that includes discontiguous and phrasal trans-
lation rules with non-monotonic word order.
Using the probabilistic SCFG shown in the top
half of Table 1 we stochastically generated three
thousand parallel sentence pairs as training data for
our model. We then ran the Gibbs sampler for fifty
iterations through the data.
The bottom half of Table 1 lists the five rules
with the highest marginal probability estimated by
the sampler. Encouragingly our model was able to
recover a grammar very close to the original. Even
for such a small grammar the space of derivations
is enormous and the task of recovering it from a
data sample is non-trivial. The divergence from the
true probabilities is due to the effect of the prior
assigning shorter rules higher probability. With a
larger data sample we would expect the influence of
the prior in the posterior to diminish.
</bodyText>
<subsectionHeader confidence="0.993332">
5.2 Machine Translation Evaluation
</subsectionHeader>
<bodyText confidence="0.987775">
Ultimately the efficacy of a model for SCFG induc-
tion will be judged on its ability to underpin a state-
of-the-art SMT system. Here we evaluate our model
by applying it to learning word alignments for par-
allel corpora from which SMT systems are induced.
We train models across a range of corpora sizes and
for language pairs that exhibit the type of complex
alignment phenomena that we are interested in mod-
eling: Chinese → English (ZH-EN), Urdu → English
(UR-EN) and German → English (DE-EN).
</bodyText>
<subsectionHeader confidence="0.793447">
Data and Baselines
</subsectionHeader>
<bodyText confidence="0.99918">
The UR-EN corpus is the smallest of those used in
our experiments and is taken from the NIST 2009
</bodyText>
<equation confidence="0.972717916666667">
GRAMMAR RULE TRUE PROBABILITY
X→ h X1 a X2 |X1 X2 1 i 0.2
X→ h b c d  |3 2 i 0.2
X→ h b d  |3 i 0.2
X→ h d  |3 i 0.2
X→ h c d  |3 1 i 0.2
SAMPLED RULE SAMPLED PROBABILITY
X→ h d  |3 i 0.25
X→ h b d  |3 i 0.24
X→ h c d  |3 1 i 0.24
X→ h b c d  |3 2 i 0.211
X→ h X1 a X2 |X1 X2 1 i 0.012
</equation>
<tableCaption confidence="0.941747">
Table 1: Manually created SCFG used to generate
synthetic data, and the five most probable inferred
rules by our model.
</tableCaption>
<table confidence="0.999641333333333">
ZH-EN UR-EN DE-EN
NIST NIST EUROPARL
TRAIN (SRC) 8.6M 1.2M 34M
TRAIN (TRG) 9.5M 1.0M 36M
DEV (SRC) 22K 18K 26K
DEV (TRG) 27K 16K 28K
</table>
<tableCaption confidence="0.999584">
Table 2: Corpora statistics (in words).
</tableCaption>
<bodyText confidence="0.999825611111111">
translation evaluation.3 The ZH-EN data is of a
medium scale and comes from the FBIS corpus.
The DE-EN pair constitutes the largest corpus and
is taken from Europarl, the proceedings of the Euro-
pean Parliament (Koehn, 2003). Statistics for the
data are shown in Table 2. We measure translation
quality via the BLEU score (Papineni et al., 2001).
All translation systems employ a Hiero
translation model during decoding. Baseline
word alignments were obtained by running
GIZA++ in both directions and symmetrizing using
the grow-diag-final-and heuristic (Och and
Ney, 2003; Koehn et al., 2003). Decoding was
performed with the cdec decoder (Dyer et al.,
2010) with the synchronous grammar extracted
using the techniques developed by Lopez (2008).
All translation systems include a 5-gram language
model built from a five hundred million token subset
</bodyText>
<footnote confidence="0.6312715">
3http://www.itl.nist.gov/iad/mig/tests/
mt/2009/
</footnote>
<page confidence="0.581002">
228
</page>
<table confidence="0.9558028">
LANGUAGE TEST MODEL 4 MODEL 1 PYP-SCFG
PAIR SET BASELINE INITIALISATION WEAK M1 INIT. STRONG HMM INIT.
UR-EN MT09 23.1 18.5 23.7 24.0
ZH-EN MT03-08 29.4 19.8 28.3 29.8
DE-EN EUROPARL 28.4 25.5 27.8 29.2
</table>
<tableCaption confidence="0.990582">
Table 3: Results for the SMT experiments in BLEU. The baseline is produced using a full GIZA++ run. The
</tableCaption>
<bodyText confidence="0.817534">
MODEL 1 INITIALISATION column is from the initialisation alignments using MODEL 1 and no sampling.
The PYP-SCFG columns show results for the 500th sample for both MODEL 1 and HMM initialisations.
of all the English data made available for the NIST
2009 shared task (Graff, 2003).
</bodyText>
<subsectionHeader confidence="0.936484">
Experimental Setup
</subsectionHeader>
<bodyText confidence="0.99998865">
To obtain the PYP-SCFG word alignments we
ran the sampler for five hundred iterations for each
of the language pairs and experimental conditions
described below. We used the approach of Newman
et al. (2007) to distribute the sampler across multi-
ple threads. The strength 0 and discount d hyper-
parameters of the Pitman-Yor Processes, and the ter-
minal penalty 0 (Section 3.3), were inferred using
slice sampling (Neal, 2000).
The Gibbs sampler requires an initial set of
derivations from which to commence sampling. In
our experiments we investigated both weak and
a strong initialisations, the former based on word
alignments from IBM Model 1 and the latter on
alignments from an HMM model (Vogel et al.,
1996). For decoding we used the word alignments
implied by the derivations in the final sample to
extract a Hiero grammar with the same standard set
of relative frequency, length, and language model
features used for the baseline.
</bodyText>
<subsectionHeader confidence="0.594798">
Weak Initialisation
</subsectionHeader>
<bodyText confidence="0.999965909090909">
Our first translation experiments ascertain the
degree to which our proposed Gibbs sampling
inference algorithm is able to learn good
synchronous derivations for the PYP-SCFG model.
A number of prior works on alignment with Gibbs
samplers have only evaluated models initialised
with the more complex GIZA++ alignment models
(Blunsom et al., 2009; DeNero et al., 2008), as a
result it can be difficult to separate the performance
of the sampler from that of the initialisation.
In order to do this, we initialise the sampler
</bodyText>
<table confidence="0.9955308">
LANGUAGE PAIR PYP-SCFG
MODEL 1 INIT. HMM INIT.
UR-EN 1.93/2.08 1.45/1.58
ZH-EN 3.47/4.28 1.69/2.37
DE-EN 4.05/4.77 1.50/2.04
</table>
<tableCaption confidence="0.99768">
Table 4: Average source/target rule lengths in the
</tableCaption>
<bodyText confidence="0.970543695652174">
PYP-SCFG models after the 500th sample for the
different initialisations.
using just the MODEL 1 distribution used in the
PYP-SCFG model’s base distribution. We denote
this a weak initialisation as no alignment models
outside of those included in the PYP-SCFG model
influence the resulting word alignments. The
BLEU scores for translation systems built from the
five hundredth sample are show in the WEAK M1
INIT. column of Table 3. Additionally we build a
translation system from the MODEL 1 alignment
used to initialise the sampler without using using our
PYP-SCFG model or sampling. BLEU scores are
shown in the MODEL 1 INITIALISATION column
of Table 3. Firstly it is clear MODEL 1 is indeed a
weak initialiser as the resulting translation systems
achieve uniformly low BLEU scores. In contrast, the
models built from the output of the Gibbs sampler
for the PYP-SCFG model achieve BLEU scores
comparable to those of the MODEL 4 BASELINE.
Thus the sampler has moved a good distance from
its initialisation, and done so in a direction that
results in better synchronous derivations.
</bodyText>
<subsectionHeader confidence="0.697144">
Strong Initialisation
</subsectionHeader>
<bodyText confidence="0.986557647058824">
Given we have established that the sampler can
produce state-of-the-art translation results from a
229
unique grammar rules in PYP
weak initialisation, it is instructive to investigate
whether initialising the model with a strong
alignment system, the GIZA++ HMM (Vogel et
al., 1996), leads to further improvements. Column
HMM INIT. of Table 3 shows the results for
initialising with the HMM word alignments and
sampling for 500 iterations. Starting with a stronger
initial sample results in both quicker mixing and
better translation quality for the same number of
sampling iterations.
Table 4 compares the average lengths of the rules
produced by the sampler with both the strong and
weak initialisers. As the size of the training corpora
increases (UR-EN → ZH-EN → DE-EN) we see that
the average size of the rules produced by the weakly
initialised sampler also increases, while that of the
strongly initialised model stays relatively uniform.
Initially both samplers start out with a large num-
ber of long rules and as the sampling progresses
the rules are broken down into smaller, more gen-
eralisable, pieces. As such we conclude from these
metrics that after five hundred samples the strongly
initialised model has converged to sampling from a
mode of the distribution while the weakly initialised
model converges more slowly and on the longer cor-
pora is still travelling towards a mode. This sug-
gests that longer sampling runs, and Gibbs operators
that make simultaneous updates to multiple parts
of a derivation, would enable the weakly initialised
model to obtain better translation results.
</bodyText>
<subsectionHeader confidence="0.966021">
Grammar Analysis
</subsectionHeader>
<bodyText confidence="0.9998973125">
The BLEU scores are informative as a measure of
translation quality but we also explored some of the
differences in the grammars obtained from the PYP-
SCFG model compared to the standard approach. In
Figures 3 and 4 we show some basic statistics of
the grammars our model produces. From Figure 3
we see that the number of unique rules in the PYP-
SCFG grammar decreases steadily as the sampler
iterates through the data, so the model is finding an
increasingly sparser distribution with fewer but bet-
ter quality rules as sampling progresses. Note that
the gradient of the curves appears to be a function of
the size of the corpus and suggests that the model
built from the large DE-EN corpus would benefit
from a longer sampling run. Figure 4 shows the dis-
tribution of rules with a given arity as a percentage
</bodyText>
<figure confidence="0.987635">
0 20 40 60 80 100
samples
</figure>
<figureCaption confidence="0.988229">
Figure 3: Unique grammar rules for each language
</figureCaption>
<bodyText confidence="0.99775375">
pair as a function of the number of samples. The
number of rule types decreases monotonically as
sampling continues. Rule counts are displayed by
normalised corpus size (see Table 2).
</bodyText>
<equation confidence="0.9945003">
X→ h 底  |end of i
X→ h 届 全  |ninth i*
X→ h 运作 X  |charter X i
X→ h 信心  |confidence in i
X→ h 中国 政府 X  |the chinese government X i
X→ h 都 是  |are i
X→ h 新华社 北京 X  |beijing , X i*
X→ h 有关 部门  |departments concerned i
X→ h新华f: ± 华盛顿 X  |washington , X i
X→ h 鲍IR 尔 X1 了 X2 ,  |he X1 X2 , i*
</equation>
<bodyText confidence="0.9924242">
Table 5: The five highest ZH-EN probability rules in
the Hiero grammar built from the PYP-SCFG that
are not in the baseline Hiero grammar (top), and the
top five rules in the baseline Hiero grammar that
are not in the PYP-SCFG grammar (bottom). An
* indicates a bad translation rule.
of the full grammar after the final sampling iteration.
The model prior biases the results to shorter rules as
the vast majority of the model probability mass is on
rules with zero, one or two nonterminals.
Tables 5 and 6 show the most probable rules in the
Hiero translation system obtained using the PYP-
SCFG alignments that are not present in the TM
from the GIZA++ alignments and visa versa. For
both language pairs, four of the top five rules in
</bodyText>
<figure confidence="0.933952705882353">
340
320
300
280
260
240
220
200
180
160
140
ur-en (* 1k)
zh-en (* 3k)
de-en (* 10k)
230
X→ h yh  |it is i
X→ h zmyn  |the earth i
</figure>
<bodyText confidence="0.9669344">
X→ h yhy X  |the same X i
X→ h X1 nhyN X2 gy  |X2 not be X1 i
X→ h X1 gY kh X2  |recommend that X2 X1 i*
X→ h hwN gY  |will i
X→ h Gyr mlky  |international i*
X→ h X1 *rAye kY X2  |X2 to X1 sources i*
X→ h nY X1 nhyN kyA X2  |did not X1 X2 i*
X→ h xAtwn X1 ky X2  |woman X2 the X1i
Table 6: Five of the top scoring rules in the UR-EN
Hiero grammar from sampled PYP-SCFG align-
ments (top) versus the baseline UR-EN Hiero gram-
mar rules not in the sampled grammar (bottom). An
* indicates a bad translation rule.
Figure 4: The percentage of rules with a given arity
in the final grammar of the PYP-SCFG model.
the PYP-SCFG grammar that are not in the heuris-
tically extracted grammar are correct and minimal
phrasal units of translation, whereas only two of the
top probability rules in the GIZA++ grammar are of
good translation quality.
</bodyText>
<sectionHeader confidence="0.991377" genericHeader="conclusions">
6 Conclusion and Further Work
</sectionHeader>
<bodyText confidence="0.999781913043478">
In this paper we have presented a nonparametric
Bayesian model for learning SCFGs directly
from parallel corpora. We have also introduced
a novel Gibbs sampller that allows for efficient
posterior inference. We show state-of-the-art
results and learn complex translation phenomena,
including discontiguous and many-to-many
phrasal alignments, without applying any heuristic
restrictions on the model to make learning tractable.
Our evaluation shows that we can use a principled
approach to induce SCFGs designed specifically
to utilize the full power of grammar based SMT
instead of relying on complex word alignment
heuristics with inherent bias.
Future work includes the obvious extension to
learning SCFGs that contain multiple nonterminals
instead of a single nonterminal grammar. We also
expect that expanding our sampler beyond strict
binary sampling may allow us to explore the space
of hierarchical word alignments more quickly
allowing for faster mixing. We expect with these
extensions our model of grammar induction may
further improve translation output.
</bodyText>
<sectionHeader confidence="0.995579" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9997588">
This work was supported by a grant from Google,
Inc. and EPRSRC grant no. EP/I010858/1 (Leven-
berg and Blunsom), the U. S. Army Research Lab-
oratory and U. S. Army Research Office under con-
tract/grant no. W911NF-10-1-0533 (Dyer).
</bodyText>
<sectionHeader confidence="0.993139" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.98038332">
P. Blunsom and T. Cohn. 2011. A hierarchical pitman-
yor process hmm for unsupervised part of speech
induction. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies, pages 865–874, Port-
land, Oregon, USA, June. Association for Computa-
tional Linguistics.
P. Blunsom, T. Cohn, C. Dyer, and M. Osborne. 2009.
A gibbs sampler for phrasal synchronous grammar
induction. In Proceedings of the Joint Conference of
the 47th Annual Meeting of the ACL and the 4th Inter-
national Joint Conference on Natural Language Pro-
cessing of the AFNLP, pages 782–790, Suntec, Singa-
pore, August. Association for Computational Linguis-
tics.
P. F. Brown, V. J. D. Pietra, R. L. Mercer, S. A. D. Pietra,
and J. C. Lai. 1992. An estimate of an upper bound
for the entropy of english. Computational Linguistics,
18(1):31–40.
P. F. Brown, V. J. D. Pietra, S. A. D. Pietra, and R. L.
Mercer. 1993. The mathematics of statistical machine
translation: parameter estimation. Computational Lin-
guistics, 19(2):263–311.
C. Cherry and D. Lin. 2007. Inversion transduc-
tion grammar for joint phrasal translation modeling.
</reference>
<figure confidence="0.989836916666667">
0 1 2 3+
arity
zh-en
ur-en
de-en
0.4
% of rules
0.3
0.2
0.1
0
231
</figure>
<reference confidence="0.999414656862745">
In Proceedings of SSST, NAACL-HLT 2007 / AMTA
Workshop on Syntax and Structure in Statistical Trans-
lation, pages 17–24, Rochester, New York, April.
Association for Computational Linguistics.
D. Chiang. 2007. Hierarchical phrase-based translation.
Computational Linguistics, 33(2):201–228.
J. DeNero, A. Bouchard-Cˆot´e, and D. Klein. 2008. Sam-
pling alignment structure under a Bayesian translation
model. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Processing,
pages 314–323, Honolulu, Hawaii, October. Associa-
tion for Computational Linguistics.
C. Dyer, A. Lopez, J. Ganitkevitch, J. Weese, F. Ture,
P. Blunsom, H. Setiawan, V. Eidelman, and P. Resnik.
2010. cdec: A decoder, alignment, and learning
framework for finite-state and context-free translation
models. In Proceedings of the ACL 2010 System
Demonstrations, ACLDemos ’10, pages 7–12.
C. Dyer. 2010. Two monolingual parses are better than
one (synchronous parse). In Proc. ofNAACL.
M. Galley, M. Hopkins, K. Knight, and D. Marcu. 2004.
What’s in a translation rule? In D. M. Susan Dumais
and S. Roukos, editors, HLT-NAACL 2004: Main
Proceedings, pages 273–280, Boston, Massachusetts,
USA, May 2 - May 7. Association for Computational
Linguistics.
S. Goldwater, T. L. Griffiths, and M. Johnson. 2006.
Contextual dependencies in unsupervised word seg-
mentation. In Proceedings of the 21st International
Conference on Computational Linguistics and 44th
Annual Meeting of the Association for Computational
Linguistics, Syndney, Australia.
D. Graff. 2003. English Gigaword. Linguistic Data Con-
sortium (LDC-2003T05).
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In NAACL ’03: Proceedings
of the 2003 Conference of the North American Chapter
of the Association for Computational Linguistics on
Human Language Technology, pages 48–54, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
P. Koehn. 2003. Europarl: A multilingual corpus for
evaluation of machine translation.
L. Lee. 2002. Fast context-free grammar parsing
requires fast Boolean matrix multiplication. Journal
of the ACM, 49(1):1–15.
P. M. Lewis, II and R. E. Stearns. 1968. Syntax-directed
transduction. J. ACM, 15:465–488, July.
A. Lopez. 2008. Machine Translation by Pattern Match-
ing. Ph.D. thesis, University of Maryland.
D. Marcu and D. Wong. 2002. A phrase-based,joint
probability model for statistical machine translation.
In Proceedings of the 2002 Conference on Empirical
Methods in Natural Language Processing, pages 133–
139. Association for Computational Linguistics, July.
R. Neal. 2000. Slice sampling. Annals of Statistics,
31:705–767.
G. Neubig, T. Watanabe, E. Sumita, S. Mori, and
T. Kawahara. 2011. An unsupervised model for joint
phrase alignment and extraction. In Proceedings of the
49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies,
pages 632–641, Portland, Oregon, USA, June. Associ-
ation for Computational Linguistics.
D. Newman, A. Asuncion, P. Smyth, and M. Welling.
2007. Distributed inference for latent dirichlet allo-
cation. In NIPS. MIT Press.
F. J. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1):19–51, March.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2001.
Bleu: a method for automatic evaluation of machine
translation. In ACL ’02: Proceedings of the 40th
Annual Meeting on Association for Computational
Linguistics, pages 311–318, Morristown, NJ, USA.
Association for Computational Linguistics.
J. Pitman and M. Yor. 1997. The two-parameter Poisson-
Dirichlet distribution derived from a stable subordina-
tor. Ann. Probab., 25:855–900.
Y. W. Teh. 2006. A hierarchical Bayesian language
model based on Pitman-Yor processes. In Proceed-
ings of the 21st International Conference on Com-
putational Linguistics and 44th Annual Meeting of
the Association for Computational Linguistics, pages
985–992.
S. Vogel, H. Ney, and C. Tillmann. 1996. HMM-based
word alignment in statistical translation. In Proceed-
ings of the 16th conference on Computational linguis-
tics, pages 836–841, Morristown, NJ, USA. Associa-
tion for Computational Linguistics.
F. Wood, J. Gasthaus, C. Archambeau, L. James, and
Y. W. Teh. 2011. The sequence memoizer. Commu-
nications of the Association for Computing Machines,
54(2):91–98.
D. Wu. 1997. Stochastic inversion transduction gram-
mars and bilingual parsing of parallel corpora. Com-
putational Linguistics, 23:377–403, September.
H. Zhang, C. Quirk, R. C. Moore, and D. Gildea. 2008.
Bayesian learning of non-compositional phrases with
synchronous parsing. In Proceedings of ACL-08:
HLT, pages 97–105, Columbus, Ohio, June. Associ-
ation for Computational Linguistics.
</reference>
<page confidence="0.75843">
232
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.936697">
<title confidence="0.999358">A Bayesian Model for Learning SCFGs with Discontiguous Rules</title>
<author confidence="0.999935">Abby Levenberg Chris Dyer Phil Blunsom</author>
<affiliation confidence="0.9994515">Dept. of Computer Science School of Computer Science Dept. of Computer Science University of Oxford Carnegie Mellon Univeristy University of</affiliation>
<email confidence="0.956343">ablev@cs.ox.ac.ukcdyer@cs.cmu.edupblunsom@cs.ox.ac.uk</email>
<abstract confidence="0.9985508125">We describe a nonparametric model and corresponding inference algorithm for learning Synchronous Context Free Grammar derivations for parallel text. The model employs a Pitman-Yor Process prior which uses a novel base distribution over synchronous grammar rules. Through both synthetic grammar induction and statistical machine translation experiments, we show that our model learns complex translational correspondences— including discontiguous, many-to-many alignments—and produces competitive translation results. inference is efficient and we present results on significantly larger corpora than prior work.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>EPRSRC grant no</author>
</authors>
<title>Army Research Office under contract/grant no.</title>
<booktitle>EP/I010858/1 (Levenberg and Blunsom), the U. S. Army Research Laboratory</booktitle>
<pages>911--10</pages>
<location>(Dyer).</location>
<marker>no, </marker>
<rawString>Inc. and EPRSRC grant no. EP/I010858/1 (Levenberg and Blunsom), the U. S. Army Research Laboratory and U. S. Army Research Office under contract/grant no. W911NF-10-1-0533 (Dyer).</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Blunsom</author>
<author>T Cohn</author>
</authors>
<title>A hierarchical pitmanyor process hmm for unsupervised part of speech induction.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>865--874</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="6905" citStr="Blunsom and Cohn, 2011" startWordPosition="1023" endWordPosition="1026">guous phrasal translation units. Surprisingly, the freedom that this extra power affords allows the Gibbs sampler we propose to mix more quickly, allowing state-of-the-art results from a simple initialiser. 3 Model We use a nonparametric generative model based on the 2-parameter Pitman-Yor process (PYP) (Pitman and Yor, 1997), a generalisation of the Dirichlet Process, which has been used for various NLP modeling tasks with state-of-the-art results such as language modeling, word segmentation, text compression and part of speech induction (Teh, 2006; Goldwater et al., 2006; Wood et al., 2011; Blunsom and Cohn, 2011). In this section we first provide a brief definition of the SCFG formalism and then describe our PYP prior for them. 3.1 Synchronous Context-Free Grammar An synchronous context-free grammar (SCFG) is a 5-tuple hE, A, V, S, Ri that generalises context-free grammar to generate strings concurrently in two languages (Lewis and Stearns, 1968). E is a finite set of source language terminal symbols, A is a finite set of target language terminal symbols, V is a set of nonterminal symbols, with a designated start symbol S, and R is a set of synchronous rewrite rules. A string pair is generated by star</context>
</contexts>
<marker>Blunsom, Cohn, 2011</marker>
<rawString>P. Blunsom and T. Cohn. 2011. A hierarchical pitmanyor process hmm for unsupervised part of speech induction. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 865–874, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Blunsom</author>
<author>T Cohn</author>
<author>C Dyer</author>
<author>M Osborne</author>
</authors>
<title>A gibbs sampler for phrasal synchronous grammar induction.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>782--790</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Suntec, Singapore,</location>
<contexts>
<context position="2786" citStr="Blunsom et al., 2009" startWordPosition="396" endWordPosition="399">by capturing non-compositional and idiomatic translations; 2) align discontiguous phrases in both the source and target languages; 3) have no restrictions on the length of a rule, the number of nonterminal symbols per rule, or their configuration. Learning synchronous grammars is hard due to the high polynomial complexity of dynamic programming and the exponential space of possible rules. As such most prior work for learning SCFGs has relied on inference algorithms that were heuristically constrained or biased by word-based alignment models and small experiments (Wu, 1997; Zhang et al., 2008; Blunsom et al., 2009; Neubig et al., 2011). In contrast to these previous attempts, our SCFG model scales to large datasets (over 1.3M sentence pairs) without imposing restrictions on the form of the grammar rules or otherwise constraining the set of learnable rules (e.g., with a word alignment). We validate our sampler by demonstrating its ability to recover grammars used to generate synthetic datasets. We then evaluate our model by inducing word alignments for SMT experiments in several typologically diverse language pairs and across a range of corpora sizes. Our results attest to our model’s ability to learn s</context>
<context position="5054" citStr="Blunsom et al., 2009" startWordPosition="739" endWordPosition="742">biasing the model towards learning minimal translation units, and integrating out the parameters such that a much smaller set of statistics would suffice for inference with a Gibbs sampler. However this work fell short by not evaluating the model independently, instead only presenting results in which it was combined with a standard word-alignment initialisation, thus leaving open the question of its efficacy. The fact that flat phrasal models lack a structured approach to reordering has led many researchers to pursue SCFG induction instead (Wu, 1997; Cherry and Lin, 2007; Zhang et al., 2008; Blunsom et al., 2009). The asymptotic time complexity of the inside algorithm for even the simplest SCFG models is O(|s|3|t|3), too high to be practical for most real translation data. A popular solution to this problem is to heuristically restrict inference to derivations which agree with an independent alignment model (Cherry and Lin, 2007; Zhang et al., 2008). However this may have the unintended effect of biasing the model back towards the initial alignments that they attempt to improve upon. More recently Neubig et al. (2011) reported a novel Bayesian model for phrasal alignment and extraction that was able t</context>
<context position="24213" citStr="Blunsom et al., 2009" startWordPosition="3984" endWordPosition="3987">MM model (Vogel et al., 1996). For decoding we used the word alignments implied by the derivations in the final sample to extract a Hiero grammar with the same standard set of relative frequency, length, and language model features used for the baseline. Weak Initialisation Our first translation experiments ascertain the degree to which our proposed Gibbs sampling inference algorithm is able to learn good synchronous derivations for the PYP-SCFG model. A number of prior works on alignment with Gibbs samplers have only evaluated models initialised with the more complex GIZA++ alignment models (Blunsom et al., 2009; DeNero et al., 2008), as a result it can be difficult to separate the performance of the sampler from that of the initialisation. In order to do this, we initialise the sampler LANGUAGE PAIR PYP-SCFG MODEL 1 INIT. HMM INIT. UR-EN 1.93/2.08 1.45/1.58 ZH-EN 3.47/4.28 1.69/2.37 DE-EN 4.05/4.77 1.50/2.04 Table 4: Average source/target rule lengths in the PYP-SCFG models after the 500th sample for the different initialisations. using just the MODEL 1 distribution used in the PYP-SCFG model’s base distribution. We denote this a weak initialisation as no alignment models outside of those included i</context>
</contexts>
<marker>Blunsom, Cohn, Dyer, Osborne, 2009</marker>
<rawString>P. Blunsom, T. Cohn, C. Dyer, and M. Osborne. 2009. A gibbs sampler for phrasal synchronous grammar induction. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 782–790, Suntec, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>V J D Pietra</author>
<author>R L Mercer</author>
<author>S A D Pietra</author>
<author>J C Lai</author>
</authors>
<title>An estimate of an upper bound for the entropy of english.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<volume>18</volume>
<issue>1</issue>
<contexts>
<context position="994" citStr="Brown et al. (1992)" startWordPosition="125" endWordPosition="128">algorithm for learning Synchronous Context Free Grammar derivations for parallel text. The model employs a Pitman-Yor Process prior which uses a novel base distribution over synchronous grammar rules. Through both synthetic grammar induction and statistical machine translation experiments, we show that our model learns complex translational correspondences— including discontiguous, many-to-many alignments—and produces competitive translation results. Further, inference is efficient and we present results on significantly larger corpora than prior work. 1 Introduction In the twenty years since Brown et al. (1992) pioneered the first word-based statistical machine translation (SMT) models substantially more expressive models of translational equivalence have been developed. The prevalence of complex phrasal, discontiguous, and non-monotonic translation phenomena in real-world applications of machine translation has driven the development of hierarchical and syntactic models based on synchronous context-free grammars (SCFGs). Such models are now widely used in translation and represent the state-of-the-art in most language pairs (Galley et al., 2004; Chiang, 2007). However, while the models used for tra</context>
</contexts>
<marker>Brown, Pietra, Mercer, Pietra, Lai, 1992</marker>
<rawString>P. F. Brown, V. J. D. Pietra, R. L. Mercer, S. A. D. Pietra, and J. C. Lai. 1992. An estimate of an upper bound for the entropy of english. Computational Linguistics, 18(1):31–40.</rawString>
</citation>
<citation valid="false">
<authors>
<author>P F Brown</author>
<author>V J D Pietra</author>
<author>S A D Pietra</author>
<author>R L</author>
</authors>
<marker>Brown, Pietra, Pietra, L, </marker>
<rawString>P. F. Brown, V. J. D. Pietra, S. A. D. Pietra, and R. L.</rawString>
</citation>
<citation valid="true">
<date></date>
<booktitle>In Proceedings of SSST, NAACL-HLT 2007 / AMTA Workshop on Syntax and Structure in Statistical Translation,</booktitle>
<pages>17--24</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Rochester, New York,</location>
<marker></marker>
<rawString>In Proceedings of SSST, NAACL-HLT 2007 / AMTA Workshop on Syntax and Structure in Statistical Translation, pages 17–24, Rochester, New York, April. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="1554" citStr="Chiang, 2007" startWordPosition="205" endWordPosition="206">duction In the twenty years since Brown et al. (1992) pioneered the first word-based statistical machine translation (SMT) models substantially more expressive models of translational equivalence have been developed. The prevalence of complex phrasal, discontiguous, and non-monotonic translation phenomena in real-world applications of machine translation has driven the development of hierarchical and syntactic models based on synchronous context-free grammars (SCFGs). Such models are now widely used in translation and represent the state-of-the-art in most language pairs (Galley et al., 2004; Chiang, 2007). However, while the models used for translation have evolved, the way in which they are learnt has not: naive word-based models are still used to infer translational correspondences from parallel corpora. In this work we bring the learning of the minimal units of translation in step with the representational power of modern translation models. We present a nonparametric Bayesian model of translation based on SCFGs, and we use its posterior distribution to infer synchronous derivations for a parallel corpus using a novel Gibbs sampler. Our model is able to: 1) directly model many-to-many align</context>
<context position="8425" citStr="Chiang, 2007" startWordPosition="1294" endWordPosition="1295"> are examples:1 VP → h schlage NP1 NP2 vor |suggest NP2 to NP1 i NP → h die Kommission |the commission i 1The nonterminal alignment a is indicated through subscripts on the nonterminals. 224 In a probabilistic SCFG, rules are associated with probabilities such that the probabilities of all rewrites of a particular LHS category sum to 1. Translation with SCFGs is carried out by parsing the source language with the monolingual source language projection of the grammar (using standard monolingual parsing algorithms), which induces a parallel tree structure and translation in the target language (Chiang, 2007). Alignment or synchronous parsing is the process of concurrently parsing both the source and target sentences, uncovering the derivation or derivations that give rise to a string pair (Wu, 1997; Dyer, 2010). Our goal is to infer the most probable SCFG derivations that explain a corpus of parallel sentences, given a nonparametric prior over probabilistic SCFGs. In this work we will consider grammars with a single nonterminal category X. 3.2 Pitman-Yor Process SCFG Before training we have no way of knowing how many rules will be needed in our grammar to adequately represent the data. By using t</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>D. Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2):201–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J DeNero</author>
<author>A Bouchard-Cˆot´e</author>
<author>D Klein</author>
</authors>
<title>Sampling alignment structure under a Bayesian translation model.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>314--323</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Honolulu, Hawaii,</location>
<marker>DeNero, Bouchard-Cˆot´e, Klein, 2008</marker>
<rawString>J. DeNero, A. Bouchard-Cˆot´e, and D. Klein. 2008. Sampling alignment structure under a Bayesian translation model. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 314–323, Honolulu, Hawaii, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Dyer</author>
<author>A Lopez</author>
<author>J Ganitkevitch</author>
<author>J Weese</author>
<author>F Ture</author>
<author>P Blunsom</author>
<author>H Setiawan</author>
<author>V Eidelman</author>
<author>P Resnik</author>
</authors>
<title>cdec: A decoder, alignment, and learning framework for finite-state and context-free translation models.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACL 2010 System Demonstrations, ACLDemos ’10,</booktitle>
<pages>7--12</pages>
<contexts>
<context position="22063" citStr="Dyer et al., 2010" startWordPosition="3643" endWordPosition="3646">is of a medium scale and comes from the FBIS corpus. The DE-EN pair constitutes the largest corpus and is taken from Europarl, the proceedings of the European Parliament (Koehn, 2003). Statistics for the data are shown in Table 2. We measure translation quality via the BLEU score (Papineni et al., 2001). All translation systems employ a Hiero translation model during decoding. Baseline word alignments were obtained by running GIZA++ in both directions and symmetrizing using the grow-diag-final-and heuristic (Och and Ney, 2003; Koehn et al., 2003). Decoding was performed with the cdec decoder (Dyer et al., 2010) with the synchronous grammar extracted using the techniques developed by Lopez (2008). All translation systems include a 5-gram language model built from a five hundred million token subset 3http://www.itl.nist.gov/iad/mig/tests/ mt/2009/ 228 LANGUAGE TEST MODEL 4 MODEL 1 PYP-SCFG PAIR SET BASELINE INITIALISATION WEAK M1 INIT. STRONG HMM INIT. UR-EN MT09 23.1 18.5 23.7 24.0 ZH-EN MT03-08 29.4 19.8 28.3 29.8 DE-EN EUROPARL 28.4 25.5 27.8 29.2 Table 3: Results for the SMT experiments in BLEU. The baseline is produced using a full GIZA++ run. The MODEL 1 INITIALISATION column is from the initial</context>
</contexts>
<marker>Dyer, Lopez, Ganitkevitch, Weese, Ture, Blunsom, Setiawan, Eidelman, Resnik, 2010</marker>
<rawString>C. Dyer, A. Lopez, J. Ganitkevitch, J. Weese, F. Ture, P. Blunsom, H. Setiawan, V. Eidelman, and P. Resnik. 2010. cdec: A decoder, alignment, and learning framework for finite-state and context-free translation models. In Proceedings of the ACL 2010 System Demonstrations, ACLDemos ’10, pages 7–12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Dyer</author>
</authors>
<title>Two monolingual parses are better than one (synchronous parse).</title>
<date>2010</date>
<booktitle>In Proc. ofNAACL.</booktitle>
<contexts>
<context position="8632" citStr="Dyer, 2010" startWordPosition="1326" endWordPosition="1327"> SCFG, rules are associated with probabilities such that the probabilities of all rewrites of a particular LHS category sum to 1. Translation with SCFGs is carried out by parsing the source language with the monolingual source language projection of the grammar (using standard monolingual parsing algorithms), which induces a parallel tree structure and translation in the target language (Chiang, 2007). Alignment or synchronous parsing is the process of concurrently parsing both the source and target sentences, uncovering the derivation or derivations that give rise to a string pair (Wu, 1997; Dyer, 2010). Our goal is to infer the most probable SCFG derivations that explain a corpus of parallel sentences, given a nonparametric prior over probabilistic SCFGs. In this work we will consider grammars with a single nonterminal category X. 3.2 Pitman-Yor Process SCFG Before training we have no way of knowing how many rules will be needed in our grammar to adequately represent the data. By using the PitmanYor process as a prior on the parameters of a synchronous grammar we can formulate a model which prefers smaller numbers of rules that are reused often, thereby avoiding degenerate grammars consisti</context>
</contexts>
<marker>Dyer, 2010</marker>
<rawString>C. Dyer. 2010. Two monolingual parses are better than one (synchronous parse). In Proc. ofNAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Galley</author>
<author>M Hopkins</author>
<author>K Knight</author>
<author>D Marcu</author>
</authors>
<title>What’s in a translation rule? In</title>
<date>2004</date>
<booktitle>HLT-NAACL 2004: Main Proceedings,</booktitle>
<volume>2</volume>
<pages>273--280</pages>
<editor>D. M. Susan Dumais and S. Roukos, editors,</editor>
<location>Boston, Massachusetts, USA,</location>
<contexts>
<context position="1539" citStr="Galley et al., 2004" startWordPosition="201" endWordPosition="204">n prior work. 1 Introduction In the twenty years since Brown et al. (1992) pioneered the first word-based statistical machine translation (SMT) models substantially more expressive models of translational equivalence have been developed. The prevalence of complex phrasal, discontiguous, and non-monotonic translation phenomena in real-world applications of machine translation has driven the development of hierarchical and syntactic models based on synchronous context-free grammars (SCFGs). Such models are now widely used in translation and represent the state-of-the-art in most language pairs (Galley et al., 2004; Chiang, 2007). However, while the models used for translation have evolved, the way in which they are learnt has not: naive word-based models are still used to infer translational correspondences from parallel corpora. In this work we bring the learning of the minimal units of translation in step with the representational power of modern translation models. We present a nonparametric Bayesian model of translation based on SCFGs, and we use its posterior distribution to infer synchronous derivations for a parallel corpus using a novel Gibbs sampler. Our model is able to: 1) directly model man</context>
</contexts>
<marker>Galley, Hopkins, Knight, Marcu, 2004</marker>
<rawString>M. Galley, M. Hopkins, K. Knight, and D. Marcu. 2004. What’s in a translation rule? In D. M. Susan Dumais and S. Roukos, editors, HLT-NAACL 2004: Main Proceedings, pages 273–280, Boston, Massachusetts, USA, May 2 - May 7. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Goldwater</author>
<author>T L Griffiths</author>
<author>M Johnson</author>
</authors>
<title>Contextual dependencies in unsupervised word segmentation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Syndney, Australia.</location>
<contexts>
<context position="6861" citStr="Goldwater et al., 2006" startWordPosition="1015" endWordPosition="1018">Our model goes further by allowing discontiguous phrasal translation units. Surprisingly, the freedom that this extra power affords allows the Gibbs sampler we propose to mix more quickly, allowing state-of-the-art results from a simple initialiser. 3 Model We use a nonparametric generative model based on the 2-parameter Pitman-Yor process (PYP) (Pitman and Yor, 1997), a generalisation of the Dirichlet Process, which has been used for various NLP modeling tasks with state-of-the-art results such as language modeling, word segmentation, text compression and part of speech induction (Teh, 2006; Goldwater et al., 2006; Wood et al., 2011; Blunsom and Cohn, 2011). In this section we first provide a brief definition of the SCFG formalism and then describe our PYP prior for them. 3.1 Synchronous Context-Free Grammar An synchronous context-free grammar (SCFG) is a 5-tuple hE, A, V, S, Ri that generalises context-free grammar to generate strings concurrently in two languages (Lewis and Stearns, 1968). E is a finite set of source language terminal symbols, A is a finite set of target language terminal symbols, V is a set of nonterminal symbols, with a designated start symbol S, and R is a set of synchronous rewri</context>
</contexts>
<marker>Goldwater, Griffiths, Johnson, 2006</marker>
<rawString>S. Goldwater, T. L. Griffiths, and M. Johnson. 2006. Contextual dependencies in unsupervised word segmentation. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, Syndney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Graff</author>
</authors>
<date>2003</date>
<booktitle>English Gigaword. Linguistic Data Consortium (LDC-2003T05).</booktitle>
<contexts>
<context position="22892" citStr="Graff, 2003" startWordPosition="3776" endWordPosition="3777">ad/mig/tests/ mt/2009/ 228 LANGUAGE TEST MODEL 4 MODEL 1 PYP-SCFG PAIR SET BASELINE INITIALISATION WEAK M1 INIT. STRONG HMM INIT. UR-EN MT09 23.1 18.5 23.7 24.0 ZH-EN MT03-08 29.4 19.8 28.3 29.8 DE-EN EUROPARL 28.4 25.5 27.8 29.2 Table 3: Results for the SMT experiments in BLEU. The baseline is produced using a full GIZA++ run. The MODEL 1 INITIALISATION column is from the initialisation alignments using MODEL 1 and no sampling. The PYP-SCFG columns show results for the 500th sample for both MODEL 1 and HMM initialisations. of all the English data made available for the NIST 2009 shared task (Graff, 2003). Experimental Setup To obtain the PYP-SCFG word alignments we ran the sampler for five hundred iterations for each of the language pairs and experimental conditions described below. We used the approach of Newman et al. (2007) to distribute the sampler across multiple threads. The strength 0 and discount d hyperparameters of the Pitman-Yor Processes, and the terminal penalty 0 (Section 3.3), were inferred using slice sampling (Neal, 2000). The Gibbs sampler requires an initial set of derivations from which to commence sampling. In our experiments we investigated both weak and a strong initial</context>
</contexts>
<marker>Graff, 2003</marker>
<rawString>D. Graff. 2003. English Gigaword. Linguistic Data Consortium (LDC-2003T05).</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>F J Och</author>
<author>D Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In NAACL ’03: Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology,</booktitle>
<pages>48--54</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="21997" citStr="Koehn et al., 2003" startWordPosition="3632" endWordPosition="3635">ora statistics (in words). translation evaluation.3 The ZH-EN data is of a medium scale and comes from the FBIS corpus. The DE-EN pair constitutes the largest corpus and is taken from Europarl, the proceedings of the European Parliament (Koehn, 2003). Statistics for the data are shown in Table 2. We measure translation quality via the BLEU score (Papineni et al., 2001). All translation systems employ a Hiero translation model during decoding. Baseline word alignments were obtained by running GIZA++ in both directions and symmetrizing using the grow-diag-final-and heuristic (Och and Ney, 2003; Koehn et al., 2003). Decoding was performed with the cdec decoder (Dyer et al., 2010) with the synchronous grammar extracted using the techniques developed by Lopez (2008). All translation systems include a 5-gram language model built from a five hundred million token subset 3http://www.itl.nist.gov/iad/mig/tests/ mt/2009/ 228 LANGUAGE TEST MODEL 4 MODEL 1 PYP-SCFG PAIR SET BASELINE INITIALISATION WEAK M1 INIT. STRONG HMM INIT. UR-EN MT09 23.1 18.5 23.7 24.0 ZH-EN MT03-08 29.4 19.8 28.3 29.8 DE-EN EUROPARL 28.4 25.5 27.8 29.2 Table 3: Results for the SMT experiments in BLEU. The baseline is produced using a full</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical phrase-based translation. In NAACL ’03: Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology, pages 48–54, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
</authors>
<title>Europarl: A multilingual corpus for evaluation of machine translation.</title>
<date>2003</date>
<contexts>
<context position="21628" citStr="Koehn, 2003" startWordPosition="3578" endWordPosition="3579">5 X→ h b d |3 i 0.24 X→ h c d |3 1 i 0.24 X→ h b c d |3 2 i 0.211 X→ h X1 a X2 |X1 X2 1 i 0.012 Table 1: Manually created SCFG used to generate synthetic data, and the five most probable inferred rules by our model. ZH-EN UR-EN DE-EN NIST NIST EUROPARL TRAIN (SRC) 8.6M 1.2M 34M TRAIN (TRG) 9.5M 1.0M 36M DEV (SRC) 22K 18K 26K DEV (TRG) 27K 16K 28K Table 2: Corpora statistics (in words). translation evaluation.3 The ZH-EN data is of a medium scale and comes from the FBIS corpus. The DE-EN pair constitutes the largest corpus and is taken from Europarl, the proceedings of the European Parliament (Koehn, 2003). Statistics for the data are shown in Table 2. We measure translation quality via the BLEU score (Papineni et al., 2001). All translation systems employ a Hiero translation model during decoding. Baseline word alignments were obtained by running GIZA++ in both directions and symmetrizing using the grow-diag-final-and heuristic (Och and Ney, 2003; Koehn et al., 2003). Decoding was performed with the cdec decoder (Dyer et al., 2010) with the synchronous grammar extracted using the techniques developed by Lopez (2008). All translation systems include a 5-gram language model built from a five hun</context>
</contexts>
<marker>Koehn, 2003</marker>
<rawString>P. Koehn. 2003. Europarl: A multilingual corpus for evaluation of machine translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Lee</author>
</authors>
<title>Fast context-free grammar parsing requires fast Boolean matrix multiplication.</title>
<date>2002</date>
<journal>Journal of the ACM,</journal>
<volume>49</volume>
<issue>1</issue>
<contexts>
<context position="15490" citStr="Lee (2002)" startWordPosition="2516" endWordPosition="2517"> bispan variables must be set to 0 otherwise they would violate the strict nesting constraint required for valid SCFG derivations. We discuss below how to exploit this fact to limit the number of binary variables that must be resampled for each sentence. To be valid, a Gibbs sampler must be ergodic and satisfy detailed balance. Ergodicity requires that there is non-zero probability that any state in the sampler be reachable from any other state. Clearly 2Our grid representation is the synchronous generalisation of the well-known correspondence between CFG derivations and Boolean matrices; see Lee (2002) for an overview. PM1_(s, t) + PM1_(s, t) 226 AMNA SUCCESSFUL BE WILL Amna will succeed AMNA SUCCESSFUL BE WILL Amna will succeed {mna kAmyAb hw gy {mna kAmyAb hw gy (a) An example grid representation of a synchronous derivation. The SCFG rules (annotated with their bispans) that correspond to this setting of the grid are: X[0,4,0,3] → h X[0,1,0,1] X[1,4,1,3] |X[0,1,0,1] X[1,4,1,3] i X[0,1,0,1] → h {mna |Amna i X[1,4,1,3] → h kAmyAb hw gy |will succeed i (b) The toggle operator resamples a bispan variable (here, x[1,3,2,3], shown in blue) to determine whether it should be subtracted from the i</context>
</contexts>
<marker>Lee, 2002</marker>
<rawString>L. Lee. 2002. Fast context-free grammar parsing requires fast Boolean matrix multiplication. Journal of the ACM, 49(1):1–15.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P M Lewis</author>
<author>R E Stearns</author>
</authors>
<title>Syntax-directed transduction.</title>
<date>1968</date>
<journal>J. ACM,</journal>
<pages>15--465</pages>
<contexts>
<context position="7245" citStr="Lewis and Stearns, 1968" startWordPosition="1078" endWordPosition="1081">sation of the Dirichlet Process, which has been used for various NLP modeling tasks with state-of-the-art results such as language modeling, word segmentation, text compression and part of speech induction (Teh, 2006; Goldwater et al., 2006; Wood et al., 2011; Blunsom and Cohn, 2011). In this section we first provide a brief definition of the SCFG formalism and then describe our PYP prior for them. 3.1 Synchronous Context-Free Grammar An synchronous context-free grammar (SCFG) is a 5-tuple hE, A, V, S, Ri that generalises context-free grammar to generate strings concurrently in two languages (Lewis and Stearns, 1968). E is a finite set of source language terminal symbols, A is a finite set of target language terminal symbols, V is a set of nonterminal symbols, with a designated start symbol S, and R is a set of synchronous rewrite rules. A string pair is generated by starting with the pair hS1 |S1i and recursively applying rewrite rules of the form X → hs, t, ai where the left hand side (LHS) X is a nonterminal in V , s is a string in (E ∪ V )*, t is a string in (A ∪ V )* and a specifies a one-to-one mapping (bijection) between nonterminal symbols in s and t. The following are examples:1 VP → h schlage NP</context>
</contexts>
<marker>Lewis, Stearns, 1968</marker>
<rawString>P. M. Lewis, II and R. E. Stearns. 1968. Syntax-directed transduction. J. ACM, 15:465–488, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Lopez</author>
</authors>
<title>Machine Translation by Pattern Matching.</title>
<date>2008</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Maryland.</institution>
<contexts>
<context position="22149" citStr="Lopez (2008)" startWordPosition="3657" endWordPosition="3658">corpus and is taken from Europarl, the proceedings of the European Parliament (Koehn, 2003). Statistics for the data are shown in Table 2. We measure translation quality via the BLEU score (Papineni et al., 2001). All translation systems employ a Hiero translation model during decoding. Baseline word alignments were obtained by running GIZA++ in both directions and symmetrizing using the grow-diag-final-and heuristic (Och and Ney, 2003; Koehn et al., 2003). Decoding was performed with the cdec decoder (Dyer et al., 2010) with the synchronous grammar extracted using the techniques developed by Lopez (2008). All translation systems include a 5-gram language model built from a five hundred million token subset 3http://www.itl.nist.gov/iad/mig/tests/ mt/2009/ 228 LANGUAGE TEST MODEL 4 MODEL 1 PYP-SCFG PAIR SET BASELINE INITIALISATION WEAK M1 INIT. STRONG HMM INIT. UR-EN MT09 23.1 18.5 23.7 24.0 ZH-EN MT03-08 29.4 19.8 28.3 29.8 DE-EN EUROPARL 28.4 25.5 27.8 29.2 Table 3: Results for the SMT experiments in BLEU. The baseline is produced using a full GIZA++ run. The MODEL 1 INITIALISATION column is from the initialisation alignments using MODEL 1 and no sampling. The PYP-SCFG columns show results fo</context>
</contexts>
<marker>Lopez, 2008</marker>
<rawString>A. Lopez. 2008. Machine Translation by Pattern Matching. Ph.D. thesis, University of Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Marcu</author>
<author>D Wong</author>
</authors>
<title>A phrase-based,joint probability model for statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>133--139</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics,</institution>
<contexts>
<context position="3865" citStr="Marcu and Wong (2002)" startWordPosition="558" endWordPosition="561">ents in several typologically diverse language pairs and across a range of corpora sizes. Our results attest to our model’s ability to learn synchronous grammars encoding complex translation phenomena. 223 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 223–232, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics 2 Prior Work The goal of directly inducing phrasal translation models from parallel corpora has received a lot of attention in the NLP and SMT literature. Marcu and Wong (2002) presented an ambitious maximum likelihood model and EM inference algorithm for learning phrasal translation representations. The first issue this model faced was a massive parameter space and intractable inference. However a more subtle issue is that likelihood based models of this form suffer from a degenerate solution, resulting in the model learning whole sentences as phrases rather than minimal units of translation. DeNero et al. (2008) recognised this problem and proposed a nonparametric Bayesian prior for contiguous phrases. This had the dual benefits of biasing the model towards learni</context>
</contexts>
<marker>Marcu, Wong, 2002</marker>
<rawString>D. Marcu and D. Wong. 2002. A phrase-based,joint probability model for statistical machine translation. In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing, pages 133– 139. Association for Computational Linguistics, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Neal</author>
</authors>
<title>Slice sampling.</title>
<date>2000</date>
<journal>Annals of Statistics,</journal>
<pages>31--705</pages>
<contexts>
<context position="23335" citStr="Neal, 2000" startWordPosition="3847" endWordPosition="3848">G columns show results for the 500th sample for both MODEL 1 and HMM initialisations. of all the English data made available for the NIST 2009 shared task (Graff, 2003). Experimental Setup To obtain the PYP-SCFG word alignments we ran the sampler for five hundred iterations for each of the language pairs and experimental conditions described below. We used the approach of Newman et al. (2007) to distribute the sampler across multiple threads. The strength 0 and discount d hyperparameters of the Pitman-Yor Processes, and the terminal penalty 0 (Section 3.3), were inferred using slice sampling (Neal, 2000). The Gibbs sampler requires an initial set of derivations from which to commence sampling. In our experiments we investigated both weak and a strong initialisations, the former based on word alignments from IBM Model 1 and the latter on alignments from an HMM model (Vogel et al., 1996). For decoding we used the word alignments implied by the derivations in the final sample to extract a Hiero grammar with the same standard set of relative frequency, length, and language model features used for the baseline. Weak Initialisation Our first translation experiments ascertain the degree to which our</context>
</contexts>
<marker>Neal, 2000</marker>
<rawString>R. Neal. 2000. Slice sampling. Annals of Statistics, 31:705–767.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Neubig</author>
<author>T Watanabe</author>
<author>E Sumita</author>
<author>S Mori</author>
<author>T Kawahara</author>
</authors>
<title>An unsupervised model for joint phrase alignment and extraction.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>632--641</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="2808" citStr="Neubig et al., 2011" startWordPosition="400" endWordPosition="403">sitional and idiomatic translations; 2) align discontiguous phrases in both the source and target languages; 3) have no restrictions on the length of a rule, the number of nonterminal symbols per rule, or their configuration. Learning synchronous grammars is hard due to the high polynomial complexity of dynamic programming and the exponential space of possible rules. As such most prior work for learning SCFGs has relied on inference algorithms that were heuristically constrained or biased by word-based alignment models and small experiments (Wu, 1997; Zhang et al., 2008; Blunsom et al., 2009; Neubig et al., 2011). In contrast to these previous attempts, our SCFG model scales to large datasets (over 1.3M sentence pairs) without imposing restrictions on the form of the grammar rules or otherwise constraining the set of learnable rules (e.g., with a word alignment). We validate our sampler by demonstrating its ability to recover grammars used to generate synthetic datasets. We then evaluate our model by inducing word alignments for SMT experiments in several typologically diverse language pairs and across a range of corpora sizes. Our results attest to our model’s ability to learn synchronous grammars en</context>
<context position="5569" citStr="Neubig et al. (2011)" startWordPosition="821" endWordPosition="824"> pursue SCFG induction instead (Wu, 1997; Cherry and Lin, 2007; Zhang et al., 2008; Blunsom et al., 2009). The asymptotic time complexity of the inside algorithm for even the simplest SCFG models is O(|s|3|t|3), too high to be practical for most real translation data. A popular solution to this problem is to heuristically restrict inference to derivations which agree with an independent alignment model (Cherry and Lin, 2007; Zhang et al., 2008). However this may have the unintended effect of biasing the model back towards the initial alignments that they attempt to improve upon. More recently Neubig et al. (2011) reported a novel Bayesian model for phrasal alignment and extraction that was able to model phrases of multiple granularities via a synchronous Adaptor Grammar. However this model suffered from the common problem of intractable inference and results were presented for a very small number of samples from a heuristically pruned beam, making interpreting the results difficult. Blunsom et al. (2009) presented an approach similar to ours that implemented a Gibbs sampler for a nonparametric Bayesian model of ITG. While that work managed to scale to a non-trivially sized corpus, like other works it </context>
</contexts>
<marker>Neubig, Watanabe, Sumita, Mori, Kawahara, 2011</marker>
<rawString>G. Neubig, T. Watanabe, E. Sumita, S. Mori, and T. Kawahara. 2011. An unsupervised model for joint phrase alignment and extraction. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 632–641, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Newman</author>
<author>A Asuncion</author>
<author>P Smyth</author>
<author>M Welling</author>
</authors>
<title>Distributed inference for latent dirichlet allocation. In NIPS.</title>
<date>2007</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="23119" citStr="Newman et al. (2007)" startWordPosition="3810" endWordPosition="3813"> 27.8 29.2 Table 3: Results for the SMT experiments in BLEU. The baseline is produced using a full GIZA++ run. The MODEL 1 INITIALISATION column is from the initialisation alignments using MODEL 1 and no sampling. The PYP-SCFG columns show results for the 500th sample for both MODEL 1 and HMM initialisations. of all the English data made available for the NIST 2009 shared task (Graff, 2003). Experimental Setup To obtain the PYP-SCFG word alignments we ran the sampler for five hundred iterations for each of the language pairs and experimental conditions described below. We used the approach of Newman et al. (2007) to distribute the sampler across multiple threads. The strength 0 and discount d hyperparameters of the Pitman-Yor Processes, and the terminal penalty 0 (Section 3.3), were inferred using slice sampling (Neal, 2000). The Gibbs sampler requires an initial set of derivations from which to commence sampling. In our experiments we investigated both weak and a strong initialisations, the former based on word alignments from IBM Model 1 and the latter on alignments from an HMM model (Vogel et al., 1996). For decoding we used the word alignments implied by the derivations in the final sample to extr</context>
</contexts>
<marker>Newman, Asuncion, Smyth, Welling, 2007</marker>
<rawString>D. Newman, A. Asuncion, P. Smyth, and M. Welling. 2007. Distributed inference for latent dirichlet allocation. In NIPS. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="21976" citStr="Och and Ney, 2003" startWordPosition="3628" endWordPosition="3631">K 28K Table 2: Corpora statistics (in words). translation evaluation.3 The ZH-EN data is of a medium scale and comes from the FBIS corpus. The DE-EN pair constitutes the largest corpus and is taken from Europarl, the proceedings of the European Parliament (Koehn, 2003). Statistics for the data are shown in Table 2. We measure translation quality via the BLEU score (Papineni et al., 2001). All translation systems employ a Hiero translation model during decoding. Baseline word alignments were obtained by running GIZA++ in both directions and symmetrizing using the grow-diag-final-and heuristic (Och and Ney, 2003; Koehn et al., 2003). Decoding was performed with the cdec decoder (Dyer et al., 2010) with the synchronous grammar extracted using the techniques developed by Lopez (2008). All translation systems include a 5-gram language model built from a five hundred million token subset 3http://www.itl.nist.gov/iad/mig/tests/ mt/2009/ 228 LANGUAGE TEST MODEL 4 MODEL 1 PYP-SCFG PAIR SET BASELINE INITIALISATION WEAK M1 INIT. STRONG HMM INIT. UR-EN MT09 23.1 18.5 23.7 24.0 ZH-EN MT03-08 29.4 19.8 28.3 29.8 DE-EN EUROPARL 28.4 25.5 27.8 29.2 Table 3: Results for the SMT experiments in BLEU. The baseline is </context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>F. J. Och and H. Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W-J Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2001</date>
<booktitle>In ACL ’02: Proceedings of the 40th Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>311--318</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="21749" citStr="Papineni et al., 2001" startWordPosition="3597" endWordPosition="3600">ly created SCFG used to generate synthetic data, and the five most probable inferred rules by our model. ZH-EN UR-EN DE-EN NIST NIST EUROPARL TRAIN (SRC) 8.6M 1.2M 34M TRAIN (TRG) 9.5M 1.0M 36M DEV (SRC) 22K 18K 26K DEV (TRG) 27K 16K 28K Table 2: Corpora statistics (in words). translation evaluation.3 The ZH-EN data is of a medium scale and comes from the FBIS corpus. The DE-EN pair constitutes the largest corpus and is taken from Europarl, the proceedings of the European Parliament (Koehn, 2003). Statistics for the data are shown in Table 2. We measure translation quality via the BLEU score (Papineni et al., 2001). All translation systems employ a Hiero translation model during decoding. Baseline word alignments were obtained by running GIZA++ in both directions and symmetrizing using the grow-diag-final-and heuristic (Och and Ney, 2003; Koehn et al., 2003). Decoding was performed with the cdec decoder (Dyer et al., 2010) with the synchronous grammar extracted using the techniques developed by Lopez (2008). All translation systems include a 5-gram language model built from a five hundred million token subset 3http://www.itl.nist.gov/iad/mig/tests/ mt/2009/ 228 LANGUAGE TEST MODEL 4 MODEL 1 PYP-SCFG PAI</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2001</marker>
<rawString>K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2001. Bleu: a method for automatic evaluation of machine translation. In ACL ’02: Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, pages 311–318, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Pitman</author>
<author>M Yor</author>
</authors>
<title>The two-parameter PoissonDirichlet distribution derived from a stable subordinator.</title>
<date>1997</date>
<journal>Ann. Probab.,</journal>
<pages>25--855</pages>
<contexts>
<context position="6609" citStr="Pitman and Yor, 1997" startWordPosition="977" endWordPosition="980">ch similar to ours that implemented a Gibbs sampler for a nonparametric Bayesian model of ITG. While that work managed to scale to a non-trivially sized corpus, like other works it relied on a state-of-the-art word alignment model for initialisation. Our model goes further by allowing discontiguous phrasal translation units. Surprisingly, the freedom that this extra power affords allows the Gibbs sampler we propose to mix more quickly, allowing state-of-the-art results from a simple initialiser. 3 Model We use a nonparametric generative model based on the 2-parameter Pitman-Yor process (PYP) (Pitman and Yor, 1997), a generalisation of the Dirichlet Process, which has been used for various NLP modeling tasks with state-of-the-art results such as language modeling, word segmentation, text compression and part of speech induction (Teh, 2006; Goldwater et al., 2006; Wood et al., 2011; Blunsom and Cohn, 2011). In this section we first provide a brief definition of the SCFG formalism and then describe our PYP prior for them. 3.1 Synchronous Context-Free Grammar An synchronous context-free grammar (SCFG) is a 5-tuple hE, A, V, S, Ri that generalises context-free grammar to generate strings concurrently in two</context>
</contexts>
<marker>Pitman, Yor, 1997</marker>
<rawString>J. Pitman and M. Yor. 1997. The two-parameter PoissonDirichlet distribution derived from a stable subordinator. Ann. Probab., 25:855–900.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y W Teh</author>
</authors>
<title>A hierarchical Bayesian language model based on Pitman-Yor processes.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>985--992</pages>
<contexts>
<context position="6837" citStr="Teh, 2006" startWordPosition="1013" endWordPosition="1014">alisation. Our model goes further by allowing discontiguous phrasal translation units. Surprisingly, the freedom that this extra power affords allows the Gibbs sampler we propose to mix more quickly, allowing state-of-the-art results from a simple initialiser. 3 Model We use a nonparametric generative model based on the 2-parameter Pitman-Yor process (PYP) (Pitman and Yor, 1997), a generalisation of the Dirichlet Process, which has been used for various NLP modeling tasks with state-of-the-art results such as language modeling, word segmentation, text compression and part of speech induction (Teh, 2006; Goldwater et al., 2006; Wood et al., 2011; Blunsom and Cohn, 2011). In this section we first provide a brief definition of the SCFG formalism and then describe our PYP prior for them. 3.1 Synchronous Context-Free Grammar An synchronous context-free grammar (SCFG) is a 5-tuple hE, A, V, S, Ri that generalises context-free grammar to generate strings concurrently in two languages (Lewis and Stearns, 1968). E is a finite set of source language terminal symbols, A is a finite set of target language terminal symbols, V is a set of nonterminal symbols, with a designated start symbol S, and R is a </context>
<context position="10320" citStr="Teh (2006)" startWordPosition="1614" endWordPosition="1615"> compared to infrequent ones. The strength parameter controls the variance, or concentration, about the base distribution. In our model, a draw from a PYP is a distribution over SCFG rules with a particular LHS (in fact, it is a distribution over all well-formed rules). From this distribution we can in turn draw individual rules: !9X — PY(d, B, !90), X —* (s, t, a) — !9X. Although the PYP has no known analytical form, we can marginalise out the !9X’s and reason about Figure 1: Example generation of a synchronous grammar rule in our !90. individual rules directly using the process described by Teh (2006). In this process, at time n a rule rn is generated by stochastically deciding whether to make another copy of a previously generated rule or to draw a new one from the base distribution, !90. Let V = (cp1, cp2,...) be the sequence of draws from !90; thus V is the total number of draws from !90. A rule rn corresponds to a selection of a cpk. Let ck be a counter indicating the number of times cpk has been selected. In particular, we set rn to cpk with probability ck — d B + n , and increment ck, or with probability B + d · V B + n , we draw a new rule from !90, append it to V, and use it for rn</context>
</contexts>
<marker>Teh, 2006</marker>
<rawString>Y. W. Teh. 2006. A hierarchical Bayesian language model based on Pitman-Yor processes. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 985–992.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Vogel</author>
<author>H Ney</author>
<author>C Tillmann</author>
</authors>
<title>HMM-based word alignment in statistical translation.</title>
<date>1996</date>
<booktitle>In Proceedings of the 16th conference on Computational linguistics,</booktitle>
<pages>836--841</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="23622" citStr="Vogel et al., 1996" startWordPosition="3893" endWordPosition="3896">each of the language pairs and experimental conditions described below. We used the approach of Newman et al. (2007) to distribute the sampler across multiple threads. The strength 0 and discount d hyperparameters of the Pitman-Yor Processes, and the terminal penalty 0 (Section 3.3), were inferred using slice sampling (Neal, 2000). The Gibbs sampler requires an initial set of derivations from which to commence sampling. In our experiments we investigated both weak and a strong initialisations, the former based on word alignments from IBM Model 1 and the latter on alignments from an HMM model (Vogel et al., 1996). For decoding we used the word alignments implied by the derivations in the final sample to extract a Hiero grammar with the same standard set of relative frequency, length, and language model features used for the baseline. Weak Initialisation Our first translation experiments ascertain the degree to which our proposed Gibbs sampling inference algorithm is able to learn good synchronous derivations for the PYP-SCFG model. A number of prior works on alignment with Gibbs samplers have only evaluated models initialised with the more complex GIZA++ alignment models (Blunsom et al., 2009; DeNero </context>
<context position="25957" citStr="Vogel et al., 1996" startWordPosition="4263" endWordPosition="4266">ow BLEU scores. In contrast, the models built from the output of the Gibbs sampler for the PYP-SCFG model achieve BLEU scores comparable to those of the MODEL 4 BASELINE. Thus the sampler has moved a good distance from its initialisation, and done so in a direction that results in better synchronous derivations. Strong Initialisation Given we have established that the sampler can produce state-of-the-art translation results from a 229 unique grammar rules in PYP weak initialisation, it is instructive to investigate whether initialising the model with a strong alignment system, the GIZA++ HMM (Vogel et al., 1996), leads to further improvements. Column HMM INIT. of Table 3 shows the results for initialising with the HMM word alignments and sampling for 500 iterations. Starting with a stronger initial sample results in both quicker mixing and better translation quality for the same number of sampling iterations. Table 4 compares the average lengths of the rules produced by the sampler with both the strong and weak initialisers. As the size of the training corpora increases (UR-EN → ZH-EN → DE-EN) we see that the average size of the rules produced by the weakly initialised sampler also increases, while t</context>
</contexts>
<marker>Vogel, Ney, Tillmann, 1996</marker>
<rawString>S. Vogel, H. Ney, and C. Tillmann. 1996. HMM-based word alignment in statistical translation. In Proceedings of the 16th conference on Computational linguistics, pages 836–841, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Wood</author>
<author>J Gasthaus</author>
<author>C Archambeau</author>
<author>L James</author>
<author>Y W Teh</author>
</authors>
<title>The sequence memoizer.</title>
<date>2011</date>
<journal>Communications of the Association for Computing Machines,</journal>
<volume>54</volume>
<issue>2</issue>
<contexts>
<context position="6880" citStr="Wood et al., 2011" startWordPosition="1019" endWordPosition="1022">y allowing discontiguous phrasal translation units. Surprisingly, the freedom that this extra power affords allows the Gibbs sampler we propose to mix more quickly, allowing state-of-the-art results from a simple initialiser. 3 Model We use a nonparametric generative model based on the 2-parameter Pitman-Yor process (PYP) (Pitman and Yor, 1997), a generalisation of the Dirichlet Process, which has been used for various NLP modeling tasks with state-of-the-art results such as language modeling, word segmentation, text compression and part of speech induction (Teh, 2006; Goldwater et al., 2006; Wood et al., 2011; Blunsom and Cohn, 2011). In this section we first provide a brief definition of the SCFG formalism and then describe our PYP prior for them. 3.1 Synchronous Context-Free Grammar An synchronous context-free grammar (SCFG) is a 5-tuple hE, A, V, S, Ri that generalises context-free grammar to generate strings concurrently in two languages (Lewis and Stearns, 1968). E is a finite set of source language terminal symbols, A is a finite set of target language terminal symbols, V is a set of nonterminal symbols, with a designated start symbol S, and R is a set of synchronous rewrite rules. A string </context>
</contexts>
<marker>Wood, Gasthaus, Archambeau, James, Teh, 2011</marker>
<rawString>F. Wood, J. Gasthaus, C. Archambeau, L. James, and Y. W. Teh. 2011. The sequence memoizer. Communications of the Association for Computing Machines, 54(2):91–98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Wu</author>
</authors>
<title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<pages>23--377</pages>
<contexts>
<context position="2744" citStr="Wu, 1997" startWordPosition="390" endWordPosition="391">many-to-many alignments, thereby capturing non-compositional and idiomatic translations; 2) align discontiguous phrases in both the source and target languages; 3) have no restrictions on the length of a rule, the number of nonterminal symbols per rule, or their configuration. Learning synchronous grammars is hard due to the high polynomial complexity of dynamic programming and the exponential space of possible rules. As such most prior work for learning SCFGs has relied on inference algorithms that were heuristically constrained or biased by word-based alignment models and small experiments (Wu, 1997; Zhang et al., 2008; Blunsom et al., 2009; Neubig et al., 2011). In contrast to these previous attempts, our SCFG model scales to large datasets (over 1.3M sentence pairs) without imposing restrictions on the form of the grammar rules or otherwise constraining the set of learnable rules (e.g., with a word alignment). We validate our sampler by demonstrating its ability to recover grammars used to generate synthetic datasets. We then evaluate our model by inducing word alignments for SMT experiments in several typologically diverse language pairs and across a range of corpora sizes. Our result</context>
<context position="4989" citStr="Wu, 1997" startWordPosition="729" endWordPosition="730">r contiguous phrases. This had the dual benefits of biasing the model towards learning minimal translation units, and integrating out the parameters such that a much smaller set of statistics would suffice for inference with a Gibbs sampler. However this work fell short by not evaluating the model independently, instead only presenting results in which it was combined with a standard word-alignment initialisation, thus leaving open the question of its efficacy. The fact that flat phrasal models lack a structured approach to reordering has led many researchers to pursue SCFG induction instead (Wu, 1997; Cherry and Lin, 2007; Zhang et al., 2008; Blunsom et al., 2009). The asymptotic time complexity of the inside algorithm for even the simplest SCFG models is O(|s|3|t|3), too high to be practical for most real translation data. A popular solution to this problem is to heuristically restrict inference to derivations which agree with an independent alignment model (Cherry and Lin, 2007; Zhang et al., 2008). However this may have the unintended effect of biasing the model back towards the initial alignments that they attempt to improve upon. More recently Neubig et al. (2011) reported a novel Ba</context>
<context position="8619" citStr="Wu, 1997" startWordPosition="1324" endWordPosition="1325">babilistic SCFG, rules are associated with probabilities such that the probabilities of all rewrites of a particular LHS category sum to 1. Translation with SCFGs is carried out by parsing the source language with the monolingual source language projection of the grammar (using standard monolingual parsing algorithms), which induces a parallel tree structure and translation in the target language (Chiang, 2007). Alignment or synchronous parsing is the process of concurrently parsing both the source and target sentences, uncovering the derivation or derivations that give rise to a string pair (Wu, 1997; Dyer, 2010). Our goal is to infer the most probable SCFG derivations that explain a corpus of parallel sentences, given a nonparametric prior over probabilistic SCFGs. In this work we will consider grammars with a single nonterminal category X. 3.2 Pitman-Yor Process SCFG Before training we have no way of knowing how many rules will be needed in our grammar to adequately represent the data. By using the PitmanYor process as a prior on the parameters of a synchronous grammar we can formulate a model which prefers smaller numbers of rules that are reused often, thereby avoiding degenerate gram</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>D. Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational Linguistics, 23:377–403, September.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
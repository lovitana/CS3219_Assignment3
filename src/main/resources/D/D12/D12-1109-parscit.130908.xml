<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.988804">
Left-to-Right Tree-to-String Decoding with Prediction
</title>
<author confidence="0.998004">
Yang Fengt Yang Liu$ Qun Liu* Trevor Cohnt
</author>
<affiliation confidence="0.9941235">
t Department of Computer Science
The University of Sheffield, Sheffield, UK
</affiliation>
<email confidence="0.858886">
{y.feng, t.cohn}@sheffield.ac.uk
</email>
<affiliation confidence="0.859659714285714">
t State Key Laboratory on Intelligent Technology and Systems
Tsinghua National Laboratory for Information Science and Technology
Department of Computer Sci. and Tech., Tsinghua University, Beijing, China
liuyang2011@tsinghua.edu.cn
*Key Laboratory of Intelligent Information Processing
Institute of Computing Technology
Chinese Academy of Sciences, Beijing, China
</affiliation>
<email confidence="0.982641">
liuqun@ict.ac.cn
</email>
<sectionHeader confidence="0.995481" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999165933333333">
Decoding algorithms for syntax based ma-
chine translation suffer from high compu-
tational complexity, a consequence of in-
tersecting a language model with a con-
text free grammar. Left-to-right decoding,
which generates the target string in order,
can improve decoding efficiency by simpli-
fying the language model evaluation. This
paper presents a novel left to right decod-
ing algorithm for tree-to-string translation, us-
ing a bottom-up parsing strategy and dynamic
future cost estimation for each partial trans-
lation. Our method outperforms previously
published tree-to-string decoders, including a
competing left-to-right method.
</bodyText>
<sectionHeader confidence="0.999136" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999994625">
In recent years there has been rapid progress in the
development of tree-to-string models for statistical
machine translation. These models use the syntac-
tic parse tree of the source language to inform its
translation, which allows the models to capture con-
sistent syntactic transformations between the source
and target languages, e.g., from subject-verb-object
to subject-object-verb word orderings. Decoding al-
gorithms for grammar-based translation seek to find
the best string in the intersection between a weighted
context free grammar (the translation mode, given a
source string/tree) and a weighted finite state accep-
tor (an n-gram language model). This intersection
is problematic, as it results in an intractably large
grammar, and makes exact search impossible.
Most researchers have resorted to approximate
search, typically beam search (Chiang, 2007). The
decoder parses the source sentence, recording the
target translations for each span.&apos; As the partial
translation hypothesis grows, its component ngrams
are scored and the hypothesis score is updated. This
decoding method though is inefficient as it requires
recording the language model context (n −1 words)
on the left and right edges of each chart cell. These
contexts allow for boundary ngrams to be evaluated
when the cell is used in another grammar produc-
tion. In contrast, if the target string is generated
in left-to-right order, then only one language model
context is required, and the problem of language
model evaluation is vastly simplified.
In this paper, we develop a novel method of left-
to-right decoding for tree-to-string translation using
a shift-reduce parsing strategy. A central issue in
any decoding algorithm is the technique used for
pruning the search space. Our left-to-right decod-
ing algorithm groups hypotheses, which cover the
same number of source words, into a bin. Pruning
requires the evaluation of different hypotheses in the
same bin, and elimating the least promising options.
As each hypotheses may cover different sets of tree
</bodyText>
<footnote confidence="0.82959375">
&apos;The process is analogous for tree-to-string models, except
that only rules and spans matching those in the source trees are
considered. Typically nodes are visited according to a post-
order traversal.
</footnote>
<page confidence="0.904899">
1191
</page>
<note confidence="0.7975365">
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1191–1200, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.999767666666667">
nodes, it is necessary to consider the cost of uncov-
ered nodes, i.e., the future cost. We show that a good
future cost estimate is essential for accurate and effi-
cient search, leading to high quality translation out-
put.
Other researchers have also considered the left-
to-right decoding algorithm for tree-to-string mod-
els. Huang and Mi (2010) developed an Earley-
style parsing algorithm (Earley, 1970). In their ap-
proach, hypotheses covering the same number of
tree nodes were binned together. Their method uses
a top-down depth-first search, with a mechanism for
early elimation of some rules which lead to dead-
ends in the search. Huang and Mi (2010)’s method
was shown to outperform the traditional post-order-
traversal decoding algorithm, considering fewer hy-
potheses and thus decoding much faster at the same
level of performance. However their algorithm used
a very rough estimate of future cost, resulting in
more search errors than our approach.
Our experiments show that compared with the
Earley-style left-to-right decoding (Huang and Mi,
2010) and the traditional post-order-traversal de-
coding (Liu et al., 2006) algorithms, our algorithm
achieves a significant improvement on search capac-
ity and better translation performance at the same
level of speed.
</bodyText>
<sectionHeader confidence="0.97985" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.9999285">
A typical tree-to-string system (Liu et al., 2006;
Huang et al., 2006) searches through a 1-best source
parse tree for the best derivation. It transduces the
source tree into a target-language string using a Syn-
chronous Tree Substitution Grammar (STSG). The
grammar rules are extracted from bilingual word
alignments using the GHKM algorithm (Galley et
al., 2004).
We will briefly review the traditional decoding al-
gorithm (Liu et al., 2006) and the Earley-style top-
down decoding algorithm (Huang and Mi, 2010) for
the tree-to-string model.
</bodyText>
<subsectionHeader confidence="0.984028">
2.1 Traditional Decoding
</subsectionHeader>
<bodyText confidence="0.9528256">
The traditional decoding algorithm processes source
tree nodes one by one according to a post-order
traversal. For each node, it applies matched STSG
rules by substituting each non-terminal with its cor-
in theory beam search
</bodyText>
<equation confidence="0.77175">
traditional O(nc�|V |4(9−1)) O(ncb2)
top-down O(c(cr)d|V |9−1) O(ncb)
bottom-up O((cr)d|V |9−1) O(nub)
</equation>
<tableCaption confidence="0.953993">
Table 1: Time complexity of different algorithms. tra-
</tableCaption>
<bodyText confidence="0.7127291">
ditional : Liu et al. (2006), top-down : Huang and Mi
(2010). n is the source sentence length, b is the beam
width, c is the number of rules used for each node, V
is the target word vocabulary, g is the order of the lan-
guage model, d is the depth of the source parse tree, u is
the number of viable prefixes for each node and r is the
maximum arity of each rule.
responding translation. For the derivation in Figure
1 (b), the traditional algorithm applies r2 at node
NN2
</bodyText>
<equation confidence="0.984134">
r2 : NN2 (jieguo) → the result,
</equation>
<bodyText confidence="0.709508">
to obtain “the result” as the translation of NN2. Next
it applies r4 at node NP,
</bodyText>
<equation confidence="0.982223">
r4 : NP ( NN1 (toupiao), x1 : NN2 )
→ x1 of the vote
</equation>
<bodyText confidence="0.999744444444444">
and replaces NN2 with its translation “the result”,
then it gets the translation of NP as “the result of the
vote”.
This algorithm needs to contain boundary words
at both left and right extremities of the target string
for the purpose of LM evaluation, which leads to a
high time complexity. The time complexity in the-
ory and with beam search (Huang and Mi, 2010) is
shown in Table 1.
</bodyText>
<subsectionHeader confidence="0.99659">
2.2 Earley-style Top-down Decoding
</subsectionHeader>
<bodyText confidence="0.999939416666667">
The Earley-style decoding algorithm performs a top-
down depth-first parsing and generates the target
translation left to right. It applies Context-Free
Grammar (CFG) rules and employs three actions:
predict, scan and complete (Section 3.1 describes
how to convert STSG rules into CFG rules). We can
simulate its translation process using a stack with a
dot . indicating which symbol to process next. For
the derivation in Figure 1(b) and CFG rules in Fig-
ure 1(c), Figure 2 illustrates the whole translation
process.
The time complexity is shown in Table 1 .
</bodyText>
<page confidence="0.99733">
1192
</page>
<sectionHeader confidence="0.995343" genericHeader="method">
3 Bottom-Up Left-to-Right Decoding
</sectionHeader>
<bodyText confidence="0.999362541666667">
We propose a novel method of left-to-right decoding
for tree-to-string translation using a bottom-up pars-
ing strategy. We use viable prefixes (Aho and John-
son, 1974) to indicate all possible target strings the
translations of each node should starts with. There-
fore, given a tree node to expand, our algorithm
can drop immediately to target terminals no matter
whether there is a gap or not. We say that there is a
gap between two symbols in a derivation when there
are many rules separating them, e.g. IP + ... →
NN2. For the derivation in Figure 1(b), our algo-
rithm starts from the root node IP and applies r2
first although there is a gap between IP and NN2.
Then it applies r4, r5 and r6 in sequence to generate
the translation “the result of the vote was released
at night”. Our algorithm takes the gap as a black-
box and does not need to fix which partial deriva-
tion should be used for the gap at the moment. So it
can get target strings as soon as possible and thereby
perform more accurate pruning. A valid derivation
is generated only when the source tree is completely
matched by rules.
Our bottom-up decoding algorithm involves the
following steps:
</bodyText>
<listItem confidence="0.9939726">
1. Match STSG rules against the source tree.
2. Convert STSG rules to CFG rules.
3. Collect the viable prefix set for each node in a
post-order transversal.
4. Search bottom-up for the best derivation.
</listItem>
<subsectionHeader confidence="0.65284">
3.1 From STSG to CFG
</subsectionHeader>
<bodyText confidence="0.931744666666667">
After rule matching, each tree node has its applica-
ble STSG rule set. Given a matched STSG rule, our
decoding algorithm only needs to consider the tree
node the rule can be applied to and the target side,
so we follow Huang and Mi (2010) to convert STSG
rules to CFG rules. For example, an STSG rule
NP ( NN1 (toupiao), x1 : NN2 ) → x1 of the vote
can be converted to a CFG rule
NP → NN2 of the vote
The target non-terminals are replaced with corre-
sponding source non-terminals. Figure 1 (c) shows
all converted CFG rules for the toy example. Note
</bodyText>
<listItem confidence="0.683875833333333">
(a) Source parse tree
the result of the vote was released at night
(b) A derivation
r1: NN1 → the vote
r2: NN2 → the result
r3: NP → NN2 of NN1
r4: NP → NN2 of the vote
r5: VP → was released at night
r6: IP → NP VP
r7: IP → NN2 of the vote VP
rg: IP → VP NP
(c) Target-side CFG rule set
</listItem>
<figureCaption confidence="0.997433">
Figure 1: A toy example.
</figureCaption>
<bodyText confidence="0.996848666666667">
that different STSG rules might be converted to the
same CFG rule despite having different source tree
structures.
</bodyText>
<subsectionHeader confidence="0.999584">
3.2 Viable Prefix
</subsectionHeader>
<bodyText confidence="0.999977444444444">
During decoding, how do we decide which rules
should be used next given a partial derivation, es-
pecially when there is a gap? A key observation is
that some rules should be excluded. For example,
any derivation for Figure 1(a) will never begin with
r1 as there is no translation starting with “the vote”.
In order to know which rules can be excluded for
each node, we can recursively calculate the start-
ing terminal strings for each node. For example,
</bodyText>
<figure confidence="0.9988036">
IP
NP
VP
NT
VV
NN1
NN2
jieguˇo
t´oup`ıao
g¯ongb`u
wˇansh`ang
NP VP
r6: IP
⇓ ⇓
r4: NP r5: VP
NN1 NN2 NT VV
t´oup`ıao wˇansh`ang g¯ongb`u
⇓
r2: NN2
jieguˇo
</figure>
<page confidence="0.958106">
1193
</page>
<bodyText confidence="0.9860484">
NN1: {the vote} NN2: {the result}
NT: 0 VV: 0
NP: {the result}
VP: {was released at night}
IP: {the result, was released at night}
</bodyText>
<tableCaption confidence="0.979967">
Table 2: The Viable prefix sets for Figure 1 (c)
</tableCaption>
<bodyText confidence="0.95875625">
according to r1, the starting terminal string of the
translation for NN1 is “the vote”. According to r2,
the starting terminal string for NN2 is “the result”.
According to r3, the starting terminal string of NP
must include that of NN2. Table 2 lists the starting
terminal strings of all nodes in Figure 1(a). As the
translations of node IP should begin with either “the
result” or “was released at night”, the first rule must
be either r2 or r5. Therefore, r1 will never be used
as the first rule in any derivation.
We refer to starting terminal strings of a node as
a viable prefixes, a term borrowed from LR pars-
ing (Aho and Johnson, 1974). Viable prefixes are
used to decide which rule should be used to ensure
efficient left-to-right target generation. Formally, as-
sume that VN denotes the set of non-terminals (i.e.,
source tree node labels), VT denotes the set of ter-
minals (i.e., target words), v1, v2 E VN, w E VT ,
7r E {VT U VN}*, we say that w is a viable prefix of
v1 if and only if:
</bodyText>
<listItem confidence="0.999888333333333">
• v1 w, or
• v1 wv27r, or
• v1 v27r, and w is a viable prefix of v2.
</listItem>
<bodyText confidence="0.9829445">
Note that we bundle all successive terminals in one
symbol.
</bodyText>
<subsectionHeader confidence="0.998215">
3.3 Shift-Reduce Parsing
</subsectionHeader>
<bodyText confidence="0.980285185185185">
We use a shift-reduce algorithm to search for the
best deviation. The algorithm maintains a stack of
dotted rules (Earley, 1970). Given the source tree in
Figure 1(a), the stack is initialized with a dotted rule
for the root node IP:
[. IP].
Then, the algorithm selects one viable prefix of IP
and appends it to the stack with the dot at the begin-
ning (predict):
[. IP] [. the result]2.
Then, a scan action is performed to produce a partial
translation “the result”:
[. IP] [the result.].
Next, the algorithm searches for the CFG rules start-
ing with “the result” and gets r2. Then, it pops the
rightmost dotted rule and append the left-hand side
(LHS) of r2 to the stack (complete):
[- IP] [NN2 -.
Next, the algorithm chooses r� whose right-hand
side “NN2 of the vote” matches the rightmost dot-
ted rule in the stack3 and grows the rightmost dotted
rule:
[. IP] [NN2 . of the vote].
Figure 3 shows the whole process of derivation
generation.
Formally, we define four actions on the rightmost
rule in the stack:
</bodyText>
<listItem confidence="0.983523307692308">
• Predict. If the symbol after the dot in the right-
most dotted rule is a non-terminal v, this action
chooses a viable prefix w of v and generates a
new dotted rule for w with the dot at the begin-
ning. For example:
[. IP] predict
� [. IP] [. the result]
• Scan. If the symbol after the dot in the right-
most dotted rule is a terminal string w, this ac-
tion advances the dot to update the current par-
tial translation. For example:
[. IP] [. the result] scan � [. IP] [the result .]
• Complete. If the rightmost dotted rule ends
</listItem>
<bodyText confidence="0.597675625">
with a dot and it happens to be the right-hand
side of a rule, then this action removes the
right-most dotted rule. Besides, if the symbol
after the dot in the new rightmost rule corre-
sponds to the same tree node as the LHS non-
terminal of the rule, this action advance the dot.
For example,
[. IP] [NP . VP] [was released at night.]
</bodyText>
<subsubsectionHeader confidence="0.250157">
complete
</subsubsectionHeader>
<bodyText confidence="0.302623">
� [. IP] [NP VP .]
</bodyText>
<footnote confidence="0.999928">
2There are another option: “was released at night”
3Here there is an alternative: r3 or r7
</footnote>
<page confidence="0.952945">
1194
</page>
<figure confidence="0.756608636363636">
step action rule used stack
0 [. IP]
1 p r6 [. IP] [. NP VP]
2 p r4 [. IP] [. NP VP] [. NN2 of the vote]
3 p r2 [. IP] [. NP VP] [. NN2 of the vote] [. the result]
4 s [. IP] [. NP VP] [. NN2 of the vote] [the result.]
5 c [. IP] [. NP VP] [NN2 . of the vote]
6 s [. IP] [. NP VP] [NN2 of the vote .]
7 c [. IP] [NP . VP]
8 p r5 [. IP] [NP . VP] [. was released at night]
9 s [. IP] [NP . VP] [was released at night.]
10 c [. IP] [NP VP .]
11 c [IP .]
hypothesis
the result
the result
the result of the vote
the result of the vote
the result of the vote
the ... vote was ... night
the ... vote was ... night
the ... vote was ... night
</figure>
<figureCaption confidence="0.996684666666667">
Figure 2: Simulation of top-down translation process for the derivation in Figure 1(b). Actions: p, predict; s, scan; c,
complete. “the ... vote” and “was ... released” are the abbreviated form of “the result of the vote” and “was released at
night”, respectively.
</figureCaption>
<figure confidence="0.989924647058824">
step action rule used stack number
0 [. IP] 0
1 p [. IP] [. the result] 0
2 s [. IP] [the result .] 1
3 c r2 [. IP] [NN2 .] 1
4 g r4 or r7 [. IP] [NN2 . of the vote] 1
5 s [. IP] [NN2 of the vote.] 2
6 c r4 [. IP] [NP .] 2
7 g r6 [. IP] [NP . VP] 2
8 p [. IP] [NP . VP] [. was released at night] 2
9 s [. IP] [NP . VP] [was released at night.] 4
10 c r5 [. IP] [NP VP .] 4
11 c r6 [IP .] 4
hypothesis
the result
the result
the result
</figure>
<figureCaption confidence="0.996673111111111">
the result of the vote
the result of the vote
the result of the vote
the result of the vote
the ... vote was ... night
the ... vote was ... night
the ... vote was ... night
Figure 3: Simulation of bottom-up translation process for the derivation in Figure 1(b). Actions: p, predict; s, scan; c,
complete; g, grow. The column of number gives the number of source words the hypothesis covers.
</figureCaption>
<bodyText confidence="0.82681375">
If the string cannot rewrite on the frontier non-
terminal, then we add the LHS to the stack with
the dot after it. For example:
complete
</bodyText>
<listItem confidence="0.954124166666667">
[. IP] [the result .] ) [. IP] [NN2 .]
• Grow. If the right-most dotted rule ends with
a dot and it happens to be the starting part of
a CFG rule, this action appends one symbol of
the remainder of that rule to the stack 4. For
example:
</listItem>
<footnote confidence="0.7840475">
4We bundle the successive terminals in one rule into a sym-
bol
</footnote>
<bodyText confidence="0.981247769230769">
[. IP] [NN2 .] grow ��[. IP] [NN2 . of the vote]
From the above definition, we can find that there
may be an ambiguity about whether to use a com-
plete action or a grow action. Similarly, predict ac-
tions must select a viable prefix form the set for a
node. For example in step 5, although we select
to perform complete with r4 in the example, r7 is
applicable, too. In our implementation, if both r4
and r7 are applicable, we apply them both to gener-
ate two seperate hypotheses. To limit the exponen-
tial explosion of hypotheses (Knight, 1999), we use
beam search over bins of similar partial hypotheses
(Koehn, 2004).
</bodyText>
<page confidence="0.95908">
1195
</page>
<figure confidence="0.768863">
IP
</figure>
<figureCaption confidence="0.9705765">
Figure 4: The translation forest composed of applicable
CFG rules for the partial derivation of step 3 in Figure 3.
</figureCaption>
<subsectionHeader confidence="0.865155">
3.4 Future Cost
</subsectionHeader>
<bodyText confidence="0.999966166666667">
Partial derivations covering different tree nodes may
be grouped in the same bin for beam pruning5. In
order to perform more accurate pruning, we take into
consideration future cost, the cost of the uncovered
part. The merit of a derivation is the covered cost
(the cost of the covered part) plus the future cost.
We borrow ideas from the Inside-Outside algorithm
(Charniak and Johnson, 2005; Huang, 2008; Mi et
al., 2008) to compute the merit. In our algorithm,
the merit of a derivation is just the Viterbi inside cost
Q of the root node calculated with the derivations
continuing from the current derivation.
Given a partial derivation, we calculate its future
cost by searching through the translation forest de-
fined by all applicable CFG rules. Figure 4 shows
the translation forest for the derivation of step 3. We
calculate the future cost for each node as follows:
given a node v, we define its cost function f(v) as
</bodyText>
<equation confidence="0.928137">
f(v) = { 1 v is completed
lm(v) v is a terminal string
maxrER„ f(r)HIlErhs(r) f(7) otherwise
</equation>
<bodyText confidence="0.99985675">
where VN is the non-terminal set, VT is the terminal
set, v, 7r E VN U VT +, R„ is the set of currently ap-
plicable rules for v, rhs(r) is the right-hand symbol
set of r, lm is the local language model probability,
f(r) is calculated using a linear model whose fea-
tures are bidirectional translation probabilities and
lexical probabilities of r. For the translation forest
in Figure 4, if we calculate the future cost of NP with
</bodyText>
<footnote confidence="0.712911">
5Section 3.7 will describe the binning scheme
</footnote>
<equation confidence="0.941173333333333">
r4, then
f(NP) = f(r4) · f(NN2) · lm(of the vote)
= f(r4) · 1 · lm(of the vote)
</equation>
<bodyText confidence="0.999976666666667">
Note that we calculate lm(of the vote) locally and do
not take “the result” derived from NN2 as the con-
text. The lm probability of “the result” has been in-
cluded in the covered cost.
As a partial derivation grows, some CFG rules
will conflict with the derivation (i.e. inapplicable)
and the translation forest will change accordingly.
For example, when we reach step 5 from step 3 (see
Figure 4 for its translation forest), r3 is inapplica-
ble and thereby should be ruled out. Then the nodes
on the path from the last covered node (it is “of the
vote” in step 5) to the root node should update their
future cost, as they may employ r3 to produce the
future cost. In step 5, NP and IP should be updated.
In this sense, we say that the future cost is dynamic.
</bodyText>
<subsectionHeader confidence="0.993449">
3.5 Comparison with Top-Down Decoding
</subsectionHeader>
<bodyText confidence="0.999908230769231">
In order to generate the translation “the result” based
on the derivation in Figure 1(b), Huang and Mi’s
top-down algorithm needs to specify which rules to
apply starting from the root node until it yields “the
result”. In this derivation, rule r6 is applied to IP, r4
to NP, r2 to NN2. That is to say, it needs to repre-
sent the partial derivation from IP to NN2 explicitly.
This can be a problem when combined with beam
pruning. If the beam size is small, it may discard the
intermediate hypotheses and thus never consider the
string. In our example with a beam of 1, we must
select a rule for IP among r6, r7 and r8 although we
do not get any information for NP and VP.
Instead, our bottom-up algorithm allows top-
down and bottom-up information to be used together
with the help of viable prefixes. This allows us to
encode more candidate derivations than the purely
top-down method. In the above example, our al-
gorithm does not specify the derivation for the gap
from IP and “the result”. In fact, all derivations
composed of currently applicable rules are allowed.
When needed, our algorithm derives the derivation
dynamically using applicable rules. So when our
algorithm performs pruning at the root node, it has
got much more information and consequently intro-
duces fewer pruning errors.
</bodyText>
<equation confidence="0.651080571428571">
VP
NP
r6 r7
r3 r4 r5
of the vote
NN2 of NN1
was released at night
</equation>
<page confidence="0.972721">
1196
</page>
<subsectionHeader confidence="0.987069">
3.6 Time Complexity
</subsectionHeader>
<bodyText confidence="0.999254736842105">
Assume the depth of the source tree is d, the max-
imum number of matched rules for each node is c,
the maximum arity of each rule is r, the language
model order is g and the target-language vocabulary
is V, then the time complexity of our algorithm is
O((cr)d|V |9−1). Analysis is as follows:
Our algorithm expands partial paths with termi-
nal strings to generate new hypotheses, so the time
complexity depends on the number of partial paths
used. We split a path which is from the root node to a
leaf node with a node on it (called the end node) and
get the segment from the root node to the end node
as a partial path, so the length of the partial path is
not definite with a maximum of d. If the length is
d′(d′ &lt; d), then the number of partial paths is (cr)d′.
Besides, we use the rightest g − 1 words to signa-
ture each partial path, so we can get (cr)d′|V |9−1
states. For each state, the number of viable prefixes
produced by predict operation is cd−d′, so the total
</bodyText>
<construct confidence="0.673011">
time complexity is f = O((cr)d′|V |9−
O(cdrd′|V |9−1) = O((cr)d|V |9−1).
</construct>
<subsectionHeader confidence="0.997851">
3.7 Beam Search
</subsectionHeader>
<bodyText confidence="0.999939851851852">
To make decoding tractable, we employ beam search
(Koehn, 2004) and choose “binning” as follows: hy-
potheses covering the same number of source words
are grouped in a bin. When expanding a hypothe-
sis in a beam (bin), we take series of actions until
new terminals are appended to the hypothesis, then
add the new hypothesis to the corresponding beam.
Figure 3 shows the number of source words each hy-
pothesis covers.
Among the actions, only the scan action changes
the number of source words each hypothesis cov-
ers. Although the complete action does not change
source word number, it changes the covered cost of
hypotheses. So in our implementation, we take scan
and complete as “closure” actions. That is to say,
once there are some complete actions after a scan ac-
tion, we finish all the compete actions until the next
action is grow. The predict and grow actions decide
which rules can be used to expand hypotheses next,
so we update the applicable rule set during these two
actions.
Given a source sentence with n words, we main-
tain n beams, and let each beam hold b hypotheses
at most. Besides, we prune viable prefixes of each
node up to u, so each hypothesis can expand to u
new hypotheses at most, so the time complexity of
beam search is O(nub).
</bodyText>
<sectionHeader confidence="0.999712" genericHeader="method">
4 Related Work
</sectionHeader>
<bodyText confidence="0.999954576923077">
Watanabe et al. (2006) present a novel Earley-
style top-down decoding algorithm for hierarchical
phrase-based model (Chiang, 2005). Their frame-
work extracts Greibach Normal Form rules only,
which always has at least one terminal on the left
of each rule, and discards other rules.
Dyer and Resnik (2010) describe a translation
model that combines the merits of syntax-based
models and phrase-based models. Their decoder
works in two passes: for first pass, the decoder col-
lects a context-free forest and performs tree-based
source reordering without a LM. For the second
pass, the decoder adds a LM and performs bottom-
up CKY decoding.
Feng et al. (2010) proposed a shift-reduce algo-
rithm to add BTG constraints to phrase-based mod-
els. This algorithm constructs a BTG tree in a
reduce-eager manner while the algorithm in this pa-
per searches for a best derivation which must be de-
rived from the source tree.
Galley and Manning (2008) use the shift-reduce
algorithm to conduct hierarchical phrase reordering
so as to capture long-distance reordering. This al-
gorithm shows good performance on phrase-based
models, but can not be applied to syntax-based mod-
els directly.
</bodyText>
<sectionHeader confidence="0.998565" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999869333333333">
In the experiments, we use two baseline systems:
our in-house tree-to-string decoder implemented ac-
cording to Liu et al. (2006) (denoted as traditional)
and the Earley-style top-down decoder implemented
according to Huang and Mi (2010) (denoted as top-
down), respectively. We compare our bottom-up
left-to-right decoder (denoted as bottom-up) with
the baseline in terms of performance, translation
quality and decoding speed with different beam
sizes, and search capacity. Lastly, we show the in-
fluence of future cost. All systems are implemented
in C++.
</bodyText>
<equation confidence="0.981746">
1cd−d′) =
</equation>
<page confidence="0.987543">
1197
</page>
<subsectionHeader confidence="0.993791">
5.1 Data Setup
</subsectionHeader>
<bodyText confidence="0.999990588235294">
We used the FBIS corpus consisting of about 250K
Chinese-English sentence pairs as the training set.
We aligned the sentence pairs using the GIZA++
toolkit (Och and Ney, 2003) and extracted tree-to-
string rules according to the GHKM algorithm (Gal-
ley et al., 2004). We used the SRILM toolkit (Stol-
cke, 2002) to train a 4-gram language model on the
Xinhua portion of the GIGAWORD corpus.
We used the 2002 NIST MT Chinese-English test
set (571 sentences) as the development set and the
2005 NIST MT Chinese-English test set (1082 sen-
tences) as the test set. We evaluated translation qual-
ity using BLEU-metric (Papineni et al., 2002) with
case-insensitive n-gram matching up to n = 4. We
used the standard minimum error rate training (Och,
2003) to tune feature weights to maximize BLEU
score on the development set.
</bodyText>
<subsectionHeader confidence="0.999399">
5.2 Performance Comparison
</subsectionHeader>
<bodyText confidence="0.999928115384615">
Our bottom-up left-to-right decoder employs the
same features as the traditional decoder: rule proba-
bility, lexical probability, language model probabil-
ity, rule count and word count. In order to compare
them fairly, we used the same beam size which is 20
and employed cube pruning technique (Huang and
Chiang, 2005).
We show the results in Table 3. From the re-
sults, we can see that the bottom-up decoder out-
performs top-down decoder and traditional decoder
by 1.1 and 0.8 BLEU points respectively and the
improvements are statistically significant using the
sign-test of Collins et al. (2005) (p &lt; 0.01). The
improvement may result from dynamically search-
ing for a whole derivation which leads to more ac-
curate estimation of a partial derivation. The addi-
tional time consumption of the bottom-up decoder
against the top-down decoder comes from dynamic
future cost computation.
Next we compare decoding speed versus transla-
tion quality using various beam sizes. The results
are shown in Figure 5. We can see that our bottom-
up decoder can produce better BLEU score at the
same decoding speed. At small beams (decoding
time around 0.5 second), the improvement of trans-
lation quality is much bigger.
</bodyText>
<table confidence="0.9995905">
System BLEU(%) Time (s)
Traditional 29.8 0.84
Top-down 29.5 0.41
Bottom-up 30.6 0.81
</table>
<tableCaption confidence="0.99814">
Table 3: Performance comparison.
</tableCaption>
<figure confidence="0.7117115">
0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8
Avg Decoding Time (secs per sentence)
</figure>
<figureCaption confidence="0.9976435">
Figure 5: BLEU score against decoding time with various
beam size.
</figureCaption>
<subsectionHeader confidence="0.997104">
5.3 Search Capacity Comparison
</subsectionHeader>
<bodyText confidence="0.99989048">
We also compare the search capacity of the bottom-
up decoder and the traditional decoder. We do this
in the following way: we let both decoders use the
same weights tuned on the traditional decoder, then
we compare their translation scores of the same test
sentence.
From the results in Table 4, we can see that for
many test sentences, the bottom-up decoder finds
target translations with higher score, which have
been ruled out by the traditional decoder. This may
result from more accurate pruning method. Yet for
some sentences, the traditional decoder can attain
higher translation score. The reason may be that the
traditional decoder can hold more than two nonter-
minals when cube pruning, while the bottom-up de-
coder always performs dual-arity pruning.
Next, we check whether higher translation scores
bring higher BLEU scores. We compute the BLEU
score of both decoders on the test sentence set on
which bottom-up decoder gets higher translation
scores than the traditional decoder does. We record
the results in Figure 6. The result shows that higher
score indeed bring higher BLEU score, but the im-
provement of BLEU score is not large. This is be-
cause the features we use don’t reflect the real statis-
</bodyText>
<figure confidence="0.988518933333333">
BLEU Score
29.8
29.6
29.4
30.8
30.6
30.4
30.2
30.0
bottom-up
top-down
traditional
1198
10 20 30 40
Beam Size
</figure>
<figureCaption confidence="0.99504475">
Figure 6: BLEU score with various beam sizes on the sub
test set consisting of sentences on which the bottom-up
decoder gets higher translation score than the traditional
decoder does.
</figureCaption>
<table confidence="0.957477833333333">
b &gt; _ &lt;
10 728 67% 347 32% 7 1%
20 657 61% 412 38% 13 1%
30 615 57% 446 41% 21 2%
40 526 49% 523 48% 33 3%
50 315 29% 705 65% 62 6%
</table>
<tableCaption confidence="0.941572">
Table 4: Search capacity comparison. The first column is
beam size, the following three columns denote the num-
ber of test sentences, on which the translation scores of
the bottom-up decoder are greater, equal to, lower than
that of the traditional decoder.
</tableCaption>
<table confidence="0.997781">
System BLEU(%) Time (s)
with 30.6 0.81
without 28.8 0.39
</table>
<tableCaption confidence="0.677069666666667">
Table 5: Influence of future cost. The results of the
bottom-up decoder with and without future cost are given
in the second and three rows, respectively.
</tableCaption>
<bodyText confidence="0.99519875">
tical distribution of hypotheses well. In addition, the
weights are tuned on the traditional decoder, not on
the bottom-up decoder. The bottom-up decoder can
perform better with weights tuned by itself.
</bodyText>
<subsectionHeader confidence="0.957231">
5.4 Influence of Future Cost
</subsectionHeader>
<bodyText confidence="0.9999455">
Next, we will show the impact of future cost via ex-
periments. We give the results of the bottom-up de-
coder with and without future cost in Table 5. From
the result, we can conclude that future cost plays a
significant role in decoding. If the bottom-up de-
coder does not employ future cost, its performance
will be influenced dramatically. Furthermore, cal-
culating dynamic future cost is time consuming. If
the bottom-up decoder does not use future cost, it
decodes faster than the top-down decoder. This is
because the top-down decoder has IT I beams, while
the bottom-up decoder has n beams, where T is the
source parse tree and n is the length of the source
sentence.
</bodyText>
<sectionHeader confidence="0.999693" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999957230769231">
In this paper, we describe a bottom-up left-to-right
decoding algorithm for tree-to-string model. With
the help of viable prefixes, the algorithm generates
a translation by constructing a target-side CFG tree
according to a post-order traversal. In addition, it
takes into consideration a dynamic future cost to es-
timate hypotheses.
On the 2005 NIST Chinese-English MT transla-
tion test set, our decoder outperforms the top-down
decoder and the traditional decoder by 1.1 and 0.8
BLEU points respectively and shows more powerful
search ability. Experiments also prove that future
cost is important for more accurate pruning.
</bodyText>
<sectionHeader confidence="0.998966" genericHeader="acknowledgments">
7 Acknowledgements
</sectionHeader>
<bodyText confidence="0.998660333333333">
We would like to thank Haitao Mi and Douwe
Gelling for their feedback, and anonymous review-
ers for their valuable comments and suggestions.
This work was supported in part by EPSRC grant
EP/I034750/1 and in part by High Technology R&amp;D
Program Project No. 2011AA01A207.
</bodyText>
<sectionHeader confidence="0.999245" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999722076923077">
A. V. Aho and S. C. Johnson. 1974. Lr parsing. Com-
puting Surveys, 6:99–124.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proc. of ACL, pages 173–180.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proc. of ACL,
pages 263–270.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33:201–228.
Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005. Clause restructuring for statistical machine
translation. In Proc. of ACL, pages 531–540.
</reference>
<figure confidence="0.997391">
35.0
34.0
33.0
32.0
31.0
30.0
29.0
28.0
bottom-up
traditional
BLEU Score
</figure>
<page confidence="0.989009">
1199
</page>
<reference confidence="0.999914574074074">
Chris Dyer and Philip Resnik. 2010. Context-free re-
ordering, finite-state translation. In Proc. of NAACL,
pages 858–866, June.
Jay Earley. 1970. An efficient context-free parsing algo-
rithm. Communications of the ACM, 13:94–102.
Yang Feng, Haitao Mi, Yang Liu, and Qun Liu. 2010. An
efficient shift-reduce decoding algorithm for phrased-
based machine translation. In Proc. of Coling, pages
285–293.
Michel Galley and Christopher D. Manning. 2008. A
simple and effective hierarchical phrase reordering
model. In Proc. of EMNLP, pages 848–856.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What’s in a translation rule? In Proc of
NAACL, pages 273–280.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proc. of IWPT, pages 53–64.
Liang Huang and Haitao Mi. 2010. Efficient incremen-
tal decoding for tree-to-string translation. In Proc. of
EMNLP, pages 273–283.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In Proceedings of AMTA.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proc. of ACL,
pages 586–594.
Kevin Knight. 1999. Decoding complexity in word-
replacement translation models. Computational Lin-
guistics, 25:607–615.
Philipp Koehn. 2004. Pharaoh: A beam search decoder
for phrased-based statistical machine translation. In
Proc. of AMTA, pages 115–124.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-
string alignment template for statistical machine trans-
lation. In Proceedings of COLING-ACL, pages 609–
616, July.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proc. of ACL, pages 192–199.
Frans J. Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29:19–51.
Frans J. Och. 2003. Minimum error rate training in sta-
tistical machine translation. In Proc. of ACL, pages
160–167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalu-
ation of machine translation. In Proceedings of ACL,
pages 311–318.
Andreas Stolcke. 2002. Srilm-an extensible language
modeling toolkit. In Proc. of ICSLP.
Taro Watanabe, Hajime Tsukada, and Hideki Isozaki.
2006. Left-to-right target generation for hierarchical
phrase-based translation. In Proc. of COLING, pages
777–784.
</reference>
<page confidence="0.984543">
1200
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.622708">
<title confidence="0.999903">Left-to-Right Tree-to-String Decoding with Prediction</title>
<author confidence="0.996073">Trevor</author>
<affiliation confidence="0.9796606">of Computer The University of Sheffield, Sheffield, Key Laboratory on Intelligent Technology and Tsinghua National Laboratory for Information Science and Department of Computer Sci. and Tech., Tsinghua University, Beijing,</affiliation>
<email confidence="0.849345">liuyang2011@tsinghua.edu.cn</email>
<affiliation confidence="0.94014">Laboratory of Intelligent Information Institute of Computing Chinese Academy of Sciences, Beijing,</affiliation>
<email confidence="0.987997">liuqun@ict.ac.cn</email>
<abstract confidence="0.9990435625">Decoding algorithms for syntax based machine translation suffer from high computational complexity, a consequence of intersecting a language model with a context free grammar. Left-to-right decoding, which generates the target string in order, can improve decoding efficiency by simplifying the language model evaluation. This paper presents a novel left to right decoding algorithm for tree-to-string translation, using a bottom-up parsing strategy and dynamic future cost estimation for each partial translation. Our method outperforms previously published tree-to-string decoders, including a competing left-to-right method.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A V Aho</author>
<author>S C Johnson</author>
</authors>
<date>1974</date>
<booktitle>Lr parsing. Computing Surveys,</booktitle>
<pages>6--99</pages>
<contexts>
<context position="7762" citStr="Aho and Johnson, 1974" startWordPosition="1202" endWordPosition="1206">ntext-Free Grammar (CFG) rules and employs three actions: predict, scan and complete (Section 3.1 describes how to convert STSG rules into CFG rules). We can simulate its translation process using a stack with a dot . indicating which symbol to process next. For the derivation in Figure 1(b) and CFG rules in Figure 1(c), Figure 2 illustrates the whole translation process. The time complexity is shown in Table 1 . 1192 3 Bottom-Up Left-to-Right Decoding We propose a novel method of left-to-right decoding for tree-to-string translation using a bottom-up parsing strategy. We use viable prefixes (Aho and Johnson, 1974) to indicate all possible target strings the translations of each node should starts with. Therefore, given a tree node to expand, our algorithm can drop immediately to target terminals no matter whether there is a gap or not. We say that there is a gap between two symbols in a derivation when there are many rules separating them, e.g. IP + ... → NN2. For the derivation in Figure 1(b), our algorithm starts from the root node IP and applies r2 first although there is a gap between IP and NN2. Then it applies r4, r5 and r6 in sequence to generate the translation “the result of the vote was relea</context>
<context position="11405" citStr="Aho and Johnson, 1974" startWordPosition="1891" endWordPosition="1894"> starting terminal string of the translation for NN1 is “the vote”. According to r2, the starting terminal string for NN2 is “the result”. According to r3, the starting terminal string of NP must include that of NN2. Table 2 lists the starting terminal strings of all nodes in Figure 1(a). As the translations of node IP should begin with either “the result” or “was released at night”, the first rule must be either r2 or r5. Therefore, r1 will never be used as the first rule in any derivation. We refer to starting terminal strings of a node as a viable prefixes, a term borrowed from LR parsing (Aho and Johnson, 1974). Viable prefixes are used to decide which rule should be used to ensure efficient left-to-right target generation. Formally, assume that VN denotes the set of non-terminals (i.e., source tree node labels), VT denotes the set of terminals (i.e., target words), v1, v2 E VN, w E VT , 7r E {VT U VN}*, we say that w is a viable prefix of v1 if and only if: • v1 w, or • v1 wv27r, or • v1 v27r, and w is a viable prefix of v2. Note that we bundle all successive terminals in one symbol. 3.3 Shift-Reduce Parsing We use a shift-reduce algorithm to search for the best deviation. The algorithm maintains a</context>
</contexts>
<marker>Aho, Johnson, 1974</marker>
<rawString>A. V. Aho and S. C. Johnson. 1974. Lr parsing. Computing Surveys, 6:99–124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Coarse-tofine n-best parsing and maxent discriminative reranking.</title>
<date>2005</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>173--180</pages>
<contexts>
<context position="17213" citStr="Charniak and Johnson, 2005" startWordPosition="3061" endWordPosition="3064">eses (Knight, 1999), we use beam search over bins of similar partial hypotheses (Koehn, 2004). 1195 IP Figure 4: The translation forest composed of applicable CFG rules for the partial derivation of step 3 in Figure 3. 3.4 Future Cost Partial derivations covering different tree nodes may be grouped in the same bin for beam pruning5. In order to perform more accurate pruning, we take into consideration future cost, the cost of the uncovered part. The merit of a derivation is the covered cost (the cost of the covered part) plus the future cost. We borrow ideas from the Inside-Outside algorithm (Charniak and Johnson, 2005; Huang, 2008; Mi et al., 2008) to compute the merit. In our algorithm, the merit of a derivation is just the Viterbi inside cost Q of the root node calculated with the derivations continuing from the current derivation. Given a partial derivation, we calculate its future cost by searching through the translation forest defined by all applicable CFG rules. Figure 4 shows the translation forest for the derivation of step 3. We calculate the future cost for each node as follows: given a node v, we define its cost function f(v) as f(v) = { 1 v is completed lm(v) v is a terminal string maxrER„ f(r</context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Eugene Charniak and Mark Johnson. 2005. Coarse-tofine n-best parsing and maxent discriminative reranking. In Proc. of ACL, pages 173–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>263--270</pages>
<contexts>
<context position="23031" citStr="Chiang, 2005" startWordPosition="4119" endWordPosition="4120">the compete actions until the next action is grow. The predict and grow actions decide which rules can be used to expand hypotheses next, so we update the applicable rule set during these two actions. Given a source sentence with n words, we maintain n beams, and let each beam hold b hypotheses at most. Besides, we prune viable prefixes of each node up to u, so each hypothesis can expand to u new hypotheses at most, so the time complexity of beam search is O(nub). 4 Related Work Watanabe et al. (2006) present a novel Earleystyle top-down decoding algorithm for hierarchical phrase-based model (Chiang, 2005). Their framework extracts Greibach Normal Form rules only, which always has at least one terminal on the left of each rule, and discards other rules. Dyer and Resnik (2010) describe a translation model that combines the merits of syntax-based models and phrase-based models. Their decoder works in two passes: for first pass, the decoder collects a context-free forest and performs tree-based source reordering without a LM. For the second pass, the decoder adds a LM and performs bottomup CKY decoding. Feng et al. (2010) proposed a shift-reduce algorithm to add BTG constraints to phrase-based mod</context>
<context position="25821" citStr="Chiang, 2005" startWordPosition="4570" endWordPosition="4571">e test set. We evaluated translation quality using BLEU-metric (Papineni et al., 2002) with case-insensitive n-gram matching up to n = 4. We used the standard minimum error rate training (Och, 2003) to tune feature weights to maximize BLEU score on the development set. 5.2 Performance Comparison Our bottom-up left-to-right decoder employs the same features as the traditional decoder: rule probability, lexical probability, language model probability, rule count and word count. In order to compare them fairly, we used the same beam size which is 20 and employed cube pruning technique (Huang and Chiang, 2005). We show the results in Table 3. From the results, we can see that the bottom-up decoder outperforms top-down decoder and traditional decoder by 1.1 and 0.8 BLEU points respectively and the improvements are statistically significant using the sign-test of Collins et al. (2005) (p &lt; 0.01). The improvement may result from dynamically searching for a whole derivation which leads to more accurate estimation of a partial derivation. The additional time consumption of the bottom-up decoder against the top-down decoder comes from dynamic future cost computation. Next we compare decoding speed versus</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>David Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proc. of ACL, pages 263–270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<pages>33--201</pages>
<contexts>
<context position="2102" citStr="Chiang, 2007" startWordPosition="287" endWordPosition="288">ure consistent syntactic transformations between the source and target languages, e.g., from subject-verb-object to subject-object-verb word orderings. Decoding algorithms for grammar-based translation seek to find the best string in the intersection between a weighted context free grammar (the translation mode, given a source string/tree) and a weighted finite state acceptor (an n-gram language model). This intersection is problematic, as it results in an intractably large grammar, and makes exact search impossible. Most researchers have resorted to approximate search, typically beam search (Chiang, 2007). The decoder parses the source sentence, recording the target translations for each span.&apos; As the partial translation hypothesis grows, its component ngrams are scored and the hypothesis score is updated. This decoding method though is inefficient as it requires recording the language model context (n −1 words) on the left and right edges of each chart cell. These contexts allow for boundary ngrams to be evaluated when the cell is used in another grammar production. In contrast, if the target string is generated in left-to-right order, then only one language model context is required, and the</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33:201–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Philipp Koehn</author>
<author>Ivona Kucerova</author>
</authors>
<title>Clause restructuring for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>531--540</pages>
<contexts>
<context position="26099" citStr="Collins et al. (2005)" startWordPosition="4614" endWordPosition="4617">5.2 Performance Comparison Our bottom-up left-to-right decoder employs the same features as the traditional decoder: rule probability, lexical probability, language model probability, rule count and word count. In order to compare them fairly, we used the same beam size which is 20 and employed cube pruning technique (Huang and Chiang, 2005). We show the results in Table 3. From the results, we can see that the bottom-up decoder outperforms top-down decoder and traditional decoder by 1.1 and 0.8 BLEU points respectively and the improvements are statistically significant using the sign-test of Collins et al. (2005) (p &lt; 0.01). The improvement may result from dynamically searching for a whole derivation which leads to more accurate estimation of a partial derivation. The additional time consumption of the bottom-up decoder against the top-down decoder comes from dynamic future cost computation. Next we compare decoding speed versus translation quality using various beam sizes. The results are shown in Figure 5. We can see that our bottomup decoder can produce better BLEU score at the same decoding speed. At small beams (decoding time around 0.5 second), the improvement of translation quality is much bigg</context>
</contexts>
<marker>Collins, Koehn, Kucerova, 2005</marker>
<rawString>Michael Collins, Philipp Koehn, and Ivona Kucerova. 2005. Clause restructuring for statistical machine translation. In Proc. of ACL, pages 531–540.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Dyer</author>
<author>Philip Resnik</author>
</authors>
<title>Context-free reordering, finite-state translation.</title>
<date>2010</date>
<booktitle>In Proc. of NAACL,</booktitle>
<pages>858--866</pages>
<contexts>
<context position="23204" citStr="Dyer and Resnik (2010)" startWordPosition="4147" endWordPosition="4150">e rule set during these two actions. Given a source sentence with n words, we maintain n beams, and let each beam hold b hypotheses at most. Besides, we prune viable prefixes of each node up to u, so each hypothesis can expand to u new hypotheses at most, so the time complexity of beam search is O(nub). 4 Related Work Watanabe et al. (2006) present a novel Earleystyle top-down decoding algorithm for hierarchical phrase-based model (Chiang, 2005). Their framework extracts Greibach Normal Form rules only, which always has at least one terminal on the left of each rule, and discards other rules. Dyer and Resnik (2010) describe a translation model that combines the merits of syntax-based models and phrase-based models. Their decoder works in two passes: for first pass, the decoder collects a context-free forest and performs tree-based source reordering without a LM. For the second pass, the decoder adds a LM and performs bottomup CKY decoding. Feng et al. (2010) proposed a shift-reduce algorithm to add BTG constraints to phrase-based models. This algorithm constructs a BTG tree in a reduce-eager manner while the algorithm in this paper searches for a best derivation which must be derived from the source tre</context>
</contexts>
<marker>Dyer, Resnik, 2010</marker>
<rawString>Chris Dyer and Philip Resnik. 2010. Context-free reordering, finite-state translation. In Proc. of NAACL, pages 858–866, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jay Earley</author>
</authors>
<title>An efficient context-free parsing algorithm.</title>
<date>1970</date>
<journal>Communications of the ACM,</journal>
<pages>13--94</pages>
<contexts>
<context position="4119" citStr="Earley, 1970" startWordPosition="600" endWordPosition="601">nt Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1191–1200, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics nodes, it is necessary to consider the cost of uncovered nodes, i.e., the future cost. We show that a good future cost estimate is essential for accurate and efficient search, leading to high quality translation output. Other researchers have also considered the leftto-right decoding algorithm for tree-to-string models. Huang and Mi (2010) developed an Earleystyle parsing algorithm (Earley, 1970). In their approach, hypotheses covering the same number of tree nodes were binned together. Their method uses a top-down depth-first search, with a mechanism for early elimation of some rules which lead to deadends in the search. Huang and Mi (2010)’s method was shown to outperform the traditional post-ordertraversal decoding algorithm, considering fewer hypotheses and thus decoding much faster at the same level of performance. However their algorithm used a very rough estimate of future cost, resulting in more search errors than our approach. Our experiments show that compared with the Earle</context>
<context position="12042" citStr="Earley, 1970" startWordPosition="2015" endWordPosition="2016">ed to decide which rule should be used to ensure efficient left-to-right target generation. Formally, assume that VN denotes the set of non-terminals (i.e., source tree node labels), VT denotes the set of terminals (i.e., target words), v1, v2 E VN, w E VT , 7r E {VT U VN}*, we say that w is a viable prefix of v1 if and only if: • v1 w, or • v1 wv27r, or • v1 v27r, and w is a viable prefix of v2. Note that we bundle all successive terminals in one symbol. 3.3 Shift-Reduce Parsing We use a shift-reduce algorithm to search for the best deviation. The algorithm maintains a stack of dotted rules (Earley, 1970). Given the source tree in Figure 1(a), the stack is initialized with a dotted rule for the root node IP: [. IP]. Then, the algorithm selects one viable prefix of IP and appends it to the stack with the dot at the beginning (predict): [. IP] [. the result]2. Then, a scan action is performed to produce a partial translation “the result”: [. IP] [the result.]. Next, the algorithm searches for the CFG rules starting with “the result” and gets r2. Then, it pops the rightmost dotted rule and append the left-hand side (LHS) of r2 to the stack (complete): [- IP] [NN2 -. Next, the algorithm chooses r�</context>
</contexts>
<marker>Earley, 1970</marker>
<rawString>Jay Earley. 1970. An efficient context-free parsing algorithm. Communications of the ACM, 13:94–102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Feng</author>
<author>Haitao Mi</author>
<author>Yang Liu</author>
<author>Qun Liu</author>
</authors>
<title>An efficient shift-reduce decoding algorithm for phrasedbased machine translation.</title>
<date>2010</date>
<booktitle>In Proc. of Coling,</booktitle>
<pages>285--293</pages>
<contexts>
<context position="23554" citStr="Feng et al. (2010)" startWordPosition="4204" endWordPosition="4207">ovel Earleystyle top-down decoding algorithm for hierarchical phrase-based model (Chiang, 2005). Their framework extracts Greibach Normal Form rules only, which always has at least one terminal on the left of each rule, and discards other rules. Dyer and Resnik (2010) describe a translation model that combines the merits of syntax-based models and phrase-based models. Their decoder works in two passes: for first pass, the decoder collects a context-free forest and performs tree-based source reordering without a LM. For the second pass, the decoder adds a LM and performs bottomup CKY decoding. Feng et al. (2010) proposed a shift-reduce algorithm to add BTG constraints to phrase-based models. This algorithm constructs a BTG tree in a reduce-eager manner while the algorithm in this paper searches for a best derivation which must be derived from the source tree. Galley and Manning (2008) use the shift-reduce algorithm to conduct hierarchical phrase reordering so as to capture long-distance reordering. This algorithm shows good performance on phrase-based models, but can not be applied to syntax-based models directly. 5 Experiments In the experiments, we use two baseline systems: our in-house tree-to-str</context>
</contexts>
<marker>Feng, Mi, Liu, Liu, 2010</marker>
<rawString>Yang Feng, Haitao Mi, Yang Liu, and Qun Liu. 2010. An efficient shift-reduce decoding algorithm for phrasedbased machine translation. In Proc. of Coling, pages 285–293.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Christopher D Manning</author>
</authors>
<title>A simple and effective hierarchical phrase reordering model.</title>
<date>2008</date>
<booktitle>In Proc. of EMNLP,</booktitle>
<pages>848--856</pages>
<contexts>
<context position="23832" citStr="Galley and Manning (2008)" startWordPosition="4252" endWordPosition="4255">scribe a translation model that combines the merits of syntax-based models and phrase-based models. Their decoder works in two passes: for first pass, the decoder collects a context-free forest and performs tree-based source reordering without a LM. For the second pass, the decoder adds a LM and performs bottomup CKY decoding. Feng et al. (2010) proposed a shift-reduce algorithm to add BTG constraints to phrase-based models. This algorithm constructs a BTG tree in a reduce-eager manner while the algorithm in this paper searches for a best derivation which must be derived from the source tree. Galley and Manning (2008) use the shift-reduce algorithm to conduct hierarchical phrase reordering so as to capture long-distance reordering. This algorithm shows good performance on phrase-based models, but can not be applied to syntax-based models directly. 5 Experiments In the experiments, we use two baseline systems: our in-house tree-to-string decoder implemented according to Liu et al. (2006) (denoted as traditional) and the Earley-style top-down decoder implemented according to Huang and Mi (2010) (denoted as topdown), respectively. We compare our bottom-up left-to-right decoder (denoted as bottom-up) with the </context>
</contexts>
<marker>Galley, Manning, 2008</marker>
<rawString>Michel Galley and Christopher D. Manning. 2008. A simple and effective hierarchical phrase reordering model. In Proc. of EMNLP, pages 848–856.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Mark Hopkins</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>What’s in a translation rule?</title>
<date>2004</date>
<booktitle>In Proc of NAACL,</booktitle>
<pages>273--280</pages>
<contexts>
<context position="5359" citStr="Galley et al., 2004" startWordPosition="790" endWordPosition="793">t decoding (Huang and Mi, 2010) and the traditional post-order-traversal decoding (Liu et al., 2006) algorithms, our algorithm achieves a significant improvement on search capacity and better translation performance at the same level of speed. 2 Background A typical tree-to-string system (Liu et al., 2006; Huang et al., 2006) searches through a 1-best source parse tree for the best derivation. It transduces the source tree into a target-language string using a Synchronous Tree Substitution Grammar (STSG). The grammar rules are extracted from bilingual word alignments using the GHKM algorithm (Galley et al., 2004). We will briefly review the traditional decoding algorithm (Liu et al., 2006) and the Earley-style topdown decoding algorithm (Huang and Mi, 2010) for the tree-to-string model. 2.1 Traditional Decoding The traditional decoding algorithm processes source tree nodes one by one according to a post-order traversal. For each node, it applies matched STSG rules by substituting each non-terminal with its corin theory beam search traditional O(nc�|V |4(9−1)) O(ncb2) top-down O(c(cr)d|V |9−1) O(ncb) bottom-up O((cr)d|V |9−1) O(nub) Table 1: Time complexity of different algorithms. traditional : Liu et</context>
<context position="24928" citStr="Galley et al., 2004" startWordPosition="4420" endWordPosition="4424">i (2010) (denoted as topdown), respectively. We compare our bottom-up left-to-right decoder (denoted as bottom-up) with the baseline in terms of performance, translation quality and decoding speed with different beam sizes, and search capacity. Lastly, we show the influence of future cost. All systems are implemented in C++. 1cd−d′) = 1197 5.1 Data Setup We used the FBIS corpus consisting of about 250K Chinese-English sentence pairs as the training set. We aligned the sentence pairs using the GIZA++ toolkit (Och and Ney, 2003) and extracted tree-tostring rules according to the GHKM algorithm (Galley et al., 2004). We used the SRILM toolkit (Stolcke, 2002) to train a 4-gram language model on the Xinhua portion of the GIGAWORD corpus. We used the 2002 NIST MT Chinese-English test set (571 sentences) as the development set and the 2005 NIST MT Chinese-English test set (1082 sentences) as the test set. We evaluated translation quality using BLEU-metric (Papineni et al., 2002) with case-insensitive n-gram matching up to n = 4. We used the standard minimum error rate training (Och, 2003) to tune feature weights to maximize BLEU score on the development set. 5.2 Performance Comparison Our bottom-up left-to-r</context>
</contexts>
<marker>Galley, Hopkins, Knight, Marcu, 2004</marker>
<rawString>Michel Galley, Mark Hopkins, Kevin Knight, and Daniel Marcu. 2004. What’s in a translation rule? In Proc of NAACL, pages 273–280.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>David Chiang</author>
</authors>
<title>Better k-best parsing.</title>
<date>2005</date>
<booktitle>In Proc. of IWPT,</booktitle>
<pages>53--64</pages>
<contexts>
<context position="25821" citStr="Huang and Chiang, 2005" startWordPosition="4568" endWordPosition="4571">ces) as the test set. We evaluated translation quality using BLEU-metric (Papineni et al., 2002) with case-insensitive n-gram matching up to n = 4. We used the standard minimum error rate training (Och, 2003) to tune feature weights to maximize BLEU score on the development set. 5.2 Performance Comparison Our bottom-up left-to-right decoder employs the same features as the traditional decoder: rule probability, lexical probability, language model probability, rule count and word count. In order to compare them fairly, we used the same beam size which is 20 and employed cube pruning technique (Huang and Chiang, 2005). We show the results in Table 3. From the results, we can see that the bottom-up decoder outperforms top-down decoder and traditional decoder by 1.1 and 0.8 BLEU points respectively and the improvements are statistically significant using the sign-test of Collins et al. (2005) (p &lt; 0.01). The improvement may result from dynamically searching for a whole derivation which leads to more accurate estimation of a partial derivation. The additional time consumption of the bottom-up decoder against the top-down decoder comes from dynamic future cost computation. Next we compare decoding speed versus</context>
</contexts>
<marker>Huang, Chiang, 2005</marker>
<rawString>Liang Huang and David Chiang. 2005. Better k-best parsing. In Proc. of IWPT, pages 53–64.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>Haitao Mi</author>
</authors>
<title>Efficient incremental decoding for tree-to-string translation.</title>
<date>2010</date>
<booktitle>In Proc. of EMNLP,</booktitle>
<pages>273--283</pages>
<contexts>
<context position="4061" citStr="Huang and Mi (2010)" startWordPosition="590" endWordPosition="593">rding to a postorder traversal. 1191 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1191–1200, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics nodes, it is necessary to consider the cost of uncovered nodes, i.e., the future cost. We show that a good future cost estimate is essential for accurate and efficient search, leading to high quality translation output. Other researchers have also considered the leftto-right decoding algorithm for tree-to-string models. Huang and Mi (2010) developed an Earleystyle parsing algorithm (Earley, 1970). In their approach, hypotheses covering the same number of tree nodes were binned together. Their method uses a top-down depth-first search, with a mechanism for early elimation of some rules which lead to deadends in the search. Huang and Mi (2010)’s method was shown to outperform the traditional post-ordertraversal decoding algorithm, considering fewer hypotheses and thus decoding much faster at the same level of performance. However their algorithm used a very rough estimate of future cost, resulting in more search errors than our a</context>
<context position="5506" citStr="Huang and Mi, 2010" startWordPosition="814" endWordPosition="817">t improvement on search capacity and better translation performance at the same level of speed. 2 Background A typical tree-to-string system (Liu et al., 2006; Huang et al., 2006) searches through a 1-best source parse tree for the best derivation. It transduces the source tree into a target-language string using a Synchronous Tree Substitution Grammar (STSG). The grammar rules are extracted from bilingual word alignments using the GHKM algorithm (Galley et al., 2004). We will briefly review the traditional decoding algorithm (Liu et al., 2006) and the Earley-style topdown decoding algorithm (Huang and Mi, 2010) for the tree-to-string model. 2.1 Traditional Decoding The traditional decoding algorithm processes source tree nodes one by one according to a post-order traversal. For each node, it applies matched STSG rules by substituting each non-terminal with its corin theory beam search traditional O(nc�|V |4(9−1)) O(ncb2) top-down O(c(cr)d|V |9−1) O(ncb) bottom-up O((cr)d|V |9−1) O(nub) Table 1: Time complexity of different algorithms. traditional : Liu et al. (2006), top-down : Huang and Mi (2010). n is the source sentence length, b is the beam width, c is the number of rules used for each node, V i</context>
<context position="6943" citStr="Huang and Mi, 2010" startWordPosition="1072" endWordPosition="1075">ion. For the derivation in Figure 1 (b), the traditional algorithm applies r2 at node NN2 r2 : NN2 (jieguo) → the result, to obtain “the result” as the translation of NN2. Next it applies r4 at node NP, r4 : NP ( NN1 (toupiao), x1 : NN2 ) → x1 of the vote and replaces NN2 with its translation “the result”, then it gets the translation of NP as “the result of the vote”. This algorithm needs to contain boundary words at both left and right extremities of the target string for the purpose of LM evaluation, which leads to a high time complexity. The time complexity in theory and with beam search (Huang and Mi, 2010) is shown in Table 1. 2.2 Earley-style Top-down Decoding The Earley-style decoding algorithm performs a topdown depth-first parsing and generates the target translation left to right. It applies Context-Free Grammar (CFG) rules and employs three actions: predict, scan and complete (Section 3.1 describes how to convert STSG rules into CFG rules). We can simulate its translation process using a stack with a dot . indicating which symbol to process next. For the derivation in Figure 1(b) and CFG rules in Figure 1(c), Figure 2 illustrates the whole translation process. The time complexity is shown</context>
<context position="9216" citStr="Huang and Mi (2010)" startWordPosition="1470" endWordPosition="1473">ning. A valid derivation is generated only when the source tree is completely matched by rules. Our bottom-up decoding algorithm involves the following steps: 1. Match STSG rules against the source tree. 2. Convert STSG rules to CFG rules. 3. Collect the viable prefix set for each node in a post-order transversal. 4. Search bottom-up for the best derivation. 3.1 From STSG to CFG After rule matching, each tree node has its applicable STSG rule set. Given a matched STSG rule, our decoding algorithm only needs to consider the tree node the rule can be applied to and the target side, so we follow Huang and Mi (2010) to convert STSG rules to CFG rules. For example, an STSG rule NP ( NN1 (toupiao), x1 : NN2 ) → x1 of the vote can be converted to a CFG rule NP → NN2 of the vote The target non-terminals are replaced with corresponding source non-terminals. Figure 1 (c) shows all converted CFG rules for the toy example. Note (a) Source parse tree the result of the vote was released at night (b) A derivation r1: NN1 → the vote r2: NN2 → the result r3: NP → NN2 of NN1 r4: NP → NN2 of the vote r5: VP → was released at night r6: IP → NP VP r7: IP → NN2 of the vote VP rg: IP → VP NP (c) Target-side CFG rule set Fi</context>
<context position="24316" citStr="Huang and Mi (2010)" startWordPosition="4323" endWordPosition="4326"> while the algorithm in this paper searches for a best derivation which must be derived from the source tree. Galley and Manning (2008) use the shift-reduce algorithm to conduct hierarchical phrase reordering so as to capture long-distance reordering. This algorithm shows good performance on phrase-based models, but can not be applied to syntax-based models directly. 5 Experiments In the experiments, we use two baseline systems: our in-house tree-to-string decoder implemented according to Liu et al. (2006) (denoted as traditional) and the Earley-style top-down decoder implemented according to Huang and Mi (2010) (denoted as topdown), respectively. We compare our bottom-up left-to-right decoder (denoted as bottom-up) with the baseline in terms of performance, translation quality and decoding speed with different beam sizes, and search capacity. Lastly, we show the influence of future cost. All systems are implemented in C++. 1cd−d′) = 1197 5.1 Data Setup We used the FBIS corpus consisting of about 250K Chinese-English sentence pairs as the training set. We aligned the sentence pairs using the GIZA++ toolkit (Och and Ney, 2003) and extracted tree-tostring rules according to the GHKM algorithm (Galley e</context>
</contexts>
<marker>Huang, Mi, 2010</marker>
<rawString>Liang Huang and Haitao Mi. 2010. Efficient incremental decoding for tree-to-string translation. In Proc. of EMNLP, pages 273–283.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>Kevin Knight</author>
<author>Aravind Joshi</author>
</authors>
<title>Statistical syntax-directed translation with extended domain of locality.</title>
<date>2006</date>
<booktitle>In Proceedings of AMTA.</booktitle>
<contexts>
<context position="5066" citStr="Huang et al., 2006" startWordPosition="745" endWordPosition="748"> algorithm, considering fewer hypotheses and thus decoding much faster at the same level of performance. However their algorithm used a very rough estimate of future cost, resulting in more search errors than our approach. Our experiments show that compared with the Earley-style left-to-right decoding (Huang and Mi, 2010) and the traditional post-order-traversal decoding (Liu et al., 2006) algorithms, our algorithm achieves a significant improvement on search capacity and better translation performance at the same level of speed. 2 Background A typical tree-to-string system (Liu et al., 2006; Huang et al., 2006) searches through a 1-best source parse tree for the best derivation. It transduces the source tree into a target-language string using a Synchronous Tree Substitution Grammar (STSG). The grammar rules are extracted from bilingual word alignments using the GHKM algorithm (Galley et al., 2004). We will briefly review the traditional decoding algorithm (Liu et al., 2006) and the Earley-style topdown decoding algorithm (Huang and Mi, 2010) for the tree-to-string model. 2.1 Traditional Decoding The traditional decoding algorithm processes source tree nodes one by one according to a post-order trav</context>
</contexts>
<marker>Huang, Knight, Joshi, 2006</marker>
<rawString>Liang Huang, Kevin Knight, and Aravind Joshi. 2006. Statistical syntax-directed translation with extended domain of locality. In Proceedings of AMTA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
</authors>
<title>Forest reranking: Discriminative parsing with non-local features.</title>
<date>2008</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>586--594</pages>
<contexts>
<context position="17226" citStr="Huang, 2008" startWordPosition="3065" endWordPosition="3066">beam search over bins of similar partial hypotheses (Koehn, 2004). 1195 IP Figure 4: The translation forest composed of applicable CFG rules for the partial derivation of step 3 in Figure 3. 3.4 Future Cost Partial derivations covering different tree nodes may be grouped in the same bin for beam pruning5. In order to perform more accurate pruning, we take into consideration future cost, the cost of the uncovered part. The merit of a derivation is the covered cost (the cost of the covered part) plus the future cost. We borrow ideas from the Inside-Outside algorithm (Charniak and Johnson, 2005; Huang, 2008; Mi et al., 2008) to compute the merit. In our algorithm, the merit of a derivation is just the Viterbi inside cost Q of the root node calculated with the derivations continuing from the current derivation. Given a partial derivation, we calculate its future cost by searching through the translation forest defined by all applicable CFG rules. Figure 4 shows the translation forest for the derivation of step 3. We calculate the future cost for each node as follows: given a node v, we define its cost function f(v) as f(v) = { 1 v is completed lm(v) v is a terminal string maxrER„ f(r)HIlErhs(r) f</context>
</contexts>
<marker>Huang, 2008</marker>
<rawString>Liang Huang. 2008. Forest reranking: Discriminative parsing with non-local features. In Proc. of ACL, pages 586–594.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
</authors>
<title>Decoding complexity in wordreplacement translation models.</title>
<date>1999</date>
<journal>Computational Linguistics,</journal>
<pages>25--607</pages>
<contexts>
<context position="16606" citStr="Knight, 1999" startWordPosition="2961" endWordPosition="2962"> example: 4We bundle the successive terminals in one rule into a symbol [. IP] [NN2 .] grow ��[. IP] [NN2 . of the vote] From the above definition, we can find that there may be an ambiguity about whether to use a complete action or a grow action. Similarly, predict actions must select a viable prefix form the set for a node. For example in step 5, although we select to perform complete with r4 in the example, r7 is applicable, too. In our implementation, if both r4 and r7 are applicable, we apply them both to generate two seperate hypotheses. To limit the exponential explosion of hypotheses (Knight, 1999), we use beam search over bins of similar partial hypotheses (Koehn, 2004). 1195 IP Figure 4: The translation forest composed of applicable CFG rules for the partial derivation of step 3 in Figure 3. 3.4 Future Cost Partial derivations covering different tree nodes may be grouped in the same bin for beam pruning5. In order to perform more accurate pruning, we take into consideration future cost, the cost of the uncovered part. The merit of a derivation is the covered cost (the cost of the covered part) plus the future cost. We borrow ideas from the Inside-Outside algorithm (Charniak and Johnso</context>
</contexts>
<marker>Knight, 1999</marker>
<rawString>Kevin Knight. 1999. Decoding complexity in wordreplacement translation models. Computational Linguistics, 25:607–615.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Pharaoh: A beam search decoder for phrased-based statistical machine translation.</title>
<date>2004</date>
<booktitle>In Proc. of AMTA,</booktitle>
<pages>115--124</pages>
<contexts>
<context position="16680" citStr="Koehn, 2004" startWordPosition="2973" endWordPosition="2974">IP] [NN2 .] grow ��[. IP] [NN2 . of the vote] From the above definition, we can find that there may be an ambiguity about whether to use a complete action or a grow action. Similarly, predict actions must select a viable prefix form the set for a node. For example in step 5, although we select to perform complete with r4 in the example, r7 is applicable, too. In our implementation, if both r4 and r7 are applicable, we apply them both to generate two seperate hypotheses. To limit the exponential explosion of hypotheses (Knight, 1999), we use beam search over bins of similar partial hypotheses (Koehn, 2004). 1195 IP Figure 4: The translation forest composed of applicable CFG rules for the partial derivation of step 3 in Figure 3. 3.4 Future Cost Partial derivations covering different tree nodes may be grouped in the same bin for beam pruning5. In order to perform more accurate pruning, we take into consideration future cost, the cost of the uncovered part. The merit of a derivation is the covered cost (the cost of the covered part) plus the future cost. We borrow ideas from the Inside-Outside algorithm (Charniak and Johnson, 2005; Huang, 2008; Mi et al., 2008) to compute the merit. In our algori</context>
<context position="21697" citStr="Koehn, 2004" startWordPosition="3885" endWordPosition="3886"> node on it (called the end node) and get the segment from the root node to the end node as a partial path, so the length of the partial path is not definite with a maximum of d. If the length is d′(d′ &lt; d), then the number of partial paths is (cr)d′. Besides, we use the rightest g − 1 words to signature each partial path, so we can get (cr)d′|V |9−1 states. For each state, the number of viable prefixes produced by predict operation is cd−d′, so the total time complexity is f = O((cr)d′|V |9− O(cdrd′|V |9−1) = O((cr)d|V |9−1). 3.7 Beam Search To make decoding tractable, we employ beam search (Koehn, 2004) and choose “binning” as follows: hypotheses covering the same number of source words are grouped in a bin. When expanding a hypothesis in a beam (bin), we take series of actions until new terminals are appended to the hypothesis, then add the new hypothesis to the corresponding beam. Figure 3 shows the number of source words each hypothesis covers. Among the actions, only the scan action changes the number of source words each hypothesis covers. Although the complete action does not change source word number, it changes the covered cost of hypotheses. So in our implementation, we take scan an</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Pharaoh: A beam search decoder for phrased-based statistical machine translation. In Proc. of AMTA, pages 115–124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Tree-tostring alignment template for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING-ACL,</booktitle>
<pages>609--616</pages>
<contexts>
<context position="4839" citStr="Liu et al., 2006" startWordPosition="710" endWordPosition="713">od uses a top-down depth-first search, with a mechanism for early elimation of some rules which lead to deadends in the search. Huang and Mi (2010)’s method was shown to outperform the traditional post-ordertraversal decoding algorithm, considering fewer hypotheses and thus decoding much faster at the same level of performance. However their algorithm used a very rough estimate of future cost, resulting in more search errors than our approach. Our experiments show that compared with the Earley-style left-to-right decoding (Huang and Mi, 2010) and the traditional post-order-traversal decoding (Liu et al., 2006) algorithms, our algorithm achieves a significant improvement on search capacity and better translation performance at the same level of speed. 2 Background A typical tree-to-string system (Liu et al., 2006; Huang et al., 2006) searches through a 1-best source parse tree for the best derivation. It transduces the source tree into a target-language string using a Synchronous Tree Substitution Grammar (STSG). The grammar rules are extracted from bilingual word alignments using the GHKM algorithm (Galley et al., 2004). We will briefly review the traditional decoding algorithm (Liu et al., 2006) a</context>
<context position="24208" citStr="Liu et al. (2006)" startWordPosition="4308" endWordPosition="4311"> add BTG constraints to phrase-based models. This algorithm constructs a BTG tree in a reduce-eager manner while the algorithm in this paper searches for a best derivation which must be derived from the source tree. Galley and Manning (2008) use the shift-reduce algorithm to conduct hierarchical phrase reordering so as to capture long-distance reordering. This algorithm shows good performance on phrase-based models, but can not be applied to syntax-based models directly. 5 Experiments In the experiments, we use two baseline systems: our in-house tree-to-string decoder implemented according to Liu et al. (2006) (denoted as traditional) and the Earley-style top-down decoder implemented according to Huang and Mi (2010) (denoted as topdown), respectively. We compare our bottom-up left-to-right decoder (denoted as bottom-up) with the baseline in terms of performance, translation quality and decoding speed with different beam sizes, and search capacity. Lastly, we show the influence of future cost. All systems are implemented in C++. 1cd−d′) = 1197 5.1 Data Setup We used the FBIS corpus consisting of about 250K Chinese-English sentence pairs as the training set. We aligned the sentence pairs using the GI</context>
</contexts>
<marker>Liu, Liu, Lin, 2006</marker>
<rawString>Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-tostring alignment template for statistical machine translation. In Proceedings of COLING-ACL, pages 609– 616, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haitao Mi</author>
<author>Liang Huang</author>
<author>Qun Liu</author>
</authors>
<title>Forestbased translation.</title>
<date>2008</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>192--199</pages>
<contexts>
<context position="17244" citStr="Mi et al., 2008" startWordPosition="3067" endWordPosition="3070">ver bins of similar partial hypotheses (Koehn, 2004). 1195 IP Figure 4: The translation forest composed of applicable CFG rules for the partial derivation of step 3 in Figure 3. 3.4 Future Cost Partial derivations covering different tree nodes may be grouped in the same bin for beam pruning5. In order to perform more accurate pruning, we take into consideration future cost, the cost of the uncovered part. The merit of a derivation is the covered cost (the cost of the covered part) plus the future cost. We borrow ideas from the Inside-Outside algorithm (Charniak and Johnson, 2005; Huang, 2008; Mi et al., 2008) to compute the merit. In our algorithm, the merit of a derivation is just the Viterbi inside cost Q of the root node calculated with the derivations continuing from the current derivation. Given a partial derivation, we calculate its future cost by searching through the translation forest defined by all applicable CFG rules. Figure 4 shows the translation forest for the derivation of step 3. We calculate the future cost for each node as follows: given a node v, we define its cost function f(v) as f(v) = { 1 v is completed lm(v) v is a terminal string maxrER„ f(r)HIlErhs(r) f(7) otherwise wher</context>
</contexts>
<marker>Mi, Huang, Liu, 2008</marker>
<rawString>Haitao Mi, Liang Huang, and Qun Liu. 2008. Forestbased translation. In Proc. of ACL, pages 192–199.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frans J Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<pages>29--19</pages>
<contexts>
<context position="24840" citStr="Och and Ney, 2003" startWordPosition="4406" endWordPosition="4409">raditional) and the Earley-style top-down decoder implemented according to Huang and Mi (2010) (denoted as topdown), respectively. We compare our bottom-up left-to-right decoder (denoted as bottom-up) with the baseline in terms of performance, translation quality and decoding speed with different beam sizes, and search capacity. Lastly, we show the influence of future cost. All systems are implemented in C++. 1cd−d′) = 1197 5.1 Data Setup We used the FBIS corpus consisting of about 250K Chinese-English sentence pairs as the training set. We aligned the sentence pairs using the GIZA++ toolkit (Och and Ney, 2003) and extracted tree-tostring rules according to the GHKM algorithm (Galley et al., 2004). We used the SRILM toolkit (Stolcke, 2002) to train a 4-gram language model on the Xinhua portion of the GIGAWORD corpus. We used the 2002 NIST MT Chinese-English test set (571 sentences) as the development set and the 2005 NIST MT Chinese-English test set (1082 sentences) as the test set. We evaluated translation quality using BLEU-metric (Papineni et al., 2002) with case-insensitive n-gram matching up to n = 4. We used the standard minimum error rate training (Och, 2003) to tune feature weights to maximi</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Frans J. Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29:19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frans J Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>160--167</pages>
<contexts>
<context position="25406" citStr="Och, 2003" startWordPosition="4505" endWordPosition="4506">using the GIZA++ toolkit (Och and Ney, 2003) and extracted tree-tostring rules according to the GHKM algorithm (Galley et al., 2004). We used the SRILM toolkit (Stolcke, 2002) to train a 4-gram language model on the Xinhua portion of the GIGAWORD corpus. We used the 2002 NIST MT Chinese-English test set (571 sentences) as the development set and the 2005 NIST MT Chinese-English test set (1082 sentences) as the test set. We evaluated translation quality using BLEU-metric (Papineni et al., 2002) with case-insensitive n-gram matching up to n = 4. We used the standard minimum error rate training (Och, 2003) to tune feature weights to maximize BLEU score on the development set. 5.2 Performance Comparison Our bottom-up left-to-right decoder employs the same features as the traditional decoder: rule probability, lexical probability, language model probability, rule count and word count. In order to compare them fairly, we used the same beam size which is 20 and employed cube pruning technique (Huang and Chiang, 2005). We show the results in Table 3. From the results, we can see that the bottom-up decoder outperforms top-down decoder and traditional decoder by 1.1 and 0.8 BLEU points respectively an</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Frans J. Och. 2003. Minimum error rate training in statistical machine translation. In Proc. of ACL, pages 160–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>311--318</pages>
<contexts>
<context position="25294" citStr="Papineni et al., 2002" startWordPosition="4484" endWordPosition="4487"> the FBIS corpus consisting of about 250K Chinese-English sentence pairs as the training set. We aligned the sentence pairs using the GIZA++ toolkit (Och and Ney, 2003) and extracted tree-tostring rules according to the GHKM algorithm (Galley et al., 2004). We used the SRILM toolkit (Stolcke, 2002) to train a 4-gram language model on the Xinhua portion of the GIGAWORD corpus. We used the 2002 NIST MT Chinese-English test set (571 sentences) as the development set and the 2005 NIST MT Chinese-English test set (1082 sentences) as the test set. We evaluated translation quality using BLEU-metric (Papineni et al., 2002) with case-insensitive n-gram matching up to n = 4. We used the standard minimum error rate training (Och, 2003) to tune feature weights to maximize BLEU score on the development set. 5.2 Performance Comparison Our bottom-up left-to-right decoder employs the same features as the traditional decoder: rule probability, lexical probability, language model probability, rule count and word count. In order to compare them fairly, we used the same beam size which is 20 and employed cube pruning technique (Huang and Chiang, 2005). We show the results in Table 3. From the results, we can see that the b</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of ACL, pages 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>Srilm-an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proc. of ICSLP.</booktitle>
<contexts>
<context position="24971" citStr="Stolcke, 2002" startWordPosition="4430" endWordPosition="4432">ompare our bottom-up left-to-right decoder (denoted as bottom-up) with the baseline in terms of performance, translation quality and decoding speed with different beam sizes, and search capacity. Lastly, we show the influence of future cost. All systems are implemented in C++. 1cd−d′) = 1197 5.1 Data Setup We used the FBIS corpus consisting of about 250K Chinese-English sentence pairs as the training set. We aligned the sentence pairs using the GIZA++ toolkit (Och and Ney, 2003) and extracted tree-tostring rules according to the GHKM algorithm (Galley et al., 2004). We used the SRILM toolkit (Stolcke, 2002) to train a 4-gram language model on the Xinhua portion of the GIGAWORD corpus. We used the 2002 NIST MT Chinese-English test set (571 sentences) as the development set and the 2005 NIST MT Chinese-English test set (1082 sentences) as the test set. We evaluated translation quality using BLEU-metric (Papineni et al., 2002) with case-insensitive n-gram matching up to n = 4. We used the standard minimum error rate training (Och, 2003) to tune feature weights to maximize BLEU score on the development set. 5.2 Performance Comparison Our bottom-up left-to-right decoder employs the same features as t</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. Srilm-an extensible language modeling toolkit. In Proc. of ICSLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taro Watanabe</author>
<author>Hajime Tsukada</author>
<author>Hideki Isozaki</author>
</authors>
<title>Left-to-right target generation for hierarchical phrase-based translation.</title>
<date>2006</date>
<booktitle>In Proc. of COLING,</booktitle>
<pages>777--784</pages>
<contexts>
<context position="22924" citStr="Watanabe et al. (2006)" startWordPosition="4103" endWordPosition="4106">plete as “closure” actions. That is to say, once there are some complete actions after a scan action, we finish all the compete actions until the next action is grow. The predict and grow actions decide which rules can be used to expand hypotheses next, so we update the applicable rule set during these two actions. Given a source sentence with n words, we maintain n beams, and let each beam hold b hypotheses at most. Besides, we prune viable prefixes of each node up to u, so each hypothesis can expand to u new hypotheses at most, so the time complexity of beam search is O(nub). 4 Related Work Watanabe et al. (2006) present a novel Earleystyle top-down decoding algorithm for hierarchical phrase-based model (Chiang, 2005). Their framework extracts Greibach Normal Form rules only, which always has at least one terminal on the left of each rule, and discards other rules. Dyer and Resnik (2010) describe a translation model that combines the merits of syntax-based models and phrase-based models. Their decoder works in two passes: for first pass, the decoder collects a context-free forest and performs tree-based source reordering without a LM. For the second pass, the decoder adds a LM and performs bottomup CK</context>
</contexts>
<marker>Watanabe, Tsukada, Isozaki, 2006</marker>
<rawString>Taro Watanabe, Hajime Tsukada, and Hideki Isozaki. 2006. Left-to-right target generation for hierarchical phrase-based translation. In Proc. of COLING, pages 777–784.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
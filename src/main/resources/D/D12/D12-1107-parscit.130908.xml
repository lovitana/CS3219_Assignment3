<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.740961">
Language Model Rest Costs and Space-Efficient Storage
</title>
<author confidence="0.957318">
Kenneth Heafield*,† Philipp Koehn† Alon Lavie*
</author>
<affiliation confidence="0.9372935">
* Language Technologies Institute
Carnegie Mellon University
</affiliation>
<address confidence="0.6700385">
5000 Forbes Avenue
Pittsburgh, PA 15213, USA
</address>
<email confidence="0.982412">
{heafield,alavie}@cs.cmu.edu
</email>
<affiliation confidence="0.6155565">
† School of Informatics
University of Edinburgh
</affiliation>
<address confidence="0.9513155">
10 Crichton Street
Edinburgh EH8 9AB, UK
</address>
<email confidence="0.995275">
pkoehn@inf.ed.ac.uk
</email>
<sectionHeader confidence="0.984641" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999887290322581">
Approximate search algorithms, such as cube
pruning in syntactic machine translation, rely
on the language model to estimate probabili-
ties of sentence fragments. We contribute two
changes that trade between accuracy of these
estimates and memory, holding sentence-level
scores constant. Common practice uses lower-
order entries in an N-gram model to score
the first few words of a fragment; this vio-
lates assumptions made by common smooth-
ing strategies, including Kneser-Ney. Instead,
we use a unigram model to score the first
word, a bigram for the second, etc. This im-
proves search at the expense of memory. Con-
versely, we show how to save memory by col-
lapsing probability and backoff into a single
value without changing sentence-level scores,
at the expense of less accurate estimates for
sentence fragments. These changes can be
stacked, achieving better estimates with un-
changed memory usage. In order to interpret
changes in search accuracy, we adjust the pop
limit so that accuracy is unchanged and re-
port the change in CPU time. In a German-
English Moses system with target-side syntax,
improved estimates yielded a 63% reduction
in CPU time; for a Hiero-style version, the
reduction is 21%. The compressed language
model uses 26% less RAM while equivalent
search quality takes 27% more CPU. Source
code is released as part of KenLM.
</bodyText>
<sectionHeader confidence="0.992474" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.990828666666667">
Language model storage is typically evaluated in
terms of speed, space, and accuracy. We introduce
a fourth dimension, rest cost quality, that captures
how well the model scores sentence fragments for
purposes of approximate search. Rest cost quality is
distinct from accuracy in the sense that the score of
a complete sentence is held constant. We first show
how to improve rest cost quality over standard prac-
tice by using additional space. Then, conversely, we
show how to compress the language model by mak-
ing a pessimistic rest cost assumption1.
Language models are designed to assign probabil-
ity to sentences. However, approximate search algo-
rithms use estimates for sentence fragments. If the
language model has order N (an N-gram model),
then the first N − 1 words of the fragment have in-
complete context and the last N − 1 words have not
been completely used as context. Our baseline is
common practice (Koehn et al., 2007; Dyer et al.,
2010; Li et al., 2009) that uses lower-order entries
from the language model for the first words in the
fragment and no rest cost adjustment for the last few
words. Formally, the baseline estimate for sentence
fragment wk1 is
</bodyText>
<equation confidence="0.9789315">
k
n−1
ri pN (wn  |wn−N+1)
n=N
</equation>
<bodyText confidence="0.99963">
where each wn is a word and pN is an N-gram lan-
guage model.
The problem with the baseline estimate lies in
lower order entries pN(wn|wn−1
</bodyText>
<note confidence="0.5227915">
1 ). Commonly used
Kneser-Ney (Kneser and Ney, 1995) smoothing,
</note>
<bodyText confidence="0.822980666666667">
1Here, the term rest cost means an adjustment to the score of
a sentence fragment but not to whole sentences. The adjustment
may be good or bad for approximate search.
</bodyText>
<note confidence="0.5188405">
pN(wn|wn−1
1 )
N−1ri
n=1
</note>
<page confidence="0.382989">
1169
</page>
<note confidence="0.9478975">
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1169–1178, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.993691152542373">
including the modified version (Chen and Good-
man, 1998), assumes that a lower-order entry will
only be used because a longer match could not
be found2. Formally, these entries actually eval-
uate pN(wn|wn−1
1 , did not find wn0 ). For purposes
of scoring sentence fragments, additional context is
simply indeterminate, and the assumption may not
hold.
As an example, we built 5-gram and unigram lan-
guage models with Kneser-Ney smoothing on the
same data. Sentence fragments frequently begin
with “the”. Using a lower-order entry from the 5-
gram model, log10 p5(the) = −2.49417. The uni-
gram model does not condition on backing off, as-
signing log10 p1(the) = −1.28504. Intuitively, the
5-gram model is surprised, by more than an order of
magnitude, to see “the” without matching words that
precede it.
To remedy the situation, we train N language
models on the same data. Each model pn is an n-
gram model (it has order n). We then use pn to
score the nth word of a sentence fragment. Thus,
a unigram model scores the first word of a sentence
fragment, a bigram model scores the second word,
and so on until either the n-gram is not present in
the model or the first N −1 words have been scored.
Storing probabilities from these models requires one
additional value per n-gram in the model, except for
N-grams where this probability is already stored.
Conversely, we can lower memory consumption
relative to the baseline at the expense of poorer rest
costs. Baseline models store two entries per n-gram:
probability and backoff. We will show that the prob-
ability and backoff values in a language model can
be collapsed into a single value for each n-gram
without changing sentence probability. This trans-
formation saves memory by halving the number of
values stored per entry, but it makes rest cost esti-
mates worse. Specifically, the rest cost pessimisti-
cally assumes that the model will back off to uni-
grams immediately following the sentence fragment.
The two modifications can be used independently
or simultaneously. To measure the impact of their
different rest costs, we experiment with cube prun-
ing (Chiang, 2007) in syntactic machine transla-
2Other smoothing techniques, including Witten-Bell (Witten
and Bell, 1991), do not make this assumption.
tion. Cube pruning’s goal is to find high-scoring
sentence fragments for the root non-terminal in the
parse tree. It does so by going bottom-up in the parse
tree, searching for high-scoring sentence fragments
for each non-terminal. Within each non-terminal, it
generates a fixed number of high-scoring sentence
fragments; this is known as the pop limit. Increasing
the pop limit therefore makes search more accurate
but costs more time. By moderating the pop limit,
improved accuracy can be interpreted as a reduction
in CPU time and vice-versa.
</bodyText>
<sectionHeader confidence="0.999442" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999925617647059">
Vilar and Ney (2011) study several modifications to
cube pruning and cube growing (Huang and Chiang,
2007). Most relevant is their use of a class-based
language model for the first of two decoding passes.
This first pass is cheaper because translation alter-
natives are likely to fall into the same class. Entries
are scored with the maximum probability over class
members (thereby making them no longer normal-
ized). Thus, paths that score highly in this first pass
may contain high-scoring paths under the lexicalized
language model, so the second pass more fully ex-
plores these options. The rest cost estimates we de-
scribe here could be applied in both passes, so our
work is largely orthogonal.
Zens and Ney (2008) present rest costs for phrase-
based translation. These rest costs are based on fac-
tors external to the sentence fragment, namely out-
put that the decoder may generate in the future. Our
rest costs examine words internal to the sentence
fragment, namely the first and last few words. We
also differ by focusing on syntactic translation.
A wide variety of work has been done on language
model compression. While data structure compres-
sion (Raj and Whittaker, 2003; Heafield, 2011) and
randomized data structures (Talbot and Osborne,
2007; Guthrie and Hepple, 2010) are useful, here
we are concerned solely with the values stored by
these data structures. Quantization (Whittaker and
Raj, 2001; Federico and Bertoldi, 2006) uses less
bits to store each numerical value at the expense
of model quality, including scores of full sentences,
and is compatible with our approach. In fact, the
lower-order probabilities might be quantized further
than normal since these are used solely for rest cost
</bodyText>
<page confidence="0.479537">
1170
</page>
<bodyText confidence="0.999860222222222">
purposes. Our compression technique reduces stor-
age from two values, probability and backoff, to one
value, theoretically halving the bits per value (ex-
cept N-grams which all have backoff 1). This makes
the storage requirement for higher-quality modified
Kneser-Ney smoothing comparable to stupid back-
off (Brants et al., 2007). Whether to use one smooth-
ing technique or the other then becomes largely an
issue of training costs and quality after quantization.
</bodyText>
<sectionHeader confidence="0.998048" genericHeader="method">
3 Contribution
</sectionHeader>
<subsectionHeader confidence="0.999925">
3.1 Better Rest Costs
</subsectionHeader>
<bodyText confidence="0.999991625">
As alluded to in the introduction, the first few words
of a sentence fragment are typically scored us-
ing lower-order entries from an N-gram language
model. However, Kneser-Ney smoothing (Kneser
and Ney, 1995) conditions lower-order probabilities
on backing off. Specifically, lower-order counts are
adjusted to represent the number of unique exten-
sions an n-gram has:
</bodyText>
<equation confidence="0.968461333333333">
�
|{w0 : c(wn 0) &gt; 0} |if n &lt; N
a(wn 1 ) = c(wn 1) if n = N
</equation>
<bodyText confidence="0.993700789473684">
where c(wn1) is the number of times wn1 appears in
the training data3. This adjustment is also performed
for modified Kneser-Ney smoothing. The intuition
is based on the fact that the language model will
base its probability on the longest possible match. If
an N-gram was seen in the training data, the model
will match it fully and use the smoothed count. Oth-
erwise, the full N-gram was not seen in the train-
ing data and the model resorts to a shorter n-gram
match. Probability of this shorter match is based on
how often the n-gram is seen in different contexts.
Thus, these shorter n-gram probabilities are not rep-
resentative of cases where context is short simply
because additional context is unknown at the time of
scoring.
In some cases, we are able to determine that
the model will back off and therefore the lower-
order probability makes the appropriate assumption.
Specifically, if vwn1 does not appear in the model for
any word v, then computing p(wn|vwn−1
1 ) will al-
3Counts are not modified for n-grams bound to the begin-
ning of sentence, namely those with wl = &lt;s&gt;.
ways back off to wn−1
1 or fewer words4. This crite-
rion is the same as used to minimize the length of left
language model state (Li and Khudanpur, 2008) and
can be retrieved for each n-gram without using addi-
tional memory in common data structures (Heafield
et al., 2011).
Where it is unknown if the model will back off,
we use a language model of the same order to pro-
duce a rest cost. Specifically, there are N language
models, one of each order from 1 to N. The mod-
els are trained on the same corpus with the same
smoothing parameters to the extent that they apply.
We then compile these into one data structure where
each n-gram record has three values:
</bodyText>
<listItem confidence="0.999822">
1. Probability pn from the n-gram language
model
2. Probability pN from the N-gram language
model
3. Backoff b from the N-gram language model
</listItem>
<bodyText confidence="0.9989576">
For N-grams, the two probabilities are the same and
backoff is always 1, so only one value is stored.
Without pruning, the n-gram model contains the
same n-grams as the N-gram model. With prun-
ing, the two sets may be different, so we query the
n-gram model in the normal way to score every n-
gram in the N-gram model. The idea is that pn is the
average conditional probability that will be encoun-
tered once additional context becomes known. We
also tried more complicated estimates by addition-
ally interpolating upper bound, lower bound, and pN
with weights trained on cube pruning logs; none of
these improved results in any meaningful way.
Formalizing the above, let wk1 be a sentence frag-
ment. Choose the largest s so that vws1 appears in
the model for some v; equivalently ws1 is the left
state described in Li and Khudanpur (2008). The
4Usually, this happens because wl does not appear, though
it can also happen that wl appears but all vwl were removed
by pruning or filtering.
</bodyText>
<equation confidence="0.986948055555556">
1171
!
pN(wn|wn−1
1 ) ·
!
pN(wn|wn−1
1 ) · (1)
!
n−1
pN (wn  |wn−N+1)
while our improved estimate is
pn (wn  |)! ·
!
pN(wn|wn−1
1 ) · (2)
!
n−1
pN (wn  |wn−N+1)
</equation>
<bodyText confidence="0.999993615384615">
The difference between these equations is that pn is
used for words in the left state i.e. 1 &lt; n &lt; s.
We have also abused notation by using pN to denote
both probabilities stored explicitly in the model and
the model’s backoff-smoothed probabilities when
not present. It is not necessary to store backoffs for
pn because s was chosen such that all queried n-
grams appear in the model.
This modification to the language model improves
rest costs (and therefore quality or CPU time) at the
expense of using more memory to store pn. In the
next section, we do the opposite: make rest costs
worse to reduce storage size.
</bodyText>
<subsectionHeader confidence="0.999643">
3.2 Less Memory
</subsectionHeader>
<bodyText confidence="0.9956777">
Many language model smoothing strategies, includ-
ing modified Kneser-Ney smoothing, use the back-
off algorithm shown in Figure 1. Given an n-gram
wn1 , the backoff algorithm bases probability on as
much context as possible. Equivalently, it finds
the minimum f so that wnf is in the model then
uses p(wn|wn−1
f ) as a basis. Backoff penalties b
are charged because a longer match was not found,
forming the product
</bodyText>
<equation confidence="0.741280523809524">
p(wn|wn−1
1 ) = p(wn|wn−1
f )
Notably, the backoff penalties {b(wn−1
j )}n−1
j=1 are in-
dependent of wn, though which backoff penalties are
charged depends on f and therefore wn.
backoff +— 1
for f = 1 —* n do
if wnf is in the model then
return p(wn|wn−1
f ) · backoff
else
if wn−1
f is in the model then
backoff +— backoff · b(wn−1
f )
end if
end if
end for
</equation>
<figureCaption confidence="0.998042">
Figure 1: The baseline backoff algorithm to com-
</figureCaption>
<figure confidence="0.9068246">
pute p(wn|wn−1
1 ). It always terminates with a prob-
ability because even unknown words are treated as a
unigram.
for f = 1 —* n do
if wnf is in the model then
return q(wn|wn−1
f )
end if
end for
</figure>
<figureCaption confidence="0.999051">
Figure 2: The run-time pessimistic backoff algo-
</figureCaption>
<bodyText confidence="0.8620588">
rith to find q(wn wi —1
m ). It assumes that q has been
computed at model building time.
In order to save memory, we propose to account
for backoff in a different way, defining q
</bodyText>
<equation confidence="0.837775666666667">
p(wn|wn−1
f )Qn j=f b(wn j )
q(wn|wn−1
1 ) =
Qn−1
j=f b(wn−1
</equation>
<bodyText confidence="0.980633333333333">
j )
where again wnf is the longest matching entry in the
model. The idea is that q is a term in the telescop-
ing series that scores a sentence fragment, shown
in equation (1) or (2). The numerator pessimisti-
cally charges all backoff penalties, as if the next
word wn+1 will only match a unigram. When wn+1
is scored, the denominator of q(wn+1|wn1 ) cancels
out backoff terms that were wrongly charged. Once
these terms are canceled, all that is left is p, the cor-
rect backoff penalties, and terms on the edge of the
series.
</bodyText>
<figure confidence="0.9064378">
baseline estimate is
pb) Ys
(wk = n=1
N+1Y
n=s+1
k
Y
n=N
pr(wk1) = Ys
n=1
N+1Y
n=s+1
k
Y
n=N
b(wn−1
j ) (3)
f−1 Y
j=1
1172
</figure>
<figureCaption confidence="0.519413">
Proposition 1. The terms of q telescope. Formally,
</figureCaption>
<bodyText confidence="0.720365">
let wk 1 be a sentence fragment and f take the mini-
mum value so that wkf is in the model. Then,
</bodyText>
<equation confidence="0.841585">
q(wk 1) = p(wk1)
</equation>
<bodyText confidence="0.784165">
Finally, the conditional probability folds as desired
</bodyText>
<equation confidence="0.43914275">
q(wk1) = p(wk1)
k
Y
j=f
b(wkj )
Yk
j=f
b(wkj )
</equation>
<bodyText confidence="0.925061666666667">
Proof. By induction on k. When k = 1, f = 1 since
the word w1 is either in the vocabulary or mapped to
&lt;unk&gt; and treated like a unigram.
</bodyText>
<equation confidence="0.971775571428571">
1
p(w1) Qj=1 b(w1 j )
q(w1) = Q0 = p(w1)b(w1)
j=1 b(w0 j )
Fork &gt; 1,
q(wk1) = q(wk−1
1 )q(wk|wk−1
1 )
q(wk−1
1 )p(wk|wk−1
f ) Qkj=f b(wkj )
Qk−1
j=f b(wk−1
j )
</equation>
<bodyText confidence="0.999982">
where f has the lowest value such that wkf is in the
model. Applying the inductive hypothesis to expand
</bodyText>
<equation confidence="0.948155888888889">
q(wk−1
1 ), we obtain
�Qk−1 �
p(wk−1
1 ) j=e b(wk−1 f ) Qk
j ) p(wk|wk−1 j=f b(wkj )
Qk−1
j=f b(wk−1
j )
</equation>
<bodyText confidence="0.914655">
where e has the lowest value such that wk−1
</bodyText>
<equation confidence="0.755272">
e is in the
</equation>
<bodyText confidence="0.964775">
model. The backoff terms cancel to yield
By construction of e, wk−1
</bodyText>
<equation confidence="0.945773076923077">
j isnot in the model for all
j &lt; e. Hence, b(wk−1
j ) = 1 implicitly for all j &lt; e.
Multiplying by 1,
⎛ ⎞
f−1 Y Yk
p(wk−1
1 ) ⎝ b(wk−1
j ) ⎠ p(wk|wk−1
f ) b(wkj )
j=1 j=f
Recognizing the backoff equation (3) to simplify,
p(w1−1)p(wk  |w1 −1 )
</equation>
<bodyText confidence="0.999959333333333">
We note that entries ending in &lt;/s&gt; have back-
off 1, so it follows from Proposition 1 that sentence-
level scores are unchanged.
</bodyText>
<equation confidence="0.766432">
q(&lt;s&gt; wk1 &lt;/s&gt;) = p(&lt;s&gt; wk1 &lt;/s&gt;)
</equation>
<bodyText confidence="0.971988083333333">
Proposition 1 characterizes q as a pessimistic rest
cost on sentence fragments that scores sentences in
exactly the same way as the baseline using p and
b. To save memory, we simply store q in lieu of
p and b. Compared with the baseline, this halves
number of values from two to one float per n-gram,
except N-grams that already have one value. The
impact of this reduction is substantial, as seen in
Section 4.3. Run-time scoring is also simplified
as shown in Figure 2 since the language model lo-
cates the longest match wnf then returns the value
q(wn|wn−1
</bodyText>
<equation confidence="0.750679">
1 ) = q(wn|wn−1
</equation>
<bodyText confidence="0.999641434782609">
f ) without any calcula-
tion or additional lookup. Baseline language mod-
els either retrieve backoffs values with additional
lookups (Stolcke, 2002; Federico et al., 2008) or
modify the decoder to annotate sentence fragments
with backoff information (Heafield, 2011); we have
effectively moved this step to preprocessing. The
disadvantage is that q is not a proper probability and
it produces worse rest costs than does the baseline.
Language models are actually applied at two
points in syntactic machine translation: scoring lexi-
cal items in grammar rules and during cube pruning.
Grammar scoring is an offline and embarrassingly
parallel process where memory is not as tight (since
the phrase table is streamed) and fewer queries
are made, so slow non-lossy compression and even
network-based sharding can be used. We there-
fore use an ordinary language model for grammar
scoring and only apply the compressed model dur-
ing cube pruning. Grammar scoring impacts gram-
mar pruning (by selecting only top-scoring grammar
rules) and the order in which rules are tried during
cube pruning.
</bodyText>
<figure confidence="0.938271272727273">
=
⎛ ⎞
f−1 Y Yk
p(wk−1
1 ) ⎝ b(wk−1
j ) ⎠ p(wk|wk−1
f ) b(wk j )j=ej=f
Yk
j=f
b(wkj )
1173
</figure>
<subsectionHeader confidence="0.986107">
3.3 Combined Scheme
</subsectionHeader>
<bodyText confidence="0.999983166666667">
Our two language model modifications can be triv-
ially combined by using lower-order probabilities on
the left of a fragment and by charging all backoff
penalties on the right of a fragment. The net result is
a language model that uses the same memory as the
baseline but has better rest cost estimates.
</bodyText>
<sectionHeader confidence="0.999015" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999995923076923">
To measure the impact of different rest costs, we
use the Moses chart decoder (Koehn et al., 2007)
for the WMT 2011 German-English translation task
(Callison-Burch et al., 2011). Using the Moses
pipeline, we trained two syntactic German-English
systems, one with target-side syntax and the other
hierarchical with unlabeled grammar rules (Chiang,
2007). Grammar rules were extracted from Europarl
(Koehn, 2005) using the Collins parser (Collins,
1999) for syntax on the English side. The language
model interpolates, on the WMT 2010 test set, sep-
arate models built on Europarl, news commentary,
and the WMT news data for each year. Models were
built and interpolated using SRILM (Stolcke, 2002)
with modified Kneser-Ney smoothing (Kneser and
Ney, 1995; Chen and Goodman, 1998) and the de-
fault pruning settings. In all scenarios, the primary
language model has order 5. For lower-order rest
costs, we also built models with orders 1 through 4
then used the n-gram model to score n-grams in the
5-gram model. Feature weights were trained with
MERT (Och, 2003) on the baseline using a pop limit
of 1000 and 100-best output. Since final feature val-
ues are unchanged, we did not re-run MERT in each
condition. Measurements were collected by running
the decoder on the 3003-sentence test set.
</bodyText>
<subsectionHeader confidence="0.98947">
4.1 Rest Costs as Prediction
</subsectionHeader>
<bodyText confidence="0.999809125">
Scoring the first few words of a sentence fragment
is a prediction task. The goal is to predict what
the probability will be when more context becomes
known. In order to measure performance on this
task, we ran the decoder on the hierarchical system
with a pop limit of 1000. Every time more context
became known, we logged5 the prediction error (es-
timated log probability minus updated log probabil-
</bodyText>
<footnote confidence="0.358719">
5Logging was only enabled for this experiment.
</footnote>
<table confidence="0.996286833333333">
n Mean Lower Baseline
Bias MSE Var Bias MSE Var
1 -3.21 .10 .84 .83 -.12 .87 .86
2 -2.27 .04 .18 .17 -.14 .23 .24
3 -1.80 .02 .07 .07 -.09 .10 .09
4 -1.29 .01 .04 .04 -.10 .09 .08
</table>
<tableCaption confidence="0.99604">
Table 1: Bias (mean error), mean squared error, and
</tableCaption>
<bodyText confidence="0.997434052631579">
variance (of the error) for the lower-order rest cost
and the baseline. Error is the estimated log prob-
ability minus the final probability. Statistics were
computed separately for the first word of a fragment
(n = 1), the second word (n = 2), etc. The lower-
order estimates are better across the board, reducing
error in cube pruning. All numbers are in log base
ten, as is standard for ARPA-format language mod-
els. Statistics were only collected for words with
incomplete context.
ity) for both lower-order rest costs and the baseline.
Table 1 shows the results.
Cube pruning uses relative scores, so bias mat-
ters less, though positive bias will favor rules with
more arity. Variance matters the most because lower
variance means cube pruning’s relative rankings are
more accurate. Our lower-order rest costs are bet-
ter across the board in terms of absolute bias, mean
squared error, and variance.
</bodyText>
<subsectionHeader confidence="0.998872">
4.2 Pop Limit Trade-Offs
</subsectionHeader>
<bodyText confidence="0.998899352941176">
The cube pruning pop limit is a trade-off between
search accuracy and CPU time. Here, we mea-
sure how our rest costs improve (or degrade) that
trade-off. Search accuracy is measured by the aver-
age model score of single-best translations. Model
scores are scale-invariant and include a large con-
stant factor; higher is better. We also measure over-
all performance with uncased BLEU (Papineni et al.,
2002). CPU time is the sum of user and system time
used by Moses divided by the number of sentences
(3003). Timing includes time to load, though files
were forced into the disk cache in advance. Our test
machine has 64 GB of RAM and 32 cores. Results
are shown in Figures 3 and 4.
Lower-order rest costs perform better in both sys-
tems, reaching plateau model scores and BLEU with
less CPU time. The gain is much larger for tar-
</bodyText>
<table confidence="0.9556955">
1174
Pop Baseline Lower Order CPU Pessimistic CPU Combined
CPU Model BLEU CPU Model BLEU Model BLEU Model BLEU
2 3.29 -105.56 20.45 3.68 -105.44 20.79 3.74 -105.62 20.01 3.18 -105.49 20.43
10 5.21 -104.74 21.13 5.50 -104.72 21.26 5.43 -104.77 20.85 5.67 -104.75 21.10
50 23.30 -104.31 21.36 23.51 -104.24 21.38 23.68 -104.33 21.25 24.29 -104.22 21.34
500 54.61 -104.25 21.33 55.92 -104.15 21.38 54.23 -104.26 21.31 55.74 -104.15 21.40
700 64.08 -104.25 21.34 87.02 -104.14 21.42 68.74 -104.25 21.29 78.84 -104.15 21.41
</table>
<figure confidence="0.665919666666667">
0 10 20 30 40 50 60 70 80 90 0 10 20 30 40 50 60 70 80 90
CPU seconds/sentence CPU seconds/sentence
(b) Model and BLEU scores near the plateau.
</figure>
<figureCaption confidence="0.990634">
Figure 3: Target-syntax performance. CPU time and model score are averaged over 3003 sentences.
</figureCaption>
<figure confidence="0.997077115384615">
(a) Numerical results for select pop limits.
Average model score
-104.1
-104.2
-104.3
-104.4
-104.5
-104.6
Lower
Combined
Baseline
Pessimistic
Uncased BLEU
21.35
21.25
21.15
21.05
21.4
21.3
21.2
21.1
21
Lower
Combined
Baseline
Pessimistic
</figure>
<bodyText confidence="0.9999285625">
get syntax, where a pop limit of 50 outperforms the
baseline with pop limit 700. CPU time per sen-
tence is reduced to 23.5 seconds from 64.0 seconds,
a 63.3% reduction. The combined setting, using the
same memory as the baseline, shows a similar 62.1%
reduction in CPU time. We attribute this differ-
ence to improved grammar rule scoring that impacts
pruning and sorting. In the target syntax model,
the grammar is not saturated (i.e. less pruning will
still improve scores) but we nonetheless prune for
tractability reasons. The lower-order rest costs are
particularly useful for grammar pruning because lex-
ical items are typically less than five words long (and
frequently only word).
The hierarchical grammar is nearly saturated with
respect to grammar pruning, so improvement there is
due mostly to better search. In the hierarchical sys-
tem, peak BLEU 22.34 is achieved under the lower-
order condition with pop limits 50 and 200, while
other scenarios are still climbing to the plateau. With
a pop limit of 1000, the baseline’s average model
score is -101.3867. Better average models scores
are obtained from the lower-order model with pop
limit 690 using 79% of baseline CPU, the combined
model with pop limit 900 using 97% CPU, and the
pessimistic model with pop limit 1350 using 127%
CPU.
Pessimistic compression does worsen search, re-
quiring 27% more CPU in the hierarchical system to
achieve the same quality. This is worthwhile to fit
large-scale language models in memory, especially
if the alternative is a remote language model.
</bodyText>
<subsectionHeader confidence="0.999221">
4.3 Memory Usage
</subsectionHeader>
<bodyText confidence="0.99985075">
Our rest costs add a value (for lower-order prob-
abilities) or remove a value (pessimistic compres-
sion) for each n-gram except those of highest order
(n = N). The combined condition adds one value
</bodyText>
<table confidence="0.972100909090909">
1175
Pop Baseline Lower Order CPU Pessimistic CPU Combined
CPU Model BLEU CPU Model BLEU Model BLEU Model BLEU
2 2.96 -101.85 21.19 2.44 -101.80 21.63 2.71 -101.90 20.85 3.05 -101.84 21.37
10 2.80 -101.60 21.90 2.42 -101.58 22.20 2.95 -101.63 21.74 2.69 -101.60 21.98
50 3.02 -101.47 22.18 3.11 -101.46 22.34 3.46 -101.48 22.08 2.67 -101.47 22.14
690 10.83 -101.39 22.28 11.45 -101.39 22.25 10.88 -101.40 22.25 11.19 -101.39 22.23
900 13.41 -101.39 22.27 14.00 -101.38 22.24 13.38 -101.39 22.25 14.09 -101.39 22.22
1000 14.50 -101.39 22.27 15.17 -101.38 22.25 15.09 -101.39 22.26 15.23 -101.39 22.23
1350 18.52 -101.38 22.27 19.16 -101.38 22.23 18.46 -101.39 22.25 18.61 -101.38 22.23
5000 59.67 -101.38 22.24 61.41 -101.38 22.22 59.76 -101.38 22.27 61.38 -101.38 22.22
</table>
<figure confidence="0.840750333333333">
0 5 10 15 20 25 0 5 10 15 20 25
CPU seconds/sentence CPU seconds/sentence
(b) Model and BLEU scores near the plateau.
</figure>
<figureCaption confidence="0.975536">
Figure 4: Hierarchical system performance. All values are averaged over 3003 sentences.
</figureCaption>
<figure confidence="0.998885275862069">
(a) Numerical results for select pop limits.
Average model score
-101.385
-101.395
-101.405
-101.415
-101.38
-101.39
-101.41
-101.42
-101.4
Lower
Combined
Baseline
Pessimistic
Uncased BLEU
22.35
22.25
22.15
22.05
21.95
22.3
22.2
22.1
22
Lower
Combined
Baseline
Pessimistic
</figure>
<bodyText confidence="0.9992949375">
and removes another, so it uses the same memory
as the baseline. The memory footprint of adding or
removing a value depends on the number of such n-
grams, the underlying data structure, and the extent
of quantization. Our test language model has 135
million n-grams for n &lt; 5 and 56 million 5-grams.
Memory usage was measured for KenLM data struc-
tures (Heafield, 2011) and minimal perfect hashing
(Guthrie and Hepple, 2010). For minimal perfect
hashing, we assume the Compress, Hash and Dis-
place algorithm (Belazzougui et al., 2008) with 8-bit
signatures and 8-bit quantization. Table 2 shows the
results. Storage size of the smallest model is reduced
by 26%, bringing higher-quality smoothed models
in line with stupid backoff models that also store one
value per n-gram.
</bodyText>
<table confidence="0.9897882">
Structure Baseline Change %
Probing 4,072 517 13%
Trie 2,647 506 19%
8-bit quantized trie 1,236 140 11%
8-bit minimal perfect hash 540 140 26%
</table>
<tableCaption confidence="0.990455">
Table 2: Size in megabytes of our language model,
</tableCaption>
<bodyText confidence="0.90413575">
excluding operating system overhead. Change is the
cost of adding an additional value to store lower-
order probabilities. Equivalently, it is the savings
from pessimistic compression.
</bodyText>
<page confidence="0.678837">
1176
</page>
<sectionHeader confidence="0.996769" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.9999832">
Our techniques reach plateau-level BLEU scores
with less time or less memory. Efficiently stor-
ing lower-order probabilities and using them as rest
costs improves both cube pruning (21% CPU reduc-
tion in a hierarchical system) and model filtering
(net 63% CPU time reduction with target syntax) at
the expense of 13-26% more RAM for the language
model. This model filtering improvement is surpris-
ing both in the impact relative to changing the pop
limit and simplicity of implementation, since it can
be done offline. Compressing the language model to
halve the number of values per n-gram (except N-
grams) results in a 13-26% reduction in RAM with
26% over the smallest model, costing 27% more
CPU and leaving overall sentence scores unchanged.
This compression technique is likely to have more
general application outside of machine translation,
especially where only sentence-level scores are re-
quired. Source code is being released6 under the
LGPL as part of KenLM (Heafield, 2011).
</bodyText>
<sectionHeader confidence="0.990991" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999944545454546">
This work was supported by the National Sci-
ence Foundation under grants DGE-0750271, IIS-
0713402, and IIS-0915327; by the EuroMatrixPlus
project funded by the European Commission (7th
Framework Programme), and by the DARPA GALE
program. Benchmarks were run on Trestles at the
San Diego Supercomputer Center under allocation
TG-CCR110017. Trestles is part of the Extreme
Science and Engineering Discovery Environment
(XSEDE), which is supported by National Science
Foundation grant number OCI-1053575.
</bodyText>
<sectionHeader confidence="0.982259" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997881970149254">
Djamal Belazzougui, Fabiano C. Botelho, and Martin Di-
etzfelbinger. 2008. Hash, displace, and compress. In
Proceedings of the 35th international colloquium on
Automata, Languages and Programming (ICALP ’08),
pages 385–396.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.
Och, and Jeffrey Dean. 2007. Large language mod-
els in machine translation. In Proceedings of the 2007
Joint Conference on Empirical Methods in Natural
6http://kheafield.com/code/kenlm/
Language Processing and Computational Language
Learning, pages 858–867, June.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Omar Zaidan. 2011. Findings of the 2011 work-
shop on statistical machine translation. In Proceedings
of the Sixth Workshop on Statistical Machine Transla-
tion, pages 22–64, Edinburgh, Scotland, July. Associ-
ation for Computational Linguistics.
Stanley Chen and Joshua Goodman. 1998. An empirical
study of smoothing techniques for language modeling.
Technical Report TR-10-98, Harvard University, Au-
gust.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33:201–228, June.
Michael Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Univer-
sity of Pennsylvania.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Johnathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Setiawan,
Vladimir Eidelman, and Philip Resnik. 2010. cdec:
A decoder, alignment, and learning framework for
finite-state and context-free translation models. In
Proceedings of the ACL 2010 System Demonstrations,
ACLDemos ’10, pages 7–12.
Marcello Federico and Nicola Bertoldi. 2006. How
many bits are needed to store probabilities for phrase-
based translation? In Proceedings of the Workshop on
Statistical Machine Translation, pages 94–101, New
York City, June.
Marcello Federico, Nicola Bertoldi, and Mauro Cettolo.
2008. IRSTLM: an open source toolkit for handling
large scale language models. In Proceedings of Inter-
speech, Brisbane, Australia.
David Guthrie and Mark Hepple. 2010. Storing the web
in memory: Space efficient language models with con-
stant time retrieval. In Proceedings of EMNLP 2010,
Los Angeles, CA.
Kenneth Heafield, Hieu Hoang, Philipp Koehn, Tetsuo
Kiso, and Marcello Federico. 2011. Left language
model state for syntactic machine translation. In Pro-
ceedings of the International Workshop on Spoken
Language Translation, San Francisco, CA, USA, De-
cember.
Kenneth Heafield. 2011. KenLM: Faster and smaller
language model queries. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, Edin-
burgh, UK, July. Association for Computational Lin-
guistics.
Liang Huang and David Chiang. 2007. Forest rescoring:
Faster decoding with integrated language models. In
Proceedings of the 45th Annual Meeting of the Asso-
ciation for Computational Linguistics, Prague, Czech
Republic.
1177
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In Pro-
ceedings of the IEEE International Conference on
Acoustics, Speech and Signal Processing, pages 181–
184.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Annual
Meeting of the Association for Computational Linguis-
tics (ACL), Prague, Czech Republic, June.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In Proceedings of MT
Summit.
Zhifei Li and Sanjeev Khudanpur. 2008. A scalable
decoder for parsing-based machine translation with
equivalent language model state maintenance. In Pro-
ceedings of the Second ACL Workshop on Syntax and
Structure in Statistical Translation (SSST-2), pages
10–18, Columbus, Ohio, June.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Sanjeev
Khudanpur, Lane Schwartz, Wren Thornton, Jonathan
Weese, and Omar Zaidan. 2009. Joshua: An open
source toolkit for parsing-based machine translation.
In Proceedings of the Fourth Workshop on Statistical
Machine Translation, pages 135–139, Athens, Greece,
March. Association for Computational Linguistics.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In ACL ’03: Pro-
ceedings of the 41st Annual Meeting on Association
for Computational Linguistics, pages 160–167, Mor-
ristown, NJ, USA. Association for Computational Lin-
guistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evalution of machine translation. In Proceedings 40th
Annual Meeting of the Association for Computational
Linguistics, pages 311–318, Philadelphia, PA, July.
Bhiksha Raj and Ed Whittaker. 2003. Lossless compres-
sion of language model structure and word identifiers.
In Proceedings of IEEE International Conference on
Acoustics, Speech and Signal Processing, pages 388–
391.
Andreas Stolcke. 2002. SRILM - an extensible language
modeling toolkit. In Proceedings of the Seventh Inter-
national Conference on Spoken Language Processing,
pages 901–904.
David Talbot and Miles Osborne. 2007. Randomised
language modelling for statistical machine translation.
In Proceedings ofACL, pages 512–519, Prague, Czech
Republic.
David Vilar and Hermann Ney. 2011. Cardinality prun-
ing and language model heuristics for hierarchical
phrase-based translation. Machine Translation, pages
1–38, November. DOI 10.1007/s10590-011-9119-4.
Ed Whittaker and Bhiksha Raj. 2001. Quantization-
based language model compression. In Proceedings
of EUROSPEECH, pages 33–36, September.
Ian H. Witten and Timothy C. Bell. 1991. The zero-
frequency problem: Estimating the probabilities of
novel events in adaptive text compression. IEEE
Transactions on Information Theory, 37(4):1085–
1094.
Richard Zens and Hermann Ney. 2008. Improvements in
dynamic programming beam search for phrase-based
statistical machine translation. In Proceedings of the
International Workshop on Spoken Language Transla-
tion (IWSLT), Honolulu, Hawaii, October.
</reference>
<page confidence="0.809062">
1178
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.135375">
<title confidence="0.998752">Language Model Rest Costs and Space-Efficient Storage</title>
<author confidence="0.48516">Technologies</author>
<affiliation confidence="0.916998">Carnegie Mellon</affiliation>
<address confidence="0.909009666666667">5000 Forbes Pittsburgh, PA 15213, of</address>
<affiliation confidence="0.78741">University of</affiliation>
<address confidence="0.516392">10 Crichton Edinburgh EH8 9AB,</address>
<email confidence="0.987598">pkoehn@inf.ed.ac.uk</email>
<abstract confidence="0.99692359375">Approximate search algorithms, such as cube pruning in syntactic machine translation, rely on the language model to estimate probabilities of sentence fragments. We contribute two changes that trade between accuracy of these estimates and memory, holding sentence-level scores constant. Common practice uses lowerentries in an model to score the first few words of a fragment; this violates assumptions made by common smoothing strategies, including Kneser-Ney. Instead, we use a unigram model to score the first word, a bigram for the second, etc. This improves search at the expense of memory. Conversely, we show how to save memory by collapsing probability and backoff into a single value without changing sentence-level scores, at the expense of less accurate estimates for sentence fragments. These changes can be stacked, achieving better estimates with unchanged memory usage. In order to interpret changes in search accuracy, we adjust the pop limit so that accuracy is unchanged and report the change in CPU time. In a German- English Moses system with target-side syntax, improved estimates yielded a 63% reduction in CPU time; for a Hiero-style version, the reduction is 21%. The compressed language model uses 26% less RAM while equivalent search quality takes 27% more CPU. Source code is released as part of KenLM.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Djamal Belazzougui</author>
<author>Fabiano C Botelho</author>
<author>Martin Dietzfelbinger</author>
</authors>
<title>Hash, displace, and compress.</title>
<date>2008</date>
<booktitle>In Proceedings of the 35th international colloquium on Automata, Languages and Programming (ICALP ’08),</booktitle>
<pages>385--396</pages>
<contexts>
<context position="26251" citStr="Belazzougui et al., 2008" startWordPosition="4531" endWordPosition="4534">ased BLEU 22.35 22.25 22.15 22.05 21.95 22.3 22.2 22.1 22 Lower Combined Baseline Pessimistic and removes another, so it uses the same memory as the baseline. The memory footprint of adding or removing a value depends on the number of such ngrams, the underlying data structure, and the extent of quantization. Our test language model has 135 million n-grams for n &lt; 5 and 56 million 5-grams. Memory usage was measured for KenLM data structures (Heafield, 2011) and minimal perfect hashing (Guthrie and Hepple, 2010). For minimal perfect hashing, we assume the Compress, Hash and Displace algorithm (Belazzougui et al., 2008) with 8-bit signatures and 8-bit quantization. Table 2 shows the results. Storage size of the smallest model is reduced by 26%, bringing higher-quality smoothed models in line with stupid backoff models that also store one value per n-gram. Structure Baseline Change % Probing 4,072 517 13% Trie 2,647 506 19% 8-bit quantized trie 1,236 140 11% 8-bit minimal perfect hash 540 140 26% Table 2: Size in megabytes of our language model, excluding operating system overhead. Change is the cost of adding an additional value to store lowerorder probabilities. Equivalently, it is the savings from pessimis</context>
</contexts>
<marker>Belazzougui, Botelho, Dietzfelbinger, 2008</marker>
<rawString>Djamal Belazzougui, Fabiano C. Botelho, and Martin Dietzfelbinger. 2008. Hash, displace, and compress. In Proceedings of the 35th international colloquium on Automata, Languages and Programming (ICALP ’08), pages 385–396.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
<author>Ashok C Popat</author>
<author>Peng Xu</author>
<author>Franz J Och</author>
<author>Jeffrey Dean</author>
</authors>
<title>Large language models in machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural 6http://kheafield.com/code/kenlm/</booktitle>
<contexts>
<context position="8401" citStr="Brants et al., 2007" startWordPosition="1363" endWordPosition="1366">d Bertoldi, 2006) uses less bits to store each numerical value at the expense of model quality, including scores of full sentences, and is compatible with our approach. In fact, the lower-order probabilities might be quantized further than normal since these are used solely for rest cost 1170 purposes. Our compression technique reduces storage from two values, probability and backoff, to one value, theoretically halving the bits per value (except N-grams which all have backoff 1). This makes the storage requirement for higher-quality modified Kneser-Ney smoothing comparable to stupid backoff (Brants et al., 2007). Whether to use one smoothing technique or the other then becomes largely an issue of training costs and quality after quantization. 3 Contribution 3.1 Better Rest Costs As alluded to in the introduction, the first few words of a sentence fragment are typically scored using lower-order entries from an N-gram language model. However, Kneser-Ney smoothing (Kneser and Ney, 1995) conditions lower-order probabilities on backing off. Specifically, lower-order counts are adjusted to represent the number of unique extensions an n-gram has: � |{w0 : c(wn 0) &gt; 0} |if n &lt; N a(wn 1 ) = c(wn 1) if n = N w</context>
</contexts>
<marker>Brants, Popat, Xu, Och, Dean, 2007</marker>
<rawString>Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och, and Jeffrey Dean. 2007. Large language models in machine translation. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural 6http://kheafield.com/code/kenlm/</rawString>
</citation>
<citation valid="true">
<date></date>
<booktitle>Language Processing and Computational Language Learning,</booktitle>
<pages>858--867</pages>
<marker></marker>
<rawString>Language Processing and Computational Language Learning, pages 858–867, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Omar Zaidan</author>
</authors>
<title>Findings of the 2011 workshop on statistical machine translation.</title>
<date>2011</date>
<booktitle>In Proceedings of the Sixth Workshop on Statistical Machine Translation,</booktitle>
<pages>22--64</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Edinburgh, Scotland,</location>
<contexts>
<context position="18158" citStr="Callison-Burch et al., 2011" startWordPosition="3176" endWordPosition="3179">ed during cube pruning. = ⎛ ⎞ f−1 Y Yk p(wk−1 1 ) ⎝ b(wk−1 j ) ⎠ p(wk|wk−1 f ) b(wk j )j=ej=f Yk j=f b(wkj ) 1173 3.3 Combined Scheme Our two language model modifications can be trivially combined by using lower-order probabilities on the left of a fragment and by charging all backoff penalties on the right of a fragment. The net result is a language model that uses the same memory as the baseline but has better rest cost estimates. 4 Experiments To measure the impact of different rest costs, we use the Moses chart decoder (Koehn et al., 2007) for the WMT 2011 German-English translation task (Callison-Burch et al., 2011). Using the Moses pipeline, we trained two syntactic German-English systems, one with target-side syntax and the other hierarchical with unlabeled grammar rules (Chiang, 2007). Grammar rules were extracted from Europarl (Koehn, 2005) using the Collins parser (Collins, 1999) for syntax on the English side. The language model interpolates, on the WMT 2010 test set, separate models built on Europarl, news commentary, and the WMT news data for each year. Models were built and interpolated using SRILM (Stolcke, 2002) with modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998) </context>
</contexts>
<marker>Callison-Burch, Koehn, Monz, Zaidan, 2011</marker>
<rawString>Chris Callison-Burch, Philipp Koehn, Christof Monz, and Omar Zaidan. 2011. Findings of the 2011 workshop on statistical machine translation. In Proceedings of the Sixth Workshop on Statistical Machine Translation, pages 22–64, Edinburgh, Scotland, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley Chen</author>
<author>Joshua Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling.</title>
<date>1998</date>
<tech>Technical Report TR-10-98,</tech>
<institution>Harvard University,</institution>
<contexts>
<context position="3607" citStr="Chen and Goodman, 1998" startWordPosition="577" endWordPosition="581">ith the baseline estimate lies in lower order entries pN(wn|wn−1 1 ). Commonly used Kneser-Ney (Kneser and Ney, 1995) smoothing, 1Here, the term rest cost means an adjustment to the score of a sentence fragment but not to whole sentences. The adjustment may be good or bad for approximate search. pN(wn|wn−1 1 ) N−1ri n=1 1169 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1169–1178, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics including the modified version (Chen and Goodman, 1998), assumes that a lower-order entry will only be used because a longer match could not be found2. Formally, these entries actually evaluate pN(wn|wn−1 1 , did not find wn0 ). For purposes of scoring sentence fragments, additional context is simply indeterminate, and the assumption may not hold. As an example, we built 5-gram and unigram language models with Kneser-Ney smoothing on the same data. Sentence fragments frequently begin with “the”. Using a lower-order entry from the 5- gram model, log10 p5(the) = −2.49417. The unigram model does not condition on backing off, assigning log10 p1(the) =</context>
<context position="18757" citStr="Chen and Goodman, 1998" startWordPosition="3267" endWordPosition="3270">ison-Burch et al., 2011). Using the Moses pipeline, we trained two syntactic German-English systems, one with target-side syntax and the other hierarchical with unlabeled grammar rules (Chiang, 2007). Grammar rules were extracted from Europarl (Koehn, 2005) using the Collins parser (Collins, 1999) for syntax on the English side. The language model interpolates, on the WMT 2010 test set, separate models built on Europarl, news commentary, and the WMT news data for each year. Models were built and interpolated using SRILM (Stolcke, 2002) with modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998) and the default pruning settings. In all scenarios, the primary language model has order 5. For lower-order rest costs, we also built models with orders 1 through 4 then used the n-gram model to score n-grams in the 5-gram model. Feature weights were trained with MERT (Och, 2003) on the baseline using a pop limit of 1000 and 100-best output. Since final feature values are unchanged, we did not re-run MERT in each condition. Measurements were collected by running the decoder on the 3003-sentence test set. 4.1 Rest Costs as Prediction Scoring the first few words of a sentence fragment is a pred</context>
</contexts>
<marker>Chen, Goodman, 1998</marker>
<rawString>Stanley Chen and Joshua Goodman. 1998. An empirical study of smoothing techniques for language modeling. Technical Report TR-10-98, Harvard University, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<pages>33--201</pages>
<contexts>
<context position="5668" citStr="Chiang, 2007" startWordPosition="929" endWordPosition="930">ability and backoff. We will show that the probability and backoff values in a language model can be collapsed into a single value for each n-gram without changing sentence probability. This transformation saves memory by halving the number of values stored per entry, but it makes rest cost estimates worse. Specifically, the rest cost pessimistically assumes that the model will back off to unigrams immediately following the sentence fragment. The two modifications can be used independently or simultaneously. To measure the impact of their different rest costs, we experiment with cube pruning (Chiang, 2007) in syntactic machine transla2Other smoothing techniques, including Witten-Bell (Witten and Bell, 1991), do not make this assumption. tion. Cube pruning’s goal is to find high-scoring sentence fragments for the root non-terminal in the parse tree. It does so by going bottom-up in the parse tree, searching for high-scoring sentence fragments for each non-terminal. Within each non-terminal, it generates a fixed number of high-scoring sentence fragments; this is known as the pop limit. Increasing the pop limit therefore makes search more accurate but costs more time. By moderating the pop limit, </context>
<context position="18333" citStr="Chiang, 2007" startWordPosition="3202" endWordPosition="3203">mbined by using lower-order probabilities on the left of a fragment and by charging all backoff penalties on the right of a fragment. The net result is a language model that uses the same memory as the baseline but has better rest cost estimates. 4 Experiments To measure the impact of different rest costs, we use the Moses chart decoder (Koehn et al., 2007) for the WMT 2011 German-English translation task (Callison-Burch et al., 2011). Using the Moses pipeline, we trained two syntactic German-English systems, one with target-side syntax and the other hierarchical with unlabeled grammar rules (Chiang, 2007). Grammar rules were extracted from Europarl (Koehn, 2005) using the Collins parser (Collins, 1999) for syntax on the English side. The language model interpolates, on the WMT 2010 test set, separate models built on Europarl, news commentary, and the WMT news data for each year. Models were built and interpolated using SRILM (Stolcke, 2002) with modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998) and the default pruning settings. In all scenarios, the primary language model has order 5. For lower-order rest costs, we also built models with orders 1 through 4 then used </context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33:201–228, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Head-Driven Statistical Models for Natural Language Parsing.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="18432" citStr="Collins, 1999" startWordPosition="3216" endWordPosition="3217">nalties on the right of a fragment. The net result is a language model that uses the same memory as the baseline but has better rest cost estimates. 4 Experiments To measure the impact of different rest costs, we use the Moses chart decoder (Koehn et al., 2007) for the WMT 2011 German-English translation task (Callison-Burch et al., 2011). Using the Moses pipeline, we trained two syntactic German-English systems, one with target-side syntax and the other hierarchical with unlabeled grammar rules (Chiang, 2007). Grammar rules were extracted from Europarl (Koehn, 2005) using the Collins parser (Collins, 1999) for syntax on the English side. The language model interpolates, on the WMT 2010 test set, separate models built on Europarl, news commentary, and the WMT news data for each year. Models were built and interpolated using SRILM (Stolcke, 2002) with modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998) and the default pruning settings. In all scenarios, the primary language model has order 5. For lower-order rest costs, we also built models with orders 1 through 4 then used the n-gram model to score n-grams in the 5-gram model. Feature weights were trained with MERT (Och,</context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>Michael Collins. 1999. Head-Driven Statistical Models for Natural Language Parsing. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Dyer</author>
<author>Adam Lopez</author>
<author>Juri Ganitkevitch</author>
<author>Johnathan Weese</author>
<author>Ferhan Ture</author>
<author>Phil Blunsom</author>
<author>Hendra Setiawan</author>
<author>Vladimir Eidelman</author>
<author>Philip Resnik</author>
</authors>
<title>cdec: A decoder, alignment, and learning framework for finite-state and context-free translation models.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACL 2010 System Demonstrations, ACLDemos ’10,</booktitle>
<pages>7--12</pages>
<contexts>
<context position="2660" citStr="Dyer et al., 2010" startWordPosition="421" endWordPosition="424">ld constant. We first show how to improve rest cost quality over standard practice by using additional space. Then, conversely, we show how to compress the language model by making a pessimistic rest cost assumption1. Language models are designed to assign probability to sentences. However, approximate search algorithms use estimates for sentence fragments. If the language model has order N (an N-gram model), then the first N − 1 words of the fragment have incomplete context and the last N − 1 words have not been completely used as context. Our baseline is common practice (Koehn et al., 2007; Dyer et al., 2010; Li et al., 2009) that uses lower-order entries from the language model for the first words in the fragment and no rest cost adjustment for the last few words. Formally, the baseline estimate for sentence fragment wk1 is k n−1 ri pN (wn |wn−N+1) n=N where each wn is a word and pN is an N-gram language model. The problem with the baseline estimate lies in lower order entries pN(wn|wn−1 1 ). Commonly used Kneser-Ney (Kneser and Ney, 1995) smoothing, 1Here, the term rest cost means an adjustment to the score of a sentence fragment but not to whole sentences. The adjustment may be good or bad for</context>
</contexts>
<marker>Dyer, Lopez, Ganitkevitch, Weese, Ture, Blunsom, Setiawan, Eidelman, Resnik, 2010</marker>
<rawString>Chris Dyer, Adam Lopez, Juri Ganitkevitch, Johnathan Weese, Ferhan Ture, Phil Blunsom, Hendra Setiawan, Vladimir Eidelman, and Philip Resnik. 2010. cdec: A decoder, alignment, and learning framework for finite-state and context-free translation models. In Proceedings of the ACL 2010 System Demonstrations, ACLDemos ’10, pages 7–12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
</authors>
<title>How many bits are needed to store probabilities for phrasebased translation?</title>
<date>2006</date>
<booktitle>In Proceedings of the Workshop on Statistical Machine Translation,</booktitle>
<pages>94--101</pages>
<location>New York City,</location>
<contexts>
<context position="7798" citStr="Federico and Bertoldi, 2006" startWordPosition="1269" endWordPosition="1272">rnal to the sentence fragment, namely output that the decoder may generate in the future. Our rest costs examine words internal to the sentence fragment, namely the first and last few words. We also differ by focusing on syntactic translation. A wide variety of work has been done on language model compression. While data structure compression (Raj and Whittaker, 2003; Heafield, 2011) and randomized data structures (Talbot and Osborne, 2007; Guthrie and Hepple, 2010) are useful, here we are concerned solely with the values stored by these data structures. Quantization (Whittaker and Raj, 2001; Federico and Bertoldi, 2006) uses less bits to store each numerical value at the expense of model quality, including scores of full sentences, and is compatible with our approach. In fact, the lower-order probabilities might be quantized further than normal since these are used solely for rest cost 1170 purposes. Our compression technique reduces storage from two values, probability and backoff, to one value, theoretically halving the bits per value (except N-grams which all have backoff 1). This makes the storage requirement for higher-quality modified Kneser-Ney smoothing comparable to stupid backoff (Brants et al., 20</context>
</contexts>
<marker>Federico, Bertoldi, 2006</marker>
<rawString>Marcello Federico and Nicola Bertoldi. 2006. How many bits are needed to store probabilities for phrasebased translation? In Proceedings of the Workshop on Statistical Machine Translation, pages 94–101, New York City, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Mauro Cettolo</author>
</authors>
<title>IRSTLM: an open source toolkit for handling large scale language models.</title>
<date>2008</date>
<booktitle>In Proceedings of Interspeech,</booktitle>
<location>Brisbane, Australia.</location>
<contexts>
<context position="16640" citStr="Federico et al., 2008" startWordPosition="2919" endWordPosition="2922">ay as the baseline using p and b. To save memory, we simply store q in lieu of p and b. Compared with the baseline, this halves number of values from two to one float per n-gram, except N-grams that already have one value. The impact of this reduction is substantial, as seen in Section 4.3. Run-time scoring is also simplified as shown in Figure 2 since the language model locates the longest match wnf then returns the value q(wn|wn−1 1 ) = q(wn|wn−1 f ) without any calculation or additional lookup. Baseline language models either retrieve backoffs values with additional lookups (Stolcke, 2002; Federico et al., 2008) or modify the decoder to annotate sentence fragments with backoff information (Heafield, 2011); we have effectively moved this step to preprocessing. The disadvantage is that q is not a proper probability and it produces worse rest costs than does the baseline. Language models are actually applied at two points in syntactic machine translation: scoring lexical items in grammar rules and during cube pruning. Grammar scoring is an offline and embarrassingly parallel process where memory is not as tight (since the phrase table is streamed) and fewer queries are made, so slow non-lossy compressio</context>
</contexts>
<marker>Federico, Bertoldi, Cettolo, 2008</marker>
<rawString>Marcello Federico, Nicola Bertoldi, and Mauro Cettolo. 2008. IRSTLM: an open source toolkit for handling large scale language models. In Proceedings of Interspeech, Brisbane, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Guthrie</author>
<author>Mark Hepple</author>
</authors>
<title>Storing the web in memory: Space efficient language models with constant time retrieval.</title>
<date>2010</date>
<booktitle>In Proceedings of EMNLP 2010,</booktitle>
<location>Los Angeles, CA.</location>
<contexts>
<context position="7640" citStr="Guthrie and Hepple, 2010" startWordPosition="1245" endWordPosition="1248">h passes, so our work is largely orthogonal. Zens and Ney (2008) present rest costs for phrasebased translation. These rest costs are based on factors external to the sentence fragment, namely output that the decoder may generate in the future. Our rest costs examine words internal to the sentence fragment, namely the first and last few words. We also differ by focusing on syntactic translation. A wide variety of work has been done on language model compression. While data structure compression (Raj and Whittaker, 2003; Heafield, 2011) and randomized data structures (Talbot and Osborne, 2007; Guthrie and Hepple, 2010) are useful, here we are concerned solely with the values stored by these data structures. Quantization (Whittaker and Raj, 2001; Federico and Bertoldi, 2006) uses less bits to store each numerical value at the expense of model quality, including scores of full sentences, and is compatible with our approach. In fact, the lower-order probabilities might be quantized further than normal since these are used solely for rest cost 1170 purposes. Our compression technique reduces storage from two values, probability and backoff, to one value, theoretically halving the bits per value (except N-grams </context>
<context position="26142" citStr="Guthrie and Hepple, 2010" startWordPosition="4514" endWordPosition="4517">385 -101.395 -101.405 -101.415 -101.38 -101.39 -101.41 -101.42 -101.4 Lower Combined Baseline Pessimistic Uncased BLEU 22.35 22.25 22.15 22.05 21.95 22.3 22.2 22.1 22 Lower Combined Baseline Pessimistic and removes another, so it uses the same memory as the baseline. The memory footprint of adding or removing a value depends on the number of such ngrams, the underlying data structure, and the extent of quantization. Our test language model has 135 million n-grams for n &lt; 5 and 56 million 5-grams. Memory usage was measured for KenLM data structures (Heafield, 2011) and minimal perfect hashing (Guthrie and Hepple, 2010). For minimal perfect hashing, we assume the Compress, Hash and Displace algorithm (Belazzougui et al., 2008) with 8-bit signatures and 8-bit quantization. Table 2 shows the results. Storage size of the smallest model is reduced by 26%, bringing higher-quality smoothed models in line with stupid backoff models that also store one value per n-gram. Structure Baseline Change % Probing 4,072 517 13% Trie 2,647 506 19% 8-bit quantized trie 1,236 140 11% 8-bit minimal perfect hash 540 140 26% Table 2: Size in megabytes of our language model, excluding operating system overhead. Change is the cost o</context>
</contexts>
<marker>Guthrie, Hepple, 2010</marker>
<rawString>David Guthrie and Mark Hepple. 2010. Storing the web in memory: Space efficient language models with constant time retrieval. In Proceedings of EMNLP 2010, Los Angeles, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Heafield</author>
<author>Hieu Hoang</author>
<author>Philipp Koehn</author>
<author>Tetsuo Kiso</author>
<author>Marcello Federico</author>
</authors>
<title>Left language model state for syntactic machine translation.</title>
<date>2011</date>
<booktitle>In Proceedings of the International Workshop on Spoken Language Translation,</booktitle>
<location>San Francisco, CA, USA,</location>
<contexts>
<context position="10351" citStr="Heafield et al., 2011" startWordPosition="1707" endWordPosition="1710">. In some cases, we are able to determine that the model will back off and therefore the lowerorder probability makes the appropriate assumption. Specifically, if vwn1 does not appear in the model for any word v, then computing p(wn|vwn−1 1 ) will al3Counts are not modified for n-grams bound to the beginning of sentence, namely those with wl = &lt;s&gt;. ways back off to wn−1 1 or fewer words4. This criterion is the same as used to minimize the length of left language model state (Li and Khudanpur, 2008) and can be retrieved for each n-gram without using additional memory in common data structures (Heafield et al., 2011). Where it is unknown if the model will back off, we use a language model of the same order to produce a rest cost. Specifically, there are N language models, one of each order from 1 to N. The models are trained on the same corpus with the same smoothing parameters to the extent that they apply. We then compile these into one data structure where each n-gram record has three values: 1. Probability pn from the n-gram language model 2. Probability pN from the N-gram language model 3. Backoff b from the N-gram language model For N-grams, the two probabilities are the same and backoff is always 1</context>
</contexts>
<marker>Heafield, Hoang, Koehn, Kiso, Federico, 2011</marker>
<rawString>Kenneth Heafield, Hieu Hoang, Philipp Koehn, Tetsuo Kiso, and Marcello Federico. 2011. Left language model state for syntactic machine translation. In Proceedings of the International Workshop on Spoken Language Translation, San Francisco, CA, USA, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Heafield</author>
</authors>
<title>KenLM: Faster and smaller language model queries.</title>
<date>2011</date>
<booktitle>In Proceedings of the Sixth Workshop on Statistical Machine Translation,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Edinburgh, UK,</location>
<contexts>
<context position="7556" citStr="Heafield, 2011" startWordPosition="1235" endWordPosition="1236"> options. The rest cost estimates we describe here could be applied in both passes, so our work is largely orthogonal. Zens and Ney (2008) present rest costs for phrasebased translation. These rest costs are based on factors external to the sentence fragment, namely output that the decoder may generate in the future. Our rest costs examine words internal to the sentence fragment, namely the first and last few words. We also differ by focusing on syntactic translation. A wide variety of work has been done on language model compression. While data structure compression (Raj and Whittaker, 2003; Heafield, 2011) and randomized data structures (Talbot and Osborne, 2007; Guthrie and Hepple, 2010) are useful, here we are concerned solely with the values stored by these data structures. Quantization (Whittaker and Raj, 2001; Federico and Bertoldi, 2006) uses less bits to store each numerical value at the expense of model quality, including scores of full sentences, and is compatible with our approach. In fact, the lower-order probabilities might be quantized further than normal since these are used solely for rest cost 1170 purposes. Our compression technique reduces storage from two values, probability </context>
<context position="16735" citStr="Heafield, 2011" startWordPosition="2934" endWordPosition="2935">the baseline, this halves number of values from two to one float per n-gram, except N-grams that already have one value. The impact of this reduction is substantial, as seen in Section 4.3. Run-time scoring is also simplified as shown in Figure 2 since the language model locates the longest match wnf then returns the value q(wn|wn−1 1 ) = q(wn|wn−1 f ) without any calculation or additional lookup. Baseline language models either retrieve backoffs values with additional lookups (Stolcke, 2002; Federico et al., 2008) or modify the decoder to annotate sentence fragments with backoff information (Heafield, 2011); we have effectively moved this step to preprocessing. The disadvantage is that q is not a proper probability and it produces worse rest costs than does the baseline. Language models are actually applied at two points in syntactic machine translation: scoring lexical items in grammar rules and during cube pruning. Grammar scoring is an offline and embarrassingly parallel process where memory is not as tight (since the phrase table is streamed) and fewer queries are made, so slow non-lossy compression and even network-based sharding can be used. We therefore use an ordinary language model for </context>
<context position="26087" citStr="Heafield, 2011" startWordPosition="4508" endWordPosition="4509"> select pop limits. Average model score -101.385 -101.395 -101.405 -101.415 -101.38 -101.39 -101.41 -101.42 -101.4 Lower Combined Baseline Pessimistic Uncased BLEU 22.35 22.25 22.15 22.05 21.95 22.3 22.2 22.1 22 Lower Combined Baseline Pessimistic and removes another, so it uses the same memory as the baseline. The memory footprint of adding or removing a value depends on the number of such ngrams, the underlying data structure, and the extent of quantization. Our test language model has 135 million n-grams for n &lt; 5 and 56 million 5-grams. Memory usage was measured for KenLM data structures (Heafield, 2011) and minimal perfect hashing (Guthrie and Hepple, 2010). For minimal perfect hashing, we assume the Compress, Hash and Displace algorithm (Belazzougui et al., 2008) with 8-bit signatures and 8-bit quantization. Table 2 shows the results. Storage size of the smallest model is reduced by 26%, bringing higher-quality smoothed models in line with stupid backoff models that also store one value per n-gram. Structure Baseline Change % Probing 4,072 517 13% Trie 2,647 506 19% 8-bit quantized trie 1,236 140 11% 8-bit minimal perfect hash 540 140 26% Table 2: Size in megabytes of our language model, ex</context>
</contexts>
<marker>Heafield, 2011</marker>
<rawString>Kenneth Heafield. 2011. KenLM: Faster and smaller language model queries. In Proceedings of the Sixth Workshop on Statistical Machine Translation, Edinburgh, UK, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>David Chiang</author>
</authors>
<title>Forest rescoring: Faster decoding with integrated language models.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="6469" citStr="Huang and Chiang, 2007" startWordPosition="1051" endWordPosition="1054">oring sentence fragments for the root non-terminal in the parse tree. It does so by going bottom-up in the parse tree, searching for high-scoring sentence fragments for each non-terminal. Within each non-terminal, it generates a fixed number of high-scoring sentence fragments; this is known as the pop limit. Increasing the pop limit therefore makes search more accurate but costs more time. By moderating the pop limit, improved accuracy can be interpreted as a reduction in CPU time and vice-versa. 2 Related Work Vilar and Ney (2011) study several modifications to cube pruning and cube growing (Huang and Chiang, 2007). Most relevant is their use of a class-based language model for the first of two decoding passes. This first pass is cheaper because translation alternatives are likely to fall into the same class. Entries are scored with the maximum probability over class members (thereby making them no longer normalized). Thus, paths that score highly in this first pass may contain high-scoring paths under the lexicalized language model, so the second pass more fully explores these options. The rest cost estimates we describe here could be applied in both passes, so our work is largely orthogonal. Zens and </context>
</contexts>
<marker>Huang, Chiang, 2007</marker>
<rawString>Liang Huang and David Chiang. 2007. Forest rescoring: Faster decoding with integrated language models. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Kneser</author>
<author>Hermann Ney</author>
</authors>
<title>Improved backing-off for m-gram language modeling.</title>
<date>1995</date>
<booktitle>In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing,</booktitle>
<pages>181--184</pages>
<contexts>
<context position="3101" citStr="Kneser and Ney, 1995" startWordPosition="501" endWordPosition="504">ds of the fragment have incomplete context and the last N − 1 words have not been completely used as context. Our baseline is common practice (Koehn et al., 2007; Dyer et al., 2010; Li et al., 2009) that uses lower-order entries from the language model for the first words in the fragment and no rest cost adjustment for the last few words. Formally, the baseline estimate for sentence fragment wk1 is k n−1 ri pN (wn |wn−N+1) n=N where each wn is a word and pN is an N-gram language model. The problem with the baseline estimate lies in lower order entries pN(wn|wn−1 1 ). Commonly used Kneser-Ney (Kneser and Ney, 1995) smoothing, 1Here, the term rest cost means an adjustment to the score of a sentence fragment but not to whole sentences. The adjustment may be good or bad for approximate search. pN(wn|wn−1 1 ) N−1ri n=1 1169 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1169–1178, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics including the modified version (Chen and Goodman, 1998), assumes that a lower-order entry will only be used because a longer match could not be found</context>
<context position="8780" citStr="Kneser and Ney, 1995" startWordPosition="1424" endWordPosition="1427"> backoff, to one value, theoretically halving the bits per value (except N-grams which all have backoff 1). This makes the storage requirement for higher-quality modified Kneser-Ney smoothing comparable to stupid backoff (Brants et al., 2007). Whether to use one smoothing technique or the other then becomes largely an issue of training costs and quality after quantization. 3 Contribution 3.1 Better Rest Costs As alluded to in the introduction, the first few words of a sentence fragment are typically scored using lower-order entries from an N-gram language model. However, Kneser-Ney smoothing (Kneser and Ney, 1995) conditions lower-order probabilities on backing off. Specifically, lower-order counts are adjusted to represent the number of unique extensions an n-gram has: � |{w0 : c(wn 0) &gt; 0} |if n &lt; N a(wn 1 ) = c(wn 1) if n = N where c(wn1) is the number of times wn1 appears in the training data3. This adjustment is also performed for modified Kneser-Ney smoothing. The intuition is based on the fact that the language model will base its probability on the longest possible match. If an N-gram was seen in the training data, the model will match it fully and use the smoothed count. Otherwise, the full N-</context>
<context position="18732" citStr="Kneser and Ney, 1995" startWordPosition="3263" endWordPosition="3266">translation task (Callison-Burch et al., 2011). Using the Moses pipeline, we trained two syntactic German-English systems, one with target-side syntax and the other hierarchical with unlabeled grammar rules (Chiang, 2007). Grammar rules were extracted from Europarl (Koehn, 2005) using the Collins parser (Collins, 1999) for syntax on the English side. The language model interpolates, on the WMT 2010 test set, separate models built on Europarl, news commentary, and the WMT news data for each year. Models were built and interpolated using SRILM (Stolcke, 2002) with modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998) and the default pruning settings. In all scenarios, the primary language model has order 5. For lower-order rest costs, we also built models with orders 1 through 4 then used the n-gram model to score n-grams in the 5-gram model. Feature weights were trained with MERT (Och, 2003) on the baseline using a pop limit of 1000 and 100-best output. Since final feature values are unchanged, we did not re-run MERT in each condition. Measurements were collected by running the decoder on the 3003-sentence test set. 4.1 Rest Costs as Prediction Scoring the first few words of a se</context>
</contexts>
<marker>Kneser, Ney, 1995</marker>
<rawString>Reinhard Kneser and Hermann Ney. 1995. Improved backing-off for m-gram language modeling. In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, pages 181– 184.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<location>Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra</location>
<contexts>
<context position="2641" citStr="Koehn et al., 2007" startWordPosition="417" endWordPosition="420">plete sentence is held constant. We first show how to improve rest cost quality over standard practice by using additional space. Then, conversely, we show how to compress the language model by making a pessimistic rest cost assumption1. Language models are designed to assign probability to sentences. However, approximate search algorithms use estimates for sentence fragments. If the language model has order N (an N-gram model), then the first N − 1 words of the fragment have incomplete context and the last N − 1 words have not been completely used as context. Our baseline is common practice (Koehn et al., 2007; Dyer et al., 2010; Li et al., 2009) that uses lower-order entries from the language model for the first words in the fragment and no rest cost adjustment for the last few words. Formally, the baseline estimate for sentence fragment wk1 is k n−1 ri pN (wn |wn−N+1) n=N where each wn is a word and pN is an N-gram language model. The problem with the baseline estimate lies in lower order entries pN(wn|wn−1 1 ). Commonly used Kneser-Ney (Kneser and Ney, 1995) smoothing, 1Here, the term rest cost means an adjustment to the score of a sentence fragment but not to whole sentences. The adjustment may</context>
<context position="18079" citStr="Koehn et al., 2007" startWordPosition="3165" endWordPosition="3168">g only top-scoring grammar rules) and the order in which rules are tried during cube pruning. = ⎛ ⎞ f−1 Y Yk p(wk−1 1 ) ⎝ b(wk−1 j ) ⎠ p(wk|wk−1 f ) b(wk j )j=ej=f Yk j=f b(wkj ) 1173 3.3 Combined Scheme Our two language model modifications can be trivially combined by using lower-order probabilities on the left of a fragment and by charging all backoff penalties on the right of a fragment. The net result is a language model that uses the same memory as the baseline but has better rest cost estimates. 4 Experiments To measure the impact of different rest costs, we use the Moses chart decoder (Koehn et al., 2007) for the WMT 2011 German-English translation task (Callison-Burch et al., 2011). Using the Moses pipeline, we trained two syntactic German-English systems, one with target-side syntax and the other hierarchical with unlabeled grammar rules (Chiang, 2007). Grammar rules were extracted from Europarl (Koehn, 2005) using the Collins parser (Collins, 1999) for syntax on the English side. The language model interpolates, on the WMT 2010 test set, separate models built on Europarl, news commentary, and the WMT news data for each year. Models were built and interpolated using SRILM (Stolcke, 2002) wit</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Annual Meeting of the Association for Computational Linguistics (ACL), Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Europarl: A parallel corpus for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of MT Summit.</booktitle>
<contexts>
<context position="18391" citStr="Koehn, 2005" startWordPosition="3210" endWordPosition="3211">fragment and by charging all backoff penalties on the right of a fragment. The net result is a language model that uses the same memory as the baseline but has better rest cost estimates. 4 Experiments To measure the impact of different rest costs, we use the Moses chart decoder (Koehn et al., 2007) for the WMT 2011 German-English translation task (Callison-Burch et al., 2011). Using the Moses pipeline, we trained two syntactic German-English systems, one with target-side syntax and the other hierarchical with unlabeled grammar rules (Chiang, 2007). Grammar rules were extracted from Europarl (Koehn, 2005) using the Collins parser (Collins, 1999) for syntax on the English side. The language model interpolates, on the WMT 2010 test set, separate models built on Europarl, news commentary, and the WMT news data for each year. Models were built and interpolated using SRILM (Stolcke, 2002) with modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998) and the default pruning settings. In all scenarios, the primary language model has order 5. For lower-order rest costs, we also built models with orders 1 through 4 then used the n-gram model to score n-grams in the 5-gram model. Fea</context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>Philipp Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In Proceedings of MT Summit.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhifei Li</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>A scalable decoder for parsing-based machine translation with equivalent language model state maintenance.</title>
<date>2008</date>
<booktitle>In Proceedings of the Second ACL Workshop on Syntax and Structure in Statistical Translation (SSST-2),</booktitle>
<pages>10--18</pages>
<location>Columbus, Ohio,</location>
<contexts>
<context position="10232" citStr="Li and Khudanpur, 2008" startWordPosition="1687" endWordPosition="1690">e not representative of cases where context is short simply because additional context is unknown at the time of scoring. In some cases, we are able to determine that the model will back off and therefore the lowerorder probability makes the appropriate assumption. Specifically, if vwn1 does not appear in the model for any word v, then computing p(wn|vwn−1 1 ) will al3Counts are not modified for n-grams bound to the beginning of sentence, namely those with wl = &lt;s&gt;. ways back off to wn−1 1 or fewer words4. This criterion is the same as used to minimize the length of left language model state (Li and Khudanpur, 2008) and can be retrieved for each n-gram without using additional memory in common data structures (Heafield et al., 2011). Where it is unknown if the model will back off, we use a language model of the same order to produce a rest cost. Specifically, there are N language models, one of each order from 1 to N. The models are trained on the same corpus with the same smoothing parameters to the extent that they apply. We then compile these into one data structure where each n-gram record has three values: 1. Probability pn from the n-gram language model 2. Probability pN from the N-gram language mo</context>
<context position="11714" citStr="Li and Khudanpur (2008)" startWordPosition="1953" endWordPosition="1956">ay be different, so we query the n-gram model in the normal way to score every ngram in the N-gram model. The idea is that pn is the average conditional probability that will be encountered once additional context becomes known. We also tried more complicated estimates by additionally interpolating upper bound, lower bound, and pN with weights trained on cube pruning logs; none of these improved results in any meaningful way. Formalizing the above, let wk1 be a sentence fragment. Choose the largest s so that vws1 appears in the model for some v; equivalently ws1 is the left state described in Li and Khudanpur (2008). The 4Usually, this happens because wl does not appear, though it can also happen that wl appears but all vwl were removed by pruning or filtering. 1171 ! pN(wn|wn−1 1 ) · ! pN(wn|wn−1 1 ) · (1) ! n−1 pN (wn |wn−N+1) while our improved estimate is pn (wn |)! · ! pN(wn|wn−1 1 ) · (2) ! n−1 pN (wn |wn−N+1) The difference between these equations is that pn is used for words in the left state i.e. 1 &lt; n &lt; s. We have also abused notation by using pN to denote both probabilities stored explicitly in the model and the model’s backoff-smoothed probabilities when not present. It is not necessary to st</context>
</contexts>
<marker>Li, Khudanpur, 2008</marker>
<rawString>Zhifei Li and Sanjeev Khudanpur. 2008. A scalable decoder for parsing-based machine translation with equivalent language model state maintenance. In Proceedings of the Second ACL Workshop on Syntax and Structure in Statistical Translation (SSST-2), pages 10–18, Columbus, Ohio, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhifei Li</author>
<author>Chris Callison-Burch</author>
<author>Chris Dyer</author>
<author>Sanjeev Khudanpur</author>
<author>Lane Schwartz</author>
<author>Wren Thornton</author>
<author>Jonathan Weese</author>
<author>Omar Zaidan</author>
</authors>
<title>Joshua: An open source toolkit for parsing-based machine translation.</title>
<date>2009</date>
<booktitle>In Proceedings of the Fourth Workshop on Statistical Machine Translation,</booktitle>
<pages>135--139</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Athens, Greece,</location>
<contexts>
<context position="2678" citStr="Li et al., 2009" startWordPosition="425" endWordPosition="428">st show how to improve rest cost quality over standard practice by using additional space. Then, conversely, we show how to compress the language model by making a pessimistic rest cost assumption1. Language models are designed to assign probability to sentences. However, approximate search algorithms use estimates for sentence fragments. If the language model has order N (an N-gram model), then the first N − 1 words of the fragment have incomplete context and the last N − 1 words have not been completely used as context. Our baseline is common practice (Koehn et al., 2007; Dyer et al., 2010; Li et al., 2009) that uses lower-order entries from the language model for the first words in the fragment and no rest cost adjustment for the last few words. Formally, the baseline estimate for sentence fragment wk1 is k n−1 ri pN (wn |wn−N+1) n=N where each wn is a word and pN is an N-gram language model. The problem with the baseline estimate lies in lower order entries pN(wn|wn−1 1 ). Commonly used Kneser-Ney (Kneser and Ney, 1995) smoothing, 1Here, the term rest cost means an adjustment to the score of a sentence fragment but not to whole sentences. The adjustment may be good or bad for approximate searc</context>
</contexts>
<marker>Li, Callison-Burch, Dyer, Khudanpur, Schwartz, Thornton, Weese, Zaidan, 2009</marker>
<rawString>Zhifei Li, Chris Callison-Burch, Chris Dyer, Sanjeev Khudanpur, Lane Schwartz, Wren Thornton, Jonathan Weese, and Omar Zaidan. 2009. Joshua: An open source toolkit for parsing-based machine translation. In Proceedings of the Fourth Workshop on Statistical Machine Translation, pages 135–139, Athens, Greece, March. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In ACL ’03: Proceedings of the 41st Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>160--167</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="19038" citStr="Och, 2003" startWordPosition="3318" endWordPosition="3319">999) for syntax on the English side. The language model interpolates, on the WMT 2010 test set, separate models built on Europarl, news commentary, and the WMT news data for each year. Models were built and interpolated using SRILM (Stolcke, 2002) with modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998) and the default pruning settings. In all scenarios, the primary language model has order 5. For lower-order rest costs, we also built models with orders 1 through 4 then used the n-gram model to score n-grams in the 5-gram model. Feature weights were trained with MERT (Och, 2003) on the baseline using a pop limit of 1000 and 100-best output. Since final feature values are unchanged, we did not re-run MERT in each condition. Measurements were collected by running the decoder on the 3003-sentence test set. 4.1 Rest Costs as Prediction Scoring the first few words of a sentence fragment is a prediction task. The goal is to predict what the probability will be when more context becomes known. In order to measure performance on this task, we ran the decoder on the hierarchical system with a pop limit of 1000. Every time more context became known, we logged5 the prediction e</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In ACL ’03: Proceedings of the 41st Annual Meeting on Association for Computational Linguistics, pages 160–167, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: A method for automatic evalution of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>311--318</pages>
<location>Philadelphia, PA,</location>
<contexts>
<context position="21300" citStr="Papineni et al., 2002" startWordPosition="3708" endWordPosition="3711">cause lower variance means cube pruning’s relative rankings are more accurate. Our lower-order rest costs are better across the board in terms of absolute bias, mean squared error, and variance. 4.2 Pop Limit Trade-Offs The cube pruning pop limit is a trade-off between search accuracy and CPU time. Here, we measure how our rest costs improve (or degrade) that trade-off. Search accuracy is measured by the average model score of single-best translations. Model scores are scale-invariant and include a large constant factor; higher is better. We also measure overall performance with uncased BLEU (Papineni et al., 2002). CPU time is the sum of user and system time used by Moses divided by the number of sentences (3003). Timing includes time to load, though files were forced into the disk cache in advance. Our test machine has 64 GB of RAM and 32 cores. Results are shown in Figures 3 and 4. Lower-order rest costs perform better in both systems, reaching plateau model scores and BLEU with less CPU time. The gain is much larger for tar1174 Pop Baseline Lower Order CPU Pessimistic CPU Combined CPU Model BLEU CPU Model BLEU Model BLEU Model BLEU 2 3.29 -105.56 20.45 3.68 -105.44 20.79 3.74 -105.62 20.01 3.18 -105</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: A method for automatic evalution of machine translation. In Proceedings 40th Annual Meeting of the Association for Computational Linguistics, pages 311–318, Philadelphia, PA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bhiksha Raj</author>
<author>Ed Whittaker</author>
</authors>
<title>Lossless compression of language model structure and word identifiers.</title>
<date>2003</date>
<booktitle>In Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing,</booktitle>
<pages>388--391</pages>
<contexts>
<context position="7539" citStr="Raj and Whittaker, 2003" startWordPosition="1231" endWordPosition="1234">more fully explores these options. The rest cost estimates we describe here could be applied in both passes, so our work is largely orthogonal. Zens and Ney (2008) present rest costs for phrasebased translation. These rest costs are based on factors external to the sentence fragment, namely output that the decoder may generate in the future. Our rest costs examine words internal to the sentence fragment, namely the first and last few words. We also differ by focusing on syntactic translation. A wide variety of work has been done on language model compression. While data structure compression (Raj and Whittaker, 2003; Heafield, 2011) and randomized data structures (Talbot and Osborne, 2007; Guthrie and Hepple, 2010) are useful, here we are concerned solely with the values stored by these data structures. Quantization (Whittaker and Raj, 2001; Federico and Bertoldi, 2006) uses less bits to store each numerical value at the expense of model quality, including scores of full sentences, and is compatible with our approach. In fact, the lower-order probabilities might be quantized further than normal since these are used solely for rest cost 1170 purposes. Our compression technique reduces storage from two val</context>
</contexts>
<marker>Raj, Whittaker, 2003</marker>
<rawString>Bhiksha Raj and Ed Whittaker. 2003. Lossless compression of language model structure and word identifiers. In Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing, pages 388– 391.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM - an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of the Seventh International Conference on Spoken Language Processing,</booktitle>
<pages>901--904</pages>
<contexts>
<context position="16616" citStr="Stolcke, 2002" startWordPosition="2917" endWordPosition="2918">ctly the same way as the baseline using p and b. To save memory, we simply store q in lieu of p and b. Compared with the baseline, this halves number of values from two to one float per n-gram, except N-grams that already have one value. The impact of this reduction is substantial, as seen in Section 4.3. Run-time scoring is also simplified as shown in Figure 2 since the language model locates the longest match wnf then returns the value q(wn|wn−1 1 ) = q(wn|wn−1 f ) without any calculation or additional lookup. Baseline language models either retrieve backoffs values with additional lookups (Stolcke, 2002; Federico et al., 2008) or modify the decoder to annotate sentence fragments with backoff information (Heafield, 2011); we have effectively moved this step to preprocessing. The disadvantage is that q is not a proper probability and it produces worse rest costs than does the baseline. Language models are actually applied at two points in syntactic machine translation: scoring lexical items in grammar rules and during cube pruning. Grammar scoring is an offline and embarrassingly parallel process where memory is not as tight (since the phrase table is streamed) and fewer queries are made, so s</context>
<context position="18675" citStr="Stolcke, 2002" startWordPosition="3257" endWordPosition="3258">oehn et al., 2007) for the WMT 2011 German-English translation task (Callison-Burch et al., 2011). Using the Moses pipeline, we trained two syntactic German-English systems, one with target-side syntax and the other hierarchical with unlabeled grammar rules (Chiang, 2007). Grammar rules were extracted from Europarl (Koehn, 2005) using the Collins parser (Collins, 1999) for syntax on the English side. The language model interpolates, on the WMT 2010 test set, separate models built on Europarl, news commentary, and the WMT news data for each year. Models were built and interpolated using SRILM (Stolcke, 2002) with modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998) and the default pruning settings. In all scenarios, the primary language model has order 5. For lower-order rest costs, we also built models with orders 1 through 4 then used the n-gram model to score n-grams in the 5-gram model. Feature weights were trained with MERT (Och, 2003) on the baseline using a pop limit of 1000 and 100-best output. Since final feature values are unchanged, we did not re-run MERT in each condition. Measurements were collected by running the decoder on the 3003-sentence test set. 4.1 Res</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM - an extensible language modeling toolkit. In Proceedings of the Seventh International Conference on Spoken Language Processing, pages 901–904.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Talbot</author>
<author>Miles Osborne</author>
</authors>
<title>Randomised language modelling for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>512--519</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="7613" citStr="Talbot and Osborne, 2007" startWordPosition="1241" endWordPosition="1244">re could be applied in both passes, so our work is largely orthogonal. Zens and Ney (2008) present rest costs for phrasebased translation. These rest costs are based on factors external to the sentence fragment, namely output that the decoder may generate in the future. Our rest costs examine words internal to the sentence fragment, namely the first and last few words. We also differ by focusing on syntactic translation. A wide variety of work has been done on language model compression. While data structure compression (Raj and Whittaker, 2003; Heafield, 2011) and randomized data structures (Talbot and Osborne, 2007; Guthrie and Hepple, 2010) are useful, here we are concerned solely with the values stored by these data structures. Quantization (Whittaker and Raj, 2001; Federico and Bertoldi, 2006) uses less bits to store each numerical value at the expense of model quality, including scores of full sentences, and is compatible with our approach. In fact, the lower-order probabilities might be quantized further than normal since these are used solely for rest cost 1170 purposes. Our compression technique reduces storage from two values, probability and backoff, to one value, theoretically halving the bits</context>
</contexts>
<marker>Talbot, Osborne, 2007</marker>
<rawString>David Talbot and Miles Osborne. 2007. Randomised language modelling for statistical machine translation. In Proceedings ofACL, pages 512–519, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Vilar</author>
<author>Hermann Ney</author>
</authors>
<title>Cardinality pruning and language model heuristics for hierarchical phrase-based translation.</title>
<date>2011</date>
<booktitle>Machine Translation,</booktitle>
<pages>1--38</pages>
<publisher>DOI</publisher>
<contexts>
<context position="6383" citStr="Vilar and Ney (2011)" startWordPosition="1038" endWordPosition="1041">l, 1991), do not make this assumption. tion. Cube pruning’s goal is to find high-scoring sentence fragments for the root non-terminal in the parse tree. It does so by going bottom-up in the parse tree, searching for high-scoring sentence fragments for each non-terminal. Within each non-terminal, it generates a fixed number of high-scoring sentence fragments; this is known as the pop limit. Increasing the pop limit therefore makes search more accurate but costs more time. By moderating the pop limit, improved accuracy can be interpreted as a reduction in CPU time and vice-versa. 2 Related Work Vilar and Ney (2011) study several modifications to cube pruning and cube growing (Huang and Chiang, 2007). Most relevant is their use of a class-based language model for the first of two decoding passes. This first pass is cheaper because translation alternatives are likely to fall into the same class. Entries are scored with the maximum probability over class members (thereby making them no longer normalized). Thus, paths that score highly in this first pass may contain high-scoring paths under the lexicalized language model, so the second pass more fully explores these options. The rest cost estimates we descr</context>
</contexts>
<marker>Vilar, Ney, 2011</marker>
<rawString>David Vilar and Hermann Ney. 2011. Cardinality pruning and language model heuristics for hierarchical phrase-based translation. Machine Translation, pages 1–38, November. DOI 10.1007/s10590-011-9119-4.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ed Whittaker</author>
<author>Bhiksha Raj</author>
</authors>
<title>Quantizationbased language model compression.</title>
<date>2001</date>
<booktitle>In Proceedings of EUROSPEECH,</booktitle>
<pages>33--36</pages>
<contexts>
<context position="7768" citStr="Whittaker and Raj, 2001" startWordPosition="1265" endWordPosition="1268">are based on factors external to the sentence fragment, namely output that the decoder may generate in the future. Our rest costs examine words internal to the sentence fragment, namely the first and last few words. We also differ by focusing on syntactic translation. A wide variety of work has been done on language model compression. While data structure compression (Raj and Whittaker, 2003; Heafield, 2011) and randomized data structures (Talbot and Osborne, 2007; Guthrie and Hepple, 2010) are useful, here we are concerned solely with the values stored by these data structures. Quantization (Whittaker and Raj, 2001; Federico and Bertoldi, 2006) uses less bits to store each numerical value at the expense of model quality, including scores of full sentences, and is compatible with our approach. In fact, the lower-order probabilities might be quantized further than normal since these are used solely for rest cost 1170 purposes. Our compression technique reduces storage from two values, probability and backoff, to one value, theoretically halving the bits per value (except N-grams which all have backoff 1). This makes the storage requirement for higher-quality modified Kneser-Ney smoothing comparable to stu</context>
</contexts>
<marker>Whittaker, Raj, 2001</marker>
<rawString>Ed Whittaker and Bhiksha Raj. 2001. Quantizationbased language model compression. In Proceedings of EUROSPEECH, pages 33–36, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian H Witten</author>
<author>Timothy C Bell</author>
</authors>
<title>The zerofrequency problem: Estimating the probabilities of novel events in adaptive text compression.</title>
<date>1991</date>
<journal>IEEE Transactions on Information Theory,</journal>
<volume>37</volume>
<issue>4</issue>
<pages>1094</pages>
<contexts>
<context position="5771" citStr="Witten and Bell, 1991" startWordPosition="940" endWordPosition="943">can be collapsed into a single value for each n-gram without changing sentence probability. This transformation saves memory by halving the number of values stored per entry, but it makes rest cost estimates worse. Specifically, the rest cost pessimistically assumes that the model will back off to unigrams immediately following the sentence fragment. The two modifications can be used independently or simultaneously. To measure the impact of their different rest costs, we experiment with cube pruning (Chiang, 2007) in syntactic machine transla2Other smoothing techniques, including Witten-Bell (Witten and Bell, 1991), do not make this assumption. tion. Cube pruning’s goal is to find high-scoring sentence fragments for the root non-terminal in the parse tree. It does so by going bottom-up in the parse tree, searching for high-scoring sentence fragments for each non-terminal. Within each non-terminal, it generates a fixed number of high-scoring sentence fragments; this is known as the pop limit. Increasing the pop limit therefore makes search more accurate but costs more time. By moderating the pop limit, improved accuracy can be interpreted as a reduction in CPU time and vice-versa. 2 Related Work Vilar an</context>
</contexts>
<marker>Witten, Bell, 1991</marker>
<rawString>Ian H. Witten and Timothy C. Bell. 1991. The zerofrequency problem: Estimating the probabilities of novel events in adaptive text compression. IEEE Transactions on Information Theory, 37(4):1085– 1094.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Zens</author>
<author>Hermann Ney</author>
</authors>
<title>Improvements in dynamic programming beam search for phrase-based statistical machine translation.</title>
<date>2008</date>
<booktitle>In Proceedings of the International Workshop on Spoken Language Translation (IWSLT),</booktitle>
<location>Honolulu, Hawaii,</location>
<contexts>
<context position="7079" citStr="Zens and Ney (2008)" startWordPosition="1154" endWordPosition="1157">ng, 2007). Most relevant is their use of a class-based language model for the first of two decoding passes. This first pass is cheaper because translation alternatives are likely to fall into the same class. Entries are scored with the maximum probability over class members (thereby making them no longer normalized). Thus, paths that score highly in this first pass may contain high-scoring paths under the lexicalized language model, so the second pass more fully explores these options. The rest cost estimates we describe here could be applied in both passes, so our work is largely orthogonal. Zens and Ney (2008) present rest costs for phrasebased translation. These rest costs are based on factors external to the sentence fragment, namely output that the decoder may generate in the future. Our rest costs examine words internal to the sentence fragment, namely the first and last few words. We also differ by focusing on syntactic translation. A wide variety of work has been done on language model compression. While data structure compression (Raj and Whittaker, 2003; Heafield, 2011) and randomized data structures (Talbot and Osborne, 2007; Guthrie and Hepple, 2010) are useful, here we are concerned sole</context>
</contexts>
<marker>Zens, Ney, 2008</marker>
<rawString>Richard Zens and Hermann Ney. 2008. Improvements in dynamic programming beam search for phrase-based statistical machine translation. In Proceedings of the International Workshop on Spoken Language Translation (IWSLT), Honolulu, Hawaii, October.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<note confidence="0.7080765">
Modelling Sequential Text with an Adaptive Topic Model
Huidong Jin*
CSIRO Mathematics, Informatics
and Statistics,
</note>
<address confidence="0.480203">
Canberra, Australia
</address>
<email confidence="0.967988">
warren.jin@csiro.au
</email>
<author confidence="0.937479">
Lan Du*
</author>
<affiliation confidence="0.994065">
Department of Computing
Macquarie University
</affiliation>
<address confidence="0.567118">
Sydney, Australia
</address>
<email confidence="0.99578">
lan.du@mq.edu.au
</email>
<author confidence="0.850344">
Wray Buntine*
</author>
<affiliation confidence="0.804779333333333">
Canberra Research Lab
National ICT Australia
Canberra, Australia
</affiliation>
<email confidence="0.995323">
wray.buntine@nicta.com.au
</email>
<sectionHeader confidence="0.996626" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.996563785714286">
Topic models are increasingly being used for
text analysis tasks, often times replacing ear-
lier semantic techniques such as latent seman-
tic analysis. In this paper, we develop a novel
adaptive topic model with the ability to adapt
topics from both the previous segment and the
parent document. For this proposed model, a
Gibbs sampler is developed for doing poste-
rior inference. Experimental results show that
with topic adaptation, our model significantly
improves over existing approaches in terms of
perplexity, and is able to uncover clear se-
quential structure on, for example, Herman
Melville’s book “Moby Dick”.
</bodyText>
<sectionHeader confidence="0.998772" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999583352941177">
Natural language text usually consists of topically
structured and coherent components, such as groups
of sentences that form paragraphs and groups of
paragraphs that form sections. Topical coherence in
documents facilitates readers’ comprehension, and
reflects the author’s intended structure. Capturing
this structural topical dependency should lead to im-
proved topic modelling. It also seems reasonable
to propose that text analysis tasks that involve the
structure of a document, for instance, summarisation
and segmentation, should also be improved by topic
models that better model that structure.
Recently, topic models are increasingly being
used for text analysis tasks such as summarisa-
*This work was partially done when Du was at College of
Engineering &amp; Computer Science, the Australian National Uni-
versity when working together with Buntine and Jin there.
tion (Arora and Ravindran, 2008) and segmenta-
tion (Misra et al., 2011; Eisenstein and Barzilay,
2008), often times replacing earlier semantic tech-
niques such as latent semantic analysis (Deerwester
et al., 1990). Topic models can be improved by bet-
ter modelling the semantic aspects of text, for in-
stance integrating collocations into the model (John-
son, 2010; Hardisty et al., 2010) or encouraging top-
ics to be more semantically coherent (Newman et
al., 2011) based on lexical coherence models (New-
man et al., 2010), modelling the structural aspects
of documents, for instance modelling a document
as a set of segments (Du et al., 2010; Wang et al.,
2011; Chen et al., 2009), or improving the under-
lying statistical methods (Teh et al., 2006; Wallach
et al., 2009). Topic models, like statistical parsing
methods, are using more sophisticated latent vari-
able methods in order to model different aspects of
these problems.
In this paper, we are interested in developing a
new topic model which can take into account the
structural topic dependency by following the higher
level document subject structure, but we hope to re-
tain the general flavour of topic models, where com-
ponents (e.g., sentences) can be a mixture of topics.
Thus we need to depart from the earlier HMM style
models, see, e.g., (Blei and Moreno, 2001; Gruber
et al., 2007). Inspired by the idea that documents
usually exhibits internal structure (e.g., (Wang et al.,
2011)), in which semantically related units are clus-
tered together to form semantically structural seg-
ments, we treat documents as sequences of segments
(e.g., sentences, paragraphs, sections, or chapters).
In this way, we can model the topic correlation be-
</bodyText>
<page confidence="0.977029">
535
</page>
<note confidence="0.989628">
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 535–545, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics
</note>
<figureCaption confidence="0.994510333333333">
Figure 1: Different structural relationships for topics of
sections in a 4-part document, hierarchical (H), sequen-
tial (S), both (B) or mixed (M).
</figureCaption>
<bodyText confidence="0.999903327586208">
tween the segments in a “bag of segments” fashion,
i.e., beyond the “bag of words” assumption, and re-
veal how topics evolve among segments.
Indeed, we were impressed by the improvement
in perplexity obtained by the segmented topic model
(STM) (Du et al., 2010), so we considered the prob-
lem of whether one can add sequence information
into a structured topic model as well. Figure 1 illus-
trates the type of structural information being con-
sidered, where the vectors are some representation
of the content. STM is represented by the hierar-
chical model. A strictly sequential model would
seem unrealistic for some documents, for instance
books. A topic model using the strictly sequential
model was developed (Du et al., 2012) but it report-
edly performs halfway between STM and LDA. In
this paper, we develop an adaptive topic model to
go beyond a strictly sequential model while allow
some hierarchical influence. There are two possible
hybrids, one called “mixed” has distinct breaks in
the sequence, while the other called “both” overlays
both sequence and hierarchy and there could be rel-
ative strengths associated with the arrows. We em-
ploy the “both” hybrid but use the relative strengths
to adaptively allow it to approximate the “mixed”
hybrid.
Research in Machine Learning and Natural Lan-
guage Processing has attempted to model various
topical dependencies. Some work considers struc-
ture within the sentence level by mixing hidden
Markov models (HMMs) and topics on a word by
word basis: the aspect HMM (Blei and Moreno,
2001) and the HMM-LDA model (Griffiths et al.,
2005) that models both short-range syntactic depen-
dencies and longer semantic dependencies. These
models operate at a finer level than we are consider-
ing at a segment (like paragraph or section) level. To
make a tool like the HMM work at higher levels, one
needs to make stronger assumptions, for instance as-
signing each sentence a single topic and then topic
specific word models can be used: the hidden topic
Markov model (Gruber et al., 2007) that models the
transitional topic structure; a global model based on
the generalised Mallows model (Chen et al., 2009),
and a HMM based content model (Barzilay and
Lee, 2004). Researchers have also considered time-
series of topics: various kinds of dynamic topic
models, following early work of (Blei and Lafferty,
2006), represent a collection as a sequence of sub-
collections in epochs. Here, one is modelling the
collections over broad epochs, not the structure of a
single document that our model considers.
This paper is organised as follows. We first
present background theory in Section 2. Then the
new model is presented in Section 3, followed by
Gibbs sampling theory and algorithm in Sections 4
and 5 respectively. Experiments are reported in Sec-
tion 6 with a conclusion in Section 7.
</bodyText>
<sectionHeader confidence="0.975624" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.9999495">
The basic topic model is first presented in Sec-
tion 2.1, as a point of departure. In seeking to de-
velop a general sequential topic model, we hope
to go beyond a strictly sequential model and allow
some hierarchical influence. This, however, presents
two challenges: modelling and statistical inference.
Hierarchical inference (and thus sequential infer-
ence) over probability vectors can be handled us-
ing the theory of hierarchical Poisson-Dirichlet pro-
cesses (PDPs). This is presented in Section 2.2.
</bodyText>
<subsectionHeader confidence="0.964025">
2.1 The LDA model
</subsectionHeader>
<bodyText confidence="0.999796">
The benchmark model for topic modelling is latent
Dirichlet allocation (LDA) (Blei et al., 2003), a la-
tent variable model of documents. Documents are
indexed by i, and words w are observed data. The
latent variables are µz (the topic distribution for a
document) and z (the topic assignments for observed
words), and the model parameter of Gk’s (word dis-
tributions). These notation are later extended in Ta-
</bodyText>
<figure confidence="0.90562">
9
li1 v2 v3 v4 li1 v2
li1 v2 6 v4
(H) µ (S)
(B)
9
li1 v2
(M)
9
113 114
113
L14
</figure>
<page confidence="0.980912">
536
</page>
<bodyText confidence="0.647176">
ble 1. The generative model is as follows:
</bodyText>
<equation confidence="0.997154">
~φk ∼ DirichletW (~γ) ∀ k
~µi ∼ DirichletK (~α) ∀ i
zi,l ∼ DiscreteK (~µi) ∀ i, l
� �
wi,l ∼ DiscreteK ~φzi,l ∀ i, l .
</equation>
<bodyText confidence="0.9904362">
DirichletK(·) is a K-dimensional Dirichlet distribu-
tion. The hyper-parameter γ~ is a Dirichlet prior on
word distributions (i.e., a Dirichlet smoothing on the
multinomial parameter ~φk (Blei et al., 2003)) and the
Dirichlet prior α~ on topic distributions.
</bodyText>
<subsectionHeader confidence="0.998356">
2.2 Hierarchical PDPs
</subsectionHeader>
<bodyText confidence="0.999715166666667">
A discrete probability vector µ~ of finite dimension
K is sampled from some distribution Fτ(~µ0) with
a parameter set, say τ, and is also dependent on
a parent probability vector ~µ0 also of finite dimen-
sion K. Then a sample of size N is taken ac-
cording to the probability vector ~µ, represented as
</bodyText>
<equation confidence="0.6325452">
z~ ∈ {1, ..., K}N. This data is collected into counts
n~ = (n1, ..., nK) where nk is the number of data in
z~ with value k and Ek nk = N. This situation is
represented as follows:
µ~ ∼ Fτ (~µ0); ~zi ∼ DiscreteK(~µ) for i = 1, ..., N .
</equation>
<bodyText confidence="0.999547">
Commonly in topic modelling, the Dirichlet distri-
bution is used for discrete probability vectors. In
this case Fτ(~µ0) ≡ DirichletK(b~µ0), τ ≡ (K, b)
where b is the concentration parameter. Bayesian
analysis yields a marginalised likelihood, after inte-
grating out ~µ, of
</bodyText>
<equation confidence="0.9922105">
p (~zlτ, ~µ0, Dirichlet) = Beta (~n + b~µ0) (1)
Beta (b~µ0)
</equation>
<bodyText confidence="0.999118555555556">
where Beta(·) is the vector valued function normal-
ising the Dirichlet distribution. A problem here is
that p(~z|b, ~µ0) is an intractable function of ~µ0.
Dirichlet processes and Poisson-Dirichlet pro-
cesses alleviate this problem by using an auxiliary
variable trick (Robert and Casella, 2004). That is,
we introduce an auxiliary variable over which we
also sample but do not need to record. The auxiliary
variable is the table count1 which is a tk for each nk
</bodyText>
<footnote confidence="0.943254333333333">
1Based on the Chinese Restaurant analogy (Teh et al., 2006),
each table has a dish, a data value, while data, the customer, is
assigned to tables, and multiple tables can serve the same dish.
</footnote>
<bodyText confidence="0.956612333333333">
and it represents the number of “tables” over which
the nk “customers” are spread out. Thus the follow-
ing constraints hold:
</bodyText>
<equation confidence="0.981377">
0 ≤ tk ≤ nk and tk = 0 iff nk = 0 . (2)
</equation>
<bodyText confidence="0.9990285">
When the distribution over probability vectors fol-
lows a Poisson-Dirichlet process which has two pa-
rameters τ ≡ (a, b) and the parent distribution ~µ0,
then Fτ (~µ0) ≡ PDP(a, b, ~µ0). Here a is the dis-
count parameter, b the concentration parameter and
~µ0 the base measure. In this case Bayesian analysis
yields an augmented marginalised likelihood (Bun-
tine and Hutter, 2012), after integrating out ~µ, of
</bodyText>
<equation confidence="0.9961585">
((b)NT fj Stk,a (µ0,k)tk (3)
k
</equation>
<bodyText confidence="0.999742903225806">
where T = Ek tk, (x|y)N = HN−1 n=0(x + ny) de-
notes the Pochhammer symbol, (x)N = (x|1)N, and
SNM,a is a generalized Stirling number that is readily
tabulated (Buntine and Hutter, 2012).
There are two fundamental things to notice about
Equation (3). Positively, the term in ~µ0 takes the
form of a multinomial likelihood, so we can prop-
agate it up and perform inference on ~µ0 unen-
cumbered by the functional mess of Equation (1).
Thus Poisson-Dirichlet processes allow one to do
Bayesian reasoning on hierarchies of probability
vectors (Teh, 2006; Teh et al., 2006). Negatively,
however, one needs to sample the auxiliary vari-
ables t~ leading to some problems: The range of tk,
{0, ..., nk}, is broad. Also, contributions from in-
dividual data zi have been lost so the mixing of the
MCMC can sometimes be slow. We confirmed these
problems on our first implementation of the Adap-
tive Topic Model presented next in Section 3.
A further improvement on PDP sampling is
achieved in (Chen et al., 2011), where another aux-
iliary variable is introduced, a so-called table in-
dicator, that for each datum zi indicates whether
it is the “head of its table” (recall the nk data are
spread over tk tables, each table has one and only
one “head”). Let ri = 1 if zi is the “head of its
table,” and zero otherwise. According to this “ta-
ble” logic, the number of tables for nk must be the
number of data zi that are also head of table, so
tk = ENi=1 1zi=k1ri=1. Moreover, given this def-
inition, the first constraint of Equation (2) on tk is
</bodyText>
<equation confidence="0.835717">
p (~z,~t��τ, ~µ0, PDP) =
</equation>
<page confidence="0.967804">
537
</page>
<bodyText confidence="0.999578333333333">
automatically satisfied. Finally, with tk tables then
there must be exactly tk heads of table, and we are
indifferent about which data are heads of table, thus
</bodyText>
<equation confidence="0.913753">
1
p (~z, r~��τ, ~µ0, PDP) = p (~z, fl τ, ~µ0, PDP) H (tk /k /
</equation>
<bodyText confidence="0.95111325">
(4)
When using this marginalised likelihood in a Gibbs
sampler, the zi themselves are usually latent so also
sampled, and we develop a blocked Gibbs sampler
for (zi, ri). Since r~ only appears indirectly through
the table counts ~t, one does not need to store the ~r,
instead just resamples an ri when needed according
to the proportion tw/nw where zi = w.
</bodyText>
<sectionHeader confidence="0.94298" genericHeader="method">
3 The proposed Adaptive Topic Model
</sectionHeader>
<bodyText confidence="0.999797076923077">
In this section an adaptive topic model (AdaTM) is
developed, a fully structured topic model, by using
a PDP to simultaneously model the hierarchical and
the sequential topic structures. Documents are as-
sumed to be broken into a sequence of segments.
Topic distributions are used to mimic the subjects of
documents and subtopics of their segments. The no-
tations and terminologies used in the following sec-
tions are given in Table 1.
In AdaTM, the two topic structures are captured
by drawing topic distributions from the PDPs with
two base distributions as follows. The document
topic distribution ~µi and the jth segment topic dis-
</bodyText>
<figureCaption confidence="0.846441333333333">
Figure 2: The adaptive topic model: µ~ is the document
topic distribution, ~ν1, ~ν2, ... , ~νJ are the segment topic
distributions, and ρ~ is a set of the mixture weights.
</figureCaption>
<bodyText confidence="0.999671727272727">
tribution ~νi,j are linearly combined to give a base
distribution for the (j + 1)th segment’s topic dis-
tribution ~νi,j+1. The topic distribution of the first
segment, i.e., ~νi,1, is drawn directly with the base
distribution ~µi. Call this generative process topic
adaptation. The graphical representation of AdaTM
is shown in Figure 2, and clearly shows the combi-
nation of sequence and hierarchy for the topic prob-
abilities. Note the linear combination at each node
~νi,j is weighted with latent proportions ρi,j.
The resultant model for AdaTM is:
</bodyText>
<figure confidence="0.990874875">
α
�
A
。。。
V V2
1
Vi
z
z
z
。。。
L1
L2
Li
v
0
</figure>
<equation confidence="0.970809111111111">
K
~φk — DirichletW (~γ) b k
~µi — DirichletK (~α) b i
ρi,j — Beta(λS, λT) b i, j
~
νi,j — PDP (ρi,j~νi,j−1 + (1 — ρi,j)~µi, a, b)
zi,j,l — DiscreteK (~νi,j) b i, j, l
� l
wi,j,l — DiscreteK ~φzz���� b i, j, l .
</equation>
<bodyText confidence="0.99950825">
For notational convenience, let ~νi,0 = ~µi. Assume
the dimensionality of the Dirichlet distribution (i.e.,
the number of topics) is known and fixed, and word
probabilities are parameterised with a KxW matrix
</bodyText>
<equation confidence="0.495986">
.6 = ( ~φ1, ..., ~φK).
</equation>
<sectionHeader confidence="0.570804" genericHeader="method">
4 Gibbs Sampling Formulation
</sectionHeader>
<bodyText confidence="0.951447666666667">
Given observations and model parameters, comput-
ing the posterior distribution of latent variables is in-
feasible for AdaTM due to the intractable computa-
</bodyText>
<tableCaption confidence="0.974418">
Table 1: List of notation for AdaTM
</tableCaption>
<figure confidence="0.974682333333333">
K number of topics
I number of documents
Ji number of segments in document i
Li,j number of words in document i, segment j
W number of words in dictionary
~µi document topic probabilities for document i
α~ K-dimensional prior for each ~µi
~νi,j segment topic probabilities for document i and
segment j
</figure>
<bodyText confidence="0.839041285714286">
ρi,j mixture weight associating with the link be-
tween ~νi.j and ~νi,j_1
�~ word probability vectors as a K x W matrix
~φk word probability vector for topic k, entries in 4)
γ~ W-dimensional prior for each ~φk
wi,j,l word in document i, segment j, position l
zi,j,l topic for word wi,j,l
</bodyText>
<page confidence="0.992477">
538
</page>
<tableCaption confidence="0.997817">
Table 2: List of statistics for AdaTM
</tableCaption>
<bodyText confidence="0.945392594594595">
Mi,k,,,, the total number of words in document i with
dictionary index w and being assigned to topic
k
Mk,,,, total Mi,k,,,, for document i, i.e., Ei Mi,k,,,,
~Mk vector of W values Mk,,,,
ni,j,k topic count in document i segment j for topic k
Ni,j topic total in document i segment j, i.e.,
EK
k=1 ni,j,k
ti,j,k table count in the CPR for document i and para-
graph j, for topic k that is inherited back to
paragraph j − 1 and ~µi,j_1.
si,j,k table count in the CPR for document i and para-
graph j, for topic k that is inherited back to the
document and ~µi.
Ti,j total table count in the CRP for document i and
segment j, equal to EKk=1 ti,j,k.
Si,j total table count in the CRP for document i and
segment j, equal to EKk=1 si,j,k.
~ti,j table count vector of ti,j,k’s for segment j.
~si,j table count vector of si,j,k’s for segment j.
tion of marginal probabilities. Therefore, we have to
use approximate inference techniques. This section
proposes a blocked Gibbs sampling algorithm based
on methods from Chen et al. (2011). Table 2 lists
all statistics needed in the algorithm. Note for easier
understanding, terminologies of the Chinese Restau-
rant Process (Teh et al., 2006) will be used, i.e., cus-
tomers, dishes and restaurants, correspond to words,
topics and segments respectively.
The first major complication, over the use of the
hierarchical PDP and Equation (3) and the table in-
dicator trick of Equation (4), is handling the lin-
ear combination of ρi,j~νi,j−1 + (1 − ρi,j)~µi used
in the PDPs. We manage this as follows: First,
Equation (3) shows that a contribution of the form
(µ0,k)tk results. In our case, this becomes
</bodyText>
<equation confidence="0.806478">
H ,
k 1pi,jνi,j−1,k + (1 − ρi,j)µi,k)ti,j,k
(
</equation>
<bodyText confidence="0.998608625">
where t0i,j,k is the corresponding introduced auxil-
iary variable the table count which is involved with
constraints on ni,j,k +ti,j+1,k, from Equation (2). To
deal with this power of a sum, we break the counts
t0i,j,k into two parts, those that contribute to ~νi,j−1
and those that contribute to ~µi. We call these parts
ti,j,k and si,j,k respectively. The product can then be
expanded and ρi,j integrated out. This yields:
</bodyText>
<equation confidence="0.990032666666667">
H
Beta (Si,j + λS, Ti,j + λT )
k
</equation>
<bodyText confidence="0.999522884615385">
The powers νi,j−1,k and µsi,j,k
ti,j,k i,k can then be pushed
up to the next nodes in the PDP/Dirichlet hierarchy.
Note the standard constraints and table indicators are
also needed here.
The precise form of the table indicators needs to
be considered as well since there is a hierarchy for
them, and this is the second major complication in
the model. As discussed in Chen et al. (2011), table
indicators are not required to be recorded, instead,
randomly sampled in Gibbs cycles. The table indi-
cators when known can be used to reconstruct the
table counts ti,j,k and si,j,k, and are reconstructed
by sampling from them. For now, denote the table
indicators as ui,j,l for word wi,j,l.
To complete a formulation suitable for Gibbs
sampling, we first compute the marginal distribu-
tion of the observations ~w1:I,1:J (words), the topic
assignments ~z1:I,1:J and the table indicators ~u1:I,1:J.
The Dirichlet integral is used to integrate out the
document topic distributions ~µ1:I and the topic-
by-words matrix ~�, and the joint posterior dis-
tribution computed for a PDP is used to recur-
sively marginalise out the segment topic distribu-
tions ~ν1:I,1:J. With these variables marginalised out,
we derive the following marginal distribution
</bodyText>
<equation confidence="0.998528142857143">
p(~z1:I,1:J, ~w1:I,1:J, ~u1:I,1:J I ~α,~γ, a, b) = (5)
BetaK (~α + EJi
j=1 si,j )
BetaK (~α)
�−1
1 (ni,j,k + ti,j+1,k) n&apos;i,j,k+ti,j+1,k
H 1 (ti,j,k + si,j,k) S ti,j,k+si,j,k,a
</equation>
<bodyText confidence="0.66076">
And the following constraints apply:
</bodyText>
<equation confidence="0.9993705">
ti,j,k + si,j,k G ni,j,k + ti,j+1,k, (6)
ti,j,k + si,j,k = 0 iff ni,j,k + ti,j+1,k = 0 . (7)
</equation>
<bodyText confidence="0.9996045">
The first constraint falls out naturally when table in-
dicators are used. For convenience of the formulas,
</bodyText>
<equation confidence="0.99623932">
ti,j,k
νi,j−1 k µ
si,j,k
i,k
HI
i=1
HI
i=1
HI
i=1
Beta (Si,j + λS, Ti,j + λT) (b)Ni,j+Ti,j+1
Ji
H
j=1
(b|a)Ti,j+Si,j
Ji
H
j=1
k=1
� �
BetaW γ~ + ~Mk
BetaW (~γ)
K
H
k=1
</equation>
<page confidence="0.778506">
539
</page>
<bodyText confidence="0.978652321428571">
set ti,Jz+1,k = 0 (there is no Ji + 1 segment) and
ti,1,k = 0 (the first segment only uses µi).
Now let us consider again the table indicators
ui,j,l for word wi,j,l. If this word is in topic k at doc-
ument i and segment j, then it contributes a count to
ni,j,k. It also indicates if it contributes a new table,
or a count to t0i,j,k for the PDP at this node. How-
ever, as we discussed above, this then contributes to
either ti,j,k or si,j,k. If it contributes to ti,j,k, then
it recurses up to contribute a data count to the PDP
for document i segment j − 1. Thus it also needs a
table indicator at that node. Consequently, the table
indicator ui,j,l for word wi,j,l must specify whether
it contributes a table to all PDP nodes reachable by
it in the graph.
We define ui,j,l specifically as ui,j,l = (u1, u2)
such that u1 E [−1, 0, 1] and u2 E [1, • • • , j],
where u2 indicates segment denoted by node vj
up to which wi,j,l contributes a table. Given u2,
u1 = −1 denotes wi,j,l contributes a table count to
si,u2,k and ti,j&apos;,k for u2 &lt; j0 &lt; j; u1 = 0 denotes
wi,j,l does not contribute a table to node u2, but con-
tributes a table count to ti j&apos; k for u2 &lt; j0 &lt; j; and
u1 = 1 denotes wi,j,l contributes a table count to
each ti,j&apos;,k for u2 &lt; j0 &lt; j.
Now, we are ready to compute the conditional
probabilities for jointly sampling topics and table in-
dicators from the model posterior of Equation (5).
</bodyText>
<sectionHeader confidence="0.957323" genericHeader="method">
5 Gibbs Sampling Algorithm
</sectionHeader>
<bodyText confidence="0.977489842105263">
The Gibbs sampler iterates over words, doing a
blocked sample of (zi,j,l, ui,j,l). The first task is to
reconstruct ui,j,l since it is not stored. Since the pos-
terior of Equation (5) does not explicitly mention
the ui,j,l’s, they occur indirectly through the table
counts, and we can randomly reconstruct them by
sampling them uniformly from the space of possi-
bilities. Following this, we then remove the values
(zi,j,l, ui,j,l) from the full set of statistics. Finally,
we block sample new values for (zi,j,l,ui,j,l) and
add them to the statistics. The new ui,j,l is subse-
quently forgotten and the zi,j,l recorded.
Reconstructing table indicator ui,j,l: We start at
the node indexed i, j. If si,j,k+ti,j,k = 1 and ni,j,k+
ti,j+1,k &gt; 1 then no tables can be removed since
there is only one table but several customers at the
table. Thus ui,j,l = (u1, u2) = (0, j) and there is no
sampling. Otherwise, by symmetry arguments, we
sample u1 via
</bodyText>
<equation confidence="0.99586">
p(u1 = −1, 0,1Ju2 = j, zi,j,l = k) a
(si,j,k, ti,j,k, ni,j,k + ti,j+1,k − si,j,k − ti,j,k) ,
</equation>
<bodyText confidence="0.969784833333333">
since there are ni,j,k+ti,j+1,k data distributed across
the three possibilities. If after sampling u1 = −1,
the data contributes a table count up to µi and so
ui,j,l = (u1,u2) = (−1, j). If u1 = 0, the ui,j,l =
(u1, u2) = (0, j). Otherwise, the data contributes a
table count up to the parent PDP for vi,j−1 and we
recurse, repeating the sampling process at the parent
node. Note, however, that the table indicator (0, j0)
for j0 &lt; j is equivalent to the table indicator (1, j0 +
1) as far as statistics is concerned.
Block sampling (zi,j,l, ui,j,l): The full set of pos-
sibilities are, for each possible topic zi,j,l = k:
</bodyText>
<listItem confidence="0.910954666666667">
• no tables are created, so ui,j,l = (0, j),
• tables are created contributing a table count all
the way up to node j0 (&lt; j) but stop at j0 and
do not subsequently contribute a count to µi, so
ui,j,l = (1, j0),
• tables are created contributing a table count all
the way up to node j0 &lt; j but stop at j0 and
also subsequently contribute a count to µi, so
ui,j,l = (−1, j0).
</listItem>
<bodyText confidence="0.999845444444444">
These three possibilities lead to detailed but fairly
straight forward changes to the posterior of Equa-
tion (5). Thus a full blocked sampler for (zi,j,l, ui,j,l)
can be constructed.
Estimates: learnt values of µi, vi,j, Ok are needed
for evaluation, perplexity calculations, etc. These
are estimated by taking averages after the Gibbs
sampler has burnt in, using the standard posterior
means for Dirichlets and Poisson-Dirichlets.
</bodyText>
<sectionHeader confidence="0.999598" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<bodyText confidence="0.999220333333333">
In the experimental work, we have three objectives:
(1) to explore the setting of hyper-parameters, (2) to
compare the model with the earlier sequential LDA
(SeqLDA) of (Du et al., 2012), STM of (Du et al.,
2010) and standard LDA, and (3) to view the results
in detail on a number of characteristic problems.
</bodyText>
<page confidence="0.998805">
540
</page>
<tableCaption confidence="0.998493">
Table 3: Datasets
</tableCaption>
<table confidence="0.999851666666667">
#docs #segs #words vocab
Pat-A 500 51,748 2,146,464 16,573
Pat-B 397 9,123 417,631 7,663
Pat-G06 500 11,938 655,694 6,844
Pat-H 500 11,662 562,439 10,114
Pat-F 140 3,181 166,091 4,674
Prince-C 1 26 10,588 3,292
Prince-P 1 192 10.588 3,292
Moby Dick 1 135 88,802 16,223
</table>
<subsectionHeader confidence="0.979674">
6.1 Datasets
</subsectionHeader>
<bodyText confidence="0.999749615384615">
For general testing, five patent datasets are ran-
domly selected from U.S. patents granted in 2009
and 2010. Patents in Pat-A are selected from in-
ternational patent class (IPC) “A”, which is about
“HUMAN NECESSITIES”; those in Pat-B are se-
lected from class “B60” about “VEHICLES IN
GENERAL”; those in Pat-H are selected from
class “H” about “ELECTRICITY”; those in Pat-
F are selected from class “F” about “MECHAN-
ICAL ENGINEERING; LIGHTING; HEATING;
WEAPONS; BLASTING”; and those in Pat-G are
selected from class “G06” about “COMPUTING;
CALCULATING; COUNTING”. All the patents in
these five datasets are split into paragraphs that are
taken as segments, and the sequence of paragraphs
in each patent is reserved in order to maintain the
original layout. All the stop words, the top 10 com-
mon words, the uncommon words (i.e., words in less
than five patents) and numbers have been removed.
Two books used for more detailed investigation
are “The Prince” by Niccol`o Machiavelli and “Moby
Dick” by Herman Melville. They are split into chap-
ters and/or paragraphs which are treated as seg-
ments, and only stop-words are removed. Table 3
shows in detail the statistics of these datasets after
preprocessing.
</bodyText>
<subsectionHeader confidence="0.986338">
6.2 Design
</subsectionHeader>
<bodyText confidence="0.999728">
Perplexity, a standard measure of dictionary-based
compressibility, is used for comparison. When re-
porting test perplexities, the held-out perplexity
measure (Rosen-Zvi et al., 2004) is used to evaluate
the generalisation capability to the unseen data. This
is known to be unbiased. To compute the held-out
perplexity, 20% of patents in each data set was ran-
</bodyText>
<figureCaption confidence="0.999871333333333">
Figure 3: Analysis of parameters of Poisson-Dirichlet
process. (a) shows how perplexity changes with b; (b)
shows how it changes with a.
Figure 4: Analysis of the two parameters for Beta distri-
bution. (a) how perplexity changes with AS; (b) how it
changes with AT.
</figureCaption>
<bodyText confidence="0.99991725">
domly held out from training to be used for testing.
For this, 1000 Gibbs cycles were done for burn-in
followed by 500 cycles with a lag for 100 for pa-
rameter estimation.
We implemented all the four models, e.g., LDA,
STM, SeqTM and AdaTM in C, and ran them on a
desktop with Intel Core i5 CPU (2.8GHzx4), even
though our code is not multi-threaded. Perplexity
calculations, data input and handling, etc., were the
same for all algorithms. We note that the current
AdaTM implementation is an order of magnitude
slower than regular LDA per major Gibbs cycle.
</bodyText>
<subsectionHeader confidence="0.999546">
6.3 Hyper-parameters in AdaTM
</subsectionHeader>
<bodyText confidence="0.989362636363636">
Experiments on the impact of the hyper-parameters
on the patent data sets were as follows: First, fixing
K = 50, the Beta parameters AT = 1 and AS = 1,
optimise symmetric a, and do two variations fix-a:
a = 0.0, trying b = 1, 5, 10, 25,..., 300, and fix-b:
b = 10, trying a = 0.1, 0.2, ..., 0.9. Second, fix-AT
(fix-AS): fix a = 0.2 and AT(AS) = 1, optimise
b and a, change AS(AT) = 0.1,1,10, 50,100, 200.
Figures 3 and 4 show the corresponding plots. Fig-
ure 3(b) and Figure 4(a) show that varying the val-
ues of a and AS does not significantly change the
</bodyText>
<figure confidence="0.999833418181818">
(a) fix a = 0 (b) fix b = 10
Perplexity
1300
1200
1100
1000
900
800
1025 50 100 150 200 250 300
b
Pat−B
Pat−F
Pat−G
Pat−H
Perplexity
1250
1100
950
8000.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
a
Pat−B
Pat−G
Pat−H
Pat−F
1200
1200
1100
1100
1000
Perplexity
Perplexity
1000
Pat−B
Pat−F
Pat−G
Pat−H
Pat−A
900
Pat−A
Pat−B
Pat−F
Pat−H
Pat−G
900
800
800
700
700
600
600
0 50 100 150 200
Lamda_S
0 50 100 150 200
Lamda_T
(a) fix AT = 1 (b) fix AS = 1
</figure>
<page confidence="0.812948">
541
</page>
<figureCaption confidence="0.999428">
Figure 5: Perplexity comparisons.
</figureCaption>
<figure confidence="0.999176053191489">
(a) Pat-A
(b) Pat-H
(c) Pat-B
(d) Pat-F
(e) Pat-G
(f) Shuffle
160
1420
LOA_O
LOA_P
SeqLOA
STM
AdaTM
Perplexity
LOA_O
LOA_P
SeqLOA
STM
AdaTM
Perplexity Difference
Pat−A
Pat−B
Pat−F
Pat−G
Pat−H
1910
140
1270
115
1710
Perplexity
1120
90
1510
65
970
1310
40
820
1110
15
0
−10
6700510 25 50100 150
Number of Topics
9100510 25 Number of Topics
150
ics
510 25 50 100 150
Number of Topics
2430
1600
LOA_O
LOA_P
SeqLOA
STM
AdaTM
LDA_D
LDA_P
SeqLDA
STM
AdaTM
Perplexity
LOA_O
LOA_P
SeqLOA
STM
AdaTM
Perplexity
1810
2130
1450
1660
1830
Perplexity
300
1510
1530
1150
1360
1230
1000
1210
930
850
1060
700
630
510 25 50100 15
Number of Topics
9100510 25 50100 15
Number of Topics
510 25 50 100 150
Number of Topics
</figure>
<bodyText confidence="0.9307584">
perplexity. In contrast, Figure 3(a) shows different
b values significantly change perplexity. Therefore,
we sought to optimise b. The experiment of fixing
AS = 1 and changing AT shows a small AT is pre-
ferred.
</bodyText>
<subsectionHeader confidence="0.979599">
6.4 Perplexity Comparison
</subsectionHeader>
<bodyText confidence="0.999975708333334">
Perplexity comparisons were done with the default
settings a = 0.2, α = 0.1, -y = 0.01, AS = 1,
AT = 1 and b optimised automatically using the
scheme from (Du et al., 2012). Figure 5 shows
the results on these five patent datasets for differ-
ent numbers of topics. LDA D is LDA run on whole
patents, and LDA P is LDA run on the paragraphs
within patents. Table 4 gives the p-values of a one-
tail paired t-test for AdaTM versus the others, where
lower p-value indicates AdaTM has statistically sig-
nificant lower perplexity. From this we can see that
AdaTM is statistically significantly better than Se-
qLDA and LDA, and somewhat better than STM.
In addition, we ran another set of experiments
by randomly shuffling the order of paragraphs in
each patent several times before running AdaTM.
Then, we calculate the difference between perplex-
ities with and without random shuffle. Figure 5(f)
shows the plot of differences in each data sets. The
positive difference means randomly shuffling the or-
der of paragraphs indeed increases the perplexity.
It can further prove that there does exist sequential
topic structure in patents, which confirms the finding
in (Du et al., 2012).
</bodyText>
<subsectionHeader confidence="0.916271">
6.5 Topic Evolution Comparisons
</subsectionHeader>
<bodyText confidence="0.999969461538462">
All the comparison experiments reported in this sec-
tion are run with 20 topics, the upper limit for easy
visualisation, and without optimising any parame-
ters. The Dirichlet Priors are fixed as αk = 0.1
and &apos;y,,, = 0.01. For AdaTM, SeqLDA, and STM,
a = 0.0 and b = 100 for “The Prince” and b = 200
for “Moby Dick”. These settings have proven ro-
bust in experiments. To align the topics so visual-
isations match, the sequential models are initialised
using an LDA model built at the chapter level. More-
over, all the models are run at both the chapter and
the paragraph level. With the common initialisation,
both paragraph level and chapter level models can
</bodyText>
<tableCaption confidence="0.963171">
Table 4: P-values for one-tail paired t-test on the five
patent datasets.
</tableCaption>
<table confidence="0.998251666666667">
AdaTM
Pat-G Pat-A Pat-F Pat-H Pat-B
LDA D .0001 .0001 .0002 .0001 .0001
LDA P .0041 .0030 .0022 .0071 .0096
SeqLDA .0029 .0047 .0003 .0012 .0023
STM .0220 .0066 .0210 .0629 .0853
</table>
<page confidence="0.981919">
542
</page>
<figure confidence="0.99113975">
(a) Evolution of paragraph topics for LDA
(a) AdaTM on chapters
(b) Topic alignment of LDA versus AdaTM top-
ics for chapters
</figure>
<figureCaption confidence="0.999961">
Figure 6: Analysis on “The Prince”.
</figureCaption>
<bodyText confidence="0.995614958333333">
be aligned.
To visualise topic evolution, we use a plot with
one colour per topic displayed over the sequence.
Figure 6(a) shows this for LDA run on paragraphs
of “The Prince”. The proportion of 20 topics is the
Y-axis, spread across the unit interval. The para-
graphs run along the X-axis, so the topic evolution
is clearly displayed. One can see there is no se-
quential structure in this derived by the LDA model,
and similar plots result from “Moby Dick” for LDA.
Figure 6(b) shows the alignment of topics between
the initialising model (LDA+chapters) and AdaTM
run on chapters. Each point in the matrix gives the
Hellinger distance between the corresponding top-
ics, color coded. The plots for the other models,
chapters or paragraphs, are similar so plots like Fig-
ure 6(a) for the other models can be meaningfully
compared.
Figure 7 then shows the corresponding evolution
plots for AdaTM and SeqLDA on chapters and para-
graphs. The contrast of these with LDA is stark.
The large improvement in perplexity for AdaTM
(see Section 6.4) along with no change in lexi-
cal coherence (see Section 6.2) means that the se-
</bodyText>
<figure confidence="0.7089695">
(b) AdaTM on paragraphs
(c) SeqLDA on paragraphs
</figure>
<figureCaption confidence="0.9967">
Figure 7: Topic Evolution on “The Prince”.
</figureCaption>
<bodyText confidence="0.821567333333333">
quential information is actually beneficial statisti-
cally. Note that SeqLDA, while exhibiting slightly
stronger sequential structure than AdaTM in these
</bodyText>
<figure confidence="0.743416">
(d) SeqLDA on chapters
543
(c) AdaTM on Chapters
</figure>
<figureCaption confidence="0.999895">
Figure 8: Topic Evolution on “Moby Dick”.
</figureCaption>
<bodyText confidence="0.999893852941176">
figures has significantly worse test perplexity, so its
sequential affect is too strong and harming results.
Also, note that some topics have different time se-
quence profiles between AdaTM and SeqLDA. In-
deed, inspection of the top words for each show
these topics differ somewhat. So while the LDA
to AdaTM/SeqLDA topic correspondences are quite
good due to the use of LDA initialisation, the cor-
respondences between AdaTM and SeqLDA have
degraded. We see that AdaTM has nearly as good
sequential characteristics as SeqLDA. Furthermore,
segment topic distribution vij of SeqLDA are grad-
ually deviating from the document topic distribution
µi, which is not the case for AdaTM.
Results for “Moby Dick” on chapters are com-
parable. Figure 8 shows similar topic evolution
plots for LDA, STM and AdaTM. In contrast, the
AdaTM topic evolutions are much clearer for the
less frequent topics, as shown in Figure 8(c). Var-
ious parts of this are readily interpreted from the
storyline. Here we briefly discuss topics by their
colour: black: Captain Peleg and the business of
signing on; yellow: inns, housing, bed; mauve:
Queequeg; azure: (around chapters 60-80) details
of whales aqua: (peaks at 8, 82, 88) pulpit, schools
and mythology of whaling.
We see that AdaTM can be used to understand the
topics with regards to the sequential structure of a
book. In contrast, the sequential nature for LDA and
STM is lost in the noise. It can be very interesting to
apply the proposed topic models to some text anal-
ysis tasks, such as topic segmentation, summarisa-
tion, and semantic title evaluation, which are subject
to our future work.
</bodyText>
<sectionHeader confidence="0.998205" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999972272727273">
A model for adaptive sequential topic modelling has
been developed to improve over a simple exchange-
able segments model STM (Du et al., 2010) and a
naive sequential model SeqLDA (Du et al., 2012) in
terms of perplexity and its confirmed ability to un-
cover sequential structure in the topics. One could
extract meaningful topics from a book like Herman
Melville’s “Moby Dick” and concurrently gain their
sequential profile. The current Gibbs sampler is
slower than regular LDA, so future work is to speed
up the algorithm.
</bodyText>
<sectionHeader confidence="0.998238" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.994264909090909">
The authors would like to thank all the anonymous re-
viewers for their valuable comments. Lan Du was
supported under the Australian Research Council’s
Discovery Projects funding scheme (project numbers
DP110102506 and DP110102593). Dr. Huidong Jin
was partly supported by CSIRO Mathematics, Informat-
ics and Statistics for this work. NICTA is funded by the
Australian Government as represented by the Department
of Broadband, Communications and the Digital Econ-
omy and the Australian Research Council through the
ICT Center of Excellence program.
</bodyText>
<figure confidence="0.999746">
(a) LDA on chapters
(b) STM on Chapters
</figure>
<page confidence="0.992637">
544
</page>
<sectionHeader confidence="0.993098" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999882788461539">
R. Arora and B. Ravindran. 2008. Latent Dirichlet allo-
cation and singular value decomposition based multi-
document summarization. In ICDM ’08: Proc. of
2008 Eighth IEEE Inter. Conf. on Data Mining, pages
713–718.
R. Barzilay and L. Lee. 2004. Catching the drift: Prob-
abilistic content models, with applications to genera-
tion and summarization. In HLT-NAACL 2004: Main
Proceedings, pages 113–120. Association for Compu-
tational Linguistics.
D.M. Blei and J.D. Lafferty. 2006. Dynamic topic mod-
els. In ICML ’06: Proc. of 23rd international confer-
ence on Machine learning, pages 113–120.
D.M. Blei and P.J. Moreno. 2001. Topic segmenta-
tion with an aspect hidden Markov model. In Proc.
of 24th annual international ACM SIGIR conference
on Research and development in information retrieval,
pages 343–348.
D.M. Blei, A.Y. Ng, and M.I. Jordan. 2003. Latent
Dirichlet allocation. Journal of Machine Learning Re-
search, 3:993–1022.
W. Buntine and M. Hutter. 2012. A Bayesian view
of the Poisson-Dirichlet process. Technical Report
arXiv:1007.0296v2, ArXiv, Cornell, February.
H. Chen, S.R.K. Branavan, R. Barzilay, and D.R. Karger.
2009. Global models of document structure using la-
tent permutations. In Proceedings of Human Lan-
guage Technologies: The 2009 Annual Conf. of the
North American Chapter of the Association for Com-
putational Linguistics, pages 371–379, Stroudsburg,
PA, USA. Association for Computational Linguistics.
C. Chen, L. Du, and W. Buntine. 2011. Sampling for the
Poisson-Dirichlet process. In European Conf. on Ma-
chine Learning and Principles and Practice of Knowl-
edge Discovery in Database, pages 296–311.
S.C. Deerwester, S.T. Dumais, T.K. Landauer, G.W. Fur-
nas, and R.A. Harshman. 1990. Indexing by latent
semantic analysis. Journal of the American Society of
Information Science, 41(6):391–407.
L. Du, W. Buntine, and H. Jin. 2010. A segmented topic
model based on the two-parameter Poisson-Dirichlet
process. Machine Learning, 81:5–19.
L. Du, W. Buntine, H. Jin, and C. Chen. 2012. Sequential
latent dirichlet allocation. Knowledge and Information
Systems, 31(3):475–503.
J. Eisenstein and R. Barzilay. 2008. Bayesian unsuper-
vised topic segmentation. In Proc. of Conf. on Empir-
ical Methods in Natural Language Processing, pages
334–343. Association for Computational Linguistics.
T.L. Griffiths, M. Steyvers, D.M. Blei, and J.B. Tenen-
baum. 2005. Integrating topics and syntax. In Ad-
vances in Neural Information Processing Systems 17,
pages 537–544.
A. Gruber, Y. Weiss, and M. Rosen-Zvi. 2007. Hidden
topic markov models. Journal of Machine Learning
Research - Proceedings Track, 2:163–170.
E.A. Hardisty, J. Boyd-Graber, and P. Resnik. 2010.
Modeling perspective using adaptor grammars. In
Proc. of the 2010 Conf. on Empirical Methods in Nat-
ural Language Processing, pages 284–292, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
M. Johnson. 2010. PCFGs, topic models, adaptor gram-
mars and learning topical collocations and the struc-
ture of proper names. In Proc. of 48th Annual Meeting
of the ACL, pages 1148–1157, Uppsala, Sweden, July.
Association for Computational Linguistics.
H. Misra, F. Yvon, O. Capp, and J. Jose. 2011. Text seg-
mentation: A topic modeling perspective. Information
Processing &amp; Management, 47(4):528–544.
D. Newman, J.H. Lau, K. Grieser, and T. Baldwin. 2010.
Automatic evaluation of topic coherence. In North
American Chapter of the Association for Computa-
tional Linguistics - Human Language Technologies,
pages 100–108.
D. Newman, E.V. Bonilla, and W. Buntine. 2011. Im-
proving topic coherence with regularized topic mod-
els. In J. Shawe-Taylor, R.S. Zemel, P. Bartlett,
F.C.N. Pereira, and K.Q. Weinberger, editors, Ad-
vances in Neural Information Processing Systems 24,
pages 496–504.
C.P. Robert and G. Casella. 2004. Monte Carlo statisti-
cal methods. Springer. second edition.
M. Rosen-Zvi, T. Griffiths, M. Steyvers, and P. Smyth.
2004. The author-topic model for authors and docu-
ments. In Proc. of 20th conference on Uncertainty in
Artificial Intelligence, pages 487–494.
Y. W. Teh, M. I. Jordan, M. J. Beal, and D. M. Blei. 2006.
Hierarchical Dirichlet processes. Journal of the Amer-
ican Statistical Association, 101:1566–1581.
Y. W. Teh. 2006. A hierarchical Bayesian language
model based on Pitman-Yor processes. In Proc. of
21st Inter. Conf. on Computational Linguistics and the
44th annual meeting of the Association for Computa-
tional Linguistics, pages 985–992.
H. Wallach, D. Mimno, and A. McCallum. 2009. Re-
thinking LDA: Why priors matter. In Advances in
Neural Information Processing Systems 19.
H. Wang, D. Zhang, and C. Zhai. 2011. Structural topic
model for latent topical structure analysis. In Proc. of
49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies -
Volume 1, pages 1526–1535, Stroudsburg, PA, USA.
Association for Computational Linguistics.
</reference>
<page confidence="0.99853">
545
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.209759">
<title confidence="0.954381">Modelling Sequential Text with an Adaptive Topic Model</title>
<affiliation confidence="0.9377035">CSIRO Mathematics, and</affiliation>
<address confidence="0.972237">Canberra, Australia</address>
<email confidence="0.999354">warren.jin@csiro.au</email>
<affiliation confidence="0.8618295">Department of Macquarie</affiliation>
<address confidence="0.926277">Sydney, Australia</address>
<email confidence="0.998579">lan.du@mq.edu.au</email>
<affiliation confidence="0.9965125">Canberra Research National ICT</affiliation>
<address confidence="0.982813">Canberra, Australia</address>
<email confidence="0.998884">wray.buntine@nicta.com.au</email>
<abstract confidence="0.949737066666667">Topic models are increasingly being used for text analysis tasks, often times replacing earlier semantic techniques such as latent semantic analysis. In this paper, we develop a novel adaptive topic model with the ability to adapt topics from both the previous segment and the parent document. For this proposed model, a Gibbs sampler is developed for doing posterior inference. Experimental results show that with topic adaptation, our model significantly improves over existing approaches in terms of perplexity, and is able to uncover clear sequential structure on, for example, Herman Melville’s book “Moby Dick”.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R Arora</author>
<author>B Ravindran</author>
</authors>
<title>Latent Dirichlet allocation and singular value decomposition based multidocument summarization.</title>
<date>2008</date>
<booktitle>In ICDM ’08: Proc. of 2008 Eighth IEEE Inter. Conf. on Data Mining,</booktitle>
<pages>713--718</pages>
<contexts>
<context position="1891" citStr="Arora and Ravindran, 2008" startWordPosition="267" endWordPosition="270">intended structure. Capturing this structural topical dependency should lead to improved topic modelling. It also seems reasonable to propose that text analysis tasks that involve the structure of a document, for instance, summarisation and segmentation, should also be improved by topic models that better model that structure. Recently, topic models are increasingly being used for text analysis tasks such as summarisa*This work was partially done when Du was at College of Engineering &amp; Computer Science, the Australian National University when working together with Buntine and Jin there. tion (Arora and Ravindran, 2008) and segmentation (Misra et al., 2011; Eisenstein and Barzilay, 2008), often times replacing earlier semantic techniques such as latent semantic analysis (Deerwester et al., 1990). Topic models can be improved by better modelling the semantic aspects of text, for instance integrating collocations into the model (Johnson, 2010; Hardisty et al., 2010) or encouraging topics to be more semantically coherent (Newman et al., 2011) based on lexical coherence models (Newman et al., 2010), modelling the structural aspects of documents, for instance modelling a document as a set of segments (Du et al., </context>
</contexts>
<marker>Arora, Ravindran, 2008</marker>
<rawString>R. Arora and B. Ravindran. 2008. Latent Dirichlet allocation and singular value decomposition based multidocument summarization. In ICDM ’08: Proc. of 2008 Eighth IEEE Inter. Conf. on Data Mining, pages 713–718.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Barzilay</author>
<author>L Lee</author>
</authors>
<title>Catching the drift: Probabilistic content models, with applications to generation and summarization.</title>
<date>2004</date>
<booktitle>In HLT-NAACL 2004: Main Proceedings,</booktitle>
<pages>113--120</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6138" citStr="Barzilay and Lee, 2004" startWordPosition="957" endWordPosition="960">, 2005) that models both short-range syntactic dependencies and longer semantic dependencies. These models operate at a finer level than we are considering at a segment (like paragraph or section) level. To make a tool like the HMM work at higher levels, one needs to make stronger assumptions, for instance assigning each sentence a single topic and then topic specific word models can be used: the hidden topic Markov model (Gruber et al., 2007) that models the transitional topic structure; a global model based on the generalised Mallows model (Chen et al., 2009), and a HMM based content model (Barzilay and Lee, 2004). Researchers have also considered timeseries of topics: various kinds of dynamic topic models, following early work of (Blei and Lafferty, 2006), represent a collection as a sequence of subcollections in epochs. Here, one is modelling the collections over broad epochs, not the structure of a single document that our model considers. This paper is organised as follows. We first present background theory in Section 2. Then the new model is presented in Section 3, followed by Gibbs sampling theory and algorithm in Sections 4 and 5 respectively. Experiments are reported in Section 6 with a conclu</context>
</contexts>
<marker>Barzilay, Lee, 2004</marker>
<rawString>R. Barzilay and L. Lee. 2004. Catching the drift: Probabilistic content models, with applications to generation and summarization. In HLT-NAACL 2004: Main Proceedings, pages 113–120. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D M Blei</author>
<author>J D Lafferty</author>
</authors>
<title>Dynamic topic models.</title>
<date>2006</date>
<booktitle>In ICML ’06: Proc. of 23rd international conference on Machine learning,</booktitle>
<pages>113--120</pages>
<contexts>
<context position="6283" citStr="Blei and Lafferty, 2006" startWordPosition="979" endWordPosition="982">considering at a segment (like paragraph or section) level. To make a tool like the HMM work at higher levels, one needs to make stronger assumptions, for instance assigning each sentence a single topic and then topic specific word models can be used: the hidden topic Markov model (Gruber et al., 2007) that models the transitional topic structure; a global model based on the generalised Mallows model (Chen et al., 2009), and a HMM based content model (Barzilay and Lee, 2004). Researchers have also considered timeseries of topics: various kinds of dynamic topic models, following early work of (Blei and Lafferty, 2006), represent a collection as a sequence of subcollections in epochs. Here, one is modelling the collections over broad epochs, not the structure of a single document that our model considers. This paper is organised as follows. We first present background theory in Section 2. Then the new model is presented in Section 3, followed by Gibbs sampling theory and algorithm in Sections 4 and 5 respectively. Experiments are reported in Section 6 with a conclusion in Section 7. 2 Background The basic topic model is first presented in Section 2.1, as a point of departure. In seeking to develop a general</context>
</contexts>
<marker>Blei, Lafferty, 2006</marker>
<rawString>D.M. Blei and J.D. Lafferty. 2006. Dynamic topic models. In ICML ’06: Proc. of 23rd international conference on Machine learning, pages 113–120.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D M Blei</author>
<author>P J Moreno</author>
</authors>
<title>Topic segmentation with an aspect hidden Markov model.</title>
<date>2001</date>
<booktitle>In Proc. of 24th annual international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>343--348</pages>
<contexts>
<context position="3178" citStr="Blei and Moreno, 2001" startWordPosition="481" endWordPosition="484">rlying statistical methods (Teh et al., 2006; Wallach et al., 2009). Topic models, like statistical parsing methods, are using more sophisticated latent variable methods in order to model different aspects of these problems. In this paper, we are interested in developing a new topic model which can take into account the structural topic dependency by following the higher level document subject structure, but we hope to retain the general flavour of topic models, where components (e.g., sentences) can be a mixture of topics. Thus we need to depart from the earlier HMM style models, see, e.g., (Blei and Moreno, 2001; Gruber et al., 2007). Inspired by the idea that documents usually exhibits internal structure (e.g., (Wang et al., 2011)), in which semantically related units are clustered together to form semantically structural segments, we treat documents as sequences of segments (e.g., sentences, paragraphs, sections, or chapters). In this way, we can model the topic correlation be535 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 535–545, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computati</context>
<context position="5475" citStr="Blei and Moreno, 2001" startWordPosition="845" endWordPosition="848">. There are two possible hybrids, one called “mixed” has distinct breaks in the sequence, while the other called “both” overlays both sequence and hierarchy and there could be relative strengths associated with the arrows. We employ the “both” hybrid but use the relative strengths to adaptively allow it to approximate the “mixed” hybrid. Research in Machine Learning and Natural Language Processing has attempted to model various topical dependencies. Some work considers structure within the sentence level by mixing hidden Markov models (HMMs) and topics on a word by word basis: the aspect HMM (Blei and Moreno, 2001) and the HMM-LDA model (Griffiths et al., 2005) that models both short-range syntactic dependencies and longer semantic dependencies. These models operate at a finer level than we are considering at a segment (like paragraph or section) level. To make a tool like the HMM work at higher levels, one needs to make stronger assumptions, for instance assigning each sentence a single topic and then topic specific word models can be used: the hidden topic Markov model (Gruber et al., 2007) that models the transitional topic structure; a global model based on the generalised Mallows model (Chen et al.</context>
</contexts>
<marker>Blei, Moreno, 2001</marker>
<rawString>D.M. Blei and P.J. Moreno. 2001. Topic segmentation with an aspect hidden Markov model. In Proc. of 24th annual international ACM SIGIR conference on Research and development in information retrieval, pages 343–348.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D M Blei</author>
<author>A Y Ng</author>
<author>M I Jordan</author>
</authors>
<title>Latent Dirichlet allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--993</pages>
<contexts>
<context position="7385" citStr="Blei et al., 2003" startWordPosition="1158" endWordPosition="1161">nd The basic topic model is first presented in Section 2.1, as a point of departure. In seeking to develop a general sequential topic model, we hope to go beyond a strictly sequential model and allow some hierarchical influence. This, however, presents two challenges: modelling and statistical inference. Hierarchical inference (and thus sequential inference) over probability vectors can be handled using the theory of hierarchical Poisson-Dirichlet processes (PDPs). This is presented in Section 2.2. 2.1 The LDA model The benchmark model for topic modelling is latent Dirichlet allocation (LDA) (Blei et al., 2003), a latent variable model of documents. Documents are indexed by i, and words w are observed data. The latent variables are µz (the topic distribution for a document) and z (the topic assignments for observed words), and the model parameter of Gk’s (word distributions). These notation are later extended in Ta9 li1 v2 v3 v4 li1 v2 li1 v2 6 v4 (H) µ (S) (B) 9 li1 v2 (M) 9 113 114 113 L14 536 ble 1. The generative model is as follows: ~φk ∼ DirichletW (~γ) ∀ k ~µi ∼ DirichletK (~α) ∀ i zi,l ∼ DiscreteK (~µi) ∀ i, l � � wi,l ∼ DiscreteK ~φzi,l ∀ i, l . DirichletK(·) is a K-dimensional Dirichlet di</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>D.M. Blei, A.Y. Ng, and M.I. Jordan. 2003. Latent Dirichlet allocation. Journal of Machine Learning Research, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Buntine</author>
<author>M Hutter</author>
</authors>
<title>A Bayesian view of the Poisson-Dirichlet process.</title>
<date>2012</date>
<tech>Technical Report arXiv:1007.0296v2,</tech>
<location>ArXiv, Cornell,</location>
<contexts>
<context position="10276" citStr="Buntine and Hutter, 2012" startWordPosition="1683" endWordPosition="1687">ta, the customer, is assigned to tables, and multiple tables can serve the same dish. and it represents the number of “tables” over which the nk “customers” are spread out. Thus the following constraints hold: 0 ≤ tk ≤ nk and tk = 0 iff nk = 0 . (2) When the distribution over probability vectors follows a Poisson-Dirichlet process which has two parameters τ ≡ (a, b) and the parent distribution ~µ0, then Fτ (~µ0) ≡ PDP(a, b, ~µ0). Here a is the discount parameter, b the concentration parameter and ~µ0 the base measure. In this case Bayesian analysis yields an augmented marginalised likelihood (Buntine and Hutter, 2012), after integrating out ~µ, of ((b)NT fj Stk,a (µ0,k)tk (3) k where T = Ek tk, (x|y)N = HN−1 n=0(x + ny) denotes the Pochhammer symbol, (x)N = (x|1)N, and SNM,a is a generalized Stirling number that is readily tabulated (Buntine and Hutter, 2012). There are two fundamental things to notice about Equation (3). Positively, the term in ~µ0 takes the form of a multinomial likelihood, so we can propagate it up and perform inference on ~µ0 unencumbered by the functional mess of Equation (1). Thus Poisson-Dirichlet processes allow one to do Bayesian reasoning on hierarchies of probability vectors (Te</context>
</contexts>
<marker>Buntine, Hutter, 2012</marker>
<rawString>W. Buntine and M. Hutter. 2012. A Bayesian view of the Poisson-Dirichlet process. Technical Report arXiv:1007.0296v2, ArXiv, Cornell, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Chen</author>
<author>S R K Branavan</author>
<author>R Barzilay</author>
<author>D R Karger</author>
</authors>
<title>Global models of document structure using latent permutations.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conf. of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>371--379</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="2534" citStr="Chen et al., 2009" startWordPosition="374" endWordPosition="377">et al., 2011; Eisenstein and Barzilay, 2008), often times replacing earlier semantic techniques such as latent semantic analysis (Deerwester et al., 1990). Topic models can be improved by better modelling the semantic aspects of text, for instance integrating collocations into the model (Johnson, 2010; Hardisty et al., 2010) or encouraging topics to be more semantically coherent (Newman et al., 2011) based on lexical coherence models (Newman et al., 2010), modelling the structural aspects of documents, for instance modelling a document as a set of segments (Du et al., 2010; Wang et al., 2011; Chen et al., 2009), or improving the underlying statistical methods (Teh et al., 2006; Wallach et al., 2009). Topic models, like statistical parsing methods, are using more sophisticated latent variable methods in order to model different aspects of these problems. In this paper, we are interested in developing a new topic model which can take into account the structural topic dependency by following the higher level document subject structure, but we hope to retain the general flavour of topic models, where components (e.g., sentences) can be a mixture of topics. Thus we need to depart from the earlier HMM sty</context>
<context position="6082" citStr="Chen et al., 2009" startWordPosition="947" endWordPosition="950">reno, 2001) and the HMM-LDA model (Griffiths et al., 2005) that models both short-range syntactic dependencies and longer semantic dependencies. These models operate at a finer level than we are considering at a segment (like paragraph or section) level. To make a tool like the HMM work at higher levels, one needs to make stronger assumptions, for instance assigning each sentence a single topic and then topic specific word models can be used: the hidden topic Markov model (Gruber et al., 2007) that models the transitional topic structure; a global model based on the generalised Mallows model (Chen et al., 2009), and a HMM based content model (Barzilay and Lee, 2004). Researchers have also considered timeseries of topics: various kinds of dynamic topic models, following early work of (Blei and Lafferty, 2006), represent a collection as a sequence of subcollections in epochs. Here, one is modelling the collections over broad epochs, not the structure of a single document that our model considers. This paper is organised as follows. We first present background theory in Section 2. Then the new model is presented in Section 3, followed by Gibbs sampling theory and algorithm in Sections 4 and 5 respectiv</context>
</contexts>
<marker>Chen, Branavan, Barzilay, Karger, 2009</marker>
<rawString>H. Chen, S.R.K. Branavan, R. Barzilay, and D.R. Karger. 2009. Global models of document structure using latent permutations. In Proceedings of Human Language Technologies: The 2009 Annual Conf. of the North American Chapter of the Association for Computational Linguistics, pages 371–379, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Chen</author>
<author>L Du</author>
<author>W Buntine</author>
</authors>
<title>Sampling for the Poisson-Dirichlet process.</title>
<date>2011</date>
<booktitle>In European Conf. on Machine Learning and Principles and Practice of Knowledge Discovery in Database,</booktitle>
<pages>296--311</pages>
<contexts>
<context position="11332" citStr="Chen et al., 2011" startWordPosition="1868" endWordPosition="1871">unencumbered by the functional mess of Equation (1). Thus Poisson-Dirichlet processes allow one to do Bayesian reasoning on hierarchies of probability vectors (Teh, 2006; Teh et al., 2006). Negatively, however, one needs to sample the auxiliary variables t~ leading to some problems: The range of tk, {0, ..., nk}, is broad. Also, contributions from individual data zi have been lost so the mixing of the MCMC can sometimes be slow. We confirmed these problems on our first implementation of the Adaptive Topic Model presented next in Section 3. A further improvement on PDP sampling is achieved in (Chen et al., 2011), where another auxiliary variable is introduced, a so-called table indicator, that for each datum zi indicates whether it is the “head of its table” (recall the nk data are spread over tk tables, each table has one and only one “head”). Let ri = 1 if zi is the “head of its table,” and zero otherwise. According to this “table” logic, the number of tables for nk must be the number of data zi that are also head of table, so tk = ENi=1 1zi=k1ri=1. Moreover, given this definition, the first constraint of Equation (2) on tk is p (~z,~t��τ, ~µ0, PDP) = 537 automatically satisfied. Finally, with tk t</context>
<context position="16214" citStr="Chen et al. (2011)" startWordPosition="2753" endWordPosition="2756">paragraph j − 1 and ~µi,j_1. si,j,k table count in the CPR for document i and paragraph j, for topic k that is inherited back to the document and ~µi. Ti,j total table count in the CRP for document i and segment j, equal to EKk=1 ti,j,k. Si,j total table count in the CRP for document i and segment j, equal to EKk=1 si,j,k. ~ti,j table count vector of ti,j,k’s for segment j. ~si,j table count vector of si,j,k’s for segment j. tion of marginal probabilities. Therefore, we have to use approximate inference techniques. This section proposes a blocked Gibbs sampling algorithm based on methods from Chen et al. (2011). Table 2 lists all statistics needed in the algorithm. Note for easier understanding, terminologies of the Chinese Restaurant Process (Teh et al., 2006) will be used, i.e., customers, dishes and restaurants, correspond to words, topics and segments respectively. The first major complication, over the use of the hierarchical PDP and Equation (3) and the table indicator trick of Equation (4), is handling the linear combination of ρi,j~νi,j−1 + (1 − ρi,j)~µi used in the PDPs. We manage this as follows: First, Equation (3) shows that a contribution of the form (µ0,k)tk results. In our case, this </context>
<context position="17712" citStr="Chen et al. (2011)" startWordPosition="3011" endWordPosition="3014"> parts, those that contribute to ~νi,j−1 and those that contribute to ~µi. We call these parts ti,j,k and si,j,k respectively. The product can then be expanded and ρi,j integrated out. This yields: H Beta (Si,j + λS, Ti,j + λT ) k The powers νi,j−1,k and µsi,j,k ti,j,k i,k can then be pushed up to the next nodes in the PDP/Dirichlet hierarchy. Note the standard constraints and table indicators are also needed here. The precise form of the table indicators needs to be considered as well since there is a hierarchy for them, and this is the second major complication in the model. As discussed in Chen et al. (2011), table indicators are not required to be recorded, instead, randomly sampled in Gibbs cycles. The table indicators when known can be used to reconstruct the table counts ti,j,k and si,j,k, and are reconstructed by sampling from them. For now, denote the table indicators as ui,j,l for word wi,j,l. To complete a formulation suitable for Gibbs sampling, we first compute the marginal distribution of the observations ~w1:I,1:J (words), the topic assignments ~z1:I,1:J and the table indicators ~u1:I,1:J. The Dirichlet integral is used to integrate out the document topic distributions ~µ1:I and the t</context>
</contexts>
<marker>Chen, Du, Buntine, 2011</marker>
<rawString>C. Chen, L. Du, and W. Buntine. 2011. Sampling for the Poisson-Dirichlet process. In European Conf. on Machine Learning and Principles and Practice of Knowledge Discovery in Database, pages 296–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S C Deerwester</author>
<author>S T Dumais</author>
<author>T K Landauer</author>
<author>G W Furnas</author>
<author>R A Harshman</author>
</authors>
<title>Indexing by latent semantic analysis.</title>
<date>1990</date>
<journal>Journal of the American Society of Information Science,</journal>
<volume>41</volume>
<issue>6</issue>
<contexts>
<context position="2070" citStr="Deerwester et al., 1990" startWordPosition="294" endWordPosition="297">he structure of a document, for instance, summarisation and segmentation, should also be improved by topic models that better model that structure. Recently, topic models are increasingly being used for text analysis tasks such as summarisa*This work was partially done when Du was at College of Engineering &amp; Computer Science, the Australian National University when working together with Buntine and Jin there. tion (Arora and Ravindran, 2008) and segmentation (Misra et al., 2011; Eisenstein and Barzilay, 2008), often times replacing earlier semantic techniques such as latent semantic analysis (Deerwester et al., 1990). Topic models can be improved by better modelling the semantic aspects of text, for instance integrating collocations into the model (Johnson, 2010; Hardisty et al., 2010) or encouraging topics to be more semantically coherent (Newman et al., 2011) based on lexical coherence models (Newman et al., 2010), modelling the structural aspects of documents, for instance modelling a document as a set of segments (Du et al., 2010; Wang et al., 2011; Chen et al., 2009), or improving the underlying statistical methods (Teh et al., 2006; Wallach et al., 2009). Topic models, like statistical parsing metho</context>
</contexts>
<marker>Deerwester, Dumais, Landauer, Furnas, Harshman, 1990</marker>
<rawString>S.C. Deerwester, S.T. Dumais, T.K. Landauer, G.W. Furnas, and R.A. Harshman. 1990. Indexing by latent semantic analysis. Journal of the American Society of Information Science, 41(6):391–407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Du</author>
<author>W Buntine</author>
<author>H Jin</author>
</authors>
<title>A segmented topic model based on the two-parameter Poisson-Dirichlet process.</title>
<date>2010</date>
<booktitle>Machine Learning,</booktitle>
<pages>81--5</pages>
<contexts>
<context position="2495" citStr="Du et al., 2010" startWordPosition="366" endWordPosition="369">dran, 2008) and segmentation (Misra et al., 2011; Eisenstein and Barzilay, 2008), often times replacing earlier semantic techniques such as latent semantic analysis (Deerwester et al., 1990). Topic models can be improved by better modelling the semantic aspects of text, for instance integrating collocations into the model (Johnson, 2010; Hardisty et al., 2010) or encouraging topics to be more semantically coherent (Newman et al., 2011) based on lexical coherence models (Newman et al., 2010), modelling the structural aspects of documents, for instance modelling a document as a set of segments (Du et al., 2010; Wang et al., 2011; Chen et al., 2009), or improving the underlying statistical methods (Teh et al., 2006; Wallach et al., 2009). Topic models, like statistical parsing methods, are using more sophisticated latent variable methods in order to model different aspects of these problems. In this paper, we are interested in developing a new topic model which can take into account the structural topic dependency by following the higher level document subject structure, but we hope to retain the general flavour of topic models, where components (e.g., sentences) can be a mixture of topics. Thus we </context>
<context position="4202" citStr="Du et al., 2010" startWordPosition="637" endWordPosition="640">Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 535–545, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics Figure 1: Different structural relationships for topics of sections in a 4-part document, hierarchical (H), sequential (S), both (B) or mixed (M). tween the segments in a “bag of segments” fashion, i.e., beyond the “bag of words” assumption, and reveal how topics evolve among segments. Indeed, we were impressed by the improvement in perplexity obtained by the segmented topic model (STM) (Du et al., 2010), so we considered the problem of whether one can add sequence information into a structured topic model as well. Figure 1 illustrates the type of structural information being considered, where the vectors are some representation of the content. STM is represented by the hierarchical model. A strictly sequential model would seem unrealistic for some documents, for instance books. A topic model using the strictly sequential model was developed (Du et al., 2012) but it reportedly performs halfway between STM and LDA. In this paper, we develop an adaptive topic model to go beyond a strictly seque</context>
<context position="23256" citStr="Du et al., 2010" startWordPosition="4020" endWordPosition="4023">d but fairly straight forward changes to the posterior of Equation (5). Thus a full blocked sampler for (zi,j,l, ui,j,l) can be constructed. Estimates: learnt values of µi, vi,j, Ok are needed for evaluation, perplexity calculations, etc. These are estimated by taking averages after the Gibbs sampler has burnt in, using the standard posterior means for Dirichlets and Poisson-Dirichlets. 6 Experiments In the experimental work, we have three objectives: (1) to explore the setting of hyper-parameters, (2) to compare the model with the earlier sequential LDA (SeqLDA) of (Du et al., 2012), STM of (Du et al., 2010) and standard LDA, and (3) to view the results in detail on a number of characteristic problems. 540 Table 3: Datasets #docs #segs #words vocab Pat-A 500 51,748 2,146,464 16,573 Pat-B 397 9,123 417,631 7,663 Pat-G06 500 11,938 655,694 6,844 Pat-H 500 11,662 562,439 10,114 Pat-F 140 3,181 166,091 4,674 Prince-C 1 26 10,588 3,292 Prince-P 1 192 10.588 3,292 Moby Dick 1 135 88,802 16,223 6.1 Datasets For general testing, five patent datasets are randomly selected from U.S. patents granted in 2009 and 2010. Patents in Pat-A are selected from international patent class (IPC) “A”, which is about “HU</context>
<context position="33542" citStr="Du et al., 2010" startWordPosition="5803" endWordPosition="5806">of whales aqua: (peaks at 8, 82, 88) pulpit, schools and mythology of whaling. We see that AdaTM can be used to understand the topics with regards to the sequential structure of a book. In contrast, the sequential nature for LDA and STM is lost in the noise. It can be very interesting to apply the proposed topic models to some text analysis tasks, such as topic segmentation, summarisation, and semantic title evaluation, which are subject to our future work. 7 Conclusion A model for adaptive sequential topic modelling has been developed to improve over a simple exchangeable segments model STM (Du et al., 2010) and a naive sequential model SeqLDA (Du et al., 2012) in terms of perplexity and its confirmed ability to uncover sequential structure in the topics. One could extract meaningful topics from a book like Herman Melville’s “Moby Dick” and concurrently gain their sequential profile. The current Gibbs sampler is slower than regular LDA, so future work is to speed up the algorithm. Acknowledgments The authors would like to thank all the anonymous reviewers for their valuable comments. Lan Du was supported under the Australian Research Council’s Discovery Projects funding scheme (project numbers DP</context>
</contexts>
<marker>Du, Buntine, Jin, 2010</marker>
<rawString>L. Du, W. Buntine, and H. Jin. 2010. A segmented topic model based on the two-parameter Poisson-Dirichlet process. Machine Learning, 81:5–19.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Du</author>
<author>W Buntine</author>
<author>H Jin</author>
<author>C Chen</author>
</authors>
<title>Sequential latent dirichlet allocation.</title>
<date>2012</date>
<journal>Knowledge and Information Systems,</journal>
<volume>31</volume>
<issue>3</issue>
<contexts>
<context position="4666" citStr="Du et al., 2012" startWordPosition="713" endWordPosition="716">w topics evolve among segments. Indeed, we were impressed by the improvement in perplexity obtained by the segmented topic model (STM) (Du et al., 2010), so we considered the problem of whether one can add sequence information into a structured topic model as well. Figure 1 illustrates the type of structural information being considered, where the vectors are some representation of the content. STM is represented by the hierarchical model. A strictly sequential model would seem unrealistic for some documents, for instance books. A topic model using the strictly sequential model was developed (Du et al., 2012) but it reportedly performs halfway between STM and LDA. In this paper, we develop an adaptive topic model to go beyond a strictly sequential model while allow some hierarchical influence. There are two possible hybrids, one called “mixed” has distinct breaks in the sequence, while the other called “both” overlays both sequence and hierarchy and there could be relative strengths associated with the arrows. We employ the “both” hybrid but use the relative strengths to adaptively allow it to approximate the “mixed” hybrid. Research in Machine Learning and Natural Language Processing has attempte</context>
<context position="23230" citStr="Du et al., 2012" startWordPosition="4014" endWordPosition="4017">sibilities lead to detailed but fairly straight forward changes to the posterior of Equation (5). Thus a full blocked sampler for (zi,j,l, ui,j,l) can be constructed. Estimates: learnt values of µi, vi,j, Ok are needed for evaluation, perplexity calculations, etc. These are estimated by taking averages after the Gibbs sampler has burnt in, using the standard posterior means for Dirichlets and Poisson-Dirichlets. 6 Experiments In the experimental work, we have three objectives: (1) to explore the setting of hyper-parameters, (2) to compare the model with the earlier sequential LDA (SeqLDA) of (Du et al., 2012), STM of (Du et al., 2010) and standard LDA, and (3) to view the results in detail on a number of characteristic problems. 540 Table 3: Datasets #docs #segs #words vocab Pat-A 500 51,748 2,146,464 16,573 Pat-B 397 9,123 417,631 7,663 Pat-G06 500 11,938 655,694 6,844 Pat-H 500 11,662 562,439 10,114 Pat-F 140 3,181 166,091 4,674 Prince-C 1 26 10,588 3,292 Prince-P 1 192 10.588 3,292 Moby Dick 1 135 88,802 16,223 6.1 Datasets For general testing, five patent datasets are randomly selected from U.S. patents granted in 2009 and 2010. Patents in Pat-A are selected from international patent class (IP</context>
<context position="28216" citStr="Du et al., 2012" startWordPosition="4904" endWordPosition="4907">lexity 1810 2130 1450 1660 1830 Perplexity 300 1510 1530 1150 1360 1230 1000 1210 930 850 1060 700 630 510 25 50100 15 Number of Topics 9100510 25 50100 15 Number of Topics 510 25 50 100 150 Number of Topics perplexity. In contrast, Figure 3(a) shows different b values significantly change perplexity. Therefore, we sought to optimise b. The experiment of fixing AS = 1 and changing AT shows a small AT is preferred. 6.4 Perplexity Comparison Perplexity comparisons were done with the default settings a = 0.2, α = 0.1, -y = 0.01, AS = 1, AT = 1 and b optimised automatically using the scheme from (Du et al., 2012). Figure 5 shows the results on these five patent datasets for different numbers of topics. LDA D is LDA run on whole patents, and LDA P is LDA run on the paragraphs within patents. Table 4 gives the p-values of a onetail paired t-test for AdaTM versus the others, where lower p-value indicates AdaTM has statistically significant lower perplexity. From this we can see that AdaTM is statistically significantly better than SeqLDA and LDA, and somewhat better than STM. In addition, we ran another set of experiments by randomly shuffling the order of paragraphs in each patent several times before r</context>
<context position="33596" citStr="Du et al., 2012" startWordPosition="5813" endWordPosition="5816">nd mythology of whaling. We see that AdaTM can be used to understand the topics with regards to the sequential structure of a book. In contrast, the sequential nature for LDA and STM is lost in the noise. It can be very interesting to apply the proposed topic models to some text analysis tasks, such as topic segmentation, summarisation, and semantic title evaluation, which are subject to our future work. 7 Conclusion A model for adaptive sequential topic modelling has been developed to improve over a simple exchangeable segments model STM (Du et al., 2010) and a naive sequential model SeqLDA (Du et al., 2012) in terms of perplexity and its confirmed ability to uncover sequential structure in the topics. One could extract meaningful topics from a book like Herman Melville’s “Moby Dick” and concurrently gain their sequential profile. The current Gibbs sampler is slower than regular LDA, so future work is to speed up the algorithm. Acknowledgments The authors would like to thank all the anonymous reviewers for their valuable comments. Lan Du was supported under the Australian Research Council’s Discovery Projects funding scheme (project numbers DP110102506 and DP110102593). Dr. Huidong Jin was partly</context>
</contexts>
<marker>Du, Buntine, Jin, Chen, 2012</marker>
<rawString>L. Du, W. Buntine, H. Jin, and C. Chen. 2012. Sequential latent dirichlet allocation. Knowledge and Information Systems, 31(3):475–503.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Eisenstein</author>
<author>R Barzilay</author>
</authors>
<title>Bayesian unsupervised topic segmentation.</title>
<date>2008</date>
<booktitle>In Proc. of Conf. on Empirical Methods in Natural Language Processing,</booktitle>
<pages>334--343</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1960" citStr="Eisenstein and Barzilay, 2008" startWordPosition="278" endWordPosition="281">should lead to improved topic modelling. It also seems reasonable to propose that text analysis tasks that involve the structure of a document, for instance, summarisation and segmentation, should also be improved by topic models that better model that structure. Recently, topic models are increasingly being used for text analysis tasks such as summarisa*This work was partially done when Du was at College of Engineering &amp; Computer Science, the Australian National University when working together with Buntine and Jin there. tion (Arora and Ravindran, 2008) and segmentation (Misra et al., 2011; Eisenstein and Barzilay, 2008), often times replacing earlier semantic techniques such as latent semantic analysis (Deerwester et al., 1990). Topic models can be improved by better modelling the semantic aspects of text, for instance integrating collocations into the model (Johnson, 2010; Hardisty et al., 2010) or encouraging topics to be more semantically coherent (Newman et al., 2011) based on lexical coherence models (Newman et al., 2010), modelling the structural aspects of documents, for instance modelling a document as a set of segments (Du et al., 2010; Wang et al., 2011; Chen et al., 2009), or improving the underly</context>
</contexts>
<marker>Eisenstein, Barzilay, 2008</marker>
<rawString>J. Eisenstein and R. Barzilay. 2008. Bayesian unsupervised topic segmentation. In Proc. of Conf. on Empirical Methods in Natural Language Processing, pages 334–343. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T L Griffiths</author>
<author>M Steyvers</author>
<author>D M Blei</author>
<author>J B Tenenbaum</author>
</authors>
<title>Integrating topics and syntax.</title>
<date>2005</date>
<booktitle>In Advances in Neural Information Processing Systems 17,</booktitle>
<pages>537--544</pages>
<contexts>
<context position="5522" citStr="Griffiths et al., 2005" startWordPosition="853" endWordPosition="856">mixed” has distinct breaks in the sequence, while the other called “both” overlays both sequence and hierarchy and there could be relative strengths associated with the arrows. We employ the “both” hybrid but use the relative strengths to adaptively allow it to approximate the “mixed” hybrid. Research in Machine Learning and Natural Language Processing has attempted to model various topical dependencies. Some work considers structure within the sentence level by mixing hidden Markov models (HMMs) and topics on a word by word basis: the aspect HMM (Blei and Moreno, 2001) and the HMM-LDA model (Griffiths et al., 2005) that models both short-range syntactic dependencies and longer semantic dependencies. These models operate at a finer level than we are considering at a segment (like paragraph or section) level. To make a tool like the HMM work at higher levels, one needs to make stronger assumptions, for instance assigning each sentence a single topic and then topic specific word models can be used: the hidden topic Markov model (Gruber et al., 2007) that models the transitional topic structure; a global model based on the generalised Mallows model (Chen et al., 2009), and a HMM based content model (Barzila</context>
</contexts>
<marker>Griffiths, Steyvers, Blei, Tenenbaum, 2005</marker>
<rawString>T.L. Griffiths, M. Steyvers, D.M. Blei, and J.B. Tenenbaum. 2005. Integrating topics and syntax. In Advances in Neural Information Processing Systems 17, pages 537–544.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Gruber</author>
<author>Y Weiss</author>
<author>M Rosen-Zvi</author>
</authors>
<title>Hidden topic markov models.</title>
<date>2007</date>
<journal>Journal of Machine Learning Research - Proceedings Track,</journal>
<pages>2--163</pages>
<contexts>
<context position="3200" citStr="Gruber et al., 2007" startWordPosition="485" endWordPosition="488">ods (Teh et al., 2006; Wallach et al., 2009). Topic models, like statistical parsing methods, are using more sophisticated latent variable methods in order to model different aspects of these problems. In this paper, we are interested in developing a new topic model which can take into account the structural topic dependency by following the higher level document subject structure, but we hope to retain the general flavour of topic models, where components (e.g., sentences) can be a mixture of topics. Thus we need to depart from the earlier HMM style models, see, e.g., (Blei and Moreno, 2001; Gruber et al., 2007). Inspired by the idea that documents usually exhibits internal structure (e.g., (Wang et al., 2011)), in which semantically related units are clustered together to form semantically structural segments, we treat documents as sequences of segments (e.g., sentences, paragraphs, sections, or chapters). In this way, we can model the topic correlation be535 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 535–545, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics Figur</context>
<context position="5962" citStr="Gruber et al., 2007" startWordPosition="928" endWordPosition="931">n the sentence level by mixing hidden Markov models (HMMs) and topics on a word by word basis: the aspect HMM (Blei and Moreno, 2001) and the HMM-LDA model (Griffiths et al., 2005) that models both short-range syntactic dependencies and longer semantic dependencies. These models operate at a finer level than we are considering at a segment (like paragraph or section) level. To make a tool like the HMM work at higher levels, one needs to make stronger assumptions, for instance assigning each sentence a single topic and then topic specific word models can be used: the hidden topic Markov model (Gruber et al., 2007) that models the transitional topic structure; a global model based on the generalised Mallows model (Chen et al., 2009), and a HMM based content model (Barzilay and Lee, 2004). Researchers have also considered timeseries of topics: various kinds of dynamic topic models, following early work of (Blei and Lafferty, 2006), represent a collection as a sequence of subcollections in epochs. Here, one is modelling the collections over broad epochs, not the structure of a single document that our model considers. This paper is organised as follows. We first present background theory in Section 2. The</context>
</contexts>
<marker>Gruber, Weiss, Rosen-Zvi, 2007</marker>
<rawString>A. Gruber, Y. Weiss, and M. Rosen-Zvi. 2007. Hidden topic markov models. Journal of Machine Learning Research - Proceedings Track, 2:163–170.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E A Hardisty</author>
<author>J Boyd-Graber</author>
<author>P Resnik</author>
</authors>
<title>Modeling perspective using adaptor grammars.</title>
<date>2010</date>
<booktitle>In Proc. of the 2010 Conf. on Empirical Methods in Natural Language Processing,</booktitle>
<pages>284--292</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="2242" citStr="Hardisty et al., 2010" startWordPosition="323" endWordPosition="326"> increasingly being used for text analysis tasks such as summarisa*This work was partially done when Du was at College of Engineering &amp; Computer Science, the Australian National University when working together with Buntine and Jin there. tion (Arora and Ravindran, 2008) and segmentation (Misra et al., 2011; Eisenstein and Barzilay, 2008), often times replacing earlier semantic techniques such as latent semantic analysis (Deerwester et al., 1990). Topic models can be improved by better modelling the semantic aspects of text, for instance integrating collocations into the model (Johnson, 2010; Hardisty et al., 2010) or encouraging topics to be more semantically coherent (Newman et al., 2011) based on lexical coherence models (Newman et al., 2010), modelling the structural aspects of documents, for instance modelling a document as a set of segments (Du et al., 2010; Wang et al., 2011; Chen et al., 2009), or improving the underlying statistical methods (Teh et al., 2006; Wallach et al., 2009). Topic models, like statistical parsing methods, are using more sophisticated latent variable methods in order to model different aspects of these problems. In this paper, we are interested in developing a new topic m</context>
</contexts>
<marker>Hardisty, Boyd-Graber, Resnik, 2010</marker>
<rawString>E.A. Hardisty, J. Boyd-Graber, and P. Resnik. 2010. Modeling perspective using adaptor grammars. In Proc. of the 2010 Conf. on Empirical Methods in Natural Language Processing, pages 284–292, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Johnson</author>
</authors>
<title>PCFGs, topic models, adaptor grammars and learning topical collocations and the structure of proper names.</title>
<date>2010</date>
<booktitle>In Proc. of 48th Annual Meeting of the ACL,</booktitle>
<pages>1148--1157</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="2218" citStr="Johnson, 2010" startWordPosition="320" endWordPosition="322">opic models are increasingly being used for text analysis tasks such as summarisa*This work was partially done when Du was at College of Engineering &amp; Computer Science, the Australian National University when working together with Buntine and Jin there. tion (Arora and Ravindran, 2008) and segmentation (Misra et al., 2011; Eisenstein and Barzilay, 2008), often times replacing earlier semantic techniques such as latent semantic analysis (Deerwester et al., 1990). Topic models can be improved by better modelling the semantic aspects of text, for instance integrating collocations into the model (Johnson, 2010; Hardisty et al., 2010) or encouraging topics to be more semantically coherent (Newman et al., 2011) based on lexical coherence models (Newman et al., 2010), modelling the structural aspects of documents, for instance modelling a document as a set of segments (Du et al., 2010; Wang et al., 2011; Chen et al., 2009), or improving the underlying statistical methods (Teh et al., 2006; Wallach et al., 2009). Topic models, like statistical parsing methods, are using more sophisticated latent variable methods in order to model different aspects of these problems. In this paper, we are interested in </context>
</contexts>
<marker>Johnson, 2010</marker>
<rawString>M. Johnson. 2010. PCFGs, topic models, adaptor grammars and learning topical collocations and the structure of proper names. In Proc. of 48th Annual Meeting of the ACL, pages 1148–1157, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Misra</author>
<author>F Yvon</author>
<author>O Capp</author>
<author>J Jose</author>
</authors>
<title>Text segmentation: A topic modeling perspective.</title>
<date>2011</date>
<journal>Information Processing &amp; Management,</journal>
<volume>47</volume>
<issue>4</issue>
<contexts>
<context position="1928" citStr="Misra et al., 2011" startWordPosition="274" endWordPosition="277"> topical dependency should lead to improved topic modelling. It also seems reasonable to propose that text analysis tasks that involve the structure of a document, for instance, summarisation and segmentation, should also be improved by topic models that better model that structure. Recently, topic models are increasingly being used for text analysis tasks such as summarisa*This work was partially done when Du was at College of Engineering &amp; Computer Science, the Australian National University when working together with Buntine and Jin there. tion (Arora and Ravindran, 2008) and segmentation (Misra et al., 2011; Eisenstein and Barzilay, 2008), often times replacing earlier semantic techniques such as latent semantic analysis (Deerwester et al., 1990). Topic models can be improved by better modelling the semantic aspects of text, for instance integrating collocations into the model (Johnson, 2010; Hardisty et al., 2010) or encouraging topics to be more semantically coherent (Newman et al., 2011) based on lexical coherence models (Newman et al., 2010), modelling the structural aspects of documents, for instance modelling a document as a set of segments (Du et al., 2010; Wang et al., 2011; Chen et al.,</context>
</contexts>
<marker>Misra, Yvon, Capp, Jose, 2011</marker>
<rawString>H. Misra, F. Yvon, O. Capp, and J. Jose. 2011. Text segmentation: A topic modeling perspective. Information Processing &amp; Management, 47(4):528–544.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Newman</author>
<author>J H Lau</author>
<author>K Grieser</author>
<author>T Baldwin</author>
</authors>
<title>Automatic evaluation of topic coherence.</title>
<date>2010</date>
<booktitle>In North American Chapter of the Association for Computational Linguistics - Human Language Technologies,</booktitle>
<pages>100--108</pages>
<contexts>
<context position="2375" citStr="Newman et al., 2010" startWordPosition="345" endWordPosition="349"> Computer Science, the Australian National University when working together with Buntine and Jin there. tion (Arora and Ravindran, 2008) and segmentation (Misra et al., 2011; Eisenstein and Barzilay, 2008), often times replacing earlier semantic techniques such as latent semantic analysis (Deerwester et al., 1990). Topic models can be improved by better modelling the semantic aspects of text, for instance integrating collocations into the model (Johnson, 2010; Hardisty et al., 2010) or encouraging topics to be more semantically coherent (Newman et al., 2011) based on lexical coherence models (Newman et al., 2010), modelling the structural aspects of documents, for instance modelling a document as a set of segments (Du et al., 2010; Wang et al., 2011; Chen et al., 2009), or improving the underlying statistical methods (Teh et al., 2006; Wallach et al., 2009). Topic models, like statistical parsing methods, are using more sophisticated latent variable methods in order to model different aspects of these problems. In this paper, we are interested in developing a new topic model which can take into account the structural topic dependency by following the higher level document subject structure, but we hop</context>
</contexts>
<marker>Newman, Lau, Grieser, Baldwin, 2010</marker>
<rawString>D. Newman, J.H. Lau, K. Grieser, and T. Baldwin. 2010. Automatic evaluation of topic coherence. In North American Chapter of the Association for Computational Linguistics - Human Language Technologies, pages 100–108.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Newman</author>
<author>E V Bonilla</author>
<author>W Buntine</author>
</authors>
<title>Improving topic coherence with regularized topic models.</title>
<date>2011</date>
<booktitle>Advances in Neural Information Processing Systems 24,</booktitle>
<pages>496--504</pages>
<editor>In J. Shawe-Taylor, R.S. Zemel, P. Bartlett, F.C.N. Pereira, and K.Q. Weinberger, editors,</editor>
<contexts>
<context position="2319" citStr="Newman et al., 2011" startWordPosition="336" endWordPosition="339">s partially done when Du was at College of Engineering &amp; Computer Science, the Australian National University when working together with Buntine and Jin there. tion (Arora and Ravindran, 2008) and segmentation (Misra et al., 2011; Eisenstein and Barzilay, 2008), often times replacing earlier semantic techniques such as latent semantic analysis (Deerwester et al., 1990). Topic models can be improved by better modelling the semantic aspects of text, for instance integrating collocations into the model (Johnson, 2010; Hardisty et al., 2010) or encouraging topics to be more semantically coherent (Newman et al., 2011) based on lexical coherence models (Newman et al., 2010), modelling the structural aspects of documents, for instance modelling a document as a set of segments (Du et al., 2010; Wang et al., 2011; Chen et al., 2009), or improving the underlying statistical methods (Teh et al., 2006; Wallach et al., 2009). Topic models, like statistical parsing methods, are using more sophisticated latent variable methods in order to model different aspects of these problems. In this paper, we are interested in developing a new topic model which can take into account the structural topic dependency by following</context>
</contexts>
<marker>Newman, Bonilla, Buntine, 2011</marker>
<rawString>D. Newman, E.V. Bonilla, and W. Buntine. 2011. Improving topic coherence with regularized topic models. In J. Shawe-Taylor, R.S. Zemel, P. Bartlett, F.C.N. Pereira, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems 24, pages 496–504.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C P Robert</author>
<author>G Casella</author>
</authors>
<title>Monte Carlo statistical methods.</title>
<date>2004</date>
<publisher>Springer.</publisher>
<note>second edition.</note>
<contexts>
<context position="9377" citStr="Robert and Casella, 2004" startWordPosition="1519" endWordPosition="1522"> Commonly in topic modelling, the Dirichlet distribution is used for discrete probability vectors. In this case Fτ(~µ0) ≡ DirichletK(b~µ0), τ ≡ (K, b) where b is the concentration parameter. Bayesian analysis yields a marginalised likelihood, after integrating out ~µ, of p (~zlτ, ~µ0, Dirichlet) = Beta (~n + b~µ0) (1) Beta (b~µ0) where Beta(·) is the vector valued function normalising the Dirichlet distribution. A problem here is that p(~z|b, ~µ0) is an intractable function of ~µ0. Dirichlet processes and Poisson-Dirichlet processes alleviate this problem by using an auxiliary variable trick (Robert and Casella, 2004). That is, we introduce an auxiliary variable over which we also sample but do not need to record. The auxiliary variable is the table count1 which is a tk for each nk 1Based on the Chinese Restaurant analogy (Teh et al., 2006), each table has a dish, a data value, while data, the customer, is assigned to tables, and multiple tables can serve the same dish. and it represents the number of “tables” over which the nk “customers” are spread out. Thus the following constraints hold: 0 ≤ tk ≤ nk and tk = 0 iff nk = 0 . (2) When the distribution over probability vectors follows a Poisson-Dirichlet p</context>
</contexts>
<marker>Robert, Casella, 2004</marker>
<rawString>C.P. Robert and G. Casella. 2004. Monte Carlo statistical methods. Springer. second edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Rosen-Zvi</author>
<author>T Griffiths</author>
<author>M Steyvers</author>
<author>P Smyth</author>
</authors>
<title>The author-topic model for authors and documents.</title>
<date>2004</date>
<booktitle>In Proc. of 20th conference on Uncertainty in Artificial Intelligence,</booktitle>
<pages>487--494</pages>
<contexts>
<context position="25049" citStr="Rosen-Zvi et al., 2004" startWordPosition="4310" endWordPosition="4313">, the top 10 common words, the uncommon words (i.e., words in less than five patents) and numbers have been removed. Two books used for more detailed investigation are “The Prince” by Niccol`o Machiavelli and “Moby Dick” by Herman Melville. They are split into chapters and/or paragraphs which are treated as segments, and only stop-words are removed. Table 3 shows in detail the statistics of these datasets after preprocessing. 6.2 Design Perplexity, a standard measure of dictionary-based compressibility, is used for comparison. When reporting test perplexities, the held-out perplexity measure (Rosen-Zvi et al., 2004) is used to evaluate the generalisation capability to the unseen data. This is known to be unbiased. To compute the held-out perplexity, 20% of patents in each data set was ranFigure 3: Analysis of parameters of Poisson-Dirichlet process. (a) shows how perplexity changes with b; (b) shows how it changes with a. Figure 4: Analysis of the two parameters for Beta distribution. (a) how perplexity changes with AS; (b) how it changes with AT. domly held out from training to be used for testing. For this, 1000 Gibbs cycles were done for burn-in followed by 500 cycles with a lag for 100 for parameter </context>
</contexts>
<marker>Rosen-Zvi, Griffiths, Steyvers, Smyth, 2004</marker>
<rawString>M. Rosen-Zvi, T. Griffiths, M. Steyvers, and P. Smyth. 2004. The author-topic model for authors and documents. In Proc. of 20th conference on Uncertainty in Artificial Intelligence, pages 487–494.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y W Teh</author>
<author>M I Jordan</author>
<author>M J Beal</author>
<author>D M Blei</author>
</authors>
<title>Hierarchical Dirichlet processes.</title>
<date>2006</date>
<journal>Journal of the American Statistical Association,</journal>
<pages>101--1566</pages>
<contexts>
<context position="2601" citStr="Teh et al., 2006" startWordPosition="385" endWordPosition="388">arlier semantic techniques such as latent semantic analysis (Deerwester et al., 1990). Topic models can be improved by better modelling the semantic aspects of text, for instance integrating collocations into the model (Johnson, 2010; Hardisty et al., 2010) or encouraging topics to be more semantically coherent (Newman et al., 2011) based on lexical coherence models (Newman et al., 2010), modelling the structural aspects of documents, for instance modelling a document as a set of segments (Du et al., 2010; Wang et al., 2011; Chen et al., 2009), or improving the underlying statistical methods (Teh et al., 2006; Wallach et al., 2009). Topic models, like statistical parsing methods, are using more sophisticated latent variable methods in order to model different aspects of these problems. In this paper, we are interested in developing a new topic model which can take into account the structural topic dependency by following the higher level document subject structure, but we hope to retain the general flavour of topic models, where components (e.g., sentences) can be a mixture of topics. Thus we need to depart from the earlier HMM style models, see, e.g., (Blei and Moreno, 2001; Gruber et al., 2007).</context>
<context position="9604" citStr="Teh et al., 2006" startWordPosition="1561" endWordPosition="1564">lihood, after integrating out ~µ, of p (~zlτ, ~µ0, Dirichlet) = Beta (~n + b~µ0) (1) Beta (b~µ0) where Beta(·) is the vector valued function normalising the Dirichlet distribution. A problem here is that p(~z|b, ~µ0) is an intractable function of ~µ0. Dirichlet processes and Poisson-Dirichlet processes alleviate this problem by using an auxiliary variable trick (Robert and Casella, 2004). That is, we introduce an auxiliary variable over which we also sample but do not need to record. The auxiliary variable is the table count1 which is a tk for each nk 1Based on the Chinese Restaurant analogy (Teh et al., 2006), each table has a dish, a data value, while data, the customer, is assigned to tables, and multiple tables can serve the same dish. and it represents the number of “tables” over which the nk “customers” are spread out. Thus the following constraints hold: 0 ≤ tk ≤ nk and tk = 0 iff nk = 0 . (2) When the distribution over probability vectors follows a Poisson-Dirichlet process which has two parameters τ ≡ (a, b) and the parent distribution ~µ0, then Fτ (~µ0) ≡ PDP(a, b, ~µ0). Here a is the discount parameter, b the concentration parameter and ~µ0 the base measure. In this case Bayesian analysi</context>
<context position="10902" citStr="Teh et al., 2006" startWordPosition="1792" endWordPosition="1795">ntegrating out ~µ, of ((b)NT fj Stk,a (µ0,k)tk (3) k where T = Ek tk, (x|y)N = HN−1 n=0(x + ny) denotes the Pochhammer symbol, (x)N = (x|1)N, and SNM,a is a generalized Stirling number that is readily tabulated (Buntine and Hutter, 2012). There are two fundamental things to notice about Equation (3). Positively, the term in ~µ0 takes the form of a multinomial likelihood, so we can propagate it up and perform inference on ~µ0 unencumbered by the functional mess of Equation (1). Thus Poisson-Dirichlet processes allow one to do Bayesian reasoning on hierarchies of probability vectors (Teh, 2006; Teh et al., 2006). Negatively, however, one needs to sample the auxiliary variables t~ leading to some problems: The range of tk, {0, ..., nk}, is broad. Also, contributions from individual data zi have been lost so the mixing of the MCMC can sometimes be slow. We confirmed these problems on our first implementation of the Adaptive Topic Model presented next in Section 3. A further improvement on PDP sampling is achieved in (Chen et al., 2011), where another auxiliary variable is introduced, a so-called table indicator, that for each datum zi indicates whether it is the “head of its table” (recall the nk data </context>
<context position="16367" citStr="Teh et al., 2006" startWordPosition="2777" endWordPosition="2780">j total table count in the CRP for document i and segment j, equal to EKk=1 ti,j,k. Si,j total table count in the CRP for document i and segment j, equal to EKk=1 si,j,k. ~ti,j table count vector of ti,j,k’s for segment j. ~si,j table count vector of si,j,k’s for segment j. tion of marginal probabilities. Therefore, we have to use approximate inference techniques. This section proposes a blocked Gibbs sampling algorithm based on methods from Chen et al. (2011). Table 2 lists all statistics needed in the algorithm. Note for easier understanding, terminologies of the Chinese Restaurant Process (Teh et al., 2006) will be used, i.e., customers, dishes and restaurants, correspond to words, topics and segments respectively. The first major complication, over the use of the hierarchical PDP and Equation (3) and the table indicator trick of Equation (4), is handling the linear combination of ρi,j~νi,j−1 + (1 − ρi,j)~µi used in the PDPs. We manage this as follows: First, Equation (3) shows that a contribution of the form (µ0,k)tk results. In our case, this becomes H , k 1pi,jνi,j−1,k + (1 − ρi,j)µi,k)ti,j,k ( where t0i,j,k is the corresponding introduced auxiliary variable the table count which is involved </context>
</contexts>
<marker>Teh, Jordan, Beal, Blei, 2006</marker>
<rawString>Y. W. Teh, M. I. Jordan, M. J. Beal, and D. M. Blei. 2006. Hierarchical Dirichlet processes. Journal of the American Statistical Association, 101:1566–1581.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y W Teh</author>
</authors>
<title>A hierarchical Bayesian language model based on Pitman-Yor processes.</title>
<date>2006</date>
<booktitle>In Proc. of 21st Inter. Conf. on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics,</booktitle>
<pages>985--992</pages>
<contexts>
<context position="10883" citStr="Teh, 2006" startWordPosition="1790" endWordPosition="1791">2), after integrating out ~µ, of ((b)NT fj Stk,a (µ0,k)tk (3) k where T = Ek tk, (x|y)N = HN−1 n=0(x + ny) denotes the Pochhammer symbol, (x)N = (x|1)N, and SNM,a is a generalized Stirling number that is readily tabulated (Buntine and Hutter, 2012). There are two fundamental things to notice about Equation (3). Positively, the term in ~µ0 takes the form of a multinomial likelihood, so we can propagate it up and perform inference on ~µ0 unencumbered by the functional mess of Equation (1). Thus Poisson-Dirichlet processes allow one to do Bayesian reasoning on hierarchies of probability vectors (Teh, 2006; Teh et al., 2006). Negatively, however, one needs to sample the auxiliary variables t~ leading to some problems: The range of tk, {0, ..., nk}, is broad. Also, contributions from individual data zi have been lost so the mixing of the MCMC can sometimes be slow. We confirmed these problems on our first implementation of the Adaptive Topic Model presented next in Section 3. A further improvement on PDP sampling is achieved in (Chen et al., 2011), where another auxiliary variable is introduced, a so-called table indicator, that for each datum zi indicates whether it is the “head of its table” (</context>
</contexts>
<marker>Teh, 2006</marker>
<rawString>Y. W. Teh. 2006. A hierarchical Bayesian language model based on Pitman-Yor processes. In Proc. of 21st Inter. Conf. on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, pages 985–992.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Wallach</author>
<author>D Mimno</author>
<author>A McCallum</author>
</authors>
<title>Rethinking LDA: Why priors matter.</title>
<date>2009</date>
<booktitle>In Advances in Neural Information Processing Systems 19.</booktitle>
<contexts>
<context position="2624" citStr="Wallach et al., 2009" startWordPosition="389" endWordPosition="392">chniques such as latent semantic analysis (Deerwester et al., 1990). Topic models can be improved by better modelling the semantic aspects of text, for instance integrating collocations into the model (Johnson, 2010; Hardisty et al., 2010) or encouraging topics to be more semantically coherent (Newman et al., 2011) based on lexical coherence models (Newman et al., 2010), modelling the structural aspects of documents, for instance modelling a document as a set of segments (Du et al., 2010; Wang et al., 2011; Chen et al., 2009), or improving the underlying statistical methods (Teh et al., 2006; Wallach et al., 2009). Topic models, like statistical parsing methods, are using more sophisticated latent variable methods in order to model different aspects of these problems. In this paper, we are interested in developing a new topic model which can take into account the structural topic dependency by following the higher level document subject structure, but we hope to retain the general flavour of topic models, where components (e.g., sentences) can be a mixture of topics. Thus we need to depart from the earlier HMM style models, see, e.g., (Blei and Moreno, 2001; Gruber et al., 2007). Inspired by the idea t</context>
</contexts>
<marker>Wallach, Mimno, McCallum, 2009</marker>
<rawString>H. Wallach, D. Mimno, and A. McCallum. 2009. Rethinking LDA: Why priors matter. In Advances in Neural Information Processing Systems 19.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Wang</author>
<author>D Zhang</author>
<author>C Zhai</author>
</authors>
<title>Structural topic model for latent topical structure analysis.</title>
<date>2011</date>
<booktitle>In Proc. of 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies -Volume</booktitle>
<volume>1</volume>
<pages>1526--1535</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="2514" citStr="Wang et al., 2011" startWordPosition="370" endWordPosition="373">egmentation (Misra et al., 2011; Eisenstein and Barzilay, 2008), often times replacing earlier semantic techniques such as latent semantic analysis (Deerwester et al., 1990). Topic models can be improved by better modelling the semantic aspects of text, for instance integrating collocations into the model (Johnson, 2010; Hardisty et al., 2010) or encouraging topics to be more semantically coherent (Newman et al., 2011) based on lexical coherence models (Newman et al., 2010), modelling the structural aspects of documents, for instance modelling a document as a set of segments (Du et al., 2010; Wang et al., 2011; Chen et al., 2009), or improving the underlying statistical methods (Teh et al., 2006; Wallach et al., 2009). Topic models, like statistical parsing methods, are using more sophisticated latent variable methods in order to model different aspects of these problems. In this paper, we are interested in developing a new topic model which can take into account the structural topic dependency by following the higher level document subject structure, but we hope to retain the general flavour of topic models, where components (e.g., sentences) can be a mixture of topics. Thus we need to depart from</context>
</contexts>
<marker>Wang, Zhang, Zhai, 2011</marker>
<rawString>H. Wang, D. Zhang, and C. Zhai. 2011. Structural topic model for latent topical structure analysis. In Proc. of 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies -Volume 1, pages 1526–1535, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
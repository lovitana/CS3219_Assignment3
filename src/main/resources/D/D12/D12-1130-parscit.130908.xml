<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004762">
<title confidence="0.977765">
Grounded Models of Semantic Representation
</title>
<author confidence="0.988712">
Carina Silberer and Mirella Lapata
</author>
<affiliation confidence="0.9992425">
Institute for Language, Cognition and Computation
School of Informatics, University of Edinburgh
</affiliation>
<address confidence="0.992611">
10 Crichton Street, Edinburgh EH8 9AB
</address>
<email confidence="0.998558">
c.silberer@ed.ac.uk,mlap@inf.ed.ac.uk
</email>
<sectionHeader confidence="0.998596" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999930142857143">
A popular tradition of studying semantic rep-
resentation has been driven by the assump-
tion that word meaning can be learned from
the linguistic environment, despite ample ev-
idence suggesting that language is grounded
in perception and action. In this paper we
present a comparative study of models that
represent word meaning based on linguistic
and perceptual data. Linguistic information is
approximated by naturally occurring corpora
and sensorimotor experience by feature norms
(i.e., attributes native speakers consider impor-
tant in describing the meaning of a word). The
models differ in terms of the mechanisms by
which they integrate the two modalities. Ex-
perimental results show that a closer corre-
spondence to human data can be obtained by
uncovering latent information shared among
the textual and perceptual modalities rather
than arriving at semantic knowledge by con-
catenating the two.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999894636363636">
Distributional models of lexical semantics have seen
considerable success at accounting for a wide range
of behavioral data in tasks involving semantic cog-
nition (Landauer and Dumais, 1997; Griffiths et
al., 2007). These models have also enjoyed last-
ing popularity in natural language processing. Ex-
amples involve information retrieval (Salton et al.,
1975), word sense discrimination (Sch¨utze, 1998),
text segmentation (Choi et al., 2001), and numerous
studies of lexicon acquisition (Grefenstette, 1994;
Lin, 1998). Despite their widespread use, distribu-
tional models have been criticized as “disembodied”
in that they learn exclusively from linguistic infor-
mation but are not grounded in perception and ac-
tion (Perfetti, 1998; Barsalou, 1999; Glenberg and
Kaschak, 2002).
This lack of grounding contrasts with many ex-
perimental studies suggesting that word meaning is
acquired not only from exposure to the linguistic
environment but also from our interaction with the
physical world (Landau et al., 1998; Bornstein et al.,
2004). Beyond language acquisition, there is consid-
erable evidence across both behavioral experiments
and neuroimaging studies that the perceptual asso-
ciates of words play an important role in language
processing (for a review see Barsalou (2008)).
It is thus no surprise that recent years have wit-
nessed the emergence of perceptually grounded dis-
tributional models. An important question in the for-
mulation of such models concerns the provenance
of perceptual information. A few models use fea-
ture norms as a proxy for sensorimotor experience
(Howell et al., 2005; Andrews et al., 2009; Steyvers,
2010; Johns and Jones, 2012). These are obtained
by asking native speakers to write down attributes
they consider important in describing the meaning
of a word. The attributes represent perceived phys-
ical and functional properties associated with the
referents of words. For example, apples are typi-
cally green or red, round, shiny, smooth, crunchy,
tasty, and so on; dogs have four legs and bark,
whereas chairs are used for sitting. Other models fo-
cus solely on the visual modality under the assump-
tion that it represents a major source of data from
</bodyText>
<page confidence="0.741403">
1423
</page>
<note confidence="0.8051555">
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1423–1433, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.957099321428571">
which humans can learn semantic representations
of both linguistic and non-linguistic communicative
actions (Regier, 1996). For example, Feng and Lap-
ata (2010) learn semantic representations from cor-
pora of texts paired with naturally co-occurring im-
ages (e.g., news articles and their associated pic-
tures), whereas Bruni et al. (2011) learn textual and
visual representations independently from distinct
data sources.
Aside from the type of data used to capture per-
ceptual information, another important issue con-
cerns how the two modalities (perceptual and tex-
tual) are integrated. A simple solution would be
to learn both modalities independently (Bruni et al.,
2011) or to infer one modality by means of the other
(Johns and Jones, 2012) and to arrive at a grounded
representation simply by concatenating the two. An
alternative is to learn from both modalities jointly
(Andrews et al., 2009; Feng and Lapata, 2010;
Steyvers, 2010). According to this view, seman-
tic knowledge is gained by simultaneously learning
from the statistical structure within each modality
assuming both data sources have been generated by
a shared set of meanings or topics.
In this paper we undertake the first comparative
study of perceptually grounded distributional mod-
els. We examine three models with different as-
sumptions regarding the integration of perceptual
and linguistic data. The first model, originally pro-
posed by Andrews et al. (2009), is an extension of
latent Dirichlet allocation (LDA, Blei et al. (2003)).
It simultaneously considers the distribution of words
across contexts in a text corpus and the distribu-
tion of words across perceptual features and extracts
joint information from both data sources. Our sec-
ond model is based on Johns and Jones (2012) who
represent the meaning of a word as the concatena-
tion of its textual and its perceptual vector. Interest-
ingly, their model allows to infer a perceptual vector
for words without feature norms, simply by taking
into account similar words for which perceptual in-
formation is available.
Finally, we propose Canonical Correlation Anal-
ysis (Hotelling, 1936; Hardoon et al., 2004) as our
third model. CCA is a data analysis and dimen-
sionality reduction method similar to PCA. While
PCA deals with only one data space, CCA is a tech-
nique for joint dimensionality reduction across two
Features table dog apple
has 4 legs .28 .60 0
used for eating .50 0 0
a pet 0 .40 0
is brown 0 0 0
is crunchy 0 0 .58
is round .22 0 .42
has fangs 0 0 0
</bodyText>
<tableCaption confidence="0.992704">
Table 1: Feature norms for the nouns table, dog, and
apple shown as a distribution.
</tableCaption>
<bodyText confidence="0.9998785">
(or more) spaces that provide heterogeneous repre-
sentations of the same objects. The assumption is
that the representations in these two spaces contain
some joint information that is reflected in correla-
tions between them.
In all three models we use feature norms as a
proxy for perceptual information. Despite their
shortcomings (e.g., they often cover a small frac-
tion of the vocabulary of an adult speaker due to
the effort involved in eliciting them), feature norms
provide detailed knowledge about meaning repre-
sentations and are a useful starting point for study-
ing the integration of perceptual and textual infor-
mation without being susceptible to the effects of
noise, e.g., coming from image processing. In other
words, feature norms can serve as an upper bound
of what can be achieved when integrating detailed
perceptual information with vanilla text-based dis-
tributional models.
Our experimental results demonstrate that joint
models give a better fit to human word similarity and
association data than a model that considers only
one data source, or the simple concatenation of the
two sources.
</bodyText>
<sectionHeader confidence="0.946975" genericHeader="method">
2 Perceptually Grounded Models
</sectionHeader>
<bodyText confidence="0.999935444444444">
In this study we examine semantic representation
models that rely on linguistic and perceptual data.
The linguistic environment is approximated by cor-
pora such as the British National Corpus (BNC).
As mentioned earlier, we resort to feature norms
as proxy for perceptual information. In our exper-
iments, we relied on the norming study of McRae et
al. (2005), in which a large number of human par-
ticipants were presented with a series of words and
</bodyText>
<page confidence="0.99141">
1424
</page>
<bodyText confidence="0.999562166666667">
asked to list relevant features of the words’ refer-
ents. Table 1 presents examples of features partici-
pants listed for the nouns apple, dog, and table. The
number of participants listing a certain feature for a
word can be used to compute a probability distribu-
tion over features given the word:
</bodyText>
<equation confidence="0.99463625">
P( fk|w) = f ( fk,w)
F
∑
m=1
</equation>
<bodyText confidence="0.999922166666667">
where f (fk,w) is the number of participants who
listed feature fk for word w and F is the total number
of features.
In the remainder of this section we will describe
our models and how they arrive at an integrated per-
ceptual and linguistic representation.
</bodyText>
<sectionHeader confidence="0.682066" genericHeader="method">
2.1 Feature-topic Model
</sectionHeader>
<bodyText confidence="0.9990278">
Andrews et al. (2009) present an extension of LDA
(Blei et al., 2003) where words in documents as well
as their associated features are treated as observed
variables that are explained by a generative process.
The underlying training data consists of a corpus D
where each document is represented by words and
their frequency of occurrence within the document.
In addition, those words of a document that are also
included in the feature norms are paired with one of
their features, where a feature is sampled according
to the feature distribution given that word. For ex-
ample, suppose a document dj consists of the sen-
tence Mix in the apple, celery, raisins, and apple
juice. Suppose further that all content words ex-
cept of mix and juice are included in the feature
norms. Then, a representation for dj is mix:1, ap-
ple;is red:2, celery;has leaves:1, raisin;is edible:1,
juice:1.
The plate diagram in Figure 1 illustrates the
graphical model in detail. Each document dj
in D is generated by a mixture of components
{x1,...,xc,...,xC} E C; a component xc comprises a
latent discourse topic coupled with a feature clus-
ter originating from the feature norms. A dis-
course topic belonging to xc, in turn, is a distribu-
tion φc E φ = {φ1,...,φC} over words, and a feature
cluster is a distribution ψc E ψ = {ψ1,...,ψC} over
features.
In order to create document dj, a distribution πj
over components is sampled from a Dirichlet distri-
</bodyText>
<figureCaption confidence="0.99959125">
Figure 1: Feature-topic model. The components xji of a
document dj are sampled from πj. For each xc = xji, a
word wji is drawn from distribution φc and a feature fji is
drawn from distribution ψc.
</figureCaption>
<bodyText confidence="0.973999555555556">
bution parametrized by α. To generate each word
wji E {wj1,...,wjnj}, a component xc = xji is drawn
from πj; wji is then drawn from the corresponding
distribution φc. If wji is in the feature norms, it is
coupled with a feature fji which is correspondingly
drawn from ψc. A symmetric Dirichlet prior with
hyperparameters β and γ is placed on φ and ψ, re-
spectively. The probability of the corpus D is de-
fined as:
</bodyText>
<equation confidence="0.9952745">
P((w U f)1:D|φ,ψ,α) = (2)
j=1 i=1
∏ˆ dπj∏P(πj|α)
C
∑ P(wji|xji = xc,φ)P(fji|xji = xc,ψ)P(xji = xc|πj)
c=1
</equation>
<bodyText confidence="0.824715285714286">
where D is the number of documents and C the
predefined number of components. Computing the
posterior distribution P(φ,ψ,α,β,γ|(w U f)1:D) of
the hidden variables given the data is generally in-
tractable:
P(φ,ψ,α,β,γ|(w U f)1:D) ∝ P((wU f)1:D|φ,ψ,α) P(φ|β)P(ψ|γ)P(α)P(β)P(γ) (3)
Equation (3) may be approximated using the Gibbs
</bodyText>
<figure confidence="0.929775823529412">
γ
β
α
π
x
ψ
w, f
φ
Vxc E C
Vi E {1,...,nj}
Vxc E C
Vj E {1,...,D}
f(fm,w)
(1)
1425
x1 x2 x12 ... x28 x75 x107 x119 x125 x148 x182 ... x266 x326 x349 x350
apple [ 3e-5 3e-5 0 ... 5e-4 9e-4 .09 .002 7.6e-5 2e-4 .003 ... 0 0 3e-6 0
</figure>
<figureCaption confidence="0.955082">
Figure 2: Example of the representation of the meaning of apple with the model of (Andrews et al., 2009) .
</figureCaption>
<figure confidence="0.9990705">
... d16 ... d322 ... d2469 d2470 ... dD
apple [ ... 1 ... 1 ... 0 1 ... 0 ] [
... d16 ... d322 ... d2469 d2470 ... dD
apple [ ... 1 ... 1 ... 0 1 ... 0 ] [
0 0 0 ... 0 0 0 0 �
a fruit has fangs is crunchy ... is yellow is red is green is round
.006 1.8e-5 8e-4 ... .004 .004 .006 .02 �
a fruit has fangs is crunchy ... is yellow is red is green is round
</figure>
<figureCaption confidence="0.9894735">
Figure 3: Example representation for apple before (first row) and after (second row) applying the perceptual inference
method of Johns and Jones (2012).
</figureCaption>
<bodyText confidence="0.999439647058823">
sampling procedure described in Andrews et al.
(2009).
Inducing feature-topic components from a docu-
ment collection D with the extended LDA model
just described gives two sets of parameters: word
probabilities given components PW (wi|X = xc) for
wi, i = 1,...,N, and feature probabilities given com-
ponents PF(fk|X = xc) for fk, k = 1,...,F. For exam-
ple, most of the probability mass of component x107
would be reserved for the words apple, fruit, lemon,
orange, tree and the features is red, tastes sweet,
is round and so on.
Word meaning in this model is represented by the
distribution PX|W over the learned components (see
Figure 2 for an example). Assuming a uniform dis-
tribution over components xc in D, PX|W can be ap-
proximated as:
</bodyText>
<equation confidence="0.992554">
P(wi|xc)P(xc) .
P(wi)
</equation>
<bodyText confidence="0.999736666666667">
where C is the total number of components. The
model can be also used to infer features for words
that were not originally included in the feature
norms. The probability distribution PF|W over fea-
tures given a word wi is simply inferred by summing
over all components xc for each feature fk:
</bodyText>
<equation confidence="0.972872">
P(fk|xc)P(xc|wi) (5)
</equation>
<subsectionHeader confidence="0.989737">
2.2 Global Similarity Model
</subsectionHeader>
<bodyText confidence="0.99998612">
Johns and Jones (2012) propose an approach for
generating perceptual representations for words by
means of global lexical similarity. Their model does
not place so much emphasis on the integration of
perceptual and linguistic information, rather its main
focus is on inducing perceptual representations for
words with no perceptual correlates. Their idea is to
assume that lexically similar words also share per-
ceptual features and hence it should be possible to
transfer perceptual information onto words that have
none from their linguistically similar neighbors.
Let T E 11,0}N&amp;quot;D denote a binary term-
document matrix, where each cell records the pres-
ence or absence of a term in a document. Let
P E [0,1]N&amp;quot;F denote a perceptual matrix, represent-
ing a probability distribution over features for each
word (see Table 1). A word’s meaning is repre-
sented by the concatenation of its textual and per-
ceptual vectors (see Figure 3). If a word has not
been normed, its perceptual vector will be all zeros.
Johns and Jones (2012) propose a two-step estima-
tion process for words without perceptual vectors.
Initially, a perceptual vector is constructed based on
the word’s weighted similarity to other words that
have non-zero perceptual vectors:
</bodyText>
<equation confidence="0.954061">
ti * sim(ti,p)X (6)
</equation>
<bodyText confidence="0.999984545454545">
where p is the representation of a word with a tex-
tual vector but an empty perceptual vector, ts are
composite representations consisting of textual and
perceptual vectors, sim is a measure of distributional
similarity such as cosine, X a weighting parameter,
and pin f the resulting inferred representation of the
word. The process is repeated a second time, so
as to incorporate the inferred perceptual vector in
the computation of the inferred vectors of all other
words. An example of this inference procedure is
illustrated in Figure 3.
</bodyText>
<equation confidence="0.999601933333333">
PX=xc|W=wi =
(4)
C
E
l=1
P(wi|xc)
P(wi|xl)
PF(fk|W = wi) =
C
E
c=1
N
E
i=1
pinf =
</equation>
<page confidence="0.981914">
1426
</page>
<figure confidence="0.994901714285714">
... d16 ... d322 ... d2470 ... dD
apple [ ... .006 ... .003 ... .1e-6 ... 0 [
a fruit has fangs is crunchy ... is yellow is red is green is round
.06 ... .04 .14 .09 .04 �
.13 0
k1 k2 k3 ... k409 k410 k1 k2 k3 ... k409 k410
apple [ −.003 −.01 .002 ... −.002 −.01 [ .008 −.03 −.008 ... −.02 −.07
</figure>
<figureCaption confidence="0.999719">
Figure 4: Example representation for apple before (first row) and after (second row) applying CCA.
</figureCaption>
<subsectionHeader confidence="0.994106">
2.3 Canonical Correlation Analysis
</subsectionHeader>
<bodyText confidence="0.999969961538462">
Our third model uses Canonical Correlation Analy-
sis (CCA, Hardoon et al. (2004)) to learn a joint se-
mantic representation from the textual and percep-
tual views. Given two random variables x and y
(or two sets of vectors), CCA can be seen as de-
termining two sets of basis vectors in such a way,
that the correlation between the projections of the
variables onto these bases is mutually maximized
(Borga, 2001). In effect, the representation-specific
details pertaining to the two views of the same phe-
nomenon are discarded and the underlying hidden
factors responsible for the correlation are revealed.
In our case the linguistic view is represented by a
term-document matrix, T E RN&amp;quot;D, containing infor-
mation about the occurrence of each word in each
document. The perceptual view is captured by a
perceptual matrix, P E [0,1]N&amp;quot;F, representing words
as a probability distribution over normed features.
CCA is concerned with describing linear dependen-
cies between two sets of variables of relatively low
dimensionality. Since the correlation between the
linguistic and perceptual views may exist in some
nonlinear relationship, we used a kernelized version
of CCA (Hardoon et al., 2004) which first projects
the data into a higher-dimensional feature space and
then performs CCA in this new feature space. The
two kernel matrices are KT = TT� and KP = PP�.
After applying CCA we obtain two matrices pro-
jected onto L basis vectors, Ct E RN&amp;quot;L, resulting
from the projection of the textual matrix T onto the
new basis and Cp E RN&amp;quot;L, resulting from the projec-
tion of the corresponding perceptual feature matrix.
The meaning of a word can thus be represented by
its projected textual vector in CT, its projected per-
ceptual vector in CP or their concatenation. Figure 4
shows an example of the textual and perceptual vec-
tors for the word apple which were used as input for
CCA (first row) and their new representation after
the projection onto new basis vectors (second row).
The CCA model as sketched above will only ob-
tain full representations for words with perceptual
features available. One solution would be to apply
the method from Johns and Jones (2012) to infer the
perceptual vectors and then perform CCA on the in-
ferred vectors. Another approach which we assess
experimentally (see Section 4) is to create a percep-
tual vector for a word that has none from its k-most
(textually) similar neighbors, simply by taking the
average of their perceptual vectors. This inference
procedure can be applied to the original vectors or
the projected vectors in CT and CP, respectively,
once CCA has taken place.
</bodyText>
<subsectionHeader confidence="0.75189">
2.4 Discussion
</subsectionHeader>
<bodyText confidence="0.999569666666667">
Johns and Jones (2012) primarily present a model of
perceptual inference, where textual data is used to
infer perceptual information for words not included
in feature norms. There is no means in this model to
obtain a joint representation resulting from the mu-
tual influence of the perceptual and textual views.
As shown in the example in Figure 3 the textual
vector on the left-hand side does not undergo any
transformation whatsoever. The generative model
put forward by Andrews et al. (2009) learns meaning
representations by simultaneously considering doc-
uments and features. Rather than simply adding per-
ceptual information to textual data it integrates both
modalities jointly in a single representation which
is desirable, at least from a cognitive perspective.
It is unlikely that we have separate representations
for different aspects of word meaning (Rogers et al.,
2004). Similarly to Johns and Jones (2012), An-
drews et al’s (2009) feature-topic model can also
infer perceptual representations for words that have
none. The inference is performed automatically in
an implicit manner during component induction.
In CCA, textual and perceptual data represent two
different views of the same objects and the model
operates on these views directly without combining
or manipulating any of them a priori. Instead, the
combination of the two modalities is realized via
</bodyText>
<page confidence="0.974128">
1427
</page>
<bodyText confidence="0.9991605">
correlating the linear relationships between them. A
drawback of the model lies in the need of additional
methods for inferring perceptual representations for
words not available in feature norms.
</bodyText>
<sectionHeader confidence="0.998856" genericHeader="method">
3 Experimental Setup
</sectionHeader>
<bodyText confidence="0.999758702702703">
Data All our experiments used a lemmatized ver-
sion of the British National Corpus (BNC) as a
source of textual information. The feature norms of
McRae et al. (2005) were used as a proxy for percep-
tual information. The BNC comprises 4,049 texts
totalling approximately 100 million words. McRae
et al.’s feature norms consist of 541 words and 2,526
features; 824 of these features occur with at least two
different words.
Evaluation Tasks Our evaluation experiments
compared the models discussed above on three
tasks. Two of them have been previously used
to evaluate semantic representation models, namely
word association and word similarity. In order
to simulate word association, we used the human
norms collected by (Nelson et al., 1998).1 These
were established by presenting a large number of
participants with a cue word (e.g., rice) and ask-
ing them to name an associate word in response
(e.g., Chinese, wedding, food, white). For each cue
word, the norms provide a set of associates and the
frequencies with which they were named. We can
thus compute the probability distribution over asso-
ciates for each cue. Analogously, we can estimate
the degree of similarity between a cue and its as-
sociates using our models (see the following sec-
tion for details on the similarity measures we em-
ployed). The norms contain 63,619 unique normed
cue-associate pairs in total. Of these, 25,968 pairs
were covered by all models and 520 appeared in
McRae et al.’s (2005) norms. Using correlation anal-
ysis, we examined the degree of linear relationship
between the human cue-associate probabilities and
the automatically derived similarity values.
Our word similarity experiments used the
WordSimilarity-353 test collection (Finkelstein et
al., 2002)2 which consists of relatedness judgments
</bodyText>
<footnote confidence="0.996999666666667">
1Available at http://www.usf.edu/Freeassociation.
2Available at http://www.cs.technion.ac.il/˜gabr/
resources/data/wordsim353/.
</footnote>
<bodyText confidence="0.999903230769231">
for word pairs. For each pair, a similarity judg-
ment (on a scale of 0 to 10) was elicited from 13 or
16 human subjects (e.g., tiger-cat are very similar,
whereas delay–racism are not). The average rating
for each pair represents an estimate of the perceived
similarity of the two words. The task varies slightly
from word association. Here, participants are asked
to rate perceived similarity rather than to generate
the first word that came to mind in response to a cue
word. The collection contains similarity ratings for
353 word pairs. Of these, 76 pairs appeared in our
corpus and 3 in McRae et al.’s (2005) norms. Again,
we evaluated how well model produced similarities
correlate with human ratings. Throughout this paper
we report correlation coefficients using Pearson’s r.
Our third task assessed the models’ ability to in-
fer perceptual vectors for words that have none. To
do this, we conducted 10-fold cross-validation on
McRae et al.’s (2005) norms. We treated the per-
ceptual vectors in each test fold as unseen, and used
the data in the corresponding training fold together
with the models presented in Section 2 to infer them.
Then, for each word, we examined how close the in-
ferred vector was to the actual one, via correlation
analysis.
Model Parameters The feature-topic model has a
few parameters that must be instantiated. These in-
clude, C, the number of predefined components and
the priors a, R, and y. Following Andrews et al.
(2009), the components C were set to 350.3 A vague
inverse gamma prior was placed on a, R, and y.4 To
measure word similarity within this model, we adopt
Griffiths et al.’s (2007) definition. The underlying
idea is that word association can be expressed as a
conditional distribution. If we have seen word w1,
then we can determine the probability that w2 will
be also generated by computing P(w2|w1). Assum-
ing that both w1 and w2 came from a single compo-
nent, P(w2|w1) can be estimated as:
</bodyText>
<equation confidence="0.999307666666667">
P(w2|xc)P(xc|w1)
(7)
P(xc|w1) — P(w1|xc)P(xc)
</equation>
<footnote confidence="0.7726405">
3As we explain in Section 4 the feature-topic model was
compared to a vanilla LDA model trained on the BNC only.
For that model, C was set to 250.
4That is P(•) = exp(− 1•)•−2.
</footnote>
<equation confidence="0.98400525">
C
E
c=1
P(w2|w1) =
</equation>
<page confidence="0.931618">
1428
</page>
<bodyText confidence="0.999982741935484">
where P(x,) is uniform, a single component x, is
sampled from the distribution P(x,|w1), and an over-
all estimate is obtained by averaging over all C com-
ponents.
Johns and Jones’ (2012) model uses binary tex-
tual vectors to represent word meaning. If the word
is present in a given document, that vector element
is coded as one; if it is absent, it is coded as zero.
We built a binary term-document matrix from the
BNC over 14,000 lemmas. The value of the similar-
ity weighting parameter X was set to the same values
reported by Johns and Jones (X1=3 for Step 1 and
1%2 = 13 for Step 2).
For the CCA model, we represented the textual
view with a term-document co-occurrence matrix.
Matrix cells were set to their tf-idf values.5 The tex-
tual and perceptual matrices were projected onto 410
vectors. As mentioned in Section 2.3, CCA does not
naturally lend itself to inferring perceptual vectors,
yet a perceptual vector for a word can be created
from its k-nearest neighbors. We inferred a percep-
tual vector by averaging over the perceptual vectors
of the word’s k most similar words; textual similarity
between two words was measured using the cosine
of the angle of the two vectors representing them.
To find the optimal value for k, we used one third of
Nelson’s (1998) cues as development set. The high-
est correlation was achieved with k = 2 when the
perceptual vectors were created prior to CCA and
k = 8 when they were inferred on the projected tex-
tual and perceptual matrices.
</bodyText>
<sectionHeader confidence="0.999942" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.9999425">
Our experiments were designed to answer three
questions: (1) Does the integration of perceptual and
textual information yield a better fit with behavioral
data compared to a model that considers only one
data source? (2) What is the best way to integrate
the two modalities, e.g., via simple concatenation or
jointly? (3) How accurately can we approximate the
perceptual information when the latter is absent?
To answer the first question, we assessed the mod-
els’ performance when textual and perceptual infor-
mation are both available. The results in Table 2
are thus computed on the subset of Nelson’s (1998)
</bodyText>
<footnote confidence="0.9791165">
5Experiments with a binarized version of the term-document
matrix consistently performed worse.
</footnote>
<table confidence="0.999842545454545">
Models Modality Pearson’s r
Feature-topic +t +p .35
Feature-topic +t −p .12
Feature-topic −t +p .22
Global similarity +t +p .23
Global similarity +t −p .11
Global similarity −t +p .22
CCA +t +p .32
CCA +t −p .14
CCA −t +p .29
Upper Bound — .91
</table>
<tableCaption confidence="0.965511166666667">
Table 2: Performance of feature-topic, global similarity,
and CCA models on a subset of the Nelson et al. (1998)
norms when taking into account the textual and percep-
tual modalities on their own (+t−p and −t+p) and in
combination (+t+p). All correlation coefficients are sta-
tistically significant (p &lt; 0.01).
</tableCaption>
<bodyText confidence="0.99996064">
norms (520 cue-associate pairs) that also appeared in
McRae et al. (2005) and for which a perceptual vec-
tor was present. The table shows different instanti-
ations of the three models depending on the type of
modality taken into account: textual, perceptual or
both.
As can be seen, Andrews et al.’s (2009) feature-
topic model provides a better fit with the association
data when both modalities are taken into account
(+t+p). A vanilla LDA model constructed solely
on the BNC (+t−p) or McRae et al.’s (2005) fea-
ture norms (−t+p) yields substantially lower corre-
lations. We observe a similar pattern with Johns and
Jones’ (2012) global similarity model. Concatena-
tion of perceptual and textual vectors yields the best
fit with the norming data, relying on perceptual in-
formation alone (−t+p) comes close, whereas tex-
tual information on its own seems to have a weaker
effect (+t−p).6 The CCA model takes perceptual
and textual information as input in order to find a
projection onto basis vectors that are maximally cor-
related. Although by definition the CCA model must
operate on the two views, we can nevertheless iso-
late the contribution of each modality by considering
the vectors resulting from the projection of the tex-
</bodyText>
<footnote confidence="0.992260666666667">
6In this evaluation setting, the model does not infer any per-
ceptual representations; perceptual vectors are taken directly
from McRae et al. (2005).
</footnote>
<page confidence="0.996072">
1429
</page>
<bodyText confidence="0.999947111111111">
tual matrix (+t−p), the perceptual matrix (−t+p) or
their concatenation (+t+p). We obtain best results
with the latter representation; again we observe that
the perceptual information is more dominant.
Overall we find that the feature-topic model and
CCA perform best. In fact the correlations achieved
by the two models do not differ significantly, us-
ing a t-test (Cohen and Cohen, 1983). The per-
formance of the global similarity model is signifi-
cantly worse than the feature-topic model and CCA
(p &lt; 0.01). Recall that the feature-topic model
(+t+p) represents words as distributions over com-
ponents, whereas the global similarity model sim-
ply concatenates the textual and perceptual vectors.
The same input is also given to CCA which in turn
attempts to interpret the data by inferring common
relationships between the two views. In sum, we
can conclude that the higher correlation with human
judgments indicates that integrating textual and per-
ceptual modalities jointly is preferable to concatena-
tion.
However, note that all models in Table 2 fall
short of the human upper bound which we mea-
sured by calculating the reliability of Nelson et al.’s
(1998) norms. Reliability estimates the likelihood
of a similarly-composed group of participants pre-
sented with the same task under the same circum-
stances producing identical results. We split the col-
lected cue-associate pairs randomly into two halves
and computed the correlation between them; this
correlation was averaged across 200 random splits.
These correlations were adjusted by applying the
Spearman-Brown prediction formula (Voorspoels et
al., 2008).
The results in Table 2 are computed on a small
fraction of Nelson et al.’s (1998) norms. One might
even argue that the comparison is slightly unfair as
the global similarity model is more geared towards
inferring perceptual vectors rather than integrating
the two modalities in the best possible way. To gain
a better understanding of the models’ behavior and
to allow comparisons on a larger dataset and more
equal footing, we also report results on the entire
dataset (20,556 cue-associate pairs).7 This entails
that the models will infer perceptual vectors for the
</bodyText>
<footnote confidence="0.8728915">
7This excludes the data used as development set for tuning
the k-nearest neighbors for CCA.
</footnote>
<table confidence="0.999761">
Models Pearson’s r
Feature-topic .15
Global similarity .03
Global similarity « CCA .12
k-NN « CCA .11
CCA « k-NN .12
Upper Bound .96
</table>
<tableCaption confidence="0.99153375">
Table 3: Performance of the feature-topic, global simi-
larityand CCA models on the Nelson et al. (1998) norms
(entire dataset). All correlation coefficients are statisti-
cally significant (p &lt; 0.01).
</tableCaption>
<bodyText confidence="0.999424411764706">
words that are not attested in McRae et al.’s norms.
Recall from Section 2.3 that CCA does not have
a dedicated inference mechanism. We thus experi-
mented with three options (a) interfacing the infer-
ence method of Johns and Jones (2012) with CCA
(global similarity « CCA) (b) creating a percep-
tual vector from the words’ k-nearest neighbors be-
fore (k-NN « CCA) or (c) after CCA takes place
(CCA « k-NN).
Our results are summarized in Table 3. The up-
per bound was estimated in the same fashion as for
the smaller dataset. Despite being statistically sig-
nificant (p &lt; 0.01), the correlation coefficients are
lower. This is hardly surprising as perceptual infor-
mation is approximate and in several cases likely to
be wrong. Interestingly, we observe similar mod-
eling trends, irrespective of whether the models are
performing perceptual inference or not. The feature-
topic model achieves the best fit with the data, fol-
lowed by CCA. The inference method here does not
seem to have much of an impact: CCA « k-NN
does as well as global similarity « CCA. This is
perhaps expected as the inference procedure adopted
by Johns and Jones (2012) is a generalization of our
k-nearest neighbor approach. The global similarity
model performs worst; we conjecture that this is due
to the way semantic information is integrated rather
than the inference method itself. CCA works with
similar input, yet achieves better correlations with
the human data, due to its ability to represent the
commonalities shared by the two modalities. Taken
together the results in Tables 2 and 3 provide an an-
swer to our second question. Models that capture la-
tent information shared between the two modalities
</bodyText>
<page confidence="0.996081">
1430
</page>
<tableCaption confidence="0.899128333333333">
Table 4: Mean correlation coefficients between origi-
nal and inferred feature vectors in McRae et al.’s (2005)
norms.
</tableCaption>
<table confidence="0.999911692307692">
Models Pearson’s r
Feature-topic .17
Global similarity .25
Global similarity « CCA .21
k-NN « CCA .19
CCA « k-NN .13
Models Pearson’s r
Feature-topic .35
Global similarity .08
Global similarity « CCA .38
k-NN « CCA .39
CCA « k-NN .28
Upper Bound .98
</table>
<tableCaption confidence="0.800888666666667">
Table 5: Model performance on predicting word similar-
ity. All correlation coefficients are statistically significant
(p &lt; 0.01), except for the global similarity model.
</tableCaption>
<bodyText confidence="0.999880641025641">
create more accurate semantic representations com-
pared to simply treating the two as independent data
sources.
In order to isolate the influence of the inference
method from the resulting semantic representation
we evaluated the inferred perceptual vectors on their
own by computing their correlation with the original
feature distributions in McRae et al.’s (2005) norms.
The correlation coefficients are reported in Table 4
and were computed by averaging the coefficients ob-
tained for individual words. Here, the global simi-
larity model achieves the highest correlation, and for
a good reason. It is the only model with an empha-
sis on inference, the other two models do not have
such a dedicated mechanism. CCA has in fact none,
whereas in the feature-topic model the inference of
missing perceptual information is a by-product of
the generative process. The results in Table 4 indi-
cate that the perceptual vectors are not reconstructed
very accurately (the highest correlation coefficient
is .25) and that better inference mechanisms are re-
quired for perceptual information to have a positive
impact on semantic representation.
In Table 5 we examine the models’ performance
on semantic similarity rather than association using
the WordSimilarity-353 dataset (Finkelstein et al.,
2002). The models were evaluated on 76 word pairs
that appeared in the BNC. We inferred the percep-
tual vectors for 51 words. We computed the upper
bound using the reliability method described ear-
lier. Again, the joint models achieve better results
than the simple concatenation model. The feature-
topic and CCA models perform comparably, with
the global similarity model lagging substantially be-
hind. In sum, our results indicate that the issue
of how to best integrate the two modalities has a
greater impact on the resulting semantic representa-
tions compared to the mechanism by which missing
perceptual information is inferred.
</bodyText>
<sectionHeader confidence="0.999779" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999931933333333">
In this paper, we have presented a comparative study
of semantic representation models which compute
word meaning on the basis of linguistic and per-
ceptual information. The models differ in terms
of the mechanisms by which they integrate the two
modalities. In the feature-topic model (Andrews et
al., 2009), the textual and perceptual views are in-
tegrated via a set of latent components that are in-
ferred from the joint distribution of textual words
and perceptual features. The model based on Canon-
ical Correlation Analysis (Hardoon et al., 2004) in-
tegrates the two views by deriving a consensus rep-
resentation based on the correlation between the lin-
guistic and perceptual modalities. Johns and Jones’
(2012) similarity-based model simply concatentates
the two representations. In addition, it uses the lin-
guistic representations of words to infer perceptual
information when the latter is absent.
Experiments on word association and similarity
show that all models benefit from the integration of
perceptual data. We also find that joint models are
superior as they obtain a closer fit with human judg-
ments compared with an approach that simply con-
catenates the two views. We have also examined
how these models perform on the perceptual infer-
ence task which has implications for the wider appli-
cability of grounded semantic representation mod-
els. Johns and Jones’ (2012) inference mechanism
goes some way towards reconstructing the informa-
tion contained in the feature norms, however, further
</bodyText>
<page confidence="0.972493">
1431
</page>
<bodyText confidence="0.999949962962963">
work is needed to achieve representations accurate
enough to be useful in semantic tasks.
In this paper we have used McRae et al.’s (2005)
norms without any extensive feature engineering
other than applying a frequency cut-off. In the fu-
ture we plan to experiment with feature selection
methods in an attempt to represent perceptual in-
formation more succinctly. For example, it may
be that different features are appropriate for differ-
ent word classes (e.g., color versus event denoting
nouns). Although feature norms are a useful first ap-
proximation of perceptual data, the effort involved in
eliciting them limits the scope of any computational
model based on normed data. A natural avenue for
future work would be to develop semantic represen-
tation models that exploit perceptual data that is both
naturally occurring and easily accessible (e.g., im-
ages, physical simulations).
Acknowledgments We are grateful to Brendan
Johns for his help with the re-implementation of his
model. Thanks to Frank Keller and Michael Roth for
their input on earlier versions of this work, Ioannis
Konstas for his help with the final version, and mem-
bers of the ILCC at the School of Informatics for
valuable discussions and comments. We acknowl-
edge the support of EPSRC through project grant
EP/I032916/1.
</bodyText>
<sectionHeader confidence="0.999554" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999968430555555">
M. Andrews, G. Vigliocco, and D. Vinson. 2009. Inte-
grating Experiential and Distributional Data to Learn
Semantic Representations. Psychological Review,
116(3):463–498.
Lawrence Barsalou. 1999. Perceptual Symbol Systems.
Behavioral and Brain Sciences, 22:577–609.
Lawrence W. Barsalou. 2008. Grounded Cognition. An-
nual Review of Psychology, 59:617–845.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet Allocation. Journal of Ma-
chine Learning Research, 3:993–1022, March.
Magnus Borga. 2001. Canonical Correlation - a Tutorial,
January.
M. H. Bornstein, L. R. Cote, S. Maital, K. Painter, S.-Y.
Park, and L. Pascual. 2004. Cross-linguistic Analy-
sis of Vocabulary in Young Children: Spanish, Dutch,
French, Hebrew, Italian, Korean, and American En-
glish. Child Development, 75(4):1115–1139.
Elia Bruni, Giang Binh Tran, and Marco Baroni. 2011.
Distributional Semantics from Text and Images. In
Proceedings of the GEMS 2011 Workshop on GEomet-
rical Models of Natural Language Semantics, pages
22–32, Edinburgh, UK, July. Association for Compu-
tational Linguistics.
Freddy Choi, Peter Wiemer-Hastings, and Johanna
Moore. 2001. Latent Semantic Analysis for Text Seg-
mentation. In Proceedings of the 6th EMNLP, pages
109–117, Seattle, WA.
J Cohen and P Cohen. 1983. Applied Multiple Regres-
sion/Correlation Analysis for the Behavioral Sciences.
Hillsdale, NJ: Erlbaum.
Yansong Feng and Mirella Lapata. 2010. Visual Infor-
mation in Semantic Representation. In Human Lan-
guage Technologies: The 2010 Annual Conference of
the North American Chapter of the Association for
Computational Linguistics, pages 91–99, Los Ange-
les, California, June. Association for Computational
Linguistics.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan
Ruppin. 2002. Placing Search in Context: The Con-
cept Revisited. ACM Transactions on Information Sys-
tems, 20(1):116–131, January.
Arthur M. Glenberg and Michael P. Kaschak. 2002.
Grounding Language in Action. Psychonomic Bulletin
and Review, 9(3):558–565.
Gregory Grefenstette. 1994. Explorations in Automatic
Thesaurus Discovery. Kluwer Academic Publishers.
T. L. Griffiths, M. Steyvers, and J. B. Tenenbaum. 2007.
Topics in Semantic Representation. Psychological Re-
view, 114(2):211–244.
David R. Hardoon, Sandor R. Szedmak, and John R.
Shawe-Taylor. 2004. Canonical Correlation Analysis:
An Overview with Application to Learning Methods.
Neural Computation, 16(12):2639–2664.
H Hotelling. 1936. Relations between Two Sets of Vari-
ates. Biometrika, 28:312–377.
Steve R. Howell, Damian Jankowicz, and Suzanna
Becker. 2005. A Model of Grounded Language Ac-
quisition: Sensorimotor Features Improve Lexical and
Grammatical Learning. Journal of Memory and Lan-
guage, 53(2), 258-276, 53(2):258–276.
Brendan T. Johns and Michael N. Jones. 2012. Percep-
tual Inference through Global Lexical Similarity. Top-
ics in Cognitive Science, 4(1):103–120.
B. Landau, L. Smith, and S. Jones. 1998. Object Percep-
tion and Object Naming in Early Development. Trends
in Cognitive Science, 27:19–24.
T. Landauer and S. T. Dumais. 1997. A Solution to
Plato’s Problem: the Latent Semantic Analysis The-
ory of Acquisition, Induction, and Representation of
Knowledge. Psychological Review, 104(2):211–240.
</reference>
<page confidence="0.86657">
1432
</page>
<reference confidence="0.999890411764706">
Dekang Lin. 1998. Automatic Retrieval and Clustering
of Similar Words. In Proceedings of the joint Annual
Meeting of the Association for Computational Linguis-
tics and International Conference on Computational
Linguistics, pages 768–774, Montr´eal, Canada.
K. McRae, G. S. Cree, M. S. Seidenberg, and C. McNor-
gan. 2005. Semantic Feature Production Norms for a
Large Set of Living and Nonliving Things. Behavior
Research Methods, 37(4):547–559, November.
D. L. Nelson, C. L. McEvoy, and T. A. Schreiber. 1998.
The University of South Florida Word Association,
Rhyme, and Word Fragment Norms.
C. Perfetti. 1998. The Limits of Co-occurrence: Tools
and Theories in Language Research. Discourse Pro-
cesses, (25):363–377.
Terry Regier. 1996. The Human Semantic Potential.
MIT Press, Cambridge, MA.
T. T. Rogers, M. A. Lambon Ralph, P. Garrard, S. Bozeat,
J. L. McClelland, J. R. Hodges, and K. Patterson.
2004. Structure and Deterioration of Semantic Mem-
ory: A Neuropsychological and Computational Inves-
tigation. Psychological Review, 111(1):205–235.
G Salton, A Wang, and C Yang. 1975. A Vector-space
Model for Information Retrieval. Journal of the Amer-
ican Society for Information Science, 18:613–620.
Hinrich Sch¨utze. 1998. Automatic Word Sense Discrim-
ination. Computational Linguistics, 24(1):97–124.
Mark Steyvers. 2010. Combining Feature Norms and
Text Data with Topic Models. Acta Psychologica,
133(3):234–342.
Wouter Voorspoels, Wolf Vanpaemel, and Gert Storms.
2008. Exemplars and Prototypes in Natural Language
Concepts: A Typicality-based Evaluation. Psycho-
nomic Bulletin &amp; Review, 15:630–637.
</reference>
<page confidence="0.979117">
1433
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.768993">
<title confidence="0.99248">Grounded Models of Semantic Representation</title>
<author confidence="0.841935">Silberer</author>
<affiliation confidence="0.98152">Institute for Language, Cognition and School of Informatics, University of</affiliation>
<address confidence="0.926744">10 Crichton Street, Edinburgh EH8</address>
<abstract confidence="0.998916136363636">A popular tradition of studying semantic representation has been driven by the assumption that word meaning can be learned from the linguistic environment, despite ample evsuggesting that language is in perception and action. In this paper we present a comparative study of models that represent word meaning based on linguistic and perceptual data. Linguistic information is approximated by naturally occurring corpora and sensorimotor experience by feature norms (i.e., attributes native speakers consider important in describing the meaning of a word). The models differ in terms of the mechanisms by which they integrate the two modalities. Experimental results show that a closer correspondence to human data can be obtained by uncovering latent information shared among the textual and perceptual modalities rather than arriving at semantic knowledge by concatenating the two.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Andrews</author>
<author>G Vigliocco</author>
<author>D Vinson</author>
</authors>
<title>Integrating Experiential and Distributional Data to Learn Semantic Representations.</title>
<date>2009</date>
<journal>Psychological Review,</journal>
<volume>116</volume>
<issue>3</issue>
<contexts>
<context position="2788" citStr="Andrews et al., 2009" startWordPosition="410" endWordPosition="413">u et al., 1998; Bornstein et al., 2004). Beyond language acquisition, there is considerable evidence across both behavioral experiments and neuroimaging studies that the perceptual associates of words play an important role in language processing (for a review see Barsalou (2008)). It is thus no surprise that recent years have witnessed the emergence of perceptually grounded distributional models. An important question in the formulation of such models concerns the provenance of perceptual information. A few models use feature norms as a proxy for sensorimotor experience (Howell et al., 2005; Andrews et al., 2009; Steyvers, 2010; Johns and Jones, 2012). These are obtained by asking native speakers to write down attributes they consider important in describing the meaning of a word. The attributes represent perceived physical and functional properties associated with the referents of words. For example, apples are typically green or red, round, shiny, smooth, crunchy, tasty, and so on; dogs have four legs and bark, whereas chairs are used for sitting. Other models focus solely on the visual modality under the assumption that it represents a major source of data from 1423 Proceedings of the 2012 Joint C</context>
<context position="4493" citStr="Andrews et al., 2009" startWordPosition="674" endWordPosition="677"> and their associated pictures), whereas Bruni et al. (2011) learn textual and visual representations independently from distinct data sources. Aside from the type of data used to capture perceptual information, another important issue concerns how the two modalities (perceptual and textual) are integrated. A simple solution would be to learn both modalities independently (Bruni et al., 2011) or to infer one modality by means of the other (Johns and Jones, 2012) and to arrive at a grounded representation simply by concatenating the two. An alternative is to learn from both modalities jointly (Andrews et al., 2009; Feng and Lapata, 2010; Steyvers, 2010). According to this view, semantic knowledge is gained by simultaneously learning from the statistical structure within each modality assuming both data sources have been generated by a shared set of meanings or topics. In this paper we undertake the first comparative study of perceptually grounded distributional models. We examine three models with different assumptions regarding the integration of perceptual and linguistic data. The first model, originally proposed by Andrews et al. (2009), is an extension of latent Dirichlet allocation (LDA, Blei et a</context>
<context position="8390" citStr="Andrews et al. (2009)" startWordPosition="1329" endWordPosition="1332">to list relevant features of the words’ referents. Table 1 presents examples of features participants listed for the nouns apple, dog, and table. The number of participants listing a certain feature for a word can be used to compute a probability distribution over features given the word: P( fk|w) = f ( fk,w) F ∑ m=1 where f (fk,w) is the number of participants who listed feature fk for word w and F is the total number of features. In the remainder of this section we will describe our models and how they arrive at an integrated perceptual and linguistic representation. 2.1 Feature-topic Model Andrews et al. (2009) present an extension of LDA (Blei et al., 2003) where words in documents as well as their associated features are treated as observed variables that are explained by a generative process. The underlying training data consists of a corpus D where each document is represented by words and their frequency of occurrence within the document. In addition, those words of a document that are also included in the feature norms are paired with one of their features, where a feature is sampled according to the feature distribution given that word. For example, suppose a document dj consists of the sente</context>
<context position="11172" citStr="Andrews et al., 2009" startWordPosition="1827" endWordPosition="1830">C the predefined number of components. Computing the posterior distribution P(φ,ψ,α,β,γ|(w U f)1:D) of the hidden variables given the data is generally intractable: P(φ,ψ,α,β,γ|(w U f)1:D) ∝ P((wU f)1:D|φ,ψ,α) P(φ|β)P(ψ|γ)P(α)P(β)P(γ) (3) Equation (3) may be approximated using the Gibbs γ β α π x ψ w, f φ Vxc E C Vi E {1,...,nj} Vxc E C Vj E {1,...,D} f(fm,w) (1) 1425 x1 x2 x12 ... x28 x75 x107 x119 x125 x148 x182 ... x266 x326 x349 x350 apple [ 3e-5 3e-5 0 ... 5e-4 9e-4 .09 .002 7.6e-5 2e-4 .003 ... 0 0 3e-6 0 Figure 2: Example of the representation of the meaning of apple with the model of (Andrews et al., 2009) . ... d16 ... d322 ... d2469 d2470 ... dD apple [ ... 1 ... 1 ... 0 1 ... 0 ] [ ... d16 ... d322 ... d2469 d2470 ... dD apple [ ... 1 ... 1 ... 0 1 ... 0 ] [ 0 0 0 ... 0 0 0 0 � a fruit has fangs is crunchy ... is yellow is red is green is round .006 1.8e-5 8e-4 ... .004 .004 .006 .02 � a fruit has fangs is crunchy ... is yellow is red is green is round Figure 3: Example representation for apple before (first row) and after (second row) applying the perceptual inference method of Johns and Jones (2012). sampling procedure described in Andrews et al. (2009). Inducing feature-topic components f</context>
<context position="18212" citStr="Andrews et al. (2009)" startWordPosition="3052" endWordPosition="3055"> applied to the original vectors or the projected vectors in CT and CP, respectively, once CCA has taken place. 2.4 Discussion Johns and Jones (2012) primarily present a model of perceptual inference, where textual data is used to infer perceptual information for words not included in feature norms. There is no means in this model to obtain a joint representation resulting from the mutual influence of the perceptual and textual views. As shown in the example in Figure 3 the textual vector on the left-hand side does not undergo any transformation whatsoever. The generative model put forward by Andrews et al. (2009) learns meaning representations by simultaneously considering documents and features. Rather than simply adding perceptual information to textual data it integrates both modalities jointly in a single representation which is desirable, at least from a cognitive perspective. It is unlikely that we have separate representations for different aspects of word meaning (Rogers et al., 2004). Similarly to Johns and Jones (2012), Andrews et al’s (2009) feature-topic model can also infer perceptual representations for words that have none. The inference is performed automatically in an implicit manner </context>
<context position="22681" citStr="Andrews et al. (2009)" startWordPosition="3753" endWordPosition="3756"> perceptual vectors for words that have none. To do this, we conducted 10-fold cross-validation on McRae et al.’s (2005) norms. We treated the perceptual vectors in each test fold as unseen, and used the data in the corresponding training fold together with the models presented in Section 2 to infer them. Then, for each word, we examined how close the inferred vector was to the actual one, via correlation analysis. Model Parameters The feature-topic model has a few parameters that must be instantiated. These include, C, the number of predefined components and the priors a, R, and y. Following Andrews et al. (2009), the components C were set to 350.3 A vague inverse gamma prior was placed on a, R, and y.4 To measure word similarity within this model, we adopt Griffiths et al.’s (2007) definition. The underlying idea is that word association can be expressed as a conditional distribution. If we have seen word w1, then we can determine the probability that w2 will be also generated by computing P(w2|w1). Assuming that both w1 and w2 came from a single component, P(w2|w1) can be estimated as: P(w2|xc)P(xc|w1) (7) P(xc|w1) — P(w1|xc)P(xc) 3As we explain in Section 4 the feature-topic model was compared to a</context>
<context position="34590" citStr="Andrews et al., 2009" startWordPosition="5717" endWordPosition="5720">rably, with the global similarity model lagging substantially behind. In sum, our results indicate that the issue of how to best integrate the two modalities has a greater impact on the resulting semantic representations compared to the mechanism by which missing perceptual information is inferred. 5 Conclusions In this paper, we have presented a comparative study of semantic representation models which compute word meaning on the basis of linguistic and perceptual information. The models differ in terms of the mechanisms by which they integrate the two modalities. In the feature-topic model (Andrews et al., 2009), the textual and perceptual views are integrated via a set of latent components that are inferred from the joint distribution of textual words and perceptual features. The model based on Canonical Correlation Analysis (Hardoon et al., 2004) integrates the two views by deriving a consensus representation based on the correlation between the linguistic and perceptual modalities. Johns and Jones’ (2012) similarity-based model simply concatentates the two representations. In addition, it uses the linguistic representations of words to infer perceptual information when the latter is absent. Experi</context>
</contexts>
<marker>Andrews, Vigliocco, Vinson, 2009</marker>
<rawString>M. Andrews, G. Vigliocco, and D. Vinson. 2009. Integrating Experiential and Distributional Data to Learn Semantic Representations. Psychological Review, 116(3):463–498.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lawrence Barsalou</author>
</authors>
<title>Perceptual Symbol Systems.</title>
<date>1999</date>
<booktitle>Behavioral and Brain Sciences,</booktitle>
<pages>22--577</pages>
<contexts>
<context position="1919" citStr="Barsalou, 1999" startWordPosition="276" endWordPosition="277">s involving semantic cognition (Landauer and Dumais, 1997; Griffiths et al., 2007). These models have also enjoyed lasting popularity in natural language processing. Examples involve information retrieval (Salton et al., 1975), word sense discrimination (Sch¨utze, 1998), text segmentation (Choi et al., 2001), and numerous studies of lexicon acquisition (Grefenstette, 1994; Lin, 1998). Despite their widespread use, distributional models have been criticized as “disembodied” in that they learn exclusively from linguistic information but are not grounded in perception and action (Perfetti, 1998; Barsalou, 1999; Glenberg and Kaschak, 2002). This lack of grounding contrasts with many experimental studies suggesting that word meaning is acquired not only from exposure to the linguistic environment but also from our interaction with the physical world (Landau et al., 1998; Bornstein et al., 2004). Beyond language acquisition, there is considerable evidence across both behavioral experiments and neuroimaging studies that the perceptual associates of words play an important role in language processing (for a review see Barsalou (2008)). It is thus no surprise that recent years have witnessed the emergenc</context>
</contexts>
<marker>Barsalou, 1999</marker>
<rawString>Lawrence Barsalou. 1999. Perceptual Symbol Systems. Behavioral and Brain Sciences, 22:577–609.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lawrence W Barsalou</author>
</authors>
<title>Grounded Cognition. Annual Review of Psychology,</title>
<date>2008</date>
<pages>59--617</pages>
<contexts>
<context position="2448" citStr="Barsalou (2008)" startWordPosition="356" endWordPosition="357">ormation but are not grounded in perception and action (Perfetti, 1998; Barsalou, 1999; Glenberg and Kaschak, 2002). This lack of grounding contrasts with many experimental studies suggesting that word meaning is acquired not only from exposure to the linguistic environment but also from our interaction with the physical world (Landau et al., 1998; Bornstein et al., 2004). Beyond language acquisition, there is considerable evidence across both behavioral experiments and neuroimaging studies that the perceptual associates of words play an important role in language processing (for a review see Barsalou (2008)). It is thus no surprise that recent years have witnessed the emergence of perceptually grounded distributional models. An important question in the formulation of such models concerns the provenance of perceptual information. A few models use feature norms as a proxy for sensorimotor experience (Howell et al., 2005; Andrews et al., 2009; Steyvers, 2010; Johns and Jones, 2012). These are obtained by asking native speakers to write down attributes they consider important in describing the meaning of a word. The attributes represent perceived physical and functional properties associated with t</context>
</contexts>
<marker>Barsalou, 2008</marker>
<rawString>Lawrence W. Barsalou. 2008. Grounded Cognition. Annual Review of Psychology, 59:617–845.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent Dirichlet Allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--993</pages>
<contexts>
<context position="5102" citStr="Blei et al. (2003)" startWordPosition="768" endWordPosition="771">al., 2009; Feng and Lapata, 2010; Steyvers, 2010). According to this view, semantic knowledge is gained by simultaneously learning from the statistical structure within each modality assuming both data sources have been generated by a shared set of meanings or topics. In this paper we undertake the first comparative study of perceptually grounded distributional models. We examine three models with different assumptions regarding the integration of perceptual and linguistic data. The first model, originally proposed by Andrews et al. (2009), is an extension of latent Dirichlet allocation (LDA, Blei et al. (2003)). It simultaneously considers the distribution of words across contexts in a text corpus and the distribution of words across perceptual features and extracts joint information from both data sources. Our second model is based on Johns and Jones (2012) who represent the meaning of a word as the concatenation of its textual and its perceptual vector. Interestingly, their model allows to infer a perceptual vector for words without feature norms, simply by taking into account similar words for which perceptual information is available. Finally, we propose Canonical Correlation Analysis (Hotellin</context>
<context position="8438" citStr="Blei et al., 2003" startWordPosition="1338" endWordPosition="1341">Table 1 presents examples of features participants listed for the nouns apple, dog, and table. The number of participants listing a certain feature for a word can be used to compute a probability distribution over features given the word: P( fk|w) = f ( fk,w) F ∑ m=1 where f (fk,w) is the number of participants who listed feature fk for word w and F is the total number of features. In the remainder of this section we will describe our models and how they arrive at an integrated perceptual and linguistic representation. 2.1 Feature-topic Model Andrews et al. (2009) present an extension of LDA (Blei et al., 2003) where words in documents as well as their associated features are treated as observed variables that are explained by a generative process. The underlying training data consists of a corpus D where each document is represented by words and their frequency of occurrence within the document. In addition, those words of a document that are also included in the feature norms are paired with one of their features, where a feature is sampled according to the feature distribution given that word. For example, suppose a document dj consists of the sentence Mix in the apple, celery, raisins, and apple</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent Dirichlet Allocation. Journal of Machine Learning Research, 3:993–1022, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Magnus Borga</author>
</authors>
<title>Canonical Correlation - a Tutorial,</title>
<date>2001</date>
<contexts>
<context position="15508" citStr="Borga, 2001" startWordPosition="2606" endWordPosition="2607">pple [ −.003 −.01 .002 ... −.002 −.01 [ .008 −.03 −.008 ... −.02 −.07 Figure 4: Example representation for apple before (first row) and after (second row) applying CCA. 2.3 Canonical Correlation Analysis Our third model uses Canonical Correlation Analysis (CCA, Hardoon et al. (2004)) to learn a joint semantic representation from the textual and perceptual views. Given two random variables x and y (or two sets of vectors), CCA can be seen as determining two sets of basis vectors in such a way, that the correlation between the projections of the variables onto these bases is mutually maximized (Borga, 2001). In effect, the representation-specific details pertaining to the two views of the same phenomenon are discarded and the underlying hidden factors responsible for the correlation are revealed. In our case the linguistic view is represented by a term-document matrix, T E RN&amp;quot;D, containing information about the occurrence of each word in each document. The perceptual view is captured by a perceptual matrix, P E [0,1]N&amp;quot;F, representing words as a probability distribution over normed features. CCA is concerned with describing linear dependencies between two sets of variables of relatively low dimen</context>
</contexts>
<marker>Borga, 2001</marker>
<rawString>Magnus Borga. 2001. Canonical Correlation - a Tutorial, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M H Bornstein</author>
<author>L R Cote</author>
<author>S Maital</author>
<author>K Painter</author>
<author>S-Y Park</author>
<author>L Pascual</author>
</authors>
<title>Cross-linguistic Analysis of Vocabulary in</title>
<date>2004</date>
<journal>Child Development,</journal>
<volume>75</volume>
<issue>4</issue>
<location>Young Children: Spanish, Dutch, French, Hebrew, Italian, Korean, and</location>
<contexts>
<context position="2207" citStr="Bornstein et al., 2004" startWordPosition="319" endWordPosition="322">gmentation (Choi et al., 2001), and numerous studies of lexicon acquisition (Grefenstette, 1994; Lin, 1998). Despite their widespread use, distributional models have been criticized as “disembodied” in that they learn exclusively from linguistic information but are not grounded in perception and action (Perfetti, 1998; Barsalou, 1999; Glenberg and Kaschak, 2002). This lack of grounding contrasts with many experimental studies suggesting that word meaning is acquired not only from exposure to the linguistic environment but also from our interaction with the physical world (Landau et al., 1998; Bornstein et al., 2004). Beyond language acquisition, there is considerable evidence across both behavioral experiments and neuroimaging studies that the perceptual associates of words play an important role in language processing (for a review see Barsalou (2008)). It is thus no surprise that recent years have witnessed the emergence of perceptually grounded distributional models. An important question in the formulation of such models concerns the provenance of perceptual information. A few models use feature norms as a proxy for sensorimotor experience (Howell et al., 2005; Andrews et al., 2009; Steyvers, 2010; J</context>
</contexts>
<marker>Bornstein, Cote, Maital, Painter, Park, Pascual, 2004</marker>
<rawString>M. H. Bornstein, L. R. Cote, S. Maital, K. Painter, S.-Y. Park, and L. Pascual. 2004. Cross-linguistic Analysis of Vocabulary in Young Children: Spanish, Dutch, French, Hebrew, Italian, Korean, and American English. Child Development, 75(4):1115–1139.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elia Bruni</author>
<author>Giang Binh Tran</author>
<author>Marco Baroni</author>
</authors>
<title>Distributional Semantics from Text and Images.</title>
<date>2011</date>
<booktitle>In Proceedings of the GEMS 2011 Workshop on GEometrical Models of Natural Language Semantics,</booktitle>
<pages>22--32</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Edinburgh, UK,</location>
<contexts>
<context position="3933" citStr="Bruni et al. (2011)" startWordPosition="584" endWordPosition="587">epresents a major source of data from 1423 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1423–1433, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics which humans can learn semantic representations of both linguistic and non-linguistic communicative actions (Regier, 1996). For example, Feng and Lapata (2010) learn semantic representations from corpora of texts paired with naturally co-occurring images (e.g., news articles and their associated pictures), whereas Bruni et al. (2011) learn textual and visual representations independently from distinct data sources. Aside from the type of data used to capture perceptual information, another important issue concerns how the two modalities (perceptual and textual) are integrated. A simple solution would be to learn both modalities independently (Bruni et al., 2011) or to infer one modality by means of the other (Johns and Jones, 2012) and to arrive at a grounded representation simply by concatenating the two. An alternative is to learn from both modalities jointly (Andrews et al., 2009; Feng and Lapata, 2010; Steyvers, 2010)</context>
</contexts>
<marker>Bruni, Tran, Baroni, 2011</marker>
<rawString>Elia Bruni, Giang Binh Tran, and Marco Baroni. 2011. Distributional Semantics from Text and Images. In Proceedings of the GEMS 2011 Workshop on GEometrical Models of Natural Language Semantics, pages 22–32, Edinburgh, UK, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Freddy Choi</author>
<author>Peter Wiemer-Hastings</author>
<author>Johanna Moore</author>
</authors>
<title>Latent Semantic Analysis for Text Segmentation.</title>
<date>2001</date>
<booktitle>In Proceedings of the 6th EMNLP,</booktitle>
<pages>109--117</pages>
<location>Seattle, WA.</location>
<contexts>
<context position="1614" citStr="Choi et al., 2001" startWordPosition="230" endWordPosition="233"> obtained by uncovering latent information shared among the textual and perceptual modalities rather than arriving at semantic knowledge by concatenating the two. 1 Introduction Distributional models of lexical semantics have seen considerable success at accounting for a wide range of behavioral data in tasks involving semantic cognition (Landauer and Dumais, 1997; Griffiths et al., 2007). These models have also enjoyed lasting popularity in natural language processing. Examples involve information retrieval (Salton et al., 1975), word sense discrimination (Sch¨utze, 1998), text segmentation (Choi et al., 2001), and numerous studies of lexicon acquisition (Grefenstette, 1994; Lin, 1998). Despite their widespread use, distributional models have been criticized as “disembodied” in that they learn exclusively from linguistic information but are not grounded in perception and action (Perfetti, 1998; Barsalou, 1999; Glenberg and Kaschak, 2002). This lack of grounding contrasts with many experimental studies suggesting that word meaning is acquired not only from exposure to the linguistic environment but also from our interaction with the physical world (Landau et al., 1998; Bornstein et al., 2004). Beyon</context>
</contexts>
<marker>Choi, Wiemer-Hastings, Moore, 2001</marker>
<rawString>Freddy Choi, Peter Wiemer-Hastings, and Johanna Moore. 2001. Latent Semantic Analysis for Text Segmentation. In Proceedings of the 6th EMNLP, pages 109–117, Seattle, WA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Cohen</author>
<author>P Cohen</author>
</authors>
<title>Applied Multiple Regression/Correlation Analysis for the Behavioral Sciences.</title>
<date>1983</date>
<publisher>Erlbaum.</publisher>
<location>Hillsdale, NJ:</location>
<contexts>
<context position="27926" citStr="Cohen and Cohen, 1983" startWordPosition="4646" endWordPosition="4649">ch modality by considering the vectors resulting from the projection of the tex6In this evaluation setting, the model does not infer any perceptual representations; perceptual vectors are taken directly from McRae et al. (2005). 1429 tual matrix (+t−p), the perceptual matrix (−t+p) or their concatenation (+t+p). We obtain best results with the latter representation; again we observe that the perceptual information is more dominant. Overall we find that the feature-topic model and CCA perform best. In fact the correlations achieved by the two models do not differ significantly, using a t-test (Cohen and Cohen, 1983). The performance of the global similarity model is significantly worse than the feature-topic model and CCA (p &lt; 0.01). Recall that the feature-topic model (+t+p) represents words as distributions over components, whereas the global similarity model simply concatenates the textual and perceptual vectors. The same input is also given to CCA which in turn attempts to interpret the data by inferring common relationships between the two views. In sum, we can conclude that the higher correlation with human judgments indicates that integrating textual and perceptual modalities jointly is preferable</context>
</contexts>
<marker>Cohen, Cohen, 1983</marker>
<rawString>J Cohen and P Cohen. 1983. Applied Multiple Regression/Correlation Analysis for the Behavioral Sciences. Hillsdale, NJ: Erlbaum.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yansong Feng</author>
<author>Mirella Lapata</author>
</authors>
<title>Visual Information in Semantic Representation. In Human Language Technologies: The</title>
<date>2010</date>
<booktitle>Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>91--99</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Los Angeles, California,</location>
<contexts>
<context position="3757" citStr="Feng and Lapata (2010)" startWordPosition="556" endWordPosition="560">ooth, crunchy, tasty, and so on; dogs have four legs and bark, whereas chairs are used for sitting. Other models focus solely on the visual modality under the assumption that it represents a major source of data from 1423 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1423–1433, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics which humans can learn semantic representations of both linguistic and non-linguistic communicative actions (Regier, 1996). For example, Feng and Lapata (2010) learn semantic representations from corpora of texts paired with naturally co-occurring images (e.g., news articles and their associated pictures), whereas Bruni et al. (2011) learn textual and visual representations independently from distinct data sources. Aside from the type of data used to capture perceptual information, another important issue concerns how the two modalities (perceptual and textual) are integrated. A simple solution would be to learn both modalities independently (Bruni et al., 2011) or to infer one modality by means of the other (Johns and Jones, 2012) and to arrive at </context>
</contexts>
<marker>Feng, Lapata, 2010</marker>
<rawString>Yansong Feng and Mirella Lapata. 2010. Visual Information in Semantic Representation. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 91–99, Los Angeles, California, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lev Finkelstein</author>
<author>Evgeniy Gabrilovich</author>
<author>Yossi Matias</author>
<author>Ehud Rivlin</author>
<author>Zach Solan</author>
<author>Gadi Wolfman</author>
<author>Eytan Ruppin</author>
</authors>
<title>Placing Search in Context: The Concept Revisited.</title>
<date>2002</date>
<journal>ACM Transactions on Information Systems,</journal>
<volume>20</volume>
<issue>1</issue>
<contexts>
<context position="21055" citStr="Finkelstein et al., 2002" startWordPosition="3495" endWordPosition="3498">. Analogously, we can estimate the degree of similarity between a cue and its associates using our models (see the following section for details on the similarity measures we employed). The norms contain 63,619 unique normed cue-associate pairs in total. Of these, 25,968 pairs were covered by all models and 520 appeared in McRae et al.’s (2005) norms. Using correlation analysis, we examined the degree of linear relationship between the human cue-associate probabilities and the automatically derived similarity values. Our word similarity experiments used the WordSimilarity-353 test collection (Finkelstein et al., 2002)2 which consists of relatedness judgments 1Available at http://www.usf.edu/Freeassociation. 2Available at http://www.cs.technion.ac.il/˜gabr/ resources/data/wordsim353/. for word pairs. For each pair, a similarity judgment (on a scale of 0 to 10) was elicited from 13 or 16 human subjects (e.g., tiger-cat are very similar, whereas delay–racism are not). The average rating for each pair represents an estimate of the perceived similarity of the two words. The task varies slightly from word association. Here, participants are asked to rate perceived similarity rather than to generate the first wor</context>
<context position="33644" citStr="Finkelstein et al., 2002" startWordPosition="5566" endWordPosition="5569">models do not have such a dedicated mechanism. CCA has in fact none, whereas in the feature-topic model the inference of missing perceptual information is a by-product of the generative process. The results in Table 4 indicate that the perceptual vectors are not reconstructed very accurately (the highest correlation coefficient is .25) and that better inference mechanisms are required for perceptual information to have a positive impact on semantic representation. In Table 5 we examine the models’ performance on semantic similarity rather than association using the WordSimilarity-353 dataset (Finkelstein et al., 2002). The models were evaluated on 76 word pairs that appeared in the BNC. We inferred the perceptual vectors for 51 words. We computed the upper bound using the reliability method described earlier. Again, the joint models achieve better results than the simple concatenation model. The featuretopic and CCA models perform comparably, with the global similarity model lagging substantially behind. In sum, our results indicate that the issue of how to best integrate the two modalities has a greater impact on the resulting semantic representations compared to the mechanism by which missing perceptual </context>
</contexts>
<marker>Finkelstein, Gabrilovich, Matias, Rivlin, Solan, Wolfman, Ruppin, 2002</marker>
<rawString>Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan Ruppin. 2002. Placing Search in Context: The Concept Revisited. ACM Transactions on Information Systems, 20(1):116–131, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arthur M Glenberg</author>
<author>Michael P Kaschak</author>
</authors>
<title>Grounding Language in Action.</title>
<date>2002</date>
<journal>Psychonomic Bulletin and Review,</journal>
<volume>9</volume>
<issue>3</issue>
<contexts>
<context position="1948" citStr="Glenberg and Kaschak, 2002" startWordPosition="278" endWordPosition="281">ntic cognition (Landauer and Dumais, 1997; Griffiths et al., 2007). These models have also enjoyed lasting popularity in natural language processing. Examples involve information retrieval (Salton et al., 1975), word sense discrimination (Sch¨utze, 1998), text segmentation (Choi et al., 2001), and numerous studies of lexicon acquisition (Grefenstette, 1994; Lin, 1998). Despite their widespread use, distributional models have been criticized as “disembodied” in that they learn exclusively from linguistic information but are not grounded in perception and action (Perfetti, 1998; Barsalou, 1999; Glenberg and Kaschak, 2002). This lack of grounding contrasts with many experimental studies suggesting that word meaning is acquired not only from exposure to the linguistic environment but also from our interaction with the physical world (Landau et al., 1998; Bornstein et al., 2004). Beyond language acquisition, there is considerable evidence across both behavioral experiments and neuroimaging studies that the perceptual associates of words play an important role in language processing (for a review see Barsalou (2008)). It is thus no surprise that recent years have witnessed the emergence of perceptually grounded di</context>
</contexts>
<marker>Glenberg, Kaschak, 2002</marker>
<rawString>Arthur M. Glenberg and Michael P. Kaschak. 2002. Grounding Language in Action. Psychonomic Bulletin and Review, 9(3):558–565.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregory Grefenstette</author>
</authors>
<title>Explorations in Automatic Thesaurus Discovery.</title>
<date>1994</date>
<publisher>Kluwer Academic Publishers.</publisher>
<contexts>
<context position="1679" citStr="Grefenstette, 1994" startWordPosition="240" endWordPosition="241">al and perceptual modalities rather than arriving at semantic knowledge by concatenating the two. 1 Introduction Distributional models of lexical semantics have seen considerable success at accounting for a wide range of behavioral data in tasks involving semantic cognition (Landauer and Dumais, 1997; Griffiths et al., 2007). These models have also enjoyed lasting popularity in natural language processing. Examples involve information retrieval (Salton et al., 1975), word sense discrimination (Sch¨utze, 1998), text segmentation (Choi et al., 2001), and numerous studies of lexicon acquisition (Grefenstette, 1994; Lin, 1998). Despite their widespread use, distributional models have been criticized as “disembodied” in that they learn exclusively from linguistic information but are not grounded in perception and action (Perfetti, 1998; Barsalou, 1999; Glenberg and Kaschak, 2002). This lack of grounding contrasts with many experimental studies suggesting that word meaning is acquired not only from exposure to the linguistic environment but also from our interaction with the physical world (Landau et al., 1998; Bornstein et al., 2004). Beyond language acquisition, there is considerable evidence across bot</context>
</contexts>
<marker>Grefenstette, 1994</marker>
<rawString>Gregory Grefenstette. 1994. Explorations in Automatic Thesaurus Discovery. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T L Griffiths</author>
<author>M Steyvers</author>
<author>J B Tenenbaum</author>
</authors>
<title>Topics in Semantic Representation.</title>
<date>2007</date>
<journal>Psychological Review,</journal>
<volume>114</volume>
<issue>2</issue>
<contexts>
<context position="1387" citStr="Griffiths et al., 2007" startWordPosition="198" endWordPosition="201">e speakers consider important in describing the meaning of a word). The models differ in terms of the mechanisms by which they integrate the two modalities. Experimental results show that a closer correspondence to human data can be obtained by uncovering latent information shared among the textual and perceptual modalities rather than arriving at semantic knowledge by concatenating the two. 1 Introduction Distributional models of lexical semantics have seen considerable success at accounting for a wide range of behavioral data in tasks involving semantic cognition (Landauer and Dumais, 1997; Griffiths et al., 2007). These models have also enjoyed lasting popularity in natural language processing. Examples involve information retrieval (Salton et al., 1975), word sense discrimination (Sch¨utze, 1998), text segmentation (Choi et al., 2001), and numerous studies of lexicon acquisition (Grefenstette, 1994; Lin, 1998). Despite their widespread use, distributional models have been criticized as “disembodied” in that they learn exclusively from linguistic information but are not grounded in perception and action (Perfetti, 1998; Barsalou, 1999; Glenberg and Kaschak, 2002). This lack of grounding contrasts with</context>
</contexts>
<marker>Griffiths, Steyvers, Tenenbaum, 2007</marker>
<rawString>T. L. Griffiths, M. Steyvers, and J. B. Tenenbaum. 2007. Topics in Semantic Representation. Psychological Review, 114(2):211–244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David R Hardoon</author>
<author>Sandor R Szedmak</author>
<author>John R Shawe-Taylor</author>
</authors>
<title>Canonical Correlation Analysis: An Overview with Application to Learning Methods.</title>
<date>2004</date>
<journal>Neural Computation,</journal>
<volume>16</volume>
<issue>12</issue>
<contexts>
<context position="5732" citStr="Hardoon et al., 2004" startWordPosition="869" endWordPosition="872">ultaneously considers the distribution of words across contexts in a text corpus and the distribution of words across perceptual features and extracts joint information from both data sources. Our second model is based on Johns and Jones (2012) who represent the meaning of a word as the concatenation of its textual and its perceptual vector. Interestingly, their model allows to infer a perceptual vector for words without feature norms, simply by taking into account similar words for which perceptual information is available. Finally, we propose Canonical Correlation Analysis (Hotelling, 1936; Hardoon et al., 2004) as our third model. CCA is a data analysis and dimensionality reduction method similar to PCA. While PCA deals with only one data space, CCA is a technique for joint dimensionality reduction across two Features table dog apple has 4 legs .28 .60 0 used for eating .50 0 0 a pet 0 .40 0 is brown 0 0 0 is crunchy 0 0 .58 is round .22 0 .42 has fangs 0 0 0 Table 1: Feature norms for the nouns table, dog, and apple shown as a distribution. (or more) spaces that provide heterogeneous representations of the same objects. The assumption is that the representations in these two spaces contain some joi</context>
<context position="15179" citStr="Hardoon et al. (2004)" startWordPosition="2545" endWordPosition="2548"> illustrated in Figure 3. PX=xc|W=wi = (4) C E l=1 P(wi|xc) P(wi|xl) PF(fk|W = wi) = C E c=1 N E i=1 pinf = 1426 ... d16 ... d322 ... d2470 ... dD apple [ ... .006 ... .003 ... .1e-6 ... 0 [ a fruit has fangs is crunchy ... is yellow is red is green is round .06 ... .04 .14 .09 .04 � .13 0 k1 k2 k3 ... k409 k410 k1 k2 k3 ... k409 k410 apple [ −.003 −.01 .002 ... −.002 −.01 [ .008 −.03 −.008 ... −.02 −.07 Figure 4: Example representation for apple before (first row) and after (second row) applying CCA. 2.3 Canonical Correlation Analysis Our third model uses Canonical Correlation Analysis (CCA, Hardoon et al. (2004)) to learn a joint semantic representation from the textual and perceptual views. Given two random variables x and y (or two sets of vectors), CCA can be seen as determining two sets of basis vectors in such a way, that the correlation between the projections of the variables onto these bases is mutually maximized (Borga, 2001). In effect, the representation-specific details pertaining to the two views of the same phenomenon are discarded and the underlying hidden factors responsible for the correlation are revealed. In our case the linguistic view is represented by a term-document matrix, T E</context>
<context position="34831" citStr="Hardoon et al., 2004" startWordPosition="5757" endWordPosition="5760">anism by which missing perceptual information is inferred. 5 Conclusions In this paper, we have presented a comparative study of semantic representation models which compute word meaning on the basis of linguistic and perceptual information. The models differ in terms of the mechanisms by which they integrate the two modalities. In the feature-topic model (Andrews et al., 2009), the textual and perceptual views are integrated via a set of latent components that are inferred from the joint distribution of textual words and perceptual features. The model based on Canonical Correlation Analysis (Hardoon et al., 2004) integrates the two views by deriving a consensus representation based on the correlation between the linguistic and perceptual modalities. Johns and Jones’ (2012) similarity-based model simply concatentates the two representations. In addition, it uses the linguistic representations of words to infer perceptual information when the latter is absent. Experiments on word association and similarity show that all models benefit from the integration of perceptual data. We also find that joint models are superior as they obtain a closer fit with human judgments compared with an approach that simply</context>
</contexts>
<marker>Hardoon, Szedmak, Shawe-Taylor, 2004</marker>
<rawString>David R. Hardoon, Sandor R. Szedmak, and John R. Shawe-Taylor. 2004. Canonical Correlation Analysis: An Overview with Application to Learning Methods. Neural Computation, 16(12):2639–2664.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Hotelling</author>
</authors>
<title>Relations between Two Sets of Variates.</title>
<date>1936</date>
<journal>Biometrika,</journal>
<pages>28--312</pages>
<contexts>
<context position="5709" citStr="Hotelling, 1936" startWordPosition="867" endWordPosition="868">. (2003)). It simultaneously considers the distribution of words across contexts in a text corpus and the distribution of words across perceptual features and extracts joint information from both data sources. Our second model is based on Johns and Jones (2012) who represent the meaning of a word as the concatenation of its textual and its perceptual vector. Interestingly, their model allows to infer a perceptual vector for words without feature norms, simply by taking into account similar words for which perceptual information is available. Finally, we propose Canonical Correlation Analysis (Hotelling, 1936; Hardoon et al., 2004) as our third model. CCA is a data analysis and dimensionality reduction method similar to PCA. While PCA deals with only one data space, CCA is a technique for joint dimensionality reduction across two Features table dog apple has 4 legs .28 .60 0 used for eating .50 0 0 a pet 0 .40 0 is brown 0 0 0 is crunchy 0 0 .58 is round .22 0 .42 has fangs 0 0 0 Table 1: Feature norms for the nouns table, dog, and apple shown as a distribution. (or more) spaces that provide heterogeneous representations of the same objects. The assumption is that the representations in these two </context>
</contexts>
<marker>Hotelling, 1936</marker>
<rawString>H Hotelling. 1936. Relations between Two Sets of Variates. Biometrika, 28:312–377.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steve R Howell</author>
<author>Damian Jankowicz</author>
<author>Suzanna Becker</author>
</authors>
<title>A Model of Grounded Language Acquisition: Sensorimotor Features Improve Lexical and Grammatical Learning.</title>
<date>2005</date>
<journal>Journal of Memory and Language,</journal>
<volume>53</volume>
<issue>2</issue>
<pages>258--276</pages>
<contexts>
<context position="2766" citStr="Howell et al., 2005" startWordPosition="406" endWordPosition="409">physical world (Landau et al., 1998; Bornstein et al., 2004). Beyond language acquisition, there is considerable evidence across both behavioral experiments and neuroimaging studies that the perceptual associates of words play an important role in language processing (for a review see Barsalou (2008)). It is thus no surprise that recent years have witnessed the emergence of perceptually grounded distributional models. An important question in the formulation of such models concerns the provenance of perceptual information. A few models use feature norms as a proxy for sensorimotor experience (Howell et al., 2005; Andrews et al., 2009; Steyvers, 2010; Johns and Jones, 2012). These are obtained by asking native speakers to write down attributes they consider important in describing the meaning of a word. The attributes represent perceived physical and functional properties associated with the referents of words. For example, apples are typically green or red, round, shiny, smooth, crunchy, tasty, and so on; dogs have four legs and bark, whereas chairs are used for sitting. Other models focus solely on the visual modality under the assumption that it represents a major source of data from 1423 Proceedin</context>
</contexts>
<marker>Howell, Jankowicz, Becker, 2005</marker>
<rawString>Steve R. Howell, Damian Jankowicz, and Suzanna Becker. 2005. A Model of Grounded Language Acquisition: Sensorimotor Features Improve Lexical and Grammatical Learning. Journal of Memory and Language, 53(2), 258-276, 53(2):258–276.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brendan T Johns</author>
<author>Michael N Jones</author>
</authors>
<title>Perceptual Inference through Global Lexical Similarity. Topics in</title>
<date>2012</date>
<journal>Cognitive Science,</journal>
<volume>4</volume>
<issue>1</issue>
<contexts>
<context position="2828" citStr="Johns and Jones, 2012" startWordPosition="416" endWordPosition="419">). Beyond language acquisition, there is considerable evidence across both behavioral experiments and neuroimaging studies that the perceptual associates of words play an important role in language processing (for a review see Barsalou (2008)). It is thus no surprise that recent years have witnessed the emergence of perceptually grounded distributional models. An important question in the formulation of such models concerns the provenance of perceptual information. A few models use feature norms as a proxy for sensorimotor experience (Howell et al., 2005; Andrews et al., 2009; Steyvers, 2010; Johns and Jones, 2012). These are obtained by asking native speakers to write down attributes they consider important in describing the meaning of a word. The attributes represent perceived physical and functional properties associated with the referents of words. For example, apples are typically green or red, round, shiny, smooth, crunchy, tasty, and so on; dogs have four legs and bark, whereas chairs are used for sitting. Other models focus solely on the visual modality under the assumption that it represents a major source of data from 1423 Proceedings of the 2012 Joint Conference on Empirical Methods in Natura</context>
<context position="4339" citStr="Johns and Jones, 2012" startWordPosition="649" endWordPosition="652">96). For example, Feng and Lapata (2010) learn semantic representations from corpora of texts paired with naturally co-occurring images (e.g., news articles and their associated pictures), whereas Bruni et al. (2011) learn textual and visual representations independently from distinct data sources. Aside from the type of data used to capture perceptual information, another important issue concerns how the two modalities (perceptual and textual) are integrated. A simple solution would be to learn both modalities independently (Bruni et al., 2011) or to infer one modality by means of the other (Johns and Jones, 2012) and to arrive at a grounded representation simply by concatenating the two. An alternative is to learn from both modalities jointly (Andrews et al., 2009; Feng and Lapata, 2010; Steyvers, 2010). According to this view, semantic knowledge is gained by simultaneously learning from the statistical structure within each modality assuming both data sources have been generated by a shared set of meanings or topics. In this paper we undertake the first comparative study of perceptually grounded distributional models. We examine three models with different assumptions regarding the integration of per</context>
<context position="11680" citStr="Johns and Jones (2012)" startWordPosition="1943" endWordPosition="1946"> 0 3e-6 0 Figure 2: Example of the representation of the meaning of apple with the model of (Andrews et al., 2009) . ... d16 ... d322 ... d2469 d2470 ... dD apple [ ... 1 ... 1 ... 0 1 ... 0 ] [ ... d16 ... d322 ... d2469 d2470 ... dD apple [ ... 1 ... 1 ... 0 1 ... 0 ] [ 0 0 0 ... 0 0 0 0 � a fruit has fangs is crunchy ... is yellow is red is green is round .006 1.8e-5 8e-4 ... .004 .004 .006 .02 � a fruit has fangs is crunchy ... is yellow is red is green is round Figure 3: Example representation for apple before (first row) and after (second row) applying the perceptual inference method of Johns and Jones (2012). sampling procedure described in Andrews et al. (2009). Inducing feature-topic components from a document collection D with the extended LDA model just described gives two sets of parameters: word probabilities given components PW (wi|X = xc) for wi, i = 1,...,N, and feature probabilities given components PF(fk|X = xc) for fk, k = 1,...,F. For example, most of the probability mass of component x107 would be reserved for the words apple, fruit, lemon, orange, tree and the features is red, tastes sweet, is round and so on. Word meaning in this model is represented by the distribution PX|W over </context>
<context position="13805" citStr="Johns and Jones (2012)" startWordPosition="2298" endWordPosition="2301">o share perceptual features and hence it should be possible to transfer perceptual information onto words that have none from their linguistically similar neighbors. Let T E 11,0}N&amp;quot;D denote a binary termdocument matrix, where each cell records the presence or absence of a term in a document. Let P E [0,1]N&amp;quot;F denote a perceptual matrix, representing a probability distribution over features for each word (see Table 1). A word’s meaning is represented by the concatenation of its textual and perceptual vectors (see Figure 3). If a word has not been normed, its perceptual vector will be all zeros. Johns and Jones (2012) propose a two-step estimation process for words without perceptual vectors. Initially, a perceptual vector is constructed based on the word’s weighted similarity to other words that have non-zero perceptual vectors: ti * sim(ti,p)X (6) where p is the representation of a word with a textual vector but an empty perceptual vector, ts are composite representations consisting of textual and perceptual vectors, sim is a measure of distributional similarity such as cosine, X a weighting parameter, and pin f the resulting inferred representation of the word. The process is repeated a second time, so </context>
<context position="17254" citStr="Johns and Jones (2012)" startWordPosition="2893" endWordPosition="2896">ulting from the projection of the corresponding perceptual feature matrix. The meaning of a word can thus be represented by its projected textual vector in CT, its projected perceptual vector in CP or their concatenation. Figure 4 shows an example of the textual and perceptual vectors for the word apple which were used as input for CCA (first row) and their new representation after the projection onto new basis vectors (second row). The CCA model as sketched above will only obtain full representations for words with perceptual features available. One solution would be to apply the method from Johns and Jones (2012) to infer the perceptual vectors and then perform CCA on the inferred vectors. Another approach which we assess experimentally (see Section 4) is to create a perceptual vector for a word that has none from its k-most (textually) similar neighbors, simply by taking the average of their perceptual vectors. This inference procedure can be applied to the original vectors or the projected vectors in CT and CP, respectively, once CCA has taken place. 2.4 Discussion Johns and Jones (2012) primarily present a model of perceptual inference, where textual data is used to infer perceptual information for</context>
<context position="18636" citStr="Johns and Jones (2012)" startWordPosition="3114" endWordPosition="3117"> textual views. As shown in the example in Figure 3 the textual vector on the left-hand side does not undergo any transformation whatsoever. The generative model put forward by Andrews et al. (2009) learns meaning representations by simultaneously considering documents and features. Rather than simply adding perceptual information to textual data it integrates both modalities jointly in a single representation which is desirable, at least from a cognitive perspective. It is unlikely that we have separate representations for different aspects of word meaning (Rogers et al., 2004). Similarly to Johns and Jones (2012), Andrews et al’s (2009) feature-topic model can also infer perceptual representations for words that have none. The inference is performed automatically in an implicit manner during component induction. In CCA, textual and perceptual data represent two different views of the same objects and the model operates on these views directly without combining or manipulating any of them a priori. Instead, the combination of the two modalities is realized via 1427 correlating the linear relationships between them. A drawback of the model lies in the need of additional methods for inferring perceptual </context>
<context position="30377" citStr="Johns and Jones (2012)" startWordPosition="5038" endWordPosition="5041">lopment set for tuning the k-nearest neighbors for CCA. Models Pearson’s r Feature-topic .15 Global similarity .03 Global similarity « CCA .12 k-NN « CCA .11 CCA « k-NN .12 Upper Bound .96 Table 3: Performance of the feature-topic, global similarityand CCA models on the Nelson et al. (1998) norms (entire dataset). All correlation coefficients are statistically significant (p &lt; 0.01). words that are not attested in McRae et al.’s norms. Recall from Section 2.3 that CCA does not have a dedicated inference mechanism. We thus experimented with three options (a) interfacing the inference method of Johns and Jones (2012) with CCA (global similarity « CCA) (b) creating a perceptual vector from the words’ k-nearest neighbors before (k-NN « CCA) or (c) after CCA takes place (CCA « k-NN). Our results are summarized in Table 3. The upper bound was estimated in the same fashion as for the smaller dataset. Despite being statistically significant (p &lt; 0.01), the correlation coefficients are lower. This is hardly surprising as perceptual information is approximate and in several cases likely to be wrong. Interestingly, we observe similar modeling trends, irrespective of whether the models are performing perceptual inf</context>
</contexts>
<marker>Johns, Jones, 2012</marker>
<rawString>Brendan T. Johns and Michael N. Jones. 2012. Perceptual Inference through Global Lexical Similarity. Topics in Cognitive Science, 4(1):103–120.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Landau</author>
<author>L Smith</author>
<author>S Jones</author>
</authors>
<title>Object Perception and Object Naming in Early Development. Trends in Cognitive Science,</title>
<date>1998</date>
<pages>27--19</pages>
<contexts>
<context position="2182" citStr="Landau et al., 1998" startWordPosition="315" endWordPosition="318">¨utze, 1998), text segmentation (Choi et al., 2001), and numerous studies of lexicon acquisition (Grefenstette, 1994; Lin, 1998). Despite their widespread use, distributional models have been criticized as “disembodied” in that they learn exclusively from linguistic information but are not grounded in perception and action (Perfetti, 1998; Barsalou, 1999; Glenberg and Kaschak, 2002). This lack of grounding contrasts with many experimental studies suggesting that word meaning is acquired not only from exposure to the linguistic environment but also from our interaction with the physical world (Landau et al., 1998; Bornstein et al., 2004). Beyond language acquisition, there is considerable evidence across both behavioral experiments and neuroimaging studies that the perceptual associates of words play an important role in language processing (for a review see Barsalou (2008)). It is thus no surprise that recent years have witnessed the emergence of perceptually grounded distributional models. An important question in the formulation of such models concerns the provenance of perceptual information. A few models use feature norms as a proxy for sensorimotor experience (Howell et al., 2005; Andrews et al.</context>
</contexts>
<marker>Landau, Smith, Jones, 1998</marker>
<rawString>B. Landau, L. Smith, and S. Jones. 1998. Object Perception and Object Naming in Early Development. Trends in Cognitive Science, 27:19–24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Landauer</author>
<author>S T Dumais</author>
</authors>
<title>A Solution to Plato’s Problem: the Latent Semantic Analysis Theory of Acquisition, Induction, and Representation of Knowledge.</title>
<date>1997</date>
<journal>Psychological Review,</journal>
<volume>104</volume>
<issue>2</issue>
<contexts>
<context position="1362" citStr="Landauer and Dumais, 1997" startWordPosition="194" endWordPosition="197">rms (i.e., attributes native speakers consider important in describing the meaning of a word). The models differ in terms of the mechanisms by which they integrate the two modalities. Experimental results show that a closer correspondence to human data can be obtained by uncovering latent information shared among the textual and perceptual modalities rather than arriving at semantic knowledge by concatenating the two. 1 Introduction Distributional models of lexical semantics have seen considerable success at accounting for a wide range of behavioral data in tasks involving semantic cognition (Landauer and Dumais, 1997; Griffiths et al., 2007). These models have also enjoyed lasting popularity in natural language processing. Examples involve information retrieval (Salton et al., 1975), word sense discrimination (Sch¨utze, 1998), text segmentation (Choi et al., 2001), and numerous studies of lexicon acquisition (Grefenstette, 1994; Lin, 1998). Despite their widespread use, distributional models have been criticized as “disembodied” in that they learn exclusively from linguistic information but are not grounded in perception and action (Perfetti, 1998; Barsalou, 1999; Glenberg and Kaschak, 2002). This lack of</context>
</contexts>
<marker>Landauer, Dumais, 1997</marker>
<rawString>T. Landauer and S. T. Dumais. 1997. A Solution to Plato’s Problem: the Latent Semantic Analysis Theory of Acquisition, Induction, and Representation of Knowledge. Psychological Review, 104(2):211–240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Automatic Retrieval and Clustering of Similar Words.</title>
<date>1998</date>
<booktitle>In Proceedings of the joint Annual Meeting of the Association for Computational Linguistics and International Conference on Computational Linguistics,</booktitle>
<pages>768--774</pages>
<location>Montr´eal, Canada.</location>
<contexts>
<context position="1691" citStr="Lin, 1998" startWordPosition="242" endWordPosition="243">dalities rather than arriving at semantic knowledge by concatenating the two. 1 Introduction Distributional models of lexical semantics have seen considerable success at accounting for a wide range of behavioral data in tasks involving semantic cognition (Landauer and Dumais, 1997; Griffiths et al., 2007). These models have also enjoyed lasting popularity in natural language processing. Examples involve information retrieval (Salton et al., 1975), word sense discrimination (Sch¨utze, 1998), text segmentation (Choi et al., 2001), and numerous studies of lexicon acquisition (Grefenstette, 1994; Lin, 1998). Despite their widespread use, distributional models have been criticized as “disembodied” in that they learn exclusively from linguistic information but are not grounded in perception and action (Perfetti, 1998; Barsalou, 1999; Glenberg and Kaschak, 2002). This lack of grounding contrasts with many experimental studies suggesting that word meaning is acquired not only from exposure to the linguistic environment but also from our interaction with the physical world (Landau et al., 1998; Bornstein et al., 2004). Beyond language acquisition, there is considerable evidence across both behavioral</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. Automatic Retrieval and Clustering of Similar Words. In Proceedings of the joint Annual Meeting of the Association for Computational Linguistics and International Conference on Computational Linguistics, pages 768–774, Montr´eal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K McRae</author>
<author>G S Cree</author>
<author>M S Seidenberg</author>
<author>C McNorgan</author>
</authors>
<title>Semantic Feature Production Norms for a Large Set of Living and Nonliving Things.</title>
<date>2005</date>
<journal>Behavior Research Methods,</journal>
<volume>37</volume>
<issue>4</issue>
<contexts>
<context position="7668" citStr="McRae et al. (2005)" startWordPosition="1198" endWordPosition="1201">ional models. Our experimental results demonstrate that joint models give a better fit to human word similarity and association data than a model that considers only one data source, or the simple concatenation of the two sources. 2 Perceptually Grounded Models In this study we examine semantic representation models that rely on linguistic and perceptual data. The linguistic environment is approximated by corpora such as the British National Corpus (BNC). As mentioned earlier, we resort to feature norms as proxy for perceptual information. In our experiments, we relied on the norming study of McRae et al. (2005), in which a large number of human participants were presented with a series of words and 1424 asked to list relevant features of the words’ referents. Table 1 presents examples of features participants listed for the nouns apple, dog, and table. The number of participants listing a certain feature for a word can be used to compute a probability distribution over features given the word: P( fk|w) = f ( fk,w) F ∑ m=1 where f (fk,w) is the number of participants who listed feature fk for word w and F is the total number of features. In the remainder of this section we will describe our models an</context>
<context position="19479" citStr="McRae et al. (2005)" startWordPosition="3246" endWordPosition="3249">perceptual data represent two different views of the same objects and the model operates on these views directly without combining or manipulating any of them a priori. Instead, the combination of the two modalities is realized via 1427 correlating the linear relationships between them. A drawback of the model lies in the need of additional methods for inferring perceptual representations for words not available in feature norms. 3 Experimental Setup Data All our experiments used a lemmatized version of the British National Corpus (BNC) as a source of textual information. The feature norms of McRae et al. (2005) were used as a proxy for perceptual information. The BNC comprises 4,049 texts totalling approximately 100 million words. McRae et al.’s feature norms consist of 541 words and 2,526 features; 824 of these features occur with at least two different words. Evaluation Tasks Our evaluation experiments compared the models discussed above on three tasks. Two of them have been previously used to evaluate semantic representation models, namely word association and word similarity. In order to simulate word association, we used the human norms collected by (Nelson et al., 1998).1 These were establishe</context>
<context position="26235" citStr="McRae et al. (2005)" startWordPosition="4370" endWordPosition="4373">Modality Pearson’s r Feature-topic +t +p .35 Feature-topic +t −p .12 Feature-topic −t +p .22 Global similarity +t +p .23 Global similarity +t −p .11 Global similarity −t +p .22 CCA +t +p .32 CCA +t −p .14 CCA −t +p .29 Upper Bound — .91 Table 2: Performance of feature-topic, global similarity, and CCA models on a subset of the Nelson et al. (1998) norms when taking into account the textual and perceptual modalities on their own (+t−p and −t+p) and in combination (+t+p). All correlation coefficients are statistically significant (p &lt; 0.01). norms (520 cue-associate pairs) that also appeared in McRae et al. (2005) and for which a perceptual vector was present. The table shows different instantiations of the three models depending on the type of modality taken into account: textual, perceptual or both. As can be seen, Andrews et al.’s (2009) featuretopic model provides a better fit with the association data when both modalities are taken into account (+t+p). A vanilla LDA model constructed solely on the BNC (+t−p) or McRae et al.’s (2005) feature norms (−t+p) yields substantially lower correlations. We observe a similar pattern with Johns and Jones’ (2012) global similarity model. Concatenation of perce</context>
<context position="27531" citStr="McRae et al. (2005)" startWordPosition="4584" endWordPosition="4587">on perceptual information alone (−t+p) comes close, whereas textual information on its own seems to have a weaker effect (+t−p).6 The CCA model takes perceptual and textual information as input in order to find a projection onto basis vectors that are maximally correlated. Although by definition the CCA model must operate on the two views, we can nevertheless isolate the contribution of each modality by considering the vectors resulting from the projection of the tex6In this evaluation setting, the model does not infer any perceptual representations; perceptual vectors are taken directly from McRae et al. (2005). 1429 tual matrix (+t−p), the perceptual matrix (−t+p) or their concatenation (+t+p). We obtain best results with the latter representation; again we observe that the perceptual information is more dominant. Overall we find that the feature-topic model and CCA perform best. In fact the correlations achieved by the two models do not differ significantly, using a t-test (Cohen and Cohen, 1983). The performance of the global similarity model is significantly worse than the feature-topic model and CCA (p &lt; 0.01). Recall that the feature-topic model (+t+p) represents words as distributions over co</context>
</contexts>
<marker>McRae, Cree, Seidenberg, McNorgan, 2005</marker>
<rawString>K. McRae, G. S. Cree, M. S. Seidenberg, and C. McNorgan. 2005. Semantic Feature Production Norms for a Large Set of Living and Nonliving Things. Behavior Research Methods, 37(4):547–559, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D L Nelson</author>
<author>C L McEvoy</author>
<author>T A Schreiber</author>
</authors>
<date>1998</date>
<institution>The University of South Florida Word Association, Rhyme, and Word Fragment Norms.</institution>
<contexts>
<context position="20055" citStr="Nelson et al., 1998" startWordPosition="3336" endWordPosition="3339">on. The feature norms of McRae et al. (2005) were used as a proxy for perceptual information. The BNC comprises 4,049 texts totalling approximately 100 million words. McRae et al.’s feature norms consist of 541 words and 2,526 features; 824 of these features occur with at least two different words. Evaluation Tasks Our evaluation experiments compared the models discussed above on three tasks. Two of them have been previously used to evaluate semantic representation models, namely word association and word similarity. In order to simulate word association, we used the human norms collected by (Nelson et al., 1998).1 These were established by presenting a large number of participants with a cue word (e.g., rice) and asking them to name an associate word in response (e.g., Chinese, wedding, food, white). For each cue word, the norms provide a set of associates and the frequencies with which they were named. We can thus compute the probability distribution over associates for each cue. Analogously, we can estimate the degree of similarity between a cue and its associates using our models (see the following section for details on the similarity measures we employed). The norms contain 63,619 unique normed </context>
<context position="25965" citStr="Nelson et al. (1998)" startWordPosition="4327" endWordPosition="4330"> assessed the models’ performance when textual and perceptual information are both available. The results in Table 2 are thus computed on the subset of Nelson’s (1998) 5Experiments with a binarized version of the term-document matrix consistently performed worse. Models Modality Pearson’s r Feature-topic +t +p .35 Feature-topic +t −p .12 Feature-topic −t +p .22 Global similarity +t +p .23 Global similarity +t −p .11 Global similarity −t +p .22 CCA +t +p .32 CCA +t −p .14 CCA −t +p .29 Upper Bound — .91 Table 2: Performance of feature-topic, global similarity, and CCA models on a subset of the Nelson et al. (1998) norms when taking into account the textual and perceptual modalities on their own (+t−p and −t+p) and in combination (+t+p). All correlation coefficients are statistically significant (p &lt; 0.01). norms (520 cue-associate pairs) that also appeared in McRae et al. (2005) and for which a perceptual vector was present. The table shows different instantiations of the three models depending on the type of modality taken into account: textual, perceptual or both. As can be seen, Andrews et al.’s (2009) featuretopic model provides a better fit with the association data when both modalities are taken </context>
<context position="30046" citStr="Nelson et al. (1998)" startWordPosition="4984" endWordPosition="4987">in the best possible way. To gain a better understanding of the models’ behavior and to allow comparisons on a larger dataset and more equal footing, we also report results on the entire dataset (20,556 cue-associate pairs).7 This entails that the models will infer perceptual vectors for the 7This excludes the data used as development set for tuning the k-nearest neighbors for CCA. Models Pearson’s r Feature-topic .15 Global similarity .03 Global similarity « CCA .12 k-NN « CCA .11 CCA « k-NN .12 Upper Bound .96 Table 3: Performance of the feature-topic, global similarityand CCA models on the Nelson et al. (1998) norms (entire dataset). All correlation coefficients are statistically significant (p &lt; 0.01). words that are not attested in McRae et al.’s norms. Recall from Section 2.3 that CCA does not have a dedicated inference mechanism. We thus experimented with three options (a) interfacing the inference method of Johns and Jones (2012) with CCA (global similarity « CCA) (b) creating a perceptual vector from the words’ k-nearest neighbors before (k-NN « CCA) or (c) after CCA takes place (CCA « k-NN). Our results are summarized in Table 3. The upper bound was estimated in the same fashion as for the s</context>
</contexts>
<marker>Nelson, McEvoy, Schreiber, 1998</marker>
<rawString>D. L. Nelson, C. L. McEvoy, and T. A. Schreiber. 1998. The University of South Florida Word Association, Rhyme, and Word Fragment Norms.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Perfetti</author>
</authors>
<title>The Limits of Co-occurrence: Tools and Theories</title>
<date>1998</date>
<booktitle>in Language Research. Discourse Processes,</booktitle>
<pages>25--363</pages>
<contexts>
<context position="1903" citStr="Perfetti, 1998" startWordPosition="274" endWordPosition="275">ral data in tasks involving semantic cognition (Landauer and Dumais, 1997; Griffiths et al., 2007). These models have also enjoyed lasting popularity in natural language processing. Examples involve information retrieval (Salton et al., 1975), word sense discrimination (Sch¨utze, 1998), text segmentation (Choi et al., 2001), and numerous studies of lexicon acquisition (Grefenstette, 1994; Lin, 1998). Despite their widespread use, distributional models have been criticized as “disembodied” in that they learn exclusively from linguistic information but are not grounded in perception and action (Perfetti, 1998; Barsalou, 1999; Glenberg and Kaschak, 2002). This lack of grounding contrasts with many experimental studies suggesting that word meaning is acquired not only from exposure to the linguistic environment but also from our interaction with the physical world (Landau et al., 1998; Bornstein et al., 2004). Beyond language acquisition, there is considerable evidence across both behavioral experiments and neuroimaging studies that the perceptual associates of words play an important role in language processing (for a review see Barsalou (2008)). It is thus no surprise that recent years have witnes</context>
</contexts>
<marker>Perfetti, 1998</marker>
<rawString>C. Perfetti. 1998. The Limits of Co-occurrence: Tools and Theories in Language Research. Discourse Processes, (25):363–377.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Regier</author>
</authors>
<title>The Human Semantic Potential.</title>
<date>1996</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="3720" citStr="Regier, 1996" startWordPosition="552" endWordPosition="553">een or red, round, shiny, smooth, crunchy, tasty, and so on; dogs have four legs and bark, whereas chairs are used for sitting. Other models focus solely on the visual modality under the assumption that it represents a major source of data from 1423 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1423–1433, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics which humans can learn semantic representations of both linguistic and non-linguistic communicative actions (Regier, 1996). For example, Feng and Lapata (2010) learn semantic representations from corpora of texts paired with naturally co-occurring images (e.g., news articles and their associated pictures), whereas Bruni et al. (2011) learn textual and visual representations independently from distinct data sources. Aside from the type of data used to capture perceptual information, another important issue concerns how the two modalities (perceptual and textual) are integrated. A simple solution would be to learn both modalities independently (Bruni et al., 2011) or to infer one modality by means of the other (Joh</context>
</contexts>
<marker>Regier, 1996</marker>
<rawString>Terry Regier. 1996. The Human Semantic Potential. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T T Rogers</author>
<author>M A Lambon Ralph</author>
<author>P Garrard</author>
<author>S Bozeat</author>
<author>J L McClelland</author>
<author>J R Hodges</author>
<author>K Patterson</author>
</authors>
<date>2004</date>
<booktitle>Structure and Deterioration of Semantic Memory: A Neuropsychological and Computational Investigation. Psychological Review,</booktitle>
<volume>111</volume>
<issue>1</issue>
<contexts>
<context position="18599" citStr="Rogers et al., 2004" startWordPosition="3108" endWordPosition="3111">ual influence of the perceptual and textual views. As shown in the example in Figure 3 the textual vector on the left-hand side does not undergo any transformation whatsoever. The generative model put forward by Andrews et al. (2009) learns meaning representations by simultaneously considering documents and features. Rather than simply adding perceptual information to textual data it integrates both modalities jointly in a single representation which is desirable, at least from a cognitive perspective. It is unlikely that we have separate representations for different aspects of word meaning (Rogers et al., 2004). Similarly to Johns and Jones (2012), Andrews et al’s (2009) feature-topic model can also infer perceptual representations for words that have none. The inference is performed automatically in an implicit manner during component induction. In CCA, textual and perceptual data represent two different views of the same objects and the model operates on these views directly without combining or manipulating any of them a priori. Instead, the combination of the two modalities is realized via 1427 correlating the linear relationships between them. A drawback of the model lies in the need of additio</context>
</contexts>
<marker>Rogers, Ralph, Garrard, Bozeat, McClelland, Hodges, Patterson, 2004</marker>
<rawString>T. T. Rogers, M. A. Lambon Ralph, P. Garrard, S. Bozeat, J. L. McClelland, J. R. Hodges, and K. Patterson. 2004. Structure and Deterioration of Semantic Memory: A Neuropsychological and Computational Investigation. Psychological Review, 111(1):205–235.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>A Wang</author>
<author>C Yang</author>
</authors>
<title>A Vector-space Model for Information Retrieval.</title>
<date>1975</date>
<journal>Journal of the American Society for Information Science,</journal>
<pages>18--613</pages>
<contexts>
<context position="1531" citStr="Salton et al., 1975" startWordPosition="219" endWordPosition="222">dalities. Experimental results show that a closer correspondence to human data can be obtained by uncovering latent information shared among the textual and perceptual modalities rather than arriving at semantic knowledge by concatenating the two. 1 Introduction Distributional models of lexical semantics have seen considerable success at accounting for a wide range of behavioral data in tasks involving semantic cognition (Landauer and Dumais, 1997; Griffiths et al., 2007). These models have also enjoyed lasting popularity in natural language processing. Examples involve information retrieval (Salton et al., 1975), word sense discrimination (Sch¨utze, 1998), text segmentation (Choi et al., 2001), and numerous studies of lexicon acquisition (Grefenstette, 1994; Lin, 1998). Despite their widespread use, distributional models have been criticized as “disembodied” in that they learn exclusively from linguistic information but are not grounded in perception and action (Perfetti, 1998; Barsalou, 1999; Glenberg and Kaschak, 2002). This lack of grounding contrasts with many experimental studies suggesting that word meaning is acquired not only from exposure to the linguistic environment but also from our inter</context>
</contexts>
<marker>Salton, Wang, Yang, 1975</marker>
<rawString>G Salton, A Wang, and C Yang. 1975. A Vector-space Model for Information Retrieval. Journal of the American Society for Information Science, 18:613–620.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Automatic Word Sense Discrimination.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>1</issue>
<marker>Sch¨utze, 1998</marker>
<rawString>Hinrich Sch¨utze. 1998. Automatic Word Sense Discrimination. Computational Linguistics, 24(1):97–124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steyvers</author>
</authors>
<title>Combining Feature Norms and Text Data with Topic Models.</title>
<date>2010</date>
<journal>Acta Psychologica,</journal>
<volume>133</volume>
<issue>3</issue>
<contexts>
<context position="2804" citStr="Steyvers, 2010" startWordPosition="414" endWordPosition="415">ein et al., 2004). Beyond language acquisition, there is considerable evidence across both behavioral experiments and neuroimaging studies that the perceptual associates of words play an important role in language processing (for a review see Barsalou (2008)). It is thus no surprise that recent years have witnessed the emergence of perceptually grounded distributional models. An important question in the formulation of such models concerns the provenance of perceptual information. A few models use feature norms as a proxy for sensorimotor experience (Howell et al., 2005; Andrews et al., 2009; Steyvers, 2010; Johns and Jones, 2012). These are obtained by asking native speakers to write down attributes they consider important in describing the meaning of a word. The attributes represent perceived physical and functional properties associated with the referents of words. For example, apples are typically green or red, round, shiny, smooth, crunchy, tasty, and so on; dogs have four legs and bark, whereas chairs are used for sitting. Other models focus solely on the visual modality under the assumption that it represents a major source of data from 1423 Proceedings of the 2012 Joint Conference on Emp</context>
<context position="4533" citStr="Steyvers, 2010" startWordPosition="682" endWordPosition="683">i et al. (2011) learn textual and visual representations independently from distinct data sources. Aside from the type of data used to capture perceptual information, another important issue concerns how the two modalities (perceptual and textual) are integrated. A simple solution would be to learn both modalities independently (Bruni et al., 2011) or to infer one modality by means of the other (Johns and Jones, 2012) and to arrive at a grounded representation simply by concatenating the two. An alternative is to learn from both modalities jointly (Andrews et al., 2009; Feng and Lapata, 2010; Steyvers, 2010). According to this view, semantic knowledge is gained by simultaneously learning from the statistical structure within each modality assuming both data sources have been generated by a shared set of meanings or topics. In this paper we undertake the first comparative study of perceptually grounded distributional models. We examine three models with different assumptions regarding the integration of perceptual and linguistic data. The first model, originally proposed by Andrews et al. (2009), is an extension of latent Dirichlet allocation (LDA, Blei et al. (2003)). It simultaneously considers </context>
</contexts>
<marker>Steyvers, 2010</marker>
<rawString>Mark Steyvers. 2010. Combining Feature Norms and Text Data with Topic Models. Acta Psychologica, 133(3):234–342.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wouter Voorspoels</author>
<author>Wolf Vanpaemel</author>
<author>Gert Storms</author>
</authors>
<title>Exemplars and Prototypes in Natural Language Concepts: A Typicality-based Evaluation.</title>
<date>2008</date>
<journal>Psychonomic Bulletin &amp; Review,</journal>
<pages>15--630</pages>
<contexts>
<context position="29149" citStr="Voorspoels et al., 2008" startWordPosition="4835" endWordPosition="4838">e to concatenation. However, note that all models in Table 2 fall short of the human upper bound which we measured by calculating the reliability of Nelson et al.’s (1998) norms. Reliability estimates the likelihood of a similarly-composed group of participants presented with the same task under the same circumstances producing identical results. We split the collected cue-associate pairs randomly into two halves and computed the correlation between them; this correlation was averaged across 200 random splits. These correlations were adjusted by applying the Spearman-Brown prediction formula (Voorspoels et al., 2008). The results in Table 2 are computed on a small fraction of Nelson et al.’s (1998) norms. One might even argue that the comparison is slightly unfair as the global similarity model is more geared towards inferring perceptual vectors rather than integrating the two modalities in the best possible way. To gain a better understanding of the models’ behavior and to allow comparisons on a larger dataset and more equal footing, we also report results on the entire dataset (20,556 cue-associate pairs).7 This entails that the models will infer perceptual vectors for the 7This excludes the data used a</context>
</contexts>
<marker>Voorspoels, Vanpaemel, Storms, 2008</marker>
<rawString>Wouter Voorspoels, Wolf Vanpaemel, and Gert Storms. 2008. Exemplars and Prototypes in Natural Language Concepts: A Typicality-based Evaluation. Psychonomic Bulletin &amp; Review, 15:630–637.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000048">
<title confidence="0.989038">
Generating Non-Projective Word Order in Statistical Linearization
</title>
<author confidence="0.948619">
Bernd Bohnet Anders Bj¨orkelund Jonas Kuhn Wolfgang Seeker Sina Zarriell
</author>
<affiliation confidence="0.960055">
Institut f¨ur Maschinelle Sprachverarbeitung
University of Stuttgart
</affiliation>
<email confidence="0.998107">
{bohnetbd,anders,jonas,seeker,zarriesa}@ims.uni-stuttgart.de
</email>
<sectionHeader confidence="0.998594" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9860535">
We propose a technique to generate non-
projective word orders in an efficient statisti-
cal linearization system. Our approach pre-
dicts liftings of edges in an unordered syntac-
tic tree by means of a classifier, and uses a
projective algorithm for tree linearization. We
obtain statistically significant improvements
on six typologically different languages: En-
glish, German, Dutch, Danish, Hungarian, and
Czech.
</bodyText>
<sectionHeader confidence="0.999531" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.989636333333333">
There is a growing interest in language-independent
data-driven approaches to natural language genera-
tion (NLG). An important subtask of NLG is sur-
face realization, which was recently addressed in the
2011 Shared Task on Surface Realisation (Belz et
al., 2011). Here, the input is a linguistic representa-
tion, such as a syntactic dependency tree lacking all
precedence information, and the task is to determine
a natural, coherent linearization of the words.
The standard data-driven approach is to traverse
the dependency tree deciding locally at each node on
the relative order of the head and its children. The
shared task results have proven this approach to be
both effective and efficient when applied to English.
It is what federal support should try to achieve
</bodyText>
<figureCaption confidence="0.8994215">
Figure 1: A non-projective example from the CoNLL
2009 Shared Task data set for parsing (Hajiˇc et al., 2009).
</figureCaption>
<bodyText confidence="0.999919111111111">
However, the approach can only generate pro-
jective word orders (which can be drawn with-
out any crossing edges). Figure 1 shows a non-
projective word order: the edge connecting the ex-
tracted wh-pronoun with its head crosses another
edge. Once what has been ordered relative to
achieve, there are no ways of inserting intervening
material. In this case, only ungrammatical lineariza-
tions can be produced from the unordered input tree:
</bodyText>
<listItem confidence="0.708791666666667">
(1) a. *It is federal support should try to what achieve
b. *It is federal support should try to achieve what
c. *It is try to achieve what federal support should
</listItem>
<bodyText confidence="0.999947">
Although rather infrequent in English, non-
projective word orders are quite common in lan-
guages with a less restrictive word order. In these
languages, it is often possible to find a grammati-
cally correct projective linearization for a given in-
put tree, but discourse coherence, information struc-
ture, and stylistic factors will often make speak-
ers prefer some non-projective word order.1 Figure
2 shows an object fronting example from German
where the edge between the subject and the finite
verb crosses the edge between the object and the full
verb. Various other constructions, such as extraposi-
tion of (relative) clauses or scrambling, can lead to
non-projectivity. In languages where word order is
driven to an even larger degree by information struc-
ture, such as Czech and Hungarian, non-projectivity
can likewise result from various ordering decisions.
These phenomena have been studied extensively in
</bodyText>
<note confidence="0.977179">
1A categorization of non-projective edges in the Prague
Dependency Treebank (B¨ohmov´a et al., 2000) is presented in
Hajiˇcov´a et al. (2004).
</note>
<page confidence="0.933707">
928
</page>
<note confidence="0.799636">
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 928–939, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.988456333333333">
the linguistic literature, and for certain languages,
work on rule-based generation has addressed certain
aspects of the problem.
</bodyText>
<figure confidence="0.766817333333333">
Das Mandat will er zur¨uckgeben .
the.ACC mandate.ACC want.3SG he.NOM return.INF .
’He wants to return the mandate.’
</figure>
<figureCaption confidence="0.9969175">
Figure 2: German object fronting with complex verb in-
troducing a non-projective edge.
</figureCaption>
<bodyText confidence="0.999988176470588">
In this paper, we aim for a general data-driven ap-
proach that can deal with various causes for non-
projectivity and will work for typologically dif-
ferent languages. Our technique is inspired by
work in data-driven multilingual parsing, where
non-projectivity has received considerable attention.
In pseudo-projective parsing (Kahane et al., 1998;
Nivre and Nilsson, 2005), the parsing algorithm is
restricted to projective structures, but the issue is
side-stepped by converting non-projective structures
to projective ones prior to training and application,
and then restoring the original structure afterwards.
Similarly, we split the linearization task in two
stages: initially, the input tree is modified by lifting
certain edges in such a way that new orderings be-
come possible even under a projectivity constraint;
the second stage is the original, projective lineariza-
tion step. In parsing, projectivization is a determin-
istic process that lifts edges based on the linear or-
der of a sentence. Since the linear order is exactly
what we aim to produce, this deterministic conver-
sion cannot be applied before linearization. There-
fore, we use a statistical classifier as our initial lift-
ing component. This classifier has to be trained on
suitable data, and it is an empirical question whether
the projective linearizer can take advantage of this
preceding lifting step.
We present experiments on six languages with
varying degrees of non-projective structures: En-
glish, German, Dutch, Danish, Czech and Hungar-
ian, which exhibit substantially different word order
properties. Our approach achieves significant im-
provements on all six languages. On German, we
also report results of a pilot human evaluation.
</bodyText>
<sectionHeader confidence="0.999804" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.976139608695652">
An important concept for tree linearization are word
order domains (Reape, 1989). The domains are bags
of words (constituents) that are not allowed to be dis-
continuous. A straightforward method to obtain the
word order domains from dependency trees and to
order the words in the tree is to use each word and
its children as domain and then to order the domains
and contained words recursively. As outlined in the
introduction, the direct mapping of syntactic trees to
domains does not provide the possibility to obtain
all possible correct word orders.
Linearization systems can be roughly distin-
guished as either rule-based or statistical systems. In
the 2011 Shared Task on Surface Realisation (Belz
et al., 2011), the top performing systems were all
statistical dependency realizers (Bohnet et al., 2011;
Guo et al., 2011; Stent, 2011).
Grammar-based approaches map dependency
structures or phrase structures to a tree that repre-
sents the linear precedence. These approaches are
mostly able to generate non-projective word orders.
Early work was nearly exclusively applied to phrase
structure grammars (e.g. (Kathol and Pollard, 1995;
Rambow and Joshi, 1994; Langkilde and Knight,
1998)). Concerning dependency-based frameworks,
Br¨oker (1998) used the concept of word order do-
mains to separate surface realization from linear
precedence trees. Similarly, Duchier and Debus-
mann (2001) differentiate Immediate Dominance
trees (ID-trees) from Linear Precedence trees (LP-
trees). Gerdes and Kahane (2001) apply a hierarchi-
cal topological model for generating German word
order. Bohnet (2004) employs graph grammars to
map between dependency trees and linear prece-
dence trees represented as hierarchical graphs. In the
frameworks of HPSG, LFG, and CCG, a grammar-
based generator produces word order candidates that
might be non-projective, and a ranker is used to se-
lect the best surface realization (Cahill et al., 2007;
White and Rajkumar, 2009).
Statistical methods for linearization have recently
become more popular (Langkilde and Knight, 1998;
Ringger et al., 2004; Filippova and Strube, 2009;
Wan et al., 2009; He et al., 2009; Bohnet et al., 2010;
Guo et al., 2011). They typically work by travers-
ing the syntactic structure either bottom-up (Filip-
</bodyText>
<page confidence="0.997735">
929
</page>
<bodyText confidence="0.999793866666667">
pova and Strube, 2007; Bohnet et al., 2010) or top-
down (Guo et al., 2011; Bohnet et al., 2011). These
linearizers are mostly applied to English and do not
deal with non-projective word orders. An excep-
tion is Filippova and Strube (2007), who contribute
a study on the treatment of preverbal and postver-
bal constituents for German focusing on constituent
order at the sentence level. The work most similar
to ours is that of Gamon et al. (2002). They use
machine-learning techniques to lift edges in a pre-
processing step to a surface realizer. Their objec-
tive is the same as ours: by lifting, they avoid cross-
ing edges. However, contrary to our work, they use
phrase-structure syntax and focus on a limited num-
ber of cases of crossing branches in German only.
</bodyText>
<sectionHeader confidence="0.927303" genericHeader="method">
3 Lifting Dependency Edges
</sectionHeader>
<bodyText confidence="0.999942">
In this section, we describe the first of the two stages
in our approach, namely the classifier that lifts edges
in dependency trees. The classifier we aim to train
is meant to predict liftings on a given unordered de-
pendency tree, yielding a tree that, with a perfect lin-
earization, would not have any non-projective edges.
</bodyText>
<subsectionHeader confidence="0.997977">
3.1 Preliminaries
</subsectionHeader>
<bodyText confidence="0.999961421052632">
The dependency trees we consider are of the form
displayed in Figure 1. More precisely, all words (or
nodes) form a rooted tree, where every node has ex-
actly one parent (or head). Edges point from head
to dependent, denoted in the text by h —* d, where h
is the head and d the dependent. All nodes directly
or transitively depend on an artificial root node (de-
picted in Figure 1 as the incoming edge to is).
We say that a node a dominates a node d if a is
an ancestor of d. An edge h —* d is projective iff
h dominates all nodes in the linear span between h
and d. Otherwise it is non-projective. Moreover,
a dependency tree is projective iff all its edges are
projective. Otherwise it is non-projective.
A lifting of an edge h —* d (or simply of the node
d) is an operation that replaces h —* d with g —* d,
given that there exists an edge g —* h in the tree, and
undefined otherwise (i.e. the dependent d is reat-
tached to the head of its head).2 When the lifting
</bodyText>
<footnote confidence="0.899120666666667">
2The undefined case occurs only when d depends on the
root, and hence cannot be lifted further; but these edges are by
definition projective, since the root dominates the entire tree.
</footnote>
<bodyText confidence="0.89116">
operation is applied n successive times to the same
node, we say the node was lifted n steps.
</bodyText>
<subsectionHeader confidence="0.999542">
3.2 Training
</subsectionHeader>
<bodyText confidence="0.985392785714286">
During training we make use of the projectivization
algorithm described by Nivre and Nilsson (2005).
It works by iteratively lifting the shortest non-
projective edges until the tree is projective. Here,
shortest edge refers to the edge spanning over the
fewest number of words. Since finding the shortest
edge relies on the linear order, instead of lifting the
shortest edge, we lift non-projective edges ordered
by depth in the tree, starting with the deepest nested
edge. A lifted version of the tree from Figure 1 is
shown in Figure 3. The edge of what has been lifted
three steps (the original edge is dotted), and the tree
is no longer non-projective.
It is what federal support should try to achieve
</bodyText>
<figureCaption confidence="0.99822">
Figure 3: The sentence from Figure 1, where what has
been assigned a new head (solid line). The original edge
is dotted.
</figureCaption>
<bodyText confidence="0.999958">
We model the edge lifting problem as a multi-
class classification problem and consider nodes one
at a time and ask the question “How far should this
edge be lifted?”, where classes correspond to lifting
0, 1, 2, ..., n steps. To create training instances we
use the projectivization algorithm mentioned above.
We traverse the nodes of the tree sorted by depth.
For multiple nodes at the same depth, ties are broken
by linear order, i.e. for multiple nodes at the same
depth, the leftmost is visited first. When a node is
visited, we create a training instance out of it. Its
class is determined by the number of steps it would
be lifted by the projectivization algorithm given the
linear order (in most cases the class corresponds to
no lifting, since most edges are projective). As we
traverse the nodes, we also execute the liftings (if
any) and update the tree on the fly.
The training instances derived are used to train a
logistic regression classifier using the LIBLINEAR
package (Fan et al., 2008). The features used for
the lifting classifier are described in Table 1. Since
we use linear classifiers, our feature set also con-
tains conjunctions of atomic features. The features
</bodyText>
<page confidence="0.995166">
930
</page>
<table confidence="0.999337181818182">
Atomic features
`dx E {w, wp, wgp, wch, w3, wun} morph(x), label(x), lemma(x), PoS(x)
`dx E {wgc, wne, wco} label(x), lemma(x), PoS(x)
Complex features
`dx E {w, wp, wgp} lemma(x)+PoS(x), label(x)+PoS(x), label(x)+lemma(x)
`dx E {wch, w3, wun}, y = w lemma(x)+lemma(y), PoS(y)+lemma(x), PoS(y)+lemma(x)
`dx E {w, wp, wgp}, y = HEAD(x) lemma(x)+lemma(y), lemma(x)+PoS(y), PoS(x)+lemma(y)
`dx E {w, wp, wgp}, y = HEAD(x), z = HEAD(y) PoS(x)+PoS(y)+PoS(z), label(x)+label(y)+label(z)
`dx E {wch, w3, wun}, y = HEAD(x), z = HEAD(y) PoS(x)+PoS(y)+PoS(z), label(x)+label(y)+label(z)
Non-binary features
`dx E {w, wp, wgp} SUBTREESIZE(x), RELSUBTREESIZE(x)
</table>
<tableCaption confidence="0.698444">
Table 1: Features used for lifting. w refers to the word (dependent) in question. And with respect to w, wp is the
parent; w9p is the grandparent; wch are children; ws are siblings; wun are uncles (i.e. children of the grandparent,
excluding the parent); w9c are grandchildren; wn, are nephews (i.e. grandchildren of the parent that are not children
of w); wco are cousins (i.e. grandchildren of the grandparent that are not w or siblings of w). The non-binary feature
functions refer to: SUBTREESIZE – the absolute number of nodes below x, RELSUBTREESIZE – the relative size of
the subtree rooted at x with respect to the whole tree.
</tableCaption>
<bodyText confidence="0.9993953">
involve the lemma, dependency edge label, part-of-
speech tag, and morphological features of the node
in question, and of several neighboring nodes in the
dependency tree. We also have a few non-binary fea-
tures that encode the size of the subtree headed by
the node and its ancestors.
We ran preliminary experiments to determine the
optimal architecture. First, other ways of modeling
the liftings are conceivable. To find new reattach-
ment points, Gamon et al. (2002) propose two other
ways, both using a binary classifier: applying the
classifier to each node x along the path to the root
asking “Should d be reattached to x?”; or lifting one
step at a time and applying the classifier iteratively
until it says stop. They found that the latter outper-
formed the former. We tried this method, but found
that it was inferior to the multi-class model and more
frequently over- or underlifted.
Second, to avoid data sparseness for infrequent
lifting distances, we introduce a maximum number
of liftings. We found that a maximum of 3 gave the
best performance. In the pseudocode below, we re-
fer to this number as maxsteps.3 This means that we
are able to predict the correct lifting for most (but
not all) of the non-projective edges in our data sets
(cf. Table 3).
Third, as Nivre and Nilsson (2005) do for pars-
3During training, nodes that are lifted further than maxsteps
are assigned to the class corresponding to maxsteps. This ap-
proach worked better than ignoring the training instance or
treating it as a non-lifting (i.e. a lifting of 0 steps).
ing, we experimented with marking edges that were
lifted by indicating this on the edge labels. In the
case of parsing, this step is necessary in order to re-
verse the liftings in the parser output. In our case,
it could potentially be beneficial for both the lifting
classifier, and for the linearizer. However, we found
that marking liftings at best gave similar results as
not marking, so we kept the original labels without
marking.
</bodyText>
<subsectionHeader confidence="0.99816">
3.3 Decoding
</subsectionHeader>
<bodyText confidence="0.9999786875">
In the decoding stage, an unordered tree is given and
the goal is to lift edges that would be non-projective
with respect to the gold linear order. Similarly to
how training instances are derived, the decoding al-
gorithm traverses the tree bottom-up and visits every
node once. Ties between nodes at the same depth are
broken in an arbitrary but deterministic way. When
a node is visited, the classifier is applied and the cor-
responding lifting is executed. Pseudocode is given
in Algorithm 1.4
Different orderings of nodes at the same depth
can lead to different lifts. The reason is that lift-
ings are applied immediately and this influences the
features when subsequent nodes are considered. For
instance, consider two sibling nodes ni and nj. If
ni is visited before nj, and ni is lifted, this means
</bodyText>
<footnote confidence="0.51479225">
4The MIN function is used to guarantee that the edge is not
lifted beyond the root node of the tree. This does not happen
in practice though, since the feature set of the classifier include
features that implicitly encode the proximity to the root node.
</footnote>
<page confidence="0.993703">
931
</page>
<bodyText confidence="0.999107333333333">
that at the time we visit nj, nz is no longer a sibling
of nj, but rather an uncle. An obvious extension of
the decoding algorithm presented above is to apply
beam search. This allows us to consider nj both in
the context where nz has been lifted and when it has
not been lifted.
</bodyText>
<figure confidence="0.94968475">
1 N +— NODES(T)
2 SORT-BY-DEPTH-BREAK-TIES-ARBITRARILY(N, T)
3 foreach node G N do
4 feats +— EXTRACT-FEATURES(node, T)
5 steps +— CLASSIFY(feats)
6 steps +— MIN(steps,ROOT-DIST(node))
7 LIFT(node, T, steps)
8 return T
</figure>
<figureCaption confidence="0.790169">
Algorithm 1: Greedy decoding for lifting.
</figureCaption>
<bodyText confidence="0.999540384615385">
Pseudocode for the beam search decoder is given
in Algorithm 2. The algorithm keeps an agenda of
trees to explore as each node is visited. For every
node, it clones the current tree and applies every pos-
sible lifting. Every tree also has an associated score,
which is the sum of the scores of each lifting so far.
The score of a lifting is defined to be the log proba-
bility returned from the logistic classifier. After ex-
ploring all trees in the agenda, the k-best new trees
from the beam are extracted and put back into the
agenda. When all nodes have been visited, the best
tree in the agenda is returned. For the experiments
the beam size (k in Algorithm 2) was set to 20.
</bodyText>
<figure confidence="0.933587611111111">
1 N +— NODES(T)
2 SORT-BY-DEPTH-BREAK-TIES-ARBITRARILY(N, T)
3 Tscore +— 0
4 Agenda +— {T}
5 foreach node G N do
6 Beam +— 0
7 foreach tree G Agenda do
8 feats +— EXTRACT-FEATURES(node, tree)
9 m +— MIN(maxsteps,ROOT-DIST(node))
10 foreach s G 0 .. maxsteps do
11 t +— CLONE(tree)
12 score +— GET-LIFT-SCORE(feats, s)
13 tscore = tscore + score
14 LIFT(node, t, s)
15 Beam +— Beam U {t}
16 Agenda +— EXTRACTKBEST(Beam, k)
17 return EXTRACTKBEST(Agenda, 1)
Algorithm 2: Beam decoding for lifting.
</figure>
<bodyText confidence="0.999724111111111">
While beam search allows us to explore the search
space somewhat more thoroughly, a large number of
possibilities remain unaccounted for. Again, con-
sider the sibling nodes nz and nj when nz is visited
before nj. The beam allows us to consider nj both
when nz is lifted and when it is not. However, the
situation where nj is visited before nz is still never
considered. Ideally, all permutations of nodes at the
same depth should be explored before moving on.
Unfortunately this leads to a combinatorial explo-
sion of permutations, and exhaustive search is not
tractable. As an approximation, we create two or-
derings and run the beam search twice. The dif-
ference between the orderings is that in the second
one all ties are reversed. As this bibeam consistently
improved over the beam in Algorithm 2, we only
present these results in Section 5 (there denoted sim-
ply Beam).
</bodyText>
<sectionHeader confidence="0.999075" genericHeader="method">
4 Linearization
</sectionHeader>
<bodyText confidence="0.997793035714286">
A linearizer searches for the optimal word order
given an unordered dependency tree, where the op-
timal word order is defined as the single reference
order of the dependency tree in the gold standard.
We employ a statistical linearizer that is trained on a
corpus of pairs consisting of unordered dependency
trees and their corresponding sentences. The lin-
earization method consists of the following steps:
Creating word order domains. In the first step,
we build the word order domains dh for all nodes
h E y of a dependency tree y. A domain is defined
as a node and all of its direct dependents. For ex-
ample, the tree shown in Figure 3 has the following
domains: {it, be, should}, {what, support, should, try},
{federal, support}, {try, to}, {to, achieve}
If an edge was lifted before the linearization, the
lifted node will end up in the word order domain of
its new head rather than in the domain of its original
head. This way, the linearizer can deduce word or-
ders that would result in non-projective structures in
the non-lifted tree.
Ordering the words of the domains. In the sec-
ond step, the linearizer orders the words of each do-
main. The position of a subtree is determined by the
position of the head of the subtree in the enclosing
domain. Algorithm 3 shows the tree linearization
algorithm. In our implementation, the linearizer tra-
verses the tree either top-down or bottom-up.
</bodyText>
<page confidence="0.968382">
932
</page>
<figure confidence="0.980111407407408">
1 // T is the dependency tree with lifted nodes
2 beam-size +— 1000
3 for h E T do
4 domainh +— GET-DOMAIN(T,h)
5 // initialize the beam with a empty word list
6 Agendah +— (e)
7 foreach w E domainh do
8 // beam for extending word order lists
9 Beam +— ()
10 foreach l E Agendah do
11 // clone list l and append the word w
12 if w E� l then
13 l&apos; +— APPEND(l,m)
14 Beam +— Beam ® l&apos;
15 score[l&apos;] +— COMPUTE-SCORE(l&apos;)
16 if  |Beam  |&gt; beam-size then
17 SORT-LISTS-DESCENDING-TO-
SCORE(Beam,score)
18 Agendah +— SUBLIST(0,beam-size,Beam)
19 else
20 Agendah +— Beam
21 foreach l E Beam do
22 SCOREg[l] +— SCORE[l] +
GLOBAL-SCORE(l)
23 Agendah +— Beam
24 return Beam
Algorithm 3: Dependency Tree Linearization.
</figure>
<bodyText confidence="0.999816545454545">
The linearization algorithm initializes the word
order beam (agendah) with an empty order (E) (line
6). It then iterates over the words of a domain (lines
7-20). In the first iteration, the algorithm clones and
extends the empty word order list (E) by each word
of the sentence (line 12-15). If the beam (beam)
exceeds a certain size (beam-size), it is sorted by
score and pruned to maximum beam size (beam-
size) (lines 16-20). The following example illus-
trates the extensions of the beam for the top domain
shown in Figure 3.
</bodyText>
<listItem confidence="0.69471975">
Iter. agendabe
0: (e)
1: ((it) (be) (should))
2: ((it be) (it should) (be it) (be should) ...)
</listItem>
<bodyText confidence="0.999772444444444">
The beam enables us to apply features that encode
information about the first tokens and the last token,
which are important for generating, e.g. the word
order of questions, i. e. if the last token is a question
mark then the sentence should probably be a ques-
tion (cf. feature set shown in Table 2). Furthermore,
the beam enables us to generate alternative lineariza-
tions. For this, the algorithm iterates over the alter-
native word orders of the domains in order to as-
semble different word orders on the sentence level.5
Finally, when traversing the tree bottom-up, the al-
gorithm has to use the different orders of the already
ordered subtrees as context, which also requires a
search over alternative word orders of the domains.
Training of the Linearizer. We use MIRA
(Crammer et al., 2006) for the training of the lin-
earizer. The classifier provides a score that we use to
rank the alternative word orders. Algorithm 3 calls
two functions to compute the score: compute-score
(line 15) for features based on pairs of words and tri-
grams and compute-global-score for features based
on word patterns of a domain. Table 2 shows the
feature set for the two functions. In the case that the
linearization of a word order domain is incorrect the
algorithm updates its weight vector w. The follow-
ing equation shows the update function of the weight
vector:
</bodyText>
<equation confidence="0.972068">
w = w + τh(φ(dh, T, xg) − φ(dh, T, xp))
</equation>
<bodyText confidence="0.9851245">
We update the weight vector w by adding the dif-
ference of the feature vector representation of the
correct linearization xg and the wrongly predicted
linearization xp, multiplied by τ. τ is the passive-
aggressive update factor as defined below. The suf-
fered lossh is φ(dh, T, xp) − φ(dh, T, xg).
</bodyText>
<equation confidence="0.980868">
lossh
τ =
jjφ(dh,T,xg)−φ(dh,T,xp)jj2
</equation>
<bodyText confidence="0.999822666666667">
Creating the word order of a sentence. The lin-
earizer traverses the tree either top-down or bottom-
up and assembles the results in the surface order.
The bottom-up linearization algorithm can take into
account features drawn from the already ordered
subtrees while the top-down algorithm can employ
as context only the unordered nodes. However, the
bottom-up algorithm additionally has to carry out a
search over the alternative linearization of the sub-
domains, as different orders of the subdomain pro-
vide different context features. This leads to a higher
linearization time. We implemented both, but could
only find a rather small accuracy difference. In the
following, we therefore present results only for the
top-down method.
</bodyText>
<footnote confidence="0.994382">
5The beam also makes it possible to employ a generative
language model to rerank alternative linearizations.
</footnote>
<page confidence="0.995132">
933
</page>
<table confidence="0.9995272">
Atomic features
For nodes w E lemma(w), label(w), PoS(w), num-children(w), num-grandchildren(w), label-children(w),
domainh PoS-children(w)
For domain head(w1,w2), head(w1,w2,w3), label(head), PoS(head), PoS(w1), label(wn), label(wn_1),
domainh contains-?(domainh)
Complex features
For bigrams feat2: label(w1)+label(w2), label(w1)+lemma(w2), lemma(w1)+lemma(w2), PoSw1+PoSw2
(w1, w2) E feat3: label(w1)+num-children(w2)+num-children(w1),PoS-child(w1)+label(w1)+label(w2)
domainh feat¢: label(w1)+label(w2)+lemma(w2)+PoS(w1), label(w1)+label(w2)+PoS(head)+head(w1,w2)
feats: label(w1)+label(w2)+PoS(head)+label(head)+head(w1,w2)
For trigrams feat3: lemma(w1)+lemma(w2)+lemma(w3)
(w1, w2, w3) E feat¢: PoS(w1)+PoS(w2)+PoS(w3)+head(w1,w2,w3)
domainh feats: label(w1)+label(w2)+label(w3)+PoS(w1)+head(w1,w2,w3)
For sentences feats: label(w1)+label(wn_1)+lemma(head)+lemma(w1)+lemma(wn_1)
feat7: PoS(w1)+PoS(w2)+PoS(w3)+PoS(wn_1)+PoS(wn_2)+PoS(wn_3)+contains-?(s)
</table>
<tableCaption confidence="0.999226333333333">
Table 2: Exemplified features used for scoring linearizations of a word order domain (see Algorithm 3). Atomic
features which represent properties of a node or a domain are conjoined into feature vectors of different lengths.
Linearizations are scored based on bigrams, trigrams, and global sentence-level features.
</tableCaption>
<sectionHeader confidence="0.999174" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.9984198">
We conduct experiments on six European languages
with varying degrees of word order restrictions:
While English word order is very restrictive, Czech
and Hungarian exhibit few word order constraints.
Danish, Dutch, and German (so-called V2, i. e.
verb-second, languages) show a relatively free word
order that is however more restrictive than in Hun-
garian or Czech. The English and the Czech data
are from the CoNLL 2009 Shared Task data sets
(Hajiˇc et al., 2009). The Danish and the Dutch data
are from the CoNLL 2006 Shared Task data sets
(Buchholz and Marsi, 2006). For Hungarian, we use
the Hungarian Dependency Treebank (Vincze et al.,
2010), and for German, we use a dependency con-
version by Seeker and Kuhn (2012).
</bodyText>
<table confidence="0.980767">
# sent’s np sent’s np edges np &lt; 3 lifts
English 39,279 7.63 % 0.39% 98.39%
German 36,000 28.71% 2.34% 94.98%
Dutch 13,349 36.44% 5.42% 99.80%
Danish 5,190 15.62 % 1.00% 96.72%
Hungarian 61,034 15.81% 1.45% 99.82%
Czech 38,727 22.42% 1.86% 99.84%
</table>
<tableCaption confidence="0.885888666666667">
Table 3: Size of training sets, percentage of non-
projective (np) sentences and edges, percentage of np
edges covered by 3 lifting steps.
</tableCaption>
<bodyText confidence="0.998646333333333">
Table 3 shows the sizes of the training corpora
and the percentage of non-projective sentences and
edges in the data. Note that the data sets for Dan-
ish and Dutch are quite small. English has the least
percentage of non-projective edges. Czech, Ger-
man, and Dutch show the highest percentage of non-
projective edges. The last column shows the per-
centage of non-projective edges that can be made
projective by at most 3 lifting steps.
</bodyText>
<subsectionHeader confidence="0.98536">
5.1 Setup
</subsectionHeader>
<bodyText confidence="0.999958043478261">
In our two-stage approach, we first train the lifting
classifier. The results for this classifier are reported
in Section 5.2.
Second, we train the linearizer on the output of
the lifting classifier. To assess the impact of the
lifting technique on linearization, we built four sys-
tems on each language: (a) a linearizer trained on
the original, non-lifted dependency structures (No-
lift), two trained on the automatically lifted edges
(comparing (b) the beam and (c) greedy decoding),
(d) one trained on the oracle, i. e. gold-lifted struc-
tures, which gives us an upper bound for the lifting
technique. The linearization results are reported in
Section 5.3.
In this two-stage setup, we have the problem that,
if we re-apply the lifting classifier on the data it was
trained on, the input for the linearizer will be better
during training than during testing. To provide real-
istic training data for the linearizer, we make a 10-
fold cross-validation of the lifting classifier on the
training set, and use this as training data for the lin-
earizer. The lifting classifier that is applied to the
test set is trained on the entire training set.
</bodyText>
<page confidence="0.995902">
934
</page>
<subsectionHeader confidence="0.999246">
5.2 Lifting results
</subsectionHeader>
<bodyText confidence="0.999759285714286">
To evaluate the performance of the lifting classifier,
we present precision, recall, and F-measure results
for each language. We also compute the percentage
of sentences that were handled perfectly by the lift-
ing classifier. Precision and recall are defined the
usual way in terms of true positives, false positives,
and false negatives, where true positives are edges
that should be lifted and were lifted correctly; false
positives are edges that should not be lifted but were
and edges that should be lifted and were lifted, but
were reattached in the wrong place; false negatives
are edges that should be lifted but were not.
The performance of both the greedy decoder and
the bibeam decoder are shown in Table 4. The scores
are taken on the cross-validation on the training set,
as this provides more reliable figures. The scores
are micro-averaged, i.e. all folds are concatenated
and compared to the entire training set.
Although the major evaluation of the lifting is
given by the performance of the linearizer, Table 4
gives us some clues about the lifting. We see that
precision is generally much higher than recall. We
believe this is related to the fact that some phenom-
ena encoded by non-projective edges are more sys-
tematic and thus easier to learn than others (e. g. wh-
extraction vs. relative clause extraposition). We also
find that beam search consistently yields modest in-
creases in performance.
</bodyText>
<table confidence="0.999009625">
Greedy Beam
P R F1 Perfect P R F1 Perfect
Eng 77.31 50.45 61.05 95.76 78.85 50.63 61.66 95.83
Ger 72.33 63.59 67.68 81.91 72.05 64.41 68.02 81.97
Dut 76.66 74.89 75.77 79.28 78.07 76.49 77.27 80.34
Dan 85.90 58.55 69.64 92.76 85.90 58.55 69.64 92.74
Hun 72.60 61.61 66.66 88.46 73.06 64.77 68.67 88.73
Cze 77.79 55.00 64.44 86.28 77.31 55.68 64.74 86.33
</table>
<tableCaption confidence="0.986163">
Table 4: Precision, recall, F-measure and perfect projec-
tivization results for the lifting classifier.
</tableCaption>
<subsectionHeader confidence="0.999752">
5.3 Linearization Results and Discussion
</subsectionHeader>
<bodyText confidence="0.9998094">
We evaluate the linearizer with standard metrics: n-
gram overlap measures (BLEU, NIST), edit distance
(Edit), and the proportion of exactly linearized sen-
tences (Exact). As a means to assess the impact of
lifting more precisely, we propose the word-based
measure Exactiift which only looks at the words
with an incoming lifted edge. The Exactiift score
then corresponds to the percentage of these words
that has been realized in the exact same position as
in the original sentence.
</bodyText>
<table confidence="0.99985232">
LangLift BLEU NIST Edit Exact Exactlift Nlift
EngNolift 0.911 15.09 0.922 56.40 0.00 0
EngGreedy 0.914 15.10 0.923 57.27 59.87 152
EngBeam 0.916 15.11 0.925 58.48 62.82 156
EngOracle 0.923 15.14 0.928 60.73 70.42 240
GerNolift 0.792 13.76 0.844 40.4 0.00 0
GerGreedy 0.811 13.86 0.864 42.9 55.21 480
GerBeam 0.813 13.86 0.866 43.3 56.47 487
GerOracle 0.843 13.97 0.889 49.95 72.87 634
DutNolift 0.743 11.31 0.796 30.05 0.00 0
DutGreedy 0.784 11.47 0.797 37.56 41.02 256
DutBeam 0.778 11.46 0.8 37.05 47.45 255
DutOracle 0.825 11.63 0.848 44.82 70.55 292
DanNolift 0.836 11.80 0.886 44.41 0.00 0
DanGreedy 0.852 11.88 0.90 45.96 67.65 34
DanBeam 0.858 11.90 0.90 48.76 67.65 34
DanOracle 0.865 11.92 0.90 50.93 74.42 43
HunNolift 0.755 15.70 0.839 30.71 0.00 0
HunGreedy 0.764 15.71 0.844 31.98 41,81 1,538
HunBeam 0.764 15.71 0.844 31.98 41.37 1,581
HunOracle 0.777 15.79 0.849 34.30 57.53 1,933
CzeNolift 0.693 14.32 0.789 25.14 0.00 0
CzeGreedy 0.711 14.45 0.797 26.85 42.04 923
CzeBeam 0.712 14.45 0.795 26.37 41.34 941
CzeOracle 0.729 14.52 0.806 28.79 53.12 1,282
</table>
<tableCaption confidence="0.929639">
Table 5: Performance of linearizers using different lift-
ings, Exactlift is the exact match for words with an in-
coming lifted edge, Nlift is the total number of lifted
edges.
</tableCaption>
<bodyText confidence="0.999964375">
The results are presented in Table 5. On each
language, the predicted liftings significantly im-
prove on the non-lifted baseline (except the greedy
decoding in English).6 The differences between
the beam and the greedy decoding are not signif-
icant. The scores on the oracle liftings suggest
that the impact of lifting on linearization is heav-
ily language-dependent: It is highest on the V2-
languages, and somewhat smaller on English, Hun-
garian, and Czech. This is not surprising since the
V2-languages (especially German and Dutch) have
the highest proportion of non-projective edges and
sentences (see Table 3). On the other hand, En-
glish has a very small number of non-projective
edges, such that the BLEU score (which captures
the n-gram level) reflects the improvement by only
</bodyText>
<footnote confidence="0.994121">
6We used a t-test, with α = 0.01.
</footnote>
<page confidence="0.997273">
935
</page>
<bodyText confidence="0.998773">
a small increase. However, note that, on the sen-
tence level, the percentage of exactly regenerated
sentences increases by 2 points which suggests that
a non-negligible amount of non-projective sentences
can now be generated more fluently.
</bodyText>
<figure confidence="0.976421555555556">
80
75
70
65
60
55
50
Eng Ger Dut
lan
</figure>
<figureCaption confidence="0.873069">
Figure 4: Accuracy for the linearization of the sentences’
left and right periphery, the bars are upper and lower
bounds of the non-lifted and the gold-lifted baseline.
</figureCaption>
<bodyText confidence="0.99988959375">
The Exactlift measure refines this picture: The
linearization of the non-projective edges is relatively
exact in English, and much less precise in Hungarian
and Czech where Exactlift is even low on the gold-
lifted edges. The linearization quality is also quite
moderate on Dutch where the lifting leads to con-
siderable improvements. These tendencies point to
some important underlying distinctions in the non-
projective word order phenomena over which we
are generalizing: In certain cases, the linearization
seems to systematically follow from the fact that the
edge has to be lifted, such as wh-extraction in En-
glish (Figure 1). In other cases, the non-projective
linearization is just an alternative to other grammati-
cal, but maybe less appropriate, realizations, such as
the prefield-occupation in German (Figure 2).
Since a lot of non-projective word orders affect
the clause-initial or clause-final position, we evalu-
ate the exact match of the left periphery (first three
words) and the right periphery (last three words) of
the sentence. The accuracies obtained are plotted
in Figure 4, where the lower and upper bars corre-
spond to the lower and upper bound from the non-
lifted and the gold-lifted baseline. It clearly emerges
from this figure that the range of improvements ob-
tainable from lifting is closely tied to the general
linearization quality, and also to word order prop-
erties of the languages. Thus, the range of sentences
affected by the lifting is clearly largest for the V2-
languages. The accuracies are high, but the ranges
are small for English, whereas the accuracies are low
and the ranges quite small for Czech and Hungarian.
</bodyText>
<table confidence="0.9985102">
System BLEU NIST
(Bohnet et al., 2011) (ranked 1st) 0.896 13.93
(Guo et al., 2011) (ranked 2nd) 0.862 13.68
Baseline-Non-Lifted + LM 0.896 13.94
Beam-Lifted + LM 0.901 13.96
</table>
<tableCaption confidence="0.980150666666667">
Table 6: Results on the development set of the 2011
Shared Task on Surface Realisation data, (the test set was
not officially released).
</tableCaption>
<bodyText confidence="0.999958916666666">
We also evaluated our linearizer on the data of
2011 Shared Task on Surface Realisation, which is
based on the English CoNLL 2009 data (like our
previous evaluations) but excludes information on
morphological realization. For training and evalu-
ation, we used the exact set up of the Shared Task.
For the morphological realization, we used the mor-
phological realizer of Bohnet et al. (2010) that pre-
dicts the word form using shortest edit scripts. For
the language model (LM), we use a 5-gram model
with Kneser-Ney (Kneser and Ney, 1995) smoothing
derived from 11 million sentences of the Wikipedia.
In Table 6, we compare our two linearizers (with
and without lifting) to the two top systems of the
2011 Shared Task on Surface Realisation, (Bohnet et
al., 2011) and (Guo et al., 2011). Without the lifting,
our system reaches a score comparable to the top-
ranked system in the Shared Task. With the lifting,
we get a small7 but statistically significant improve-
ment in BLEU such that our system reaches a higher
score than the top ranked systems. This shows that
the improvements we obtain from the lifting carry
over to more complex generation tasks which in-
clude morphological realization.
</bodyText>
<subsectionHeader confidence="0.990921">
5.4 Human Evaluation
</subsectionHeader>
<bodyText confidence="0.865141333333333">
We have carried out a pilot human evaluation on the
German data in order to see whether human judges
prefer word orders obtained from the lifting-based
7Remember that English has the least percentage of non-
projective edges in our data sets, which are however important
to linearize correctly (see Figure 1).
</bodyText>
<equation confidence="0.408282">
I
</equation>
<page confidence="0.991766">
936
</page>
<bodyText confidence="0.999661625">
linearizer. In particular, we wanted to check whether
the lifting-based linearizer produces more natural
word orders for sentences that had a non-projective
tree in the corpus, and maybe less natural word or-
ders on originally projective sentences. Therefore,
we divided the evaluated items into originally pro-
jective and non-projective sentences.
We asked four annotators to judge 60 sentence
pairs comparing the lifting-based against the non-
lifted linearizer using the toolkit by Kow and Belz
(2012). All annotators are students, two of them
have a background in linguistics. The items were
randomly sampled from the subset of the develop-
ment set containing those sentences where the lin-
earizers produced different surface realizations. The
items are subdivided into 30 originally projective
and 30 originally non-projective sentences.
For each item, we presented the original context
sentence from the corpus and the pair of automat-
ically produced linearizations for the current sen-
tence. The annotators had to decide on two crite-
ria: (i) which sentence do they prefer? (ii) how flu-
ent is that sentence? In both cases, we used con-
tinuous sliders as rating tools, since humans seem
to prefer them (Belz and Kow, 2011). For the first
criterion, the slider positions were mapped to values
from -50 (preference for left sentence) to 50 (pref-
erence for right sentence). If the slider position is
zero, both sentences are equally preferred. For the
second criterion, the slider positions were mapped
to values from 0 (absolutely broken sentence) to 100
(perfectly fluent sentence).
</bodyText>
<table confidence="0.9548896">
Sentences Scores Equal Lifted Non-lifted
% selected 44.58% 35.0% 20.42%
All Fluency 56.14 75.77 72.78
Preference 0 34.75 31.06
% selected 29.63% 58.33% 12.04%
Fluency 43.06 76.27 68.85
Preference 0 37.52 24.46
% selected 56.82% 15.91% 27.27%
Proj. Fluency 61.72 74.29 74.19
Preference 0 26.43 33.44
</table>
<tableCaption confidence="0.999662">
Table 7: Results from human evaluation.
</tableCaption>
<bodyText confidence="0.993275">
Table 7 presents the results averaged over all sen-
tences, as well as for the subsets of non-projective
and projective sentences. We report the percentage
of items where the judges selected both, the lifted, or
non-lifted sentence, alongside with the average flu-
ency score (0-100) and preference strength (0-50).
On the entire set of items, the judges selected both
sentences in almost half of the cases. However, on
the subset of non-projective sentences, the lifted ver-
sion is clearly preferred and has a higher average
fluency and preference strength. The percentage of
zero preference items is much higher on the sub-
set of projective sentences. Moreover, the average
fluency of the zero preference items is remarkably
higher on the projective sentences than on the non-
projective subset. We conclude that humans have
a strong preference for lifting-based linearizations
on non-projective sentences. We attribute the low
fluency score on the non-projective zero preference
items to cases where the linearizer did not get a cor-
rect lifting or could not linearize the lifting correctly
such that the lifted and the non-lifted version were
not appropriate. On the other hand, incorrect lift-
ings on projective sentences do not necessarily seem
to result in deprecated linearizations, which leads to
the high percentage of zero preferences with a good
average fluency on this subset.
</bodyText>
<sectionHeader confidence="0.999431" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9999815">
We have presented a novel technique to linearize
sentences for a range of languages that exhibit non-
projective word order. Our approach deals with non-
projectivity by lifting edges in an unordered input
tree which can then be linearized by a standard pro-
jective linearization algorithm.
We obtain significant improvements for the
lifting-based linearization on English, German,
Dutch, Danish, Czech and Hungarian, and show that
lifting has the largest impact on the V2-languages.
In a human evaluation carried out on German we
also show that human judges clearly prefer lifting-
based linearizations on originally non-projective
sentences, and, on the other hand, that incorrect lift-
ings do not necessarily result in bad realizations of
the sentence.
</bodyText>
<sectionHeader confidence="0.998812" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.922527428571429">
This work was funded by the Deutsche Forschungs-
gemeinschaft (DFG) via the SFB 732 ”Incremental
Specification in Context”. We would also like to
thank Anna H¨atty and our four annotators for their
contribution to the human evaluation.
Non-
Proj.
</bodyText>
<page confidence="0.993907">
937
</page>
<sectionHeader confidence="0.996072" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999364733333333">
A. Belz and E. Kow. 2011. Discrete vs. Continuous Rat-
ing Scales for Language Evaluation in NLP. In Pro-
ceedings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Language
Technologies, pages 230–235, Portland, Oregon, USA,
June. Association for Computational Linguistics.
A. Belz, M. White, D. Espinosa, D. Hogan, E. Kow, and
A. Stent. 2011. The First Surface Realisation Shared
Task: Overview and Evaluation Results. In ENLG’11.
A. B¨ohmov´a, J. Hajiˇc, E. Hajiˇcov´a, and B. Hladk´a. 2000.
The Prague Dependency Treebank: A Three-level an-
notation scenario. In A. Abeill´e, editor, Treebanks:
Building and using syntactically annotated corpora.,
chapter 1, pages 103–127. Kluwer Academic Publish-
ers, Amsterdam.
B. Bohnet, L. Wanner, S. Mille, and A. Burga. 2010.
Broad coverage multilingual deep sentence generation
with a stochastic multi-level realizer. In Coling 2010,
pages 98–106.
B. Bohnet, S. Mille, B. Favre, and L. Wanner. 2011.
&lt;stumaba&gt;: From deep representation to surface. In
Proceedings of the Generation Challenges Session at
the 13th European Workshop on NLG, pages 232–235,
Nancy, France.
B. Bohnet. 2004. A Graph Grammar Approach to Map
Between Dependency Trees and Topological Models.
In IJCNLP, pages 636–645.
N. Br¨oker. 1998. Separating Surface Order and Syntactic
Relations in a Dependency Grammar. In COLING-
ACL 98.
S. Buchholz and E. Marsi. 2006. CoNLL-X shared
task on multilingual dependency parsing. In Proceed-
ings of the Tenth Conference on Computational Natu-
ral Language Learning, pages 149–164, Morristown,
NJ, USA. Association for Computational Linguistics.
A. Cahill, M. Forst, and C. Rohrer. 2007. Stochastic real-
isation ranking for a free word order language. ENLG
’07, pages 17–24.
K. Crammer, O. Dekel, S. Shalev-Shwartz, and Y. Singer.
2006. Online Passive-Aggressive Algorithms. Jour-
nal of Machine Learning Research, 7:551–585.
D. Duchier and R. Debusmann. 2001. Topological de-
pendency trees: A constraint-based account of linear
precedence. In Proceedings of the ACL.
R. Fan, K. Chang, C. Hsieh, X. Wang, and C. Lin. 2008.
LIBLINEAR: A library for large linear classification.
Journal of Machine Learning Research, 9:1871–1874.
K. Filippova and M. Strube. 2007. Generating con-
stituent order in german clauses. In ACL, pages 320–
327.
K. Filippova and M. Strube. 2009. Tree linearization in
English: improving language model based approaches.
In NAACL, pages 225–228, Morristown, NJ, USA. As-
sociation for Computational Linguistics.
M. Gamon, E. Ringger, R. Moore, S. Corston-Olivier,
and Z. Zhang. 2002. Extraposition: A case study in
German sentence realization. In Proceedings of Col-
ing 2002. Association for Computational Linguistics.
K. Gerdes and S. Kahane. 2001. Word order in german:
A formal dependency grammar using a topological hi-
erarchy. In Proceedings of the ACL.
Y. Guo, D. Hogan, and J. van Genabith. 2011. Dcu at
generation challenges 2011 surface realisation track.
In Proceedings of the Generation Challenges Session
at the 13th European Workshop on NLG, pages 227–
229.
J. Hajiˇc, M. Ciaramita, R. Johansson, D. Kawahara, M.-
A. Mart´ı, L. M`arquez, A. Meyers, J. Nivre, S. Pad´o,
J. Step´anek, P. Stran´ak, M. Surdeanu, N. Xue, and
Y. Zhang. 2009. The CoNLL-2009 shared task:
Syntactic and Semantic dependencies in multiple lan-
guages. In Proceedings of the 13th CoNLL Shared
Task, pages 1–18, Boulder, Colorado.
E. Hajiˇcov´a, J. Havelka, P. Sgall, K. Vesel´a, and D. Ze-
man. 2004. Issues of projectivity in the prague de-
pendency treebank. Prague Bulletin of Mathematical
Linguistics, 81.
W. He, H. Wang, Y. Guo, and T. Liu. 2009. Dependency
Based Chinese Sentence Realization. In Proceedings
of the ACL and of the IJCNLP, pages 809–816.
S. Kahane, A. Nasr, and O. Rambow. 1998. Pseudo-
projectivity: A polynomially parsable non-projective
dependency grammar. In COLING-ACL, pages 646–
652.
A. Kathol and C. Pollard. 1995. Extraposition via com-
plex domain formation. In Meeting of the Association
for Computational Linguistics, pages 174–180.
R. Kneser and H. Ney. 1995. In In Proceedings of the
IEEE International Conference on Acoustics, Speech
and Signal Processing, pages 181–184.
E. Kow and A. Belz. 2012. LGRT-Eval: A Toolkit for
Creating Online Language Evaluation Experiments.
In Proceedings of the 8th International Conference on
Language Resources and Evaluation (LREC’12).
I. Langkilde and K. Knight. 1998. Generation
that exploits corpus-based statistical knowledge. In
COLING-ACL, pages 704–710.
J. Nivre and J. Nilsson. 2005. Pseudo-projective de-
pendency parsing. In Proceedings of the 43rd Annual
Meeting of the Association for Computational Linguis-
tics (ACL’05), pages 99–106, Ann Arbor, Michigan,
June. Association for Computational Linguistics.
O. Rambow and A. K. Joshi. 1994. A formal look at
dependency grammars and phrase-structure grammars,
with special consideration of word-order phenomena.
</reference>
<page confidence="0.98001">
938
</page>
<reference confidence="0.999813151515151">
In Leo Wanner, editor, Current Issues in Meaning-Text
Theory. Pinter, London, UK.
M. Reape. 1989. A logical treatment of semi-free word
order and bounded discontinuous constituency. In
Proceedings of the EACL, EACL ’89, pages 103–110.
E. Ringger, M. Gamon, R. C. Moore, D. Rojas, M. Smets,
and S. Corston-Oliver. 2004. Linguistically informed
statistical models of constituent structure for ordering
in sentence realization. In COLING ’04, pages 673–
679.
W. Seeker and J. Kuhn. 2012. Making Ellipses Explicit
in Dependency Conversion for a German Treebank. In
Proceedings of LREC 2012, Istanbul, Turkey. Euro-
pean Language Resources Association (ELRA).
A. Stent. 2011. Att-0: Submission to generation chal-
lenges 2011 surface realization shared task. In Pro-
ceedings of the Generation Challenges Session at the
13th European Workshop on Natural Language Gener-
ation, pages 230–231, Nancy, France, September. As-
sociation for Computational Linguistics.
V. Vincze, D. Szauter, A. Alm´asi, G. M´ora, Z. Alexin,
and J. Csirik. 2010. Hungarian Dependency Tree-
bank. In Proceedings of the Seventh conference
on International Language Resources and Evaluation
(LREC 2010), pages 1855–1862, Valletta, Malta.
S. Wan, M. Dras, R. Dale, and C. Paris. 2009. Improving
grammaticality in statistical sentence generation: In-
troducing a dependency spanning tree algorithm with
an argument satisfaction model. In EACL, pages 852–
860.
M. White and R. Rajkumar. 2009. Perceptron reranking
for CCG realization. In EMNLP’09, pages 410–419,
Singapore, August.
</reference>
<page confidence="0.998774">
939
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.291646">
<title confidence="0.999858">Generating Non-Projective Word Order in Statistical Linearization</title>
<author confidence="0.996912">Bernd Bohnet Anders Bj¨orkelund Jonas Kuhn Wolfgang Seeker Sina</author>
<affiliation confidence="0.996979">Institut f¨ur Maschinelle University of</affiliation>
<abstract confidence="0.995252333333333">We propose a technique to generate nonprojective word orders in an efficient statistical linearization system. Our approach preedges in an unordered syntactic tree by means of a classifier, and uses a projective algorithm for tree linearization. We obtain statistically significant improvements on six typologically different languages: En-</abstract>
<note confidence="0.4866835">glish, German, Dutch, Danish, Hungarian, and Czech.</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Belz</author>
<author>E Kow</author>
</authors>
<title>Discrete vs. Continuous Rating Scales for Language Evaluation in NLP.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>230--235</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="38023" citStr="Belz and Kow, 2011" startWordPosition="6213" endWordPosition="6216">y sampled from the subset of the development set containing those sentences where the linearizers produced different surface realizations. The items are subdivided into 30 originally projective and 30 originally non-projective sentences. For each item, we presented the original context sentence from the corpus and the pair of automatically produced linearizations for the current sentence. The annotators had to decide on two criteria: (i) which sentence do they prefer? (ii) how fluent is that sentence? In both cases, we used continuous sliders as rating tools, since humans seem to prefer them (Belz and Kow, 2011). For the first criterion, the slider positions were mapped to values from -50 (preference for left sentence) to 50 (preference for right sentence). If the slider position is zero, both sentences are equally preferred. For the second criterion, the slider positions were mapped to values from 0 (absolutely broken sentence) to 100 (perfectly fluent sentence). Sentences Scores Equal Lifted Non-lifted % selected 44.58% 35.0% 20.42% All Fluency 56.14 75.77 72.78 Preference 0 34.75 31.06 % selected 29.63% 58.33% 12.04% Fluency 43.06 76.27 68.85 Preference 0 37.52 24.46 % selected 56.82% 15.91% 27.27</context>
</contexts>
<marker>Belz, Kow, 2011</marker>
<rawString>A. Belz and E. Kow. 2011. Discrete vs. Continuous Rating Scales for Language Evaluation in NLP. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 230–235, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Belz</author>
<author>M White</author>
<author>D Espinosa</author>
<author>D Hogan</author>
<author>E Kow</author>
<author>A Stent</author>
</authors>
<title>The First Surface Realisation Shared Task: Overview and Evaluation Results.</title>
<date>2011</date>
<booktitle>In ENLG’11.</booktitle>
<contexts>
<context position="962" citStr="Belz et al., 2011" startWordPosition="125" endWordPosition="128"> efficient statistical linearization system. Our approach predicts liftings of edges in an unordered syntactic tree by means of a classifier, and uses a projective algorithm for tree linearization. We obtain statistically significant improvements on six typologically different languages: English, German, Dutch, Danish, Hungarian, and Czech. 1 Introduction There is a growing interest in language-independent data-driven approaches to natural language generation (NLG). An important subtask of NLG is surface realization, which was recently addressed in the 2011 Shared Task on Surface Realisation (Belz et al., 2011). Here, the input is a linguistic representation, such as a syntactic dependency tree lacking all precedence information, and the task is to determine a natural, coherent linearization of the words. The standard data-driven approach is to traverse the dependency tree deciding locally at each node on the relative order of the head and its children. The shared task results have proven this approach to be both effective and efficient when applied to English. It is what federal support should try to achieve Figure 1: A non-projective example from the CoNLL 2009 Shared Task data set for parsing (Ha</context>
<context position="6245" citStr="Belz et al., 2011" startWordPosition="953" endWordPosition="956"> bags of words (constituents) that are not allowed to be discontinuous. A straightforward method to obtain the word order domains from dependency trees and to order the words in the tree is to use each word and its children as domain and then to order the domains and contained words recursively. As outlined in the introduction, the direct mapping of syntactic trees to domains does not provide the possibility to obtain all possible correct word orders. Linearization systems can be roughly distinguished as either rule-based or statistical systems. In the 2011 Shared Task on Surface Realisation (Belz et al., 2011), the top performing systems were all statistical dependency realizers (Bohnet et al., 2011; Guo et al., 2011; Stent, 2011). Grammar-based approaches map dependency structures or phrase structures to a tree that represents the linear precedence. These approaches are mostly able to generate non-projective word orders. Early work was nearly exclusively applied to phrase structure grammars (e.g. (Kathol and Pollard, 1995; Rambow and Joshi, 1994; Langkilde and Knight, 1998)). Concerning dependency-based frameworks, Br¨oker (1998) used the concept of word order domains to separate surface realizati</context>
</contexts>
<marker>Belz, White, Espinosa, Hogan, Kow, Stent, 2011</marker>
<rawString>A. Belz, M. White, D. Espinosa, D. Hogan, E. Kow, and A. Stent. 2011. The First Surface Realisation Shared Task: Overview and Evaluation Results. In ENLG’11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A B¨ohmov´a</author>
<author>J Hajiˇc</author>
<author>E Hajiˇcov´a</author>
<author>B Hladk´a</author>
</authors>
<title>The Prague Dependency Treebank: A Three-level annotation scenario.</title>
<date>2000</date>
<pages>103--127</pages>
<editor>In A. Abeill´e, editor,</editor>
<publisher>Kluwer Academic Publishers,</publisher>
<location>Amsterdam.</location>
<marker>B¨ohmov´a, Hajiˇc, Hajiˇcov´a, Hladk´a, 2000</marker>
<rawString>A. B¨ohmov´a, J. Hajiˇc, E. Hajiˇcov´a, and B. Hladk´a. 2000. The Prague Dependency Treebank: A Three-level annotation scenario. In A. Abeill´e, editor, Treebanks: Building and using syntactically annotated corpora., chapter 1, pages 103–127. Kluwer Academic Publishers, Amsterdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Bohnet</author>
<author>L Wanner</author>
<author>S Mille</author>
<author>A Burga</author>
</authors>
<title>Broad coverage multilingual deep sentence generation with a stochastic multi-level realizer. In Coling</title>
<date>2010</date>
<pages>98--106</pages>
<contexts>
<context position="7681" citStr="Bohnet et al., 2010" startWordPosition="1167" endWordPosition="1170">ical model for generating German word order. Bohnet (2004) employs graph grammars to map between dependency trees and linear precedence trees represented as hierarchical graphs. In the frameworks of HPSG, LFG, and CCG, a grammarbased generator produces word order candidates that might be non-projective, and a ranker is used to select the best surface realization (Cahill et al., 2007; White and Rajkumar, 2009). Statistical methods for linearization have recently become more popular (Langkilde and Knight, 1998; Ringger et al., 2004; Filippova and Strube, 2009; Wan et al., 2009; He et al., 2009; Bohnet et al., 2010; Guo et al., 2011). They typically work by traversing the syntactic structure either bottom-up (Filip929 pova and Strube, 2007; Bohnet et al., 2010) or topdown (Guo et al., 2011; Bohnet et al., 2011). These linearizers are mostly applied to English and do not deal with non-projective word orders. An exception is Filippova and Strube (2007), who contribute a study on the treatment of preverbal and postverbal constituents for German focusing on constituent order at the sentence level. The work most similar to ours is that of Gamon et al. (2002). They use machine-learning techniques to lift edge</context>
<context position="35668" citStr="Bohnet et al. (2010)" startWordPosition="5829" endWordPosition="5832">1) (ranked 2nd) 0.862 13.68 Baseline-Non-Lifted + LM 0.896 13.94 Beam-Lifted + LM 0.901 13.96 Table 6: Results on the development set of the 2011 Shared Task on Surface Realisation data, (the test set was not officially released). We also evaluated our linearizer on the data of 2011 Shared Task on Surface Realisation, which is based on the English CoNLL 2009 data (like our previous evaluations) but excludes information on morphological realization. For training and evaluation, we used the exact set up of the Shared Task. For the morphological realization, we used the morphological realizer of Bohnet et al. (2010) that predicts the word form using shortest edit scripts. For the language model (LM), we use a 5-gram model with Kneser-Ney (Kneser and Ney, 1995) smoothing derived from 11 million sentences of the Wikipedia. In Table 6, we compare our two linearizers (with and without lifting) to the two top systems of the 2011 Shared Task on Surface Realisation, (Bohnet et al., 2011) and (Guo et al., 2011). Without the lifting, our system reaches a score comparable to the topranked system in the Shared Task. With the lifting, we get a small7 but statistically significant improvement in BLEU such that our sy</context>
</contexts>
<marker>Bohnet, Wanner, Mille, Burga, 2010</marker>
<rawString>B. Bohnet, L. Wanner, S. Mille, and A. Burga. 2010. Broad coverage multilingual deep sentence generation with a stochastic multi-level realizer. In Coling 2010, pages 98–106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Bohnet</author>
<author>S Mille</author>
<author>B Favre</author>
<author>L Wanner</author>
</authors>
<title>From deep representation to surface.</title>
<date>2011</date>
<booktitle>In Proceedings of the Generation Challenges Session at the 13th European Workshop on NLG,</booktitle>
<pages>232--235</pages>
<location>Nancy, France.</location>
<contexts>
<context position="6336" citStr="Bohnet et al., 2011" startWordPosition="966" endWordPosition="969">method to obtain the word order domains from dependency trees and to order the words in the tree is to use each word and its children as domain and then to order the domains and contained words recursively. As outlined in the introduction, the direct mapping of syntactic trees to domains does not provide the possibility to obtain all possible correct word orders. Linearization systems can be roughly distinguished as either rule-based or statistical systems. In the 2011 Shared Task on Surface Realisation (Belz et al., 2011), the top performing systems were all statistical dependency realizers (Bohnet et al., 2011; Guo et al., 2011; Stent, 2011). Grammar-based approaches map dependency structures or phrase structures to a tree that represents the linear precedence. These approaches are mostly able to generate non-projective word orders. Early work was nearly exclusively applied to phrase structure grammars (e.g. (Kathol and Pollard, 1995; Rambow and Joshi, 1994; Langkilde and Knight, 1998)). Concerning dependency-based frameworks, Br¨oker (1998) used the concept of word order domains to separate surface realization from linear precedence trees. Similarly, Duchier and Debusmann (2001) differentiate Imme</context>
<context position="7881" citStr="Bohnet et al., 2011" startWordPosition="1203" endWordPosition="1206">HPSG, LFG, and CCG, a grammarbased generator produces word order candidates that might be non-projective, and a ranker is used to select the best surface realization (Cahill et al., 2007; White and Rajkumar, 2009). Statistical methods for linearization have recently become more popular (Langkilde and Knight, 1998; Ringger et al., 2004; Filippova and Strube, 2009; Wan et al., 2009; He et al., 2009; Bohnet et al., 2010; Guo et al., 2011). They typically work by traversing the syntactic structure either bottom-up (Filip929 pova and Strube, 2007; Bohnet et al., 2010) or topdown (Guo et al., 2011; Bohnet et al., 2011). These linearizers are mostly applied to English and do not deal with non-projective word orders. An exception is Filippova and Strube (2007), who contribute a study on the treatment of preverbal and postverbal constituents for German focusing on constituent order at the sentence level. The work most similar to ours is that of Gamon et al. (2002). They use machine-learning techniques to lift edges in a preprocessing step to a surface realizer. Their objective is the same as ours: by lifting, they avoid crossing edges. However, contrary to our work, they use phrase-structure syntax and focus o</context>
<context position="35006" citStr="Bohnet et al., 2011" startWordPosition="5720" endWordPosition="5723">re plotted in Figure 4, where the lower and upper bars correspond to the lower and upper bound from the nonlifted and the gold-lifted baseline. It clearly emerges from this figure that the range of improvements obtainable from lifting is closely tied to the general linearization quality, and also to word order properties of the languages. Thus, the range of sentences affected by the lifting is clearly largest for the V2- languages. The accuracies are high, but the ranges are small for English, whereas the accuracies are low and the ranges quite small for Czech and Hungarian. System BLEU NIST (Bohnet et al., 2011) (ranked 1st) 0.896 13.93 (Guo et al., 2011) (ranked 2nd) 0.862 13.68 Baseline-Non-Lifted + LM 0.896 13.94 Beam-Lifted + LM 0.901 13.96 Table 6: Results on the development set of the 2011 Shared Task on Surface Realisation data, (the test set was not officially released). We also evaluated our linearizer on the data of 2011 Shared Task on Surface Realisation, which is based on the English CoNLL 2009 data (like our previous evaluations) but excludes information on morphological realization. For training and evaluation, we used the exact set up of the Shared Task. For the morphological realizati</context>
</contexts>
<marker>Bohnet, Mille, Favre, Wanner, 2011</marker>
<rawString>B. Bohnet, S. Mille, B. Favre, and L. Wanner. 2011. &lt;stumaba&gt;: From deep representation to surface. In Proceedings of the Generation Challenges Session at the 13th European Workshop on NLG, pages 232–235, Nancy, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Bohnet</author>
</authors>
<title>A Graph Grammar Approach to Map Between Dependency Trees and Topological Models. In</title>
<date>2004</date>
<booktitle>IJCNLP,</booktitle>
<pages>636--645</pages>
<contexts>
<context position="7120" citStr="Bohnet (2004)" startWordPosition="1079" endWordPosition="1080"> are mostly able to generate non-projective word orders. Early work was nearly exclusively applied to phrase structure grammars (e.g. (Kathol and Pollard, 1995; Rambow and Joshi, 1994; Langkilde and Knight, 1998)). Concerning dependency-based frameworks, Br¨oker (1998) used the concept of word order domains to separate surface realization from linear precedence trees. Similarly, Duchier and Debusmann (2001) differentiate Immediate Dominance trees (ID-trees) from Linear Precedence trees (LPtrees). Gerdes and Kahane (2001) apply a hierarchical topological model for generating German word order. Bohnet (2004) employs graph grammars to map between dependency trees and linear precedence trees represented as hierarchical graphs. In the frameworks of HPSG, LFG, and CCG, a grammarbased generator produces word order candidates that might be non-projective, and a ranker is used to select the best surface realization (Cahill et al., 2007; White and Rajkumar, 2009). Statistical methods for linearization have recently become more popular (Langkilde and Knight, 1998; Ringger et al., 2004; Filippova and Strube, 2009; Wan et al., 2009; He et al., 2009; Bohnet et al., 2010; Guo et al., 2011). They typically wor</context>
</contexts>
<marker>Bohnet, 2004</marker>
<rawString>B. Bohnet. 2004. A Graph Grammar Approach to Map Between Dependency Trees and Topological Models. In IJCNLP, pages 636–645.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Br¨oker</author>
</authors>
<title>Separating Surface Order and Syntactic Relations in a Dependency Grammar. In</title>
<date>1998</date>
<booktitle>COLINGACL 98.</booktitle>
<marker>Br¨oker, 1998</marker>
<rawString>N. Br¨oker. 1998. Separating Surface Order and Syntactic Relations in a Dependency Grammar. In COLINGACL 98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Buchholz</author>
<author>E Marsi</author>
</authors>
<title>CoNLL-X shared task on multilingual dependency parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of the Tenth Conference on Computational Natural Language Learning,</booktitle>
<pages>149--164</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="26272" citStr="Buchholz and Marsi, 2006" startWordPosition="4276" endWordPosition="4279">grams, and global sentence-level features. 5 Experiments We conduct experiments on six European languages with varying degrees of word order restrictions: While English word order is very restrictive, Czech and Hungarian exhibit few word order constraints. Danish, Dutch, and German (so-called V2, i. e. verb-second, languages) show a relatively free word order that is however more restrictive than in Hungarian or Czech. The English and the Czech data are from the CoNLL 2009 Shared Task data sets (Hajiˇc et al., 2009). The Danish and the Dutch data are from the CoNLL 2006 Shared Task data sets (Buchholz and Marsi, 2006). For Hungarian, we use the Hungarian Dependency Treebank (Vincze et al., 2010), and for German, we use a dependency conversion by Seeker and Kuhn (2012). # sent’s np sent’s np edges np &lt; 3 lifts English 39,279 7.63 % 0.39% 98.39% German 36,000 28.71% 2.34% 94.98% Dutch 13,349 36.44% 5.42% 99.80% Danish 5,190 15.62 % 1.00% 96.72% Hungarian 61,034 15.81% 1.45% 99.82% Czech 38,727 22.42% 1.86% 99.84% Table 3: Size of training sets, percentage of nonprojective (np) sentences and edges, percentage of np edges covered by 3 lifting steps. Table 3 shows the sizes of the training corpora and the perce</context>
</contexts>
<marker>Buchholz, Marsi, 2006</marker>
<rawString>S. Buchholz and E. Marsi. 2006. CoNLL-X shared task on multilingual dependency parsing. In Proceedings of the Tenth Conference on Computational Natural Language Learning, pages 149–164, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Cahill</author>
<author>M Forst</author>
<author>C Rohrer</author>
</authors>
<title>Stochastic realisation ranking for a free word order language.</title>
<date>2007</date>
<journal>ENLG</journal>
<volume>07</volume>
<pages>17--24</pages>
<contexts>
<context position="7447" citStr="Cahill et al., 2007" startWordPosition="1130" endWordPosition="1133">surface realization from linear precedence trees. Similarly, Duchier and Debusmann (2001) differentiate Immediate Dominance trees (ID-trees) from Linear Precedence trees (LPtrees). Gerdes and Kahane (2001) apply a hierarchical topological model for generating German word order. Bohnet (2004) employs graph grammars to map between dependency trees and linear precedence trees represented as hierarchical graphs. In the frameworks of HPSG, LFG, and CCG, a grammarbased generator produces word order candidates that might be non-projective, and a ranker is used to select the best surface realization (Cahill et al., 2007; White and Rajkumar, 2009). Statistical methods for linearization have recently become more popular (Langkilde and Knight, 1998; Ringger et al., 2004; Filippova and Strube, 2009; Wan et al., 2009; He et al., 2009; Bohnet et al., 2010; Guo et al., 2011). They typically work by traversing the syntactic structure either bottom-up (Filip929 pova and Strube, 2007; Bohnet et al., 2010) or topdown (Guo et al., 2011; Bohnet et al., 2011). These linearizers are mostly applied to English and do not deal with non-projective word orders. An exception is Filippova and Strube (2007), who contribute a study</context>
</contexts>
<marker>Cahill, Forst, Rohrer, 2007</marker>
<rawString>A. Cahill, M. Forst, and C. Rohrer. 2007. Stochastic realisation ranking for a free word order language. ENLG ’07, pages 17–24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Crammer</author>
<author>O Dekel</author>
<author>S Shalev-Shwartz</author>
<author>Y Singer</author>
</authors>
<title>Online Passive-Aggressive Algorithms.</title>
<date>2006</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>7--551</pages>
<contexts>
<context position="22640" citStr="Crammer et al., 2006" startWordPosition="3776" endWordPosition="3779">, i. e. if the last token is a question mark then the sentence should probably be a question (cf. feature set shown in Table 2). Furthermore, the beam enables us to generate alternative linearizations. For this, the algorithm iterates over the alternative word orders of the domains in order to assemble different word orders on the sentence level.5 Finally, when traversing the tree bottom-up, the algorithm has to use the different orders of the already ordered subtrees as context, which also requires a search over alternative word orders of the domains. Training of the Linearizer. We use MIRA (Crammer et al., 2006) for the training of the linearizer. The classifier provides a score that we use to rank the alternative word orders. Algorithm 3 calls two functions to compute the score: compute-score (line 15) for features based on pairs of words and trigrams and compute-global-score for features based on word patterns of a domain. Table 2 shows the feature set for the two functions. In the case that the linearization of a word order domain is incorrect the algorithm updates its weight vector w. The following equation shows the update function of the weight vector: w = w + τh(φ(dh, T, xg) − φ(dh, T, xp)) We</context>
</contexts>
<marker>Crammer, Dekel, Shalev-Shwartz, Singer, 2006</marker>
<rawString>K. Crammer, O. Dekel, S. Shalev-Shwartz, and Y. Singer. 2006. Online Passive-Aggressive Algorithms. Journal of Machine Learning Research, 7:551–585.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Duchier</author>
<author>R Debusmann</author>
</authors>
<title>Topological dependency trees: A constraint-based account of linear precedence.</title>
<date>2001</date>
<booktitle>In Proceedings of the ACL.</booktitle>
<contexts>
<context position="6917" citStr="Duchier and Debusmann (2001)" startWordPosition="1048" endWordPosition="1052">tical dependency realizers (Bohnet et al., 2011; Guo et al., 2011; Stent, 2011). Grammar-based approaches map dependency structures or phrase structures to a tree that represents the linear precedence. These approaches are mostly able to generate non-projective word orders. Early work was nearly exclusively applied to phrase structure grammars (e.g. (Kathol and Pollard, 1995; Rambow and Joshi, 1994; Langkilde and Knight, 1998)). Concerning dependency-based frameworks, Br¨oker (1998) used the concept of word order domains to separate surface realization from linear precedence trees. Similarly, Duchier and Debusmann (2001) differentiate Immediate Dominance trees (ID-trees) from Linear Precedence trees (LPtrees). Gerdes and Kahane (2001) apply a hierarchical topological model for generating German word order. Bohnet (2004) employs graph grammars to map between dependency trees and linear precedence trees represented as hierarchical graphs. In the frameworks of HPSG, LFG, and CCG, a grammarbased generator produces word order candidates that might be non-projective, and a ranker is used to select the best surface realization (Cahill et al., 2007; White and Rajkumar, 2009). Statistical methods for linearization hav</context>
</contexts>
<marker>Duchier, Debusmann, 2001</marker>
<rawString>D. Duchier and R. Debusmann. 2001. Topological dependency trees: A constraint-based account of linear precedence. In Proceedings of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Fan</author>
<author>K Chang</author>
<author>C Hsieh</author>
<author>X Wang</author>
<author>C Lin</author>
</authors>
<title>LIBLINEAR: A library for large linear classification.</title>
<date>2008</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>9--1871</pages>
<contexts>
<context position="12001" citStr="Fan et al., 2008" startWordPosition="1939" endWordPosition="1942">same depth, ties are broken by linear order, i.e. for multiple nodes at the same depth, the leftmost is visited first. When a node is visited, we create a training instance out of it. Its class is determined by the number of steps it would be lifted by the projectivization algorithm given the linear order (in most cases the class corresponds to no lifting, since most edges are projective). As we traverse the nodes, we also execute the liftings (if any) and update the tree on the fly. The training instances derived are used to train a logistic regression classifier using the LIBLINEAR package (Fan et al., 2008). The features used for the lifting classifier are described in Table 1. Since we use linear classifiers, our feature set also contains conjunctions of atomic features. The features 930 Atomic features `dx E {w, wp, wgp, wch, w3, wun} morph(x), label(x), lemma(x), PoS(x) `dx E {wgc, wne, wco} label(x), lemma(x), PoS(x) Complex features `dx E {w, wp, wgp} lemma(x)+PoS(x), label(x)+PoS(x), label(x)+lemma(x) `dx E {wch, w3, wun}, y = w lemma(x)+lemma(y), PoS(y)+lemma(x), PoS(y)+lemma(x) `dx E {w, wp, wgp}, y = HEAD(x) lemma(x)+lemma(y), lemma(x)+PoS(y), PoS(x)+lemma(y) `dx E {w, wp, wgp}, y = HEA</context>
</contexts>
<marker>Fan, Chang, Hsieh, Wang, Lin, 2008</marker>
<rawString>R. Fan, K. Chang, C. Hsieh, X. Wang, and C. Lin. 2008. LIBLINEAR: A library for large linear classification. Journal of Machine Learning Research, 9:1871–1874.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Filippova</author>
<author>M Strube</author>
</authors>
<title>Generating constituent order in german clauses.</title>
<date>2007</date>
<booktitle>In ACL,</booktitle>
<pages>320--327</pages>
<contexts>
<context position="8023" citStr="Filippova and Strube (2007)" startWordPosition="1226" endWordPosition="1229">t the best surface realization (Cahill et al., 2007; White and Rajkumar, 2009). Statistical methods for linearization have recently become more popular (Langkilde and Knight, 1998; Ringger et al., 2004; Filippova and Strube, 2009; Wan et al., 2009; He et al., 2009; Bohnet et al., 2010; Guo et al., 2011). They typically work by traversing the syntactic structure either bottom-up (Filip929 pova and Strube, 2007; Bohnet et al., 2010) or topdown (Guo et al., 2011; Bohnet et al., 2011). These linearizers are mostly applied to English and do not deal with non-projective word orders. An exception is Filippova and Strube (2007), who contribute a study on the treatment of preverbal and postverbal constituents for German focusing on constituent order at the sentence level. The work most similar to ours is that of Gamon et al. (2002). They use machine-learning techniques to lift edges in a preprocessing step to a surface realizer. Their objective is the same as ours: by lifting, they avoid crossing edges. However, contrary to our work, they use phrase-structure syntax and focus on a limited number of cases of crossing branches in German only. 3 Lifting Dependency Edges In this section, we describe the first of the two </context>
</contexts>
<marker>Filippova, Strube, 2007</marker>
<rawString>K. Filippova and M. Strube. 2007. Generating constituent order in german clauses. In ACL, pages 320– 327.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Filippova</author>
<author>M Strube</author>
</authors>
<title>Tree linearization in English: improving language model based approaches.</title>
<date>2009</date>
<booktitle>In NAACL,</booktitle>
<pages>225--228</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="7625" citStr="Filippova and Strube, 2009" startWordPosition="1155" endWordPosition="1158">LPtrees). Gerdes and Kahane (2001) apply a hierarchical topological model for generating German word order. Bohnet (2004) employs graph grammars to map between dependency trees and linear precedence trees represented as hierarchical graphs. In the frameworks of HPSG, LFG, and CCG, a grammarbased generator produces word order candidates that might be non-projective, and a ranker is used to select the best surface realization (Cahill et al., 2007; White and Rajkumar, 2009). Statistical methods for linearization have recently become more popular (Langkilde and Knight, 1998; Ringger et al., 2004; Filippova and Strube, 2009; Wan et al., 2009; He et al., 2009; Bohnet et al., 2010; Guo et al., 2011). They typically work by traversing the syntactic structure either bottom-up (Filip929 pova and Strube, 2007; Bohnet et al., 2010) or topdown (Guo et al., 2011; Bohnet et al., 2011). These linearizers are mostly applied to English and do not deal with non-projective word orders. An exception is Filippova and Strube (2007), who contribute a study on the treatment of preverbal and postverbal constituents for German focusing on constituent order at the sentence level. The work most similar to ours is that of Gamon et al. (</context>
</contexts>
<marker>Filippova, Strube, 2009</marker>
<rawString>K. Filippova and M. Strube. 2009. Tree linearization in English: improving language model based approaches. In NAACL, pages 225–228, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Gamon</author>
<author>E Ringger</author>
<author>R Moore</author>
<author>S Corston-Olivier</author>
<author>Z Zhang</author>
</authors>
<title>Extraposition: A case study in German sentence realization.</title>
<date>2002</date>
<booktitle>In Proceedings of Coling</booktitle>
<contexts>
<context position="8230" citStr="Gamon et al. (2002)" startWordPosition="1262" endWordPosition="1265">d Strube, 2009; Wan et al., 2009; He et al., 2009; Bohnet et al., 2010; Guo et al., 2011). They typically work by traversing the syntactic structure either bottom-up (Filip929 pova and Strube, 2007; Bohnet et al., 2010) or topdown (Guo et al., 2011; Bohnet et al., 2011). These linearizers are mostly applied to English and do not deal with non-projective word orders. An exception is Filippova and Strube (2007), who contribute a study on the treatment of preverbal and postverbal constituents for German focusing on constituent order at the sentence level. The work most similar to ours is that of Gamon et al. (2002). They use machine-learning techniques to lift edges in a preprocessing step to a surface realizer. Their objective is the same as ours: by lifting, they avoid crossing edges. However, contrary to our work, they use phrase-structure syntax and focus on a limited number of cases of crossing branches in German only. 3 Lifting Dependency Edges In this section, we describe the first of the two stages in our approach, namely the classifier that lifts edges in dependency trees. The classifier we aim to train is meant to predict liftings on a given unordered dependency tree, yielding a tree that, wit</context>
<context position="13937" citStr="Gamon et al. (2002)" startWordPosition="2243" endWordPosition="2246">fer to: SUBTREESIZE – the absolute number of nodes below x, RELSUBTREESIZE – the relative size of the subtree rooted at x with respect to the whole tree. involve the lemma, dependency edge label, part-ofspeech tag, and morphological features of the node in question, and of several neighboring nodes in the dependency tree. We also have a few non-binary features that encode the size of the subtree headed by the node and its ancestors. We ran preliminary experiments to determine the optimal architecture. First, other ways of modeling the liftings are conceivable. To find new reattachment points, Gamon et al. (2002) propose two other ways, both using a binary classifier: applying the classifier to each node x along the path to the root asking “Should d be reattached to x?”; or lifting one step at a time and applying the classifier iteratively until it says stop. They found that the latter outperformed the former. We tried this method, but found that it was inferior to the multi-class model and more frequently over- or underlifted. Second, to avoid data sparseness for infrequent lifting distances, we introduce a maximum number of liftings. We found that a maximum of 3 gave the best performance. In the pse</context>
</contexts>
<marker>Gamon, Ringger, Moore, Corston-Olivier, Zhang, 2002</marker>
<rawString>M. Gamon, E. Ringger, R. Moore, S. Corston-Olivier, and Z. Zhang. 2002. Extraposition: A case study in German sentence realization. In Proceedings of Coling 2002. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Gerdes</author>
<author>S Kahane</author>
</authors>
<title>Word order in german: A formal dependency grammar using a topological hierarchy.</title>
<date>2001</date>
<booktitle>In Proceedings of the ACL.</booktitle>
<contexts>
<context position="7033" citStr="Gerdes and Kahane (2001)" startWordPosition="1064" endWordPosition="1067"> structures or phrase structures to a tree that represents the linear precedence. These approaches are mostly able to generate non-projective word orders. Early work was nearly exclusively applied to phrase structure grammars (e.g. (Kathol and Pollard, 1995; Rambow and Joshi, 1994; Langkilde and Knight, 1998)). Concerning dependency-based frameworks, Br¨oker (1998) used the concept of word order domains to separate surface realization from linear precedence trees. Similarly, Duchier and Debusmann (2001) differentiate Immediate Dominance trees (ID-trees) from Linear Precedence trees (LPtrees). Gerdes and Kahane (2001) apply a hierarchical topological model for generating German word order. Bohnet (2004) employs graph grammars to map between dependency trees and linear precedence trees represented as hierarchical graphs. In the frameworks of HPSG, LFG, and CCG, a grammarbased generator produces word order candidates that might be non-projective, and a ranker is used to select the best surface realization (Cahill et al., 2007; White and Rajkumar, 2009). Statistical methods for linearization have recently become more popular (Langkilde and Knight, 1998; Ringger et al., 2004; Filippova and Strube, 2009; Wan et</context>
</contexts>
<marker>Gerdes, Kahane, 2001</marker>
<rawString>K. Gerdes and S. Kahane. 2001. Word order in german: A formal dependency grammar using a topological hierarchy. In Proceedings of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Guo</author>
<author>D Hogan</author>
<author>J van Genabith</author>
</authors>
<title>Dcu at generation challenges 2011 surface realisation track.</title>
<date>2011</date>
<booktitle>In Proceedings of the Generation Challenges Session at the 13th European Workshop on NLG,</booktitle>
<pages>227--229</pages>
<marker>Guo, Hogan, van Genabith, 2011</marker>
<rawString>Y. Guo, D. Hogan, and J. van Genabith. 2011. Dcu at generation challenges 2011 surface realisation track. In Proceedings of the Generation Challenges Session at the 13th European Workshop on NLG, pages 227– 229.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hajiˇc</author>
<author>M Ciaramita</author>
<author>R Johansson</author>
<author>D Kawahara</author>
<author>M-A Mart´ı</author>
<author>L M`arquez</author>
<author>A Meyers</author>
<author>J Nivre</author>
<author>S Pad´o</author>
<author>J Step´anek</author>
<author>P Stran´ak</author>
<author>M Surdeanu</author>
<author>N Xue</author>
<author>Y Zhang</author>
</authors>
<title>The CoNLL-2009 shared task: Syntactic and Semantic dependencies in multiple languages.</title>
<date>2009</date>
<booktitle>In Proceedings of the 13th CoNLL Shared Task,</booktitle>
<pages>1--18</pages>
<location>Boulder, Colorado.</location>
<marker>Hajiˇc, Ciaramita, Johansson, Kawahara, Mart´ı, M`arquez, Meyers, Nivre, Pad´o, Step´anek, Stran´ak, Surdeanu, Xue, Zhang, 2009</marker>
<rawString>J. Hajiˇc, M. Ciaramita, R. Johansson, D. Kawahara, M.-A. Mart´ı, L. M`arquez, A. Meyers, J. Nivre, S. Pad´o, J. Step´anek, P. Stran´ak, M. Surdeanu, N. Xue, and Y. Zhang. 2009. The CoNLL-2009 shared task: Syntactic and Semantic dependencies in multiple languages. In Proceedings of the 13th CoNLL Shared Task, pages 1–18, Boulder, Colorado.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Hajiˇcov´a</author>
<author>J Havelka</author>
<author>P Sgall</author>
<author>K Vesel´a</author>
<author>D Zeman</author>
</authors>
<title>Issues of projectivity in the prague dependency treebank.</title>
<date>2004</date>
<journal>Prague Bulletin of Mathematical Linguistics,</journal>
<volume>81</volume>
<marker>Hajiˇcov´a, Havelka, Sgall, Vesel´a, Zeman, 2004</marker>
<rawString>E. Hajiˇcov´a, J. Havelka, P. Sgall, K. Vesel´a, and D. Zeman. 2004. Issues of projectivity in the prague dependency treebank. Prague Bulletin of Mathematical Linguistics, 81.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W He</author>
<author>H Wang</author>
<author>Y Guo</author>
<author>T Liu</author>
</authors>
<title>Dependency Based Chinese Sentence Realization.</title>
<date>2009</date>
<booktitle>In Proceedings of the ACL and of the IJCNLP,</booktitle>
<pages>809--816</pages>
<contexts>
<context position="7660" citStr="He et al., 2009" startWordPosition="1163" endWordPosition="1166">rarchical topological model for generating German word order. Bohnet (2004) employs graph grammars to map between dependency trees and linear precedence trees represented as hierarchical graphs. In the frameworks of HPSG, LFG, and CCG, a grammarbased generator produces word order candidates that might be non-projective, and a ranker is used to select the best surface realization (Cahill et al., 2007; White and Rajkumar, 2009). Statistical methods for linearization have recently become more popular (Langkilde and Knight, 1998; Ringger et al., 2004; Filippova and Strube, 2009; Wan et al., 2009; He et al., 2009; Bohnet et al., 2010; Guo et al., 2011). They typically work by traversing the syntactic structure either bottom-up (Filip929 pova and Strube, 2007; Bohnet et al., 2010) or topdown (Guo et al., 2011; Bohnet et al., 2011). These linearizers are mostly applied to English and do not deal with non-projective word orders. An exception is Filippova and Strube (2007), who contribute a study on the treatment of preverbal and postverbal constituents for German focusing on constituent order at the sentence level. The work most similar to ours is that of Gamon et al. (2002). They use machine-learning te</context>
</contexts>
<marker>He, Wang, Guo, Liu, 2009</marker>
<rawString>W. He, H. Wang, Y. Guo, and T. Liu. 2009. Dependency Based Chinese Sentence Realization. In Proceedings of the ACL and of the IJCNLP, pages 809–816.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kahane</author>
<author>A Nasr</author>
<author>O Rambow</author>
</authors>
<title>Pseudoprojectivity: A polynomially parsable non-projective dependency grammar.</title>
<date>1998</date>
<booktitle>In COLING-ACL,</booktitle>
<pages>646--652</pages>
<contexts>
<context position="4149" citStr="Kahane et al., 1998" startWordPosition="624" endWordPosition="627"> on rule-based generation has addressed certain aspects of the problem. Das Mandat will er zur¨uckgeben . the.ACC mandate.ACC want.3SG he.NOM return.INF . ’He wants to return the mandate.’ Figure 2: German object fronting with complex verb introducing a non-projective edge. In this paper, we aim for a general data-driven approach that can deal with various causes for nonprojectivity and will work for typologically different languages. Our technique is inspired by work in data-driven multilingual parsing, where non-projectivity has received considerable attention. In pseudo-projective parsing (Kahane et al., 1998; Nivre and Nilsson, 2005), the parsing algorithm is restricted to projective structures, but the issue is side-stepped by converting non-projective structures to projective ones prior to training and application, and then restoring the original structure afterwards. Similarly, we split the linearization task in two stages: initially, the input tree is modified by lifting certain edges in such a way that new orderings become possible even under a projectivity constraint; the second stage is the original, projective linearization step. In parsing, projectivization is a deterministic process tha</context>
</contexts>
<marker>Kahane, Nasr, Rambow, 1998</marker>
<rawString>S. Kahane, A. Nasr, and O. Rambow. 1998. Pseudoprojectivity: A polynomially parsable non-projective dependency grammar. In COLING-ACL, pages 646– 652.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kathol</author>
<author>C Pollard</author>
</authors>
<title>Extraposition via complex domain formation.</title>
<date>1995</date>
<booktitle>In Meeting of the Association for Computational Linguistics,</booktitle>
<pages>174--180</pages>
<contexts>
<context position="6666" citStr="Kathol and Pollard, 1995" startWordPosition="1014" endWordPosition="1017">obtain all possible correct word orders. Linearization systems can be roughly distinguished as either rule-based or statistical systems. In the 2011 Shared Task on Surface Realisation (Belz et al., 2011), the top performing systems were all statistical dependency realizers (Bohnet et al., 2011; Guo et al., 2011; Stent, 2011). Grammar-based approaches map dependency structures or phrase structures to a tree that represents the linear precedence. These approaches are mostly able to generate non-projective word orders. Early work was nearly exclusively applied to phrase structure grammars (e.g. (Kathol and Pollard, 1995; Rambow and Joshi, 1994; Langkilde and Knight, 1998)). Concerning dependency-based frameworks, Br¨oker (1998) used the concept of word order domains to separate surface realization from linear precedence trees. Similarly, Duchier and Debusmann (2001) differentiate Immediate Dominance trees (ID-trees) from Linear Precedence trees (LPtrees). Gerdes and Kahane (2001) apply a hierarchical topological model for generating German word order. Bohnet (2004) employs graph grammars to map between dependency trees and linear precedence trees represented as hierarchical graphs. In the frameworks of HPSG,</context>
</contexts>
<marker>Kathol, Pollard, 1995</marker>
<rawString>A. Kathol and C. Pollard. 1995. Extraposition via complex domain formation. In Meeting of the Association for Computational Linguistics, pages 174–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Kneser</author>
<author>H Ney</author>
</authors>
<date>1995</date>
<booktitle>In In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing,</booktitle>
<pages>181--184</pages>
<contexts>
<context position="35815" citStr="Kneser and Ney, 1995" startWordPosition="5855" endWordPosition="5858">Shared Task on Surface Realisation data, (the test set was not officially released). We also evaluated our linearizer on the data of 2011 Shared Task on Surface Realisation, which is based on the English CoNLL 2009 data (like our previous evaluations) but excludes information on morphological realization. For training and evaluation, we used the exact set up of the Shared Task. For the morphological realization, we used the morphological realizer of Bohnet et al. (2010) that predicts the word form using shortest edit scripts. For the language model (LM), we use a 5-gram model with Kneser-Ney (Kneser and Ney, 1995) smoothing derived from 11 million sentences of the Wikipedia. In Table 6, we compare our two linearizers (with and without lifting) to the two top systems of the 2011 Shared Task on Surface Realisation, (Bohnet et al., 2011) and (Guo et al., 2011). Without the lifting, our system reaches a score comparable to the topranked system in the Shared Task. With the lifting, we get a small7 but statistically significant improvement in BLEU such that our system reaches a higher score than the top ranked systems. This shows that the improvements we obtain from the lifting carry over to more complex gen</context>
</contexts>
<marker>Kneser, Ney, 1995</marker>
<rawString>R. Kneser and H. Ney. 1995. In In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, pages 181–184.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Kow</author>
<author>A Belz</author>
</authors>
<title>LGRT-Eval: A Toolkit for Creating Online Language Evaluation Experiments.</title>
<date>2012</date>
<booktitle>In Proceedings of the 8th International Conference on Language Resources and Evaluation (LREC’12).</booktitle>
<contexts>
<context position="37305" citStr="Kow and Belz (2012)" startWordPosition="6096" endWordPosition="6099">tage of nonprojective edges in our data sets, which are however important to linearize correctly (see Figure 1). I 936 linearizer. In particular, we wanted to check whether the lifting-based linearizer produces more natural word orders for sentences that had a non-projective tree in the corpus, and maybe less natural word orders on originally projective sentences. Therefore, we divided the evaluated items into originally projective and non-projective sentences. We asked four annotators to judge 60 sentence pairs comparing the lifting-based against the nonlifted linearizer using the toolkit by Kow and Belz (2012). All annotators are students, two of them have a background in linguistics. The items were randomly sampled from the subset of the development set containing those sentences where the linearizers produced different surface realizations. The items are subdivided into 30 originally projective and 30 originally non-projective sentences. For each item, we presented the original context sentence from the corpus and the pair of automatically produced linearizations for the current sentence. The annotators had to decide on two criteria: (i) which sentence do they prefer? (ii) how fluent is that sent</context>
</contexts>
<marker>Kow, Belz, 2012</marker>
<rawString>E. Kow and A. Belz. 2012. LGRT-Eval: A Toolkit for Creating Online Language Evaluation Experiments. In Proceedings of the 8th International Conference on Language Resources and Evaluation (LREC’12).</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Langkilde</author>
<author>K Knight</author>
</authors>
<title>Generation that exploits corpus-based statistical knowledge.</title>
<date>1998</date>
<booktitle>In COLING-ACL,</booktitle>
<pages>704--710</pages>
<contexts>
<context position="6719" citStr="Langkilde and Knight, 1998" startWordPosition="1022" endWordPosition="1025">tion systems can be roughly distinguished as either rule-based or statistical systems. In the 2011 Shared Task on Surface Realisation (Belz et al., 2011), the top performing systems were all statistical dependency realizers (Bohnet et al., 2011; Guo et al., 2011; Stent, 2011). Grammar-based approaches map dependency structures or phrase structures to a tree that represents the linear precedence. These approaches are mostly able to generate non-projective word orders. Early work was nearly exclusively applied to phrase structure grammars (e.g. (Kathol and Pollard, 1995; Rambow and Joshi, 1994; Langkilde and Knight, 1998)). Concerning dependency-based frameworks, Br¨oker (1998) used the concept of word order domains to separate surface realization from linear precedence trees. Similarly, Duchier and Debusmann (2001) differentiate Immediate Dominance trees (ID-trees) from Linear Precedence trees (LPtrees). Gerdes and Kahane (2001) apply a hierarchical topological model for generating German word order. Bohnet (2004) employs graph grammars to map between dependency trees and linear precedence trees represented as hierarchical graphs. In the frameworks of HPSG, LFG, and CCG, a grammarbased generator produces word</context>
</contexts>
<marker>Langkilde, Knight, 1998</marker>
<rawString>I. Langkilde and K. Knight. 1998. Generation that exploits corpus-based statistical knowledge. In COLING-ACL, pages 704–710.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
<author>J Nilsson</author>
</authors>
<title>Pseudo-projective dependency parsing.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05),</booktitle>
<pages>99--106</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="4175" citStr="Nivre and Nilsson, 2005" startWordPosition="628" endWordPosition="631">tion has addressed certain aspects of the problem. Das Mandat will er zur¨uckgeben . the.ACC mandate.ACC want.3SG he.NOM return.INF . ’He wants to return the mandate.’ Figure 2: German object fronting with complex verb introducing a non-projective edge. In this paper, we aim for a general data-driven approach that can deal with various causes for nonprojectivity and will work for typologically different languages. Our technique is inspired by work in data-driven multilingual parsing, where non-projectivity has received considerable attention. In pseudo-projective parsing (Kahane et al., 1998; Nivre and Nilsson, 2005), the parsing algorithm is restricted to projective structures, but the issue is side-stepped by converting non-projective structures to projective ones prior to training and application, and then restoring the original structure afterwards. Similarly, we split the linearization task in two stages: initially, the input tree is modified by lifting certain edges in such a way that new orderings become possible even under a projectivity constraint; the second stage is the original, projective linearization step. In parsing, projectivization is a deterministic process that lifts edges based on the</context>
<context position="10271" citStr="Nivre and Nilsson (2005)" startWordPosition="1637" endWordPosition="1640">d (or simply of the node d) is an operation that replaces h —* d with g —* d, given that there exists an edge g —* h in the tree, and undefined otherwise (i.e. the dependent d is reattached to the head of its head).2 When the lifting 2The undefined case occurs only when d depends on the root, and hence cannot be lifted further; but these edges are by definition projective, since the root dominates the entire tree. operation is applied n successive times to the same node, we say the node was lifted n steps. 3.2 Training During training we make use of the projectivization algorithm described by Nivre and Nilsson (2005). It works by iteratively lifting the shortest nonprojective edges until the tree is projective. Here, shortest edge refers to the edge spanning over the fewest number of words. Since finding the shortest edge relies on the linear order, instead of lifting the shortest edge, we lift non-projective edges ordered by depth in the tree, starting with the deepest nested edge. A lifted version of the tree from Figure 1 is shown in Figure 3. The edge of what has been lifted three steps (the original edge is dotted), and the tree is no longer non-projective. It is what federal support should try to ac</context>
<context position="14766" citStr="Nivre and Nilsson (2005)" startWordPosition="2389" endWordPosition="2392">the classifier iteratively until it says stop. They found that the latter outperformed the former. We tried this method, but found that it was inferior to the multi-class model and more frequently over- or underlifted. Second, to avoid data sparseness for infrequent lifting distances, we introduce a maximum number of liftings. We found that a maximum of 3 gave the best performance. In the pseudocode below, we refer to this number as maxsteps.3 This means that we are able to predict the correct lifting for most (but not all) of the non-projective edges in our data sets (cf. Table 3). Third, as Nivre and Nilsson (2005) do for pars3During training, nodes that are lifted further than maxsteps are assigned to the class corresponding to maxsteps. This approach worked better than ignoring the training instance or treating it as a non-lifting (i.e. a lifting of 0 steps). ing, we experimented with marking edges that were lifted by indicating this on the edge labels. In the case of parsing, this step is necessary in order to reverse the liftings in the parser output. In our case, it could potentially be beneficial for both the lifting classifier, and for the linearizer. However, we found that marking liftings at be</context>
</contexts>
<marker>Nivre, Nilsson, 2005</marker>
<rawString>J. Nivre and J. Nilsson. 2005. Pseudo-projective dependency parsing. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05), pages 99–106, Ann Arbor, Michigan, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Rambow</author>
<author>A K Joshi</author>
</authors>
<title>A formal look at dependency grammars and phrase-structure grammars, with special consideration of word-order phenomena.</title>
<date>1994</date>
<contexts>
<context position="6690" citStr="Rambow and Joshi, 1994" startWordPosition="1018" endWordPosition="1021">t word orders. Linearization systems can be roughly distinguished as either rule-based or statistical systems. In the 2011 Shared Task on Surface Realisation (Belz et al., 2011), the top performing systems were all statistical dependency realizers (Bohnet et al., 2011; Guo et al., 2011; Stent, 2011). Grammar-based approaches map dependency structures or phrase structures to a tree that represents the linear precedence. These approaches are mostly able to generate non-projective word orders. Early work was nearly exclusively applied to phrase structure grammars (e.g. (Kathol and Pollard, 1995; Rambow and Joshi, 1994; Langkilde and Knight, 1998)). Concerning dependency-based frameworks, Br¨oker (1998) used the concept of word order domains to separate surface realization from linear precedence trees. Similarly, Duchier and Debusmann (2001) differentiate Immediate Dominance trees (ID-trees) from Linear Precedence trees (LPtrees). Gerdes and Kahane (2001) apply a hierarchical topological model for generating German word order. Bohnet (2004) employs graph grammars to map between dependency trees and linear precedence trees represented as hierarchical graphs. In the frameworks of HPSG, LFG, and CCG, a grammar</context>
</contexts>
<marker>Rambow, Joshi, 1994</marker>
<rawString>O. Rambow and A. K. Joshi. 1994. A formal look at dependency grammars and phrase-structure grammars, with special consideration of word-order phenomena.</rawString>
</citation>
<citation valid="false">
<booktitle>Current Issues in Meaning-Text Theory.</booktitle>
<editor>In Leo Wanner, editor,</editor>
<publisher>Pinter,</publisher>
<location>London, UK.</location>
<marker></marker>
<rawString>In Leo Wanner, editor, Current Issues in Meaning-Text Theory. Pinter, London, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Reape</author>
</authors>
<title>A logical treatment of semi-free word order and bounded discontinuous constituency.</title>
<date>1989</date>
<booktitle>In Proceedings of the EACL, EACL ’89,</booktitle>
<pages>103--110</pages>
<contexts>
<context position="5610" citStr="Reape, 1989" startWordPosition="850" endWordPosition="851">component. This classifier has to be trained on suitable data, and it is an empirical question whether the projective linearizer can take advantage of this preceding lifting step. We present experiments on six languages with varying degrees of non-projective structures: English, German, Dutch, Danish, Czech and Hungarian, which exhibit substantially different word order properties. Our approach achieves significant improvements on all six languages. On German, we also report results of a pilot human evaluation. 2 Related Work An important concept for tree linearization are word order domains (Reape, 1989). The domains are bags of words (constituents) that are not allowed to be discontinuous. A straightforward method to obtain the word order domains from dependency trees and to order the words in the tree is to use each word and its children as domain and then to order the domains and contained words recursively. As outlined in the introduction, the direct mapping of syntactic trees to domains does not provide the possibility to obtain all possible correct word orders. Linearization systems can be roughly distinguished as either rule-based or statistical systems. In the 2011 Shared Task on Surf</context>
</contexts>
<marker>Reape, 1989</marker>
<rawString>M. Reape. 1989. A logical treatment of semi-free word order and bounded discontinuous constituency. In Proceedings of the EACL, EACL ’89, pages 103–110.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Ringger</author>
<author>M Gamon</author>
<author>R C Moore</author>
<author>D Rojas</author>
<author>M Smets</author>
<author>S Corston-Oliver</author>
</authors>
<title>Linguistically informed statistical models of constituent structure for ordering in sentence realization.</title>
<date>2004</date>
<booktitle>In COLING ’04,</booktitle>
<pages>673--679</pages>
<contexts>
<context position="7597" citStr="Ringger et al., 2004" startWordPosition="1151" endWordPosition="1154">ear Precedence trees (LPtrees). Gerdes and Kahane (2001) apply a hierarchical topological model for generating German word order. Bohnet (2004) employs graph grammars to map between dependency trees and linear precedence trees represented as hierarchical graphs. In the frameworks of HPSG, LFG, and CCG, a grammarbased generator produces word order candidates that might be non-projective, and a ranker is used to select the best surface realization (Cahill et al., 2007; White and Rajkumar, 2009). Statistical methods for linearization have recently become more popular (Langkilde and Knight, 1998; Ringger et al., 2004; Filippova and Strube, 2009; Wan et al., 2009; He et al., 2009; Bohnet et al., 2010; Guo et al., 2011). They typically work by traversing the syntactic structure either bottom-up (Filip929 pova and Strube, 2007; Bohnet et al., 2010) or topdown (Guo et al., 2011; Bohnet et al., 2011). These linearizers are mostly applied to English and do not deal with non-projective word orders. An exception is Filippova and Strube (2007), who contribute a study on the treatment of preverbal and postverbal constituents for German focusing on constituent order at the sentence level. The work most similar to ou</context>
</contexts>
<marker>Ringger, Gamon, Moore, Rojas, Smets, Corston-Oliver, 2004</marker>
<rawString>E. Ringger, M. Gamon, R. C. Moore, D. Rojas, M. Smets, and S. Corston-Oliver. 2004. Linguistically informed statistical models of constituent structure for ordering in sentence realization. In COLING ’04, pages 673– 679.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Seeker</author>
<author>J Kuhn</author>
</authors>
<title>Making Ellipses Explicit in Dependency Conversion for a German Treebank.</title>
<date>2012</date>
<booktitle>In Proceedings of LREC 2012, Istanbul, Turkey. European Language Resources Association (ELRA).</booktitle>
<contexts>
<context position="26425" citStr="Seeker and Kuhn (2012)" startWordPosition="4302" endWordPosition="4305">hile English word order is very restrictive, Czech and Hungarian exhibit few word order constraints. Danish, Dutch, and German (so-called V2, i. e. verb-second, languages) show a relatively free word order that is however more restrictive than in Hungarian or Czech. The English and the Czech data are from the CoNLL 2009 Shared Task data sets (Hajiˇc et al., 2009). The Danish and the Dutch data are from the CoNLL 2006 Shared Task data sets (Buchholz and Marsi, 2006). For Hungarian, we use the Hungarian Dependency Treebank (Vincze et al., 2010), and for German, we use a dependency conversion by Seeker and Kuhn (2012). # sent’s np sent’s np edges np &lt; 3 lifts English 39,279 7.63 % 0.39% 98.39% German 36,000 28.71% 2.34% 94.98% Dutch 13,349 36.44% 5.42% 99.80% Danish 5,190 15.62 % 1.00% 96.72% Hungarian 61,034 15.81% 1.45% 99.82% Czech 38,727 22.42% 1.86% 99.84% Table 3: Size of training sets, percentage of nonprojective (np) sentences and edges, percentage of np edges covered by 3 lifting steps. Table 3 shows the sizes of the training corpora and the percentage of non-projective sentences and edges in the data. Note that the data sets for Danish and Dutch are quite small. English has the least percentage o</context>
</contexts>
<marker>Seeker, Kuhn, 2012</marker>
<rawString>W. Seeker and J. Kuhn. 2012. Making Ellipses Explicit in Dependency Conversion for a German Treebank. In Proceedings of LREC 2012, Istanbul, Turkey. European Language Resources Association (ELRA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stent</author>
</authors>
<title>Att-0: Submission to generation challenges 2011 surface realization shared task.</title>
<date>2011</date>
<booktitle>In Proceedings of the Generation Challenges Session at the 13th European Workshop on Natural Language Generation,</booktitle>
<pages>230--231</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Nancy, France,</location>
<contexts>
<context position="6368" citStr="Stent, 2011" startWordPosition="974" endWordPosition="975"> from dependency trees and to order the words in the tree is to use each word and its children as domain and then to order the domains and contained words recursively. As outlined in the introduction, the direct mapping of syntactic trees to domains does not provide the possibility to obtain all possible correct word orders. Linearization systems can be roughly distinguished as either rule-based or statistical systems. In the 2011 Shared Task on Surface Realisation (Belz et al., 2011), the top performing systems were all statistical dependency realizers (Bohnet et al., 2011; Guo et al., 2011; Stent, 2011). Grammar-based approaches map dependency structures or phrase structures to a tree that represents the linear precedence. These approaches are mostly able to generate non-projective word orders. Early work was nearly exclusively applied to phrase structure grammars (e.g. (Kathol and Pollard, 1995; Rambow and Joshi, 1994; Langkilde and Knight, 1998)). Concerning dependency-based frameworks, Br¨oker (1998) used the concept of word order domains to separate surface realization from linear precedence trees. Similarly, Duchier and Debusmann (2001) differentiate Immediate Dominance trees (ID-trees)</context>
</contexts>
<marker>Stent, 2011</marker>
<rawString>A. Stent. 2011. Att-0: Submission to generation challenges 2011 surface realization shared task. In Proceedings of the Generation Challenges Session at the 13th European Workshop on Natural Language Generation, pages 230–231, Nancy, France, September. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Vincze</author>
<author>D Szauter</author>
<author>A Alm´asi</author>
<author>G M´ora</author>
<author>Z Alexin</author>
<author>J Csirik</author>
</authors>
<title>Hungarian Dependency Treebank.</title>
<date>2010</date>
<booktitle>In Proceedings of the Seventh conference on International Language Resources and Evaluation (LREC 2010),</booktitle>
<pages>1855--1862</pages>
<location>Valletta,</location>
<marker>Vincze, Szauter, Alm´asi, M´ora, Alexin, Csirik, 2010</marker>
<rawString>V. Vincze, D. Szauter, A. Alm´asi, G. M´ora, Z. Alexin, and J. Csirik. 2010. Hungarian Dependency Treebank. In Proceedings of the Seventh conference on International Language Resources and Evaluation (LREC 2010), pages 1855–1862, Valletta, Malta.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Wan</author>
<author>M Dras</author>
<author>R Dale</author>
<author>C Paris</author>
</authors>
<title>Improving grammaticality in statistical sentence generation: Introducing a dependency spanning tree algorithm with an argument satisfaction model.</title>
<date>2009</date>
<booktitle>In EACL,</booktitle>
<pages>852--860</pages>
<contexts>
<context position="7643" citStr="Wan et al., 2009" startWordPosition="1159" endWordPosition="1162">(2001) apply a hierarchical topological model for generating German word order. Bohnet (2004) employs graph grammars to map between dependency trees and linear precedence trees represented as hierarchical graphs. In the frameworks of HPSG, LFG, and CCG, a grammarbased generator produces word order candidates that might be non-projective, and a ranker is used to select the best surface realization (Cahill et al., 2007; White and Rajkumar, 2009). Statistical methods for linearization have recently become more popular (Langkilde and Knight, 1998; Ringger et al., 2004; Filippova and Strube, 2009; Wan et al., 2009; He et al., 2009; Bohnet et al., 2010; Guo et al., 2011). They typically work by traversing the syntactic structure either bottom-up (Filip929 pova and Strube, 2007; Bohnet et al., 2010) or topdown (Guo et al., 2011; Bohnet et al., 2011). These linearizers are mostly applied to English and do not deal with non-projective word orders. An exception is Filippova and Strube (2007), who contribute a study on the treatment of preverbal and postverbal constituents for German focusing on constituent order at the sentence level. The work most similar to ours is that of Gamon et al. (2002). They use ma</context>
</contexts>
<marker>Wan, Dras, Dale, Paris, 2009</marker>
<rawString>S. Wan, M. Dras, R. Dale, and C. Paris. 2009. Improving grammaticality in statistical sentence generation: Introducing a dependency spanning tree algorithm with an argument satisfaction model. In EACL, pages 852– 860.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M White</author>
<author>R Rajkumar</author>
</authors>
<title>Perceptron reranking for CCG realization. In</title>
<date>2009</date>
<booktitle>EMNLP’09,</booktitle>
<pages>410--419</pages>
<location>Singapore,</location>
<contexts>
<context position="7474" citStr="White and Rajkumar, 2009" startWordPosition="1134" endWordPosition="1137">rom linear precedence trees. Similarly, Duchier and Debusmann (2001) differentiate Immediate Dominance trees (ID-trees) from Linear Precedence trees (LPtrees). Gerdes and Kahane (2001) apply a hierarchical topological model for generating German word order. Bohnet (2004) employs graph grammars to map between dependency trees and linear precedence trees represented as hierarchical graphs. In the frameworks of HPSG, LFG, and CCG, a grammarbased generator produces word order candidates that might be non-projective, and a ranker is used to select the best surface realization (Cahill et al., 2007; White and Rajkumar, 2009). Statistical methods for linearization have recently become more popular (Langkilde and Knight, 1998; Ringger et al., 2004; Filippova and Strube, 2009; Wan et al., 2009; He et al., 2009; Bohnet et al., 2010; Guo et al., 2011). They typically work by traversing the syntactic structure either bottom-up (Filip929 pova and Strube, 2007; Bohnet et al., 2010) or topdown (Guo et al., 2011; Bohnet et al., 2011). These linearizers are mostly applied to English and do not deal with non-projective word orders. An exception is Filippova and Strube (2007), who contribute a study on the treatment of prever</context>
</contexts>
<marker>White, Rajkumar, 2009</marker>
<rawString>M. White and R. Rajkumar. 2009. Perceptron reranking for CCG realization. In EMNLP’09, pages 410–419, Singapore, August.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
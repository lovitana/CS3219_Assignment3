<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000444">
<title confidence="0.995685">
Joint Learning for Coreference Resolution with Markov Logic
</title>
<author confidence="0.999755">
Yang Song&apos;, Jing Jiang2, Wayne Xin Zhao3, Sujian Li&apos;, Houfeng Wang&apos;
</author>
<affiliation confidence="0.999502">
&apos;Key Laboratory of Computational Linguistics (Peking University) Ministry of Education,China
2School of Information Systems, Singapore Management University, Singapore
3School of Electronics Engineering and Computer Science, Peking University, China
</affiliation>
<email confidence="0.98925">
{ysong, lisujian, wanghf}@pku.edu.cn, jingjiang@smu.edu.sg, batmanfly@gmail.com
</email>
<sectionHeader confidence="0.9956" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999772105263158">
Pairwise coreference resolution models must
merge pairwise coreference decisions to gen-
erate final outputs. Traditional merging meth-
ods adopt different strategies such as the best-
first method and enforcing the transitivity con-
straint, but most of these methods are used
independently of the pairwise learning meth-
ods as an isolated inference procedure at the
end. We propose a joint learning model which
combines pairwise classification and mention
clustering with Markov logic. Experimen-
tal results show that our joint learning sys-
tem outperforms independent learning sys-
tems. Our system gives a better performance
than all the learning-based systems from the
CoNLL-2011 shared task on the same dataset.
Compared with the best system from CoNLL-
2011, which employs a rule-based method,
our system shows competitive performance.
</bodyText>
<sectionHeader confidence="0.999134" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999848021276596">
The task of noun phrase coreference resolution is to
determine which mentions in a text refer to the same
real-world entity. Many methods have been pro-
posed for this problem. Among them the mention-
pair model (McCarthy and Lehnert, 1995) is one of
the most influential ones and can achieve the state-
of-the-art performance (Bengtson and Roth, 2008).
The mention-pair model splits the task into three
parts: mention detection, pairwise classification and
mention clustering. Mention detection aims to iden-
tify anaphoric noun phrases, including proper nouns,
common noun phrases and pronouns. Pairwise clas-
sification takes a pair of detected anaphoric noun
phrase candidates and determines whether they re-
fer to the same entity. Because these classification
decisions are local, they do not guarantee that can-
didate mentions are partitioned into clusters. There-
fore a mention clustering step is needed to resolve
conflicts and generate the final mention clusters.
Much work has been done following the mention-
pair model (Soon et al., 2001; Ng and Cardie, 2002).
In most work, pairwise classification and mention
clustering are done sequentially. A major weak-
ness of this approach is that pairwise classification
considers only local information, which may not be
sufficient to make correct decisions. One way to
address this weakness is to jointly learn the pair-
wise classification model and the mention cluster-
ing model. This idea has been explored to some
extent by McCallum and Wellner (2005) using con-
ditional undirected graphical models and by Finley
and Joachims (2005) using an SVM-based super-
vised clustering method.
In this paper, we study how to use a different
learning framework, Markov logic (Richardson and
Domingos, 2006), to learn a joint model for both
pairwise classification and mention clustering un-
der the mention-pair model. We choose Markov
logic because of its appealing properties. Markov
logic is based on first-order logic, which makes
the learned models readily interpretable by humans.
Moreover, joint learning is natural under the Markov
logic framework, with local pairwise classification
and global mention clustering both formulated as
weighted first-order clauses. In fact, Markov logic
has been previously used by Poon and Domingos
(2008) for coreference resolution and achieved good
</bodyText>
<page confidence="0.906288">
1245
</page>
<note confidence="0.776972">
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1245–1254, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.999942771428572">
results, but it was used for unsupervised coreference
resolution and the method was based on a different
model, the entity-mention model.
More specifically, to combine mention cluster-
ing with pairwise classification, we adopt the com-
monly used strategies (such as best-first clustering
and transitivity constraint), and formulate them as
first-order logic formulas under the Markov logic
framework. Best-first clustering has been previously
studied by Ng and Cardie (2002) and Bengtson and
Roth (2008) and found to be effective. Transitivity
constraint has been applied to coreference resolution
by Klenner (2007) and Finkel and Manning (2008),
and also achieved good performance.
We evaluate Markov logic-based method on the
dataset from CoNLL-2011 shared task. Our ex-
periment results demonstrate the advantage of joint
learning of pairwise classification and mention clus-
tering over independent learning. We examine
best-first clustering and transitivity constraint in our
methods, and find that both are very useful for coref-
erence resolution. Compared with the state of the
art, our method outperforms a baseline that repre-
sents a typical system using the mention-pair model.
Our method is also better than all learning systems
from the CoNLL-2011 shared task based on the re-
ported performance. Even with the top system from
CoNLL-2011, our performance is still competitive.
In the rest of this paper, we first describe a stan-
dard pairwise coreference resolution system in Sec-
tion 2. We then present our Markov logic model for
pairwise coreference resolution in Section 3. Exper-
imental results are given in Section 4. Finally we
discuss related work in Section 5 and conclude in
Section 6.
</bodyText>
<sectionHeader confidence="0.9904475" genericHeader="introduction">
2 Standard Pairwise Coreference
Resolution
</sectionHeader>
<bodyText confidence="0.9982125">
In this section, we describe standard learning-based
framework for pairwise coreference resolution. The
major steps include mention detection, pairwise
classification and mention clustering.
</bodyText>
<subsectionHeader confidence="0.995812">
2.1 Mention Detection
</subsectionHeader>
<bodyText confidence="0.9997244">
For mention detection, traditional methods include
learning-based and rule-based methods. Which kind
of method to choose depends on specific dataset. In
this paper, we first consider all the noun phrases
in the given text as candidate mentions. With-
out gold standard mention boundaries, we use a
well-known preprocessing tool from Stanford’s NLP
group1 to extract noun phrases. After obtaining all
the extracted noun phrases, we also use a rule-based
method to remove some erroneous candidates based
on previous studies (e.g. Lee et al. (2011), Uryupina
et al. (2011)). Some examples of these erroneous
candidates include stop words (e.g. uh, hmm), web
addresses (e.g. http://www.google.com),
numbers (e.g. $9,000) and pleonastic “it” pronouns.
</bodyText>
<subsectionHeader confidence="0.996259">
2.2 Pairwise Classification
</subsectionHeader>
<bodyText confidence="0.999956533333333">
For pairwise classification, traditional learning-
based methods usually adopt a classification model
such as maximum entropy models and support vec-
tor machines. Training instances (i.e. positive and
negative mention pairs) are constructed from known
coreference chains, and features are defined to rep-
resent these instances.
In this paper, we build a baseline system that uses
maximum entropy models as the classification algo-
rithm. For generation of training instances, we fol-
low the method of Bengtson and Roth (2008). For
each predicted mention m, we generate a positive
mention pair between m and its closest preceding
antecedent, and negative mention pairs by pairing m
with each of its preceding predicted mentions which
are not coreferential with m. To avoid having too
many negative instances, we impose a maximum
sentence distance between the two mentions when
constructing mention pairs. This is based on the in-
tuition that for each anaphoric mention, its preced-
ing antecedent should appear quite near it, and most
coreferential mention pairs which have a long sen-
tence distance can be resolved using string match-
ing. During the testing phase, we generate men-
tion pairs for each mention candidate with each of
its preceding mention candidates and use the learned
model to make coreference decisions for these men-
tion pairs. We also impose the sentence distance
constraint and use string matching for mention pairs
with a sentence distance exceeding the threshold.
</bodyText>
<footnote confidence="0.989221">
1http://nlp.stanford.edu/software/corenlp.shtml
</footnote>
<page confidence="0.984685">
1246
</page>
<subsectionHeader confidence="0.999039">
2.3 Mention Clustering
</subsectionHeader>
<bodyText confidence="0.999946631578948">
After obtaining the coreferential results for all men-
tion pairs, some clustering method should be used to
generate the final output. One strategy is the single-
link method, which links all the mention pairs that
have a prediction probability higher than a threshold
value. Two other alternative methods are the best-
first clustering method and clustering with the tran-
sitivity constraint. Best-first clustering means that
for each candidate mention m, we select the best
one from all its preceding candidate mentions based
on the prediction probabilities. A threshold value
is given to filter out those mention pairs that have a
low probability to be coreferential. Transitivity con-
straint means that if a and b are coreferential and
b and c are coreferential, then a and c must also be
coreferential. Previous work has found that best-first
clustering and transitivity constraint-based cluster-
ing are better than the single-link method. Finally
we remove all the singleton mentions.
</bodyText>
<sectionHeader confidence="0.993986" genericHeader="method">
3 Markov Logic for Pairwise Coreference
Resolution
</sectionHeader>
<bodyText confidence="0.999693933333333">
In this section, we present our method for joint
learning of pairwise classification and mention clus-
tering using Markov logic. For mention detection,
training instance generation and postprocessing, our
method follows the same procedures as described in
Section 2. In what follows, we will first describe
the basic Markov logic networks (MLN) framework,
and then introduce the first-order logic formulas we
use in our MLN including local formulas and global
formulas which perform pairwise classification and
mention clustering respectively. Through this way,
these two isolated parts are combined together, and
joint learning and inference can be performed in a
single framework. Finally we present inference and
parameter learning methods.
</bodyText>
<subsectionHeader confidence="0.999238">
3.1 Markov Logic Networks
</subsectionHeader>
<bodyText confidence="0.999972666666667">
Markov logic networks combine Markov networks
with first-order logic (Richardson and Domingos,
2006; Riedel, 2008). A Markov logic network con-
sists of a set of first-order clauses (which we will re-
fer to as formulas in the rest of the paper) just like in
first-order logic. However, different from first-order
logic where a formula represents a hard constraint,
in an MLN, these constraints are softened and they
can be violated with some penalty. An MLN M
is therefore a set of weighted formulas {(ϕZ, wZ)}Z,
where ϕZ is a first order formula and wZ is the penalty
(the formula’s weight). These weighted formulas
define a probability distribution over sets of ground
atoms or so-called possible worlds. Let y denote a
possible world, then we define p(y) as follows:
</bodyText>
<equation confidence="0.9984074">
( ∑ )
1∑
p(y) = Z exp wZ foi
C (y) . (1)
(oi,wi)∈M C∈Cnϕi
</equation>
<bodyText confidence="0.997829366666667">
Here each c is a binding of free variables in ϕZ to
constants. Each foi crepresents a binary feature func-
tion that returns 1 if the ground formula we get by
replacing the free variables in ϕZ with the constants
in c under the given possible world y is true, and 0
otherwise. noi denotes the number of free variables
of a formula ϕZ. Cnϕi is the set of all bindings for the
free variables in ϕZ. Z is a normalization constant.
This distribution corresponds to a Markov network
where nodes represent ground atoms and factors rep-
resent ground formulas.
Each formula consists of a set of first-order predi-
cates, logical connectors and variables. Take the fol-
lowing formula as one example:
(ϕZ, wZ) : headMatch(a, b)∧(a ≠ b) ⇒ coref (a, b).
The formula above indicates that if two different
candidate mentions a and b have the same head
word, then they are coreferential. Here a and b are
variables which can represent any candidate men-
tion, headMatch and coref are observed predicate
and hidden predicate respectively. An observed
predicate is one whose value is known from the ob-
servations when its free variables are assigned some
constants. A hidden predicate is one whose value is
not known from the observations. From this exam-
ple, we can see that headMatch is an observed pred-
icate because we can check whether two candidate
mentions have the same head word. coref is a hid-
den predicate because this is something we would
like to predict.
</bodyText>
<subsectionHeader confidence="0.992948">
3.2 Formulas
</subsectionHeader>
<bodyText confidence="0.999864">
We use two kinds of formulas for pairwise classi-
fication and mention clustering, respectively. For
</bodyText>
<page confidence="0.961275">
1247
</page>
<bodyText confidence="0.880667484848485">
describing the attributes of mi
mentionType(i,t) mi has mention type NAM(named entities), NOM(nominal) or PRO(pronouns).
entityType(i,e) mi has entity type PERSON, ORG, GPE or UN...
genderType(i,g) mi has gender type MALE, FEMALE, NEUTRAL or UN.
numberType(i,n) mi has number type SINGULAR, PLURAL or UN.
hasHead(i,h) mi has head word h, here h can represent all possible head words.
firstMention(i) mi is the first mention in its sentence.
reflexive(i) mi is reflexive.
possessive(i) mi is possessive.
definite(i) mi is definite noun phrase.
indefinite(i) mi is indefinite noun phrase.
demonstrative(i) mi is demonstrative.
describing the attributes of relations between mj and mi
mentionDistance(j,i,m) Distance between mj and mi in mentions.
sentenceDistance(j,i,s) Distance between mj and mi in sentences.
bothMatch(j,i,b) Gender and number of both mj and mi match: AGREE YES, AGREE NO
and AGREE UN).
closestMatch(j,i,c) mj is the first agreement in number and gender when looking backward
from mi: CAGREE YES, CAGREE NO and CAGREE UN.
exactStrMatch(j,i) Exact strings match between mj and mi.
pronounStrMatch(j,i) Both are pronouns and their strings match.
nopronounStrMatch(j,i) Both are not pronouns and their strings match.
properStrMatch(j,i) Both are proper names and their strings match.
headMatch(j,i) Head word strings match between mj and mi.
subStrMatch(j,i) Sub-word strings match between mj and mi.
animacyMatch(j,i) Animacy types match between mj and mi.
nested(j,i) mj/i is included in mi/j.
c command(j,i) mj/i C-Commands mi/j.
sameSpeaker(j,i) mj and mi have the same speaker.
entityTypeMatch(j,i) Entity types match between mj and mi.
alias(j,i) mj/i is an alias of mi/j.
srlMatch(j,i) mj and mi have the same semantic role.
verbMatch(j,i) mj and mi have semantic role for the same verb.
</bodyText>
<tableCaption confidence="0.997072">
Table 1: Observed predicates.
</tableCaption>
<bodyText confidence="0.999921625">
pairwise classification, because the decisions are lo-
cal, we use a set of local formulas. For mention
clustering, we use global formulas to implement
best-first clustering or transitivity constraint. We
naturally combine pairwise classification with men-
tion clustering via local and global formulas in the
Markov logic framework, which is the essence of
“joint learning” in our work.
</bodyText>
<subsectionHeader confidence="0.944695">
3.2.1 Local Formulas
</subsectionHeader>
<bodyText confidence="0.999873333333333">
A local formula relates any observed predicates to
exactly one hidden predicate. For our problem, we
define a list of observed predicates to describe the
properties of individual candidate mentions and the
relations between two candidate mentions, shown in
Table 1. For our problem, we have only one hidden
predicate, i.e. coref. Most of our local formulas are
from existing work (e.g. Soon et al. (2001), Ng and
Cardie (2002), Sapena et al. (2011)). They are listed
in Table 2, where the symbol “+” indicates that for
every value of the variable preceding “+” there is a
separate weight for the corresponding formula.
</bodyText>
<subsectionHeader confidence="0.802622">
3.2.2 Global Formulas
</subsectionHeader>
<bodyText confidence="0.999919333333333">
Global formulas are designed to add global con-
straints for hidden predicates. Since in our problem
there is only one hidden predicate, i.e. coref, our
global formulas incorporate correlations among dif-
ferent ground atoms of the coref predicates. Next we
will show the best-first and transitivity global con-
straints. Note that we treat them as hard constraints
so we do not set any weights for these global formu-
las.
</bodyText>
<page confidence="0.935272">
1248
</page>
<subsectionHeader confidence="0.354202">
Lexical Features
</subsectionHeader>
<equation confidence="0.957441457142857">
mentionType(j,t1+) n mentionType(i,t2+) n exactStrMatch(j,i) n j# i =:� coref(j,i)
mentionType(j,t1+) n mentionType(i,t2+) n pronounStrMatch (j,i) n j# i =:� coref(j,i)
mentionType(j,t1+) n mentionType(i,t2+) n properStrMatch(j,i) n j# i =:� coref(j,i)
mentionType(j,t1+) n mentionType(i,t2+) n nopronounStrMatch(j,i) n j# i =:� coref(j,i)
mentionType(j,t1+) n mentionType(i,t2+) n headMatch(j,i) n j# i =:� coref(j,i)
mentionType(j,t1+) n mentionType(i,t2+) n subStrMatch(j,i) n j# i =:� coref(j,i)
hasHead(j,h1+) n hasHead(i,h2+) n j# i =:� coref(j,i)
Grammatical Features
mentionType(j,t1+) n mentionType(i,t2+) n genderType(j,g1+) n genderType(i,g2+) n j# i =:� coref(j,i)
mentionType(j,t1+) n mentionType(i,t2+) n numberType(j,n1+) n numberType(i,n2+) n j# i coref(j,i)
mentionType(j,t1+) n mentionType(i,t2+) n bothMatch(j,i,b+) n j# i =:� coref(j,i)
mentionType(j,t1+) n mentionType(i,t2+) n closestMatch(j,i,c+) n j# i =:� coref(j,i)
mentionType(j,t1+) n mentionType(i,t2+) n animacyMatch(j,i) n j# i =:� coref(j,i)
mentionType(j,t1+) n mentionType(i,t2+) n nested(j,i) n j# i =:� coref(j,i)
mentionType(j,t1+) n mentionType(i,t2+) n c command(j,i) n j# i=:� coref(j,i)
(mentionType(j,t1+) V mentionType(i,t2+)) n j# i =:� coref(j,i)
(reflexive(j) V reflexive(i)) n j# i =:� coref(j,i)
(possessive(j) V possessive(i)) n j# i =:� coref(j,i)
(definite(j) V definite(i)) n j# i =:� coref(j,i)
(indefinite(j) V indefinite(i)) n j# i =:� coref(j,i)
(demonstrative(j) V demonstrative(i)) n j# i =:� coref(j,i)
Distance and position Features
mentionType(j,t1+) n mentionType(i,t2+) n sentenceDistance(j,i,s+) n j# i =:� coref(j,i)
mentionType(j,t1+) n mentionType(i,t2+) n mentionDistance (j,i,m+) n j# i =:� coref(j,i)
(firstMention(j) V firstMention(i)) n j# i =:� coref(j,i)
Semantic Features
mentionType(j,t1+) n mentionType(i,t2+) n alias(j,i) n j# i =:� coref(j,i)
mentionType(j,t1+) n mentionType(i,t2+) n sameSpeaker(j,i) n j# i =:� coref(j,i)
mentionType(j,t1+) n mentionType(i,t2+) n entityTypeMatch(j,i) n j# i =:� coref(j,i)
mentionType(j,t1+) n mentionType(i,t2+) n srlMatch(j,i) n j# i =:� coref(j,i)
mentionType(j,t1+) n mentionType(i,t2+) n verbMatch(j,i) n j# i =:� coref(j,i)
(entityType(j,e1+) V entityType(i,e2+)) n j# i =:� coref(j,i)
Table 2: Local Formulas.
Best-First constraint:
coref(j, i) =:� scoref(k, i) Vj, k &lt; i(k =� j) (2)
</equation>
<bodyText confidence="0.992697833333334">
Here we assume that coref(j,i) returns true if can-
didate mentions j and i are coreferential and false
otherwise. Therefore for each candidate mention i,
we should only select at most one candidate mention
j to return true for the predicate coref(j,i) from all its
preceding candidate mentions.
</bodyText>
<equation confidence="0.88610875">
Transitivity constraint:
coref(j,k)ncoref(k,i)nj &lt; k &lt; i coref(j,i) (3)
coref(j, k)ncoref(j, i)nj &lt; k &lt; i coref(k, i) (4)
coref(j, i)ncoref(k, i)nj &lt; k &lt; i coref(j, k) (5)
</equation>
<bodyText confidence="0.9998885">
With the transitivity constraint, it means for given
mentions j, k and i, if any two pairs of them are
coreferential, then the third pair of them should be
also coreferential.
We use best-first clustering and transitivity con-
straint in our joint learning model respectively. De-
tailed comparisons between them will be shown in
Section 4.
</bodyText>
<subsectionHeader confidence="0.815521">
3.3 Inference
</subsectionHeader>
<bodyText confidence="0.999989">
We use MAP inference which is implemented by In-
teger Linear Programming (ILP). Its objective is to
maximize a posteriori probability as follows. Here
we use x to represent all the observed ground atoms
and y to represent the hidden ground atoms. For-
mally, we have
</bodyText>
<equation confidence="0.9950085">
y� = arg max p(y|x) ^_ arg max s(y, x),
y y
</equation>
<bodyText confidence="0.6751325">
where
∑
</bodyText>
<equation confidence="0.9888145">
s(y, x) =
(ϕi,wi)∈M
</equation>
<bodyText confidence="0.999832333333333">
Each hidden ground atom can only takes a value of
either 0 or 1. And global formulas should be satis-
fied as hard constraints when inferring the best P. So
</bodyText>
<figure confidence="0.399073">
∑wz fϕi
C∈Cnϕi C (y,x). (6)
</figure>
<page confidence="0.960754">
1249
</page>
<bodyText confidence="0.99916275">
the problem can be easily solved using ILP. Detailed
introduction about transforming ground Markov net-
works in Markov logic into an ILP problem can be
found in (Riedel, 2008).
</bodyText>
<subsectionHeader confidence="0.981702">
3.4 Parameter Learning
</subsectionHeader>
<bodyText confidence="0.999655833333333">
For parameter learning, we employ the online
learner MIRA (Crammer and Singer, 2003), which
establishes a large margin between the score of the
gold solution and all wrong solutions to learn the
weights. This is achieved by solving the quadratic
program as follows
</bodyText>
<equation confidence="0.997653666666667">
min 11 wt − wt−1 11 . (7)
s.t. s(yi, xi) − s(y′, xi) &gt; L(yi, y′)
by′ =� yi, (yi, xi) E D
</equation>
<bodyText confidence="0.9953369375">
Here D = {(yi, xi)}Ni�1 represents N training in-
stances (each instance represents one single docu-
ment in the dataset) and t represents the number of
iterations. In our problem, we adopt 1-best MIRA,
which means that in each iteration we try to find wt
which can guarantee the difference between the right
solution yi and the best solution y′ (i.e. the one with
the highest score s(y′, xi), equivalent to y� in Section
3.3)) is at least as big as the loss L(yi, y′), while
changing wt−1 as little as possible. The number of
false ground atoms of coref predicate is selected as
loss function in our experiments. Hard global con-
straints (i.e. best-first clustering or transitivity con-
straint) must be satisfied when inferring the best y′
in each iteration, which can make learned weights
more effective.
</bodyText>
<sectionHeader confidence="0.999643" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999931">
In this section, we will first describe the dataset and
evaluation metrics we use. We will then present the
effect of our joint learning method, and finally dis-
cuss the comparison with the state of the art.
</bodyText>
<subsectionHeader confidence="0.996896">
4.1 Data Set
</subsectionHeader>
<bodyText confidence="0.9997554">
We use the dataset from the CoNLL-2011 shared
task, “Modeling Unrestricted Coreference in
OntoNotes” (Pradhan et al., 2011)2. It uses the En-
glish portion of the OntoNotes v4.0 corpus. There
are three important differences between OntoNotes
</bodyText>
<footnote confidence="0.802052">
2http://conll.cemantix.org/2011/
</footnote>
<bodyText confidence="0.999211863636363">
and another well-known coreference dataset from
ACE. First, OntoNotes does not label any singleton
entity cluster, which has only one reference in the
text. Second, only identity coreference is tagged in
OntoNotes, but not appositives or predicate nomi-
natives. Third, ACE only considers mentions which
belong to ACE entity types, whereas OntoNotes
considers more entity types. The shared task is to
automatically identify both entity coreference and
event coreference, although we only focus on entity
coreference in this paper. We don’t assume that
gold standard mention boundaries are given. So we
develop a heuristic method for mention detection.
See details in Section 2.1.
The training set consists of 1674 documents from
newswire, magazine articles, broadcast news, broad-
cast conversations and webpages, and the develop-
ment set consists of 202 documents from the same
source. For training set, there are 101264 mentions
from 26612 entities. And for development set, there
are 14291 mentions from 3752 entities (Pradhan et
al., 2011).
</bodyText>
<subsectionHeader confidence="0.978547">
4.2 Evaluation Metrics
</subsectionHeader>
<bodyText confidence="0.9999449">
We use the same evaluation metrics as used in
CoNLL-2011. Specifically, for mention detection,
we use precision, recall and the F-measure. A men-
tion is considered to be correct only if it matches
the exact same span of characters in the annotation
key. For coreference resolution, MUC (Vilain et al.,
1995), B-CUBED (Bagga and Baldwin, 1998) and
CEAF-E (Luo, 2005) are used for evaluation. The
unweighted average F score of them is used to com-
pare different systems.
</bodyText>
<subsectionHeader confidence="0.998968">
4.3 The Effect of Joint Learning
</subsectionHeader>
<bodyText confidence="0.999982666666667">
To assess the performance of our method, we set up
several variations of our system to compare with the
joint learning system. The MLN-Local system uses
only the local formulas described in Table 2 with-
out any global constraints under the MLN frame-
work. By default, the MLN-Local system uses the
single-link method to generate clustering results.
The MLN-Local+BF system replaces the single-link
method with best-first clustering to infer mention
clustering results after learning the weights for all
the local formulas. The MLN-Local+Trans sys-
tem replaces the best-first clustering with transitivity
</bodyText>
<page confidence="0.965382">
1250
</page>
<table confidence="0.998508714285714">
System Mention Detection MUC B-cube CEAF Avg
R P F R P F R P F R P F F
MLN-Local 62.52 74.75 68.09 56.07 65.55 60.44 65.67 72.95 69.12 45.55 37.19 40.95 56.84
MLN-Local+BF 65.74 73.2 69.27 56.79 64.08 60.22 65.71 74.18 69.69 47.29 40.53 43.65 57.85
MLN-Local+Trans 68.49 70.32 69.40 57.16 60.98 59.01 66.97 72.90 69.81 46.96 43.34 45.08 57.97
MLN-Joint(BF) 64.36 75.25 69.38 55.47 66.95 60.67 64.14 77.75 70.29 50.47 39.85 44.53 58.50
MLN-Joint(Trans) 64.46 75.37 69.49 55.48 67.15 60.76 64.00 78.11 70.36 50.63 39.84 44.60 58.57
</table>
<tableCaption confidence="0.999986">
Table 3: Comparison between different MLN-based systems, using 10-fold cross validation on the training dataset.
</tableCaption>
<bodyText confidence="0.9998415">
constraint. The MLN-Joint system is a joint model
for both pairwise classification and mention cluster-
ing. It can combine either best-first clustering or en-
forcing transitivity constraint with pairwise classifi-
cation, and we denote these two variants of MLN-
Joint as MLN-Joint(BF) and MLN-Joint(Trans) re-
spectively.
To compare the performance of the various sys-
tems above, we use 10-fold cross validation on
the training dataset. We empirically find that our
method has a fast convergence rate, to learn the
MLN model, we set the number of iterations to be
10.
The performance of these compared systems is
shown in Table 3. To provide some context for
the performance of this task, we report the median
average F-score of the official results of CoNLL-
2011, which is 50.12 (Pradhan et al., 2011). We can
see that MLN-Local achieves an average F-score of
56.84, which is well above the median score. When
adding best-first or transitivity constraint which
is independent of pairwise classification, MLN-
Local+BF and MLN-Local+Trans achieve better re-
sults of 57.85 and 57.97. Most of all, we can see
that the joint learning model (MLN-Joint(BF) or
MLN-Joint(Trans)) significantly outperforms inde-
pendent learning model (MLN-Local+BF or MLN-
Local+Trans) no matter whether best-first clustering
or transitivity constraint is used (based on a paired 2-
tailed t-test with p &lt; 0.05) with the score of 58.50
or 58.57, which shows the effectiveness of our pro-
posed joint learning method.
Best-first clustering and transitivity constraint
are very useful in Markov logic framework, and
both MLN-Local and MLN-Joint benefit from them.
For MLN-Joint, these two clustering methods re-
sult in similar performance. But actually, transi-
tivity is harder than best-first, because it signifi-
cantly increases the number of formulas for con-
straints and slows down the learning process. In
our experiments, we find that MLN-Joint(Trans)3 is
much slower than MLN-Joint(BF). Overall, MLN-
Joint(BF) has a good trade-off between effectiveness
and efficiency.
</bodyText>
<subsectionHeader confidence="0.997557">
4.4 Comparison with the State of the Art
</subsectionHeader>
<bodyText confidence="0.99956625">
In order to compare our method with the state-of-
the-art systems, we consider the following systems.
We implemented a traditional pairwise coreference
system using Maximum Entropy as the base classi-
fier and best-first clustering to link the results. We
used the same set of local features in MLN-Joint.
We refer to this system as MaxEnt+BF. To replace
best-first clustering with transitivity constraint, we
have another system named as MaxEnt+Trans. We
also consider the best 3 systems from CoNLL-2011
shared task. Chang’s system uses ILP to perform
best-first clustering after training a pairwise corefer-
ence model. Sapena’s system uses a relaxation label-
ing method to iteratively perform function optimiza-
tion for labeling each mention’s entity after learning
the weights for features under a C4.5 learner. Lee’s
system is a purely rule-based one. They use a battery
of sieves by precision (from highest to lowest) to it-
eratively choose antecedent for each mention. They
obtained the highest score in CoNLL-2011.
Table 4 shows the comparisons of our system with
the state-of-the-art systems on the development set
of CoNLL-2011. From the results, we can see that
our joint learning systems are obviously better than
</bodyText>
<footnote confidence="0.9185634">
3For MLN-Joint(Trans), not all training instances can be
learnt in a reasonable amount of time, so we set up a time out
threshold of 100 seconds. If the model cannot response in 100
seconds for some training instance, we remove it from the train-
ing set.
</footnote>
<page confidence="0.899057">
1251
</page>
<table confidence="0.998669111111111">
System Mention Detection MUC B-cube CEAF Avg
R P F R P F R P F R P F F
MLN-Joint(BF) 67.33 72.94 70.02 58.03 64.05 60.89 67.11 73.88 70.33 47.6 41.92 44.58 58.60
MLN-Joint(Trans) 67.28 72.88 69.97 58.00 64.10 60.90 67.12 74.13 70.45 47.70 41.96 44.65 58.67
MaxEnt+BF 60.54 76.64 67.64 52.20 68.52 59.26 60.85 80.15 69.18 51.6 37.05 43.13 57.19
MaxEnt+Trans 61.36 76.11 67.94 51.46 68.40 58.73 59.79 81.69 69.04 53.03 37.84 44.17 57.31
Lee’s System - - - 57.50 59.10 58.30 71.00 69.20 70.10 48.10 46.50 47.30 58.60
Sapena’s System 92.45 27.34 42.20 54.53 62.25 58.13 63.72 73.83 68.40 47.20 40.01 43.31 56.61
Chang’s System - - 64.69 - - 55.8 - - 69.29 - - 43.96 56.35
</table>
<tableCaption confidence="0.99994">
Table 4: Comparisons with state-of-the-art systems on the development dataset.
</tableCaption>
<bodyText confidence="0.682246428571428">
MaxEnt+BF and MaxEnt+Trans. They also out-
perform the learning-based systems of Sapena et al.
(2011) and Chang et al. (2011), and perform com-
petitively with Lee’s system (Lee et al., 2011). Note
that Lee’s system is purely rule-based, while our
methods are developed in a theoretically sound way,
i.e., Markov logic framework.
</bodyText>
<sectionHeader confidence="0.999778" genericHeader="method">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999966524590164">
Supervised noun phrase coreference resolution has
been extensively studied. Besides the mention-pair
model, two other commonly used models are the
entity-mention model (Luo et al., 2004; Yang et al.,
2008) and ranking models (Denis and Baldridge,
2008; Rahman and Ng, 2009). Interested readers
can refer to the literature review by Ng (2010).
Under the mention-pair model, Klenner (2007)
and Finkel and Manning (2008) applied Integer Lin-
ear Programming (ILP) to enforce transitivity on the
pairwise classification results. Chang et al. (2011)
used the same ILP technique to incorporate best-first
clustering and generate the mention clusters. In all
these studies, however, mention clustering is com-
bined with pairwise classification only at the infer-
ence stage but not at the learning stage.
To perform joint learning of pairwise classifi-
cation and mention clustering, in (McCallum and
Wellner, 2005), each mention pair corresponds to
a binary variable indicating whether the two men-
tions are coreferential, and the dependence between
these variables is modeled by conditional undirected
graphical models. Finley and Joachims (2005) pro-
posed a general SVM-based framework for super-
vised clustering that learns item-pair similarity mea-
sures, and applied the framework to noun phrase
coreference resolution. In our work, we take a differ-
ent approach and apply Markov logic. As we have
shown in Section 3, given the flexibility of Markov
logic, it is straightforward to perform joint learning
of pairwise classification and mention clustering.
In recent years, Markov logic has been widely
used in natural language processing problems (Poon
and Domingos, 2009; Yoshikawa et al., 2009; Che
and Liu, 2010). For coreference resolution, the most
notable one is unsupervised coreference resolution
by Poon and Domingos (2008). Poon and Domin-
gos (2008) followed the entity-mention model while
we follow the mention-pair model, which are quite
different approaches. To seek good performance in
an unsupervised way, Poon and Domingos (2008)
highly rely on two important strong indicators:
appositives and predicate nominatives. However,
OntoNotes corpus (state-of-art NLP data collection)
on coreference layer for CoNLL-2011 has excluded
these two conditions of annotations (appositives and
predicate nominatives) from their judging guide-
lines. Compared with it, our methods are more ap-
plicable for real dataset. Huang et al. (2009) used
Markov logic to predict coreference probabilities
for mention pairs followed by correlation cluster-
ing to generate the final results. Although they also
perform joint learning, at the inference stage, they
still make pairwise coreference decisions and clus-
ter mentions sequentially. Unlike their method, We
formulate the two steps into a single framework.
Besides combining pairwise classification and
mention clustering, there has also been some work
that jointly performs mention detection and coref-
erence resolution. Daum´e and Marcu (2005) de-
veloped such a model based on the Learning as
</bodyText>
<page confidence="0.980057">
1252
</page>
<bodyText confidence="0.999643714285714">
Search Optimization (LaSO) framework. Rahman
and Ng (2009) proposed to learn a cluster-ranker
for discourse-new mention detection jointly with
coreference resolution. Denis and Baldridge (2007)
adopted an Integer Linear Programming (ILP) for-
mulation for coreference resolution which models
anaphoricity and coreference as a joint task.
</bodyText>
<sectionHeader confidence="0.999179" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.998907857142857">
In this paper we present a joint learning method with
Markov logic which naturally combines pairwise
classification and mention clustering. Experimental
results show that the joint learning method signifi-
cantly outperforms baseline methods. Our method
is also better than all the learning-based systems in
CoNLL-2011 and reaches the same level of perfor-
mance with the best system.
In the future we will try to design more global
constraints and explore deeper relations between
training instances generation and mention cluster-
ing. We will also attempt to introduce more predi-
cates and transform structure learning techniques for
MLN into coreference problems.
</bodyText>
<sectionHeader confidence="0.997475" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999561">
Part of the work was done when the first author
was a visiting student in the Singapore Manage-
ment University. And this work was partially sup-
ported by the National High Technology Research
and Development Program of China(863 Program)
(No.2012AA011101), the National Natural Science
Foundation of China (No.91024009, No.60973053,
No.90920011), and the Specialized Research Fund
for the Doctoral Program of Higher Education of
China (Grant No. 20090001110047).
</bodyText>
<sectionHeader confidence="0.998532" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998974523809524">
Amit Bagga and Breck Baldwin. 1998. Algorithms for
scoring coreference chains. In The First International
Conference on Language Resources and Evaluation
Workshop on Linguistics Coreference, pages 563–566.
Eric Bengtson and Dan Roth. 2008. Understanding
the value of features for coreference resolution. In
EMNLP.
K. Chang, R. Samdani, A. Rozovskaya, N. Rizzolo,
M. Sammons, and D. Roth. 2011. Inference pro-
tocols for coreference resolution. In CoNLL Shared
Task, pages 40–44, Portland, Oregon, USA. Associa-
tion for Computational Linguistics.
Wanxiang Che and Ting Liu. 2010. Jointly modeling
wsd and srl with markov logic. In Chu-Ren Huang
and Dan Jurafsky, editors, COLING, pages 161–169.
Tsinghua University Press.
Koby Crammer and Yoram Singer. 2003. Ultraconserva-
tive online algorithms for multiclass problems. Jour-
nal of Machine Learning Research, 3:951–991.
III Hal Daum´e and Daniel Marcu. 2005. A large-scale
exploration of effective global features for a joint en-
tity detection and tracking model. In HLT ’05: Pro-
ceedings of the conference on Human Language Tech-
nology and Empirical Methods in Natural Language
Processing, pages 97–104, Morristown, NJ, USA. As-
sociation for Computational Linguistics.
Pascal Denis and Jason Baldridge. 2007. Joint determi-
nation of anaphoricity and coreference resolution us-
ing integer programming. In Human Language Tech-
nologies 2007: The Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics; Proceedings of the Main Conference, pages
236–243, Rochester, New York, April. Association for
Computational Linguistics.
Pascal Denis and Jason Baldridge. 2008. Specialized
models and ranking for coreference resolution. In
EMNLP, pages 660–669.
Jenny Rose Finkel and Christopher D. Manning. 2008.
Enforcing transitivity in coreference resolution. In
ACL (Short Papers), pages 45–48. The Association for
Computer Linguistics.
T. Finley and T. Joachims. 2005. Supervised clustering
with support vector machines. In International Con-
ference on Machine Learning (ICML), pages 217–224.
Shujian Huang, Yabing Zhang, Junsheng Zhou, and Jia-
jun Chen. 2009. Coreference resolution using markov
logic networks. In Proceedings of Computational Lin-
guistics and Intelligent Text Processing: 10th Interna-
tional Conference, CICLing 2009.
M. Klenner. 2007. Enforcing consistency on coreference
sets. In RANLP.
Heeyoung Lee, Yves Peirsman, Angel Chang, Nathanael
Chambers, Mihai Surdeanu, and Dan Jurafsky. 2011.
Stanford’s multi-pass sieve coreference resolution sys-
tem at the conll-2011 shared task. In Proceedings of
the Fifteenth Conference on Computational Natural
Language Learning: Shared Task, pages 28–34, Port-
land, Oregon, USA, June. Association for Computa-
tional Linguistics.
Xiaoqiang Luo, Abe Ittycheriah, Hongyan Jing, A Kamb-
hatla, and Salim Roukos. 2004. A mention-
synchronous coreference resolution algorithm based
on the bell tree. In Proc. of the ACL, pages 135–142.
</reference>
<page confidence="0.534923">
1253
</page>
<reference confidence="0.999883084507043">
Xiaoqiang Luo. 2005. On coreference resolution perfor-
mance metrics. In Proc. of HLT/EMNLP, pages 25–
32.
Andrew McCallum and Ben Wellner. 2005. Conditional
models of identity uncertainty with application to noun
coreference. In Advances in Neural Information Pro-
cessing Systems, pages 905–912. MIT Press.
J. McCarthy and W. Lehnert. 1995. Using decision
trees for coreference resolution. In Proceedings of the
14th International Joint Conference on Artificial Intel-
ligence.
Vincent Ng and Claire Cardie. 2002. Improving machine
learning approaches to coreference resolution. In Pro-
ceedings of the ACL, pages 104–111.
Vincent Ng. 2010. Supervised noun phrase coreference
research: The first fifteen years. In ACL, pages 1396–
1411. The Association for Computer Linguistics.
Hoifung Poon and Pedro Domingos. 2008. Joint unsu-
pervised coreference resolution with markov logic. In
EMNLP, pages 650–659.
Hoifung Poon and Pedro Domingos. 2009. Unsuper-
vised semantic parsing. In EMNLP, pages 1–10.
Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,
Martha Palmer, Ralph Weischedel, and Nianwen Xue.
2011. Conll-2011 shared task: Modeling unrestricted
coreference in ontonotes. In Proceedings of the Fif-
teenth Conference on Computational Natural Lan-
guage Learning: Shared Task, pages 1–27, Portland,
Oregon, USA, June. Association for Computational
Linguistics.
Altaf Rahman and Vincent Ng. 2009. Supervised models
for coreference resolution. In Proceedings of EMNLP,
pages 968–977.
Matthew Richardson and Pedro Domingos. 2006.
Markov logic networks. Machine Learning, 62(1-
2):107–136.
Sebastian Riedel. 2008. Improving the accuracy and ef-
ficiency of map inference for markov logic. In UAI,
pages 468–475. AUAI Press.
Emili Sapena, Lluis Padr´o, and Jordi Turmo. 2011. Re-
laxcor participation in conll shared task on coreference
resolution. In Proceedings of the Fifteenth Confer-
ence on Computational Natural Language Learning:
Shared Task, pages 35–39, Portland, Oregon, USA,
June. Association for Computational Linguistics.
Wee Meng Soon, Hwee Tou Ng, and Chung Yong Lim.
2001. A machine learning approach to coreference
resolution of noun phrases. Computational Linguis-
tics, 27(4):521–544.
Olga Uryupina, Sriparna Saha, Asif Ekbal, and Massimo
Poesio. 2011. Multi-metric optimization for coref-
erence: The unitn / iitp / essex submission to the 2011
conll shared task. In Proceedings of the Fifteenth Con-
ference on Computational Natural Language Learn-
ing: Shared Task, pages 61–65, Portland, Oregon,
USA, June. Association for Computational Linguis-
tics.
Marc B. Vilain, John D. Burger, John S. Aberdeen, Den-
nis Connolly, and Lynette Hirschman. 1995. A model-
theoretic coreference scoring scheme. In MUC, pages
45–52.
Xiaofeng Yang, Jian Su, Jun Lang, Chew Lim Tan, Ting
Liu, and Sheng Li. 2008. An entity-mention model for
coreference resolution with inductive logic program-
ming. In ACL, pages 843–851. The Association for
Computer Linguistics.
Katsumasa Yoshikawa, Sebastian Riedel, Masayuki Asa-
hara, and Yuji Matsumoto. 2009. Jointly identifying
temporal relations with markov logic. In ACL/AFNLP,
pages 405–413. The Association for Computer Lin-
guistics.
</reference>
<page confidence="0.993751">
1254
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.311463">
<title confidence="0.970936">Joint Learning for Coreference Resolution with Markov Logic</title>
<author confidence="0.940444">Jing Wayne Xin Sujian Houfeng</author>
<affiliation confidence="0.983000333333333">Laboratory of Computational Linguistics (Peking University) Ministry of of Information Systems, Singapore Management University, of Electronics Engineering and Computer Science, Peking University,</affiliation>
<email confidence="0.998232">lisujian,jingjiang@smu.edu.sg,batmanfly@gmail.com</email>
<abstract confidence="0.99729025">Pairwise coreference resolution models must merge pairwise coreference decisions to generate final outputs. Traditional merging methods adopt different strategies such as the bestfirst method and enforcing the transitivity constraint, but most of these methods are used independently of the pairwise learning methods as an isolated inference procedure at the end. We propose a joint learning model which combines pairwise classification and mention clustering with Markov logic. Experimental results show that our joint learning system outperforms independent learning systems. Our system gives a better performance than all the learning-based systems from the</abstract>
<note confidence="0.639721">CoNLL-2011 shared task on the same dataset. Compared with the best system from CoNLL- 2011, which employs a rule-based method, our system shows competitive performance.</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Amit Bagga</author>
<author>Breck Baldwin</author>
</authors>
<title>Algorithms for scoring coreference chains.</title>
<date>1998</date>
<booktitle>In The First International Conference on Language Resources and Evaluation Workshop on Linguistics Coreference,</booktitle>
<pages>563--566</pages>
<contexts>
<context position="22615" citStr="Bagga and Baldwin, 1998" startWordPosition="3482" endWordPosition="3485">t conversations and webpages, and the development set consists of 202 documents from the same source. For training set, there are 101264 mentions from 26612 entities. And for development set, there are 14291 mentions from 3752 entities (Pradhan et al., 2011). 4.2 Evaluation Metrics We use the same evaluation metrics as used in CoNLL-2011. Specifically, for mention detection, we use precision, recall and the F-measure. A mention is considered to be correct only if it matches the exact same span of characters in the annotation key. For coreference resolution, MUC (Vilain et al., 1995), B-CUBED (Bagga and Baldwin, 1998) and CEAF-E (Luo, 2005) are used for evaluation. The unweighted average F score of them is used to compare different systems. 4.3 The Effect of Joint Learning To assess the performance of our method, we set up several variations of our system to compare with the joint learning system. The MLN-Local system uses only the local formulas described in Table 2 without any global constraints under the MLN framework. By default, the MLN-Local system uses the single-link method to generate clustering results. The MLN-Local+BF system replaces the single-link method with best-first clustering to infer me</context>
</contexts>
<marker>Bagga, Baldwin, 1998</marker>
<rawString>Amit Bagga and Breck Baldwin. 1998. Algorithms for scoring coreference chains. In The First International Conference on Language Resources and Evaluation Workshop on Linguistics Coreference, pages 563–566.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Bengtson</author>
<author>Dan Roth</author>
</authors>
<title>Understanding the value of features for coreference resolution.</title>
<date>2008</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="1656" citStr="Bengtson and Roth, 2008" startWordPosition="233" endWordPosition="236">ing systems. Our system gives a better performance than all the learning-based systems from the CoNLL-2011 shared task on the same dataset. Compared with the best system from CoNLL2011, which employs a rule-based method, our system shows competitive performance. 1 Introduction The task of noun phrase coreference resolution is to determine which mentions in a text refer to the same real-world entity. Many methods have been proposed for this problem. Among them the mentionpair model (McCarthy and Lehnert, 1995) is one of the most influential ones and can achieve the stateof-the-art performance (Bengtson and Roth, 2008). The mention-pair model splits the task into three parts: mention detection, pairwise classification and mention clustering. Mention detection aims to identify anaphoric noun phrases, including proper nouns, common noun phrases and pronouns. Pairwise classification takes a pair of detected anaphoric noun phrase candidates and determines whether they refer to the same entity. Because these classification decisions are local, they do not guarantee that candidate mentions are partitioned into clusters. Therefore a mention clustering step is needed to resolve conflicts and generate the final ment</context>
<context position="4372" citStr="Bengtson and Roth (2008)" startWordPosition="637" endWordPosition="640"> Language Learning, pages 1245–1254, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics results, but it was used for unsupervised coreference resolution and the method was based on a different model, the entity-mention model. More specifically, to combine mention clustering with pairwise classification, we adopt the commonly used strategies (such as best-first clustering and transitivity constraint), and formulate them as first-order logic formulas under the Markov logic framework. Best-first clustering has been previously studied by Ng and Cardie (2002) and Bengtson and Roth (2008) and found to be effective. Transitivity constraint has been applied to coreference resolution by Klenner (2007) and Finkel and Manning (2008), and also achieved good performance. We evaluate Markov logic-based method on the dataset from CoNLL-2011 shared task. Our experiment results demonstrate the advantage of joint learning of pairwise classification and mention clustering over independent learning. We examine best-first clustering and transitivity constraint in our methods, and find that both are very useful for coreference resolution. Compared with the state of the art, our method outperf</context>
<context position="7112" citStr="Bengtson and Roth (2008)" startWordPosition="1048" endWordPosition="1051">. http://www.google.com), numbers (e.g. $9,000) and pleonastic “it” pronouns. 2.2 Pairwise Classification For pairwise classification, traditional learningbased methods usually adopt a classification model such as maximum entropy models and support vector machines. Training instances (i.e. positive and negative mention pairs) are constructed from known coreference chains, and features are defined to represent these instances. In this paper, we build a baseline system that uses maximum entropy models as the classification algorithm. For generation of training instances, we follow the method of Bengtson and Roth (2008). For each predicted mention m, we generate a positive mention pair between m and its closest preceding antecedent, and negative mention pairs by pairing m with each of its preceding predicted mentions which are not coreferential with m. To avoid having too many negative instances, we impose a maximum sentence distance between the two mentions when constructing mention pairs. This is based on the intuition that for each anaphoric mention, its preceding antecedent should appear quite near it, and most coreferential mention pairs which have a long sentence distance can be resolved using string m</context>
</contexts>
<marker>Bengtson, Roth, 2008</marker>
<rawString>Eric Bengtson and Dan Roth. 2008. Understanding the value of features for coreference resolution. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Chang</author>
<author>R Samdani</author>
<author>A Rozovskaya</author>
<author>N Rizzolo</author>
<author>M Sammons</author>
<author>D Roth</author>
</authors>
<title>Inference protocols for coreference resolution.</title>
<date>2011</date>
<booktitle>In CoNLL Shared Task,</booktitle>
<pages>40--44</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA.</location>
<contexts>
<context position="28438" citStr="Chang et al. (2011)" startWordPosition="4420" endWordPosition="4423">58.67 MaxEnt+BF 60.54 76.64 67.64 52.20 68.52 59.26 60.85 80.15 69.18 51.6 37.05 43.13 57.19 MaxEnt+Trans 61.36 76.11 67.94 51.46 68.40 58.73 59.79 81.69 69.04 53.03 37.84 44.17 57.31 Lee’s System - - - 57.50 59.10 58.30 71.00 69.20 70.10 48.10 46.50 47.30 58.60 Sapena’s System 92.45 27.34 42.20 54.53 62.25 58.13 63.72 73.83 68.40 47.20 40.01 43.31 56.61 Chang’s System - - 64.69 - - 55.8 - - 69.29 - - 43.96 56.35 Table 4: Comparisons with state-of-the-art systems on the development dataset. MaxEnt+BF and MaxEnt+Trans. They also outperform the learning-based systems of Sapena et al. (2011) and Chang et al. (2011), and perform competitively with Lee’s system (Lee et al., 2011). Note that Lee’s system is purely rule-based, while our methods are developed in a theoretically sound way, i.e., Markov logic framework. 5 Related Work Supervised noun phrase coreference resolution has been extensively studied. Besides the mention-pair model, two other commonly used models are the entity-mention model (Luo et al., 2004; Yang et al., 2008) and ranking models (Denis and Baldridge, 2008; Rahman and Ng, 2009). Interested readers can refer to the literature review by Ng (2010). Under the mention-pair model, Klenner (</context>
</contexts>
<marker>Chang, Samdani, Rozovskaya, Rizzolo, Sammons, Roth, 2011</marker>
<rawString>K. Chang, R. Samdani, A. Rozovskaya, N. Rizzolo, M. Sammons, and D. Roth. 2011. Inference protocols for coreference resolution. In CoNLL Shared Task, pages 40–44, Portland, Oregon, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wanxiang Che</author>
<author>Ting Liu</author>
</authors>
<title>Jointly modeling wsd and srl with markov logic.</title>
<date>2010</date>
<booktitle>In Chu-Ren Huang</booktitle>
<pages>161--169</pages>
<editor>and Dan Jurafsky, editors, COLING,</editor>
<publisher>Tsinghua University Press.</publisher>
<contexts>
<context position="30357" citStr="Che and Liu, 2010" startWordPosition="4714" endWordPosition="4717">ected graphical models. Finley and Joachims (2005) proposed a general SVM-based framework for supervised clustering that learns item-pair similarity measures, and applied the framework to noun phrase coreference resolution. In our work, we take a different approach and apply Markov logic. As we have shown in Section 3, given the flexibility of Markov logic, it is straightforward to perform joint learning of pairwise classification and mention clustering. In recent years, Markov logic has been widely used in natural language processing problems (Poon and Domingos, 2009; Yoshikawa et al., 2009; Che and Liu, 2010). For coreference resolution, the most notable one is unsupervised coreference resolution by Poon and Domingos (2008). Poon and Domingos (2008) followed the entity-mention model while we follow the mention-pair model, which are quite different approaches. To seek good performance in an unsupervised way, Poon and Domingos (2008) highly rely on two important strong indicators: appositives and predicate nominatives. However, OntoNotes corpus (state-of-art NLP data collection) on coreference layer for CoNLL-2011 has excluded these two conditions of annotations (appositives and predicate nominative</context>
</contexts>
<marker>Che, Liu, 2010</marker>
<rawString>Wanxiang Che and Ting Liu. 2010. Jointly modeling wsd and srl with markov logic. In Chu-Ren Huang and Dan Jurafsky, editors, COLING, pages 161–169. Tsinghua University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koby Crammer</author>
<author>Yoram Singer</author>
</authors>
<title>Ultraconservative online algorithms for multiclass problems.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--951</pages>
<contexts>
<context position="19633" citStr="Crammer and Singer, 2003" startWordPosition="2993" endWordPosition="2996">served ground atoms and y to represent the hidden ground atoms. Formally, we have y� = arg max p(y|x) ^_ arg max s(y, x), y y where ∑ s(y, x) = (ϕi,wi)∈M Each hidden ground atom can only takes a value of either 0 or 1. And global formulas should be satisfied as hard constraints when inferring the best P. So ∑wz fϕi C∈Cnϕi C (y,x). (6) 1249 the problem can be easily solved using ILP. Detailed introduction about transforming ground Markov networks in Markov logic into an ILP problem can be found in (Riedel, 2008). 3.4 Parameter Learning For parameter learning, we employ the online learner MIRA (Crammer and Singer, 2003), which establishes a large margin between the score of the gold solution and all wrong solutions to learn the weights. This is achieved by solving the quadratic program as follows min 11 wt − wt−1 11 . (7) s.t. s(yi, xi) − s(y′, xi) &gt; L(yi, y′) by′ =� yi, (yi, xi) E D Here D = {(yi, xi)}Ni�1 represents N training instances (each instance represents one single document in the dataset) and t represents the number of iterations. In our problem, we adopt 1-best MIRA, which means that in each iteration we try to find wt which can guarantee the difference between the right solution yi and the best </context>
</contexts>
<marker>Crammer, Singer, 2003</marker>
<rawString>Koby Crammer and Yoram Singer. 2003. Ultraconservative online algorithms for multiclass problems. Journal of Machine Learning Research, 3:951–991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
<author>Daniel Marcu</author>
</authors>
<title>A large-scale exploration of effective global features for a joint entity detection and tracking model.</title>
<date>2005</date>
<booktitle>In HLT ’05: Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing,</booktitle>
<pages>97--104</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<marker>Daum´e, Marcu, 2005</marker>
<rawString>III Hal Daum´e and Daniel Marcu. 2005. A large-scale exploration of effective global features for a joint entity detection and tracking model. In HLT ’05: Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, pages 97–104, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascal Denis</author>
<author>Jason Baldridge</author>
</authors>
<title>Joint determination of anaphoricity and coreference resolution using integer programming.</title>
<date>2007</date>
<booktitle>In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference,</booktitle>
<pages>236--243</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Rochester, New York,</location>
<contexts>
<context position="31874" citStr="Denis and Baldridge (2007)" startWordPosition="4932" endWordPosition="4935">nt learning, at the inference stage, they still make pairwise coreference decisions and cluster mentions sequentially. Unlike their method, We formulate the two steps into a single framework. Besides combining pairwise classification and mention clustering, there has also been some work that jointly performs mention detection and coreference resolution. Daum´e and Marcu (2005) developed such a model based on the Learning as 1252 Search Optimization (LaSO) framework. Rahman and Ng (2009) proposed to learn a cluster-ranker for discourse-new mention detection jointly with coreference resolution. Denis and Baldridge (2007) adopted an Integer Linear Programming (ILP) formulation for coreference resolution which models anaphoricity and coreference as a joint task. 6 Conclusion In this paper we present a joint learning method with Markov logic which naturally combines pairwise classification and mention clustering. Experimental results show that the joint learning method significantly outperforms baseline methods. Our method is also better than all the learning-based systems in CoNLL-2011 and reaches the same level of performance with the best system. In the future we will try to design more global constraints and</context>
</contexts>
<marker>Denis, Baldridge, 2007</marker>
<rawString>Pascal Denis and Jason Baldridge. 2007. Joint determination of anaphoricity and coreference resolution using integer programming. In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference, pages 236–243, Rochester, New York, April. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascal Denis</author>
<author>Jason Baldridge</author>
</authors>
<title>Specialized models and ranking for coreference resolution. In</title>
<date>2008</date>
<booktitle>EMNLP,</booktitle>
<pages>660--669</pages>
<contexts>
<context position="28907" citStr="Denis and Baldridge, 2008" startWordPosition="4492" endWordPosition="4495">tems on the development dataset. MaxEnt+BF and MaxEnt+Trans. They also outperform the learning-based systems of Sapena et al. (2011) and Chang et al. (2011), and perform competitively with Lee’s system (Lee et al., 2011). Note that Lee’s system is purely rule-based, while our methods are developed in a theoretically sound way, i.e., Markov logic framework. 5 Related Work Supervised noun phrase coreference resolution has been extensively studied. Besides the mention-pair model, two other commonly used models are the entity-mention model (Luo et al., 2004; Yang et al., 2008) and ranking models (Denis and Baldridge, 2008; Rahman and Ng, 2009). Interested readers can refer to the literature review by Ng (2010). Under the mention-pair model, Klenner (2007) and Finkel and Manning (2008) applied Integer Linear Programming (ILP) to enforce transitivity on the pairwise classification results. Chang et al. (2011) used the same ILP technique to incorporate best-first clustering and generate the mention clusters. In all these studies, however, mention clustering is combined with pairwise classification only at the inference stage but not at the learning stage. To perform joint learning of pairwise classification and m</context>
</contexts>
<marker>Denis, Baldridge, 2008</marker>
<rawString>Pascal Denis and Jason Baldridge. 2008. Specialized models and ranking for coreference resolution. In EMNLP, pages 660–669.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Christopher D Manning</author>
</authors>
<title>Enforcing transitivity in coreference resolution.</title>
<date>2008</date>
<booktitle>In ACL (Short Papers),</booktitle>
<pages>45--48</pages>
<institution>The Association for Computer Linguistics.</institution>
<contexts>
<context position="4514" citStr="Finkel and Manning (2008)" startWordPosition="658" endWordPosition="661">s used for unsupervised coreference resolution and the method was based on a different model, the entity-mention model. More specifically, to combine mention clustering with pairwise classification, we adopt the commonly used strategies (such as best-first clustering and transitivity constraint), and formulate them as first-order logic formulas under the Markov logic framework. Best-first clustering has been previously studied by Ng and Cardie (2002) and Bengtson and Roth (2008) and found to be effective. Transitivity constraint has been applied to coreference resolution by Klenner (2007) and Finkel and Manning (2008), and also achieved good performance. We evaluate Markov logic-based method on the dataset from CoNLL-2011 shared task. Our experiment results demonstrate the advantage of joint learning of pairwise classification and mention clustering over independent learning. We examine best-first clustering and transitivity constraint in our methods, and find that both are very useful for coreference resolution. Compared with the state of the art, our method outperforms a baseline that represents a typical system using the mention-pair model. Our method is also better than all learning systems from the Co</context>
<context position="29073" citStr="Finkel and Manning (2008)" startWordPosition="4518" endWordPosition="4521">orm competitively with Lee’s system (Lee et al., 2011). Note that Lee’s system is purely rule-based, while our methods are developed in a theoretically sound way, i.e., Markov logic framework. 5 Related Work Supervised noun phrase coreference resolution has been extensively studied. Besides the mention-pair model, two other commonly used models are the entity-mention model (Luo et al., 2004; Yang et al., 2008) and ranking models (Denis and Baldridge, 2008; Rahman and Ng, 2009). Interested readers can refer to the literature review by Ng (2010). Under the mention-pair model, Klenner (2007) and Finkel and Manning (2008) applied Integer Linear Programming (ILP) to enforce transitivity on the pairwise classification results. Chang et al. (2011) used the same ILP technique to incorporate best-first clustering and generate the mention clusters. In all these studies, however, mention clustering is combined with pairwise classification only at the inference stage but not at the learning stage. To perform joint learning of pairwise classification and mention clustering, in (McCallum and Wellner, 2005), each mention pair corresponds to a binary variable indicating whether the two mentions are coreferential, and the </context>
</contexts>
<marker>Finkel, Manning, 2008</marker>
<rawString>Jenny Rose Finkel and Christopher D. Manning. 2008. Enforcing transitivity in coreference resolution. In ACL (Short Papers), pages 45–48. The Association for Computer Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Finley</author>
<author>T Joachims</author>
</authors>
<title>Supervised clustering with support vector machines.</title>
<date>2005</date>
<booktitle>In International Conference on Machine Learning (ICML),</booktitle>
<pages>217--224</pages>
<contexts>
<context position="2881" citStr="Finley and Joachims (2005)" startWordPosition="422" endWordPosition="425">tion clusters. Much work has been done following the mentionpair model (Soon et al., 2001; Ng and Cardie, 2002). In most work, pairwise classification and mention clustering are done sequentially. A major weakness of this approach is that pairwise classification considers only local information, which may not be sufficient to make correct decisions. One way to address this weakness is to jointly learn the pairwise classification model and the mention clustering model. This idea has been explored to some extent by McCallum and Wellner (2005) using conditional undirected graphical models and by Finley and Joachims (2005) using an SVM-based supervised clustering method. In this paper, we study how to use a different learning framework, Markov logic (Richardson and Domingos, 2006), to learn a joint model for both pairwise classification and mention clustering under the mention-pair model. We choose Markov logic because of its appealing properties. Markov logic is based on first-order logic, which makes the learned models readily interpretable by humans. Moreover, joint learning is natural under the Markov logic framework, with local pairwise classification and global mention clustering both formulated as weight</context>
<context position="29789" citStr="Finley and Joachims (2005)" startWordPosition="4624" endWordPosition="4627">cation results. Chang et al. (2011) used the same ILP technique to incorporate best-first clustering and generate the mention clusters. In all these studies, however, mention clustering is combined with pairwise classification only at the inference stage but not at the learning stage. To perform joint learning of pairwise classification and mention clustering, in (McCallum and Wellner, 2005), each mention pair corresponds to a binary variable indicating whether the two mentions are coreferential, and the dependence between these variables is modeled by conditional undirected graphical models. Finley and Joachims (2005) proposed a general SVM-based framework for supervised clustering that learns item-pair similarity measures, and applied the framework to noun phrase coreference resolution. In our work, we take a different approach and apply Markov logic. As we have shown in Section 3, given the flexibility of Markov logic, it is straightforward to perform joint learning of pairwise classification and mention clustering. In recent years, Markov logic has been widely used in natural language processing problems (Poon and Domingos, 2009; Yoshikawa et al., 2009; Che and Liu, 2010). For coreference resolution, th</context>
</contexts>
<marker>Finley, Joachims, 2005</marker>
<rawString>T. Finley and T. Joachims. 2005. Supervised clustering with support vector machines. In International Conference on Machine Learning (ICML), pages 217–224.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shujian Huang</author>
<author>Yabing Zhang</author>
<author>Junsheng Zhou</author>
<author>Jiajun Chen</author>
</authors>
<title>Coreference resolution using markov logic networks.</title>
<date>2009</date>
<booktitle>In Proceedings of Computational Linguistics and Intelligent Text Processing: 10th International Conference, CICLing</booktitle>
<contexts>
<context position="31078" citStr="Huang et al. (2009)" startWordPosition="4817" endWordPosition="4820">Domingos (2008). Poon and Domingos (2008) followed the entity-mention model while we follow the mention-pair model, which are quite different approaches. To seek good performance in an unsupervised way, Poon and Domingos (2008) highly rely on two important strong indicators: appositives and predicate nominatives. However, OntoNotes corpus (state-of-art NLP data collection) on coreference layer for CoNLL-2011 has excluded these two conditions of annotations (appositives and predicate nominatives) from their judging guidelines. Compared with it, our methods are more applicable for real dataset. Huang et al. (2009) used Markov logic to predict coreference probabilities for mention pairs followed by correlation clustering to generate the final results. Although they also perform joint learning, at the inference stage, they still make pairwise coreference decisions and cluster mentions sequentially. Unlike their method, We formulate the two steps into a single framework. Besides combining pairwise classification and mention clustering, there has also been some work that jointly performs mention detection and coreference resolution. Daum´e and Marcu (2005) developed such a model based on the Learning as 12</context>
</contexts>
<marker>Huang, Zhang, Zhou, Chen, 2009</marker>
<rawString>Shujian Huang, Yabing Zhang, Junsheng Zhou, and Jiajun Chen. 2009. Coreference resolution using markov logic networks. In Proceedings of Computational Linguistics and Intelligent Text Processing: 10th International Conference, CICLing 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Klenner</author>
</authors>
<title>Enforcing consistency on coreference sets.</title>
<date>2007</date>
<booktitle>In RANLP.</booktitle>
<contexts>
<context position="4484" citStr="Klenner (2007)" startWordPosition="655" endWordPosition="656"> results, but it was used for unsupervised coreference resolution and the method was based on a different model, the entity-mention model. More specifically, to combine mention clustering with pairwise classification, we adopt the commonly used strategies (such as best-first clustering and transitivity constraint), and formulate them as first-order logic formulas under the Markov logic framework. Best-first clustering has been previously studied by Ng and Cardie (2002) and Bengtson and Roth (2008) and found to be effective. Transitivity constraint has been applied to coreference resolution by Klenner (2007) and Finkel and Manning (2008), and also achieved good performance. We evaluate Markov logic-based method on the dataset from CoNLL-2011 shared task. Our experiment results demonstrate the advantage of joint learning of pairwise classification and mention clustering over independent learning. We examine best-first clustering and transitivity constraint in our methods, and find that both are very useful for coreference resolution. Compared with the state of the art, our method outperforms a baseline that represents a typical system using the mention-pair model. Our method is also better than al</context>
<context position="29043" citStr="Klenner (2007)" startWordPosition="4515" endWordPosition="4516">l. (2011), and perform competitively with Lee’s system (Lee et al., 2011). Note that Lee’s system is purely rule-based, while our methods are developed in a theoretically sound way, i.e., Markov logic framework. 5 Related Work Supervised noun phrase coreference resolution has been extensively studied. Besides the mention-pair model, two other commonly used models are the entity-mention model (Luo et al., 2004; Yang et al., 2008) and ranking models (Denis and Baldridge, 2008; Rahman and Ng, 2009). Interested readers can refer to the literature review by Ng (2010). Under the mention-pair model, Klenner (2007) and Finkel and Manning (2008) applied Integer Linear Programming (ILP) to enforce transitivity on the pairwise classification results. Chang et al. (2011) used the same ILP technique to incorporate best-first clustering and generate the mention clusters. In all these studies, however, mention clustering is combined with pairwise classification only at the inference stage but not at the learning stage. To perform joint learning of pairwise classification and mention clustering, in (McCallum and Wellner, 2005), each mention pair corresponds to a binary variable indicating whether the two mentio</context>
</contexts>
<marker>Klenner, 2007</marker>
<rawString>M. Klenner. 2007. Enforcing consistency on coreference sets. In RANLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heeyoung Lee</author>
<author>Yves Peirsman</author>
<author>Angel Chang</author>
<author>Nathanael Chambers</author>
<author>Mihai Surdeanu</author>
<author>Dan Jurafsky</author>
</authors>
<title>Stanford’s multi-pass sieve coreference resolution system at the conll-2011 shared task.</title>
<date>2011</date>
<booktitle>In Proceedings of the Fifteenth Conference on Computational Natural Language Learning: Shared Task,</booktitle>
<pages>28--34</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="6364" citStr="Lee et al. (2011)" startWordPosition="940" endWordPosition="943">on, pairwise classification and mention clustering. 2.1 Mention Detection For mention detection, traditional methods include learning-based and rule-based methods. Which kind of method to choose depends on specific dataset. In this paper, we first consider all the noun phrases in the given text as candidate mentions. Without gold standard mention boundaries, we use a well-known preprocessing tool from Stanford’s NLP group1 to extract noun phrases. After obtaining all the extracted noun phrases, we also use a rule-based method to remove some erroneous candidates based on previous studies (e.g. Lee et al. (2011), Uryupina et al. (2011)). Some examples of these erroneous candidates include stop words (e.g. uh, hmm), web addresses (e.g. http://www.google.com), numbers (e.g. $9,000) and pleonastic “it” pronouns. 2.2 Pairwise Classification For pairwise classification, traditional learningbased methods usually adopt a classification model such as maximum entropy models and support vector machines. Training instances (i.e. positive and negative mention pairs) are constructed from known coreference chains, and features are defined to represent these instances. In this paper, we build a baseline system that</context>
<context position="28502" citStr="Lee et al., 2011" startWordPosition="4431" endWordPosition="4434">.18 51.6 37.05 43.13 57.19 MaxEnt+Trans 61.36 76.11 67.94 51.46 68.40 58.73 59.79 81.69 69.04 53.03 37.84 44.17 57.31 Lee’s System - - - 57.50 59.10 58.30 71.00 69.20 70.10 48.10 46.50 47.30 58.60 Sapena’s System 92.45 27.34 42.20 54.53 62.25 58.13 63.72 73.83 68.40 47.20 40.01 43.31 56.61 Chang’s System - - 64.69 - - 55.8 - - 69.29 - - 43.96 56.35 Table 4: Comparisons with state-of-the-art systems on the development dataset. MaxEnt+BF and MaxEnt+Trans. They also outperform the learning-based systems of Sapena et al. (2011) and Chang et al. (2011), and perform competitively with Lee’s system (Lee et al., 2011). Note that Lee’s system is purely rule-based, while our methods are developed in a theoretically sound way, i.e., Markov logic framework. 5 Related Work Supervised noun phrase coreference resolution has been extensively studied. Besides the mention-pair model, two other commonly used models are the entity-mention model (Luo et al., 2004; Yang et al., 2008) and ranking models (Denis and Baldridge, 2008; Rahman and Ng, 2009). Interested readers can refer to the literature review by Ng (2010). Under the mention-pair model, Klenner (2007) and Finkel and Manning (2008) applied Integer Linear Progr</context>
</contexts>
<marker>Lee, Peirsman, Chang, Chambers, Surdeanu, Jurafsky, 2011</marker>
<rawString>Heeyoung Lee, Yves Peirsman, Angel Chang, Nathanael Chambers, Mihai Surdeanu, and Dan Jurafsky. 2011. Stanford’s multi-pass sieve coreference resolution system at the conll-2011 shared task. In Proceedings of the Fifteenth Conference on Computational Natural Language Learning: Shared Task, pages 28–34, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaoqiang Luo</author>
<author>Abe Ittycheriah</author>
<author>Hongyan Jing</author>
<author>A Kambhatla</author>
<author>Salim Roukos</author>
</authors>
<title>A mentionsynchronous coreference resolution algorithm based on the bell tree.</title>
<date>2004</date>
<booktitle>In Proc. of the ACL,</booktitle>
<pages>135--142</pages>
<contexts>
<context position="28841" citStr="Luo et al., 2004" startWordPosition="4481" endWordPosition="4484">3.96 56.35 Table 4: Comparisons with state-of-the-art systems on the development dataset. MaxEnt+BF and MaxEnt+Trans. They also outperform the learning-based systems of Sapena et al. (2011) and Chang et al. (2011), and perform competitively with Lee’s system (Lee et al., 2011). Note that Lee’s system is purely rule-based, while our methods are developed in a theoretically sound way, i.e., Markov logic framework. 5 Related Work Supervised noun phrase coreference resolution has been extensively studied. Besides the mention-pair model, two other commonly used models are the entity-mention model (Luo et al., 2004; Yang et al., 2008) and ranking models (Denis and Baldridge, 2008; Rahman and Ng, 2009). Interested readers can refer to the literature review by Ng (2010). Under the mention-pair model, Klenner (2007) and Finkel and Manning (2008) applied Integer Linear Programming (ILP) to enforce transitivity on the pairwise classification results. Chang et al. (2011) used the same ILP technique to incorporate best-first clustering and generate the mention clusters. In all these studies, however, mention clustering is combined with pairwise classification only at the inference stage but not at the learning</context>
</contexts>
<marker>Luo, Ittycheriah, Jing, Kambhatla, Roukos, 2004</marker>
<rawString>Xiaoqiang Luo, Abe Ittycheriah, Hongyan Jing, A Kambhatla, and Salim Roukos. 2004. A mentionsynchronous coreference resolution algorithm based on the bell tree. In Proc. of the ACL, pages 135–142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaoqiang Luo</author>
</authors>
<title>On coreference resolution performance metrics.</title>
<date>2005</date>
<booktitle>In Proc. of HLT/EMNLP,</booktitle>
<pages>25--32</pages>
<contexts>
<context position="22638" citStr="Luo, 2005" startWordPosition="3488" endWordPosition="3489"> development set consists of 202 documents from the same source. For training set, there are 101264 mentions from 26612 entities. And for development set, there are 14291 mentions from 3752 entities (Pradhan et al., 2011). 4.2 Evaluation Metrics We use the same evaluation metrics as used in CoNLL-2011. Specifically, for mention detection, we use precision, recall and the F-measure. A mention is considered to be correct only if it matches the exact same span of characters in the annotation key. For coreference resolution, MUC (Vilain et al., 1995), B-CUBED (Bagga and Baldwin, 1998) and CEAF-E (Luo, 2005) are used for evaluation. The unweighted average F score of them is used to compare different systems. 4.3 The Effect of Joint Learning To assess the performance of our method, we set up several variations of our system to compare with the joint learning system. The MLN-Local system uses only the local formulas described in Table 2 without any global constraints under the MLN framework. By default, the MLN-Local system uses the single-link method to generate clustering results. The MLN-Local+BF system replaces the single-link method with best-first clustering to infer mention clustering result</context>
</contexts>
<marker>Luo, 2005</marker>
<rawString>Xiaoqiang Luo. 2005. On coreference resolution performance metrics. In Proc. of HLT/EMNLP, pages 25– 32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew McCallum</author>
<author>Ben Wellner</author>
</authors>
<title>Conditional models of identity uncertainty with application to noun coreference.</title>
<date>2005</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>905--912</pages>
<publisher>MIT Press.</publisher>
<contexts>
<context position="2801" citStr="McCallum and Wellner (2005)" startWordPosition="410" endWordPosition="413">mention clustering step is needed to resolve conflicts and generate the final mention clusters. Much work has been done following the mentionpair model (Soon et al., 2001; Ng and Cardie, 2002). In most work, pairwise classification and mention clustering are done sequentially. A major weakness of this approach is that pairwise classification considers only local information, which may not be sufficient to make correct decisions. One way to address this weakness is to jointly learn the pairwise classification model and the mention clustering model. This idea has been explored to some extent by McCallum and Wellner (2005) using conditional undirected graphical models and by Finley and Joachims (2005) using an SVM-based supervised clustering method. In this paper, we study how to use a different learning framework, Markov logic (Richardson and Domingos, 2006), to learn a joint model for both pairwise classification and mention clustering under the mention-pair model. We choose Markov logic because of its appealing properties. Markov logic is based on first-order logic, which makes the learned models readily interpretable by humans. Moreover, joint learning is natural under the Markov logic framework, with local</context>
<context position="29557" citStr="McCallum and Wellner, 2005" startWordPosition="4591" endWordPosition="4594"> Interested readers can refer to the literature review by Ng (2010). Under the mention-pair model, Klenner (2007) and Finkel and Manning (2008) applied Integer Linear Programming (ILP) to enforce transitivity on the pairwise classification results. Chang et al. (2011) used the same ILP technique to incorporate best-first clustering and generate the mention clusters. In all these studies, however, mention clustering is combined with pairwise classification only at the inference stage but not at the learning stage. To perform joint learning of pairwise classification and mention clustering, in (McCallum and Wellner, 2005), each mention pair corresponds to a binary variable indicating whether the two mentions are coreferential, and the dependence between these variables is modeled by conditional undirected graphical models. Finley and Joachims (2005) proposed a general SVM-based framework for supervised clustering that learns item-pair similarity measures, and applied the framework to noun phrase coreference resolution. In our work, we take a different approach and apply Markov logic. As we have shown in Section 3, given the flexibility of Markov logic, it is straightforward to perform joint learning of pairwis</context>
</contexts>
<marker>McCallum, Wellner, 2005</marker>
<rawString>Andrew McCallum and Ben Wellner. 2005. Conditional models of identity uncertainty with application to noun coreference. In Advances in Neural Information Processing Systems, pages 905–912. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J McCarthy</author>
<author>W Lehnert</author>
</authors>
<title>Using decision trees for coreference resolution.</title>
<date>1995</date>
<booktitle>In Proceedings of the 14th International Joint Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="1546" citStr="McCarthy and Lehnert, 1995" startWordPosition="215" endWordPosition="218">stering with Markov logic. Experimental results show that our joint learning system outperforms independent learning systems. Our system gives a better performance than all the learning-based systems from the CoNLL-2011 shared task on the same dataset. Compared with the best system from CoNLL2011, which employs a rule-based method, our system shows competitive performance. 1 Introduction The task of noun phrase coreference resolution is to determine which mentions in a text refer to the same real-world entity. Many methods have been proposed for this problem. Among them the mentionpair model (McCarthy and Lehnert, 1995) is one of the most influential ones and can achieve the stateof-the-art performance (Bengtson and Roth, 2008). The mention-pair model splits the task into three parts: mention detection, pairwise classification and mention clustering. Mention detection aims to identify anaphoric noun phrases, including proper nouns, common noun phrases and pronouns. Pairwise classification takes a pair of detected anaphoric noun phrase candidates and determines whether they refer to the same entity. Because these classification decisions are local, they do not guarantee that candidate mentions are partitioned</context>
</contexts>
<marker>McCarthy, Lehnert, 1995</marker>
<rawString>J. McCarthy and W. Lehnert. 1995. Using decision trees for coreference resolution. In Proceedings of the 14th International Joint Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vincent Ng</author>
<author>Claire Cardie</author>
</authors>
<title>Improving machine learning approaches to coreference resolution.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL,</booktitle>
<pages>104--111</pages>
<contexts>
<context position="2366" citStr="Ng and Cardie, 2002" startWordPosition="341" endWordPosition="344">ssification and mention clustering. Mention detection aims to identify anaphoric noun phrases, including proper nouns, common noun phrases and pronouns. Pairwise classification takes a pair of detected anaphoric noun phrase candidates and determines whether they refer to the same entity. Because these classification decisions are local, they do not guarantee that candidate mentions are partitioned into clusters. Therefore a mention clustering step is needed to resolve conflicts and generate the final mention clusters. Much work has been done following the mentionpair model (Soon et al., 2001; Ng and Cardie, 2002). In most work, pairwise classification and mention clustering are done sequentially. A major weakness of this approach is that pairwise classification considers only local information, which may not be sufficient to make correct decisions. One way to address this weakness is to jointly learn the pairwise classification model and the mention clustering model. This idea has been explored to some extent by McCallum and Wellner (2005) using conditional undirected graphical models and by Finley and Joachims (2005) using an SVM-based supervised clustering method. In this paper, we study how to use </context>
<context position="4343" citStr="Ng and Cardie (2002)" startWordPosition="632" endWordPosition="635">and Computational Natural Language Learning, pages 1245–1254, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics results, but it was used for unsupervised coreference resolution and the method was based on a different model, the entity-mention model. More specifically, to combine mention clustering with pairwise classification, we adopt the commonly used strategies (such as best-first clustering and transitivity constraint), and formulate them as first-order logic formulas under the Markov logic framework. Best-first clustering has been previously studied by Ng and Cardie (2002) and Bengtson and Roth (2008) and found to be effective. Transitivity constraint has been applied to coreference resolution by Klenner (2007) and Finkel and Manning (2008), and also achieved good performance. We evaluate Markov logic-based method on the dataset from CoNLL-2011 shared task. Our experiment results demonstrate the advantage of joint learning of pairwise classification and mention clustering over independent learning. We examine best-first clustering and transitivity constraint in our methods, and find that both are very useful for coreference resolution. Compared with the state o</context>
<context position="15000" citStr="Ng and Cardie (2002)" startWordPosition="2301" endWordPosition="2304">y combine pairwise classification with mention clustering via local and global formulas in the Markov logic framework, which is the essence of “joint learning” in our work. 3.2.1 Local Formulas A local formula relates any observed predicates to exactly one hidden predicate. For our problem, we define a list of observed predicates to describe the properties of individual candidate mentions and the relations between two candidate mentions, shown in Table 1. For our problem, we have only one hidden predicate, i.e. coref. Most of our local formulas are from existing work (e.g. Soon et al. (2001), Ng and Cardie (2002), Sapena et al. (2011)). They are listed in Table 2, where the symbol “+” indicates that for every value of the variable preceding “+” there is a separate weight for the corresponding formula. 3.2.2 Global Formulas Global formulas are designed to add global constraints for hidden predicates. Since in our problem there is only one hidden predicate, i.e. coref, our global formulas incorporate correlations among different ground atoms of the coref predicates. Next we will show the best-first and transitivity global constraints. Note that we treat them as hard constraints so we do not set any weig</context>
</contexts>
<marker>Ng, Cardie, 2002</marker>
<rawString>Vincent Ng and Claire Cardie. 2002. Improving machine learning approaches to coreference resolution. In Proceedings of the ACL, pages 104–111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vincent Ng</author>
</authors>
<title>Supervised noun phrase coreference research: The first fifteen years.</title>
<date>2010</date>
<booktitle>In ACL,</booktitle>
<pages>1396--1411</pages>
<institution>The Association for Computer Linguistics.</institution>
<contexts>
<context position="28997" citStr="Ng (2010)" startWordPosition="4509" endWordPosition="4510">ms of Sapena et al. (2011) and Chang et al. (2011), and perform competitively with Lee’s system (Lee et al., 2011). Note that Lee’s system is purely rule-based, while our methods are developed in a theoretically sound way, i.e., Markov logic framework. 5 Related Work Supervised noun phrase coreference resolution has been extensively studied. Besides the mention-pair model, two other commonly used models are the entity-mention model (Luo et al., 2004; Yang et al., 2008) and ranking models (Denis and Baldridge, 2008; Rahman and Ng, 2009). Interested readers can refer to the literature review by Ng (2010). Under the mention-pair model, Klenner (2007) and Finkel and Manning (2008) applied Integer Linear Programming (ILP) to enforce transitivity on the pairwise classification results. Chang et al. (2011) used the same ILP technique to incorporate best-first clustering and generate the mention clusters. In all these studies, however, mention clustering is combined with pairwise classification only at the inference stage but not at the learning stage. To perform joint learning of pairwise classification and mention clustering, in (McCallum and Wellner, 2005), each mention pair corresponds to a bin</context>
</contexts>
<marker>Ng, 2010</marker>
<rawString>Vincent Ng. 2010. Supervised noun phrase coreference research: The first fifteen years. In ACL, pages 1396– 1411. The Association for Computer Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hoifung Poon</author>
<author>Pedro Domingos</author>
</authors>
<title>Joint unsupervised coreference resolution with markov logic.</title>
<date>2008</date>
<booktitle>In EMNLP,</booktitle>
<pages>650--659</pages>
<contexts>
<context position="3579" citStr="Poon and Domingos (2008)" startWordPosition="526" endWordPosition="529">how to use a different learning framework, Markov logic (Richardson and Domingos, 2006), to learn a joint model for both pairwise classification and mention clustering under the mention-pair model. We choose Markov logic because of its appealing properties. Markov logic is based on first-order logic, which makes the learned models readily interpretable by humans. Moreover, joint learning is natural under the Markov logic framework, with local pairwise classification and global mention clustering both formulated as weighted first-order clauses. In fact, Markov logic has been previously used by Poon and Domingos (2008) for coreference resolution and achieved good 1245 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1245–1254, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics results, but it was used for unsupervised coreference resolution and the method was based on a different model, the entity-mention model. More specifically, to combine mention clustering with pairwise classification, we adopt the commonly used strategies (such as best-first clustering and transitivity const</context>
<context position="30474" citStr="Poon and Domingos (2008)" startWordPosition="4730" endWordPosition="4733">ring that learns item-pair similarity measures, and applied the framework to noun phrase coreference resolution. In our work, we take a different approach and apply Markov logic. As we have shown in Section 3, given the flexibility of Markov logic, it is straightforward to perform joint learning of pairwise classification and mention clustering. In recent years, Markov logic has been widely used in natural language processing problems (Poon and Domingos, 2009; Yoshikawa et al., 2009; Che and Liu, 2010). For coreference resolution, the most notable one is unsupervised coreference resolution by Poon and Domingos (2008). Poon and Domingos (2008) followed the entity-mention model while we follow the mention-pair model, which are quite different approaches. To seek good performance in an unsupervised way, Poon and Domingos (2008) highly rely on two important strong indicators: appositives and predicate nominatives. However, OntoNotes corpus (state-of-art NLP data collection) on coreference layer for CoNLL-2011 has excluded these two conditions of annotations (appositives and predicate nominatives) from their judging guidelines. Compared with it, our methods are more applicable for real dataset. Huang et al. (2</context>
</contexts>
<marker>Poon, Domingos, 2008</marker>
<rawString>Hoifung Poon and Pedro Domingos. 2008. Joint unsupervised coreference resolution with markov logic. In EMNLP, pages 650–659.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hoifung Poon</author>
<author>Pedro Domingos</author>
</authors>
<title>Unsupervised semantic parsing.</title>
<date>2009</date>
<booktitle>In EMNLP,</booktitle>
<pages>1--10</pages>
<contexts>
<context position="30313" citStr="Poon and Domingos, 2009" startWordPosition="4706" endWordPosition="4709">n these variables is modeled by conditional undirected graphical models. Finley and Joachims (2005) proposed a general SVM-based framework for supervised clustering that learns item-pair similarity measures, and applied the framework to noun phrase coreference resolution. In our work, we take a different approach and apply Markov logic. As we have shown in Section 3, given the flexibility of Markov logic, it is straightforward to perform joint learning of pairwise classification and mention clustering. In recent years, Markov logic has been widely used in natural language processing problems (Poon and Domingos, 2009; Yoshikawa et al., 2009; Che and Liu, 2010). For coreference resolution, the most notable one is unsupervised coreference resolution by Poon and Domingos (2008). Poon and Domingos (2008) followed the entity-mention model while we follow the mention-pair model, which are quite different approaches. To seek good performance in an unsupervised way, Poon and Domingos (2008) highly rely on two important strong indicators: appositives and predicate nominatives. However, OntoNotes corpus (state-of-art NLP data collection) on coreference layer for CoNLL-2011 has excluded these two conditions of annot</context>
</contexts>
<marker>Poon, Domingos, 2009</marker>
<rawString>Hoifung Poon and Pedro Domingos. 2009. Unsupervised semantic parsing. In EMNLP, pages 1–10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Pradhan</author>
<author>Lance Ramshaw</author>
<author>Mitchell Marcus</author>
<author>Martha Palmer</author>
<author>Ralph Weischedel</author>
<author>Nianwen Xue</author>
</authors>
<title>Conll-2011 shared task: Modeling unrestricted coreference in ontonotes.</title>
<date>2011</date>
<booktitle>In Proceedings of the Fifteenth Conference on Computational Natural Language Learning: Shared Task,</booktitle>
<pages>1--27</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="21061" citStr="Pradhan et al., 2011" startWordPosition="3245" endWordPosition="3248">toms of coref predicate is selected as loss function in our experiments. Hard global constraints (i.e. best-first clustering or transitivity constraint) must be satisfied when inferring the best y′ in each iteration, which can make learned weights more effective. 4 Experiments In this section, we will first describe the dataset and evaluation metrics we use. We will then present the effect of our joint learning method, and finally discuss the comparison with the state of the art. 4.1 Data Set We use the dataset from the CoNLL-2011 shared task, “Modeling Unrestricted Coreference in OntoNotes” (Pradhan et al., 2011)2. It uses the English portion of the OntoNotes v4.0 corpus. There are three important differences between OntoNotes 2http://conll.cemantix.org/2011/ and another well-known coreference dataset from ACE. First, OntoNotes does not label any singleton entity cluster, which has only one reference in the text. Second, only identity coreference is tagged in OntoNotes, but not appositives or predicate nominatives. Third, ACE only considers mentions which belong to ACE entity types, whereas OntoNotes considers more entity types. The shared task is to automatically identify both entity coreference and </context>
<context position="24816" citStr="Pradhan et al., 2011" startWordPosition="3838" endWordPosition="3841">vity constraint with pairwise classification, and we denote these two variants of MLNJoint as MLN-Joint(BF) and MLN-Joint(Trans) respectively. To compare the performance of the various systems above, we use 10-fold cross validation on the training dataset. We empirically find that our method has a fast convergence rate, to learn the MLN model, we set the number of iterations to be 10. The performance of these compared systems is shown in Table 3. To provide some context for the performance of this task, we report the median average F-score of the official results of CoNLL2011, which is 50.12 (Pradhan et al., 2011). We can see that MLN-Local achieves an average F-score of 56.84, which is well above the median score. When adding best-first or transitivity constraint which is independent of pairwise classification, MLNLocal+BF and MLN-Local+Trans achieve better results of 57.85 and 57.97. Most of all, we can see that the joint learning model (MLN-Joint(BF) or MLN-Joint(Trans)) significantly outperforms independent learning model (MLN-Local+BF or MLNLocal+Trans) no matter whether best-first clustering or transitivity constraint is used (based on a paired 2- tailed t-test with p &lt; 0.05) with the score of 58</context>
</contexts>
<marker>Pradhan, Ramshaw, Marcus, Palmer, Weischedel, Xue, 2011</marker>
<rawString>Sameer Pradhan, Lance Ramshaw, Mitchell Marcus, Martha Palmer, Ralph Weischedel, and Nianwen Xue. 2011. Conll-2011 shared task: Modeling unrestricted coreference in ontonotes. In Proceedings of the Fifteenth Conference on Computational Natural Language Learning: Shared Task, pages 1–27, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Altaf Rahman</author>
<author>Vincent Ng</author>
</authors>
<title>Supervised models for coreference resolution.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>968--977</pages>
<contexts>
<context position="28929" citStr="Rahman and Ng, 2009" startWordPosition="4496" endWordPosition="4499">aset. MaxEnt+BF and MaxEnt+Trans. They also outperform the learning-based systems of Sapena et al. (2011) and Chang et al. (2011), and perform competitively with Lee’s system (Lee et al., 2011). Note that Lee’s system is purely rule-based, while our methods are developed in a theoretically sound way, i.e., Markov logic framework. 5 Related Work Supervised noun phrase coreference resolution has been extensively studied. Besides the mention-pair model, two other commonly used models are the entity-mention model (Luo et al., 2004; Yang et al., 2008) and ranking models (Denis and Baldridge, 2008; Rahman and Ng, 2009). Interested readers can refer to the literature review by Ng (2010). Under the mention-pair model, Klenner (2007) and Finkel and Manning (2008) applied Integer Linear Programming (ILP) to enforce transitivity on the pairwise classification results. Chang et al. (2011) used the same ILP technique to incorporate best-first clustering and generate the mention clusters. In all these studies, however, mention clustering is combined with pairwise classification only at the inference stage but not at the learning stage. To perform joint learning of pairwise classification and mention clustering, in </context>
<context position="31739" citStr="Rahman and Ng (2009)" startWordPosition="4915" endWordPosition="4918"> probabilities for mention pairs followed by correlation clustering to generate the final results. Although they also perform joint learning, at the inference stage, they still make pairwise coreference decisions and cluster mentions sequentially. Unlike their method, We formulate the two steps into a single framework. Besides combining pairwise classification and mention clustering, there has also been some work that jointly performs mention detection and coreference resolution. Daum´e and Marcu (2005) developed such a model based on the Learning as 1252 Search Optimization (LaSO) framework. Rahman and Ng (2009) proposed to learn a cluster-ranker for discourse-new mention detection jointly with coreference resolution. Denis and Baldridge (2007) adopted an Integer Linear Programming (ILP) formulation for coreference resolution which models anaphoricity and coreference as a joint task. 6 Conclusion In this paper we present a joint learning method with Markov logic which naturally combines pairwise classification and mention clustering. Experimental results show that the joint learning method significantly outperforms baseline methods. Our method is also better than all the learning-based systems in CoN</context>
</contexts>
<marker>Rahman, Ng, 2009</marker>
<rawString>Altaf Rahman and Vincent Ng. 2009. Supervised models for coreference resolution. In Proceedings of EMNLP, pages 968–977.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Richardson</author>
<author>Pedro Domingos</author>
</authors>
<title>Markov logic networks.</title>
<date>2006</date>
<booktitle>Machine Learning,</booktitle>
<pages>62--1</pages>
<contexts>
<context position="3042" citStr="Richardson and Domingos, 2006" startWordPosition="447" endWordPosition="450">ntion clustering are done sequentially. A major weakness of this approach is that pairwise classification considers only local information, which may not be sufficient to make correct decisions. One way to address this weakness is to jointly learn the pairwise classification model and the mention clustering model. This idea has been explored to some extent by McCallum and Wellner (2005) using conditional undirected graphical models and by Finley and Joachims (2005) using an SVM-based supervised clustering method. In this paper, we study how to use a different learning framework, Markov logic (Richardson and Domingos, 2006), to learn a joint model for both pairwise classification and mention clustering under the mention-pair model. We choose Markov logic because of its appealing properties. Markov logic is based on first-order logic, which makes the learned models readily interpretable by humans. Moreover, joint learning is natural under the Markov logic framework, with local pairwise classification and global mention clustering both formulated as weighted first-order clauses. In fact, Markov logic has been previously used by Poon and Domingos (2008) for coreference resolution and achieved good 1245 Proceedings </context>
<context position="10044" citStr="Richardson and Domingos, 2006" startWordPosition="1495" endWordPosition="1498">as described in Section 2. In what follows, we will first describe the basic Markov logic networks (MLN) framework, and then introduce the first-order logic formulas we use in our MLN including local formulas and global formulas which perform pairwise classification and mention clustering respectively. Through this way, these two isolated parts are combined together, and joint learning and inference can be performed in a single framework. Finally we present inference and parameter learning methods. 3.1 Markov Logic Networks Markov logic networks combine Markov networks with first-order logic (Richardson and Domingos, 2006; Riedel, 2008). A Markov logic network consists of a set of first-order clauses (which we will refer to as formulas in the rest of the paper) just like in first-order logic. However, different from first-order logic where a formula represents a hard constraint, in an MLN, these constraints are softened and they can be violated with some penalty. An MLN M is therefore a set of weighted formulas {(ϕZ, wZ)}Z, where ϕZ is a first order formula and wZ is the penalty (the formula’s weight). These weighted formulas define a probability distribution over sets of ground atoms or so-called possible wor</context>
</contexts>
<marker>Richardson, Domingos, 2006</marker>
<rawString>Matthew Richardson and Pedro Domingos. 2006. Markov logic networks. Machine Learning, 62(1-2):107–136.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Riedel</author>
</authors>
<title>Improving the accuracy and efficiency of map inference for markov logic.</title>
<date>2008</date>
<booktitle>In UAI,</booktitle>
<pages>468--475</pages>
<publisher>AUAI Press.</publisher>
<contexts>
<context position="10059" citStr="Riedel, 2008" startWordPosition="1499" endWordPosition="1500">hat follows, we will first describe the basic Markov logic networks (MLN) framework, and then introduce the first-order logic formulas we use in our MLN including local formulas and global formulas which perform pairwise classification and mention clustering respectively. Through this way, these two isolated parts are combined together, and joint learning and inference can be performed in a single framework. Finally we present inference and parameter learning methods. 3.1 Markov Logic Networks Markov logic networks combine Markov networks with first-order logic (Richardson and Domingos, 2006; Riedel, 2008). A Markov logic network consists of a set of first-order clauses (which we will refer to as formulas in the rest of the paper) just like in first-order logic. However, different from first-order logic where a formula represents a hard constraint, in an MLN, these constraints are softened and they can be violated with some penalty. An MLN M is therefore a set of weighted formulas {(ϕZ, wZ)}Z, where ϕZ is a first order formula and wZ is the penalty (the formula’s weight). These weighted formulas define a probability distribution over sets of ground atoms or so-called possible worlds. Let y deno</context>
<context position="19524" citStr="Riedel, 2008" startWordPosition="2979" endWordPosition="2980">jective is to maximize a posteriori probability as follows. Here we use x to represent all the observed ground atoms and y to represent the hidden ground atoms. Formally, we have y� = arg max p(y|x) ^_ arg max s(y, x), y y where ∑ s(y, x) = (ϕi,wi)∈M Each hidden ground atom can only takes a value of either 0 or 1. And global formulas should be satisfied as hard constraints when inferring the best P. So ∑wz fϕi C∈Cnϕi C (y,x). (6) 1249 the problem can be easily solved using ILP. Detailed introduction about transforming ground Markov networks in Markov logic into an ILP problem can be found in (Riedel, 2008). 3.4 Parameter Learning For parameter learning, we employ the online learner MIRA (Crammer and Singer, 2003), which establishes a large margin between the score of the gold solution and all wrong solutions to learn the weights. This is achieved by solving the quadratic program as follows min 11 wt − wt−1 11 . (7) s.t. s(yi, xi) − s(y′, xi) &gt; L(yi, y′) by′ =� yi, (yi, xi) E D Here D = {(yi, xi)}Ni�1 represents N training instances (each instance represents one single document in the dataset) and t represents the number of iterations. In our problem, we adopt 1-best MIRA, which means that in ea</context>
</contexts>
<marker>Riedel, 2008</marker>
<rawString>Sebastian Riedel. 2008. Improving the accuracy and efficiency of map inference for markov logic. In UAI, pages 468–475. AUAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emili Sapena</author>
<author>Lluis Padr´o</author>
<author>Jordi Turmo</author>
</authors>
<title>Relaxcor participation in conll shared task on coreference resolution.</title>
<date>2011</date>
<booktitle>In Proceedings of the Fifteenth Conference on Computational Natural Language Learning: Shared Task,</booktitle>
<pages>35--39</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<marker>Sapena, Padr´o, Turmo, 2011</marker>
<rawString>Emili Sapena, Lluis Padr´o, and Jordi Turmo. 2011. Relaxcor participation in conll shared task on coreference resolution. In Proceedings of the Fifteenth Conference on Computational Natural Language Learning: Shared Task, pages 35–39, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wee Meng Soon</author>
<author>Hwee Tou Ng</author>
<author>Chung Yong Lim</author>
</authors>
<title>A machine learning approach to coreference resolution of noun phrases.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>4</issue>
<contexts>
<context position="2344" citStr="Soon et al., 2001" startWordPosition="337" endWordPosition="340">ction, pairwise classification and mention clustering. Mention detection aims to identify anaphoric noun phrases, including proper nouns, common noun phrases and pronouns. Pairwise classification takes a pair of detected anaphoric noun phrase candidates and determines whether they refer to the same entity. Because these classification decisions are local, they do not guarantee that candidate mentions are partitioned into clusters. Therefore a mention clustering step is needed to resolve conflicts and generate the final mention clusters. Much work has been done following the mentionpair model (Soon et al., 2001; Ng and Cardie, 2002). In most work, pairwise classification and mention clustering are done sequentially. A major weakness of this approach is that pairwise classification considers only local information, which may not be sufficient to make correct decisions. One way to address this weakness is to jointly learn the pairwise classification model and the mention clustering model. This idea has been explored to some extent by McCallum and Wellner (2005) using conditional undirected graphical models and by Finley and Joachims (2005) using an SVM-based supervised clustering method. In this paper</context>
<context position="14978" citStr="Soon et al. (2001)" startWordPosition="2297" endWordPosition="2300">straint. We naturally combine pairwise classification with mention clustering via local and global formulas in the Markov logic framework, which is the essence of “joint learning” in our work. 3.2.1 Local Formulas A local formula relates any observed predicates to exactly one hidden predicate. For our problem, we define a list of observed predicates to describe the properties of individual candidate mentions and the relations between two candidate mentions, shown in Table 1. For our problem, we have only one hidden predicate, i.e. coref. Most of our local formulas are from existing work (e.g. Soon et al. (2001), Ng and Cardie (2002), Sapena et al. (2011)). They are listed in Table 2, where the symbol “+” indicates that for every value of the variable preceding “+” there is a separate weight for the corresponding formula. 3.2.2 Global Formulas Global formulas are designed to add global constraints for hidden predicates. Since in our problem there is only one hidden predicate, i.e. coref, our global formulas incorporate correlations among different ground atoms of the coref predicates. Next we will show the best-first and transitivity global constraints. Note that we treat them as hard constraints so </context>
</contexts>
<marker>Soon, Ng, Lim, 2001</marker>
<rawString>Wee Meng Soon, Hwee Tou Ng, and Chung Yong Lim. 2001. A machine learning approach to coreference resolution of noun phrases. Computational Linguistics, 27(4):521–544.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olga Uryupina</author>
<author>Sriparna Saha</author>
<author>Asif Ekbal</author>
<author>Massimo Poesio</author>
</authors>
<title>Multi-metric optimization for coreference: The unitn / iitp / essex submission to the 2011 conll shared task.</title>
<date>2011</date>
<booktitle>In Proceedings of the Fifteenth Conference on Computational Natural Language Learning: Shared Task,</booktitle>
<pages>61--65</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="6388" citStr="Uryupina et al. (2011)" startWordPosition="944" endWordPosition="947">fication and mention clustering. 2.1 Mention Detection For mention detection, traditional methods include learning-based and rule-based methods. Which kind of method to choose depends on specific dataset. In this paper, we first consider all the noun phrases in the given text as candidate mentions. Without gold standard mention boundaries, we use a well-known preprocessing tool from Stanford’s NLP group1 to extract noun phrases. After obtaining all the extracted noun phrases, we also use a rule-based method to remove some erroneous candidates based on previous studies (e.g. Lee et al. (2011), Uryupina et al. (2011)). Some examples of these erroneous candidates include stop words (e.g. uh, hmm), web addresses (e.g. http://www.google.com), numbers (e.g. $9,000) and pleonastic “it” pronouns. 2.2 Pairwise Classification For pairwise classification, traditional learningbased methods usually adopt a classification model such as maximum entropy models and support vector machines. Training instances (i.e. positive and negative mention pairs) are constructed from known coreference chains, and features are defined to represent these instances. In this paper, we build a baseline system that uses maximum entropy mo</context>
</contexts>
<marker>Uryupina, Saha, Ekbal, Poesio, 2011</marker>
<rawString>Olga Uryupina, Sriparna Saha, Asif Ekbal, and Massimo Poesio. 2011. Multi-metric optimization for coreference: The unitn / iitp / essex submission to the 2011 conll shared task. In Proceedings of the Fifteenth Conference on Computational Natural Language Learning: Shared Task, pages 61–65, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc B Vilain</author>
<author>John D Burger</author>
<author>John S Aberdeen</author>
<author>Dennis Connolly</author>
<author>Lynette Hirschman</author>
</authors>
<title>A modeltheoretic coreference scoring scheme.</title>
<date>1995</date>
<booktitle>In MUC,</booktitle>
<pages>45--52</pages>
<contexts>
<context position="22580" citStr="Vilain et al., 1995" startWordPosition="3477" endWordPosition="3480">icles, broadcast news, broadcast conversations and webpages, and the development set consists of 202 documents from the same source. For training set, there are 101264 mentions from 26612 entities. And for development set, there are 14291 mentions from 3752 entities (Pradhan et al., 2011). 4.2 Evaluation Metrics We use the same evaluation metrics as used in CoNLL-2011. Specifically, for mention detection, we use precision, recall and the F-measure. A mention is considered to be correct only if it matches the exact same span of characters in the annotation key. For coreference resolution, MUC (Vilain et al., 1995), B-CUBED (Bagga and Baldwin, 1998) and CEAF-E (Luo, 2005) are used for evaluation. The unweighted average F score of them is used to compare different systems. 4.3 The Effect of Joint Learning To assess the performance of our method, we set up several variations of our system to compare with the joint learning system. The MLN-Local system uses only the local formulas described in Table 2 without any global constraints under the MLN framework. By default, the MLN-Local system uses the single-link method to generate clustering results. The MLN-Local+BF system replaces the single-link method wit</context>
</contexts>
<marker>Vilain, Burger, Aberdeen, Connolly, Hirschman, 1995</marker>
<rawString>Marc B. Vilain, John D. Burger, John S. Aberdeen, Dennis Connolly, and Lynette Hirschman. 1995. A modeltheoretic coreference scoring scheme. In MUC, pages 45–52.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaofeng Yang</author>
<author>Jian Su</author>
<author>Jun Lang</author>
<author>Chew Lim Tan</author>
<author>Ting Liu</author>
<author>Sheng Li</author>
</authors>
<title>An entity-mention model for coreference resolution with inductive logic programming.</title>
<date>2008</date>
<booktitle>In ACL,</booktitle>
<pages>843--851</pages>
<institution>The Association for Computer Linguistics.</institution>
<contexts>
<context position="28861" citStr="Yang et al., 2008" startWordPosition="4485" endWordPosition="4488">: Comparisons with state-of-the-art systems on the development dataset. MaxEnt+BF and MaxEnt+Trans. They also outperform the learning-based systems of Sapena et al. (2011) and Chang et al. (2011), and perform competitively with Lee’s system (Lee et al., 2011). Note that Lee’s system is purely rule-based, while our methods are developed in a theoretically sound way, i.e., Markov logic framework. 5 Related Work Supervised noun phrase coreference resolution has been extensively studied. Besides the mention-pair model, two other commonly used models are the entity-mention model (Luo et al., 2004; Yang et al., 2008) and ranking models (Denis and Baldridge, 2008; Rahman and Ng, 2009). Interested readers can refer to the literature review by Ng (2010). Under the mention-pair model, Klenner (2007) and Finkel and Manning (2008) applied Integer Linear Programming (ILP) to enforce transitivity on the pairwise classification results. Chang et al. (2011) used the same ILP technique to incorporate best-first clustering and generate the mention clusters. In all these studies, however, mention clustering is combined with pairwise classification only at the inference stage but not at the learning stage. To perform j</context>
</contexts>
<marker>Yang, Su, Lang, Tan, Liu, Li, 2008</marker>
<rawString>Xiaofeng Yang, Jian Su, Jun Lang, Chew Lim Tan, Ting Liu, and Sheng Li. 2008. An entity-mention model for coreference resolution with inductive logic programming. In ACL, pages 843–851. The Association for Computer Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katsumasa Yoshikawa</author>
<author>Sebastian Riedel</author>
<author>Masayuki Asahara</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Jointly identifying temporal relations with markov logic.</title>
<date>2009</date>
<booktitle>In ACL/AFNLP,</booktitle>
<pages>405--413</pages>
<institution>The Association for Computer Linguistics.</institution>
<contexts>
<context position="30337" citStr="Yoshikawa et al., 2009" startWordPosition="4710" endWordPosition="4713">led by conditional undirected graphical models. Finley and Joachims (2005) proposed a general SVM-based framework for supervised clustering that learns item-pair similarity measures, and applied the framework to noun phrase coreference resolution. In our work, we take a different approach and apply Markov logic. As we have shown in Section 3, given the flexibility of Markov logic, it is straightforward to perform joint learning of pairwise classification and mention clustering. In recent years, Markov logic has been widely used in natural language processing problems (Poon and Domingos, 2009; Yoshikawa et al., 2009; Che and Liu, 2010). For coreference resolution, the most notable one is unsupervised coreference resolution by Poon and Domingos (2008). Poon and Domingos (2008) followed the entity-mention model while we follow the mention-pair model, which are quite different approaches. To seek good performance in an unsupervised way, Poon and Domingos (2008) highly rely on two important strong indicators: appositives and predicate nominatives. However, OntoNotes corpus (state-of-art NLP data collection) on coreference layer for CoNLL-2011 has excluded these two conditions of annotations (appositives and </context>
</contexts>
<marker>Yoshikawa, Riedel, Asahara, Matsumoto, 2009</marker>
<rawString>Katsumasa Yoshikawa, Sebastian Riedel, Masayuki Asahara, and Yuji Matsumoto. 2009. Jointly identifying temporal relations with markov logic. In ACL/AFNLP, pages 405–413. The Association for Computer Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
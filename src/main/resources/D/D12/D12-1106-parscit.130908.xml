<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000122">
<title confidence="0.994187">
A coherence model based on syntactic patterns
</title>
<author confidence="0.979377">
Annie Louis Ani Nenkova
</author>
<affiliation confidence="0.822445">
University of Pennsylvania University of Pennsylvania
Philadelphia, PA 19104, USA Philadelphia, PA 19104, USA
</affiliation>
<email confidence="0.999188">
lannie@seas.upenn.edu nenkova@seas.upenn.edu
</email>
<sectionHeader confidence="0.995643" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999882375">
We introduce a model of coherence which
captures the intentional discourse structure in
text. Our work is based on the hypothesis that
syntax provides a proxy for the communica-
tive goal of a sentence and therefore the se-
quence of sentences in a coherent discourse
should exhibit detectable structural patterns.
Results show that our method has high dis-
criminating power for separating out coherent
and incoherent news articles reaching accura-
cies of up to 90%. We also show that our syn-
tactic patterns are correlated with manual an-
notations of intentional structure for academic
conference articles and can successfully pre-
dict the coherence of abstract, introduction
and related work sections of these articles.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999945291666667">
Recent studies have introduced successful automatic
methods to predict the structure and coherence of
texts. They include entity approaches for local co-
herence which track the repetition and syntactic re-
alization of entities in adjacent sentences (Barzilay
and Lapata, 2008; Elsner and Charniak, 2008) and
content approaches for global coherence which view
texts as a sequence of topics, each characterized by a
particular distribution of lexical items (Barzilay and
Lee, 2004; Fung and Ngai, 2006). Other work has
shown that co-occurrence of words (Lapata, 2003;
Soricut and Marcu, 2006) and discourse relations
(Pitler and Nenkova, 2008; Lin et al., 2011) also pre-
dict coherence.
Early theories (Grosz and Sidner, 1986) posited
that there are three factors which collectively con-
tribute to coherence: intentional structure (purpose
of discourse), attentional structure (what items are
discussed) and the organization of discourse seg-
ments. The highly successful entity approaches cap-
ture attentional structure and content approaches are
related to topic segments but intentional structure
has largely been neglected. Every discourse has a
purpose: explaining a concept, narrating an event,
critiquing an idea and so on. As a result each sen-
tence in the article has a communicative goal and the
sequence of goals helps the author achieve the dis-
course purpose. In this work, we introduce a model
to capture coherence from the intentional structure
dimension. Our key proposal is that syntactic pat-
terns are a useful proxy for intentional structure.
This idea is motivated from the fact that cer-
tain sentence types such as questions and definitions
have distinguishable and unique syntactic structure.
For example, consider the opening sentences of two
descriptive articles1 shown in Table 1. Sentences
(1a) and (2a) are typical instances of definition sen-
tences. Definitions are written with the concept to
be defined expressed as a noun phrase followed by
a copular verb (is/are). The predicate contains two
parts: the first is a noun phrase reporting the concept
as part of a larger class (eg. an aqueduct is a water
supply), the second component is a relative clause
listing unique properties of the concept. These are
examples of syntactic patterns related to the com-
municative goals of individual sentences. Similarly,
sentences (1b) and (2b) which provide further de-
tails about the concept also have some distinguish-
</bodyText>
<footnote confidence="0.963692">
1Wikipedia articles on “Aqueduct” and “Cytokine Recep-
tors”
</footnote>
<page confidence="0.888475">
1157
</page>
<note confidence="0.891200571428571">
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1157–1168, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics
1a) An aqueduct is a water supply or navigable channel
constructed to convey water.
b) In modern engineering, the term is used for any system
of pipes, canals, tunnels, and other structures used for
this purpose.
</note>
<tableCaption confidence="0.824365666666667">
2a) Cytokine receptors are receptors that binds cytokines.
b) In recent years, the cytokine receptors have come to
demand more attention because their deficiency has now been
directly linked to certain debilitating immunodeficiency states.
Table 1: The first two sentences of two descriptive arti-
cles
</tableCaption>
<bodyText confidence="0.9986977">
ing syntactic features such as the presence of a top-
icalized phrase providing the focus of the sentence.
The two sets of sentences have similar sequence of
communicative goals and so we can expect the syn-
tax of adjacent sentences to also be related.
We aim to characterize this relationship on a
broad scale using a coherence model based entirely
on syntax. The model relies on two assumptions
which summarize our intuitions about syntax and in-
tentional structure:
</bodyText>
<listItem confidence="0.9719552">
1. Sentences with similar syntax are likely to have
the same communicative goal.
2. Regularities in intentional structure will be
manifested in syntactic regularities between ad-
jacent sentences.
</listItem>
<bodyText confidence="0.999924404761905">
There is also evidence from recent work that sup-
ports these assumptions. Cheung and Penn (2010)
find that a better syntactic parse of a sentence can be
derived when the syntax of adjacent sentences is also
taken into account. Lin et al. (2009) report that the
syntactic productions in adjacent sentences are pow-
erful features for predicting which discourse relation
(cause, contrast, etc.) holds between them. Cocco et
al. (2011) show that significant associations exist be-
tween certain part of speech tags and sentence types
such as explanation, dialog and argumentation.
In our model, syntax is represented either as parse
tree productions or a sequence of phrasal nodes aug-
mented with part of speech tags. Our best perform-
ing method uses a Hidden Markov Model to learn
the patterns in these syntactic items. Sections 3 and
5 discuss the representations and their specific im-
plementations and relative advantages. Results show
that syntax models can distinguish coherent and in-
coherent news articles from two domains with 75-
90% accuracies over a 50% baseline. In addition,
the syntax coherence scores turn out complementary
to scores given by lexical and entity models.
We also study our models’ predictions on aca-
demic articles, a genre where intentional structure
is widely studied. Sections in these articles have
well-defined purposes and we find recurring sen-
tence types such as motivation, citations, descrip-
tion, and speculations. There is a large body of work
(Swales, 1990; Teufel et al., 1999; Liakata et al.,
2010) concerned with defining and annotating these
sentence types (called zones) in conference articles.
In Section 6, we describe how indeed some patterns
captured by the syntax-based models are correlated
with zone categories that were proposed in prior lit-
erature. We also present results on coherence pre-
diction: our model can distinguish the introduction
section of conference papers from its perturbed ver-
sions with over 70% accuracy. Further, our model
is able to identify conference from workshop papers
with good accuracies, given that we can expect these
articles to vary in purpose.
</bodyText>
<sectionHeader confidence="0.992697" genericHeader="method">
2 Evidence for syntactic coherence
</sectionHeader>
<bodyText confidence="0.988814083333333">
We first present a pilot study that confirms that ad-
jacent sentences in discourse exhibit stable patterns
of syntactic co-occurrence. This study validates our
second assumption relating the syntax of adjacent
sentences. Later in Section 6, we examine syntac-
tic patterns in individual sentences (assumption 1)
using a corpus of academic articles where sentences
were manually annotated with communicative goals.
Prior work has reported that certain grammatical
productions are repeated in adjacent sentences more
often than would be expected by chance (Reitter et
al., 2006; Cheung and Penn, 2010). We analyze all
co-occurrence patterns rather than just repetitions.
We use the gold standard parse trees from the
Penn Treebank (Marcus et al., 1994). Our unit of
analysis is a pair of adjacent sentences (S1, S2) and
we choose to use Section 0 of the corpus which has
99 documents and 1727 sentence pairs. We enumer-
ate all productions that appear in the syntactic parse
of any sentence and exclude those that appear less
than 25 times, resulting in a list of 197 unique pro-
ductions. Then all ordered pairs2 (P1, P2) of pro-
ductions are formed. For each pair, we compute
2(p1, p2) and (p2, p1) are considered as different pairs.
</bodyText>
<page confidence="0.731169">
1158
</page>
<equation confidence="0.935904">
P1, P2
NP —. NP NP-ADV
QP —. CD CD
VP —. VB VP
NP-SBJ —. NNP NNP
NP-LOC —. NNP
S-TPC-1 —. NP-SBJ VP
Sentence 1
</equation>
<bodyText confidence="0.999383857142857">
The two concerns said they entered into a definitive
merger agreement under which Ratners will begin a tender
offer for all of Weisfield’s common shares for [$57.50 each]NP.
“The refund pool may not [be held hostage through another”
round of appeals]VP,” Judge Curry said.
“It has to be considered as an additional risk for the investor,”
said Gary P. Smaby of Smaby Group Inc., [Minneapolis]NP-LOC.
</bodyText>
<subsectionHeader confidence="0.796015">
Sentence 2
</subsectionHeader>
<bodyText confidence="0.970478142857143">
Also on the takeover front, Jaguar’s ADRs rose
1/4 to 13 7/8 on turnover of [4.4 million]QP.
[Commonwealth Edison]NP-SBJ said it is already
appealing the underlying commission order and
is considering appealing Judge Curry’s order.
[“Cray Computer will be a concept”
“stock,”]S-TPC-1 he said.
</bodyText>
<tableCaption confidence="0.979998">
Table 2: Example sentences for preferred production sequences. The span of the LHS of the corresponding production
is indicated by [] braces.
</tableCaption>
<bodyText confidence="0.998494432432432">
the following: c(p1p2) = number of sentence pairs
where p1 E S1 and p2 E S2; c(p1-,p2) = num-
ber of pairs where p1 E S1 and p2 E� S2; c(-,p1p2)
and c(-,p1-,p2) are computed similarly. Then we
perform a chi-square test to understand if the ob-
served count c(p1p2) is significantly (95% confi-
dence level) greater or lesser than the expected value
if occurrences of p1 and p2 were independent.
Of the 38,809 production pairs, we found that
1,168 pairs occurred in consecutive sentences sig-
nificantly more often than chance and 172 appeared
significantly fewer times than expected. In Table 2
we list, grouped in three simple categories, the 25
pairs of the first kind with most significant p-values.
Some of the preferred pairs are indeed repetitions
as pointed out by prior work. But they form only a
small fraction (5%) of the total preferred production
pairs indicating that there are several other classes
of syntactic regularities beyond priming. Some of
these other sequences can be explained by the fact
that these articles come from the finance domain:
they involve productions containing numbers and
quantities. An example for this type is shown in Ta-
ble 2. Finally, there is also a class that is not repe-
titions or readily observed as domain-specific. The
most frequent one reflects a pattern where the first
sentence introduces a subject and predicate and the
subject in the second sentence is pronominalized.
Examples for two other patterns are given in Table
2. For the sequence (VP --+ VB VP  |NP-SBJ --+ NNP
NNP), a bare verb is present in S1 and is often asso-
ciated with modals. In the corpus, these statements
often present hypothesis or speculation. The follow-
ing sentence S2 has an entity, a person or organiza-
tion, giving an explanation or opinion on the state-
ment. This pattern roughly correponds to a SPECU-
LATE followed by ENDORSE sequence of intentions.
</bodyText>
<equation confidence="0.998634310344827">
p1 p2 c(p1p2)
— Repetition —
VP —. VBD SBAR VP —. VBD SBAR 83
QP—.$CD CD QP—.$CDCD 18
NP —. $ CD -NONE- NP —. $ CD -NONE- 16
NP —. QP -NONE- NP —. QP -NONE- 15
NP-ADV —. DT NN NP-ADV —. DT NN 10
NP —. NP NP-ADV NP —. NP NP-ADV 7
— Quantities/Amounts —
NP —. QP -NONE- QP —. $ CD CD 16
QP —. $ CD CD NP —. QP -NONE- 15
NP —. NP NP-ADV NP —. QP -NONE- 11
NP-ADV —. DT NN NP —. QP -NONE- 11
NP —. NP NP-ADV NP-ADV — .DT NN 9
NP —. $ CD -NONE- NP-ADV —. DT NN 8
NP-ADV —. DT NN NP —. $ CD -NONE- 8
NP-ADV —. DT NN NP —. NP NP-ADV 8
NP —. NP NP-ADV QP —. CD CD 6
— Other —
S —. NP-SBJ VP NP-SBJ —. PRP 290
VP —. VBD SBAR PP-TMP —. IN NP 79
S —. NP-SBJ-1 VP VP —. VBD SBAR 43
VP — .VBD NP VP —. VBD VP 31
VP —. VB VP NP-SBJ —. NNP NNP 27
NP-SBJ-1 —. NNP NNP VP —. VBD NP 13
VP —. VBZ NP S —. PP-TMP, NP-SBJ VP . 8
NP-SBJ —. JJ NNS VP —. VBP NP 8
NP-PRD —. NP PP NP-PRD —. NP SBAR 7
NP-LOC —. NNP S-TPC-1 —. NP-SBJ VP 6
</equation>
<tableCaption confidence="0.98543">
Table 3: Top patterns in productions from WSJ
</tableCaption>
<bodyText confidence="0.999057272727273">
Similarly, in all the six adjacent sentence pairs from
our corpus containing the items (NP-LOC ---, NNP  |S-
TPC-1 --+ NP-SBJ VP), p1 introduces a location name,
and is often associated with the title of a person or
organization. The next sentence has a quote from
that person, where the quotation forms the topical-
ized clause in p2. Here the intentional structure is
INTRODUCE X / STATEMENT BY X.
In the remainder of the paper we formalize our
representation of syntax and the derived model of
coherence and test its efficacy in three domains.
</bodyText>
<sectionHeader confidence="0.985566" genericHeader="method">
3 Coherence models using syntax
</sectionHeader>
<bodyText confidence="0.770182142857143">
We first describe the two representations of sentence
structure we adopted for our analysis.3 Next, we
3Our representations are similar to features used for rerank-
ing in parsing. Our first representation corresponds to “rules”
features (Charniak and Johnson, 2005; Collins and Koo, 2005),
and our second representation is related to “spines” (Carreras et
al., 2008) and edge annotation(Huang, 2008).
</bodyText>
<page confidence="0.984051">
1159
</page>
<bodyText confidence="0.99986175">
present two coherence models: a local model which
captures the co-occurrence of structural features in
adjacent sentences and a global one which learns
from clusters of sentences with similar syntax.
</bodyText>
<subsectionHeader confidence="0.999891">
3.1 Representing syntax
</subsectionHeader>
<bodyText confidence="0.999623690476191">
Our models rely exclusively on syntactic cues. We
derive representations from constituent parses of the
sentences, and terminals (words) are removed from
the parse tree before any processing is done. The
leaf nodes in our parse trees are part of speech tags.
Productions: In this representation we view each
sentence as the set of grammatical productions, LHS
—* RHS, which appear in the parse of the sen-
tence. As we already pointed out, the right-hand side
(RHS) contains only non-terminal nodes. This rep-
resentation is straightforward, however, some pro-
ductions can be rather specific with long right hand
sides. Another apparent drawback of this represen-
tation is that it contains sequence information only
about nodes that belong to the same constituent.
d-sequence: In this representation we aim to pre-
serve more sequence information about adjacent
constituents in the sentence. The simplest approach
would be to represent the sentence as the sequence
of part of speech (POS) tags but then we lose all
the abstraction provided by higher level nodes in
tree. Instead, we introduce a more general represen-
tation, d-sequence where the level of abstraction can
be controlled using a parameter d. The parse tree is
truncated to depth at most d, and the leaves of the
resulting tree listed left to right form the d-sequence
representation. For example, in Figure 1, the line
depicts the cutoff at depth 2.
Next the representation is further augmented; all
phrasal nodes in the d-sequence are annotated (con-
catenated) with the left-most leaf that they domi-
nate in the full non-lexicalized parse tree. This is
shown as suffixes on the S, NP and VP nodes in
the figure. Such annotation conveys richer informa-
tion about the structure of the subtree below nodes
in the d-sequence. For example, “the chairs”, “his
chairs”, “comfortable chairs” will be represented as
NPDT, NPPRP$ and NP77. In the resulting representa-
tions, sentences are viewed as sequences of syntactic
words (w1,w2...,wk), k &lt; p, where p is the length of
the full POS sequence and each wi is either POS tag
or a phrasal node+POS tag combination.
</bodyText>
<figureCaption confidence="0.998886">
Figure 1: Example for d-sequence representation
</figureCaption>
<bodyText confidence="0.999969333333333">
In our example, at depth-2, the quotation sentence
gets the representation (w1=“ , w2=SDT , w3=, , w4=” ,
wS=NPNNP , w6=VPVBD , w,=.) where the actual quote
is omitted. Sentences that contain attributions are
likely to appear more similar to each other when
compared using this representation in contrast to
representations derived from word or POS sequence.
The depth-3 sequence is also indicated in the figure.
The main verb of a sentence is central to its struc-
ture, so the parameter d is always set to be greater
than that of the main verb and is tuned to optimize
performance for coherence prediction.
</bodyText>
<subsectionHeader confidence="0.999567">
3.2 Implementing the model
</subsectionHeader>
<bodyText confidence="0.999985">
We adapt two models of coherence to operate over
the two syntactic representations.
</bodyText>
<subsectionHeader confidence="0.961571">
3.2.1 Local co-occurrence model
</subsectionHeader>
<bodyText confidence="0.999152666666667">
This model is a direct extension from our pilot
study. It allows us to test the assumption that coher-
ent discourse is characterized by syntactic regulari-
ties in adjacent sentences. We estimate the proba-
bilities of pairs of syntactic items from adjacent sen-
tences in the training data and use these probabilities
to compute the coherence of new texts.
The coherence of a text T containing n sentences
(S1...Sn) is computed as:
</bodyText>
<equation confidence="0.565447">
p(Sji |Ski−1)
</equation>
<bodyText confidence="0.99968125">
where Sy, indicates the yth item of S, Items
are either productions or syntactic word unigrams
depending on the representation. The conditional
probabilities are computed with smoothing:
</bodyText>
<figure confidence="0.801515375">
P(T) _ n |Si |1 |Si−1|
ri H �
i=2 j=1 k=1
|Si−1|
1160
Cluster a
Cluster b
ADJP --+ JJ PP  |VP --+ VBZ ADJP
</figure>
<listItem confidence="0.774569333333333">
[1] This method VP-[is ADJP-[capable of sequence-specific
detection of DNA with high accuracy]-ADJP]-VP .
[2] The same VP-[is ADJP-[true for synthetic polyamines
such as polyallylamine]-ADJP]-VP .
VP --+ VB VP  |VP --+ MD VP
[1] Our results for the difference in reactivity VP-[can
VP-[be linked to experimental observations]-VP]-VP .
[2] These phenomena taken together VP-[can VP-[be considered
as the signature of the gelation process]-VP]-VP .
</listItem>
<tableCaption confidence="0.990368">
Table 4: Example syntactic similarity clusters. The top two descriptive productions for each cluster are also listed.
</tableCaption>
<equation confidence="0.9984045">
c(wi, wj) + δC
p(wj|wi) = c(wi) + δC ∗ |V |
</equation>
<bodyText confidence="0.9977345">
where wi and wj are syntactic items and c(wi, wj) is
the number of sentences that contain the item wi im-
mediately followed by a sentence that contains wj.
|V  |is the vocabulary size for syntactic items.
</bodyText>
<subsectionHeader confidence="0.940096">
3.2.2 Global structure
</subsectionHeader>
<bodyText confidence="0.999954863636364">
Now we turn to a global coherence approach
that implements the assumption that sentences with
similar syntax have the same communicative goal
as well as captures the patterns in communicative
goals in the discourse. This approach uses a Hid-
den Markov Model (HMM) which has been a popu-
lar implementation for modeling coherence (Barzi-
lay and Lee, 2004; Fung and Ngai, 2006; Elsner
et al., 2007). The hidden states in our model de-
pict communicative goals by encoding a probability
distribution over syntactic items. This distribution
gives higher weight to syntactic items that are more
likely for that communicative goal. Transitions be-
tween states record the common patterns in inten-
tional structure for the domain.
In this syntax-HMM, states hk are created by
clustering the sentences from the documents in the
training set by syntactic similarity. For the pro-
ductions representation of syntax, the features for
clustering are the number of times a given produc-
tion appeared in the parse of the sentence. For the
d-sequence approach, the features are n-grams of
size one to four of syntactic words from the se-
quence. Clustering was done by optimizing for av-
erage cosine similarity and was implemented using
the CLUTO toolkit (Zhao et al., 2005). C clusters
are formed and taken as the states of the model. Ta-
ble 4 shows sentences from two clusters formed on
the abstracts of journal articles using the productions
representation. One of them, cluster (a), appears
to capture descriptive sentences and cluster (b) in-
volves mostly speculation type sentences.
The emission probabilities for each state are mod-
eled as a (syntactic) language model derived from
the sentences in it. For productions representa-
tion, this is the unigram distribution of produc-
tions from the sentences in hk. For d-sequences,
the distribution is computed for bigrams of syntac-
tic words. These language models use Lidstone
smoothing with constant δE. The probability for a
sentence Sl to be generated from state hk, pE(Sl|hk)
is computed using these syntactic language models.
The transition probability pM from a state hi to
state hj is computed as:
</bodyText>
<equation confidence="0.999729">
d(hi, hj) + δM
pM(hj|hi) = d(hi) + δM ∗ C
</equation>
<bodyText confidence="0.9997978">
where d(hi) is the number of documents whose sen-
tences appear in hi and d(hi, hj) is the number of
documents which have a sentence in hi which is im-
mediately followed by a sentence in hj. In addi-
tion to the C states, we add one initial hS and one
final hF state to capture document beginning and
end. Transitions from hS to any state hk records
how likely it is for hk to be the starting state for doc-
uments of that domain. δM is a smoothing constant.
The likelihood of a text with n sentences is given
</bodyText>
<equation confidence="0.968502">
by P(T) = � �� t�1 p�(ht|ht−1)pE(St|ht).
h1...hn
</equation>
<bodyText confidence="0.9999332">
All model parameters—the number of clusters
C, smoothing constants δC, δE, δM and d for
d-sequences—are tuned to optimize how well the
model can distinguish coherent from incoherent ar-
ticles. We describe these settings in Section 5.1.
</bodyText>
<sectionHeader confidence="0.954367" genericHeader="method">
4 Content and entity grid models
</sectionHeader>
<bodyText confidence="0.9996945">
We compare the syntax model with content model
and entity grid methods. These approaches are the
most popular ones from prior work and also allow
us to test the complementary nature of syntax with
</bodyText>
<page confidence="0.966778">
1161
</page>
<bodyText confidence="0.9999494">
lexical statistics and entity structure. This section
explains how we implemented these approaches.
Content models introduced by Barzilay and Lee
(2004) and Fung and Ngai (2006) use lexically
driven HMMs to capture coherence. The hidden
states represent the topics of the domain and en-
code a probability distribution over words. Transi-
tions between states record the probable succession
of topics. We built a content model using our HMM
implementation. Clusters are created using word bi-
gram features after replacing numbers and proper
names with tags NUM and PROP. The emissions are
given by a bigram language model on words from
the clustered sentences. Barzilay and Lee (2004)
also employ an iterative clustering procedure before
finalizing the states of the HMM but our method
only uses one-step clustering. Despite the differ-
ence, the content model accuracies for our imple-
mentation are quite close to that from the original.
For the entity grid model, we follow the gen-
erative approach proposed by Lapata and Barzilay
(2005). A text is converted into a matrix, where rows
correspond to sentences, in the order in which they
appear in the article. Columns are created one for
each entity appearing in the text. Each cell (i,j) is
filled with the grammatical role ri,j of the entity j
in sentence i. We computed the entity grids using
the Brown Coherence Toolkit4. The probability of
the text (T) is defined using the likely sequence of
grammatical role transitions.
</bodyText>
<equation confidence="0.651126">
p(ri,j|ri−1,j...ri−h,j)
</equation>
<bodyText confidence="0.9999292">
for m entities and n sentences. Parameter h controls
the history size for transitions and is tuned during
development. When h = 1, for example, only the
grammatical role for the entity in the previous sen-
tence is considered and earlier roles are ignored.
</bodyText>
<sectionHeader confidence="0.95021" genericHeader="method">
5 Evaluating syntactic coherence
</sectionHeader>
<bodyText confidence="0.999587666666667">
We follow the common approach from prior work
and use pairs of articles, where one has the original
document order and the other is a random permuta-
tion of the sentences from the same document. Since
the original article is always more coherent than a
random permutation, a model can be evaluated using
</bodyText>
<footnote confidence="0.772394">
4http://www.cs.brown.edu/~melsner/manual.html
</footnote>
<bodyText confidence="0.99986372972973">
the accuracy with which it can identify the original
article in the pair, i.e. it assigns higher probability
to the original article. This setting is not ideal but
has become the de facto standard for evaluation of
coherence models (Barzilay and Lee, 2004; Elsner
et al., 2007; Barzilay and Lapata, 2008; Karamanis
et al., 2009; Lin et al., 2011; Elsner and Charniak,
2011). It is however based on a reasonable assump-
tion as recent work (Lin et al., 2011) shows that peo-
ple identify the original article as more coherent than
its permutations with over 90% accuracy and asses-
sors also have high agreement. Later, we present
an experiment distinguishing conference from work-
shop articles as a more realistic evaluation.
We use two corpora that are widely employed for
coherence prediction (Barzilay and Lee, 2004; El-
sner et al., 2007; Barzilay and Lapata, 2008; Lin et
al., 2011). One contains reports on airplane acci-
dents from the National Transportation Safety Board
and the other has reports about earthquakes from the
Associated Press. These articles are about 10 sen-
tences long. These corpora were chosen since within
each dataset, the articles have the same intentional
structure. Further, these corpora are also standard
ones used in prior work on lexical, entity and dis-
course relation based coherence models. Later in
Section 6, we show that the models perform well on
the academic genre and longer articles too.
For each of the two corpora, we have 100 arti-
cles for training and 100 (accidents) and 99 (earth-
quakes) for testing. A maximum of 20 random per-
mutations were generated for each test article to cre-
ate the pairwise data (total of 1986 test pairs for the
accident corpus and 1956 for earthquakes).5 The
baseline accuracy for random prediction is 50%.
The articles were parsed using the Stanford parser
(Klein and Manning, 2003).
</bodyText>
<subsectionHeader confidence="0.999078">
5.1 Accuracy of the syntax model
</subsectionHeader>
<bodyText confidence="0.999835333333333">
For each model, the relevant parameters were tuned
using 10-fold cross validation on the training data.
In each fold, 90 documents were used for training
and evaluation was done on permutations from the
remaining articles. After tuning, the final model was
trained on all 100 articles in the training set.
</bodyText>
<footnote confidence="0.990574">
5We downloaded the permutations from http://people.
</footnote>
<equation confidence="0.930351">
csail.mit.edu/regina/coherence/CLsubmission/
n
i=1
P(T) =
M
H
j=1
</equation>
<page confidence="0.992589">
1162
</page>
<figure confidence="0.9982225">
Model
Prodns
d-seq
POS
Prodns
d-seq
Egrid
Content
</figure>
<bodyText confidence="0.999882625">
Table 5 shows the results on the test set. The
best number of clusters and depth for d-sequences
are also indicated. Overall, the syntax models work
quite well, with accuracies at least 15% or more ab-
solute improvement over the baseline.
In the local co-occurrence approach, both pro-
ductions and d-sequences provide 72% accuracy for
the accidents corpus. For the earthquake corpus,
the accuracies are lower and the d-sequence method
works better. The best depth setting for d-sequence
is rather small: depth of main verb (MVP) + 2 (or 1),
and indicates that a fairly abstract level of nodes is
preferred for the patterns. For comparison, we also
provide results using just the POS tags in the model
and this is worse than the d-sequence approach.
The global HMM model is better than the local
model for each representation type giving 2 to 38%
better accuracies. Here we see a different trend for
the d-sequence representation, with better results for
greater depths. At such depths (8 and 9) below the
main verb, the nodes are mostly POS tags.
Overall both productions and d-sequence work
competitively and give the best accuracies when im-
plemented with the global approach.
</bodyText>
<subsectionHeader confidence="0.999802">
5.2 Comparison with other approaches
</subsectionHeader>
<bodyText confidence="0.999321181818182">
For our implementations of the content and entity
grid models, the best accuracies are 71% on the ac-
cidents corpus and 85% on the earthquakes one, sim-
ilar to the syntactic models.
Ideally, we would like to combine models but we
do not have separate training data. So we perform
the following classification experiment which com-
bines the predictions made by different models on
the test set. Each test pair (article and permutation)
forms one example and is given a class value of 0 or
1 depending on whether the first article in the pair
is the original one or the second one. The example
is represented as an n-dimensional vector, where n
is the number of models we wish to combine. For
instance, to combine content models and entity grid,
two features are created: one of these records the dif-
ference in log probabilities for the two articles from
the content model, the other feature indicates the dif-
ference in probabilities from the entity grid.
A logistic regression classifier is trained to pre-
dict the class using these features. The test pairs are
created such that an equal number of examples have
</bodyText>
<table confidence="0.885728153846154">
Accidents Earthquake
Parameter Acc Parameter Acc
A. Local co-occurrence
72.8 55.0
dep. MVP+2 71.8 dep. MVP+1 65.1
61.3 42.6
B. HMM-syntax
clus. 37 74.6 clus.5 93.8
dep. MVP+8 82.2 dep. MVP+9 86.5
clus. 8 clus. 45
C. Other approaches
history 1 67.6 history 1 82.2
clus. 48 71.4 clus. 23 84.5
</table>
<tableCaption confidence="0.999822">
Table 5: Accuracies on accident and earthquake corpora
</tableCaption>
<bodyText confidence="0.999963">
class 0 and 1, so the baseline accuracy is 50%. We
run this experiment using 10-fold cross validation on
the test set after first obtaining the log probabilities
from individual models. In each fold, the training is
done using the pairs from 90 articles and tested on
permutations from the remaining 10 articles. These
accuracies are reported in Table 6. When the accu-
racy of a combination is better than that using any of
its smaller subsets, the value is bolded.
We find that syntax supplements both content and
entity grid methods. While on the airplane corpus
syntax only combines well with the entity grid, on
the earthquake corpus, both entity and content ap-
proaches give better accuracies when combined with
syntax. However, adding all three approaches does
not outperform combinations of any two of them.
This result can be due to the simple approach that
we tested for combination. In prior work, content
and entity grid methods have been combined gen-
eratively (Elsner et al., 2007) and using discrimina-
tive training with different objectives (Soricut and
</bodyText>
<figure confidence="0.990914655172414">
Model
Egrid + HMM-prodn
Egrid + HMM-d-seq
Egrid + Content + HMM-prodn
Egrid + Content + HMM-d-seq
Egrid + Content + HMM-prodn
Content + Egrid
Content + HMM-prodn
Content + HMM-d-seq
+ HMM-d-seq
Table 6: Accuracies for combined approaches
Accid.
76.8
74.2
82.1
79.6
84.2
79.5
84.1
83.6
Earthq.
90.7
95.3
90.3
93.9
91.1
95.0
92.3
95.7
</figure>
<page confidence="0.97049">
1163
</page>
<bodyText confidence="0.997608">
Marcu, 2006). Such approaches might bring out
the complementary strengths of the different aspects
better and we leave such analysis for future work.
</bodyText>
<sectionHeader confidence="0.749726" genericHeader="method">
6 Predictions on academic articles
</sectionHeader>
<bodyText confidence="0.999005025641026">
The distinctive intentional structure of academic ar-
ticles has motivated several proposals to define and
annotate the communicative purpose (argumentative
zone) of each sentence (Swales, 1990; Teufel et al.,
1999; Liakata et al., 2010). Supervised classifiers
were also built to identify these zones (Teufel and
Moens, 2000; Guo et al., 2011). So we expect that
these articles form a good testbed for our models. In
the remainder of the paper, we examine how unsu-
pervised patterns discovered by our approach relate
to zones and how well our models predict coherence
for articles from this genre.
We employ two corpora of scientific articles.
ART Corpus: contains a set of 225 Chemistry jour-
nal articles that were manually annotated for inten-
tional structure (Liakata and Soldatova, 2008). Each
sentence was assigned one of 11 zone labels: Result,
Conclusion, Objective, Method, Goal, Background,
Observation, Experiment, Motivation, Model, Hy-
pothesis. For our study, we use the annotation of
the introduction and the abstract sections. We divide
the data into training, development and test sets. For
abstracts, we have 75, 50 and 100 for these sets re-
spectively. For introductions, this split is 75, 31, 82.6
ACL Anthology Network (AAN) Corpus: Radev
et al. (2009) provides the full text of publications
from ACL venues. These articles do not have any
zone annotations. The AAN corpus is produced
from OCR analysis and no section marking is avail-
able. To recreate these, we use the Parscit tagger7
(Councill et al., 2008). We use articles from years
1999 to 2011. For training, we randomly choose 70
articles from ACL and NAACL main conferences.
Similarly, we obtain a development corpus of 36
ACL-NAACL articles. We create two test sets: one
has 500 ACL-NAACL conference articles and an-
other has 500 articles from ACL-sponsored work-
shops. We only choose articles in which all three
sections—abstract, introduction and related work—
</bodyText>
<footnote confidence="0.999929666666667">
6Some articles did not have labelled ‘introduction’ sections
resulting in fewer examples for this setup.
7http://aye.comp.nus.edu.sg/parsCit/
</footnote>
<bodyText confidence="0.997488416666667">
could be successfully identified using Parscit.8
This data was sentence-segmented using MxTer-
minator (Reynar and Ratnaparkhi, 1997) and parsed
with the Stanford Parser (Klein and Manning, 2003).
For each corpus and each section, we train all our
syntactic models: the two local coherence models
using the production and d-sequence representations
and the HMM models with the two representations.
These models are tuned on the respective develop-
ment data, on the task of differentiating the original
from a permuted section. For this purpose, we cre-
ated a maximum of 30 permutations per article.
</bodyText>
<subsectionHeader confidence="0.999918">
6.1 Comparison with ART Corpus zones
</subsectionHeader>
<bodyText confidence="0.9998289">
We perform this analysis using the ART corpus. The
zone annotations present in this corpus allow us to
directly test our first assumption in this work, that
sentences with similar syntax have the same com-
municative goal.
For this analysis, we use the the HMM-prod
model for abstracts and the HMM-d-seq model for
introductions. These models were chosen because
they gave the best performance on the ART corpus
development sets.9 We examine the clusters cre-
ated by these models on the training data and check
whether there are clusters which strongly involve
sentences from some particular annotated zone.
For each possible pair of cluster and zone (Ci,
Zj), we compute c(Ci, Zj): the number of sentences
in Ci that are annotated as zone Zj. Then we use a
chi-square test to identify pairs for which c(Ci, Zj)
is significantly greater than expected (there is a “pos-
itive” association between Ci and Zj) and pairs
where c(Ci, Zj) is significantly less than chance (Ci
is not associated with Zj). A 95% confidence level
was used to determine significance.
The HMM-prod model for abstracts has 9 clusters
(named Clus0 to 8) and the HMM-d-seq model for
introductions has 6 clusters (Clus0 to 5). The pair-
ings of these clusters with zones which turned out to
be significant are reported in Table 7. We also re-
port for each positively associated cluster-zone pair,
the following numbers: matches c(Ci, Zj), preci-
sion c(Ci, Zj)/|Ci |and recall c(Ci, Zj)/|Zj|.
</bodyText>
<footnote confidence="0.9991355">
8We also exclude introduction and related work sections
longer than 50 sentences and those shorter than 4 sentences
since they often have inaccurate section boundaries.
9Their test accuracies are reported in the next section.
</footnote>
<page confidence="0.979124">
1164
</page>
<table confidence="0.997606611111111">
Abstracts (HMM-prod 9 clusters)
Positive associations matches prec. recall
Clus5 - Model 7 17.1 43.8
Clus7 - Objective 27 27.6 32.9
Clus7 - Goal 16 16.3 55.2
Clus0 - Conclusion 15 50.0 12.1
Clus6 - Conclusion 27 51.9 21.8
Not associated: Clus7 - Conclusion,
Clus8 - Conclusion
Introductions (HMM-d-seq 6 clusters)
Positive associations matches prec. recall
Clus2-Background 161 64.9 14.2
Clus3-Objective 37 7.9 38.5
Clus4-Goal 29 9.8 32.6
Clus4-Hypothesis 12 4.1 52.2
Clus5-Motivation 61 12.9 37.4
Not associated: Clus1 - Motivation, Clus2 - Goal,
Clus4 - Background, Clus 5 - Model
</table>
<tableCaption confidence="0.998234">
Table 7: Cluster-Zone mappings on the ART Corpus
</tableCaption>
<bodyText confidence="0.999754692307692">
The presence of significant associations validate
our intuitions that syntax provides clues about com-
municative goals. Some clusters overwhelmingly
contain the same zone, indicated by high precision,
for example 64% of sentences in Clus2 from intro-
duction sections are background sentences. Other
clusters have high recall of a zone, 55% of all goal
sentences from the abstracts training data is captured
by Clus7. It is particularly interesting to see that
Clus7 of abstracts captures both objective and goal
zone sentences and for introductions, Clus4 is a mix
of hypothesis and goal sentences which intuitively
are closely related categories.
</bodyText>
<subsectionHeader confidence="0.998917">
6.2 Original versus permuted sections
</subsectionHeader>
<bodyText confidence="0.999990551724138">
We also explore the accuracy of the syntax models
for predicting coherence of articles from the test set
of ART corpus and the 500 test articles from ACL-
NAACL conferences. We use the same experimen-
tal setup as before and create pairs of original and
permuted versions of the test articles. We created a
maximum of 20 permutations for each article. The
baseline accuracy is 50% as before.
For the ART corpus, we also built an oracle model
of annotated zones. We train a first order Markov
Chain to record the sequence of zones in the training
articles. For testing, we assume that the oracle zone
is provided for each sentence and use the model to
predict the likelihood of the zone sequence. Results
from this model represent an upper bound because
an accurate hypothesis of the communicative goal is
available for each sentence.
The accuracies are presented in Table 8. Overall,
the HMM-d-seq model provides the best accuracies.
The highest results are obtained for ACL introduc-
tion sections (74%). These results are lower than
that obtained on the earthquake/accident corpus but
the task here is much harder: the articles are longer
and the ACL corpus also has OCR errors which af-
fect sentence segmentation and parsing accuracies.
When the oracle zones are known, the accuracies are
much higher on the ART corpus indicating that the
intentional structure of academic articles is very pre-
dictive of their coherence.
</bodyText>
<subsectionHeader confidence="0.990736">
6.3 Conference versus workshop papers
</subsectionHeader>
<bodyText confidence="0.999943625">
Finally, we test whether the syntax-based model can
distinguish the structure of conference from work-
shop articles. Conferences publish more complete
and tested work and workshops often present pre-
liminary studies. Workshops are also venues to dis-
cuss a focused and specialized topic. So the way
information is conveyed in the abstracts and intro-
ductions would vary in these articles.
We perform this analysis on the ACL corpus and
no permutations are used, only the original text of
the 500 articles each in the conference and work-
shop test sets. While permutation examples provide
cheap training/test data, they have a few unrealistic
properties. For example, both original and permuted
articles have the same length. Further some permu-
tations could result in an outstandingly incoherent
sample which is easily distinguished from the origi-
nal articles. So we use the conference versus work-
shop task as another evaluation of our model.
We designed a classification experiment for this
task which combines features from the different syn-
tax models that were trained on the ACL conference
training set. We include four features indicating the
perplexity of an article under each model (Local-
prod, Local-d-seq, HMM-prod, HMM-d-seq). We
use perplexity rather than probability because the
length of the articles vary widely in contrast to the
previous permutation-based tests, where both per-
mutation and original article have the same length.
We compute perplexity as P(T)−1/&apos;, where n is
the number of words in the article. We also obtain
the most likely state sequence for the article under
</bodyText>
<page confidence="0.968642">
1165
</page>
<table confidence="0.999725833333333">
Data Section Test pairs Local-prod Local-d-seq BMM-prod BMM-d-seq Oracle zones
ART Corpus Abstract 1633 57.0 52.9 64.1 55.0 80.8
Intro 1640 44.5 54.6 58.1 64.6 94.0
ACL Conference Abstract 8815 44.0 47.2 58.2 63.7
Intro 9966 54.5 53.0 64.4 74.0
Rel. wk. 10,000 54.6 54.4 57.3 67.3
</table>
<tableCaption confidence="0.999931">
Table 8: Accuracy in differentiating permutation from original sections on ACL and ART test sets.
</tableCaption>
<bodyText confidence="0.999946342105263">
HMM-prod and HMM-d-seq models using Viterbi
decoding. Then the proportion of sentences from
each state of the two models are added as features.
We also add some fine-grained features from the
local model. We represent sentences in the train-
ing set as either productions or d-sequence items and
compute pairs of associated items (xi7 xj) from ad-
jacent sentences using the same chi-square test as
in our pilot study. The most significant (lowest p-
values) 30 pairs (each for production and d-seq) are
taken as features.10 For a test article, we compute
features that represent how often each pair is present
in the article such that xi is in 5m and xj is in 5m+1.
We perform this experiment for each section and
there are about 90 to 140 features for the different
sections. We cast the problem as a binary classifi-
cation task: conference articles belong to one class
and workshop to the other. Each class has 500 ar-
ticles and so the baseline random accuracy is 50%.
We perform 10-fold cross validation using logistic
regression. Our results were 59.3% accuracy for dis-
tinguishing abstracts of conference verus workshop,
50.3% for introductions and 55.4% for related work.
For abstracts and related work, these accuracies are
significantly better than baseline (95% confidence
level from a two-sided paired t-test comparing the
accuracies from the 10 folds). It is possible that in-
troductions in either case, talk in general about the
field and importance of the problem addressed and
hence have similar structure.
Our accuracies are not as high as on permutations
examples because the task is clearly harder. It may
also be the case that the prediction is more difficult
for certain papers than for others. So we also ana-
lyze our results by the confidence provided by the
classifier for the predicted class. We consider only
the examples predicted above a certain confidence
level and compute the accuracy on these predictions.
</bodyText>
<footnote confidence="0.9691965">
10A cutoff is applied such that the pair was seen at least 25
times in the training data.
</footnote>
<table confidence="0.9989205">
Conf. Abstract Intro Rel wk
&gt;= 0.5 59.3 (100.0) 50.3 (100.0) 55.4 (100.0)
&gt;= 0.6 63.8 (67.2) 50.8 (71.1) 58.6 (75.9)
&gt;= 0.7 67.2 (32.0) 54.4 (38.6) 63.3 (52.8)
&gt;= 0.8 74.0 (10.0) 51.6 (22.0) 63.0 (25.7)
&gt;= 0.9 91.7 (2.0) 30.6 (5.0) 68.1 (7.2)
</table>
<tableCaption confidence="0.8756645">
Table 9: Accuracy (% examples) above each confidence
level for the conference versus workshop task.
</tableCaption>
<bodyText confidence="0.999954133333333">
These results are shown in Table 9. The proportion
of examples under each setting is also indicated.
When only examples above 0.6 confidence are ex-
amined, the classifier has a higher accuracy of 63.8%
for abstracts and covers close to 70% of the exam-
ples. Similarly, when a cutoff of 0.7 is applied to the
confidence for predicting related work sections, we
achieve 63.3% accuracy for 53% of examples. So
we can consider that 30 to 47% of the examples in
the two sections respectively are harder to tell apart.
Interestingly however even high confidence predic-
tions on introductions remain incorrect.
These results show that our model can success-
fully distinguish the structure of articles beyond just
clearly incoherent permutation examples.
</bodyText>
<sectionHeader confidence="0.998509" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999992545454545">
Our work is the first to develop an unsupervised
model for intentional structure and to show that
it has good accuracy for coherence prediction and
also complements entity and lexical structure of dis-
course. This result raises interesting questions about
how patterns captured by these different coherence
metrics vary and how they can be combined usefully
for predicting coherence. We plan to explore these
ideas in future work. We also want to analyze genre
differences to understand if the strength of these co-
herence dimensions varies with genre.
</bodyText>
<sectionHeader confidence="0.997618" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.950241">
This work is partially supported by a Google re-
search grant and NSF CAREER 0953445 award.
</bodyText>
<page confidence="0.990986">
1166
</page>
<sectionHeader confidence="0.995865" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99992140776699">
Regina Barzilay and Mirella Lapata. 2008. Modeling
local coherence: An entity-based approach. Computa-
tional Linguistics, 34(1):1–34.
Regina Barzilay and Lillian Lee. 2004. Catching the
drift: Probabilistic content models, with applications
to generation and summarization. In Proceedings of
NAACL-HLT, pages 113–120.
Xavier Carreras, Michael Collins, and Terry Koo. 2008.
Tag, dynamic programming, and the perceptron for
efficient, feature-rich parsing. In Proceedings of
CoNLL, pages 9–16.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proceedings ofACL, pages 173–180.
Jackie C.K. Cheung and Gerald Penn. 2010. Utilizing
extra-sentential context for parsing. In Proceedings of
EMNLP, pages 23–33.
Christelle Cocco, Rapha¨el Pittier, Franc¸ois Bavaud, and
Aris Xanthos. 2011. Segmentation and clustering of
textual sequences: a typological approach. In Pro-
ceedings of RANLP, pages 427–433.
Michael Collins and Terry Koo. 2005. Discrimina-
tive reranking for natural language parsing. Compu-
tational Linguistics, 31:25–70.
Isaac G. Councill, C. Lee Giles, and Min-Yen Kan. 2008.
Parscit: An open-source crf reference string parsing
package. In Proceedings of LREC, pages 661–667.
Micha Elsner and Eugene Charniak. 2008. Coreference-
inspired coherence modeling. In Proceedings of ACL-
HLT, Short Papers, pages 41–44.
Micha Elsner and Eugene Charniak. 2011. Extending
the entity grid with entity-specific features. In Pro-
ceedings of ACL-HLT, pages 125–129.
Micha Elsner, Joseph Austerweil, and Eugene Charniak.
2007. A unified local and global model for discourse
coherence. In Proceedings of NAACL-HLT, pages
436–443.
Pascale Fung and Grace Ngai. 2006. One story, one
flow: Hidden markov story models for multilingual
multidocument summarization. ACM Transactions on
Speech and Language Processing, 3(2):1–16.
Barbara J. Grosz and Candace L. Sidner. 1986. Atten-
tion, intentions, and the structure of discourse. Com-
putational Linguistics, 3(12):175–204.
Yufan Guo, Anna Korhonen, and Thierry Poibeau. 2011.
A weakly-supervised approach to argumentative zon-
ing of scientific documents. In Proceedings of
EMNLP, pages 273–283.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of
ACL-HLT, pages 586–594, June.
Nikiforos Karamanis, Chris Mellish, Massimo Poesio,
and Jon Oberlander. 2009. Evaluating centering for
information ordering using corpora. Computational
Linguistics, 35(1):29–46.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of ACL, pages
423–430.
Mirella Lapata and Regina Barzilay. 2005. Automatic
evaluation of text coherence: Models and representa-
tions. In Proceedings of IJCAI.
Mirella Lapata. 2003. Probabilistic text structuring: Ex-
periments with sentence ordering. In Proceedings of
ACL, pages 545–552.
Maria Liakata and Larisa Soldatova. 2008. Guidelines
for the annotation of general scientific concepts. JISC
Project Report.
Maria Liakata, Simone Teufel, Advaith Siddharthan, and
Colin Batchelor. 2010. Corpora for the conceptualisa-
tion and zoning of scientific papers. In Proceedings of
LREC.
Ziheng Lin, Min-Yen Kan, and Hwee Tou Ng. 2009.
Recognizing implicit discourse relations in the Penn
Discourse Treebank. In Proceedings of EMNLP,
pages 343–351.
Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2011. Au-
tomatically evaluating text coherence using discourse
relations. In Proceedings of ACL-HLT, pages 997–
1006.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1994. Building a large annotated cor-
pus of english: The penn treebank. Computational
Linguistics, 19(2):313–330.
Emily Pitler and Ani Nenkova. 2008. Revisiting read-
ability: A unified framework for predicting text qual-
ity. In Proceedings of EMNLP, pages 186–195.
Dragomir R. Radev, Mark Thomas Joseph, Bryan Gib-
son, and Pradeep Muthukrishnan. 2009. A Bibliomet-
ric and Network Analysis of the field of Computational
Linguistics. Journal of the American Society for Infor-
mation Science and Technology.
David Reitter, Johanna D. Moore, and Frank Keller.
2006. Priming of Syntactic Rules in Task-Oriented
Dialogue and Spontaneous Conversation. In Proceed-
ings of the 28th Annual Conference of the Cognitive
Science Society, pages 685–690.
Jeffrey C. Reynar and Adwait Ratnaparkhi. 1997. A
maximum entropy approach to identifying sentence
boundaries. In Proceedings of the fifth conference on
Applied natural language processing, pages 16–19.
Radu Soricut and Daniel Marcu. 2006. Discourse gener-
ation using utility-trained coherence models. In Pro-
ceedings of COLING-ACL, pages 803–810.
</reference>
<page confidence="0.875727">
1167
</page>
<reference confidence="0.9986828">
John Swales. 1990. Genre analysis: English in academic
and research settings, volume 11. Cambridge Univer-
sity Press.
Simone Teufel and Marc Moens. 2000. What’s yours
and what’s mine: determining intellectual attribution
in scientific text. In Proceedings of EMNLP, pages 9–
17.
Simone Teufel, Jean Carletta, and Marc Moens. 1999.
An annotation scheme for discourse-level argumen-
tation in research articles. In Proceedings of EACL,
pages 110–117.
Ying Zhao, George Karypis, and Usama Fayyad.
2005. Hierarchical clustering algorithms for docu-
ment datasets. Data Mining and Knowledge Discov-
ery, 10:141–168.
</reference>
<page confidence="0.995221">
1168
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.924822">
<title confidence="0.991004">A coherence model based on syntactic patterns</title>
<author confidence="0.999257">Annie Louis Ani Nenkova</author>
<affiliation confidence="0.999924">University of Pennsylvania University of Pennsylvania</affiliation>
<address confidence="0.999117">Philadelphia, PA 19104, USA Philadelphia, PA 19104, USA</address>
<email confidence="0.999791">lannie@seas.upenn.edunenkova@seas.upenn.edu</email>
<abstract confidence="0.9953734">We introduce a model of coherence which captures the intentional discourse structure in text. Our work is based on the hypothesis that syntax provides a proxy for the communicative goal of a sentence and therefore the sequence of sentences in a coherent discourse should exhibit detectable structural patterns. Results show that our method has high discriminating power for separating out coherent and incoherent news articles reaching accuracies of up to 90%. We also show that our syntactic patterns are correlated with manual annotations of intentional structure for academic conference articles and can successfully pre-</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Mirella Lapata</author>
</authors>
<title>Modeling local coherence: An entity-based approach.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>1</issue>
<contexts>
<context position="1235" citStr="Barzilay and Lapata, 2008" startWordPosition="179" endWordPosition="182">parating out coherent and incoherent news articles reaching accuracies of up to 90%. We also show that our syntactic patterns are correlated with manual annotations of intentional structure for academic conference articles and can successfully predict the coherence of abstract, introduction and related work sections of these articles. 1 Introduction Recent studies have introduced successful automatic methods to predict the structure and coherence of texts. They include entity approaches for local coherence which track the repetition and syntactic realization of entities in adjacent sentences (Barzilay and Lapata, 2008; Elsner and Charniak, 2008) and content approaches for global coherence which view texts as a sequence of topics, each characterized by a particular distribution of lexical items (Barzilay and Lee, 2004; Fung and Ngai, 2006). Other work has shown that co-occurrence of words (Lapata, 2003; Soricut and Marcu, 2006) and discourse relations (Pitler and Nenkova, 2008; Lin et al., 2011) also predict coherence. Early theories (Grosz and Sidner, 1986) posited that there are three factors which collectively contribute to coherence: intentional structure (purpose of discourse), attentional structure (w</context>
<context position="23333" citStr="Barzilay and Lapata, 2008" startWordPosition="3881" endWordPosition="3884">ch from prior work and use pairs of articles, where one has the original document order and the other is a random permutation of the sentences from the same document. Since the original article is always more coherent than a random permutation, a model can be evaluated using 4http://www.cs.brown.edu/~melsner/manual.html the accuracy with which it can identify the original article in the pair, i.e. it assigns higher probability to the original article. This setting is not ideal but has become the de facto standard for evaluation of coherence models (Barzilay and Lee, 2004; Elsner et al., 2007; Barzilay and Lapata, 2008; Karamanis et al., 2009; Lin et al., 2011; Elsner and Charniak, 2011). It is however based on a reasonable assumption as recent work (Lin et al., 2011) shows that people identify the original article as more coherent than its permutations with over 90% accuracy and assessors also have high agreement. Later, we present an experiment distinguishing conference from workshop articles as a more realistic evaluation. We use two corpora that are widely employed for coherence prediction (Barzilay and Lee, 2004; Elsner et al., 2007; Barzilay and Lapata, 2008; Lin et al., 2011). One contains reports on</context>
</contexts>
<marker>Barzilay, Lapata, 2008</marker>
<rawString>Regina Barzilay and Mirella Lapata. 2008. Modeling local coherence: An entity-based approach. Computational Linguistics, 34(1):1–34.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Lillian Lee</author>
</authors>
<title>Catching the drift: Probabilistic content models, with applications to generation and summarization.</title>
<date>2004</date>
<booktitle>In Proceedings of NAACL-HLT,</booktitle>
<pages>113--120</pages>
<contexts>
<context position="1438" citStr="Barzilay and Lee, 2004" startWordPosition="210" endWordPosition="213">ference articles and can successfully predict the coherence of abstract, introduction and related work sections of these articles. 1 Introduction Recent studies have introduced successful automatic methods to predict the structure and coherence of texts. They include entity approaches for local coherence which track the repetition and syntactic realization of entities in adjacent sentences (Barzilay and Lapata, 2008; Elsner and Charniak, 2008) and content approaches for global coherence which view texts as a sequence of topics, each characterized by a particular distribution of lexical items (Barzilay and Lee, 2004; Fung and Ngai, 2006). Other work has shown that co-occurrence of words (Lapata, 2003; Soricut and Marcu, 2006) and discourse relations (Pitler and Nenkova, 2008; Lin et al., 2011) also predict coherence. Early theories (Grosz and Sidner, 1986) posited that there are three factors which collectively contribute to coherence: intentional structure (purpose of discourse), attentional structure (what items are discussed) and the organization of discourse segments. The highly successful entity approaches capture attentional structure and content approaches are related to topic segments but intenti</context>
<context position="18059" citStr="Barzilay and Lee, 2004" startWordPosition="3001" endWordPosition="3005">d. c(wi, wj) + δC p(wj|wi) = c(wi) + δC ∗ |V | where wi and wj are syntactic items and c(wi, wj) is the number of sentences that contain the item wi immediately followed by a sentence that contains wj. |V |is the vocabulary size for syntactic items. 3.2.2 Global structure Now we turn to a global coherence approach that implements the assumption that sentences with similar syntax have the same communicative goal as well as captures the patterns in communicative goals in the discourse. This approach uses a Hidden Markov Model (HMM) which has been a popular implementation for modeling coherence (Barzilay and Lee, 2004; Fung and Ngai, 2006; Elsner et al., 2007). The hidden states in our model depict communicative goals by encoding a probability distribution over syntactic items. This distribution gives higher weight to syntactic items that are more likely for that communicative goal. Transitions between states record the common patterns in intentional structure for the domain. In this syntax-HMM, states hk are created by clustering the sentences from the documents in the training set by syntactic similarity. For the productions representation of syntax, the features for clustering are the number of times a </context>
<context position="21050" citStr="Barzilay and Lee (2004)" startWordPosition="3508" endWordPosition="3511"> All model parameters—the number of clusters C, smoothing constants δC, δE, δM and d for d-sequences—are tuned to optimize how well the model can distinguish coherent from incoherent articles. We describe these settings in Section 5.1. 4 Content and entity grid models We compare the syntax model with content model and entity grid methods. These approaches are the most popular ones from prior work and also allow us to test the complementary nature of syntax with 1161 lexical statistics and entity structure. This section explains how we implemented these approaches. Content models introduced by Barzilay and Lee (2004) and Fung and Ngai (2006) use lexically driven HMMs to capture coherence. The hidden states represent the topics of the domain and encode a probability distribution over words. Transitions between states record the probable succession of topics. We built a content model using our HMM implementation. Clusters are created using word bigram features after replacing numbers and proper names with tags NUM and PROP. The emissions are given by a bigram language model on words from the clustered sentences. Barzilay and Lee (2004) also employ an iterative clustering procedure before finalizing the stat</context>
<context position="23285" citStr="Barzilay and Lee, 2004" startWordPosition="3873" endWordPosition="3876">ntactic coherence We follow the common approach from prior work and use pairs of articles, where one has the original document order and the other is a random permutation of the sentences from the same document. Since the original article is always more coherent than a random permutation, a model can be evaluated using 4http://www.cs.brown.edu/~melsner/manual.html the accuracy with which it can identify the original article in the pair, i.e. it assigns higher probability to the original article. This setting is not ideal but has become the de facto standard for evaluation of coherence models (Barzilay and Lee, 2004; Elsner et al., 2007; Barzilay and Lapata, 2008; Karamanis et al., 2009; Lin et al., 2011; Elsner and Charniak, 2011). It is however based on a reasonable assumption as recent work (Lin et al., 2011) shows that people identify the original article as more coherent than its permutations with over 90% accuracy and assessors also have high agreement. Later, we present an experiment distinguishing conference from workshop articles as a more realistic evaluation. We use two corpora that are widely employed for coherence prediction (Barzilay and Lee, 2004; Elsner et al., 2007; Barzilay and Lapata, </context>
</contexts>
<marker>Barzilay, Lee, 2004</marker>
<rawString>Regina Barzilay and Lillian Lee. 2004. Catching the drift: Probabilistic content models, with applications to generation and summarization. In Proceedings of NAACL-HLT, pages 113–120.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Carreras</author>
<author>Michael Collins</author>
<author>Terry Koo</author>
</authors>
<title>Tag, dynamic programming, and the perceptron for efficient, feature-rich parsing.</title>
<date>2008</date>
<booktitle>In Proceedings of CoNLL,</booktitle>
<pages>9--16</pages>
<contexts>
<context position="12973" citStr="Carreras et al., 2008" startWordPosition="2166" endWordPosition="2169">clause in p2. Here the intentional structure is INTRODUCE X / STATEMENT BY X. In the remainder of the paper we formalize our representation of syntax and the derived model of coherence and test its efficacy in three domains. 3 Coherence models using syntax We first describe the two representations of sentence structure we adopted for our analysis.3 Next, we 3Our representations are similar to features used for reranking in parsing. Our first representation corresponds to “rules” features (Charniak and Johnson, 2005; Collins and Koo, 2005), and our second representation is related to “spines” (Carreras et al., 2008) and edge annotation(Huang, 2008). 1159 present two coherence models: a local model which captures the co-occurrence of structural features in adjacent sentences and a global one which learns from clusters of sentences with similar syntax. 3.1 Representing syntax Our models rely exclusively on syntactic cues. We derive representations from constituent parses of the sentences, and terminals (words) are removed from the parse tree before any processing is done. The leaf nodes in our parse trees are part of speech tags. Productions: In this representation we view each sentence as the set of gramm</context>
</contexts>
<marker>Carreras, Collins, Koo, 2008</marker>
<rawString>Xavier Carreras, Michael Collins, and Terry Koo. 2008. Tag, dynamic programming, and the perceptron for efficient, feature-rich parsing. In Proceedings of CoNLL, pages 9–16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Coarse-tofine n-best parsing and maxent discriminative reranking.</title>
<date>2005</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>173--180</pages>
<contexts>
<context position="12871" citStr="Charniak and Johnson, 2005" startWordPosition="2150" endWordPosition="2153">r organization. The next sentence has a quote from that person, where the quotation forms the topicalized clause in p2. Here the intentional structure is INTRODUCE X / STATEMENT BY X. In the remainder of the paper we formalize our representation of syntax and the derived model of coherence and test its efficacy in three domains. 3 Coherence models using syntax We first describe the two representations of sentence structure we adopted for our analysis.3 Next, we 3Our representations are similar to features used for reranking in parsing. Our first representation corresponds to “rules” features (Charniak and Johnson, 2005; Collins and Koo, 2005), and our second representation is related to “spines” (Carreras et al., 2008) and edge annotation(Huang, 2008). 1159 present two coherence models: a local model which captures the co-occurrence of structural features in adjacent sentences and a global one which learns from clusters of sentences with similar syntax. 3.1 Representing syntax Our models rely exclusively on syntactic cues. We derive representations from constituent parses of the sentences, and terminals (words) are removed from the parse tree before any processing is done. The leaf nodes in our parse trees </context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Eugene Charniak and Mark Johnson. 2005. Coarse-tofine n-best parsing and maxent discriminative reranking. In Proceedings ofACL, pages 173–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jackie C K Cheung</author>
<author>Gerald Penn</author>
</authors>
<title>Utilizing extra-sentential context for parsing.</title>
<date>2010</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>23--33</pages>
<contexts>
<context position="4959" citStr="Cheung and Penn (2010)" startWordPosition="759" endWordPosition="762">ve similar sequence of communicative goals and so we can expect the syntax of adjacent sentences to also be related. We aim to characterize this relationship on a broad scale using a coherence model based entirely on syntax. The model relies on two assumptions which summarize our intuitions about syntax and intentional structure: 1. Sentences with similar syntax are likely to have the same communicative goal. 2. Regularities in intentional structure will be manifested in syntactic regularities between adjacent sentences. There is also evidence from recent work that supports these assumptions. Cheung and Penn (2010) find that a better syntactic parse of a sentence can be derived when the syntax of adjacent sentences is also taken into account. Lin et al. (2009) report that the syntactic productions in adjacent sentences are powerful features for predicting which discourse relation (cause, contrast, etc.) holds between them. Cocco et al. (2011) show that significant associations exist between certain part of speech tags and sentence types such as explanation, dialog and argumentation. In our model, syntax is represented either as parse tree productions or a sequence of phrasal nodes augmented with part of</context>
<context position="7613" citStr="Cheung and Penn, 2010" startWordPosition="1174" endWordPosition="1177">e for syntactic coherence We first present a pilot study that confirms that adjacent sentences in discourse exhibit stable patterns of syntactic co-occurrence. This study validates our second assumption relating the syntax of adjacent sentences. Later in Section 6, we examine syntactic patterns in individual sentences (assumption 1) using a corpus of academic articles where sentences were manually annotated with communicative goals. Prior work has reported that certain grammatical productions are repeated in adjacent sentences more often than would be expected by chance (Reitter et al., 2006; Cheung and Penn, 2010). We analyze all co-occurrence patterns rather than just repetitions. We use the gold standard parse trees from the Penn Treebank (Marcus et al., 1994). Our unit of analysis is a pair of adjacent sentences (S1, S2) and we choose to use Section 0 of the corpus which has 99 documents and 1727 sentence pairs. We enumerate all productions that appear in the syntactic parse of any sentence and exclude those that appear less than 25 times, resulting in a list of 197 unique productions. Then all ordered pairs2 (P1, P2) of productions are formed. For each pair, we compute 2(p1, p2) and (p2, p1) are co</context>
</contexts>
<marker>Cheung, Penn, 2010</marker>
<rawString>Jackie C.K. Cheung and Gerald Penn. 2010. Utilizing extra-sentential context for parsing. In Proceedings of EMNLP, pages 23–33.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christelle Cocco</author>
<author>Franc¸ois Bavaud Rapha¨el Pittier</author>
<author>Aris Xanthos</author>
</authors>
<title>Segmentation and clustering of textual sequences: a typological approach.</title>
<date>2011</date>
<booktitle>In Proceedings of RANLP,</booktitle>
<pages>427--433</pages>
<marker>Cocco, Rapha¨el Pittier, Xanthos, 2011</marker>
<rawString>Christelle Cocco, Rapha¨el Pittier, Franc¸ois Bavaud, and Aris Xanthos. 2011. Segmentation and clustering of textual sequences: a typological approach. In Proceedings of RANLP, pages 427–433.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Terry Koo</author>
</authors>
<title>Discriminative reranking for natural language parsing.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<pages>31--25</pages>
<contexts>
<context position="12895" citStr="Collins and Koo, 2005" startWordPosition="2154" endWordPosition="2157">tence has a quote from that person, where the quotation forms the topicalized clause in p2. Here the intentional structure is INTRODUCE X / STATEMENT BY X. In the remainder of the paper we formalize our representation of syntax and the derived model of coherence and test its efficacy in three domains. 3 Coherence models using syntax We first describe the two representations of sentence structure we adopted for our analysis.3 Next, we 3Our representations are similar to features used for reranking in parsing. Our first representation corresponds to “rules” features (Charniak and Johnson, 2005; Collins and Koo, 2005), and our second representation is related to “spines” (Carreras et al., 2008) and edge annotation(Huang, 2008). 1159 present two coherence models: a local model which captures the co-occurrence of structural features in adjacent sentences and a global one which learns from clusters of sentences with similar syntax. 3.1 Representing syntax Our models rely exclusively on syntactic cues. We derive representations from constituent parses of the sentences, and terminals (words) are removed from the parse tree before any processing is done. The leaf nodes in our parse trees are part of speech tags.</context>
</contexts>
<marker>Collins, Koo, 2005</marker>
<rawString>Michael Collins and Terry Koo. 2005. Discriminative reranking for natural language parsing. Computational Linguistics, 31:25–70.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Isaac G Councill</author>
<author>C Lee Giles</author>
<author>Min-Yen Kan</author>
</authors>
<title>Parscit: An open-source crf reference string parsing package.</title>
<date>2008</date>
<booktitle>In Proceedings of LREC,</booktitle>
<pages>661--667</pages>
<contexts>
<context position="31167" citStr="Councill et al., 2008" startWordPosition="5179" endWordPosition="5182">rvation, Experiment, Motivation, Model, Hypothesis. For our study, we use the annotation of the introduction and the abstract sections. We divide the data into training, development and test sets. For abstracts, we have 75, 50 and 100 for these sets respectively. For introductions, this split is 75, 31, 82.6 ACL Anthology Network (AAN) Corpus: Radev et al. (2009) provides the full text of publications from ACL venues. These articles do not have any zone annotations. The AAN corpus is produced from OCR analysis and no section marking is available. To recreate these, we use the Parscit tagger7 (Councill et al., 2008). We use articles from years 1999 to 2011. For training, we randomly choose 70 articles from ACL and NAACL main conferences. Similarly, we obtain a development corpus of 36 ACL-NAACL articles. We create two test sets: one has 500 ACL-NAACL conference articles and another has 500 articles from ACL-sponsored workshops. We only choose articles in which all three sections—abstract, introduction and related work— 6Some articles did not have labelled ‘introduction’ sections resulting in fewer examples for this setup. 7http://aye.comp.nus.edu.sg/parsCit/ could be successfully identified using Parscit</context>
</contexts>
<marker>Councill, Giles, Kan, 2008</marker>
<rawString>Isaac G. Councill, C. Lee Giles, and Min-Yen Kan. 2008. Parscit: An open-source crf reference string parsing package. In Proceedings of LREC, pages 661–667.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Micha Elsner</author>
<author>Eugene Charniak</author>
</authors>
<title>Coreferenceinspired coherence modeling.</title>
<date>2008</date>
<booktitle>In Proceedings of ACLHLT, Short Papers,</booktitle>
<pages>41--44</pages>
<contexts>
<context position="1263" citStr="Elsner and Charniak, 2008" startWordPosition="183" endWordPosition="186">ncoherent news articles reaching accuracies of up to 90%. We also show that our syntactic patterns are correlated with manual annotations of intentional structure for academic conference articles and can successfully predict the coherence of abstract, introduction and related work sections of these articles. 1 Introduction Recent studies have introduced successful automatic methods to predict the structure and coherence of texts. They include entity approaches for local coherence which track the repetition and syntactic realization of entities in adjacent sentences (Barzilay and Lapata, 2008; Elsner and Charniak, 2008) and content approaches for global coherence which view texts as a sequence of topics, each characterized by a particular distribution of lexical items (Barzilay and Lee, 2004; Fung and Ngai, 2006). Other work has shown that co-occurrence of words (Lapata, 2003; Soricut and Marcu, 2006) and discourse relations (Pitler and Nenkova, 2008; Lin et al., 2011) also predict coherence. Early theories (Grosz and Sidner, 1986) posited that there are three factors which collectively contribute to coherence: intentional structure (purpose of discourse), attentional structure (what items are discussed) and</context>
</contexts>
<marker>Elsner, Charniak, 2008</marker>
<rawString>Micha Elsner and Eugene Charniak. 2008. Coreferenceinspired coherence modeling. In Proceedings of ACLHLT, Short Papers, pages 41–44.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Micha Elsner</author>
<author>Eugene Charniak</author>
</authors>
<title>Extending the entity grid with entity-specific features.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL-HLT,</booktitle>
<pages>125--129</pages>
<contexts>
<context position="23403" citStr="Elsner and Charniak, 2011" startWordPosition="3893" endWordPosition="3896">nal document order and the other is a random permutation of the sentences from the same document. Since the original article is always more coherent than a random permutation, a model can be evaluated using 4http://www.cs.brown.edu/~melsner/manual.html the accuracy with which it can identify the original article in the pair, i.e. it assigns higher probability to the original article. This setting is not ideal but has become the de facto standard for evaluation of coherence models (Barzilay and Lee, 2004; Elsner et al., 2007; Barzilay and Lapata, 2008; Karamanis et al., 2009; Lin et al., 2011; Elsner and Charniak, 2011). It is however based on a reasonable assumption as recent work (Lin et al., 2011) shows that people identify the original article as more coherent than its permutations with over 90% accuracy and assessors also have high agreement. Later, we present an experiment distinguishing conference from workshop articles as a more realistic evaluation. We use two corpora that are widely employed for coherence prediction (Barzilay and Lee, 2004; Elsner et al., 2007; Barzilay and Lapata, 2008; Lin et al., 2011). One contains reports on airplane accidents from the National Transportation Safety Board and </context>
</contexts>
<marker>Elsner, Charniak, 2011</marker>
<rawString>Micha Elsner and Eugene Charniak. 2011. Extending the entity grid with entity-specific features. In Proceedings of ACL-HLT, pages 125–129.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Micha Elsner</author>
<author>Joseph Austerweil</author>
<author>Eugene Charniak</author>
</authors>
<title>A unified local and global model for discourse coherence.</title>
<date>2007</date>
<booktitle>In Proceedings of NAACL-HLT,</booktitle>
<pages>436--443</pages>
<contexts>
<context position="18102" citStr="Elsner et al., 2007" startWordPosition="3010" endWordPosition="3013">| where wi and wj are syntactic items and c(wi, wj) is the number of sentences that contain the item wi immediately followed by a sentence that contains wj. |V |is the vocabulary size for syntactic items. 3.2.2 Global structure Now we turn to a global coherence approach that implements the assumption that sentences with similar syntax have the same communicative goal as well as captures the patterns in communicative goals in the discourse. This approach uses a Hidden Markov Model (HMM) which has been a popular implementation for modeling coherence (Barzilay and Lee, 2004; Fung and Ngai, 2006; Elsner et al., 2007). The hidden states in our model depict communicative goals by encoding a probability distribution over syntactic items. This distribution gives higher weight to syntactic items that are more likely for that communicative goal. Transitions between states record the common patterns in intentional structure for the domain. In this syntax-HMM, states hk are created by clustering the sentences from the documents in the training set by syntactic similarity. For the productions representation of syntax, the features for clustering are the number of times a given production appeared in the parse of t</context>
<context position="23306" citStr="Elsner et al., 2007" startWordPosition="3877" endWordPosition="3880">low the common approach from prior work and use pairs of articles, where one has the original document order and the other is a random permutation of the sentences from the same document. Since the original article is always more coherent than a random permutation, a model can be evaluated using 4http://www.cs.brown.edu/~melsner/manual.html the accuracy with which it can identify the original article in the pair, i.e. it assigns higher probability to the original article. This setting is not ideal but has become the de facto standard for evaluation of coherence models (Barzilay and Lee, 2004; Elsner et al., 2007; Barzilay and Lapata, 2008; Karamanis et al., 2009; Lin et al., 2011; Elsner and Charniak, 2011). It is however based on a reasonable assumption as recent work (Lin et al., 2011) shows that people identify the original article as more coherent than its permutations with over 90% accuracy and assessors also have high agreement. Later, we present an experiment distinguishing conference from workshop articles as a more realistic evaluation. We use two corpora that are widely employed for coherence prediction (Barzilay and Lee, 2004; Elsner et al., 2007; Barzilay and Lapata, 2008; Lin et al., 201</context>
<context position="29048" citStr="Elsner et al., 2007" startWordPosition="4839" endWordPosition="4842">y of a combination is better than that using any of its smaller subsets, the value is bolded. We find that syntax supplements both content and entity grid methods. While on the airplane corpus syntax only combines well with the entity grid, on the earthquake corpus, both entity and content approaches give better accuracies when combined with syntax. However, adding all three approaches does not outperform combinations of any two of them. This result can be due to the simple approach that we tested for combination. In prior work, content and entity grid methods have been combined generatively (Elsner et al., 2007) and using discriminative training with different objectives (Soricut and Model Egrid + HMM-prodn Egrid + HMM-d-seq Egrid + Content + HMM-prodn Egrid + Content + HMM-d-seq Egrid + Content + HMM-prodn Content + Egrid Content + HMM-prodn Content + HMM-d-seq + HMM-d-seq Table 6: Accuracies for combined approaches Accid. 76.8 74.2 82.1 79.6 84.2 79.5 84.1 83.6 Earthq. 90.7 95.3 90.3 93.9 91.1 95.0 92.3 95.7 1163 Marcu, 2006). Such approaches might bring out the complementary strengths of the different aspects better and we leave such analysis for future work. 6 Predictions on academic articles The</context>
</contexts>
<marker>Elsner, Austerweil, Charniak, 2007</marker>
<rawString>Micha Elsner, Joseph Austerweil, and Eugene Charniak. 2007. A unified local and global model for discourse coherence. In Proceedings of NAACL-HLT, pages 436–443.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascale Fung</author>
<author>Grace Ngai</author>
</authors>
<title>One story, one flow: Hidden markov story models for multilingual multidocument summarization.</title>
<date>2006</date>
<journal>ACM Transactions on Speech and Language Processing,</journal>
<volume>3</volume>
<issue>2</issue>
<contexts>
<context position="1460" citStr="Fung and Ngai, 2006" startWordPosition="214" endWordPosition="217"> successfully predict the coherence of abstract, introduction and related work sections of these articles. 1 Introduction Recent studies have introduced successful automatic methods to predict the structure and coherence of texts. They include entity approaches for local coherence which track the repetition and syntactic realization of entities in adjacent sentences (Barzilay and Lapata, 2008; Elsner and Charniak, 2008) and content approaches for global coherence which view texts as a sequence of topics, each characterized by a particular distribution of lexical items (Barzilay and Lee, 2004; Fung and Ngai, 2006). Other work has shown that co-occurrence of words (Lapata, 2003; Soricut and Marcu, 2006) and discourse relations (Pitler and Nenkova, 2008; Lin et al., 2011) also predict coherence. Early theories (Grosz and Sidner, 1986) posited that there are three factors which collectively contribute to coherence: intentional structure (purpose of discourse), attentional structure (what items are discussed) and the organization of discourse segments. The highly successful entity approaches capture attentional structure and content approaches are related to topic segments but intentional structure has lar</context>
<context position="18080" citStr="Fung and Ngai, 2006" startWordPosition="3006" endWordPosition="3009">i) = c(wi) + δC ∗ |V | where wi and wj are syntactic items and c(wi, wj) is the number of sentences that contain the item wi immediately followed by a sentence that contains wj. |V |is the vocabulary size for syntactic items. 3.2.2 Global structure Now we turn to a global coherence approach that implements the assumption that sentences with similar syntax have the same communicative goal as well as captures the patterns in communicative goals in the discourse. This approach uses a Hidden Markov Model (HMM) which has been a popular implementation for modeling coherence (Barzilay and Lee, 2004; Fung and Ngai, 2006; Elsner et al., 2007). The hidden states in our model depict communicative goals by encoding a probability distribution over syntactic items. This distribution gives higher weight to syntactic items that are more likely for that communicative goal. Transitions between states record the common patterns in intentional structure for the domain. In this syntax-HMM, states hk are created by clustering the sentences from the documents in the training set by syntactic similarity. For the productions representation of syntax, the features for clustering are the number of times a given production appe</context>
<context position="21075" citStr="Fung and Ngai (2006)" startWordPosition="3513" endWordPosition="3516">mber of clusters C, smoothing constants δC, δE, δM and d for d-sequences—are tuned to optimize how well the model can distinguish coherent from incoherent articles. We describe these settings in Section 5.1. 4 Content and entity grid models We compare the syntax model with content model and entity grid methods. These approaches are the most popular ones from prior work and also allow us to test the complementary nature of syntax with 1161 lexical statistics and entity structure. This section explains how we implemented these approaches. Content models introduced by Barzilay and Lee (2004) and Fung and Ngai (2006) use lexically driven HMMs to capture coherence. The hidden states represent the topics of the domain and encode a probability distribution over words. Transitions between states record the probable succession of topics. We built a content model using our HMM implementation. Clusters are created using word bigram features after replacing numbers and proper names with tags NUM and PROP. The emissions are given by a bigram language model on words from the clustered sentences. Barzilay and Lee (2004) also employ an iterative clustering procedure before finalizing the states of the HMM but our met</context>
</contexts>
<marker>Fung, Ngai, 2006</marker>
<rawString>Pascale Fung and Grace Ngai. 2006. One story, one flow: Hidden markov story models for multilingual multidocument summarization. ACM Transactions on Speech and Language Processing, 3(2):1–16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara J Grosz</author>
<author>Candace L Sidner</author>
</authors>
<title>Attention, intentions, and the structure of discourse.</title>
<date>1986</date>
<journal>Computational Linguistics,</journal>
<volume>3</volume>
<issue>12</issue>
<contexts>
<context position="1683" citStr="Grosz and Sidner, 1986" startWordPosition="249" endWordPosition="252"> of texts. They include entity approaches for local coherence which track the repetition and syntactic realization of entities in adjacent sentences (Barzilay and Lapata, 2008; Elsner and Charniak, 2008) and content approaches for global coherence which view texts as a sequence of topics, each characterized by a particular distribution of lexical items (Barzilay and Lee, 2004; Fung and Ngai, 2006). Other work has shown that co-occurrence of words (Lapata, 2003; Soricut and Marcu, 2006) and discourse relations (Pitler and Nenkova, 2008; Lin et al., 2011) also predict coherence. Early theories (Grosz and Sidner, 1986) posited that there are three factors which collectively contribute to coherence: intentional structure (purpose of discourse), attentional structure (what items are discussed) and the organization of discourse segments. The highly successful entity approaches capture attentional structure and content approaches are related to topic segments but intentional structure has largely been neglected. Every discourse has a purpose: explaining a concept, narrating an event, critiquing an idea and so on. As a result each sentence in the article has a communicative goal and the sequence of goals helps t</context>
</contexts>
<marker>Grosz, Sidner, 1986</marker>
<rawString>Barbara J. Grosz and Candace L. Sidner. 1986. Attention, intentions, and the structure of discourse. Computational Linguistics, 3(12):175–204.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yufan Guo</author>
<author>Anna Korhonen</author>
<author>Thierry Poibeau</author>
</authors>
<title>A weakly-supervised approach to argumentative zoning of scientific documents.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>273--283</pages>
<contexts>
<context position="29987" citStr="Guo et al., 2011" startWordPosition="4986" endWordPosition="4989">8 74.2 82.1 79.6 84.2 79.5 84.1 83.6 Earthq. 90.7 95.3 90.3 93.9 91.1 95.0 92.3 95.7 1163 Marcu, 2006). Such approaches might bring out the complementary strengths of the different aspects better and we leave such analysis for future work. 6 Predictions on academic articles The distinctive intentional structure of academic articles has motivated several proposals to define and annotate the communicative purpose (argumentative zone) of each sentence (Swales, 1990; Teufel et al., 1999; Liakata et al., 2010). Supervised classifiers were also built to identify these zones (Teufel and Moens, 2000; Guo et al., 2011). So we expect that these articles form a good testbed for our models. In the remainder of the paper, we examine how unsupervised patterns discovered by our approach relate to zones and how well our models predict coherence for articles from this genre. We employ two corpora of scientific articles. ART Corpus: contains a set of 225 Chemistry journal articles that were manually annotated for intentional structure (Liakata and Soldatova, 2008). Each sentence was assigned one of 11 zone labels: Result, Conclusion, Objective, Method, Goal, Background, Observation, Experiment, Motivation, Model, Hy</context>
</contexts>
<marker>Guo, Korhonen, Poibeau, 2011</marker>
<rawString>Yufan Guo, Anna Korhonen, and Thierry Poibeau. 2011. A weakly-supervised approach to argumentative zoning of scientific documents. In Proceedings of EMNLP, pages 273–283.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
</authors>
<title>Forest reranking: Discriminative parsing with non-local features.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-HLT,</booktitle>
<pages>586--594</pages>
<contexts>
<context position="13006" citStr="Huang, 2008" startWordPosition="2172" endWordPosition="2173">e is INTRODUCE X / STATEMENT BY X. In the remainder of the paper we formalize our representation of syntax and the derived model of coherence and test its efficacy in three domains. 3 Coherence models using syntax We first describe the two representations of sentence structure we adopted for our analysis.3 Next, we 3Our representations are similar to features used for reranking in parsing. Our first representation corresponds to “rules” features (Charniak and Johnson, 2005; Collins and Koo, 2005), and our second representation is related to “spines” (Carreras et al., 2008) and edge annotation(Huang, 2008). 1159 present two coherence models: a local model which captures the co-occurrence of structural features in adjacent sentences and a global one which learns from clusters of sentences with similar syntax. 3.1 Representing syntax Our models rely exclusively on syntactic cues. We derive representations from constituent parses of the sentences, and terminals (words) are removed from the parse tree before any processing is done. The leaf nodes in our parse trees are part of speech tags. Productions: In this representation we view each sentence as the set of grammatical productions, LHS —* RHS, w</context>
</contexts>
<marker>Huang, 2008</marker>
<rawString>Liang Huang. 2008. Forest reranking: Discriminative parsing with non-local features. In Proceedings of ACL-HLT, pages 586–594, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nikiforos Karamanis</author>
<author>Chris Mellish</author>
<author>Massimo Poesio</author>
<author>Jon Oberlander</author>
</authors>
<title>Evaluating centering for information ordering using corpora.</title>
<date>2009</date>
<journal>Computational Linguistics,</journal>
<volume>35</volume>
<issue>1</issue>
<contexts>
<context position="23357" citStr="Karamanis et al., 2009" startWordPosition="3885" endWordPosition="3888">pairs of articles, where one has the original document order and the other is a random permutation of the sentences from the same document. Since the original article is always more coherent than a random permutation, a model can be evaluated using 4http://www.cs.brown.edu/~melsner/manual.html the accuracy with which it can identify the original article in the pair, i.e. it assigns higher probability to the original article. This setting is not ideal but has become the de facto standard for evaluation of coherence models (Barzilay and Lee, 2004; Elsner et al., 2007; Barzilay and Lapata, 2008; Karamanis et al., 2009; Lin et al., 2011; Elsner and Charniak, 2011). It is however based on a reasonable assumption as recent work (Lin et al., 2011) shows that people identify the original article as more coherent than its permutations with over 90% accuracy and assessors also have high agreement. Later, we present an experiment distinguishing conference from workshop articles as a more realistic evaluation. We use two corpora that are widely employed for coherence prediction (Barzilay and Lee, 2004; Elsner et al., 2007; Barzilay and Lapata, 2008; Lin et al., 2011). One contains reports on airplane accidents from</context>
</contexts>
<marker>Karamanis, Mellish, Poesio, Oberlander, 2009</marker>
<rawString>Nikiforos Karamanis, Chris Mellish, Massimo Poesio, and Jon Oberlander. 2009. Evaluating centering for information ordering using corpora. Computational Linguistics, 35(1):29–46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>423--430</pages>
<contexts>
<context position="24876" citStr="Klein and Manning, 2003" startWordPosition="4139" endWordPosition="4142">ndard ones used in prior work on lexical, entity and discourse relation based coherence models. Later in Section 6, we show that the models perform well on the academic genre and longer articles too. For each of the two corpora, we have 100 articles for training and 100 (accidents) and 99 (earthquakes) for testing. A maximum of 20 random permutations were generated for each test article to create the pairwise data (total of 1986 test pairs for the accident corpus and 1956 for earthquakes).5 The baseline accuracy for random prediction is 50%. The articles were parsed using the Stanford parser (Klein and Manning, 2003). 5.1 Accuracy of the syntax model For each model, the relevant parameters were tuned using 10-fold cross validation on the training data. In each fold, 90 documents were used for training and evaluation was done on permutations from the remaining articles. After tuning, the final model was trained on all 100 articles in the training set. 5We downloaded the permutations from http://people. csail.mit.edu/regina/coherence/CLsubmission/ n i=1 P(T) = M H j=1 1162 Model Prodns d-seq POS Prodns d-seq Egrid Content Table 5 shows the results on the test set. The best number of clusters and depth for d</context>
<context position="31914" citStr="Klein and Manning, 2003" startWordPosition="5287" endWordPosition="5290">. Similarly, we obtain a development corpus of 36 ACL-NAACL articles. We create two test sets: one has 500 ACL-NAACL conference articles and another has 500 articles from ACL-sponsored workshops. We only choose articles in which all three sections—abstract, introduction and related work— 6Some articles did not have labelled ‘introduction’ sections resulting in fewer examples for this setup. 7http://aye.comp.nus.edu.sg/parsCit/ could be successfully identified using Parscit.8 This data was sentence-segmented using MxTerminator (Reynar and Ratnaparkhi, 1997) and parsed with the Stanford Parser (Klein and Manning, 2003). For each corpus and each section, we train all our syntactic models: the two local coherence models using the production and d-sequence representations and the HMM models with the two representations. These models are tuned on the respective development data, on the task of differentiating the original from a permuted section. For this purpose, we created a maximum of 30 permutations per article. 6.1 Comparison with ART Corpus zones We perform this analysis using the ART corpus. The zone annotations present in this corpus allow us to directly test our first assumption in this work, that sent</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Accurate unlexicalized parsing. In Proceedings of ACL, pages 423–430.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mirella Lapata</author>
<author>Regina Barzilay</author>
</authors>
<title>Automatic evaluation of text coherence: Models and representations.</title>
<date>2005</date>
<booktitle>In Proceedings of IJCAI.</booktitle>
<contexts>
<context position="21928" citStr="Lapata and Barzilay (2005)" startWordPosition="3651" endWordPosition="3654">lt a content model using our HMM implementation. Clusters are created using word bigram features after replacing numbers and proper names with tags NUM and PROP. The emissions are given by a bigram language model on words from the clustered sentences. Barzilay and Lee (2004) also employ an iterative clustering procedure before finalizing the states of the HMM but our method only uses one-step clustering. Despite the difference, the content model accuracies for our implementation are quite close to that from the original. For the entity grid model, we follow the generative approach proposed by Lapata and Barzilay (2005). A text is converted into a matrix, where rows correspond to sentences, in the order in which they appear in the article. Columns are created one for each entity appearing in the text. Each cell (i,j) is filled with the grammatical role ri,j of the entity j in sentence i. We computed the entity grids using the Brown Coherence Toolkit4. The probability of the text (T) is defined using the likely sequence of grammatical role transitions. p(ri,j|ri−1,j...ri−h,j) for m entities and n sentences. Parameter h controls the history size for transitions and is tuned during development. When h = 1, for </context>
</contexts>
<marker>Lapata, Barzilay, 2005</marker>
<rawString>Mirella Lapata and Regina Barzilay. 2005. Automatic evaluation of text coherence: Models and representations. In Proceedings of IJCAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mirella Lapata</author>
</authors>
<title>Probabilistic text structuring: Experiments with sentence ordering.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>545--552</pages>
<contexts>
<context position="1524" citStr="Lapata, 2003" startWordPosition="226" endWordPosition="227">d work sections of these articles. 1 Introduction Recent studies have introduced successful automatic methods to predict the structure and coherence of texts. They include entity approaches for local coherence which track the repetition and syntactic realization of entities in adjacent sentences (Barzilay and Lapata, 2008; Elsner and Charniak, 2008) and content approaches for global coherence which view texts as a sequence of topics, each characterized by a particular distribution of lexical items (Barzilay and Lee, 2004; Fung and Ngai, 2006). Other work has shown that co-occurrence of words (Lapata, 2003; Soricut and Marcu, 2006) and discourse relations (Pitler and Nenkova, 2008; Lin et al., 2011) also predict coherence. Early theories (Grosz and Sidner, 1986) posited that there are three factors which collectively contribute to coherence: intentional structure (purpose of discourse), attentional structure (what items are discussed) and the organization of discourse segments. The highly successful entity approaches capture attentional structure and content approaches are related to topic segments but intentional structure has largely been neglected. Every discourse has a purpose: explaining a</context>
</contexts>
<marker>Lapata, 2003</marker>
<rawString>Mirella Lapata. 2003. Probabilistic text structuring: Experiments with sentence ordering. In Proceedings of ACL, pages 545–552.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maria Liakata</author>
<author>Larisa Soldatova</author>
</authors>
<title>Guidelines for the annotation of general scientific concepts.</title>
<date>2008</date>
<tech>JISC Project Report.</tech>
<contexts>
<context position="30432" citStr="Liakata and Soldatova, 2008" startWordPosition="5060" endWordPosition="5063">) of each sentence (Swales, 1990; Teufel et al., 1999; Liakata et al., 2010). Supervised classifiers were also built to identify these zones (Teufel and Moens, 2000; Guo et al., 2011). So we expect that these articles form a good testbed for our models. In the remainder of the paper, we examine how unsupervised patterns discovered by our approach relate to zones and how well our models predict coherence for articles from this genre. We employ two corpora of scientific articles. ART Corpus: contains a set of 225 Chemistry journal articles that were manually annotated for intentional structure (Liakata and Soldatova, 2008). Each sentence was assigned one of 11 zone labels: Result, Conclusion, Objective, Method, Goal, Background, Observation, Experiment, Motivation, Model, Hypothesis. For our study, we use the annotation of the introduction and the abstract sections. We divide the data into training, development and test sets. For abstracts, we have 75, 50 and 100 for these sets respectively. For introductions, this split is 75, 31, 82.6 ACL Anthology Network (AAN) Corpus: Radev et al. (2009) provides the full text of publications from ACL venues. These articles do not have any zone annotations. The AAN corpus i</context>
</contexts>
<marker>Liakata, Soldatova, 2008</marker>
<rawString>Maria Liakata and Larisa Soldatova. 2008. Guidelines for the annotation of general scientific concepts. JISC Project Report.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maria Liakata</author>
<author>Simone Teufel</author>
<author>Advaith Siddharthan</author>
<author>Colin Batchelor</author>
</authors>
<title>Corpora for the conceptualisation and zoning of scientific papers.</title>
<date>2010</date>
<booktitle>In Proceedings of LREC.</booktitle>
<contexts>
<context position="6393" citStr="Liakata et al., 2010" startWordPosition="989" endWordPosition="992">ntages. Results show that syntax models can distinguish coherent and incoherent news articles from two domains with 75- 90% accuracies over a 50% baseline. In addition, the syntax coherence scores turn out complementary to scores given by lexical and entity models. We also study our models’ predictions on academic articles, a genre where intentional structure is widely studied. Sections in these articles have well-defined purposes and we find recurring sentence types such as motivation, citations, description, and speculations. There is a large body of work (Swales, 1990; Teufel et al., 1999; Liakata et al., 2010) concerned with defining and annotating these sentence types (called zones) in conference articles. In Section 6, we describe how indeed some patterns captured by the syntax-based models are correlated with zone categories that were proposed in prior literature. We also present results on coherence prediction: our model can distinguish the introduction section of conference papers from its perturbed versions with over 70% accuracy. Further, our model is able to identify conference from workshop papers with good accuracies, given that we can expect these articles to vary in purpose. 2 Evidence </context>
<context position="29880" citStr="Liakata et al., 2010" startWordPosition="4969" endWordPosition="4972">grid Content + HMM-prodn Content + HMM-d-seq + HMM-d-seq Table 6: Accuracies for combined approaches Accid. 76.8 74.2 82.1 79.6 84.2 79.5 84.1 83.6 Earthq. 90.7 95.3 90.3 93.9 91.1 95.0 92.3 95.7 1163 Marcu, 2006). Such approaches might bring out the complementary strengths of the different aspects better and we leave such analysis for future work. 6 Predictions on academic articles The distinctive intentional structure of academic articles has motivated several proposals to define and annotate the communicative purpose (argumentative zone) of each sentence (Swales, 1990; Teufel et al., 1999; Liakata et al., 2010). Supervised classifiers were also built to identify these zones (Teufel and Moens, 2000; Guo et al., 2011). So we expect that these articles form a good testbed for our models. In the remainder of the paper, we examine how unsupervised patterns discovered by our approach relate to zones and how well our models predict coherence for articles from this genre. We employ two corpora of scientific articles. ART Corpus: contains a set of 225 Chemistry journal articles that were manually annotated for intentional structure (Liakata and Soldatova, 2008). Each sentence was assigned one of 11 zone labe</context>
</contexts>
<marker>Liakata, Teufel, Siddharthan, Batchelor, 2010</marker>
<rawString>Maria Liakata, Simone Teufel, Advaith Siddharthan, and Colin Batchelor. 2010. Corpora for the conceptualisation and zoning of scientific papers. In Proceedings of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ziheng Lin</author>
<author>Min-Yen Kan</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Recognizing implicit discourse relations in the Penn Discourse Treebank.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>343--351</pages>
<contexts>
<context position="5107" citStr="Lin et al. (2009)" startWordPosition="786" endWordPosition="789">ship on a broad scale using a coherence model based entirely on syntax. The model relies on two assumptions which summarize our intuitions about syntax and intentional structure: 1. Sentences with similar syntax are likely to have the same communicative goal. 2. Regularities in intentional structure will be manifested in syntactic regularities between adjacent sentences. There is also evidence from recent work that supports these assumptions. Cheung and Penn (2010) find that a better syntactic parse of a sentence can be derived when the syntax of adjacent sentences is also taken into account. Lin et al. (2009) report that the syntactic productions in adjacent sentences are powerful features for predicting which discourse relation (cause, contrast, etc.) holds between them. Cocco et al. (2011) show that significant associations exist between certain part of speech tags and sentence types such as explanation, dialog and argumentation. In our model, syntax is represented either as parse tree productions or a sequence of phrasal nodes augmented with part of speech tags. Our best performing method uses a Hidden Markov Model to learn the patterns in these syntactic items. Sections 3 and 5 discuss the rep</context>
</contexts>
<marker>Lin, Kan, Ng, 2009</marker>
<rawString>Ziheng Lin, Min-Yen Kan, and Hwee Tou Ng. 2009. Recognizing implicit discourse relations in the Penn Discourse Treebank. In Proceedings of EMNLP, pages 343–351.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ziheng Lin</author>
<author>Hwee Tou Ng</author>
<author>Min-Yen Kan</author>
</authors>
<title>Automatically evaluating text coherence using discourse relations.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL-HLT,</booktitle>
<pages>997--1006</pages>
<contexts>
<context position="1619" citStr="Lin et al., 2011" startWordPosition="239" endWordPosition="242">l automatic methods to predict the structure and coherence of texts. They include entity approaches for local coherence which track the repetition and syntactic realization of entities in adjacent sentences (Barzilay and Lapata, 2008; Elsner and Charniak, 2008) and content approaches for global coherence which view texts as a sequence of topics, each characterized by a particular distribution of lexical items (Barzilay and Lee, 2004; Fung and Ngai, 2006). Other work has shown that co-occurrence of words (Lapata, 2003; Soricut and Marcu, 2006) and discourse relations (Pitler and Nenkova, 2008; Lin et al., 2011) also predict coherence. Early theories (Grosz and Sidner, 1986) posited that there are three factors which collectively contribute to coherence: intentional structure (purpose of discourse), attentional structure (what items are discussed) and the organization of discourse segments. The highly successful entity approaches capture attentional structure and content approaches are related to topic segments but intentional structure has largely been neglected. Every discourse has a purpose: explaining a concept, narrating an event, critiquing an idea and so on. As a result each sentence in the ar</context>
<context position="23375" citStr="Lin et al., 2011" startWordPosition="3889" endWordPosition="3892"> one has the original document order and the other is a random permutation of the sentences from the same document. Since the original article is always more coherent than a random permutation, a model can be evaluated using 4http://www.cs.brown.edu/~melsner/manual.html the accuracy with which it can identify the original article in the pair, i.e. it assigns higher probability to the original article. This setting is not ideal but has become the de facto standard for evaluation of coherence models (Barzilay and Lee, 2004; Elsner et al., 2007; Barzilay and Lapata, 2008; Karamanis et al., 2009; Lin et al., 2011; Elsner and Charniak, 2011). It is however based on a reasonable assumption as recent work (Lin et al., 2011) shows that people identify the original article as more coherent than its permutations with over 90% accuracy and assessors also have high agreement. Later, we present an experiment distinguishing conference from workshop articles as a more realistic evaluation. We use two corpora that are widely employed for coherence prediction (Barzilay and Lee, 2004; Elsner et al., 2007; Barzilay and Lapata, 2008; Lin et al., 2011). One contains reports on airplane accidents from the National Tran</context>
</contexts>
<marker>Lin, Ng, Kan, 2011</marker>
<rawString>Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2011. Automatically evaluating text coherence using discourse relations. In Proceedings of ACL-HLT, pages 997– 1006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of english: The penn treebank.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="7764" citStr="Marcus et al., 1994" startWordPosition="1198" endWordPosition="1201">rence. This study validates our second assumption relating the syntax of adjacent sentences. Later in Section 6, we examine syntactic patterns in individual sentences (assumption 1) using a corpus of academic articles where sentences were manually annotated with communicative goals. Prior work has reported that certain grammatical productions are repeated in adjacent sentences more often than would be expected by chance (Reitter et al., 2006; Cheung and Penn, 2010). We analyze all co-occurrence patterns rather than just repetitions. We use the gold standard parse trees from the Penn Treebank (Marcus et al., 1994). Our unit of analysis is a pair of adjacent sentences (S1, S2) and we choose to use Section 0 of the corpus which has 99 documents and 1727 sentence pairs. We enumerate all productions that appear in the syntactic parse of any sentence and exclude those that appear less than 25 times, resulting in a list of 197 unique productions. Then all ordered pairs2 (P1, P2) of productions are formed. For each pair, we compute 2(p1, p2) and (p2, p1) are considered as different pairs. 1158 P1, P2 NP —. NP NP-ADV QP —. CD CD VP —. VB VP NP-SBJ —. NNP NNP NP-LOC —. NNP S-TPC-1 —. NP-SBJ VP Sentence 1 The tw</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1994</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1994. Building a large annotated corpus of english: The penn treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emily Pitler</author>
<author>Ani Nenkova</author>
</authors>
<title>Revisiting readability: A unified framework for predicting text quality.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>186--195</pages>
<contexts>
<context position="1600" citStr="Pitler and Nenkova, 2008" startWordPosition="235" endWordPosition="238"> have introduced successful automatic methods to predict the structure and coherence of texts. They include entity approaches for local coherence which track the repetition and syntactic realization of entities in adjacent sentences (Barzilay and Lapata, 2008; Elsner and Charniak, 2008) and content approaches for global coherence which view texts as a sequence of topics, each characterized by a particular distribution of lexical items (Barzilay and Lee, 2004; Fung and Ngai, 2006). Other work has shown that co-occurrence of words (Lapata, 2003; Soricut and Marcu, 2006) and discourse relations (Pitler and Nenkova, 2008; Lin et al., 2011) also predict coherence. Early theories (Grosz and Sidner, 1986) posited that there are three factors which collectively contribute to coherence: intentional structure (purpose of discourse), attentional structure (what items are discussed) and the organization of discourse segments. The highly successful entity approaches capture attentional structure and content approaches are related to topic segments but intentional structure has largely been neglected. Every discourse has a purpose: explaining a concept, narrating an event, critiquing an idea and so on. As a result each</context>
</contexts>
<marker>Pitler, Nenkova, 2008</marker>
<rawString>Emily Pitler and Ani Nenkova. 2008. Revisiting readability: A unified framework for predicting text quality. In Proceedings of EMNLP, pages 186–195.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dragomir R Radev</author>
<author>Mark Thomas Joseph</author>
<author>Bryan Gibson</author>
<author>Pradeep Muthukrishnan</author>
</authors>
<title>A Bibliometric and Network Analysis of the field of Computational Linguistics.</title>
<date>2009</date>
<journal>Journal of the American Society for Information Science and Technology.</journal>
<contexts>
<context position="30910" citStr="Radev et al. (2009)" startWordPosition="5135" endWordPosition="5138">Corpus: contains a set of 225 Chemistry journal articles that were manually annotated for intentional structure (Liakata and Soldatova, 2008). Each sentence was assigned one of 11 zone labels: Result, Conclusion, Objective, Method, Goal, Background, Observation, Experiment, Motivation, Model, Hypothesis. For our study, we use the annotation of the introduction and the abstract sections. We divide the data into training, development and test sets. For abstracts, we have 75, 50 and 100 for these sets respectively. For introductions, this split is 75, 31, 82.6 ACL Anthology Network (AAN) Corpus: Radev et al. (2009) provides the full text of publications from ACL venues. These articles do not have any zone annotations. The AAN corpus is produced from OCR analysis and no section marking is available. To recreate these, we use the Parscit tagger7 (Councill et al., 2008). We use articles from years 1999 to 2011. For training, we randomly choose 70 articles from ACL and NAACL main conferences. Similarly, we obtain a development corpus of 36 ACL-NAACL articles. We create two test sets: one has 500 ACL-NAACL conference articles and another has 500 articles from ACL-sponsored workshops. We only choose articles </context>
</contexts>
<marker>Radev, Joseph, Gibson, Muthukrishnan, 2009</marker>
<rawString>Dragomir R. Radev, Mark Thomas Joseph, Bryan Gibson, and Pradeep Muthukrishnan. 2009. A Bibliometric and Network Analysis of the field of Computational Linguistics. Journal of the American Society for Information Science and Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Reitter</author>
<author>Johanna D Moore</author>
<author>Frank Keller</author>
</authors>
<title>Priming of Syntactic Rules in Task-Oriented Dialogue and Spontaneous Conversation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 28th Annual Conference of the Cognitive Science Society,</booktitle>
<pages>685--690</pages>
<contexts>
<context position="7589" citStr="Reitter et al., 2006" startWordPosition="1170" endWordPosition="1173"> in purpose. 2 Evidence for syntactic coherence We first present a pilot study that confirms that adjacent sentences in discourse exhibit stable patterns of syntactic co-occurrence. This study validates our second assumption relating the syntax of adjacent sentences. Later in Section 6, we examine syntactic patterns in individual sentences (assumption 1) using a corpus of academic articles where sentences were manually annotated with communicative goals. Prior work has reported that certain grammatical productions are repeated in adjacent sentences more often than would be expected by chance (Reitter et al., 2006; Cheung and Penn, 2010). We analyze all co-occurrence patterns rather than just repetitions. We use the gold standard parse trees from the Penn Treebank (Marcus et al., 1994). Our unit of analysis is a pair of adjacent sentences (S1, S2) and we choose to use Section 0 of the corpus which has 99 documents and 1727 sentence pairs. We enumerate all productions that appear in the syntactic parse of any sentence and exclude those that appear less than 25 times, resulting in a list of 197 unique productions. Then all ordered pairs2 (P1, P2) of productions are formed. For each pair, we compute 2(p1,</context>
</contexts>
<marker>Reitter, Moore, Keller, 2006</marker>
<rawString>David Reitter, Johanna D. Moore, and Frank Keller. 2006. Priming of Syntactic Rules in Task-Oriented Dialogue and Spontaneous Conversation. In Proceedings of the 28th Annual Conference of the Cognitive Science Society, pages 685–690.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey C Reynar</author>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>A maximum entropy approach to identifying sentence boundaries.</title>
<date>1997</date>
<booktitle>In Proceedings of the fifth conference on Applied natural language processing,</booktitle>
<pages>16--19</pages>
<contexts>
<context position="31852" citStr="Reynar and Ratnaparkhi, 1997" startWordPosition="5277" endWordPosition="5280"> we randomly choose 70 articles from ACL and NAACL main conferences. Similarly, we obtain a development corpus of 36 ACL-NAACL articles. We create two test sets: one has 500 ACL-NAACL conference articles and another has 500 articles from ACL-sponsored workshops. We only choose articles in which all three sections—abstract, introduction and related work— 6Some articles did not have labelled ‘introduction’ sections resulting in fewer examples for this setup. 7http://aye.comp.nus.edu.sg/parsCit/ could be successfully identified using Parscit.8 This data was sentence-segmented using MxTerminator (Reynar and Ratnaparkhi, 1997) and parsed with the Stanford Parser (Klein and Manning, 2003). For each corpus and each section, we train all our syntactic models: the two local coherence models using the production and d-sequence representations and the HMM models with the two representations. These models are tuned on the respective development data, on the task of differentiating the original from a permuted section. For this purpose, we created a maximum of 30 permutations per article. 6.1 Comparison with ART Corpus zones We perform this analysis using the ART corpus. The zone annotations present in this corpus allow us</context>
</contexts>
<marker>Reynar, Ratnaparkhi, 1997</marker>
<rawString>Jeffrey C. Reynar and Adwait Ratnaparkhi. 1997. A maximum entropy approach to identifying sentence boundaries. In Proceedings of the fifth conference on Applied natural language processing, pages 16–19.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radu Soricut</author>
<author>Daniel Marcu</author>
</authors>
<title>Discourse generation using utility-trained coherence models.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING-ACL,</booktitle>
<pages>803--810</pages>
<contexts>
<context position="1550" citStr="Soricut and Marcu, 2006" startWordPosition="228" endWordPosition="231">s of these articles. 1 Introduction Recent studies have introduced successful automatic methods to predict the structure and coherence of texts. They include entity approaches for local coherence which track the repetition and syntactic realization of entities in adjacent sentences (Barzilay and Lapata, 2008; Elsner and Charniak, 2008) and content approaches for global coherence which view texts as a sequence of topics, each characterized by a particular distribution of lexical items (Barzilay and Lee, 2004; Fung and Ngai, 2006). Other work has shown that co-occurrence of words (Lapata, 2003; Soricut and Marcu, 2006) and discourse relations (Pitler and Nenkova, 2008; Lin et al., 2011) also predict coherence. Early theories (Grosz and Sidner, 1986) posited that there are three factors which collectively contribute to coherence: intentional structure (purpose of discourse), attentional structure (what items are discussed) and the organization of discourse segments. The highly successful entity approaches capture attentional structure and content approaches are related to topic segments but intentional structure has largely been neglected. Every discourse has a purpose: explaining a concept, narrating an eve</context>
</contexts>
<marker>Soricut, Marcu, 2006</marker>
<rawString>Radu Soricut and Daniel Marcu. 2006. Discourse generation using utility-trained coherence models. In Proceedings of COLING-ACL, pages 803–810.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Swales</author>
</authors>
<title>Genre analysis: English in academic and research settings, volume 11.</title>
<date>1990</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="6349" citStr="Swales, 1990" startWordPosition="983" endWordPosition="984">c implementations and relative advantages. Results show that syntax models can distinguish coherent and incoherent news articles from two domains with 75- 90% accuracies over a 50% baseline. In addition, the syntax coherence scores turn out complementary to scores given by lexical and entity models. We also study our models’ predictions on academic articles, a genre where intentional structure is widely studied. Sections in these articles have well-defined purposes and we find recurring sentence types such as motivation, citations, description, and speculations. There is a large body of work (Swales, 1990; Teufel et al., 1999; Liakata et al., 2010) concerned with defining and annotating these sentence types (called zones) in conference articles. In Section 6, we describe how indeed some patterns captured by the syntax-based models are correlated with zone categories that were proposed in prior literature. We also present results on coherence prediction: our model can distinguish the introduction section of conference papers from its perturbed versions with over 70% accuracy. Further, our model is able to identify conference from workshop papers with good accuracies, given that we can expect th</context>
<context position="29836" citStr="Swales, 1990" startWordPosition="4963" endWordPosition="4964">d + Content + HMM-prodn Content + Egrid Content + HMM-prodn Content + HMM-d-seq + HMM-d-seq Table 6: Accuracies for combined approaches Accid. 76.8 74.2 82.1 79.6 84.2 79.5 84.1 83.6 Earthq. 90.7 95.3 90.3 93.9 91.1 95.0 92.3 95.7 1163 Marcu, 2006). Such approaches might bring out the complementary strengths of the different aspects better and we leave such analysis for future work. 6 Predictions on academic articles The distinctive intentional structure of academic articles has motivated several proposals to define and annotate the communicative purpose (argumentative zone) of each sentence (Swales, 1990; Teufel et al., 1999; Liakata et al., 2010). Supervised classifiers were also built to identify these zones (Teufel and Moens, 2000; Guo et al., 2011). So we expect that these articles form a good testbed for our models. In the remainder of the paper, we examine how unsupervised patterns discovered by our approach relate to zones and how well our models predict coherence for articles from this genre. We employ two corpora of scientific articles. ART Corpus: contains a set of 225 Chemistry journal articles that were manually annotated for intentional structure (Liakata and Soldatova, 2008). Ea</context>
</contexts>
<marker>Swales, 1990</marker>
<rawString>John Swales. 1990. Genre analysis: English in academic and research settings, volume 11. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simone Teufel</author>
<author>Marc Moens</author>
</authors>
<title>What’s yours and what’s mine: determining intellectual attribution in scientific text.</title>
<date>2000</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>9--17</pages>
<contexts>
<context position="29968" citStr="Teufel and Moens, 2000" startWordPosition="4982" endWordPosition="4985">ed approaches Accid. 76.8 74.2 82.1 79.6 84.2 79.5 84.1 83.6 Earthq. 90.7 95.3 90.3 93.9 91.1 95.0 92.3 95.7 1163 Marcu, 2006). Such approaches might bring out the complementary strengths of the different aspects better and we leave such analysis for future work. 6 Predictions on academic articles The distinctive intentional structure of academic articles has motivated several proposals to define and annotate the communicative purpose (argumentative zone) of each sentence (Swales, 1990; Teufel et al., 1999; Liakata et al., 2010). Supervised classifiers were also built to identify these zones (Teufel and Moens, 2000; Guo et al., 2011). So we expect that these articles form a good testbed for our models. In the remainder of the paper, we examine how unsupervised patterns discovered by our approach relate to zones and how well our models predict coherence for articles from this genre. We employ two corpora of scientific articles. ART Corpus: contains a set of 225 Chemistry journal articles that were manually annotated for intentional structure (Liakata and Soldatova, 2008). Each sentence was assigned one of 11 zone labels: Result, Conclusion, Objective, Method, Goal, Background, Observation, Experiment, Mo</context>
</contexts>
<marker>Teufel, Moens, 2000</marker>
<rawString>Simone Teufel and Marc Moens. 2000. What’s yours and what’s mine: determining intellectual attribution in scientific text. In Proceedings of EMNLP, pages 9– 17.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simone Teufel</author>
<author>Jean Carletta</author>
<author>Marc Moens</author>
</authors>
<title>An annotation scheme for discourse-level argumentation in research articles.</title>
<date>1999</date>
<booktitle>In Proceedings of EACL,</booktitle>
<pages>110--117</pages>
<contexts>
<context position="6370" citStr="Teufel et al., 1999" startWordPosition="985" endWordPosition="988">ons and relative advantages. Results show that syntax models can distinguish coherent and incoherent news articles from two domains with 75- 90% accuracies over a 50% baseline. In addition, the syntax coherence scores turn out complementary to scores given by lexical and entity models. We also study our models’ predictions on academic articles, a genre where intentional structure is widely studied. Sections in these articles have well-defined purposes and we find recurring sentence types such as motivation, citations, description, and speculations. There is a large body of work (Swales, 1990; Teufel et al., 1999; Liakata et al., 2010) concerned with defining and annotating these sentence types (called zones) in conference articles. In Section 6, we describe how indeed some patterns captured by the syntax-based models are correlated with zone categories that were proposed in prior literature. We also present results on coherence prediction: our model can distinguish the introduction section of conference papers from its perturbed versions with over 70% accuracy. Further, our model is able to identify conference from workshop papers with good accuracies, given that we can expect these articles to vary </context>
<context position="29857" citStr="Teufel et al., 1999" startWordPosition="4965" endWordPosition="4968">HMM-prodn Content + Egrid Content + HMM-prodn Content + HMM-d-seq + HMM-d-seq Table 6: Accuracies for combined approaches Accid. 76.8 74.2 82.1 79.6 84.2 79.5 84.1 83.6 Earthq. 90.7 95.3 90.3 93.9 91.1 95.0 92.3 95.7 1163 Marcu, 2006). Such approaches might bring out the complementary strengths of the different aspects better and we leave such analysis for future work. 6 Predictions on academic articles The distinctive intentional structure of academic articles has motivated several proposals to define and annotate the communicative purpose (argumentative zone) of each sentence (Swales, 1990; Teufel et al., 1999; Liakata et al., 2010). Supervised classifiers were also built to identify these zones (Teufel and Moens, 2000; Guo et al., 2011). So we expect that these articles form a good testbed for our models. In the remainder of the paper, we examine how unsupervised patterns discovered by our approach relate to zones and how well our models predict coherence for articles from this genre. We employ two corpora of scientific articles. ART Corpus: contains a set of 225 Chemistry journal articles that were manually annotated for intentional structure (Liakata and Soldatova, 2008). Each sentence was assig</context>
</contexts>
<marker>Teufel, Carletta, Moens, 1999</marker>
<rawString>Simone Teufel, Jean Carletta, and Marc Moens. 1999. An annotation scheme for discourse-level argumentation in research articles. In Proceedings of EACL, pages 110–117.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ying Zhao</author>
<author>George Karypis</author>
<author>Usama Fayyad</author>
</authors>
<title>Hierarchical clustering algorithms for document datasets. Data Mining and Knowledge Discovery,</title>
<date>2005</date>
<pages>10--141</pages>
<contexts>
<context position="18954" citStr="Zhao et al., 2005" startWordPosition="3148" endWordPosition="3151">tions between states record the common patterns in intentional structure for the domain. In this syntax-HMM, states hk are created by clustering the sentences from the documents in the training set by syntactic similarity. For the productions representation of syntax, the features for clustering are the number of times a given production appeared in the parse of the sentence. For the d-sequence approach, the features are n-grams of size one to four of syntactic words from the sequence. Clustering was done by optimizing for average cosine similarity and was implemented using the CLUTO toolkit (Zhao et al., 2005). C clusters are formed and taken as the states of the model. Table 4 shows sentences from two clusters formed on the abstracts of journal articles using the productions representation. One of them, cluster (a), appears to capture descriptive sentences and cluster (b) involves mostly speculation type sentences. The emission probabilities for each state are modeled as a (syntactic) language model derived from the sentences in it. For productions representation, this is the unigram distribution of productions from the sentences in hk. For d-sequences, the distribution is computed for bigrams of </context>
</contexts>
<marker>Zhao, Karypis, Fayyad, 2005</marker>
<rawString>Ying Zhao, George Karypis, and Usama Fayyad. 2005. Hierarchical clustering algorithms for document datasets. Data Mining and Knowledge Discovery, 10:141–168.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
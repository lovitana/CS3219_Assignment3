<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003818">
<title confidence="0.99922">
Document-Wide Decoding for
Phrase-Based Statistical Machine Translation
</title>
<author confidence="0.994608">
Christian Hardmeier Joakim Nivre J¨org Tiedemann
</author>
<affiliation confidence="0.996691">
Uppsala University
Department of Linguistics and Philology
</affiliation>
<address confidence="0.990579">
Box 635, 751 26 Uppsala, Sweden
</address>
<email confidence="0.994236">
firstname.lastname@lingfil.uu.se
</email>
<sectionHeader confidence="0.997292" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999245933333333">
Independence between sentences is an as-
sumption deeply entrenched in the models and
algorithms used for statistical machine trans-
lation (SMT), particularly in the popular dy-
namic programming beam search decoding al-
gorithm. This restriction is an obstacle to re-
search on more sophisticated discourse-level
models for SMT. We propose a stochastic lo-
cal search decoding method for phrase-based
SMT, which permits free document-wide de-
pendencies in the models. We explore the sta-
bility and the search parameters of this method
and demonstrate that it can be successfully
used to optimise a document-level semantic
language model.
</bodyText>
<sectionHeader confidence="0.993113" genericHeader="keywords">
1 Motivation
</sectionHeader>
<bodyText confidence="0.999986547169811">
In the field of translation studies, it is undisputed that
discourse-wide context must be considered care-
fully for good translation results (Hatim and Mason,
1990). By contrast, the state of the art in statistical
machine translation (SMT), despite significant ad-
vances in the last twenty years, still assumes that
texts can be translated sentence by sentence under
strict independence assumptions, even though it is
well known that certain linguistic phenomena such
as pronominal anaphora cannot be translated cor-
rectly without referring to extra-sentential context.
This is true both for the phrase-based and the syntax-
based approach to SMT. In the rest of this paper, we
shall concentrate on phrase-based SMT.
One reason why it is difficult to experiment
with document-wide models for phrase-based SMT
is that the dynamic programming (DP) algorithm
which has been used almost exclusively for decod-
ing SMT models in the recent literature has very
strong assumptions of locality built into it. DP
beam search for phrase-based SMT was described
by Koehn et al. (2003), extending earlier work on
word-based SMT (Tillmann et al., 1997; Och et al.,
2001; Tillmann and Ney, 2003). This algorithm con-
structs output sentences by starting with an empty
hypothesis and adding output words at the end until
translations for all source words have been gener-
ated. The core models of phrase-based SMT, in par-
ticular the n-gram language model (LM), only de-
pend on a constant number of output words to the
left of the word being generated. This fact is ex-
ploited by the search algorithm with a DP technique
called hypothesis recombination (Och et al., 2001),
which permits the elimination of hypotheses from
the search space if they coincide in a certain number
of final words with a better hypothesis and no future
expansion can possibly invert the relative ranking of
the two hypotheses under the given models. Hypoth-
esis recombination achieves a substantial reduction
of the search space without affecting search optimal-
ity and makes it possible to use aggressive pruning
techniques for fast search while still obtaining good
results.
The downside of this otherwise excellent ap-
proach is that it only works well with models that
have a local dependency structure similar to that
of an n-gram language model, so they only de-
pend on a small context window for each target
word. Sentence-local models with longer dependen-
cies can be added, but doing so greatly increases
the risk for search errors by inhibiting hypothesis
recombination. Cross-sentence dependencies can-
not be directly integrated into DP SMT decoding in
</bodyText>
<page confidence="0.966114">
1179
</page>
<note confidence="0.788503">
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1179–1190, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.999369944444445">
any obvious way, especially if joint optimisation of
a number of interdependent decisions over an entire
document is required. Research into models with
a more varied, non-local dependency structure is to
some extent stifled by the difficulty of decoding such
models effectively, as can be seen by the problems
some researchers encountered when they attempted
to solve discourse-level problems. Consider, for in-
stance, the work on cache-based language models
by Tiedemann (2010) and Gong et al. (2011), where
error propagation was a serious issue, or the works
on pronominal anaphora by Le Nagard and Koehn
(2010), who implemented cross-sentence dependen-
cies with an ad-hoc two-pass decoding strategy, and
Hardmeier and Federico (2010) with the use of an
external decoder driver to manage backward-only
dependencies between sentences.
In this paper, we present a method for decoding
complete documents in phrase-based SMT. Our de-
coder uses a local search approach whose state con-
sists of a complete translation of an entire document
at any time. The initial state is improved by the ap-
plication of a series of operations using a hill climb-
ing strategy to find a (local) maximum of the score
function. This setup gives us complete freedom to
define scoring functions over the entire document.
Moreover, by optionally initialising the state with
the output of a traditional DP decoder, we can en-
sure that the final hypothesis is no worse than what
would have been found by DP search alone. We start
by describing the decoding algorithm and the state
operations used by our decoder, then we present em-
pirical results demonstrating the effectiveness of our
approach and its usability with a document-level se-
mantic language model, and finally we discuss some
related work.
</bodyText>
<sectionHeader confidence="0.947015" genericHeader="introduction">
2 SMT Decoding by Hill Climbing
</sectionHeader>
<bodyText confidence="0.999853">
In this section, we formally describe the phrase-
based SMT model implemented by our decoder as
well as the decoding algorithm we use.
</bodyText>
<sectionHeader confidence="0.613843" genericHeader="method">
2.1 SMT Model
</sectionHeader>
<bodyText confidence="0.9994861">
Our decoder is based on local search, so its state at
any time is a representation of a complete translation
of the entire document. Even though the decoder op-
erates at the document level, it is important to keep
track of sentence boundaries, and the individual op-
erations that are applied to the state are still confined
to sentence scope, so it is useful to decompose the
state of a document into the state of its sentences,
and we define the overall state S as a sequence of
sentence states:
</bodyText>
<equation confidence="0.97626">
S = S1S2 ...SN, (1)
</equation>
<bodyText confidence="0.9992555625">
where N is the number of sentences. This implies
that we constrain the decoder to emit exactly one
output sentence per input sentence.
Let i be the number of a sentence and mi the num-
ber of input tokens of this sentence, p and q (with
1 &lt; p &lt; q &lt; mi) be positions in the input sentence
and [p;q] denote the set of positions from p up to and
including q. We say that [p;q] precedes [p&apos;;q&apos;], or
[p;q] � [p&apos;;q&apos;], if q &lt; p&apos;. Let (Di([p;q]) be the set of
translations for the source phrase covering positions
[p;q] in the input sentence i as given by the phrase
table. We call A = ([p;q],0) an anchored phrase
pair with coverage C(A) = [p;q] if 0 E (Di([p;q]) is
a target phrase translating the source words at posi-
tions [p;q]. Then a sequence of ni anchored phrase
pairs
</bodyText>
<equation confidence="0.917634">
Si = A1A2 ...Ani (2)
</equation>
<bodyText confidence="0.559709">
is a valid sentence state for sentence i if the follow-
ing two conditions hold:
</bodyText>
<listItem confidence="0.9995125">
1. The coverage sets C(Aj) for j in 1,...,ni are
mutually disjoint, and
2. the anchored phrase pairs jointly cover the
complete input sentence, or
</listItem>
<equation confidence="0.983682333333333">
ni
U C(Aj) = [1;mi]. (3)
j=1
</equation>
<bodyText confidence="0.9832214">
Let f (S) be a scoring function mapping a state S
to a real number. As usual in SMT, it is assumed that
the scoring function can be decomposed into a linear
combination of K feature functions hk(S), each with
a constant weight Xk, so
</bodyText>
<equation confidence="0.998893">
K
f (S) = E Xkhk(S). (4)
k=1
</equation>
<bodyText confidence="0.9057455">
The problem addressed by the decoder is the search
for the state Sˆ with maximal score, such that
</bodyText>
<equation confidence="0.9976945">
Sˆ = argmax
S f(S). (5)
</equation>
<page confidence="0.857064">
1180
</page>
<bodyText confidence="0.99998825">
The feature functions implemented in our baseline
system are identical to the ones found in the popular
Moses SMT system (Koehn et al., 2007). In particu-
lar, our decoder has the following feature functions:
</bodyText>
<listItem confidence="0.96121">
1. phrase translation scores provided by the
phrase table including forward and backward
conditional probabilities, lexical weights and a
phrase penalty (Koehn et al., 2003),
2. n-gram language model scores implemented
with the KenLM toolkit (Heafield, 2011),
3. a word penalty score,
4. a distortion model with geometric decay
(Koehn et al., 2003), and
5. a feature indicating the number of times a given
distortion limit is exceeded in the current state.
</listItem>
<bodyText confidence="0.999882222222222">
In our experiments, the last feature is used with a
fixed weight of negative infinity in order to limit the
gaps between the coverage sets of adjacent anchored
phrase pairs to a maximum value. In DP search, the
distortion limit is usually enforced directly by the
search algorithm and is not added as a feature. In
our decoder, however, this restriction is not required
to limit complexity, so we decided to add it among
the scoring models.
</bodyText>
<subsectionHeader confidence="0.999347">
2.2 Decoding Algorithm
</subsectionHeader>
<bodyText confidence="0.95673175">
The decoding algorithm we use (algorithm 1) is
very simple. It starts with a given initial document
state. In the main loop, which extends from line 3
to line 12, it generates a successor state S0 for the
current state S by calling the function Neighbour,
which non-deterministically applies one of the oper-
ations described in section 3 of this paper to S. The
score of the new state is compared to that of the pre-
vious one. If it meets a given acceptance criterion,
S0 becomes the current state, else search continues
from the previous state S. For the experiments in
this paper, we use the hill climbing acceptance cri-
terion, which simply accepts a new state if its score
is higher than that of the current state. Other accep-
tance criteria are possible and could be used to en-
dow the search algorithm with stochastic behaviour.
The main loop is repeated until a maximum num-
ber of steps (step limit) is reached or until a maxi-
mum number of moves are rejected in a row (rejec-
tion limit).
Algorithm 1 Decoding algorithm
Input: an initial document state S;
search parameters maxsteps and maxrejected
Output: a modified document state
</bodyText>
<listItem confidence="0.995549214285714">
1: nsteps ← 0
2: nrejected ← 0
3: while nsteps &lt; maxsteps and
nrejected &lt; maxrejected do
4: S0 ← Neighbour(S)
5: if Accept(f (S0), f (S)) then
6: S ← S0
7: nrejected ← 0
8: else
9: nrejected ← nrejected + 1
10: end if
11: nsteps ← nsteps + 1
12: end while
13: return S
</listItem>
<bodyText confidence="0.999955333333333">
A notable difference between this algorithm and
other hill climbing algorithms that have been used
for SMT decoding (Germann et al., 2004; Langlais
et al., 2007) is its non-determinism. Previous work
for sentence-level decoding employed a steepest as-
cent strategy which amounts to enumerating the
complete neighbourhood of the current state as de-
fined by the state operations and selecting the next
state to be the best state found in the neighbourhood
of the current one. Enumerating all neighbours of
a given state, costly as it is, has the advantage that
it makes it easy to prove local optimality of a state
by recognising that all possible successor states have
lower scores. It can be rather inefficient, since at
every step only one modification will be adopted;
many of the modifications that are discarded will
very likely be generated anew in the next iteration.
As we extend the decoder to the document level,
the size of the neighbourhood that would have to be
explored in this way increases considerably. More-
over, the inefficiency of the steepest ascent approach
potentially increases as well. Very likely, a promis-
ing move in one sentence will remain promising af-
ter a modification has been applied to another sen-
</bodyText>
<page confidence="0.956855">
1181
</page>
<bodyText confidence="0.99997985">
tence, even though this is not guaranteed to be true
in the presence of cross-sentence models. We there-
fore adopt a first-choice hill climbing strategy that
non-deterministically generates successor states and
accepts the first one that meets the acceptance cri-
terion. This frees us from the necessity of gener-
ating the full set of successors for each state. On
the downside, if the full successor set is not known,
it is no longer possible to prove local optimality of a
state, so we are forced to use a different condition for
halting the search. We use a combination of two lim-
its: The step limit is a hard limit on the resources the
user is willing to expend on the search problem. The
value of the rejection limit determines how much of
the neighbourhood is searched for better successors
before a state is accepted as a solution; it is related
to the probability that a state returned as a solution
is in fact locally optimal.
To simplify notations in the description of the in-
dividual state operations, we write
</bodyText>
<equation confidence="0.9946485">
Si −→ S0 (6)
i
</equation>
<bodyText confidence="0.698315">
to signify that a state operation, when presented with
a document state as in equation 1 and acting on sen-
tence i, returns a new document state of
</bodyText>
<equation confidence="0.99086">
S0 = S1...Si−1 S0i Si+1...SN. (7)
Similarly,
Si : Aj ...Aj+h−1 −→ A01...A0h0 (8)
is equivalent to
Si −→ A1 ...Aj−1 A01 ...A0h0 Aj+h ...Ani (9)
</equation>
<bodyText confidence="0.99973725">
and indicates that the operation returns a state in
which a sequence of h consecutive anchored phrase
pairs has been replaced by another sequence of h0
anchored phrase pairs.
</bodyText>
<subsectionHeader confidence="0.996434">
2.3 Efficiency Considerations
</subsectionHeader>
<bodyText confidence="0.998883727272727">
When implementing the feature functions for the de-
coder, we have to exercise some care to avoid re-
computing scores for the whole document at every
iteration. To achieve this, the scores are computed
completely only once, at the beginning of the de-
coding run. In subsequent iterations, scoring func-
tions are presented with the scores of the previous
iteration and a list of modifications produced by the
state operation, a set of tuples hi,r,s,A01 ...A0h0i, each
indicating that the document should be modified as
described by
</bodyText>
<equation confidence="0.947346">
Si : Ar...As −→ A01...A0h0. (10)
</equation>
<bodyText confidence="0.999961047619047">
If a feature function is decomposable in some way,
as all the standard features developed under the con-
straints of DP search are, it can then update the state
simply by subtracting and adding score components
pertaining to the modified parts of the document.
Feature functions have the possibility to store their
own state information along with the document state
to make sure the required information is available.
Thus, the framework makes it possible to exploit de-
composability for efficient scoring without impos-
ing any particular decomposition on the features as
beam search does.
To make scoring even more efficient, scores are
computed in two passes: First, every feature func-
tion is asked to provide an upper bound on the score
that will be obtained for the new state. In some
cases, it is possible to calculate reasonable upper
bounds much more efficiently than computing the
exact feature value. If the upper bound fails to meet
the acceptance criterion, the new state is discarded
right away; if not, the full score is computed and the
acceptance criterion is tested again.
Among the basic SMT models, this two-pass
strategy is only used for the n-gram LM, which re-
quires fairly expensive parameter lookups for scor-
ing. The scores of all the other baseline models are
fully computed during the first scoring pass. The
n-gram model is more complex. In its state informa-
tion, it keeps track of the LM score and LM library
state for each word. The first scoring pass then iden-
tifies the words whose LM scores are affected by the
current search step. This includes the words changed
by the search operation as well as the words whose
LM history is modified. The range of the history de-
pendencies can be determined precisely by consider-
ing the “valid state length” information provided by
the KenLM library. In the first pass, the LM scores
of the affected words are subtracted from the total
score. The model only looks up the new LM scores
for the affected words and updates the total score
if the new search state passes the first acceptance
check. This two-pass scoring approach allows us
</bodyText>
<page confidence="0.974333">
1182
</page>
<bodyText confidence="0.999957266666667">
to avoid LM lookups altogether for states that will
be rejected anyhow because of low scores from the
other models, e. g. because the distortion limit is vi-
olated.
Model score updates become more complex and
slower as the number of dependencies of a model in-
creases. While our decoding algorithm does not im-
pose any formal restrictions on the number or type
of dependencies that can be handled, there will be
practical limits beyond which decoding becomes un-
acceptably slow or the scoring code becomes very
difficult to maintain. These limits are however fairly
independent of the types of dependencies handled
by a model, which permits the exploration of more
varied model types than those handled by DP search.
</bodyText>
<subsectionHeader confidence="0.975915">
2.4 State Initialisation
</subsectionHeader>
<bodyText confidence="0.9995243125">
Before the hill climbing decoding algorithm can be
run, an initial state must be generated. The closer the
initial state is to an optimum, the less work remains
to be done for the algorithm. If the algorithm is to be
self-contained, initialisation must be relatively unin-
formed and can only rely on some general prior as-
sumptions about what might be a good initial guess.
On the other hand, if optimal results are sought after,
it pays off to invest some effort into a good starting
point. One way to do this is to run DP search first.
For uninformed initialisation, we chose to imple-
ment a very simple procedure based only on the ob-
servation that, at least for language pairs involving
the major European languages, it is usually a good
guess to keep the word order of the output very sim-
ilar to that of the input. We therefore create the ini-
tial state by selecting, for each sentence in the docu-
ment, a sequence of anchored phrase pairs covering
the input sentence in monotonic order, that is, such
that for all pairs of adjacent anchored phrase pairs
Aj and Aj+1, we have that C(Aj) � C(Aj+1).
For initialisation with DP search, we first run the
Moses decoder (Koehn et al., 2007) with default
search parameters and the same models as those
used by our decoder. Then we extract the best output
hypothesis from the search graph of the decoder and
map it into a sequence of anchored phrase pairs in
the obvious way. When the document-level decoder
is used with models that are incompatible with beam
search, Moses can be run with a subset of the mod-
els in order to find an approximation of the solution
which is then refined with the complete feature set.
</bodyText>
<sectionHeader confidence="0.99536" genericHeader="method">
3 State Operations
</sectionHeader>
<bodyText confidence="0.999979">
Given a document state S, the decoder uses a neigh-
bourhood function Neighbour to simulate a move
in the state space. The neighbourhood function non-
deterministically selects a type of state operation and
a location in the document to apply it to and returns
the resulting new state. We use a set of three opera-
tions that has the property that every possible docu-
ment state can be reached from every other state in
a sequence of moves.
Designing operations for state transitions in lo-
cal search for phrase-based SMT is a problem that
has been addressed in the literature (Langlais et
al., 2007; Arun et al., 2010). Our decoder’s first-
choice hill climbing strategy never enumerates the
full neighbourhood of a state. We therefore place
less emphasis than previous work on defining a com-
pact neighbourhood, but allow the decoder to make
quite extensive changes to a state in a single step
with a certain probability. Otherwise our operations
are similar to those used by Arun et al. (2010).
All of the operations described in this paper make
changes to a single sentence only. Each time it is
called, the Neighbour function selects a sentence
in the document with a probability proportional to
the number of input tokens in each sentence to en-
sure a fair distribution of the decoder’s attention over
the words in the document regardless of varying sen-
tence lengths.
</bodyText>
<subsectionHeader confidence="0.99875">
3.1 Changing Phrase Translations
</subsectionHeader>
<bodyText confidence="0.9999095">
The change-phrase-translation operation re-
places the translation of a single phrase with a ran-
dom translation with the same coverage taken from
the phrase table. Formally, the operation selects an
anchored phrase pair Aj by drawing uniformly from
the elements of Si and then draws a new translation
0&apos; uniformly from the set (Di(C(Aj)). The new state
is given by
</bodyText>
<equation confidence="0.911005">
Si : Aj ) (C(Aj),0&apos;). (11)
</equation>
<subsectionHeader confidence="0.998796">
3.2 Changing Word Order
</subsectionHeader>
<bodyText confidence="0.9982385">
The swap-phrases operation affects the output
word order without changing the phrase translations.
</bodyText>
<page confidence="0.947226">
1183
</page>
<bodyText confidence="0.999413">
It exchanges two anchored phrase pairs Aj and Aj+h,
resulting in an output state of
</bodyText>
<equation confidence="0.396972">
Si :Aj...Aj+h −� Aj+h Aj+1 ...Aj+h−1 Aj. (12)
</equation>
<bodyText confidence="0.999887">
The start location j is drawn uniformly from the el-
igible sentence positions; the swap range h comes
from a geometric distribution with configurable de-
cay. Other word-order changes such as a one-way
move operation that does not require another move-
ment in exchange or more advanced permutations
can easily be defined.
</bodyText>
<subsectionHeader confidence="0.986205">
3.3 Resegmentation
</subsectionHeader>
<bodyText confidence="0.998208285714286">
The most complex operation is resegment, which
allows the decoder to modify the segmentation of the
source phrase. It takes a number of anchored phrase
pairs that form a contiguous block both in the input
and in the output and replaces them with a new set
of phrase pairs covering the same span of the input
sentence. Formally,
</bodyText>
<equation confidence="0.994421833333333">
Si : Aj ...Aj+h−1 −� A, 1 ...A, (13)
h,
such that
j+h−1 h,
U C(Aj,) = U C(A,j,) = [p;q] (14)
j,=j j,=1
</equation>
<bodyText confidence="0.998189866666667">
for some p and q, where, for j, = 1,...,h,, we
have that A,j, = ([pj,;qj,],oj,), all [pj,;qj,] are mu-
tually disjoint and each Oj, is randomly drawn from
(&apos;i([pj,;qj,]).
Regardless of the ordering of Aj ...Aj+h−1, the
resegment operation always generates a sequence
of anchored phrase pairs in linear order, such that
C(A,j,) --&lt; C(A,j,+1) for j, = 1,...,h, −1.
As for the other operations, j is generated uni-
formly and h is drawn from a geometric distribution
with a decay parameter. The new segmentation is
generated by extending the sequence of anchored
phrase pairs with random elements starting at the
next free position, proceeding from left to right until
the whole range [p;q] is covered.
</bodyText>
<sectionHeader confidence="0.998933" genericHeader="evaluation">
4 Experimental Results
</sectionHeader>
<bodyText confidence="0.999976">
In this section, we present the results of a series
of experiments with our document decoder. The
goal of our experiments is to demonstrate the be-
haviour of the decoder and characterise its response
to changes in the fundamental search parameters.
The SMT models for our experiments were cre-
ated with a subset of the training data for the
English-French shared task at the WMT 2011 work-
shop (Callison-Burch et al., 2011). The phrase ta-
ble was trained on Europarl, news-commentary and
UN data. To reduce the training data to a manage-
able size, singleton phrase pairs were removed be-
fore the phrase scoring step. Significance-based fil-
tering (Johnson et al., 2007) was applied to the re-
sulting phrase table. The language model was a 5-
gram model with Kneser-Ney smoothing trained on
the monolingual News corpus with IRSTLM (Fed-
erico et al., 2008). Feature weights were trained with
Minimum Error-Rate Training (MERT) (Och, 2003)
on the news-test2008 development set using the DP
beam search decoder and the MERT implementation
of the Moses toolkit (Koehn et al., 2007). Experi-
mental results are reported for the newstest2009 test
set, a corpus of 111 newswire documents totalling
2,525 sentences or 65,595 English input tokens.
</bodyText>
<subsectionHeader confidence="0.98635">
4.1 Stability
</subsectionHeader>
<bodyText confidence="0.999932608695652">
An important difference between our decoder and
the classical DP decoder as well as previous work in
SMT decoding with local search is that our decoder
is inherently non-deterministic. This implies that re-
peated runs of the decoder with the same search pa-
rameters, input and models will not, in general, find
the same local maximum of the score space. The
first empirical question we ask is therefore how dif-
ferent the results are under repeated runs. The re-
sults in this and the next section were obtained with
random state initialisation, i. e. without running the
DP beam search decoder.
Figure 1 shows the results of 7 decoder runs with
the models described above, translating the news-
test2009 test set, with a step limit of 227 and a rejec-
tion limit of 100,000. The x-axis of both plots shows
the number of decoding steps on a logarithmic scale,
so the number of steps is doubled between two adja-
cent points on the same curve. In the left plot, the
y-axis indicates the model score optimised by the
decoder summed over all 2525 sentences of the doc-
ument. In the right plot, the case-sensitive BLEU
score (Papineni et al., 2002) of the current decoder
</bodyText>
<page confidence="0.997476">
1184
</page>
<figureCaption confidence="0.999831">
Figure 1: Score stability in repeated decoder runs
</figureCaption>
<bodyText confidence="0.999697384615385">
state against a reference translation is displayed.
We note, as expected, that the decoder achieves
a considerable improvement of the initial state with
diminishing returns as decoding continues. Be-
tween 28 and 214 steps, the score increases at a
roughly logarithmic pace, then the curve flattens out,
which is partly due to the fact that decoding for
some documents effectively stopped when the max-
imum number of rejections was reached. The BLEU
score curve shows a similar increase, from an initial
score below 5 % to a maximum of around 21.5 %.
This is below the score of 22.45 % achieved by the
beam search decoder with the same models, which
is not surprising considering that our decoder ap-
proximates a more difficult search problem, from
which a number of strong independence assump-
tions have been lifted, without, at the moment, hav-
ing any stronger models at its disposal to exploit this
additional freedom for better translation.
In terms of stability, there are no dramatic differ-
ences between the decoder runs. Indeed, the small
differences that exist are hardly discernible in the
plots. The model scores at the end of the decod-
ing run range between −158767.9 and −158716.9,
a relative difference of only about 0.03 %. Final
BLEU scores range from 21.41 % to 21.63 %, an in-
terval that is not negligible, but comparable to the
variance observed when, e. g., feature weights from
repeated MERT runs are used with one and the same
SMT system. Note that these results were obtained
with random state initialisation. With DP initialisa-
tion, score differences between repeated runs rarely
exceed 0.02 absolute BLEU percentage points.
Overall, we conclude that the decoding results of
our algorithm are reasonably stable despite the non-
determinism inherent in the procedure. In our sub-
sequent experiments, the evaluation scores reported
are calculated as the mean of three runs for each ex-
periment.
</bodyText>
<subsectionHeader confidence="0.997779">
4.2 Search Algorithm Parameters
</subsectionHeader>
<bodyText confidence="0.99997852173913">
The hill climbing algorithm we use has two param-
eters which govern the trade-off between decoding
time and the accuracy with which a local maximum
is identified: The step limit stops the search pro-
cess after a certain number of steps regardless of the
search progress made or lack thereof. The rejection
limit stops the search after a certain number of un-
successful attempts to make a step, when continued
search does not seem to be promising. In most of our
experiments, we used a step limit of 227 pz� 1.3· 108
and a rejection limit of 105. In practice, decoding
terminates by reaching the rejection limit for the vast
majority of documents. We therefore examined the
effect of different rejection limits on the learning
curves. The results are shown in figure 2.
The results show that continued search does pay
off to a certain extent. Indeed, the curve for re-
jection limit 107 seems to indicate that the model
score increases roughly logarithmically, albeit to a
higher base, even after the curve has started to flat-
ten out at 214 steps. At a certain point, however, the
probability of finding a good successor state drops
rather sharply by about two orders of magnitude, as
</bodyText>
<page confidence="0.99577">
1185
</page>
<figureCaption confidence="0.999549">
Figure 2: Search performance at different rejection limits
</figureCaption>
<bodyText confidence="0.999944882352941">
evidenced by the fact that a rejection limit of 106
does not give a large improvement over one of 105,
while one of 107 does. The continued model score
improvement also results in an increase in BLEU
scores, and with a BLEU score of 22.1 % the system
with rejection limit 107 is fairly close to the score of
22.45 % obtained by DP beam search.
Obviously, more exact search comes at a cost, and
in this case, it comes at a considerable cost, which is
an explosion of the time required to decode the test
set from 4 minutes at rejection limit 103 to 224 min-
utes at rejection limit 105 and 38 hours 45 minutes
at limit 107. The DP decoder takes 31 minutes for
the same task. We conclude that the rejection limit
of 105 selected for our experiments, while techni-
cally suboptimal, realises a good trade-off between
decoding time and accuracy.
</bodyText>
<subsectionHeader confidence="0.99892">
4.3 A Semantic Document Language Model
</subsectionHeader>
<bodyText confidence="0.999907351351352">
In this section, we present the results of the applica-
tion of our decoder to an actual SMT model with
cross-sentence features. Our model addresses the
problem of lexical cohesion. In particular, it rewards
the use of semantically related words in the trans-
lation output by the decoder, where semantic dis-
tance is measured with a word space model based
on Latent Semantic Analysis (LSA). LSA has been
applied to semantic language modelling in previous
research with some success (Coccaro and Jurafsky,
1998; Bellegarda, 2000; Wandmacher and Antoine,
2007). In SMT, it has mostly been used for domain
adaptation (Kim and Khudanpur, 2004; Tam et al.,
2007), or to measure sentence similarities (Banchs
and Costa-juss`a, 2011).
The model we use is inspired by Bellegarda
(2000). It is a Markov model, similar to a stan-
dard n-gram model, and assigns to each content
word a score given a history of n preceding content
words, where n = 30 below. Scoring relies on a 30-
dimensional LSA word vector space trained with the
S-Space software (Jurgens and Stevens, 2010). The
score is defined based on the cosine similarity be-
tween the word vector of the predicted word and the
mean word vector of the words in the history, which
is converted to a probability by histogram lookup
as suggested by Bellegarda (2000). The model is
structurally different from a regular n-gram model
in that word vector n-grams are defined over content
words occurring in the word vector model only and
can cross sentence boundaries. Stop words, identi-
fied by an extensive stop word list and amounting to
around 60 % of the tokens, are scored by a different
mechanism based on their relative frequency (undis-
counted unigram probability) in the training corpus.
In sum, the score produced by the semantic docu-
ment LM has the following form:
</bodyText>
<equation confidence="0.942532">
punigr(w) if w is a stop word, else
apcos(wlh) if w is known, else (15)
e if w is unknown,
</equation>
<bodyText confidence="0.99997675">
where a is the proportion of content words in the
training corpus and e is a small fixed probability.
It is integrated into the decoder as an extra feature
function. Since we lack an automatic method for
</bodyText>
<equation confidence="0.992177">
h(w1h) = {
</equation>
<page confidence="0.940536">
1186
</page>
<bodyText confidence="0.9999733">
training the feature weights of document-wide fea-
tures, its weight was selected by grid search over
a number of values, comparing translation perfor-
mance for the newstest2009 test set.
In these experiments, we used DP beam search
to initialise the state of our local search decoder.
Three results are presented (table 1): The first table
row shows the baseline performance using DP beam
search with standard sentence-local features only.
The scores in the second row were obtained by run-
ning the hill climbing decoder with DP initialisation,
but without adding any models. A marginal increase
in scores for all three test sets demonstrates that the
hill climbing decoder manages to fix some of the
search errors made by the DP search. The last row
contains the scores obtained by adding in the seman-
tic language model. Scores are presented for three
publicly available test sets from recent WMT Ma-
chine Translation shared tasks, of which one (news-
test2009) was used to monitor progress during de-
velopment and select the final model.
Adding the semantic language model results in a
small increase in NIST scores (Doddington, 2002)
for all three test sets as well as a small BLEU score
gain (Papineni et al., 2002) for two out of three cor-
pora. We note that the NIST score turned out to re-
act more sensitively to improvements due to the se-
mantic LM in all our experiments, which is reason-
able because the model specifically targets content
words, which benefit from the information weight-
ing done by the NIST score. While the results
we present do not constitute compelling evidence
in favour of our semantic LM in its current form,
they do suggest that this model could be improved
to realise higher gains from cross-sentence seman-
tic information. They support our claim that cross-
sentence models should be examined more closely
and that existing methods should be adapted to deal
with them, a problem addressed by our main contri-
bution, the local search document decoder.
</bodyText>
<sectionHeader confidence="0.999946" genericHeader="related work">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999994541666667">
Even though DP beam search (Koehn et al., 2003)
has been the dominant approach to SMT decoding
in recent years, methods based on local search have
been explored at various times. For word-based
SMT, greedy hill-climbing techniques were advo-
cated as a faster replacement for beam search (Ger-
mann et al., 2001; Germann, 2003; Germann et al.,
2004), and a problem formulation specifically tar-
geting word reordering with an efficient word re-
ordering algorithm has been proposed (Eisner and
Tromble, 2006).
A local search decoder has been advanced as a
faster alternative to beam search also for phrase-
based SMT (Langlais et al., 2007; Langlais et al.,
2008). That work anticipates many of the features
found in our decoder, including the use of local
search to refine an initial hypothesis produced by
DP beam search. The possibility of using models
that do not fit well into the beam search paradigm is
mentioned and illustrated with the example of a re-
versed n-gram language model, which the authors
claim would be difficult to implement in a beam
search decoder. Similarly to the work by Germann
et al. (2001), their decoder is deterministic and ex-
plores the entire neighbourhood of a state in order
to identify the most promising step. Our main con-
tribution with respect to the work by Langlais et al.
(2007) is the introduction of the possibility of han-
dling document-level models by lifting the assump-
tion of sentence independence. As a consequence,
enumerating the entire neighbourhood becomes too
expensive, which is why we resort to a “first-choice”
strategy that non-deterministically generates states
and accepts the first one encountered that meets the
acceptance criterion.
More recently, Gibbs sampling was proposed as
a way to generate samples from the posterior distri-
bution of a phrase-based SMT decoder (Arun et al.,
2009; Arun et al., 2010), a process that resembles
local search in its use of a set of state-modifying
operators to generate a sequence of decoder states.
Where local search seeks for the best state attainable
from a given initial state, Gibbs sampling produces
a representative sample from the posterior. Like all
work on SMT decoding that we know of, the Gibbs
sampler presented by Arun et al. (2010) assumes in-
dependence of sentences and considers the complete
neighbourhood of each state before taking a sample.
</bodyText>
<sectionHeader confidence="0.999596" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.989861">
In the last twenty years of SMT research, there has
been a strong assumption that sentences in a text
</bodyText>
<page confidence="0.988221">
1187
</page>
<table confidence="0.9742146">
newstest2009 newstest2010 newstest2011
BLEU NIST BLEU NIST BLEU NIST
DP search only 22.56 6.513 27.27 7.034 24.94 7.170
DP + hill climbing 22.60 6.518 27.33 7.046 24.97 7.169
with semantic LM 22.71 6.549 27.53 7.087 24.90 7.199
</table>
<tableCaption confidence="0.999907">
Table 1: Experimental results with a cross-sentence semantic language model
</tableCaption>
<bodyText confidence="0.999856">
are independent of one another, and discourse con-
text has been largely neglected. Several factors have
contributed to this. Developing good discourse-level
models is difficult, and considering the modest trans-
lation quality that has long been achieved by SMT,
there have been more pressing problems to solve and
lower hanging fruit to pick. However, we argue that
the popular DP beam search algorithm, which deliv-
ers excellent decoding performance, but imposes a
particular kind of local dependency structure on the
feature models, has also had its share in driving re-
searchers away from discourse-level problems.
In this paper, we have presented a decoding pro-
cedure for phrase-based SMT that makes it possi-
ble to define feature models with cross-sentence de-
pendencies. Our algorithm can be combined with
DP beam search to leverage the quality of the tradi-
tional approach with increased flexibility for models
at the discourse level. We have presented prelimi-
nary results on a cross-sentence semantic language
model addressing the problem of lexical cohesion to
demonstrate that this kind of models is worth explor-
ing further. Besides lexical cohesion, cross-sentence
models are relevant for other linguistic phenomena
such as pronominal anaphora or verb tense selection.
We believe that SMT research has reached a point of
maturity where discourse phenomena should not be
ignored any longer, and we consider our decoder to
be a step towards this goal.
</bodyText>
<sectionHeader confidence="0.998978" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999084094339623">
Abhishek Arun, Chris Dyer, Barry Haddow, Phil Blun-
som, Adam Lopez, and Philipp Koehn. 2009. Monte
carlo inference and maximization for phrase-based
translation. In Proceedings of the Thirteenth Confer-
ence on Computational Natural Language Learning
(CoNLL-2009), pages 102–110, Boulder, Colorado,
June. Association for Computational Linguistics.
Abhishek Arun, Barry Haddow, Philipp Koehn, Adam
Lopez, Chris Dyer, and Phil Blunsom. 2010. Monte
Carlo techniques for phrase-based translation. Ma-
chine translation, 24(2):103–121.
Rafael E. Banchs and Marta R. Costa-juss`a. 2011. A se-
mantic feature for Statistical Machine Translation. In
Proceedings of Fifth Workshop on Syntax, Semantics
and Structure in Statistical Translation, pages 126–
134, Portland, Oregon, USA, June. Association for
Computational Linguistics.
Jerome R. Bellegarda. 2000. Exploiting latent semantic
information in statistical language modeling. Proceed-
ings of the IEEE, 88(8):1279–1296.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Omar Zaidan. 2011. Findings of the 2011 Work-
shop on Statistical Machine Translation. In Proceed-
ings of the Sixth Workshop on Statistical Machine
Translation, pages 22–64, Edinburgh, Scotland, July.
Association for Computational Linguistics.
Noah Coccaro and Daniel Jurafsky. 1998. Towards bet-
ter integration of semantic predictors in statistical lan-
guage modeling. In Proceedings of the 5th Interna-
tional Conference on Spoken Language Processing,
Sydney.
George Doddington. 2002. Automatic evaluation of ma-
chine translation quality using n-gram co-occurrence
statistics. In Proceedings of the second Interna-
tional conference on Human Language Technology
Research, pages 138–145, San Diego.
Jason Eisner and Roy W. Tromble. 2006. Local search
with very large-scale neighborhoods for optimal per-
mutations in machine translation. In Proceedings of
the HLT-NAACL Workshop on Computationally Hard
Problems and Joint Inference in Speech and Language
Processing, pages 57–75.
Marcello Federico, Nicola Bertoldi, and Mauro Cettolo.
2008. IRSTLM: an open source toolkit for handling
large scale language models. In Interspeech 2008,
pages 1618–1621. ISCA.
Ulrich Germann, Michael Jahr, Kevin Knight, Daniel
Marcu, and Kenji Yamada. 2001. Fast decoding and
optimal decoding for machine translation. In Proceed-
ings of 39th Annual Meeting of the Association for
Computational Linguistics, pages 228–235, Toulouse,
France, July. Association for Computational Linguis-
tics.
</reference>
<page confidence="0.866047">
1188
</page>
<reference confidence="0.999716377358491">
Ulrich Germann, Michael Jahr, Kevin Knight, Daniel
Marcu, and Kenji Yamada. 2004. Fast and optimal de-
coding for machine translation. Artificial Intelligence,
154(1–2):127–143.
Ulrich Germann. 2003. Greedy decoding for Statis-
tical Machine Translation in almost linear time. In
Proceedings of the 2003 Human Language Technol-
ogy Conference of the North American Chapter of the
Association for Computational Linguistics.
Zhengxian Gong, Min Zhang, and Guodong Zhou.
2011. Cache-based document-level Statistical Ma-
chine Translation. In Proceedings of the 2011 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 909–919, Edinburgh, Scotland, UK.,
July. Association for Computational Linguistics.
Christian Hardmeier and Marcello Federico. 2010. Mod-
elling Pronominal Anaphora in Statistical Machine
Translation. In Proceedings of the seventh Inter-
national Workshop on Spoken Language Translation
(IWSLT), pages 283–289.
Basil Hatim and Ian Mason. 1990. Discourse and the
Translator. Language in Social Life Series. Longman,
London.
Kenneth Heafield. 2011. KenLM: faster and smaller
language model queries. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, pages
187–197, Edinburgh, Scotland, July. Association for
Computational Linguistics.
Howard Johnson, Joel Martin, George Foster, and Roland
Kuhn. 2007. Improving translation quality by dis-
carding most of the phrasetable. In Proceedings of the
2007 Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natu-
ral Language Learning (EMNLP-CoNLL), pages 967–
975, Prague, Czech Republic, June. Association for
Computational Linguistics.
David Jurgens and Keith Stevens. 2010. The S-Space
package: An open source package for word space
models. In Proceedings of the ACL 2010 System
Demonstrations, pages 30–35, Uppsala, Sweden, July.
Association for Computational Linguistics.
Woosung Kim and Sanjeev Khudanpur. 2004. Cross-
lingual latent semantic analysis for language model-
ing. In IEEE international conference on acoustics,
speech, and signal processing (ICASSP), volume 1,
pages 257–260, Montr´eal.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the 2003 conference of the North Ameri-
can chapter of the Association for Computational Lin-
guistics on Human Language Technology, pages 48–
54, Edmonton.
Philipp Koehn, Hieu Hoang, Alexandra Birch, et al.
2007. Moses: open source toolkit for Statistical Ma-
chine Translation. In Annual meeting of the Associ-
ation for Computational Linguistics: Demonstration
session, pages 177–180, Prague.
Philippe Langlais, Alexandre Patry, and Fabrizio Gotti.
2007. A greedy decoder for phrase-based statistical
machine translation. In TMI-2007: Proceedings of
the 11th International Conference on Theoretical and
Methodological Issues in Machine Translation, pages
104–113, Sk¨ovde.
Philippe Langlais, Alexandre Patry, and Fabrizio Gotti.
2008. Recherche locale pour la traduction statistique
par segments. In TALN 2008, pages 119–128, Avi-
gnon, France, June. ATALA.
Ronan Le Nagard and Philipp Koehn. 2010. Aiding pro-
noun translation with co-reference resolution. In Pro-
ceedings of the Joint Fifth Workshop on Statistical Ma-
chine Translation and MetricsMATR, pages 252–261,
Uppsala, Sweden, July. Association for Computational
Linguistics.
Franz Josef Och, Nicola Ueffing, and Hermann Ney.
2001. An efficient A* search algorithm for Statisti-
cal Machine Translation. In Proceedings of the Data-
Driven Machine Translation Workshop, 39th Annual
Meeting of the Association for Computational Linguis-
tics (ACL), pages 55–62, Toulouse.
Franz Josef Och. 2003. Minimum error rate training in
Statistical Machine Translation. In Proceedings of the
41st annual meeting of the Association for Computa-
tional Linguistics, pages 160–167, Sapporo (Japan).
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of Machine Translation. In Proceedings of the
40th annual meeting of the Association for Computa-
tional Linguistics, pages 311–318, Philadelphia. ACL.
Yik-Cheung Tam, Ian Lane, and Tanja Schultz. 2007.
Bilingual LSA-based adaptation for Statistical Ma-
chine Translation. Machine Translation, 21(4):187–
207.
J¨org Tiedemann. 2010. To cache or not to cache? Ex-
periments with adaptive models in Statistical Machine
Translation. In Proceedings of the ACL 2010 Joint
Fifth Workshop on Statistical Machine Translation and
Metrics MATR, pages 189–194, Uppsala, Sweden. As-
sociation for Computational Linguistics.
Christoph Tillmann and Hermann Ney. 2003. Word re-
ordering and a Dynamic Programming beam search al-
gorithm for Statistical Machine Translation. Compu-
tational linguistics, 29(1):97–133.
Christoph Tillmann, Stephan Vogel, Hermann Ney, and
Alex Zubiaga. 1997. A DP-based search using mono-
tone alignments in Statistical Translation. In Proceed-
ings of the 35th Annual Meeting of the Association for
</reference>
<page confidence="0.913372">
1189
</page>
<reference confidence="0.999466090909091">
Computational Linguistics, pages 289–296, Madrid,
Spain, July. Association for Computational Linguis-
tics.
Tonio Wandmacher and Jean-Yves Antoine. 2007.
Methods to integrate a language model with seman-
tic information for a word prediction component. In
Proceedings of the 2007 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning (EMNLP-
CoNLL), pages 506–513, Prague, Czech Republic,
June. Association for Computational Linguistics.
</reference>
<page confidence="0.992083">
1190
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.147610">
<title confidence="0.9996865">Document-Wide Decoding Phrase-Based Statistical Machine Translation</title>
<author confidence="0.994607">Christian Hardmeier Joakim Nivre J¨org</author>
<affiliation confidence="0.870419">Uppsala Department of Linguistics and</affiliation>
<address confidence="0.955718">Box 635, 751 26 Uppsala,</address>
<email confidence="0.99788">firstname.lastname@lingfil.uu.se</email>
<abstract confidence="0.997174385714286">Independence between sentences is an assumption deeply entrenched in the models and algorithms used for statistical machine translation (SMT), particularly in the popular dynamic programming beam search decoding algorithm. This restriction is an obstacle to research on more sophisticated discourse-level models for SMT. We propose a stochastic local search decoding method for phrase-based SMT, which permits free document-wide dependencies in the models. We explore the stability and the search parameters of this method and demonstrate that it can be successfully used to optimise a document-level semantic language model. 1 Motivation In the field of translation studies, it is undisputed that discourse-wide context must be considered carefully for good translation results (Hatim and Mason, 1990). By contrast, the state of the art in statistical machine translation (SMT), despite significant advances in the last twenty years, still assumes that texts can be translated sentence by sentence under strict independence assumptions, even though it is well known that certain linguistic phenomena such as pronominal anaphora cannot be translated correctly without referring to extra-sentential context. This is true both for the phrase-based and the syntaxbased approach to SMT. In the rest of this paper, we shall concentrate on phrase-based SMT. One reason why it is difficult to experiment with document-wide models for phrase-based SMT is that the dynamic programming (DP) algorithm which has been used almost exclusively for decoding SMT models in the recent literature has very strong assumptions of locality built into it. DP beam search for phrase-based SMT was described by Koehn et al. (2003), extending earlier work on word-based SMT (Tillmann et al., 1997; Och et al., 2001; Tillmann and Ney, 2003). This algorithm constructs output sentences by starting with an empty hypothesis and adding output words at the end until translations for all source words have been generated. The core models of phrase-based SMT, in particular the n-gram language model (LM), only depend on a constant number of output words to the left of the word being generated. This fact is exploited by the search algorithm with a DP technique recombination et al., 2001), which permits the elimination of hypotheses from the search space if they coincide in a certain number of final words with a better hypothesis and no future expansion can possibly invert the relative ranking of the two hypotheses under the given models. Hypothesis recombination achieves a substantial reduction of the search space without affecting search optimality and makes it possible to use aggressive pruning techniques for fast search while still obtaining good results. The downside of this otherwise excellent approach is that it only works well with models that have a local dependency structure similar to that of an n-gram language model, so they only depend on a small context window for each target word. Sentence-local models with longer dependencies can be added, but doing so greatly increases the risk for search errors by inhibiting hypothesis recombination. Cross-sentence dependencies cannot be directly integrated into DP SMT decoding in</abstract>
<date confidence="0.526161">1179</date>
<note confidence="0.961749">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural pages 1179–1190, Jeju Island, Korea, 12–14 July 2012. Association for Computational Linguistics</note>
<abstract confidence="0.998575469841272">any obvious way, especially if joint optimisation of a number of interdependent decisions over an entire document is required. Research into models with a more varied, non-local dependency structure is to some extent stifled by the difficulty of decoding such models effectively, as can be seen by the problems some researchers encountered when they attempted to solve discourse-level problems. Consider, for instance, the work on cache-based language models by Tiedemann (2010) and Gong et al. (2011), where error propagation was a serious issue, or the works on pronominal anaphora by Le Nagard and Koehn (2010), who implemented cross-sentence dependencies with an ad-hoc two-pass decoding strategy, and Hardmeier and Federico (2010) with the use of an external decoder driver to manage backward-only dependencies between sentences. In this paper, we present a method for decoding complete documents in phrase-based SMT. Our decoder uses a local search approach whose state consists of a complete translation of an entire document at any time. The initial state is improved by the application of a series of operations using a hill climbing strategy to find a (local) maximum of the score function. This setup gives us complete freedom to define scoring functions over the entire document. Moreover, by optionally initialising the state with the output of a traditional DP decoder, we can ensure that the final hypothesis is no worse than what would have been found by DP search alone. We start by describing the decoding algorithm and the state operations used by our decoder, then we present empirical results demonstrating the effectiveness of our approach and its usability with a document-level semantic language model, and finally we discuss some related work. 2 SMT Decoding by Hill Climbing In this section, we formally describe the phrasebased SMT model implemented by our decoder as well as the decoding algorithm we use. 2.1 SMT Model Our decoder is based on local search, so its state at any time is a representation of a complete translation of the entire document. Even though the decoder operates at the document level, it is important to keep track of sentence boundaries, and the individual operations that are applied to the state are still confined to sentence scope, so it is useful to decompose the state of a document into the state of its sentences, we define the overall state a sequence of sentence states: the number of sentences. This implies that we constrain the decoder to emit exactly one output sentence per input sentence. the number of a sentence and numof input tokens of this sentence, be positions in the input sentence the set of positions from to and We say that or if Let the set of translations for the source phrase covering positions the input sentence given by the phrase We call phrase coverage = a target phrase translating the source words at posi- Then a sequence of phrase pairs a valid sentence state for sentence the following two conditions hold: The coverage sets mutually disjoint, and 2. the anchored phrase pairs jointly cover the complete input sentence, or = a scoring function mapping a state to a real number. As usual in SMT, it is assumed that the scoring function can be decomposed into a linear of functions each with constant weight so K = The problem addressed by the decoder is the search the state maximal score, such that 1180 The feature functions implemented in our baseline system are identical to the ones found in the popular Moses SMT system (Koehn et al., 2007). In particular, our decoder has the following feature functions: 1. phrase translation scores provided by the phrase table including forward and backward conditional probabilities, lexical weights and a phrase penalty (Koehn et al., 2003), 2. n-gram language model scores implemented with the KenLM toolkit (Heafield, 2011), 3. a word penalty score, 4. a distortion model with geometric decay (Koehn et al., 2003), and 5. a feature indicating the number of times a given distortion limit is exceeded in the current state. In our experiments, the last feature is used with a fixed weight of negative infinity in order to limit the gaps between the coverage sets of adjacent anchored phrase pairs to a maximum value. In DP search, the distortion limit is usually enforced directly by the search algorithm and is not added as a feature. In our decoder, however, this restriction is not required to limit complexity, so we decided to add it among the scoring models. 2.2 Decoding Algorithm The decoding algorithm we use (algorithm 1) is very simple. It starts with a given initial document state. In the main loop, which extends from line 3 line 12, it generates a successor state for the state calling the function which non-deterministically applies one of the operdescribed in section 3 of this paper to The score of the new state is compared to that of the previous one. If it meets a given acceptance criterion, the current state, else search continues the previous state For the experiments in paper, we use the climbing acceptance criwhich simply accepts a new state if its score is higher than that of the current state. Other acceptance criteria are possible and could be used to endow the search algorithm with stochastic behaviour. The main loop is repeated until a maximum numof steps is reached or until a maxinumber of moves are rejected in a row 1 algorithm initial document state parameters modified document state nsteps nrejected while 4: ← if S nrejected 8: else nrejected 10: end if nsteps 12: end while return A notable difference between this algorithm and other hill climbing algorithms that have been used for SMT decoding (Germann et al., 2004; Langlais et al., 2007) is its non-determinism. Previous work sentence-level decoding employed a aswhich amounts to enumerating the complete neighbourhood of the current state as defined by the state operations and selecting the next state to be the best state found in the neighbourhood of the current one. Enumerating all neighbours of a given state, costly as it is, has the advantage that it makes it easy to prove local optimality of a state by recognising that all possible successor states have lower scores. It can be rather inefficient, since at every step only one modification will be adopted; many of the modifications that are discarded will very likely be generated anew in the next iteration. As we extend the decoder to the document level, the size of the neighbourhood that would have to be explored in this way increases considerably. Moreover, the inefficiency of the steepest ascent approach potentially increases as well. Very likely, a promising move in one sentence will remain promising afa modification has been applied to another sen- 1181 tence, even though this is not guaranteed to be true in the presence of cross-sentence models. We thereadopt a hill climbing that non-deterministically generates successor states and accepts the first one that meets the acceptance criterion. This frees us from the necessity of generating the full set of successors for each state. On the downside, if the full successor set is not known, it is no longer possible to prove local optimality of a state, so we are forced to use a different condition for halting the search. We use a combination of two limits: The step limit is a hard limit on the resources the user is willing to expend on the search problem. The value of the rejection limit determines how much of the neighbourhood is searched for better successors before a state is accepted as a solution; it is related to the probability that a state returned as a solution is in fact locally optimal. To simplify notations in the description of the individual state operations, we write (6) i to signify that a state operation, when presented with a document state as in equation 1 and acting on senreturns a new document state of Similarly, is equivalent to (9) and indicates that the operation returns a state in a sequence of anchored phrase has been replaced by another sequence of anchored phrase pairs. 2.3 Efficiency Considerations When implementing the feature functions for the decoder, we have to exercise some care to avoid recomputing scores for the whole document at every iteration. To achieve this, the scores are computed completely only once, at the beginning of the decoding run. In subsequent iterations, scoring functions are presented with the scores of the previous iteration and a list of modifications produced by the operation, a set of tuples each indicating that the document should be modified as described by −→ If a feature function is decomposable in some way, as all the standard features developed under the constraints of DP search are, it can then update the state simply by subtracting and adding score components pertaining to the modified parts of the document. Feature functions have the possibility to store their own state information along with the document state to make sure the required information is available. Thus, the framework makes it possible to exploit decomposability for efficient scoring without imposing any particular decomposition on the features as beam search does. To make scoring even more efficient, scores are computed in two passes: First, every feature function is asked to provide an upper bound on the score that will be obtained for the new state. In some cases, it is possible to calculate reasonable upper bounds much more efficiently than computing the exact feature value. If the upper bound fails to meet the acceptance criterion, the new state is discarded right away; if not, the full score is computed and the acceptance criterion is tested again. Among the basic SMT models, this two-pass strategy is only used for the n-gram LM, which requires fairly expensive parameter lookups for scoring. The scores of all the other baseline models are fully computed during the first scoring pass. The n-gram model is more complex. In its state information, it keeps track of the LM score and LM library state for each word. The first scoring pass then identifies the words whose LM scores are affected by the current search step. This includes the words changed by the search operation as well as the words whose LM history is modified. The range of the history dependencies can be determined precisely by considering the “valid state length” information provided by the KenLM library. In the first pass, the LM scores of the affected words are subtracted from the total score. The model only looks up the new LM scores for the affected words and updates the total score if the new search state passes the first acceptance check. This two-pass scoring approach allows us 1182 to avoid LM lookups altogether for states that will be rejected anyhow because of low scores from the other models, e. g. because the distortion limit is violated. Model score updates become more complex and slower as the number of dependencies of a model increases. While our decoding algorithm does not impose any formal restrictions on the number or type of dependencies that can be handled, there will be practical limits beyond which decoding becomes unacceptably slow or the scoring code becomes very difficult to maintain. These limits are however fairly independent of the types of dependencies handled by a model, which permits the exploration of more varied model types than those handled by DP search. 2.4 State Initialisation Before the hill climbing decoding algorithm can be run, an initial state must be generated. The closer the initial state is to an optimum, the less work remains to be done for the algorithm. If the algorithm is to be self-contained, initialisation must be relatively uninformed and can only rely on some general prior assumptions about what might be a good initial guess. On the other hand, if optimal results are sought after, it pays off to invest some effort into a good starting point. One way to do this is to run DP search first. For uninformed initialisation, we chose to implement a very simple procedure based only on the observation that, at least for language pairs involving the major European languages, it is usually a good guess to keep the word order of the output very similar to that of the input. We therefore create the initial state by selecting, for each sentence in the document, a sequence of anchored phrase pairs covering the input sentence in monotonic order, that is, such that for all pairs of adjacent anchored phrase pairs we have that For initialisation with DP search, we first run the Moses decoder (Koehn et al., 2007) with default search parameters and the same models as those used by our decoder. Then we extract the best output hypothesis from the search graph of the decoder and map it into a sequence of anchored phrase pairs in the obvious way. When the document-level decoder is used with models that are incompatible with beam search, Moses can be run with a subset of the models in order to find an approximation of the solution which is then refined with the complete feature set. 3 State Operations a document state the decoder uses a neighfunction simulate a move in the state space. The neighbourhood function nondeterministically selects a type of state operation and a location in the document to apply it to and returns the resulting new state. We use a set of three operations that has the property that every possible document state can be reached from every other state in a sequence of moves. Designing operations for state transitions in local search for phrase-based SMT is a problem that has been addressed in the literature (Langlais et al., 2007; Arun et al., 2010). Our decoder’s firstchoice hill climbing strategy never enumerates the full neighbourhood of a state. We therefore place less emphasis than previous work on defining a compact neighbourhood, but allow the decoder to make quite extensive changes to a state in a single step with a certain probability. Otherwise our operations are similar to those used by Arun et al. (2010). All of the operations described in this paper make changes to a single sentence only. Each time it is the selects a sentence in the document with a probability proportional to the number of input tokens in each sentence to ensure a fair distribution of the decoder’s attention over the words in the document regardless of varying sentence lengths. 3.1 Changing Phrase Translations replaces the translation of a single phrase with a random translation with the same coverage taken from the phrase table. Formally, the operation selects an phrase pair drawing uniformly from elements of then draws a new translation from the set The new state is given by 3.2 Changing Word Order affects the output word order without changing the phrase translations. 1183 exchanges two anchored phrase pairs resulting in an output state of start location drawn uniformly from the elsentence positions; the swap range from a geometric distribution with configurable decay. Other word-order changes such as a one-way move operation that does not require another movement in exchange or more advanced permutations can easily be defined. 3.3 Resegmentation most complex operation is which allows the decoder to modify the segmentation of the source phrase. It takes a number of anchored phrase pairs that form a contiguous block both in the input and in the output and replaces them with a new set of phrase pairs covering the same span of the input sentence. Formally, such that = = some where, for we that all mudisjoint and each randomly drawn from of the ordering of the always generates a sequence of anchored phrase pairs in linear order, such that --&lt; for the other operations, generated uniand drawn from a geometric distribution with a decay parameter. The new segmentation is generated by extending the sequence of anchored phrase pairs with random elements starting at the next free position, proceeding from left to right until whole range covered. 4 Experimental Results In this section, we present the results of a series of experiments with our document decoder. The goal of our experiments is to demonstrate the behaviour of the decoder and characterise its response to changes in the fundamental search parameters. The SMT models for our experiments were created with a subset of the training data for the English-French shared task at the WMT 2011 workshop (Callison-Burch et al., 2011). The phrase table was trained on Europarl, news-commentary and UN data. To reduce the training data to a manageable size, singleton phrase pairs were removed before the phrase scoring step. Significance-based filtering (Johnson et al., 2007) was applied to the resulting phrase table. The language model was a 5gram model with Kneser-Ney smoothing trained on the monolingual News corpus with IRSTLM (Federico et al., 2008). Feature weights were trained with Minimum Error-Rate Training (MERT) (Och, 2003) on the news-test2008 development set using the DP beam search decoder and the MERT implementation of the Moses toolkit (Koehn et al., 2007). Experimental results are reported for the newstest2009 test set, a corpus of 111 newswire documents totalling 2,525 sentences or 65,595 English input tokens. 4.1 Stability An important difference between our decoder and the classical DP decoder as well as previous work in SMT decoding with local search is that our decoder is inherently non-deterministic. This implies that repeated runs of the decoder with the same search parameters, input and models will not, in general, find the same local maximum of the score space. The first empirical question we ask is therefore how different the results are under repeated runs. The results in this and the next section were obtained with random state initialisation, i. e. without running the DP beam search decoder. Figure 1 shows the results of 7 decoder runs with the models described above, translating the newstest set, with a step limit of a rejeclimit of 100,000. The of both plots shows the number of decoding steps on a logarithmic scale, so the number of steps is doubled between two adjacent points on the same curve. In the left plot, the indicates the model score optimised by the decoder summed over all 2525 sentences of the document. In the right plot, the case-sensitive BLEU score (Papineni et al., 2002) of the current decoder 1184 Figure 1: Score stability in repeated decoder runs state against a reference translation is displayed. We note, as expected, that the decoder achieves a considerable improvement of the initial state with diminishing returns as decoding continues. Beand steps, the score increases at a roughly logarithmic pace, then the curve flattens out, which is partly due to the fact that decoding for some documents effectively stopped when the maximum number of rejections was reached. The BLEU score curve shows a similar increase, from an initial score below 5 % to a maximum of around 21.5 %. This is below the score of 22.45 % achieved by the beam search decoder with the same models, which is not surprising considering that our decoder approximates a more difficult search problem, from which a number of strong independence assumptions have been lifted, without, at the moment, having any stronger models at its disposal to exploit this additional freedom for better translation. In terms of stability, there are no dramatic differences between the decoder runs. Indeed, the small differences that exist are hardly discernible in the plots. The model scores at the end of the decodrun range between and a relative difference of only about 0.03 %. Final BLEU scores range from 21.41 % to 21.63 %, an interval that is not negligible, but comparable to the variance observed when, e. g., feature weights from repeated MERT runs are used with one and the same SMT system. Note that these results were obtained with random state initialisation. With DP initialisation, score differences between repeated runs rarely exceed 0.02 absolute BLEU percentage points. Overall, we conclude that the decoding results of our algorithm are reasonably stable despite the nondeterminism inherent in the procedure. In our subsequent experiments, the evaluation scores reported are calculated as the mean of three runs for each experiment. 4.2 Search Algorithm Parameters The hill climbing algorithm we use has two parameters which govern the trade-off between decoding time and the accuracy with which a local maximum identified: The limit the search process after a certain number of steps regardless of the progress made or lack thereof. The the search after a certain number of unsuccessful attempts to make a step, when continued search does not seem to be promising. In most of our we used a step limit of pz� a rejection limit of In practice, decoding terminates by reaching the rejection limit for the vast majority of documents. We therefore examined the effect of different rejection limits on the learning curves. The results are shown in figure 2. The results show that continued search does pay off to a certain extent. Indeed, the curve for relimit seems to indicate that the model score increases roughly logarithmically, albeit to a higher base, even after the curve has started to flatout at steps. At a certain point, however, the probability of finding a good successor state drops rather sharply by about two orders of magnitude, as 1185 Figure 2: Search performance at different rejection limits by the fact that a rejection limit of not give a large improvement over one of one of does. The continued model score improvement also results in an increase in BLEU scores, and with a BLEU score of 22.1 % the system rejection limit is fairly close to the score of 22.45 % obtained by DP beam search. Obviously, more exact search comes at a cost, and in this case, it comes at a considerable cost, which is an explosion of the time required to decode the test from 4 minutes at rejection limit to 224 minat rejection limit and 38 hours 45 minutes limit The DP decoder takes 31 minutes for the same task. We conclude that the rejection limit selected for our experiments, while technically suboptimal, realises a good trade-off between decoding time and accuracy. 4.3 A Semantic Document Language Model In this section, we present the results of the application of our decoder to an actual SMT model with cross-sentence features. Our model addresses the of In particular, it rewards the use of semantically related words in the translation output by the decoder, where semantic distance is measured with a word space model based on Latent Semantic Analysis (LSA). LSA has been applied to semantic language modelling in previous research with some success (Coccaro and Jurafsky, 1998; Bellegarda, 2000; Wandmacher and Antoine, 2007). In SMT, it has mostly been used for domain adaptation (Kim and Khudanpur, 2004; Tam et al., 2007), or to measure sentence similarities (Banchs and Costa-juss`a, 2011). The model we use is inspired by Bellegarda (2000). It is a Markov model, similar to a standard n-gram model, and assigns to each content a score given a history of content where below. Scoring relies on a 30dimensional LSA word vector space trained with the S-Space software (Jurgens and Stevens, 2010). The score is defined based on the cosine similarity between the word vector of the predicted word and the mean word vector of the words in the history, which is converted to a probability by histogram lookup as suggested by Bellegarda (2000). The model is structurally different from a regular n-gram model in that word vector n-grams are defined over content words occurring in the word vector model only and can cross sentence boundaries. Stop words, identified by an extensive stop word list and amounting to around 60 % of the tokens, are scored by a different mechanism based on their relative frequency (undiscounted unigram probability) in the training corpus. In sum, the score produced by the semantic document LM has the following form: a stop word, else known, else (15) unknown, the proportion of content words in the corpus and a small fixed probability. It is integrated into the decoder as an extra feature function. Since we lack an automatic method for = { 1186 training the feature weights of document-wide features, its weight was selected by grid search over a number of values, comparing translation performance for the newstest2009 test set. In these experiments, we used DP beam search to initialise the state of our local search decoder. Three results are presented (table 1): The first table row shows the baseline performance using DP beam search with standard sentence-local features only. The scores in the second row were obtained by running the hill climbing decoder with DP initialisation, but without adding any models. A marginal increase in scores for all three test sets demonstrates that the hill climbing decoder manages to fix some of the search errors made by the DP search. The last row contains the scores obtained by adding in the semantic language model. Scores are presented for three publicly available test sets from recent WMT Machine Translation shared tasks, of which one (newstest2009) was used to monitor progress during development and select the final model. Adding the semantic language model results in a small increase in NIST scores (Doddington, 2002) for all three test sets as well as a small BLEU score gain (Papineni et al., 2002) for two out of three corpora. We note that the NIST score turned out to react more sensitively to improvements due to the semantic LM in all our experiments, which is reasonable because the model specifically targets content words, which benefit from the information weighting done by the NIST score. While the results we present do not constitute compelling evidence in favour of our semantic LM in its current form, they do suggest that this model could be improved to realise higher gains from cross-sentence semantic information. They support our claim that crosssentence models should be examined more closely and that existing methods should be adapted to deal with them, a problem addressed by our main contribution, the local search document decoder. 5 Related Work Even though DP beam search (Koehn et al., 2003) has been the dominant approach to SMT decoding in recent years, methods based on local search have been explored at various times. For word-based greedy hill-climbing techniques were advocated as a faster replacement for beam search (Germann et al., 2001; Germann, 2003; Germann et al., 2004), and a problem formulation specifically targeting word reordering with an efficient word reordering algorithm has been proposed (Eisner and Tromble, 2006). A local search decoder has been advanced as a faster alternative to beam search also for phrasebased SMT (Langlais et al., 2007; Langlais et al., 2008). That work anticipates many of the features found in our decoder, including the use of local search to refine an initial hypothesis produced by DP beam search. The possibility of using models that do not fit well into the beam search paradigm is mentioned and illustrated with the example of a reversed n-gram language model, which the authors claim would be difficult to implement in a beam search decoder. Similarly to the work by Germann et al. (2001), their decoder is deterministic and explores the entire neighbourhood of a state in order to identify the most promising step. Our main contribution with respect to the work by Langlais et al.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Abhishek Arun</author>
<author>Chris Dyer</author>
<author>Barry Haddow</author>
<author>Phil Blunsom</author>
<author>Adam Lopez</author>
<author>Philipp Koehn</author>
</authors>
<title>Monte carlo inference and maximization for phrase-based translation.</title>
<date>2009</date>
<booktitle>In Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL-2009),</booktitle>
<pages>102--110</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Boulder, Colorado,</location>
<contexts>
<context position="33990" citStr="Arun et al., 2009" startWordPosition="5782" endWordPosition="5785">t promising step. Our main contribution with respect to the work by Langlais et al. (2007) is the introduction of the possibility of handling document-level models by lifting the assumption of sentence independence. As a consequence, enumerating the entire neighbourhood becomes too expensive, which is why we resort to a “first-choice” strategy that non-deterministically generates states and accepts the first one encountered that meets the acceptance criterion. More recently, Gibbs sampling was proposed as a way to generate samples from the posterior distribution of a phrase-based SMT decoder (Arun et al., 2009; Arun et al., 2010), a process that resembles local search in its use of a set of state-modifying operators to generate a sequence of decoder states. Where local search seeks for the best state attainable from a given initial state, Gibbs sampling produces a representative sample from the posterior. Like all work on SMT decoding that we know of, the Gibbs sampler presented by Arun et al. (2010) assumes independence of sentences and considers the complete neighbourhood of each state before taking a sample. 6 Conclusion In the last twenty years of SMT research, there has been a strong assumptio</context>
</contexts>
<marker>Arun, Dyer, Haddow, Blunsom, Lopez, Koehn, 2009</marker>
<rawString>Abhishek Arun, Chris Dyer, Barry Haddow, Phil Blunsom, Adam Lopez, and Philipp Koehn. 2009. Monte carlo inference and maximization for phrase-based translation. In Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL-2009), pages 102–110, Boulder, Colorado, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Abhishek Arun</author>
<author>Barry Haddow</author>
<author>Philipp Koehn</author>
<author>Adam Lopez</author>
<author>Chris Dyer</author>
<author>Phil Blunsom</author>
</authors>
<title>Monte Carlo techniques for phrase-based translation.</title>
<date>2010</date>
<booktitle>Machine translation,</booktitle>
<pages>24--2</pages>
<contexts>
<context position="18621" citStr="Arun et al., 2010" startWordPosition="3174" endWordPosition="3177">ations Given a document state S, the decoder uses a neighbourhood function Neighbour to simulate a move in the state space. The neighbourhood function nondeterministically selects a type of state operation and a location in the document to apply it to and returns the resulting new state. We use a set of three operations that has the property that every possible document state can be reached from every other state in a sequence of moves. Designing operations for state transitions in local search for phrase-based SMT is a problem that has been addressed in the literature (Langlais et al., 2007; Arun et al., 2010). Our decoder’s firstchoice hill climbing strategy never enumerates the full neighbourhood of a state. We therefore place less emphasis than previous work on defining a compact neighbourhood, but allow the decoder to make quite extensive changes to a state in a single step with a certain probability. Otherwise our operations are similar to those used by Arun et al. (2010). All of the operations described in this paper make changes to a single sentence only. Each time it is called, the Neighbour function selects a sentence in the document with a probability proportional to the number of input t</context>
<context position="34010" citStr="Arun et al., 2010" startWordPosition="5786" endWordPosition="5789">ur main contribution with respect to the work by Langlais et al. (2007) is the introduction of the possibility of handling document-level models by lifting the assumption of sentence independence. As a consequence, enumerating the entire neighbourhood becomes too expensive, which is why we resort to a “first-choice” strategy that non-deterministically generates states and accepts the first one encountered that meets the acceptance criterion. More recently, Gibbs sampling was proposed as a way to generate samples from the posterior distribution of a phrase-based SMT decoder (Arun et al., 2009; Arun et al., 2010), a process that resembles local search in its use of a set of state-modifying operators to generate a sequence of decoder states. Where local search seeks for the best state attainable from a given initial state, Gibbs sampling produces a representative sample from the posterior. Like all work on SMT decoding that we know of, the Gibbs sampler presented by Arun et al. (2010) assumes independence of sentences and considers the complete neighbourhood of each state before taking a sample. 6 Conclusion In the last twenty years of SMT research, there has been a strong assumption that sentences in </context>
</contexts>
<marker>Arun, Haddow, Koehn, Lopez, Dyer, Blunsom, 2010</marker>
<rawString>Abhishek Arun, Barry Haddow, Philipp Koehn, Adam Lopez, Chris Dyer, and Phil Blunsom. 2010. Monte Carlo techniques for phrase-based translation. Machine translation, 24(2):103–121.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rafael E Banchs</author>
<author>Marta R Costa-juss`a</author>
</authors>
<title>A semantic feature for Statistical Machine Translation.</title>
<date>2011</date>
<booktitle>In Proceedings of Fifth Workshop on Syntax, Semantics and Structure in Statistical Translation,</booktitle>
<pages>126--134</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<marker>Banchs, Costa-juss`a, 2011</marker>
<rawString>Rafael E. Banchs and Marta R. Costa-juss`a. 2011. A semantic feature for Statistical Machine Translation. In Proceedings of Fifth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 126– 134, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerome R Bellegarda</author>
</authors>
<title>Exploiting latent semantic information in statistical language modeling.</title>
<date>2000</date>
<booktitle>Proceedings of the IEEE,</booktitle>
<pages>88--8</pages>
<contexts>
<context position="28572" citStr="Bellegarda, 2000" startWordPosition="4866" endWordPosition="4867"> a good trade-off between decoding time and accuracy. 4.3 A Semantic Document Language Model In this section, we present the results of the application of our decoder to an actual SMT model with cross-sentence features. Our model addresses the problem of lexical cohesion. In particular, it rewards the use of semantically related words in the translation output by the decoder, where semantic distance is measured with a word space model based on Latent Semantic Analysis (LSA). LSA has been applied to semantic language modelling in previous research with some success (Coccaro and Jurafsky, 1998; Bellegarda, 2000; Wandmacher and Antoine, 2007). In SMT, it has mostly been used for domain adaptation (Kim and Khudanpur, 2004; Tam et al., 2007), or to measure sentence similarities (Banchs and Costa-juss`a, 2011). The model we use is inspired by Bellegarda (2000). It is a Markov model, similar to a standard n-gram model, and assigns to each content word a score given a history of n preceding content words, where n = 30 below. Scoring relies on a 30- dimensional LSA word vector space trained with the S-Space software (Jurgens and Stevens, 2010). The score is defined based on the cosine similarity between th</context>
</contexts>
<marker>Bellegarda, 2000</marker>
<rawString>Jerome R. Bellegarda. 2000. Exploiting latent semantic information in statistical language modeling. Proceedings of the IEEE, 88(8):1279–1296.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Omar Zaidan</author>
</authors>
<date>2011</date>
<booktitle>Findings of the 2011 Workshop on Statistical Machine Translation. In Proceedings of the Sixth Workshop on Statistical Machine Translation,</booktitle>
<pages>22--64</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Edinburgh, Scotland,</location>
<contexts>
<context position="21960" citStr="Callison-Burch et al., 2011" startWordPosition="3739" endWordPosition="3742">erated by extending the sequence of anchored phrase pairs with random elements starting at the next free position, proceeding from left to right until the whole range [p;q] is covered. 4 Experimental Results In this section, we present the results of a series of experiments with our document decoder. The goal of our experiments is to demonstrate the behaviour of the decoder and characterise its response to changes in the fundamental search parameters. The SMT models for our experiments were created with a subset of the training data for the English-French shared task at the WMT 2011 workshop (Callison-Burch et al., 2011). The phrase table was trained on Europarl, news-commentary and UN data. To reduce the training data to a manageable size, singleton phrase pairs were removed before the phrase scoring step. Significance-based filtering (Johnson et al., 2007) was applied to the resulting phrase table. The language model was a 5- gram model with Kneser-Ney smoothing trained on the monolingual News corpus with IRSTLM (Federico et al., 2008). Feature weights were trained with Minimum Error-Rate Training (MERT) (Och, 2003) on the news-test2008 development set using the DP beam search decoder and the MERT implement</context>
</contexts>
<marker>Callison-Burch, Koehn, Monz, Zaidan, 2011</marker>
<rawString>Chris Callison-Burch, Philipp Koehn, Christof Monz, and Omar Zaidan. 2011. Findings of the 2011 Workshop on Statistical Machine Translation. In Proceedings of the Sixth Workshop on Statistical Machine Translation, pages 22–64, Edinburgh, Scotland, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noah Coccaro</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Towards better integration of semantic predictors in statistical language modeling.</title>
<date>1998</date>
<booktitle>In Proceedings of the 5th International Conference on Spoken Language Processing,</booktitle>
<location>Sydney.</location>
<contexts>
<context position="28554" citStr="Coccaro and Jurafsky, 1998" startWordPosition="4862" endWordPosition="4865">nically suboptimal, realises a good trade-off between decoding time and accuracy. 4.3 A Semantic Document Language Model In this section, we present the results of the application of our decoder to an actual SMT model with cross-sentence features. Our model addresses the problem of lexical cohesion. In particular, it rewards the use of semantically related words in the translation output by the decoder, where semantic distance is measured with a word space model based on Latent Semantic Analysis (LSA). LSA has been applied to semantic language modelling in previous research with some success (Coccaro and Jurafsky, 1998; Bellegarda, 2000; Wandmacher and Antoine, 2007). In SMT, it has mostly been used for domain adaptation (Kim and Khudanpur, 2004; Tam et al., 2007), or to measure sentence similarities (Banchs and Costa-juss`a, 2011). The model we use is inspired by Bellegarda (2000). It is a Markov model, similar to a standard n-gram model, and assigns to each content word a score given a history of n preceding content words, where n = 30 below. Scoring relies on a 30- dimensional LSA word vector space trained with the S-Space software (Jurgens and Stevens, 2010). The score is defined based on the cosine sim</context>
</contexts>
<marker>Coccaro, Jurafsky, 1998</marker>
<rawString>Noah Coccaro and Daniel Jurafsky. 1998. Towards better integration of semantic predictors in statistical language modeling. In Proceedings of the 5th International Conference on Spoken Language Processing, Sydney.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Doddington</author>
</authors>
<title>Automatic evaluation of machine translation quality using n-gram co-occurrence statistics.</title>
<date>2002</date>
<booktitle>In Proceedings of the second International conference on Human Language Technology Research,</booktitle>
<pages>138--145</pages>
<location>San Diego.</location>
<contexts>
<context position="31297" citStr="Doddington, 2002" startWordPosition="5333" endWordPosition="5334">coder with DP initialisation, but without adding any models. A marginal increase in scores for all three test sets demonstrates that the hill climbing decoder manages to fix some of the search errors made by the DP search. The last row contains the scores obtained by adding in the semantic language model. Scores are presented for three publicly available test sets from recent WMT Machine Translation shared tasks, of which one (newstest2009) was used to monitor progress during development and select the final model. Adding the semantic language model results in a small increase in NIST scores (Doddington, 2002) for all three test sets as well as a small BLEU score gain (Papineni et al., 2002) for two out of three corpora. We note that the NIST score turned out to react more sensitively to improvements due to the semantic LM in all our experiments, which is reasonable because the model specifically targets content words, which benefit from the information weighting done by the NIST score. While the results we present do not constitute compelling evidence in favour of our semantic LM in its current form, they do suggest that this model could be improved to realise higher gains from cross-sentence sema</context>
</contexts>
<marker>Doddington, 2002</marker>
<rawString>George Doddington. 2002. Automatic evaluation of machine translation quality using n-gram co-occurrence statistics. In Proceedings of the second International conference on Human Language Technology Research, pages 138–145, San Diego.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
<author>Roy W Tromble</author>
</authors>
<title>Local search with very large-scale neighborhoods for optimal permutations in machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the HLT-NAACL Workshop on Computationally Hard Problems and Joint Inference in Speech and Language Processing,</booktitle>
<pages>57--75</pages>
<contexts>
<context position="32655" citStr="Eisner and Tromble, 2006" startWordPosition="5562" endWordPosition="5565">e adapted to deal with them, a problem addressed by our main contribution, the local search document decoder. 5 Related Work Even though DP beam search (Koehn et al., 2003) has been the dominant approach to SMT decoding in recent years, methods based on local search have been explored at various times. For word-based SMT, greedy hill-climbing techniques were advocated as a faster replacement for beam search (Germann et al., 2001; Germann, 2003; Germann et al., 2004), and a problem formulation specifically targeting word reordering with an efficient word reordering algorithm has been proposed (Eisner and Tromble, 2006). A local search decoder has been advanced as a faster alternative to beam search also for phrasebased SMT (Langlais et al., 2007; Langlais et al., 2008). That work anticipates many of the features found in our decoder, including the use of local search to refine an initial hypothesis produced by DP beam search. The possibility of using models that do not fit well into the beam search paradigm is mentioned and illustrated with the example of a reversed n-gram language model, which the authors claim would be difficult to implement in a beam search decoder. Similarly to the work by Germann et al</context>
</contexts>
<marker>Eisner, Tromble, 2006</marker>
<rawString>Jason Eisner and Roy W. Tromble. 2006. Local search with very large-scale neighborhoods for optimal permutations in machine translation. In Proceedings of the HLT-NAACL Workshop on Computationally Hard Problems and Joint Inference in Speech and Language Processing, pages 57–75.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Mauro Cettolo</author>
</authors>
<title>IRSTLM: an open source toolkit for handling large scale language models. In Interspeech</title>
<date>2008</date>
<pages>1618--1621</pages>
<publisher>ISCA.</publisher>
<contexts>
<context position="22385" citStr="Federico et al., 2008" startWordPosition="3810" endWordPosition="3814">ental search parameters. The SMT models for our experiments were created with a subset of the training data for the English-French shared task at the WMT 2011 workshop (Callison-Burch et al., 2011). The phrase table was trained on Europarl, news-commentary and UN data. To reduce the training data to a manageable size, singleton phrase pairs were removed before the phrase scoring step. Significance-based filtering (Johnson et al., 2007) was applied to the resulting phrase table. The language model was a 5- gram model with Kneser-Ney smoothing trained on the monolingual News corpus with IRSTLM (Federico et al., 2008). Feature weights were trained with Minimum Error-Rate Training (MERT) (Och, 2003) on the news-test2008 development set using the DP beam search decoder and the MERT implementation of the Moses toolkit (Koehn et al., 2007). Experimental results are reported for the newstest2009 test set, a corpus of 111 newswire documents totalling 2,525 sentences or 65,595 English input tokens. 4.1 Stability An important difference between our decoder and the classical DP decoder as well as previous work in SMT decoding with local search is that our decoder is inherently non-deterministic. This implies that r</context>
</contexts>
<marker>Federico, Bertoldi, Cettolo, 2008</marker>
<rawString>Marcello Federico, Nicola Bertoldi, and Mauro Cettolo. 2008. IRSTLM: an open source toolkit for handling large scale language models. In Interspeech 2008, pages 1618–1621. ISCA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ulrich Germann</author>
<author>Michael Jahr</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
<author>Kenji Yamada</author>
</authors>
<title>Fast decoding and optimal decoding for machine translation.</title>
<date>2001</date>
<booktitle>In Proceedings of 39th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>228--235</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Toulouse, France,</location>
<contexts>
<context position="32462" citStr="Germann et al., 2001" startWordPosition="5532" endWordPosition="5536">proved to realise higher gains from cross-sentence semantic information. They support our claim that crosssentence models should be examined more closely and that existing methods should be adapted to deal with them, a problem addressed by our main contribution, the local search document decoder. 5 Related Work Even though DP beam search (Koehn et al., 2003) has been the dominant approach to SMT decoding in recent years, methods based on local search have been explored at various times. For word-based SMT, greedy hill-climbing techniques were advocated as a faster replacement for beam search (Germann et al., 2001; Germann, 2003; Germann et al., 2004), and a problem formulation specifically targeting word reordering with an efficient word reordering algorithm has been proposed (Eisner and Tromble, 2006). A local search decoder has been advanced as a faster alternative to beam search also for phrasebased SMT (Langlais et al., 2007; Langlais et al., 2008). That work anticipates many of the features found in our decoder, including the use of local search to refine an initial hypothesis produced by DP beam search. The possibility of using models that do not fit well into the beam search paradigm is mention</context>
</contexts>
<marker>Germann, Jahr, Knight, Marcu, Yamada, 2001</marker>
<rawString>Ulrich Germann, Michael Jahr, Kevin Knight, Daniel Marcu, and Kenji Yamada. 2001. Fast decoding and optimal decoding for machine translation. In Proceedings of 39th Annual Meeting of the Association for Computational Linguistics, pages 228–235, Toulouse, France, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ulrich Germann</author>
<author>Michael Jahr</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
<author>Kenji Yamada</author>
</authors>
<title>Fast and optimal decoding for machine translation.</title>
<date>2004</date>
<journal>Artificial Intelligence,</journal>
<pages>154--1</pages>
<contexts>
<context position="10307" citStr="Germann et al., 2004" startWordPosition="1735" endWordPosition="1738">il a maximum number of moves are rejected in a row (rejection limit). Algorithm 1 Decoding algorithm Input: an initial document state S; search parameters maxsteps and maxrejected Output: a modified document state 1: nsteps ← 0 2: nrejected ← 0 3: while nsteps &lt; maxsteps and nrejected &lt; maxrejected do 4: S0 ← Neighbour(S) 5: if Accept(f (S0), f (S)) then 6: S ← S0 7: nrejected ← 0 8: else 9: nrejected ← nrejected + 1 10: end if 11: nsteps ← nsteps + 1 12: end while 13: return S A notable difference between this algorithm and other hill climbing algorithms that have been used for SMT decoding (Germann et al., 2004; Langlais et al., 2007) is its non-determinism. Previous work for sentence-level decoding employed a steepest ascent strategy which amounts to enumerating the complete neighbourhood of the current state as defined by the state operations and selecting the next state to be the best state found in the neighbourhood of the current one. Enumerating all neighbours of a given state, costly as it is, has the advantage that it makes it easy to prove local optimality of a state by recognising that all possible successor states have lower scores. It can be rather inefficient, since at every step only o</context>
<context position="32500" citStr="Germann et al., 2004" startWordPosition="5539" endWordPosition="5542">ross-sentence semantic information. They support our claim that crosssentence models should be examined more closely and that existing methods should be adapted to deal with them, a problem addressed by our main contribution, the local search document decoder. 5 Related Work Even though DP beam search (Koehn et al., 2003) has been the dominant approach to SMT decoding in recent years, methods based on local search have been explored at various times. For word-based SMT, greedy hill-climbing techniques were advocated as a faster replacement for beam search (Germann et al., 2001; Germann, 2003; Germann et al., 2004), and a problem formulation specifically targeting word reordering with an efficient word reordering algorithm has been proposed (Eisner and Tromble, 2006). A local search decoder has been advanced as a faster alternative to beam search also for phrasebased SMT (Langlais et al., 2007; Langlais et al., 2008). That work anticipates many of the features found in our decoder, including the use of local search to refine an initial hypothesis produced by DP beam search. The possibility of using models that do not fit well into the beam search paradigm is mentioned and illustrated with the example of</context>
</contexts>
<marker>Germann, Jahr, Knight, Marcu, Yamada, 2004</marker>
<rawString>Ulrich Germann, Michael Jahr, Kevin Knight, Daniel Marcu, and Kenji Yamada. 2004. Fast and optimal decoding for machine translation. Artificial Intelligence, 154(1–2):127–143.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ulrich Germann</author>
</authors>
<title>Greedy decoding for Statistical Machine Translation in almost linear time.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="32477" citStr="Germann, 2003" startWordPosition="5537" endWordPosition="5538">er gains from cross-sentence semantic information. They support our claim that crosssentence models should be examined more closely and that existing methods should be adapted to deal with them, a problem addressed by our main contribution, the local search document decoder. 5 Related Work Even though DP beam search (Koehn et al., 2003) has been the dominant approach to SMT decoding in recent years, methods based on local search have been explored at various times. For word-based SMT, greedy hill-climbing techniques were advocated as a faster replacement for beam search (Germann et al., 2001; Germann, 2003; Germann et al., 2004), and a problem formulation specifically targeting word reordering with an efficient word reordering algorithm has been proposed (Eisner and Tromble, 2006). A local search decoder has been advanced as a faster alternative to beam search also for phrasebased SMT (Langlais et al., 2007; Langlais et al., 2008). That work anticipates many of the features found in our decoder, including the use of local search to refine an initial hypothesis produced by DP beam search. The possibility of using models that do not fit well into the beam search paradigm is mentioned and illustra</context>
</contexts>
<marker>Germann, 2003</marker>
<rawString>Ulrich Germann. 2003. Greedy decoding for Statistical Machine Translation in almost linear time. In Proceedings of the 2003 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhengxian Gong</author>
<author>Min Zhang</author>
<author>Guodong Zhou</author>
</authors>
<title>Cache-based document-level Statistical Machine Translation.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>909--919</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Edinburgh, Scotland, UK.,</location>
<contexts>
<context position="4246" citStr="Gong et al. (2011)" startWordPosition="652" endWordPosition="655">age Learning, pages 1179–1190, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics any obvious way, especially if joint optimisation of a number of interdependent decisions over an entire document is required. Research into models with a more varied, non-local dependency structure is to some extent stifled by the difficulty of decoding such models effectively, as can be seen by the problems some researchers encountered when they attempted to solve discourse-level problems. Consider, for instance, the work on cache-based language models by Tiedemann (2010) and Gong et al. (2011), where error propagation was a serious issue, or the works on pronominal anaphora by Le Nagard and Koehn (2010), who implemented cross-sentence dependencies with an ad-hoc two-pass decoding strategy, and Hardmeier and Federico (2010) with the use of an external decoder driver to manage backward-only dependencies between sentences. In this paper, we present a method for decoding complete documents in phrase-based SMT. Our decoder uses a local search approach whose state consists of a complete translation of an entire document at any time. The initial state is improved by the application of a s</context>
</contexts>
<marker>Gong, Zhang, Zhou, 2011</marker>
<rawString>Zhengxian Gong, Min Zhang, and Guodong Zhou. 2011. Cache-based document-level Statistical Machine Translation. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 909–919, Edinburgh, Scotland, UK., July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian Hardmeier</author>
<author>Marcello Federico</author>
</authors>
<title>Modelling Pronominal Anaphora in Statistical Machine Translation.</title>
<date>2010</date>
<booktitle>In Proceedings of the seventh International Workshop on Spoken Language Translation (IWSLT),</booktitle>
<pages>283--289</pages>
<contexts>
<context position="4480" citStr="Hardmeier and Federico (2010)" startWordPosition="687" endWordPosition="690">ocument is required. Research into models with a more varied, non-local dependency structure is to some extent stifled by the difficulty of decoding such models effectively, as can be seen by the problems some researchers encountered when they attempted to solve discourse-level problems. Consider, for instance, the work on cache-based language models by Tiedemann (2010) and Gong et al. (2011), where error propagation was a serious issue, or the works on pronominal anaphora by Le Nagard and Koehn (2010), who implemented cross-sentence dependencies with an ad-hoc two-pass decoding strategy, and Hardmeier and Federico (2010) with the use of an external decoder driver to manage backward-only dependencies between sentences. In this paper, we present a method for decoding complete documents in phrase-based SMT. Our decoder uses a local search approach whose state consists of a complete translation of an entire document at any time. The initial state is improved by the application of a series of operations using a hill climbing strategy to find a (local) maximum of the score function. This setup gives us complete freedom to define scoring functions over the entire document. Moreover, by optionally initialising the st</context>
</contexts>
<marker>Hardmeier, Federico, 2010</marker>
<rawString>Christian Hardmeier and Marcello Federico. 2010. Modelling Pronominal Anaphora in Statistical Machine Translation. In Proceedings of the seventh International Workshop on Spoken Language Translation (IWSLT), pages 283–289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Basil Hatim</author>
<author>Ian Mason</author>
</authors>
<title>Discourse and the Translator. Language in Social Life Series.</title>
<date>1990</date>
<publisher>Longman,</publisher>
<location>London.</location>
<contexts>
<context position="1056" citStr="Hatim and Mason, 1990" startWordPosition="147" endWordPosition="150">rogramming beam search decoding algorithm. This restriction is an obstacle to research on more sophisticated discourse-level models for SMT. We propose a stochastic local search decoding method for phrase-based SMT, which permits free document-wide dependencies in the models. We explore the stability and the search parameters of this method and demonstrate that it can be successfully used to optimise a document-level semantic language model. 1 Motivation In the field of translation studies, it is undisputed that discourse-wide context must be considered carefully for good translation results (Hatim and Mason, 1990). By contrast, the state of the art in statistical machine translation (SMT), despite significant advances in the last twenty years, still assumes that texts can be translated sentence by sentence under strict independence assumptions, even though it is well known that certain linguistic phenomena such as pronominal anaphora cannot be translated correctly without referring to extra-sentential context. This is true both for the phrase-based and the syntaxbased approach to SMT. In the rest of this paper, we shall concentrate on phrase-based SMT. One reason why it is difficult to experiment with </context>
</contexts>
<marker>Hatim, Mason, 1990</marker>
<rawString>Basil Hatim and Ian Mason. 1990. Discourse and the Translator. Language in Social Life Series. Longman, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Heafield</author>
</authors>
<title>KenLM: faster and smaller language model queries.</title>
<date>2011</date>
<booktitle>In Proceedings of the Sixth Workshop on Statistical Machine Translation,</booktitle>
<pages>187--197</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Edinburgh, Scotland,</location>
<contexts>
<context position="8106" citStr="Heafield, 2011" startWordPosition="1340" endWordPosition="1341">) = E Xkhk(S). (4) k=1 The problem addressed by the decoder is the search for the state Sˆ with maximal score, such that Sˆ = argmax S f(S). (5) 1180 The feature functions implemented in our baseline system are identical to the ones found in the popular Moses SMT system (Koehn et al., 2007). In particular, our decoder has the following feature functions: 1. phrase translation scores provided by the phrase table including forward and backward conditional probabilities, lexical weights and a phrase penalty (Koehn et al., 2003), 2. n-gram language model scores implemented with the KenLM toolkit (Heafield, 2011), 3. a word penalty score, 4. a distortion model with geometric decay (Koehn et al., 2003), and 5. a feature indicating the number of times a given distortion limit is exceeded in the current state. In our experiments, the last feature is used with a fixed weight of negative infinity in order to limit the gaps between the coverage sets of adjacent anchored phrase pairs to a maximum value. In DP search, the distortion limit is usually enforced directly by the search algorithm and is not added as a feature. In our decoder, however, this restriction is not required to limit complexity, so we deci</context>
</contexts>
<marker>Heafield, 2011</marker>
<rawString>Kenneth Heafield. 2011. KenLM: faster and smaller language model queries. In Proceedings of the Sixth Workshop on Statistical Machine Translation, pages 187–197, Edinburgh, Scotland, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Howard Johnson</author>
<author>Joel Martin</author>
<author>George Foster</author>
<author>Roland Kuhn</author>
</authors>
<title>Improving translation quality by discarding most of the phrasetable.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</booktitle>
<pages>967--975</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="22202" citStr="Johnson et al., 2007" startWordPosition="3779" endWordPosition="3782">f a series of experiments with our document decoder. The goal of our experiments is to demonstrate the behaviour of the decoder and characterise its response to changes in the fundamental search parameters. The SMT models for our experiments were created with a subset of the training data for the English-French shared task at the WMT 2011 workshop (Callison-Burch et al., 2011). The phrase table was trained on Europarl, news-commentary and UN data. To reduce the training data to a manageable size, singleton phrase pairs were removed before the phrase scoring step. Significance-based filtering (Johnson et al., 2007) was applied to the resulting phrase table. The language model was a 5- gram model with Kneser-Ney smoothing trained on the monolingual News corpus with IRSTLM (Federico et al., 2008). Feature weights were trained with Minimum Error-Rate Training (MERT) (Och, 2003) on the news-test2008 development set using the DP beam search decoder and the MERT implementation of the Moses toolkit (Koehn et al., 2007). Experimental results are reported for the newstest2009 test set, a corpus of 111 newswire documents totalling 2,525 sentences or 65,595 English input tokens. 4.1 Stability An important differen</context>
</contexts>
<marker>Johnson, Martin, Foster, Kuhn, 2007</marker>
<rawString>Howard Johnson, Joel Martin, George Foster, and Roland Kuhn. 2007. Improving translation quality by discarding most of the phrasetable. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 967– 975, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Jurgens</author>
<author>Keith Stevens</author>
</authors>
<title>The S-Space package: An open source package for word space models.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACL 2010 System Demonstrations,</booktitle>
<pages>30--35</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="29108" citStr="Jurgens and Stevens, 2010" startWordPosition="4956" endWordPosition="4959">lling in previous research with some success (Coccaro and Jurafsky, 1998; Bellegarda, 2000; Wandmacher and Antoine, 2007). In SMT, it has mostly been used for domain adaptation (Kim and Khudanpur, 2004; Tam et al., 2007), or to measure sentence similarities (Banchs and Costa-juss`a, 2011). The model we use is inspired by Bellegarda (2000). It is a Markov model, similar to a standard n-gram model, and assigns to each content word a score given a history of n preceding content words, where n = 30 below. Scoring relies on a 30- dimensional LSA word vector space trained with the S-Space software (Jurgens and Stevens, 2010). The score is defined based on the cosine similarity between the word vector of the predicted word and the mean word vector of the words in the history, which is converted to a probability by histogram lookup as suggested by Bellegarda (2000). The model is structurally different from a regular n-gram model in that word vector n-grams are defined over content words occurring in the word vector model only and can cross sentence boundaries. Stop words, identified by an extensive stop word list and amounting to around 60 % of the tokens, are scored by a different mechanism based on their relative</context>
</contexts>
<marker>Jurgens, Stevens, 2010</marker>
<rawString>David Jurgens and Keith Stevens. 2010. The S-Space package: An open source package for word space models. In Proceedings of the ACL 2010 System Demonstrations, pages 30–35, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Woosung Kim</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>Crosslingual latent semantic analysis for language modeling.</title>
<date>2004</date>
<booktitle>In IEEE international conference on acoustics, speech, and signal processing (ICASSP),</booktitle>
<volume>1</volume>
<pages>257--260</pages>
<location>Montr´eal.</location>
<contexts>
<context position="28683" citStr="Kim and Khudanpur, 2004" startWordPosition="4882" endWordPosition="4885">tion, we present the results of the application of our decoder to an actual SMT model with cross-sentence features. Our model addresses the problem of lexical cohesion. In particular, it rewards the use of semantically related words in the translation output by the decoder, where semantic distance is measured with a word space model based on Latent Semantic Analysis (LSA). LSA has been applied to semantic language modelling in previous research with some success (Coccaro and Jurafsky, 1998; Bellegarda, 2000; Wandmacher and Antoine, 2007). In SMT, it has mostly been used for domain adaptation (Kim and Khudanpur, 2004; Tam et al., 2007), or to measure sentence similarities (Banchs and Costa-juss`a, 2011). The model we use is inspired by Bellegarda (2000). It is a Markov model, similar to a standard n-gram model, and assigns to each content word a score given a history of n preceding content words, where n = 30 below. Scoring relies on a 30- dimensional LSA word vector space trained with the S-Space software (Jurgens and Stevens, 2010). The score is defined based on the cosine similarity between the word vector of the predicted word and the mean word vector of the words in the history, which is converted to</context>
</contexts>
<marker>Kim, Khudanpur, 2004</marker>
<rawString>Woosung Kim and Sanjeev Khudanpur. 2004. Crosslingual latent semantic analysis for language modeling. In IEEE international conference on acoustics, speech, and signal processing (ICASSP), volume 1, pages 257–260, Montr´eal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 conference of the North American chapter of the Association for Computational Linguistics on Human Language Technology,</booktitle>
<pages>48--54</pages>
<location>Edmonton.</location>
<contexts>
<context position="1960" citStr="Koehn et al. (2003)" startWordPosition="290" endWordPosition="293">tic phenomena such as pronominal anaphora cannot be translated correctly without referring to extra-sentential context. This is true both for the phrase-based and the syntaxbased approach to SMT. In the rest of this paper, we shall concentrate on phrase-based SMT. One reason why it is difficult to experiment with document-wide models for phrase-based SMT is that the dynamic programming (DP) algorithm which has been used almost exclusively for decoding SMT models in the recent literature has very strong assumptions of locality built into it. DP beam search for phrase-based SMT was described by Koehn et al. (2003), extending earlier work on word-based SMT (Tillmann et al., 1997; Och et al., 2001; Tillmann and Ney, 2003). This algorithm constructs output sentences by starting with an empty hypothesis and adding output words at the end until translations for all source words have been generated. The core models of phrase-based SMT, in particular the n-gram language model (LM), only depend on a constant number of output words to the left of the word being generated. This fact is exploited by the search algorithm with a DP technique called hypothesis recombination (Och et al., 2001), which permits the elim</context>
<context position="8021" citStr="Koehn et al., 2003" startWordPosition="1326" endWordPosition="1329">inear combination of K feature functions hk(S), each with a constant weight Xk, so K f (S) = E Xkhk(S). (4) k=1 The problem addressed by the decoder is the search for the state Sˆ with maximal score, such that Sˆ = argmax S f(S). (5) 1180 The feature functions implemented in our baseline system are identical to the ones found in the popular Moses SMT system (Koehn et al., 2007). In particular, our decoder has the following feature functions: 1. phrase translation scores provided by the phrase table including forward and backward conditional probabilities, lexical weights and a phrase penalty (Koehn et al., 2003), 2. n-gram language model scores implemented with the KenLM toolkit (Heafield, 2011), 3. a word penalty score, 4. a distortion model with geometric decay (Koehn et al., 2003), and 5. a feature indicating the number of times a given distortion limit is exceeded in the current state. In our experiments, the last feature is used with a fixed weight of negative infinity in order to limit the gaps between the coverage sets of adjacent anchored phrase pairs to a maximum value. In DP search, the distortion limit is usually enforced directly by the search algorithm and is not added as a feature. In o</context>
<context position="32202" citStr="Koehn et al., 2003" startWordPosition="5490" endWordPosition="5493">ically targets content words, which benefit from the information weighting done by the NIST score. While the results we present do not constitute compelling evidence in favour of our semantic LM in its current form, they do suggest that this model could be improved to realise higher gains from cross-sentence semantic information. They support our claim that crosssentence models should be examined more closely and that existing methods should be adapted to deal with them, a problem addressed by our main contribution, the local search document decoder. 5 Related Work Even though DP beam search (Koehn et al., 2003) has been the dominant approach to SMT decoding in recent years, methods based on local search have been explored at various times. For word-based SMT, greedy hill-climbing techniques were advocated as a faster replacement for beam search (Germann et al., 2001; Germann, 2003; Germann et al., 2004), and a problem formulation specifically targeting word reordering with an efficient word reordering algorithm has been proposed (Eisner and Tromble, 2006). A local search decoder has been advanced as a faster alternative to beam search also for phrasebased SMT (Langlais et al., 2007; Langlais et al.,</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of the 2003 conference of the North American chapter of the Association for Computational Linguistics on Human Language Technology, pages 48– 54, Edmonton.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
</authors>
<title>Moses: open source toolkit for Statistical Machine Translation.</title>
<date>2007</date>
<booktitle>In Annual meeting of the Association for Computational Linguistics: Demonstration session,</booktitle>
<pages>177--180</pages>
<location>Prague.</location>
<contexts>
<context position="7782" citStr="Koehn et al., 2007" startWordPosition="1291" endWordPosition="1294">phrase pairs jointly cover the complete input sentence, or ni U C(Aj) = [1;mi]. (3) j=1 Let f (S) be a scoring function mapping a state S to a real number. As usual in SMT, it is assumed that the scoring function can be decomposed into a linear combination of K feature functions hk(S), each with a constant weight Xk, so K f (S) = E Xkhk(S). (4) k=1 The problem addressed by the decoder is the search for the state Sˆ with maximal score, such that Sˆ = argmax S f(S). (5) 1180 The feature functions implemented in our baseline system are identical to the ones found in the popular Moses SMT system (Koehn et al., 2007). In particular, our decoder has the following feature functions: 1. phrase translation scores provided by the phrase table including forward and backward conditional probabilities, lexical weights and a phrase penalty (Koehn et al., 2003), 2. n-gram language model scores implemented with the KenLM toolkit (Heafield, 2011), 3. a word penalty score, 4. a distortion model with geometric decay (Koehn et al., 2003), and 5. a feature indicating the number of times a given distortion limit is exceeded in the current state. In our experiments, the last feature is used with a fixed weight of negative </context>
<context position="17517" citStr="Koehn et al., 2007" startWordPosition="2978" endWordPosition="2981">n, we chose to implement a very simple procedure based only on the observation that, at least for language pairs involving the major European languages, it is usually a good guess to keep the word order of the output very similar to that of the input. We therefore create the initial state by selecting, for each sentence in the document, a sequence of anchored phrase pairs covering the input sentence in monotonic order, that is, such that for all pairs of adjacent anchored phrase pairs Aj and Aj+1, we have that C(Aj) � C(Aj+1). For initialisation with DP search, we first run the Moses decoder (Koehn et al., 2007) with default search parameters and the same models as those used by our decoder. Then we extract the best output hypothesis from the search graph of the decoder and map it into a sequence of anchored phrase pairs in the obvious way. When the document-level decoder is used with models that are incompatible with beam search, Moses can be run with a subset of the models in order to find an approximation of the solution which is then refined with the complete feature set. 3 State Operations Given a document state S, the decoder uses a neighbourhood function Neighbour to simulate a move in the sta</context>
<context position="22607" citStr="Koehn et al., 2007" startWordPosition="3845" endWordPosition="3848">ed on Europarl, news-commentary and UN data. To reduce the training data to a manageable size, singleton phrase pairs were removed before the phrase scoring step. Significance-based filtering (Johnson et al., 2007) was applied to the resulting phrase table. The language model was a 5- gram model with Kneser-Ney smoothing trained on the monolingual News corpus with IRSTLM (Federico et al., 2008). Feature weights were trained with Minimum Error-Rate Training (MERT) (Och, 2003) on the news-test2008 development set using the DP beam search decoder and the MERT implementation of the Moses toolkit (Koehn et al., 2007). Experimental results are reported for the newstest2009 test set, a corpus of 111 newswire documents totalling 2,525 sentences or 65,595 English input tokens. 4.1 Stability An important difference between our decoder and the classical DP decoder as well as previous work in SMT decoding with local search is that our decoder is inherently non-deterministic. This implies that repeated runs of the decoder with the same search parameters, input and models will not, in general, find the same local maximum of the score space. The first empirical question we ask is therefore how different the results</context>
</contexts>
<marker>Koehn, Hoang, Birch, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, et al. 2007. Moses: open source toolkit for Statistical Machine Translation. In Annual meeting of the Association for Computational Linguistics: Demonstration session, pages 177–180, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philippe Langlais</author>
<author>Alexandre Patry</author>
<author>Fabrizio Gotti</author>
</authors>
<title>A greedy decoder for phrase-based statistical machine translation.</title>
<date>2007</date>
<booktitle>In TMI-2007: Proceedings of the 11th International Conference on Theoretical and Methodological Issues in Machine Translation,</booktitle>
<pages>104--113</pages>
<contexts>
<context position="10331" citStr="Langlais et al., 2007" startWordPosition="1739" endWordPosition="1742"> moves are rejected in a row (rejection limit). Algorithm 1 Decoding algorithm Input: an initial document state S; search parameters maxsteps and maxrejected Output: a modified document state 1: nsteps ← 0 2: nrejected ← 0 3: while nsteps &lt; maxsteps and nrejected &lt; maxrejected do 4: S0 ← Neighbour(S) 5: if Accept(f (S0), f (S)) then 6: S ← S0 7: nrejected ← 0 8: else 9: nrejected ← nrejected + 1 10: end if 11: nsteps ← nsteps + 1 12: end while 13: return S A notable difference between this algorithm and other hill climbing algorithms that have been used for SMT decoding (Germann et al., 2004; Langlais et al., 2007) is its non-determinism. Previous work for sentence-level decoding employed a steepest ascent strategy which amounts to enumerating the complete neighbourhood of the current state as defined by the state operations and selecting the next state to be the best state found in the neighbourhood of the current one. Enumerating all neighbours of a given state, costly as it is, has the advantage that it makes it easy to prove local optimality of a state by recognising that all possible successor states have lower scores. It can be rather inefficient, since at every step only one modification will be </context>
<context position="18601" citStr="Langlais et al., 2007" startWordPosition="3170" endWordPosition="3173">ature set. 3 State Operations Given a document state S, the decoder uses a neighbourhood function Neighbour to simulate a move in the state space. The neighbourhood function nondeterministically selects a type of state operation and a location in the document to apply it to and returns the resulting new state. We use a set of three operations that has the property that every possible document state can be reached from every other state in a sequence of moves. Designing operations for state transitions in local search for phrase-based SMT is a problem that has been addressed in the literature (Langlais et al., 2007; Arun et al., 2010). Our decoder’s firstchoice hill climbing strategy never enumerates the full neighbourhood of a state. We therefore place less emphasis than previous work on defining a compact neighbourhood, but allow the decoder to make quite extensive changes to a state in a single step with a certain probability. Otherwise our operations are similar to those used by Arun et al. (2010). All of the operations described in this paper make changes to a single sentence only. Each time it is called, the Neighbour function selects a sentence in the document with a probability proportional to t</context>
<context position="32784" citStr="Langlais et al., 2007" startWordPosition="5585" endWordPosition="5588">ugh DP beam search (Koehn et al., 2003) has been the dominant approach to SMT decoding in recent years, methods based on local search have been explored at various times. For word-based SMT, greedy hill-climbing techniques were advocated as a faster replacement for beam search (Germann et al., 2001; Germann, 2003; Germann et al., 2004), and a problem formulation specifically targeting word reordering with an efficient word reordering algorithm has been proposed (Eisner and Tromble, 2006). A local search decoder has been advanced as a faster alternative to beam search also for phrasebased SMT (Langlais et al., 2007; Langlais et al., 2008). That work anticipates many of the features found in our decoder, including the use of local search to refine an initial hypothesis produced by DP beam search. The possibility of using models that do not fit well into the beam search paradigm is mentioned and illustrated with the example of a reversed n-gram language model, which the authors claim would be difficult to implement in a beam search decoder. Similarly to the work by Germann et al. (2001), their decoder is deterministic and explores the entire neighbourhood of a state in order to identify the most promising</context>
</contexts>
<marker>Langlais, Patry, Gotti, 2007</marker>
<rawString>Philippe Langlais, Alexandre Patry, and Fabrizio Gotti. 2007. A greedy decoder for phrase-based statistical machine translation. In TMI-2007: Proceedings of the 11th International Conference on Theoretical and Methodological Issues in Machine Translation, pages 104–113, Sk¨ovde.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philippe Langlais</author>
<author>Alexandre Patry</author>
<author>Fabrizio Gotti</author>
</authors>
<title>Recherche locale pour la traduction statistique par segments. In TALN</title>
<date>2008</date>
<pages>119--128</pages>
<publisher>ATALA.</publisher>
<location>Avignon, France,</location>
<contexts>
<context position="32808" citStr="Langlais et al., 2008" startWordPosition="5589" endWordPosition="5592">hn et al., 2003) has been the dominant approach to SMT decoding in recent years, methods based on local search have been explored at various times. For word-based SMT, greedy hill-climbing techniques were advocated as a faster replacement for beam search (Germann et al., 2001; Germann, 2003; Germann et al., 2004), and a problem formulation specifically targeting word reordering with an efficient word reordering algorithm has been proposed (Eisner and Tromble, 2006). A local search decoder has been advanced as a faster alternative to beam search also for phrasebased SMT (Langlais et al., 2007; Langlais et al., 2008). That work anticipates many of the features found in our decoder, including the use of local search to refine an initial hypothesis produced by DP beam search. The possibility of using models that do not fit well into the beam search paradigm is mentioned and illustrated with the example of a reversed n-gram language model, which the authors claim would be difficult to implement in a beam search decoder. Similarly to the work by Germann et al. (2001), their decoder is deterministic and explores the entire neighbourhood of a state in order to identify the most promising step. Our main contribu</context>
</contexts>
<marker>Langlais, Patry, Gotti, 2008</marker>
<rawString>Philippe Langlais, Alexandre Patry, and Fabrizio Gotti. 2008. Recherche locale pour la traduction statistique par segments. In TALN 2008, pages 119–128, Avignon, France, June. ATALA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Le Nagard</author>
<author>Philipp Koehn</author>
</authors>
<title>Aiding pronoun translation with co-reference resolution.</title>
<date>2010</date>
<booktitle>In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR,</booktitle>
<pages>252--261</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<marker>Le Nagard, Koehn, 2010</marker>
<rawString>Ronan Le Nagard and Philipp Koehn. 2010. Aiding pronoun translation with co-reference resolution. In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR, pages 252–261, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Nicola Ueffing</author>
<author>Hermann Ney</author>
</authors>
<title>An efficient A* search algorithm for Statistical Machine Translation.</title>
<date>2001</date>
<booktitle>In Proceedings of the DataDriven Machine Translation Workshop, 39th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>55--62</pages>
<location>Toulouse.</location>
<contexts>
<context position="2043" citStr="Och et al., 2001" startWordPosition="304" endWordPosition="307">ring to extra-sentential context. This is true both for the phrase-based and the syntaxbased approach to SMT. In the rest of this paper, we shall concentrate on phrase-based SMT. One reason why it is difficult to experiment with document-wide models for phrase-based SMT is that the dynamic programming (DP) algorithm which has been used almost exclusively for decoding SMT models in the recent literature has very strong assumptions of locality built into it. DP beam search for phrase-based SMT was described by Koehn et al. (2003), extending earlier work on word-based SMT (Tillmann et al., 1997; Och et al., 2001; Tillmann and Ney, 2003). This algorithm constructs output sentences by starting with an empty hypothesis and adding output words at the end until translations for all source words have been generated. The core models of phrase-based SMT, in particular the n-gram language model (LM), only depend on a constant number of output words to the left of the word being generated. This fact is exploited by the search algorithm with a DP technique called hypothesis recombination (Och et al., 2001), which permits the elimination of hypotheses from the search space if they coincide in a certain number of</context>
</contexts>
<marker>Och, Ueffing, Ney, 2001</marker>
<rawString>Franz Josef Och, Nicola Ueffing, and Hermann Ney. 2001. An efficient A* search algorithm for Statistical Machine Translation. In Proceedings of the DataDriven Machine Translation Workshop, 39th Annual Meeting of the Association for Computational Linguistics (ACL), pages 55–62, Toulouse.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in Statistical Machine Translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st annual meeting of the Association for Computational Linguistics,</booktitle>
<pages>160--167</pages>
<location>Sapporo</location>
<contexts>
<context position="22467" citStr="Och, 2003" startWordPosition="3824" endWordPosition="3825">training data for the English-French shared task at the WMT 2011 workshop (Callison-Burch et al., 2011). The phrase table was trained on Europarl, news-commentary and UN data. To reduce the training data to a manageable size, singleton phrase pairs were removed before the phrase scoring step. Significance-based filtering (Johnson et al., 2007) was applied to the resulting phrase table. The language model was a 5- gram model with Kneser-Ney smoothing trained on the monolingual News corpus with IRSTLM (Federico et al., 2008). Feature weights were trained with Minimum Error-Rate Training (MERT) (Och, 2003) on the news-test2008 development set using the DP beam search decoder and the MERT implementation of the Moses toolkit (Koehn et al., 2007). Experimental results are reported for the newstest2009 test set, a corpus of 111 newswire documents totalling 2,525 sentences or 65,595 English input tokens. 4.1 Stability An important difference between our decoder and the classical DP decoder as well as previous work in SMT decoding with local search is that our decoder is inherently non-deterministic. This implies that repeated runs of the decoder with the same search parameters, input and models will</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in Statistical Machine Translation. In Proceedings of the 41st annual meeting of the Association for Computational Linguistics, pages 160–167, Sapporo (Japan).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of Machine Translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th annual meeting of the Association for Computational Linguistics,</booktitle>
<pages>311--318</pages>
<publisher>ACL.</publisher>
<location>Philadelphia.</location>
<contexts>
<context position="23913" citStr="Papineni et al., 2002" startWordPosition="4071" endWordPosition="4074">h random state initialisation, i. e. without running the DP beam search decoder. Figure 1 shows the results of 7 decoder runs with the models described above, translating the newstest2009 test set, with a step limit of 227 and a rejection limit of 100,000. The x-axis of both plots shows the number of decoding steps on a logarithmic scale, so the number of steps is doubled between two adjacent points on the same curve. In the left plot, the y-axis indicates the model score optimised by the decoder summed over all 2525 sentences of the document. In the right plot, the case-sensitive BLEU score (Papineni et al., 2002) of the current decoder 1184 Figure 1: Score stability in repeated decoder runs state against a reference translation is displayed. We note, as expected, that the decoder achieves a considerable improvement of the initial state with diminishing returns as decoding continues. Between 28 and 214 steps, the score increases at a roughly logarithmic pace, then the curve flattens out, which is partly due to the fact that decoding for some documents effectively stopped when the maximum number of rejections was reached. The BLEU score curve shows a similar increase, from an initial score below 5 % to </context>
<context position="31380" citStr="Papineni et al., 2002" startWordPosition="5348" endWordPosition="5351">se in scores for all three test sets demonstrates that the hill climbing decoder manages to fix some of the search errors made by the DP search. The last row contains the scores obtained by adding in the semantic language model. Scores are presented for three publicly available test sets from recent WMT Machine Translation shared tasks, of which one (newstest2009) was used to monitor progress during development and select the final model. Adding the semantic language model results in a small increase in NIST scores (Doddington, 2002) for all three test sets as well as a small BLEU score gain (Papineni et al., 2002) for two out of three corpora. We note that the NIST score turned out to react more sensitively to improvements due to the semantic LM in all our experiments, which is reasonable because the model specifically targets content words, which benefit from the information weighting done by the NIST score. While the results we present do not constitute compelling evidence in favour of our semantic LM in its current form, they do suggest that this model could be improved to realise higher gains from cross-sentence semantic information. They support our claim that crosssentence models should be examin</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: a method for automatic evaluation of Machine Translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311–318, Philadelphia. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yik-Cheung Tam</author>
<author>Ian Lane</author>
<author>Tanja Schultz</author>
</authors>
<title>Bilingual LSA-based adaptation for Statistical Machine Translation.</title>
<date>2007</date>
<journal>Machine Translation,</journal>
<volume>21</volume>
<issue>4</issue>
<pages>207</pages>
<contexts>
<context position="28702" citStr="Tam et al., 2007" startWordPosition="4886" endWordPosition="4889">lts of the application of our decoder to an actual SMT model with cross-sentence features. Our model addresses the problem of lexical cohesion. In particular, it rewards the use of semantically related words in the translation output by the decoder, where semantic distance is measured with a word space model based on Latent Semantic Analysis (LSA). LSA has been applied to semantic language modelling in previous research with some success (Coccaro and Jurafsky, 1998; Bellegarda, 2000; Wandmacher and Antoine, 2007). In SMT, it has mostly been used for domain adaptation (Kim and Khudanpur, 2004; Tam et al., 2007), or to measure sentence similarities (Banchs and Costa-juss`a, 2011). The model we use is inspired by Bellegarda (2000). It is a Markov model, similar to a standard n-gram model, and assigns to each content word a score given a history of n preceding content words, where n = 30 below. Scoring relies on a 30- dimensional LSA word vector space trained with the S-Space software (Jurgens and Stevens, 2010). The score is defined based on the cosine similarity between the word vector of the predicted word and the mean word vector of the words in the history, which is converted to a probability by h</context>
</contexts>
<marker>Tam, Lane, Schultz, 2007</marker>
<rawString>Yik-Cheung Tam, Ian Lane, and Tanja Schultz. 2007. Bilingual LSA-based adaptation for Statistical Machine Translation. Machine Translation, 21(4):187– 207.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J¨org Tiedemann</author>
</authors>
<title>To cache or not to cache? Experiments with adaptive models in Statistical Machine Translation.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACL 2010 Joint Fifth Workshop on Statistical Machine Translation and Metrics MATR,</booktitle>
<pages>189--194</pages>
<institution>Uppsala, Sweden. Association for Computational Linguistics.</institution>
<contexts>
<context position="4223" citStr="Tiedemann (2010)" startWordPosition="649" endWordPosition="650">ational Natural Language Learning, pages 1179–1190, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics any obvious way, especially if joint optimisation of a number of interdependent decisions over an entire document is required. Research into models with a more varied, non-local dependency structure is to some extent stifled by the difficulty of decoding such models effectively, as can be seen by the problems some researchers encountered when they attempted to solve discourse-level problems. Consider, for instance, the work on cache-based language models by Tiedemann (2010) and Gong et al. (2011), where error propagation was a serious issue, or the works on pronominal anaphora by Le Nagard and Koehn (2010), who implemented cross-sentence dependencies with an ad-hoc two-pass decoding strategy, and Hardmeier and Federico (2010) with the use of an external decoder driver to manage backward-only dependencies between sentences. In this paper, we present a method for decoding complete documents in phrase-based SMT. Our decoder uses a local search approach whose state consists of a complete translation of an entire document at any time. The initial state is improved by</context>
</contexts>
<marker>Tiedemann, 2010</marker>
<rawString>J¨org Tiedemann. 2010. To cache or not to cache? Experiments with adaptive models in Statistical Machine Translation. In Proceedings of the ACL 2010 Joint Fifth Workshop on Statistical Machine Translation and Metrics MATR, pages 189–194, Uppsala, Sweden. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph Tillmann</author>
<author>Hermann Ney</author>
</authors>
<title>Word reordering and a Dynamic Programming beam search algorithm for Statistical Machine Translation. Computational linguistics,</title>
<date>2003</date>
<pages>29--1</pages>
<contexts>
<context position="2068" citStr="Tillmann and Ney, 2003" startWordPosition="308" endWordPosition="311">ential context. This is true both for the phrase-based and the syntaxbased approach to SMT. In the rest of this paper, we shall concentrate on phrase-based SMT. One reason why it is difficult to experiment with document-wide models for phrase-based SMT is that the dynamic programming (DP) algorithm which has been used almost exclusively for decoding SMT models in the recent literature has very strong assumptions of locality built into it. DP beam search for phrase-based SMT was described by Koehn et al. (2003), extending earlier work on word-based SMT (Tillmann et al., 1997; Och et al., 2001; Tillmann and Ney, 2003). This algorithm constructs output sentences by starting with an empty hypothesis and adding output words at the end until translations for all source words have been generated. The core models of phrase-based SMT, in particular the n-gram language model (LM), only depend on a constant number of output words to the left of the word being generated. This fact is exploited by the search algorithm with a DP technique called hypothesis recombination (Och et al., 2001), which permits the elimination of hypotheses from the search space if they coincide in a certain number of final words with a bette</context>
</contexts>
<marker>Tillmann, Ney, 2003</marker>
<rawString>Christoph Tillmann and Hermann Ney. 2003. Word reordering and a Dynamic Programming beam search algorithm for Statistical Machine Translation. Computational linguistics, 29(1):97–133.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph Tillmann</author>
<author>Stephan Vogel</author>
<author>Hermann Ney</author>
<author>Alex Zubiaga</author>
</authors>
<title>A DP-based search using monotone alignments in Statistical Translation.</title>
<date>1997</date>
<booktitle>In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>289--296</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Madrid, Spain,</location>
<contexts>
<context position="2025" citStr="Tillmann et al., 1997" startWordPosition="300" endWordPosition="303">correctly without referring to extra-sentential context. This is true both for the phrase-based and the syntaxbased approach to SMT. In the rest of this paper, we shall concentrate on phrase-based SMT. One reason why it is difficult to experiment with document-wide models for phrase-based SMT is that the dynamic programming (DP) algorithm which has been used almost exclusively for decoding SMT models in the recent literature has very strong assumptions of locality built into it. DP beam search for phrase-based SMT was described by Koehn et al. (2003), extending earlier work on word-based SMT (Tillmann et al., 1997; Och et al., 2001; Tillmann and Ney, 2003). This algorithm constructs output sentences by starting with an empty hypothesis and adding output words at the end until translations for all source words have been generated. The core models of phrase-based SMT, in particular the n-gram language model (LM), only depend on a constant number of output words to the left of the word being generated. This fact is exploited by the search algorithm with a DP technique called hypothesis recombination (Och et al., 2001), which permits the elimination of hypotheses from the search space if they coincide in a</context>
</contexts>
<marker>Tillmann, Vogel, Ney, Zubiaga, 1997</marker>
<rawString>Christoph Tillmann, Stephan Vogel, Hermann Ney, and Alex Zubiaga. 1997. A DP-based search using monotone alignments in Statistical Translation. In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics, pages 289–296, Madrid, Spain, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tonio Wandmacher</author>
<author>Jean-Yves Antoine</author>
</authors>
<title>Methods to integrate a language model with semantic information for a word prediction component.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL),</booktitle>
<pages>506--513</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="28603" citStr="Wandmacher and Antoine, 2007" startWordPosition="4868" endWordPosition="4871">between decoding time and accuracy. 4.3 A Semantic Document Language Model In this section, we present the results of the application of our decoder to an actual SMT model with cross-sentence features. Our model addresses the problem of lexical cohesion. In particular, it rewards the use of semantically related words in the translation output by the decoder, where semantic distance is measured with a word space model based on Latent Semantic Analysis (LSA). LSA has been applied to semantic language modelling in previous research with some success (Coccaro and Jurafsky, 1998; Bellegarda, 2000; Wandmacher and Antoine, 2007). In SMT, it has mostly been used for domain adaptation (Kim and Khudanpur, 2004; Tam et al., 2007), or to measure sentence similarities (Banchs and Costa-juss`a, 2011). The model we use is inspired by Bellegarda (2000). It is a Markov model, similar to a standard n-gram model, and assigns to each content word a score given a history of n preceding content words, where n = 30 below. Scoring relies on a 30- dimensional LSA word vector space trained with the S-Space software (Jurgens and Stevens, 2010). The score is defined based on the cosine similarity between the word vector of the predicted </context>
</contexts>
<marker>Wandmacher, Antoine, 2007</marker>
<rawString>Tonio Wandmacher and Jean-Yves Antoine. 2007. Methods to integrate a language model with semantic information for a word prediction component. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL), pages 506–513, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
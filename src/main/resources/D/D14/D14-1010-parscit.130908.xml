<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000029">
<title confidence="0.996341">
Semi-Supervised Chinese Word Segmentation Using Partial-Label
Learning With Conditional Random Fields
</title>
<author confidence="0.984044">
Fan Yang Paul Vozila
</author>
<affiliation confidence="0.93199">
Nuance Communications Inc. Nuance Communications Inc.
</affiliation>
<email confidence="0.992795">
fan.yang@nuance.com paul.vozila@nuance.com
</email>
<sectionHeader confidence="0.993743" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999930052631579">
There is rich knowledge encoded in on-
line web data. For example, punctua-
tion and entity tags in Wikipedia data
define some word boundaries in a sen-
tence. In this paper we adopt partial-label
learning with conditional random fields to
make use of this valuable knowledge for
semi-supervised Chinese word segmenta-
tion. The basic idea of partial-label learn-
ing is to optimize a cost function that
marginalizes the probability mass in the
constrained space that encodes this knowl-
edge. By integrating some domain adap-
tation techniques, such as EasyAdapt, our
result reaches an F-measure of 95.98% on
the CTB-6 corpus, a significant improve-
ment from both the supervised baseline
and a previous proposed approach, namely
constrained decode.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999969016666667">
A general approach for supervised Chinese word
segmentation is to formulate it as a character se-
quence labeling problem, to label each charac-
ter with its location in a word. For example,
Xue (2003) proposes a four-label scheme based on
some linguistic intuitions: ‘B’ for the beginning
character of a word, ‘I’ for the internal characters,
‘E’ for the ending character, and ‘S’ for single-
character word. Thus the word sequence “洽谈会
很 成功” can be turned into a character sequence
with labels as 洽\B 谈\I 会\E 很\S 成\B 功\E.
A machine learning algorithm for sequence label-
ing, such as conditional random fields (CRF) (Laf-
ferty et al., 2001), can be applied to the labelled
training data to learn a model.
Labelled data for supervised learning of Chi-
nese word segmentation, however, is usually ex-
pensive and tends to be of a limited amount. Re-
searchers are thus interested in semi-supervised
learning, which is to make use of unlabelled data
to further improve the performance of supervised
learning. There is a large amount of unlabelled
data available, for example, the Gigaword corpus
in the LDC catalog or the Chinese Wikipedia on
the web.
Faced with the large amount of unlabelled data,
an intuitive idea is to use self-training or EM, by
first training a baseline model (from the supervised
data) and then iteratively decoding the unlabelled
data and updating the baseline model. Jiao et al.
(2006) and Mann and McCallum (2007) further
propose to minimize the entropy of the predicted
label distribution on unlabeled data and use it as
a regularization term in CRF (i.e. entropy reg-
ularization). Beyond these ideas, Liang (2005)
and Sun and Xu (2011) experiment with deriv-
ing a large set of statistical features such as mu-
tual information and accessor variety from un-
labelled data, and add them to supervised dis-
criminative training. Zeng et al. (2013b) experi-
ment with graph propagation to extract informa-
tion from unlabelled data to regularize the CRF
training. Yang and Vozila (2013), Zhang et al.
(2013), and Zeng et al. (2013a) experiment with
co-training for semi-supervised Chinese word seg-
mentation. All these approaches only leverage
the distribution of the unlabelled data, yet do not
make use of the knowledge that the unlabelled data
might have integrated in.
There could be valuable information encoded
within the unlabelled data that researchers can take
advantage of. For example, punctuation creates
natural word boundaries (Li and Sun, 2009): the
character before a comma can only be labelled
as either ‘S’ or ‘E’, while the character after a
comma can only be labelled as ‘S’ or ‘B’. Fur-
thermore, entity tags (HTML tags or Wikipedia
tags) on the web, such as emphasis and cross refer-
ence, also provide rich information for word seg-
mentation: they might define a word or at least
</bodyText>
<page confidence="0.972736">
90
</page>
<note confidence="0.9741035">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 90–98,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</note>
<figureCaption confidence="0.999805">
Figure 1: Sausage constraint (partial labels) from natural annotations and punctuation
</figureCaption>
<bodyText confidence="0.999014">
give word boundary information similar to punc-
tuation. Jiang et al. (2013) refer to such structural
information on the web as natural annotations, and
propose that they encode knowledge for NLP. For
Chinese word segmentation, natural annotations
and punctuation create a sausages constraint for
the possible labels, as illustrated in Figure 1. In
the sentence “近年来,人工智能和机器学习迅
猛发展。”, the first character 近 can only be la-
belled with ‘S’ or ‘B’; and the characters 来 before
the comma and 展 before the Chinese period can
only be labelled as ‘S’ or ‘E’. “人工智能” and “机
器学习” are two Wikipedia entities, and so they
define the word boundaries before the first char-
acter and after the last character of the entities as
well. The single character 和 between these two
entities has only one label ‘S’. This sausage con-
straint thus encodes rich information for word seg-
mentation.
To make use of the knowledge encoded in the
sausage constraint, Jiang et al. (2013) adopt a con-
strained decode approach. They first train a base-
line model with labelled data, and then run con-
strained decode on the unlabelled data by binding
the search space with the sausage; and so the de-
coded labels are consistent with the sausage con-
straint. The unlabelled data, together with the
labels from constrained decode, are then selec-
tively added to the labelled data for training the
final model. This approach, using constrained de-
code as a middle step, provides an indirect way
of leaning the knowledge. However, the middle
step, constrained decode, has the risk of reinforc-
ing the errors in the baseline model: the decoded
labels added to the training data for building the
final model might contain errors introduced from
the baseline model. The knowledge encoded in
&apos;Also referred to as confusion network.
the data carrying the information from punctuation
and natural annotations is thus polluted by the er-
rorful re-decoded labels.
A sentence where each character has exactly
one label is fully-labelled; and a sentence where
each character receives all possible labels is zero-
labelled. A sentence with sausage-constrained la-
bels can be viewed as partially-labelled. These
partial labels carry valuable information that re-
searchers would like to learn in a model, yet the
normal CRF training typically uses fully-labelled
sentences. Recently, T¨ackstr¨om et al. (2013) pro-
pose an approach to train a CRF model directly
from partial labels. The basic idea is to marginal-
ize the probability mass of the constrained sausage
in the cost function. The normal CRF training us-
ing fully-labelled sentences is a special case where
the sausage constraint is a linear line; while on
the other hand a zero-labelled sentence, where the
sausage constraint is the full lattice, makes no con-
tribution in the learning since the sum of proba-
bilities is deemed to be one. This new approach,
without the need of using constrained re-decoding
as a middle step, provides a direct means to learn
the knowledge in the partial labels.
In this research we explore using the partial-
label learning for semi-supervised Chinese word
segmentation. We use the CTB-6 corpus as the
labelled training, development and test data, and
use the Chinese Wikipedia as the unlabelled data.
We first train a baseline model with labelled data
only, and then selectively add Wikipedia data with
partial labels to build a second model. Because
the Wikipedia data is out of domain and has dis-
tribution bias, we also experiment with two do-
main adaptation techniques: model interpolation
and EasyAdapt (Daum´e III, 2007). Our result
reaches an F-measure of 95.98%, an absolute im-
provement of 0.72% over the very strong base-
</bodyText>
<page confidence="0.998032">
91
</page>
<bodyText confidence="0.999880111111111">
line (corresponding to 15.19% relative error re-
duction), and 0.33% over the constrained decode
approach (corresponding to 7.59% relative error
reduction). We conduct a detailed error analy-
sis, illustrating how partial-label learning excels
constrained decode in learning the knowledge en-
coded in the Wikipedia data. As a note, our result
also out-performs (Wang et al., 2011) and (Sun
and Xu, 2011).
</bodyText>
<sectionHeader confidence="0.955473" genericHeader="method">
2 Partial-Label Learning with CRF
</sectionHeader>
<bodyText confidence="0.989959555555556">
In this section, we review in more detail the
partial-label learning algorithm with CRF pro-
posed by (T¨ackstr¨om et al., 2013). CRF is an
exponential model that expresses the conditional
probability of the labels given a sequence, as
Equation 1, where y denotes the labels, x denotes
the sequence, 4b(x, y) denotes the feature func-
tions, and θ is the parameter vector. Z(x) =
∑y exp(θT4b(x, y)) is the normalization term.
</bodyText>
<equation confidence="0.994732">
exp(θT�(x, y))
pθ(y|x) = (1)
Z(x)
</equation>
<bodyText confidence="0.993709833333333">
In full-label training, where each item in the se-
quence is labelled with exactly one tag, maximum
likelihood is typically used as the optimization tar-
get. We simply sum up the log-likelihood of the n
labelled sequences in the training set, as shown in
Equation 2.
</bodyText>
<equation confidence="0.89914425">
log pθ(y|x)
(2)
(θT4&apos;(xi,yi) − log Z(xi))
The gradient is calculated as Equation 3, in
</equation>
<bodyText confidence="0.918903166666667">
which the first term 1 ∑n i=1 �j is the empirical
n
expectation of feature function 4bj, and the second
term E[4bj] is the model expectation. Typically a
forward-backward process is adopted for calculat-
ing the latter.
</bodyText>
<equation confidence="0.922886">
n
i=1
</equation>
<bodyText confidence="0.9999196">
In partial-label training, each item in the se-
quence receives multiple labels, and so for each
sequence we have a sausage constraint, denoted as
Y(x, y). The marginal probability of the sausage
is defined as Equation 4.
</bodyText>
<equation confidence="0.994857">
pθ( Y� (x, �y)|x) = ∑ pθ(y|x) (4)
yEYˆ(x,˜y)
</equation>
<bodyText confidence="0.998741">
The optimization target thus is to maximize the
probability mass of the sausage, as shown in Equa-
tion 5.
</bodyText>
<equation confidence="0.999016">
L(θ) = ∑n logpθ( Y�(xi, 9i)|xi) (5)
i=1
</equation>
<bodyText confidence="0.893178357142857">
A gradient-based approach such as L-BFGS
(Liu and Nocedal, 1989) can be employed to op-
timize Equation 5. The gradient is calculated as
Equation 6, where EˆY (x ˜y) [4bj] is the empirical ex-
pectation of feature function 4bj constrained by the
sausage, and E[4bj] is the same model expectation
as in standard CRF. EˆY (x,˜y)[4bj] can be calculated
via a forward-backward process in the constrained
sausage.
∂θj
∂ L(θ) = EˆY (x,˜y)[`Fj] − E[`bj] (6)
For fully-labelled sentences, EˆY (x,˜y)[bj] =
n∑n 1 4bj, and so the standard CRF is actually
a special case of the partial-label learning.
</bodyText>
<sectionHeader confidence="0.991834" genericHeader="method">
3 Experiment setup
</sectionHeader>
<bodyText confidence="0.999461666666667">
In this section we describe the basic setup for
our experiments of semi-supervised Chinese word
segmentation.
</bodyText>
<subsectionHeader confidence="0.996882">
3.1 Data
</subsectionHeader>
<bodyText confidence="0.999989823529412">
We use the CTB-6 corpus as the labelled data. We
follow the official CTB-6 guideline in splitting the
corpus into a training set, a development set, and a
test set. The training set has 23420 sentences; the
development set has 2079 sentences; and the test
set has 2796 sentences. These are fully-labelled
data.
For unlabelled data we use the Chinese
Wikipedia. The Wikipedia data is quite noisy
and asks for a lot of cleaning. We first filter out
references and lists etc., and sentences with ob-
viously bad segmentations, for example, where
every character is separated by a space. We
also remove sentences that contain mostly En-
glish words. We then convert all characters into
full-width. We also convert traditional Chinese
characters into simplified characters using the tool
</bodyText>
<equation confidence="0.999568142857143">
L(θ) = ∑n
i=1
= ∑n
i=1
∂θj
∂ 1
L(θ) = n ∑bj − E[`Fj] (3)
</equation>
<page confidence="0.829564">
92
</page>
<bodyText confidence="0.999811454545455">
mediawiki-zhconverter2. We then randomly select
7737 sentences and reserve them as the test set.
To create the partial labels in the Wikipedia
data, we use the information from cross-reference,
emphasis, and punctuation. In our pilot study we
found that it’s beneficial to force a cross-reference
or emphasis entity as a word when the item has
2 or 3 characters. That is, if an entity in the
Wikipedia has three characters it receives the la-
bels of “BIE”; and if it has two characters it is la-
belled as “BE”.3
</bodyText>
<subsectionHeader confidence="0.999529">
3.2 Supervised baseline model
</subsectionHeader>
<bodyText confidence="0.8777978">
We create the baseline supervised model by using
an order-1 linear CRF with L2 regularization, to
label a character sequence with the four candidate
labels “BIES”. We use the tool wapiti (Lavergne
et al., 2010).
Following Sun et al. (2009), Sun (2010), and
Low et al. (2005), we extract two types of fea-
tures: character-level features and word-level fea-
tures. Given a character c0 in the character se-
quence ...c−2c−1c0c1c2...:
</bodyText>
<listItem confidence="0.9277426">
Character-level features :
• Character unigrams: c−2, c−1, c0, c1, c2
• Character bigrams: c−2c−1, c−1c−0,
c0c1, c1c2
• Consecutive character equivalence:
</listItem>
<equation confidence="0.92184">
?c−2 = c−1, ?c−1 = c−0 , ?c0 = c1,
?c1 = c2
• Separated character equivalence:
?c−3 = c−1, ?c−2 = c0, ?c−1 = c1,
?c0 = c2, ?c1 = c3
</equation>
<listItem confidence="0.92073375">
• Whether the current character is a punc-
tuation: ?Punct(c0)
• Character sequence pattern:
T (C−2)T (C−1)T (C0)T (C1)T (C2).
</listItem>
<bodyText confidence="0.976366375">
We classify all characters into four
types. Type one has three characters
‘4-f’ (year) ‘A’ (month) ‘Q’ (date).
Type two includes number characters.
Type three includes English characters.
All others are Type four characters.
Thus “t4-fEAS” would generate the
character sequence pattern “41213”.
</bodyText>
<footnote confidence="0.999345333333333">
2https://github.com/tszming/mediawiki-zhconverter
3Another possibility is to label it as “SS” but we find that
it’s very rare the case.
</footnote>
<listItem confidence="0.967658105263158">
Word-level features :
• The identity of the string c[s : i] (i−6 &lt;
s &lt; i), if it matches a word from the
list of word unigrams; multiple features
could be generated.
• The identity of the string c[i : e] (i &lt;
e &lt; i+6), if it matches a word; multiple
features could be generated.
• The identity of the bi-gram c[s : i −
1]c[i : e] (i − 6 &lt; s, e &lt; i + 6), if
it matches a word bigram; multiple fea-
tures could be generated.
• The identity of the bi-gram c[s : i]c[i +
1 : e] (i−6 &lt; s, e &lt; i+6), if it matches
a word bigram; multiple features could
be generated.
• Idiom. We use the idiom list from (Sun
and Xu, 2011). If the current character
c0 and its surrounding context compose
</listItem>
<bodyText confidence="0.83718775">
an idiom, we generate a feature for c0 of
its position in the idiom. For example, if
c−1c0c1c2 is an idiom, we generate fea-
ture “Idiom-2” for c0.
The above features together with label bigrams
are fed to wapiti for training. The supervised base-
line model is created with the CTB-6 corpus with-
out the use of Wikipedia data.
</bodyText>
<subsectionHeader confidence="0.989787">
3.3 Partial-label learning
</subsectionHeader>
<bodyText confidence="0.9994509">
The overall process of applying partial-label learn-
ing to Wikipedia data is shown in Algorithm 1.
Following (Jiang et al., 2013), we first train the
supervised baseline model, and use it to estimate
the potential contribution for each sentence in the
Wikipedia training data. We label the sentence
with the baseline model, and then compare the
labels with the constrained sausage. For each
character, a consistent label is defined as an ele-
ment in the constrained labels. For example, if
the constrained labels for a character are “SB”,
the label ‘S’ or ‘B’ is consistent but ‘I’ or ‘E’ is
not. The number of inconsistent labels for each
sentence is then used as its potential contribution
to the partial-label learning: higher number indi-
cates that the partial-labels for the sentence con-
tain more knowledge that the baseline system does
not integrate, and so have higher potential contri-
bution. The Wikipedia training sentences are then
ranked by their potential contribution, and the top
</bodyText>
<page confidence="0.99679">
93
</page>
<figureCaption confidence="0.997358">
Figure 2: Encoded knowledge: inconsistency ratio
and label reduction
</figureCaption>
<bodyText confidence="0.9808068">
K sentences together with their partial labels are
then added to the CTB-6 training data to build a
new model, using partial-label learning.4 In our
experiments, we try six data points with K =
100k, 200k, 300k, 400k, 500k, 600k. Figure 2
gives a rough idea of the knowledge encoded in
Wikipedia for these data points with inconsistency
ratio and label reduction. Inconsistency ratio is the
percentage of characters that have inconsistent la-
bels; and label reduction is the percentage of the
labels reduced in the full lattice.
We modify wapiti to implement the partial-label
learning as described in Section 2. Same as base-
line, L2 regularization is adopted.
Algorithm 1 Partial-label learning
</bodyText>
<listItem confidence="0.998638111111111">
1. Train supervised baseline model Mo
2. For each sentence x in Wiki-Train:
3. y ← Decode(x, Mo)
4. diff ← Inconsistent(y, Y(x, y))
5. if diff &gt; 0:
6. C ← C ∪ ( Y(x, y), diff)
7. Sort(C, diff, reverse)
8. Train model Mpl with CTB-6 and top K sen-
tences in C using partial-label learning
</listItem>
<subsectionHeader confidence="0.976658">
3.4 Constrained decode
</subsectionHeader>
<bodyText confidence="0.999838">
Jiang et al. (2013) implement the constrained de-
code algorithm with perceptron. However, CRF
is generally believed to out-perform perceptron,
yet the comparison of CRF vs perceptron is out
</bodyText>
<footnote confidence="0.698561">
4Knowledge is sparsely distributed in the Wikipedia data.
Using the Wikipedia data without the CTB-6 data for partial-
label learning does not necessarily guarantee convergence.
Also the CTB-6 training data helps to learn that certain la-
bel transitions, such as “B B” or “E E”, are not legal.
</footnote>
<bodyText confidence="0.999651117647059">
of the scope of this paper. Thus for fair compar-
ison, we re-implement the constrained decode al-
gorithm with CRF.
Algorithm 2 shows the constrained decode im-
plementation. We first train the baseline model
with the CTB-6 data. We then use this baseline
model to run normal decode and constrained de-
code for each sentence in the Wikipedia training
set. If the normal decode and constrained decode
have different labels, we add the constrained de-
code together with the number of different labels
to the filtered Wikipedia training corpus. The fil-
tered Wikipedia training corpus is then sorted us-
ing the number of different labels, and the top K
sentences with constrained decoded labels are then
added to the CTB-6 training data for building a
new model using normal CRF.
</bodyText>
<listItem confidence="0.952412909090909">
Algorithm 2 Constrained decode
1. Train supervised baseline model Mo
2. For each sentence x in Wiki-Train:
3. y ← Decode(x, Mo)
4. y� ← ConstrainedDecode(x, Mo)
5. diff ← Difference(y, y)
6. if diff &gt; 0:
7. C ← C ∪ (y, diff)
8. Sort(C, diff, reverse)
9. Train model Md with CTB-6 and top K sen-
tences in C using normal CRF
</listItem>
<sectionHeader confidence="0.919223" genericHeader="method">
4 Evaluation on Wikipedia test set
</sectionHeader>
<bodyText confidence="0.99997805">
In order to determine how well the models learn
the encoded knowledge (i.e. partial labels) from
the Wikipedia data, we first evaluate the mod-
els against the Wikipedia test set. The Wikipedia
test set, however, is only partially-labelled. Thus
the metric we use here is consistent label accu-
racy, similar to how we rank the sentences in Sec-
tion 3.3, defined as whether a predicted label for
a character is an element in the constrained la-
bels. Because partial labels are only sparsely dis-
tributed in the test data, a lot of characters receive
all four labels in the constrained sausage. Eval-
uating against characters with all four labels do
not really represent the models’ difference as it is
deemed to be consistent. Thus beyond evaluating
against all characters in the Wikipedia test set (re-
ferred to as Full measurement), we also evaluate
against characters that are only constrained with
less than four labels (referred to as Label mea-
surement). The Label measurement focuses on en-
</bodyText>
<page confidence="0.998642">
94
</page>
<bodyText confidence="0.999714066666666">
coded knowledge in the test set and so can better
represent the model’s capability of learning from
the partial labels.
Results are shown in Figure 3 with the Full
measurement and in Figure 4 with the Label mea-
surement. The x axes are the size of Wikipedia
training data, as explained in Section 3.3. As
can be seen, both constrained decode and partial-
label learning perform much better than the base-
line supervised model that is trained from CTB-6
data only, indicating that both of them are learning
the encoded knowledge from the Wikipedia train-
ing data. Also we see the trend that the perfor-
mance improves with more data in training, also
suggesting the learning of encoded knowledge.
Most importantly, we see that partial-label learn-
ing consistently out-performs constrained decode
in all data points. With the Label measurement,
partial-label learning gives 1.7% or higher abso-
lute improvement over constrained decode across
all data points. At the data point of 600k, con-
strained decode gives an accuracy of 97.14%,
while partial-label learning gives 98.93% (base-
line model gives 87.08%). The relative gain (from
learning the knowledge) of partial-label learning
over constrained decode is thus 18% ((98.93 −
97.14)/(97.14 − 87.08)). These results suggest
that partial-label learning is more effective in
learning the encoded knowledge in the Wikipedia
data than constrained decode.
</bodyText>
<sectionHeader confidence="0.991447" genericHeader="method">
5 CTB evaluation
</sectionHeader>
<subsectionHeader confidence="0.993354">
5.1 Model adaptation
</subsectionHeader>
<bodyText confidence="0.999976222222222">
Our ultimate goal, however, is to determine
whether we can leverage the encoded knowledge
in the Wikipedia data to improve the word seg-
mentation in CTB-6. We run our models against
the CTB-6 test set, with results shown in Fig-
ure 5. Because we have fully-labelled sentences
in the CTB-6 data, we adopt the F-measure as
our evaluation metric here. The baseline model
achieves 95.26% in F-measure, providing a state-
of-the-art supervised performance. Constrained
decode is able to improve on this already very
strong baseline performance, and we see the nice
trend of higher performance with more unlabeled
data for training, indicating that constrained de-
code is making use of the encoded knowledge in
the Wikipedia data to help CTB-6 segmentation.
When we look at the partial-label model, how-
ever, the results tell a totally different story.
</bodyText>
<figureCaption confidence="0.999663333333333">
Figure 3: Wiki label evaluation results: Full
Figure 4: Wiki label evaluation results: Label
Figure 5: CTB evaluation results
</figureCaption>
<page confidence="0.996447">
95
</page>
<bodyText confidence="0.999923066666667">
First, it actually performs worse than the base-
line model, and the more data added to train-
ing, the worse the performance is. In the previ-
ous section we show that partial-label learning is
more effective in learning the encoded knowledge
in Wikipedia data than constrained decode. So,
what goes wrong? We hypothesize that there is
an out-of-domain distribution bias in the partial la-
bels, and so the more data we add, the worse the
in-domain performance is. Constrained decode
actually helps to smooth out the out-of-domain
distribution bias by using the re-decoded labels
with the in-domain supervised baseline model.
For example, both the baseline model and con-
strained decode correctly give the segmentation
“提供/了/运输/和/给排水/之/便”, while partial-
label learning gives incorrect segmentation “提
供/了/运输/和/给/排水/之/便”. Looking at the
Wikipedia training data, 排水 is tagged as an en-
tity 13 times; and 给排水, although occurs 13
times in the data, is never tagged as an entity.
Partial-label learning, which focuses on the tagged
entities, thus overrules the segmentation of 给排
水. Constrained decode, on the other hand, by us-
ing the correctly re-decoded labels from the base-
line model, observes enough evidence to correctly
segment 给排水 as a word.
To smooth out the out-of-domain distribution
bias, we experiment with two approaches: model
interpolation and EasyAdapt (Daum´e III, 2007).
</bodyText>
<subsubsectionHeader confidence="0.750601">
5.1.1 Model interpolation
</subsubsectionHeader>
<bodyText confidence="0.9999544">
We linearly interpolate the model of partial-label
learning Mpl with the baseline model M0 to create
the final model Mpl� , as shown in Equation 7. The
interpolation weight is optimized via a grid search
between 0.0 and 1.0 with a step of 0.1, tuned on
the CTB-6 development set. Again we modify
wapiti so that it takes two models and an interpo-
lation weight as input. For each model it creates
a search lattice with posteriors, and then linearly
combines the two lattices using the interpolation
weight to create the final search space for decod-
ing. As shown in Figure 5, model Mpl� consis-
tently out-performs constrained decode in all data
points. We also see the trend of better performance
with more training data.
</bodyText>
<equation confidence="0.867774">
Mpl� = A * M0 + (1 − A) * Mpl (7)
</equation>
<subsectionHeader confidence="0.42784">
5.1.2 EasyAdapt
</subsectionHeader>
<bodyText confidence="0.999193878787879">
EasyAdapt is a straightforward technique but has
been shown effective in many domain adaptation
tasks (Daum´e III, 2007). We train the model
Mpl
ea with feature augmentation. For each out-of-
domain training instance &lt; xo, yo &gt;, where xo
is the input features and yo is the (partial) labels,
we copy the features and file them as an additional
feature set, and so the training instance becomes &lt;
xo, xo, yo &gt;. The in-domain training data remains
the same. Consistent with (Daum´e III, 2007),
EasyAdapt gives us the best performance, as show
in Figure 5. Furthermore, unlike in (Jiang et al.,
2013) where they find a plateau, our results show
no harm adding more training data for partial-label
learning when integrated with domain adaptation,
although the performance seems to saturate after
400k sentences.
Finally, we search for the parameter setting of
best performance on the CTB-6 development set,
which is to use EasyAdapt with K = 600k sen-
tences of Wikipedia data. With this setting, the
performance on the CTB-6 test set is 95.98%
in F-measure. This is 0.72% absolute improve-
ment over supervised baseline (corresponding to
15.19% relative error reduction), and 0.33% ab-
solute improvement over constrained decode (cor-
responding to 7.59% relative error reduction); the
differences are both statistically significant (p &lt;
0.001).5 As a note, this result out-performs (Sun
and Xu, 2011) (95.44%) and (Wang et al., 2011)
(95.79%), and the differences are also statistically
significant (p &lt; 0.001).
</bodyText>
<subsectionHeader confidence="0.99999">
5.2 Analysis with examples
</subsectionHeader>
<bodyText confidence="0.999921357142857">
To better understand why partial-label learning is
more effective in learning the encoded knowledge,
we look at cases where M0 and Mcd have the in-
correct segmentation while Mpl (and its domain
adaptation variance Mpl � and Mpl
ea) have the cor-
rect segmentation. We find that the majority is
due to the error in re-decoded labels outside of en-
coded knowledge. For example, M0 and Mcd give
the segmentation “地震/为/里/氏/6.9/ 级”, yet the
correct segmentation given by partial-label learn-
ing is “地震/为/里氏/6.9/ 级”. Looking at the
Wikipedia training data, there are 38 tagged enti-
ties of 里氏, but there are another 190 mentions of
</bodyText>
<footnote confidence="0.927759">
5Statistical significance is evaluated with z-test using the
</footnote>
<page confidence="0.612641">
√
</page>
<bodyText confidence="0.8984275">
standard deviation of F * (1 − F)/N, where F is the F-
measure and N is the number of words.
</bodyText>
<page confidence="0.970792">
96
</page>
<bodyText confidence="0.999957659574468">
里氏 that are not tagged as an entity. Thus for con-
strained decode it sees 38 cases of “里\B 氏\E”
and 190 cases of “里\S 氏\S” in the Wikipedia
training data. The former comes from the encoded
knowledge while the latter comes from re-decoded
labels by the baseline model. The much bigger
number of incorrect labels from the baseline re-
decoding badly pollute the encoded knowledge.
This example illustrates that constrained decode
reinforces the errors from the baseline. On the
other hand, the training materials for partial-label
learning are purely the encoded knowledge, which
is not impacted by the baseline model error. In this
example, partial-label learning focuses only on the
38 cases of “里\B 氏\E” and so is able to learn
that 里氏 is a word.
As a final remark, we want to make a point that,
although the re-decoded labels serve to smooth out
the distribution bias, the Wikipedia data is indeed
not the ideal data set for such a purpose, because
it itself is out of domain. The performance tends
to degrade when we apply the baseline model to
re-decode the out-of-domain Wikipedia data. The
errorful re-decoded labels, when being used to
train the model Mid, could lead to further er-
rors. For example, the baseline model Mo is able
to give the correct segmentation “电脑/元器件”
in the CTB-6 test set. However, when it is ap-
plied to the Wikipedia data for constrained de-
code, for the seven occurrences of 元器件, three of
which are correctly labelled as “元\B 器\I 件\E”,
but the other four have incorrect labels. The fi-
nal model Mid trained from these labels then
gives incorrect segmentation“两/市/生产/的/电
脑/元/器件/大量/销往/世界/各地” in the CTB-
6 test set. On the other hand, model interpolation
or EasyAdapt with partial-label learning, focusing
only on the encoded knowledge and not being im-
pacted by the errorful re-decoded labels, performs
correctly in this case. For a more fair comparison
between partial-label learning and constrained de-
code, we have also plotted the results of model in-
terpolation and EasyAdapt for constrained decode
in Figure 5. As can be seen, they improve on con-
strained decode a bit but still fall behind the cor-
respondent domain adaptation approach of partial-
label learning.
</bodyText>
<sectionHeader confidence="0.992288" genericHeader="conclusions">
6 Conclusion and future work
</sectionHeader>
<bodyText confidence="0.999987064516129">
There is rich information encoded in online web
data. For example, punctuation and entity tags de-
fine some word boundaries. In this paper we show
the effectiveness of partial-label learning in digest-
ing the encoded knowledge from Wikipedia data
for the task of Chinese word segmentation. Unlike
approaches such as constrained decode that use
the errorful re-decoded labels, partial-label learn-
ing provides a direct means to learn the encoded
knowledge. By integrating some domain adap-
tation techniques such as EasyAdapt, we achieve
an F-measure of 95.98% in the CTB-6 corpus, a
significant improvement from both the supervised
baseline and constrained decode. Our results also
beat (Wang et al., 2011) and (Sun and Xu, 2011).
In this research we employ a sausage constraint
to encode the knowledge for Chinese word seg-
mentation. However, a sausage constraint does
not reflect the legal label sequence. For exam-
ple, in Figure 1 the links between label ‘B’ and
label ‘S’, between ‘S’ and ‘E’, and between ‘E’
and ‘I’ are illegal, and can confuse the machine
learning. In our current work we solve this issue
by adding some fully-labelled data into training.
Instead we can easily extend our work to use a lat-
tice constraint by removing the illegal transitions
from the sausage. The partial-label learning stands
the same, by executing the forward-backward pro-
cess in the constrained lattice. In future work we
will examine partial-label learning with this more
enforced lattice constraint in depth.
</bodyText>
<sectionHeader confidence="0.997639" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99983975">
The authors would like to thank Wenbin Jiang, Xi-
aodong Zeng, and Weiwei Sun for helpful discus-
sions, and the anonymous reviewers for insightful
comments.
</bodyText>
<sectionHeader confidence="0.99857" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.996145066666667">
Hal Daum´e III. 2007. Frustratingly easy domain adap-
tation. In Annual meetingassociation for computa-
tional linguistics, pages 256–263. Association for
Computational Linguistics.
Wenbin Jiang, Meng Sun, Yajuan Lv, Yating Yang,
and Qun Liu. 2013. Discriminative learning with
natural annotations: Word segmentation as a case
study. In Proceedings of The 51st Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 761–769.
Feng Jiao, Shaojun Wang, Chi-Hoon Lee, Russell
Greiner, and Dale Schuurmans. 2006. Semi-
supervised conditional random fields for improved
sequence segmentation and labeling. In Proceed-
ings of the 21st International Conference on Com-
</reference>
<page confidence="0.993201">
97
</page>
<reference confidence="0.999631585106383">
putational Linguistics and the 44th Annual Meet-
ing of the Association for Computational Linguis-
tics, ACL-44, pages 209–216.
John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling se-
quence data. In Proceedings of the Eighteenth In-
ternational Conference on Machine Learning, pages
282–289, San Francisco, CA, USA.
Thomas Lavergne, Olivier Capp´e, and Franc¸ois Yvon.
2010. Practical very large scale CRFs. In Proceed-
ings the 48th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 504–513.
Association for Computational Linguistics, July.
Zhongguo Li and Maosong Sun. 2009. Punctuation as
implicit annotations for Chinese word segmentation.
Computational Linguistics, 35:505–512.
Percy Liang. 2005. Semi-supervised learning for natu-
ral language. Master’s thesis, MASSACHUSETTS
INSTITUTE OF TECHNOLOGY, May.
D. C. Liu and J. Nocedal. 1989. On the limited mem-
ory bfgs method for large scale optimization. Math-
ematical Programming, 45(3):503–528, December.
Jin Kiat Low, Hwee Tou Ng, and Wenyuan Guo. 2005.
A maximum entropy approach to chinese word seg-
mentation. In Proceedings of the Fourth SIGHAN
Workshop on Chinese Language Processing, pages
161–164, San Francisco, CA, USA.
Gideon S. Mann and Andrew McCallum. 2007. Ef-
ficient computation of entropy gradient for semi-
supervised conditional random fields. In Human
Language Technologies 2007: The Conference of
the North American Chapter of the Association
for Computational Linguistics; Companion Volume,
Short Papers, NAACL-Short ’07, pages 109–112.
Weiwei Sun and Jia Xu. 2011. Enhancing chinese
word segmentation using unlabeled data. In Pro-
ceedings of the 2011 Conference on Empirical Meth-
ods in Natural Language Processing, pages 970–
979, Edinburgh, Scotland, UK., July.
Xu Sun, Yaozhong Zhang, Takuya Matsuzaki, Yoshi-
masa Tsuruoka, and Jun’ichi Tsujii. 2009. A dis-
criminative latent variable Chinese segmenter with
hybrid word/character information. In Proceedings
of Human Language Technologies: The 2009 An-
nual Conference of the North American Chapter
of the Association for Computational Linguistics,
pages 56–64.
Weiwei Sun. 2010. Word-based and character-based
word segmentation models: comparison and com-
bination. In Proceedings of the 23rd International
Conference on Computational Linguistics: Posters,
pages 1211–1219.
Oscar T¨ackstr¨om, Dipanjan Das, Slav Petrov, Ryan
McDonald, and Joakim Nivre. 2013. Token and
type constraints for cross-lingual part-of-speech tag-
ging. Transactions of the Association for Computa-
tional Linguistics, 1:1–12.
Yiou Wang, Jun’ichi Kazama, Yoshimasa Tsuruoka,
Wenliang Chen, Yujie Zhang, and Kentaro Tori-
sawa. 2011. Improving chinese word segmentation
and POS tagging with semi-supervised methods us-
ing large auto-analyzed data. In Proceedings of the
5th International Joint Conference on Natural Lan-
guage Processing, pages 309–317.
Nianwen Xue. 2003. Chinese word segmentation as
character tagging. Computational Linguistics and
Chinese Language Processing, pages 29–48.
Fan Yang and Paul Vozila. 2013. An empirical study
of semi-supervised Chinese word segmentation us-
ing co-training. In Proceedings of the 2013 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1191–1200, Seattle, Washington,
USA, October. Association for Computational Lin-
guistics.
Xiaodong Zeng, Derek F. Wong, Lidia S. Chao, and
Isabel Trancoso. 2013a. Co-regularizing character-
based and word-based models for semi-supervised
chinese word segmentation. In Proceedings of The
51st Annual Meeting of the Association for Compu-
tational Linguistics, pages 171–176.
Xiaodong Zeng, Derek F. Wong, Lidia S. Chao,
and Isabel Trancoso. 2013b. Graph-based semi-
supervised model for joint chinese word segmen-
tation and part-of-speech tagging. In Proceedings
of The 51st Annual Meeting of the Association for
Computational Linguistics, pages 770–779.
Longkai Zhang, Houfeng Wang, Xu Sun, and Mairgup
Mansur. 2013. Exploring representations from un-
labeled data with co-training for Chinese word seg-
mentation. In Proceedings of the 2013 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 311–321, Seattle, Washington, USA,
October. Association for Computational Linguistics.
</reference>
<page confidence="0.99625">
98
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.720610">
<title confidence="0.999111">Semi-Supervised Chinese Word Segmentation Using Learning With Conditional Random Fields</title>
<author confidence="0.999952">Fan Yang Paul Vozila</author>
<affiliation confidence="0.999804">Nuance Communications Inc. Nuance Communications Inc.</affiliation>
<email confidence="0.998843">fan.yang@nuance.compaul.vozila@nuance.com</email>
<abstract confidence="0.985178842105263">There is rich knowledge encoded in online web data. For example, punctuation and entity tags in Wikipedia data define some word boundaries in a sentence. In this paper we adopt partial-label learning with conditional random fields to make use of this valuable knowledge for semi-supervised Chinese word segmentation. The basic idea of partial-label learning is to optimize a cost function that marginalizes the probability mass in the constrained space that encodes this knowledge. By integrating some domain adaptation techniques, such as EasyAdapt, our reaches an F-measure of the CTB-6 corpus, a significant improvement from both the supervised baseline and a previous proposed approach, namely</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
</authors>
<title>Frustratingly easy domain adaptation.</title>
<date>2007</date>
<booktitle>In Annual meetingassociation for computational linguistics,</booktitle>
<pages>256--263</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Daum´e, 2007</marker>
<rawString>Hal Daum´e III. 2007. Frustratingly easy domain adaptation. In Annual meetingassociation for computational linguistics, pages 256–263. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenbin Jiang</author>
<author>Meng Sun</author>
<author>Yajuan Lv</author>
<author>Yating Yang</author>
<author>Qun Liu</author>
</authors>
<title>Discriminative learning with natural annotations: Word segmentation as a case study.</title>
<date>2013</date>
<booktitle>In Proceedings of The 51st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>761--769</pages>
<contexts>
<context position="4130" citStr="Jiang et al. (2013)" startWordPosition="656" endWordPosition="659">ile the character after a comma can only be labelled as ‘S’ or ‘B’. Furthermore, entity tags (HTML tags or Wikipedia tags) on the web, such as emphasis and cross reference, also provide rich information for word segmentation: they might define a word or at least 90 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 90–98, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics Figure 1: Sausage constraint (partial labels) from natural annotations and punctuation give word boundary information similar to punctuation. Jiang et al. (2013) refer to such structural information on the web as natural annotations, and propose that they encode knowledge for NLP. For Chinese word segmentation, natural annotations and punctuation create a sausages constraint for the possible labels, as illustrated in Figure 1. In the sentence “近年来,人工智能和机器学习迅 猛发展。”, the first character 近 can only be labelled with ‘S’ or ‘B’; and the characters 来 before the comma and 展 before the Chinese period can only be labelled as ‘S’ or ‘E’. “人工智能” and “机 器学习” are two Wikipedia entities, and so they define the word boundaries before the first character and after th</context>
<context position="14110" citStr="Jiang et al., 2013" startWordPosition="2354" endWordPosition="2357"> could be generated. • Idiom. We use the idiom list from (Sun and Xu, 2011). If the current character c0 and its surrounding context compose an idiom, we generate a feature for c0 of its position in the idiom. For example, if c−1c0c1c2 is an idiom, we generate feature “Idiom-2” for c0. The above features together with label bigrams are fed to wapiti for training. The supervised baseline model is created with the CTB-6 corpus without the use of Wikipedia data. 3.3 Partial-label learning The overall process of applying partial-label learning to Wikipedia data is shown in Algorithm 1. Following (Jiang et al., 2013), we first train the supervised baseline model, and use it to estimate the potential contribution for each sentence in the Wikipedia training data. We label the sentence with the baseline model, and then compare the labels with the constrained sausage. For each character, a consistent label is defined as an element in the constrained labels. For example, if the constrained labels for a character are “SB”, the label ‘S’ or ‘B’ is consistent but ‘I’ or ‘E’ is not. The number of inconsistent labels for each sentence is then used as its potential contribution to the partial-label learning: higher </context>
<context position="16068" citStr="Jiang et al. (2013)" startWordPosition="2683" endWordPosition="2686"> characters that have inconsistent labels; and label reduction is the percentage of the labels reduced in the full lattice. We modify wapiti to implement the partial-label learning as described in Section 2. Same as baseline, L2 regularization is adopted. Algorithm 1 Partial-label learning 1. Train supervised baseline model Mo 2. For each sentence x in Wiki-Train: 3. y ← Decode(x, Mo) 4. diff ← Inconsistent(y, Y(x, y)) 5. if diff &gt; 0: 6. C ← C ∪ ( Y(x, y), diff) 7. Sort(C, diff, reverse) 8. Train model Mpl with CTB-6 and top K sentences in C using partial-label learning 3.4 Constrained decode Jiang et al. (2013) implement the constrained decode algorithm with perceptron. However, CRF is generally believed to out-perform perceptron, yet the comparison of CRF vs perceptron is out 4Knowledge is sparsely distributed in the Wikipedia data. Using the Wikipedia data without the CTB-6 data for partiallabel learning does not necessarily guarantee convergence. Also the CTB-6 training data helps to learn that certain label transitions, such as “B B” or “E E”, are not legal. of the scope of this paper. Thus for fair comparison, we re-implement the constrained decode algorithm with CRF. Algorithm 2 shows the cons</context>
<context position="23804" citStr="Jiang et al., 2013" startWordPosition="3967" endWordPosition="3970">Mpl (7) 5.1.2 EasyAdapt EasyAdapt is a straightforward technique but has been shown effective in many domain adaptation tasks (Daum´e III, 2007). We train the model Mpl ea with feature augmentation. For each out-ofdomain training instance &lt; xo, yo &gt;, where xo is the input features and yo is the (partial) labels, we copy the features and file them as an additional feature set, and so the training instance becomes &lt; xo, xo, yo &gt;. The in-domain training data remains the same. Consistent with (Daum´e III, 2007), EasyAdapt gives us the best performance, as show in Figure 5. Furthermore, unlike in (Jiang et al., 2013) where they find a plateau, our results show no harm adding more training data for partial-label learning when integrated with domain adaptation, although the performance seems to saturate after 400k sentences. Finally, we search for the parameter setting of best performance on the CTB-6 development set, which is to use EasyAdapt with K = 600k sentences of Wikipedia data. With this setting, the performance on the CTB-6 test set is 95.98% in F-measure. This is 0.72% absolute improvement over supervised baseline (corresponding to 15.19% relative error reduction), and 0.33% absolute improvement o</context>
</contexts>
<marker>Jiang, Sun, Lv, Yang, Liu, 2013</marker>
<rawString>Wenbin Jiang, Meng Sun, Yajuan Lv, Yating Yang, and Qun Liu. 2013. Discriminative learning with natural annotations: Word segmentation as a case study. In Proceedings of The 51st Annual Meeting of the Association for Computational Linguistics, pages 761–769.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Feng Jiao</author>
<author>Shaojun Wang</author>
<author>Chi-Hoon Lee</author>
<author>Russell Greiner</author>
<author>Dale Schuurmans</author>
</authors>
<title>Semisupervised conditional random fields for improved sequence segmentation and labeling.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics, ACL-44,</booktitle>
<pages>209--216</pages>
<contexts>
<context position="2375" citStr="Jiao et al. (2006)" startWordPosition="375" endWordPosition="378">usually expensive and tends to be of a limited amount. Researchers are thus interested in semi-supervised learning, which is to make use of unlabelled data to further improve the performance of supervised learning. There is a large amount of unlabelled data available, for example, the Gigaword corpus in the LDC catalog or the Chinese Wikipedia on the web. Faced with the large amount of unlabelled data, an intuitive idea is to use self-training or EM, by first training a baseline model (from the supervised data) and then iteratively decoding the unlabelled data and updating the baseline model. Jiao et al. (2006) and Mann and McCallum (2007) further propose to minimize the entropy of the predicted label distribution on unlabeled data and use it as a regularization term in CRF (i.e. entropy regularization). Beyond these ideas, Liang (2005) and Sun and Xu (2011) experiment with deriving a large set of statistical features such as mutual information and accessor variety from unlabelled data, and add them to supervised discriminative training. Zeng et al. (2013b) experiment with graph propagation to extract information from unlabelled data to regularize the CRF training. Yang and Vozila (2013), Zhang et a</context>
</contexts>
<marker>Jiao, Wang, Lee, Greiner, Schuurmans, 2006</marker>
<rawString>Feng Jiao, Shaojun Wang, Chi-Hoon Lee, Russell Greiner, and Dale Schuurmans. 2006. Semisupervised conditional random fields for improved sequence segmentation and labeling. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics, ACL-44, pages 209–216.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John D Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando C N Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings of the Eighteenth International Conference on Machine Learning,</booktitle>
<pages>282--289</pages>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="1612" citStr="Lafferty et al., 2001" startWordPosition="247" endWordPosition="251">pproach for supervised Chinese word segmentation is to formulate it as a character sequence labeling problem, to label each character with its location in a word. For example, Xue (2003) proposes a four-label scheme based on some linguistic intuitions: ‘B’ for the beginning character of a word, ‘I’ for the internal characters, ‘E’ for the ending character, and ‘S’ for singlecharacter word. Thus the word sequence “洽谈会 很 成功” can be turned into a character sequence with labels as 洽\B 谈\I 会\E 很\S 成\B 功\E. A machine learning algorithm for sequence labeling, such as conditional random fields (CRF) (Lafferty et al., 2001), can be applied to the labelled training data to learn a model. Labelled data for supervised learning of Chinese word segmentation, however, is usually expensive and tends to be of a limited amount. Researchers are thus interested in semi-supervised learning, which is to make use of unlabelled data to further improve the performance of supervised learning. There is a large amount of unlabelled data available, for example, the Gigaword corpus in the LDC catalog or the Chinese Wikipedia on the web. Faced with the large amount of unlabelled data, an intuitive idea is to use self-training or EM, </context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John D. Lafferty, Andrew McCallum, and Fernando C. N. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of the Eighteenth International Conference on Machine Learning, pages 282–289, San Francisco, CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Lavergne</author>
<author>Olivier Capp´e</author>
<author>Franc¸ois Yvon</author>
</authors>
<title>Practical very large scale CRFs.</title>
<date>2010</date>
<booktitle>In Proceedings the 48th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>504--513</pages>
<marker>Lavergne, Capp´e, Yvon, 2010</marker>
<rawString>Thomas Lavergne, Olivier Capp´e, and Franc¸ois Yvon. 2010. Practical very large scale CRFs. In Proceedings the 48th Annual Meeting of the Association for Computational Linguistics (ACL), pages 504–513. Association for Computational Linguistics, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhongguo Li</author>
<author>Maosong Sun</author>
</authors>
<title>Punctuation as implicit annotations for Chinese word segmentation.</title>
<date>2009</date>
<journal>Computational Linguistics,</journal>
<pages>35--505</pages>
<contexts>
<context position="3435" citStr="Li and Sun, 2009" startWordPosition="544" endWordPosition="547"> al. (2013b) experiment with graph propagation to extract information from unlabelled data to regularize the CRF training. Yang and Vozila (2013), Zhang et al. (2013), and Zeng et al. (2013a) experiment with co-training for semi-supervised Chinese word segmentation. All these approaches only leverage the distribution of the unlabelled data, yet do not make use of the knowledge that the unlabelled data might have integrated in. There could be valuable information encoded within the unlabelled data that researchers can take advantage of. For example, punctuation creates natural word boundaries (Li and Sun, 2009): the character before a comma can only be labelled as either ‘S’ or ‘E’, while the character after a comma can only be labelled as ‘S’ or ‘B’. Furthermore, entity tags (HTML tags or Wikipedia tags) on the web, such as emphasis and cross reference, also provide rich information for word segmentation: they might define a word or at least 90 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 90–98, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics Figure 1: Sausage constraint (partial labels) from natural annotat</context>
</contexts>
<marker>Li, Sun, 2009</marker>
<rawString>Zhongguo Li and Maosong Sun. 2009. Punctuation as implicit annotations for Chinese word segmentation. Computational Linguistics, 35:505–512.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
</authors>
<title>Semi-supervised learning for natural language. Master’s thesis,</title>
<date>2005</date>
<publisher>INSTITUTE OF TECHNOLOGY,</publisher>
<location>MASSACHUSETTS</location>
<contexts>
<context position="2605" citStr="Liang (2005)" startWordPosition="414" endWordPosition="415">nt of unlabelled data available, for example, the Gigaword corpus in the LDC catalog or the Chinese Wikipedia on the web. Faced with the large amount of unlabelled data, an intuitive idea is to use self-training or EM, by first training a baseline model (from the supervised data) and then iteratively decoding the unlabelled data and updating the baseline model. Jiao et al. (2006) and Mann and McCallum (2007) further propose to minimize the entropy of the predicted label distribution on unlabeled data and use it as a regularization term in CRF (i.e. entropy regularization). Beyond these ideas, Liang (2005) and Sun and Xu (2011) experiment with deriving a large set of statistical features such as mutual information and accessor variety from unlabelled data, and add them to supervised discriminative training. Zeng et al. (2013b) experiment with graph propagation to extract information from unlabelled data to regularize the CRF training. Yang and Vozila (2013), Zhang et al. (2013), and Zeng et al. (2013a) experiment with co-training for semi-supervised Chinese word segmentation. All these approaches only leverage the distribution of the unlabelled data, yet do not make use of the knowledge that th</context>
</contexts>
<marker>Liang, 2005</marker>
<rawString>Percy Liang. 2005. Semi-supervised learning for natural language. Master’s thesis, MASSACHUSETTS INSTITUTE OF TECHNOLOGY, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D C Liu</author>
<author>J Nocedal</author>
</authors>
<title>On the limited memory bfgs method for large scale optimization.</title>
<date>1989</date>
<booktitle>Mathematical Programming,</booktitle>
<volume>45</volume>
<issue>3</issue>
<contexts>
<context position="9643" citStr="Liu and Nocedal, 1989" startWordPosition="1573" endWordPosition="1576">unction 4bj, and the second term E[4bj] is the model expectation. Typically a forward-backward process is adopted for calculating the latter. n i=1 In partial-label training, each item in the sequence receives multiple labels, and so for each sequence we have a sausage constraint, denoted as Y(x, y). The marginal probability of the sausage is defined as Equation 4. pθ( Y� (x, �y)|x) = ∑ pθ(y|x) (4) yEYˆ(x,˜y) The optimization target thus is to maximize the probability mass of the sausage, as shown in Equation 5. L(θ) = ∑n logpθ( Y�(xi, 9i)|xi) (5) i=1 A gradient-based approach such as L-BFGS (Liu and Nocedal, 1989) can be employed to optimize Equation 5. The gradient is calculated as Equation 6, where EˆY (x ˜y) [4bj] is the empirical expectation of feature function 4bj constrained by the sausage, and E[4bj] is the same model expectation as in standard CRF. EˆY (x,˜y)[4bj] can be calculated via a forward-backward process in the constrained sausage. ∂θj ∂ L(θ) = EˆY (x,˜y)[`Fj] − E[`bj] (6) For fully-labelled sentences, EˆY (x,˜y)[bj] = n∑n 1 4bj, and so the standard CRF is actually a special case of the partial-label learning. 3 Experiment setup In this section we describe the basic setup for our experi</context>
</contexts>
<marker>Liu, Nocedal, 1989</marker>
<rawString>D. C. Liu and J. Nocedal. 1989. On the limited memory bfgs method for large scale optimization. Mathematical Programming, 45(3):503–528, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jin Kiat Low</author>
<author>Hwee Tou Ng</author>
<author>Wenyuan Guo</author>
</authors>
<title>A maximum entropy approach to chinese word segmentation.</title>
<date>2005</date>
<booktitle>In Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing,</booktitle>
<pages>161--164</pages>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="11957" citStr="Low et al. (2005)" startWordPosition="1969" endWordPosition="1972">hasis, and punctuation. In our pilot study we found that it’s beneficial to force a cross-reference or emphasis entity as a word when the item has 2 or 3 characters. That is, if an entity in the Wikipedia has three characters it receives the labels of “BIE”; and if it has two characters it is labelled as “BE”.3 3.2 Supervised baseline model We create the baseline supervised model by using an order-1 linear CRF with L2 regularization, to label a character sequence with the four candidate labels “BIES”. We use the tool wapiti (Lavergne et al., 2010). Following Sun et al. (2009), Sun (2010), and Low et al. (2005), we extract two types of features: character-level features and word-level features. Given a character c0 in the character sequence ...c−2c−1c0c1c2...: Character-level features : • Character unigrams: c−2, c−1, c0, c1, c2 • Character bigrams: c−2c−1, c−1c−0, c0c1, c1c2 • Consecutive character equivalence: ?c−2 = c−1, ?c−1 = c−0 , ?c0 = c1, ?c1 = c2 • Separated character equivalence: ?c−3 = c−1, ?c−2 = c0, ?c−1 = c1, ?c0 = c2, ?c1 = c3 • Whether the current character is a punctuation: ?Punct(c0) • Character sequence pattern: T (C−2)T (C−1)T (C0)T (C1)T (C2). We classify all characters into fou</context>
</contexts>
<marker>Low, Ng, Guo, 2005</marker>
<rawString>Jin Kiat Low, Hwee Tou Ng, and Wenyuan Guo. 2005. A maximum entropy approach to chinese word segmentation. In Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing, pages 161–164, San Francisco, CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gideon S Mann</author>
<author>Andrew McCallum</author>
</authors>
<title>Efficient computation of entropy gradient for semisupervised conditional random fields.</title>
<date>2007</date>
<booktitle>In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Companion Volume, Short Papers, NAACL-Short ’07,</booktitle>
<pages>109--112</pages>
<contexts>
<context position="2404" citStr="Mann and McCallum (2007)" startWordPosition="380" endWordPosition="383">ends to be of a limited amount. Researchers are thus interested in semi-supervised learning, which is to make use of unlabelled data to further improve the performance of supervised learning. There is a large amount of unlabelled data available, for example, the Gigaword corpus in the LDC catalog or the Chinese Wikipedia on the web. Faced with the large amount of unlabelled data, an intuitive idea is to use self-training or EM, by first training a baseline model (from the supervised data) and then iteratively decoding the unlabelled data and updating the baseline model. Jiao et al. (2006) and Mann and McCallum (2007) further propose to minimize the entropy of the predicted label distribution on unlabeled data and use it as a regularization term in CRF (i.e. entropy regularization). Beyond these ideas, Liang (2005) and Sun and Xu (2011) experiment with deriving a large set of statistical features such as mutual information and accessor variety from unlabelled data, and add them to supervised discriminative training. Zeng et al. (2013b) experiment with graph propagation to extract information from unlabelled data to regularize the CRF training. Yang and Vozila (2013), Zhang et al. (2013), and Zeng et al. (2</context>
</contexts>
<marker>Mann, McCallum, 2007</marker>
<rawString>Gideon S. Mann and Andrew McCallum. 2007. Efficient computation of entropy gradient for semisupervised conditional random fields. In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Companion Volume, Short Papers, NAACL-Short ’07, pages 109–112.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Weiwei Sun</author>
<author>Jia Xu</author>
</authors>
<title>Enhancing chinese word segmentation using unlabeled data.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>970--979</pages>
<location>Edinburgh, Scotland, UK.,</location>
<contexts>
<context position="2627" citStr="Sun and Xu (2011)" startWordPosition="417" endWordPosition="420">data available, for example, the Gigaword corpus in the LDC catalog or the Chinese Wikipedia on the web. Faced with the large amount of unlabelled data, an intuitive idea is to use self-training or EM, by first training a baseline model (from the supervised data) and then iteratively decoding the unlabelled data and updating the baseline model. Jiao et al. (2006) and Mann and McCallum (2007) further propose to minimize the entropy of the predicted label distribution on unlabeled data and use it as a regularization term in CRF (i.e. entropy regularization). Beyond these ideas, Liang (2005) and Sun and Xu (2011) experiment with deriving a large set of statistical features such as mutual information and accessor variety from unlabelled data, and add them to supervised discriminative training. Zeng et al. (2013b) experiment with graph propagation to extract information from unlabelled data to regularize the CRF training. Yang and Vozila (2013), Zhang et al. (2013), and Zeng et al. (2013a) experiment with co-training for semi-supervised Chinese word segmentation. All these approaches only leverage the distribution of the unlabelled data, yet do not make use of the knowledge that the unlabelled data migh</context>
<context position="8102" citStr="Sun and Xu, 2011" startWordPosition="1308" endWordPosition="1311">s, we also experiment with two domain adaptation techniques: model interpolation and EasyAdapt (Daum´e III, 2007). Our result reaches an F-measure of 95.98%, an absolute improvement of 0.72% over the very strong base91 line (corresponding to 15.19% relative error reduction), and 0.33% over the constrained decode approach (corresponding to 7.59% relative error reduction). We conduct a detailed error analysis, illustrating how partial-label learning excels constrained decode in learning the knowledge encoded in the Wikipedia data. As a note, our result also out-performs (Wang et al., 2011) and (Sun and Xu, 2011). 2 Partial-Label Learning with CRF In this section, we review in more detail the partial-label learning algorithm with CRF proposed by (T¨ackstr¨om et al., 2013). CRF is an exponential model that expresses the conditional probability of the labels given a sequence, as Equation 1, where y denotes the labels, x denotes the sequence, 4b(x, y) denotes the feature functions, and θ is the parameter vector. Z(x) = ∑y exp(θT4b(x, y)) is the normalization term. exp(θT�(x, y)) pθ(y|x) = (1) Z(x) In full-label training, where each item in the sequence is labelled with exactly one tag, maximum likelihood</context>
<context position="13566" citStr="Sun and Xu, 2011" startWordPosition="2261" endWordPosition="2264">se. Word-level features : • The identity of the string c[s : i] (i−6 &lt; s &lt; i), if it matches a word from the list of word unigrams; multiple features could be generated. • The identity of the string c[i : e] (i &lt; e &lt; i+6), if it matches a word; multiple features could be generated. • The identity of the bi-gram c[s : i − 1]c[i : e] (i − 6 &lt; s, e &lt; i + 6), if it matches a word bigram; multiple features could be generated. • The identity of the bi-gram c[s : i]c[i + 1 : e] (i−6 &lt; s, e &lt; i+6), if it matches a word bigram; multiple features could be generated. • Idiom. We use the idiom list from (Sun and Xu, 2011). If the current character c0 and its surrounding context compose an idiom, we generate a feature for c0 of its position in the idiom. For example, if c−1c0c1c2 is an idiom, we generate feature “Idiom-2” for c0. The above features together with label bigrams are fed to wapiti for training. The supervised baseline model is created with the CTB-6 corpus without the use of Wikipedia data. 3.3 Partial-label learning The overall process of applying partial-label learning to Wikipedia data is shown in Algorithm 1. Following (Jiang et al., 2013), we first train the supervised baseline model, and use </context>
<context position="24597" citStr="Sun and Xu, 2011" startWordPosition="4090" endWordPosition="4093"> to saturate after 400k sentences. Finally, we search for the parameter setting of best performance on the CTB-6 development set, which is to use EasyAdapt with K = 600k sentences of Wikipedia data. With this setting, the performance on the CTB-6 test set is 95.98% in F-measure. This is 0.72% absolute improvement over supervised baseline (corresponding to 15.19% relative error reduction), and 0.33% absolute improvement over constrained decode (corresponding to 7.59% relative error reduction); the differences are both statistically significant (p &lt; 0.001).5 As a note, this result out-performs (Sun and Xu, 2011) (95.44%) and (Wang et al., 2011) (95.79%), and the differences are also statistically significant (p &lt; 0.001). 5.2 Analysis with examples To better understand why partial-label learning is more effective in learning the encoded knowledge, we look at cases where M0 and Mcd have the incorrect segmentation while Mpl (and its domain adaptation variance Mpl � and Mpl ea) have the correct segmentation. We find that the majority is due to the error in re-decoded labels outside of encoded knowledge. For example, M0 and Mcd give the segmentation “地震/为/里/氏/6.9/ 级”, yet the correct segmentation given by</context>
<context position="28453" citStr="Sun and Xu, 2011" startWordPosition="4735" endWordPosition="4738">word boundaries. In this paper we show the effectiveness of partial-label learning in digesting the encoded knowledge from Wikipedia data for the task of Chinese word segmentation. Unlike approaches such as constrained decode that use the errorful re-decoded labels, partial-label learning provides a direct means to learn the encoded knowledge. By integrating some domain adaptation techniques such as EasyAdapt, we achieve an F-measure of 95.98% in the CTB-6 corpus, a significant improvement from both the supervised baseline and constrained decode. Our results also beat (Wang et al., 2011) and (Sun and Xu, 2011). In this research we employ a sausage constraint to encode the knowledge for Chinese word segmentation. However, a sausage constraint does not reflect the legal label sequence. For example, in Figure 1 the links between label ‘B’ and label ‘S’, between ‘S’ and ‘E’, and between ‘E’ and ‘I’ are illegal, and can confuse the machine learning. In our current work we solve this issue by adding some fully-labelled data into training. Instead we can easily extend our work to use a lattice constraint by removing the illegal transitions from the sausage. The partial-label learning stands the same, by e</context>
</contexts>
<marker>Sun, Xu, 2011</marker>
<rawString>Weiwei Sun and Jia Xu. 2011. Enhancing chinese word segmentation using unlabeled data. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 970– 979, Edinburgh, Scotland, UK., July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xu Sun</author>
<author>Yaozhong Zhang</author>
<author>Takuya Matsuzaki</author>
<author>Yoshimasa Tsuruoka</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>A discriminative latent variable Chinese segmenter with hybrid word/character information.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>56--64</pages>
<contexts>
<context position="11922" citStr="Sun et al. (2009)" startWordPosition="1962" endWordPosition="1965">formation from cross-reference, emphasis, and punctuation. In our pilot study we found that it’s beneficial to force a cross-reference or emphasis entity as a word when the item has 2 or 3 characters. That is, if an entity in the Wikipedia has three characters it receives the labels of “BIE”; and if it has two characters it is labelled as “BE”.3 3.2 Supervised baseline model We create the baseline supervised model by using an order-1 linear CRF with L2 regularization, to label a character sequence with the four candidate labels “BIES”. We use the tool wapiti (Lavergne et al., 2010). Following Sun et al. (2009), Sun (2010), and Low et al. (2005), we extract two types of features: character-level features and word-level features. Given a character c0 in the character sequence ...c−2c−1c0c1c2...: Character-level features : • Character unigrams: c−2, c−1, c0, c1, c2 • Character bigrams: c−2c−1, c−1c−0, c0c1, c1c2 • Consecutive character equivalence: ?c−2 = c−1, ?c−1 = c−0 , ?c0 = c1, ?c1 = c2 • Separated character equivalence: ?c−3 = c−1, ?c−2 = c0, ?c−1 = c1, ?c0 = c2, ?c1 = c3 • Whether the current character is a punctuation: ?Punct(c0) • Character sequence pattern: T (C−2)T (C−1)T (C0)T (C1)T (C2). </context>
</contexts>
<marker>Sun, Zhang, Matsuzaki, Tsuruoka, Tsujii, 2009</marker>
<rawString>Xu Sun, Yaozhong Zhang, Takuya Matsuzaki, Yoshimasa Tsuruoka, and Jun’ichi Tsujii. 2009. A discriminative latent variable Chinese segmenter with hybrid word/character information. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 56–64.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Weiwei Sun</author>
</authors>
<title>Word-based and character-based word segmentation models: comparison and combination.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics: Posters,</booktitle>
<pages>1211--1219</pages>
<contexts>
<context position="11934" citStr="Sun (2010)" startWordPosition="1966" endWordPosition="1967">s-reference, emphasis, and punctuation. In our pilot study we found that it’s beneficial to force a cross-reference or emphasis entity as a word when the item has 2 or 3 characters. That is, if an entity in the Wikipedia has three characters it receives the labels of “BIE”; and if it has two characters it is labelled as “BE”.3 3.2 Supervised baseline model We create the baseline supervised model by using an order-1 linear CRF with L2 regularization, to label a character sequence with the four candidate labels “BIES”. We use the tool wapiti (Lavergne et al., 2010). Following Sun et al. (2009), Sun (2010), and Low et al. (2005), we extract two types of features: character-level features and word-level features. Given a character c0 in the character sequence ...c−2c−1c0c1c2...: Character-level features : • Character unigrams: c−2, c−1, c0, c1, c2 • Character bigrams: c−2c−1, c−1c−0, c0c1, c1c2 • Consecutive character equivalence: ?c−2 = c−1, ?c−1 = c−0 , ?c0 = c1, ?c1 = c2 • Separated character equivalence: ?c−3 = c−1, ?c−2 = c0, ?c−1 = c1, ?c0 = c2, ?c1 = c3 • Whether the current character is a punctuation: ?Punct(c0) • Character sequence pattern: T (C−2)T (C−1)T (C0)T (C1)T (C2). We classify </context>
</contexts>
<marker>Sun, 2010</marker>
<rawString>Weiwei Sun. 2010. Word-based and character-based word segmentation models: comparison and combination. In Proceedings of the 23rd International Conference on Computational Linguistics: Posters, pages 1211–1219.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oscar T¨ackstr¨om</author>
<author>Dipanjan Das</author>
<author>Slav Petrov</author>
<author>Ryan McDonald</author>
<author>Joakim Nivre</author>
</authors>
<title>Token and type constraints for cross-lingual part-of-speech tagging.</title>
<date>2013</date>
<journal>Transactions of the Association for Computational Linguistics,</journal>
<pages>1--1</pages>
<marker>T¨ackstr¨om, Das, Petrov, McDonald, Nivre, 2013</marker>
<rawString>Oscar T¨ackstr¨om, Dipanjan Das, Slav Petrov, Ryan McDonald, and Joakim Nivre. 2013. Token and type constraints for cross-lingual part-of-speech tagging. Transactions of the Association for Computational Linguistics, 1:1–12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yiou Wang</author>
<author>Jun’ichi Kazama</author>
<author>Yoshimasa Tsuruoka</author>
<author>Wenliang Chen</author>
<author>Yujie Zhang</author>
<author>Kentaro Torisawa</author>
</authors>
<title>Improving chinese word segmentation and POS tagging with semi-supervised methods using large auto-analyzed data.</title>
<date>2011</date>
<booktitle>In Proceedings of the 5th International Joint Conference on Natural Language Processing,</booktitle>
<pages>309--317</pages>
<contexts>
<context position="8079" citStr="Wang et al., 2011" startWordPosition="1303" endWordPosition="1306">and has distribution bias, we also experiment with two domain adaptation techniques: model interpolation and EasyAdapt (Daum´e III, 2007). Our result reaches an F-measure of 95.98%, an absolute improvement of 0.72% over the very strong base91 line (corresponding to 15.19% relative error reduction), and 0.33% over the constrained decode approach (corresponding to 7.59% relative error reduction). We conduct a detailed error analysis, illustrating how partial-label learning excels constrained decode in learning the knowledge encoded in the Wikipedia data. As a note, our result also out-performs (Wang et al., 2011) and (Sun and Xu, 2011). 2 Partial-Label Learning with CRF In this section, we review in more detail the partial-label learning algorithm with CRF proposed by (T¨ackstr¨om et al., 2013). CRF is an exponential model that expresses the conditional probability of the labels given a sequence, as Equation 1, where y denotes the labels, x denotes the sequence, 4b(x, y) denotes the feature functions, and θ is the parameter vector. Z(x) = ∑y exp(θT4b(x, y)) is the normalization term. exp(θT�(x, y)) pθ(y|x) = (1) Z(x) In full-label training, where each item in the sequence is labelled with exactly one </context>
<context position="24630" citStr="Wang et al., 2011" startWordPosition="4096" endWordPosition="4099">s. Finally, we search for the parameter setting of best performance on the CTB-6 development set, which is to use EasyAdapt with K = 600k sentences of Wikipedia data. With this setting, the performance on the CTB-6 test set is 95.98% in F-measure. This is 0.72% absolute improvement over supervised baseline (corresponding to 15.19% relative error reduction), and 0.33% absolute improvement over constrained decode (corresponding to 7.59% relative error reduction); the differences are both statistically significant (p &lt; 0.001).5 As a note, this result out-performs (Sun and Xu, 2011) (95.44%) and (Wang et al., 2011) (95.79%), and the differences are also statistically significant (p &lt; 0.001). 5.2 Analysis with examples To better understand why partial-label learning is more effective in learning the encoded knowledge, we look at cases where M0 and Mcd have the incorrect segmentation while Mpl (and its domain adaptation variance Mpl � and Mpl ea) have the correct segmentation. We find that the majority is due to the error in re-decoded labels outside of encoded knowledge. For example, M0 and Mcd give the segmentation “地震/为/里/氏/6.9/ 级”, yet the correct segmentation given by partial-label learning is “地震/为/</context>
<context position="28430" citStr="Wang et al., 2011" startWordPosition="4730" endWordPosition="4733">entity tags define some word boundaries. In this paper we show the effectiveness of partial-label learning in digesting the encoded knowledge from Wikipedia data for the task of Chinese word segmentation. Unlike approaches such as constrained decode that use the errorful re-decoded labels, partial-label learning provides a direct means to learn the encoded knowledge. By integrating some domain adaptation techniques such as EasyAdapt, we achieve an F-measure of 95.98% in the CTB-6 corpus, a significant improvement from both the supervised baseline and constrained decode. Our results also beat (Wang et al., 2011) and (Sun and Xu, 2011). In this research we employ a sausage constraint to encode the knowledge for Chinese word segmentation. However, a sausage constraint does not reflect the legal label sequence. For example, in Figure 1 the links between label ‘B’ and label ‘S’, between ‘S’ and ‘E’, and between ‘E’ and ‘I’ are illegal, and can confuse the machine learning. In our current work we solve this issue by adding some fully-labelled data into training. Instead we can easily extend our work to use a lattice constraint by removing the illegal transitions from the sausage. The partial-label learnin</context>
</contexts>
<marker>Wang, Kazama, Tsuruoka, Chen, Zhang, Torisawa, 2011</marker>
<rawString>Yiou Wang, Jun’ichi Kazama, Yoshimasa Tsuruoka, Wenliang Chen, Yujie Zhang, and Kentaro Torisawa. 2011. Improving chinese word segmentation and POS tagging with semi-supervised methods using large auto-analyzed data. In Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 309–317.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
</authors>
<title>Chinese word segmentation as character tagging.</title>
<date>2003</date>
<booktitle>Computational Linguistics and Chinese Language Processing,</booktitle>
<pages>29--48</pages>
<contexts>
<context position="1176" citStr="Xue (2003)" startWordPosition="175" endWordPosition="176">rtial-label learning is to optimize a cost function that marginalizes the probability mass in the constrained space that encodes this knowledge. By integrating some domain adaptation techniques, such as EasyAdapt, our result reaches an F-measure of 95.98% on the CTB-6 corpus, a significant improvement from both the supervised baseline and a previous proposed approach, namely constrained decode. 1 Introduction A general approach for supervised Chinese word segmentation is to formulate it as a character sequence labeling problem, to label each character with its location in a word. For example, Xue (2003) proposes a four-label scheme based on some linguistic intuitions: ‘B’ for the beginning character of a word, ‘I’ for the internal characters, ‘E’ for the ending character, and ‘S’ for singlecharacter word. Thus the word sequence “洽谈会 很 成功” can be turned into a character sequence with labels as 洽\B 谈\I 会\E 很\S 成\B 功\E. A machine learning algorithm for sequence labeling, such as conditional random fields (CRF) (Lafferty et al., 2001), can be applied to the labelled training data to learn a model. Labelled data for supervised learning of Chinese word segmentation, however, is usually expensive a</context>
</contexts>
<marker>Xue, 2003</marker>
<rawString>Nianwen Xue. 2003. Chinese word segmentation as character tagging. Computational Linguistics and Chinese Language Processing, pages 29–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fan Yang</author>
<author>Paul Vozila</author>
</authors>
<title>An empirical study of semi-supervised Chinese word segmentation using co-training.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1191--1200</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Seattle, Washington, USA,</location>
<contexts>
<context position="2963" citStr="Yang and Vozila (2013)" startWordPosition="472" endWordPosition="475">baseline model. Jiao et al. (2006) and Mann and McCallum (2007) further propose to minimize the entropy of the predicted label distribution on unlabeled data and use it as a regularization term in CRF (i.e. entropy regularization). Beyond these ideas, Liang (2005) and Sun and Xu (2011) experiment with deriving a large set of statistical features such as mutual information and accessor variety from unlabelled data, and add them to supervised discriminative training. Zeng et al. (2013b) experiment with graph propagation to extract information from unlabelled data to regularize the CRF training. Yang and Vozila (2013), Zhang et al. (2013), and Zeng et al. (2013a) experiment with co-training for semi-supervised Chinese word segmentation. All these approaches only leverage the distribution of the unlabelled data, yet do not make use of the knowledge that the unlabelled data might have integrated in. There could be valuable information encoded within the unlabelled data that researchers can take advantage of. For example, punctuation creates natural word boundaries (Li and Sun, 2009): the character before a comma can only be labelled as either ‘S’ or ‘E’, while the character after a comma can only be labelled</context>
</contexts>
<marker>Yang, Vozila, 2013</marker>
<rawString>Fan Yang and Paul Vozila. 2013. An empirical study of semi-supervised Chinese word segmentation using co-training. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1191–1200, Seattle, Washington, USA, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaodong Zeng</author>
<author>Derek F Wong</author>
<author>Lidia S Chao</author>
<author>Isabel Trancoso</author>
</authors>
<title>Co-regularizing characterbased and word-based models for semi-supervised chinese word segmentation.</title>
<date>2013</date>
<booktitle>In Proceedings of The 51st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>171--176</pages>
<contexts>
<context position="2828" citStr="Zeng et al. (2013" startWordPosition="451" endWordPosition="454"> by first training a baseline model (from the supervised data) and then iteratively decoding the unlabelled data and updating the baseline model. Jiao et al. (2006) and Mann and McCallum (2007) further propose to minimize the entropy of the predicted label distribution on unlabeled data and use it as a regularization term in CRF (i.e. entropy regularization). Beyond these ideas, Liang (2005) and Sun and Xu (2011) experiment with deriving a large set of statistical features such as mutual information and accessor variety from unlabelled data, and add them to supervised discriminative training. Zeng et al. (2013b) experiment with graph propagation to extract information from unlabelled data to regularize the CRF training. Yang and Vozila (2013), Zhang et al. (2013), and Zeng et al. (2013a) experiment with co-training for semi-supervised Chinese word segmentation. All these approaches only leverage the distribution of the unlabelled data, yet do not make use of the knowledge that the unlabelled data might have integrated in. There could be valuable information encoded within the unlabelled data that researchers can take advantage of. For example, punctuation creates natural word boundaries (Li and Sun</context>
</contexts>
<marker>Zeng, Wong, Chao, Trancoso, 2013</marker>
<rawString>Xiaodong Zeng, Derek F. Wong, Lidia S. Chao, and Isabel Trancoso. 2013a. Co-regularizing characterbased and word-based models for semi-supervised chinese word segmentation. In Proceedings of The 51st Annual Meeting of the Association for Computational Linguistics, pages 171–176.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaodong Zeng</author>
<author>Derek F Wong</author>
<author>Lidia S Chao</author>
<author>Isabel Trancoso</author>
</authors>
<title>Graph-based semisupervised model for joint chinese word segmentation and part-of-speech tagging.</title>
<date>2013</date>
<booktitle>In Proceedings of The 51st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>770--779</pages>
<contexts>
<context position="2828" citStr="Zeng et al. (2013" startWordPosition="451" endWordPosition="454"> by first training a baseline model (from the supervised data) and then iteratively decoding the unlabelled data and updating the baseline model. Jiao et al. (2006) and Mann and McCallum (2007) further propose to minimize the entropy of the predicted label distribution on unlabeled data and use it as a regularization term in CRF (i.e. entropy regularization). Beyond these ideas, Liang (2005) and Sun and Xu (2011) experiment with deriving a large set of statistical features such as mutual information and accessor variety from unlabelled data, and add them to supervised discriminative training. Zeng et al. (2013b) experiment with graph propagation to extract information from unlabelled data to regularize the CRF training. Yang and Vozila (2013), Zhang et al. (2013), and Zeng et al. (2013a) experiment with co-training for semi-supervised Chinese word segmentation. All these approaches only leverage the distribution of the unlabelled data, yet do not make use of the knowledge that the unlabelled data might have integrated in. There could be valuable information encoded within the unlabelled data that researchers can take advantage of. For example, punctuation creates natural word boundaries (Li and Sun</context>
</contexts>
<marker>Zeng, Wong, Chao, Trancoso, 2013</marker>
<rawString>Xiaodong Zeng, Derek F. Wong, Lidia S. Chao, and Isabel Trancoso. 2013b. Graph-based semisupervised model for joint chinese word segmentation and part-of-speech tagging. In Proceedings of The 51st Annual Meeting of the Association for Computational Linguistics, pages 770–779.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Longkai Zhang</author>
<author>Houfeng Wang</author>
<author>Xu Sun</author>
<author>Mairgup Mansur</author>
</authors>
<title>Exploring representations from unlabeled data with co-training for Chinese word segmentation.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>311--321</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Seattle, Washington, USA,</location>
<contexts>
<context position="2984" citStr="Zhang et al. (2013)" startWordPosition="476" endWordPosition="479">al. (2006) and Mann and McCallum (2007) further propose to minimize the entropy of the predicted label distribution on unlabeled data and use it as a regularization term in CRF (i.e. entropy regularization). Beyond these ideas, Liang (2005) and Sun and Xu (2011) experiment with deriving a large set of statistical features such as mutual information and accessor variety from unlabelled data, and add them to supervised discriminative training. Zeng et al. (2013b) experiment with graph propagation to extract information from unlabelled data to regularize the CRF training. Yang and Vozila (2013), Zhang et al. (2013), and Zeng et al. (2013a) experiment with co-training for semi-supervised Chinese word segmentation. All these approaches only leverage the distribution of the unlabelled data, yet do not make use of the knowledge that the unlabelled data might have integrated in. There could be valuable information encoded within the unlabelled data that researchers can take advantage of. For example, punctuation creates natural word boundaries (Li and Sun, 2009): the character before a comma can only be labelled as either ‘S’ or ‘E’, while the character after a comma can only be labelled as ‘S’ or ‘B’. Furth</context>
</contexts>
<marker>Zhang, Wang, Sun, Mansur, 2013</marker>
<rawString>Longkai Zhang, Houfeng Wang, Xu Sun, and Mairgup Mansur. 2013. Exploring representations from unlabeled data with co-training for Chinese word segmentation. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 311–321, Seattle, Washington, USA, October. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
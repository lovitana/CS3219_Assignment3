<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.994116">
Sometimes Average is Best: The Importance of Averaging for Prediction
using MCMC Inference in Topic Modeling
</title>
<author confidence="0.993556">
Viet-An Nguyen
</author>
<affiliation confidence="0.9963475">
Computer Science
University of Maryland
</affiliation>
<address confidence="0.767742">
College Park, MD
</address>
<email confidence="0.997202">
vietan@cs.umd.edu
</email>
<author confidence="0.98718">
Jordan Boyd-Graber
</author>
<affiliation confidence="0.836859">
Computer Science
University of Colorado
Boulder, CO
</affiliation>
<email confidence="0.984645">
jbg@boydgraber.org
</email>
<author confidence="0.927761">
Philip Resnik
</author>
<affiliation confidence="0.928103">
Linguistics and UMIACS
University of Maryland
</affiliation>
<address confidence="0.764681">
College Park, MD
</address>
<email confidence="0.99853">
resnik@umd.edu
</email>
<sectionHeader confidence="0.997379" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9996036">
Markov chain Monte Carlo (MCMC) approxi-
mates the posterior distribution of latent vari-
able models by generating many samples and
averaging over them. In practice, however, it
is often more convenient to cut corners, using
only a single sample or following a suboptimal
averaging strategy. We systematically study dif-
ferent strategies for averaging MCMC samples
and show empirically that averaging properly
leads to significant improvements in prediction.
</bodyText>
<sectionHeader confidence="0.999643" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999769928571429">
Probabilistic topic models are powerful methods to un-
cover hidden thematic structures in text by projecting
each document into a low dimensional space spanned
by a set of topics, each of which is a distribution over
words. Topic models such as latent Dirichlet alloca-
tion (Blei et al., 2003, LDA) and its extensions discover
these topics from text, which allows for effective ex-
ploration, analysis, and summarization of the otherwise
unstructured corpora (Blei, 2012; Blei, 2014).
In addition to exploratory data analysis, a typical goal
of topic models is prediction. Given a set of unanno-
tated training data, unsupervised topic models try to
learn good topics that can generalize to unseen text.
Supervised topic models jointly capture both the text
and associated metadata such as a continuous response
variable (Blei and McAuliffe, 2007; Zhu et al., 2009;
Nguyen et al., 2013), single label (Rosen-Zvi et al.,
2004; Lacoste-Julien et al., 2008; Wang et al., 2009)
or multiple labels (Ramage et al., 2009; Ramage et al.,
2011) to predict metadata from text.
Probabilistic topic modeling requires estimating the
posterior distribution. Exact computation of the poste-
rior is often intractable, which motivates approximate
inference techniques (Asuncion et al., 2009). One popu-
lar approach is Markov chain Monte Carlo (MCMC), a
class of inference algorithms to approximate the target
posterior distribution. To make prediction, MCMC al-
gorithms generate samples on training data to estimate
corpus-level latent variables, and use them to generate
samples to estimate document-level latent variables for
test data. The underlying theory requires averaging on
both training and test samples, but in practice it is often
convenient to cut corners: either skip averaging entirely
by using just the values of the last sample or use a single
training sample and average over test samples.
We systematically study non-averaging and averaging
strategies when performing predictions using MCMC in
topic modeling (Section 2). Using popular unsupervised
(LDA in Section 3) and supervised (SLDA in Section 4)
topic models via thorough experimentation, we show
empirically that cutting corners on averaging leads to
consistently poorer prediction.
</bodyText>
<sectionHeader confidence="0.916348" genericHeader="introduction">
2 Learning and Predicting with MCMC
</sectionHeader>
<bodyText confidence="0.999659111111111">
While reviewing all of MCMC is beyond the scope of
this paper, we need to briefly review key concepts.1 To
estimate a target density p(x) in a high-dimensional
space X, MCMC generates samples {xt}Tt=1 while ex-
ploring X using the Markov assumption. Under this
assumption, sample xt+1 depends on sample xt only,
forming a Markov chain, which allows the sampler to
spend more time in the most important regions of the
density. Two concepts control sample collection:
Burn-in B: Depending on the initial value of the
Markov chain, MCMC algorithms take time to reach
the target distribution. Thus, in practice, samples before
a burn-in period B are often discarded.
Sample-lag L: Averaging over samples to estimate
the target distribution requires i.i.d. samples. However,
future samples depend on the current samples (i.e., the
Markov assumption). To avoid autocorrelation, we dis-
card all but every L samples.
</bodyText>
<subsectionHeader confidence="0.966169">
2.1 MCMC in Topic Modeling
</subsectionHeader>
<bodyText confidence="0.998781583333333">
As generative probabilistic models, topic models define
a joint distribution over latent variables and observable
evidence. In our setting, the latent variables consist of
corpus-level global variables g and document-level lo-
cal variables l; while the evidence consists of words w
and additional metadata y—the latter omitted in unsu-
pervised models.
During training, MCMC estimates the posterior
p(g, lTR  |wTR, yTR) by generating a training Markov
chain of TTR samples.2 Each training sample i pro-
vides a set of fully realized global latent variables ˆg(i),
which can generate test data. During test time, given a
</bodyText>
<footnote confidence="0.999729833333333">
1For more details please refer to Neal (1993), Andrieu et
al. (2003), Resnik and Hardisty (2010).
2We omit hyperparameters for clarity. We split data into
training (TR) and testing (TE) folds, and denote the training
iteration i and the testing iteration j within the corresponding
Markov chains.
</footnote>
<page confidence="0.697154">
1752
</page>
<note confidence="0.410288">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1752–1757,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</note>
<page confidence="0.76475">
2
</page>
<figure confidence="0.988220027027027">
3
4
1
Samples used in Single Final (SF)
Samples used in Single Average (SA)
Samples used in Multiple Final (MF)
Samples used in Multiple Average (MA)
Training chain
Discarded samples during training
Selected samples during training
Selected samples during test
Discarded samples during test
e p od odT,
g BTR g l gLTR g l gLTR
e 1 lg gLTE
e BTE
3 4
4
4
sample i in
training chain
(learned model)
g pe od TTR
3 4
4
4
sample j in
test chain i
(prediction S(i,j))
test chain i
3 4
1 2
2
4
2
4
Single test chains
</figure>
<figureCaption confidence="0.876287">
Figure 1: Illustration of training and test chains in MCMC, showing samples used in four prediction strategies studied
in this paper: Single Final (SF), Single Average (SA), Multiple Final (MF), and Multiple Average (MA).
</figureCaption>
<bodyText confidence="0.979098142857143">
learned model from training sample i, we generate a test
Markov chain of TTE samples to estimate the local latent
variables p(lTE  |wTE, ˆg(i)) of test data. Each sample
j of test chain i provides a fully estimated local latent
variables ˆlTE(i, j) to make a prediction.
Figure 1 shows an overview. To reduce the ef-
fects of unconverged and autocorrelated samples, dur-
ing training we use a burn-in period of BTR and a
sample-lag of LTR iterations. We use TTR = {i  |i ∈
(BTR, TTR] ∧ (i − BTR) mod LTR = 0} to denote the
set of indices of the selected models. Similarly, BTE
and LTE are the test burn-in and sample-lag. The
set of indices of selected samples in test chains is
TTE = {j  |j ∈ (BTE, TTE] ∧ (j − BTE) mod LTE = 0}.
</bodyText>
<subsectionHeader confidence="0.999896">
2.2 Averaging Strategies
</subsectionHeader>
<bodyText confidence="0.999530333333333">
We use S(i, j) to denote the prediction obtained from
sample j of the test chain i. We now discuss different
strategies to obtain the final prediction:
</bodyText>
<listItem confidence="0.9866756">
• Single Final (SF) uses the last sample of last test
chain to obtain the predicted value,
SSF = S(TTR, TTE). (1)
• Single Average (SA) averages over multiple sam-
ples in the last test chain
</listItem>
<equation confidence="0.912561">
1
SSA = |TTE |E S(TTR, j) (2)
</equation>
<bodyText confidence="0.992713428571429">
This is a common averaging strategy in which we
obtain a point estimate of the global latent variables
at the end of the training chain. Then, a single test
chain is generated on the test data and multiple sam-
ples of this test chain are averaged to obtain the final
prediction (Chang, 2012; Singh et al., 2012; Jiang et
al., 2012; Zhu et al., 2014).
</bodyText>
<listItem confidence="0.99905">
• Multiple Final (MF) averages over the last sam-
ples of multiple test chains from multiple models
</listItem>
<equation confidence="0.9118425">
SMF = 1|TTR |E S(i, TTE). (3)
iETTR
</equation>
<listItem confidence="0.9968775">
• Multiple Average (MA) averages over all samples
of multiple test chains for distinct models,
</listItem>
<equation confidence="0.9445755">
1 E E S(i, j), (4)
|TTE |iETTR jETTE
</equation>
<sectionHeader confidence="0.99057" genericHeader="method">
3 Unsupervised Topic Models
</sectionHeader>
<bodyText confidence="0.999689428571429">
We evaluate the predictive performance of the unsu-
pervised topic model LDA using different averaging
strategies in Section 2.
LDA: Proposed by Blei et al. in 2003, LDA posits that
each document d is a multinomial distribution θd over
K topics, each of which is a multinomial distribution
Ok over the vocabulary. LDA’s generative process is:
</bodyText>
<listItem confidence="0.953791428571429">
1. For each topic k ∈ [1, K]
(a) Draw word distribution Ok ∼ Dir(β)
2. For each document d ∈ [1, D]
(a) Draw topic distribution θd ∼ Dir(α)
(b) For each word n ∈ [1, Nd]
i. Draw topic zd,n ∼ Mult(θd)
ii. Draw word wd,n ∼ Mult(Ozd,n)
</listItem>
<bodyText confidence="0.998783833333333">
In LDA, the global latent variables are topics {Ok}Kk=1
and the local latent variables for each document d are
topic proportions θd.
Train: During training, we use collapsed Gibbs sam-
pling to assign each token in the training data with a
topic (Steyvers and Griffiths, 2006). The probability of
</bodyText>
<equation confidence="0.941430666666667">
jETTE
1
SMA =
|TTR|
1753
assigning token n of training document d to topic k is
p(zTR
d,n = k  |zTR −d,n, wTR −d,n, wTR
d,n = v) ∝
, (5)
N−d,n
TR,k,· + V β
</equation>
<bodyText confidence="0.997564">
where NTR,d,k is the number of tokens in the training
document d assigned to topic k, and NTR,k,v is the num-
ber of times word type v assigned to topic k. Marginal
counts are denoted by ·, and −d,n denotes the count
excluding the assignment of token n in document d.
At each training iteration i, we estimate the distribu-
tion over words ˆφk(i) of topic k as
</bodyText>
<equation confidence="0.999014333333333">
NTR,k,v(i) + β
(i) = (6)
NTR,k,·(i) + V β
</equation>
<bodyText confidence="0.970538733333333">
where the counts NTR,k,v(i) and NTR,k,·(i) are taken at
training iteration i.
Test: Because we lack explicit topic annotations for
these data (c.f. Nguyen et al. (2012)), we use perplexity–
a widely-used metric to measure the predictive power
of topic models on held-old documents. To compute
perplexity, we follow the estimating θ method (Wal-
lach et al., 2009, Section 5.1) and evenly split each test
document d into wTE1 dand wTE2
d . We first run Gibbs
sampling on wTE1
d to estimate the topic proportion ˆθTE
d
of test document d. The probability of assigning topic k
to token n in wTE1
</bodyText>
<equation confidence="0.958935571428571">
d is p(zTE1
d,n = k  |zTE1
−d,n, wTE1, ˆφ(i)) ∝
N−d,n ˆφk,wTE1 (7)
TE1,d,k + α · ���(i)
N−d,n
TE1,d,· + Kα
</equation>
<bodyText confidence="0.999987666666667">
where NTE1,d,k is the number of tokens in wTE1 dassigned
to topic k. At each iteration j in test chain i, we can
estimate the topic proportion vector ˆθTE
</bodyText>
<equation confidence="0.8674036">
d (i, j) for test
document d as
ˆθTE
d,k(i,j) = NTE1,d,k(i,j) + α (8)
NTE1,d,·(i,j) + Kα
</equation>
<bodyText confidence="0.913344">
where both the counts NTE1,d,k(i, j) and NTE1,d,·(i, j)
are taken using sample j of test chain i.
Prediction: Given ˆθTE
d (i, j) and ˆφ(i) at sample j
of test chain i, we compute the predicted likeli-
hood for each unseen token wTE2
</bodyText>
<equation confidence="0.957267166666667">
d,n as S(i, j) ≡
p(wTE2
d,n  |ˆθTE
d (i,j), ˆφ(i)) = EKk=1 ˆθTE
d,k(i,j) · ˆφk,wTE2
���(i).
</equation>
<bodyText confidence="0.9832657">
Using different strategies described in Section 2,
we obtain the final predicted likelihood for each un-
seen token p(wTE2
d,n |ˆθTE
d , ˆφ) and compute the perplex-
ity as exp (−(Ed En log(p(
where NTE2 is the number of tokens in wTE2.
Setup: We use three Internet review datasets in our
experiment. For all datasets, we preprocess by tokeniz-
ing, removing stopwords, stemming, adding bigrams to
</bodyText>
<figure confidence="0.997085583333333">
Restaurant Reviews
●
●
●
600 700 800 900 1000
Movie Reviews
2100
2050
2000
1950
600 700 800 900 1000
Hotel Reviews
</figure>
<figureCaption confidence="0.834035">
Figure 2: Perplexity of LDA using different averaging
strategies with different number of training iterations
TTR. Perplexity generally decreases with additional
training iterations, but the drop is more pronounced
with multiple test chains.
</figureCaption>
<bodyText confidence="0.6961655">
the vocabulary, and we filter using TF-IDF to obtain a
vocabulary of 10,000 words.3 The three datasets are:
</bodyText>
<listItem confidence="0.994512833333333">
• HOTEL: 240,060 reviews of hotels from TripAdvi-
sor (Wang et al., 2010).
• RESTAURANT: 25,459 reviews of restaurants from
Yelp (Jo and Oh, 2011).
• MOVIE: 5,006 reviews of movies from Rotten
Tomatoes (Pang and Lee, 2005)
</listItem>
<bodyText confidence="0.999887529411765">
We report cross-validated average performance over
five folds, and use K = 50 topics for all datasets. To
update the hyperparameters, we use slice sampling (Wal-
lach, 2008, p. 62).4
Results: Figure 2 shows the perplexity of the four
averaging methods, computed with different number
of training iterations TTR. SA outperforms SF, showing
the benefits of averaging over multiple test samples
from a single test chain. However, both multiple chain
methods (MF and MA) significantly outperform these
two methods.
This result is consistent with Asuncion et al. (2009),
who run multiple training chains but a single test chain
for each training chain and average over them. This
is more costly since training chains are usually signif-
icantly longer than test chains. In addition, multiple
training chains are sensitive to their initialization.
</bodyText>
<footnote confidence="0.7341195">
3To find bigrams, we begin with bigram candidates that
occur at least 10 times in the corpus and use a X2 test to filter
out those having a X2 value less than 5. We then treat selected
bigrams as single word types and add them to the vocabulary.
MCMC setup: TTR = 1, 000, BTR = 500, LTR = 50,
TTE = 100, BTE = 50 and LTE = 5.
</footnote>
<figure confidence="0.993033785714286">
800
775
750
●
●
●
600 700 800 900 1000
Number of training iterations
Multiple−Average ● Multiple−Final Single−Average Single−Final
N−d,n
TR,d,k + α
NTRd,,
k,v+ β
·
N−dd,n + Kα
TR ,
�
wTE2
d,n  |ˆθTE
d , ˆφ)))/NTE2
Perplexity
1240
1200
1160
2150
●
●
●
</figure>
<page confidence="0.73276">
4
1754
</page>
<figure confidence="0.999648565217392">
(a) Restaurant reviews
(b) Movie reviews
(c) Hotel reviews
MSE
MSE
MSE
0.500
13000
0.475
12000
0.450
11000
10000
0.425
9000
0.400
1000 2000 3000 4000 5000
1000 2000 3000 4000 5000
600 700 800 900 1000
pR.squared
pR.squared
pR.squared
34000
0.600
33000
0.575
32000
0.550
31000
0.525
30000
0.500
Multiple Average
Multiple Final
Single Average
Single Final
0.75
0.70
0.65
0.60
0.40
0.35
0.30
0.25
1000 2000 3000 4000 5000 1000 2000 3000 4000 5000 600 700 800 900 1000
Number of iterations Number of iterations Number of iterations
</figure>
<figureCaption confidence="0.999919">
Figure 3: Performance of SLDA using different averaging strategies computed at each training iteration.
</figureCaption>
<sectionHeader confidence="0.992801" genericHeader="method">
4 Supervised Topic Models
</sectionHeader>
<bodyText confidence="0.999412071428571">
We evaluate the performance of different prediction
methods using supervised latent Dirichlet allocation
(SLDA) (Blei and McAuliffe, 2007) for sentiment anal-
ysis: predicting review ratings given review text. Each
review text is the document wd and the metadata yd is
the associated rating.
SLDA: Going beyond LDA, SLDA captures the rela-
tionship between latent topics and metadata by mod-
eling each document’s continuous response variable
using a normal linear model, whose covariates are
the document’s empirical distribution of topics: yd —
N(ηT ¯zd, ρ) where η is the regression parameter vec-
tor and ¯zd is the empirical distribution over topics of
document d. The generative process of SLDA is:
</bodyText>
<listItem confidence="0.9485401">
1. For each topic k E [1, K]
(a) Draw word distribution φk — Dir(β)
(b) Draw parameter ηk — N(µ, σ)
2. For each document d E [1, D]
(a) Draw topic distribution θd — Dir(α)
(b) For each word n E [1, Nd]
i. Draw topic zd,n — Mult(θd)
ii. Draw word wd,n — Mult(φzd,n)
(c) Draw response yd — N(ηT ¯zd, ρ) where
zd k = Nd Endd1 I [zd,n = k]
</listItem>
<bodyText confidence="0.996218416666667">
where I [x] = 1 if x is true, and 0 otherwise.
In SLDA, in addition to the K multinomials {φk}Kk=1,
the global latent variables also contain the regression
parameter ηk for each topic k. The local latent variables
of SLDA resembles LDA’s: the topic proportion vector
θd for each document d.
Train: For posterior inference during training, follow-
ing Boyd-Graber and Resnik (2010), we use stochastic
EM, which alternates between (1) a Gibbs sampling
step to assign a topic to each token, and (2) optimizing
the regression parameters. The probability of assigning
topic k to token n in the training document d is
</bodyText>
<equation confidence="0.997608857142857">
p(zTR
d,n = k  |zTR −d,n, wTR −d,n, wTR
d,n = v) OC
N−d,n
TR,k,v + β (9)
N−d,n
TR,k,· + V β
</equation>
<bodyText confidence="0.992475363636364">
where µd,n = (EKk&apos;=1 ηk&apos;N−d,n
TR,d,k&apos; + ηk)/NTR,d is the
mean of the Gaussian generating yd if zTR
d,n = k. Here,
NTR,d,k is the number of times topic k is assigned to
tokens in the training document d; NTR,k,v is the number
of times word type v is assigned to topic k; · represents
marginal counts and −d,n indicates counts excluding the
assignment of token n in document d.
We optimize the regression parameters η using L-
BFGS (Liu and Nocedal, 1989) via the likelihood
</bodyText>
<equation confidence="0.9947166">
D // K
�(η) = −2ρ �lydR—�TZdR)2—Zσ E(ηk−µ)2 (10)
d=1 k=1
At each iteration i in the training chain, the estimated
ˆφk(i)
</equation>
<bodyText confidence="0.993866">
and a regression parameter ˆηk(i) for each topic k.
Test: Like LDA, at test time we sample the topic as-
signments for all tokens in the test data
</bodyText>
<equation confidence="0.918373666666667">
ˆφk,wTE
d,n
(11)
</equation>
<bodyText confidence="0.947804666666667">
Prediction: The predicted value S(i, j) in this case is
the estimated value of the metadata review rating
S(i, j) = ˆyTE
</bodyText>
<equation confidence="0.446669">
d (i,j) = ˆη(i)T ¯zTE
d (i,j), (12)
</equation>
<bodyText confidence="0.939889">
where the empirical topic distribution of test document d
is zTE
</bodyText>
<equation confidence="0.871711">
d,k (i, j) — NTE ,d En=1d [zd n, (i, j) = kJ .
N(yd; µd,n, ρ) � N−d,n
</equation>
<figure confidence="0.988045203703704">
TR,d,k + α
N−d,n
TR,d,· + Kα
global latent variables include the a multinomial
p(zTE N−d,n
d,n =k  |zTE−d,n, wTE) OC TE,d,k + α
N−d,n
TE,d,· + Kα
1755
(a) Restaurant reviews
MSE
(a) Restaurant reviews
MSE
(a) Restaurant reviews
MSE
0.48
0.90
0.46
0.80
0.44
0.70
0.42
0.60
0.40
50 100 150 200
40 60
80 50 100 150 200
pR−squared
pR−squared
pR−squared
0.40
0.60
0.30
0.58
0.56
0.20
0.54
0.10
0.52
0.00
MLR
SLDA−MA
SLDA−MF
SLDA−SA
SLDA−SF
SVR
0.70
0.65
0.60
0.40
0.35
0.30
50 100 150 200 40 60 80 50 100 150 200
Number of Topics Number of Topics Number of Topics
</figure>
<figureCaption confidence="0.776344333333333">
Figure 4: Performance of SLDA using different averaging strategies computed at the final training iteration TTR,
compared with two baselines MLR and SVR. Methods using multiple test chains (MF and MA) perform as well as or
better than the two baselines, whereas methods using a single test chain (SF and SA) perform significantly worse.
</figureCaption>
<bodyText confidence="0.992069545454545">
Experimental setup: We use the same data as in Sec-
tion 3. For all datasets, the metadata are the review
rating, ranging from 1 to 5 stars, which is standard-
ized using z-normalization. We use two evaluation
metrics: mean squared error (MSE) and predictive R-
squared (Blei and McAuliffe, 2007).
For comparison, we consider two baselines: (1) multi-
ple linear regression (MLR), which models the metadata
as a linear function of the features, and (2) support vec-
tor regression (Joachims, 1999, SVR). Both baselines
use the normalized frequencies of unigrams and bigrams
as features. As in the unsupervised case, we report av-
erage performance over five cross-validated folds. For
all models, we use a development set to tune their pa-
rameter(s) and use the set of parameters that gives best
results on the development data at test.5
Results: Figure 3 shows SLDA prediction results with
different averaging strategies, computed at different
training iterations.6 Consistent with the unsupervised
results in Section 3, SA outperforms SF, but both are
outperformed significantly by the two methods using
multiple test chains (MF and MA).
We also compare the performance of the four pre-
diction methods obtained at the final iteration TTR of
the training chain with the two baselines. The results in
Figure 4 show that the two baselines (MLR and SVR) out-
perform significantly the SLDA using only a single test
5For MLR we use a Gaussian prior N(0, 1/A) with A =
a · 10b where a E [1, 9] and b E [1, 4]; for SVR, we use
SVMlight (Joachims, 1999) and vary C E [1, 50], which
trades off between training error and margin; for SLDA, we fix
v = 10 and vary p E {0.1, 0.5, 1.0, 1.5, 2.01, which trades
off between the likelihood of words and response variable.
</bodyText>
<equation confidence="0.527596333333333">
6MCMC setup: TTR = 5, 000 for RESTAURANT and
MOVIE and 1, 000 for HOTEL; for all datasets BTR = 500,
LTR = 50, TTE = 100, BTE = 20 and LTE = 5.
</equation>
<bodyText confidence="0.999304666666667">
chains (SF and SA). Methods using multiple test chains
(MF and MA), on the other hand, match the baseline 7
(HOTEL) or do better (RESTAURANT and MOVIE).
</bodyText>
<sectionHeader confidence="0.998532" genericHeader="conclusions">
5 Discussion and Conclusion
</sectionHeader>
<bodyText confidence="0.999962777777778">
MCMC relies on averaging multiple samples to approxi-
mate target densities. When used for prediction, MCMC
needs to generate and average over both training sam-
ples to learn from training data and test samples to make
prediction. We have shown that simple averaging—not
more aggressive, ad hoc approximations like taking the
final sample (either training or test)—is not just a ques-
tion of theoretical aesthetics, but an important factor in
obtaining good prediction performance.
Compared with SVR and MLR baselines, SLDA using
multiple test chains (MF and MA) performs as well as
or better, while SLDA using a single test chain (SF and
SA) falters. This simple experimental setup choice can
determine whether a model improves over reasonable
baselines. In addition, better prediction with shorter
training is possible with multiple test chains. Thus, we
conclude that averaging using multiple chains produces
above-average results.
</bodyText>
<sectionHeader confidence="0.998136" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999792">
We thank Jonathan Chang, Ke Zhai and Mohit Iyyer for
helpful discussions, and thank the anonymous reviewers
for insightful comments. This research was supported
in part by NSF under grant #1211153 (Resnik) and
#1018625 (Boyd-Graber and Resnik). Any opinions,
findings, conclusions, or recommendations expressed
here are those of the authors and do not necessarily
reflect the view of the sponsor.
</bodyText>
<footnote confidence="0.9817735">
7This gap is because SLDA has not converged after 1,000
training iterations (Figure 3).
</footnote>
<page confidence="0.99293">
1756
</page>
<sectionHeader confidence="0.998292" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999610894117647">
Christophe Andrieu, Nando de Freitas, Arnaud Doucet, and
Michael I. Jordan. 2003. An introduction to MCMC for
machine learning. Machine Learning, 50(1-2):5–43.
Arthur Asuncion, Max Welling, Padhraic Smyth, and
Yee Whye Teh. 2009. On smoothing and inference for
topic models. In UAI.
David M. Blei and Jon D. McAuliffe. 2007. Supervised topic
models. In NIPS.
David M. Blei, Andrew Ng, and Michael Jordan. 2003. Latent
Dirichlet allocation. JMLR, 3.
David M. Blei. 2012. Probabilistic topic models. Commun.
ACM, 55(4):77–84, April.
David M. Blei. 2014. Build, compute, critique, repeat: Data
analysis with latent variable models. Annual Review of
Statistics and Its Application, 1(1):203–232.
Jordan Boyd-Graber and Philip Resnik. 2010. Holistic sen-
timent analysis across languages: Multilingual supervised
latent Dirichlet allocation. In EMNLP.
Jonathan Chang. 2012. lda: Collapsed Gibbs sampling meth-
ods for topic models. http://cran.r-project.
org/web/packages/lda/index.html. [Online;
accessed 02-June-2014].
Qixia Jiang, Jun Zhu, Maosong Sun, and Eric P. Xing. 2012.
Monte Carlo methods for maximum margin supervised
topic models. In NIPS.
Yohan Jo and Alice H. Oh. 2011. Aspect and sentiment
unification model for online review analysis. In WSDM.
Thorsten Joachims. 1999. Making large-scale SVM learning
practical. In Advances in Kernel Methods - Support Vector
Learning, chapter 11. Cambridge, MA.
Simon Lacoste-Julien, Fei Sha, and Michael I. Jordan. 2008.
DiscLDA: Discriminative learning for dimensionality re-
duction and classification. In NIPS.
D. Liu and J. Nocedal. 1989. On the limited memory BFGS
method for large scale optimization. Math. Prog.
Radford M. Neal. 1993. Probabilistic inference using Markov
chain Monte Carlo methods. Technical Report CRG-TR-
93-1, University of Toronto.
Viet-An Nguyen, Jordan Boyd-Graber, and Philip Resnik.
2012. SITS: A hierarchical nonparametric model using
speaker identity for topic segmentation in multiparty con-
versations. In ACL.
Viet-An Nguyen, Jordan Boyd-Graber, and Philip Resnik.
2013. Lexical and hierarchical topic regression. In Neural
Information Processing Systems.
Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting class
relationships for sentiment categorization with respect to
rating scales. In ACL.
Daniel Ramage, David Hall, Ramesh Nallapati, and Christo-
pher Manning. 2009. Labeled LDA: A supervised topic
model for credit attribution in multi-labeled corpora. In
EMNLP.
Daniel Ramage, Christopher D. Manning, and Susan Dumais.
2011. Partially labeled topic models for interpretable text
mining. In KDD, pages 457–465.
Philip Resnik and Eric Hardisty. 2010. Gibbs
sampling for the uninitiated. Technical Report
UMIACS-TR-2010-04, University of Maryland.
http://drum.lib.umd.edu//handle/1903/10058.
Michal Rosen-Zvi, Thomas L. Griffiths, Mark Steyvers, and
Padhraic Smyth. 2004. The author-topic model for authors
and documents. In UAI.
Sameer Singh, Michael Wick, and Andrew McCallum. 2012.
Monte Carlo MCMC: Efficient inference by approximate
sampling. In EMNLP, pages 1104–1113.
Mark Steyvers and Tom Griffiths. 2006. Probabilistic topic
models. In T. Landauer, D. Mcnamara, S. Dennis, and
W. Kintsch, editors, Latent Semantic Analysis: A Road to
Meaning. Laurence Erlbaum.
Hanna M. Wallach, Iain Murray, Ruslan Salakhutdinov, and
David Mimno. 2009. Evaluation methods for topic models.
In Leon Bottou and Michael Littman, editors, ICML.
Hanna M Wallach. 2008. Structured Topic Models for Lan-
guage. Ph.D. thesis, University of Cambridge.
Chong Wang, David Blei, and Li Fei-Fei. 2009. Simultaneous
image classification and annotation. In CVPR.
Hongning Wang, Yue Lu, and Chengxiang Zhai. 2010. La-
tent aspect rating analysis on review text data: A rating
regression approach. In SIGKDD, pages 783–792.
Jun Zhu, Amr Ahmed, and Eric P. Xing. 2009. MedLDA:
maximum margin supervised topic models for regression
and classification. In ICML.
Jun Zhu, Ning Chen, Hugh Perkins, and Bo Zhang. 2014.
Gibbs max-margin topic models with data augmentation.
Journal of Machine Learning Research, 15:1073–1110.
</reference>
<page confidence="0.993556">
1757
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.302167">
<title confidence="0.9996175">Sometimes Average is Best: The Importance of Averaging for in Topic Modeling</title>
<author confidence="0.848022">Viet-An</author>
<affiliation confidence="0.876986333333333">Computer University of Park,</affiliation>
<email confidence="0.999319">vietan@cs.umd.edu</email>
<author confidence="0.974569">Jordan</author>
<affiliation confidence="0.973472">Computer University of</affiliation>
<email confidence="0.892036">jbg@boydgraber.org</email>
<author confidence="0.983822">Philip</author>
<affiliation confidence="0.875973333333333">and University of Park,</affiliation>
<email confidence="0.999815">resnik@umd.edu</email>
<abstract confidence="0.997977272727273">chain Monte Carlo approximates the posterior distribution of latent variable models by generating many samples and averaging over them. In practice, however, it is often more convenient to cut corners, using only a single sample or following a suboptimal averaging strategy. We systematically study difstrategies for averaging and show empirically that averaging properly leads to significant improvements in prediction.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Christophe Andrieu</author>
<author>Nando de Freitas</author>
<author>Arnaud Doucet</author>
<author>Michael I Jordan</author>
</authors>
<title>An introduction to MCMC for machine learning.</title>
<date>2003</date>
<booktitle>Machine Learning,</booktitle>
<pages>50--1</pages>
<marker>Andrieu, de Freitas, Doucet, Jordan, 2003</marker>
<rawString>Christophe Andrieu, Nando de Freitas, Arnaud Doucet, and Michael I. Jordan. 2003. An introduction to MCMC for machine learning. Machine Learning, 50(1-2):5–43.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arthur Asuncion</author>
<author>Max Welling</author>
</authors>
<title>Padhraic Smyth, and Yee Whye Teh.</title>
<date>2009</date>
<booktitle>In UAI.</booktitle>
<marker>Asuncion, Welling, 2009</marker>
<rawString>Arthur Asuncion, Max Welling, Padhraic Smyth, and Yee Whye Teh. 2009. On smoothing and inference for topic models. In UAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Jon D McAuliffe</author>
</authors>
<title>Supervised topic models.</title>
<date>2007</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="1700" citStr="Blei and McAuliffe, 2007" startWordPosition="248" endWordPosition="251">words. Topic models such as latent Dirichlet allocation (Blei et al., 2003, LDA) and its extensions discover these topics from text, which allows for effective exploration, analysis, and summarization of the otherwise unstructured corpora (Blei, 2012; Blei, 2014). In addition to exploratory data analysis, a typical goal of topic models is prediction. Given a set of unannotated training data, unsupervised topic models try to learn good topics that can generalize to unseen text. Supervised topic models jointly capture both the text and associated metadata such as a continuous response variable (Blei and McAuliffe, 2007; Zhu et al., 2009; Nguyen et al., 2013), single label (Rosen-Zvi et al., 2004; Lacoste-Julien et al., 2008; Wang et al., 2009) or multiple labels (Ramage et al., 2009; Ramage et al., 2011) to predict metadata from text. Probabilistic topic modeling requires estimating the posterior distribution. Exact computation of the posterior is often intractable, which motivates approximate inference techniques (Asuncion et al., 2009). One popular approach is Markov chain Monte Carlo (MCMC), a class of inference algorithms to approximate the target posterior distribution. To make prediction, MCMC algorit</context>
<context position="13655" citStr="Blei and McAuliffe, 2007" startWordPosition="2313" endWordPosition="2316">000 5000 600 700 800 900 1000 pR.squared pR.squared pR.squared 34000 0.600 33000 0.575 32000 0.550 31000 0.525 30000 0.500 Multiple Average Multiple Final Single Average Single Final 0.75 0.70 0.65 0.60 0.40 0.35 0.30 0.25 1000 2000 3000 4000 5000 1000 2000 3000 4000 5000 600 700 800 900 1000 Number of iterations Number of iterations Number of iterations Figure 3: Performance of SLDA using different averaging strategies computed at each training iteration. 4 Supervised Topic Models We evaluate the performance of different prediction methods using supervised latent Dirichlet allocation (SLDA) (Blei and McAuliffe, 2007) for sentiment analysis: predicting review ratings given review text. Each review text is the document wd and the metadata yd is the associated rating. SLDA: Going beyond LDA, SLDA captures the relationship between latent topics and metadata by modeling each document’s continuous response variable using a normal linear model, whose covariates are the document’s empirical distribution of topics: yd — N(ηT ¯zd, ρ) where η is the regression parameter vector and ¯zd is the empirical distribution over topics of document d. The generative process of SLDA is: 1. For each topic k E [1, K] (a) Draw wor</context>
<context position="17482" citStr="Blei and McAuliffe, 2007" startWordPosition="3015" endWordPosition="3018">gure 4: Performance of SLDA using different averaging strategies computed at the final training iteration TTR, compared with two baselines MLR and SVR. Methods using multiple test chains (MF and MA) perform as well as or better than the two baselines, whereas methods using a single test chain (SF and SA) perform significantly worse. Experimental setup: We use the same data as in Section 3. For all datasets, the metadata are the review rating, ranging from 1 to 5 stars, which is standardized using z-normalization. We use two evaluation metrics: mean squared error (MSE) and predictive Rsquared (Blei and McAuliffe, 2007). For comparison, we consider two baselines: (1) multiple linear regression (MLR), which models the metadata as a linear function of the features, and (2) support vector regression (Joachims, 1999, SVR). Both baselines use the normalized frequencies of unigrams and bigrams as features. As in the unsupervised case, we report average performance over five cross-validated folds. For all models, we use a development set to tune their parameter(s) and use the set of parameters that gives best results on the development data at test.5 Results: Figure 3 shows SLDA prediction results with different av</context>
</contexts>
<marker>Blei, McAuliffe, 2007</marker>
<rawString>David M. Blei and Jon D. McAuliffe. 2007. Supervised topic models. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Ng</author>
<author>Michael Jordan</author>
</authors>
<title>Latent Dirichlet allocation.</title>
<date>2003</date>
<journal>JMLR,</journal>
<volume>3</volume>
<contexts>
<context position="1150" citStr="Blei et al., 2003" startWordPosition="164" endWordPosition="167"> them. In practice, however, it is often more convenient to cut corners, using only a single sample or following a suboptimal averaging strategy. We systematically study different strategies for averaging MCMC samples and show empirically that averaging properly leads to significant improvements in prediction. 1 Introduction Probabilistic topic models are powerful methods to uncover hidden thematic structures in text by projecting each document into a low dimensional space spanned by a set of topics, each of which is a distribution over words. Topic models such as latent Dirichlet allocation (Blei et al., 2003, LDA) and its extensions discover these topics from text, which allows for effective exploration, analysis, and summarization of the otherwise unstructured corpora (Blei, 2012; Blei, 2014). In addition to exploratory data analysis, a typical goal of topic models is prediction. Given a set of unannotated training data, unsupervised topic models try to learn good topics that can generalize to unseen text. Supervised topic models jointly capture both the text and associated metadata such as a continuous response variable (Blei and McAuliffe, 2007; Zhu et al., 2009; Nguyen et al., 2013), single l</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M. Blei, Andrew Ng, and Michael Jordan. 2003. Latent Dirichlet allocation. JMLR, 3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
</authors>
<title>Probabilistic topic models.</title>
<date>2012</date>
<journal>Commun. ACM,</journal>
<volume>55</volume>
<issue>4</issue>
<contexts>
<context position="1326" citStr="Blei, 2012" startWordPosition="191" endWordPosition="192">ategies for averaging MCMC samples and show empirically that averaging properly leads to significant improvements in prediction. 1 Introduction Probabilistic topic models are powerful methods to uncover hidden thematic structures in text by projecting each document into a low dimensional space spanned by a set of topics, each of which is a distribution over words. Topic models such as latent Dirichlet allocation (Blei et al., 2003, LDA) and its extensions discover these topics from text, which allows for effective exploration, analysis, and summarization of the otherwise unstructured corpora (Blei, 2012; Blei, 2014). In addition to exploratory data analysis, a typical goal of topic models is prediction. Given a set of unannotated training data, unsupervised topic models try to learn good topics that can generalize to unseen text. Supervised topic models jointly capture both the text and associated metadata such as a continuous response variable (Blei and McAuliffe, 2007; Zhu et al., 2009; Nguyen et al., 2013), single label (Rosen-Zvi et al., 2004; Lacoste-Julien et al., 2008; Wang et al., 2009) or multiple labels (Ramage et al., 2009; Ramage et al., 2011) to predict metadata from text. Proba</context>
</contexts>
<marker>Blei, 2012</marker>
<rawString>David M. Blei. 2012. Probabilistic topic models. Commun. ACM, 55(4):77–84, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
</authors>
<title>Build, compute, critique, repeat: Data analysis with latent variable models. Annual Review of Statistics and Its Application,</title>
<date>2014</date>
<contexts>
<context position="1339" citStr="Blei, 2014" startWordPosition="193" endWordPosition="194">averaging MCMC samples and show empirically that averaging properly leads to significant improvements in prediction. 1 Introduction Probabilistic topic models are powerful methods to uncover hidden thematic structures in text by projecting each document into a low dimensional space spanned by a set of topics, each of which is a distribution over words. Topic models such as latent Dirichlet allocation (Blei et al., 2003, LDA) and its extensions discover these topics from text, which allows for effective exploration, analysis, and summarization of the otherwise unstructured corpora (Blei, 2012; Blei, 2014). In addition to exploratory data analysis, a typical goal of topic models is prediction. Given a set of unannotated training data, unsupervised topic models try to learn good topics that can generalize to unseen text. Supervised topic models jointly capture both the text and associated metadata such as a continuous response variable (Blei and McAuliffe, 2007; Zhu et al., 2009; Nguyen et al., 2013), single label (Rosen-Zvi et al., 2004; Lacoste-Julien et al., 2008; Wang et al., 2009) or multiple labels (Ramage et al., 2009; Ramage et al., 2011) to predict metadata from text. Probabilistic topi</context>
</contexts>
<marker>Blei, 2014</marker>
<rawString>David M. Blei. 2014. Build, compute, critique, repeat: Data analysis with latent variable models. Annual Review of Statistics and Its Application, 1(1):203–232.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jordan Boyd-Graber</author>
<author>Philip Resnik</author>
</authors>
<title>Holistic sentiment analysis across languages: Multilingual supervised latent Dirichlet allocation.</title>
<date>2010</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="14928" citStr="Boyd-Graber and Resnik (2010)" startWordPosition="2543" endWordPosition="2546">r ηk — N(µ, σ) 2. For each document d E [1, D] (a) Draw topic distribution θd — Dir(α) (b) For each word n E [1, Nd] i. Draw topic zd,n — Mult(θd) ii. Draw word wd,n — Mult(φzd,n) (c) Draw response yd — N(ηT ¯zd, ρ) where zd k = Nd Endd1 I [zd,n = k] where I [x] = 1 if x is true, and 0 otherwise. In SLDA, in addition to the K multinomials {φk}Kk=1, the global latent variables also contain the regression parameter ηk for each topic k. The local latent variables of SLDA resembles LDA’s: the topic proportion vector θd for each document d. Train: For posterior inference during training, following Boyd-Graber and Resnik (2010), we use stochastic EM, which alternates between (1) a Gibbs sampling step to assign a topic to each token, and (2) optimizing the regression parameters. The probability of assigning topic k to token n in the training document d is p(zTR d,n = k |zTR −d,n, wTR −d,n, wTR d,n = v) OC N−d,n TR,k,v + β (9) N−d,n TR,k,· + V β where µd,n = (EKk&apos;=1 ηk&apos;N−d,n TR,d,k&apos; + ηk)/NTR,d is the mean of the Gaussian generating yd if zTR d,n = k. Here, NTR,d,k is the number of times topic k is assigned to tokens in the training document d; NTR,k,v is the number of times word type v is assigned to topic k; · repre</context>
</contexts>
<marker>Boyd-Graber, Resnik, 2010</marker>
<rawString>Jordan Boyd-Graber and Philip Resnik. 2010. Holistic sentiment analysis across languages: Multilingual supervised latent Dirichlet allocation. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Chang</author>
</authors>
<title>lda: Collapsed Gibbs sampling methods for topic models.</title>
<date>2012</date>
<note>http://cran.r-project. org/web/packages/lda/index.html. [Online; accessed 02-June-2014].</note>
<contexts>
<context position="7318" citStr="Chang, 2012" startWordPosition="1189" endWordPosition="1190">rom sample j of the test chain i. We now discuss different strategies to obtain the final prediction: • Single Final (SF) uses the last sample of last test chain to obtain the predicted value, SSF = S(TTR, TTE). (1) • Single Average (SA) averages over multiple samples in the last test chain 1 SSA = |TTE |E S(TTR, j) (2) This is a common averaging strategy in which we obtain a point estimate of the global latent variables at the end of the training chain. Then, a single test chain is generated on the test data and multiple samples of this test chain are averaged to obtain the final prediction (Chang, 2012; Singh et al., 2012; Jiang et al., 2012; Zhu et al., 2014). • Multiple Final (MF) averages over the last samples of multiple test chains from multiple models SMF = 1|TTR |E S(i, TTE). (3) iETTR • Multiple Average (MA) averages over all samples of multiple test chains for distinct models, 1 E E S(i, j), (4) |TTE |iETTR jETTE 3 Unsupervised Topic Models We evaluate the predictive performance of the unsupervised topic model LDA using different averaging strategies in Section 2. LDA: Proposed by Blei et al. in 2003, LDA posits that each document d is a multinomial distribution θd over K topics, e</context>
</contexts>
<marker>Chang, 2012</marker>
<rawString>Jonathan Chang. 2012. lda: Collapsed Gibbs sampling methods for topic models. http://cran.r-project. org/web/packages/lda/index.html. [Online; accessed 02-June-2014].</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qixia Jiang</author>
<author>Jun Zhu</author>
<author>Maosong Sun</author>
<author>Eric P Xing</author>
</authors>
<title>Monte Carlo methods for maximum margin supervised topic models.</title>
<date>2012</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="7358" citStr="Jiang et al., 2012" startWordPosition="1195" endWordPosition="1198"> We now discuss different strategies to obtain the final prediction: • Single Final (SF) uses the last sample of last test chain to obtain the predicted value, SSF = S(TTR, TTE). (1) • Single Average (SA) averages over multiple samples in the last test chain 1 SSA = |TTE |E S(TTR, j) (2) This is a common averaging strategy in which we obtain a point estimate of the global latent variables at the end of the training chain. Then, a single test chain is generated on the test data and multiple samples of this test chain are averaged to obtain the final prediction (Chang, 2012; Singh et al., 2012; Jiang et al., 2012; Zhu et al., 2014). • Multiple Final (MF) averages over the last samples of multiple test chains from multiple models SMF = 1|TTR |E S(i, TTE). (3) iETTR • Multiple Average (MA) averages over all samples of multiple test chains for distinct models, 1 E E S(i, j), (4) |TTE |iETTR jETTE 3 Unsupervised Topic Models We evaluate the predictive performance of the unsupervised topic model LDA using different averaging strategies in Section 2. LDA: Proposed by Blei et al. in 2003, LDA posits that each document d is a multinomial distribution θd over K topics, each of which is a multinomial distributi</context>
</contexts>
<marker>Jiang, Zhu, Sun, Xing, 2012</marker>
<rawString>Qixia Jiang, Jun Zhu, Maosong Sun, and Eric P. Xing. 2012. Monte Carlo methods for maximum margin supervised topic models. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yohan Jo</author>
<author>Alice H Oh</author>
</authors>
<title>Aspect and sentiment unification model for online review analysis.</title>
<date>2011</date>
<booktitle>In WSDM.</booktitle>
<contexts>
<context position="11361" citStr="Jo and Oh, 2011" startWordPosition="1923" endWordPosition="1926">ams to Restaurant Reviews ● ● ● 600 700 800 900 1000 Movie Reviews 2100 2050 2000 1950 600 700 800 900 1000 Hotel Reviews Figure 2: Perplexity of LDA using different averaging strategies with different number of training iterations TTR. Perplexity generally decreases with additional training iterations, but the drop is more pronounced with multiple test chains. the vocabulary, and we filter using TF-IDF to obtain a vocabulary of 10,000 words.3 The three datasets are: • HOTEL: 240,060 reviews of hotels from TripAdvisor (Wang et al., 2010). • RESTAURANT: 25,459 reviews of restaurants from Yelp (Jo and Oh, 2011). • MOVIE: 5,006 reviews of movies from Rotten Tomatoes (Pang and Lee, 2005) We report cross-validated average performance over five folds, and use K = 50 topics for all datasets. To update the hyperparameters, we use slice sampling (Wallach, 2008, p. 62).4 Results: Figure 2 shows the perplexity of the four averaging methods, computed with different number of training iterations TTR. SA outperforms SF, showing the benefits of averaging over multiple test samples from a single test chain. However, both multiple chain methods (MF and MA) significantly outperform these two methods. This result is</context>
</contexts>
<marker>Jo, Oh, 2011</marker>
<rawString>Yohan Jo and Alice H. Oh. 2011. Aspect and sentiment unification model for online review analysis. In WSDM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Making large-scale SVM learning practical.</title>
<date>1999</date>
<booktitle>In Advances in Kernel Methods - Support Vector Learning, chapter 11.</booktitle>
<location>Cambridge, MA.</location>
<contexts>
<context position="17678" citStr="Joachims, 1999" startWordPosition="3048" endWordPosition="3049">as well as or better than the two baselines, whereas methods using a single test chain (SF and SA) perform significantly worse. Experimental setup: We use the same data as in Section 3. For all datasets, the metadata are the review rating, ranging from 1 to 5 stars, which is standardized using z-normalization. We use two evaluation metrics: mean squared error (MSE) and predictive Rsquared (Blei and McAuliffe, 2007). For comparison, we consider two baselines: (1) multiple linear regression (MLR), which models the metadata as a linear function of the features, and (2) support vector regression (Joachims, 1999, SVR). Both baselines use the normalized frequencies of unigrams and bigrams as features. As in the unsupervised case, we report average performance over five cross-validated folds. For all models, we use a development set to tune their parameter(s) and use the set of parameters that gives best results on the development data at test.5 Results: Figure 3 shows SLDA prediction results with different averaging strategies, computed at different training iterations.6 Consistent with the unsupervised results in Section 3, SA outperforms SF, but both are outperformed significantly by the two methods</context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>Thorsten Joachims. 1999. Making large-scale SVM learning practical. In Advances in Kernel Methods - Support Vector Learning, chapter 11. Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simon Lacoste-Julien</author>
<author>Fei Sha</author>
<author>Michael I Jordan</author>
</authors>
<title>DiscLDA: Discriminative learning for dimensionality reduction and classification.</title>
<date>2008</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="1807" citStr="Lacoste-Julien et al., 2008" startWordPosition="266" endWordPosition="269">over these topics from text, which allows for effective exploration, analysis, and summarization of the otherwise unstructured corpora (Blei, 2012; Blei, 2014). In addition to exploratory data analysis, a typical goal of topic models is prediction. Given a set of unannotated training data, unsupervised topic models try to learn good topics that can generalize to unseen text. Supervised topic models jointly capture both the text and associated metadata such as a continuous response variable (Blei and McAuliffe, 2007; Zhu et al., 2009; Nguyen et al., 2013), single label (Rosen-Zvi et al., 2004; Lacoste-Julien et al., 2008; Wang et al., 2009) or multiple labels (Ramage et al., 2009; Ramage et al., 2011) to predict metadata from text. Probabilistic topic modeling requires estimating the posterior distribution. Exact computation of the posterior is often intractable, which motivates approximate inference techniques (Asuncion et al., 2009). One popular approach is Markov chain Monte Carlo (MCMC), a class of inference algorithms to approximate the target posterior distribution. To make prediction, MCMC algorithms generate samples on training data to estimate corpus-level latent variables, and use them to generate s</context>
</contexts>
<marker>Lacoste-Julien, Sha, Jordan, 2008</marker>
<rawString>Simon Lacoste-Julien, Fei Sha, and Michael I. Jordan. 2008. DiscLDA: Discriminative learning for dimensionality reduction and classification. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Liu</author>
<author>J Nocedal</author>
</authors>
<title>On the limited memory BFGS method for large scale optimization.</title>
<date>1989</date>
<journal>Math. Prog.</journal>
<contexts>
<context position="15702" citStr="Liu and Nocedal, 1989" startWordPosition="2689" endWordPosition="2692">. The probability of assigning topic k to token n in the training document d is p(zTR d,n = k |zTR −d,n, wTR −d,n, wTR d,n = v) OC N−d,n TR,k,v + β (9) N−d,n TR,k,· + V β where µd,n = (EKk&apos;=1 ηk&apos;N−d,n TR,d,k&apos; + ηk)/NTR,d is the mean of the Gaussian generating yd if zTR d,n = k. Here, NTR,d,k is the number of times topic k is assigned to tokens in the training document d; NTR,k,v is the number of times word type v is assigned to topic k; · represents marginal counts and −d,n indicates counts excluding the assignment of token n in document d. We optimize the regression parameters η using LBFGS (Liu and Nocedal, 1989) via the likelihood D // K �(η) = −2ρ �lydR—�TZdR)2—Zσ E(ηk−µ)2 (10) d=1 k=1 At each iteration i in the training chain, the estimated ˆφk(i) and a regression parameter ˆηk(i) for each topic k. Test: Like LDA, at test time we sample the topic assignments for all tokens in the test data ˆφk,wTE d,n (11) Prediction: The predicted value S(i, j) in this case is the estimated value of the metadata review rating S(i, j) = ˆyTE d (i,j) = ˆη(i)T ¯zTE d (i,j), (12) where the empirical topic distribution of test document d is zTE d,k (i, j) — NTE ,d En=1d [zd n, (i, j) = kJ . N(yd; µd,n, ρ) � N−d,n TR,d,</context>
</contexts>
<marker>Liu, Nocedal, 1989</marker>
<rawString>D. Liu and J. Nocedal. 1989. On the limited memory BFGS method for large scale optimization. Math. Prog.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radford M Neal</author>
</authors>
<title>Probabilistic inference using Markov chain Monte Carlo methods.</title>
<date>1993</date>
<tech>Technical Report CRG-TR93-1,</tech>
<institution>University of Toronto.</institution>
<contexts>
<context position="4722" citStr="Neal (1993)" startWordPosition="717" endWordPosition="718"> define a joint distribution over latent variables and observable evidence. In our setting, the latent variables consist of corpus-level global variables g and document-level local variables l; while the evidence consists of words w and additional metadata y—the latter omitted in unsupervised models. During training, MCMC estimates the posterior p(g, lTR |wTR, yTR) by generating a training Markov chain of TTR samples.2 Each training sample i provides a set of fully realized global latent variables ˆg(i), which can generate test data. During test time, given a 1For more details please refer to Neal (1993), Andrieu et al. (2003), Resnik and Hardisty (2010). 2We omit hyperparameters for clarity. We split data into training (TR) and testing (TE) folds, and denote the training iteration i and the testing iteration j within the corresponding Markov chains. 1752 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1752–1757, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics 2 3 4 1 Samples used in Single Final (SF) Samples used in Single Average (SA) Samples used in Multiple Final (MF) Samples used in Multiple Average </context>
</contexts>
<marker>Neal, 1993</marker>
<rawString>Radford M. Neal. 1993. Probabilistic inference using Markov chain Monte Carlo methods. Technical Report CRG-TR93-1, University of Toronto.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Viet-An Nguyen</author>
<author>Jordan Boyd-Graber</author>
<author>Philip Resnik</author>
</authors>
<title>SITS: A hierarchical nonparametric model using speaker identity for topic segmentation in multiparty conversations.</title>
<date>2012</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="9264" citStr="Nguyen et al. (2012)" startWordPosition="1549" endWordPosition="1552">n, wTR −d,n, wTR d,n = v) ∝ , (5) N−d,n TR,k,· + V β where NTR,d,k is the number of tokens in the training document d assigned to topic k, and NTR,k,v is the number of times word type v assigned to topic k. Marginal counts are denoted by ·, and −d,n denotes the count excluding the assignment of token n in document d. At each training iteration i, we estimate the distribution over words ˆφk(i) of topic k as NTR,k,v(i) + β (i) = (6) NTR,k,·(i) + V β where the counts NTR,k,v(i) and NTR,k,·(i) are taken at training iteration i. Test: Because we lack explicit topic annotations for these data (c.f. Nguyen et al. (2012)), we use perplexity– a widely-used metric to measure the predictive power of topic models on held-old documents. To compute perplexity, we follow the estimating θ method (Wallach et al., 2009, Section 5.1) and evenly split each test document d into wTE1 dand wTE2 d . We first run Gibbs sampling on wTE1 d to estimate the topic proportion ˆθTE d of test document d. The probability of assigning topic k to token n in wTE1 d is p(zTE1 d,n = k |zTE1 −d,n, wTE1, ˆφ(i)) ∝ N−d,n ˆφk,wTE1 (7) TE1,d,k + α · ���(i) N−d,n TE1,d,· + Kα where NTE1,d,k is the number of tokens in wTE1 dassigned to topic k. At</context>
</contexts>
<marker>Nguyen, Boyd-Graber, Resnik, 2012</marker>
<rawString>Viet-An Nguyen, Jordan Boyd-Graber, and Philip Resnik. 2012. SITS: A hierarchical nonparametric model using speaker identity for topic segmentation in multiparty conversations. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Viet-An Nguyen</author>
<author>Jordan Boyd-Graber</author>
<author>Philip Resnik</author>
</authors>
<title>Lexical and hierarchical topic regression.</title>
<date>2013</date>
<booktitle>In Neural Information Processing Systems.</booktitle>
<contexts>
<context position="1740" citStr="Nguyen et al., 2013" startWordPosition="256" endWordPosition="259"> allocation (Blei et al., 2003, LDA) and its extensions discover these topics from text, which allows for effective exploration, analysis, and summarization of the otherwise unstructured corpora (Blei, 2012; Blei, 2014). In addition to exploratory data analysis, a typical goal of topic models is prediction. Given a set of unannotated training data, unsupervised topic models try to learn good topics that can generalize to unseen text. Supervised topic models jointly capture both the text and associated metadata such as a continuous response variable (Blei and McAuliffe, 2007; Zhu et al., 2009; Nguyen et al., 2013), single label (Rosen-Zvi et al., 2004; Lacoste-Julien et al., 2008; Wang et al., 2009) or multiple labels (Ramage et al., 2009; Ramage et al., 2011) to predict metadata from text. Probabilistic topic modeling requires estimating the posterior distribution. Exact computation of the posterior is often intractable, which motivates approximate inference techniques (Asuncion et al., 2009). One popular approach is Markov chain Monte Carlo (MCMC), a class of inference algorithms to approximate the target posterior distribution. To make prediction, MCMC algorithms generate samples on training data to</context>
</contexts>
<marker>Nguyen, Boyd-Graber, Resnik, 2013</marker>
<rawString>Viet-An Nguyen, Jordan Boyd-Graber, and Philip Resnik. 2013. Lexical and hierarchical topic regression. In Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales.</title>
<date>2005</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="11437" citStr="Pang and Lee, 2005" startWordPosition="1936" endWordPosition="1939">050 2000 1950 600 700 800 900 1000 Hotel Reviews Figure 2: Perplexity of LDA using different averaging strategies with different number of training iterations TTR. Perplexity generally decreases with additional training iterations, but the drop is more pronounced with multiple test chains. the vocabulary, and we filter using TF-IDF to obtain a vocabulary of 10,000 words.3 The three datasets are: • HOTEL: 240,060 reviews of hotels from TripAdvisor (Wang et al., 2010). • RESTAURANT: 25,459 reviews of restaurants from Yelp (Jo and Oh, 2011). • MOVIE: 5,006 reviews of movies from Rotten Tomatoes (Pang and Lee, 2005) We report cross-validated average performance over five folds, and use K = 50 topics for all datasets. To update the hyperparameters, we use slice sampling (Wallach, 2008, p. 62).4 Results: Figure 2 shows the perplexity of the four averaging methods, computed with different number of training iterations TTR. SA outperforms SF, showing the benefits of averaging over multiple test samples from a single test chain. However, both multiple chain methods (MF and MA) significantly outperform these two methods. This result is consistent with Asuncion et al. (2009), who run multiple training chains bu</context>
</contexts>
<marker>Pang, Lee, 2005</marker>
<rawString>Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Ramage</author>
<author>David Hall</author>
<author>Ramesh Nallapati</author>
<author>Christopher Manning</author>
</authors>
<title>Labeled LDA: A supervised topic model for credit attribution in multi-labeled corpora.</title>
<date>2009</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="1867" citStr="Ramage et al., 2009" startWordPosition="277" endWordPosition="280"> analysis, and summarization of the otherwise unstructured corpora (Blei, 2012; Blei, 2014). In addition to exploratory data analysis, a typical goal of topic models is prediction. Given a set of unannotated training data, unsupervised topic models try to learn good topics that can generalize to unseen text. Supervised topic models jointly capture both the text and associated metadata such as a continuous response variable (Blei and McAuliffe, 2007; Zhu et al., 2009; Nguyen et al., 2013), single label (Rosen-Zvi et al., 2004; Lacoste-Julien et al., 2008; Wang et al., 2009) or multiple labels (Ramage et al., 2009; Ramage et al., 2011) to predict metadata from text. Probabilistic topic modeling requires estimating the posterior distribution. Exact computation of the posterior is often intractable, which motivates approximate inference techniques (Asuncion et al., 2009). One popular approach is Markov chain Monte Carlo (MCMC), a class of inference algorithms to approximate the target posterior distribution. To make prediction, MCMC algorithms generate samples on training data to estimate corpus-level latent variables, and use them to generate samples to estimate document-level latent variables for test </context>
</contexts>
<marker>Ramage, Hall, Nallapati, Manning, 2009</marker>
<rawString>Daniel Ramage, David Hall, Ramesh Nallapati, and Christopher Manning. 2009. Labeled LDA: A supervised topic model for credit attribution in multi-labeled corpora. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Ramage</author>
<author>Christopher D Manning</author>
<author>Susan Dumais</author>
</authors>
<title>Partially labeled topic models for interpretable text mining.</title>
<date>2011</date>
<booktitle>In KDD,</booktitle>
<pages>457--465</pages>
<contexts>
<context position="1889" citStr="Ramage et al., 2011" startWordPosition="281" endWordPosition="284">ization of the otherwise unstructured corpora (Blei, 2012; Blei, 2014). In addition to exploratory data analysis, a typical goal of topic models is prediction. Given a set of unannotated training data, unsupervised topic models try to learn good topics that can generalize to unseen text. Supervised topic models jointly capture both the text and associated metadata such as a continuous response variable (Blei and McAuliffe, 2007; Zhu et al., 2009; Nguyen et al., 2013), single label (Rosen-Zvi et al., 2004; Lacoste-Julien et al., 2008; Wang et al., 2009) or multiple labels (Ramage et al., 2009; Ramage et al., 2011) to predict metadata from text. Probabilistic topic modeling requires estimating the posterior distribution. Exact computation of the posterior is often intractable, which motivates approximate inference techniques (Asuncion et al., 2009). One popular approach is Markov chain Monte Carlo (MCMC), a class of inference algorithms to approximate the target posterior distribution. To make prediction, MCMC algorithms generate samples on training data to estimate corpus-level latent variables, and use them to generate samples to estimate document-level latent variables for test data. The underlying t</context>
</contexts>
<marker>Ramage, Manning, Dumais, 2011</marker>
<rawString>Daniel Ramage, Christopher D. Manning, and Susan Dumais. 2011. Partially labeled topic models for interpretable text mining. In KDD, pages 457–465.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
<author>Eric Hardisty</author>
</authors>
<title>Gibbs sampling for the uninitiated.</title>
<date>2010</date>
<tech>Technical Report UMIACS-TR-2010-04,</tech>
<institution>University of Maryland.</institution>
<note>http://drum.lib.umd.edu//handle/1903/10058.</note>
<contexts>
<context position="4773" citStr="Resnik and Hardisty (2010)" startWordPosition="723" endWordPosition="726">tent variables and observable evidence. In our setting, the latent variables consist of corpus-level global variables g and document-level local variables l; while the evidence consists of words w and additional metadata y—the latter omitted in unsupervised models. During training, MCMC estimates the posterior p(g, lTR |wTR, yTR) by generating a training Markov chain of TTR samples.2 Each training sample i provides a set of fully realized global latent variables ˆg(i), which can generate test data. During test time, given a 1For more details please refer to Neal (1993), Andrieu et al. (2003), Resnik and Hardisty (2010). 2We omit hyperparameters for clarity. We split data into training (TR) and testing (TE) folds, and denote the training iteration i and the testing iteration j within the corresponding Markov chains. 1752 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1752–1757, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics 2 3 4 1 Samples used in Single Final (SF) Samples used in Single Average (SA) Samples used in Multiple Final (MF) Samples used in Multiple Average (MA) Training chain Discarded samples during traini</context>
</contexts>
<marker>Resnik, Hardisty, 2010</marker>
<rawString>Philip Resnik and Eric Hardisty. 2010. Gibbs sampling for the uninitiated. Technical Report UMIACS-TR-2010-04, University of Maryland. http://drum.lib.umd.edu//handle/1903/10058.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michal Rosen-Zvi</author>
<author>Thomas L Griffiths</author>
<author>Mark Steyvers</author>
<author>Padhraic Smyth</author>
</authors>
<title>The author-topic model for authors and documents.</title>
<date>2004</date>
<booktitle>In UAI.</booktitle>
<contexts>
<context position="1778" citStr="Rosen-Zvi et al., 2004" startWordPosition="262" endWordPosition="265"> and its extensions discover these topics from text, which allows for effective exploration, analysis, and summarization of the otherwise unstructured corpora (Blei, 2012; Blei, 2014). In addition to exploratory data analysis, a typical goal of topic models is prediction. Given a set of unannotated training data, unsupervised topic models try to learn good topics that can generalize to unseen text. Supervised topic models jointly capture both the text and associated metadata such as a continuous response variable (Blei and McAuliffe, 2007; Zhu et al., 2009; Nguyen et al., 2013), single label (Rosen-Zvi et al., 2004; Lacoste-Julien et al., 2008; Wang et al., 2009) or multiple labels (Ramage et al., 2009; Ramage et al., 2011) to predict metadata from text. Probabilistic topic modeling requires estimating the posterior distribution. Exact computation of the posterior is often intractable, which motivates approximate inference techniques (Asuncion et al., 2009). One popular approach is Markov chain Monte Carlo (MCMC), a class of inference algorithms to approximate the target posterior distribution. To make prediction, MCMC algorithms generate samples on training data to estimate corpus-level latent variable</context>
</contexts>
<marker>Rosen-Zvi, Griffiths, Steyvers, Smyth, 2004</marker>
<rawString>Michal Rosen-Zvi, Thomas L. Griffiths, Mark Steyvers, and Padhraic Smyth. 2004. The author-topic model for authors and documents. In UAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Singh</author>
<author>Michael Wick</author>
<author>Andrew McCallum</author>
</authors>
<title>Monte Carlo MCMC: Efficient inference by approximate sampling.</title>
<date>2012</date>
<booktitle>In EMNLP,</booktitle>
<pages>1104--1113</pages>
<contexts>
<context position="7338" citStr="Singh et al., 2012" startWordPosition="1191" endWordPosition="1194">of the test chain i. We now discuss different strategies to obtain the final prediction: • Single Final (SF) uses the last sample of last test chain to obtain the predicted value, SSF = S(TTR, TTE). (1) • Single Average (SA) averages over multiple samples in the last test chain 1 SSA = |TTE |E S(TTR, j) (2) This is a common averaging strategy in which we obtain a point estimate of the global latent variables at the end of the training chain. Then, a single test chain is generated on the test data and multiple samples of this test chain are averaged to obtain the final prediction (Chang, 2012; Singh et al., 2012; Jiang et al., 2012; Zhu et al., 2014). • Multiple Final (MF) averages over the last samples of multiple test chains from multiple models SMF = 1|TTR |E S(i, TTE). (3) iETTR • Multiple Average (MA) averages over all samples of multiple test chains for distinct models, 1 E E S(i, j), (4) |TTE |iETTR jETTE 3 Unsupervised Topic Models We evaluate the predictive performance of the unsupervised topic model LDA using different averaging strategies in Section 2. LDA: Proposed by Blei et al. in 2003, LDA posits that each document d is a multinomial distribution θd over K topics, each of which is a mu</context>
</contexts>
<marker>Singh, Wick, McCallum, 2012</marker>
<rawString>Sameer Singh, Michael Wick, and Andrew McCallum. 2012. Monte Carlo MCMC: Efficient inference by approximate sampling. In EMNLP, pages 1104–1113.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steyvers</author>
<author>Tom Griffiths</author>
</authors>
<title>Probabilistic topic models.</title>
<date>2006</date>
<editor>In T. Landauer, D. Mcnamara, S. Dennis, and W. Kintsch, editors,</editor>
<contexts>
<context position="8521" citStr="Steyvers and Griffiths, 2006" startWordPosition="1404" endWordPosition="1407">bution θd over K topics, each of which is a multinomial distribution Ok over the vocabulary. LDA’s generative process is: 1. For each topic k ∈ [1, K] (a) Draw word distribution Ok ∼ Dir(β) 2. For each document d ∈ [1, D] (a) Draw topic distribution θd ∼ Dir(α) (b) For each word n ∈ [1, Nd] i. Draw topic zd,n ∼ Mult(θd) ii. Draw word wd,n ∼ Mult(Ozd,n) In LDA, the global latent variables are topics {Ok}Kk=1 and the local latent variables for each document d are topic proportions θd. Train: During training, we use collapsed Gibbs sampling to assign each token in the training data with a topic (Steyvers and Griffiths, 2006). The probability of jETTE 1 SMA = |TTR| 1753 assigning token n of training document d to topic k is p(zTR d,n = k |zTR −d,n, wTR −d,n, wTR d,n = v) ∝ , (5) N−d,n TR,k,· + V β where NTR,d,k is the number of tokens in the training document d assigned to topic k, and NTR,k,v is the number of times word type v assigned to topic k. Marginal counts are denoted by ·, and −d,n denotes the count excluding the assignment of token n in document d. At each training iteration i, we estimate the distribution over words ˆφk(i) of topic k as NTR,k,v(i) + β (i) = (6) NTR,k,·(i) + V β where the counts NTR,k,v(</context>
</contexts>
<marker>Steyvers, Griffiths, 2006</marker>
<rawString>Mark Steyvers and Tom Griffiths. 2006. Probabilistic topic models. In T. Landauer, D. Mcnamara, S. Dennis, and W. Kintsch, editors, Latent Semantic Analysis: A Road to Meaning. Laurence Erlbaum.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hanna M Wallach</author>
<author>Iain Murray</author>
<author>Ruslan Salakhutdinov</author>
<author>David Mimno</author>
</authors>
<title>Evaluation methods for topic models.</title>
<date>2009</date>
<editor>In Leon Bottou and Michael Littman, editors, ICML.</editor>
<contexts>
<context position="9456" citStr="Wallach et al., 2009" startWordPosition="1579" endWordPosition="1583">ed to topic k. Marginal counts are denoted by ·, and −d,n denotes the count excluding the assignment of token n in document d. At each training iteration i, we estimate the distribution over words ˆφk(i) of topic k as NTR,k,v(i) + β (i) = (6) NTR,k,·(i) + V β where the counts NTR,k,v(i) and NTR,k,·(i) are taken at training iteration i. Test: Because we lack explicit topic annotations for these data (c.f. Nguyen et al. (2012)), we use perplexity– a widely-used metric to measure the predictive power of topic models on held-old documents. To compute perplexity, we follow the estimating θ method (Wallach et al., 2009, Section 5.1) and evenly split each test document d into wTE1 dand wTE2 d . We first run Gibbs sampling on wTE1 d to estimate the topic proportion ˆθTE d of test document d. The probability of assigning topic k to token n in wTE1 d is p(zTE1 d,n = k |zTE1 −d,n, wTE1, ˆφ(i)) ∝ N−d,n ˆφk,wTE1 (7) TE1,d,k + α · ���(i) N−d,n TE1,d,· + Kα where NTE1,d,k is the number of tokens in wTE1 dassigned to topic k. At each iteration j in test chain i, we can estimate the topic proportion vector ˆθTE d (i, j) for test document d as ˆθTE d,k(i,j) = NTE1,d,k(i,j) + α (8) NTE1,d,·(i,j) + Kα where both the coun</context>
</contexts>
<marker>Wallach, Murray, Salakhutdinov, Mimno, 2009</marker>
<rawString>Hanna M. Wallach, Iain Murray, Ruslan Salakhutdinov, and David Mimno. 2009. Evaluation methods for topic models. In Leon Bottou and Michael Littman, editors, ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hanna M Wallach</author>
</authors>
<title>Structured Topic Models for Language.</title>
<date>2008</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Cambridge.</institution>
<contexts>
<context position="11608" citStr="Wallach, 2008" startWordPosition="1965" endWordPosition="1967">nerally decreases with additional training iterations, but the drop is more pronounced with multiple test chains. the vocabulary, and we filter using TF-IDF to obtain a vocabulary of 10,000 words.3 The three datasets are: • HOTEL: 240,060 reviews of hotels from TripAdvisor (Wang et al., 2010). • RESTAURANT: 25,459 reviews of restaurants from Yelp (Jo and Oh, 2011). • MOVIE: 5,006 reviews of movies from Rotten Tomatoes (Pang and Lee, 2005) We report cross-validated average performance over five folds, and use K = 50 topics for all datasets. To update the hyperparameters, we use slice sampling (Wallach, 2008, p. 62).4 Results: Figure 2 shows the perplexity of the four averaging methods, computed with different number of training iterations TTR. SA outperforms SF, showing the benefits of averaging over multiple test samples from a single test chain. However, both multiple chain methods (MF and MA) significantly outperform these two methods. This result is consistent with Asuncion et al. (2009), who run multiple training chains but a single test chain for each training chain and average over them. This is more costly since training chains are usually significantly longer than test chains. In additi</context>
</contexts>
<marker>Wallach, 2008</marker>
<rawString>Hanna M Wallach. 2008. Structured Topic Models for Language. Ph.D. thesis, University of Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chong Wang</author>
<author>David Blei</author>
<author>Li Fei-Fei</author>
</authors>
<title>Simultaneous image classification and annotation.</title>
<date>2009</date>
<booktitle>In CVPR.</booktitle>
<contexts>
<context position="1827" citStr="Wang et al., 2009" startWordPosition="270" endWordPosition="273">which allows for effective exploration, analysis, and summarization of the otherwise unstructured corpora (Blei, 2012; Blei, 2014). In addition to exploratory data analysis, a typical goal of topic models is prediction. Given a set of unannotated training data, unsupervised topic models try to learn good topics that can generalize to unseen text. Supervised topic models jointly capture both the text and associated metadata such as a continuous response variable (Blei and McAuliffe, 2007; Zhu et al., 2009; Nguyen et al., 2013), single label (Rosen-Zvi et al., 2004; Lacoste-Julien et al., 2008; Wang et al., 2009) or multiple labels (Ramage et al., 2009; Ramage et al., 2011) to predict metadata from text. Probabilistic topic modeling requires estimating the posterior distribution. Exact computation of the posterior is often intractable, which motivates approximate inference techniques (Asuncion et al., 2009). One popular approach is Markov chain Monte Carlo (MCMC), a class of inference algorithms to approximate the target posterior distribution. To make prediction, MCMC algorithms generate samples on training data to estimate corpus-level latent variables, and use them to generate samples to estimate d</context>
</contexts>
<marker>Wang, Blei, Fei-Fei, 2009</marker>
<rawString>Chong Wang, David Blei, and Li Fei-Fei. 2009. Simultaneous image classification and annotation. In CVPR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hongning Wang</author>
<author>Yue Lu</author>
<author>Chengxiang Zhai</author>
</authors>
<title>Latent aspect rating analysis on review text data: A rating regression approach.</title>
<date>2010</date>
<booktitle>In SIGKDD,</booktitle>
<pages>783--792</pages>
<contexts>
<context position="11288" citStr="Wang et al., 2010" startWordPosition="1911" endWordPosition="1914">ets, we preprocess by tokenizing, removing stopwords, stemming, adding bigrams to Restaurant Reviews ● ● ● 600 700 800 900 1000 Movie Reviews 2100 2050 2000 1950 600 700 800 900 1000 Hotel Reviews Figure 2: Perplexity of LDA using different averaging strategies with different number of training iterations TTR. Perplexity generally decreases with additional training iterations, but the drop is more pronounced with multiple test chains. the vocabulary, and we filter using TF-IDF to obtain a vocabulary of 10,000 words.3 The three datasets are: • HOTEL: 240,060 reviews of hotels from TripAdvisor (Wang et al., 2010). • RESTAURANT: 25,459 reviews of restaurants from Yelp (Jo and Oh, 2011). • MOVIE: 5,006 reviews of movies from Rotten Tomatoes (Pang and Lee, 2005) We report cross-validated average performance over five folds, and use K = 50 topics for all datasets. To update the hyperparameters, we use slice sampling (Wallach, 2008, p. 62).4 Results: Figure 2 shows the perplexity of the four averaging methods, computed with different number of training iterations TTR. SA outperforms SF, showing the benefits of averaging over multiple test samples from a single test chain. However, both multiple chain metho</context>
</contexts>
<marker>Wang, Lu, Zhai, 2010</marker>
<rawString>Hongning Wang, Yue Lu, and Chengxiang Zhai. 2010. Latent aspect rating analysis on review text data: A rating regression approach. In SIGKDD, pages 783–792.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun Zhu</author>
<author>Amr Ahmed</author>
<author>Eric P Xing</author>
</authors>
<title>MedLDA: maximum margin supervised topic models for regression and classification.</title>
<date>2009</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="1718" citStr="Zhu et al., 2009" startWordPosition="252" endWordPosition="255">s latent Dirichlet allocation (Blei et al., 2003, LDA) and its extensions discover these topics from text, which allows for effective exploration, analysis, and summarization of the otherwise unstructured corpora (Blei, 2012; Blei, 2014). In addition to exploratory data analysis, a typical goal of topic models is prediction. Given a set of unannotated training data, unsupervised topic models try to learn good topics that can generalize to unseen text. Supervised topic models jointly capture both the text and associated metadata such as a continuous response variable (Blei and McAuliffe, 2007; Zhu et al., 2009; Nguyen et al., 2013), single label (Rosen-Zvi et al., 2004; Lacoste-Julien et al., 2008; Wang et al., 2009) or multiple labels (Ramage et al., 2009; Ramage et al., 2011) to predict metadata from text. Probabilistic topic modeling requires estimating the posterior distribution. Exact computation of the posterior is often intractable, which motivates approximate inference techniques (Asuncion et al., 2009). One popular approach is Markov chain Monte Carlo (MCMC), a class of inference algorithms to approximate the target posterior distribution. To make prediction, MCMC algorithms generate sampl</context>
</contexts>
<marker>Zhu, Ahmed, Xing, 2009</marker>
<rawString>Jun Zhu, Amr Ahmed, and Eric P. Xing. 2009. MedLDA: maximum margin supervised topic models for regression and classification. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun Zhu</author>
<author>Ning Chen</author>
<author>Hugh Perkins</author>
<author>Bo Zhang</author>
</authors>
<title>Gibbs max-margin topic models with data augmentation.</title>
<date>2014</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>15--1073</pages>
<contexts>
<context position="7377" citStr="Zhu et al., 2014" startWordPosition="1199" endWordPosition="1202">erent strategies to obtain the final prediction: • Single Final (SF) uses the last sample of last test chain to obtain the predicted value, SSF = S(TTR, TTE). (1) • Single Average (SA) averages over multiple samples in the last test chain 1 SSA = |TTE |E S(TTR, j) (2) This is a common averaging strategy in which we obtain a point estimate of the global latent variables at the end of the training chain. Then, a single test chain is generated on the test data and multiple samples of this test chain are averaged to obtain the final prediction (Chang, 2012; Singh et al., 2012; Jiang et al., 2012; Zhu et al., 2014). • Multiple Final (MF) averages over the last samples of multiple test chains from multiple models SMF = 1|TTR |E S(i, TTE). (3) iETTR • Multiple Average (MA) averages over all samples of multiple test chains for distinct models, 1 E E S(i, j), (4) |TTE |iETTR jETTE 3 Unsupervised Topic Models We evaluate the predictive performance of the unsupervised topic model LDA using different averaging strategies in Section 2. LDA: Proposed by Blei et al. in 2003, LDA posits that each document d is a multinomial distribution θd over K topics, each of which is a multinomial distribution Ok over the voca</context>
</contexts>
<marker>Zhu, Chen, Perkins, Zhang, 2014</marker>
<rawString>Jun Zhu, Ning Chen, Hugh Perkins, and Bo Zhang. 2014. Gibbs max-margin topic models with data augmentation. Journal of Machine Learning Research, 15:1073–1110.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
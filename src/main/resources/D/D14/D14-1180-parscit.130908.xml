<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000011">
<title confidence="0.858733">
Type-based MCMC for Sampling Tree Fragments from Forests
</title>
<author confidence="0.994125">
Xiaochang Peng and Daniel Gildea
</author>
<affiliation confidence="0.9953835">
Department of Computer Science
University of Rochester
</affiliation>
<address confidence="0.28692">
Rochester, NY 14627
</address>
<sectionHeader confidence="0.96649" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999816055555556">
This paper applies type-based Markov
Chain Monte Carlo (MCMC) algorithms
to the problem of learning Synchronous
Context-Free Grammar (SCFG) rules
from a forest that represents all possible
rules consistent with a fixed word align-
ment. While type-based MCMC has been
shown to be effective in a number of NLP
applications, our setting, where the tree
structure of the sentence is itself a hid-
den variable, presents a number of chal-
lenges to type-based inference. We de-
scribe methods for defining variable types
and efficiently indexing variables in or-
der to overcome these challenges. These
methods lead to improvements in both log
likelihood and BLEU score in our experi-
ments.
</bodyText>
<sectionHeader confidence="0.998784" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999793573770492">
In previous work, sampling methods have been
used to learn Tree Substitution Grammar (TSG)
rules from derivation trees (Post and Gildea, 2009;
Cohn et al., 2009) for TSG learning. Here, at each
node in the derivation tree, there is a binary vari-
able indicating whether the node is internal to a
TSG rule or is a split point, which we refer to as
a cut, between two rules. The problem of extract-
ing machine translation rules from word-aligned
bitext is a similar problem in that we wish to au-
tomatically learn the best granularity for the rules
with which to analyze each sentence. The prob-
lem of rule extraction is more complex, however,
because the tree structure of the sentence is also
unknown.
In machine translation applications, most pre-
vious work on joint alignment and rule extrac-
tion models uses heuristic methods to extract rules
from learned word alignment or bracketing struc-
tures (Zhang et al., 2008; Blunsom et al., 2009;
DeNero et al., 2008; Levenberg et al., 2012).
Chung et al. (2014) present a MCMC algorithm
schedule to learn Hiero-style SCFG rules (Chiang,
2007) by sampling tree fragments from phrase de-
composition forests, which represent all possible
rules that are consistent with a set of fixed word
alignments. Assuming fixed word alignments re-
duces the complexity of the sampling problem,
and has generally been effective in most state-
of-the-art machine translation systems. The al-
gorithm for sampling rules from a forest is as
follows: from the root of the phrase decomposi-
tion forest, one samples a cut variable, denoting
whether the current node is a cut, and an edge vari-
able, denoting which incoming hyperedge is cho-
sen, at each node of the current tree in a top-down
manner. This sampling schedule is efficient in that
it only samples the current tree and will not waste
time on updating variables that are unlikely to be
used in any tree.
As with many other token-based Gibbs Sam-
pling applications, sampling one node at a time
can result in slow mixing due to the strong cou-
pling between variables. One general remedy is
to sample blocks of coupled variables. Cohn and
Blunsom (2010) and Yamangil and Shieber (2013)
used blocked sampling algorithms that sample the
whole tree structure associated with one sentence
at a time for TSG and TAG learning. However, this
kind of blocking does not deal with the coupling of
variables correlated with the same type of struc-
ture across sentences. Liang et al. (2010) intro-
duced a type-based sampling schedule which up-
dates a block of variables of the same type jointly.
The type of a variable is defined as the combina-
tion of new structural choices added when assign-
ing different values to the variable. Type-based
MCMC tackles the coupling issue by assigning the
same type to variables that are strongly coupled.
In this paper, we follow the phrase decompo-
sition forest construction procedures of Chung et
</bodyText>
<page confidence="0.923657">
1735
</page>
<note confidence="0.948408">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1735–1745,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</note>
<equation confidence="0.852593">
V. �� � A 17 ��
</equation>
<bodyText confidence="0.999293954545454">
al. (2014) and present a type-based MCMC algo-
rithm for sampling tree fragments from phrase de-
composition forests which samples the variables
of the same type jointly. We define the type of the
cut variable for each node in our sampling sched-
ule. While type-based MCMC has been proven
to be effective in a number of NLP applications,
our sample-edge, sample-cut setting is more com-
plicated as our tree structure is unknown. We
need additional steps to maintain the cut type in-
formation when the tree structure is changed as
we sample the edge variable. Like other type-
based MCMC applications, we need bookkeep-
ing of node sites to be sampled in order to loop
through sites of the same type efficiently. As noted
by Liang et al. (2010), indexing by the complete
type information is too expensive in some appli-
cations like TSG learning. Our setting is different
from TSG learning in that the internal structure of
each SCFG rule is abstracted away when deriving
the rule type from the tree fragment sampled.
We make the following contributions:
</bodyText>
<listItem confidence="0.95914">
1. We apply type-based MCMC to the setting of
SCFG learning and have achieved better log
likelihood and BLEU score result.
</listItem>
<bodyText confidence="0.916615285714286">
2. We present an innovative way of storing the
type information by indexing on partial type
information and then filtering the retrieved
nodes according to the full type information,
which enables efficient updates to maintain
the type information while the amount of
bookkeeping is reduced significantly.
</bodyText>
<listItem confidence="0.868512">
3. We replace the two-stage sampling schedule
of Liang et al. (2010) with a simpler and
faster one-stage method.
4. We use parallel programming to do inexact
type-based MCMC, which leads to a speed
up of four times in comparison with non-
parallel type-based MCMC, while the like-
lihood result of the Markov Chain does not
change. This strategy should also work with
other type-based MCMC applications.
</listItem>
<sectionHeader confidence="0.9991935" genericHeader="method">
2 MCMC for Sampling Tree Fragments
from Forests
</sectionHeader>
<subsectionHeader confidence="0.979861">
2.1 Phrase Decomposition Forest
</subsectionHeader>
<bodyText confidence="0.9987625">
The phrase decomposition forest provides a com-
pact representation of all machine translation rules
</bodyText>
<figure confidence="0.995385142857143">
I
have
a
date
with
her
today
</figure>
<figureCaption confidence="0.999103">
Figure 1: Example word alignment, with boxes
</figureCaption>
<bodyText confidence="0.97461978125">
showing valid phrase pairs. In this example, all
individual alignment points are also valid phrase
pairs.
that are consistent with our fixed input word align-
ment (Chung et al., 2014), and our sampling algo-
rithm selects trees from this forest.
As in Hiero, our grammars will make use of a
single nonterminal X, and will contain rules with
a mixture of nonterminals and terminals on the
righthand side (r.h.s.), with at most two nontermi-
nal occurrences on the r.h.s. Under this restric-
tion, the maximum number of rules that can be
extracted from an input sentence pair is O(n12)
with respect to the length of the sentence pair,
as the left and right boundaries of the lefthand
side (l.h.s.) nonterminal and each of the two r.h.s.
nonterminals can take O(n) positions in each of
the two languages. This complexity leads us to
explore sampling algorithms instead of using dy-
namic programming.
A span [i, j] is a set of contiguous word in-
dices {i, i + 1,...,j − 11. Given an aligned
Chinese-English sentence pair, a phrase n is a pair
of spans n = ([i1, j1], [i2, j2]) such that Chinese
words in positions [i1,j1] are aligned only to En-
glish words in positions [i2, j2], and vice versa. A
phrase forest H = (V, E) is a hypergraph made
of a set of hypernodes V and a set of hyperedges
E. Each node n = ([i1, j1], [i2, j2]) E V is a
tight phrase as defined by Koehn et al. (2003),
i.e., a phrase containing no unaligned words at its
boundaries. A phrase n = ([i1, j1], [i2, j2]) covers
</bodyText>
<equation confidence="0.754448333333333">
n0 = ([i01,j01], [i02,j02]) if
i1 G i01 n j01 � j1 n i2 G i02 n j02 � j2
Each edge in E, written as T —* n, is made of a
</equation>
<page confidence="0.981901">
1736
</page>
<figureCaption confidence="0.999686">
Figure 2: A phrase decomposition forest extracted
</figureCaption>
<bodyText confidence="0.99996296875">
from the sentence pair A fn �k �q ��, I
have a date with her today). Each edge is a min-
imal SCFG rule, and the rules at the bottom level
are phrase pairs. Unaligned word “a” shows up
in the rule X —* X1X2, X1aX2 after unaligned
words are put back into the alignment matrix. The
highlighted portion of the forest shows an SCFG
rule built by composing minimal rules.
set of non-intersecting tail nodes T C V , and a
single head node n E V that covers each tail node.
We say an edge T —* n is minimal if there does
not exist another edge T0 —* n such that T0 covers
T. A minimal edge is an SCFG rule that cannot
be decomposed by factoring out some part of its
r.h.s. as a separate rule. We define a phrase de-
composition forest to be made of all phrases from
a sentence pair, connected by all minimal SCFG
rules. A phrase decomposition forest compactly
represents all possible SCFG rules that are consis-
tent with word alignments. For the example word
alignment shown in Figure 1, the phrase decom-
position forest is shown in Figure 2. Each boxed
phrase in Figure 1 corresponds to a node in the
forest of Figure 2, while hyperedges in Figure 2
represent ways of building phrases out of shorter
phrases.
A phrase decomposition forest has the impor-
tant property that any SCFG rule consistent with
the word alignment corresponds to a contiguous
fragment of some complete tree found in the for-
est. For example, the highlighted tree fragment
of the forest in Figure 2 corresponds to the SCFG
</bodyText>
<equation confidence="0.9157185">
rule:
X —* �n X2 �q X1, have a X1 with X2
</equation>
<bodyText confidence="0.999728222222222">
Thus any valid SCFG rule can be formed by se-
lecting a set of adjacent hyperedges from the for-
est and composing the minimal SCFG rules speci-
fied by each hyperedge. Therefore, the problem of
SCFG rules extraction can be solved by sampling
tree fragments from the phrase decomposition for-
est. We use a bottom-up algorithm to construct the
phrase decomposition forest from the word align-
ments.
</bodyText>
<subsectionHeader confidence="0.999851">
2.2 Sampling Tree Fragments From Forest
</subsectionHeader>
<bodyText confidence="0.999987105263158">
We formulate the rule sampling procedure into
two phases: first we select a tree from a forest,
then we select the cuts in the tree to denote the split
points between fragments, with each fragment cor-
responding to a SCFG rule. A tree can be speci-
fied by attaching a variable en to each node n in
the forest, indicating which hyperedge is turned
on at the current node. Thus each assignment will
specify a unique tree by tracing the edge variables
from the root down to the leaves. We also attach
a cut variable zn to each node, indicating whether
the node is a split point between two adjacent frag-
ments.
Let all the edge variables form the random vec-
tor Y and all the cut variables form the random
vector Z. Given an assignment y to the edge vari-
ables and assignment z to the cut variables, our de-
sired distribution is proportional to the product of
weights of the rules specified by the assignment:
</bodyText>
<equation confidence="0.998754">
Pt(Y = y,Z = z) a � w(r) (1)
rET(y,z)
</equation>
<bodyText confidence="0.9999352">
where τ(y, z) is the set of rules identified by the
assignment. We use a generative model based on
a Dirichlet Process (DP) defined over composed
rules. We draw a distribution G over rules from a
DP, and then rules from G.
</bodyText>
<equation confidence="0.9769985">
G  |α, P0 —Dir(α, P0)
r  |G —G
</equation>
<bodyText confidence="0.984716666666667">
For the base distribution P0, we use a uniform
distribution where all rules of the same size have
equal probability:
</bodyText>
<equation confidence="0.99998504">
P0(r) = Vf |rf|V e |re |(2)
([4 6], [1 4])
([2 4], [4 6])
([1 21, [6 71)
([4 5], [1 2])
([5 61, [3 41)
([2 3], [4 5])
([3 41, [5 61)
([0 61, [0 71)
X � Xi X2, Xi X2
([0 11, [0 11)
([1 61, [1 71)
X � Xi X2, X2 Xi X � Xi X2, X2 Xi
X — R, I
([1 41, [4 71)
X � Xi X2, X2 Xi
([2 61, [1 61)
X — X. X2, X2 XI
X — X. X2, X. a X.
X — XI X., XI X.
X — **, today
X — *&apos;, have
X — Yi&amp; date
X — R, with
X — k9, her
</equation>
<page confidence="0.908092">
1737
</page>
<bodyText confidence="0.999876357142857">
where Vf and Ve are the vocabulary sizes of the
source language and the target language, and |rf|
and |re |are the lengths of the source side and tar-
get side of rule r. By marginalizing out G we get a
simple posterior distribution over rules which can
be described using the Chinese Restaurant Process
(CRP). For this analogy, we imagine a restaurant
has infinite number of tables that represent rule
types and customers that represent translation rule
instances. Each customer enters the restaurant and
chooses a table to sit at. Let zi be the table chosen
by the i-th customer, then the customer chooses a
table k either having been seated or a new table
with probability:
</bodyText>
<equation confidence="0.981494">
� nk
i−1+α 1 ≤ k ≤ K
P(zi = k|z−i) = (3)
α
i−1+α k = K + 1
</equation>
<bodyText confidence="0.999976133333333">
where z−i is the current seating arrangement, nk is
the number of customers at the table k, K is the to-
tal number of occupied tables. If the customer sits
at a new table, the new table will be assigned a rule
label r with probability P0(r). We can see from
Equation 3 that the only history related to the cur-
rent table assignment is the counts in z−i. There-
fore, we define a table of counts N = {NC}C∈I
which memorizes different categories of counts in
z−i. I is an index set for different categories of
counts. Each NC is a vector of counts for category
C. We have P(ri = r|z−i) = P(ri = r|N). If
we marginalize over tables labeled with the same
rule, we get the following probability over rule r
given the previous count table N:
</bodyText>
<equation confidence="0.999543333333333">
NR(r) + αP0(r)
P(ri = r|N) = (4)
n + α
</equation>
<bodyText confidence="0.999975777777778">
here in the case of DP, I = {R}, where R is the
index for the category of rule counts. NR(r) is the
number of times that rule r has been observed in
z−i, n = Er NR(r) is the total number of rules
observed.
We also define a Pitman-Yor Process (PYP)
(Pitman and Yor, 1997) over rules of each length l.
We draw the rule distribution G from a PYP, and
then rules of length l are drawn from G.
</bodyText>
<equation confidence="0.852641">
G|α, d, P0 ∼ PY (α, d, P0)
r|G ∼ G
</equation>
<bodyText confidence="0.88514625">
The first two parameters, a concentration parame-
ter α and a discount parameter d, control the shape
of distribution G by controlling the size and the
Algorithm 1 Top-down Sampling Algorithm
</bodyText>
<listItem confidence="0.907930818181818">
1: queue.push(root)
2: while queue is not empty do
3: n = queue.pop()
4: SAMPLEEDGE(n)
5: SAMPLECUT(n)
6: for each child c of node n do
7: queue.push(c)
8: end for
9: end while
number of clusters. Integrating over G, we have
the following PYP posterior probability:
</listItem>
<equation confidence="0.998987">
NR(r) − Trd + (Tld + α)P0(r)
P(ri = r|N) = (5)
NL(l) + α
</equation>
<bodyText confidence="0.999093846153846">
here for the case of PYP, I = {R, L}. We have an
additional index L for the category of rule length
counts, and NL(l) is the total number of rules of
length l observed in z−i. Tr is the number of ta-
bles labeled with r in z−i. The length of the rule is
drawn from a Poisson distribution, so a rule length
probability P(l; A) = λle−λ
l! is multiplied by this
probability to calculate the real posterior probabil-
ity for each rule. In order to simplify the tedious
book-keeping, we estimate the number of tables
using the following equations (Huang and Renals,
2010):
</bodyText>
<equation confidence="0.900107">
Tr = NR(r)d (6)
�Tl = NR(r)d (7)
r:|r|=l
</equation>
<bodyText confidence="0.999996769230769">
We use the top-down sampling algorithm of
Chung et al. (2014) (see Algorithm 1). Starting
from the root of the forest, we sample a value for
the edge variable denoting which incoming hyper-
edge of the node is turned on in the current tree,
and then we sample a cut value for the node de-
noting whether the node is a split point between
two fragments in the tree. For each node n, we de-
note the composed rule type that we get when we
set the cut of node n to 0 as r1 and the two split
rule types that we get when we set the cut to 1 as
r2, r3. We sample the cut value zi of the current
node according to the posterior probability:
</bodyText>
<equation confidence="0.9931325">
� P(r1|N) if z = 0
P z — z N P(r11N) +P r2|N)P(pjN&apos;) (8)
(2 — ) = P(r2  |N�P(r3 |N&apos;) otherwise
P(r1|N)+P(r2|N)P(r3|N )
</equation>
<bodyText confidence="0.9999854">
where the posterior probability P(ri|N) is accord-
ing to either a DP or a PYP, and N, N0 are tables of
counts. In the case of DP, N, N0 differ only in the
rule counts of r2, where N0R(r2) = NR(r2) + 1.
In the case of PYP, there is an extra difference that
</bodyText>
<page confidence="0.989823">
1738
</page>
<figureCaption confidence="0.867453">
Figure 3: An example of cut type: Consider
the two nodes marked in bold, ([26], [16]),
</figureCaption>
<bodyText confidence="0.999496142857143">
([14], [47]). These two non-split nodes are
internal to the same composed rule: X —*
X1X2X3, X3X2X1. We keep these two sites with
the same index. However, when we set the cut
value of these two nodes to 1, as the rules imme-
diately above and immediately below are different
for these two sites, they are not of the same type.
</bodyText>
<equation confidence="0.8653985">
NL&apos;(l) = NL(l) + 1, where l is the rule length of
r2.
</equation>
<bodyText confidence="0.999197">
As for edge variables ei, we refer to the set of
composed rules turned on below n including the
composed rule fragments having n as an internal
or root node as {r1, ... , rm1. We have the follow-
ing posterior probability over the edge variable ei:
</bodyText>
<equation confidence="0.9998955">
P(ei = e|N) a �m P(ri|Ni 1) � deg(v) (9)
i=1 vEτ(e)nin(n)
</equation>
<bodyText confidence="0.9533515">
where deg(v) is the number of incoming edges for
node v, in(n) is the set of nodes in all subtrees
under n, and T(e) is the tree specified when we
set ei = e. N0 to Nm are tables of counts where
</bodyText>
<equation confidence="0.99658325">
N0 = N, NiR(ri) = Ni 1
R (ri) + 1 in the case of
DP and additionally NiL(li) = Ni 1
L (li) + 1 in the
</equation>
<bodyText confidence="0.938616">
case of PYP, where li is the rule length of ri.
</bodyText>
<sectionHeader confidence="0.997328" genericHeader="method">
3 Type-based MCMC Sampling
</sectionHeader>
<bodyText confidence="0.999616333333333">
Our goal in this paper is to organize blocks of vari-
ables that are strongly coupled into types and sam-
ple variables of each type jointly. One major prop-
erty of type-based MCMC is that the joint proba-
bility of variables of the same type should be ex-
changeable so that the order of the variables does
not matter. Also, the choices of the variables to
be sampled jointly should not interfere with each
other, which we define as a conflict. In this section,
we define the type of cut variables in our sampling
schedule and explain that with the two priors we
introduced before, the joint probability of the vari-
ables will satisfy the exchangeability property. We
will also discuss how to check conflict sites in our
application.
In type-based MCMC, we need bookkeeping of
sites as we need to loop through them to search for
sites having the same type efficiently. In our two-
stage sample-edge, sample-cut schedule, updating
the edge variable would change the tree structure
and trigger updates for the cut variable types in
both the old and the new subtree. We come up with
an efficient bookkeeping strategy to index on par-
tial type information which significantly reduces
the bookkeeping size, while updates are quite effi-
cient when the tree structure is changed. The detail
will become clear below.
</bodyText>
<subsectionHeader confidence="0.964013">
3.1 Type-based MCMC
</subsectionHeader>
<bodyText confidence="0.993501888888889">
We refer to each node site to be sampled as a pair
(t, n), indicating node n of forest t. For each site
(t, n) and the corresponding composed rule types
r1 obtained when we set n’s cut value to 0 and
r2, r3 obtained when we set the cut value to 1, the
cut variable type of site (t, n) is:
type(t, n) def = (r1, r2, r3)
We say that the cut variables of two sites are of
the same type if the composed rule types r1, r2 and
r3 are exactly the same. For example, in Figure 3,
assume that all the nodes in the hypergraph are
currently set to be split points except for the two
nodes marked in bold, ([26], [16]), ([14], [47]).
Considering these two non-split nodes, the com-
posed rule types they are internal to (r1) are ex-
actly the same. However, the situation changes if
we set the cut variables of these two nodes to be 1,
i.e., all of the nodes in the hypergraph are now split
points. As the rule type immediately above and
the rule type immediately below the two nodes (r2
and r3) are now different, they are not of the same
type.
We sample the cut value zi according to Equa-
tion 8. As each rule is sampled according to
a DP or PYP posterior and the joint probabili-
ties according to both posteriors are exchangeable,
we can see from Equation 8 that the joint prob-
</bodyText>
<equation confidence="0.999893041666667">
([4 6], [1 4])
([2 4], [4 6])
([1 2], [6 7])
([4 5], [1 2])
([5 6], [3 4])
([2 3], [4 5])
([3 4], [5 6])
([0 6], [0 7])
X � X, Xa, X, X2
([0 1], [0 1])
([1 6], [1 7])
X � Xi Xa, Xz Xi X � Xi Xz, Xa Xi
X — R, I
([1 4], [4 7])
X � X1 X2, X2 X1
([2 6], [1 6])
X — X1 X2, X. X1
X � X, X2, X, a Xa
X � X, X2, X, X2
X — +K, today
X — ;4, have
X — #A]°*, date
X — R, with
X — M, her
</equation>
<page confidence="0.946763">
1739
</page>
<bodyText confidence="0.996838428571429">
ability of a sequence of cut variables is also ex-
changeable. Consider a set of sites S containing
n cut variables zS = (z1, ..., zn) of the same type.
This exchangeability property leads to the fact that
any sequence containing same number of cuts (cut
value of 1) would have same probability. We have
the following probability distribution:
</bodyText>
<equation confidence="0.99964975">
P(zS|N) a n−m� P(r1|Ni−1)
i=1
�m P(r2 |¯Ni−1)P(r3 |ˆNi−1) def = g(m) (10)
i=1
</equation>
<bodyText confidence="0.99988225">
where N is the count table for all the other vari-
ables except for S. m = Eni=1 zi is the number of
cut sites. The variables N, ¯N, and Nˆ keep track of
the counts as the derivation proceeds step by step:
</bodyText>
<equation confidence="0.999975125">
N0 = N
NiR(r1) = Ni−1
R (r1) + 1
¯N0 = Nn−m
ˆNi−1 R(r2) = ¯Ni−1
R (r2) + 1
¯NiR(r3) = ˆNi−1
R (r3) + 1
</equation>
<bodyText confidence="0.99949175">
For PYP, we add extra count indices for rule length
counts similarly.
Given the exchangeability property of the cut
variables, we can calculate the posterior probabil-
</bodyText>
<equation confidence="0.840320857142857">
ity of m = Eni= (n )
1 zz by summing over all
m
combinations of the cut sites:
p(m|N) a
zS:m=Ei zi
� p(zS|N) = (m)g(m) (11)
</equation>
<subsectionHeader confidence="0.997059">
3.2 Sampling Cut-types
</subsectionHeader>
<bodyText confidence="0.985160294117647">
Given Equation 11 and the exchangeability prop-
erty, our sampling strategy falls out naturally: first
we sample m according to Equation 11, then con-
ditioned on m, we pick m sites of zS as cut sites
out of the (m) combinations with uniform proba-
bility.
Now we proceed to define conflict sites. In ad-
dition to exchangeability, another important prop-
erty of type-based MCMC is that the type of each
site to be sampled should be independent of the
assignment of the other sites sampled at the same
time. That is, in our case, setting the cut value of
each site should not change the (r1, r2, r3) triple
of another site. We can see that the cut value of
the current site would have effect on and only on
Algorithm 2 Type-based MCMC Algorithm for
Sampling One Site
</bodyText>
<listItem confidence="0.987450161290322">
1: sample one type of sites, currently sample site
(node, parent)
2: if parent is None or node is sampled then
3: return
4: end if
5: old = node.cut
6: node.cut = 0
7: r1 = composed rule(parent)
8: node.cut = 1
9: r2 = composed rule(parent)
10: r3 = composed rule(node)
11: node.cut = old
12: sites =
13: for sites s E index[r1] do
14: for sites s0 in rule rooted at s do
15: if s0 of type (r1, r2, r3) and no conflict
then
16: add s0 to sites
17: end if
18: end for
19: end for
20: for sites s E index[r3] do
21: if s of type (r1, r2, r3) and no conflict then
22: add s to sites
23: end if
24: end for
25: sample m according to Equation 11
26: remove sites from index
27: uniformly choose m in sites to be cut sites.
28: add new cut sites to index
29: mark all nodes in sites as sampled
</listItem>
<bodyText confidence="0.999749588235294">
the nodes in the r1 fragment. We denote nodes(r)
as the node set for all nodes within fragment r.
Then for bz, z0 E S, z is not in conflict with z0 if
and only if nodes(r1) n nodes(r01) = 0, where r1
and r01 are the corresponding composed rule types
when we set z, z0 to 0.
Another crucial issue in type-based sampling is
the bookkeeping of sampling sites, as we need to
loop through all sites having the same type with
the current node. We only maintain the type in-
formation of nodes that are currently turned on in
the chosen tree of the forest, as we only sample
these nodes. It is common practice to directly use
the type value of each variable as an index and
maintain a set of sites for each type. However,
maintaining a (r1, r2, r3) triple for each node in
the chosen tree is too memory heavy in our appli-
</bodyText>
<page confidence="0.969779">
1740
</page>
<bodyText confidence="0.99937">
cation.
In our two-stage sample-edge, sample-cut
schedule, there is an additional issue that we must
deal with efficiently: when we have chosen a new
incoming edge for the current node, we also have
to update the bookkeeping index as the current tree
structure is changed. Cut variable types in the old
subtree will be turned off and a new subtree of
variable types will be turned on. In the extreme
case, when we have chosen a new incoming edge
at the root node, we have chosen a new tree in the
forest. So, we need to remove appearances of cut
variable types in the old tree and add all cut vari-
able types in the newly chosen tree.
Our strategy to deal with these two issues is to
build a small, simple index, at the cost of some
additional computation when retrieving nodes of a
specified type. To be precise, we build an index
from (single) rule types r to all occurrences of r in
the data, where each occurrence is represented as
a pointer to the root of r in the forest. Our strategy
has two important differences from the standard
strategy of building an index having the complete
type (r1, r2, r3) as the key and having every node
as an entry. Specifically:
</bodyText>
<listItem confidence="0.97462775">
1. We index only the roots of the current rules,
rather than every node, and
2. We key on a single rule type, rather than a
triple of rule types.
</listItem>
<bodyText confidence="0.998635740740741">
Differences (1) and (2) both serve to keep the in-
dex small, and the dramatic savings in memory is
essential to making our algorithm practical. Fur-
thermore, difference (1) reduces the amount of
work that needs to be done when an edge variable
is resampled. While we must still re-index the en-
tire subtree under the changed edge variable, we
need only to re-index the roots of the current tree
fragments, rather than all nodes in the subtree.
Given this indexing strategy, we now proceed
to describe the process for retrieving nodes of a
specified type (r1, r2, r3). These nodes fall into
one of two cases:
1. Internal nodes, i.e., nodes whose cut variable
is currently set to 0. These nodes must be
contained in a fragment of rule type r1, and
must furthermore have r2 above them, and r3
below them. We retrieve these nodes by look-
ing up r1 in the index, iterating over all nodes
in each fragment retrieved, and retaining only
those with r2 above and r3 below. (Lines 13–
19 in Algorithm 2.)
2. Boundary nodes, i.e., nodes whose cut vari-
able is currently set to 1. These nodes must
form the root of a fragment r3, and have a
fragment r2 above them. We retrieve these
nodes by looking up r3 in the index, and then
checking each node retrieved to retain only
those nodes with r2 above them in the current
tree. (Lines 20–24 in Algorithm 2.)
This process of winnowing down the nodes re-
trieved by the index adds some computational
overhead to our algorithm, but we find that it is
minimal in practice.
We still use the top-down sampling schedule of
Algorithm 1, except that in the sample-edge step,
when we choose a new incoming edge, we add
additional steps to update the bookkeeping index.
Furthermore, in the sample-cut step, we sample
all non-conflict sites having the same type with n
jointly. Our full algorithm for sampling one cut-
type is shown in Algorithm 2. When sampling
each site, we record a parent node of the near-
est cut ancestor of the current node so that we
can build r1 and r2 more quickly, as they are both
rooted at parent. We first identify the type of the
current site. Then we search the bookkeeping in-
dex to find possible candidate sites of the same
type, as described above. As for conflict check-
ing, we keep a set of nodes that includes all nodes
in the r1 fragment of previous non-conflict sites. If
the r1 fragment of the current site has any node in
common with this node set, we arrive at a conflict
site.
</bodyText>
<sectionHeader confidence="0.727143" genericHeader="method">
4 Methods of Further Optimization
</sectionHeader>
<subsectionHeader confidence="0.99977">
4.1 One-stage Sampling Schedule
</subsectionHeader>
<bodyText confidence="0.999980666666667">
Instead of calculating the posterior of each m ac-
cording to Equation 11 and then sampling m, we
can build our real m more greedily.
</bodyText>
<equation confidence="0.972091">
n
P(zS|N) = P(zi|Ni−1) (12)
i=1
</equation>
<bodyText confidence="0.988493166666667">
where N, N0, ... , Nn are count tables, and N0 =
N. Ni is the new count table after we update Ni−1
according to the assignment of zi. This equation
gives us a hint to sample each zi according to
P(zi|Ni−1) and then update the count table Ni−1
according to the assignment of zi. This greedy
</bodyText>
<page confidence="0.97405">
1741
</page>
<bodyText confidence="0.999971857142857">
sampling saves us the effort to calculate each m
by multiplying over each posterior of cut variables
but directly samples the real m. In our exper-
iment, this one-stage sampling strategy gives us
a 1.5 times overall speed up in comparison with
the two-stage sampling schedule of Liang et al.
(2010).
</bodyText>
<subsectionHeader confidence="0.997026">
4.2 Parallel Implementation
</subsectionHeader>
<bodyText confidence="0.999996392857143">
As our type-based sampler involves tedious book-
keeping and frequent conflict checking and mis-
match of cut types, one iteration of the type-based
sampler is slower than an iteration of the token-
based sampler when run on a single processor.
In order to speed up our sampling procedure, we
used a parallel sampling strategy similar to that of
Blunsom et al. (2009) and Feng and Cohn (2013),
who use multiple processors to perform inexact
Gibbs Sampling, and find equivalent performance
in comparison with an exact Gibbs Sampler with
significant speed up. In our application, we split
the data into several subsets and assign each sub-
set to a processor. Each processor performs type-
based sampling on its subset using local counts
and local bookkeeping, and communicates the up-
date of the local counts after each iteration. All
the updates are then aggregated to generate global
counts and then we refresh the local counts of
each processor. We do not communicate the up-
date on the bookkeeping of each processor. In this
implementation, we have a slightly “out-of-date”
counts at each processor and a smaller bookkeep-
ing of sites of the same type, but we can perform
type-based sampling independently on each pro-
cessor. Our experiments show that, with proper
division of the dataset, the final performance does
not change, while the speed up is significant.
</bodyText>
<sectionHeader confidence="0.998176" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999891166666667">
We used the same LDC Chinese-English parallel
corpus as Chung et al. (2014),1 which is composed
of newswire text. The corpus consists of 41K sen-
tence pairs, which has 1M words on the English
side. The corpus has a 392-sentence development
set with four references for parameter tuning, and
</bodyText>
<footnote confidence="0.81726725">
1The data are randomly sampled from various differ-
ent sources (LDC2006E86, LDC2006E93, LDC2002E18,
LDC2002L27, LDC2003E07, LDC2003E14, LDC2004T08,
LDC2005T06, LDC2005T10, LDC2005T34, LDC2006E26,
LDC2005E83, LDC2006E34, LDC2006E85, LDC2006E92,
LDC2006E24, LDC2006E92, LDC2006E24) The language
model is trained on the English side of entire data (1.65M
sentences, which is 39.3M words.)
</footnote>
<bodyText confidence="0.998602625">
a 428-sentence test set with four references for
testing.2 The development set and the test set have
sentences with less than 30 words. A trigram lan-
guage model was used for all experiments. We
plotted the log likelihood graph to compare the
convergence property of each sampling schedule
and calculated BLEU (Papineni et al., 2002) for
evaluation.
</bodyText>
<subsectionHeader confidence="0.998806">
5.1 Experiment Settings
</subsectionHeader>
<bodyText confidence="0.991761131578947">
We use the top-down token-based sampling al-
gorithm of Chung et al. (2014) as our baseline.
We use the same SCFG decoder for translation
with both the baseline and the grammars sam-
pled using our type-based MCMC sampler. The
features included in our experiments are differ-
ently normalized rule counts and lexical weight-
ings (Koehn et al., 2003) of each rule. Weights are
tuned using Pairwise Ranking Optimization (Hop-
kins and May, 2011) using a grammar extracted by
the standard heuristic method (Chiang, 2007) and
the development set. The same weights are used
throughout our experiments.
First we want to compare the DP likelihood
of the baseline with our type-based MCMC sam-
pler to see if type-based sampling would converge
to a better sampling result. In order to verify if
type-based MCMC really converges to a good op-
timum point, we use simulated annealing (Kirk-
patrick et al., 1983) to search possible better opti-
mum points. We sample from the real distribution
modified by an annealing parameter Q:
z ∼ P(z)β
We increase our Q from 0.1 to 1.3, and then de-
crease from 1.3 to 1.0, changing by 0.1 every 3
iterations. We also run an inexact parallel ap-
proximation of type-based MCMC in comparison
with the non-parallel sampling to find out if par-
allel programming is feasible to speed up type-
based MCMC sampling without affecting the per-
formance greatly. We do not compare the PYP
likelihood because the approximation renders it
impossible to calculate the real PYP likelihood.
We also calculate the BLEU score to compare the
grammars extracted using each sampling sched-
ule. We just report the BLEU result of grammars
sampled using PYP as for all our schedules, since
PYP always performs better than DP.
</bodyText>
<footnote confidence="0.963084">
2They are from newswire portion of NIST MT evaluation
data from 2004, 2005, and 2006.
</footnote>
<page confidence="0.963421">
1742
</page>
<figure confidence="0.337393">
aro C
CL CCD °
</figure>
<bodyText confidence="0.999978722222223">
As for parameter settings, we used d = 0.5 for
the Pitman-Yor discount parameter. Though we
have a separate PYP for each rule length, we used
same a = 5 for all rule sizes in all experiments,
including experiments using DP. For rule length
probability, a Poisson distribution where A = 2
was used for all experiments.3
For each sentence sample, we initialize all the
nodes in the forest to be cut sites and choose an
incoming edge for each node uniformly. For each
experiment, we run for 160 iterations. For each
DP experiment, we draw the log likelihood graph
for each sampling schedule before it finally con-
verges. For each PYP experiment, we tried aver-
aging the grammars from every 10th iteration to
construct a single grammar and use this grammar
for decoding. We tune the number of grammars
included for averaging by comparing the BLEU
score on the dev set and report the BLEU score
result on the test with the same averaging of gram-
mars.
As each tree fragment sampled from the for-
est represents a unique translation rule, we do not
need to explicitly extract the rules; we merely
need to collect them and count them. However,
the fragments sampled include purely non-lexical
rules that do not conform to the rule constraints
of Hiero, and rules that are not useful for trans-
lation. In order to get rid of this type of rule,
we prune every rule that has scope (Hopkins and
Langmead, 2010) greater than two. Whereas Hi-
ero does not allow two adjacent nonterminals in
the source side, our pruning criterion allows some
rules of scope two that are not allowed by Hiero.
For example, the following rule (only source side
shown) has scope two but is not allowed by Hiero:
</bodyText>
<figure confidence="0.8313102">
CD E&apos; 0-n C
CL Co UQ �, t
C
0 c�&apos;o
X → w1X1X2w2X3
</figure>
<subsectionHeader confidence="0.996342">
5.2 Experiment Results
</subsectionHeader>
<bodyText confidence="0.999888">
Figure 4 shows the log likelihood result of our
type-based MCMC sampling schedule and the
baseline top-down sampling. We can see that type-
based sampling converges to a much better re-
sult than non-type-based top-down sampling. This
shows that type-based MCMC escapes some local
optima that are hard for token-based methods to
escape. This further strengthens the idea that sam-
pling a block of strongly coupled variables jointly
</bodyText>
<footnote confidence="0.998489">
3The priors are the same as the work of Chung et al.
(2014). The priors are set to be the same because other priors
turn out not to affect much of the final performance and add
additional difficulty for tuning.
</footnote>
<figureCaption confidence="0.97978675">
Figure 4: Log likelihood result of type-based
MCMC sampling against non-type-based MCMC
sampling, simulated annealing is used to verify if
type-based MCMC converges to a good likelihood
</figureCaption>
<figure confidence="0.955478333333333">
Log likelihood: parallel type-based vs non-parallel type-based
0 10 20 30 40 50 60
Iteration #
</figure>
<figureCaption confidence="0.967983">
Figure 5: parallelization result for type-based
MCMC
</figureCaption>
<bodyText confidence="0.999687411764706">
helps solve the slow mixing problem of token-
based sampling methods. Another interesting ob-
servation is that, even though theoretically these
two sampling methods should finally converge to
the same point, in practice a worse sampling al-
gorithm is prone to get trapped at local optima,
and it will be hard for its Markov chain to es-
cape it. We can also see from Figure 4 that the
log likelihood result only improves slightly using
simulated annealing. One possible explanation is
that the Markov chain has already converged to
a very good optimum point with type-based sam-
pling and it is hard to search for a better optimum.
Figure 5 shows the parallelization result of type-
based MCMC sampling when we run the program
on five processors. We can see from the graph that
when running on five processors, the likelihood fi-
</bodyText>
<figure confidence="0.993599826086956">
Log likelihood: type-based vs non-type-base vs simulated annealing
0 10 20 30 40 50 60
Iteration #
−3.5
—4.0
—4.5
—5.0
—5.5
—6.0
type-based + simulated annealing
type-based
non-type-based
parallel type-based
non-parallel type-based
&apos;CDS io
�:s O O C
−3.5
—4.0
—4.5
—5.0
−5.5
−6.0
−6.5
</figure>
<page confidence="0.907541">
1743
</page>
<table confidence="0.996202">
Sampling Schedule iteration dev test
Non-type-based averaged (0-90) 25.62 24.98
Type-based averaged (0-100) 25.88 25.20
Parallel Type-based averaged (0-90) 25.75 25.04
</table>
<tableCaption confidence="0.99993">
Table 1: Comparisons of BLEU score results
</tableCaption>
<bodyText confidence="0.999993442307692">
nally converges to the same likelihood result as
non-parallel type-based MCMC sampling. How-
ever, when we use more processors, the likelihood
eventually becomes lower than with non-parallel
sampling. This is because when we increase the
number of processors, we split the dataset into
very small subsets. As we maintain the bookkeep-
ing for each subset separately and do not com-
municate the updates to each subset, the power of
type-based sampling is weakened with bookkeep-
ing for very few sites of each type. In the extreme
case, when we use too many processors in parallel,
the bookkeeping would have a singleton site for
each type. In this case, the approximation would
degrade to the scenario of approximating token-
based sampling. By choosing a proper size of divi-
sion of the dataset and by maintaining local book-
keeping for each subset, the parallel approxima-
tion can converge to almost the same point as non-
parallel sampling. As shown in our experimental
results, the speed up is very significant with the
running time decreasing from thirty minutes per
iteration to just seven minutes when running on
five processors. Part of the speed up comes from
the smaller bookkeeping since with fewer sites for
each index, there is less mismatch or conflict of
sites.
Table 1 shows the BLEU score results for type-
based MCMC and the baseline. For non-type-
based top-down sampling, the best BLEU score re-
sult on dev is achieved when averaging the gram-
mars of every 10th iteration from the 0th to the
90th iteration, while our type-based method gets
the best result by averaging over every 10th itera-
tion from the 0th to the 100th iteration. We can see
that the BLEU score on dev for type-based MCMC
and the corresponding BLEU score on test are
both better than the result for the non-type-based
method, though not significantly. This shows that
the better likelihood of our Markov Chain using
type-based MCMC does result in better transla-
tion.
We have also done experiments calculating the
BLEU score result of the inexact parallel imple-
mentation. We can see from Table 1 that, while the
likelihood of the approximation does not change
in comparison with the exact type-based MCMC,
there is a gap between the BLEU score results. We
think this difference might come from the incon-
sistency of the grammars sampled by each proces-
sor within each iteration, as they do not communi-
cate the update within each iteration.
</bodyText>
<sectionHeader confidence="0.999363" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999997590909091">
We presented a novel type-based MCMC algo-
rithm for sampling tree fragments from phrase de-
composition forests. While the hidden tree struc-
ture in our settings makes it difficult to maintain
the constantly changing type information, we have
come up with a compact way to store the type in-
formation of variables and proposed efficient ways
to update the bookkeeping index. Under the addi-
tional hidden structure limitation, we have shown
that type-based MCMC sampling still works and
results in both better likelihood and BLEU score.
We also came with techniques to speed up the
type-based MCMC sampling schedule while not
affecting the final sampling likelihood result. A re-
maining issue with parallelization is the inconsis-
tency of the grammar within an iteration between
processors. One possible solution would be using
better averaging methods instead of simply aver-
aging over every few iterations. Another interest-
ing extension for our methods would be to also de-
fine types for the edge variables, and then sample
both cut and edge types jointly.
</bodyText>
<sectionHeader confidence="0.998436" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999992333333333">
We gratefully acknowledge the assistance of
Licheng Fang and Tagyoung Chung. This work
was partially funded by NSF grant IIS-0910611.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999614923076923">
Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles Os-
borne. 2009. A Gibbs sampler for phrasal syn-
chronous grammar induction. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP: Vol-
ume 2, pages 782–790, Singapore.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201–228.
Tagyoung Chung, Licheng Fang, Daniel Gildea, and
Daniel ˇStefankoviˇc. 2014. Sampling tree fragments
from forests. Computational Linguistics, 40:203–
229.
</reference>
<page confidence="0.858055">
1744
</page>
<reference confidence="0.999883594936709">
Trevor Cohn and Phil Blunsom. 2010. Blocked in-
ference in Bayesian tree substitution grammars. In
Proceedings of the 48th Annual Meeting of the As-
sociation for Computational Linguistics (ACL-10),
pages 225–230, Uppsala, Sweden.
Trevor Cohn, Sharon Goldwater, and Phil Blun-
som. 2009. Inducing compact but accurate tree-
substitution grammars. In Proceedings of Human
Language Technologies: The 2009 Annual Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics, pages 548–556,
Boulder, Colorado, June. Association for Computa-
tional Linguistics.
John DeNero, Alexandre Bouchard-Cote, and Dan
Klein. 2008. Sampling alignment structure under
a Bayesian translation model. In Proceedings of
EMNLP, pages 314–323, Honolulu, HI.
Yang Feng and Trevor Cohn. 2013. A markov
model of machine translation using non-parametric
bayesian inference. In Proceedings of the 51st An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 333–
342, Sofia, Bulgaria, August. Association for Com-
putational Linguistics.
Mark Hopkins and Greg Langmead. 2010. SCFG de-
coding without binarization. In Proceedings of the
2010 Conference on Empirical Methods in Natural
Language Processing, pages 646–655, Cambridge,
MA, October.
Mark Hopkins and Jonathan May. 2011. Tuning as
ranking. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1352–1362, Edinburgh, Scotland, UK.,
July.
Songfang Huang and Steve Renals. 2010. Power law
discounting for n-gram language models. In Proc.
IEEE International Conference on Acoustic, Speech,
and Signal Processing (ICASSP’10), pages 5178–
5181, Dallas, Texas, USA.
Scott Kirkpatrick, C. D. Gelatt, Jr., and Mario P. Vec-
chi. 1983. Optimization by Simulated Annealing.
Science, 220(4598):671–680.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of NAACL-03, pages 48–54, Edmonton,
Alberta.
Abby Levenberg, Chris Dyer, and Phil Blunsom. 2012.
A Bayesian model for learning SCFGs with discon-
tiguous rules. In Proceedings of the 2012 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, pages 223–232, Jeju Island, Korea.
Percy Liang, Michael I Jordan, and Dan Klein. 2010.
Type-based mcmc. In Human Language Technolo-
gies: The 2010 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 573–581. Association for
Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evaluation of machine translation. In Proceedings
ofACL-02, pages 311–318, Philadelphia, PA.
Jim Pitman and Marc Yor. 1997. The two-parameter
Poisson-Dirichlet distribution derived from a stable
subordinator. Annals of Probability, 25(2):855–900.
Matt Post and Daniel Gildea. 2009. Bayesian learning
of a tree substitution grammar. In Proc. Association
for Computational Linguistics (short paper), pages
45–48, Singapore.
Elif Yamangil and Stuart M Shieber. 2013. Non-
parametric bayesian inference and efficient parsing
for tree-adjoining grammars. In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics. Association of Computational
Linguistics.
Hao Zhang, Daniel Gildea, and David Chiang. 2008.
Extracting synchronous grammar rules from word-
level alignments in linear time. In COLING-08,
pages 1081–1088, Manchester, UK.
</reference>
<page confidence="0.991719">
1745
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.816780">
<title confidence="0.997708">Type-based MCMC for Sampling Tree Fragments from Forests</title>
<author confidence="0.982361">Peng</author>
<affiliation confidence="0.9998">Department of Computer University of</affiliation>
<address confidence="0.999061">Rochester, NY 14627</address>
<abstract confidence="0.991010894736842">This paper applies type-based Markov Chain Monte Carlo (MCMC) algorithms to the problem of learning Synchronous Context-Free Grammar (SCFG) rules from a forest that represents all possible rules consistent with a fixed word alignment. While type-based MCMC has been shown to be effective in a number of NLP applications, our setting, where the tree structure of the sentence is itself a hidden variable, presents a number of challenges to type-based inference. We describe methods for defining variable types and efficiently indexing variables in order to overcome these challenges. These methods lead to improvements in both log likelihood and BLEU score in our experiments.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Phil Blunsom</author>
<author>Trevor Cohn</author>
<author>Chris Dyer</author>
<author>Miles Osborne</author>
</authors>
<title>A Gibbs sampler for phrasal synchronous grammar induction.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP:</booktitle>
<volume>2</volume>
<pages>782--790</pages>
<contexts>
<context position="1799" citStr="Blunsom et al., 2009" startWordPosition="292" endWordPosition="295">lit point, which we refer to as a cut, between two rules. The problem of extracting machine translation rules from word-aligned bitext is a similar problem in that we wish to automatically learn the best granularity for the rules with which to analyze each sentence. The problem of rule extraction is more complex, however, because the tree structure of the sentence is also unknown. In machine translation applications, most previous work on joint alignment and rule extraction models uses heuristic methods to extract rules from learned word alignment or bracketing structures (Zhang et al., 2008; Blunsom et al., 2009; DeNero et al., 2008; Levenberg et al., 2012). Chung et al. (2014) present a MCMC algorithm schedule to learn Hiero-style SCFG rules (Chiang, 2007) by sampling tree fragments from phrase decomposition forests, which represent all possible rules that are consistent with a set of fixed word alignments. Assuming fixed word alignments reduces the complexity of the sampling problem, and has generally been effective in most stateof-the-art machine translation systems. The algorithm for sampling rules from a forest is as follows: from the root of the phrase decomposition forest, one samples a cut va</context>
<context position="27925" citStr="Blunsom et al. (2009)" startWordPosition="5251" endWordPosition="5254">r each posterior of cut variables but directly samples the real m. In our experiment, this one-stage sampling strategy gives us a 1.5 times overall speed up in comparison with the two-stage sampling schedule of Liang et al. (2010). 4.2 Parallel Implementation As our type-based sampler involves tedious bookkeeping and frequent conflict checking and mismatch of cut types, one iteration of the type-based sampler is slower than an iteration of the tokenbased sampler when run on a single processor. In order to speed up our sampling procedure, we used a parallel sampling strategy similar to that of Blunsom et al. (2009) and Feng and Cohn (2013), who use multiple processors to perform inexact Gibbs Sampling, and find equivalent performance in comparison with an exact Gibbs Sampler with significant speed up. In our application, we split the data into several subsets and assign each subset to a processor. Each processor performs typebased sampling on its subset using local counts and local bookkeeping, and communicates the update of the local counts after each iteration. All the updates are then aggregated to generate global counts and then we refresh the local counts of each processor. We do not communicate th</context>
</contexts>
<marker>Blunsom, Cohn, Dyer, Osborne, 2009</marker>
<rawString>Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles Osborne. 2009. A Gibbs sampler for phrasal synchronous grammar induction. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 2, pages 782–790, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="1947" citStr="Chiang, 2007" startWordPosition="318" endWordPosition="319"> in that we wish to automatically learn the best granularity for the rules with which to analyze each sentence. The problem of rule extraction is more complex, however, because the tree structure of the sentence is also unknown. In machine translation applications, most previous work on joint alignment and rule extraction models uses heuristic methods to extract rules from learned word alignment or bracketing structures (Zhang et al., 2008; Blunsom et al., 2009; DeNero et al., 2008; Levenberg et al., 2012). Chung et al. (2014) present a MCMC algorithm schedule to learn Hiero-style SCFG rules (Chiang, 2007) by sampling tree fragments from phrase decomposition forests, which represent all possible rules that are consistent with a set of fixed word alignments. Assuming fixed word alignments reduces the complexity of the sampling problem, and has generally been effective in most stateof-the-art machine translation systems. The algorithm for sampling rules from a forest is as follows: from the root of the phrase decomposition forest, one samples a cut variable, denoting whether the current node is a cut, and an edge variable, denoting which incoming hyperedge is chosen, at each node of the current t</context>
<context position="30487" citStr="Chiang, 2007" startWordPosition="5657" endWordPosition="5658">ch sampling schedule and calculated BLEU (Papineni et al., 2002) for evaluation. 5.1 Experiment Settings We use the top-down token-based sampling algorithm of Chung et al. (2014) as our baseline. We use the same SCFG decoder for translation with both the baseline and the grammars sampled using our type-based MCMC sampler. The features included in our experiments are differently normalized rule counts and lexical weightings (Koehn et al., 2003) of each rule. Weights are tuned using Pairwise Ranking Optimization (Hopkins and May, 2011) using a grammar extracted by the standard heuristic method (Chiang, 2007) and the development set. The same weights are used throughout our experiments. First we want to compare the DP likelihood of the baseline with our type-based MCMC sampler to see if type-based sampling would converge to a better sampling result. In order to verify if type-based MCMC really converges to a good optimum point, we use simulated annealing (Kirkpatrick et al., 1983) to search possible better optimum points. We sample from the real distribution modified by an annealing parameter Q: z ∼ P(z)β We increase our Q from 0.1 to 1.3, and then decrease from 1.3 to 1.0, changing by 0.1 every 3</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2):201–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tagyoung Chung</author>
<author>Licheng Fang</author>
<author>Daniel Gildea</author>
<author>Daniel ˇStefankoviˇc</author>
</authors>
<title>Sampling tree fragments from forests. Computational Linguistics,</title>
<date>2014</date>
<pages>40--203</pages>
<marker>Chung, Fang, Gildea, ˇStefankoviˇc, 2014</marker>
<rawString>Tagyoung Chung, Licheng Fang, Daniel Gildea, and Daniel ˇStefankoviˇc. 2014. Sampling tree fragments from forests. Computational Linguistics, 40:203– 229.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Cohn</author>
<author>Phil Blunsom</author>
</authors>
<title>Blocked inference in Bayesian tree substitution grammars.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL-10),</booktitle>
<pages>225--230</pages>
<location>Uppsala,</location>
<contexts>
<context position="2982" citStr="Cohn and Blunsom (2010)" startWordPosition="493" endWordPosition="496">composition forest, one samples a cut variable, denoting whether the current node is a cut, and an edge variable, denoting which incoming hyperedge is chosen, at each node of the current tree in a top-down manner. This sampling schedule is efficient in that it only samples the current tree and will not waste time on updating variables that are unlikely to be used in any tree. As with many other token-based Gibbs Sampling applications, sampling one node at a time can result in slow mixing due to the strong coupling between variables. One general remedy is to sample blocks of coupled variables. Cohn and Blunsom (2010) and Yamangil and Shieber (2013) used blocked sampling algorithms that sample the whole tree structure associated with one sentence at a time for TSG and TAG learning. However, this kind of blocking does not deal with the coupling of variables correlated with the same type of structure across sentences. Liang et al. (2010) introduced a type-based sampling schedule which updates a block of variables of the same type jointly. The type of a variable is defined as the combination of new structural choices added when assigning different values to the variable. Type-based MCMC tackles the coupling i</context>
</contexts>
<marker>Cohn, Blunsom, 2010</marker>
<rawString>Trevor Cohn and Phil Blunsom. 2010. Blocked inference in Bayesian tree substitution grammars. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL-10), pages 225–230, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Cohn</author>
<author>Sharon Goldwater</author>
<author>Phil Blunsom</author>
</authors>
<title>Inducing compact but accurate treesubstitution grammars.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>548--556</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Boulder, Colorado,</location>
<contexts>
<context position="1026" citStr="Cohn et al., 2009" startWordPosition="157" endWordPosition="160"> While type-based MCMC has been shown to be effective in a number of NLP applications, our setting, where the tree structure of the sentence is itself a hidden variable, presents a number of challenges to type-based inference. We describe methods for defining variable types and efficiently indexing variables in order to overcome these challenges. These methods lead to improvements in both log likelihood and BLEU score in our experiments. 1 Introduction In previous work, sampling methods have been used to learn Tree Substitution Grammar (TSG) rules from derivation trees (Post and Gildea, 2009; Cohn et al., 2009) for TSG learning. Here, at each node in the derivation tree, there is a binary variable indicating whether the node is internal to a TSG rule or is a split point, which we refer to as a cut, between two rules. The problem of extracting machine translation rules from word-aligned bitext is a similar problem in that we wish to automatically learn the best granularity for the rules with which to analyze each sentence. The problem of rule extraction is more complex, however, because the tree structure of the sentence is also unknown. In machine translation applications, most previous work on join</context>
</contexts>
<marker>Cohn, Goldwater, Blunsom, 2009</marker>
<rawString>Trevor Cohn, Sharon Goldwater, and Phil Blunsom. 2009. Inducing compact but accurate treesubstitution grammars. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 548–556, Boulder, Colorado, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John DeNero</author>
<author>Alexandre Bouchard-Cote</author>
<author>Dan Klein</author>
</authors>
<title>Sampling alignment structure under a Bayesian translation model.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>314--323</pages>
<location>Honolulu, HI.</location>
<contexts>
<context position="1820" citStr="DeNero et al., 2008" startWordPosition="296" endWordPosition="299">fer to as a cut, between two rules. The problem of extracting machine translation rules from word-aligned bitext is a similar problem in that we wish to automatically learn the best granularity for the rules with which to analyze each sentence. The problem of rule extraction is more complex, however, because the tree structure of the sentence is also unknown. In machine translation applications, most previous work on joint alignment and rule extraction models uses heuristic methods to extract rules from learned word alignment or bracketing structures (Zhang et al., 2008; Blunsom et al., 2009; DeNero et al., 2008; Levenberg et al., 2012). Chung et al. (2014) present a MCMC algorithm schedule to learn Hiero-style SCFG rules (Chiang, 2007) by sampling tree fragments from phrase decomposition forests, which represent all possible rules that are consistent with a set of fixed word alignments. Assuming fixed word alignments reduces the complexity of the sampling problem, and has generally been effective in most stateof-the-art machine translation systems. The algorithm for sampling rules from a forest is as follows: from the root of the phrase decomposition forest, one samples a cut variable, denoting whet</context>
</contexts>
<marker>DeNero, Bouchard-Cote, Klein, 2008</marker>
<rawString>John DeNero, Alexandre Bouchard-Cote, and Dan Klein. 2008. Sampling alignment structure under a Bayesian translation model. In Proceedings of EMNLP, pages 314–323, Honolulu, HI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Feng</author>
<author>Trevor Cohn</author>
</authors>
<title>A markov model of machine translation using non-parametric bayesian inference.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>333--342</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="27950" citStr="Feng and Cohn (2013)" startWordPosition="5256" endWordPosition="5259">riables but directly samples the real m. In our experiment, this one-stage sampling strategy gives us a 1.5 times overall speed up in comparison with the two-stage sampling schedule of Liang et al. (2010). 4.2 Parallel Implementation As our type-based sampler involves tedious bookkeeping and frequent conflict checking and mismatch of cut types, one iteration of the type-based sampler is slower than an iteration of the tokenbased sampler when run on a single processor. In order to speed up our sampling procedure, we used a parallel sampling strategy similar to that of Blunsom et al. (2009) and Feng and Cohn (2013), who use multiple processors to perform inexact Gibbs Sampling, and find equivalent performance in comparison with an exact Gibbs Sampler with significant speed up. In our application, we split the data into several subsets and assign each subset to a processor. Each processor performs typebased sampling on its subset using local counts and local bookkeeping, and communicates the update of the local counts after each iteration. All the updates are then aggregated to generate global counts and then we refresh the local counts of each processor. We do not communicate the update on the bookkeepi</context>
</contexts>
<marker>Feng, Cohn, 2013</marker>
<rawString>Yang Feng and Trevor Cohn. 2013. A markov model of machine translation using non-parametric bayesian inference. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 333– 342, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hopkins</author>
<author>Greg Langmead</author>
</authors>
<title>SCFG decoding without binarization.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>646--655</pages>
<location>Cambridge, MA,</location>
<contexts>
<context position="33185" citStr="Hopkins and Langmead, 2010" startWordPosition="6130" endWordPosition="6133">e tune the number of grammars included for averaging by comparing the BLEU score on the dev set and report the BLEU score result on the test with the same averaging of grammars. As each tree fragment sampled from the forest represents a unique translation rule, we do not need to explicitly extract the rules; we merely need to collect them and count them. However, the fragments sampled include purely non-lexical rules that do not conform to the rule constraints of Hiero, and rules that are not useful for translation. In order to get rid of this type of rule, we prune every rule that has scope (Hopkins and Langmead, 2010) greater than two. Whereas Hiero does not allow two adjacent nonterminals in the source side, our pruning criterion allows some rules of scope two that are not allowed by Hiero. For example, the following rule (only source side shown) has scope two but is not allowed by Hiero: CD E&apos; 0-n C CL Co UQ �, t C 0 c�&apos;o X → w1X1X2w2X3 5.2 Experiment Results Figure 4 shows the log likelihood result of our type-based MCMC sampling schedule and the baseline top-down sampling. We can see that typebased sampling converges to a much better result than non-type-based top-down sampling. This shows that type-ba</context>
</contexts>
<marker>Hopkins, Langmead, 2010</marker>
<rawString>Mark Hopkins and Greg Langmead. 2010. SCFG decoding without binarization. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 646–655, Cambridge, MA, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hopkins</author>
<author>Jonathan May</author>
</authors>
<title>Tuning as ranking.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1352--1362</pages>
<location>Edinburgh, Scotland, UK.,</location>
<contexts>
<context position="30413" citStr="Hopkins and May, 2011" startWordPosition="5643" endWordPosition="5647">ents. We plotted the log likelihood graph to compare the convergence property of each sampling schedule and calculated BLEU (Papineni et al., 2002) for evaluation. 5.1 Experiment Settings We use the top-down token-based sampling algorithm of Chung et al. (2014) as our baseline. We use the same SCFG decoder for translation with both the baseline and the grammars sampled using our type-based MCMC sampler. The features included in our experiments are differently normalized rule counts and lexical weightings (Koehn et al., 2003) of each rule. Weights are tuned using Pairwise Ranking Optimization (Hopkins and May, 2011) using a grammar extracted by the standard heuristic method (Chiang, 2007) and the development set. The same weights are used throughout our experiments. First we want to compare the DP likelihood of the baseline with our type-based MCMC sampler to see if type-based sampling would converge to a better sampling result. In order to verify if type-based MCMC really converges to a good optimum point, we use simulated annealing (Kirkpatrick et al., 1983) to search possible better optimum points. We sample from the real distribution modified by an annealing parameter Q: z ∼ P(z)β We increase our Q f</context>
</contexts>
<marker>Hopkins, May, 2011</marker>
<rawString>Mark Hopkins and Jonathan May. 2011. Tuning as ranking. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1352–1362, Edinburgh, Scotland, UK., July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Songfang Huang</author>
<author>Steve Renals</author>
</authors>
<title>Power law discounting for n-gram language models.</title>
<date>2010</date>
<booktitle>In Proc. IEEE International Conference on Acoustic, Speech, and Signal Processing (ICASSP’10),</booktitle>
<pages>5178--5181</pages>
<location>Dallas, Texas, USA.</location>
<contexts>
<context position="14414" citStr="Huang and Renals, 2010" startWordPosition="2633" endWordPosition="2636">ty: NR(r) − Trd + (Tld + α)P0(r) P(ri = r|N) = (5) NL(l) + α here for the case of PYP, I = {R, L}. We have an additional index L for the category of rule length counts, and NL(l) is the total number of rules of length l observed in z−i. Tr is the number of tables labeled with r in z−i. The length of the rule is drawn from a Poisson distribution, so a rule length probability P(l; A) = λle−λ l! is multiplied by this probability to calculate the real posterior probability for each rule. In order to simplify the tedious book-keeping, we estimate the number of tables using the following equations (Huang and Renals, 2010): Tr = NR(r)d (6) �Tl = NR(r)d (7) r:|r|=l We use the top-down sampling algorithm of Chung et al. (2014) (see Algorithm 1). Starting from the root of the forest, we sample a value for the edge variable denoting which incoming hyperedge of the node is turned on in the current tree, and then we sample a cut value for the node denoting whether the node is a split point between two fragments in the tree. For each node n, we denote the composed rule type that we get when we set the cut of node n to 0 as r1 and the two split rule types that we get when we set the cut to 1 as r2, r3. We sample the cu</context>
</contexts>
<marker>Huang, Renals, 2010</marker>
<rawString>Songfang Huang and Steve Renals. 2010. Power law discounting for n-gram language models. In Proc. IEEE International Conference on Acoustic, Speech, and Signal Processing (ICASSP’10), pages 5178– 5181, Dallas, Texas, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Kirkpatrick</author>
<author>C D Gelatt</author>
<author>Mario P Vecchi</author>
</authors>
<date>1983</date>
<journal>Optimization by Simulated Annealing. Science,</journal>
<volume>220</volume>
<issue>4598</issue>
<contexts>
<context position="30866" citStr="Kirkpatrick et al., 1983" startWordPosition="5719" endWordPosition="5723">e differently normalized rule counts and lexical weightings (Koehn et al., 2003) of each rule. Weights are tuned using Pairwise Ranking Optimization (Hopkins and May, 2011) using a grammar extracted by the standard heuristic method (Chiang, 2007) and the development set. The same weights are used throughout our experiments. First we want to compare the DP likelihood of the baseline with our type-based MCMC sampler to see if type-based sampling would converge to a better sampling result. In order to verify if type-based MCMC really converges to a good optimum point, we use simulated annealing (Kirkpatrick et al., 1983) to search possible better optimum points. We sample from the real distribution modified by an annealing parameter Q: z ∼ P(z)β We increase our Q from 0.1 to 1.3, and then decrease from 1.3 to 1.0, changing by 0.1 every 3 iterations. We also run an inexact parallel approximation of type-based MCMC in comparison with the non-parallel sampling to find out if parallel programming is feasible to speed up typebased MCMC sampling without affecting the performance greatly. We do not compare the PYP likelihood because the approximation renders it impossible to calculate the real PYP likelihood. We als</context>
</contexts>
<marker>Kirkpatrick, Gelatt, Vecchi, 1983</marker>
<rawString>Scott Kirkpatrick, C. D. Gelatt, Jr., and Mario P. Vecchi. 1983. Optimization by Simulated Annealing. Science, 220(4598):671–680.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of NAACL-03,</booktitle>
<pages>48--54</pages>
<location>Edmonton, Alberta.</location>
<contexts>
<context position="7459" citStr="Koehn et al. (2003)" startWordPosition="1260" endWordPosition="1263">) positions in each of the two languages. This complexity leads us to explore sampling algorithms instead of using dynamic programming. A span [i, j] is a set of contiguous word indices {i, i + 1,...,j − 11. Given an aligned Chinese-English sentence pair, a phrase n is a pair of spans n = ([i1, j1], [i2, j2]) such that Chinese words in positions [i1,j1] are aligned only to English words in positions [i2, j2], and vice versa. A phrase forest H = (V, E) is a hypergraph made of a set of hypernodes V and a set of hyperedges E. Each node n = ([i1, j1], [i2, j2]) E V is a tight phrase as defined by Koehn et al. (2003), i.e., a phrase containing no unaligned words at its boundaries. A phrase n = ([i1, j1], [i2, j2]) covers n0 = ([i01,j01], [i02,j02]) if i1 G i01 n j01 � j1 n i2 G i02 n j02 � j2 Each edge in E, written as T —* n, is made of a 1736 Figure 2: A phrase decomposition forest extracted from the sentence pair A fn �k �q ��, I have a date with her today). Each edge is a minimal SCFG rule, and the rules at the bottom level are phrase pairs. Unaligned word “a” shows up in the rule X —* X1X2, X1aX2 after unaligned words are put back into the alignment matrix. The highlighted portion of the forest shows</context>
<context position="30321" citStr="Koehn et al., 2003" startWordPosition="5629" endWordPosition="5632">have sentences with less than 30 words. A trigram language model was used for all experiments. We plotted the log likelihood graph to compare the convergence property of each sampling schedule and calculated BLEU (Papineni et al., 2002) for evaluation. 5.1 Experiment Settings We use the top-down token-based sampling algorithm of Chung et al. (2014) as our baseline. We use the same SCFG decoder for translation with both the baseline and the grammars sampled using our type-based MCMC sampler. The features included in our experiments are differently normalized rule counts and lexical weightings (Koehn et al., 2003) of each rule. Weights are tuned using Pairwise Ranking Optimization (Hopkins and May, 2011) using a grammar extracted by the standard heuristic method (Chiang, 2007) and the development set. The same weights are used throughout our experiments. First we want to compare the DP likelihood of the baseline with our type-based MCMC sampler to see if type-based sampling would converge to a better sampling result. In order to verify if type-based MCMC really converges to a good optimum point, we use simulated annealing (Kirkpatrick et al., 1983) to search possible better optimum points. We sample fr</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of NAACL-03, pages 48–54, Edmonton, Alberta.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Abby Levenberg</author>
<author>Chris Dyer</author>
<author>Phil Blunsom</author>
</authors>
<title>A Bayesian model for learning SCFGs with discontiguous rules.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>223--232</pages>
<location>Jeju Island,</location>
<contexts>
<context position="1845" citStr="Levenberg et al., 2012" startWordPosition="300" endWordPosition="303">een two rules. The problem of extracting machine translation rules from word-aligned bitext is a similar problem in that we wish to automatically learn the best granularity for the rules with which to analyze each sentence. The problem of rule extraction is more complex, however, because the tree structure of the sentence is also unknown. In machine translation applications, most previous work on joint alignment and rule extraction models uses heuristic methods to extract rules from learned word alignment or bracketing structures (Zhang et al., 2008; Blunsom et al., 2009; DeNero et al., 2008; Levenberg et al., 2012). Chung et al. (2014) present a MCMC algorithm schedule to learn Hiero-style SCFG rules (Chiang, 2007) by sampling tree fragments from phrase decomposition forests, which represent all possible rules that are consistent with a set of fixed word alignments. Assuming fixed word alignments reduces the complexity of the sampling problem, and has generally been effective in most stateof-the-art machine translation systems. The algorithm for sampling rules from a forest is as follows: from the root of the phrase decomposition forest, one samples a cut variable, denoting whether the current node is a</context>
</contexts>
<marker>Levenberg, Dyer, Blunsom, 2012</marker>
<rawString>Abby Levenberg, Chris Dyer, and Phil Blunsom. 2012. A Bayesian model for learning SCFGs with discontiguous rules. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 223–232, Jeju Island, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Michael I Jordan</author>
<author>Dan Klein</author>
</authors>
<title>Type-based mcmc.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North</booktitle>
<contexts>
<context position="3306" citStr="Liang et al. (2010)" startWordPosition="547" endWordPosition="550">ariables that are unlikely to be used in any tree. As with many other token-based Gibbs Sampling applications, sampling one node at a time can result in slow mixing due to the strong coupling between variables. One general remedy is to sample blocks of coupled variables. Cohn and Blunsom (2010) and Yamangil and Shieber (2013) used blocked sampling algorithms that sample the whole tree structure associated with one sentence at a time for TSG and TAG learning. However, this kind of blocking does not deal with the coupling of variables correlated with the same type of structure across sentences. Liang et al. (2010) introduced a type-based sampling schedule which updates a block of variables of the same type jointly. The type of a variable is defined as the combination of new structural choices added when assigning different values to the variable. Type-based MCMC tackles the coupling issue by assigning the same type to variables that are strongly coupled. In this paper, we follow the phrase decomposition forest construction procedures of Chung et 1735 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1735–1745, October 25-29, 2014, Doha, Qatar. c�2014 </context>
<context position="4696" citStr="Liang et al. (2010)" startWordPosition="782" endWordPosition="785">which samples the variables of the same type jointly. We define the type of the cut variable for each node in our sampling schedule. While type-based MCMC has been proven to be effective in a number of NLP applications, our sample-edge, sample-cut setting is more complicated as our tree structure is unknown. We need additional steps to maintain the cut type information when the tree structure is changed as we sample the edge variable. Like other typebased MCMC applications, we need bookkeeping of node sites to be sampled in order to loop through sites of the same type efficiently. As noted by Liang et al. (2010), indexing by the complete type information is too expensive in some applications like TSG learning. Our setting is different from TSG learning in that the internal structure of each SCFG rule is abstracted away when deriving the rule type from the tree fragment sampled. We make the following contributions: 1. We apply type-based MCMC to the setting of SCFG learning and have achieved better log likelihood and BLEU score result. 2. We present an innovative way of storing the type information by indexing on partial type information and then filtering the retrieved nodes according to the full typ</context>
<context position="27534" citStr="Liang et al. (2010)" startWordPosition="5186" endWordPosition="5189">S|N) = P(zi|Ni−1) (12) i=1 where N, N0, ... , Nn are count tables, and N0 = N. Ni is the new count table after we update Ni−1 according to the assignment of zi. This equation gives us a hint to sample each zi according to P(zi|Ni−1) and then update the count table Ni−1 according to the assignment of zi. This greedy 1741 sampling saves us the effort to calculate each m by multiplying over each posterior of cut variables but directly samples the real m. In our experiment, this one-stage sampling strategy gives us a 1.5 times overall speed up in comparison with the two-stage sampling schedule of Liang et al. (2010). 4.2 Parallel Implementation As our type-based sampler involves tedious bookkeeping and frequent conflict checking and mismatch of cut types, one iteration of the type-based sampler is slower than an iteration of the tokenbased sampler when run on a single processor. In order to speed up our sampling procedure, we used a parallel sampling strategy similar to that of Blunsom et al. (2009) and Feng and Cohn (2013), who use multiple processors to perform inexact Gibbs Sampling, and find equivalent performance in comparison with an exact Gibbs Sampler with significant speed up. In our application</context>
</contexts>
<marker>Liang, Jordan, Klein, 2010</marker>
<rawString>Percy Liang, Michael I Jordan, and Dan Klein. 2010. Type-based mcmc. In Human Language Technologies: The 2010 Annual Conference of the North</rawString>
</citation>
<citation valid="false">
<title>American Chapter of the Association for Computational Linguistics,</title>
<pages>573--581</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker></marker>
<rawString>American Chapter of the Association for Computational Linguistics, pages 573–581. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: A method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings ofACL-02,</booktitle>
<pages>311--318</pages>
<location>Philadelphia, PA.</location>
<contexts>
<context position="29938" citStr="Papineni et al., 2002" startWordPosition="5566" endWordPosition="5569">18, LDC2002L27, LDC2003E07, LDC2003E14, LDC2004T08, LDC2005T06, LDC2005T10, LDC2005T34, LDC2006E26, LDC2005E83, LDC2006E34, LDC2006E85, LDC2006E92, LDC2006E24, LDC2006E92, LDC2006E24) The language model is trained on the English side of entire data (1.65M sentences, which is 39.3M words.) a 428-sentence test set with four references for testing.2 The development set and the test set have sentences with less than 30 words. A trigram language model was used for all experiments. We plotted the log likelihood graph to compare the convergence property of each sampling schedule and calculated BLEU (Papineni et al., 2002) for evaluation. 5.1 Experiment Settings We use the top-down token-based sampling algorithm of Chung et al. (2014) as our baseline. We use the same SCFG decoder for translation with both the baseline and the grammars sampled using our type-based MCMC sampler. The features included in our experiments are differently normalized rule counts and lexical weightings (Koehn et al., 2003) of each rule. Weights are tuned using Pairwise Ranking Optimization (Hopkins and May, 2011) using a grammar extracted by the standard heuristic method (Chiang, 2007) and the development set. The same weights are used</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: A method for automatic evaluation of machine translation. In Proceedings ofACL-02, pages 311–318, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jim Pitman</author>
<author>Marc Yor</author>
</authors>
<title>The two-parameter Poisson-Dirichlet distribution derived from a stable subordinator.</title>
<date>1997</date>
<journal>Annals of Probability,</journal>
<volume>25</volume>
<issue>2</issue>
<contexts>
<context position="13185" citStr="Pitman and Yor, 1997" startWordPosition="2398" endWordPosition="2401">ifferent categories of counts in z−i. I is an index set for different categories of counts. Each NC is a vector of counts for category C. We have P(ri = r|z−i) = P(ri = r|N). If we marginalize over tables labeled with the same rule, we get the following probability over rule r given the previous count table N: NR(r) + αP0(r) P(ri = r|N) = (4) n + α here in the case of DP, I = {R}, where R is the index for the category of rule counts. NR(r) is the number of times that rule r has been observed in z−i, n = Er NR(r) is the total number of rules observed. We also define a Pitman-Yor Process (PYP) (Pitman and Yor, 1997) over rules of each length l. We draw the rule distribution G from a PYP, and then rules of length l are drawn from G. G|α, d, P0 ∼ PY (α, d, P0) r|G ∼ G The first two parameters, a concentration parameter α and a discount parameter d, control the shape of distribution G by controlling the size and the Algorithm 1 Top-down Sampling Algorithm 1: queue.push(root) 2: while queue is not empty do 3: n = queue.pop() 4: SAMPLEEDGE(n) 5: SAMPLECUT(n) 6: for each child c of node n do 7: queue.push(c) 8: end for 9: end while number of clusters. Integrating over G, we have the following PYP posterior pro</context>
</contexts>
<marker>Pitman, Yor, 1997</marker>
<rawString>Jim Pitman and Marc Yor. 1997. The two-parameter Poisson-Dirichlet distribution derived from a stable subordinator. Annals of Probability, 25(2):855–900.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matt Post</author>
<author>Daniel Gildea</author>
</authors>
<title>Bayesian learning of a tree substitution grammar.</title>
<date>2009</date>
<booktitle>In Proc. Association for Computational Linguistics (short paper),</booktitle>
<pages>45--48</pages>
<contexts>
<context position="1006" citStr="Post and Gildea, 2009" startWordPosition="153" endWordPosition="156">a fixed word alignment. While type-based MCMC has been shown to be effective in a number of NLP applications, our setting, where the tree structure of the sentence is itself a hidden variable, presents a number of challenges to type-based inference. We describe methods for defining variable types and efficiently indexing variables in order to overcome these challenges. These methods lead to improvements in both log likelihood and BLEU score in our experiments. 1 Introduction In previous work, sampling methods have been used to learn Tree Substitution Grammar (TSG) rules from derivation trees (Post and Gildea, 2009; Cohn et al., 2009) for TSG learning. Here, at each node in the derivation tree, there is a binary variable indicating whether the node is internal to a TSG rule or is a split point, which we refer to as a cut, between two rules. The problem of extracting machine translation rules from word-aligned bitext is a similar problem in that we wish to automatically learn the best granularity for the rules with which to analyze each sentence. The problem of rule extraction is more complex, however, because the tree structure of the sentence is also unknown. In machine translation applications, most p</context>
</contexts>
<marker>Post, Gildea, 2009</marker>
<rawString>Matt Post and Daniel Gildea. 2009. Bayesian learning of a tree substitution grammar. In Proc. Association for Computational Linguistics (short paper), pages 45–48, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elif Yamangil</author>
<author>Stuart M Shieber</author>
</authors>
<title>Nonparametric bayesian inference and efficient parsing for tree-adjoining grammars.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics. Association of Computational Linguistics.</booktitle>
<contexts>
<context position="3014" citStr="Yamangil and Shieber (2013)" startWordPosition="498" endWordPosition="501">les a cut variable, denoting whether the current node is a cut, and an edge variable, denoting which incoming hyperedge is chosen, at each node of the current tree in a top-down manner. This sampling schedule is efficient in that it only samples the current tree and will not waste time on updating variables that are unlikely to be used in any tree. As with many other token-based Gibbs Sampling applications, sampling one node at a time can result in slow mixing due to the strong coupling between variables. One general remedy is to sample blocks of coupled variables. Cohn and Blunsom (2010) and Yamangil and Shieber (2013) used blocked sampling algorithms that sample the whole tree structure associated with one sentence at a time for TSG and TAG learning. However, this kind of blocking does not deal with the coupling of variables correlated with the same type of structure across sentences. Liang et al. (2010) introduced a type-based sampling schedule which updates a block of variables of the same type jointly. The type of a variable is defined as the combination of new structural choices added when assigning different values to the variable. Type-based MCMC tackles the coupling issue by assigning the same type </context>
</contexts>
<marker>Yamangil, Shieber, 2013</marker>
<rawString>Elif Yamangil and Stuart M Shieber. 2013. Nonparametric bayesian inference and efficient parsing for tree-adjoining grammars. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics. Association of Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hao Zhang</author>
<author>Daniel Gildea</author>
<author>David Chiang</author>
</authors>
<title>Extracting synchronous grammar rules from wordlevel alignments in linear time.</title>
<date>2008</date>
<booktitle>In COLING-08,</booktitle>
<pages>1081--1088</pages>
<location>Manchester, UK.</location>
<contexts>
<context position="1777" citStr="Zhang et al., 2008" startWordPosition="288" endWordPosition="291"> TSG rule or is a split point, which we refer to as a cut, between two rules. The problem of extracting machine translation rules from word-aligned bitext is a similar problem in that we wish to automatically learn the best granularity for the rules with which to analyze each sentence. The problem of rule extraction is more complex, however, because the tree structure of the sentence is also unknown. In machine translation applications, most previous work on joint alignment and rule extraction models uses heuristic methods to extract rules from learned word alignment or bracketing structures (Zhang et al., 2008; Blunsom et al., 2009; DeNero et al., 2008; Levenberg et al., 2012). Chung et al. (2014) present a MCMC algorithm schedule to learn Hiero-style SCFG rules (Chiang, 2007) by sampling tree fragments from phrase decomposition forests, which represent all possible rules that are consistent with a set of fixed word alignments. Assuming fixed word alignments reduces the complexity of the sampling problem, and has generally been effective in most stateof-the-art machine translation systems. The algorithm for sampling rules from a forest is as follows: from the root of the phrase decomposition forest</context>
</contexts>
<marker>Zhang, Gildea, Chiang, 2008</marker>
<rawString>Hao Zhang, Daniel Gildea, and David Chiang. 2008. Extracting synchronous grammar rules from wordlevel alignments in linear time. In COLING-08, pages 1081–1088, Manchester, UK.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
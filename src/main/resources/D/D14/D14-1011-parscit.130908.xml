<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000004">
<title confidence="0.9947495">
Accurate Word Segmentation and POS Tagging for Japanese Microblogs:
Corpus Annotation and Joint Modeling with Lexical Normalization
</title>
<author confidence="0.982581">
Nobuhiro Kaji*† and Masaru Kitsuregawa†$
</author>
<affiliation confidence="0.993028666666667">
*National Institute of Information and Communications Technology
†Institute of Industrial Science, The University of Tokyo
$National Institute of Informatics
</affiliation>
<email confidence="0.997669">
{kaji, kitsure}@tkl.iis.u-tokyo.ac.jp
</email>
<sectionHeader confidence="0.99386" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999772411764706">
Microblogs have recently received
widespread interest from NLP re-
searchers. However, current tools for
Japanese word segmentation and POS
tagging still perform poorly on microblog
texts. We developed an annotated corpus
and proposed a joint model for over-
coming this situation. Our annotated
corpus of microblog texts enables not
only training of accurate statistical models
but also quantitative evaluation of their
performance. Our joint model with lexical
normalization handles the orthographic
diversity of microblog texts. We con-
ducted an experiment to demonstrate
that the corpus and model substantially
contribute to boosting accuracy.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999603647058824">
Microblogs, such as Twitter1 and Weibo2, have re-
cently become an important target of NLP tech-
nology. Since microblogs offer an instant way of
posting textual messages, they have been given
increasing attention as valuable sources for such
actions as mining opinions (Jiang et al., 2011)
and detecting sudden events such as earthquake
(Sakaki et al., 2010).
However, many studies have reported that cur-
rent NLP tools do not perform well on microblog
texts (Foster et al., 2011; Gimpel et al., 2011). In
the case of Japanese text processing, the most se-
rious problem is poor accuracy of word segmen-
tation and POS tagging. Since these two tasks
are positioned as the fundamental step in the text
processing pipeline, their accuracy is vital for all
downstream applications.
</bodyText>
<footnote confidence="0.9999715">
1https://twitter.com
2https://www.weibo.com
</footnote>
<subsectionHeader confidence="0.998162">
1.1 Development of annotated corpus
</subsectionHeader>
<bodyText confidence="0.999980217391304">
The main obstacle that makes word segmentation
and POS tagging in the microblog domain chal-
lenging is the lack of annotated corpora. Because
current annotated corpora are from other domains,
such as news articles, it is difficult to train models
that perform well on microblog texts. Moreover,
system performance cannot be evaluated quantita-
tively.
We remedied this situation by developing an an-
notated corpus of Japanese microblogs. We col-
lected 1831 sentences from Twitter and manually
annotated these sentences with word boundaries,
POS tags, and normalized forms of words (c.f.,
Section 1.2).
We, for the first time, present a comprehen-
sive empirical study of Japanese word segmenta-
tion and POS tagging on microblog texts by us-
ing this corpus. Specifically, we investigated how
well current models trained on existing corpora
perform in the microblog domain. We also ex-
plored performance gains achieved by using our
corpus for training, and by jointly performing lex-
ical normalization (c.f., Section 1.2).
</bodyText>
<subsectionHeader confidence="0.945602">
1.2 Joint modeling with lexical normalization
</subsectionHeader>
<bodyText confidence="0.9999562">
Orthographic diversity in microblog texts causes a
problem when training a statistical model for word
segmentation and POS tagging. Microblog texts
frequently contain informal words that are spelled
in a non-standard manner, e.g., “oredi (already)”,
“b4 (before)”, and “talkin (talking)” (Han and
Baldwin, 2011). Such words, hereafter referred
to as ill-spelled words, are so productive that they
considerably increase the vocabulary size. This
makes training of statistical models difficult.
We address this problem by jointly conducting
lexical normalization. Although a wide variety
of ill-spelled words are used in microblog texts,
many can be normalized into well-spelled equiva-
lents, which conform to standard rules of spelling.
</bodyText>
<page confidence="0.983192">
99
</page>
<note confidence="0.9105">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 99–109,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.9999211">
A joint model with lexical normalization is able
to handle orthographic diversity by exploiting in-
formation obtainable from the well-spelled equiv-
alents.
The proposed joint model was empirically eval-
uated on the microblog corpus we developed. Our
experiment demonstrated that the proposed model
can perform word segmentation and POS tag-
ging substantially better than current state-of-the-
art models.
</bodyText>
<subsectionHeader confidence="0.945322">
1.3 Summary
</subsectionHeader>
<bodyText confidence="0.927841">
Contributions of this paper are the following:
</bodyText>
<listItem confidence="0.796031583333333">
• We developed a microblog corpus that en-
ables not only training of accurate models but
also quantitative evaluation for word segmen-
tation and POS tagging in the microblog do-
main.3
• We propose a joint model with lexical nor-
malization for better handling of ortho-
graphic diversity in microblog texts. In par-
ticular, we present a new method of training
the joint model using a partially annotated
corpus (c.f., Section 7.4).
• We, for the first time, present a comprehen-
</listItem>
<bodyText confidence="0.960041875">
sive empirical study of word segmentation
and POS tagging for microblogs. The experi-
mental results demonstrated that both the mi-
croblog corpus and joint model greatly con-
tributes to training accurate models for word
segmentation and POS tagging.
The remainder of this paper is organized as fol-
lows. Section 2 reviews related work. Section 3
discusses the task of lexical normalization and in-
troduces terminology. Section 4 presents our mi-
croblog corpus and results of our corpus analysis.
Section 5 presents an overview of our joint model
with lexical normalization, and Sections 6 and 7
provide details of the model. Section 8 presents
experimental results and discussions, and Section
9 presents concluding remarks.
</bodyText>
<sectionHeader confidence="0.999791" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.9920005">
Researchers have recently developed various mi-
croblog corpora annotated with rich linguistic in-
formation. Gimpel et al. (2011) and Foster et
al. (2011) annotated English microblog posts with
</bodyText>
<footnote confidence="0.565969">
3Please contact the first author for this corpus.
</footnote>
<bodyText confidence="0.99848935">
POS tags. Han and Baldwin (2011) released a mi-
croblog corpus annotated with normalized forms
of words. A Chinese microblog corpus annotated
with word boundaries was developed for SIGHAN
bakeoff (Duan et al., 2012). However, there are
no microblog corpora annotated with word bound-
aries, POS tags, and normalized sentences.
There has been a surge of interest in lexical nor-
malization with the advent of microblogs (Han and
Baldwin, 2011; Liu et al., 2012; Han et al., 2012;
Wang and Ng, 2013; Zhang et al., 2013; Ling et
al., 2013; Yang and Eisenstein, 2013; Wang et al.,
2013). However, these studies did not address en-
hancing word segmentation.
Wang et al. (2013) proposed a method of joint
ill-spelled word recognition and word segmenta-
tion. With their method, informal spellings are
merely recognized and not normalized. Therefore,
they did not investigate how to exploit the infor-
mation obtainable from well-spelled equivalents
to increase word segmentation accuracy.
Some studies also explored integrating the lexi-
cal normalization process into word segmentation
and POS tagging (Ikeda et al., 2009; Sasano et al.,
2013). A strength of our joint model is that it uses
rich character-level and word-level features used
in state-of-the-art models of joint word segmenta-
tion and POS tagging (Kudo et al., 2004; Neubig
et al., 2011; Kaji and Kitsuregawa, 2013). Thanks
to these features, our model performed much bet-
ter than Sasano et al.’s system, which is the only
publicly available system that jointly conducts lex-
ical normalization, in the experiments (see Section
8). Another advantage is that our model can be
trained on a partially annotated corpus. Further-
more, we present a comprehensive evaluation in
terms of precision and recall on our microblog cor-
pus. Such an evaluation has not been conducted in
previous work due to the lack of annotated cor-
pora.4
</bodyText>
<sectionHeader confidence="0.99049" genericHeader="method">
3 Lexical Normalization Task
</sectionHeader>
<bodyText confidence="0.9993578">
This section explains the task of lexical normal-
ization addressed in this paper. Since lexical nor-
malization is a relatively new research topic, there
are no precise definitions of a lexical normaliza-
tion task that are widely accepted by researchers.
</bodyText>
<footnote confidence="0.999488">
4Very recently, Saito et al. (2014) conducted similar em-
pirical evaluation on microblog corpus. However, they used
biased dataset, in which every sentence includes at least one
ill-spelled words.
</footnote>
<page confidence="0.992485">
100
</page>
<tableCaption confidence="0.8836425">
Table 1: Examples of our target ill-spelled words
and their well-spelled equivalents. Phonemes are
shown between slashes. English translations are
provided in parentheses.
</tableCaption>
<subsectionHeader confidence="0.484634">
Ill-spelled word Well-spelled equivalent
</subsectionHeader>
<bodyText confidence="0.6229442">
すげえ /sugee/ すごい /sugoi/ (great)
戻ろ /modoro/ 戻ろう /modorou/ (going to return)
うまいいいい /umaiiii/ うまい /umai/ (yummy)
Therefore, it is important to clarify our task setting
before discussing our joint model.
</bodyText>
<subsectionHeader confidence="0.999767">
3.1 Target ill-spelled words
</subsectionHeader>
<bodyText confidence="0.999958735294118">
Many studies on lexical normalization have
pointed out that phonological factors are deeply
involved in the process of deriving ill-spelled
words. Xia et al. (2006) investigated a Chi-
nese chat corpus and reported that 99.2% of the
ill-spelled words were derived by phonetic map-
ping from well-spelled equivalents. Wang and
Ng (2013) analyzed 200 Chinese messages from
Weibo and 200 English SMS messages from the
NUS SMS corpus (How and Kan, 2005). Their
analysis revealed that most ill-spelled words were
derived from well-spelled equivalents based on
pronunciation similarity.
On top of these investigations, we focused on
ill-spelled words that are derived by phonologi-
cal mapping from well-spelled words by assum-
ing that such ill-spelled words are dominant in
Japanese microblogs as well. We also assume
that these ill-spelled words can be normalized into
well-spelled equivalents on a word-to-word basis,
as assumed in a previous study (Han and Baldwin,
2011). The validity of these two assumptions is
empirically assessed in Section 4.
Table 1 lists examples of our target ill-spelled
words, their well-spelled equivalents, and their
phonemes. The ill-spelled word in the first row
is formed by changing the continuous two vowels
from /oi/ to /ee/. This type of change in pronun-
ciation is often observed in Japanese spoken lan-
guage. The second row presents contractions. The
last vowel character “う” /u/ of the well-spelled
word is dropped. The third row illustrates word
lengthening. The ill-spelled word is derived by re-
peating the vowel character “い” /i/.
</bodyText>
<subsectionHeader confidence="0.989367">
3.2 Terminology
</subsectionHeader>
<bodyText confidence="0.999932259259259">
We now introduce the terminology that will be
used throughout the remainder of this paper. The
term word surface form (or surface form for short)
is used to refer to the word form observed in an
actual text, while word normal form (or normal
form) refers to the normalized word form. Note
that surface forms of well-spelled words are al-
ways identical to their normal forms.
It is possible that the word surface form and nor-
mal form have distinct POS tags, although they are
identical in most cases. Take the ill-spelled word “
戻ろ” /modoro/ as an example (the second row of
Table 1). According to the JUMAN POS tag set,5
POS of its surface form is CONTRACTED VERB,
while that of its normal form is VERB.6 To handle
such a case, we strictly distinguish between these
two POS tags by referring to them as surface POS
tags and normal POS tags, respectively.
Given these terms, the tasks addressed in this
paper can be stated as follows. Word segmenta-
tion is a task of segmenting a sentence into a se-
quence of word surface forms, and POS tagging
is a task of providing surface POS tags. The task
of joint lexical normalization, word segmentation,
and POS tagging is to map a sentence into a se-
quence of quadruplets: word surface form, surface
POS tag, normal form, and normal POS tag.
</bodyText>
<sectionHeader confidence="0.992174" genericHeader="method">
4 Microblog Corpus
</sectionHeader>
<bodyText confidence="0.99989825">
This section introduces our microblog corpus. We
first explain the process of developing the corpus
then present the results of our agreement study and
corpus analysis.
</bodyText>
<subsectionHeader confidence="0.999734">
4.1 Data collection and annotation
</subsectionHeader>
<bodyText confidence="0.999994615384615">
The corpus was developed by manually annotating
text messages posted to Twitter.
The posts to be annotated were collected as fol-
lows. 171,386 Japanese posts were collected using
the Twitter API7 on December 6, 2013. Among
these, 1000 posts were randomly selected then
manually split into sentences. As a result, we ob-
tained 1831 sentences as a source of the corpus.
Two human participants annotated the 1831
sentences with surface forms and surface POS
tags. Since much effort has already been done to
annotate corpora with this information, the anno-
tation process here follows the guidelines used to
</bodyText>
<footnote confidence="0.998752">
5http://nlp.ist.i.kyoto-u.ac.jp/index.php?JUMAN
6In this paper, we use simplified POS tags for explana-
tion purposes. Remind that these tags are different from the
original ones defined in JUMAN POS tag set.
7https://stream.twitter.com/1.1/statuses/sample.json
</footnote>
<page confidence="0.998046">
101
</page>
<bodyText confidence="0.999934428571429">
develop such corpora in previous studies (Kuro-
hashi and Nagao, 1998; Hashimoto et al., 2011).
The two participants also annotated ill-spelled
words with their normal forms and normal POS
tags. Although this paper targets only infor-
mal phonological variations (c.f., Section 3),
other types of ill-spelled words were also anno-
tated to investigate their frequency distribution
in microblog texts. Specifically, besides infor-
mal phonological variations, spelling errors and
Twitter-specific abbreviations were annotated. As
a result, 833 ill-spelled words were identified (Ta-
ble 2). They were all annotated with normal forms
and normal POS tags.
</bodyText>
<subsectionHeader confidence="0.99884">
4.2 Agreement study
</subsectionHeader>
<bodyText confidence="0.99999553125">
We investigated the inter-annotator agreement to
check the reliability of the annotation. During the
annotation process, the two participants collabo-
ratively annotated around 90% of the sentences
(specifically, 1647 sentences) with normal forms
and normal POS tags, and elaborated an annota-
tion guideline through discussion. They then inde-
pendently annotated the remaining 184 sentences
(1431 words), which were used for the agreement
study. Our annotation guideline is shown in the
supplementary material.
We first explored the extent to which the
two participants agreed in distinguishing between
well-spelled words and ill-spelled words. For this
task, we observed Cohen’s kappa of 0.96 (almost
perfect agreement). This results show that it is
easy for humans to distinguish between these two
types of words.
Next, we investigated whether the two partici-
pants could give ill-spelled words with the same
normal forms and normal POS tags. For this pur-
pose, we regarded the normal forms and normal
POS tags annotated by one participant as goldstan-
dards and calculated precision and recall achieved
by the other participant. We observed moder-
ate agreement between the two participants: 70%
(56/80) precision and 73% (56/76) recall. We
manually analyzed the conflicted examples and
found that there were more than one acceptable
normal form in many of these cases. Therefore,
we would like to note that the precision and recall
reported above are rather pessimistic estimations.
</bodyText>
<subsectionHeader confidence="0.998986">
4.3 Analysis
</subsectionHeader>
<bodyText confidence="0.998782">
We conducted corpus analysis to confirm the fea-
sibility of our approach.
</bodyText>
<tableCaption confidence="0.976424">
Table 2: Frequency distribution over three types of
ill-spelled words in corpus.
</tableCaption>
<table confidence="0.9897832">
Type Frequency
Informal phonological variation 804 (92.9%)
Spelling error 27 (3.1%)
Twitter-specific abbreviation 34 (3.9%)
Total 865 (100%)
</table>
<tableCaption confidence="0.600868">
Table 2 illustrates that phonological variations
</tableCaption>
<bodyText confidence="0.986155933333334">
constitute a vast majority of ill-spelled words in
Japanese microblog texts. In addition, analysis
of the 804 phonological variations showed that
793 of them can be normalized into single words.
These represent the validity of the two assump-
tions we made in Section 3.1.
We then investigated whether lexical normaliza-
tion can decrease the number of out-of-vocabulary
words. For the 793 ill-spelled words, we counted
how many of their surface forms and normal
forms were not registered in the JUMAN dictio-
nary.8 The result suggests that 411 (51.8%) and
74 (9.3%) are not registered in the dictionary. This
indicates the effectiveness of lexical normalization
for decreasing out-of-vocabulary words.
</bodyText>
<sectionHeader confidence="0.906756" genericHeader="method">
5 Overview of Joint Model
</sectionHeader>
<bodyText confidence="0.999493">
This section gives an overview of our joint model
with lexical normalization for accurate word seg-
mentation and POS tagging.
</bodyText>
<subsectionHeader confidence="0.995995">
5.1 Lattice-based approach
</subsectionHeader>
<bodyText confidence="0.999965388888889">
A lattice-based approach has been commonly
adopted to perform joint word segmentation and
POS tagging (Jiang et al., 2008; Kudo et al., 2004;
Kaji and Kitsuregawa, 2013). In this approach, an
input sentence is transformed into a word lattice
in which the edges are labeled with surface POS
tags (Figure 1). Given such a lattice, word seg-
mentation and POS tagging can be performed at
the same time by traversing the lattice. A discrim-
inative model is typically used for the traversal.
An advantage of this approach is that, while the
lattice can represent an exponentially large num-
ber of candidate analyses, it can be quickly tra-
versed using dynamic programming (Kudo et al.,
2004; Kaji and Kitsuregawa, 2013) or beam search
(Jiang et al., 2008). In addition, a discriminative
model allows the use of rich word-level features
to find the correct analysis.
</bodyText>
<footnote confidence="0.970895">
8http://nlp.ist.i.kyoto-u.ac.jp/index.php?JUMAN
</footnote>
<page confidence="0.981463">
102
</page>
<figure confidence="0.3651405">
Input sentence: ᮾி㒔䛻ఫ䜐 (To live in Tokyo metropolis)
Word lattice:
</figure>
<figureCaption confidence="0.9780485">
Figure 1: Example lattice (Kudo et al., 2004; Kaji
and Kitsuregawa, 2013). Circle and arrow repre-
sent node and edge, respectively. Bold edges rep-
resent correct analysis.
</figureCaption>
<table confidence="0.634776">
Input sentence: ⱥㄒ䜟䛛䜣䛽䛗 (Not to understand English)
Word lattice:
Normalized sentence: ⱥㄒ 䜟䛛䜙 䛺䛔
</table>
<figureCaption confidence="0.955741">
Figure 2: Lattice used to perform joint task. Nor-
</figureCaption>
<bodyText confidence="0.925406176470588">
mal forms and normal POS tags are shown in
parentheses. As indicated by dotted arrows, nor-
malized sentence can be obtained by concatenat-
ing normal forms associated with edges in correct
analysis.
We propose extending the lattice-based ap-
proach to jointly perform lexical normalization,
word segmentation, and POS tagging. We trans-
form an input sentence into a word lattice in which
the edges are labeled with not only surface POS
tags but normal forms and normal POS tags (Fig-
ure 2). By traversing such a lattice, the three
tasks can be performed at the same time. This ap-
proach can not only exploit rich information ob-
tainable from word normal forms, but also achieve
efficiency similar to the original lattice-based ap-
proach.
</bodyText>
<subsectionHeader confidence="0.949219">
5.2 Issues
</subsectionHeader>
<bodyText confidence="0.993073428571429">
Issues on how to develop this lattice-based ap-
proach is detailed in Sections 6 and 7.
Section 6 describes how to generate a word lat-
tice from an input sentence. This is done us-
ing a hybrid approach that combines a statistical
model and normalization dictionary. The normal-
ization dictionary is specifically a list of quadru-
</bodyText>
<tableCaption confidence="0.978338666666667">
Table 3: Normalization dictionary. Columns rep-
resent entry ID, surface form, surface POS, normal
form, and normal POS, respectively.
</tableCaption>
<table confidence="0.999858">
ID Surf. Surf. POS Norm. Norm. POS
A すごい ADJECTIVE すごい ADJECTIVE
B すげえ ADJECTIVE すごい ADJECTIVE
C 戻ろう VERB 戻ろう VERB
D 戻ろ CONTR. VERB 戻ろう VERB
E うまい ADJECTIVE うまい ADJECTIVE
F うまいいいい ADJECTIVE うまい ADJECTIVE
</table>
<tableCaption confidence="0.911509833333333">
Table 4: Tag dictionary.
ID Surf. form Surf. POS
a すごい (great) ADJECTIVE
b 戻ろう (going to return) VERB
c 戻ろ (gonna return) CONTR. VERB
d うまい (yummy) ADJECTIVE
</tableCaption>
<bodyText confidence="0.9981856">
plets: word surface form, surface POS tag, normal
form, and normal POS tag (Table 3).
Section 7 describes a discriminative model for
the lattice traversal. Our feature design as well as
two training methods are presented.
</bodyText>
<sectionHeader confidence="0.983038" genericHeader="method">
6 Word Lattice Generation
</sectionHeader>
<bodyText confidence="0.9997145">
In this section, we first describe a method of con-
structing a normalization dictionary then present a
method of generating a word lattice from an input
sentence.
</bodyText>
<subsectionHeader confidence="0.999809">
6.1 Construction of normalization dictionary
</subsectionHeader>
<bodyText confidence="0.99965055">
Although large-scale normalization dictionaries
are difficult to obtain, tag dictionaries, which list
pairs of word surface forms and their surface POS
tags (Table 4), are widely available in many lan-
guages including Japanese. Therefore, we use an
existing tag dictionary to construct the normaliza-
tion dictionary.
Due to space limitations, we give only a brief
overview of our construction method, omitting its
details. We note that our method uses hand-crafted
rules similar to those used in (Sasano et al., 2013);
hence, the proposal of this method is not an im-
portant contribution. To make our experimental
results reproducible, our normalization dictionary,
as well as a tool for constructing it, is released as
supplementary material.
Our method of constructing the normalization
dictionary takes three steps. The following ex-
plains each step using Tables 3 and 4 as running
examples.
</bodyText>
<figure confidence="0.5192516875">
Noun Noun Noun Verb
Noun Noun Suffix Particle
ᮾ ி 㒔 䛻 ఫ䜐
Noun
(ⱥㄒ, Noun)
Noun
(࿴ḷ, Noun)
Suffix
(䛺䛔, Suffix)
Particle
(䛽, Particle)
ⱥㄒ 䜟䛛 䜣 䛽䛗
Verb
(䜟䛛䜙, Verb)
Suffix
(䛺䛔, Suffix)
</figure>
<page confidence="0.99527">
103
</page>
<bodyText confidence="0.999949822222222">
Step 1 A tag dictionary generally contains a
small number of ill-spelled words, although well-
spelled words constitute a vast majority. We iden-
tify such ill-spelled words by using a manually-
tailored list of surface POS tags indicative of in-
formal spelling (e.g., CONTRACTED VERB). For
example, entry (c) in Table 4 is identified as an
ill-spelled word in this step.
Step 2 The tag dictionary is augmented with
normal forms and normal POS tags to construct
a small normalization dictionary. For ill-spelled
words identified in step 1, the normal forms and
normal POS tags are determined by hand-crafted
rules. For example, the normal form is derived by
appending the vowel character “う” /u/ to the sur-
face form, if the surface POS tag is CONTRACTED
VERB. This rule derives entry (D) in Table 3 from
entry (c) in Table 4. For well-spelled words, on
the other hand, the normal forms and normal POS
tags are simply set the same as the surface forms
and surface POS tags. For example, entries (A),
(C), and (E) in Table 3 are generated from entries
(a), (b), and (d) in Table 4, respectively.
Step 3 Because the normalization dictionary
constructed in step 2 contains only a few ill-
spelled words, it is expanded in this step. For this
purpose, we use hand-crafted rules to derive ill-
spelled words from the entries already registered
in the normalization dictionary. Some rules are
taken from (Sasano et al., 2013), while the others
are newly tailored. In Table 3, for example, entry
(B) is derived from entry (A) by applying the rule
that substitutes “ごい” /goi/ with “げえ” /gee/.
A small problem that arises in step 3 is how to
handle lengthened words, such as entry (F) in Ta-
ble 3. While lengthened words can be easily de-
rived using simple rules (Brody and Diakopoulos,
2011; Sasano et al., 2013), such rules infinitely
increase the number of entries because an unlim-
ited number of lengthened words can be derived
by repeating characters. To address this problem,
no lengthened words are added to the normaliza-
tion dictionary in step 3. We instead use rules
to skip repetitive characters in an input sentence
when performing dictionary match.
</bodyText>
<subsectionHeader confidence="0.99949">
6.2 A hybrid approach
</subsectionHeader>
<bodyText confidence="0.999981923076923">
A word lattice is generated using both a statisti-
cal method (Kaji and Kitsuregawa, 2013) and the
normalization dictionary.
We begin by generating a word lattice which en-
codes only word surface forms and surface POS
tags (c.f., Figure 1) using the statistical method
proposed by Kaji and Kitsuregawa (2013). Inter-
ested readers may refer to their paper for details.
Each edge in the lattice is then labeled with nor-
mal forms and normal POS tags. Note that a sin-
gle edge can have more than one candidate normal
form and normal POS tag. In such a case, new
edges are accordingly added to the lattice.
The edges are labeled with normal forms and
normal POS tags in the following manner. First,
every edge is labeled with a normal form and
normal POS tag that are identical with the sur-
face form and surface POS tag. This is based on
our observation that most words are well-spelled
ones. The edge is not provided with further nor-
mal forms and normal POS tags, if the normaliza-
tion dictionary contains a well-spelled word that
has the same surface form as the edge. Otherwise,
we allow the edge to have all pairs of normal forms
and normal POS tags that are obtained by using the
normalization dictionary.
</bodyText>
<sectionHeader confidence="0.99584" genericHeader="method">
7 Discriminative Lattice Traversal
</sectionHeader>
<bodyText confidence="0.9999225">
This section explains a discriminative model for
traversing the word lattice. The lattice traversal
with a discriminative model can formally be writ-
ten as
</bodyText>
<equation confidence="0.889917">
(w, t,v, s) = arg max f(x, w, t, v, s) · 0.
(w,t,v,s)∈L(x)
</equation>
<bodyText confidence="0.9999805">
Here, x denotes an input sentence, w, t, v, and s
denote a sequence of word surface forms, surface
POS tags, normal forms, and normal POS tags, re-
spectively, L(x) represents a set of candidate anal-
yses represented by the word lattice, and f(·) and
0 are feature and weight vectors.
We now describe features, a decoding method,
and two training methods.
</bodyText>
<subsectionHeader confidence="0.839585">
7.1 Features
</subsectionHeader>
<bodyText confidence="0.998318444444445">
We use character-level and word-level features
used for word segmentation and POS tagging in
(Kaji and Kitsuregawa, 2013). To take advan-
tage of joint model with lexical normalization, the
word-level features are extracted from not only
surface forms but also normal forms. See (Kaji
and Kitsuregawa, 2013) for the original features.
In addition, several new features are introduced
in this paper. We use the quadruplets (wi, ti, vi, si)
</bodyText>
<page confidence="0.998084">
104
</page>
<bodyText confidence="0.99994385">
and pairs of surface and normal POS tags (ti, si)
as binary features to capture probable mappings
between ill-spelled words and their well-spelled
equivalents. We use another binary feature indi-
cating whether a quadruplet (wi, ti, vi, si) is reg-
istered in the normalization dictionary. Also, we
use a bigram language model feature, which pre-
vents sentences from being normalized into un-
grammatical and/or incomprehensible ones. The
language model features are associated with nor-
malized bigrams, (vi−1,si−1, vi, si), and take as
the values the logarithmic frequency log10(f + 1),
where f represents the bigram frequency (Kaji and
Kitsuregawa, 2011). Since it is difficult to obtain
a precise value of f, it is approximated by the fre-
quency of the surface bigram, (wi−1, ti−1, wi, ti),
calculated from a large raw corpus automatically
analyzed using a system of joint word segmenta-
tion and POS tagging. See Section 8.1 for the raw
corpus and system used in the experiments.
</bodyText>
<subsectionHeader confidence="0.992358">
7.2 Decoding
</subsectionHeader>
<bodyText confidence="0.999974857142857">
It is easy to find the best analysis (w, t, v, s)
among the candidates represented by the word lat-
tice. Although we use several new features, we
can still locate the best analysis by using the same
dynamic programming algorithm as in previous
studies (Kudo et al., 2004; Kaji and Kitsuregawa,
2013).
</bodyText>
<subsectionHeader confidence="0.970889">
7.3 Training on a fully annotated corpus
</subsectionHeader>
<bodyText confidence="0.9998016875">
It is straightforward to train the joint model pro-
vided with a fully annotated corpus, which is la-
beled with word surface forms, surface POS tags,
normal forms, and normal POS tags.
We use structured perceptron (Collins, 2002)
for the training (Algorithm 1). The training be-
gins by initializing θ as a zero vector (line 1).
It then reads the annotated corpus C (line 2-9).
Given a training example, (x, w, t, v, s) ∈ C, the
algorithm locates the best analysis, ( w, t, v, s),
based on the current weight vector (line 4). If
the best analysis differs from the oracle analy-
sis, (w, t, v, s), the weight vector is updated (line
5-7). After going through the annotated corpus
m times (m=10 in our experiment), the averaged
weight vector is returned (line 10).
</bodyText>
<subsectionHeader confidence="0.986006">
7.4 Training on a partially annotated corpus
</subsectionHeader>
<bodyText confidence="0.999910333333333">
Although the training with the perceptron algo-
rithm requires a fully annotated corpus, it is labor-
intensive to fully annotate sentences. This consid-
</bodyText>
<figure confidence="0.392581538461539">
Algorithm 1 Perceptron training
1: θ +— 0
2: for i = 1... m do
3: for (x, w, t, v, s) E C do
4: ( ˆw, ˆt, ˆv, ˆs) +— DECODING(x, θ)
5: if (w, t, v, s) =� ( ˆw, ˆt, ˆv, ˆs) then
6: θ +— θ + f(x, w, t, v, s) − f(x, ˆw, ˆt, ˆv, ˆs)
7: end if
8: end for
9: end for
10: return AVERAGE(θ)
Algorithm 2 Latent perceptron training
1: θ +— 0
</figure>
<listItem confidence="0.8617145">
2: for i = 1... m do
3: for (x, w, t) E C′ do
4: ( ˆw, ˆt, ˆv, ˆs) +— DECODING(x, θ)
5: (w, t, ¯v, ¯s) +— CONSTRAINEDDECODING(x, θ)
6: if w =� wˆ or t =� tˆ then
7: θ +— θ + f(x, w, t, ¯v, ¯s) − f(x, ˆw, ˆt, ˆv, ˆs)
8: end if
9: end for
10: end for
11: return AVERAGE(θ)
</listItem>
<bodyText confidence="0.996821789473684">
eration motivates us to explore training our model
with less supervision. We specifically explore us-
ing a corpus annotated with only word boundaries
and POS tags.
We use the latent perceptron algorithm (Sun et
al., 2013) to train the joint model from such a par-
tially annotated corpus (Algorithm 2). In this sce-
nario, a training example is a sentence x paired
with a sequence of word surface forms w and sur-
face POS tags t (c.f., line 3). Similarly to the
perceptron algorithm, we locate the best analy-
sis ( w, t, v, s) for a given training example, (line
4). We also locate the best analysis, (w, t, v, s),
among those having the same surface forms w and
surface POS tags t as the training example (line
5). If the surface forms and surface POS tags of
the former analysis differ from the annotations of
the training example, parameter is updated by re-
garding the latter analysis as an oracle (line 6-8).
</bodyText>
<sectionHeader confidence="0.996941" genericHeader="evaluation">
8 Experiments
</sectionHeader>
<bodyText confidence="0.99990175">
We conducted experiments to investigate how the
microblog corpus and joint model contribute to
improving accuracy of word segmentation and
POS tagging in the microblog domain.
</bodyText>
<subsectionHeader confidence="0.998434">
8.1 Setting
</subsectionHeader>
<bodyText confidence="0.9840335">
We constructed the normalization dictionary from
the JUMAN dictionary 7.0.9 While JUMAN dic-
</bodyText>
<footnote confidence="0.966516">
9http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?JUMAN
</footnote>
<page confidence="0.998582">
105
</page>
<bodyText confidence="0.999970214285714">
tionary contains 750,156 entries, the normaliza-
tion dictionary contains 112,458,326 entries.
Some features taken from the previous study
(Kaji and Kitsuregawa, 2013) are induced using a
tag dictionary. For this we used two tag dictionar-
ies. One is JUMAN dictionary 7.0 and the other
is a tag dictionary constructed by listing surface
forms and surface POS tags in the normalization
dictionary.
To compute the language model features, one
billion sentences from Twitter posts were analyzed
using MeCab 0.996.10 We used all bigrams ap-
pearing at least 10 times in the auto-analyzed sen-
tences.
</bodyText>
<subsectionHeader confidence="0.8851255">
8.2 Results of word segmentation and POS
tagging
</subsectionHeader>
<bodyText confidence="0.999894774193548">
We first investigated the performance of models
trained on an existing annotated corpus form news
texts. For this experiment, our joint model as
well as three state-of-the-art models (Kudo et al.,
2004)11(Neubig et al., 2011)12(Kaji and Kitsure-
gawa, 2013) were trained on Kyoto University
Text corpus 4.0 (Kurohashi and Nagao, 1998).
Since this training corpus is not annotated with
normal forms and normal POS tags, our model
was trained using the latent perceptron. Table
5 summarizes the word-level Fl-scores (Kudo et
al., 2004) on our microblog corpus. The two
columns represent the results for word segmenta-
tion (Seg) and joint word segmentation and POS
tagging (Seg+Tag), respectively.
We also conducted 5-fold crossvalidation on
our microblog corpus to evaluate performance im-
provement when these models are trained on mi-
croblog texts (Table 6). In addition to the models
in Table 5, results of a rule-based system (Sasano
et al., 2013)13 and our joint model trained using
the perceptron algorithm are also presented. No-
tice that Proposed and Proposed (latent) repre-
sent our model trained using perceptron and latent
perceptron, respectively.
From Tables 5 and 6, as expected, we see that
the models trained on news texts performed poorly
on microblog texts, while their performance sig-
nificantly boosted when trained on the microblog
texts. This demonstrates the importance of corpus
annotation. An exception was Kudo04. Its perfor-
</bodyText>
<footnote confidence="0.9996835">
10https://code.google.com/p/mecab
11https://code.google.com/p/mecab
12http://www.phontron.com/kytea/
13http://nlp.ist.i.kyoto-u.ac.jp/index.php?JUMAN
</footnote>
<tableCaption confidence="0.980885">
Table 5: Performance of models trained on the
news articles.
</tableCaption>
<table confidence="0.9991632">
Seg Seg+Tag
Kudo04 81.8 71.0
Neubig11 80.5 69.1
Kaji13 83.2 73.1
Proposed (latent) 83.0 73.9
</table>
<bodyText confidence="0.997951085714286">
mance improved only slightly, even when it was
trained on the microblog texts. We believe this is
because their model uses dictionary-based rules to
prune candidate analyses; thus, it could not per-
form well in the microblog domain, where out-of-
vocabulary words are abundant.
Table 6 also illustrates that our joint models
achieved Fl-score better than the state-of-the-art
models trained on the microblog texts. This
shows that modeling the derivation process of ill-
spelled words makes training easier. We con-
ducted bootstrap resampling (with 1000 samples)
to investigate the significance of the improvements
achieved with our joint model. The results showed
that all improvements over the baselines were sta-
tistically significant (p &lt; 0.01). The difference
between Proposed and Proposed (latent) were
also statistically significant (p &lt; 0.01).
The results of Proposed (latent) are interest-
ing. Table 5 illustrates that our joint model per-
forms well even when it is trained on a news cor-
pus that rarely contains ill-spelled words and is
not at all annotated with normal forms and nor-
mal POS tags. This indicates the robustness of our
training method and the importance of modeling
word derivation process in the microblog domain.
In Table 6, we observed that Proposed (latent),
which uses less supervision, performed better than
Proposed. The reason for this will be examined
later.
In summary, we can conclude that both the mi-
croblog corpus and joint model significantly con-
tribute to training accurate models for word seg-
mentation and POS tagging in the microblog do-
main.
</bodyText>
<subsectionHeader confidence="0.957218">
8.3 Results of lexical normalization
</subsectionHeader>
<bodyText confidence="0.927033857142857">
While the main goal with this study was to en-
hance word segmentation and POS tagging in the
microblog domain, it is interesting to explore how
well our joint model can normalize ill-spelled
words.
Table 7 illustrates precision, recall, and Fl-
score for the lexical normalization task. To put
</bodyText>
<page confidence="0.997932">
106
</page>
<tableCaption confidence="0.977455">
Table 6: Results of 5-fold cross-validation on mi-
croblog corpus.
</tableCaption>
<table confidence="0.999764571428572">
Seg Seg+Tag
Kudo04 82.7 71.7
Neubig11 88.6 75.9
Kaji13 90.9 82.1
Sasano13 82.7 73.3
Proposed 91.3 83.2
Proposed (latent) 91.4 83.7
</table>
<tableCaption confidence="0.9646155">
Table 7: Results of lexical normalization task in
terms of precision, recall, and F1-score.
</tableCaption>
<table confidence="0.9996625">
Precision Recall Fl
Neubig11 69.2 35.9 47.3
Proposed 77.1 44.6 56.6
Proposed (latent) 53.7 24.7 33.9
</table>
<bodyText confidence="0.997999961538461">
the results into context, we report on the baseline
results of a tagging model proposed by Neubig et
al. (2011). This baseline conducts lexical normal-
ization by regarding it as two independent tagging
tasks (i.e., tasks of tagging normal forms and nor-
mal POS tags). The result of the baseline model is
also obtained using 5-fold crossvalidation.
Table 7 illustrates that Proposed performed sig-
nificantly better than the simple tagging model,
Neubig11. This suggests the effectiveness of our
joint model. On the other hand, Proposed (latent)
performed poorly in this task. From this result, we
can argue that Proposed (latent) can achieve su-
perior performance in word segmentation and POS
tagging (Table 6) because it gave up correctly nor-
malizing ill-spelled words, focusing on word seg-
mentation and POS tagging.
The experimental results so far suggest the fol-
lowing strategy for training our joint model. If ac-
curacy of word segmentation and POS tagging is
the main concern, we can use the latent percep-
tron. This approach has the advantage of being
able to use a partially annotated corpus. On the
other hand, if performance of lexical normaliza-
tion is crucial, we have to use the standard percep-
tron algorithm.
</bodyText>
<subsectionHeader confidence="0.9626">
8.4 Error analysis
</subsectionHeader>
<bodyText confidence="0.999974047619048">
We manually analyzed erroneous outputs and ob-
served several tendencies.
We found that a word lattice sometimes missed
the correct output. Such an error was, for example,
observed in a sentence including many ill-spelled
words, e.g., ‘�#H0)H�a ,�C-:n1-9-7A! (be
nervous about what other people think!)’, where
the part ‘:�C-:n1-9-7A’ is in ill-spelled words.
Improving the lattice generation algorithm is con-
sidered necessary to achieve further performance
gain.
Even if the correct analysis appears in the word
lattice, our model sometimes failed to handle
ill-spelled words, incorrectly analyzing them as
out-of-vocabulary words. For example, the pro-
posed method treated the phrase ‘�������
(snack time)’ as a single out-of-vocabulary word,
even though the correct analysis was found in the
word lattice. More sophisticated features would
be required to accurately distinguish between ill-
spelled and out-of-vocabulary words.
</bodyText>
<sectionHeader confidence="0.986482" genericHeader="conclusions">
9 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999996269230769">
We presented our attempts towards developing an
accurate model for word segmentation and POS
tagging in the microblog domain. To this end, we,
for the first time, developed an annotated corpus
of microblogs. We also proposed a joint model
with lexical normalization to handle orthographic
diversity in the microblog text. Intensive exper-
iments demonstrated that we could successfully
improve the performance of word segmentation
and POS tagging on microblog texts. We believe
this study will have a large practical impact on a
various research areas that target microblogs.
One limitation of our approach is that it cannot
handle certain types of ill-spelled words. For ex-
ample, the current model cannot handle the cases
in which there are no one-to-one-mappings be-
tween well-spelled and ill-spelled words. Also,
our model cannot handle spelling errors, which
are considered relatively frequent in the microblog
than news domains. The treatment of these prob-
lems would require further research.
Another future research is to speed-up our
model. Since the joint model with lexical normal-
ization significantly increases the search space,
it is much slower than the original lattice-based
model for word segmentation and POS tagging.
</bodyText>
<sectionHeader confidence="0.998297" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999036666666667">
The authors would like to thank Naoki Yoshinaga
for his help in developing the microblog corpus as
well as fruitful discussions.
</bodyText>
<page confidence="0.998312">
107
</page>
<sectionHeader confidence="0.987969" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999617632075471">
Samuel Brody and Nicholas Diakopoulos. 2011.
Cooooooooooooooollllllllllllll! !!!!!!!!!!!!! using
word lengthening to detect sentiment in microblogs.
In Proceedings of EMNLP, pages 562–570.
Michael Collins. 2002. Discriminative training meth-
ods for hidden Markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of EMNLP, pages 1–8.
Huiming Duan, Zhifang Sui, Ye Tian, and Wenjie Li.
2012. The CIPS-SIGHAN CLP 2012 Chinese word
segmentation on microblog corpora bakeoff. In Pro-
ceedings of the Second CIPS-SIGHAN Joint Conr-
erence on Chinese Language Processing, pages 35–
40.
Jennifer Foster, Ozlem Cetinoglu, Joachim Wagner,
Joseph Le Roux, Stephen Hogan, Joakim Nivre,
Deirdre Hogan, and Josef van Genabith. 2011.
#hardtoparse: POS tagging and parsing the twit-
terverse. In Proceedings of AAAI Workshop on
Analysing Microtext, pages 20–25.
Kevin Gimpel, Nathan Schneider, Brendan O’Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein,
Michael Heilman, Dani Yogatama, Jeffrey Flanigan,
and Noah A. Smith. 2011. Part-of-speech tagging
for twitter: Annotation, features, and experiments.
In Proceedings ofACL, pages 42–47.
Bo Han and Timothy Baldwin. 2011. Lexical normal-
ization of short text messages: Makin sens a #twitter.
In Proceedings ofACL, pages 368–378.
Bo Han, Paul Cook, and Timothy Baldwin. 2012. Au-
tomatically constructing a normalisation dictionary
for microblogs. In Proceedings of EMNLP-CoNLL,
pages 421–432.
Chikara Hashimoto, Sadao Kurohashi, Daisuke Kawa-
hara, Keiji Shinzato, and Masaaki Nagata. 2011.
Construction of a blog corpus with syntac-
tic, anaphoric, and semantic annotations (in
Japanese). Journal of Natural Language Process-
ing, 18(2):175–201.
Yijiu How and Min-Yen Kan. 2005. Optimizing pre-
dictive text entry for short message service on mo-
bile phones. In Proceedings of Human Computer
Interfaces International.
Kazushi Ikeda, Tadashi Yanagihara, Kazunori Mat-
sumoto, and Yasuhiro Takishima. 2009. Unsuper-
vised text normalization approach for morphological
analysis of blog documents. In Proceedings of Aus-
tralasian Joint Conference on Advances in Artificial
Intelligence, pages 401–411.
Wenbin Jiang, Haitao Mi, and Qun Liu. 2008. Word
lattice reranking for Chinese word segmentation and
part-of-speech tagging. In Proceedings of Coling,
pages 385–392.
Long Jiang, Mo Yu, Ming Zhou, Xiaohua Liu, and
Tiejun Zhao. 2011. Target-dependent Twitter sen-
timent classification. In Proceedings of ACL, pages
151–160.
Nobuhiro Kaji and Masaru Kitsuregawa. 2011. Split-
ting noun compounds via monolingual and bilingual
paraphrasing: A study on Japanese Katakana words.
In Proceedings of EMNLP, pages 959–969.
Nobuhiro Kaji and Masaru Kitsuregawa. 2013. Effi-
cient word lattice generation for joint word segmen-
tation and POS tagging in Japanese. In Proceedings
of IJCNLP, pages 153–161.
Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto.
2004. Applying conditional random fields to
Japanese morphological analysis. In Proceedings of
EMNLP, pages 230–237.
Sadao Kurohashi and Makoto Nagao. 1998. Building a
Japanese parsed corpus while improving the parsing
system. In Proceedings of LREC, pages 719–724.
Wang Ling, Chris Dyer, Alan W Black, and Isabel
Trancoso. 2013. Paraphrasing 4 microblog normal-
ization. In Proceedings of EMNLP, pages 73–84.
Xiaohua Liu, Ming Zhou, Xiangyang Zhou,
Zhongyang Fu, and Furu Wei. 2012. Joint
inference of named entity recognition and normal-
ization for tweets. In Proceedings of ACL, pages
526–535.
Graham Neubig, Yousuke Nakata, and Shinsuke Mori.
2011. Pointwise prediction for robust adaptable
Japanese morphological analysis. In Proceedings of
ACL, pages 529–533.
Itsumi Saito, Kugatsu Sadamitsu, Hisako Asano, and
Yoshihiro Matsuo. 2014. Morphological analysis
for Japanese noisy text based on character-level and
word-level normalization. In Proceedings of COL-
ING, pages 1773–1782.
Takeshi Sakaki, Makoto Okazaki, and Yutaka Matsuo.
2010. Earthquak shakes Twitter users: real-time
event detection by social sensors. In Proceedings
of WWW, pages 851–860.
Ryohei Sasano, Sadao Kurohashi, and Manabu Oku-
mura. 2013. A simple approach to unknown word
processing in Japanese morphological analysis. In
Proceedings of IJCNLP, pages 162–170.
Xu Sun, Takuya Matsuzaki, and Wenjie Li. 2013.
Latent structured perceptrons for large-scale learn-
ing with hidden information. IEEE Transactions
on Knowledge and Data Engineering, 25(9):2063–
2075.
Aobo Wang and Min-Yen Kan. 2013. Mining informal
language from Chinese microtext: Joint word recog-
nition and segmentation. In Proceedings of ACL,
pages 731–741.
</reference>
<page confidence="0.986989">
108
</page>
<reference confidence="0.996709052631579">
Pidong Wang and Hwee Tou Ng. 2013. A beam-search
decoder for normalization of social media text with
application to machine translation. In Proceedings
of NAACL, pages 471–481.
Aobo Wang, Min-Yen Kan, Daniel Andrade, Takashi
Onishi, and Kai Ishikawa. 2013. Chinese informal
word normalization: an experimental study. In Pro-
ceedings of IJCNLP, pages 127–135.
Yunqing Xia, Kam-Fai Wong, and Wenjie Li. 2006. A
phonetic-based approach to Chinese chat text nor-
malization. In Proceedings of ACL, pages 993–
1000.
Yi Yang and Jacob Eisenstein. 2013. A log-linear
model for unsupervised text normalization. In Pro-
ceedings of EMNLP, pages 61–72.
Congle Zhang, Tyler Baldwin, Howard Ho, Benny
Kimelfeld, and Yunyao Li. 2013. Adaptive parser-
centric text normalization. In Proceedings of ACL,
pages 1159–1168.
</reference>
<page confidence="0.998959">
109
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.292551">
<title confidence="0.9254485">Accurate Word Segmentation and POS Tagging for Japanese Microblogs: Corpus Annotation and Joint Modeling with Lexical Normalization</title>
<note confidence="0.480235">Institute of Information and Communications of Industrial Science, The University of Institute of</note>
<abstract confidence="0.999148611111111">Microblogs have recently received widespread interest from NLP researchers. However, current tools for Japanese word segmentation and POS tagging still perform poorly on microblog texts. We developed an annotated corpus and proposed a joint model for overcoming this situation. Our annotated corpus of microblog texts enables not only training of accurate statistical models but also quantitative evaluation of their performance. Our joint model with lexical normalization handles the orthographic diversity of microblog texts. We conducted an experiment to demonstrate that the corpus and model substantially contribute to boosting accuracy.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Samuel Brody</author>
<author>Nicholas Diakopoulos</author>
</authors>
<title>Cooooooooooooooollllllllllllll! !!!!!!!!!!!!! using word lengthening to detect sentiment in microblogs.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>562--570</pages>
<contexts>
<context position="22031" citStr="Brody and Diakopoulos, 2011" startWordPosition="3478" endWordPosition="3481">d in step 2 contains only a few illspelled words, it is expanded in this step. For this purpose, we use hand-crafted rules to derive illspelled words from the entries already registered in the normalization dictionary. Some rules are taken from (Sasano et al., 2013), while the others are newly tailored. In Table 3, for example, entry (B) is derived from entry (A) by applying the rule that substitutes “ごい” /goi/ with “げえ” /gee/. A small problem that arises in step 3 is how to handle lengthened words, such as entry (F) in Table 3. While lengthened words can be easily derived using simple rules (Brody and Diakopoulos, 2011; Sasano et al., 2013), such rules infinitely increase the number of entries because an unlimited number of lengthened words can be derived by repeating characters. To address this problem, no lengthened words are added to the normalization dictionary in step 3. We instead use rules to skip repetitive characters in an input sentence when performing dictionary match. 6.2 A hybrid approach A word lattice is generated using both a statistical method (Kaji and Kitsuregawa, 2013) and the normalization dictionary. We begin by generating a word lattice which encodes only word surface forms and surfac</context>
</contexts>
<marker>Brody, Diakopoulos, 2011</marker>
<rawString>Samuel Brody and Nicholas Diakopoulos. 2011. Cooooooooooooooollllllllllllll! !!!!!!!!!!!!! using word lengthening to detect sentiment in microblogs. In Proceedings of EMNLP, pages 562–570.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="26231" citStr="Collins, 2002" startWordPosition="4191" endWordPosition="4192">d system used in the experiments. 7.2 Decoding It is easy to find the best analysis (w, t, v, s) among the candidates represented by the word lattice. Although we use several new features, we can still locate the best analysis by using the same dynamic programming algorithm as in previous studies (Kudo et al., 2004; Kaji and Kitsuregawa, 2013). 7.3 Training on a fully annotated corpus It is straightforward to train the joint model provided with a fully annotated corpus, which is labeled with word surface forms, surface POS tags, normal forms, and normal POS tags. We use structured perceptron (Collins, 2002) for the training (Algorithm 1). The training begins by initializing θ as a zero vector (line 1). It then reads the annotated corpus C (line 2-9). Given a training example, (x, w, t, v, s) ∈ C, the algorithm locates the best analysis, ( w, t, v, s), based on the current weight vector (line 4). If the best analysis differs from the oracle analysis, (w, t, v, s), the weight vector is updated (line 5-7). After going through the annotated corpus m times (m=10 in our experiment), the averaged weight vector is returned (line 10). 7.4 Training on a partially annotated corpus Although the training wit</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithms. In Proceedings of EMNLP, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Huiming Duan</author>
<author>Zhifang Sui</author>
<author>Ye Tian</author>
<author>Wenjie Li</author>
</authors>
<title>Chinese word segmentation on microblog corpora bakeoff.</title>
<date>2012</date>
<booktitle>The CIPS-SIGHAN CLP</booktitle>
<pages>35--40</pages>
<contexts>
<context position="5988" citStr="Duan et al., 2012" startWordPosition="897" endWordPosition="900">d Sections 6 and 7 provide details of the model. Section 8 presents experimental results and discussions, and Section 9 presents concluding remarks. 2 Related Work Researchers have recently developed various microblog corpora annotated with rich linguistic information. Gimpel et al. (2011) and Foster et al. (2011) annotated English microblog posts with 3Please contact the first author for this corpus. POS tags. Han and Baldwin (2011) released a microblog corpus annotated with normalized forms of words. A Chinese microblog corpus annotated with word boundaries was developed for SIGHAN bakeoff (Duan et al., 2012). However, there are no microblog corpora annotated with word boundaries, POS tags, and normalized sentences. There has been a surge of interest in lexical normalization with the advent of microblogs (Han and Baldwin, 2011; Liu et al., 2012; Han et al., 2012; Wang and Ng, 2013; Zhang et al., 2013; Ling et al., 2013; Yang and Eisenstein, 2013; Wang et al., 2013). However, these studies did not address enhancing word segmentation. Wang et al. (2013) proposed a method of joint ill-spelled word recognition and word segmentation. With their method, informal spellings are merely recognized and not n</context>
</contexts>
<marker>Duan, Sui, Tian, Li, 2012</marker>
<rawString>Huiming Duan, Zhifang Sui, Ye Tian, and Wenjie Li. 2012. The CIPS-SIGHAN CLP 2012 Chinese word segmentation on microblog corpora bakeoff. In Proceedings of the Second CIPS-SIGHAN Joint Conrerence on Chinese Language Processing, pages 35– 40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jennifer Foster</author>
<author>Ozlem Cetinoglu</author>
<author>Joachim Wagner</author>
<author>Joseph Le Roux</author>
<author>Stephen Hogan</author>
<author>Joakim Nivre</author>
<author>Deirdre Hogan</author>
<author>Josef van Genabith</author>
</authors>
<title>hardtoparse: POS tagging and parsing the twitterverse.</title>
<date>2011</date>
<booktitle>In Proceedings of AAAI Workshop on Analysing Microtext,</booktitle>
<pages>20--25</pages>
<marker>Foster, Cetinoglu, Wagner, Le Roux, Hogan, Nivre, Hogan, van Genabith, 2011</marker>
<rawString>Jennifer Foster, Ozlem Cetinoglu, Joachim Wagner, Joseph Le Roux, Stephen Hogan, Joakim Nivre, Deirdre Hogan, and Josef van Genabith. 2011. #hardtoparse: POS tagging and parsing the twitterverse. In Proceedings of AAAI Workshop on Analysing Microtext, pages 20–25.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Gimpel</author>
<author>Nathan Schneider</author>
<author>Brendan O’Connor</author>
<author>Dipanjan Das</author>
<author>Daniel Mills</author>
<author>Jacob Eisenstein</author>
<author>Michael Heilman</author>
<author>Dani Yogatama</author>
<author>Jeffrey Flanigan</author>
<author>Noah A Smith</author>
</authors>
<title>Part-of-speech tagging for twitter: Annotation, features, and experiments.</title>
<date>2011</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>42--47</pages>
<marker>Gimpel, Schneider, O’Connor, Das, Mills, Eisenstein, Heilman, Yogatama, Flanigan, Smith, 2011</marker>
<rawString>Kevin Gimpel, Nathan Schneider, Brendan O’Connor, Dipanjan Das, Daniel Mills, Jacob Eisenstein, Michael Heilman, Dani Yogatama, Jeffrey Flanigan, and Noah A. Smith. 2011. Part-of-speech tagging for twitter: Annotation, features, and experiments. In Proceedings ofACL, pages 42–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Han</author>
<author>Timothy Baldwin</author>
</authors>
<title>Lexical normalization of short text messages: Makin sens a #twitter.</title>
<date>2011</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>368--378</pages>
<contexts>
<context position="3254" citStr="Han and Baldwin, 2011" startWordPosition="476" endWordPosition="479">s. Specifically, we investigated how well current models trained on existing corpora perform in the microblog domain. We also explored performance gains achieved by using our corpus for training, and by jointly performing lexical normalization (c.f., Section 1.2). 1.2 Joint modeling with lexical normalization Orthographic diversity in microblog texts causes a problem when training a statistical model for word segmentation and POS tagging. Microblog texts frequently contain informal words that are spelled in a non-standard manner, e.g., “oredi (already)”, “b4 (before)”, and “talkin (talking)” (Han and Baldwin, 2011). Such words, hereafter referred to as ill-spelled words, are so productive that they considerably increase the vocabulary size. This makes training of statistical models difficult. We address this problem by jointly conducting lexical normalization. Although a wide variety of ill-spelled words are used in microblog texts, many can be normalized into well-spelled equivalents, which conform to standard rules of spelling. 99 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 99–109, October 25-29, 2014, Doha, Qatar. c�2014 Association for Comput</context>
<context position="5807" citStr="Han and Baldwin (2011)" startWordPosition="869" endWordPosition="872"> and introduces terminology. Section 4 presents our microblog corpus and results of our corpus analysis. Section 5 presents an overview of our joint model with lexical normalization, and Sections 6 and 7 provide details of the model. Section 8 presents experimental results and discussions, and Section 9 presents concluding remarks. 2 Related Work Researchers have recently developed various microblog corpora annotated with rich linguistic information. Gimpel et al. (2011) and Foster et al. (2011) annotated English microblog posts with 3Please contact the first author for this corpus. POS tags. Han and Baldwin (2011) released a microblog corpus annotated with normalized forms of words. A Chinese microblog corpus annotated with word boundaries was developed for SIGHAN bakeoff (Duan et al., 2012). However, there are no microblog corpora annotated with word boundaries, POS tags, and normalized sentences. There has been a surge of interest in lexical normalization with the advent of microblogs (Han and Baldwin, 2011; Liu et al., 2012; Han et al., 2012; Wang and Ng, 2013; Zhang et al., 2013; Ling et al., 2013; Yang and Eisenstein, 2013; Wang et al., 2013). However, these studies did not address enhancing word </context>
<context position="9527" citStr="Han and Baldwin, 2011" startWordPosition="1450" endWordPosition="1453"> 200 Chinese messages from Weibo and 200 English SMS messages from the NUS SMS corpus (How and Kan, 2005). Their analysis revealed that most ill-spelled words were derived from well-spelled equivalents based on pronunciation similarity. On top of these investigations, we focused on ill-spelled words that are derived by phonological mapping from well-spelled words by assuming that such ill-spelled words are dominant in Japanese microblogs as well. We also assume that these ill-spelled words can be normalized into well-spelled equivalents on a word-to-word basis, as assumed in a previous study (Han and Baldwin, 2011). The validity of these two assumptions is empirically assessed in Section 4. Table 1 lists examples of our target ill-spelled words, their well-spelled equivalents, and their phonemes. The ill-spelled word in the first row is formed by changing the continuous two vowels from /oi/ to /ee/. This type of change in pronunciation is often observed in Japanese spoken language. The second row presents contractions. The last vowel character “う” /u/ of the well-spelled word is dropped. The third row illustrates word lengthening. The ill-spelled word is derived by repeating the vowel character “い” /i/.</context>
</contexts>
<marker>Han, Baldwin, 2011</marker>
<rawString>Bo Han and Timothy Baldwin. 2011. Lexical normalization of short text messages: Makin sens a #twitter. In Proceedings ofACL, pages 368–378.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Han</author>
<author>Paul Cook</author>
<author>Timothy Baldwin</author>
</authors>
<title>Automatically constructing a normalisation dictionary for microblogs.</title>
<date>2012</date>
<booktitle>In Proceedings of EMNLP-CoNLL,</booktitle>
<pages>421--432</pages>
<contexts>
<context position="6246" citStr="Han et al., 2012" startWordPosition="941" endWordPosition="944">ormation. Gimpel et al. (2011) and Foster et al. (2011) annotated English microblog posts with 3Please contact the first author for this corpus. POS tags. Han and Baldwin (2011) released a microblog corpus annotated with normalized forms of words. A Chinese microblog corpus annotated with word boundaries was developed for SIGHAN bakeoff (Duan et al., 2012). However, there are no microblog corpora annotated with word boundaries, POS tags, and normalized sentences. There has been a surge of interest in lexical normalization with the advent of microblogs (Han and Baldwin, 2011; Liu et al., 2012; Han et al., 2012; Wang and Ng, 2013; Zhang et al., 2013; Ling et al., 2013; Yang and Eisenstein, 2013; Wang et al., 2013). However, these studies did not address enhancing word segmentation. Wang et al. (2013) proposed a method of joint ill-spelled word recognition and word segmentation. With their method, informal spellings are merely recognized and not normalized. Therefore, they did not investigate how to exploit the information obtainable from well-spelled equivalents to increase word segmentation accuracy. Some studies also explored integrating the lexical normalization process into word segmentation and</context>
</contexts>
<marker>Han, Cook, Baldwin, 2012</marker>
<rawString>Bo Han, Paul Cook, and Timothy Baldwin. 2012. Automatically constructing a normalisation dictionary for microblogs. In Proceedings of EMNLP-CoNLL, pages 421–432.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chikara Hashimoto</author>
<author>Sadao Kurohashi</author>
<author>Daisuke Kawahara</author>
<author>Keiji Shinzato</author>
<author>Masaaki Nagata</author>
</authors>
<title>Construction of a blog corpus with syntactic, anaphoric, and semantic annotations (in Japanese).</title>
<date>2011</date>
<journal>Journal of Natural Language Processing,</journal>
<volume>18</volume>
<issue>2</issue>
<contexts>
<context position="12604" citStr="Hashimoto et al., 2011" startWordPosition="1958" endWordPosition="1961">nces as a source of the corpus. Two human participants annotated the 1831 sentences with surface forms and surface POS tags. Since much effort has already been done to annotate corpora with this information, the annotation process here follows the guidelines used to 5http://nlp.ist.i.kyoto-u.ac.jp/index.php?JUMAN 6In this paper, we use simplified POS tags for explanation purposes. Remind that these tags are different from the original ones defined in JUMAN POS tag set. 7https://stream.twitter.com/1.1/statuses/sample.json 101 develop such corpora in previous studies (Kurohashi and Nagao, 1998; Hashimoto et al., 2011). The two participants also annotated ill-spelled words with their normal forms and normal POS tags. Although this paper targets only informal phonological variations (c.f., Section 3), other types of ill-spelled words were also annotated to investigate their frequency distribution in microblog texts. Specifically, besides informal phonological variations, spelling errors and Twitter-specific abbreviations were annotated. As a result, 833 ill-spelled words were identified (Table 2). They were all annotated with normal forms and normal POS tags. 4.2 Agreement study We investigated the inter-ann</context>
</contexts>
<marker>Hashimoto, Kurohashi, Kawahara, Shinzato, Nagata, 2011</marker>
<rawString>Chikara Hashimoto, Sadao Kurohashi, Daisuke Kawahara, Keiji Shinzato, and Masaaki Nagata. 2011. Construction of a blog corpus with syntactic, anaphoric, and semantic annotations (in Japanese). Journal of Natural Language Processing, 18(2):175–201.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yijiu How</author>
<author>Min-Yen Kan</author>
</authors>
<title>Optimizing predictive text entry for short message service on mobile phones.</title>
<date>2005</date>
<booktitle>In Proceedings of Human Computer Interfaces International.</booktitle>
<contexts>
<context position="9010" citStr="How and Kan, 2005" startWordPosition="1373" endWordPosition="1376">(going to return) うまいいいい /umaiiii/ うまい /umai/ (yummy) Therefore, it is important to clarify our task setting before discussing our joint model. 3.1 Target ill-spelled words Many studies on lexical normalization have pointed out that phonological factors are deeply involved in the process of deriving ill-spelled words. Xia et al. (2006) investigated a Chinese chat corpus and reported that 99.2% of the ill-spelled words were derived by phonetic mapping from well-spelled equivalents. Wang and Ng (2013) analyzed 200 Chinese messages from Weibo and 200 English SMS messages from the NUS SMS corpus (How and Kan, 2005). Their analysis revealed that most ill-spelled words were derived from well-spelled equivalents based on pronunciation similarity. On top of these investigations, we focused on ill-spelled words that are derived by phonological mapping from well-spelled words by assuming that such ill-spelled words are dominant in Japanese microblogs as well. We also assume that these ill-spelled words can be normalized into well-spelled equivalents on a word-to-word basis, as assumed in a previous study (Han and Baldwin, 2011). The validity of these two assumptions is empirically assessed in Section 4. Table</context>
</contexts>
<marker>How, Kan, 2005</marker>
<rawString>Yijiu How and Min-Yen Kan. 2005. Optimizing predictive text entry for short message service on mobile phones. In Proceedings of Human Computer Interfaces International.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kazushi Ikeda</author>
<author>Tadashi Yanagihara</author>
<author>Kazunori Matsumoto</author>
<author>Yasuhiro Takishima</author>
</authors>
<title>Unsupervised text normalization approach for morphological analysis of blog documents.</title>
<date>2009</date>
<booktitle>In Proceedings of Australasian Joint Conference on Advances in Artificial Intelligence,</booktitle>
<pages>401--411</pages>
<contexts>
<context position="6878" citStr="Ikeda et al., 2009" startWordPosition="1038" endWordPosition="1041"> 2013; Zhang et al., 2013; Ling et al., 2013; Yang and Eisenstein, 2013; Wang et al., 2013). However, these studies did not address enhancing word segmentation. Wang et al. (2013) proposed a method of joint ill-spelled word recognition and word segmentation. With their method, informal spellings are merely recognized and not normalized. Therefore, they did not investigate how to exploit the information obtainable from well-spelled equivalents to increase word segmentation accuracy. Some studies also explored integrating the lexical normalization process into word segmentation and POS tagging (Ikeda et al., 2009; Sasano et al., 2013). A strength of our joint model is that it uses rich character-level and word-level features used in state-of-the-art models of joint word segmentation and POS tagging (Kudo et al., 2004; Neubig et al., 2011; Kaji and Kitsuregawa, 2013). Thanks to these features, our model performed much better than Sasano et al.’s system, which is the only publicly available system that jointly conducts lexical normalization, in the experiments (see Section 8). Another advantage is that our model can be trained on a partially annotated corpus. Furthermore, we present a comprehensive eval</context>
</contexts>
<marker>Ikeda, Yanagihara, Matsumoto, Takishima, 2009</marker>
<rawString>Kazushi Ikeda, Tadashi Yanagihara, Kazunori Matsumoto, and Yasuhiro Takishima. 2009. Unsupervised text normalization approach for morphological analysis of blog documents. In Proceedings of Australasian Joint Conference on Advances in Artificial Intelligence, pages 401–411.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenbin Jiang</author>
<author>Haitao Mi</author>
<author>Qun Liu</author>
</authors>
<title>Word lattice reranking for Chinese word segmentation and part-of-speech tagging.</title>
<date>2008</date>
<booktitle>In Proceedings of Coling,</booktitle>
<pages>385--392</pages>
<contexts>
<context position="16006" citStr="Jiang et al., 2008" startWordPosition="2471" endWordPosition="2474">he 793 ill-spelled words, we counted how many of their surface forms and normal forms were not registered in the JUMAN dictionary.8 The result suggests that 411 (51.8%) and 74 (9.3%) are not registered in the dictionary. This indicates the effectiveness of lexical normalization for decreasing out-of-vocabulary words. 5 Overview of Joint Model This section gives an overview of our joint model with lexical normalization for accurate word segmentation and POS tagging. 5.1 Lattice-based approach A lattice-based approach has been commonly adopted to perform joint word segmentation and POS tagging (Jiang et al., 2008; Kudo et al., 2004; Kaji and Kitsuregawa, 2013). In this approach, an input sentence is transformed into a word lattice in which the edges are labeled with surface POS tags (Figure 1). Given such a lattice, word segmentation and POS tagging can be performed at the same time by traversing the lattice. A discriminative model is typically used for the traversal. An advantage of this approach is that, while the lattice can represent an exponentially large number of candidate analyses, it can be quickly traversed using dynamic programming (Kudo et al., 2004; Kaji and Kitsuregawa, 2013) or beam sea</context>
</contexts>
<marker>Jiang, Mi, Liu, 2008</marker>
<rawString>Wenbin Jiang, Haitao Mi, and Qun Liu. 2008. Word lattice reranking for Chinese word segmentation and part-of-speech tagging. In Proceedings of Coling, pages 385–392.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Long Jiang</author>
<author>Mo Yu</author>
<author>Ming Zhou</author>
<author>Xiaohua Liu</author>
<author>Tiejun Zhao</author>
</authors>
<title>Target-dependent Twitter sentiment classification.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>151--160</pages>
<contexts>
<context position="1322" citStr="Jiang et al., 2011" startWordPosition="180" endWordPosition="183"> not only training of accurate statistical models but also quantitative evaluation of their performance. Our joint model with lexical normalization handles the orthographic diversity of microblog texts. We conducted an experiment to demonstrate that the corpus and model substantially contribute to boosting accuracy. 1 Introduction Microblogs, such as Twitter1 and Weibo2, have recently become an important target of NLP technology. Since microblogs offer an instant way of posting textual messages, they have been given increasing attention as valuable sources for such actions as mining opinions (Jiang et al., 2011) and detecting sudden events such as earthquake (Sakaki et al., 2010). However, many studies have reported that current NLP tools do not perform well on microblog texts (Foster et al., 2011; Gimpel et al., 2011). In the case of Japanese text processing, the most serious problem is poor accuracy of word segmentation and POS tagging. Since these two tasks are positioned as the fundamental step in the text processing pipeline, their accuracy is vital for all downstream applications. 1https://twitter.com 2https://www.weibo.com 1.1 Development of annotated corpus The main obstacle that makes word s</context>
</contexts>
<marker>Jiang, Yu, Zhou, Liu, Zhao, 2011</marker>
<rawString>Long Jiang, Mo Yu, Ming Zhou, Xiaohua Liu, and Tiejun Zhao. 2011. Target-dependent Twitter sentiment classification. In Proceedings of ACL, pages 151–160.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nobuhiro Kaji</author>
<author>Masaru Kitsuregawa</author>
</authors>
<title>Splitting noun compounds via monolingual and bilingual paraphrasing: A study on Japanese Katakana words.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>959--969</pages>
<contexts>
<context position="25326" citStr="Kaji and Kitsuregawa, 2011" startWordPosition="4031" endWordPosition="4034">and normal POS tags (ti, si) as binary features to capture probable mappings between ill-spelled words and their well-spelled equivalents. We use another binary feature indicating whether a quadruplet (wi, ti, vi, si) is registered in the normalization dictionary. Also, we use a bigram language model feature, which prevents sentences from being normalized into ungrammatical and/or incomprehensible ones. The language model features are associated with normalized bigrams, (vi−1,si−1, vi, si), and take as the values the logarithmic frequency log10(f + 1), where f represents the bigram frequency (Kaji and Kitsuregawa, 2011). Since it is difficult to obtain a precise value of f, it is approximated by the frequency of the surface bigram, (wi−1, ti−1, wi, ti), calculated from a large raw corpus automatically analyzed using a system of joint word segmentation and POS tagging. See Section 8.1 for the raw corpus and system used in the experiments. 7.2 Decoding It is easy to find the best analysis (w, t, v, s) among the candidates represented by the word lattice. Although we use several new features, we can still locate the best analysis by using the same dynamic programming algorithm as in previous studies (Kudo et al</context>
</contexts>
<marker>Kaji, Kitsuregawa, 2011</marker>
<rawString>Nobuhiro Kaji and Masaru Kitsuregawa. 2011. Splitting noun compounds via monolingual and bilingual paraphrasing: A study on Japanese Katakana words. In Proceedings of EMNLP, pages 959–969.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nobuhiro Kaji</author>
<author>Masaru Kitsuregawa</author>
</authors>
<title>Efficient word lattice generation for joint word segmentation and POS tagging in Japanese.</title>
<date>2013</date>
<booktitle>In Proceedings of IJCNLP,</booktitle>
<pages>153--161</pages>
<contexts>
<context position="7136" citStr="Kaji and Kitsuregawa, 2013" startWordPosition="1081" endWordPosition="1084">entation. With their method, informal spellings are merely recognized and not normalized. Therefore, they did not investigate how to exploit the information obtainable from well-spelled equivalents to increase word segmentation accuracy. Some studies also explored integrating the lexical normalization process into word segmentation and POS tagging (Ikeda et al., 2009; Sasano et al., 2013). A strength of our joint model is that it uses rich character-level and word-level features used in state-of-the-art models of joint word segmentation and POS tagging (Kudo et al., 2004; Neubig et al., 2011; Kaji and Kitsuregawa, 2013). Thanks to these features, our model performed much better than Sasano et al.’s system, which is the only publicly available system that jointly conducts lexical normalization, in the experiments (see Section 8). Another advantage is that our model can be trained on a partially annotated corpus. Furthermore, we present a comprehensive evaluation in terms of precision and recall on our microblog corpus. Such an evaluation has not been conducted in previous work due to the lack of annotated corpora.4 3 Lexical Normalization Task This section explains the task of lexical normalization addressed </context>
<context position="16054" citStr="Kaji and Kitsuregawa, 2013" startWordPosition="2479" endWordPosition="2482">w many of their surface forms and normal forms were not registered in the JUMAN dictionary.8 The result suggests that 411 (51.8%) and 74 (9.3%) are not registered in the dictionary. This indicates the effectiveness of lexical normalization for decreasing out-of-vocabulary words. 5 Overview of Joint Model This section gives an overview of our joint model with lexical normalization for accurate word segmentation and POS tagging. 5.1 Lattice-based approach A lattice-based approach has been commonly adopted to perform joint word segmentation and POS tagging (Jiang et al., 2008; Kudo et al., 2004; Kaji and Kitsuregawa, 2013). In this approach, an input sentence is transformed into a word lattice in which the edges are labeled with surface POS tags (Figure 1). Given such a lattice, word segmentation and POS tagging can be performed at the same time by traversing the lattice. A discriminative model is typically used for the traversal. An advantage of this approach is that, while the lattice can represent an exponentially large number of candidate analyses, it can be quickly traversed using dynamic programming (Kudo et al., 2004; Kaji and Kitsuregawa, 2013) or beam search (Jiang et al., 2008). In addition, a discrim</context>
<context position="22510" citStr="Kaji and Kitsuregawa, 2013" startWordPosition="3556" endWordPosition="3559"> handle lengthened words, such as entry (F) in Table 3. While lengthened words can be easily derived using simple rules (Brody and Diakopoulos, 2011; Sasano et al., 2013), such rules infinitely increase the number of entries because an unlimited number of lengthened words can be derived by repeating characters. To address this problem, no lengthened words are added to the normalization dictionary in step 3. We instead use rules to skip repetitive characters in an input sentence when performing dictionary match. 6.2 A hybrid approach A word lattice is generated using both a statistical method (Kaji and Kitsuregawa, 2013) and the normalization dictionary. We begin by generating a word lattice which encodes only word surface forms and surface POS tags (c.f., Figure 1) using the statistical method proposed by Kaji and Kitsuregawa (2013). Interested readers may refer to their paper for details. Each edge in the lattice is then labeled with normal forms and normal POS tags. Note that a single edge can have more than one candidate normal form and normal POS tag. In such a case, new edges are accordingly added to the lattice. The edges are labeled with normal forms and normal POS tags in the following manner. First,</context>
<context position="24358" citStr="Kaji and Kitsuregawa, 2013" startWordPosition="3879" endWordPosition="3882">ce. The lattice traversal with a discriminative model can formally be written as (w, t,v, s) = arg max f(x, w, t, v, s) · 0. (w,t,v,s)∈L(x) Here, x denotes an input sentence, w, t, v, and s denote a sequence of word surface forms, surface POS tags, normal forms, and normal POS tags, respectively, L(x) represents a set of candidate analyses represented by the word lattice, and f(·) and 0 are feature and weight vectors. We now describe features, a decoding method, and two training methods. 7.1 Features We use character-level and word-level features used for word segmentation and POS tagging in (Kaji and Kitsuregawa, 2013). To take advantage of joint model with lexical normalization, the word-level features are extracted from not only surface forms but also normal forms. See (Kaji and Kitsuregawa, 2013) for the original features. In addition, several new features are introduced in this paper. We use the quadruplets (wi, ti, vi, si) 104 and pairs of surface and normal POS tags (ti, si) as binary features to capture probable mappings between ill-spelled words and their well-spelled equivalents. We use another binary feature indicating whether a quadruplet (wi, ti, vi, si) is registered in the normalization dictio</context>
<context position="25962" citStr="Kaji and Kitsuregawa, 2013" startWordPosition="4144" endWordPosition="4147">t is difficult to obtain a precise value of f, it is approximated by the frequency of the surface bigram, (wi−1, ti−1, wi, ti), calculated from a large raw corpus automatically analyzed using a system of joint word segmentation and POS tagging. See Section 8.1 for the raw corpus and system used in the experiments. 7.2 Decoding It is easy to find the best analysis (w, t, v, s) among the candidates represented by the word lattice. Although we use several new features, we can still locate the best analysis by using the same dynamic programming algorithm as in previous studies (Kudo et al., 2004; Kaji and Kitsuregawa, 2013). 7.3 Training on a fully annotated corpus It is straightforward to train the joint model provided with a fully annotated corpus, which is labeled with word surface forms, surface POS tags, normal forms, and normal POS tags. We use structured perceptron (Collins, 2002) for the training (Algorithm 1). The training begins by initializing θ as a zero vector (line 1). It then reads the annotated corpus C (line 2-9). Given a training example, (x, w, t, v, s) ∈ C, the algorithm locates the best analysis, ( w, t, v, s), based on the current weight vector (line 4). If the best analysis differs from th</context>
<context position="28976" citStr="Kaji and Kitsuregawa, 2013" startWordPosition="4703" endWordPosition="4706">notations of the training example, parameter is updated by regarding the latter analysis as an oracle (line 6-8). 8 Experiments We conducted experiments to investigate how the microblog corpus and joint model contribute to improving accuracy of word segmentation and POS tagging in the microblog domain. 8.1 Setting We constructed the normalization dictionary from the JUMAN dictionary 7.0.9 While JUMAN dic9http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?JUMAN 105 tionary contains 750,156 entries, the normalization dictionary contains 112,458,326 entries. Some features taken from the previous study (Kaji and Kitsuregawa, 2013) are induced using a tag dictionary. For this we used two tag dictionaries. One is JUMAN dictionary 7.0 and the other is a tag dictionary constructed by listing surface forms and surface POS tags in the normalization dictionary. To compute the language model features, one billion sentences from Twitter posts were analyzed using MeCab 0.996.10 We used all bigrams appearing at least 10 times in the auto-analyzed sentences. 8.2 Results of word segmentation and POS tagging We first investigated the performance of models trained on an existing annotated corpus form news texts. For this experiment, </context>
</contexts>
<marker>Kaji, Kitsuregawa, 2013</marker>
<rawString>Nobuhiro Kaji and Masaru Kitsuregawa. 2013. Efficient word lattice generation for joint word segmentation and POS tagging in Japanese. In Proceedings of IJCNLP, pages 153–161.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudo</author>
<author>Kaoru Yamamoto</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Applying conditional random fields to Japanese morphological analysis.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>230--237</pages>
<contexts>
<context position="7086" citStr="Kudo et al., 2004" startWordPosition="1073" endWordPosition="1076">l-spelled word recognition and word segmentation. With their method, informal spellings are merely recognized and not normalized. Therefore, they did not investigate how to exploit the information obtainable from well-spelled equivalents to increase word segmentation accuracy. Some studies also explored integrating the lexical normalization process into word segmentation and POS tagging (Ikeda et al., 2009; Sasano et al., 2013). A strength of our joint model is that it uses rich character-level and word-level features used in state-of-the-art models of joint word segmentation and POS tagging (Kudo et al., 2004; Neubig et al., 2011; Kaji and Kitsuregawa, 2013). Thanks to these features, our model performed much better than Sasano et al.’s system, which is the only publicly available system that jointly conducts lexical normalization, in the experiments (see Section 8). Another advantage is that our model can be trained on a partially annotated corpus. Furthermore, we present a comprehensive evaluation in terms of precision and recall on our microblog corpus. Such an evaluation has not been conducted in previous work due to the lack of annotated corpora.4 3 Lexical Normalization Task This section exp</context>
<context position="16025" citStr="Kudo et al., 2004" startWordPosition="2475" endWordPosition="2478">ords, we counted how many of their surface forms and normal forms were not registered in the JUMAN dictionary.8 The result suggests that 411 (51.8%) and 74 (9.3%) are not registered in the dictionary. This indicates the effectiveness of lexical normalization for decreasing out-of-vocabulary words. 5 Overview of Joint Model This section gives an overview of our joint model with lexical normalization for accurate word segmentation and POS tagging. 5.1 Lattice-based approach A lattice-based approach has been commonly adopted to perform joint word segmentation and POS tagging (Jiang et al., 2008; Kudo et al., 2004; Kaji and Kitsuregawa, 2013). In this approach, an input sentence is transformed into a word lattice in which the edges are labeled with surface POS tags (Figure 1). Given such a lattice, word segmentation and POS tagging can be performed at the same time by traversing the lattice. A discriminative model is typically used for the traversal. An advantage of this approach is that, while the lattice can represent an exponentially large number of candidate analyses, it can be quickly traversed using dynamic programming (Kudo et al., 2004; Kaji and Kitsuregawa, 2013) or beam search (Jiang et al., </context>
<context position="25933" citStr="Kudo et al., 2004" startWordPosition="4140" endWordPosition="4143">awa, 2011). Since it is difficult to obtain a precise value of f, it is approximated by the frequency of the surface bigram, (wi−1, ti−1, wi, ti), calculated from a large raw corpus automatically analyzed using a system of joint word segmentation and POS tagging. See Section 8.1 for the raw corpus and system used in the experiments. 7.2 Decoding It is easy to find the best analysis (w, t, v, s) among the candidates represented by the word lattice. Although we use several new features, we can still locate the best analysis by using the same dynamic programming algorithm as in previous studies (Kudo et al., 2004; Kaji and Kitsuregawa, 2013). 7.3 Training on a fully annotated corpus It is straightforward to train the joint model provided with a fully annotated corpus, which is labeled with word surface forms, surface POS tags, normal forms, and normal POS tags. We use structured perceptron (Collins, 2002) for the training (Algorithm 1). The training begins by initializing θ as a zero vector (line 1). It then reads the annotated corpus C (line 2-9). Given a training example, (x, w, t, v, s) ∈ C, the algorithm locates the best analysis, ( w, t, v, s), based on the current weight vector (line 4). If the </context>
<context position="29652" citStr="Kudo et al., 2004" startWordPosition="4814" endWordPosition="4817">tag dictionaries. One is JUMAN dictionary 7.0 and the other is a tag dictionary constructed by listing surface forms and surface POS tags in the normalization dictionary. To compute the language model features, one billion sentences from Twitter posts were analyzed using MeCab 0.996.10 We used all bigrams appearing at least 10 times in the auto-analyzed sentences. 8.2 Results of word segmentation and POS tagging We first investigated the performance of models trained on an existing annotated corpus form news texts. For this experiment, our joint model as well as three state-of-the-art models (Kudo et al., 2004)11(Neubig et al., 2011)12(Kaji and Kitsuregawa, 2013) were trained on Kyoto University Text corpus 4.0 (Kurohashi and Nagao, 1998). Since this training corpus is not annotated with normal forms and normal POS tags, our model was trained using the latent perceptron. Table 5 summarizes the word-level Fl-scores (Kudo et al., 2004) on our microblog corpus. The two columns represent the results for word segmentation (Seg) and joint word segmentation and POS tagging (Seg+Tag), respectively. We also conducted 5-fold crossvalidation on our microblog corpus to evaluate performance improvement when thes</context>
</contexts>
<marker>Kudo, Yamamoto, Matsumoto, 2004</marker>
<rawString>Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto. 2004. Applying conditional random fields to Japanese morphological analysis. In Proceedings of EMNLP, pages 230–237.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sadao Kurohashi</author>
<author>Makoto Nagao</author>
</authors>
<title>Building a Japanese parsed corpus while improving the parsing system.</title>
<date>1998</date>
<booktitle>In Proceedings of LREC,</booktitle>
<pages>719--724</pages>
<contexts>
<context position="12579" citStr="Kurohashi and Nagao, 1998" startWordPosition="1953" endWordPosition="1957">ult, we obtained 1831 sentences as a source of the corpus. Two human participants annotated the 1831 sentences with surface forms and surface POS tags. Since much effort has already been done to annotate corpora with this information, the annotation process here follows the guidelines used to 5http://nlp.ist.i.kyoto-u.ac.jp/index.php?JUMAN 6In this paper, we use simplified POS tags for explanation purposes. Remind that these tags are different from the original ones defined in JUMAN POS tag set. 7https://stream.twitter.com/1.1/statuses/sample.json 101 develop such corpora in previous studies (Kurohashi and Nagao, 1998; Hashimoto et al., 2011). The two participants also annotated ill-spelled words with their normal forms and normal POS tags. Although this paper targets only informal phonological variations (c.f., Section 3), other types of ill-spelled words were also annotated to investigate their frequency distribution in microblog texts. Specifically, besides informal phonological variations, spelling errors and Twitter-specific abbreviations were annotated. As a result, 833 ill-spelled words were identified (Table 2). They were all annotated with normal forms and normal POS tags. 4.2 Agreement study We i</context>
<context position="29782" citStr="Kurohashi and Nagao, 1998" startWordPosition="4833" endWordPosition="4836">urface POS tags in the normalization dictionary. To compute the language model features, one billion sentences from Twitter posts were analyzed using MeCab 0.996.10 We used all bigrams appearing at least 10 times in the auto-analyzed sentences. 8.2 Results of word segmentation and POS tagging We first investigated the performance of models trained on an existing annotated corpus form news texts. For this experiment, our joint model as well as three state-of-the-art models (Kudo et al., 2004)11(Neubig et al., 2011)12(Kaji and Kitsuregawa, 2013) were trained on Kyoto University Text corpus 4.0 (Kurohashi and Nagao, 1998). Since this training corpus is not annotated with normal forms and normal POS tags, our model was trained using the latent perceptron. Table 5 summarizes the word-level Fl-scores (Kudo et al., 2004) on our microblog corpus. The two columns represent the results for word segmentation (Seg) and joint word segmentation and POS tagging (Seg+Tag), respectively. We also conducted 5-fold crossvalidation on our microblog corpus to evaluate performance improvement when these models are trained on microblog texts (Table 6). In addition to the models in Table 5, results of a rule-based system (Sasano et</context>
</contexts>
<marker>Kurohashi, Nagao, 1998</marker>
<rawString>Sadao Kurohashi and Makoto Nagao. 1998. Building a Japanese parsed corpus while improving the parsing system. In Proceedings of LREC, pages 719–724.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wang Ling</author>
<author>Chris Dyer</author>
<author>Alan W Black</author>
<author>Isabel Trancoso</author>
</authors>
<title>Paraphrasing 4 microblog normalization.</title>
<date>2013</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>73--84</pages>
<contexts>
<context position="6304" citStr="Ling et al., 2013" startWordPosition="953" endWordPosition="956">nnotated English microblog posts with 3Please contact the first author for this corpus. POS tags. Han and Baldwin (2011) released a microblog corpus annotated with normalized forms of words. A Chinese microblog corpus annotated with word boundaries was developed for SIGHAN bakeoff (Duan et al., 2012). However, there are no microblog corpora annotated with word boundaries, POS tags, and normalized sentences. There has been a surge of interest in lexical normalization with the advent of microblogs (Han and Baldwin, 2011; Liu et al., 2012; Han et al., 2012; Wang and Ng, 2013; Zhang et al., 2013; Ling et al., 2013; Yang and Eisenstein, 2013; Wang et al., 2013). However, these studies did not address enhancing word segmentation. Wang et al. (2013) proposed a method of joint ill-spelled word recognition and word segmentation. With their method, informal spellings are merely recognized and not normalized. Therefore, they did not investigate how to exploit the information obtainable from well-spelled equivalents to increase word segmentation accuracy. Some studies also explored integrating the lexical normalization process into word segmentation and POS tagging (Ikeda et al., 2009; Sasano et al., 2013). A </context>
</contexts>
<marker>Ling, Dyer, Black, Trancoso, 2013</marker>
<rawString>Wang Ling, Chris Dyer, Alan W Black, and Isabel Trancoso. 2013. Paraphrasing 4 microblog normalization. In Proceedings of EMNLP, pages 73–84.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaohua Liu</author>
<author>Ming Zhou</author>
<author>Xiangyang Zhou</author>
<author>Zhongyang Fu</author>
<author>Furu Wei</author>
</authors>
<title>Joint inference of named entity recognition and normalization for tweets.</title>
<date>2012</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>526--535</pages>
<contexts>
<context position="6228" citStr="Liu et al., 2012" startWordPosition="937" endWordPosition="940">ich linguistic information. Gimpel et al. (2011) and Foster et al. (2011) annotated English microblog posts with 3Please contact the first author for this corpus. POS tags. Han and Baldwin (2011) released a microblog corpus annotated with normalized forms of words. A Chinese microblog corpus annotated with word boundaries was developed for SIGHAN bakeoff (Duan et al., 2012). However, there are no microblog corpora annotated with word boundaries, POS tags, and normalized sentences. There has been a surge of interest in lexical normalization with the advent of microblogs (Han and Baldwin, 2011; Liu et al., 2012; Han et al., 2012; Wang and Ng, 2013; Zhang et al., 2013; Ling et al., 2013; Yang and Eisenstein, 2013; Wang et al., 2013). However, these studies did not address enhancing word segmentation. Wang et al. (2013) proposed a method of joint ill-spelled word recognition and word segmentation. With their method, informal spellings are merely recognized and not normalized. Therefore, they did not investigate how to exploit the information obtainable from well-spelled equivalents to increase word segmentation accuracy. Some studies also explored integrating the lexical normalization process into wor</context>
</contexts>
<marker>Liu, Zhou, Zhou, Fu, Wei, 2012</marker>
<rawString>Xiaohua Liu, Ming Zhou, Xiangyang Zhou, Zhongyang Fu, and Furu Wei. 2012. Joint inference of named entity recognition and normalization for tweets. In Proceedings of ACL, pages 526–535.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graham Neubig</author>
<author>Yousuke Nakata</author>
<author>Shinsuke Mori</author>
</authors>
<title>Pointwise prediction for robust adaptable Japanese morphological analysis.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>529--533</pages>
<contexts>
<context position="7107" citStr="Neubig et al., 2011" startWordPosition="1077" endWordPosition="1080">gnition and word segmentation. With their method, informal spellings are merely recognized and not normalized. Therefore, they did not investigate how to exploit the information obtainable from well-spelled equivalents to increase word segmentation accuracy. Some studies also explored integrating the lexical normalization process into word segmentation and POS tagging (Ikeda et al., 2009; Sasano et al., 2013). A strength of our joint model is that it uses rich character-level and word-level features used in state-of-the-art models of joint word segmentation and POS tagging (Kudo et al., 2004; Neubig et al., 2011; Kaji and Kitsuregawa, 2013). Thanks to these features, our model performed much better than Sasano et al.’s system, which is the only publicly available system that jointly conducts lexical normalization, in the experiments (see Section 8). Another advantage is that our model can be trained on a partially annotated corpus. Furthermore, we present a comprehensive evaluation in terms of precision and recall on our microblog corpus. Such an evaluation has not been conducted in previous work due to the lack of annotated corpora.4 3 Lexical Normalization Task This section explains the task of lex</context>
<context position="29675" citStr="Neubig et al., 2011" startWordPosition="4817" endWordPosition="4820"> is JUMAN dictionary 7.0 and the other is a tag dictionary constructed by listing surface forms and surface POS tags in the normalization dictionary. To compute the language model features, one billion sentences from Twitter posts were analyzed using MeCab 0.996.10 We used all bigrams appearing at least 10 times in the auto-analyzed sentences. 8.2 Results of word segmentation and POS tagging We first investigated the performance of models trained on an existing annotated corpus form news texts. For this experiment, our joint model as well as three state-of-the-art models (Kudo et al., 2004)11(Neubig et al., 2011)12(Kaji and Kitsuregawa, 2013) were trained on Kyoto University Text corpus 4.0 (Kurohashi and Nagao, 1998). Since this training corpus is not annotated with normal forms and normal POS tags, our model was trained using the latent perceptron. Table 5 summarizes the word-level Fl-scores (Kudo et al., 2004) on our microblog corpus. The two columns represent the results for word segmentation (Seg) and joint word segmentation and POS tagging (Seg+Tag), respectively. We also conducted 5-fold crossvalidation on our microblog corpus to evaluate performance improvement when these models are trained on</context>
<context position="33599" citStr="Neubig et al. (2011)" startWordPosition="5424" endWordPosition="5427">ze ill-spelled words. Table 7 illustrates precision, recall, and Flscore for the lexical normalization task. To put 106 Table 6: Results of 5-fold cross-validation on microblog corpus. Seg Seg+Tag Kudo04 82.7 71.7 Neubig11 88.6 75.9 Kaji13 90.9 82.1 Sasano13 82.7 73.3 Proposed 91.3 83.2 Proposed (latent) 91.4 83.7 Table 7: Results of lexical normalization task in terms of precision, recall, and F1-score. Precision Recall Fl Neubig11 69.2 35.9 47.3 Proposed 77.1 44.6 56.6 Proposed (latent) 53.7 24.7 33.9 the results into context, we report on the baseline results of a tagging model proposed by Neubig et al. (2011). This baseline conducts lexical normalization by regarding it as two independent tagging tasks (i.e., tasks of tagging normal forms and normal POS tags). The result of the baseline model is also obtained using 5-fold crossvalidation. Table 7 illustrates that Proposed performed significantly better than the simple tagging model, Neubig11. This suggests the effectiveness of our joint model. On the other hand, Proposed (latent) performed poorly in this task. From this result, we can argue that Proposed (latent) can achieve superior performance in word segmentation and POS tagging (Table 6) becau</context>
</contexts>
<marker>Neubig, Nakata, Mori, 2011</marker>
<rawString>Graham Neubig, Yousuke Nakata, and Shinsuke Mori. 2011. Pointwise prediction for robust adaptable Japanese morphological analysis. In Proceedings of ACL, pages 529–533.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Itsumi Saito</author>
<author>Kugatsu Sadamitsu</author>
<author>Hisako Asano</author>
<author>Yoshihiro Matsuo</author>
</authors>
<title>Morphological analysis for Japanese noisy text based on character-level and word-level normalization.</title>
<date>2014</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>1773--1782</pages>
<contexts>
<context position="7956" citStr="Saito et al. (2014)" startWordPosition="1215" endWordPosition="1218"> Section 8). Another advantage is that our model can be trained on a partially annotated corpus. Furthermore, we present a comprehensive evaluation in terms of precision and recall on our microblog corpus. Such an evaluation has not been conducted in previous work due to the lack of annotated corpora.4 3 Lexical Normalization Task This section explains the task of lexical normalization addressed in this paper. Since lexical normalization is a relatively new research topic, there are no precise definitions of a lexical normalization task that are widely accepted by researchers. 4Very recently, Saito et al. (2014) conducted similar empirical evaluation on microblog corpus. However, they used biased dataset, in which every sentence includes at least one ill-spelled words. 100 Table 1: Examples of our target ill-spelled words and their well-spelled equivalents. Phonemes are shown between slashes. English translations are provided in parentheses. Ill-spelled word Well-spelled equivalent すげえ /sugee/ すごい /sugoi/ (great) 戻ろ /modoro/ 戻ろう /modorou/ (going to return) うまいいいい /umaiiii/ うまい /umai/ (yummy) Therefore, it is important to clarify our task setting before discussing our joint model. 3.1 Target ill-spell</context>
</contexts>
<marker>Saito, Sadamitsu, Asano, Matsuo, 2014</marker>
<rawString>Itsumi Saito, Kugatsu Sadamitsu, Hisako Asano, and Yoshihiro Matsuo. 2014. Morphological analysis for Japanese noisy text based on character-level and word-level normalization. In Proceedings of COLING, pages 1773–1782.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takeshi Sakaki</author>
<author>Makoto Okazaki</author>
<author>Yutaka Matsuo</author>
</authors>
<title>Earthquak shakes Twitter users: real-time event detection by social sensors.</title>
<date>2010</date>
<booktitle>In Proceedings of WWW,</booktitle>
<pages>851--860</pages>
<contexts>
<context position="1391" citStr="Sakaki et al., 2010" startWordPosition="191" endWordPosition="194">ive evaluation of their performance. Our joint model with lexical normalization handles the orthographic diversity of microblog texts. We conducted an experiment to demonstrate that the corpus and model substantially contribute to boosting accuracy. 1 Introduction Microblogs, such as Twitter1 and Weibo2, have recently become an important target of NLP technology. Since microblogs offer an instant way of posting textual messages, they have been given increasing attention as valuable sources for such actions as mining opinions (Jiang et al., 2011) and detecting sudden events such as earthquake (Sakaki et al., 2010). However, many studies have reported that current NLP tools do not perform well on microblog texts (Foster et al., 2011; Gimpel et al., 2011). In the case of Japanese text processing, the most serious problem is poor accuracy of word segmentation and POS tagging. Since these two tasks are positioned as the fundamental step in the text processing pipeline, their accuracy is vital for all downstream applications. 1https://twitter.com 2https://www.weibo.com 1.1 Development of annotated corpus The main obstacle that makes word segmentation and POS tagging in the microblog domain challenging is th</context>
</contexts>
<marker>Sakaki, Okazaki, Matsuo, 2010</marker>
<rawString>Takeshi Sakaki, Makoto Okazaki, and Yutaka Matsuo. 2010. Earthquak shakes Twitter users: real-time event detection by social sensors. In Proceedings of WWW, pages 851–860.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryohei Sasano</author>
<author>Sadao Kurohashi</author>
<author>Manabu Okumura</author>
</authors>
<title>A simple approach to unknown word processing in Japanese morphological analysis.</title>
<date>2013</date>
<booktitle>In Proceedings of IJCNLP,</booktitle>
<pages>162--170</pages>
<contexts>
<context position="6900" citStr="Sasano et al., 2013" startWordPosition="1042" endWordPosition="1045"> 2013; Ling et al., 2013; Yang and Eisenstein, 2013; Wang et al., 2013). However, these studies did not address enhancing word segmentation. Wang et al. (2013) proposed a method of joint ill-spelled word recognition and word segmentation. With their method, informal spellings are merely recognized and not normalized. Therefore, they did not investigate how to exploit the information obtainable from well-spelled equivalents to increase word segmentation accuracy. Some studies also explored integrating the lexical normalization process into word segmentation and POS tagging (Ikeda et al., 2009; Sasano et al., 2013). A strength of our joint model is that it uses rich character-level and word-level features used in state-of-the-art models of joint word segmentation and POS tagging (Kudo et al., 2004; Neubig et al., 2011; Kaji and Kitsuregawa, 2013). Thanks to these features, our model performed much better than Sasano et al.’s system, which is the only publicly available system that jointly conducts lexical normalization, in the experiments (see Section 8). Another advantage is that our model can be trained on a partially annotated corpus. Furthermore, we present a comprehensive evaluation in terms of pre</context>
<context position="19703" citStr="Sasano et al., 2013" startWordPosition="3075" endWordPosition="3078">method of generating a word lattice from an input sentence. 6.1 Construction of normalization dictionary Although large-scale normalization dictionaries are difficult to obtain, tag dictionaries, which list pairs of word surface forms and their surface POS tags (Table 4), are widely available in many languages including Japanese. Therefore, we use an existing tag dictionary to construct the normalization dictionary. Due to space limitations, we give only a brief overview of our construction method, omitting its details. We note that our method uses hand-crafted rules similar to those used in (Sasano et al., 2013); hence, the proposal of this method is not an important contribution. To make our experimental results reproducible, our normalization dictionary, as well as a tool for constructing it, is released as supplementary material. Our method of constructing the normalization dictionary takes three steps. The following explains each step using Tables 3 and 4 as running examples. Noun Noun Noun Verb Noun Noun Suffix Particle  ி 㒔 䛻 ఫ䜐 Noun (ⱥㄒ, Noun) Noun (ḷ, Noun) Suffix (䛺䛔, Suffix) Particle (䛽, Particle) ⱥㄒ 䜟䛛 䜣 䛽䛗 Verb (䜟䛛䜙, Verb) Suffix (䛺䛔, Suffix) 103 Step 1 A tag dictionary generally contai</context>
<context position="21670" citStr="Sasano et al., 2013" startWordPosition="3412" endWordPosition="3415">le 3 from entry (c) in Table 4. For well-spelled words, on the other hand, the normal forms and normal POS tags are simply set the same as the surface forms and surface POS tags. For example, entries (A), (C), and (E) in Table 3 are generated from entries (a), (b), and (d) in Table 4, respectively. Step 3 Because the normalization dictionary constructed in step 2 contains only a few illspelled words, it is expanded in this step. For this purpose, we use hand-crafted rules to derive illspelled words from the entries already registered in the normalization dictionary. Some rules are taken from (Sasano et al., 2013), while the others are newly tailored. In Table 3, for example, entry (B) is derived from entry (A) by applying the rule that substitutes “ごい” /goi/ with “げえ” /gee/. A small problem that arises in step 3 is how to handle lengthened words, such as entry (F) in Table 3. While lengthened words can be easily derived using simple rules (Brody and Diakopoulos, 2011; Sasano et al., 2013), such rules infinitely increase the number of entries because an unlimited number of lengthened words can be derived by repeating characters. To address this problem, no lengthened words are added to the normalizatio</context>
<context position="30393" citStr="Sasano et al., 2013" startWordPosition="4931" endWordPosition="4934">ao, 1998). Since this training corpus is not annotated with normal forms and normal POS tags, our model was trained using the latent perceptron. Table 5 summarizes the word-level Fl-scores (Kudo et al., 2004) on our microblog corpus. The two columns represent the results for word segmentation (Seg) and joint word segmentation and POS tagging (Seg+Tag), respectively. We also conducted 5-fold crossvalidation on our microblog corpus to evaluate performance improvement when these models are trained on microblog texts (Table 6). In addition to the models in Table 5, results of a rule-based system (Sasano et al., 2013)13 and our joint model trained using the perceptron algorithm are also presented. Notice that Proposed and Proposed (latent) represent our model trained using perceptron and latent perceptron, respectively. From Tables 5 and 6, as expected, we see that the models trained on news texts performed poorly on microblog texts, while their performance significantly boosted when trained on the microblog texts. This demonstrates the importance of corpus annotation. An exception was Kudo04. Its perfor10https://code.google.com/p/mecab 11https://code.google.com/p/mecab 12http://www.phontron.com/kytea/ 13h</context>
</contexts>
<marker>Sasano, Kurohashi, Okumura, 2013</marker>
<rawString>Ryohei Sasano, Sadao Kurohashi, and Manabu Okumura. 2013. A simple approach to unknown word processing in Japanese morphological analysis. In Proceedings of IJCNLP, pages 162–170.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xu Sun</author>
<author>Takuya Matsuzaki</author>
<author>Wenjie Li</author>
</authors>
<title>Latent structured perceptrons for large-scale learning with hidden information.</title>
<date>2013</date>
<journal>IEEE Transactions on Knowledge and Data Engineering,</journal>
<volume>25</volume>
<issue>9</issue>
<contexts>
<context position="27777" citStr="Sun et al., 2013" startWordPosition="4507" endWordPosition="4510">− f(x, ˆw, ˆt, ˆv, ˆs) 7: end if 8: end for 9: end for 10: return AVERAGE(θ) Algorithm 2 Latent perceptron training 1: θ +— 0 2: for i = 1... m do 3: for (x, w, t) E C′ do 4: ( ˆw, ˆt, ˆv, ˆs) +— DECODING(x, θ) 5: (w, t, ¯v, ¯s) +— CONSTRAINEDDECODING(x, θ) 6: if w =� wˆ or t =� tˆ then 7: θ +— θ + f(x, w, t, ¯v, ¯s) − f(x, ˆw, ˆt, ˆv, ˆs) 8: end if 9: end for 10: end for 11: return AVERAGE(θ) eration motivates us to explore training our model with less supervision. We specifically explore using a corpus annotated with only word boundaries and POS tags. We use the latent perceptron algorithm (Sun et al., 2013) to train the joint model from such a partially annotated corpus (Algorithm 2). In this scenario, a training example is a sentence x paired with a sequence of word surface forms w and surface POS tags t (c.f., line 3). Similarly to the perceptron algorithm, we locate the best analysis ( w, t, v, s) for a given training example, (line 4). We also locate the best analysis, (w, t, v, s), among those having the same surface forms w and surface POS tags t as the training example (line 5). If the surface forms and surface POS tags of the former analysis differ from the annotations of the training ex</context>
</contexts>
<marker>Sun, Matsuzaki, Li, 2013</marker>
<rawString>Xu Sun, Takuya Matsuzaki, and Wenjie Li. 2013. Latent structured perceptrons for large-scale learning with hidden information. IEEE Transactions on Knowledge and Data Engineering, 25(9):2063– 2075.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aobo Wang</author>
<author>Min-Yen Kan</author>
</authors>
<title>Mining informal language from Chinese microtext: Joint word recognition and segmentation.</title>
<date>2013</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>731--741</pages>
<marker>Wang, Kan, 2013</marker>
<rawString>Aobo Wang and Min-Yen Kan. 2013. Mining informal language from Chinese microtext: Joint word recognition and segmentation. In Proceedings of ACL, pages 731–741.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pidong Wang</author>
<author>Hwee Tou Ng</author>
</authors>
<title>A beam-search decoder for normalization of social media text with application to machine translation.</title>
<date>2013</date>
<booktitle>In Proceedings of NAACL,</booktitle>
<pages>471--481</pages>
<contexts>
<context position="6265" citStr="Wang and Ng, 2013" startWordPosition="945" endWordPosition="948">t al. (2011) and Foster et al. (2011) annotated English microblog posts with 3Please contact the first author for this corpus. POS tags. Han and Baldwin (2011) released a microblog corpus annotated with normalized forms of words. A Chinese microblog corpus annotated with word boundaries was developed for SIGHAN bakeoff (Duan et al., 2012). However, there are no microblog corpora annotated with word boundaries, POS tags, and normalized sentences. There has been a surge of interest in lexical normalization with the advent of microblogs (Han and Baldwin, 2011; Liu et al., 2012; Han et al., 2012; Wang and Ng, 2013; Zhang et al., 2013; Ling et al., 2013; Yang and Eisenstein, 2013; Wang et al., 2013). However, these studies did not address enhancing word segmentation. Wang et al. (2013) proposed a method of joint ill-spelled word recognition and word segmentation. With their method, informal spellings are merely recognized and not normalized. Therefore, they did not investigate how to exploit the information obtainable from well-spelled equivalents to increase word segmentation accuracy. Some studies also explored integrating the lexical normalization process into word segmentation and POS tagging (Ikeda</context>
<context position="8896" citStr="Wang and Ng (2013)" startWordPosition="1353" endWordPosition="1356">n parentheses. Ill-spelled word Well-spelled equivalent すげえ /sugee/ すごい /sugoi/ (great) 戻ろ /modoro/ 戻ろう /modorou/ (going to return) うまいいいい /umaiiii/ うまい /umai/ (yummy) Therefore, it is important to clarify our task setting before discussing our joint model. 3.1 Target ill-spelled words Many studies on lexical normalization have pointed out that phonological factors are deeply involved in the process of deriving ill-spelled words. Xia et al. (2006) investigated a Chinese chat corpus and reported that 99.2% of the ill-spelled words were derived by phonetic mapping from well-spelled equivalents. Wang and Ng (2013) analyzed 200 Chinese messages from Weibo and 200 English SMS messages from the NUS SMS corpus (How and Kan, 2005). Their analysis revealed that most ill-spelled words were derived from well-spelled equivalents based on pronunciation similarity. On top of these investigations, we focused on ill-spelled words that are derived by phonological mapping from well-spelled words by assuming that such ill-spelled words are dominant in Japanese microblogs as well. We also assume that these ill-spelled words can be normalized into well-spelled equivalents on a word-to-word basis, as assumed in a previou</context>
</contexts>
<marker>Wang, Ng, 2013</marker>
<rawString>Pidong Wang and Hwee Tou Ng. 2013. A beam-search decoder for normalization of social media text with application to machine translation. In Proceedings of NAACL, pages 471–481.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aobo Wang</author>
<author>Min-Yen Kan</author>
<author>Daniel Andrade</author>
<author>Takashi Onishi</author>
<author>Kai Ishikawa</author>
</authors>
<title>Chinese informal word normalization: an experimental study.</title>
<date>2013</date>
<booktitle>In Proceedings of IJCNLP,</booktitle>
<pages>127--135</pages>
<contexts>
<context position="6351" citStr="Wang et al., 2013" startWordPosition="961" endWordPosition="964">contact the first author for this corpus. POS tags. Han and Baldwin (2011) released a microblog corpus annotated with normalized forms of words. A Chinese microblog corpus annotated with word boundaries was developed for SIGHAN bakeoff (Duan et al., 2012). However, there are no microblog corpora annotated with word boundaries, POS tags, and normalized sentences. There has been a surge of interest in lexical normalization with the advent of microblogs (Han and Baldwin, 2011; Liu et al., 2012; Han et al., 2012; Wang and Ng, 2013; Zhang et al., 2013; Ling et al., 2013; Yang and Eisenstein, 2013; Wang et al., 2013). However, these studies did not address enhancing word segmentation. Wang et al. (2013) proposed a method of joint ill-spelled word recognition and word segmentation. With their method, informal spellings are merely recognized and not normalized. Therefore, they did not investigate how to exploit the information obtainable from well-spelled equivalents to increase word segmentation accuracy. Some studies also explored integrating the lexical normalization process into word segmentation and POS tagging (Ikeda et al., 2009; Sasano et al., 2013). A strength of our joint model is that it uses ric</context>
</contexts>
<marker>Wang, Kan, Andrade, Onishi, Ishikawa, 2013</marker>
<rawString>Aobo Wang, Min-Yen Kan, Daniel Andrade, Takashi Onishi, and Kai Ishikawa. 2013. Chinese informal word normalization: an experimental study. In Proceedings of IJCNLP, pages 127–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yunqing Xia</author>
<author>Kam-Fai Wong</author>
<author>Wenjie Li</author>
</authors>
<title>A phonetic-based approach to Chinese chat text normalization.</title>
<date>2006</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>993--1000</pages>
<contexts>
<context position="8729" citStr="Xia et al. (2006)" startWordPosition="1326" endWordPosition="1329">rds. 100 Table 1: Examples of our target ill-spelled words and their well-spelled equivalents. Phonemes are shown between slashes. English translations are provided in parentheses. Ill-spelled word Well-spelled equivalent すげえ /sugee/ すごい /sugoi/ (great) 戻ろ /modoro/ 戻ろう /modorou/ (going to return) うまいいいい /umaiiii/ うまい /umai/ (yummy) Therefore, it is important to clarify our task setting before discussing our joint model. 3.1 Target ill-spelled words Many studies on lexical normalization have pointed out that phonological factors are deeply involved in the process of deriving ill-spelled words. Xia et al. (2006) investigated a Chinese chat corpus and reported that 99.2% of the ill-spelled words were derived by phonetic mapping from well-spelled equivalents. Wang and Ng (2013) analyzed 200 Chinese messages from Weibo and 200 English SMS messages from the NUS SMS corpus (How and Kan, 2005). Their analysis revealed that most ill-spelled words were derived from well-spelled equivalents based on pronunciation similarity. On top of these investigations, we focused on ill-spelled words that are derived by phonological mapping from well-spelled words by assuming that such ill-spelled words are dominant in Ja</context>
</contexts>
<marker>Xia, Wong, Li, 2006</marker>
<rawString>Yunqing Xia, Kam-Fai Wong, and Wenjie Li. 2006. A phonetic-based approach to Chinese chat text normalization. In Proceedings of ACL, pages 993– 1000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yi Yang</author>
<author>Jacob Eisenstein</author>
</authors>
<title>A log-linear model for unsupervised text normalization.</title>
<date>2013</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>61--72</pages>
<contexts>
<context position="6331" citStr="Yang and Eisenstein, 2013" startWordPosition="957" endWordPosition="960">croblog posts with 3Please contact the first author for this corpus. POS tags. Han and Baldwin (2011) released a microblog corpus annotated with normalized forms of words. A Chinese microblog corpus annotated with word boundaries was developed for SIGHAN bakeoff (Duan et al., 2012). However, there are no microblog corpora annotated with word boundaries, POS tags, and normalized sentences. There has been a surge of interest in lexical normalization with the advent of microblogs (Han and Baldwin, 2011; Liu et al., 2012; Han et al., 2012; Wang and Ng, 2013; Zhang et al., 2013; Ling et al., 2013; Yang and Eisenstein, 2013; Wang et al., 2013). However, these studies did not address enhancing word segmentation. Wang et al. (2013) proposed a method of joint ill-spelled word recognition and word segmentation. With their method, informal spellings are merely recognized and not normalized. Therefore, they did not investigate how to exploit the information obtainable from well-spelled equivalents to increase word segmentation accuracy. Some studies also explored integrating the lexical normalization process into word segmentation and POS tagging (Ikeda et al., 2009; Sasano et al., 2013). A strength of our joint model</context>
</contexts>
<marker>Yang, Eisenstein, 2013</marker>
<rawString>Yi Yang and Jacob Eisenstein. 2013. A log-linear model for unsupervised text normalization. In Proceedings of EMNLP, pages 61–72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Congle Zhang</author>
<author>Tyler Baldwin</author>
<author>Howard Ho</author>
<author>Benny Kimelfeld</author>
<author>Yunyao Li</author>
</authors>
<title>Adaptive parsercentric text normalization.</title>
<date>2013</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>1159--1168</pages>
<contexts>
<context position="6285" citStr="Zhang et al., 2013" startWordPosition="949" endWordPosition="952">ster et al. (2011) annotated English microblog posts with 3Please contact the first author for this corpus. POS tags. Han and Baldwin (2011) released a microblog corpus annotated with normalized forms of words. A Chinese microblog corpus annotated with word boundaries was developed for SIGHAN bakeoff (Duan et al., 2012). However, there are no microblog corpora annotated with word boundaries, POS tags, and normalized sentences. There has been a surge of interest in lexical normalization with the advent of microblogs (Han and Baldwin, 2011; Liu et al., 2012; Han et al., 2012; Wang and Ng, 2013; Zhang et al., 2013; Ling et al., 2013; Yang and Eisenstein, 2013; Wang et al., 2013). However, these studies did not address enhancing word segmentation. Wang et al. (2013) proposed a method of joint ill-spelled word recognition and word segmentation. With their method, informal spellings are merely recognized and not normalized. Therefore, they did not investigate how to exploit the information obtainable from well-spelled equivalents to increase word segmentation accuracy. Some studies also explored integrating the lexical normalization process into word segmentation and POS tagging (Ikeda et al., 2009; Sasan</context>
</contexts>
<marker>Zhang, Baldwin, Ho, Kimelfeld, Li, 2013</marker>
<rawString>Congle Zhang, Tyler Baldwin, Howard Ho, Benny Kimelfeld, and Yunyao Li. 2013. Adaptive parsercentric text normalization. In Proceedings of ACL, pages 1159–1168.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000014">
<title confidence="0.9987935">
Evaluating Neural Word Representations in
Tensor-Based Compositional Settings
</title>
<author confidence="0.999544">
Dmitrijs Milajevs&apos; Dimitri Kartsaklis2 Mehrnoosh Sadrzadeh&apos; Matthew Purver&apos;
</author>
<affiliation confidence="0.908065333333333">
&apos;Queen Mary University of London
School of Electronic Engineering
and Computer Science
</affiliation>
<address confidence="0.822614">
Mile End Road, London, UK
</address>
<email confidence="0.997376">
{d.milajevs,m.sadrzadeh,m.purver}@qmul.ac.uk
</email>
<sectionHeader confidence="0.993867" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999986318181818">
We provide a comparative study be-
tween neural word representations and
traditional vector spaces based on co-
occurrence counts, in a number of com-
positional tasks. We use three differ-
ent semantic spaces and implement seven
tensor-based compositional models, which
we then test (together with simpler ad-
ditive and multiplicative approaches) in
tasks involving verb disambiguation and
sentence similarity. To check their scala-
bility, we additionally evaluate the spaces
using simple compositional methods on
larger-scale tasks with less constrained
language: paraphrase detection and di-
alogue act tagging. In the more con-
strained tasks, co-occurrence vectors are
competitive, although choice of composi-
tional method is important; on the larger-
scale tasks, they are outperformed by neu-
ral word embeddings, which show robust,
stable performance across the tasks.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9998693125">
Neural word embeddings (Bengio et al., 2006;
Collobert and Weston, 2008; Mikolov et al.,
2013a) have received much attention in the dis-
tributional semantics community, and have shown
state-of-the-art performance in many natural lan-
guage processing tasks. While they have been
compared with co-occurrence based models in
simple similarity tasks at the word level (Levy et
al., 2014; Baroni et al., 2014), we are aware of
only one work that attempts a comparison of the
two approaches in compositional settings (Blacoe
and Lapata, 2012), and this is limited to additive
and multiplicative composition, compared against
composition via a neural autoencoder.
The purpose of this paper is to provide a more
complete picture regarding the potential of neu-
</bodyText>
<affiliation confidence="0.9949755">
2University of Oxford
Department of Computer Science
</affiliation>
<address confidence="0.939104">
Parks Road, Oxford, UK
</address>
<email confidence="0.992906">
dimitri.kartsaklis@cs.ox.ac.uk
</email>
<bodyText confidence="0.999977829268293">
ral word embeddings in compositional tasks, and
meaningfully compare them with the traditional
distributional approach based on co-occurrence
counts. We are especially interested in investi-
gating the performance of neural word vectors in
compositional models involving general mathe-
matical composition operators, rather than in the
more task- or domain-specific deep-learning com-
positional settings they have generally been used
with so far (for example, by Socher et al. (2012),
Kalchbrenner and Blunsom (2013) and many oth-
ers).
In particular, this is the first large-scale study
to date that applies neural word representations in
tensor-based compositional distributional models
of meaning similar to those formalized by Coecke
et al. (2010). We test a range of implementations
based on this framework, together with additive
and multiplicative approaches (Mitchell and Lap-
ata, 2008), in a variety of different tasks. Specif-
ically, we use the verb disambiguation task of
Grefenstette and Sadrzadeh (2011a) and the tran-
sitive sentence similarity task of Kartsaklis and
Sadrzadeh (2014) as small-scale focused experi-
ments on pre-defined sentence structures. Addi-
tionally, we evaluate our vector spaces on para-
phrase detection (using the Microsoft Research
Paraphrase Corpus of Dolan et al. (2005)) and di-
alogue act tagging using the Switchboard Corpus
(see e.g. (Stolcke et al., 2000)).
In all of the above tasks, we compare the neural
word embeddings of Mikolov et al. (2013a) with
two vector spaces both based on co-occurrence
counts and produced by standard distributional
techniques, as described in detail below. The gen-
eral picture we get from the results is that in almost
all cases the neural vectors are more effective than
the traditional approaches.
We proceed as follows: Section 2 provides a
concise introduction to distributional word repre-
sentations in natural language processing. Section
</bodyText>
<page confidence="0.960514">
708
</page>
<note confidence="0.9123965">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 708–719,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.993782625">
3 takes a closer look to the subject of composi-
tionality in vector space models of meaning and
describes the range of compositional operators ex-
amined here. In Section 4 we provide details about
the vector spaces used in the experiments. Our ex-
perimental work is described in detail in Section 5,
and the results are discussed in Section 6. Finally,
Section 7 provides conclusions.
</bodyText>
<sectionHeader confidence="0.949755" genericHeader="method">
2 Meaning representation
</sectionHeader>
<bodyText confidence="0.999063651162791">
There are several approaches to the representation
of word, phrase and sentence meaning. As nat-
ural languages are highly creative and it is very
rare to see the same sentence twice, any practical
approach dealing with large text segments must
be compositional, constructing the meaning of
phrases and sentences from their constituent parts.
The ideal method would therefore express not
only the similarity in meaning between those con-
stituent parts, but also between the results of their
composition, and do this in ways which fit with
linguistic structure and generalisations thereof.
Formal semantics Formal approaches to the
semantics of natural language have long built
upon the classical idea of compositionality –
that the meaning of a sentence is a function
of the meanings of its parts (Frege, 1892). In
compositional type-logical approaches, predicate-
argument structures representing phrases and sen-
tences are built from their constituent parts by β-
reduction within the lambda calculus framework
(Montague, 1970): for example, given a represen-
tation of John as john&apos; and sleeps as Ax.sleep&apos;(x),
the meaning of the sentence “John sleeps”
can be constructed as Ax.sleep&apos;(x)(john&apos;) =
sleep&apos;(john&apos;). Given a suitable pairing between
words and semantic representations of them, this
method can produce structured sentential repre-
sentations with broad coverage and good gener-
alisability (see e.g. (Bos, 2008)). The above logi-
cal approach is extremely powerful because it can
capture complex aspects of meaning such as quan-
tifiers and their interaction (see e.g. (Copestake et
al., 2005)), and enables inference using well stud-
ied and developed logical methods (see e.g. (Bos
and Gabsdil, 2000)).
Distributional hypothesis However, such for-
mal approaches are less able to express similar-
ity in meaning. We would like to capture the
intuition that while John and Mary are distinct,
they are rather similar to each other (both of them
are humans) and dissimilar to words such as dog,
pavement or idea. The same applies at the phrase
and sentence level: “dogs chase cats” is similar in
meaning to “hounds pursue kittens”, but less so to
“cats chase dogs” (despite the lexical overlap).
Distributional methods provide a way to address
this problem. By representing words and phrases
as vectors or tensors in a (usually highly dimen-
sional) vector space, one can express similarity
in meaning via a suitable distance metric within
that space (usually cosine distance); furthermore,
composition can be modelled via suitable linear-
algebraic operations.
Co-occurrence-based word representations
One way to produce such vectorial representa-
tions is to directly exploit Harris (1954)’s intuition
that semantically similar words tend to appear in
similar contexts. We can construct a vector space
in which the dimensions correspond to contexts,
usually taken to be words as well. The word
vector components can then be calculated from
the frequency with which a word has co-occurred
with the corresponding contexts in a window of
words, with a predefined length.
Table 1 shows 5 3-dimensional vectors for the
words Mary, John, girl, boy and idea. The words
philosophy, book and school signify vector space
dimensions. As the vector for John is closer to
Mary than it is to idea in the vector space—a di-
rect consequence of the fact that John’s contexts
are similar to Mary’s and dissimilar to idea’s—we
can infer that John is semantically more similar to
Mary than to idea.
Many variants of this approach exist: perfor-
mance on word similarity tasks has been shown
to be improved by replacing raw counts with
weighted values (e.g. mutual information)—see
(Turney et al., 2010) and below for discussion, and
(Kiela and Clark, 2014) for a detailed comparison.
philosophy book school
Mary 0 10 22
John 4 60 59
girl 0 19 93
boy 0 12 164
idea 10 47 39
</bodyText>
<tableCaption confidence="0.983528">
Table 1: Word co-occurrence frequencies ex-
tracted from the BNC (Leech et al., 1994).
</tableCaption>
<page confidence="0.998806">
709
</page>
<bodyText confidence="0.999172">
Neural word embeddings Deep learning tech-
niques exploit the distributional hypothesis dif-
ferently. Instead of relying on observed co-
occurrence frequencies, a neural language model
is trained to maximise some objective function re-
lated to e.g. the probability of observing the sur-
rounding words in some context (Mikolov et al.,
2013b):
</bodyText>
<equation confidence="0.766742">
log p(wt+j|wt) (1)
</equation>
<bodyText confidence="0.999485130434783">
Optimizing the above function, for example, pro-
duces vectors which maximise the conditional
probability of observing words in a context around
the target word wt, where c is the size of the
training window, and w1w2i · · · wT a sequence of
words forming a training instance. Therefore, the
resulting vectors will capture the distributional in-
tuition and can express degrees of lexical similar-
ity.
This method has an obvious advantage com-
pared to co-occurrence method: since now the
context is predicted, the model in principle can
be much more robust in data sparsity prob-
lems, which is always an important issue for co-
occurrence word spaces. Additionally, neural vec-
tors have also proven successful in other tasks
(Mikolov et al., 2013c), since they seem to en-
code not only attributional similarity (the degree to
which similar words are close to each other), but
also relational similarity (Turney, 2006). For ex-
ample, it is possible to extract the singular:plural
relation (apple:apples, car:cars) using vector sub-
traction:
</bodyText>
<equation confidence="0.922277">
→
apple − apples ≈ −→
→ car − −−→
cars
</equation>
<bodyText confidence="0.933541666666667">
Perhaps even more importantly, semantic relation-
ships are preserved in a very intuitive way:
−−→
</bodyText>
<equation confidence="0.752504571428571">
king −−−→
man ≈ −−−→
queen − −−−−→
woman
allowing the formation of analogy queries similar
−−→ man + −−−−→
to king − −−→ woman = ?, obtaining −−−→
</equation>
<bodyText confidence="0.872088">
queen as
the result.1
Both neural and co-occurrence-based ap-
proaches have advantages over classical formal
approaches in their ability to capture lexical se-
mantics and degrees of similarity; their success at
</bodyText>
<footnote confidence="0.590555">
1Levy et al. (2014) improved Mikolov et al. (2013c)’s
method of retrieving relational similarities by changing the
underlying objective function.
</footnote>
<bodyText confidence="0.9998485">
extending this to the sentence level and to more
complex semantic phenomena, though, depends
on their applicability within compositional mod-
els, which is the subject of the next section.
</bodyText>
<sectionHeader confidence="0.988732" genericHeader="method">
3 Compositional models
</sectionHeader>
<bodyText confidence="0.999759288888889">
Compositional distributional models represent
meaning of a sequence of words by a vector, ob-
tained by combining meaning vectors of the words
within the sequence using some vector composi-
tion operation. In a general classification of these
models, one can distinguish between three broad
cases: simplistic models which combine word
vectors irrespective of their order or relation to one
another, models which exploit linear word order,
and models which use grammatical structure.
The first approach combines word vectors
by vector addition or point-wise multiplication
(Mitchell and Lapata, 2008)—as this is indepen-
dent of word order, it cannot capture the differ-
ence between the two sentences “dogs chase cats”
and “cats chase dogs”. The second approach has
generally been implemented using some form of
deep learning, and captures word order, but not by
necessarily caring about the grammatical structure
of the sentence. Here, one works by recursively
building and combining vectors for subsequences
of words within the sentence using e.g. autoen-
coders (Socher et al., 2012) or convolutional fil-
ters (Kalchbrenner et al., 2014). We do not con-
sider this approach in this paper. This is because,
as mentioned in the introduction, their vectors and
composition operators are task-specific. These are
trained directly to achieve specific objectives in
certain pre-determined tasks. We are interested
in vector and composition operators that work for
any compositional task, and which can be com-
bined with results in linguistics and formal se-
mantics to provide generalisable models that can
canonically extend to complex semantic phenom-
ena. The third (i.e. the grammatical) approach
promises a way to achieve this, and has been in-
stantiated in various ways in the work of Baroni
and Zamparelli (2010),Grefenstette and Sadrzadeh
(2011a), and Kartsaklis et al. (2012).
General framework Formally, we can spec-
ify the vector representation of a word sequence
w1w2 · · · wn as the vector →−s = w1 −→ *−→ w2 *···*−→wn,
where * is a vector operator, such as addition +,
point-wise multiplication O, tensor product ⊗, or
matrix multiplication ×.
</bodyText>
<equation confidence="0.93177025">
�
−c&lt;j&lt;c,j�0
1 T
T t=1
</equation>
<page confidence="0.94193">
710
</page>
<bodyText confidence="0.983868781818182">
In the simplest compositional models (the first
approach described above), * is + or O, e.g. see
(Mitchell and Lapata, 2008). Grammar-based
compositional models (the third approach) are
based on a generalisation of the notion of vectors,
known as tensors. Whereas a vector →−v is an ele-
ment of an atomic vector space V , a tensor z is an
element of a tensor space V ⊗ W ⊗ · · · ⊗ Z. The
number of tensored spaces is referred to by the or-
der of the space. Using a general duality theorem
from multi-linear algebra (Bourbaki, 1989), it fol-
lows that tensors are in one-one correspondence
with multi-linear maps, that is we have:
z ∈ V ⊗W ⊗···⊗Z ∼= fz : V → W → ··· → Z
In such a tensor-based formalism, meanings of
nouns are vectors and meanings of predicates such
as adjectives and verbs are tensors. Meaning of a
string of words is obtained by applying the compo-
sitions of multi-linear map duals of the tensors to
the vectors. For the sake of demonstration, take
the case of an intransitive sentence “Sbj Verb”;
−→
the meaning of the subject is a vector Sbj ∈ V
and the meaning of the intransitive verb is a ten-
sor Verb ∈ V ⊗ W. Meaning of the sentence is
obtained by applying fV erb to Sbj, as follows:
Sbj Verb = fV erb( Sbj)
By tensor-map duality, the above becomes
equivalent to the following, where composition
has now become the familiar notion of matrix mul-
tiplication, that is * is ×:
Verb × Sbj
In general and for words with tensors of order
higher than two, * becomes a generalisation of ×,
referred to by tensor contraction, see e.g. Kartsak-
lis and Sadrzadeh (2013). Since the creation and
manipulation of tensors of order higher than 2 is
difficult, one can work with simplified versions of
tensors, faithful to their underlying mathematical
basis; these have found intuitive interpretations,
e.g. see Grefenstette and Sadrzadeh (2011a), Kart-
saklis and Sadrzadeh (2014). In such cases, * be-
comes a combination of a range of operations such
as ×, ⊗, O, and +.
Specific models In the current paper we will ex-
periment with a variety of models. In Table 2, we
present these models in terms of their composi-
tion operators and a reference to the main paper in
which each model was introduced. For the sim-
ple compositional models the sentence is a string
of any number of words; for the grammar-based
models, we consider simple transitive sentences
“Sbj Verb Obj” and introduce the following abbre-
viations for the concrete method used to build a
tensor for the verb:
</bodyText>
<listItem confidence="0.988436">
1. Verb is a verb matrix computed using the for-
mula Ez Sbjz ⊗−−→
</listItem>
<bodyText confidence="0.999426333333333">
−−→Objz, where Sbjz and Objz are
the subjects and objects of the verb across the
corpus. These models are referred to by rela-
tional (Grefenstette and Sadrzadeh, 2011a);
they are generalisations of predicate seman-
tics of transitive verbs, from pairs of individ-
uals to pairs of vectors. The models reduce
the order 3 tensor of a transitive verb to an
order 2 tensor (i.e. a matrix).
</bodyText>
<listItem confidence="0.821584">
2. Verb is a verb matrix computed using the for-
mula Verb ⊗ Verb, where Verb is the distri-
butional vector of the verb. These models are
referred to by Kronecker, which is the term
sometimes used to denote the outer prod-
uct of tensors (Grefenstette and Sadrzadeh,
2011b). This models also reduces the order
3 tensor of a transitive verb to an order 2 ten-
sor.
3. The models of the last five lines of the table
use the so-called Frobenius operators from
categorical compositional distributional se-
mantics (Kartsaklis et al., 2012) to expand
the relational matrices of verbs from order 2
to order 3. The expansion is obtained by ei-
ther copying the dimension of the subject into
the space provided by the third tensor, hence
referred to by Copy-Sbj, or copying the di-
mension of the object in that space, hence re-
ferred to by Copy-Obj; furthermore, we can
take addition, multiplication, or outer product
of these, which are referred to by Frobenius-
Add, Frobenius-Mult, and Frobenius-Outer
(Kartsaklis and Sadrzadeh, 2014).
</listItem>
<sectionHeader confidence="0.970417" genericHeader="method">
4 Semantic word spaces
</sectionHeader>
<bodyText confidence="0.999814142857143">
Co-occurrence-based vector space instantiations
have received a lot of attention from the scientific
community (refer to (Kiela and Clark, 2014; Pola-
jnar and Clark, 2014) for recent studies). We in-
stantiate two co-occurrence-based vectors spaces
with different underlying corpora and weighting
schemes.
</bodyText>
<page confidence="0.984753">
711
</page>
<subsectionHeader confidence="0.597576">
Method Sentence Linear algebraic formula Reference
</subsectionHeader>
<equation confidence="0.938472">
Addition w1w2 · · · wn w1 + ��
�� w2 + · · · + wn�� Mitchell and Lapata (2008)
Multiplication w1w2 · · · wn w1�� O w2�� O · · · O wn ��Mitchell and Lapata (2008)
</equation>
<tableCaption confidence="0.989418">
Table 2: Compositional methods.
</tableCaption>
<figure confidence="0.987249882352941">
Obj) Grefenstette and Sadrzadeh (2011a)
��
Obj) Grefenstette and Sadrzadeh (2011b)
�� Obj O (Verb&apos; x ��
O (Verb x Obj)) + (�� Sbj)) Kartsaklis and Sadrzadeh (2014)
Sbj Verb Obj (�� Sbj O (Verb x �� Obj)) O (�� Obj O (Verb&apos; x ��
Frob. mult. Sbj)) Kartsaklis and Sadrzadeh (2014)
Sbj Verb Obj (�� Sbj O (Verb x �� Obj)) ® (�� Obj O (Verb&apos; x ��
Frob. outer Sbj)) Kartsaklis and Sadrzadeh (2014)
Copy object Sbj Verb Obj Sbj O (Verb x Obj) Kartsaklis et al. (2012)
Copy subject Sbj Verb Obj Obj�� O (Verb&apos; x ��Sbj) Kartsaklis et al. (2012)
Sbj Verb Obj (��
Frob. add. Sbj
Relational Sbj Verb Obj Verb O ( Sbj®
��
Kronecker Sbj Verb Obj
Verb O (Sbj ®
</figure>
<bodyText confidence="0.985695205882353">
GS11 Our first word space is based on a typ-
ical configuration that has been used in the past
extensively for compositional distributional mod-
els (see below for details), so it will serve as a
useful baseline for the current work. In this vec-
tor space, the co-occurrence counts are extracted
from the British National Corpus (BNC) (Leech et
al., 1994). As basis words, we use the most fre-
quent nouns, verbs, adjectives and adverbs (POS
tags SUBST, VERB, ADJ and ADV in the BNC
XML distribution2). The vector space is lemma-
tized, that is, it contains only “canonical” forms of
words.
In order to weight the raw co-occurrence counts,
we use positive point-wise mutual information
(PPMI). The component value for a target word
t and a context word c is given by:
�PPMI(t, c) = max 0, log
where p(clt) is the probability of word c given t
in a symmetric window of length 5 and p(c) is the
probability of coverall.
Vector spaces based on point-wise mutual in-
formation (or variants thereof) have been success-
fully applied in various distributional and compo-
sitional tasks; see e.g. Grefenstette and Sadrzadeh
(2011a), Mitchell and Lapata (2008), Levy et al.
(2014) for details. PPMI has been shown to
achieve state-of-the-art results (Levy et al., 2014)
and is suggested by the review of Kiela and Clark
(2014). Our use here of the BNC as a corpus
and the window length of 5 is based on previ-
ous use and better performance of these param-
eters in a number of compositional experiments
(Grefenstette and Sadrzadeh, 2011a; Grefenstette
</bodyText>
<footnote confidence="0.890197">
2http://www.natcorp.ox.ac.uk/
</footnote>
<bodyText confidence="0.974642454545454">
and Sadrzadeh, 2011b; Mitchell and Lapata, 2008;
Kartsaklis et al., 2012).
KS14 In this variation, we train a vector space
from the ukWaC corpus3 (Ferraresi et al., 2008),
originally using as a basis the 2,000 content words
with the highest frequency (but excluding a list of
stop words as well as the 50 most frequent content
words since they exhibit low information content).
The vector space is again lemmatized. As context
we consider a 5-word window from either side of
the target word, while as our weighting scheme we
use local mutual information (i.e. point-wise mu-
tual information multiplied by raw counts). In a
further step, the vector space was normalized and
projected onto a 300-dimensional space using sin-
gular value decomposition (SVD).
In general, dimensionality reduction produces
more compact word representations that are robust
against potential noise in the corpus (Landauer and
Dumais, 1997; Sch¨utze, 1997). SVD has been
shown to perform well on a variety of tasks similar
to ours (Baroni and Zamparelli, 2010; Kartsaklis
and Sadrzadeh, 2014).
Neural word embeddings (NWE) For our neu-
ral setting, we used the skip-gram model of
Mikolov et al. (2013b) trained with negative sam-
pling. The specific implementation that was tested
in our experiments was a 300-dimensional vec-
tor space learned from the Google News corpus
and provided by the word2vec4 toolkit. Fur-
thermore, the gensim library (ˇReh˚uˇrek and So-
jka, 2010) was used for accessing the vectors.
On the contrary with the previously described co-
</bodyText>
<footnote confidence="0.9979985">
3http://wacky.sslmit.unibo.it/
4https://code.google.com/p/word2vec/
</footnote>
<equation confidence="0.657969666666667">
�
p(c|t)
p(c)
</equation>
<page confidence="0.97654">
712
</page>
<bodyText confidence="0.997331">
occurrence vector spaces, this version is not lem-
matized.
The negative sampling method improves the ob-
jective function of Equation 1 by introducing neg-
ative examples to the training algorithm. Assume
that the probability of a specific (c, t) pair of words
(where t is a target word and c another word in
the same context with t), coming from the training
data, is denoted as p(D = 1|c, t). The objective
function is then expressed as follows:
</bodyText>
<equation confidence="0.9902775">
ri p(D = 1|c, t) (2)
(c,t)ED
</equation>
<bodyText confidence="0.9995">
That is, the goal is to set the model parameters in
a way that maximizes the probability of all obser-
vations coming from the training data. Assume
now that D&apos; is a set of randomly selected incorrect
(c&apos;, t&apos;) pairs that do not occur in D, then Equation
2 above can be recasted in the following way:
</bodyText>
<equation confidence="0.848216">
ri p(D = 1|c, t) � p(D = 0|c&apos;, t&apos;)
(c,t)ED (c&apos;,t&apos;)ED&apos;
(3)
</equation>
<bodyText confidence="0.999480111111111">
In other words, the model tries to distinguish a tar-
get word t from random draws that come from a
noise distribution. In the implementation we used
for our experiments, c is always selected from
a 5-word window around t. More details about
the negative sampling approach can be found in
(Mikolov et al., 2013b); the note of Goldberg and
Levy (2014) also provides an intuitive explanation
of the underlying setting.
</bodyText>
<sectionHeader confidence="0.999564" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999961666666667">
Our experiments explore the use of the vector
spaces above, together with the compositional op-
erators described in Section 3, in a range of tasks
all of which require semantic composition: verb
sense disambiguation; sentence similarity; para-
phrasing; and dialogue act tagging.
</bodyText>
<subsectionHeader confidence="0.945037">
5.1 Disambiguation
</subsectionHeader>
<bodyText confidence="0.999943">
We use the transitive verb disambiguation dataset
described in Grefenstette and Sadrzadeh (2011a)5.
This dataset consists of ambiguous transitive verbs
together with their arguments, landmark verbs
that identify one of the verb senses, and human
judgements that specify how similar is the disam-
biguated sense of the verb in the given context to
</bodyText>
<footnote confidence="0.993745666666667">
5This and the sentence similarity dataset are avail-
able at http://www.cs.ox.ac.uk/activities/
compdistmeaning/
</footnote>
<bodyText confidence="0.999883206896552">
one of the landmarks. This is similar to the in-
transitive dataset described in (Mitchell and Lap-
ata, 2008). Consider the sentence “system meets
specification”; here, meets is the ambiguous tran-
sitive verb, and system and specification are its ar-
guments in this context. Possible landmarks for
meet are satisfy and visit; for this sentence, the
human judgements show that the disambiguated
meaning of the verb is more similar to the land-
mark satisfy and less similar to visit.
The task is to estimate the similarity of the sense
of a verb in a context with a given landmark. To
get our similarity measures, we compose the verb
with its arguments using one of our compositional
models; we do the same for the landmark and then
compute the cosine similarity of the two vectors.
We evaluate the performance by averaging the hu-
man judgements for the same verb, argument and
landmark entries, and calculating the Spearman’s
correlation between the average values and the co-
sine scores. As a baseline, we compare this with
the correlation produced by using only the verb
vector, without composing it with its arguments.
Table 3 shows the results of the experiment.
NWE copy-object composition yields the best cor-
relation with the human judgements, and top per-
formance across all vector spaces and models with
a Spearman ρ of 0.456. For the KS14 space, the
best result comes from Frobenius outer (0.350),
</bodyText>
<table confidence="0.999893818181818">
Method GS11 KS14 NWE
Verb only 0.212 0.325 0.107
Addition 0.103 0.275 0.149
Multiplication 0.348 0.041 0.095
Kronecker 0.304 0.176 0.117
Relational 0.285 0.341 0.362
Copy subject 0.089 0.317 0.131
Copy object 0.334 0.331 0.456
Frobenius add. 0.261 0.344 0.359
Frobenius mult. 0.233 0.341 0.239
Frobenius outer 0.284 0.350 0.375
</table>
<tableCaption confidence="0.997903">
Table 3: Spearman ρ correlations of models with
</tableCaption>
<bodyText confidence="0.8758348">
human judgements for the word sense disam-
biguation task. The best result (NWE Copy ob-
ject) outperforms the nearest co-occurrence-based
competitor (KS14 Frobenius outer) with a statisti-
cally significant difference (p &lt; 0.05, t-test).
</bodyText>
<page confidence="0.997204">
713
</page>
<bodyText confidence="0.9997246875">
while the best operator for the GS11 space is
point-wise multiplication (0.348).
For simple point-wise composition, only mul-
tiplicative GS11 and additive NWE improve over
their corresponding verb-only baselines (but both
perform worse than the KS14 baseline). With
tensor-based composition in co-occurrence based
spaces, copy subject yields lower results than
the corresponding baselines. Other composition
methods, except Kronecker for KS14, improve
over the verb-only baselines. Finally we should
note that, despite the small training corpus, the
GS11 vector space performs comparatively well:
for instance, Kronecker model improves the pre-
viously reported score of 0.28 (Grefenstette and
Sadrzadeh, 2011b).
</bodyText>
<subsectionHeader confidence="0.999136">
5.2 Sentence similarity
</subsectionHeader>
<bodyText confidence="0.9999852">
In this experiment we use the transitive sen-
tence similarity dataset described in Kartsaklis and
Sadrzadeh (2014). The dataset consists of transi-
tive sentence pairs and a human similarity judge-
ment6. The task is to estimate a similarity measure
between two sentences. As in the disambiguation
task, we first compose word vectors to obtain sen-
tence vectors, then compute cosine similarity of
them. We average the human judgements for iden-
tical sentence pairs to compute a correlation with
cosine scores.
Table 4 shows the results. Again, the best
performing vector space is KS14, but this time
with addition: the Spearman ρ correlation score
with averaged human judgements is 0.732. Addi-
tion was the means for the other vector spaces to
achieve top performance as well: GS11 and NWE
got 0.682 and 0.689 respectively.
None of the models in tensor-based composi-
tion outperformed addition. KS14 performs worse
with tensor-based methods here than in the other
vector spaces. However, GS11 and NWE, except
copy subject for both of them and Frobenius multi-
plication for NWE, improved over their verb-only
baselines.
</bodyText>
<subsectionHeader confidence="0.995506">
5.3 Paraphrasing
</subsectionHeader>
<bodyText confidence="0.999811">
In this experiment we evaluate our vector spaces
on a mainstream paraphrase detection task.
</bodyText>
<footnote confidence="0.579573">
6The textual content of this dataset is the same as that of
(Kartsaklis and Sadrzadeh, 2013), the difference is that the
dataset of (Kartsaklis and Sadrzadeh, 2014) has updated hu-
man judgements whereas the previous dataset used the orig-
inal annotations of the intransitive dataset of (Mitchell and
Lapata, 2010).
</footnote>
<table confidence="0.999801363636364">
Method GS11 KS14 NWE
Verb only 0.491 0.602 0.561
Addition 0.682 0.732 0.689
Multiplication 0.597 0.321 0.341
Kronecker 0.581 0.408 0.561
Relational 0.558 0.437 0.618
Copy subject 0.370 0.448 0.405
Copy object 0.571 0.306 0.655
Frobenius add. 0.566 0.460 0.585
Frobenius mult. 0.525 0.226 0.387
Frobenius outer 0.560 0.439 0.622
</table>
<tableCaption confidence="0.982071">
Table 4: Results for sentence similarity. There
</tableCaption>
<bodyText confidence="0.975395942857143">
is no statistically significant difference between
KS14 addition and NWE addition (the second best
result).
Specifically, we get classification results on the
Microsoft Research Paraphrase Corpus paraphrase
corpus (Dolan et al., 2005) working in the follow-
ing way: we construct vectors for the sentences
of each pair; if the cosine similarity between the
two sentence vectors exceeds a certain threshold,
the pair is classified as a paraphrase, otherwise as
not a paraphrase. For this experiment and that of
Section 5.4 below, we investigate only the addi-
tion and point-wise multiplication compositional
models, since at their current stage of development
tensor-based models can only efficiently handle
sentences of fixed structure. Nevertheless, the
simple point-wise compositional models still al-
low for a direct comparison of the vector spaces,
which is the main goal of this paper.
For each vector space and model, a number of
different thresholds were tested on the first 2000
pairs of the training set, which we used as a de-
velopment set; in each case, the best-performed
threshold was selected for a single run of our
“classifier” on the test set (1726 pairs). Addition-
ally, we evaluate the NWE model with a lemma-
tized version of the corpus, so that the experimen-
tal setup is maximally similar for all vector spaces.
The results are shown in the first part of Table 5.
Additive NWE gives the highest performance,
with both lemmatized and un-lemmatized versions
outperforming the GS11 and KS14 spaces. In
the un-lemmatized case, the accuracy of our sim-
ple “classifier” (0.73) is close to state-of-the-art
range. The state-of-the art result (0.77 accuracy
</bodyText>
<page confidence="0.996046">
714
</page>
<table confidence="0.999590571428571">
Model Baseline Co-occurrence Neural word embeddings
GS11 KS14 Unlemmatized Lemmatized
Accuracy F-Score Accuracy F-Score Accuracy F-Score Accuracy F-Score Accuracy F-Score
MSR addition 0.65 0.75 0.62 0.79 0.70 0.80 0.73 0.82 0.72 0.81
MSR multiplication 0.52 0.58 0.66 0.80 0.42 0.34 0.41 0.36
SWDA addition 0.60 0.58 0.35 0.35 0.40 0.35 0.63 0.60 0.44 0.40
SWDA multiplication 0.32 0.16 0.39 0.33 0.58 0.53 0.43 0.38
</table>
<tableCaption confidence="0.9869195">
Table 5: Results for paraphrase detection (MSR) and dialog act tagging (SWDA) tasks. All top results
significantly outperform corresponding nearest competitors (for accuracy): p &lt; 0.05, x2 test.
</tableCaption>
<bodyText confidence="0.999503">
and 0.84 F-score7) by the time of this writing has
been obtained using 8 machine translation metrics
and three constituent classifiers (Madnani et al.,
2012).
The multiplicative model gives lower results
than the additive model across all vector spaces.
The KS14 vector space shows the steadiest per-
formance, with a drop in accuracy of only 0.04
and no drop in F-score, while for the GS11 and
NWE spaces both accuracy and F-score experi-
enced drops by more than 0.20.
</bodyText>
<subsectionHeader confidence="0.986687">
5.4 Dialogue act tagging
</subsectionHeader>
<bodyText confidence="0.9999215">
As our last experiment, we evaluate the word
spaces on a dialogue act tagging task (Stolcke et
al., 2000) over the Switchboard corpus (Godfrey
et al., 1992). Switchboard is a collection of ap-
proximately 2500 dialogs over a telephone line by
500 speakers from the U.S. on predefined topics.8
The experiment pipeline follows (Milajevs and
Purver, 2014). The input utterances are prepro-
cessed so that the parts of interrupted utterances
are concatenated (Webb et al., 2005). Disfluency
markers and commas are removed from the utter-
ance raw texts. For GS11 and KS14 the utterance
tokens are POS-tagged and lemmatized; for NWE,
we test the vectors in both a lemmatized and an
un-lemmatized version of the corpus.9 We split
the training and testing utterances as suggested by
Stolcke et al. (2000). Utterance vectors are then
obtained as in the previous experiments; they are
reduced to 50 dimensions using SVD and a k-
nearest-neighbour classifier is trained on these re-
duced utterance vectors (the 5 closest neighbours
by Euclidean distance are retrieved to make a clas-
</bodyText>
<footnote confidence="0.983879857142857">
7F-scores use the standard definition F = 2(precision ∗
recall)/(precision + recall).
8The dataset and a Python interface to it are available
at http://compprag.christopherpotts.net/
swda.html
9We use WordNetLemmatizer of the NLTK library
(Bird, 2006).
</footnote>
<bodyText confidence="0.9953963125">
sification decision). The results are shown in the
second part of Table 5.
Un-lemmatized NWE addition gave the best ac-
curacy (0.63) and F-score (0.60) (averaged over
tag classes), i.e. similar results to (Milajevs and
Purver, 2014)—although note that the dimension-
ality of our NWE vectors is 10 times lower than
theirs. Multiplicative NWE outperformed the cor-
responding model in (Milajevs and Purver, 2014).
In general, addition consistently outperforms mul-
tiplication for all the models. Lemmatization
dramatically lowers tagging accuracy: the lem-
matized GS11, KS14 and NWE models perform
much worse than un-lemmatized NWE, suggest-
ing that morphological features are important for
this task.
</bodyText>
<sectionHeader confidence="0.998552" genericHeader="method">
6 Discussion
</sectionHeader>
<bodyText confidence="0.998657043478261">
Previous comparisons of co-occurrence-based and
neural word vector representations vary widely
in their conclusions. While Baroni et al. (2014)
conclude that “context-predicting models obtain
a thorough and resounding victory against their
count-based counterparts”, this seems to contra-
dict, at least at the first consideration, the more
conservative conclusion of Levy et al. (2014) that
“analogy recovery is not restricted to neural word
embeddings [... ] a similar amount of relational
similarities can be recovered from traditional dis-
tributional word representations” and the findings
of Blacoe and Lapata (2012) that “shallow ap-
proaches are as good as more computationally in-
tensive alternatives” on phrase similarity and para-
phrase detection tasks.
It seems clear that neural word embeddings
have an advantage when used in tasks for which
they have been trained; our main questions here
are whether they outperform co-occurrence based
alternatives across the board; and which ap-
proach lends itself better to composition using
general mathematical operators. To partially an-
</bodyText>
<page confidence="0.996106">
715
</page>
<bodyText confidence="0.999937780487805">
swer this question, we can compare model be-
haviour against the baselines in isolation.
For the disambiguation and sentence similarity
tasks the baseline is the similarity between verbs
only, ignoring the context—see above. For the
paraphrase task, we take the global vector-based
similarity reported in (Mihalcea et al., 2006): 0.65
accuracy and 0.75 F-score. For the dialogue act
tagging task the baseline is the accuracy of the
bag-of-unigrams model in (Milajevs and Purver,
2014): 0.60.
Sections 5.1 and 5.2 show that although the best
choice of vector representation might vary, for
small-scale tasks all methods give fairly compet-
itive results. The choice of compositional oper-
ator seems to be more important and more task-
specific: while a tensor-based operation (Frobe-
nius copy-object) performs best for verb disam-
biguation, the best result for sentence similarity
is achieved by a simple additive model, with all
other compositional methods behaving worse than
the verb-only baseline in the KS14 case. GS11 and
NWE, on the other hand, outperform their base-
lines with a number of compositional methods, al-
though both of them achieve lower performance
than KS14 overall.
Based on only small-scale experiment results,
one could conclude that there is little significant
difference between the two ways of obtaining vec-
tors. GS11 and NWE show similar behaviour in
comparison to their baselines, while it is possible
to tune a co-occurrence based vector space (KS14)
and obtain the best result. Large scale tasks reveal
another pattern: the GS11 vector space, which be-
haves stably on the small scale, drags behind the
KS14 and NWE spaces in the paraphrase detec-
tion task. In addition, NWE consistently yields
best results. Finally, only the NWE space was able
to provide adequate results on the dialogue act tag-
ging task. Table 6 summarizes model performance
with regard to baselines.
</bodyText>
<sectionHeader confidence="0.993768" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999700142857143">
In this work we compared the performance of two
co-occurrence-based semantic spaces with vectors
learned by a neural network in compositional set-
tings. We carried out two small-scale tasks (word
sense disambiguation and sentence similarity) and
two large-scale tasks (paraphrase detection and di-
alogue act tagging).
</bodyText>
<table confidence="0.9992868">
Task GS11 KS14 NWE
Disambiguation + + +
Sentence similarity + – +
Paraphrase − + +
Dialog act tagging − − +
</table>
<tableCaption confidence="0.988933">
Table 6: Summary of vector space performance
</tableCaption>
<bodyText confidence="0.989604071428572">
against baselines. General improvement (cases
where more than a half of the models perform bet-
ter) and decrease with regard to a corresponding
baseline is respectively marked by + and −. A
bold value means that the model gave the best re-
sult in the task.
On small-scale tasks, where the sentence struc-
tures are predefined and relatively constrained,
NWE gives better or similar results to count-based
vectors. Tensor-based composition does not al-
ways outperform simple compositional operators,
but for most of the cases gives results within the
same range.
On large-scale tasks, neural vectors are more
successful than the co-occurrence based alterna-
tives. However, this study does not reveal whether
this is because of their neural nature, or just be-
cause they are trained on a larger amount of data.
The question of whether neural vectors outper-
form co-occurrence vectors therefore requires fur-
ther detailed comparison to be entirely resolved;
our experiments suggest that this is indeed the case
in large-scale tasks, but the difference in size and
nature of the original corpora may be a confound-
ing factor. In any case, it is clear that the neural
vectors of word2vec package perform steadily
off-the-shelf across a large variety of tasks. The
size of the vector space (3 million words) and the
available code-base that simplifies the access to
the vectors, makes this set a good and safe choice
for experiments in the future. Of course, even bet-
ter performances can be achieved by training neu-
ral language models specifically for a given task
(see e.g. Kalchbrenner et al. (2014)).
The choice of compositional operator (tensor-
based or a simple point-wise operation) depends
strongly on the task and dataset: tensor-based
composition performed best with the verb dis-
ambiguation task, where the verb senses depend
strongly on the arguments of the verb. However, it
seems to depend less on the nature of the vectors
itself: in the disambiguation task, tensor-based
</bodyText>
<page confidence="0.993968">
716
</page>
<bodyText confidence="0.99993475">
composition proved best for both co-occurrence-
based and neural vectors; in the sentence similar-
ity task, where point-wise operators proved best,
this was again true across vector spaces.
</bodyText>
<sectionHeader confidence="0.990278" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999777545454545">
We would like to thank the three anonymous
reviewers for their fruitful comments. Sup-
port by EPSRC grant EP/F042728/1 is grate-
fully acknowledged by Milajevs, Kartsaklis and
Sadrzadeh. Purver is partly supported by Con-
CreTe: the project ConCreTe acknowledges the fi-
nancial support of the Future and Emerging Tech-
nologies (FET) programme within the Seventh
Framework Programme for Research of the Eu-
ropean Commission, under FET grant number
611733.
</bodyText>
<sectionHeader confidence="0.998892" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999249953488372">
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, pages
1183–1193. Association for Computational Linguis-
tics.
Marco Baroni, Georgiana Dinu, and Germ´an
Kruszewski. 2014. Don’t count, predict! a
systematic comparison of context-counting vs.
context-predicting semantic vectors. In Proceedings
of the 52nd Annual Meeting of the Association for
Computational Linguistics, volume 1.
Yoshua Bengio, Holger Schwenk, Jean-S´ebastien
Sen´ecal, Fr´ederic Morin, and Jean-Luc Gauvain.
2006. Neural probabilistic language models. In
Innovations in Machine Learning, pages 137–186.
Springer.
Steven Bird. 2006. NLTK: the natural language
toolkit. In Proceedings of the COLING/ACL on In-
teractive presentation sessions, pages 69–72. Asso-
ciation for Computational Linguistics.
William Blacoe and Mirella Lapata. 2012. A com-
parison of vector-based representations for semantic
composition. In Proceedings of the 2012 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, pages 546–556. Association for Compu-
tational Linguistics.
Johan Bos and Malte Gabsdil. 2000. First-order infer-
ence and the interpretation of questions and answers.
Proceedings of Gotelog, pages 43–50.
Johan Bos. 2008. Wide-coverage semantic analy-
sis with boxer. In Johan Bos and Rodolfo Del-
monte, editors, Semantics in Text Processing. STEP
2008 Conference Proceedings, Research in Compu-
tational Semantics, pages 277–286. College Publi-
cations.
N. Bourbaki. 1989. Commutative Algebra: Chapters
1-7. Srpinger Verlag, Berlin/New York.
Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen
Clark. 2010. Mathematical foundations for a com-
positional distributional model of meaning. CoRR,
abs/1003.4394.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In Pro-
ceedings of the 25th international conference on
Machine learning, pages 160–167. ACM.
Ann Copestake, Dan Flickinger, Carl Pollard, and
Ivan A Sag. 2005. Minimal recursion semantics:
An introduction. Research on Language and Com-
putation, 3(2-3):281–332.
Bill Dolan, Chris Brockett, and Chris Quirk. 2005. Mi-
crosoft research paraphrase corpus. Retrieved May,
29:2013.
Adriano Ferraresi, Eros Zanchetta, Marco Baroni, and
Silvia Bernardini. 2008. Introducing and evaluating
ukWaC, a very large web-derived corpus of English.
In Proceedings of the 4th Web as Corpus Workshop
(WAC-4) Can we beat Google, pages 47–54.
Gottlob Frege. 1892. On sense and reference. Ludlow
(1997), pages 563–584.
John J Godfrey, Edward C Holliman, and Jane Mc-
Daniel. 1992. Switchboard: Telephone speech cor-
pus for research and development. In Acoustics,
Speech, and Signal Processing, 1992. ICASSP-92.,
1992 IEEE International Conference on, volume 1,
pages 517–520. IEEE.
Yoav Goldberg and Omer Levy. 2014. word2vec
Explained: deriving Mikolov et al.’s negative-
sampling word-embedding method. arXiv preprint
arXiv:1402.3722.
Edward Grefenstette and Mehrnoosh Sadrzadeh.
2011a. Experimental support for a categorical com-
positional distributional model of meaning. In Pro-
ceedings of the Conference on Empirical Methods
in Natural Language Processing, pages 1394–1404.
Association for Computational Linguistics.
Edward Grefenstette and Mehrnoosh Sadrzadeh.
2011b. Experimenting with transitive verbs in a Dis-
CoCat. In Proceedings of the GEMS 2011 Work-
shop on GEometrical Models of Natural Language
Semantics, pages 62–66, Edinburgh, UK, July. As-
sociation for Computational Linguistics.
Z.S. Harris. 1954. Distributional structure. Word.
</reference>
<page confidence="0.969073">
717
</page>
<reference confidence="0.999687603773585">
Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent
convolutional neural networks for discourse compo-
sitionality. In Proceedings of the Workshop on Con-
tinuous Vector Space Models and their Composition-
ality, pages 119–126, Sofia, Bulgaria, August. Asso-
ciation for Computational Linguistics.
Nal Kalchbrenner, Edward Grefenstette, and Phil Blun-
som. 2014. A convolutional neural network for
modelling sentences. Proceedings of the 52nd An-
nual Meeting of the Association for Computational
Linguistics, June.
Dimitri Kartsaklis and Mehrnoosh Sadrzadeh. 2013.
Prior disambiguation of word tensors for construct-
ing sentence vectors. In Proceedings of the 2013
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNL), pages 1590–1601, Seat-
tle, USA, October. Association for Computational
Linguistics.
Dimitri Kartsaklis and Mehrnoosh Sadrzadeh. 2014. A
study of entanglement in a categorical framework of
natural language. In Proceedings of the 11th Work-
shop on Quantum Physics and Logic (QPL), Kyoto,
Japan, June.
Dimitri Kartsaklis, Mehrnoosh Sadrzadeh, and Stephen
Pulman. 2012. A unified sentence space for
categorical distributional-compositional semantics:
Theory and experiments. In Proceedings of COL-
ING 2012: Posters, pages 549–558, Mumbai, India,
December. The COLING 2012 Organizing Commit-
tee.
Douwe Kiela and Stephen Clark. 2014. A systematic
study of semantic vector space model parameters.
In Proceedings of the 2nd Workshop on Continu-
ous Vector Space Models and their Compositionality
(CVSC), pages 21–30, Gothenburg, Sweden, April.
Association for Computational Linguistics.
T. Landauer and S. Dumais. 1997. A Solution
to Plato’s Problem: The Latent Semantic Analysis
Theory of Acquision, Induction, and Representation
of Knowledge. Psychological Review.
Geoffrey Leech, Roger Garside, and Michael Bryant.
1994. Claws4: the tagging of the british national
corpus. In Proceedings of the 15th conference
on Computational linguistics-Volume 1, pages 622–
628. Association for Computational Linguistics.
Omer Levy, Yoav Goldberg, and Israel Ramat-Gan.
2014. Linguistic regularities in sparse and explicit
word representations. In Proceedings of the Eigh-
teenth Conference on Computational Natural Lan-
guage Learning, Baltimore, Maryland, USA, June.
Association for Computational Linguistics.
Nitin Madnani, Joel Tetreault, and Martin Chodorow.
2012. Re-examining machine translation metrics
for paraphrase identification. In Proceedings of the
2012 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 182–190. Asso-
ciation for Computational Linguistics.
Rada Mihalcea, Courtney Corley, and Carlo Strappa-
rava. 2006. Corpus-based and knowledge-based
measures of text semantic similarity. In AAAI, vol-
ume 6, pages 775–780.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013a. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013b. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in Neural Information Processing
Systems, pages 3111–3119.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013c. Linguistic regularities in continuous space
word representations. In Proceedings of NAACL-
HLT, pages 746–751.
Dmitrijs Milajevs and Matthew Purver. 2014. Inves-
tigating the contribution of distributional semantic
information for dialogue act classification. In Pro-
ceedings of the 2nd Workshop on Continuous Vector
Space Models and their Compositionality (CVSC),
pages 40–47, Gothenburg, Sweden, April. Associa-
tion for Computational Linguistics.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings
of ACL-08: HLT, pages 236–244. Association for
Computational Linguistics.
Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive Sci-
ence, 34(8):1388–1439.
Richard Montague. 1970. Universal grammar. Theo-
ria, 36(3):373–398.
Tamara Polajnar and Stephen Clark. 2014. Improv-
ing distributional semantic vectors through context
selection and normalisation. In Proceedings of the
14th Conference of the European Chapter of the As-
sociation for Computational Linguistics, pages 230–
238, Gothenburg, Sweden, April. Association for
Computational Linguistics.
Radim ˇReh˚uˇrek and Petr Sojka. 2010. Software
Framework for Topic Modelling with Large Cor-
pora. In Proceedings of the LREC 2010 Workshop
on New Challenges for NLP Frameworks, pages 45–
50, Valletta, Malta, May. ELRA. http://is.
muni.cz/publication/884893/en.
Hinrich Sch¨utze. 1997. Ambiguity resolution in natu-
ral language learning. csli. Stanford, CA, 4:12–36.
</reference>
<page confidence="0.974254">
718
</page>
<reference confidence="0.999483125">
Richard Socher, Brody Huval, Christopher D Manning,
and Andrew Y Ng. 2012. Semantic compositional-
ity through recursive matrix-vector spaces. In Pro-
ceedings of the 2012 Joint Conference on Empiri-
cal Methods in Natural Language Processing and
Computational Natural Language Learning, pages
1201–1211. Association for Computational Linguis-
tics.
Andreas Stolcke, Klaus Ries, Noah Coccaro, Eliza-
beth Shriberg, Rebecca Bates, Daniel Jurafsky, Paul
Taylor, Carol Van Ess-Dykema, Rachel Martin, and
Marie Meteer. 2000. Dialogue act modeling for
automatic tagging and recognition of conversational
speech. Computational Linguistics, 26(3):339–373.
Peter D Turney, Patrick Pantel, et al. 2010. From
frequency to meaning: Vector space models of se-
mantics. Journal of artificial intelligence research,
37(1):141–188.
Peter D Turney. 2006. Similarity of semantic relations.
Computational Linguistics, 32(3):379–416.
Nick Webb, Mark Hepple, and Yorick Wilks. 2005.
Dialogue act classification based on intra-utterance
features. In Proceedings of the AAAI Workshop on
Spoken Language Understanding. Citeseer.
</reference>
<page confidence="0.998653">
719
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.473486">
<title confidence="0.986244">Evaluating Neural Word Representations Tensor-Based Compositional Settings</title>
<author confidence="0.594688">Mehrnoosh</author>
<affiliation confidence="0.884693666666667">Mary University of School of Electronic and Computer</affiliation>
<address confidence="0.803005">Mile End Road, London, UK</address>
<abstract confidence="0.999542869565218">We provide a comparative study between neural word representations and traditional vector spaces based on cooccurrence counts, in a number of compositional tasks. We use three different semantic spaces and implement seven tensor-based compositional models, which we then test (together with simpler additive and multiplicative approaches) in tasks involving verb disambiguation and sentence similarity. To check their scalability, we additionally evaluate the spaces using simple compositional methods on larger-scale tasks with less constrained language: paraphrase detection and dialogue act tagging. In the more constrained tasks, co-occurrence vectors are competitive, although choice of compositional method is important; on the largerscale tasks, they are outperformed by neural word embeddings, which show robust, stable performance across the tasks.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Roberto Zamparelli</author>
</authors>
<title>Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1183--1193</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="12512" citStr="Baroni and Zamparelli (2010)" startWordPosition="1937" endWordPosition="1940">is paper. This is because, as mentioned in the introduction, their vectors and composition operators are task-specific. These are trained directly to achieve specific objectives in certain pre-determined tasks. We are interested in vector and composition operators that work for any compositional task, and which can be combined with results in linguistics and formal semantics to provide generalisable models that can canonically extend to complex semantic phenomena. The third (i.e. the grammatical) approach promises a way to achieve this, and has been instantiated in various ways in the work of Baroni and Zamparelli (2010),Grefenstette and Sadrzadeh (2011a), and Kartsaklis et al. (2012). General framework Formally, we can specify the vector representation of a word sequence w1w2 · · · wn as the vector →−s = w1 −→ *−→ w2 *···*−→wn, where * is a vector operator, such as addition +, point-wise multiplication O, tensor product ⊗, or matrix multiplication ×. � −c&lt;j&lt;c,j�0 1 T T t=1 710 In the simplest compositional models (the first approach described above), * is + or O, e.g. see (Mitchell and Lapata, 2008). Grammar-based compositional models (the third approach) are based on a generalisation of the notion of vector</context>
<context position="20620" citStr="Baroni and Zamparelli, 2010" startWordPosition="3359" endWordPosition="3362">mmatized. As context we consider a 5-word window from either side of the target word, while as our weighting scheme we use local mutual information (i.e. point-wise mutual information multiplied by raw counts). In a further step, the vector space was normalized and projected onto a 300-dimensional space using singular value decomposition (SVD). In general, dimensionality reduction produces more compact word representations that are robust against potential noise in the corpus (Landauer and Dumais, 1997; Sch¨utze, 1997). SVD has been shown to perform well on a variety of tasks similar to ours (Baroni and Zamparelli, 2010; Kartsaklis and Sadrzadeh, 2014). Neural word embeddings (NWE) For our neural setting, we used the skip-gram model of Mikolov et al. (2013b) trained with negative sampling. The specific implementation that was tested in our experiments was a 300-dimensional vector space learned from the Google News corpus and provided by the word2vec4 toolkit. Furthermore, the gensim library (ˇReh˚uˇrek and Sojka, 2010) was used for accessing the vectors. On the contrary with the previously described co3http://wacky.sslmit.unibo.it/ 4https://code.google.com/p/word2vec/ � p(c|t) p(c) 712 occurrence vector spac</context>
</contexts>
<marker>Baroni, Zamparelli, 2010</marker>
<rawString>Marco Baroni and Roberto Zamparelli. 2010. Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1183–1193. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Georgiana Dinu</author>
<author>Germ´an Kruszewski</author>
</authors>
<title>Don’t count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<volume>1</volume>
<contexts>
<context position="1596" citStr="Baroni et al., 2014" startWordPosition="221" endWordPosition="224"> are competitive, although choice of compositional method is important; on the largerscale tasks, they are outperformed by neural word embeddings, which show robust, stable performance across the tasks. 1 Introduction Neural word embeddings (Bengio et al., 2006; Collobert and Weston, 2008; Mikolov et al., 2013a) have received much attention in the distributional semantics community, and have shown state-of-the-art performance in many natural language processing tasks. While they have been compared with co-occurrence based models in simple similarity tasks at the word level (Levy et al., 2014; Baroni et al., 2014), we are aware of only one work that attempts a comparison of the two approaches in compositional settings (Blacoe and Lapata, 2012), and this is limited to additive and multiplicative composition, compared against composition via a neural autoencoder. The purpose of this paper is to provide a more complete picture regarding the potential of neu2University of Oxford Department of Computer Science Parks Road, Oxford, UK dimitri.kartsaklis@cs.ox.ac.uk ral word embeddings in compositional tasks, and meaningfully compare them with the traditional distributional approach based on co-occurrence coun</context>
<context position="32793" citStr="Baroni et al. (2014)" startWordPosition="5286" endWordPosition="5289">4)—although note that the dimensionality of our NWE vectors is 10 times lower than theirs. Multiplicative NWE outperformed the corresponding model in (Milajevs and Purver, 2014). In general, addition consistently outperforms multiplication for all the models. Lemmatization dramatically lowers tagging accuracy: the lemmatized GS11, KS14 and NWE models perform much worse than un-lemmatized NWE, suggesting that morphological features are important for this task. 6 Discussion Previous comparisons of co-occurrence-based and neural word vector representations vary widely in their conclusions. While Baroni et al. (2014) conclude that “context-predicting models obtain a thorough and resounding victory against their count-based counterparts”, this seems to contradict, at least at the first consideration, the more conservative conclusion of Levy et al. (2014) that “analogy recovery is not restricted to neural word embeddings [... ] a similar amount of relational similarities can be recovered from traditional distributional word representations” and the findings of Blacoe and Lapata (2012) that “shallow approaches are as good as more computationally intensive alternatives” on phrase similarity and paraphrase det</context>
</contexts>
<marker>Baroni, Dinu, Kruszewski, 2014</marker>
<rawString>Marco Baroni, Georgiana Dinu, and Germ´an Kruszewski. 2014. Don’t count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, volume 1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>Holger Schwenk</author>
<author>Jean-S´ebastien Sen´ecal</author>
<author>Fr´ederic Morin</author>
<author>Jean-Luc Gauvain</author>
</authors>
<title>Neural probabilistic language models.</title>
<date>2006</date>
<booktitle>In Innovations in Machine Learning,</booktitle>
<pages>137--186</pages>
<publisher>Springer.</publisher>
<marker>Bengio, Schwenk, Sen´ecal, Morin, Gauvain, 2006</marker>
<rawString>Yoshua Bengio, Holger Schwenk, Jean-S´ebastien Sen´ecal, Fr´ederic Morin, and Jean-Luc Gauvain. 2006. Neural probabilistic language models. In Innovations in Machine Learning, pages 137–186. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven Bird</author>
</authors>
<title>NLTK: the natural language toolkit.</title>
<date>2006</date>
<booktitle>In Proceedings of the COLING/ACL on Interactive presentation sessions,</booktitle>
<pages>69--72</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="31942" citStr="Bird, 2006" startWordPosition="5163" endWordPosition="5164">pus.9 We split the training and testing utterances as suggested by Stolcke et al. (2000). Utterance vectors are then obtained as in the previous experiments; they are reduced to 50 dimensions using SVD and a knearest-neighbour classifier is trained on these reduced utterance vectors (the 5 closest neighbours by Euclidean distance are retrieved to make a clas7F-scores use the standard definition F = 2(precision ∗ recall)/(precision + recall). 8The dataset and a Python interface to it are available at http://compprag.christopherpotts.net/ swda.html 9We use WordNetLemmatizer of the NLTK library (Bird, 2006). sification decision). The results are shown in the second part of Table 5. Un-lemmatized NWE addition gave the best accuracy (0.63) and F-score (0.60) (averaged over tag classes), i.e. similar results to (Milajevs and Purver, 2014)—although note that the dimensionality of our NWE vectors is 10 times lower than theirs. Multiplicative NWE outperformed the corresponding model in (Milajevs and Purver, 2014). In general, addition consistently outperforms multiplication for all the models. Lemmatization dramatically lowers tagging accuracy: the lemmatized GS11, KS14 and NWE models perform much wor</context>
</contexts>
<marker>Bird, 2006</marker>
<rawString>Steven Bird. 2006. NLTK: the natural language toolkit. In Proceedings of the COLING/ACL on Interactive presentation sessions, pages 69–72. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Blacoe</author>
<author>Mirella Lapata</author>
</authors>
<title>A comparison of vector-based representations for semantic composition.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>546--556</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1728" citStr="Blacoe and Lapata, 2012" startWordPosition="243" endWordPosition="246">l word embeddings, which show robust, stable performance across the tasks. 1 Introduction Neural word embeddings (Bengio et al., 2006; Collobert and Weston, 2008; Mikolov et al., 2013a) have received much attention in the distributional semantics community, and have shown state-of-the-art performance in many natural language processing tasks. While they have been compared with co-occurrence based models in simple similarity tasks at the word level (Levy et al., 2014; Baroni et al., 2014), we are aware of only one work that attempts a comparison of the two approaches in compositional settings (Blacoe and Lapata, 2012), and this is limited to additive and multiplicative composition, compared against composition via a neural autoencoder. The purpose of this paper is to provide a more complete picture regarding the potential of neu2University of Oxford Department of Computer Science Parks Road, Oxford, UK dimitri.kartsaklis@cs.ox.ac.uk ral word embeddings in compositional tasks, and meaningfully compare them with the traditional distributional approach based on co-occurrence counts. We are especially interested in investigating the performance of neural word vectors in compositional models involving general m</context>
<context position="33268" citStr="Blacoe and Lapata (2012)" startWordPosition="5355" endWordPosition="5358">ssion Previous comparisons of co-occurrence-based and neural word vector representations vary widely in their conclusions. While Baroni et al. (2014) conclude that “context-predicting models obtain a thorough and resounding victory against their count-based counterparts”, this seems to contradict, at least at the first consideration, the more conservative conclusion of Levy et al. (2014) that “analogy recovery is not restricted to neural word embeddings [... ] a similar amount of relational similarities can be recovered from traditional distributional word representations” and the findings of Blacoe and Lapata (2012) that “shallow approaches are as good as more computationally intensive alternatives” on phrase similarity and paraphrase detection tasks. It seems clear that neural word embeddings have an advantage when used in tasks for which they have been trained; our main questions here are whether they outperform co-occurrence based alternatives across the board; and which approach lends itself better to composition using general mathematical operators. To partially an715 swer this question, we can compare model behaviour against the baselines in isolation. For the disambiguation and sentence similarity</context>
</contexts>
<marker>Blacoe, Lapata, 2012</marker>
<rawString>William Blacoe and Mirella Lapata. 2012. A comparison of vector-based representations for semantic composition. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 546–556. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johan Bos</author>
<author>Malte Gabsdil</author>
</authors>
<title>First-order inference and the interpretation of questions and answers.</title>
<date>2000</date>
<booktitle>Proceedings of Gotelog,</booktitle>
<pages>43--50</pages>
<contexts>
<context position="6259" citStr="Bos and Gabsdil, 2000" startWordPosition="929" endWordPosition="932">&apos; and sleeps as Ax.sleep&apos;(x), the meaning of the sentence “John sleeps” can be constructed as Ax.sleep&apos;(x)(john&apos;) = sleep&apos;(john&apos;). Given a suitable pairing between words and semantic representations of them, this method can produce structured sentential representations with broad coverage and good generalisability (see e.g. (Bos, 2008)). The above logical approach is extremely powerful because it can capture complex aspects of meaning such as quantifiers and their interaction (see e.g. (Copestake et al., 2005)), and enables inference using well studied and developed logical methods (see e.g. (Bos and Gabsdil, 2000)). Distributional hypothesis However, such formal approaches are less able to express similarity in meaning. We would like to capture the intuition that while John and Mary are distinct, they are rather similar to each other (both of them are humans) and dissimilar to words such as dog, pavement or idea. The same applies at the phrase and sentence level: “dogs chase cats” is similar in meaning to “hounds pursue kittens”, but less so to “cats chase dogs” (despite the lexical overlap). Distributional methods provide a way to address this problem. By representing words and phrases as vectors or t</context>
</contexts>
<marker>Bos, Gabsdil, 2000</marker>
<rawString>Johan Bos and Malte Gabsdil. 2000. First-order inference and the interpretation of questions and answers. Proceedings of Gotelog, pages 43–50.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johan Bos</author>
</authors>
<title>Wide-coverage semantic analysis with boxer.</title>
<date>2008</date>
<booktitle>Semantics in Text Processing. STEP 2008 Conference Proceedings, Research in Computational Semantics,</booktitle>
<pages>277--286</pages>
<editor>In Johan Bos and Rodolfo Delmonte, editors,</editor>
<publisher>College Publications.</publisher>
<contexts>
<context position="5974" citStr="Bos, 2008" startWordPosition="885" endWordPosition="886">892). In compositional type-logical approaches, predicateargument structures representing phrases and sentences are built from their constituent parts by β- reduction within the lambda calculus framework (Montague, 1970): for example, given a representation of John as john&apos; and sleeps as Ax.sleep&apos;(x), the meaning of the sentence “John sleeps” can be constructed as Ax.sleep&apos;(x)(john&apos;) = sleep&apos;(john&apos;). Given a suitable pairing between words and semantic representations of them, this method can produce structured sentential representations with broad coverage and good generalisability (see e.g. (Bos, 2008)). The above logical approach is extremely powerful because it can capture complex aspects of meaning such as quantifiers and their interaction (see e.g. (Copestake et al., 2005)), and enables inference using well studied and developed logical methods (see e.g. (Bos and Gabsdil, 2000)). Distributional hypothesis However, such formal approaches are less able to express similarity in meaning. We would like to capture the intuition that while John and Mary are distinct, they are rather similar to each other (both of them are humans) and dissimilar to words such as dog, pavement or idea. The same </context>
</contexts>
<marker>Bos, 2008</marker>
<rawString>Johan Bos. 2008. Wide-coverage semantic analysis with boxer. In Johan Bos and Rodolfo Delmonte, editors, Semantics in Text Processing. STEP 2008 Conference Proceedings, Research in Computational Semantics, pages 277–286. College Publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Bourbaki</author>
</authors>
<title>Commutative Algebra: Chapters 1-7. Srpinger Verlag,</title>
<date>1989</date>
<location>Berlin/New York.</location>
<contexts>
<context position="13406" citStr="Bourbaki, 1989" startWordPosition="2102" endWordPosition="2103">plication O, tensor product ⊗, or matrix multiplication ×. � −c&lt;j&lt;c,j�0 1 T T t=1 710 In the simplest compositional models (the first approach described above), * is + or O, e.g. see (Mitchell and Lapata, 2008). Grammar-based compositional models (the third approach) are based on a generalisation of the notion of vectors, known as tensors. Whereas a vector →−v is an element of an atomic vector space V , a tensor z is an element of a tensor space V ⊗ W ⊗ · · · ⊗ Z. The number of tensored spaces is referred to by the order of the space. Using a general duality theorem from multi-linear algebra (Bourbaki, 1989), it follows that tensors are in one-one correspondence with multi-linear maps, that is we have: z ∈ V ⊗W ⊗···⊗Z ∼= fz : V → W → ··· → Z In such a tensor-based formalism, meanings of nouns are vectors and meanings of predicates such as adjectives and verbs are tensors. Meaning of a string of words is obtained by applying the compositions of multi-linear map duals of the tensors to the vectors. For the sake of demonstration, take the case of an intransitive sentence “Sbj Verb”; −→ the meaning of the subject is a vector Sbj ∈ V and the meaning of the intransitive verb is a tensor Verb ∈ V ⊗ W. M</context>
</contexts>
<marker>Bourbaki, 1989</marker>
<rawString>N. Bourbaki. 1989. Commutative Algebra: Chapters 1-7. Srpinger Verlag, Berlin/New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bob Coecke</author>
<author>Mehrnoosh Sadrzadeh</author>
<author>Stephen Clark</author>
</authors>
<title>Mathematical foundations for a compositional distributional model of meaning.</title>
<date>2010</date>
<journal>CoRR,</journal>
<pages>1003--4394</pages>
<contexts>
<context position="2794" citStr="Coecke et al. (2010)" startWordPosition="395" endWordPosition="398"> on co-occurrence counts. We are especially interested in investigating the performance of neural word vectors in compositional models involving general mathematical composition operators, rather than in the more task- or domain-specific deep-learning compositional settings they have generally been used with so far (for example, by Socher et al. (2012), Kalchbrenner and Blunsom (2013) and many others). In particular, this is the first large-scale study to date that applies neural word representations in tensor-based compositional distributional models of meaning similar to those formalized by Coecke et al. (2010). We test a range of implementations based on this framework, together with additive and multiplicative approaches (Mitchell and Lapata, 2008), in a variety of different tasks. Specifically, we use the verb disambiguation task of Grefenstette and Sadrzadeh (2011a) and the transitive sentence similarity task of Kartsaklis and Sadrzadeh (2014) as small-scale focused experiments on pre-defined sentence structures. Additionally, we evaluate our vector spaces on paraphrase detection (using the Microsoft Research Paraphrase Corpus of Dolan et al. (2005)) and dialogue act tagging using the Switchboar</context>
</contexts>
<marker>Coecke, Sadrzadeh, Clark, 2010</marker>
<rawString>Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen Clark. 2010. Mathematical foundations for a compositional distributional model of meaning. CoRR, abs/1003.4394.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
</authors>
<title>A unified architecture for natural language processing: Deep neural networks with multitask learning.</title>
<date>2008</date>
<booktitle>In Proceedings of the 25th international conference on Machine learning,</booktitle>
<pages>160--167</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="1265" citStr="Collobert and Weston, 2008" startWordPosition="170" endWordPosition="173">licative approaches) in tasks involving verb disambiguation and sentence similarity. To check their scalability, we additionally evaluate the spaces using simple compositional methods on larger-scale tasks with less constrained language: paraphrase detection and dialogue act tagging. In the more constrained tasks, co-occurrence vectors are competitive, although choice of compositional method is important; on the largerscale tasks, they are outperformed by neural word embeddings, which show robust, stable performance across the tasks. 1 Introduction Neural word embeddings (Bengio et al., 2006; Collobert and Weston, 2008; Mikolov et al., 2013a) have received much attention in the distributional semantics community, and have shown state-of-the-art performance in many natural language processing tasks. While they have been compared with co-occurrence based models in simple similarity tasks at the word level (Levy et al., 2014; Baroni et al., 2014), we are aware of only one work that attempts a comparison of the two approaches in compositional settings (Blacoe and Lapata, 2012), and this is limited to additive and multiplicative composition, compared against composition via a neural autoencoder. The purpose of t</context>
</contexts>
<marker>Collobert, Weston, 2008</marker>
<rawString>Ronan Collobert and Jason Weston. 2008. A unified architecture for natural language processing: Deep neural networks with multitask learning. In Proceedings of the 25th international conference on Machine learning, pages 160–167. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ann Copestake</author>
<author>Dan Flickinger</author>
<author>Carl Pollard</author>
<author>Ivan A Sag</author>
</authors>
<title>Minimal recursion semantics: An introduction.</title>
<date>2005</date>
<booktitle>Research on Language and Computation,</booktitle>
<pages>3--2</pages>
<contexts>
<context position="6152" citStr="Copestake et al., 2005" startWordPosition="912" endWordPosition="915">n within the lambda calculus framework (Montague, 1970): for example, given a representation of John as john&apos; and sleeps as Ax.sleep&apos;(x), the meaning of the sentence “John sleeps” can be constructed as Ax.sleep&apos;(x)(john&apos;) = sleep&apos;(john&apos;). Given a suitable pairing between words and semantic representations of them, this method can produce structured sentential representations with broad coverage and good generalisability (see e.g. (Bos, 2008)). The above logical approach is extremely powerful because it can capture complex aspects of meaning such as quantifiers and their interaction (see e.g. (Copestake et al., 2005)), and enables inference using well studied and developed logical methods (see e.g. (Bos and Gabsdil, 2000)). Distributional hypothesis However, such formal approaches are less able to express similarity in meaning. We would like to capture the intuition that while John and Mary are distinct, they are rather similar to each other (both of them are humans) and dissimilar to words such as dog, pavement or idea. The same applies at the phrase and sentence level: “dogs chase cats” is similar in meaning to “hounds pursue kittens”, but less so to “cats chase dogs” (despite the lexical overlap). Dist</context>
</contexts>
<marker>Copestake, Flickinger, Pollard, Sag, 2005</marker>
<rawString>Ann Copestake, Dan Flickinger, Carl Pollard, and Ivan A Sag. 2005. Minimal recursion semantics: An introduction. Research on Language and Computation, 3(2-3):281–332.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bill Dolan</author>
<author>Chris Brockett</author>
<author>Chris Quirk</author>
</authors>
<title>Microsoft research paraphrase corpus. Retrieved</title>
<date>2005</date>
<pages>29--2013</pages>
<contexts>
<context position="3347" citStr="Dolan et al. (2005)" startWordPosition="478" endWordPosition="481">els of meaning similar to those formalized by Coecke et al. (2010). We test a range of implementations based on this framework, together with additive and multiplicative approaches (Mitchell and Lapata, 2008), in a variety of different tasks. Specifically, we use the verb disambiguation task of Grefenstette and Sadrzadeh (2011a) and the transitive sentence similarity task of Kartsaklis and Sadrzadeh (2014) as small-scale focused experiments on pre-defined sentence structures. Additionally, we evaluate our vector spaces on paraphrase detection (using the Microsoft Research Paraphrase Corpus of Dolan et al. (2005)) and dialogue act tagging using the Switchboard Corpus (see e.g. (Stolcke et al., 2000)). In all of the above tasks, we compare the neural word embeddings of Mikolov et al. (2013a) with two vector spaces both based on co-occurrence counts and produced by standard distributional techniques, as described in detail below. The general picture we get from the results is that in almost all cases the neural vectors are more effective than the traditional approaches. We proceed as follows: Section 2 provides a concise introduction to distributional word representations in natural language processing.</context>
<context position="28092" citStr="Dolan et al., 2005" startWordPosition="4544" endWordPosition="4547">, 2010). Method GS11 KS14 NWE Verb only 0.491 0.602 0.561 Addition 0.682 0.732 0.689 Multiplication 0.597 0.321 0.341 Kronecker 0.581 0.408 0.561 Relational 0.558 0.437 0.618 Copy subject 0.370 0.448 0.405 Copy object 0.571 0.306 0.655 Frobenius add. 0.566 0.460 0.585 Frobenius mult. 0.525 0.226 0.387 Frobenius outer 0.560 0.439 0.622 Table 4: Results for sentence similarity. There is no statistically significant difference between KS14 addition and NWE addition (the second best result). Specifically, we get classification results on the Microsoft Research Paraphrase Corpus paraphrase corpus (Dolan et al., 2005) working in the following way: we construct vectors for the sentences of each pair; if the cosine similarity between the two sentence vectors exceeds a certain threshold, the pair is classified as a paraphrase, otherwise as not a paraphrase. For this experiment and that of Section 5.4 below, we investigate only the addition and point-wise multiplication compositional models, since at their current stage of development tensor-based models can only efficiently handle sentences of fixed structure. Nevertheless, the simple point-wise compositional models still allow for a direct comparison of the </context>
</contexts>
<marker>Dolan, Brockett, Quirk, 2005</marker>
<rawString>Bill Dolan, Chris Brockett, and Chris Quirk. 2005. Microsoft research paraphrase corpus. Retrieved May, 29:2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adriano Ferraresi</author>
<author>Eros Zanchetta</author>
<author>Marco Baroni</author>
<author>Silvia Bernardini</author>
</authors>
<title>Introducing and evaluating ukWaC, a very large web-derived corpus of English.</title>
<date>2008</date>
<booktitle>In Proceedings of the 4th Web as Corpus Workshop (WAC-4) Can we beat Google,</booktitle>
<pages>47--54</pages>
<contexts>
<context position="19757" citStr="Ferraresi et al., 2008" startWordPosition="3221" endWordPosition="3224">2011a), Mitchell and Lapata (2008), Levy et al. (2014) for details. PPMI has been shown to achieve state-of-the-art results (Levy et al., 2014) and is suggested by the review of Kiela and Clark (2014). Our use here of the BNC as a corpus and the window length of 5 is based on previous use and better performance of these parameters in a number of compositional experiments (Grefenstette and Sadrzadeh, 2011a; Grefenstette 2http://www.natcorp.ox.ac.uk/ and Sadrzadeh, 2011b; Mitchell and Lapata, 2008; Kartsaklis et al., 2012). KS14 In this variation, we train a vector space from the ukWaC corpus3 (Ferraresi et al., 2008), originally using as a basis the 2,000 content words with the highest frequency (but excluding a list of stop words as well as the 50 most frequent content words since they exhibit low information content). The vector space is again lemmatized. As context we consider a 5-word window from either side of the target word, while as our weighting scheme we use local mutual information (i.e. point-wise mutual information multiplied by raw counts). In a further step, the vector space was normalized and projected onto a 300-dimensional space using singular value decomposition (SVD). In general, dimen</context>
</contexts>
<marker>Ferraresi, Zanchetta, Baroni, Bernardini, 2008</marker>
<rawString>Adriano Ferraresi, Eros Zanchetta, Marco Baroni, and Silvia Bernardini. 2008. Introducing and evaluating ukWaC, a very large web-derived corpus of English. In Proceedings of the 4th Web as Corpus Workshop (WAC-4) Can we beat Google, pages 47–54.</rawString>
</citation>
<citation valid="true">
<title>On sense and reference.</title>
<date>1997</date>
<pages>563--584</pages>
<location>Ludlow</location>
<marker>1997</marker>
<rawString>Gottlob Frege. 1892. On sense and reference. Ludlow (1997), pages 563–584.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John J Godfrey</author>
<author>Edward C Holliman</author>
<author>Jane McDaniel</author>
</authors>
<title>Switchboard: Telephone speech corpus for research and development.</title>
<date>1992</date>
<booktitle>In Acoustics, Speech, and Signal Processing,</booktitle>
<volume>1</volume>
<pages>517--520</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="30784" citStr="Godfrey et al., 1992" startWordPosition="4980" endWordPosition="4983">ime of this writing has been obtained using 8 machine translation metrics and three constituent classifiers (Madnani et al., 2012). The multiplicative model gives lower results than the additive model across all vector spaces. The KS14 vector space shows the steadiest performance, with a drop in accuracy of only 0.04 and no drop in F-score, while for the GS11 and NWE spaces both accuracy and F-score experienced drops by more than 0.20. 5.4 Dialogue act tagging As our last experiment, we evaluate the word spaces on a dialogue act tagging task (Stolcke et al., 2000) over the Switchboard corpus (Godfrey et al., 1992). Switchboard is a collection of approximately 2500 dialogs over a telephone line by 500 speakers from the U.S. on predefined topics.8 The experiment pipeline follows (Milajevs and Purver, 2014). The input utterances are preprocessed so that the parts of interrupted utterances are concatenated (Webb et al., 2005). Disfluency markers and commas are removed from the utterance raw texts. For GS11 and KS14 the utterance tokens are POS-tagged and lemmatized; for NWE, we test the vectors in both a lemmatized and an un-lemmatized version of the corpus.9 We split the training and testing utterances as</context>
</contexts>
<marker>Godfrey, Holliman, McDaniel, 1992</marker>
<rawString>John J Godfrey, Edward C Holliman, and Jane McDaniel. 1992. Switchboard: Telephone speech corpus for research and development. In Acoustics, Speech, and Signal Processing, 1992. ICASSP-92., 1992 IEEE International Conference on, volume 1, pages 517–520. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Goldberg</author>
<author>Omer Levy</author>
</authors>
<title>word2vec Explained: deriving Mikolov et al.’s negativesampling word-embedding method. arXiv preprint arXiv:1402.3722.</title>
<date>2014</date>
<contexts>
<context position="22374" citStr="Goldberg and Levy (2014)" startWordPosition="3661" endWordPosition="3664">bability of all observations coming from the training data. Assume now that D&apos; is a set of randomly selected incorrect (c&apos;, t&apos;) pairs that do not occur in D, then Equation 2 above can be recasted in the following way: ri p(D = 1|c, t) � p(D = 0|c&apos;, t&apos;) (c,t)ED (c&apos;,t&apos;)ED&apos; (3) In other words, the model tries to distinguish a target word t from random draws that come from a noise distribution. In the implementation we used for our experiments, c is always selected from a 5-word window around t. More details about the negative sampling approach can be found in (Mikolov et al., 2013b); the note of Goldberg and Levy (2014) also provides an intuitive explanation of the underlying setting. 5 Experiments Our experiments explore the use of the vector spaces above, together with the compositional operators described in Section 3, in a range of tasks all of which require semantic composition: verb sense disambiguation; sentence similarity; paraphrasing; and dialogue act tagging. 5.1 Disambiguation We use the transitive verb disambiguation dataset described in Grefenstette and Sadrzadeh (2011a)5. This dataset consists of ambiguous transitive verbs together with their arguments, landmark verbs that identify one of the </context>
</contexts>
<marker>Goldberg, Levy, 2014</marker>
<rawString>Yoav Goldberg and Omer Levy. 2014. word2vec Explained: deriving Mikolov et al.’s negativesampling word-embedding method. arXiv preprint arXiv:1402.3722.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Grefenstette</author>
<author>Mehrnoosh Sadrzadeh</author>
</authors>
<title>Experimental support for a categorical compositional distributional model of meaning.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1394--1404</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="3056" citStr="Grefenstette and Sadrzadeh (2011" startWordPosition="435" endWordPosition="438">g compositional settings they have generally been used with so far (for example, by Socher et al. (2012), Kalchbrenner and Blunsom (2013) and many others). In particular, this is the first large-scale study to date that applies neural word representations in tensor-based compositional distributional models of meaning similar to those formalized by Coecke et al. (2010). We test a range of implementations based on this framework, together with additive and multiplicative approaches (Mitchell and Lapata, 2008), in a variety of different tasks. Specifically, we use the verb disambiguation task of Grefenstette and Sadrzadeh (2011a) and the transitive sentence similarity task of Kartsaklis and Sadrzadeh (2014) as small-scale focused experiments on pre-defined sentence structures. Additionally, we evaluate our vector spaces on paraphrase detection (using the Microsoft Research Paraphrase Corpus of Dolan et al. (2005)) and dialogue act tagging using the Switchboard Corpus (see e.g. (Stolcke et al., 2000)). In all of the above tasks, we compare the neural word embeddings of Mikolov et al. (2013a) with two vector spaces both based on co-occurrence counts and produced by standard distributional techniques, as described in d</context>
<context position="12545" citStr="Grefenstette and Sadrzadeh (2011" startWordPosition="1940" endWordPosition="1943"> mentioned in the introduction, their vectors and composition operators are task-specific. These are trained directly to achieve specific objectives in certain pre-determined tasks. We are interested in vector and composition operators that work for any compositional task, and which can be combined with results in linguistics and formal semantics to provide generalisable models that can canonically extend to complex semantic phenomena. The third (i.e. the grammatical) approach promises a way to achieve this, and has been instantiated in various ways in the work of Baroni and Zamparelli (2010),Grefenstette and Sadrzadeh (2011a), and Kartsaklis et al. (2012). General framework Formally, we can specify the vector representation of a word sequence w1w2 · · · wn as the vector →−s = w1 −→ *−→ w2 *···*−→wn, where * is a vector operator, such as addition +, point-wise multiplication O, tensor product ⊗, or matrix multiplication ×. � −c&lt;j&lt;c,j�0 1 T T t=1 710 In the simplest compositional models (the first approach described above), * is + or O, e.g. see (Mitchell and Lapata, 2008). Grammar-based compositional models (the third approach) are based on a generalisation of the notion of vectors, known as tensors. Whereas a ve</context>
<context position="14722" citStr="Grefenstette and Sadrzadeh (2011" startWordPosition="2336" endWordPosition="2339"> fV erb( Sbj) By tensor-map duality, the above becomes equivalent to the following, where composition has now become the familiar notion of matrix multiplication, that is * is ×: Verb × Sbj In general and for words with tensors of order higher than two, * becomes a generalisation of ×, referred to by tensor contraction, see e.g. Kartsaklis and Sadrzadeh (2013). Since the creation and manipulation of tensors of order higher than 2 is difficult, one can work with simplified versions of tensors, faithful to their underlying mathematical basis; these have found intuitive interpretations, e.g. see Grefenstette and Sadrzadeh (2011a), Kartsaklis and Sadrzadeh (2014). In such cases, * becomes a combination of a range of operations such as ×, ⊗, O, and +. Specific models In the current paper we will experiment with a variety of models. In Table 2, we present these models in terms of their composition operators and a reference to the main paper in which each model was introduced. For the simple compositional models the sentence is a string of any number of words; for the grammar-based models, we consider simple transitive sentences “Sbj Verb Obj” and introduce the following abbreviations for the concrete method used to bui</context>
<context position="16060" citStr="Grefenstette and Sadrzadeh, 2011" startWordPosition="2578" endWordPosition="2581">Sbjz and Objz are the subjects and objects of the verb across the corpus. These models are referred to by relational (Grefenstette and Sadrzadeh, 2011a); they are generalisations of predicate semantics of transitive verbs, from pairs of individuals to pairs of vectors. The models reduce the order 3 tensor of a transitive verb to an order 2 tensor (i.e. a matrix). 2. Verb is a verb matrix computed using the formula Verb ⊗ Verb, where Verb is the distributional vector of the verb. These models are referred to by Kronecker, which is the term sometimes used to denote the outer product of tensors (Grefenstette and Sadrzadeh, 2011b). This models also reduces the order 3 tensor of a transitive verb to an order 2 tensor. 3. The models of the last five lines of the table use the so-called Frobenius operators from categorical compositional distributional semantics (Kartsaklis et al., 2012) to expand the relational matrices of verbs from order 2 to order 3. The expansion is obtained by either copying the dimension of the subject into the space provided by the third tensor, hence referred to by Copy-Sbj, or copying the dimension of the object in that space, hence referred to by Copy-Obj; furthermore, we can take addition, mu</context>
<context position="17422" citStr="Grefenstette and Sadrzadeh (2011" startWordPosition="2805" endWordPosition="2808">d Sadrzadeh, 2014). 4 Semantic word spaces Co-occurrence-based vector space instantiations have received a lot of attention from the scientific community (refer to (Kiela and Clark, 2014; Polajnar and Clark, 2014) for recent studies). We instantiate two co-occurrence-based vectors spaces with different underlying corpora and weighting schemes. 711 Method Sentence Linear algebraic formula Reference Addition w1w2 · · · wn w1 + �� �� w2 + · · · + wn�� Mitchell and Lapata (2008) Multiplication w1w2 · · · wn w1�� O w2�� O · · · O wn ��Mitchell and Lapata (2008) Table 2: Compositional methods. Obj) Grefenstette and Sadrzadeh (2011a) �� Obj) Grefenstette and Sadrzadeh (2011b) �� Obj O (Verb&apos; x �� O (Verb x Obj)) + (�� Sbj)) Kartsaklis and Sadrzadeh (2014) Sbj Verb Obj (�� Sbj O (Verb x �� Obj)) O (�� Obj O (Verb&apos; x �� Frob. mult. Sbj)) Kartsaklis and Sadrzadeh (2014) Sbj Verb Obj (�� Sbj O (Verb x �� Obj)) ® (�� Obj O (Verb&apos; x �� Frob. outer Sbj)) Kartsaklis and Sadrzadeh (2014) Copy object Sbj Verb Obj Sbj O (Verb x Obj) Kartsaklis et al. (2012) Copy subject Sbj Verb Obj Obj�� O (Verb&apos; x ��Sbj) Kartsaklis et al. (2012) Sbj Verb Obj (�� Frob. add. Sbj Relational Sbj Verb Obj Verb O ( Sbj® �� Kronecker Sbj Verb Obj Verb </context>
<context position="19138" citStr="Grefenstette and Sadrzadeh (2011" startWordPosition="3120" endWordPosition="3123"> XML distribution2). The vector space is lemmatized, that is, it contains only “canonical” forms of words. In order to weight the raw co-occurrence counts, we use positive point-wise mutual information (PPMI). The component value for a target word t and a context word c is given by: �PPMI(t, c) = max 0, log where p(clt) is the probability of word c given t in a symmetric window of length 5 and p(c) is the probability of coverall. Vector spaces based on point-wise mutual information (or variants thereof) have been successfully applied in various distributional and compositional tasks; see e.g. Grefenstette and Sadrzadeh (2011a), Mitchell and Lapata (2008), Levy et al. (2014) for details. PPMI has been shown to achieve state-of-the-art results (Levy et al., 2014) and is suggested by the review of Kiela and Clark (2014). Our use here of the BNC as a corpus and the window length of 5 is based on previous use and better performance of these parameters in a number of compositional experiments (Grefenstette and Sadrzadeh, 2011a; Grefenstette 2http://www.natcorp.ox.ac.uk/ and Sadrzadeh, 2011b; Mitchell and Lapata, 2008; Kartsaklis et al., 2012). KS14 In this variation, we train a vector space from the ukWaC corpus3 (Ferr</context>
<context position="22846" citStr="Grefenstette and Sadrzadeh (2011" startWordPosition="3729" endWordPosition="3732">d from a 5-word window around t. More details about the negative sampling approach can be found in (Mikolov et al., 2013b); the note of Goldberg and Levy (2014) also provides an intuitive explanation of the underlying setting. 5 Experiments Our experiments explore the use of the vector spaces above, together with the compositional operators described in Section 3, in a range of tasks all of which require semantic composition: verb sense disambiguation; sentence similarity; paraphrasing; and dialogue act tagging. 5.1 Disambiguation We use the transitive verb disambiguation dataset described in Grefenstette and Sadrzadeh (2011a)5. This dataset consists of ambiguous transitive verbs together with their arguments, landmark verbs that identify one of the verb senses, and human judgements that specify how similar is the disambiguated sense of the verb in the given context to 5This and the sentence similarity dataset are available at http://www.cs.ox.ac.uk/activities/ compdistmeaning/ one of the landmarks. This is similar to the intransitive dataset described in (Mitchell and Lapata, 2008). Consider the sentence “system meets specification”; here, meets is the ambiguous transitive verb, and system and specification are </context>
<context position="25923" citStr="Grefenstette and Sadrzadeh, 2011" startWordPosition="4208" endWordPosition="4211">.348). For simple point-wise composition, only multiplicative GS11 and additive NWE improve over their corresponding verb-only baselines (but both perform worse than the KS14 baseline). With tensor-based composition in co-occurrence based spaces, copy subject yields lower results than the corresponding baselines. Other composition methods, except Kronecker for KS14, improve over the verb-only baselines. Finally we should note that, despite the small training corpus, the GS11 vector space performs comparatively well: for instance, Kronecker model improves the previously reported score of 0.28 (Grefenstette and Sadrzadeh, 2011b). 5.2 Sentence similarity In this experiment we use the transitive sentence similarity dataset described in Kartsaklis and Sadrzadeh (2014). The dataset consists of transitive sentence pairs and a human similarity judgement6. The task is to estimate a similarity measure between two sentences. As in the disambiguation task, we first compose word vectors to obtain sentence vectors, then compute cosine similarity of them. We average the human judgements for identical sentence pairs to compute a correlation with cosine scores. Table 4 shows the results. Again, the best performing vector space is</context>
</contexts>
<marker>Grefenstette, Sadrzadeh, 2011</marker>
<rawString>Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011a. Experimental support for a categorical compositional distributional model of meaning. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1394–1404. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Grefenstette</author>
<author>Mehrnoosh Sadrzadeh</author>
</authors>
<title>Experimenting with transitive verbs in a DisCoCat.</title>
<date>2011</date>
<booktitle>In Proceedings of the GEMS 2011 Workshop on GEometrical Models of Natural Language Semantics,</booktitle>
<pages>62--66</pages>
<publisher>Association for</publisher>
<location>Edinburgh, UK,</location>
<note>Distributional structure. Word.</note>
<contexts>
<context position="3056" citStr="Grefenstette and Sadrzadeh (2011" startWordPosition="435" endWordPosition="438">g compositional settings they have generally been used with so far (for example, by Socher et al. (2012), Kalchbrenner and Blunsom (2013) and many others). In particular, this is the first large-scale study to date that applies neural word representations in tensor-based compositional distributional models of meaning similar to those formalized by Coecke et al. (2010). We test a range of implementations based on this framework, together with additive and multiplicative approaches (Mitchell and Lapata, 2008), in a variety of different tasks. Specifically, we use the verb disambiguation task of Grefenstette and Sadrzadeh (2011a) and the transitive sentence similarity task of Kartsaklis and Sadrzadeh (2014) as small-scale focused experiments on pre-defined sentence structures. Additionally, we evaluate our vector spaces on paraphrase detection (using the Microsoft Research Paraphrase Corpus of Dolan et al. (2005)) and dialogue act tagging using the Switchboard Corpus (see e.g. (Stolcke et al., 2000)). In all of the above tasks, we compare the neural word embeddings of Mikolov et al. (2013a) with two vector spaces both based on co-occurrence counts and produced by standard distributional techniques, as described in d</context>
<context position="12545" citStr="Grefenstette and Sadrzadeh (2011" startWordPosition="1940" endWordPosition="1943"> mentioned in the introduction, their vectors and composition operators are task-specific. These are trained directly to achieve specific objectives in certain pre-determined tasks. We are interested in vector and composition operators that work for any compositional task, and which can be combined with results in linguistics and formal semantics to provide generalisable models that can canonically extend to complex semantic phenomena. The third (i.e. the grammatical) approach promises a way to achieve this, and has been instantiated in various ways in the work of Baroni and Zamparelli (2010),Grefenstette and Sadrzadeh (2011a), and Kartsaklis et al. (2012). General framework Formally, we can specify the vector representation of a word sequence w1w2 · · · wn as the vector →−s = w1 −→ *−→ w2 *···*−→wn, where * is a vector operator, such as addition +, point-wise multiplication O, tensor product ⊗, or matrix multiplication ×. � −c&lt;j&lt;c,j�0 1 T T t=1 710 In the simplest compositional models (the first approach described above), * is + or O, e.g. see (Mitchell and Lapata, 2008). Grammar-based compositional models (the third approach) are based on a generalisation of the notion of vectors, known as tensors. Whereas a ve</context>
<context position="14722" citStr="Grefenstette and Sadrzadeh (2011" startWordPosition="2336" endWordPosition="2339"> fV erb( Sbj) By tensor-map duality, the above becomes equivalent to the following, where composition has now become the familiar notion of matrix multiplication, that is * is ×: Verb × Sbj In general and for words with tensors of order higher than two, * becomes a generalisation of ×, referred to by tensor contraction, see e.g. Kartsaklis and Sadrzadeh (2013). Since the creation and manipulation of tensors of order higher than 2 is difficult, one can work with simplified versions of tensors, faithful to their underlying mathematical basis; these have found intuitive interpretations, e.g. see Grefenstette and Sadrzadeh (2011a), Kartsaklis and Sadrzadeh (2014). In such cases, * becomes a combination of a range of operations such as ×, ⊗, O, and +. Specific models In the current paper we will experiment with a variety of models. In Table 2, we present these models in terms of their composition operators and a reference to the main paper in which each model was introduced. For the simple compositional models the sentence is a string of any number of words; for the grammar-based models, we consider simple transitive sentences “Sbj Verb Obj” and introduce the following abbreviations for the concrete method used to bui</context>
<context position="16060" citStr="Grefenstette and Sadrzadeh, 2011" startWordPosition="2578" endWordPosition="2581">Sbjz and Objz are the subjects and objects of the verb across the corpus. These models are referred to by relational (Grefenstette and Sadrzadeh, 2011a); they are generalisations of predicate semantics of transitive verbs, from pairs of individuals to pairs of vectors. The models reduce the order 3 tensor of a transitive verb to an order 2 tensor (i.e. a matrix). 2. Verb is a verb matrix computed using the formula Verb ⊗ Verb, where Verb is the distributional vector of the verb. These models are referred to by Kronecker, which is the term sometimes used to denote the outer product of tensors (Grefenstette and Sadrzadeh, 2011b). This models also reduces the order 3 tensor of a transitive verb to an order 2 tensor. 3. The models of the last five lines of the table use the so-called Frobenius operators from categorical compositional distributional semantics (Kartsaklis et al., 2012) to expand the relational matrices of verbs from order 2 to order 3. The expansion is obtained by either copying the dimension of the subject into the space provided by the third tensor, hence referred to by Copy-Sbj, or copying the dimension of the object in that space, hence referred to by Copy-Obj; furthermore, we can take addition, mu</context>
<context position="17422" citStr="Grefenstette and Sadrzadeh (2011" startWordPosition="2805" endWordPosition="2808">d Sadrzadeh, 2014). 4 Semantic word spaces Co-occurrence-based vector space instantiations have received a lot of attention from the scientific community (refer to (Kiela and Clark, 2014; Polajnar and Clark, 2014) for recent studies). We instantiate two co-occurrence-based vectors spaces with different underlying corpora and weighting schemes. 711 Method Sentence Linear algebraic formula Reference Addition w1w2 · · · wn w1 + �� �� w2 + · · · + wn�� Mitchell and Lapata (2008) Multiplication w1w2 · · · wn w1�� O w2�� O · · · O wn ��Mitchell and Lapata (2008) Table 2: Compositional methods. Obj) Grefenstette and Sadrzadeh (2011a) �� Obj) Grefenstette and Sadrzadeh (2011b) �� Obj O (Verb&apos; x �� O (Verb x Obj)) + (�� Sbj)) Kartsaklis and Sadrzadeh (2014) Sbj Verb Obj (�� Sbj O (Verb x �� Obj)) O (�� Obj O (Verb&apos; x �� Frob. mult. Sbj)) Kartsaklis and Sadrzadeh (2014) Sbj Verb Obj (�� Sbj O (Verb x �� Obj)) ® (�� Obj O (Verb&apos; x �� Frob. outer Sbj)) Kartsaklis and Sadrzadeh (2014) Copy object Sbj Verb Obj Sbj O (Verb x Obj) Kartsaklis et al. (2012) Copy subject Sbj Verb Obj Obj�� O (Verb&apos; x ��Sbj) Kartsaklis et al. (2012) Sbj Verb Obj (�� Frob. add. Sbj Relational Sbj Verb Obj Verb O ( Sbj® �� Kronecker Sbj Verb Obj Verb </context>
<context position="19138" citStr="Grefenstette and Sadrzadeh (2011" startWordPosition="3120" endWordPosition="3123"> XML distribution2). The vector space is lemmatized, that is, it contains only “canonical” forms of words. In order to weight the raw co-occurrence counts, we use positive point-wise mutual information (PPMI). The component value for a target word t and a context word c is given by: �PPMI(t, c) = max 0, log where p(clt) is the probability of word c given t in a symmetric window of length 5 and p(c) is the probability of coverall. Vector spaces based on point-wise mutual information (or variants thereof) have been successfully applied in various distributional and compositional tasks; see e.g. Grefenstette and Sadrzadeh (2011a), Mitchell and Lapata (2008), Levy et al. (2014) for details. PPMI has been shown to achieve state-of-the-art results (Levy et al., 2014) and is suggested by the review of Kiela and Clark (2014). Our use here of the BNC as a corpus and the window length of 5 is based on previous use and better performance of these parameters in a number of compositional experiments (Grefenstette and Sadrzadeh, 2011a; Grefenstette 2http://www.natcorp.ox.ac.uk/ and Sadrzadeh, 2011b; Mitchell and Lapata, 2008; Kartsaklis et al., 2012). KS14 In this variation, we train a vector space from the ukWaC corpus3 (Ferr</context>
<context position="22846" citStr="Grefenstette and Sadrzadeh (2011" startWordPosition="3729" endWordPosition="3732">d from a 5-word window around t. More details about the negative sampling approach can be found in (Mikolov et al., 2013b); the note of Goldberg and Levy (2014) also provides an intuitive explanation of the underlying setting. 5 Experiments Our experiments explore the use of the vector spaces above, together with the compositional operators described in Section 3, in a range of tasks all of which require semantic composition: verb sense disambiguation; sentence similarity; paraphrasing; and dialogue act tagging. 5.1 Disambiguation We use the transitive verb disambiguation dataset described in Grefenstette and Sadrzadeh (2011a)5. This dataset consists of ambiguous transitive verbs together with their arguments, landmark verbs that identify one of the verb senses, and human judgements that specify how similar is the disambiguated sense of the verb in the given context to 5This and the sentence similarity dataset are available at http://www.cs.ox.ac.uk/activities/ compdistmeaning/ one of the landmarks. This is similar to the intransitive dataset described in (Mitchell and Lapata, 2008). Consider the sentence “system meets specification”; here, meets is the ambiguous transitive verb, and system and specification are </context>
<context position="25923" citStr="Grefenstette and Sadrzadeh, 2011" startWordPosition="4208" endWordPosition="4211">.348). For simple point-wise composition, only multiplicative GS11 and additive NWE improve over their corresponding verb-only baselines (but both perform worse than the KS14 baseline). With tensor-based composition in co-occurrence based spaces, copy subject yields lower results than the corresponding baselines. Other composition methods, except Kronecker for KS14, improve over the verb-only baselines. Finally we should note that, despite the small training corpus, the GS11 vector space performs comparatively well: for instance, Kronecker model improves the previously reported score of 0.28 (Grefenstette and Sadrzadeh, 2011b). 5.2 Sentence similarity In this experiment we use the transitive sentence similarity dataset described in Kartsaklis and Sadrzadeh (2014). The dataset consists of transitive sentence pairs and a human similarity judgement6. The task is to estimate a similarity measure between two sentences. As in the disambiguation task, we first compose word vectors to obtain sentence vectors, then compute cosine similarity of them. We average the human judgements for identical sentence pairs to compute a correlation with cosine scores. Table 4 shows the results. Again, the best performing vector space is</context>
</contexts>
<marker>Grefenstette, Sadrzadeh, 2011</marker>
<rawString>Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011b. Experimenting with transitive verbs in a DisCoCat. In Proceedings of the GEMS 2011 Workshop on GEometrical Models of Natural Language Semantics, pages 62–66, Edinburgh, UK, July. Association for Computational Linguistics. Z.S. Harris. 1954. Distributional structure. Word.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nal Kalchbrenner</author>
<author>Phil Blunsom</author>
</authors>
<title>Recurrent convolutional neural networks for discourse compositionality.</title>
<date>2013</date>
<booktitle>In Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality,</booktitle>
<pages>119--126</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="2561" citStr="Kalchbrenner and Blunsom (2013)" startWordPosition="360" endWordPosition="363">ential of neu2University of Oxford Department of Computer Science Parks Road, Oxford, UK dimitri.kartsaklis@cs.ox.ac.uk ral word embeddings in compositional tasks, and meaningfully compare them with the traditional distributional approach based on co-occurrence counts. We are especially interested in investigating the performance of neural word vectors in compositional models involving general mathematical composition operators, rather than in the more task- or domain-specific deep-learning compositional settings they have generally been used with so far (for example, by Socher et al. (2012), Kalchbrenner and Blunsom (2013) and many others). In particular, this is the first large-scale study to date that applies neural word representations in tensor-based compositional distributional models of meaning similar to those formalized by Coecke et al. (2010). We test a range of implementations based on this framework, together with additive and multiplicative approaches (Mitchell and Lapata, 2008), in a variety of different tasks. Specifically, we use the verb disambiguation task of Grefenstette and Sadrzadeh (2011a) and the transitive sentence similarity task of Kartsaklis and Sadrzadeh (2014) as small-scale focused </context>
</contexts>
<marker>Kalchbrenner, Blunsom, 2013</marker>
<rawString>Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent convolutional neural networks for discourse compositionality. In Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality, pages 119–126, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nal Kalchbrenner</author>
<author>Edward Grefenstette</author>
<author>Phil Blunsom</author>
</authors>
<title>A convolutional neural network for modelling sentences.</title>
<date>2014</date>
<booktitle>Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<contexts>
<context position="11844" citStr="Kalchbrenner et al., 2014" startWordPosition="1831" endWordPosition="1834">nes word vectors by vector addition or point-wise multiplication (Mitchell and Lapata, 2008)—as this is independent of word order, it cannot capture the difference between the two sentences “dogs chase cats” and “cats chase dogs”. The second approach has generally been implemented using some form of deep learning, and captures word order, but not by necessarily caring about the grammatical structure of the sentence. Here, one works by recursively building and combining vectors for subsequences of words within the sentence using e.g. autoencoders (Socher et al., 2012) or convolutional filters (Kalchbrenner et al., 2014). We do not consider this approach in this paper. This is because, as mentioned in the introduction, their vectors and composition operators are task-specific. These are trained directly to achieve specific objectives in certain pre-determined tasks. We are interested in vector and composition operators that work for any compositional task, and which can be combined with results in linguistics and formal semantics to provide generalisable models that can canonically extend to complex semantic phenomena. The third (i.e. the grammatical) approach promises a way to achieve this, and has been inst</context>
<context position="37689" citStr="Kalchbrenner et al. (2014)" startWordPosition="6066" endWordPosition="6069">ts suggest that this is indeed the case in large-scale tasks, but the difference in size and nature of the original corpora may be a confounding factor. In any case, it is clear that the neural vectors of word2vec package perform steadily off-the-shelf across a large variety of tasks. The size of the vector space (3 million words) and the available code-base that simplifies the access to the vectors, makes this set a good and safe choice for experiments in the future. Of course, even better performances can be achieved by training neural language models specifically for a given task (see e.g. Kalchbrenner et al. (2014)). The choice of compositional operator (tensorbased or a simple point-wise operation) depends strongly on the task and dataset: tensor-based composition performed best with the verb disambiguation task, where the verb senses depend strongly on the arguments of the verb. However, it seems to depend less on the nature of the vectors itself: in the disambiguation task, tensor-based 716 composition proved best for both co-occurrencebased and neural vectors; in the sentence similarity task, where point-wise operators proved best, this was again true across vector spaces. Acknowledgements We would </context>
</contexts>
<marker>Kalchbrenner, Grefenstette, Blunsom, 2014</marker>
<rawString>Nal Kalchbrenner, Edward Grefenstette, and Phil Blunsom. 2014. A convolutional neural network for modelling sentences. Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dimitri Kartsaklis</author>
<author>Mehrnoosh Sadrzadeh</author>
</authors>
<title>Prior disambiguation of word tensors for constructing sentence vectors.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNL),</booktitle>
<pages>1590--1601</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Seattle, USA,</location>
<contexts>
<context position="14452" citStr="Kartsaklis and Sadrzadeh (2013)" startWordPosition="2296" endWordPosition="2300">monstration, take the case of an intransitive sentence “Sbj Verb”; −→ the meaning of the subject is a vector Sbj ∈ V and the meaning of the intransitive verb is a tensor Verb ∈ V ⊗ W. Meaning of the sentence is obtained by applying fV erb to Sbj, as follows: Sbj Verb = fV erb( Sbj) By tensor-map duality, the above becomes equivalent to the following, where composition has now become the familiar notion of matrix multiplication, that is * is ×: Verb × Sbj In general and for words with tensors of order higher than two, * becomes a generalisation of ×, referred to by tensor contraction, see e.g. Kartsaklis and Sadrzadeh (2013). Since the creation and manipulation of tensors of order higher than 2 is difficult, one can work with simplified versions of tensors, faithful to their underlying mathematical basis; these have found intuitive interpretations, e.g. see Grefenstette and Sadrzadeh (2011a), Kartsaklis and Sadrzadeh (2014). In such cases, * becomes a combination of a range of operations such as ×, ⊗, O, and +. Specific models In the current paper we will experiment with a variety of models. In Table 2, we present these models in terms of their composition operators and a reference to the main paper in which each</context>
<context position="27261" citStr="Kartsaklis and Sadrzadeh, 2013" startWordPosition="4421" endWordPosition="4424">2. Addition was the means for the other vector spaces to achieve top performance as well: GS11 and NWE got 0.682 and 0.689 respectively. None of the models in tensor-based composition outperformed addition. KS14 performs worse with tensor-based methods here than in the other vector spaces. However, GS11 and NWE, except copy subject for both of them and Frobenius multiplication for NWE, improved over their verb-only baselines. 5.3 Paraphrasing In this experiment we evaluate our vector spaces on a mainstream paraphrase detection task. 6The textual content of this dataset is the same as that of (Kartsaklis and Sadrzadeh, 2013), the difference is that the dataset of (Kartsaklis and Sadrzadeh, 2014) has updated human judgements whereas the previous dataset used the original annotations of the intransitive dataset of (Mitchell and Lapata, 2010). Method GS11 KS14 NWE Verb only 0.491 0.602 0.561 Addition 0.682 0.732 0.689 Multiplication 0.597 0.321 0.341 Kronecker 0.581 0.408 0.561 Relational 0.558 0.437 0.618 Copy subject 0.370 0.448 0.405 Copy object 0.571 0.306 0.655 Frobenius add. 0.566 0.460 0.585 Frobenius mult. 0.525 0.226 0.387 Frobenius outer 0.560 0.439 0.622 Table 4: Results for sentence similarity. There is </context>
</contexts>
<marker>Kartsaklis, Sadrzadeh, 2013</marker>
<rawString>Dimitri Kartsaklis and Mehrnoosh Sadrzadeh. 2013. Prior disambiguation of word tensors for constructing sentence vectors. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNL), pages 1590–1601, Seattle, USA, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dimitri Kartsaklis</author>
<author>Mehrnoosh Sadrzadeh</author>
</authors>
<title>A study of entanglement in a categorical framework of natural language.</title>
<date>2014</date>
<booktitle>In Proceedings of the 11th Workshop on Quantum Physics and Logic (QPL),</booktitle>
<location>Kyoto, Japan,</location>
<contexts>
<context position="3137" citStr="Kartsaklis and Sadrzadeh (2014)" startWordPosition="447" endWordPosition="450">y Socher et al. (2012), Kalchbrenner and Blunsom (2013) and many others). In particular, this is the first large-scale study to date that applies neural word representations in tensor-based compositional distributional models of meaning similar to those formalized by Coecke et al. (2010). We test a range of implementations based on this framework, together with additive and multiplicative approaches (Mitchell and Lapata, 2008), in a variety of different tasks. Specifically, we use the verb disambiguation task of Grefenstette and Sadrzadeh (2011a) and the transitive sentence similarity task of Kartsaklis and Sadrzadeh (2014) as small-scale focused experiments on pre-defined sentence structures. Additionally, we evaluate our vector spaces on paraphrase detection (using the Microsoft Research Paraphrase Corpus of Dolan et al. (2005)) and dialogue act tagging using the Switchboard Corpus (see e.g. (Stolcke et al., 2000)). In all of the above tasks, we compare the neural word embeddings of Mikolov et al. (2013a) with two vector spaces both based on co-occurrence counts and produced by standard distributional techniques, as described in detail below. The general picture we get from the results is that in almost all ca</context>
<context position="14757" citStr="Kartsaklis and Sadrzadeh (2014)" startWordPosition="2340" endWordPosition="2344"> the above becomes equivalent to the following, where composition has now become the familiar notion of matrix multiplication, that is * is ×: Verb × Sbj In general and for words with tensors of order higher than two, * becomes a generalisation of ×, referred to by tensor contraction, see e.g. Kartsaklis and Sadrzadeh (2013). Since the creation and manipulation of tensors of order higher than 2 is difficult, one can work with simplified versions of tensors, faithful to their underlying mathematical basis; these have found intuitive interpretations, e.g. see Grefenstette and Sadrzadeh (2011a), Kartsaklis and Sadrzadeh (2014). In such cases, * becomes a combination of a range of operations such as ×, ⊗, O, and +. Specific models In the current paper we will experiment with a variety of models. In Table 2, we present these models in terms of their composition operators and a reference to the main paper in which each model was introduced. For the simple compositional models the sentence is a string of any number of words; for the grammar-based models, we consider simple transitive sentences “Sbj Verb Obj” and introduce the following abbreviations for the concrete method used to build a tensor for the verb: 1. Verb i</context>
<context position="16808" citStr="Kartsaklis and Sadrzadeh, 2014" startWordPosition="2703" endWordPosition="2706">five lines of the table use the so-called Frobenius operators from categorical compositional distributional semantics (Kartsaklis et al., 2012) to expand the relational matrices of verbs from order 2 to order 3. The expansion is obtained by either copying the dimension of the subject into the space provided by the third tensor, hence referred to by Copy-Sbj, or copying the dimension of the object in that space, hence referred to by Copy-Obj; furthermore, we can take addition, multiplication, or outer product of these, which are referred to by FrobeniusAdd, Frobenius-Mult, and Frobenius-Outer (Kartsaklis and Sadrzadeh, 2014). 4 Semantic word spaces Co-occurrence-based vector space instantiations have received a lot of attention from the scientific community (refer to (Kiela and Clark, 2014; Polajnar and Clark, 2014) for recent studies). We instantiate two co-occurrence-based vectors spaces with different underlying corpora and weighting schemes. 711 Method Sentence Linear algebraic formula Reference Addition w1w2 · · · wn w1 + �� �� w2 + · · · + wn�� Mitchell and Lapata (2008) Multiplication w1w2 · · · wn w1�� O w2�� O · · · O wn ��Mitchell and Lapata (2008) Table 2: Compositional methods. Obj) Grefenstette and S</context>
<context position="20653" citStr="Kartsaklis and Sadrzadeh, 2014" startWordPosition="3363" endWordPosition="3366">der a 5-word window from either side of the target word, while as our weighting scheme we use local mutual information (i.e. point-wise mutual information multiplied by raw counts). In a further step, the vector space was normalized and projected onto a 300-dimensional space using singular value decomposition (SVD). In general, dimensionality reduction produces more compact word representations that are robust against potential noise in the corpus (Landauer and Dumais, 1997; Sch¨utze, 1997). SVD has been shown to perform well on a variety of tasks similar to ours (Baroni and Zamparelli, 2010; Kartsaklis and Sadrzadeh, 2014). Neural word embeddings (NWE) For our neural setting, we used the skip-gram model of Mikolov et al. (2013b) trained with negative sampling. The specific implementation that was tested in our experiments was a 300-dimensional vector space learned from the Google News corpus and provided by the word2vec4 toolkit. Furthermore, the gensim library (ˇReh˚uˇrek and Sojka, 2010) was used for accessing the vectors. On the contrary with the previously described co3http://wacky.sslmit.unibo.it/ 4https://code.google.com/p/word2vec/ � p(c|t) p(c) 712 occurrence vector spaces, this version is not lemmatize</context>
<context position="26064" citStr="Kartsaklis and Sadrzadeh (2014)" startWordPosition="4228" endWordPosition="4231">both perform worse than the KS14 baseline). With tensor-based composition in co-occurrence based spaces, copy subject yields lower results than the corresponding baselines. Other composition methods, except Kronecker for KS14, improve over the verb-only baselines. Finally we should note that, despite the small training corpus, the GS11 vector space performs comparatively well: for instance, Kronecker model improves the previously reported score of 0.28 (Grefenstette and Sadrzadeh, 2011b). 5.2 Sentence similarity In this experiment we use the transitive sentence similarity dataset described in Kartsaklis and Sadrzadeh (2014). The dataset consists of transitive sentence pairs and a human similarity judgement6. The task is to estimate a similarity measure between two sentences. As in the disambiguation task, we first compose word vectors to obtain sentence vectors, then compute cosine similarity of them. We average the human judgements for identical sentence pairs to compute a correlation with cosine scores. Table 4 shows the results. Again, the best performing vector space is KS14, but this time with addition: the Spearman ρ correlation score with averaged human judgements is 0.732. Addition was the means for the </context>
<context position="27333" citStr="Kartsaklis and Sadrzadeh, 2014" startWordPosition="4432" endWordPosition="4435">formance as well: GS11 and NWE got 0.682 and 0.689 respectively. None of the models in tensor-based composition outperformed addition. KS14 performs worse with tensor-based methods here than in the other vector spaces. However, GS11 and NWE, except copy subject for both of them and Frobenius multiplication for NWE, improved over their verb-only baselines. 5.3 Paraphrasing In this experiment we evaluate our vector spaces on a mainstream paraphrase detection task. 6The textual content of this dataset is the same as that of (Kartsaklis and Sadrzadeh, 2013), the difference is that the dataset of (Kartsaklis and Sadrzadeh, 2014) has updated human judgements whereas the previous dataset used the original annotations of the intransitive dataset of (Mitchell and Lapata, 2010). Method GS11 KS14 NWE Verb only 0.491 0.602 0.561 Addition 0.682 0.732 0.689 Multiplication 0.597 0.321 0.341 Kronecker 0.581 0.408 0.561 Relational 0.558 0.437 0.618 Copy subject 0.370 0.448 0.405 Copy object 0.571 0.306 0.655 Frobenius add. 0.566 0.460 0.585 Frobenius mult. 0.525 0.226 0.387 Frobenius outer 0.560 0.439 0.622 Table 4: Results for sentence similarity. There is no statistically significant difference between KS14 addition and NWE ad</context>
</contexts>
<marker>Kartsaklis, Sadrzadeh, 2014</marker>
<rawString>Dimitri Kartsaklis and Mehrnoosh Sadrzadeh. 2014. A study of entanglement in a categorical framework of natural language. In Proceedings of the 11th Workshop on Quantum Physics and Logic (QPL), Kyoto, Japan, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dimitri Kartsaklis</author>
<author>Mehrnoosh Sadrzadeh</author>
<author>Stephen Pulman</author>
</authors>
<title>A unified sentence space for categorical distributional-compositional semantics: Theory and experiments.</title>
<date>2012</date>
<booktitle>In Proceedings of COLING 2012: Posters,</booktitle>
<pages>549--558</pages>
<location>Mumbai, India,</location>
<contexts>
<context position="12577" citStr="Kartsaklis et al. (2012)" startWordPosition="1945" endWordPosition="1948">ctors and composition operators are task-specific. These are trained directly to achieve specific objectives in certain pre-determined tasks. We are interested in vector and composition operators that work for any compositional task, and which can be combined with results in linguistics and formal semantics to provide generalisable models that can canonically extend to complex semantic phenomena. The third (i.e. the grammatical) approach promises a way to achieve this, and has been instantiated in various ways in the work of Baroni and Zamparelli (2010),Grefenstette and Sadrzadeh (2011a), and Kartsaklis et al. (2012). General framework Formally, we can specify the vector representation of a word sequence w1w2 · · · wn as the vector →−s = w1 −→ *−→ w2 *···*−→wn, where * is a vector operator, such as addition +, point-wise multiplication O, tensor product ⊗, or matrix multiplication ×. � −c&lt;j&lt;c,j�0 1 T T t=1 710 In the simplest compositional models (the first approach described above), * is + or O, e.g. see (Mitchell and Lapata, 2008). Grammar-based compositional models (the third approach) are based on a generalisation of the notion of vectors, known as tensors. Whereas a vector →−v is an element of an ato</context>
<context position="16320" citStr="Kartsaklis et al., 2012" startWordPosition="2622" endWordPosition="2625">. The models reduce the order 3 tensor of a transitive verb to an order 2 tensor (i.e. a matrix). 2. Verb is a verb matrix computed using the formula Verb ⊗ Verb, where Verb is the distributional vector of the verb. These models are referred to by Kronecker, which is the term sometimes used to denote the outer product of tensors (Grefenstette and Sadrzadeh, 2011b). This models also reduces the order 3 tensor of a transitive verb to an order 2 tensor. 3. The models of the last five lines of the table use the so-called Frobenius operators from categorical compositional distributional semantics (Kartsaklis et al., 2012) to expand the relational matrices of verbs from order 2 to order 3. The expansion is obtained by either copying the dimension of the subject into the space provided by the third tensor, hence referred to by Copy-Sbj, or copying the dimension of the object in that space, hence referred to by Copy-Obj; furthermore, we can take addition, multiplication, or outer product of these, which are referred to by FrobeniusAdd, Frobenius-Mult, and Frobenius-Outer (Kartsaklis and Sadrzadeh, 2014). 4 Semantic word spaces Co-occurrence-based vector space instantiations have received a lot of attention from t</context>
<context position="17845" citStr="Kartsaklis et al. (2012)" startWordPosition="2890" endWordPosition="2893"> �� w2 + · · · + wn�� Mitchell and Lapata (2008) Multiplication w1w2 · · · wn w1�� O w2�� O · · · O wn ��Mitchell and Lapata (2008) Table 2: Compositional methods. Obj) Grefenstette and Sadrzadeh (2011a) �� Obj) Grefenstette and Sadrzadeh (2011b) �� Obj O (Verb&apos; x �� O (Verb x Obj)) + (�� Sbj)) Kartsaklis and Sadrzadeh (2014) Sbj Verb Obj (�� Sbj O (Verb x �� Obj)) O (�� Obj O (Verb&apos; x �� Frob. mult. Sbj)) Kartsaklis and Sadrzadeh (2014) Sbj Verb Obj (�� Sbj O (Verb x �� Obj)) ® (�� Obj O (Verb&apos; x �� Frob. outer Sbj)) Kartsaklis and Sadrzadeh (2014) Copy object Sbj Verb Obj Sbj O (Verb x Obj) Kartsaklis et al. (2012) Copy subject Sbj Verb Obj Obj�� O (Verb&apos; x ��Sbj) Kartsaklis et al. (2012) Sbj Verb Obj (�� Frob. add. Sbj Relational Sbj Verb Obj Verb O ( Sbj® �� Kronecker Sbj Verb Obj Verb O (Sbj ® GS11 Our first word space is based on a typical configuration that has been used in the past extensively for compositional distributional models (see below for details), so it will serve as a useful baseline for the current work. In this vector space, the co-occurrence counts are extracted from the British National Corpus (BNC) (Leech et al., 1994). As basis words, we use the most frequent nouns, verbs, adjecti</context>
<context position="19660" citStr="Kartsaklis et al., 2012" startWordPosition="3204" endWordPosition="3207">y applied in various distributional and compositional tasks; see e.g. Grefenstette and Sadrzadeh (2011a), Mitchell and Lapata (2008), Levy et al. (2014) for details. PPMI has been shown to achieve state-of-the-art results (Levy et al., 2014) and is suggested by the review of Kiela and Clark (2014). Our use here of the BNC as a corpus and the window length of 5 is based on previous use and better performance of these parameters in a number of compositional experiments (Grefenstette and Sadrzadeh, 2011a; Grefenstette 2http://www.natcorp.ox.ac.uk/ and Sadrzadeh, 2011b; Mitchell and Lapata, 2008; Kartsaklis et al., 2012). KS14 In this variation, we train a vector space from the ukWaC corpus3 (Ferraresi et al., 2008), originally using as a basis the 2,000 content words with the highest frequency (but excluding a list of stop words as well as the 50 most frequent content words since they exhibit low information content). The vector space is again lemmatized. As context we consider a 5-word window from either side of the target word, while as our weighting scheme we use local mutual information (i.e. point-wise mutual information multiplied by raw counts). In a further step, the vector space was normalized and p</context>
</contexts>
<marker>Kartsaklis, Sadrzadeh, Pulman, 2012</marker>
<rawString>Dimitri Kartsaklis, Mehrnoosh Sadrzadeh, and Stephen Pulman. 2012. A unified sentence space for categorical distributional-compositional semantics: Theory and experiments. In Proceedings of COLING 2012: Posters, pages 549–558, Mumbai, India, December. The COLING 2012 Organizing Committee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douwe Kiela</author>
<author>Stephen Clark</author>
</authors>
<title>A systematic study of semantic vector space model parameters.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2nd Workshop on Continuous Vector Space Models and their Compositionality (CVSC),</booktitle>
<pages>21--30</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Gothenburg, Sweden,</location>
<contexts>
<context position="8281" citStr="Kiela and Clark, 2014" startWordPosition="1257" endWordPosition="1260">he words Mary, John, girl, boy and idea. The words philosophy, book and school signify vector space dimensions. As the vector for John is closer to Mary than it is to idea in the vector space—a direct consequence of the fact that John’s contexts are similar to Mary’s and dissimilar to idea’s—we can infer that John is semantically more similar to Mary than to idea. Many variants of this approach exist: performance on word similarity tasks has been shown to be improved by replacing raw counts with weighted values (e.g. mutual information)—see (Turney et al., 2010) and below for discussion, and (Kiela and Clark, 2014) for a detailed comparison. philosophy book school Mary 0 10 22 John 4 60 59 girl 0 19 93 boy 0 12 164 idea 10 47 39 Table 1: Word co-occurrence frequencies extracted from the BNC (Leech et al., 1994). 709 Neural word embeddings Deep learning techniques exploit the distributional hypothesis differently. Instead of relying on observed cooccurrence frequencies, a neural language model is trained to maximise some objective function related to e.g. the probability of observing the surrounding words in some context (Mikolov et al., 2013b): log p(wt+j|wt) (1) Optimizing the above function, for examp</context>
<context position="16976" citStr="Kiela and Clark, 2014" startWordPosition="2727" endWordPosition="2730">of verbs from order 2 to order 3. The expansion is obtained by either copying the dimension of the subject into the space provided by the third tensor, hence referred to by Copy-Sbj, or copying the dimension of the object in that space, hence referred to by Copy-Obj; furthermore, we can take addition, multiplication, or outer product of these, which are referred to by FrobeniusAdd, Frobenius-Mult, and Frobenius-Outer (Kartsaklis and Sadrzadeh, 2014). 4 Semantic word spaces Co-occurrence-based vector space instantiations have received a lot of attention from the scientific community (refer to (Kiela and Clark, 2014; Polajnar and Clark, 2014) for recent studies). We instantiate two co-occurrence-based vectors spaces with different underlying corpora and weighting schemes. 711 Method Sentence Linear algebraic formula Reference Addition w1w2 · · · wn w1 + �� �� w2 + · · · + wn�� Mitchell and Lapata (2008) Multiplication w1w2 · · · wn w1�� O w2�� O · · · O wn ��Mitchell and Lapata (2008) Table 2: Compositional methods. Obj) Grefenstette and Sadrzadeh (2011a) �� Obj) Grefenstette and Sadrzadeh (2011b) �� Obj O (Verb&apos; x �� O (Verb x Obj)) + (�� Sbj)) Kartsaklis and Sadrzadeh (2014) Sbj Verb Obj (�� Sbj O (Ver</context>
<context position="19334" citStr="Kiela and Clark (2014)" startWordPosition="3153" endWordPosition="3156">I). The component value for a target word t and a context word c is given by: �PPMI(t, c) = max 0, log where p(clt) is the probability of word c given t in a symmetric window of length 5 and p(c) is the probability of coverall. Vector spaces based on point-wise mutual information (or variants thereof) have been successfully applied in various distributional and compositional tasks; see e.g. Grefenstette and Sadrzadeh (2011a), Mitchell and Lapata (2008), Levy et al. (2014) for details. PPMI has been shown to achieve state-of-the-art results (Levy et al., 2014) and is suggested by the review of Kiela and Clark (2014). Our use here of the BNC as a corpus and the window length of 5 is based on previous use and better performance of these parameters in a number of compositional experiments (Grefenstette and Sadrzadeh, 2011a; Grefenstette 2http://www.natcorp.ox.ac.uk/ and Sadrzadeh, 2011b; Mitchell and Lapata, 2008; Kartsaklis et al., 2012). KS14 In this variation, we train a vector space from the ukWaC corpus3 (Ferraresi et al., 2008), originally using as a basis the 2,000 content words with the highest frequency (but excluding a list of stop words as well as the 50 most frequent content words since they exh</context>
</contexts>
<marker>Kiela, Clark, 2014</marker>
<rawString>Douwe Kiela and Stephen Clark. 2014. A systematic study of semantic vector space model parameters. In Proceedings of the 2nd Workshop on Continuous Vector Space Models and their Compositionality (CVSC), pages 21–30, Gothenburg, Sweden, April. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Landauer</author>
<author>S Dumais</author>
</authors>
<title>A Solution to Plato’s Problem: The Latent Semantic Analysis Theory of Acquision, Induction, and Representation of Knowledge. Psychological Review.</title>
<date>1997</date>
<contexts>
<context position="20500" citStr="Landauer and Dumais, 1997" startWordPosition="3338" endWordPosition="3341">s well as the 50 most frequent content words since they exhibit low information content). The vector space is again lemmatized. As context we consider a 5-word window from either side of the target word, while as our weighting scheme we use local mutual information (i.e. point-wise mutual information multiplied by raw counts). In a further step, the vector space was normalized and projected onto a 300-dimensional space using singular value decomposition (SVD). In general, dimensionality reduction produces more compact word representations that are robust against potential noise in the corpus (Landauer and Dumais, 1997; Sch¨utze, 1997). SVD has been shown to perform well on a variety of tasks similar to ours (Baroni and Zamparelli, 2010; Kartsaklis and Sadrzadeh, 2014). Neural word embeddings (NWE) For our neural setting, we used the skip-gram model of Mikolov et al. (2013b) trained with negative sampling. The specific implementation that was tested in our experiments was a 300-dimensional vector space learned from the Google News corpus and provided by the word2vec4 toolkit. Furthermore, the gensim library (ˇReh˚uˇrek and Sojka, 2010) was used for accessing the vectors. On the contrary with the previously </context>
</contexts>
<marker>Landauer, Dumais, 1997</marker>
<rawString>T. Landauer and S. Dumais. 1997. A Solution to Plato’s Problem: The Latent Semantic Analysis Theory of Acquision, Induction, and Representation of Knowledge. Psychological Review.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey Leech</author>
<author>Roger Garside</author>
<author>Michael Bryant</author>
</authors>
<title>Claws4: the tagging of the british national corpus.</title>
<date>1994</date>
<booktitle>In Proceedings of the 15th conference on Computational linguistics-Volume 1,</booktitle>
<pages>622--628</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="8481" citStr="Leech et al., 1994" startWordPosition="1298" endWordPosition="1301">nsequence of the fact that John’s contexts are similar to Mary’s and dissimilar to idea’s—we can infer that John is semantically more similar to Mary than to idea. Many variants of this approach exist: performance on word similarity tasks has been shown to be improved by replacing raw counts with weighted values (e.g. mutual information)—see (Turney et al., 2010) and below for discussion, and (Kiela and Clark, 2014) for a detailed comparison. philosophy book school Mary 0 10 22 John 4 60 59 girl 0 19 93 boy 0 12 164 idea 10 47 39 Table 1: Word co-occurrence frequencies extracted from the BNC (Leech et al., 1994). 709 Neural word embeddings Deep learning techniques exploit the distributional hypothesis differently. Instead of relying on observed cooccurrence frequencies, a neural language model is trained to maximise some objective function related to e.g. the probability of observing the surrounding words in some context (Mikolov et al., 2013b): log p(wt+j|wt) (1) Optimizing the above function, for example, produces vectors which maximise the conditional probability of observing words in a context around the target word wt, where c is the size of the training window, and w1w2i · · · wT a sequence of </context>
<context position="18381" citStr="Leech et al., 1994" startWordPosition="2989" endWordPosition="2992">drzadeh (2014) Copy object Sbj Verb Obj Sbj O (Verb x Obj) Kartsaklis et al. (2012) Copy subject Sbj Verb Obj Obj�� O (Verb&apos; x ��Sbj) Kartsaklis et al. (2012) Sbj Verb Obj (�� Frob. add. Sbj Relational Sbj Verb Obj Verb O ( Sbj® �� Kronecker Sbj Verb Obj Verb O (Sbj ® GS11 Our first word space is based on a typical configuration that has been used in the past extensively for compositional distributional models (see below for details), so it will serve as a useful baseline for the current work. In this vector space, the co-occurrence counts are extracted from the British National Corpus (BNC) (Leech et al., 1994). As basis words, we use the most frequent nouns, verbs, adjectives and adverbs (POS tags SUBST, VERB, ADJ and ADV in the BNC XML distribution2). The vector space is lemmatized, that is, it contains only “canonical” forms of words. In order to weight the raw co-occurrence counts, we use positive point-wise mutual information (PPMI). The component value for a target word t and a context word c is given by: �PPMI(t, c) = max 0, log where p(clt) is the probability of word c given t in a symmetric window of length 5 and p(c) is the probability of coverall. Vector spaces based on point-wise mutual </context>
</contexts>
<marker>Leech, Garside, Bryant, 1994</marker>
<rawString>Geoffrey Leech, Roger Garside, and Michael Bryant. 1994. Claws4: the tagging of the british national corpus. In Proceedings of the 15th conference on Computational linguistics-Volume 1, pages 622– 628. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omer Levy</author>
<author>Yoav Goldberg</author>
<author>Israel Ramat-Gan</author>
</authors>
<title>Linguistic regularities in sparse and explicit word representations.</title>
<date>2014</date>
<booktitle>In Proceedings of the Eighteenth Conference on Computational Natural Language Learning,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, Maryland, USA,</location>
<contexts>
<context position="1574" citStr="Levy et al., 2014" startWordPosition="217" endWordPosition="220">-occurrence vectors are competitive, although choice of compositional method is important; on the largerscale tasks, they are outperformed by neural word embeddings, which show robust, stable performance across the tasks. 1 Introduction Neural word embeddings (Bengio et al., 2006; Collobert and Weston, 2008; Mikolov et al., 2013a) have received much attention in the distributional semantics community, and have shown state-of-the-art performance in many natural language processing tasks. While they have been compared with co-occurrence based models in simple similarity tasks at the word level (Levy et al., 2014; Baroni et al., 2014), we are aware of only one work that attempts a comparison of the two approaches in compositional settings (Blacoe and Lapata, 2012), and this is limited to additive and multiplicative composition, compared against composition via a neural autoencoder. The purpose of this paper is to provide a more complete picture regarding the potential of neu2University of Oxford Department of Computer Science Parks Road, Oxford, UK dimitri.kartsaklis@cs.ox.ac.uk ral word embeddings in compositional tasks, and meaningfully compare them with the traditional distributional approach based</context>
<context position="10378" citStr="Levy et al. (2014)" startWordPosition="1610" endWordPosition="1613">, it is possible to extract the singular:plural relation (apple:apples, car:cars) using vector subtraction: → apple − apples ≈ −→ → car − −−→ cars Perhaps even more importantly, semantic relationships are preserved in a very intuitive way: −−→ king −−−→ man ≈ −−−→ queen − −−−−→ woman allowing the formation of analogy queries similar −−→ man + −−−−→ to king − −−→ woman = ?, obtaining −−−→ queen as the result.1 Both neural and co-occurrence-based approaches have advantages over classical formal approaches in their ability to capture lexical semantics and degrees of similarity; their success at 1Levy et al. (2014) improved Mikolov et al. (2013c)’s method of retrieving relational similarities by changing the underlying objective function. extending this to the sentence level and to more complex semantic phenomena, though, depends on their applicability within compositional models, which is the subject of the next section. 3 Compositional models Compositional distributional models represent meaning of a sequence of words by a vector, obtained by combining meaning vectors of the words within the sequence using some vector composition operation. In a general classification of these models, one can distingu</context>
<context position="19188" citStr="Levy et al. (2014)" startWordPosition="3128" endWordPosition="3131"> contains only “canonical” forms of words. In order to weight the raw co-occurrence counts, we use positive point-wise mutual information (PPMI). The component value for a target word t and a context word c is given by: �PPMI(t, c) = max 0, log where p(clt) is the probability of word c given t in a symmetric window of length 5 and p(c) is the probability of coverall. Vector spaces based on point-wise mutual information (or variants thereof) have been successfully applied in various distributional and compositional tasks; see e.g. Grefenstette and Sadrzadeh (2011a), Mitchell and Lapata (2008), Levy et al. (2014) for details. PPMI has been shown to achieve state-of-the-art results (Levy et al., 2014) and is suggested by the review of Kiela and Clark (2014). Our use here of the BNC as a corpus and the window length of 5 is based on previous use and better performance of these parameters in a number of compositional experiments (Grefenstette and Sadrzadeh, 2011a; Grefenstette 2http://www.natcorp.ox.ac.uk/ and Sadrzadeh, 2011b; Mitchell and Lapata, 2008; Kartsaklis et al., 2012). KS14 In this variation, we train a vector space from the ukWaC corpus3 (Ferraresi et al., 2008), originally using as a basis t</context>
<context position="33034" citStr="Levy et al. (2014)" startWordPosition="5320" endWordPosition="5323"> all the models. Lemmatization dramatically lowers tagging accuracy: the lemmatized GS11, KS14 and NWE models perform much worse than un-lemmatized NWE, suggesting that morphological features are important for this task. 6 Discussion Previous comparisons of co-occurrence-based and neural word vector representations vary widely in their conclusions. While Baroni et al. (2014) conclude that “context-predicting models obtain a thorough and resounding victory against their count-based counterparts”, this seems to contradict, at least at the first consideration, the more conservative conclusion of Levy et al. (2014) that “analogy recovery is not restricted to neural word embeddings [... ] a similar amount of relational similarities can be recovered from traditional distributional word representations” and the findings of Blacoe and Lapata (2012) that “shallow approaches are as good as more computationally intensive alternatives” on phrase similarity and paraphrase detection tasks. It seems clear that neural word embeddings have an advantage when used in tasks for which they have been trained; our main questions here are whether they outperform co-occurrence based alternatives across the board; and which </context>
</contexts>
<marker>Levy, Goldberg, Ramat-Gan, 2014</marker>
<rawString>Omer Levy, Yoav Goldberg, and Israel Ramat-Gan. 2014. Linguistic regularities in sparse and explicit word representations. In Proceedings of the Eighteenth Conference on Computational Natural Language Learning, Baltimore, Maryland, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nitin Madnani</author>
<author>Joel Tetreault</author>
<author>Martin Chodorow</author>
</authors>
<title>Re-examining machine translation metrics for paraphrase identification.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>182--190</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="30293" citStr="Madnani et al., 2012" startWordPosition="4895" endWordPosition="4898">Score Accuracy F-Score MSR addition 0.65 0.75 0.62 0.79 0.70 0.80 0.73 0.82 0.72 0.81 MSR multiplication 0.52 0.58 0.66 0.80 0.42 0.34 0.41 0.36 SWDA addition 0.60 0.58 0.35 0.35 0.40 0.35 0.63 0.60 0.44 0.40 SWDA multiplication 0.32 0.16 0.39 0.33 0.58 0.53 0.43 0.38 Table 5: Results for paraphrase detection (MSR) and dialog act tagging (SWDA) tasks. All top results significantly outperform corresponding nearest competitors (for accuracy): p &lt; 0.05, x2 test. and 0.84 F-score7) by the time of this writing has been obtained using 8 machine translation metrics and three constituent classifiers (Madnani et al., 2012). The multiplicative model gives lower results than the additive model across all vector spaces. The KS14 vector space shows the steadiest performance, with a drop in accuracy of only 0.04 and no drop in F-score, while for the GS11 and NWE spaces both accuracy and F-score experienced drops by more than 0.20. 5.4 Dialogue act tagging As our last experiment, we evaluate the word spaces on a dialogue act tagging task (Stolcke et al., 2000) over the Switchboard corpus (Godfrey et al., 1992). Switchboard is a collection of approximately 2500 dialogs over a telephone line by 500 speakers from the U.</context>
</contexts>
<marker>Madnani, Tetreault, Chodorow, 2012</marker>
<rawString>Nitin Madnani, Joel Tetreault, and Martin Chodorow. 2012. Re-examining machine translation metrics for paraphrase identification. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 182–190. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Courtney Corley</author>
<author>Carlo Strapparava</author>
</authors>
<title>Corpus-based and knowledge-based measures of text semantic similarity.</title>
<date>2006</date>
<booktitle>In AAAI,</booktitle>
<volume>6</volume>
<pages>775--780</pages>
<contexts>
<context position="34061" citStr="Mihalcea et al., 2006" startWordPosition="5476" endWordPosition="5479">embeddings have an advantage when used in tasks for which they have been trained; our main questions here are whether they outperform co-occurrence based alternatives across the board; and which approach lends itself better to composition using general mathematical operators. To partially an715 swer this question, we can compare model behaviour against the baselines in isolation. For the disambiguation and sentence similarity tasks the baseline is the similarity between verbs only, ignoring the context—see above. For the paraphrase task, we take the global vector-based similarity reported in (Mihalcea et al., 2006): 0.65 accuracy and 0.75 F-score. For the dialogue act tagging task the baseline is the accuracy of the bag-of-unigrams model in (Milajevs and Purver, 2014): 0.60. Sections 5.1 and 5.2 show that although the best choice of vector representation might vary, for small-scale tasks all methods give fairly competitive results. The choice of compositional operator seems to be more important and more taskspecific: while a tensor-based operation (Frobenius copy-object) performs best for verb disambiguation, the best result for sentence similarity is achieved by a simple additive model, with all other </context>
</contexts>
<marker>Mihalcea, Corley, Strapparava, 2006</marker>
<rawString>Rada Mihalcea, Courtney Corley, and Carlo Strapparava. 2006. Corpus-based and knowledge-based measures of text semantic similarity. In AAAI, volume 6, pages 775–780.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.</title>
<date>2013</date>
<contexts>
<context position="1287" citStr="Mikolov et al., 2013" startWordPosition="174" endWordPosition="177">s involving verb disambiguation and sentence similarity. To check their scalability, we additionally evaluate the spaces using simple compositional methods on larger-scale tasks with less constrained language: paraphrase detection and dialogue act tagging. In the more constrained tasks, co-occurrence vectors are competitive, although choice of compositional method is important; on the largerscale tasks, they are outperformed by neural word embeddings, which show robust, stable performance across the tasks. 1 Introduction Neural word embeddings (Bengio et al., 2006; Collobert and Weston, 2008; Mikolov et al., 2013a) have received much attention in the distributional semantics community, and have shown state-of-the-art performance in many natural language processing tasks. While they have been compared with co-occurrence based models in simple similarity tasks at the word level (Levy et al., 2014; Baroni et al., 2014), we are aware of only one work that attempts a comparison of the two approaches in compositional settings (Blacoe and Lapata, 2012), and this is limited to additive and multiplicative composition, compared against composition via a neural autoencoder. The purpose of this paper is to provid</context>
<context position="3526" citStr="Mikolov et al. (2013" startWordPosition="510" endWordPosition="513">es (Mitchell and Lapata, 2008), in a variety of different tasks. Specifically, we use the verb disambiguation task of Grefenstette and Sadrzadeh (2011a) and the transitive sentence similarity task of Kartsaklis and Sadrzadeh (2014) as small-scale focused experiments on pre-defined sentence structures. Additionally, we evaluate our vector spaces on paraphrase detection (using the Microsoft Research Paraphrase Corpus of Dolan et al. (2005)) and dialogue act tagging using the Switchboard Corpus (see e.g. (Stolcke et al., 2000)). In all of the above tasks, we compare the neural word embeddings of Mikolov et al. (2013a) with two vector spaces both based on co-occurrence counts and produced by standard distributional techniques, as described in detail below. The general picture we get from the results is that in almost all cases the neural vectors are more effective than the traditional approaches. We proceed as follows: Section 2 provides a concise introduction to distributional word representations in natural language processing. Section 708 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 708–719, October 25-29, 2014, Doha, Qatar. c�2014 Association fo</context>
<context position="8818" citStr="Mikolov et al., 2013" startWordPosition="1350" endWordPosition="1353">on)—see (Turney et al., 2010) and below for discussion, and (Kiela and Clark, 2014) for a detailed comparison. philosophy book school Mary 0 10 22 John 4 60 59 girl 0 19 93 boy 0 12 164 idea 10 47 39 Table 1: Word co-occurrence frequencies extracted from the BNC (Leech et al., 1994). 709 Neural word embeddings Deep learning techniques exploit the distributional hypothesis differently. Instead of relying on observed cooccurrence frequencies, a neural language model is trained to maximise some objective function related to e.g. the probability of observing the surrounding words in some context (Mikolov et al., 2013b): log p(wt+j|wt) (1) Optimizing the above function, for example, produces vectors which maximise the conditional probability of observing words in a context around the target word wt, where c is the size of the training window, and w1w2i · · · wT a sequence of words forming a training instance. Therefore, the resulting vectors will capture the distributional intuition and can express degrees of lexical similarity. This method has an obvious advantage compared to co-occurrence method: since now the context is predicted, the model in principle can be much more robust in data sparsity problems,</context>
<context position="10408" citStr="Mikolov et al. (2013" startWordPosition="1615" endWordPosition="1618">the singular:plural relation (apple:apples, car:cars) using vector subtraction: → apple − apples ≈ −→ → car − −−→ cars Perhaps even more importantly, semantic relationships are preserved in a very intuitive way: −−→ king −−−→ man ≈ −−−→ queen − −−−−→ woman allowing the formation of analogy queries similar −−→ man + −−−−→ to king − −−→ woman = ?, obtaining −−−→ queen as the result.1 Both neural and co-occurrence-based approaches have advantages over classical formal approaches in their ability to capture lexical semantics and degrees of similarity; their success at 1Levy et al. (2014) improved Mikolov et al. (2013c)’s method of retrieving relational similarities by changing the underlying objective function. extending this to the sentence level and to more complex semantic phenomena, though, depends on their applicability within compositional models, which is the subject of the next section. 3 Compositional models Compositional distributional models represent meaning of a sequence of words by a vector, obtained by combining meaning vectors of the words within the sequence using some vector composition operation. In a general classification of these models, one can distinguish between three broad cases:</context>
<context position="20759" citStr="Mikolov et al. (2013" startWordPosition="3382" endWordPosition="3385">n (i.e. point-wise mutual information multiplied by raw counts). In a further step, the vector space was normalized and projected onto a 300-dimensional space using singular value decomposition (SVD). In general, dimensionality reduction produces more compact word representations that are robust against potential noise in the corpus (Landauer and Dumais, 1997; Sch¨utze, 1997). SVD has been shown to perform well on a variety of tasks similar to ours (Baroni and Zamparelli, 2010; Kartsaklis and Sadrzadeh, 2014). Neural word embeddings (NWE) For our neural setting, we used the skip-gram model of Mikolov et al. (2013b) trained with negative sampling. The specific implementation that was tested in our experiments was a 300-dimensional vector space learned from the Google News corpus and provided by the word2vec4 toolkit. Furthermore, the gensim library (ˇReh˚uˇrek and Sojka, 2010) was used for accessing the vectors. On the contrary with the previously described co3http://wacky.sslmit.unibo.it/ 4https://code.google.com/p/word2vec/ � p(c|t) p(c) 712 occurrence vector spaces, this version is not lemmatized. The negative sampling method improves the objective function of Equation 1 by introducing negative exam</context>
<context position="22334" citStr="Mikolov et al., 2013" startWordPosition="3654" endWordPosition="3657">ters in a way that maximizes the probability of all observations coming from the training data. Assume now that D&apos; is a set of randomly selected incorrect (c&apos;, t&apos;) pairs that do not occur in D, then Equation 2 above can be recasted in the following way: ri p(D = 1|c, t) � p(D = 0|c&apos;, t&apos;) (c,t)ED (c&apos;,t&apos;)ED&apos; (3) In other words, the model tries to distinguish a target word t from random draws that come from a noise distribution. In the implementation we used for our experiments, c is always selected from a 5-word window around t. More details about the negative sampling approach can be found in (Mikolov et al., 2013b); the note of Goldberg and Levy (2014) also provides an intuitive explanation of the underlying setting. 5 Experiments Our experiments explore the use of the vector spaces above, together with the compositional operators described in Section 3, in a range of tasks all of which require semantic composition: verb sense disambiguation; sentence similarity; paraphrasing; and dialogue act tagging. 5.1 Disambiguation We use the transitive verb disambiguation dataset described in Grefenstette and Sadrzadeh (2011a)5. This dataset consists of ambiguous transitive verbs together with their arguments, </context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013a. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Greg S Corrado</author>
<author>Jeff Dean</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality.</title>
<date>2013</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>3111--3119</pages>
<contexts>
<context position="1287" citStr="Mikolov et al., 2013" startWordPosition="174" endWordPosition="177">s involving verb disambiguation and sentence similarity. To check their scalability, we additionally evaluate the spaces using simple compositional methods on larger-scale tasks with less constrained language: paraphrase detection and dialogue act tagging. In the more constrained tasks, co-occurrence vectors are competitive, although choice of compositional method is important; on the largerscale tasks, they are outperformed by neural word embeddings, which show robust, stable performance across the tasks. 1 Introduction Neural word embeddings (Bengio et al., 2006; Collobert and Weston, 2008; Mikolov et al., 2013a) have received much attention in the distributional semantics community, and have shown state-of-the-art performance in many natural language processing tasks. While they have been compared with co-occurrence based models in simple similarity tasks at the word level (Levy et al., 2014; Baroni et al., 2014), we are aware of only one work that attempts a comparison of the two approaches in compositional settings (Blacoe and Lapata, 2012), and this is limited to additive and multiplicative composition, compared against composition via a neural autoencoder. The purpose of this paper is to provid</context>
<context position="3526" citStr="Mikolov et al. (2013" startWordPosition="510" endWordPosition="513">es (Mitchell and Lapata, 2008), in a variety of different tasks. Specifically, we use the verb disambiguation task of Grefenstette and Sadrzadeh (2011a) and the transitive sentence similarity task of Kartsaklis and Sadrzadeh (2014) as small-scale focused experiments on pre-defined sentence structures. Additionally, we evaluate our vector spaces on paraphrase detection (using the Microsoft Research Paraphrase Corpus of Dolan et al. (2005)) and dialogue act tagging using the Switchboard Corpus (see e.g. (Stolcke et al., 2000)). In all of the above tasks, we compare the neural word embeddings of Mikolov et al. (2013a) with two vector spaces both based on co-occurrence counts and produced by standard distributional techniques, as described in detail below. The general picture we get from the results is that in almost all cases the neural vectors are more effective than the traditional approaches. We proceed as follows: Section 2 provides a concise introduction to distributional word representations in natural language processing. Section 708 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 708–719, October 25-29, 2014, Doha, Qatar. c�2014 Association fo</context>
<context position="8818" citStr="Mikolov et al., 2013" startWordPosition="1350" endWordPosition="1353">on)—see (Turney et al., 2010) and below for discussion, and (Kiela and Clark, 2014) for a detailed comparison. philosophy book school Mary 0 10 22 John 4 60 59 girl 0 19 93 boy 0 12 164 idea 10 47 39 Table 1: Word co-occurrence frequencies extracted from the BNC (Leech et al., 1994). 709 Neural word embeddings Deep learning techniques exploit the distributional hypothesis differently. Instead of relying on observed cooccurrence frequencies, a neural language model is trained to maximise some objective function related to e.g. the probability of observing the surrounding words in some context (Mikolov et al., 2013b): log p(wt+j|wt) (1) Optimizing the above function, for example, produces vectors which maximise the conditional probability of observing words in a context around the target word wt, where c is the size of the training window, and w1w2i · · · wT a sequence of words forming a training instance. Therefore, the resulting vectors will capture the distributional intuition and can express degrees of lexical similarity. This method has an obvious advantage compared to co-occurrence method: since now the context is predicted, the model in principle can be much more robust in data sparsity problems,</context>
<context position="10408" citStr="Mikolov et al. (2013" startWordPosition="1615" endWordPosition="1618">the singular:plural relation (apple:apples, car:cars) using vector subtraction: → apple − apples ≈ −→ → car − −−→ cars Perhaps even more importantly, semantic relationships are preserved in a very intuitive way: −−→ king −−−→ man ≈ −−−→ queen − −−−−→ woman allowing the formation of analogy queries similar −−→ man + −−−−→ to king − −−→ woman = ?, obtaining −−−→ queen as the result.1 Both neural and co-occurrence-based approaches have advantages over classical formal approaches in their ability to capture lexical semantics and degrees of similarity; their success at 1Levy et al. (2014) improved Mikolov et al. (2013c)’s method of retrieving relational similarities by changing the underlying objective function. extending this to the sentence level and to more complex semantic phenomena, though, depends on their applicability within compositional models, which is the subject of the next section. 3 Compositional models Compositional distributional models represent meaning of a sequence of words by a vector, obtained by combining meaning vectors of the words within the sequence using some vector composition operation. In a general classification of these models, one can distinguish between three broad cases:</context>
<context position="20759" citStr="Mikolov et al. (2013" startWordPosition="3382" endWordPosition="3385">n (i.e. point-wise mutual information multiplied by raw counts). In a further step, the vector space was normalized and projected onto a 300-dimensional space using singular value decomposition (SVD). In general, dimensionality reduction produces more compact word representations that are robust against potential noise in the corpus (Landauer and Dumais, 1997; Sch¨utze, 1997). SVD has been shown to perform well on a variety of tasks similar to ours (Baroni and Zamparelli, 2010; Kartsaklis and Sadrzadeh, 2014). Neural word embeddings (NWE) For our neural setting, we used the skip-gram model of Mikolov et al. (2013b) trained with negative sampling. The specific implementation that was tested in our experiments was a 300-dimensional vector space learned from the Google News corpus and provided by the word2vec4 toolkit. Furthermore, the gensim library (ˇReh˚uˇrek and Sojka, 2010) was used for accessing the vectors. On the contrary with the previously described co3http://wacky.sslmit.unibo.it/ 4https://code.google.com/p/word2vec/ � p(c|t) p(c) 712 occurrence vector spaces, this version is not lemmatized. The negative sampling method improves the objective function of Equation 1 by introducing negative exam</context>
<context position="22334" citStr="Mikolov et al., 2013" startWordPosition="3654" endWordPosition="3657">ters in a way that maximizes the probability of all observations coming from the training data. Assume now that D&apos; is a set of randomly selected incorrect (c&apos;, t&apos;) pairs that do not occur in D, then Equation 2 above can be recasted in the following way: ri p(D = 1|c, t) � p(D = 0|c&apos;, t&apos;) (c,t)ED (c&apos;,t&apos;)ED&apos; (3) In other words, the model tries to distinguish a target word t from random draws that come from a noise distribution. In the implementation we used for our experiments, c is always selected from a 5-word window around t. More details about the negative sampling approach can be found in (Mikolov et al., 2013b); the note of Goldberg and Levy (2014) also provides an intuitive explanation of the underlying setting. 5 Experiments Our experiments explore the use of the vector spaces above, together with the compositional operators described in Section 3, in a range of tasks all of which require semantic composition: verb sense disambiguation; sentence similarity; paraphrasing; and dialogue act tagging. 5.1 Disambiguation We use the transitive verb disambiguation dataset described in Grefenstette and Sadrzadeh (2011a)5. This dataset consists of ambiguous transitive verbs together with their arguments, </context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013b. Distributed representations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems, pages 3111–3119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Wen-tau Yih</author>
<author>Geoffrey Zweig</author>
</authors>
<title>Linguistic regularities in continuous space word representations.</title>
<date>2013</date>
<booktitle>In Proceedings of NAACLHLT,</booktitle>
<pages>746--751</pages>
<contexts>
<context position="1287" citStr="Mikolov et al., 2013" startWordPosition="174" endWordPosition="177">s involving verb disambiguation and sentence similarity. To check their scalability, we additionally evaluate the spaces using simple compositional methods on larger-scale tasks with less constrained language: paraphrase detection and dialogue act tagging. In the more constrained tasks, co-occurrence vectors are competitive, although choice of compositional method is important; on the largerscale tasks, they are outperformed by neural word embeddings, which show robust, stable performance across the tasks. 1 Introduction Neural word embeddings (Bengio et al., 2006; Collobert and Weston, 2008; Mikolov et al., 2013a) have received much attention in the distributional semantics community, and have shown state-of-the-art performance in many natural language processing tasks. While they have been compared with co-occurrence based models in simple similarity tasks at the word level (Levy et al., 2014; Baroni et al., 2014), we are aware of only one work that attempts a comparison of the two approaches in compositional settings (Blacoe and Lapata, 2012), and this is limited to additive and multiplicative composition, compared against composition via a neural autoencoder. The purpose of this paper is to provid</context>
<context position="3526" citStr="Mikolov et al. (2013" startWordPosition="510" endWordPosition="513">es (Mitchell and Lapata, 2008), in a variety of different tasks. Specifically, we use the verb disambiguation task of Grefenstette and Sadrzadeh (2011a) and the transitive sentence similarity task of Kartsaklis and Sadrzadeh (2014) as small-scale focused experiments on pre-defined sentence structures. Additionally, we evaluate our vector spaces on paraphrase detection (using the Microsoft Research Paraphrase Corpus of Dolan et al. (2005)) and dialogue act tagging using the Switchboard Corpus (see e.g. (Stolcke et al., 2000)). In all of the above tasks, we compare the neural word embeddings of Mikolov et al. (2013a) with two vector spaces both based on co-occurrence counts and produced by standard distributional techniques, as described in detail below. The general picture we get from the results is that in almost all cases the neural vectors are more effective than the traditional approaches. We proceed as follows: Section 2 provides a concise introduction to distributional word representations in natural language processing. Section 708 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 708–719, October 25-29, 2014, Doha, Qatar. c�2014 Association fo</context>
<context position="8818" citStr="Mikolov et al., 2013" startWordPosition="1350" endWordPosition="1353">on)—see (Turney et al., 2010) and below for discussion, and (Kiela and Clark, 2014) for a detailed comparison. philosophy book school Mary 0 10 22 John 4 60 59 girl 0 19 93 boy 0 12 164 idea 10 47 39 Table 1: Word co-occurrence frequencies extracted from the BNC (Leech et al., 1994). 709 Neural word embeddings Deep learning techniques exploit the distributional hypothesis differently. Instead of relying on observed cooccurrence frequencies, a neural language model is trained to maximise some objective function related to e.g. the probability of observing the surrounding words in some context (Mikolov et al., 2013b): log p(wt+j|wt) (1) Optimizing the above function, for example, produces vectors which maximise the conditional probability of observing words in a context around the target word wt, where c is the size of the training window, and w1w2i · · · wT a sequence of words forming a training instance. Therefore, the resulting vectors will capture the distributional intuition and can express degrees of lexical similarity. This method has an obvious advantage compared to co-occurrence method: since now the context is predicted, the model in principle can be much more robust in data sparsity problems,</context>
<context position="10408" citStr="Mikolov et al. (2013" startWordPosition="1615" endWordPosition="1618">the singular:plural relation (apple:apples, car:cars) using vector subtraction: → apple − apples ≈ −→ → car − −−→ cars Perhaps even more importantly, semantic relationships are preserved in a very intuitive way: −−→ king −−−→ man ≈ −−−→ queen − −−−−→ woman allowing the formation of analogy queries similar −−→ man + −−−−→ to king − −−→ woman = ?, obtaining −−−→ queen as the result.1 Both neural and co-occurrence-based approaches have advantages over classical formal approaches in their ability to capture lexical semantics and degrees of similarity; their success at 1Levy et al. (2014) improved Mikolov et al. (2013c)’s method of retrieving relational similarities by changing the underlying objective function. extending this to the sentence level and to more complex semantic phenomena, though, depends on their applicability within compositional models, which is the subject of the next section. 3 Compositional models Compositional distributional models represent meaning of a sequence of words by a vector, obtained by combining meaning vectors of the words within the sequence using some vector composition operation. In a general classification of these models, one can distinguish between three broad cases:</context>
<context position="20759" citStr="Mikolov et al. (2013" startWordPosition="3382" endWordPosition="3385">n (i.e. point-wise mutual information multiplied by raw counts). In a further step, the vector space was normalized and projected onto a 300-dimensional space using singular value decomposition (SVD). In general, dimensionality reduction produces more compact word representations that are robust against potential noise in the corpus (Landauer and Dumais, 1997; Sch¨utze, 1997). SVD has been shown to perform well on a variety of tasks similar to ours (Baroni and Zamparelli, 2010; Kartsaklis and Sadrzadeh, 2014). Neural word embeddings (NWE) For our neural setting, we used the skip-gram model of Mikolov et al. (2013b) trained with negative sampling. The specific implementation that was tested in our experiments was a 300-dimensional vector space learned from the Google News corpus and provided by the word2vec4 toolkit. Furthermore, the gensim library (ˇReh˚uˇrek and Sojka, 2010) was used for accessing the vectors. On the contrary with the previously described co3http://wacky.sslmit.unibo.it/ 4https://code.google.com/p/word2vec/ � p(c|t) p(c) 712 occurrence vector spaces, this version is not lemmatized. The negative sampling method improves the objective function of Equation 1 by introducing negative exam</context>
<context position="22334" citStr="Mikolov et al., 2013" startWordPosition="3654" endWordPosition="3657">ters in a way that maximizes the probability of all observations coming from the training data. Assume now that D&apos; is a set of randomly selected incorrect (c&apos;, t&apos;) pairs that do not occur in D, then Equation 2 above can be recasted in the following way: ri p(D = 1|c, t) � p(D = 0|c&apos;, t&apos;) (c,t)ED (c&apos;,t&apos;)ED&apos; (3) In other words, the model tries to distinguish a target word t from random draws that come from a noise distribution. In the implementation we used for our experiments, c is always selected from a 5-word window around t. More details about the negative sampling approach can be found in (Mikolov et al., 2013b); the note of Goldberg and Levy (2014) also provides an intuitive explanation of the underlying setting. 5 Experiments Our experiments explore the use of the vector spaces above, together with the compositional operators described in Section 3, in a range of tasks all of which require semantic composition: verb sense disambiguation; sentence similarity; paraphrasing; and dialogue act tagging. 5.1 Disambiguation We use the transitive verb disambiguation dataset described in Grefenstette and Sadrzadeh (2011a)5. This dataset consists of ambiguous transitive verbs together with their arguments, </context>
</contexts>
<marker>Mikolov, Yih, Zweig, 2013</marker>
<rawString>Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. 2013c. Linguistic regularities in continuous space word representations. In Proceedings of NAACLHLT, pages 746–751.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitrijs Milajevs</author>
<author>Matthew Purver</author>
</authors>
<title>Investigating the contribution of distributional semantic information for dialogue act classification.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2nd Workshop on Continuous Vector Space Models and their Compositionality (CVSC),</booktitle>
<pages>40--47</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Gothenburg, Sweden,</location>
<contexts>
<context position="30978" citStr="Milajevs and Purver, 2014" startWordPosition="5010" endWordPosition="5013">itive model across all vector spaces. The KS14 vector space shows the steadiest performance, with a drop in accuracy of only 0.04 and no drop in F-score, while for the GS11 and NWE spaces both accuracy and F-score experienced drops by more than 0.20. 5.4 Dialogue act tagging As our last experiment, we evaluate the word spaces on a dialogue act tagging task (Stolcke et al., 2000) over the Switchboard corpus (Godfrey et al., 1992). Switchboard is a collection of approximately 2500 dialogs over a telephone line by 500 speakers from the U.S. on predefined topics.8 The experiment pipeline follows (Milajevs and Purver, 2014). The input utterances are preprocessed so that the parts of interrupted utterances are concatenated (Webb et al., 2005). Disfluency markers and commas are removed from the utterance raw texts. For GS11 and KS14 the utterance tokens are POS-tagged and lemmatized; for NWE, we test the vectors in both a lemmatized and an un-lemmatized version of the corpus.9 We split the training and testing utterances as suggested by Stolcke et al. (2000). Utterance vectors are then obtained as in the previous experiments; they are reduced to 50 dimensions using SVD and a knearest-neighbour classifier is traine</context>
<context position="32350" citStr="Milajevs and Purver, 2014" startWordPosition="5225" endWordPosition="5228">tion F = 2(precision ∗ recall)/(precision + recall). 8The dataset and a Python interface to it are available at http://compprag.christopherpotts.net/ swda.html 9We use WordNetLemmatizer of the NLTK library (Bird, 2006). sification decision). The results are shown in the second part of Table 5. Un-lemmatized NWE addition gave the best accuracy (0.63) and F-score (0.60) (averaged over tag classes), i.e. similar results to (Milajevs and Purver, 2014)—although note that the dimensionality of our NWE vectors is 10 times lower than theirs. Multiplicative NWE outperformed the corresponding model in (Milajevs and Purver, 2014). In general, addition consistently outperforms multiplication for all the models. Lemmatization dramatically lowers tagging accuracy: the lemmatized GS11, KS14 and NWE models perform much worse than un-lemmatized NWE, suggesting that morphological features are important for this task. 6 Discussion Previous comparisons of co-occurrence-based and neural word vector representations vary widely in their conclusions. While Baroni et al. (2014) conclude that “context-predicting models obtain a thorough and resounding victory against their count-based counterparts”, this seems to contradict, at leas</context>
<context position="34217" citStr="Milajevs and Purver, 2014" startWordPosition="5501" endWordPosition="5504">d alternatives across the board; and which approach lends itself better to composition using general mathematical operators. To partially an715 swer this question, we can compare model behaviour against the baselines in isolation. For the disambiguation and sentence similarity tasks the baseline is the similarity between verbs only, ignoring the context—see above. For the paraphrase task, we take the global vector-based similarity reported in (Mihalcea et al., 2006): 0.65 accuracy and 0.75 F-score. For the dialogue act tagging task the baseline is the accuracy of the bag-of-unigrams model in (Milajevs and Purver, 2014): 0.60. Sections 5.1 and 5.2 show that although the best choice of vector representation might vary, for small-scale tasks all methods give fairly competitive results. The choice of compositional operator seems to be more important and more taskspecific: while a tensor-based operation (Frobenius copy-object) performs best for verb disambiguation, the best result for sentence similarity is achieved by a simple additive model, with all other compositional methods behaving worse than the verb-only baseline in the KS14 case. GS11 and NWE, on the other hand, outperform their baselines with a number</context>
</contexts>
<marker>Milajevs, Purver, 2014</marker>
<rawString>Dmitrijs Milajevs and Matthew Purver. 2014. Investigating the contribution of distributional semantic information for dialogue act classification. In Proceedings of the 2nd Workshop on Continuous Vector Space Models and their Compositionality (CVSC), pages 40–47, Gothenburg, Sweden, April. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Mitchell</author>
<author>Mirella Lapata</author>
</authors>
<title>Vector-based models of semantic composition.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>236--244</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2936" citStr="Mitchell and Lapata, 2008" startWordPosition="415" endWordPosition="419">nvolving general mathematical composition operators, rather than in the more task- or domain-specific deep-learning compositional settings they have generally been used with so far (for example, by Socher et al. (2012), Kalchbrenner and Blunsom (2013) and many others). In particular, this is the first large-scale study to date that applies neural word representations in tensor-based compositional distributional models of meaning similar to those formalized by Coecke et al. (2010). We test a range of implementations based on this framework, together with additive and multiplicative approaches (Mitchell and Lapata, 2008), in a variety of different tasks. Specifically, we use the verb disambiguation task of Grefenstette and Sadrzadeh (2011a) and the transitive sentence similarity task of Kartsaklis and Sadrzadeh (2014) as small-scale focused experiments on pre-defined sentence structures. Additionally, we evaluate our vector spaces on paraphrase detection (using the Microsoft Research Paraphrase Corpus of Dolan et al. (2005)) and dialogue act tagging using the Switchboard Corpus (see e.g. (Stolcke et al., 2000)). In all of the above tasks, we compare the neural word embeddings of Mikolov et al. (2013a) with tw</context>
<context position="11310" citStr="Mitchell and Lapata, 2008" startWordPosition="1746" endWordPosition="1749">ection. 3 Compositional models Compositional distributional models represent meaning of a sequence of words by a vector, obtained by combining meaning vectors of the words within the sequence using some vector composition operation. In a general classification of these models, one can distinguish between three broad cases: simplistic models which combine word vectors irrespective of their order or relation to one another, models which exploit linear word order, and models which use grammatical structure. The first approach combines word vectors by vector addition or point-wise multiplication (Mitchell and Lapata, 2008)—as this is independent of word order, it cannot capture the difference between the two sentences “dogs chase cats” and “cats chase dogs”. The second approach has generally been implemented using some form of deep learning, and captures word order, but not by necessarily caring about the grammatical structure of the sentence. Here, one works by recursively building and combining vectors for subsequences of words within the sentence using e.g. autoencoders (Socher et al., 2012) or convolutional filters (Kalchbrenner et al., 2014). We do not consider this approach in this paper. This is because,</context>
<context position="13001" citStr="Mitchell and Lapata, 2008" startWordPosition="2022" endWordPosition="2025">ammatical) approach promises a way to achieve this, and has been instantiated in various ways in the work of Baroni and Zamparelli (2010),Grefenstette and Sadrzadeh (2011a), and Kartsaklis et al. (2012). General framework Formally, we can specify the vector representation of a word sequence w1w2 · · · wn as the vector →−s = w1 −→ *−→ w2 *···*−→wn, where * is a vector operator, such as addition +, point-wise multiplication O, tensor product ⊗, or matrix multiplication ×. � −c&lt;j&lt;c,j�0 1 T T t=1 710 In the simplest compositional models (the first approach described above), * is + or O, e.g. see (Mitchell and Lapata, 2008). Grammar-based compositional models (the third approach) are based on a generalisation of the notion of vectors, known as tensors. Whereas a vector →−v is an element of an atomic vector space V , a tensor z is an element of a tensor space V ⊗ W ⊗ · · · ⊗ Z. The number of tensored spaces is referred to by the order of the space. Using a general duality theorem from multi-linear algebra (Bourbaki, 1989), it follows that tensors are in one-one correspondence with multi-linear maps, that is we have: z ∈ V ⊗W ⊗···⊗Z ∼= fz : V → W → ··· → Z In such a tensor-based formalism, meanings of nouns are ve</context>
<context position="17269" citStr="Mitchell and Lapata (2008)" startWordPosition="2777" endWordPosition="2780">take addition, multiplication, or outer product of these, which are referred to by FrobeniusAdd, Frobenius-Mult, and Frobenius-Outer (Kartsaklis and Sadrzadeh, 2014). 4 Semantic word spaces Co-occurrence-based vector space instantiations have received a lot of attention from the scientific community (refer to (Kiela and Clark, 2014; Polajnar and Clark, 2014) for recent studies). We instantiate two co-occurrence-based vectors spaces with different underlying corpora and weighting schemes. 711 Method Sentence Linear algebraic formula Reference Addition w1w2 · · · wn w1 + �� �� w2 + · · · + wn�� Mitchell and Lapata (2008) Multiplication w1w2 · · · wn w1�� O w2�� O · · · O wn ��Mitchell and Lapata (2008) Table 2: Compositional methods. Obj) Grefenstette and Sadrzadeh (2011a) �� Obj) Grefenstette and Sadrzadeh (2011b) �� Obj O (Verb&apos; x �� O (Verb x Obj)) + (�� Sbj)) Kartsaklis and Sadrzadeh (2014) Sbj Verb Obj (�� Sbj O (Verb x �� Obj)) O (�� Obj O (Verb&apos; x �� Frob. mult. Sbj)) Kartsaklis and Sadrzadeh (2014) Sbj Verb Obj (�� Sbj O (Verb x �� Obj)) ® (�� Obj O (Verb&apos; x �� Frob. outer Sbj)) Kartsaklis and Sadrzadeh (2014) Copy object Sbj Verb Obj Sbj O (Verb x Obj) Kartsaklis et al. (2012) Copy subject Sbj Verb O</context>
<context position="19168" citStr="Mitchell and Lapata (2008)" startWordPosition="3124" endWordPosition="3127">e is lemmatized, that is, it contains only “canonical” forms of words. In order to weight the raw co-occurrence counts, we use positive point-wise mutual information (PPMI). The component value for a target word t and a context word c is given by: �PPMI(t, c) = max 0, log where p(clt) is the probability of word c given t in a symmetric window of length 5 and p(c) is the probability of coverall. Vector spaces based on point-wise mutual information (or variants thereof) have been successfully applied in various distributional and compositional tasks; see e.g. Grefenstette and Sadrzadeh (2011a), Mitchell and Lapata (2008), Levy et al. (2014) for details. PPMI has been shown to achieve state-of-the-art results (Levy et al., 2014) and is suggested by the review of Kiela and Clark (2014). Our use here of the BNC as a corpus and the window length of 5 is based on previous use and better performance of these parameters in a number of compositional experiments (Grefenstette and Sadrzadeh, 2011a; Grefenstette 2http://www.natcorp.ox.ac.uk/ and Sadrzadeh, 2011b; Mitchell and Lapata, 2008; Kartsaklis et al., 2012). KS14 In this variation, we train a vector space from the ukWaC corpus3 (Ferraresi et al., 2008), originall</context>
<context position="23313" citStr="Mitchell and Lapata, 2008" startWordPosition="3799" endWordPosition="3803">ity; paraphrasing; and dialogue act tagging. 5.1 Disambiguation We use the transitive verb disambiguation dataset described in Grefenstette and Sadrzadeh (2011a)5. This dataset consists of ambiguous transitive verbs together with their arguments, landmark verbs that identify one of the verb senses, and human judgements that specify how similar is the disambiguated sense of the verb in the given context to 5This and the sentence similarity dataset are available at http://www.cs.ox.ac.uk/activities/ compdistmeaning/ one of the landmarks. This is similar to the intransitive dataset described in (Mitchell and Lapata, 2008). Consider the sentence “system meets specification”; here, meets is the ambiguous transitive verb, and system and specification are its arguments in this context. Possible landmarks for meet are satisfy and visit; for this sentence, the human judgements show that the disambiguated meaning of the verb is more similar to the landmark satisfy and less similar to visit. The task is to estimate the similarity of the sense of a verb in a context with a given landmark. To get our similarity measures, we compose the verb with its arguments using one of our compositional models; we do the same for the</context>
</contexts>
<marker>Mitchell, Lapata, 2008</marker>
<rawString>Jeff Mitchell and Mirella Lapata. 2008. Vector-based models of semantic composition. In Proceedings of ACL-08: HLT, pages 236–244. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Mitchell</author>
<author>Mirella Lapata</author>
</authors>
<title>Composition in distributional models of semantics.</title>
<date>2010</date>
<journal>Cognitive Science,</journal>
<volume>34</volume>
<issue>8</issue>
<contexts>
<context position="27480" citStr="Mitchell and Lapata, 2010" startWordPosition="4455" endWordPosition="4458">se with tensor-based methods here than in the other vector spaces. However, GS11 and NWE, except copy subject for both of them and Frobenius multiplication for NWE, improved over their verb-only baselines. 5.3 Paraphrasing In this experiment we evaluate our vector spaces on a mainstream paraphrase detection task. 6The textual content of this dataset is the same as that of (Kartsaklis and Sadrzadeh, 2013), the difference is that the dataset of (Kartsaklis and Sadrzadeh, 2014) has updated human judgements whereas the previous dataset used the original annotations of the intransitive dataset of (Mitchell and Lapata, 2010). Method GS11 KS14 NWE Verb only 0.491 0.602 0.561 Addition 0.682 0.732 0.689 Multiplication 0.597 0.321 0.341 Kronecker 0.581 0.408 0.561 Relational 0.558 0.437 0.618 Copy subject 0.370 0.448 0.405 Copy object 0.571 0.306 0.655 Frobenius add. 0.566 0.460 0.585 Frobenius mult. 0.525 0.226 0.387 Frobenius outer 0.560 0.439 0.622 Table 4: Results for sentence similarity. There is no statistically significant difference between KS14 addition and NWE addition (the second best result). Specifically, we get classification results on the Microsoft Research Paraphrase Corpus paraphrase corpus (Dolan e</context>
</contexts>
<marker>Mitchell, Lapata, 2010</marker>
<rawString>Jeff Mitchell and Mirella Lapata. 2010. Composition in distributional models of semantics. Cognitive Science, 34(8):1388–1439.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Montague</author>
</authors>
<title>Universal grammar.</title>
<date>1970</date>
<journal>Theoria,</journal>
<volume>36</volume>
<issue>3</issue>
<contexts>
<context position="5584" citStr="Montague, 1970" startWordPosition="827" endWordPosition="828"> meaning between those constituent parts, but also between the results of their composition, and do this in ways which fit with linguistic structure and generalisations thereof. Formal semantics Formal approaches to the semantics of natural language have long built upon the classical idea of compositionality – that the meaning of a sentence is a function of the meanings of its parts (Frege, 1892). In compositional type-logical approaches, predicateargument structures representing phrases and sentences are built from their constituent parts by β- reduction within the lambda calculus framework (Montague, 1970): for example, given a representation of John as john&apos; and sleeps as Ax.sleep&apos;(x), the meaning of the sentence “John sleeps” can be constructed as Ax.sleep&apos;(x)(john&apos;) = sleep&apos;(john&apos;). Given a suitable pairing between words and semantic representations of them, this method can produce structured sentential representations with broad coverage and good generalisability (see e.g. (Bos, 2008)). The above logical approach is extremely powerful because it can capture complex aspects of meaning such as quantifiers and their interaction (see e.g. (Copestake et al., 2005)), and enables inference using w</context>
</contexts>
<marker>Montague, 1970</marker>
<rawString>Richard Montague. 1970. Universal grammar. Theoria, 36(3):373–398.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tamara Polajnar</author>
<author>Stephen Clark</author>
</authors>
<title>Improving distributional semantic vectors through context selection and normalisation.</title>
<date>2014</date>
<booktitle>In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>230--238</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Gothenburg, Sweden,</location>
<contexts>
<context position="17003" citStr="Polajnar and Clark, 2014" startWordPosition="2731" endWordPosition="2735">o order 3. The expansion is obtained by either copying the dimension of the subject into the space provided by the third tensor, hence referred to by Copy-Sbj, or copying the dimension of the object in that space, hence referred to by Copy-Obj; furthermore, we can take addition, multiplication, or outer product of these, which are referred to by FrobeniusAdd, Frobenius-Mult, and Frobenius-Outer (Kartsaklis and Sadrzadeh, 2014). 4 Semantic word spaces Co-occurrence-based vector space instantiations have received a lot of attention from the scientific community (refer to (Kiela and Clark, 2014; Polajnar and Clark, 2014) for recent studies). We instantiate two co-occurrence-based vectors spaces with different underlying corpora and weighting schemes. 711 Method Sentence Linear algebraic formula Reference Addition w1w2 · · · wn w1 + �� �� w2 + · · · + wn�� Mitchell and Lapata (2008) Multiplication w1w2 · · · wn w1�� O w2�� O · · · O wn ��Mitchell and Lapata (2008) Table 2: Compositional methods. Obj) Grefenstette and Sadrzadeh (2011a) �� Obj) Grefenstette and Sadrzadeh (2011b) �� Obj O (Verb&apos; x �� O (Verb x Obj)) + (�� Sbj)) Kartsaklis and Sadrzadeh (2014) Sbj Verb Obj (�� Sbj O (Verb x �� Obj)) O (�� Obj O (V</context>
</contexts>
<marker>Polajnar, Clark, 2014</marker>
<rawString>Tamara Polajnar and Stephen Clark. 2014. Improving distributional semantic vectors through context selection and normalisation. In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 230– 238, Gothenburg, Sweden, April. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radim ˇReh˚uˇrek</author>
<author>Petr Sojka</author>
</authors>
<title>Software Framework for Topic Modelling with Large Corpora.</title>
<date>2010</date>
<booktitle>In Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks,</booktitle>
<pages>45--50</pages>
<location>Valletta, Malta,</location>
<note>ELRA. http://is. muni.cz/publication/884893/en.</note>
<marker>ˇReh˚uˇrek, Sojka, 2010</marker>
<rawString>Radim ˇReh˚uˇrek and Petr Sojka. 2010. Software Framework for Topic Modelling with Large Corpora. In Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks, pages 45– 50, Valletta, Malta, May. ELRA. http://is. muni.cz/publication/884893/en.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Ambiguity resolution in natural language learning. csli.</title>
<date>1997</date>
<pages>4--12</pages>
<location>Stanford, CA,</location>
<marker>Sch¨utze, 1997</marker>
<rawString>Hinrich Sch¨utze. 1997. Ambiguity resolution in natural language learning. csli. Stanford, CA, 4:12–36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Brody Huval</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Semantic compositionality through recursive matrix-vector spaces.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>1201--1211</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2528" citStr="Socher et al. (2012)" startWordPosition="356" endWordPosition="359">ture regarding the potential of neu2University of Oxford Department of Computer Science Parks Road, Oxford, UK dimitri.kartsaklis@cs.ox.ac.uk ral word embeddings in compositional tasks, and meaningfully compare them with the traditional distributional approach based on co-occurrence counts. We are especially interested in investigating the performance of neural word vectors in compositional models involving general mathematical composition operators, rather than in the more task- or domain-specific deep-learning compositional settings they have generally been used with so far (for example, by Socher et al. (2012), Kalchbrenner and Blunsom (2013) and many others). In particular, this is the first large-scale study to date that applies neural word representations in tensor-based compositional distributional models of meaning similar to those formalized by Coecke et al. (2010). We test a range of implementations based on this framework, together with additive and multiplicative approaches (Mitchell and Lapata, 2008), in a variety of different tasks. Specifically, we use the verb disambiguation task of Grefenstette and Sadrzadeh (2011a) and the transitive sentence similarity task of Kartsaklis and Sadrzad</context>
<context position="11791" citStr="Socher et al., 2012" startWordPosition="1823" endWordPosition="1826">grammatical structure. The first approach combines word vectors by vector addition or point-wise multiplication (Mitchell and Lapata, 2008)—as this is independent of word order, it cannot capture the difference between the two sentences “dogs chase cats” and “cats chase dogs”. The second approach has generally been implemented using some form of deep learning, and captures word order, but not by necessarily caring about the grammatical structure of the sentence. Here, one works by recursively building and combining vectors for subsequences of words within the sentence using e.g. autoencoders (Socher et al., 2012) or convolutional filters (Kalchbrenner et al., 2014). We do not consider this approach in this paper. This is because, as mentioned in the introduction, their vectors and composition operators are task-specific. These are trained directly to achieve specific objectives in certain pre-determined tasks. We are interested in vector and composition operators that work for any compositional task, and which can be combined with results in linguistics and formal semantics to provide generalisable models that can canonically extend to complex semantic phenomena. The third (i.e. the grammatical) appro</context>
</contexts>
<marker>Socher, Huval, Manning, Ng, 2012</marker>
<rawString>Richard Socher, Brody Huval, Christopher D Manning, and Andrew Y Ng. 2012. Semantic compositionality through recursive matrix-vector spaces. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1201–1211. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
<author>Klaus Ries</author>
<author>Noah Coccaro</author>
<author>Elizabeth Shriberg</author>
<author>Rebecca Bates</author>
<author>Daniel Jurafsky</author>
<author>Paul Taylor</author>
<author>Carol Van Ess-Dykema</author>
<author>Rachel Martin</author>
<author>Marie Meteer</author>
</authors>
<title>Dialogue act modeling for automatic tagging and recognition of conversational speech.</title>
<date>2000</date>
<journal>Computational Linguistics,</journal>
<volume>26</volume>
<issue>3</issue>
<marker>Stolcke, Ries, Coccaro, Shriberg, Bates, Jurafsky, Taylor, Van Ess-Dykema, Martin, Meteer, 2000</marker>
<rawString>Andreas Stolcke, Klaus Ries, Noah Coccaro, Elizabeth Shriberg, Rebecca Bates, Daniel Jurafsky, Paul Taylor, Carol Van Ess-Dykema, Rachel Martin, and Marie Meteer. 2000. Dialogue act modeling for automatic tagging and recognition of conversational speech. Computational Linguistics, 26(3):339–373.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
<author>Patrick Pantel</author>
</authors>
<title>From frequency to meaning: Vector space models of semantics.</title>
<date>2010</date>
<journal>Journal of artificial intelligence research,</journal>
<volume>37</volume>
<issue>1</issue>
<marker>Turney, Pantel, 2010</marker>
<rawString>Peter D Turney, Patrick Pantel, et al. 2010. From frequency to meaning: Vector space models of semantics. Journal of artificial intelligence research, 37(1):141–188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>Similarity of semantic relations.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>3</issue>
<contexts>
<context position="9747" citStr="Turney, 2006" startWordPosition="1505" endWordPosition="1506">ulting vectors will capture the distributional intuition and can express degrees of lexical similarity. This method has an obvious advantage compared to co-occurrence method: since now the context is predicted, the model in principle can be much more robust in data sparsity problems, which is always an important issue for cooccurrence word spaces. Additionally, neural vectors have also proven successful in other tasks (Mikolov et al., 2013c), since they seem to encode not only attributional similarity (the degree to which similar words are close to each other), but also relational similarity (Turney, 2006). For example, it is possible to extract the singular:plural relation (apple:apples, car:cars) using vector subtraction: → apple − apples ≈ −→ → car − −−→ cars Perhaps even more importantly, semantic relationships are preserved in a very intuitive way: −−→ king −−−→ man ≈ −−−→ queen − −−−−→ woman allowing the formation of analogy queries similar −−→ man + −−−−→ to king − −−→ woman = ?, obtaining −−−→ queen as the result.1 Both neural and co-occurrence-based approaches have advantages over classical formal approaches in their ability to capture lexical semantics and degrees of similarity; their</context>
</contexts>
<marker>Turney, 2006</marker>
<rawString>Peter D Turney. 2006. Similarity of semantic relations. Computational Linguistics, 32(3):379–416.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nick Webb</author>
<author>Mark Hepple</author>
<author>Yorick Wilks</author>
</authors>
<title>Dialogue act classification based on intra-utterance features.</title>
<date>2005</date>
<booktitle>In Proceedings of the AAAI Workshop on Spoken Language Understanding. Citeseer.</booktitle>
<contexts>
<context position="31098" citStr="Webb et al., 2005" startWordPosition="5029" endWordPosition="5032">4 and no drop in F-score, while for the GS11 and NWE spaces both accuracy and F-score experienced drops by more than 0.20. 5.4 Dialogue act tagging As our last experiment, we evaluate the word spaces on a dialogue act tagging task (Stolcke et al., 2000) over the Switchboard corpus (Godfrey et al., 1992). Switchboard is a collection of approximately 2500 dialogs over a telephone line by 500 speakers from the U.S. on predefined topics.8 The experiment pipeline follows (Milajevs and Purver, 2014). The input utterances are preprocessed so that the parts of interrupted utterances are concatenated (Webb et al., 2005). Disfluency markers and commas are removed from the utterance raw texts. For GS11 and KS14 the utterance tokens are POS-tagged and lemmatized; for NWE, we test the vectors in both a lemmatized and an un-lemmatized version of the corpus.9 We split the training and testing utterances as suggested by Stolcke et al. (2000). Utterance vectors are then obtained as in the previous experiments; they are reduced to 50 dimensions using SVD and a knearest-neighbour classifier is trained on these reduced utterance vectors (the 5 closest neighbours by Euclidean distance are retrieved to make a clas7F-scor</context>
</contexts>
<marker>Webb, Hepple, Wilks, 2005</marker>
<rawString>Nick Webb, Mark Hepple, and Yorick Wilks. 2005. Dialogue act classification based on intra-utterance features. In Proceedings of the AAAI Workshop on Spoken Language Understanding. Citeseer.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.900758">
PCFG Induction for Unsupervised Parsing and Language Modelling
</title>
<author confidence="0.772633">
James Scicluna
</author>
<affiliation confidence="0.629227">
Universit´e de Nantes,
</affiliation>
<note confidence="0.791159">
CNRS, LINA, UMR6241,
F-44000, France
james.scicluna@univ-nantes.fr
Colin de la Higuera
Universit´e de Nantes,
CNRS, LINA, UMR6241,
F-44000, France
</note>
<email confidence="0.743382">
cdlh@univ-nantes.fr
</email>
<sectionHeader confidence="0.990098" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999862869565217">
The task of unsupervised induction of
probabilistic context-free grammars
(PCFGs) has attracted a lot of attention
in the field of computational linguistics.
Although it is a difficult task, work in this
area is still very much in demand since
it can contribute to the advancement of
language parsing and modelling. In this
work, we describe a new algorithm for
PCFG induction based on a principled
approach and capable of inducing accurate
yet compact artificial natural language
grammars and typical context-free gram-
mars. Moreover, this algorithm can work
on large grammars and datasets and infers
correctly even from small samples. Our
analysis shows that the type of grammars
induced by our algorithm are, in theory,
capable of modelling natural language.
One of our experiments shows that our
algorithm can potentially outperform the
state-of-the-art in unsupervised parsing on
the WSJ10 corpus.
</bodyText>
<sectionHeader confidence="0.998882" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999975904761905">
The task of unsupervised induction of PCFGs has
attracted a lot of attention in the field of compu-
tational linguistics. This task can take the form
of either parameter search or structure learning.
In parameter search, a CFG is fixed and the fo-
cus is on assigning probabilities to this grammar
using Bayesian methods (Johnson et al., 2007) or
maximum likelihood estimation (Lari and Young,
1990). In structure learning, the focus is on build-
ing the right grammar rules from scratch. We take
the latter approach.
Unsupervised structure learning of (P)CFGs is
a notoriously difficult task (de la Higuera, 2010;
Clark and Lappin, 2010), with theoretical results
showing that in general it is either impossible
to achieve (Gold, 1967; de la Higuera, 1997)
or requires impractical resources (Horning, 1969;
Yang, 2012). At the same time, it is well known
that context-free structures are needed for better
language parsing and modelling, since less expres-
sive models (such as HMMs) are not good enough
(Manning and Sch¨utze, 2001; Jurafsky and Mar-
tin, 2008). Moreover, the trend is towards unsu-
pervised (rather than supervised) learning meth-
ods due to the lack in most languages of annotated
data and the applicability in wider domains (Merlo
et al., 2010). Thus, despite its difficulty, unsuper-
vised PCFG grammar induction (or induction of
other similarly expressive models) is still an im-
portant task in computational linguistics.
In this paper, we describe a new algorithm for
PCFG induction based on a principled approach
and capable of inducing accurate yet compact
grammars. Moreover, this algorithm can work on
large grammars and datasets and infers correctly
even from small samples. We show that our algo-
rithm is capable of achieving competitive results
in both unsupervised parsing and language mod-
elling of typical context-free languages and arti-
ficial natural language grammars. We also show
that the type of grammars we propose to learn are,
in theory, capable of modelling natural language.
</bodyText>
<sectionHeader confidence="0.998453" genericHeader="introduction">
2 Preliminaries
</sectionHeader>
<subsectionHeader confidence="0.997506">
2.1 Grammars and Languages
</subsectionHeader>
<bodyText confidence="0.989183555555555">
A context-free grammar (CFG) is a 4-tuple
(N, E, P, I), where N is the set of non-terminals,
E the set of terminals, P the set of production rules
and I a set of starting non-terminals (i.e. multi-
ple starting non-terminals are possible). The lan-
guage generated from a particular non-terminal A
is L(A) = {wjA =*=&gt;. w} and the language gen-
erated by a grammar G is L(G) = US∈I L(S).
A CFG is in Chomsky Normal Form (CNF) if ev-
</bodyText>
<page confidence="0.862489">
1353
</page>
<note confidence="0.928164">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1353–1362,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.961224722222222">
ery production rule is of the form N —* NN or
N —* Σ.
A probabilistic context-free grammar (PCFG)
is a CFG with a probability value assigned to every
rule and every starting non-terminal. The prob-
ability of a leftmost derivation from a PCFG is
the product of the starting non-terminal probabil-
ity and the production probabilities used in the
derivation. The probability of a string generated
by a PCFG is the sum of all its leftmost deriva-
tions’ probabilities. The stochastic language gen-
erated from a PCFG G is (L(G), φG), where φG
is the distribution over Σ* defined by the probabil-
ities assigned to the strings by G. For a PCFG to
be consistent, the probabilities of the strings in its
stochastic language must add up to 1 (Wetherell,
1980). Any PCFG mentioned from now onwards
is assumed to be consistent.
</bodyText>
<subsectionHeader confidence="0.994483">
2.2 Congruence Relations
</subsectionHeader>
<bodyText confidence="0.989587785714286">
A congruence relation — on Σ* is any equivalence
relation on Σ* that respects the following condi-
tion: if u — v and x — y then ux — vy. The con-
gruence classes of a congruence relation are sim-
ply its equivalence classes. The congruence class
of w E Σ* w.r.t. a congruence relation — is de-
noted by [w]—. The set of contexts of a substring w
with respect to a language L, denoted Con(w, L),
is {(l, r) E Σ* x Σ*  |lwr E L}. Two strings u
and v are syntactically congruent with respect to
L, written u =L v, if Con(u, L) = Con(v, L).
This is a congruence relation on Σ*. The con-
text distribution of a substring w w.r.t. a stochastic
language (L, φ), denoted C(L,φ)
</bodyText>
<equation confidence="0.8638616">
w , is a distribution
whose support is all the possible contexts over al-
phabet Σ (i.e. Σ* x Σ*) and is defined as follows:
C(L,φ) w(l, r) = φ(lwr)
�l�,r��Σ∗ φ(l&apos;wr&apos;)
</equation>
<bodyText confidence="0.850908833333333">
Two strings u and v are stochastically congru-
ent with respect to (L, φ), written u —=(L,φ) v, if
C(L,φ)
u is equal to C(L,φ)
v . This is a congruence
relation on Σ*.
</bodyText>
<subsectionHeader confidence="0.998782">
2.3 Congruential Grammars
</subsectionHeader>
<bodyText confidence="0.983422655172414">
Clark (2010a) defines Congruential CFGs (C-
CFGs) as being all the CFGs G which, for any
non-terminal A, if u E L(A) then L(A) C_
[u]_L(G) (where [u]_L(G) is the syntactic congru-
ence class of u w.r.t. the language of G). This
class of grammars was defined with learnability
in mind. Since these grammars have a direct
relationship between congruence classes and the
non-terminals, their learnability is reduced to that
of finding the correct congruence classes (Clark,
2010a).
This class of grammars is closely related
to the class of NTS-grammars (Boasson and
S´enizergues, 1985). Any C-CFG is an NTS-
grammar but not vice-versa. However, it is not
known whether languages generated by C-CFGs
are all NTS-languages (Clark, 2010a). Note that
NTS-languages are a subclass of deterministic
context-free languages and contain the regular
languages, the substitutable (Clark and Eyraud,
2007) and k-l-substitutable context-free languages
(Yoshinaka, 2008), the very simple languages and
other CFLs such as the Dyck language (Boasson
and S´enizergues, 1985).
We define a slightly more restrictive class of
grammars, which we shall call Strongly Congru-
ential CFGs (SC-CFGs). A CFG G is a SC-
CFG if, for any non-terminal A, if u E L(A)
then L(A) = [u]_L(G). The probabilistic equiv-
alent of this is the class of Strongly Congruential
PCFGs (SC-PCFGs), defined as all the PCFGs G
which, for any non-terminal A, if u E L(A) then
L(A) = [u]—=(L(G),φ). In other words, the non-
terminals (i.e. syntactic categories in natural lan-
guage) of these grammars directly correspond to
classes of substitutable strings (i.e. substitutable
words and phrases in NL). One may ask whether
this is too strict a restriction for natural language
grammars. We argue that it is not, for the follow-
ing reasons.
First of all, this restriction complies with the ap-
proach taken by American structural linguists for
the identification of syntactic categories, as shown
by Rauh (2010): ”[Zellig and Fries] identified
syntactic categories as distribution classes, em-
ploying substitution tests and excluding semantic
properties of the items analysed. Both describe
syntactic categories exclusively on the basis of
their syntactic environments and independently of
any inherent properties of the members of these
categories”.
Secondly, we know that such grammars are ca-
pable of describing languages generated by gram-
mars that contain typical natural language gram-
matical structures (see Section 4.1; artificial natu-
ral language grammars NL1-NL7, taken from var-
ious sources, generate languages which can be de-
scribed by SC-PCFGs).
</bodyText>
<page confidence="0.996949">
1354
</page>
<sectionHeader confidence="0.987585" genericHeader="method">
3 Algorithm
</sectionHeader>
<bodyText confidence="0.7252885">
COMINO (our algorithm) induces SC-PCFGs
from a positive sample S. The steps involved are:
</bodyText>
<listItem confidence="0.999750666666667">
1. Inducing the stochastically congruent classes
of all the substrings of S
2. Selecting which of the induced classes are
non-terminals and subsequently building a
CFG.
3. Assigning probabilities to the induced CFG.
</listItem>
<bodyText confidence="0.999930769230769">
The approach we take is very different from tra-
ditional grammar induction approaches, in which
grouping of substitutable substrings is done incre-
mentally as the same groups are chosen to rep-
resent non-terminals. We separate these two task
so that learning takes place in the grouping phase
whilst selection of non-terminals is done indepen-
dently by solving a combinatorial problem.
For the last step, the standard EM-algorithm for
PCFGs (Lari and Young, 1990) is used. In Sec-
tions 3.1 and 3.2, the first and second steps of the
algorithm are described in detail. We analyse our
algorithm in Section 3.3.
</bodyText>
<subsectionHeader confidence="0.999264">
3.1 Inducing the Congruence Classes
</subsectionHeader>
<bodyText confidence="0.9975365">
We describe in Algorithm 1 how the congruence
classes are induced.
</bodyText>
<listItem confidence="0.334452625">
Algorithm 1: Learn Congruence Classes
Input: A multiset S; parameters: n, d, i;
distance function dist on local
contexts of size k
Output: The congruence classes CC over the
substrings of S
1 Subs ← Set of all substrings of S ;
2 CC ← {{w}  |w E Subs} ;
</listItem>
<sectionHeader confidence="0.481113" genericHeader="method">
3 while True do
</sectionHeader>
<construct confidence="0.658072285714286">
Pairs ← {(x, y)  |x,y ECC, x =� y,
|S|x &gt; n , |S|y &gt; n} ;
if |Pairs |= 0 then exitloop ;
Order Pairs based on distk ;
(x, y) ← Pairs[0] ;
init = {[w]CC  |w E S} ;
if distk(x, y) &gt; d and |init |≤ i then
</construct>
<equation confidence="0.88017925">
exitloop ;
CC ← Merge(x, y, CC) ;
11 end
12 return CC ;
</equation>
<bodyText confidence="0.999704428571429">
At the beginning, each substring (or phrase for
natural language) in the sample is assigned its own
congruence class (line 2). Then, pairs of frequent
congruence classes are merged together depend-
ing on the distance between their empirical con-
text distribution, which is calculated on local con-
texts. The following points explain each keyword:
</bodyText>
<listItem confidence="0.923858853658537">
• The empirical context distribution of a sub-
string w is simply a probability distribution
over all the contexts of w, where the prob-
ability for a context (l, r) is the number of
occurrences of lwr in the sample divided by
the number of occurrences of w. This is ex-
tended to congruence classes by treating each
substring in the class as one substring (i.e. the
sum of occurrences of lwir, for all wi in the
class, divided by the sum of occurrences of
all wi).
• Due to the problem of sparsity with contexts
(in any reasonably sized corpus of natural
language, very few phrases will have more
than one occurrence of the same context),
only local contexts are considered. The lo-
cal contexts of w are the pairs of first k sym-
bols (or words for natural language) preced-
ing and following w. The lower k is, the less
sparsity is a problem, but the empirical con-
text distribution is less accurate. For natural
language corpora, k is normally set to 1 or 2.
• A frequent congruence class is one whose
substring occurrences in the sample add up
to more than a pre-defined threshold n. In-
frequent congruence classes are ignored due
to their unreliable empirical context dis-
tribution. However, as more merges are
made, more substrings are added to infre-
quent classes, thus increasing their frequency
and eventually they might be considered as
frequent classes.
• A distance function dist between samples
of distributions over contexts is needed by
the algorithm to decide which is the closest
pair of congruence classes, so that they are
merged together. We used L1-Distance and
Pearson’s chi-squared test for experiments in
Sections 4.1 and 4.2 respectively.
• After each merge, other merges are logically
deduced so as to ensure that the relation re-
</listItem>
<figure confidence="0.971810857142857">
4
5
6
7
8
9
10
</figure>
<page confidence="0.97365">
1355
</page>
<bodyText confidence="0.999953">
mains a congruence1. In practice, the vast
majority of the merges undertaken are logi-
cally deduced ones. This clearly relieves the
algorithm from taking unnecessary decisions
(thus reducing the chance of erroneous de-
cisions). On the downside, one bad merge
can have a disastrous ripple effect. Thus, to
minimize as much as possible the chance of
this happening, every merge undertaken is the
best possible one at that point in time (w.r.t.
the distance function used). The same idea is
used in DFA learning (Lang et al., 1998).
This process is repeated until either 1) no pairs
of frequent congruence classes are left to merge
(line 5) or 2) the smallest distance between the
candidate pairs is bigger or equal to a pre-defined
threshold d and the number of congruence classes
containing strings from the sample is smaller or
equal to a pre-defined threshold i (line 9).
The first condition of point 2 ensures that con-
gruence classes which are sufficiently close to
each other are merged together. The second con-
dition of point 2 ensures that the hypothesized
congruence classes are generalized enough (i.e. to
avoid undergeneralization). For natural language
examples, one would expect that a considerable
number of sentences are grouped into the same
class because of their similar structure. Obviously,
one can make use of only one of these conditions
by assigning the other a parameter value which
makes it trivially true from the outset (0 for d and
|Subs |for i).
</bodyText>
<subsectionHeader confidence="0.999865">
3.2 Building the Context-Free Grammar
</subsectionHeader>
<bodyText confidence="0.997866946428572">
Deciding which substrings are constituents (in our
case, this translates into choosing which congru-
ence classes correspond to non-terminals) is a
problematic issue and is considered a harder task
than the previous step (Klein, 2004). A path fol-
lowed by a number of authors consists in using an
Ockham’s razor or Minimal Description Length
principle approach (Stolcke, 1994; Clark, 2001;
Petasis et al., 2004). This generally leads to choos-
ing as best hypothesis the one which best com-
presses the data. Applying this principle in our
case would mean that the non-terminals should be
1for example, if a congruence class contains the phrases
”the big” and ”that small”, and another class contains ”dog
barked” and ”cat meowed”, it can be logically deduced that
the phrases ”the big dog barked”,”the big cat meowed”, ”that
small dog barked” and ”that small cat meowed” should be in
the same class.
assigned in such a way that the grammar built is
the smallest possible one (in terms of the number
of non-terminals and/or production rules) consis-
tent with the congruence classes. To our knowl-
edge, only local greedy search is used by systems
in the literature which try to follow this approach.
We propose a new method for tackling this
problem. We show that all the possible SC-CFGs
in CNF consistent with the congruence classes di-
rectly correspond to all the solutions of a boolean
formula built upon the congruence classes, where
the variables of this formula correspond to non-
terminals (and, with some minor adjustments, pro-
duction rules as well). Thus, finding the smallest
possible grammar directly translates into finding a
solution which has the smallest possible amount
of true variables. Finding a minimal solution for
this type of formula is a known NP-Hard problem
(Khanna et al., 2000). However, sophisticated lin-
ear programming solvers (Berkelaar et al., 2008)
can take care of this problem. For small examples
(e.g. all the examples in Table 1), these solvers
are able to find an exact solution in a few sec-
onds. Moreover, these solvers are capable of find-
ing good approximate solutions to larger formulas
containing a few million variables.
The formula contains one variable per congru-
ence class. All variables corresponding to congru-
ence classes containing strings from the sample
are assigned the value True (since there must be a
starting non-terminal that generates these strings).
All variables corresponding to congruence classes
containing symbols from E are assigned the value
True (since for every a E E, there must be a rule
A —* a). Finally, and most importantly, for every
congruence class [w] and for every string w in [w]
(|w |= n), the following conditional statement is
added to the formula:
</bodyText>
<equation confidence="0.912974">
v(w) ==&gt;. (v(w1,1) n v(w2,n)) V (v(w1,2) n
v(w3,n)) V ... V (v(w1,n−1) n v(wn,n))
</equation>
<bodyText confidence="0.996454818181818">
where v(x) is the variable corresponding to the
congruence class [x] and wi,j is the substring of w
from the ith to the jth symbol of w. This statement
is representing the fact that if a congruence class
[w] is chosen as a non-terminal then for each string
in w E [w], there must be at least one CNF rule
A —* BC that generates w and thus there must
be at least one division of w into w1,kwk+1,n such
that B corresponds to [w1,k] and C corresponds to
[wk+1,n].
The grammar extracted from the solution of this
</bodyText>
<page confidence="0.967688">
1356
</page>
<bodyText confidence="0.996419142857143">
formula is made up of all the possible CNF pro-
duction rules built from the chosen non-terminals.
The starting non-terminals are those which corre-
spond to congruence classes that contain at least
one string from the sample.
The following is a run of the whole process on
a simple example:
</bodyText>
<figure confidence="0.540612">
Sample {ab, aabb, aaabbb}
Congruence Classes
1 : [a], 2 : [b], 3 : [ab, aabb, aaabbb], 4 : [aa],
5 : [bb], 6 : [aab, aaabb], 7 : [abb, aabbb],
8 : [aaa], 9 : [bbb], 10 : [aaab], 11 : [abbb]
Boolean Formula
</figure>
<bodyText confidence="0.99777925">
There is one conditional statement per sub-
string. For example, X6 ⇒ (X1∧X3)∨(X4∧
X2) represents the two possible ways aab in
congruence class 6 can be split (a|ab , aa|b).
</bodyText>
<equation confidence="0.998668882352941">
Variables X1, X2 and X3 are true.
X3 ⇒ (X1 ∧ X2)
X3 ⇒ (X1 ∧ X7) ∨ (X4 ∧ X5) ∨ (X6 ∧ X2)
X3 ⇒ (X1 ∧X7)∨(X4 ∧X11)∨(X8 ∧X9)∨
(X10 ∧ X5) ∨ (X6 ∧ X2)
X4 ⇒ (X1 ∧ X1)
X5 ⇒ (X2 ∧ X2)
X6 ⇒ (X1 ∧ X3) ∨ (X4 ∧ X2)
X6 ⇒ (X1 ∧ X3) ∨ (X4 ∧ X7) ∨ (X8 ∧ X5)∨
(X10 ∧ X2)
X7 ⇒ (X1 ∧ X5) ∨ (X3 ∧ X2)
X7 ⇒ (X1∧X11)∨(X4∧X9)∨(X6∧X5)∨
(X3 ∧ X2)
X8 ⇒ (X1 ∧ X4) ∨ (X4 ∧ X1)
X9 ⇒ (X2 ∧ X5) ∨ (X5 ∧ X2)
X10 ⇒ (X1 ∧ X6) ∨ (X4 ∧ X3) ∨ (X8 ∧ X2)
X11 ⇒ (X1 ∧ X9) ∨ (X3 ∧ X5) ∨ (X7 ∧ X2)
</equation>
<sectionHeader confidence="0.29273" genericHeader="method">
Solution
</sectionHeader>
<bodyText confidence="0.854736666666667">
Running the solver on this formula will re-
turn the following true variables that make up
a minimal solution: X1, X2, X3 and X7.
Grammar
For every statement x ⇒ ... ∨ (y ∧ z) ∨ ...
where x,y and z are true, a production rule
x → yz is added. So, the following grammar
is built:
X3 is the starting non-terminal
</bodyText>
<equation confidence="0.6080165">
X3 → X1X7  |X1X2 X7 → X3X2
X1 → a X2 → b
</equation>
<subsectionHeader confidence="0.992642">
3.3 Analysis
</subsectionHeader>
<bodyText confidence="0.999964875">
In the first phase of the algorithm, we are group-
ing all the substrings of the sample 5 according to
the congruence relation ∼=(L,φ), where (L, 0) is the
target stochastic language (for natural language,
this is the language model). To do so, we are as-
suming that 5 was i.i.d. generated from (L, 0).
In the second phase, we are representing the space
of all CFGs consistent with the classes obtained
in phase one as different solutions to a boolean
formula. Here we introduce our bias in favour of
smaller grammars by finding a minimal solution to
the formula. In the last phase, probabilities are as-
signed to the grammar obtained in phase two using
the standard MLE algorithm for PCFGs.
Unlike many other systems, in our case the hy-
pothesis space of grammars is well-defined. This
allows us to analyse our algorithm in a theoreti-
cal framework and obtain theoretical learnability
results. Moreover, this gives us an idea on the
types of syntactical features our system is capable
of learning.
Assuming our algorithm always takes correct
merge decisions, the sample required for identifi-
cation needs only to be structurally complete w.r.t.
the target grammar (i.e. every production rules is
used at least once in the generation of the sample).
This means that, in theory, our algorithm can work
with very small samples (polynomial size w.r.t. the
number of rules in the target grammar).
Some approaches in the literature assume that
whenever a particular substring is a constituent
in some sentence, then it is automatically a con-
stituent in all other sentences (whenever it does not
overlap with previously chosen constituents) (van
Zaanen, 2001; Clark, 2001; Adriaans et al., 2000).
In reality, this is clearly not the case. A simple
experiment on the WSJ10 corpus reveals that only
16 of the most frequent 1009 POS sequences (oc-
curring 10 or more times in the sample) which are
at least once constituents, are in fact always con-
stituents. This assumption does not hold for am-
biguous grammars in our class.
The approach we take to solve the smallest
grammar problem can be extended to other classes
of grammars. A similar formula can be built for
grammars whose non-terminals have a one-to-one
correspondence with congruence classes contain-
ing features of their language (Clark, 2010b).
</bodyText>
<page confidence="0.995753">
1357
</page>
<sectionHeader confidence="0.993752" genericHeader="evaluation">
4 Experiments and Discussion
</sectionHeader>
<subsectionHeader confidence="0.979883">
4.1 Experiments on Artificial Data
</subsectionHeader>
<bodyText confidence="0.961181707317073">
We tested our system on 11 typical context-free
languages and 9 artificial natural language gram-
mars taken from 4 different sources (Stolcke,
1994; Langley and Stromsten, 2000; Adriaans
et al., 2000; Solan et al., 2005). The 11 CFLs in-
clude 7 described by unambiguous grammars:
UC1: a&apos;b&apos; UC2: a&apos;b&apos;c&apos;d&apos; UC3: a&apos;b&apos; n &gt; m
UC4: a�b�, p =� q UC5: Palindromes over alpha-
bet {a, b} with a central marker UC6:Palindromes
over alphabet {a, b} without a central marker
UC7: Lukasiewicz language (S -* aSS|b)
and 4 described by ambiguous grammars:
AC1: |w|a = |w|b AC2: 2|w|a = |w|b AC3: Dyck
language AC4: Regular expressions.
The 9 artificial natural language grammars are:
NL1: Grammar ’a’, Table 2 in (Langley and
Stromsten, 2000) NL2: Grammar ’b’, Table 2
in (Langley and Stromsten, 2000) NL3: Lexical
categories and constituency, pg 96 in (Stolcke,
1994) NL4:Recursive embedding of constituents,
pg 97 in (Stolcke, 1994) NL5: Agreement, pg
98 in (Stolcke, 1994) NL6: Singular/plural NPs
and number agreement, pg 99 in (Stolcke, 1994)
NL7: Experiment 3.1 grammar in (Adriaans et al.,
2000) NL8:Grammar in Table 10 (Adriaans et al.,
2000) NL9: TA1 grammar in (Solan et al., 2005).
The quality of the learned model depends on
its capacity to predict the correct structure (parse
trees) on the one hand and to predict the correct
sentence probabilities on the other (i.e. assigns
a probability distribution close to the target one).
To evaluate parse trees, we follow suggestions
given by van Zaanen and Geertzen (2008) and use
micro-precision and micro-recall over all the non-
trivial brackets. We take the harmonic mean of
these two values to obtain the Unlabelled brack-
ets Fi score (UFi). The learned distribution can
be evaluated using perplexity (when the target dis-
tribution is not known) or some similarity metric
between distributions (when the target distribution
is known). In our case, the target distribution is
</bodyText>
<table confidence="0.999148190476191">
Ex. |Σ ||N ||P|
UC1 2 3 4
UC2 4 7 9
UC3 2 3 5
UC4 2 5 9
UC5 2 3 5
UC6 2 3 8
UC7 2 2 3
AC1 2 4 9
AC2 2 5 11
AC3 2 3 5
AC4 7 8 13
NL1 9 8 15
NL2 8 8 13
NL3 12 10 18
NL4 13 11 22
NL5 16 12 23
NL6 19 17 32
NL7 12 3 9
NL8 30 10 35
NL9 50 45 81
</table>
<tableCaption confidence="0.9960655">
Table 1: Size of the alphabet, number of non-
terminals and productions rules of the grammars.
</tableCaption>
<table confidence="0.999932090909091">
Relative Entropy UFl
Ex. |S |COMINO ADIOS COMINO ABL
UC1 10 0.029 1.876 100 100
UC2 50 0.0 1.799 100 100
UC5 10 0.111 7.706 100 100
UC7 10 0.014 1.257 100 27.86
AC1 50 0.014 4.526 52.36 35.51
AC2 50 0.098 6.139 46.95 14.25
AC3 50 0.057 1.934 99.74 47.48
AC4 100 0.124 1.727 83.63 14.58
NL7 100 0.0 0.124 100 100
NL1 100 0.202 1.646 24.08 24.38
NL2 200 0.333 0.963 45.90 45.80
NL3 100 0.227 1.491 36.34 75.95
NL5 100 0.111 1.692 88.15 79.16
NL6 400 0.227 0.138 36.28 100
UC3 100 0.411 0.864 61.13 100
UC4 100 0.872 2.480 42.84 100
UC6 100 1.449 1.0 20.14 8.36
NL4 500 1.886 2.918 65.88 52.87
NL8 1000 1.496 1.531 57.77 50.04
NL9 800 1.701 1.227 12.49 28.53
</table>
<tableCaption confidence="0.992327">
Table 2: Relative Entropy and UF, results of our
</tableCaption>
<bodyText confidence="0.89961825">
system COMINO vs ADIOS and ABL respec-
tively. Best results are highlighted, close results
(i.e. with a difference of at most 0.1 for relative
entropy and 1% for UFl) are both highlighted
</bodyText>
<page confidence="0.985884">
1358
</page>
<bodyText confidence="0.999960565217391">
known. We chose relative entropy2 as a good mea-
sure of distance between distributions.
Our UF1 results over test sets of one thousand
strings were compared to results obtained by ABL
(van Zaanen, 2001), which is a system whose
primary aim is that of finding good parse trees
(rather than identifying the target language). Al-
though ABL does not obtain state-of-the-art re-
sults on natural language corpora, it proved to be
the best system (for which an implementation is
readily available) for unsupervised parsing of sen-
tences generated by artificial grammars. Results
are shown in Table 1.
We calculated the relative entropy on a test set
of one million strings generated from the target
grammar. We compared our results with ADIOS
(Solan et al., 2005), a system which obtains com-
petitive results on language modelling (Waterfall
et al., 2010) and whose primary aim is of correctly
identifying the target language (rather than finding
good parse trees). Results are shown in Table 1.
For the tests in the first section of Table 1 (i.e.
above the first dashed line), our algorithm was ca-
pable of exactly identifying the structure of the tar-
get grammar. Notwithstanding this, the bracketing
results for these tests did not always yield perfect
scores. This happened whenever the target gram-
mar was ambiguous, in which case the most prob-
able parse trees of the target and learned grammar
can be different, thus leading to incorrect bracket-
ing. For the tests in the second section of Table 1
(i.e. between the two dashed lines), our algorithm
was capable of exactly identifying the target lan-
guage (but not the grammar). In all of these cases,
the induced grammar was slightly smaller than the
target one. For the remaining tests, our algorithm
did not identify the target language. In fact, it al-
ways overgeneralised. The 3 typical CFLs UC3,
UC4 and UC6 are not identified because they are
not contained in our subclass of CFLs. Inspite of
this, the relative entropy results obtained are still
relatively good. Overall, it is fair to say that the
results obtained by our system, for both language
modelling and unsupervised parsing on artificial
data, are competitive with the results obtained by
other methods.
</bodyText>
<footnote confidence="0.812347">
2The relative entropy (or Kullback-Leibler divergence)
between a target distribution D and a hypothesized distri-
</footnote>
<equation confidence="0.314157">
( D(w) )
ln D(w). Add-one
D�(w)
</equation>
<bodyText confidence="0.576345">
smoothing is used to solve the problem of zero probabilities.
</bodyText>
<subsectionHeader confidence="0.99384">
4.2 Natural Language Experiments
</subsectionHeader>
<bodyText confidence="0.9999899">
We also experimented on natural language cor-
pora. For unsupervised parsing, we tested our
system on the WSJ10 corpus, using POS tagged
sentences as input. Due to time efficiency, we
changed the algorithm for finding congruence
classes. Instead of always choosing the best pos-
sible merge w.r.t. the distance function, a distance
threshold is set and all congruence classes whose
distance is smaller than the threshold are merged.
Also, we changed the distance function from L1-
Distance to Pearson’s χ2 test.
In a first experiment (vaguely similar to the one
done by Luque and L´opez (2010)), we constructed
the best possible SC-CFG consistent with the
merges done in the first phase and assigned prob-
abilities to this grammar using Inside-Outside.
In other words, we ran the second phase of
our system in a supervised fashion by using the
treebank to decide which are the best congru-
ence classes to choose as non-terminals. The
CNF grammar we obtained from this experiment
(COMINO-UBOUND) gives very good parsing
results which outperform results from state-of-the-
art systems DMV+CCM (Klein, 2004), U-DOP
(Bod, 2006a), UML-DOP (Bod, 2006b) and In-
cremental (Seginer, 2007) as shown in Table 2.
Moreover, the results obtained are very close to
the best results one can ever hope to obtain from
any CNF grammar on WSJ10 (CNF-UBOUND)
(Klein, 2004). However, the grammar we obtain
does not generalise enough and does not describe a
good language model. In a second experiment, we
ran the complete COMINO system. The grammar
obtained from this experiment did not give com-
petitive parsing results.
The first experiment shows that the merge deci-
sions taken in the first phase do not hinder the pos-
sibility of finding a very good grammar for pars-
ing. This means that the merge decisions taken
by our system are good in general. Manual anal-
ysis on some of the merges taken confirms this.
This experiment also shows that there exists a non-
trivial PCFG in our restrictive class of grammars
that is capable of achieving very good parsing re-
sults. This is a positive sign for the question of
how adequate SC-PCFGs are for modelling natu-
ral languages. However, the real test remains that
of finding SC-PCFGs that generate good bracket-
ings and good language models. The second ex-
periment shows that the second phase of our al-
</bodyText>
<equation confidence="0.568293">
bution D&apos; is defined as 11
w∈Σ∗
</equation>
<page confidence="0.946572">
1359
</page>
<table confidence="0.999834222222222">
Model UP UR UF1
State-of-the-art
DMV+CCM 69.3 88.0 77.6
U-DOP 70.8 88.2 78.5
UML-DOP - - 82.9
Incremental 75.6 76.2 75.9
Upper bounds
COMINO-UBOUND 75.8 96.9 85.1
CNF-UBOUND 78.8 100.0 88.1
</table>
<tableCaption confidence="0.996754">
Table 3: Parsing results on WSJ10. Note that In-
</tableCaption>
<bodyText confidence="0.99209675">
cremental is the only system listed as state-of-the-
art which parses from plain text and can generate
non-binary trees
gorithm is not giving good results. This means
that the smallest possible grammar might not be
the best grammar for parsing. Therefore, other cri-
teria alongside the grammar size are needed when
choosing a grammar consistent with the merges.
</bodyText>
<subsectionHeader confidence="0.994696">
4.3 Discussion and Future Work
</subsectionHeader>
<bodyText confidence="0.99878140625">
In order to improve our system, we think that our
algorithm has to take a less conservative merging
strategy in the first phase. Although the merges
being taken are mostly correct, our analysis shows
that not enough merging is being done. The prob-
lematic case is that of taking merge decisions on
(the many) infrequent long phrases. Although
many logically deduced merges involve infrequent
phrases and also help in increasing the frequency
of some long phrases, this proved to be not enough
to mitigate this problem. As for future work, we
think that clustering techniques can be used to help
solve this problem.
A problem faced by the system is that, in cer-
tain cases, the statistical evidence on which merge
decisions are taken does not point to the intuitively
expected merges. As an example, consider the two
POS sequences ”DT NN” and ”DT JJ NN” in the
WSJ corpus. Any linguist would agree that these
sequences are substitutable (in fact, they have lots
of local contexts in common). However, statisti-
cal evidence points otherwise, since their context
distributions are not close enough. This happens
because, in certain positions of a sentence, ”DT
NN” is far more likely to occur than ”DT JJ NN”
(w.r.t. the ratio of their total frequencies) and in
other positions, ”DT JJ NN” occurs more than ex-
pected. The following table shows the frequencies
of these two POS sequences over the whole WSJ
corpus and their frequencies in contexts (#,VBD)
and (IN,#) (the symbol # represents the end or
beginning of a sentence):
</bodyText>
<table confidence="0.99431725">
Totals (#,VBD) (IN,#)
”DT NN” 42,222 1,034 2,123
”DT JJ NN” 15,243 152 1,119
Ratios 3.16 6.80 1.90
</table>
<bodyText confidence="0.999940425">
It is clear that the ratios do not match, thus lead-
ing to context distributions which are not close
enough. Thus, this shows that basic sequences
such as ”DT NN” and ”DT JJ NN”, which lin-
guists would group into the same concept NP, are
statistically derived from different sub-concepts of
NP. Our algorithm is finding these sub-concepts,
but it is being evaluated on concepts (such as NP)
found in the treebank (created by linguists).
From the experiments we did on artificial nat-
ural language grammars, it resulted that the tar-
get grammar was always slightly bigger than the
learned grammar. Although in these cases we still
managed to identify the target language or have
a good relative entropy result, the bracketing re-
sults were in general not good. This and our sec-
ond experiment on the WSJ10 corpus show that
the smallest possible grammar might not be the
best grammar for bracketing. To not rely solely on
finding the smallest grammar, a bias can be added
in favour of congruence classes which, according
to constituency tests (like the Mutual Information
criterion in Clark (2001)), are more likely to con-
tain substrings that are constituents. This can be
done by giving different weights to the congruence
class variables in the formula and finding the so-
lution with the smallest sum of weights of its true
variables.
The use of POS tags as input can also have its
problems. Although we solve the lexical spar-
sity problem with POS tags, at the same time we
lose a lot of information. In certain cases, one
POS sequence can include raw phrases which ide-
ally are not grouped into the same congruence
class. To mitigate this problem, we can use POS
tags only for rare words and subdivide or ignore
POS tags for frequent words such as determinants
and prepositions. This will reduce the number of
raw phrases represented by POS sequences whilst
keeping lexical sparsity low.
</bodyText>
<page confidence="0.988575">
1360
</page>
<sectionHeader confidence="0.998415" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999967666666667">
We defined a new class of PCFGs that adequately
models natural language syntax. We described a
learning algorithm for this class which scales well
to large examples and is even capable of learning
from small samples. The grammars induced by
this algorithm are compact and perform well on
unsupervised parsing and language modelling of
typical CFLs and artificial natural language gram-
mars.
</bodyText>
<sectionHeader confidence="0.99414" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9723825">
The authors acknowledge partial support by the
R´egion des Pays de la Loire.
</bodyText>
<sectionHeader confidence="0.996218" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.982245839506173">
Pieter W. Adriaans, Marten Trautwein, and Marco
Vervoort. Towards High Speed Grammar Induc-
tion on Large Text Corpora. In V´aclav Hlav´ac,
Keith G. Jeffery, and JiriWiedermann, edi-
tors, SOFSEM, volume 1963 of Lecture Notes
in Computer Science, pages 173–186. Springer,
2000.
Michel Berkelaar et al. lpSolve: Interface to Lp
solve v. 5.5 to solve linear/integer programs. R
package version, 5(4), 2008.
Luc Boasson and G´eraud S´enizergues. NTS Lan-
guages Are Deterministic and Congruential. J.
Comput. Syst. Sci., 31(3):332–342, 1985.
Rens Bod. Unsupervised Parsing with U-DOP.
In Proceedings of the Tenth Conference on
Computational Natural Language Learning,
CoNLL-X ’06, pages 85–92, Stroudsburg, PA,
USA, 2006a. Association for Computational
Linguistics.
Rens Bod. An All-Subtrees Approach to Unsu-
pervised Parsing. In Proceedings of the 21st In-
ternational Conference on Computational Lin-
guistics and the 44th annual meeting of the As-
sociation for Computational Linguistics, pages
865–872. Association for Computational Lin-
guistics, 2006b.
Alexander Clark. Unsupervised Language Acqui-
sition: Theory and Practice. PhD thesis, Uni-
versity of Sussex, 2001.
Alexander Clark. Distributional Learning of Some
Context-Free Languages with a Minimally Ad-
equate Teacher. In Sempere and Garcia (2010),
pages 24–37.
Alexander Clark. Towards General Algorithms
for Grammatical Inference. In Marcus Hut-
ter, Frank Stephan, Vladimir Vovk, and Thomas
Zeugmann, editors, ALT, volume 6331 of Lec-
ture Notes in Computer Science, pages 11–30.
Springer, 2010b.
Alexander Clark and R´emi Eyraud. Polyno-
mial Identification in the Limit of Substitutable
Context-free Languages. Journal of Machine
Learning Research, 8:1725–1745, 2007.
Alexander Clark and Shalom Lappin. Unsuper-
vised Learning and Grammar Induction. In
Alexander Clark, Chris Fox, and Shalom Lap-
pin, editors, The Handbook of Computational
Linguistics and Natural Language Processing,
pages 197–220. Wiley-Blackwell, 2010.
Alexander Clark, Franc¸ois Coste, and Laurent
Miclet, editors. Grammatical Inference: Al-
gorithms and Applications, 9th International
Colloquium, ICGI 2008, Saint-Malo, France,
September 22-24, 2008, Proceedings, volume
5278 of Lecture Notes in Computer Science,
2008. Springer.
Colin de la Higuera. Characteristic Sets for
Polynomial Grammatical Inference. Machine
Learning, 27(2):125–138, 1997.
Colin de la Higuera. Grammatical Inference:
Learning Automata and Grammars. 2010.
E. Mark Gold. Language Identification in the
Limit. Information and Control, 10(5):447–
474, 1967.
James Jay Horning. A Study of Grammatical In-
ference. PhD thesis, 1969.
Mark Johnson, Thomas L. Griffiths, and Sharon
Goldwater. Bayesian Inference for PCFGs via
Markov Chain Monte Carlo. In Candace L.
Sidner, Tanja Schultz, Matthew Stone, and
ChengXiang Zhai, editors, HLT-NAACL, pages
139–146. The Association for Computational
Linguistics, 2007.
Daniel Jurafsky and James H. Martin. Speech and
Language Processing (2nd Edition) (Prentice
Hall Series in Artificial Intelligence). Prentice
Hall, 2 edition, 2008.
Sanjeev Khanna, Madhu Sudan, Luca Trevisan,
and David P. Williamson. The Approximabil-
ity of Constraint Satisfaction Problems. SIAM
J. Comput., 30(6):1863–1920, 2000.
</reference>
<page confidence="0.819918">
1361
</page>
<reference confidence="0.999503204819277">
Dan Klein. The Unsupervised Learning of Natu-
ral Language Structure. PhD thesis, Stanford
University, 2004.
Kevin J. Lang, Barak A. Pearlmutter, and Rod-
ney A. Price. Results of the Abbadingo
One DFA Learning Competition and a New
Evidence-Driven State Merging Algorithm. In
Vasant Honavar and Giora Slutzki, editors,
ICGI, volume 1433 of Lecture Notes in Com-
puter Science, pages 1–12. Springer, 1998.
Pat Langley and Sean Stromsten. Learning
Context-Free Grammars with a Simplicity Bias.
In Ramon L´opez de M´antaras and Enric Plaza,
editors, ECML, volume 1810 of Lecture Notes
in Computer Science, pages 220–228. Springer,
2000.
Karim Lari and Steve J. Young. The Estimation
of Stochastic Context-Free Grammars using the
Inside-Outside Algorithm. Computer Speech &amp;
Language, 4(1):35 – 56, 1990.
Franco M. Luque and Gabriel G. Infante L´opez.
Bounding the Maximal Parsing Performance of
Non-Terminally Separated Grammars. In Sem-
pere and Garcia (2010), pages 135–147.
Christopher D. Manning and Hinrich Sch¨utze.
Foundations of Statistical Natural Language
Processing. MIT Press, 2001.
Paola Merlo, Harry Bunt, and Joakim Nivre. Cur-
rent Trends in Parsing Technology. In Trends
in Parsing Technology, pages 1–17. Springer,
2010.
Georgios Petasis, Georgios Paliouras, Constan-
tine D. Spyropoulos, and Constantine Halat-
sis. eg-GRIDS: Context-Free Grammatical In-
ference from Positive Examples Using Genetic
Search. In Georgios Paliouras and Yasubumi
Sakakibara, editors, ICGI, volume 3264 of Lec-
ture Notes in Computer Science, pages 223–
234. Springer, 2004.
Gisa Rauh. Syntactic Categories: Their Identifi-
cation and Description in Linguistic Theories.
Oxford Surveys in Syntax &amp; Morphology No.7.
OUP Oxford, 2010.
Yoav Seginer. Fast Unsupervised Incremental
Parsing. In John A. Carroll, Antal van den
Bosch, and Annie Zaenen, editors, ACL.
The Association for Computational Linguistics,
2007.
Jos´e M. Sempere and Pedro Garcia, editors.
Grammatical Inference: Theoretical Results
and Applications, 10th International Collo-
quium, ICGI2010, Valencia, Spain, September
13-16, 2010. Proceedings, volume 6339 of Lec-
ture Notes in Computer Science, 2010. Springer.
Zach Solan, David Horn, Eytan Ruppin, and Shi-
mon Edelman. Unsupervised learning of nat-
ural languages. Proceedings of the National
Academy of Sciences of the United States of
America, 102(33):11629–11634, 2005.
Andreas Stolcke. Bayesian learning ofprobabilis-
tic language models. PhD thesis, University of
California, Berkeley, 1994.
Menno van Zaanen. Bootstrapping Structure into
Language: Alignment-Based Learning. PhD
thesis, University of Leeds, 2001.
Menno van Zaanen and Jeroen Geertzen. Prob-
lems with Evaluation of Unsupervised Empiri-
cal Grammatical Inference Systems. In Clark
et al. (2008), pages 301–303.
Heidi R. Waterfall, Ben Sandbank, Luca Onnis,
and Shimon Edelman. An Empirical Genera-
tive Framework for Computational Modeling of
Language Acquisition. Journal of Child Lan-
guage, 37:671–703, 6 2010.
Charles S. Wetherell. Probabilistic Languages: A
Review and Some Open Questions. ACM Com-
put. Surv., 12(4):361–379, 1980.
Charles Yang. Computational Models of Syntactic
Acquisition. Wiley Interdisciplinary Reviews:
Cognitive Science, 3(2):205–213, 2012.
Ryo Yoshinaka. Identification in the Limit of k, l-
Substitutable Context-Free Languages. In Clark
et al. (2008), pages 266–279.
</reference>
<page confidence="0.994472">
1362
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.315354">
<title confidence="0.999894">PCFG Induction for Unsupervised Parsing and Language Modelling</title>
<author confidence="0.974934">James</author>
<affiliation confidence="0.9838285">Universit´e de CNRS, LINA,</affiliation>
<address confidence="0.80526">F-44000,</address>
<email confidence="0.993095">james.scicluna@univ-nantes.fr</email>
<author confidence="0.615078">Colin de_la</author>
<affiliation confidence="0.9825555">Universit´e de CNRS, LINA,</affiliation>
<address confidence="0.749987">F-44000,</address>
<email confidence="0.994247">cdlh@univ-nantes.fr</email>
<abstract confidence="0.996297791666667">The task of unsupervised induction of probabilistic context-free grammars (PCFGs) has attracted a lot of attention in the field of computational linguistics. Although it is a difficult task, work in this area is still very much in demand since it can contribute to the advancement of language parsing and modelling. In this work, we describe a new algorithm for PCFG induction based on a principled approach and capable of inducing accurate yet compact artificial natural language grammars and typical context-free grammars. Moreover, this algorithm can work on large grammars and datasets and infers correctly even from small samples. Our analysis shows that the type of grammars induced by our algorithm are, in theory, capable of modelling natural language. One of our experiments shows that our algorithm can potentially outperform the state-of-the-art in unsupervised parsing on the WSJ10 corpus.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Pieter W Adriaans</author>
<author>Marten Trautwein</author>
<author>Marco Vervoort</author>
</authors>
<title>Towards High Speed Grammar Induction on Large Text Corpora.</title>
<date>2000</date>
<booktitle>of Lecture Notes in Computer Science,</booktitle>
<volume>1963</volume>
<pages>173--186</pages>
<editor>In V´aclav Hlav´ac, Keith G. Jeffery, and JiriWiedermann, editors, SOFSEM,</editor>
<publisher>Springer,</publisher>
<contexts>
<context position="19955" citStr="Adriaans et al., 2000" startWordPosition="3438" endWordPosition="3441">mple required for identification needs only to be structurally complete w.r.t. the target grammar (i.e. every production rules is used at least once in the generation of the sample). This means that, in theory, our algorithm can work with very small samples (polynomial size w.r.t. the number of rules in the target grammar). Some approaches in the literature assume that whenever a particular substring is a constituent in some sentence, then it is automatically a constituent in all other sentences (whenever it does not overlap with previously chosen constituents) (van Zaanen, 2001; Clark, 2001; Adriaans et al., 2000). In reality, this is clearly not the case. A simple experiment on the WSJ10 corpus reveals that only 16 of the most frequent 1009 POS sequences (occurring 10 or more times in the sample) which are at least once constituents, are in fact always constituents. This assumption does not hold for ambiguous grammars in our class. The approach we take to solve the smallest grammar problem can be extended to other classes of grammars. A similar formula can be built for grammars whose non-terminals have a one-to-one correspondence with congruence classes containing features of their language (Clark, 20</context>
<context position="21712" citStr="Adriaans et al., 2000" startWordPosition="3728" endWordPosition="3731">7: Lukasiewicz language (S -* aSS|b) and 4 described by ambiguous grammars: AC1: |w|a = |w|b AC2: 2|w|a = |w|b AC3: Dyck language AC4: Regular expressions. The 9 artificial natural language grammars are: NL1: Grammar ’a’, Table 2 in (Langley and Stromsten, 2000) NL2: Grammar ’b’, Table 2 in (Langley and Stromsten, 2000) NL3: Lexical categories and constituency, pg 96 in (Stolcke, 1994) NL4:Recursive embedding of constituents, pg 97 in (Stolcke, 1994) NL5: Agreement, pg 98 in (Stolcke, 1994) NL6: Singular/plural NPs and number agreement, pg 99 in (Stolcke, 1994) NL7: Experiment 3.1 grammar in (Adriaans et al., 2000) NL8:Grammar in Table 10 (Adriaans et al., 2000) NL9: TA1 grammar in (Solan et al., 2005). The quality of the learned model depends on its capacity to predict the correct structure (parse trees) on the one hand and to predict the correct sentence probabilities on the other (i.e. assigns a probability distribution close to the target one). To evaluate parse trees, we follow suggestions given by van Zaanen and Geertzen (2008) and use micro-precision and micro-recall over all the nontrivial brackets. We take the harmonic mean of these two values to obtain the Unlabelled brackets Fi score (UFi). T</context>
</contexts>
<marker>Adriaans, Trautwein, Vervoort, 2000</marker>
<rawString>Pieter W. Adriaans, Marten Trautwein, and Marco Vervoort. Towards High Speed Grammar Induction on Large Text Corpora. In V´aclav Hlav´ac, Keith G. Jeffery, and JiriWiedermann, editors, SOFSEM, volume 1963 of Lecture Notes in Computer Science, pages 173–186. Springer, 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Berkelaar</author>
</authors>
<title>lpSolve: Interface to Lp solve v. 5.5 to solve linear/integer programs. R package version,</title>
<date>2008</date>
<volume>5</volume>
<issue>4</issue>
<marker>Berkelaar, 2008</marker>
<rawString>Michel Berkelaar et al. lpSolve: Interface to Lp solve v. 5.5 to solve linear/integer programs. R package version, 5(4), 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luc Boasson</author>
<author>G´eraud S´enizergues</author>
</authors>
<date>1985</date>
<journal>NTS Languages Are Deterministic and Congruential. J. Comput. Syst. Sci.,</journal>
<volume>31</volume>
<issue>3</issue>
<marker>Boasson, S´enizergues, 1985</marker>
<rawString>Luc Boasson and G´eraud S´enizergues. NTS Languages Are Deterministic and Congruential. J. Comput. Syst. Sci., 31(3):332–342, 1985.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rens Bod</author>
</authors>
<title>Unsupervised Parsing with U-DOP.</title>
<date>2006</date>
<booktitle>In Proceedings of the Tenth Conference on Computational Natural Language Learning, CoNLL-X ’06,</booktitle>
<pages>85--92</pages>
<location>Stroudsburg, PA, USA,</location>
<contexts>
<context position="27333" citStr="Bod, 2006" startWordPosition="4720" endWordPosition="4721">t. In a first experiment (vaguely similar to the one done by Luque and L´opez (2010)), we constructed the best possible SC-CFG consistent with the merges done in the first phase and assigned probabilities to this grammar using Inside-Outside. In other words, we ran the second phase of our system in a supervised fashion by using the treebank to decide which are the best congruence classes to choose as non-terminals. The CNF grammar we obtained from this experiment (COMINO-UBOUND) gives very good parsing results which outperform results from state-of-theart systems DMV+CCM (Klein, 2004), U-DOP (Bod, 2006a), UML-DOP (Bod, 2006b) and Incremental (Seginer, 2007) as shown in Table 2. Moreover, the results obtained are very close to the best results one can ever hope to obtain from any CNF grammar on WSJ10 (CNF-UBOUND) (Klein, 2004). However, the grammar we obtain does not generalise enough and does not describe a good language model. In a second experiment, we ran the complete COMINO system. The grammar obtained from this experiment did not give competitive parsing results. The first experiment shows that the merge decisions taken in the first phase do not hinder the possibility of finding a very</context>
</contexts>
<marker>Bod, 2006</marker>
<rawString>Rens Bod. Unsupervised Parsing with U-DOP. In Proceedings of the Tenth Conference on Computational Natural Language Learning, CoNLL-X ’06, pages 85–92, Stroudsburg, PA, USA, 2006a. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rens Bod</author>
</authors>
<title>An All-Subtrees Approach to Unsupervised Parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics,</booktitle>
<pages>865--872</pages>
<contexts>
<context position="27333" citStr="Bod, 2006" startWordPosition="4720" endWordPosition="4721">t. In a first experiment (vaguely similar to the one done by Luque and L´opez (2010)), we constructed the best possible SC-CFG consistent with the merges done in the first phase and assigned probabilities to this grammar using Inside-Outside. In other words, we ran the second phase of our system in a supervised fashion by using the treebank to decide which are the best congruence classes to choose as non-terminals. The CNF grammar we obtained from this experiment (COMINO-UBOUND) gives very good parsing results which outperform results from state-of-theart systems DMV+CCM (Klein, 2004), U-DOP (Bod, 2006a), UML-DOP (Bod, 2006b) and Incremental (Seginer, 2007) as shown in Table 2. Moreover, the results obtained are very close to the best results one can ever hope to obtain from any CNF grammar on WSJ10 (CNF-UBOUND) (Klein, 2004). However, the grammar we obtain does not generalise enough and does not describe a good language model. In a second experiment, we ran the complete COMINO system. The grammar obtained from this experiment did not give competitive parsing results. The first experiment shows that the merge decisions taken in the first phase do not hinder the possibility of finding a very</context>
</contexts>
<marker>Bod, 2006</marker>
<rawString>Rens Bod. An All-Subtrees Approach to Unsupervised Parsing. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, pages 865–872. Association for Computational Linguistics, 2006b.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Clark</author>
</authors>
<title>Unsupervised Language Acquisition: Theory and Practice.</title>
<date>2001</date>
<tech>PhD thesis,</tech>
<institution>University of Sussex,</institution>
<contexts>
<context position="13851" citStr="Clark, 2001" startWordPosition="2321" endWordPosition="2322">r structure. Obviously, one can make use of only one of these conditions by assigning the other a parameter value which makes it trivially true from the outset (0 for d and |Subs |for i). 3.2 Building the Context-Free Grammar Deciding which substrings are constituents (in our case, this translates into choosing which congruence classes correspond to non-terminals) is a problematic issue and is considered a harder task than the previous step (Klein, 2004). A path followed by a number of authors consists in using an Ockham’s razor or Minimal Description Length principle approach (Stolcke, 1994; Clark, 2001; Petasis et al., 2004). This generally leads to choosing as best hypothesis the one which best compresses the data. Applying this principle in our case would mean that the non-terminals should be 1for example, if a congruence class contains the phrases ”the big” and ”that small”, and another class contains ”dog barked” and ”cat meowed”, it can be logically deduced that the phrases ”the big dog barked”,”the big cat meowed”, ”that small dog barked” and ”that small cat meowed” should be in the same class. assigned in such a way that the grammar built is the smallest possible one (in terms of the</context>
<context position="19931" citStr="Clark, 2001" startWordPosition="3436" endWordPosition="3437">sions, the sample required for identification needs only to be structurally complete w.r.t. the target grammar (i.e. every production rules is used at least once in the generation of the sample). This means that, in theory, our algorithm can work with very small samples (polynomial size w.r.t. the number of rules in the target grammar). Some approaches in the literature assume that whenever a particular substring is a constituent in some sentence, then it is automatically a constituent in all other sentences (whenever it does not overlap with previously chosen constituents) (van Zaanen, 2001; Clark, 2001; Adriaans et al., 2000). In reality, this is clearly not the case. A simple experiment on the WSJ10 corpus reveals that only 16 of the most frequent 1009 POS sequences (occurring 10 or more times in the sample) which are at least once constituents, are in fact always constituents. This assumption does not hold for ambiguous grammars in our class. The approach we take to solve the smallest grammar problem can be extended to other classes of grammars. A similar formula can be built for grammars whose non-terminals have a one-to-one correspondence with congruence classes containing features of t</context>
<context position="31900" citStr="Clark (2001)" startWordPosition="5500" endWordPosition="5501">natural language grammars, it resulted that the target grammar was always slightly bigger than the learned grammar. Although in these cases we still managed to identify the target language or have a good relative entropy result, the bracketing results were in general not good. This and our second experiment on the WSJ10 corpus show that the smallest possible grammar might not be the best grammar for bracketing. To not rely solely on finding the smallest grammar, a bias can be added in favour of congruence classes which, according to constituency tests (like the Mutual Information criterion in Clark (2001)), are more likely to contain substrings that are constituents. This can be done by giving different weights to the congruence class variables in the formula and finding the solution with the smallest sum of weights of its true variables. The use of POS tags as input can also have its problems. Although we solve the lexical sparsity problem with POS tags, at the same time we lose a lot of information. In certain cases, one POS sequence can include raw phrases which ideally are not grouped into the same congruence class. To mitigate this problem, we can use POS tags only for rare words and subd</context>
</contexts>
<marker>Clark, 2001</marker>
<rawString>Alexander Clark. Unsupervised Language Acquisition: Theory and Practice. PhD thesis, University of Sussex, 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Clark</author>
</authors>
<title>Distributional Learning of Some Context-Free Languages with a Minimally Adequate Teacher.</title>
<date>2010</date>
<booktitle>In Sempere and Garcia</booktitle>
<pages>24--37</pages>
<contexts>
<context position="5726" citStr="Clark (2010" startWordPosition="956" endWordPosition="957">. Two strings u and v are syntactically congruent with respect to L, written u =L v, if Con(u, L) = Con(v, L). This is a congruence relation on Σ*. The context distribution of a substring w w.r.t. a stochastic language (L, φ), denoted C(L,φ) w , is a distribution whose support is all the possible contexts over alphabet Σ (i.e. Σ* x Σ*) and is defined as follows: C(L,φ) w(l, r) = φ(lwr) �l�,r��Σ∗ φ(l&apos;wr&apos;) Two strings u and v are stochastically congruent with respect to (L, φ), written u —=(L,φ) v, if C(L,φ) u is equal to C(L,φ) v . This is a congruence relation on Σ*. 2.3 Congruential Grammars Clark (2010a) defines Congruential CFGs (CCFGs) as being all the CFGs G which, for any non-terminal A, if u E L(A) then L(A) C_ [u]_L(G) (where [u]_L(G) is the syntactic congruence class of u w.r.t. the language of G). This class of grammars was defined with learnability in mind. Since these grammars have a direct relationship between congruence classes and the non-terminals, their learnability is reduced to that of finding the correct congruence classes (Clark, 2010a). This class of grammars is closely related to the class of NTS-grammars (Boasson and S´enizergues, 1985). Any C-CFG is an NTSgrammar but </context>
<context position="20557" citStr="Clark, 2010" startWordPosition="3542" endWordPosition="3543">l., 2000). In reality, this is clearly not the case. A simple experiment on the WSJ10 corpus reveals that only 16 of the most frequent 1009 POS sequences (occurring 10 or more times in the sample) which are at least once constituents, are in fact always constituents. This assumption does not hold for ambiguous grammars in our class. The approach we take to solve the smallest grammar problem can be extended to other classes of grammars. A similar formula can be built for grammars whose non-terminals have a one-to-one correspondence with congruence classes containing features of their language (Clark, 2010b). 1357 4 Experiments and Discussion 4.1 Experiments on Artificial Data We tested our system on 11 typical context-free languages and 9 artificial natural language grammars taken from 4 different sources (Stolcke, 1994; Langley and Stromsten, 2000; Adriaans et al., 2000; Solan et al., 2005). The 11 CFLs include 7 described by unambiguous grammars: UC1: a&apos;b&apos; UC2: a&apos;b&apos;c&apos;d&apos; UC3: a&apos;b&apos; n &gt; m UC4: a�b�, p =� q UC5: Palindromes over alphabet {a, b} with a central marker UC6:Palindromes over alphabet {a, b} without a central marker UC7: Lukasiewicz language (S -* aSS|b) and 4 described by ambiguous g</context>
</contexts>
<marker>Clark, 2010</marker>
<rawString>Alexander Clark. Distributional Learning of Some Context-Free Languages with a Minimally Adequate Teacher. In Sempere and Garcia (2010), pages 24–37.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Alexander Clark</author>
</authors>
<title>Towards General Algorithms for Grammatical Inference.</title>
<booktitle>of Lecture Notes in Computer Science,</booktitle>
<volume>6331</volume>
<pages>11--30</pages>
<editor>In Marcus Hutter, Frank Stephan, Vladimir Vovk, and Thomas Zeugmann, editors, ALT,</editor>
<publisher>Springer,</publisher>
<marker>Clark, </marker>
<rawString>Alexander Clark. Towards General Algorithms for Grammatical Inference. In Marcus Hutter, Frank Stephan, Vladimir Vovk, and Thomas Zeugmann, editors, ALT, volume 6331 of Lecture Notes in Computer Science, pages 11–30. Springer, 2010b.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Clark</author>
<author>R´emi Eyraud</author>
</authors>
<title>Polynomial Identification in the Limit of Substitutable Context-free Languages.</title>
<date>2007</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>8</volume>
<contexts>
<context position="6599" citStr="Clark and Eyraud, 2007" startWordPosition="1091" endWordPosition="1094">arnability in mind. Since these grammars have a direct relationship between congruence classes and the non-terminals, their learnability is reduced to that of finding the correct congruence classes (Clark, 2010a). This class of grammars is closely related to the class of NTS-grammars (Boasson and S´enizergues, 1985). Any C-CFG is an NTSgrammar but not vice-versa. However, it is not known whether languages generated by C-CFGs are all NTS-languages (Clark, 2010a). Note that NTS-languages are a subclass of deterministic context-free languages and contain the regular languages, the substitutable (Clark and Eyraud, 2007) and k-l-substitutable context-free languages (Yoshinaka, 2008), the very simple languages and other CFLs such as the Dyck language (Boasson and S´enizergues, 1985). We define a slightly more restrictive class of grammars, which we shall call Strongly Congruential CFGs (SC-CFGs). A CFG G is a SCCFG if, for any non-terminal A, if u E L(A) then L(A) = [u]_L(G). The probabilistic equivalent of this is the class of Strongly Congruential PCFGs (SC-PCFGs), defined as all the PCFGs G which, for any non-terminal A, if u E L(A) then L(A) = [u]—=(L(G),φ). In other words, the nonterminals (i.e. syntactic</context>
</contexts>
<marker>Clark, Eyraud, 2007</marker>
<rawString>Alexander Clark and R´emi Eyraud. Polynomial Identification in the Limit of Substitutable Context-free Languages. Journal of Machine Learning Research, 8:1725–1745, 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Clark</author>
<author>Shalom Lappin</author>
</authors>
<title>Unsupervised Learning and Grammar Induction.</title>
<date>2010</date>
<booktitle>The Handbook of Computational Linguistics and Natural Language Processing,</booktitle>
<pages>197--220</pages>
<editor>In Alexander Clark, Chris Fox, and Shalom Lappin, editors,</editor>
<publisher>Wiley-Blackwell,</publisher>
<contexts>
<context position="1826" citStr="Clark and Lappin, 2010" startWordPosition="274" endWordPosition="277">of unsupervised induction of PCFGs has attracted a lot of attention in the field of computational linguistics. This task can take the form of either parameter search or structure learning. In parameter search, a CFG is fixed and the focus is on assigning probabilities to this grammar using Bayesian methods (Johnson et al., 2007) or maximum likelihood estimation (Lari and Young, 1990). In structure learning, the focus is on building the right grammar rules from scratch. We take the latter approach. Unsupervised structure learning of (P)CFGs is a notoriously difficult task (de la Higuera, 2010; Clark and Lappin, 2010), with theoretical results showing that in general it is either impossible to achieve (Gold, 1967; de la Higuera, 1997) or requires impractical resources (Horning, 1969; Yang, 2012). At the same time, it is well known that context-free structures are needed for better language parsing and modelling, since less expressive models (such as HMMs) are not good enough (Manning and Sch¨utze, 2001; Jurafsky and Martin, 2008). Moreover, the trend is towards unsupervised (rather than supervised) learning methods due to the lack in most languages of annotated data and the applicability in wider domains (</context>
</contexts>
<marker>Clark, Lappin, 2010</marker>
<rawString>Alexander Clark and Shalom Lappin. Unsupervised Learning and Grammar Induction. In Alexander Clark, Chris Fox, and Shalom Lappin, editors, The Handbook of Computational Linguistics and Natural Language Processing, pages 197–220. Wiley-Blackwell, 2010.</rawString>
</citation>
<citation valid="true">
<date>2008</date>
<booktitle>Grammatical Inference: Algorithms and Applications, 9th International Colloquium, ICGI 2008,</booktitle>
<volume>5278</volume>
<editor>Alexander Clark, Franc¸ois Coste, and Laurent Miclet, editors.</editor>
<publisher>Springer.</publisher>
<location>Saint-Malo, France,</location>
<contexts>
<context position="22139" citStr="(2008)" startWordPosition="3802" endWordPosition="3802">lcke, 1994) NL5: Agreement, pg 98 in (Stolcke, 1994) NL6: Singular/plural NPs and number agreement, pg 99 in (Stolcke, 1994) NL7: Experiment 3.1 grammar in (Adriaans et al., 2000) NL8:Grammar in Table 10 (Adriaans et al., 2000) NL9: TA1 grammar in (Solan et al., 2005). The quality of the learned model depends on its capacity to predict the correct structure (parse trees) on the one hand and to predict the correct sentence probabilities on the other (i.e. assigns a probability distribution close to the target one). To evaluate parse trees, we follow suggestions given by van Zaanen and Geertzen (2008) and use micro-precision and micro-recall over all the nontrivial brackets. We take the harmonic mean of these two values to obtain the Unlabelled brackets Fi score (UFi). The learned distribution can be evaluated using perplexity (when the target distribution is not known) or some similarity metric between distributions (when the target distribution is known). In our case, the target distribution is Ex. |Σ ||N ||P| UC1 2 3 4 UC2 4 7 9 UC3 2 3 5 UC4 2 5 9 UC5 2 3 5 UC6 2 3 8 UC7 2 2 3 AC1 2 4 9 AC2 2 5 11 AC3 2 3 5 AC4 7 8 13 NL1 9 8 15 NL2 8 8 13 NL3 12 10 18 NL4 13 11 22 NL5 16 12 23 NL6 19 </context>
</contexts>
<marker>2008</marker>
<rawString>Alexander Clark, Franc¸ois Coste, and Laurent Miclet, editors. Grammatical Inference: Algorithms and Applications, 9th International Colloquium, ICGI 2008, Saint-Malo, France, September 22-24, 2008, Proceedings, volume 5278 of Lecture Notes in Computer Science, 2008. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin de la Higuera</author>
</authors>
<title>Characteristic Sets for Polynomial Grammatical Inference.</title>
<date>1997</date>
<booktitle>Machine Learning,</booktitle>
<volume>27</volume>
<issue>2</issue>
<contexts>
<context position="1945" citStr="Higuera, 1997" startWordPosition="295" endWordPosition="296">e the form of either parameter search or structure learning. In parameter search, a CFG is fixed and the focus is on assigning probabilities to this grammar using Bayesian methods (Johnson et al., 2007) or maximum likelihood estimation (Lari and Young, 1990). In structure learning, the focus is on building the right grammar rules from scratch. We take the latter approach. Unsupervised structure learning of (P)CFGs is a notoriously difficult task (de la Higuera, 2010; Clark and Lappin, 2010), with theoretical results showing that in general it is either impossible to achieve (Gold, 1967; de la Higuera, 1997) or requires impractical resources (Horning, 1969; Yang, 2012). At the same time, it is well known that context-free structures are needed for better language parsing and modelling, since less expressive models (such as HMMs) are not good enough (Manning and Sch¨utze, 2001; Jurafsky and Martin, 2008). Moreover, the trend is towards unsupervised (rather than supervised) learning methods due to the lack in most languages of annotated data and the applicability in wider domains (Merlo et al., 2010). Thus, despite its difficulty, unsupervised PCFG grammar induction (or induction of other similarly</context>
</contexts>
<marker>Higuera, 1997</marker>
<rawString>Colin de la Higuera. Characteristic Sets for Polynomial Grammatical Inference. Machine Learning, 27(2):125–138, 1997.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin de la Higuera</author>
</authors>
<title>Grammatical Inference: Learning Automata and Grammars.</title>
<date>2010</date>
<contexts>
<context position="1801" citStr="Higuera, 2010" startWordPosition="272" endWordPosition="273">ction The task of unsupervised induction of PCFGs has attracted a lot of attention in the field of computational linguistics. This task can take the form of either parameter search or structure learning. In parameter search, a CFG is fixed and the focus is on assigning probabilities to this grammar using Bayesian methods (Johnson et al., 2007) or maximum likelihood estimation (Lari and Young, 1990). In structure learning, the focus is on building the right grammar rules from scratch. We take the latter approach. Unsupervised structure learning of (P)CFGs is a notoriously difficult task (de la Higuera, 2010; Clark and Lappin, 2010), with theoretical results showing that in general it is either impossible to achieve (Gold, 1967; de la Higuera, 1997) or requires impractical resources (Horning, 1969; Yang, 2012). At the same time, it is well known that context-free structures are needed for better language parsing and modelling, since less expressive models (such as HMMs) are not good enough (Manning and Sch¨utze, 2001; Jurafsky and Martin, 2008). Moreover, the trend is towards unsupervised (rather than supervised) learning methods due to the lack in most languages of annotated data and the applica</context>
</contexts>
<marker>Higuera, 2010</marker>
<rawString>Colin de la Higuera. Grammatical Inference: Learning Automata and Grammars. 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Mark Gold</author>
</authors>
<title>Language Identification in the Limit.</title>
<date>1967</date>
<journal>Information and Control,</journal>
<volume>10</volume>
<issue>5</issue>
<pages>474</pages>
<contexts>
<context position="1923" citStr="Gold, 1967" startWordPosition="291" endWordPosition="292"> This task can take the form of either parameter search or structure learning. In parameter search, a CFG is fixed and the focus is on assigning probabilities to this grammar using Bayesian methods (Johnson et al., 2007) or maximum likelihood estimation (Lari and Young, 1990). In structure learning, the focus is on building the right grammar rules from scratch. We take the latter approach. Unsupervised structure learning of (P)CFGs is a notoriously difficult task (de la Higuera, 2010; Clark and Lappin, 2010), with theoretical results showing that in general it is either impossible to achieve (Gold, 1967; de la Higuera, 1997) or requires impractical resources (Horning, 1969; Yang, 2012). At the same time, it is well known that context-free structures are needed for better language parsing and modelling, since less expressive models (such as HMMs) are not good enough (Manning and Sch¨utze, 2001; Jurafsky and Martin, 2008). Moreover, the trend is towards unsupervised (rather than supervised) learning methods due to the lack in most languages of annotated data and the applicability in wider domains (Merlo et al., 2010). Thus, despite its difficulty, unsupervised PCFG grammar induction (or induct</context>
</contexts>
<marker>Gold, 1967</marker>
<rawString>E. Mark Gold. Language Identification in the Limit. Information and Control, 10(5):447– 474, 1967.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Jay</author>
</authors>
<title>Horning. A Study of Grammatical Inference.</title>
<date>1969</date>
<tech>PhD thesis,</tech>
<marker>Jay, 1969</marker>
<rawString>James Jay Horning. A Study of Grammatical Inference. PhD thesis, 1969.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
<author>Thomas L Griffiths</author>
<author>Sharon Goldwater</author>
</authors>
<title>Bayesian Inference for PCFGs via Markov Chain Monte Carlo.</title>
<date>2007</date>
<booktitle>The Association for Computational Linguistics,</booktitle>
<pages>139--146</pages>
<editor>In Candace L. Sidner, Tanja Schultz, Matthew Stone, and ChengXiang Zhai, editors, HLT-NAACL,</editor>
<contexts>
<context position="1533" citStr="Johnson et al., 2007" startWordPosition="228" endWordPosition="231"> Our analysis shows that the type of grammars induced by our algorithm are, in theory, capable of modelling natural language. One of our experiments shows that our algorithm can potentially outperform the state-of-the-art in unsupervised parsing on the WSJ10 corpus. 1 Introduction The task of unsupervised induction of PCFGs has attracted a lot of attention in the field of computational linguistics. This task can take the form of either parameter search or structure learning. In parameter search, a CFG is fixed and the focus is on assigning probabilities to this grammar using Bayesian methods (Johnson et al., 2007) or maximum likelihood estimation (Lari and Young, 1990). In structure learning, the focus is on building the right grammar rules from scratch. We take the latter approach. Unsupervised structure learning of (P)CFGs is a notoriously difficult task (de la Higuera, 2010; Clark and Lappin, 2010), with theoretical results showing that in general it is either impossible to achieve (Gold, 1967; de la Higuera, 1997) or requires impractical resources (Horning, 1969; Yang, 2012). At the same time, it is well known that context-free structures are needed for better language parsing and modelling, since </context>
</contexts>
<marker>Johnson, Griffiths, Goldwater, 2007</marker>
<rawString>Mark Johnson, Thomas L. Griffiths, and Sharon Goldwater. Bayesian Inference for PCFGs via Markov Chain Monte Carlo. In Candace L. Sidner, Tanja Schultz, Matthew Stone, and ChengXiang Zhai, editors, HLT-NAACL, pages 139–146. The Association for Computational Linguistics, 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Jurafsky</author>
<author>James H Martin</author>
</authors>
<date>2008</date>
<booktitle>Speech and Language Processing (2nd Edition) (Prentice Hall Series in Artificial Intelligence).</booktitle>
<volume>2</volume>
<publisher>Prentice Hall,</publisher>
<note>edition,</note>
<contexts>
<context position="2246" citStr="Jurafsky and Martin, 2008" startWordPosition="340" endWordPosition="344">cus is on building the right grammar rules from scratch. We take the latter approach. Unsupervised structure learning of (P)CFGs is a notoriously difficult task (de la Higuera, 2010; Clark and Lappin, 2010), with theoretical results showing that in general it is either impossible to achieve (Gold, 1967; de la Higuera, 1997) or requires impractical resources (Horning, 1969; Yang, 2012). At the same time, it is well known that context-free structures are needed for better language parsing and modelling, since less expressive models (such as HMMs) are not good enough (Manning and Sch¨utze, 2001; Jurafsky and Martin, 2008). Moreover, the trend is towards unsupervised (rather than supervised) learning methods due to the lack in most languages of annotated data and the applicability in wider domains (Merlo et al., 2010). Thus, despite its difficulty, unsupervised PCFG grammar induction (or induction of other similarly expressive models) is still an important task in computational linguistics. In this paper, we describe a new algorithm for PCFG induction based on a principled approach and capable of inducing accurate yet compact grammars. Moreover, this algorithm can work on large grammars and datasets and infers </context>
</contexts>
<marker>Jurafsky, Martin, 2008</marker>
<rawString>Daniel Jurafsky and James H. Martin. Speech and Language Processing (2nd Edition) (Prentice Hall Series in Artificial Intelligence). Prentice Hall, 2 edition, 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanjeev Khanna</author>
<author>Madhu Sudan</author>
<author>Luca Trevisan</author>
<author>David P Williamson</author>
</authors>
<title>The Approximability of Constraint Satisfaction Problems.</title>
<date>2000</date>
<journal>SIAM J. Comput.,</journal>
<volume>30</volume>
<issue>6</issue>
<contexts>
<context position="15259" citStr="Khanna et al., 2000" startWordPosition="2555" endWordPosition="2558">w this approach. We propose a new method for tackling this problem. We show that all the possible SC-CFGs in CNF consistent with the congruence classes directly correspond to all the solutions of a boolean formula built upon the congruence classes, where the variables of this formula correspond to nonterminals (and, with some minor adjustments, production rules as well). Thus, finding the smallest possible grammar directly translates into finding a solution which has the smallest possible amount of true variables. Finding a minimal solution for this type of formula is a known NP-Hard problem (Khanna et al., 2000). However, sophisticated linear programming solvers (Berkelaar et al., 2008) can take care of this problem. For small examples (e.g. all the examples in Table 1), these solvers are able to find an exact solution in a few seconds. Moreover, these solvers are capable of finding good approximate solutions to larger formulas containing a few million variables. The formula contains one variable per congruence class. All variables corresponding to congruence classes containing strings from the sample are assigned the value True (since there must be a starting non-terminal that generates these string</context>
</contexts>
<marker>Khanna, Sudan, Trevisan, Williamson, 2000</marker>
<rawString>Sanjeev Khanna, Madhu Sudan, Luca Trevisan, and David P. Williamson. The Approximability of Constraint Satisfaction Problems. SIAM J. Comput., 30(6):1863–1920, 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
</authors>
<title>The Unsupervised Learning of Natural Language Structure. PhD thesis,</title>
<date>2004</date>
<institution>Stanford University,</institution>
<contexts>
<context position="13698" citStr="Klein, 2004" startWordPosition="2296" endWordPosition="2297">lization). For natural language examples, one would expect that a considerable number of sentences are grouped into the same class because of their similar structure. Obviously, one can make use of only one of these conditions by assigning the other a parameter value which makes it trivially true from the outset (0 for d and |Subs |for i). 3.2 Building the Context-Free Grammar Deciding which substrings are constituents (in our case, this translates into choosing which congruence classes correspond to non-terminals) is a problematic issue and is considered a harder task than the previous step (Klein, 2004). A path followed by a number of authors consists in using an Ockham’s razor or Minimal Description Length principle approach (Stolcke, 1994; Clark, 2001; Petasis et al., 2004). This generally leads to choosing as best hypothesis the one which best compresses the data. Applying this principle in our case would mean that the non-terminals should be 1for example, if a congruence class contains the phrases ”the big” and ”that small”, and another class contains ”dog barked” and ”cat meowed”, it can be logically deduced that the phrases ”the big dog barked”,”the big cat meowed”, ”that small dog bar</context>
<context position="27315" citStr="Klein, 2004" startWordPosition="4717" endWordPosition="4718">e to Pearson’s χ2 test. In a first experiment (vaguely similar to the one done by Luque and L´opez (2010)), we constructed the best possible SC-CFG consistent with the merges done in the first phase and assigned probabilities to this grammar using Inside-Outside. In other words, we ran the second phase of our system in a supervised fashion by using the treebank to decide which are the best congruence classes to choose as non-terminals. The CNF grammar we obtained from this experiment (COMINO-UBOUND) gives very good parsing results which outperform results from state-of-theart systems DMV+CCM (Klein, 2004), U-DOP (Bod, 2006a), UML-DOP (Bod, 2006b) and Incremental (Seginer, 2007) as shown in Table 2. Moreover, the results obtained are very close to the best results one can ever hope to obtain from any CNF grammar on WSJ10 (CNF-UBOUND) (Klein, 2004). However, the grammar we obtain does not generalise enough and does not describe a good language model. In a second experiment, we ran the complete COMINO system. The grammar obtained from this experiment did not give competitive parsing results. The first experiment shows that the merge decisions taken in the first phase do not hinder the possibility</context>
</contexts>
<marker>Klein, 2004</marker>
<rawString>Dan Klein. The Unsupervised Learning of Natural Language Structure. PhD thesis, Stanford University, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin J Lang</author>
<author>Barak A Pearlmutter</author>
<author>Rodney A Price</author>
</authors>
<title>Results of the Abbadingo One DFA Learning Competition and a New Evidence-Driven State Merging Algorithm.</title>
<date>1998</date>
<booktitle>In Vasant Honavar and Giora Slutzki,</booktitle>
<volume>1433</volume>
<pages>1--12</pages>
<editor>editors, ICGI,</editor>
<publisher>Springer,</publisher>
<contexts>
<context position="12484" citStr="Lang et al., 1998" startWordPosition="2097" endWordPosition="2100"> other merges are logically deduced so as to ensure that the relation re4 5 6 7 8 9 10 1355 mains a congruence1. In practice, the vast majority of the merges undertaken are logically deduced ones. This clearly relieves the algorithm from taking unnecessary decisions (thus reducing the chance of erroneous decisions). On the downside, one bad merge can have a disastrous ripple effect. Thus, to minimize as much as possible the chance of this happening, every merge undertaken is the best possible one at that point in time (w.r.t. the distance function used). The same idea is used in DFA learning (Lang et al., 1998). This process is repeated until either 1) no pairs of frequent congruence classes are left to merge (line 5) or 2) the smallest distance between the candidate pairs is bigger or equal to a pre-defined threshold d and the number of congruence classes containing strings from the sample is smaller or equal to a pre-defined threshold i (line 9). The first condition of point 2 ensures that congruence classes which are sufficiently close to each other are merged together. The second condition of point 2 ensures that the hypothesized congruence classes are generalized enough (i.e. to avoid undergene</context>
</contexts>
<marker>Lang, Pearlmutter, Price, 1998</marker>
<rawString>Kevin J. Lang, Barak A. Pearlmutter, and Rodney A. Price. Results of the Abbadingo One DFA Learning Competition and a New Evidence-Driven State Merging Algorithm. In Vasant Honavar and Giora Slutzki, editors, ICGI, volume 1433 of Lecture Notes in Computer Science, pages 1–12. Springer, 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pat Langley</author>
<author>Sean Stromsten</author>
</authors>
<title>Learning Context-Free Grammars with a Simplicity Bias.</title>
<date>2000</date>
<booktitle>In Ramon L´opez de M´antaras and Enric Plaza,</booktitle>
<volume>1810</volume>
<pages>220--228</pages>
<editor>editors, ECML,</editor>
<publisher>Springer,</publisher>
<contexts>
<context position="20805" citStr="Langley and Stromsten, 2000" startWordPosition="3577" endWordPosition="3580">s, are in fact always constituents. This assumption does not hold for ambiguous grammars in our class. The approach we take to solve the smallest grammar problem can be extended to other classes of grammars. A similar formula can be built for grammars whose non-terminals have a one-to-one correspondence with congruence classes containing features of their language (Clark, 2010b). 1357 4 Experiments and Discussion 4.1 Experiments on Artificial Data We tested our system on 11 typical context-free languages and 9 artificial natural language grammars taken from 4 different sources (Stolcke, 1994; Langley and Stromsten, 2000; Adriaans et al., 2000; Solan et al., 2005). The 11 CFLs include 7 described by unambiguous grammars: UC1: a&apos;b&apos; UC2: a&apos;b&apos;c&apos;d&apos; UC3: a&apos;b&apos; n &gt; m UC4: a�b�, p =� q UC5: Palindromes over alphabet {a, b} with a central marker UC6:Palindromes over alphabet {a, b} without a central marker UC7: Lukasiewicz language (S -* aSS|b) and 4 described by ambiguous grammars: AC1: |w|a = |w|b AC2: 2|w|a = |w|b AC3: Dyck language AC4: Regular expressions. The 9 artificial natural language grammars are: NL1: Grammar ’a’, Table 2 in (Langley and Stromsten, 2000) NL2: Grammar ’b’, Table 2 in (Langley and Stromsten,</context>
</contexts>
<marker>Langley, Stromsten, 2000</marker>
<rawString>Pat Langley and Sean Stromsten. Learning Context-Free Grammars with a Simplicity Bias. In Ramon L´opez de M´antaras and Enric Plaza, editors, ECML, volume 1810 of Lecture Notes in Computer Science, pages 220–228. Springer, 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karim Lari</author>
<author>Steve J Young</author>
</authors>
<title>The Estimation of Stochastic Context-Free Grammars using the Inside-Outside Algorithm.</title>
<date>1990</date>
<journal>Computer Speech &amp; Language,</journal>
<volume>4</volume>
<issue>1</issue>
<contexts>
<context position="1589" citStr="Lari and Young, 1990" startWordPosition="236" endWordPosition="239"> our algorithm are, in theory, capable of modelling natural language. One of our experiments shows that our algorithm can potentially outperform the state-of-the-art in unsupervised parsing on the WSJ10 corpus. 1 Introduction The task of unsupervised induction of PCFGs has attracted a lot of attention in the field of computational linguistics. This task can take the form of either parameter search or structure learning. In parameter search, a CFG is fixed and the focus is on assigning probabilities to this grammar using Bayesian methods (Johnson et al., 2007) or maximum likelihood estimation (Lari and Young, 1990). In structure learning, the focus is on building the right grammar rules from scratch. We take the latter approach. Unsupervised structure learning of (P)CFGs is a notoriously difficult task (de la Higuera, 2010; Clark and Lappin, 2010), with theoretical results showing that in general it is either impossible to achieve (Gold, 1967; de la Higuera, 1997) or requires impractical resources (Horning, 1969; Yang, 2012). At the same time, it is well known that context-free structures are needed for better language parsing and modelling, since less expressive models (such as HMMs) are not good enoug</context>
<context position="9081" citStr="Lari and Young, 1990" startWordPosition="1480" endWordPosition="1483">l the substrings of S 2. Selecting which of the induced classes are non-terminals and subsequently building a CFG. 3. Assigning probabilities to the induced CFG. The approach we take is very different from traditional grammar induction approaches, in which grouping of substitutable substrings is done incrementally as the same groups are chosen to represent non-terminals. We separate these two task so that learning takes place in the grouping phase whilst selection of non-terminals is done independently by solving a combinatorial problem. For the last step, the standard EM-algorithm for PCFGs (Lari and Young, 1990) is used. In Sections 3.1 and 3.2, the first and second steps of the algorithm are described in detail. We analyse our algorithm in Section 3.3. 3.1 Inducing the Congruence Classes We describe in Algorithm 1 how the congruence classes are induced. Algorithm 1: Learn Congruence Classes Input: A multiset S; parameters: n, d, i; distance function dist on local contexts of size k Output: The congruence classes CC over the substrings of S 1 Subs ← Set of all substrings of S ; 2 CC ← {{w} |w E Subs} ; 3 while True do Pairs ← {(x, y) |x,y ECC, x =� y, |S|x &gt; n , |S|y &gt; n} ; if |Pairs |= 0 then exitlo</context>
</contexts>
<marker>Lari, Young, 1990</marker>
<rawString>Karim Lari and Steve J. Young. The Estimation of Stochastic Context-Free Grammars using the Inside-Outside Algorithm. Computer Speech &amp; Language, 4(1):35 – 56, 1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franco M Luque</author>
<author>Gabriel G Infante L´opez</author>
</authors>
<title>Bounding the Maximal Parsing Performance of Non-Terminally Separated Grammars.</title>
<date>2010</date>
<booktitle>In Sempere and Garcia</booktitle>
<pages>135--147</pages>
<marker>Luque, L´opez, 2010</marker>
<rawString>Franco M. Luque and Gabriel G. Infante L´opez. Bounding the Maximal Parsing Performance of Non-Terminally Separated Grammars. In Sempere and Garcia (2010), pages 135–147.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Foundations of Statistical Natural Language Processing.</title>
<date>2001</date>
<publisher>MIT Press,</publisher>
<marker>Manning, Sch¨utze, 2001</marker>
<rawString>Christopher D. Manning and Hinrich Sch¨utze. Foundations of Statistical Natural Language Processing. MIT Press, 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paola Merlo</author>
<author>Harry Bunt</author>
<author>Joakim Nivre</author>
</authors>
<title>Current Trends in Parsing Technology.</title>
<date>2010</date>
<booktitle>In Trends in Parsing Technology,</booktitle>
<pages>1--17</pages>
<publisher>Springer,</publisher>
<contexts>
<context position="2445" citStr="Merlo et al., 2010" startWordPosition="374" endWordPosition="377">, with theoretical results showing that in general it is either impossible to achieve (Gold, 1967; de la Higuera, 1997) or requires impractical resources (Horning, 1969; Yang, 2012). At the same time, it is well known that context-free structures are needed for better language parsing and modelling, since less expressive models (such as HMMs) are not good enough (Manning and Sch¨utze, 2001; Jurafsky and Martin, 2008). Moreover, the trend is towards unsupervised (rather than supervised) learning methods due to the lack in most languages of annotated data and the applicability in wider domains (Merlo et al., 2010). Thus, despite its difficulty, unsupervised PCFG grammar induction (or induction of other similarly expressive models) is still an important task in computational linguistics. In this paper, we describe a new algorithm for PCFG induction based on a principled approach and capable of inducing accurate yet compact grammars. Moreover, this algorithm can work on large grammars and datasets and infers correctly even from small samples. We show that our algorithm is capable of achieving competitive results in both unsupervised parsing and language modelling of typical context-free languages and art</context>
</contexts>
<marker>Merlo, Bunt, Nivre, 2010</marker>
<rawString>Paola Merlo, Harry Bunt, and Joakim Nivre. Current Trends in Parsing Technology. In Trends in Parsing Technology, pages 1–17. Springer, 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Georgios Petasis</author>
<author>Georgios Paliouras</author>
<author>Constantine D Spyropoulos</author>
</authors>
<title>and Constantine Halatsis. eg-GRIDS: Context-Free Grammatical Inference from Positive Examples Using Genetic Search.</title>
<date>2004</date>
<booktitle>In Georgios Paliouras and Yasubumi Sakakibara,</booktitle>
<volume>3264</volume>
<pages>223--234</pages>
<editor>editors, ICGI,</editor>
<publisher>Springer,</publisher>
<contexts>
<context position="13874" citStr="Petasis et al., 2004" startWordPosition="2323" endWordPosition="2326">Obviously, one can make use of only one of these conditions by assigning the other a parameter value which makes it trivially true from the outset (0 for d and |Subs |for i). 3.2 Building the Context-Free Grammar Deciding which substrings are constituents (in our case, this translates into choosing which congruence classes correspond to non-terminals) is a problematic issue and is considered a harder task than the previous step (Klein, 2004). A path followed by a number of authors consists in using an Ockham’s razor or Minimal Description Length principle approach (Stolcke, 1994; Clark, 2001; Petasis et al., 2004). This generally leads to choosing as best hypothesis the one which best compresses the data. Applying this principle in our case would mean that the non-terminals should be 1for example, if a congruence class contains the phrases ”the big” and ”that small”, and another class contains ”dog barked” and ”cat meowed”, it can be logically deduced that the phrases ”the big dog barked”,”the big cat meowed”, ”that small dog barked” and ”that small cat meowed” should be in the same class. assigned in such a way that the grammar built is the smallest possible one (in terms of the number of non-terminal</context>
</contexts>
<marker>Petasis, Paliouras, Spyropoulos, 2004</marker>
<rawString>Georgios Petasis, Georgios Paliouras, Constantine D. Spyropoulos, and Constantine Halatsis. eg-GRIDS: Context-Free Grammatical Inference from Positive Examples Using Genetic Search. In Georgios Paliouras and Yasubumi Sakakibara, editors, ICGI, volume 3264 of Lecture Notes in Computer Science, pages 223– 234. Springer, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gisa Rauh</author>
</authors>
<title>Syntactic Categories: Their Identification and Description in Linguistic Theories.</title>
<date>2010</date>
<booktitle>Oxford Surveys in Syntax &amp; Morphology No.7. OUP</booktitle>
<location>Oxford,</location>
<contexts>
<context position="7656" citStr="Rauh (2010)" startWordPosition="1267" endWordPosition="1268">-PCFGs), defined as all the PCFGs G which, for any non-terminal A, if u E L(A) then L(A) = [u]—=(L(G),φ). In other words, the nonterminals (i.e. syntactic categories in natural language) of these grammars directly correspond to classes of substitutable strings (i.e. substitutable words and phrases in NL). One may ask whether this is too strict a restriction for natural language grammars. We argue that it is not, for the following reasons. First of all, this restriction complies with the approach taken by American structural linguists for the identification of syntactic categories, as shown by Rauh (2010): ”[Zellig and Fries] identified syntactic categories as distribution classes, employing substitution tests and excluding semantic properties of the items analysed. Both describe syntactic categories exclusively on the basis of their syntactic environments and independently of any inherent properties of the members of these categories”. Secondly, we know that such grammars are capable of describing languages generated by grammars that contain typical natural language grammatical structures (see Section 4.1; artificial natural language grammars NL1-NL7, taken from various sources, generate lang</context>
</contexts>
<marker>Rauh, 2010</marker>
<rawString>Gisa Rauh. Syntactic Categories: Their Identification and Description in Linguistic Theories. Oxford Surveys in Syntax &amp; Morphology No.7. OUP Oxford, 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Seginer</author>
</authors>
<title>Fast Unsupervised Incremental Parsing. In</title>
<date>2007</date>
<booktitle>The Association for Computational Linguistics,</booktitle>
<editor>John A. Carroll, Antal van den Bosch, and Annie Zaenen, editors, ACL.</editor>
<contexts>
<context position="27389" citStr="Seginer, 2007" startWordPosition="4728" endWordPosition="4729">e done by Luque and L´opez (2010)), we constructed the best possible SC-CFG consistent with the merges done in the first phase and assigned probabilities to this grammar using Inside-Outside. In other words, we ran the second phase of our system in a supervised fashion by using the treebank to decide which are the best congruence classes to choose as non-terminals. The CNF grammar we obtained from this experiment (COMINO-UBOUND) gives very good parsing results which outperform results from state-of-theart systems DMV+CCM (Klein, 2004), U-DOP (Bod, 2006a), UML-DOP (Bod, 2006b) and Incremental (Seginer, 2007) as shown in Table 2. Moreover, the results obtained are very close to the best results one can ever hope to obtain from any CNF grammar on WSJ10 (CNF-UBOUND) (Klein, 2004). However, the grammar we obtain does not generalise enough and does not describe a good language model. In a second experiment, we ran the complete COMINO system. The grammar obtained from this experiment did not give competitive parsing results. The first experiment shows that the merge decisions taken in the first phase do not hinder the possibility of finding a very good grammar for parsing. This means that the merge dec</context>
</contexts>
<marker>Seginer, 2007</marker>
<rawString>Yoav Seginer. Fast Unsupervised Incremental Parsing. In John A. Carroll, Antal van den Bosch, and Annie Zaenen, editors, ACL. The Association for Computational Linguistics, 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Jos´e</author>
</authors>
<title>Sempere and Pedro Garcia, editors. Grammatical Inference: Theoretical Results and Applications,</title>
<date>2010</date>
<booktitle>10th International Colloquium, ICGI2010,</booktitle>
<volume>6339</volume>
<publisher>Springer.</publisher>
<location>Valencia, Spain,</location>
<marker>Jos´e, 2010</marker>
<rawString>Jos´e M. Sempere and Pedro Garcia, editors. Grammatical Inference: Theoretical Results and Applications, 10th International Colloquium, ICGI2010, Valencia, Spain, September 13-16, 2010. Proceedings, volume 6339 of Lecture Notes in Computer Science, 2010. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zach Solan</author>
<author>David Horn</author>
<author>Eytan Ruppin</author>
<author>Shimon Edelman</author>
</authors>
<title>Unsupervised learning of natural languages.</title>
<date>2005</date>
<booktitle>Proceedings of the National Academy of Sciences of the United States of America,</booktitle>
<volume>102</volume>
<issue>33</issue>
<contexts>
<context position="20849" citStr="Solan et al., 2005" startWordPosition="3585" endWordPosition="3588">does not hold for ambiguous grammars in our class. The approach we take to solve the smallest grammar problem can be extended to other classes of grammars. A similar formula can be built for grammars whose non-terminals have a one-to-one correspondence with congruence classes containing features of their language (Clark, 2010b). 1357 4 Experiments and Discussion 4.1 Experiments on Artificial Data We tested our system on 11 typical context-free languages and 9 artificial natural language grammars taken from 4 different sources (Stolcke, 1994; Langley and Stromsten, 2000; Adriaans et al., 2000; Solan et al., 2005). The 11 CFLs include 7 described by unambiguous grammars: UC1: a&apos;b&apos; UC2: a&apos;b&apos;c&apos;d&apos; UC3: a&apos;b&apos; n &gt; m UC4: a�b�, p =� q UC5: Palindromes over alphabet {a, b} with a central marker UC6:Palindromes over alphabet {a, b} without a central marker UC7: Lukasiewicz language (S -* aSS|b) and 4 described by ambiguous grammars: AC1: |w|a = |w|b AC2: 2|w|a = |w|b AC3: Dyck language AC4: Regular expressions. The 9 artificial natural language grammars are: NL1: Grammar ’a’, Table 2 in (Langley and Stromsten, 2000) NL2: Grammar ’b’, Table 2 in (Langley and Stromsten, 2000) NL3: Lexical categories and constitue</context>
<context position="24523" citStr="Solan et al., 2005" startWordPosition="4259" endWordPosition="4262"> strings were compared to results obtained by ABL (van Zaanen, 2001), which is a system whose primary aim is that of finding good parse trees (rather than identifying the target language). Although ABL does not obtain state-of-the-art results on natural language corpora, it proved to be the best system (for which an implementation is readily available) for unsupervised parsing of sentences generated by artificial grammars. Results are shown in Table 1. We calculated the relative entropy on a test set of one million strings generated from the target grammar. We compared our results with ADIOS (Solan et al., 2005), a system which obtains competitive results on language modelling (Waterfall et al., 2010) and whose primary aim is of correctly identifying the target language (rather than finding good parse trees). Results are shown in Table 1. For the tests in the first section of Table 1 (i.e. above the first dashed line), our algorithm was capable of exactly identifying the structure of the target grammar. Notwithstanding this, the bracketing results for these tests did not always yield perfect scores. This happened whenever the target grammar was ambiguous, in which case the most probable parse trees o</context>
</contexts>
<marker>Solan, Horn, Ruppin, Edelman, 2005</marker>
<rawString>Zach Solan, David Horn, Eytan Ruppin, and Shimon Edelman. Unsupervised learning of natural languages. Proceedings of the National Academy of Sciences of the United States of America, 102(33):11629–11634, 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>Bayesian learning ofprobabilistic language models.</title>
<date>1994</date>
<tech>PhD thesis,</tech>
<institution>University of California, Berkeley,</institution>
<contexts>
<context position="13838" citStr="Stolcke, 1994" startWordPosition="2319" endWordPosition="2320">of their similar structure. Obviously, one can make use of only one of these conditions by assigning the other a parameter value which makes it trivially true from the outset (0 for d and |Subs |for i). 3.2 Building the Context-Free Grammar Deciding which substrings are constituents (in our case, this translates into choosing which congruence classes correspond to non-terminals) is a problematic issue and is considered a harder task than the previous step (Klein, 2004). A path followed by a number of authors consists in using an Ockham’s razor or Minimal Description Length principle approach (Stolcke, 1994; Clark, 2001; Petasis et al., 2004). This generally leads to choosing as best hypothesis the one which best compresses the data. Applying this principle in our case would mean that the non-terminals should be 1for example, if a congruence class contains the phrases ”the big” and ”that small”, and another class contains ”dog barked” and ”cat meowed”, it can be logically deduced that the phrases ”the big dog barked”,”the big cat meowed”, ”that small dog barked” and ”that small cat meowed” should be in the same class. assigned in such a way that the grammar built is the smallest possible one (in</context>
<context position="20776" citStr="Stolcke, 1994" startWordPosition="3575" endWordPosition="3576">nce constituents, are in fact always constituents. This assumption does not hold for ambiguous grammars in our class. The approach we take to solve the smallest grammar problem can be extended to other classes of grammars. A similar formula can be built for grammars whose non-terminals have a one-to-one correspondence with congruence classes containing features of their language (Clark, 2010b). 1357 4 Experiments and Discussion 4.1 Experiments on Artificial Data We tested our system on 11 typical context-free languages and 9 artificial natural language grammars taken from 4 different sources (Stolcke, 1994; Langley and Stromsten, 2000; Adriaans et al., 2000; Solan et al., 2005). The 11 CFLs include 7 described by unambiguous grammars: UC1: a&apos;b&apos; UC2: a&apos;b&apos;c&apos;d&apos; UC3: a&apos;b&apos; n &gt; m UC4: a�b�, p =� q UC5: Palindromes over alphabet {a, b} with a central marker UC6:Palindromes over alphabet {a, b} without a central marker UC7: Lukasiewicz language (S -* aSS|b) and 4 described by ambiguous grammars: AC1: |w|a = |w|b AC2: 2|w|a = |w|b AC3: Dyck language AC4: Regular expressions. The 9 artificial natural language grammars are: NL1: Grammar ’a’, Table 2 in (Langley and Stromsten, 2000) NL2: Grammar ’b’, Table</context>
</contexts>
<marker>Stolcke, 1994</marker>
<rawString>Andreas Stolcke. Bayesian learning ofprobabilistic language models. PhD thesis, University of California, Berkeley, 1994.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Menno van Zaanen</author>
</authors>
<title>Bootstrapping Structure into Language: Alignment-Based Learning.</title>
<date>2001</date>
<tech>PhD thesis,</tech>
<institution>University of Leeds,</institution>
<marker>van Zaanen, 2001</marker>
<rawString>Menno van Zaanen. Bootstrapping Structure into Language: Alignment-Based Learning. PhD thesis, University of Leeds, 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Menno van Zaanen</author>
<author>Jeroen Geertzen</author>
</authors>
<title>Problems with Evaluation of Unsupervised Empirical Grammatical Inference Systems.</title>
<date>2008</date>
<booktitle>In Clark</booktitle>
<pages>301--303</pages>
<marker>van Zaanen, Geertzen, 2008</marker>
<rawString>Menno van Zaanen and Jeroen Geertzen. Problems with Evaluation of Unsupervised Empirical Grammatical Inference Systems. In Clark et al. (2008), pages 301–303.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heidi R Waterfall</author>
<author>Ben Sandbank</author>
<author>Luca Onnis</author>
<author>Shimon Edelman</author>
</authors>
<title>An Empirical Generative Framework for Computational Modeling of Language Acquisition.</title>
<date>2010</date>
<journal>Journal of Child Language,</journal>
<volume>37</volume>
<contexts>
<context position="24614" citStr="Waterfall et al., 2010" startWordPosition="4273" endWordPosition="4276"> whose primary aim is that of finding good parse trees (rather than identifying the target language). Although ABL does not obtain state-of-the-art results on natural language corpora, it proved to be the best system (for which an implementation is readily available) for unsupervised parsing of sentences generated by artificial grammars. Results are shown in Table 1. We calculated the relative entropy on a test set of one million strings generated from the target grammar. We compared our results with ADIOS (Solan et al., 2005), a system which obtains competitive results on language modelling (Waterfall et al., 2010) and whose primary aim is of correctly identifying the target language (rather than finding good parse trees). Results are shown in Table 1. For the tests in the first section of Table 1 (i.e. above the first dashed line), our algorithm was capable of exactly identifying the structure of the target grammar. Notwithstanding this, the bracketing results for these tests did not always yield perfect scores. This happened whenever the target grammar was ambiguous, in which case the most probable parse trees of the target and learned grammar can be different, thus leading to incorrect bracketing. Fo</context>
</contexts>
<marker>Waterfall, Sandbank, Onnis, Edelman, 2010</marker>
<rawString>Heidi R. Waterfall, Ben Sandbank, Luca Onnis, and Shimon Edelman. An Empirical Generative Framework for Computational Modeling of Language Acquisition. Journal of Child Language, 37:671–703, 6 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles S Wetherell</author>
</authors>
<title>Probabilistic Languages: A Review and Some Open Questions.</title>
<date>1980</date>
<journal>ACM Comput. Surv.,</journal>
<volume>12</volume>
<issue>4</issue>
<contexts>
<context position="4605" citStr="Wetherell, 1980" startWordPosition="738" endWordPosition="739">alue assigned to every rule and every starting non-terminal. The probability of a leftmost derivation from a PCFG is the product of the starting non-terminal probability and the production probabilities used in the derivation. The probability of a string generated by a PCFG is the sum of all its leftmost derivations’ probabilities. The stochastic language generated from a PCFG G is (L(G), φG), where φG is the distribution over Σ* defined by the probabilities assigned to the strings by G. For a PCFG to be consistent, the probabilities of the strings in its stochastic language must add up to 1 (Wetherell, 1980). Any PCFG mentioned from now onwards is assumed to be consistent. 2.2 Congruence Relations A congruence relation — on Σ* is any equivalence relation on Σ* that respects the following condition: if u — v and x — y then ux — vy. The congruence classes of a congruence relation are simply its equivalence classes. The congruence class of w E Σ* w.r.t. a congruence relation — is denoted by [w]—. The set of contexts of a substring w with respect to a language L, denoted Con(w, L), is {(l, r) E Σ* x Σ* |lwr E L}. Two strings u and v are syntactically congruent with respect to L, written u =L v, if Co</context>
</contexts>
<marker>Wetherell, 1980</marker>
<rawString>Charles S. Wetherell. Probabilistic Languages: A Review and Some Open Questions. ACM Comput. Surv., 12(4):361–379, 1980.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Yang</author>
</authors>
<title>Computational Models of Syntactic Acquisition.</title>
<date>2012</date>
<journal>Wiley Interdisciplinary Reviews: Cognitive Science,</journal>
<volume>3</volume>
<issue>2</issue>
<contexts>
<context position="2007" citStr="Yang, 2012" startWordPosition="303" endWordPosition="304">arameter search, a CFG is fixed and the focus is on assigning probabilities to this grammar using Bayesian methods (Johnson et al., 2007) or maximum likelihood estimation (Lari and Young, 1990). In structure learning, the focus is on building the right grammar rules from scratch. We take the latter approach. Unsupervised structure learning of (P)CFGs is a notoriously difficult task (de la Higuera, 2010; Clark and Lappin, 2010), with theoretical results showing that in general it is either impossible to achieve (Gold, 1967; de la Higuera, 1997) or requires impractical resources (Horning, 1969; Yang, 2012). At the same time, it is well known that context-free structures are needed for better language parsing and modelling, since less expressive models (such as HMMs) are not good enough (Manning and Sch¨utze, 2001; Jurafsky and Martin, 2008). Moreover, the trend is towards unsupervised (rather than supervised) learning methods due to the lack in most languages of annotated data and the applicability in wider domains (Merlo et al., 2010). Thus, despite its difficulty, unsupervised PCFG grammar induction (or induction of other similarly expressive models) is still an important task in computationa</context>
</contexts>
<marker>Yang, 2012</marker>
<rawString>Charles Yang. Computational Models of Syntactic Acquisition. Wiley Interdisciplinary Reviews: Cognitive Science, 3(2):205–213, 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryo Yoshinaka</author>
</authors>
<title>Identification in the Limit of k, lSubstitutable Context-Free Languages. In Clark et al.</title>
<date>2008</date>
<pages>266--279</pages>
<contexts>
<context position="6662" citStr="Yoshinaka, 2008" startWordPosition="1099" endWordPosition="1100">tween congruence classes and the non-terminals, their learnability is reduced to that of finding the correct congruence classes (Clark, 2010a). This class of grammars is closely related to the class of NTS-grammars (Boasson and S´enizergues, 1985). Any C-CFG is an NTSgrammar but not vice-versa. However, it is not known whether languages generated by C-CFGs are all NTS-languages (Clark, 2010a). Note that NTS-languages are a subclass of deterministic context-free languages and contain the regular languages, the substitutable (Clark and Eyraud, 2007) and k-l-substitutable context-free languages (Yoshinaka, 2008), the very simple languages and other CFLs such as the Dyck language (Boasson and S´enizergues, 1985). We define a slightly more restrictive class of grammars, which we shall call Strongly Congruential CFGs (SC-CFGs). A CFG G is a SCCFG if, for any non-terminal A, if u E L(A) then L(A) = [u]_L(G). The probabilistic equivalent of this is the class of Strongly Congruential PCFGs (SC-PCFGs), defined as all the PCFGs G which, for any non-terminal A, if u E L(A) then L(A) = [u]—=(L(G),φ). In other words, the nonterminals (i.e. syntactic categories in natural language) of these grammars directly cor</context>
</contexts>
<marker>Yoshinaka, 2008</marker>
<rawString>Ryo Yoshinaka. Identification in the Limit of k, lSubstitutable Context-Free Languages. In Clark et al. (2008), pages 266–279.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
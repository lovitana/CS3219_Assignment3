<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000196">
<title confidence="0.986942">
+/-EffectWordNet: Sense-level Lexicon Acquisition for Opinion Inference
</title>
<author confidence="0.996408">
Yoonjung Choi and Janyce Wiebe
</author>
<affiliation confidence="0.9992235">
Department of Computer Science
University of Pittsburgh
</affiliation>
<email confidence="0.989721">
yjchoi, wiebe@cs.pitt.edu
</email>
<sectionHeader confidence="0.997236" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99978795">
Recently, work in NLP was initiated on a
type of opinion inference that arises when
opinions are expressed toward events
which have positive or negative effects
on entities (+/-effect events). This paper
addresses methods for creating a lexicon
of such events, to support such work on
opinion inference. Due to significant
sense ambiguity, our goal is to develop a
sense-level rather than word-level lexicon.
To maximize the effectiveness of different
types of information, we combine a
graph-based method using WordNet1
relations and a standard classifier using
gloss information. A hybrid between the
two gives the best results. Further, we
provide evidence that the model is an
effective way to guide manual annotation
to find +/-effect senses that are not in the
seed set.
</bodyText>
<sectionHeader confidence="0.999516" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999817538461539">
Opinion mining (or sentiment analysis) identifies
positive or negative opinions in many kinds of
texts such as reviews, blogs, and news articles. It
has been exploited in many application areas such
as review mining, election analysis, and infor-
mation extraction. While most previous research
focusses on explicit opinion expressions, recent
work addresses a type of opinion inference that
arises when opinions are expressed toward events
which have positive or negative effects on enti-
ties (Deng et al., 2013; Deng and Wiebe, 2014).
We call such events +/-effect events.2 Deng and
Wiebe (2014) show how sentiments toward one
</bodyText>
<footnote confidence="0.99927875">
1WordNet 3.0, http://wordnet.princeton.edu/
2While the term goodFor/badFor is used in previous pa-
pers (Deng et al., 2013; Deng and Wiebe, 2014; Deng et al.,
2014), we have since decided that +/-effect is a better term.
</footnote>
<listItem confidence="0.5430774">
entity may be propagated to other entities via
opinion inference rules. They give the following
example:
(1) The bill would curb skyrocketing
health care costs.
</listItem>
<bodyText confidence="0.999963942857143">
The writer expresses an explicit negative senti-
ment (by skyrocketing) toward the object (health
care costs). The event, curb, has a negative effect
on costs, since they are reduced. We can reason
that the writer is positive toward the event because
it has a negative effect on costs, toward which the
writer is negative. From there, we can reason that
the writer is positive toward the bill, since it is
the agent of the positive event. Deng and Wiebe
(2014) show that such inferences may be exploited
to significantly improve explicit sentiment analy-
sis systems.
However, to achieve its results, the system de-
veloped by Deng and Wiebe (2014) requires that
all instances of +/-effect events in the corpus be
manually provided as input. For the system to
be fully automatic, it needs to be able to recog-
nize +/-effect events automatically. This paper
addresses methods for creating lexicons of such
events, to support such work on opinion inference.
We have discovered that there is significant sense
ambiguity, meaning that words often have mix-
tures of senses among the classes +effect, -effect,
and Null. Thus, we develop a sense-level rather
than word-level lexicon.
One of our goals is to investigate whether
the +/-effect property tends to be shared among
semantically-related senses, and another is to
use a method that applies to all word senses, not
just to the senses of words in a given word-level
lexicon. Thus, we build a graph-based model in
which each node is a WordNet sense, and edges
represent semantic WordNet relations between
senses. In addition, we hypothesized that glosses
also contain useful information. Thus, we develop
</bodyText>
<page confidence="0.939973">
1181
</page>
<note confidence="0.897495">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1181–1191,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999695">
a supervised gloss classifier and define a hybrid
model which gives the best overall performance.
Finally, because all WordNet verb senses are
incorporated into the model, we investigate the
ability of the method to identify unlabeled senses
that are likely to be +/-effect senses. We find that
by iteratively labeling the top-weighted unlabeled
senses and rerunning the model, it may be used as
an effective method for guiding annotation efforts.
</bodyText>
<sectionHeader confidence="0.995578" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.99211890625">
There are many varieties of +/-effect events, in-
cluding creation/destruction (changes in states in-
volving existence), gain/loss (changes in states
involving possession), and benefit/injury (Anand
and Reschke, 2010; Deng et al., 2013). The cre-
ation, gain, and benefit classes are +effect events.
For example, baking a cake has a positive effect on
the cake because it is created;3 increasing the tax
rate has a positive effect on the tax rate; and com-
forting the child has a positive effect on the child.
The antonymous classes of each are -effect events:
destroying the building has a negative effect on the
building; demand decreasing has a negative effect
on demand; and killing Bill has a negative effect
on Bill.4
While sentiment (Esuli and Sebastiani, 2006;
Wilson et al., 2005; Su and Markert, 2009) and
connotation lexicons (Feng et al., 2011; Kang et
al., 2014) are related, sentiment, connotation, and
+/-effects are not the same; a single event may
have different sentiment and +/-effect polarities,
for example. Consider the following example:
perpetrate:
S: (v) perpetrate, commit, pull (perform
an act, usually with a negative connota-
tion) “perpetrate a crime”; “pull a bank
robbery”
This sense of perpetuate has a negative
connotation, and is an objective term in
SentiWordNet. However, it has a positive
effect on the object, a crime, since performing a
crime brings it into existence.
</bodyText>
<footnote confidence="0.984558428571429">
3Deng et al. (2013) point out that +/-effect objects are not
equivalent to benefactive/malefactive semantic roles. An ex-
ample they give is She baked a cake for me: a cake is the ob-
ject of the +effect event baked as just noted, while me is the
filler of its benefactive semantic role (Ziga and Kittil, 2010).
4Their annotation manual, which gives additional cases, is
available with the annotated data at http://mpqa.cs.pitt.edu/.
</footnote>
<bodyText confidence="0.952653918918919">
As we mentioned, the +/-effect ambiguity can-
not be avoided in a word-level lexicon. In the
+/-effect corpus of Deng et al. (2013),5 +/-effect
events and their agents and objects are annotated
at the word level. In that corpus, 1,411 +/-effect in-
stances are annotated; 196 different +effect words
and 286 different -effect words appear in these
instances. Among them, 10 words appear in
both +effect and -effect instances, accounting for
9.07% of all annotated instances. They show that
+/-effect events (and the inferences that motivate
this work) appear frequently in sentences with ex-
plicit sentiment. Further, all instances of +/-effect
words that are not identified as +/-effect events are
false hits from the perspective of a recognition sys-
tem.
The following is an example of a word with
senses of different classes:
purge:
S: (v) purge (oust politically) “Deng
Xiao Ping was purged several times
throughout his lifetime” -effect
S: (v) purge (clear of a charge) +effect
S: (v) purify, purge, sanctify (make pure
or free from sin or guilt) “he left the
monastery purified” +effect
S: (v) purge (rid of impurities) “purge
the water”; “purge your mind” +effect
This is part of the WordNet output for the word
purge. In the first sense, the polarity is -effect
since it has a negative effect on the object, Deng
Xizo Ping. However, the other cases have positive
effect on the object. Moreover, although a word
may not have both +effect and -effect senses, it
may have mixtures of ((+effect or -effect) and
Null). A purely word-based approach is blind to
these cases.
</bodyText>
<sectionHeader confidence="0.999964" genericHeader="related work">
3 Related Work
</sectionHeader>
<bodyText confidence="0.999316375">
Lexicons are widely used in sentiment analysis
and opinion mining. Several works such as Hatzi-
vassiloglou and McKeown (1997), Turney and
Littman (2003), Kim and Hovy (2004), Strappar-
ava and Valitutti (2004), and Peng and Park (2011)
have tackled automatic lexicon expansion or ac-
quistion. However, in most such work, the lexi-
cons are word-level rather than sense-level.
</bodyText>
<footnote confidence="0.989534">
5Called the goodFor/badFor corpus in that paper.
</footnote>
<page confidence="0.996574">
1182
</page>
<bodyText confidence="0.99998601369863">
For the related (but different) tasks of de-
veloping subjectivity, sentiment and connota-
tion lexicons, some do take a sense-level ap-
proach. Esuli and Sebastiani (2006) construct
SentiWordNet. They assume that terms with
the same polarity tend to have similar glosses. So,
they first expand a manually selected seed set of
senses using WordNet lexical relations such as
also-see and direct antonymy and train two clas-
sifiers, one for positive and another for negative.
As features, a vectorial representation of glosses
is adopted. These classifiers were applied to all
WordNet senses to measure positive, negative, and
objective scores. In extending their work (Esuli
and Sebastiani, 2007), the PageRank algorithm is
applied to rank senses in terms of how strongly
they are positive or negative. In the graph, each
sense is one node, and two nodes are connected
when they contain the same words in their Word-
Net glosses. Moreover, a random-walk step is
adopted to refine the scores in their recent work
(Baccianella et al., 2010). In contrast, our ap-
proach uses WordNet relations and graph propa-
gation in addition to gloss classification.
Gyamfi et al. (2009) construct a classifier to la-
bel the subjectivity of word senses. The hierarchi-
cal structure and domain information in WordNet
are exploited to define features in terms of sim-
ilarity (using the LCS metric in Resnik (1995))
of target senses and a seed set of senses. Also,
the similarity of glosses in WordNet is consid-
ered. Even though they investigated the hierarchi-
cal structure by LCS values, WordNet relations are
not exploited directly.
Su and Markert (2009) adopt a semi-supervised
mincut method to recognize the subjectivity of
word senses. To construct a graph, each node cor-
responds to one WordNet sense and is connected
to two classification nodes (one for subjectivity
and another for objectivity) via a weighted edge
that is assigned by a classifier. For this classifier,
WordNet glosses, relations, and monosemous
features are considered. Also, several WordNet
relations (e.g., antonymy, similiar-to, direct
hypernym, etc.) are used to connect two nodes.
Although they make use of both WordNet glosses
and relations, and gloss information is utilized
for a classifier, this classifier is generated only
for weighting edges between sense nodes and
classification nodes, not for classifying all senses.
Kang et al. (2014) present a unified model that
assigns connotation polarities to both words and
senses. They formulate the induction process as
collective inference over pairwise-Markov Ran-
dom Fields, and apply loopy belief propagation
for inference. Their approach relies on selectional
preferences of connotative predicates; the polarity
of a connotation predicate suggests the polarity of
its arguments. We have not discovered an analo-
gous type of predicate for the problem we address.
Goyal et al. (2010) generate a lexicon of patient
polarity verbs (PPVs) that impart positive or neg-
ative states on their patients. They harvest PPVs
from a Web corpus by co-occurance with Kind and
Evil agents and by bootstrapping over conjunc-
tions of verbs. Riloff et al. (2013) learn positive
sentiment phrases and negative situation phrases
from a corpus of tweets with hashtag “sarcasm”.
However, both of these methods are word-level
rather than sense-level.
Ours is the first NLP research into developing
a sense-level lexicon for events that have negative
or positive effects on entities.
</bodyText>
<sectionHeader confidence="0.7584785" genericHeader="method">
4 +/-Effect Word-Level Seed Lexicon
and Sense Annotations
</sectionHeader>
<bodyText confidence="0.999584434782609">
To create the corpus used in this work, we devel-
oped a word-level seed lexicon, and then manually
annotated all the senses of the words in that lexi-
con.
FrameNet6 is based on a theory of meaning
called Frame Semantics. In FrameNet, a Lexical
Unit (LU) is a pairing of a word with a meaning,
i.e., it corresponds to a sense in WordNet. Each
LU of a polysemous word belongs to a different
semantic frame, which is a description of a type
of event, relation, or entity and, where appropri-
ate, its participants. For instance, in the Creating
frame, the definition is that a Cause leads to the
formation of a Created entity. It has a positive
effect on the object, Created entity. This frame
contains about 10 LUs such as assemble, create,
yield, and so on. FrameNet consists of about 1,000
semantic frames and about 10,000 LUs.
FrameNet is a useful resource to select +/-effect
words since each semantic frame covers multi-
ple LUs. We believe that using FrameNet to
find +/-effect words is easier than finding +/-effect
words without any information since words may
</bodyText>
<footnote confidence="0.906529">
6FrameNet, https://framenet.icsi.berkeley.edu/fndrupal/
</footnote>
<page confidence="0.977389">
1183
</page>
<bodyText confidence="0.999942285714286">
be filtered by semantic frames. To select +/-effect
words, an annotator (who is not a co-author) first
identified promising frames as +/-effect and ex-
tracted all LUs from them. Then, he went through
them and picked out the LUs which he judged to
be +effect or -effect. In total, 736 +effect LUs and
601 -effect LUs were selected from 463 semantic
frames.
While Deng et al. (2013) and Deng and Wiebe
(2014) specifically focus on events affecting ob-
jects (i.e., themes), we do not want to limit the
lexicon to only that case. Sometimes, events have
positive or negative effects on agents or other en-
tities as well. Thus, in this paper, we consider
a sense to be +effect (-effect) if it has +effect
(-effect) on an entity, which may be the agent, the
theme, or some other entity.
In a previous paper (Choi et al., 2014), we con-
ducted a study of the sense-level +/-effect prop-
erty. For the evaluation, two annotators (who
are co-authors of that paper) independently anno-
tated senses of selected words, where some are
from pure +effect (-effect) words (i.e., all senses
of the words are classified into the same class)
and some are from mixed words (i.e., the words
have both +effect and -effect senses). In the agree-
ment study, we calculated percent agreement and
n (Artstein and Poesio, 2008), and achieved 0.84
percent agreement and 0.75 n value.
For a seed set and an evaluation set in this pa-
per, we need annotated sense-level +/-effect data.
Mappings between FrameNet and WordNet are
not perfect. Thus, we opted to manually anno-
tate the senses of the words in the word-level lexi-
con. We first extracted all words from 736 +effect
LUs and 601 -effect LUs; this extracts 606 +effect
words and 537 -effect words (the number of words
is smaller than the number of LUs because one
word can have more than one LU). Among them,
14 words (e.g., crush, order, etc.) are in both the
+effect word set and the -effect word set. That is,
these words have both +effect and -effect mean-
ings. Recall that this annotator was focusing on
frames, not on words - he did not look at all the
senses of all the words. As we will see just below,
when all the senses of all the words are annotated,
a much higher percentage of the words have both
+effect and -effect senses. We will also see that
many of the senses are revealed to be Null, show-
ing that +effect vs. Null and -effect vs. Null ambi-
guities are quite prevalent.
A different annotator (a co-author) then went
through all senses of all the words from the pre-
vious step and manually annotated each sense as
to whether it is +effect, -effect, or Null. Note that
this annotator participated in an agreement study
with positive results in Choi et al. (2014).
For the experiments in this paper, we divided
this annotated data into two equal-sized sets. One
is a fixed test set that is used to evaluate both the
graph model and the gloss classifier. The other set
is used as a seed set by the graph model, and as a
training set by the gloss classifer. Table 1 shows
the distribution of the data. In total, there are 258
+effect senses, 487 -effect senses, and 880 Null
senses. To avoid too large a bias toward the Null
class,7 we randomly chose half (i.e., the Null set
contains 440 senses). Half of each set is used as
seed and training data, and the other half is used
for evaluation.
</bodyText>
<table confidence="0.9976535">
+effect -effect Null
# annotated data 258 487 880
# Seed/TrainSet 129 243 220
# TestSet 129 244 220
</table>
<tableCaption confidence="0.999084">
Table 1: Distribution of annotated data.
</tableCaption>
<sectionHeader confidence="0.860405" genericHeader="method">
5 Graph-based Semi-Supervised
</sectionHeader>
<subsectionHeader confidence="0.931743">
Learning for WordNet Relations
</subsectionHeader>
<bodyText confidence="0.999963846153846">
WordNet (Miller et al., 1990) is organized by se-
mantic relations such as hypernymy, troponymy,
grouping, and so on. These semantic relations can
be used to build a network. Since the most fre-
quently encoded relation is the super-subordinate
relation, most verb senses are arranged into hi-
erarchies; verb senses towards the bottom of the
graph express increasingly specific manner. Thus,
by following this hierarchical information, we hy-
pothesized that +/-effect polarity tends to propa-
gate. We use a graph-based semi-supervised learn-
ing (GSSL) method to carry out the label propaga-
tion.
</bodyText>
<subsectionHeader confidence="0.990921">
5.1 Graph Formulation
</subsectionHeader>
<bodyText confidence="0.999964">
We formulate a graph for semi-supervised learning
as follows. Let G = {X, E, W} be the undirected
graph in which X is the set of nodes, E is the set
</bodyText>
<footnote confidence="0.977476">
7As mentioned in the introduction, we want our method
to be able to identify unlabeled senses that are likely to be
+/-effect senses (see Section 8); we resize the Null class to
support this goal.
</footnote>
<page confidence="0.99324">
1184
</page>
<bodyText confidence="0.999972848484848">
of edges, and W represents the edge weights (i.e.,
the weight of edge Eij is Wij). The weight matrix
is a non-negative matrix.
Each data point in X = {x1, ... ,xn} is one
sense. The labeled data of X is represented as
XL = {x1, ... ,xl} and the unlabeled data is rep-
resented as XU = {xl+1, ... ,xn}). The labeled
data XL is associated with labels YL = {y1, ...
,yl}, where yi E {1, ..., c} (c is the number of
classes). As is typical in such settings, l « n:
n is 13,767, i.e., the number of verb senses in
WordNet. Seed/TrainSet in Table 1 is the labeled
data.
To connect two nodes, WordNet relations are
utilized. We first connect nodes by the hierar-
chical relations. Since hypernym relations repre-
sent more general senses and troponym relations
represent more specific verb senses, we hypothe-
sized that hypernyms or troponyms of a verb sense
tends to have its same polarity. Verb groups rela-
tions that represent verb senses having a similar
meaning are also promising. Even though verb-
group coverage is not large, its relations are reli-
able since they are manually grouped. The entail-
ment relation is defined as the verb Y is entailed
by X if you must be doing Y by doing X. Since
pairs connected by this relation are co-extensive,
we can assume that both are the same type of
event. The synonym relation is not used because
it is already defined in senses (i.e., each node in
the graph is a synset), and the antonym relation is
also not applied since the weight matrix should be
non-negative. The weight value of all edges is 1.0.
</bodyText>
<subsectionHeader confidence="0.993658">
5.2 Label Propagation
</subsectionHeader>
<bodyText confidence="0.992708029411765">
Given a constructed graph, the label inference (or
prediction) task is to propagate the seed labels to
the unlabeled nodes. One of the classic GSSL la-
bel propagation methods is the local and global
consistency (LGC) method suggested by Zhou et
al. (2004). The LGC method is a graph transduc-
tion algorithm which is sufficiently smooth with
respect to the intrinsic structure revealed by known
labeled and unlabeled data. The cost function typ-
ically involves a tradeoff between the smoothness
of the predicted labels over the entire graph and
the accuracy of the predicted labels in fitting the
given labeled nodes XL. LGC fits in a univariate
regularization framework, where the output ma-
trix is treated as the only variable in optimization,
and the optimal solutions can be easily obtained by
solving a linear system. Thus, we adopt the LGC
method in this paper. Although there are some ro-
bust GSSL methods for handling noisy labels, we
do not need to handle noisy labels because our in-
put is the annotated data.
Let F be a n x c matrix to save the output
values of label propagation. So, we can label
each instance xi as a label yi = argmaxj≤cFij
after the label propagation. The initial discrete la-
bel matrix Y , which is also n x c, is defined as
Yij = 1 if xi is labeled as yi = j in YL, and
Yij = 0 otherwise. The vertex degree matrix
D = diag([D11, ..., Dnn]) is defined by Dii =
En j=1 Wij.
LGC defines the cost function Q which inte-
grates two penalty components, global smooth-
ness and local fitting (µ is the regularization pa-
rameter):
</bodyText>
<equation confidence="0.9762668">
Wij ll Fi − Fj ll2
√Dii Djj
n
+µ llFi − Yill2
i=1
</equation>
<bodyText confidence="0.9999729">
The first part of the cost function is the
smoothness constraint: a good classifying func-
tion should not change too much between nearby
points. That is, if xi and xj are connected with
an edge, the difference between them should be
small. The second is the fitting constraint: a good
classifying function should not change too much
from the initial label assignment. The final label
prediction matrix F can be obtained by minimiz-
ing the cost function Q.
</bodyText>
<subsectionHeader confidence="0.991409">
5.3 Experimental Results
</subsectionHeader>
<bodyText confidence="0.99321175">
Note that, in the rest of this paper, all tables except
the last one give results on the same fixed test set
(TestSet in Table 1).
We can apply the graph model in two ways.
</bodyText>
<listItem confidence="0.984836">
• UniGraph: All three classes (+effect, -effect,
and Null) are represented in one graph.
• BiGraph: Two separate graphs are first con-
structed and then combined. One graph is for
classifying +effect and Other (i.e., -effect or
Null). This graph is called +eGraph. The
other graph, called -eGraph, is for classify-
ing -effect and Other (i.e., +effect or Null).
</listItem>
<equation confidence="0.9893152">
1 n
Q = 2�
i=1
n
j=1
</equation>
<page confidence="0.939798">
1185
</page>
<table confidence="0.999968230769231">
UniGraph BiGraph BiGraph*
baseline- 0.411
accuracy
accuracy 0.630 0.623 0.658
+effect P 0.621 0.610 0.642
R 0.655 0.647 0.680
F 0.637 0.628 0.660
-effect P 0.644 0.662 0.779
R 0.720 0.677 0.612
F 0.680 0.670 0.686
Null P 0.615 0.583 0.583
R 0.516 0.550 0.695
F 0.561 0.561 0.634
</table>
<tableCaption confidence="0.924828">
Table 2: Results of UniGraph, BiGraph, and Bi-
Graph*.
</tableCaption>
<table confidence="0.9999623">
H+T +V +E
+effect P 0.653 0.642 0.651
R 0.660 0.680 0.683
F 0.656 0.660 0.667
-effect P 0.784 0.779 0.786
R 0.547 0.612 0.604
F 0.644 0.686 0.683
Null P 0.557 0.583 0.564
R 0.735 0.695 0.691
F 0.634 0.634 0.621
</table>
<tableCaption confidence="0.999424">
Table 3: Effect of each relation
</tableCaption>
<listItem confidence="0.980567">
• If a sense is labeled as +effect (-effect), but
the confidence value is less than a threshold,
we count it as Null.
</listItem>
<bodyText confidence="0.999250121212121">
These are combined into one model as fol-
lows. Nodes that are labeled as +effect by
+eGraph and Other by -eGraph are regarded
as +effect, and nodes that are labeled as
-effect by -eGraph and Other by +eGraph are
regarded as -effect. If nodes are labeled as
+effect by +eGraph and -effect by -eGraph,
they are deemed to be Null. Nodes that are
labeled Other by both graphs are also consid-
ered as Null.
We had two motivations for experimenting
with the BiGraph model: (1) SVM, the super-
vised learning method used for gloss classifica-
tion, tends to have better performance on binary
classification tasks, and (2) the two graphs of the
combined model can “negotiate” with each other
via constraints.
In Table 2, we calculate precision (P), recall (R),
and f-measure (F) for all three classes. The base-
line shown in the top row is the accuracy of a ma-
jority class classifier. The first two columns of Ta-
ble 2 show the results of UniGraph and BiGraph
when they are built using the hypernym, troponym,
and verb group relations. UniGraph outperforms
BiGraph in this experiment.
To improve the results by performing some-
thing possible with BiGraph (but not UniGraph),
constraints are added when determining the class.
As we explained, the label of instance xi is
determined by Fi in the graph. When the label
of xi is decided to be j, we can say that its con-
fidence value is Fij. There are two constraints as
follows.
</bodyText>
<listItem confidence="0.490124">
• If a sense is labeled as both +effect and -effect
by BiGraph, we choose the label with the
higher confidence value only if the higher one
is larger than a threshold and the lower one is
less than a threshold.
</listItem>
<bodyText confidence="0.999963172413793">
The thresholds are determined on Seed/TrainSet
by running BiGraph several times with different
thresholds, and choosing the one that gives the
best performance on Seed/TrainSet. (The chosen
value is 0.025 for +effect and 0.03 for -effect).
As can be seen in Table 2, BiGraph with con-
straints (called BiGraph*) outperforms not only
BiGraph without any constraints but also Uni-
Graph. Especially, for BiGraph*, the recall of the
Null class is considerably increased, showing that
constraints not only help overall, but are particu-
larly important for detecting Null cases.
Table 3 gives ablation results, showing the con-
tribution of each WordNet relation in BiGraph*.
With only hierarchical information (i.e., hyper-
nym (H) and troponym (T) relations), it already
shows good performance for all classes. How-
ever, they cannot cover some senses. Among the
13,767 verb senses in WordNet, 1,707 (12.4%)
cannot be labeled because there are not sufficient
hierarchical links to propagate polarity informa-
tion. When adding the verb group (+V) rela-
tion, it shows improvement in both +effect and
-effect. Especially, the recall for +effect and
-effect is significantly increased. In addition, the
coverage of the 13,767 verb senses increases to
95.1%. For entailment (+E), whereas adding it
shows a slight improvement in +effect (and in-
creases coverage by 1.1 percentage points), the
</bodyText>
<page confidence="0.985137">
1186
</page>
<bodyText confidence="0.999905333333333">
performance is decreased a little bit in the -effect
and Null classes. Since the average f-measure for
all classes is the highest with hypernym (H), tro-
ponym (T), and verb group (V) relations (not en-
tailment), we only consider these three relations
when constructing the graph.
</bodyText>
<sectionHeader confidence="0.9213" genericHeader="method">
6 Supervised Learning applied to
WordNet Glosses
</sectionHeader>
<bodyText confidence="0.999921857142857">
In WordNet, each sense contains a gloss consist-
ing of a definition and optional example sentences.
Since a gloss consists of several words and there
are no direct links between glosses, we believe that
a word vector representation is appropriate to uti-
lize gloss information as in Esuli and Sebastiani
(2006). For that, we adopt an SVM classifier.
</bodyText>
<subsectionHeader confidence="0.82084">
6.1 Features
</subsectionHeader>
<bodyText confidence="0.999341318181818">
Two different feature types are used.
Word Features (WF): The bag-of-words
model is applied. We do not ignore stop words
for several reasons. Since most definitions and ex-
amples are not long, each gloss contains a small
number of words. Also, among them, the total vo-
cabulary of WordNet glosses is not large. More-
over, some prepositions such as against are some-
times useful to determine the polarity (+effect or
-effect).
Sentiment Features (SF): Some glosses of
+effect (-effect) senses contain positive (negative)
words. For instance, the definition of {hurt#4,
injure#4} is “cause damage or affect negatively.”
It contains a negative word, negatively. Since a
given event may positively (negatively) affect enti-
ties, some definitions or examples already contain
positive (negative) words to express this. Thus, as
features, we check how many positive (negative)
words a given gloss contains. To detect sentiment
words, the subjectivity lexicon provided by Wil-
son et al. (2005)8 is utilized.
</bodyText>
<subsectionHeader confidence="0.999679">
6.2 Gloss Classifier
</subsectionHeader>
<bodyText confidence="0.999746875">
We have three classes, +effect, -effect, and Null.
Since SVM shows better performance on binary
classification tasks, we generate two binary clas-
sifiers, one (+eClassifier) to determine whether
a given sense is +effect or Other, and another
(-eClassifier) to classify whether a given sense is
-effect or Other. Then, they are combined as in
BiGraph.
</bodyText>
<footnote confidence="0.996848">
8Available at http://mpqa.cs.pitt.edu/
</footnote>
<subsectionHeader confidence="0.995568">
6.3 Experimental Results
</subsectionHeader>
<bodyText confidence="0.970083266666667">
Seed/TrainSet in Table 1 is used to train the two
classifiers, and TestSet is utilized for the evalua-
tion. So, the training set for +eClassifier consists
of 129 +effect instances and 463 Other instances,
and the training set for -eClassifier contains 243
-effect instances and 349 Other instances. As a
baseline, we adopt a majority class classifier.
Table 4 shows the results on TestSet. Perfor-
mance is better for the -effect than for the +effect
class, perhaps because the -effect class has more
instances.
When sentiment features (SF) are added,
all metric values increase, providing evidence
that sentiment features are helpful to determine
+/-effect classes.
</bodyText>
<table confidence="0.997964666666667">
WF WF+SF
baseline accuracy 0.411
accuracy 0.509 0.539
+effect P 0.541 0.588
R 0.354 0.393
F 0.428 0.472
-effect P 0.616 0.672
R 0.500 0.511
F 0.552 0.580
Null P 0.432 0.451
R 0.612 0.657
F 0.507 0.535
</table>
<tableCaption confidence="0.999782">
Table 4: Results of the gloss classifier.
</tableCaption>
<sectionHeader confidence="0.95358" genericHeader="method">
7 Hybrid Method
</sectionHeader>
<bodyText confidence="0.996941176470588">
To use more combined knowledge, the gloss clas-
sifier and BiGraph* can be combined. That is, for
WordNet gloss information, the gloss classifier is
utilized, and for WordNet relations, BiGraph* is
used. With the Hybrid method, we can see not
only the effect of propagation by WordNet rela-
tions but also the usefulness of gloss information
and sentiment features. Also, while BiGraph*
cannot cover all senses in WordNet, the Hybrid
method can.
The outputs of the gloss classifier and Bi-
Graph* are combined as follows. The label of
the gloss classifier is one of +effect, -effect, Null,
or Both (when a given sense is classified as both
+effect by +eClassifier and -effect by -eClassifier).
Possible labels of BiGraph* are +effect, -effect,
Null, Both, or None (when a given sense is not
</bodyText>
<page confidence="0.984769">
1187
</page>
<listItem confidence="0.96063">
labeled by BiGraph*). There are five rules:
• If both labels are +effect (-effect), it is +effect
(-effect).
• If one of them is Both and the other is +effect
(-effect), it is +effect (-effect).
• If the label of BiGraph* is None, believe the
label of the gloss classifier
• If both labels are Both, it is Null
• Otherwise, it is Null
</listItem>
<bodyText confidence="0.9998396">
The results for Hybrid are given in the first
row of the lower half of Table 5; the results for
BiGraph* are in the first row of the upper half,
for comparison. Generally, the Hybrid method
shows better performance than the gloss classifier
and BiGraph*. In the Hybrid method, since more
+/-effect senses are detected than by BiGraph*,
while precision is decreased, recall is increased
by more. However, by the same token, the over-
all performance for the Null class is decreased.
Actually, that is expected since the Null class is
determined by the Other class in the gloss clas-
sifier and BiGraph*. Through this experiment, we
see that the Hybrid method is better for classifying
+/-effect senses.
</bodyText>
<subsectionHeader confidence="0.998888">
7.1 Model Comparison
</subsectionHeader>
<bodyText confidence="0.999947052631579">
To provide evidence for our assumption that dif-
ferent models are needed for different information
to maximize effectiveness, we compare the hy-
brid method with the supervised learning and the
graph-based learning (GSSL) methods, each uti-
lizing both WordNet relations and gloss informa-
tion.
Supervised Learning (onlySL): The gloss clas-
sifier is trained with word features and sentiment
features for WordNet Gloss. To exploit Word-
Net relations in supervised learning, especially
the hierarchical information, we use least com-
mon subsumer (LCS) values as in Gyamfi et al.
(2009), which, recall, performs supervised learn-
ing of subjective/objective senses. The values are
calculated as follows. For a target sense t and a
seed set S, the maximum LCS value between a
target sense and a member of the seed set is found
as:
</bodyText>
<equation confidence="0.565995">
Score(t, S) = maxs∈SLCS(t, s)
</equation>
<bodyText confidence="0.997647911764706">
With this LCS feature and the features described
in Section 6, we run SVM on the same training and
test data. For LCS values, the similarity using the
information content proposed by Resnik (1995) is
measured. WordNet Similarity9 package provides
pre-computed pairwise similarity values for that.
Table 6 shows results of onlySL. Compared to
Table 4, while +effect and Null classes show a
slight improvement, the performance is degraded
for -effect. This means that the added feature is
rather harmful to -effect. Even though the hierar-
chical feature is very helpful to expand +/-effect,
it is not helpful for onlySL since SVM cannot cap-
ture propagation according to the hierarchy.
Graph-based Learning (onlyGraph): In Sec-
tion 5, the graph is constructed by using Word-
Net relations. To apply WordNet gloss informa-
tion in onlyGraph, we calculate a cosine similarity
between glosses. If the similarity value is higher
than a threshold, two nodes are connected with this
similarity value. The threshold is determined by
training and testing on Seed/TrainSet (the chosen
value is 0.3).
Comparing Tables 2 and 6, BiGraph* generally
outperforms onlyGraph (the exception is precision
of +effect). By gloss similarity, many nodes are
connected to each other. However, since uncertain
connections can cause incorrect propagation in the
graph, this negatively affects the performance.
Through this experiment, we see that since each
type of information has a different character, we
need different models to maximize the effective-
ness of each type. Thus, the hybrid method with
different models can have better performance.
</bodyText>
<table confidence="0.9994379">
Hybrid onlySL onlyGraph
+effect P 0.610 0.584 0.701
R 0.735 0.400 0.364
F 0.667 0.475 0.480
-effect P 0.717 0.778 0.651
R 0.669 0.316 0.562
F 0.692 0.449 0.603
Null P 0.556 0.440 0.473
R 0.520 0.813 0.679
F 0.538 0.571 0.557
</table>
<tableCaption confidence="0.999268">
Table 6: Comparison to onlySL and onlyGraph.
</tableCaption>
<footnote confidence="0.991871">
9WordNet Similarity,
http://wn-similarity.sourceforge.net/
</footnote>
<page confidence="0.952112">
1188
</page>
<table confidence="0.99990125">
+effect -effect Null
P R F P R F P R F
BiGraph* Initial 0.642 0.680 0.660 0.779 0.612 0.686 0.583 0.695 0.634
1st 0.636 0.684 0.663 0.770 0.632 0.694 0.591 0.672 0.629
2nd 0.642 0.701 0.670 0.748 0.656 0.699 0.605 0.655 0.629
3rd 0.636 0.708 0.670 0.779 0.652 0.710 0.599 0.669 0.632
4th 0.681 0.674 0.678 0.756 0.674 0.712 0.589 0.669 0.626
Hybrid Initial 0.610 0.735 0.667 0.717 0.669 0.692 0.556 0.520 0.538
1st 0.614 0.713 0.672 0.728 0.681 0.704 0.562 0.523 0.542
2nd 0.613 0.743 0.672 0.716 0.697 0.706 0.559 0.497 0.526
3rd 0.616 0.739 0.672 0.717 0.706 0.712 0.559 0.494 0.525
4th 0.688 0.681 0.684 0.712 0.764 0.732 0.565 0.527 0.545
</table>
<tableCaption confidence="0.999631">
Table 5: Results of an iterative approach.
</tableCaption>
<sectionHeader confidence="0.991792" genericHeader="method">
8 Guided Annotation
</sectionHeader>
<bodyText confidence="0.999783714285715">
Recall that Seed/TrainSet and TestSet, the data
used so far, are all the senses of the words in a
word-level +/-effect lexicon. This section presents
evidence that our method can guide annotation ef-
forts to find other words that have +/-effect senses.
A bonus is that the method pinpoints particular
+/-effect senses of those words.
All unlabeled data are senses of words that are
not included in the original lexicon. Since pre-
sumably the majority of verbs do not have any
+/-effect senses, a sense randomly selected from
WordNet is very likely to be Null. We explore an
iterative approach to guided annotation, using Bi-
Graph* and Hybrid as the method for assigning
labels.
The system is initially created as described
above using Seed/TrainSet as the initial seed set.
Each iteration has four steps: 1) rank all unlabeled
data (i.e., the data other than TestSet and the cur-
rent seed set) based on the Fij confidence values
(see Section 5.3); 2) choose the top 5% and manu-
ally annotate them (the same annotator as above
did this); 3) add them to the seed set; 4) rerun
the system using the expanded seed set. We per-
formed four iterations in this paper.
The upper and lower parts of Table 5 show the
intial results and the results after each iteration for
BiGraph* and Hybrid. Recall that these are results
on the fixed set, TestSet. Overall for both mod-
els, f-measure increases for both the +effect and
-effect classes as more seeds are added, mainly
due to improvements in recall. The evaluation on
the fixed set is also useful in the annotation process
because it trades off +/-effect vs. Null annotations.
If the new manual annotations were biased, in that
they incorrectly label Null senses as +/-effect, then
the f-measure results would instead degrade on the
fixed TestSet, since the system is created each time
using the increased seed set.
We now consider the accuracy of the system
on the newly labeled annotated data in Step 2.
Note that our method is similar to Active Learn-
ing (Tong and Koller, 2001), in that both auto-
matically identify which unlabeled instances the
human should annotate next. However, in active
learning, the goal is to find instances that are diffi-
cult for a supervised learning system. In our case,
the goal is to find needles in the haystack of Word-
Net senses. In Step 3, we add the newly labeled
senses to the seed set, enabling the model to find
unlabeled senses close to the new seeds when the
system is rerun for the next iteration.
We assess the system’s accuracy on the newly
labeled data by comparing the system’s labels with
the human’s new labels. Accuracy for +effect and
-effect is calculated such as:
</bodyText>
<sectionHeader confidence="0.560791" genericHeader="method">
# annotated +effect
</sectionHeader>
<figure confidence="0.5778206">
Accuracy+effect -
# top 5% +effect data
# annotated -effect
Accuracy−effect -
# top 5% -effect data
</figure>
<bodyText confidence="0.999786285714286">
That is, the accuracy means that out of the top 5%
of the +effect (-effect) data as scored by the sys-
tem, what percentage are correct as judged by a
human annotator. Table 7 shows the accuracy for
each iteration in the top part and the number of
senses labeled in the bottom part. As can be seen,
the accuracies range between 60% and 78%; these
</bodyText>
<page confidence="0.986207">
1189
</page>
<bodyText confidence="0.998562875">
values are much higher than what would be ex-
pected if labeling senses of words randomly cho-
sen from WordNet.10 The annotator spent, on av-
erage, approximately an hour to label 100 senses.
For finding new words with +/-effect usages, it
would be much more cost-effective if a significant
percentage of the data chosen for annotation are
senses of words that in fact have +/-effect senses.
</bodyText>
<table confidence="0.999218833333333">
1st 2nd 3rd 4th
+effect 65.63% 62.50% 63.79% 59.83%
-effect 73.55% 73.97% 77.78% 70.30%
+effect 128 122 116 117
-effect 155 146 153 145
total 283 268 269 262
</table>
<tableCaption confidence="0.9914655">
Table 7: Accuracy and frequency of the top 5% for
each iteration
</tableCaption>
<sectionHeader confidence="0.985987" genericHeader="conclusions">
9 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999975111111111">
In this paper, we investigated methods for creat-
ing a sense-level +/-effect lexicon. To maximize
the effectiveness of each type of information, we
combined a graph-based method using WordNet
relations and a standard classifier using gloss in-
formation. A hybrid between the two gives the
best results. Further, we provide evidence that the
model is an effective way to guide manual anno-
tation to find +/-effect words that are not in the
seed word-level lexicon. This is important, as the
likelihood that a random WordNet sense (and thus
word) is +effect or -effect is not large.
So as not to limit the inferences that may be
drawn, our annotations include events that are
+effect or -effect either the agent or object. In fu-
ture work, we plan to exploit corpus-based meth-
ods using patterns as in Goyal et al. (2010) com-
bined with semantic role labeling to refine the lex-
icon to distinguish which is the affected entity.
Further, to actually exploit the acquired lexicon to
process corpus data, an appropriate coarse-grained
sense disambiguation process must be added, as
Akkaya et al. (2009) and Akkaya et al. (2011) did
for subjective/objective classification.
We hope the general methodology will be ef-
fective for other semantic properties. In opin-
ion mining and sentiment analysis this is partic-
</bodyText>
<footnote confidence="0.88729125">
10For reference, in 5th iteration, the +effect accuracy is
60.18% and the -effect accuracy is 69.93%, and in 6th itera-
tion, the +effect accuracy is 59.81% and the -effect accuracy
is 69.12%.
</footnote>
<bodyText confidence="0.999815222222222">
ularly needed, because different meanings of pos-
itive and negative are appropriate for different ap-
plications. This is a way to create lexicons that are
customized with respect to one’s own definitions.
It would be promising to combine our method
with other methods to enable it to find +effect
and -effect senses that are outside the coverage
of WordNet. However, a WordNet-based lexicon
gives a substantial base to build from.
</bodyText>
<sectionHeader confidence="0.998939" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9951928">
This work was supported in part by DARPA-BAA-
12-47 DEFT grant #12475008 and National Sci-
ence Foundation grant #IIS-0916046. We would
like to thank the reviewers for their helpful sug-
gestions and comments.
</bodyText>
<sectionHeader confidence="0.999498" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99987155882353">
Cem Akkaya, Janyce Wiebe, and Rada Mihalcea.
2009. Subjectivity word sense disambiguation. In
Proceedings of EMNLP 2009, pages 190–199.
Cem Akkaya, Janyce Wiebe, Alexander Conrad, and
Rada Mihalcea. 2011. Improving the impact of sub-
jectivity word sense disambiguation on contextual
opinion analysis. In Proceedings of CoNLL 2011,
pages 87–96.
Pranna Anand and Kevin Reschke. 2010. Verb classes
as evaluativity functor classes. In Interdisciplinary
Workshop on Verbs. The Identification and Repre-
sentation of Verb Features.
Ron Artstein and Massimo Poesio. 2008. Inter-coder
agreement for computational linguistics. Comput.
Linguist., 34(4):555–596.
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. Sentiwordnet 3.0: An enhanced lexical
resource for sentiment analysis and opinion mining.
In Proceedings of LREC, pages 2200–2204.
Yoonjung Choi, Lingjia Deng, and Janyce Wiebe.
2014. Lexical acquisition for opinion inference:
A sense-level lexicon of benefactive and malefac-
tive events. In Proceedings of the 5th Workshop
on Computational Approaches to Subjectivity, Sen-
timent and Social Media Analysis (WASSA), pages
107–112. Association for Computational Linguis-
tics.
Lingjia Deng and Janyce Wiebe. 2014. Sentiment
propagation via implicature constraints. In Proceed-
ings of EACL.
Lingjia Deng, Yoonjung Choi, and Janyce Wiebe.
2013. Benefactive/malefactive event and writer atti-
tude annotation. In Proceedings of 51st ACL, pages
120–125.
</reference>
<page confidence="0.807652">
1190
</page>
<reference confidence="0.999897236842106">
Lingjia Deng, Janyce Wiebe, and Yoonjung Choi.
2014. Joint inference and disambiguation of implicit
sentiments via implicature constraints. In Proceed-
ings of COLING, page 7988.
Andrea Esuli and Fabrizio Sebastiani. 2006. Senti-
wordnet: A publicly available lexical resource for
opinion mining. In Proceedings of 5th LREC, pages
417–422.
Andrea Esuli and Fabrizio Sebastiani. 2007. Pager-
anking wordnet synsets: An application to opinion
mining. In Proceedings of ACL, pages 424–431.
Song Feng, Ritwik Bose, and Yejin Choi. 2011. Learn-
ing general connotation of words using graph-based
algorithms. In Proceedings of EMNLP, pages 1092–
1103.
Amit Goyal, Ellen Riloff, and Hal DaumeIII. 2010.
Automatically producing plot unit representations
for narrative text. In Proceedings of EMNLP, pages
77–86.
Yaw Gyamfi, Janyce Wiebe, Rada Mihalcea, and Cem
Akkaya. 2009. Integrating knowledge for subjectiv-
ity sense labeling. In Proceedings of NAACL HLT
2009, pages 10–18.
Vasileios Hatzivassiloglou and Kathleen R. McKeown.
1997. Predicting the semantic orientation of adjec-
tives. In Proceedings of ACL, pages 174–181.
Jun Seok Kang, Song Feng, Leman Akoglu, and Yejin
Choi. 2014. Connotationwordnet: Learning conno-
tation over the word+sense network. In Proceedings
of the 52nd ACL, page 15441554.
Soo-Min Kim and Eduard Hovy. 2004. Determining
the sentiment of opinions. In Proceedings of 20th
COLING, pages 1367–1373.
George A. Miller, Richard Beckwith, Christiane Fell-
baum, Derek Gross, and Katherine Miller. 1990.
Wordnet: An on-line lexical database. International
Journal of Lexicography, 13(4):235–312.
Wei Peng and Dae Hoon Park. 2011. Generate adjec-
tive sentiment dictionary for social media sentiment
analysis using constrained nonnegative matrix fac-
torization. In Proceedings of ICWSM.
Philip Resnik. 1995. Using information content to
evaluate semantic similarity. In Proceedings of 14th
IJCAI, pages 448–453.
Ellen Riloff, Ashequl Qadir, Prafulla Surve, Lalin-
dra De Silva, Nathan Gilbert, and Ruihong Huang.
2013. Sarcasm as contrast between a positive sen-
timent and negative situation. In Proceedings of
EMNLP, pages 704–714.
Carlo Strapparava and Alessandro Valitutti. 2004.
Wordnet-affect: An affective extension of wordnet.
In Proceedings of 4th LREC, pages 1083–1086.
Fangzhong Su and Katja Markert. 2009. Subjectiv-
ity recognition on word senses via semi-supervised
mincuts. In Proceedings of NAACL HLT 2009,
pages 1–9.
Simon Tong and Daphne Koller. 2001. Support vector
machin active learning with applications to text clas-
sification. Journal of Machine Learning Research,
2:45–66.
Peter Turney and Michael Littman. 2003. Measuring
praise and criticism: Inference of semantic orienta-
tion from association. ACM Transactions on Infor-
mation Systems, 21(4):315–346.
Theresa Wilson, Janyce Wiebe, , and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of HLT-
EMNLP, pages 347–354.
Dengyong Zhou, Olivier Bousquet, Thomas Navin Lal,
Jason Weston, and Bernhard Scholkopf. 2004.
Learning with local and global consistency. Ad-
vances in Neural Information Processing Systems,
16:321–329.
Fernando Ziga and Seppo Kittil. 2010. Benefactives
and malefactives, Typological perspectives and case
studies. John Benjamins Publishing.
</reference>
<page confidence="0.994512">
1191
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.969689">
<title confidence="0.998128">EffectWordNet: Sense-level Lexicon Acquisition for Opinion Inference</title>
<author confidence="0.996922">Choi</author>
<affiliation confidence="0.999402">Department of Computer University of</affiliation>
<email confidence="0.999127">yjchoi,wiebe@cs.pitt.edu</email>
<abstract confidence="0.998773285714286">Recently, work in NLP was initiated on a type of opinion inference that arises when opinions are expressed toward events which have positive or negative effects entities This paper addresses methods for creating a lexicon of such events, to support such work on opinion inference. Due to significant sense ambiguity, our goal is to develop a sense-level rather than word-level lexicon. To maximize the effectiveness of different types of information, we combine a method using relations and a standard classifier using gloss information. A hybrid between the two gives the best results. Further, we provide evidence that the model is an effective way to guide manual annotation to find +/-effect senses that are not in the seed set.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Cem Akkaya</author>
<author>Janyce Wiebe</author>
<author>Rada Mihalcea</author>
</authors>
<title>Subjectivity word sense disambiguation.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<pages>190--199</pages>
<contexts>
<context position="38362" citStr="Akkaya et al. (2009)" startWordPosition="6440" endWordPosition="6443">mportant, as the likelihood that a random WordNet sense (and thus word) is +effect or -effect is not large. So as not to limit the inferences that may be drawn, our annotations include events that are +effect or -effect either the agent or object. In future work, we plan to exploit corpus-based methods using patterns as in Goyal et al. (2010) combined with semantic role labeling to refine the lexicon to distinguish which is the affected entity. Further, to actually exploit the acquired lexicon to process corpus data, an appropriate coarse-grained sense disambiguation process must be added, as Akkaya et al. (2009) and Akkaya et al. (2011) did for subjective/objective classification. We hope the general methodology will be effective for other semantic properties. In opinion mining and sentiment analysis this is partic10For reference, in 5th iteration, the +effect accuracy is 60.18% and the -effect accuracy is 69.93%, and in 6th iteration, the +effect accuracy is 59.81% and the -effect accuracy is 69.12%. ularly needed, because different meanings of positive and negative are appropriate for different applications. This is a way to create lexicons that are customized with respect to one’s own definitions.</context>
</contexts>
<marker>Akkaya, Wiebe, Mihalcea, 2009</marker>
<rawString>Cem Akkaya, Janyce Wiebe, and Rada Mihalcea. 2009. Subjectivity word sense disambiguation. In Proceedings of EMNLP 2009, pages 190–199.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cem Akkaya</author>
<author>Janyce Wiebe</author>
<author>Alexander Conrad</author>
<author>Rada Mihalcea</author>
</authors>
<title>Improving the impact of subjectivity word sense disambiguation on contextual opinion analysis.</title>
<date>2011</date>
<booktitle>In Proceedings of CoNLL</booktitle>
<pages>87--96</pages>
<contexts>
<context position="38387" citStr="Akkaya et al. (2011)" startWordPosition="6445" endWordPosition="6448">od that a random WordNet sense (and thus word) is +effect or -effect is not large. So as not to limit the inferences that may be drawn, our annotations include events that are +effect or -effect either the agent or object. In future work, we plan to exploit corpus-based methods using patterns as in Goyal et al. (2010) combined with semantic role labeling to refine the lexicon to distinguish which is the affected entity. Further, to actually exploit the acquired lexicon to process corpus data, an appropriate coarse-grained sense disambiguation process must be added, as Akkaya et al. (2009) and Akkaya et al. (2011) did for subjective/objective classification. We hope the general methodology will be effective for other semantic properties. In opinion mining and sentiment analysis this is partic10For reference, in 5th iteration, the +effect accuracy is 60.18% and the -effect accuracy is 69.93%, and in 6th iteration, the +effect accuracy is 59.81% and the -effect accuracy is 69.12%. ularly needed, because different meanings of positive and negative are appropriate for different applications. This is a way to create lexicons that are customized with respect to one’s own definitions. It would be promising to</context>
</contexts>
<marker>Akkaya, Wiebe, Conrad, Mihalcea, 2011</marker>
<rawString>Cem Akkaya, Janyce Wiebe, Alexander Conrad, and Rada Mihalcea. 2011. Improving the impact of subjectivity word sense disambiguation on contextual opinion analysis. In Proceedings of CoNLL 2011, pages 87–96.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pranna Anand</author>
<author>Kevin Reschke</author>
</authors>
<title>Verb classes as evaluativity functor classes. In Interdisciplinary Workshop on Verbs. The Identification and Representation of Verb Features.</title>
<date>2010</date>
<contexts>
<context position="4512" citStr="Anand and Reschke, 2010" startWordPosition="697" endWordPosition="700">odel which gives the best overall performance. Finally, because all WordNet verb senses are incorporated into the model, we investigate the ability of the method to identify unlabeled senses that are likely to be +/-effect senses. We find that by iteratively labeling the top-weighted unlabeled senses and rerunning the model, it may be used as an effective method for guiding annotation efforts. 2 Background There are many varieties of +/-effect events, including creation/destruction (changes in states involving existence), gain/loss (changes in states involving possession), and benefit/injury (Anand and Reschke, 2010; Deng et al., 2013). The creation, gain, and benefit classes are +effect events. For example, baking a cake has a positive effect on the cake because it is created;3 increasing the tax rate has a positive effect on the tax rate; and comforting the child has a positive effect on the child. The antonymous classes of each are -effect events: destroying the building has a negative effect on the building; demand decreasing has a negative effect on demand; and killing Bill has a negative effect on Bill.4 While sentiment (Esuli and Sebastiani, 2006; Wilson et al., 2005; Su and Markert, 2009) and con</context>
</contexts>
<marker>Anand, Reschke, 2010</marker>
<rawString>Pranna Anand and Kevin Reschke. 2010. Verb classes as evaluativity functor classes. In Interdisciplinary Workshop on Verbs. The Identification and Representation of Verb Features.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ron Artstein</author>
<author>Massimo Poesio</author>
</authors>
<title>Inter-coder agreement for computational linguistics.</title>
<date>2008</date>
<journal>Comput. Linguist.,</journal>
<volume>34</volume>
<issue>4</issue>
<contexts>
<context position="14064" citStr="Artstein and Poesio, 2008" startWordPosition="2262" endWordPosition="2265">fect (-effect) if it has +effect (-effect) on an entity, which may be the agent, the theme, or some other entity. In a previous paper (Choi et al., 2014), we conducted a study of the sense-level +/-effect property. For the evaluation, two annotators (who are co-authors of that paper) independently annotated senses of selected words, where some are from pure +effect (-effect) words (i.e., all senses of the words are classified into the same class) and some are from mixed words (i.e., the words have both +effect and -effect senses). In the agreement study, we calculated percent agreement and n (Artstein and Poesio, 2008), and achieved 0.84 percent agreement and 0.75 n value. For a seed set and an evaluation set in this paper, we need annotated sense-level +/-effect data. Mappings between FrameNet and WordNet are not perfect. Thus, we opted to manually annotate the senses of the words in the word-level lexicon. We first extracted all words from 736 +effect LUs and 601 -effect LUs; this extracts 606 +effect words and 537 -effect words (the number of words is smaller than the number of LUs because one word can have more than one LU). Among them, 14 words (e.g., crush, order, etc.) are in both the +effect word se</context>
</contexts>
<marker>Artstein, Poesio, 2008</marker>
<rawString>Ron Artstein and Massimo Poesio. 2008. Inter-coder agreement for computational linguistics. Comput. Linguist., 34(4):555–596.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefano Baccianella</author>
<author>Andrea Esuli</author>
<author>Fabrizio Sebastiani</author>
</authors>
<title>Sentiwordnet 3.0: An enhanced lexical resource for sentiment analysis and opinion mining.</title>
<date>2010</date>
<booktitle>In Proceedings of LREC,</booktitle>
<pages>2200--2204</pages>
<contexts>
<context position="9167" citStr="Baccianella et al., 2010" startWordPosition="1461" endWordPosition="1464">ain two classifiers, one for positive and another for negative. As features, a vectorial representation of glosses is adopted. These classifiers were applied to all WordNet senses to measure positive, negative, and objective scores. In extending their work (Esuli and Sebastiani, 2007), the PageRank algorithm is applied to rank senses in terms of how strongly they are positive or negative. In the graph, each sense is one node, and two nodes are connected when they contain the same words in their WordNet glosses. Moreover, a random-walk step is adopted to refine the scores in their recent work (Baccianella et al., 2010). In contrast, our approach uses WordNet relations and graph propagation in addition to gloss classification. Gyamfi et al. (2009) construct a classifier to label the subjectivity of word senses. The hierarchical structure and domain information in WordNet are exploited to define features in terms of similarity (using the LCS metric in Resnik (1995)) of target senses and a seed set of senses. Also, the similarity of glosses in WordNet is considered. Even though they investigated the hierarchical structure by LCS values, WordNet relations are not exploited directly. Su and Markert (2009) adopt </context>
</contexts>
<marker>Baccianella, Esuli, Sebastiani, 2010</marker>
<rawString>Stefano Baccianella, Andrea Esuli, and Fabrizio Sebastiani. 2010. Sentiwordnet 3.0: An enhanced lexical resource for sentiment analysis and opinion mining. In Proceedings of LREC, pages 2200–2204.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoonjung Choi</author>
<author>Lingjia Deng</author>
<author>Janyce Wiebe</author>
</authors>
<title>Lexical acquisition for opinion inference: A sense-level lexicon of benefactive and malefactive events.</title>
<date>2014</date>
<booktitle>In Proceedings of the 5th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis (WASSA),</booktitle>
<pages>107--112</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="13591" citStr="Choi et al., 2014" startWordPosition="2183" endWordPosition="2186"> them and picked out the LUs which he judged to be +effect or -effect. In total, 736 +effect LUs and 601 -effect LUs were selected from 463 semantic frames. While Deng et al. (2013) and Deng and Wiebe (2014) specifically focus on events affecting objects (i.e., themes), we do not want to limit the lexicon to only that case. Sometimes, events have positive or negative effects on agents or other entities as well. Thus, in this paper, we consider a sense to be +effect (-effect) if it has +effect (-effect) on an entity, which may be the agent, the theme, or some other entity. In a previous paper (Choi et al., 2014), we conducted a study of the sense-level +/-effect property. For the evaluation, two annotators (who are co-authors of that paper) independently annotated senses of selected words, where some are from pure +effect (-effect) words (i.e., all senses of the words are classified into the same class) and some are from mixed words (i.e., the words have both +effect and -effect senses). In the agreement study, we calculated percent agreement and n (Artstein and Poesio, 2008), and achieved 0.84 percent agreement and 0.75 n value. For a seed set and an evaluation set in this paper, we need annotated s</context>
<context position="15465" citStr="Choi et al. (2014)" startWordPosition="2519" endWordPosition="2522">enses of all the words. As we will see just below, when all the senses of all the words are annotated, a much higher percentage of the words have both +effect and -effect senses. We will also see that many of the senses are revealed to be Null, showing that +effect vs. Null and -effect vs. Null ambiguities are quite prevalent. A different annotator (a co-author) then went through all senses of all the words from the previous step and manually annotated each sense as to whether it is +effect, -effect, or Null. Note that this annotator participated in an agreement study with positive results in Choi et al. (2014). For the experiments in this paper, we divided this annotated data into two equal-sized sets. One is a fixed test set that is used to evaluate both the graph model and the gloss classifier. The other set is used as a seed set by the graph model, and as a training set by the gloss classifer. Table 1 shows the distribution of the data. In total, there are 258 +effect senses, 487 -effect senses, and 880 Null senses. To avoid too large a bias toward the Null class,7 we randomly chose half (i.e., the Null set contains 440 senses). Half of each set is used as seed and training data, and the other h</context>
</contexts>
<marker>Choi, Deng, Wiebe, 2014</marker>
<rawString>Yoonjung Choi, Lingjia Deng, and Janyce Wiebe. 2014. Lexical acquisition for opinion inference: A sense-level lexicon of benefactive and malefactive events. In Proceedings of the 5th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis (WASSA), pages 107–112. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lingjia Deng</author>
<author>Janyce Wiebe</author>
</authors>
<title>Sentiment propagation via implicature constraints.</title>
<date>2014</date>
<booktitle>In Proceedings of EACL.</booktitle>
<contexts>
<context position="1518" citStr="Deng and Wiebe, 2014" startWordPosition="225" endWordPosition="228"> manual annotation to find +/-effect senses that are not in the seed set. 1 Introduction Opinion mining (or sentiment analysis) identifies positive or negative opinions in many kinds of texts such as reviews, blogs, and news articles. It has been exploited in many application areas such as review mining, election analysis, and information extraction. While most previous research focusses on explicit opinion expressions, recent work addresses a type of opinion inference that arises when opinions are expressed toward events which have positive or negative effects on entities (Deng et al., 2013; Deng and Wiebe, 2014). We call such events +/-effect events.2 Deng and Wiebe (2014) show how sentiments toward one 1WordNet 3.0, http://wordnet.princeton.edu/ 2While the term goodFor/badFor is used in previous papers (Deng et al., 2013; Deng and Wiebe, 2014; Deng et al., 2014), we have since decided that +/-effect is a better term. entity may be propagated to other entities via opinion inference rules. They give the following example: (1) The bill would curb skyrocketing health care costs. The writer expresses an explicit negative sentiment (by skyrocketing) toward the object (health care costs). The event, curb, </context>
<context position="13180" citStr="Deng and Wiebe (2014)" startWordPosition="2108" endWordPosition="2111">covers multiple LUs. We believe that using FrameNet to find +/-effect words is easier than finding +/-effect words without any information since words may 6FrameNet, https://framenet.icsi.berkeley.edu/fndrupal/ 1183 be filtered by semantic frames. To select +/-effect words, an annotator (who is not a co-author) first identified promising frames as +/-effect and extracted all LUs from them. Then, he went through them and picked out the LUs which he judged to be +effect or -effect. In total, 736 +effect LUs and 601 -effect LUs were selected from 463 semantic frames. While Deng et al. (2013) and Deng and Wiebe (2014) specifically focus on events affecting objects (i.e., themes), we do not want to limit the lexicon to only that case. Sometimes, events have positive or negative effects on agents or other entities as well. Thus, in this paper, we consider a sense to be +effect (-effect) if it has +effect (-effect) on an entity, which may be the agent, the theme, or some other entity. In a previous paper (Choi et al., 2014), we conducted a study of the sense-level +/-effect property. For the evaluation, two annotators (who are co-authors of that paper) independently annotated senses of selected words, where s</context>
</contexts>
<marker>Deng, Wiebe, 2014</marker>
<rawString>Lingjia Deng and Janyce Wiebe. 2014. Sentiment propagation via implicature constraints. In Proceedings of EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lingjia Deng</author>
<author>Yoonjung Choi</author>
<author>Janyce Wiebe</author>
</authors>
<title>Benefactive/malefactive event and writer attitude annotation.</title>
<date>2013</date>
<booktitle>In Proceedings of 51st ACL,</booktitle>
<pages>120--125</pages>
<contexts>
<context position="1495" citStr="Deng et al., 2013" startWordPosition="221" endWordPosition="224">ective way to guide manual annotation to find +/-effect senses that are not in the seed set. 1 Introduction Opinion mining (or sentiment analysis) identifies positive or negative opinions in many kinds of texts such as reviews, blogs, and news articles. It has been exploited in many application areas such as review mining, election analysis, and information extraction. While most previous research focusses on explicit opinion expressions, recent work addresses a type of opinion inference that arises when opinions are expressed toward events which have positive or negative effects on entities (Deng et al., 2013; Deng and Wiebe, 2014). We call such events +/-effect events.2 Deng and Wiebe (2014) show how sentiments toward one 1WordNet 3.0, http://wordnet.princeton.edu/ 2While the term goodFor/badFor is used in previous papers (Deng et al., 2013; Deng and Wiebe, 2014; Deng et al., 2014), we have since decided that +/-effect is a better term. entity may be propagated to other entities via opinion inference rules. They give the following example: (1) The bill would curb skyrocketing health care costs. The writer expresses an explicit negative sentiment (by skyrocketing) toward the object (health care co</context>
<context position="4532" citStr="Deng et al., 2013" startWordPosition="701" endWordPosition="704"> overall performance. Finally, because all WordNet verb senses are incorporated into the model, we investigate the ability of the method to identify unlabeled senses that are likely to be +/-effect senses. We find that by iteratively labeling the top-weighted unlabeled senses and rerunning the model, it may be used as an effective method for guiding annotation efforts. 2 Background There are many varieties of +/-effect events, including creation/destruction (changes in states involving existence), gain/loss (changes in states involving possession), and benefit/injury (Anand and Reschke, 2010; Deng et al., 2013). The creation, gain, and benefit classes are +effect events. For example, baking a cake has a positive effect on the cake because it is created;3 increasing the tax rate has a positive effect on the tax rate; and comforting the child has a positive effect on the child. The antonymous classes of each are -effect events: destroying the building has a negative effect on the building; demand decreasing has a negative effect on demand; and killing Bill has a negative effect on Bill.4 While sentiment (Esuli and Sebastiani, 2006; Wilson et al., 2005; Su and Markert, 2009) and connotation lexicons (F</context>
<context position="6259" citStr="Deng et al. (2013)" startWordPosition="987" endWordPosition="990"> crime, since performing a crime brings it into existence. 3Deng et al. (2013) point out that +/-effect objects are not equivalent to benefactive/malefactive semantic roles. An example they give is She baked a cake for me: a cake is the object of the +effect event baked as just noted, while me is the filler of its benefactive semantic role (Ziga and Kittil, 2010). 4Their annotation manual, which gives additional cases, is available with the annotated data at http://mpqa.cs.pitt.edu/. As we mentioned, the +/-effect ambiguity cannot be avoided in a word-level lexicon. In the +/-effect corpus of Deng et al. (2013),5 +/-effect events and their agents and objects are annotated at the word level. In that corpus, 1,411 +/-effect instances are annotated; 196 different +effect words and 286 different -effect words appear in these instances. Among them, 10 words appear in both +effect and -effect instances, accounting for 9.07% of all annotated instances. They show that +/-effect events (and the inferences that motivate this work) appear frequently in sentences with explicit sentiment. Further, all instances of +/-effect words that are not identified as +/-effect events are false hits from the perspective of </context>
<context position="13154" citStr="Deng et al. (2013)" startWordPosition="2103" endWordPosition="2106">ce each semantic frame covers multiple LUs. We believe that using FrameNet to find +/-effect words is easier than finding +/-effect words without any information since words may 6FrameNet, https://framenet.icsi.berkeley.edu/fndrupal/ 1183 be filtered by semantic frames. To select +/-effect words, an annotator (who is not a co-author) first identified promising frames as +/-effect and extracted all LUs from them. Then, he went through them and picked out the LUs which he judged to be +effect or -effect. In total, 736 +effect LUs and 601 -effect LUs were selected from 463 semantic frames. While Deng et al. (2013) and Deng and Wiebe (2014) specifically focus on events affecting objects (i.e., themes), we do not want to limit the lexicon to only that case. Sometimes, events have positive or negative effects on agents or other entities as well. Thus, in this paper, we consider a sense to be +effect (-effect) if it has +effect (-effect) on an entity, which may be the agent, the theme, or some other entity. In a previous paper (Choi et al., 2014), we conducted a study of the sense-level +/-effect property. For the evaluation, two annotators (who are co-authors of that paper) independently annotated senses </context>
</contexts>
<marker>Deng, Choi, Wiebe, 2013</marker>
<rawString>Lingjia Deng, Yoonjung Choi, and Janyce Wiebe. 2013. Benefactive/malefactive event and writer attitude annotation. In Proceedings of 51st ACL, pages 120–125.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lingjia Deng</author>
<author>Janyce Wiebe</author>
<author>Yoonjung Choi</author>
</authors>
<title>Joint inference and disambiguation of implicit sentiments via implicature constraints.</title>
<date>2014</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>7988</pages>
<contexts>
<context position="1774" citStr="Deng et al., 2014" startWordPosition="265" endWordPosition="268"> many application areas such as review mining, election analysis, and information extraction. While most previous research focusses on explicit opinion expressions, recent work addresses a type of opinion inference that arises when opinions are expressed toward events which have positive or negative effects on entities (Deng et al., 2013; Deng and Wiebe, 2014). We call such events +/-effect events.2 Deng and Wiebe (2014) show how sentiments toward one 1WordNet 3.0, http://wordnet.princeton.edu/ 2While the term goodFor/badFor is used in previous papers (Deng et al., 2013; Deng and Wiebe, 2014; Deng et al., 2014), we have since decided that +/-effect is a better term. entity may be propagated to other entities via opinion inference rules. They give the following example: (1) The bill would curb skyrocketing health care costs. The writer expresses an explicit negative sentiment (by skyrocketing) toward the object (health care costs). The event, curb, has a negative effect on costs, since they are reduced. We can reason that the writer is positive toward the event because it has a negative effect on costs, toward which the writer is negative. From there, we can reason that the writer is positive toward </context>
</contexts>
<marker>Deng, Wiebe, Choi, 2014</marker>
<rawString>Lingjia Deng, Janyce Wiebe, and Yoonjung Choi. 2014. Joint inference and disambiguation of implicit sentiments via implicature constraints. In Proceedings of COLING, page 7988.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrea Esuli</author>
<author>Fabrizio Sebastiani</author>
</authors>
<title>Sentiwordnet: A publicly available lexical resource for opinion mining.</title>
<date>2006</date>
<booktitle>In Proceedings of 5th LREC,</booktitle>
<pages>417--422</pages>
<contexts>
<context position="5060" citStr="Esuli and Sebastiani, 2006" startWordPosition="792" endWordPosition="795">s in states involving possession), and benefit/injury (Anand and Reschke, 2010; Deng et al., 2013). The creation, gain, and benefit classes are +effect events. For example, baking a cake has a positive effect on the cake because it is created;3 increasing the tax rate has a positive effect on the tax rate; and comforting the child has a positive effect on the child. The antonymous classes of each are -effect events: destroying the building has a negative effect on the building; demand decreasing has a negative effect on demand; and killing Bill has a negative effect on Bill.4 While sentiment (Esuli and Sebastiani, 2006; Wilson et al., 2005; Su and Markert, 2009) and connotation lexicons (Feng et al., 2011; Kang et al., 2014) are related, sentiment, connotation, and +/-effects are not the same; a single event may have different sentiment and +/-effect polarities, for example. Consider the following example: perpetrate: S: (v) perpetrate, commit, pull (perform an act, usually with a negative connotation) “perpetrate a crime”; “pull a bank robbery” This sense of perpetuate has a negative connotation, and is an objective term in SentiWordNet. However, it has a positive effect on the object, a crime, since perfo</context>
<context position="8305" citStr="Esuli and Sebastiani (2006)" startWordPosition="1321" endWordPosition="1324">blind to these cases. 3 Related Work Lexicons are widely used in sentiment analysis and opinion mining. Several works such as Hatzivassiloglou and McKeown (1997), Turney and Littman (2003), Kim and Hovy (2004), Strapparava and Valitutti (2004), and Peng and Park (2011) have tackled automatic lexicon expansion or acquistion. However, in most such work, the lexicons are word-level rather than sense-level. 5Called the goodFor/badFor corpus in that paper. 1182 For the related (but different) tasks of developing subjectivity, sentiment and connotation lexicons, some do take a sense-level approach. Esuli and Sebastiani (2006) construct SentiWordNet. They assume that terms with the same polarity tend to have similar glosses. So, they first expand a manually selected seed set of senses using WordNet lexical relations such as also-see and direct antonymy and train two classifiers, one for positive and another for negative. As features, a vectorial representation of glosses is adopted. These classifiers were applied to all WordNet senses to measure positive, negative, and objective scores. In extending their work (Esuli and Sebastiani, 2007), the PageRank algorithm is applied to rank senses in terms of how strongly th</context>
<context position="25765" citStr="Esuli and Sebastiani (2006)" startWordPosition="4334" endWordPosition="4337">nce is decreased a little bit in the -effect and Null classes. Since the average f-measure for all classes is the highest with hypernym (H), troponym (T), and verb group (V) relations (not entailment), we only consider these three relations when constructing the graph. 6 Supervised Learning applied to WordNet Glosses In WordNet, each sense contains a gloss consisting of a definition and optional example sentences. Since a gloss consists of several words and there are no direct links between glosses, we believe that a word vector representation is appropriate to utilize gloss information as in Esuli and Sebastiani (2006). For that, we adopt an SVM classifier. 6.1 Features Two different feature types are used. Word Features (WF): The bag-of-words model is applied. We do not ignore stop words for several reasons. Since most definitions and examples are not long, each gloss contains a small number of words. Also, among them, the total vocabulary of WordNet glosses is not large. Moreover, some prepositions such as against are sometimes useful to determine the polarity (+effect or -effect). Sentiment Features (SF): Some glosses of +effect (-effect) senses contain positive (negative) words. For instance, the defini</context>
</contexts>
<marker>Esuli, Sebastiani, 2006</marker>
<rawString>Andrea Esuli and Fabrizio Sebastiani. 2006. Sentiwordnet: A publicly available lexical resource for opinion mining. In Proceedings of 5th LREC, pages 417–422.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrea Esuli</author>
<author>Fabrizio Sebastiani</author>
</authors>
<title>Pageranking wordnet synsets: An application to opinion mining.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>424--431</pages>
<contexts>
<context position="8827" citStr="Esuli and Sebastiani, 2007" startWordPosition="1401" endWordPosition="1404">ity, sentiment and connotation lexicons, some do take a sense-level approach. Esuli and Sebastiani (2006) construct SentiWordNet. They assume that terms with the same polarity tend to have similar glosses. So, they first expand a manually selected seed set of senses using WordNet lexical relations such as also-see and direct antonymy and train two classifiers, one for positive and another for negative. As features, a vectorial representation of glosses is adopted. These classifiers were applied to all WordNet senses to measure positive, negative, and objective scores. In extending their work (Esuli and Sebastiani, 2007), the PageRank algorithm is applied to rank senses in terms of how strongly they are positive or negative. In the graph, each sense is one node, and two nodes are connected when they contain the same words in their WordNet glosses. Moreover, a random-walk step is adopted to refine the scores in their recent work (Baccianella et al., 2010). In contrast, our approach uses WordNet relations and graph propagation in addition to gloss classification. Gyamfi et al. (2009) construct a classifier to label the subjectivity of word senses. The hierarchical structure and domain information in WordNet are</context>
</contexts>
<marker>Esuli, Sebastiani, 2007</marker>
<rawString>Andrea Esuli and Fabrizio Sebastiani. 2007. Pageranking wordnet synsets: An application to opinion mining. In Proceedings of ACL, pages 424–431.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Song Feng</author>
<author>Ritwik Bose</author>
<author>Yejin Choi</author>
</authors>
<title>Learning general connotation of words using graph-based algorithms.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>1092--1103</pages>
<contexts>
<context position="5148" citStr="Feng et al., 2011" startWordPosition="807" endWordPosition="810">). The creation, gain, and benefit classes are +effect events. For example, baking a cake has a positive effect on the cake because it is created;3 increasing the tax rate has a positive effect on the tax rate; and comforting the child has a positive effect on the child. The antonymous classes of each are -effect events: destroying the building has a negative effect on the building; demand decreasing has a negative effect on demand; and killing Bill has a negative effect on Bill.4 While sentiment (Esuli and Sebastiani, 2006; Wilson et al., 2005; Su and Markert, 2009) and connotation lexicons (Feng et al., 2011; Kang et al., 2014) are related, sentiment, connotation, and +/-effects are not the same; a single event may have different sentiment and +/-effect polarities, for example. Consider the following example: perpetrate: S: (v) perpetrate, commit, pull (perform an act, usually with a negative connotation) “perpetrate a crime”; “pull a bank robbery” This sense of perpetuate has a negative connotation, and is an objective term in SentiWordNet. However, it has a positive effect on the object, a crime, since performing a crime brings it into existence. 3Deng et al. (2013) point out that +/-effect obj</context>
</contexts>
<marker>Feng, Bose, Choi, 2011</marker>
<rawString>Song Feng, Ritwik Bose, and Yejin Choi. 2011. Learning general connotation of words using graph-based algorithms. In Proceedings of EMNLP, pages 1092– 1103.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amit Goyal</author>
<author>Ellen Riloff</author>
<author>Hal DaumeIII</author>
</authors>
<title>Automatically producing plot unit representations for narrative text.</title>
<date>2010</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>77--86</pages>
<contexts>
<context position="11020" citStr="Goyal et al. (2010)" startWordPosition="1748" endWordPosition="1751">erated only for weighting edges between sense nodes and classification nodes, not for classifying all senses. Kang et al. (2014) present a unified model that assigns connotation polarities to both words and senses. They formulate the induction process as collective inference over pairwise-Markov Random Fields, and apply loopy belief propagation for inference. Their approach relies on selectional preferences of connotative predicates; the polarity of a connotation predicate suggests the polarity of its arguments. We have not discovered an analogous type of predicate for the problem we address. Goyal et al. (2010) generate a lexicon of patient polarity verbs (PPVs) that impart positive or negative states on their patients. They harvest PPVs from a Web corpus by co-occurance with Kind and Evil agents and by bootstrapping over conjunctions of verbs. Riloff et al. (2013) learn positive sentiment phrases and negative situation phrases from a corpus of tweets with hashtag “sarcasm”. However, both of these methods are word-level rather than sense-level. Ours is the first NLP research into developing a sense-level lexicon for events that have negative or positive effects on entities. 4 +/-Effect Word-Level Se</context>
<context position="38086" citStr="Goyal et al. (2010)" startWordPosition="6397" endWordPosition="6400">tions and a standard classifier using gloss information. A hybrid between the two gives the best results. Further, we provide evidence that the model is an effective way to guide manual annotation to find +/-effect words that are not in the seed word-level lexicon. This is important, as the likelihood that a random WordNet sense (and thus word) is +effect or -effect is not large. So as not to limit the inferences that may be drawn, our annotations include events that are +effect or -effect either the agent or object. In future work, we plan to exploit corpus-based methods using patterns as in Goyal et al. (2010) combined with semantic role labeling to refine the lexicon to distinguish which is the affected entity. Further, to actually exploit the acquired lexicon to process corpus data, an appropriate coarse-grained sense disambiguation process must be added, as Akkaya et al. (2009) and Akkaya et al. (2011) did for subjective/objective classification. We hope the general methodology will be effective for other semantic properties. In opinion mining and sentiment analysis this is partic10For reference, in 5th iteration, the +effect accuracy is 60.18% and the -effect accuracy is 69.93%, and in 6th iter</context>
</contexts>
<marker>Goyal, Riloff, DaumeIII, 2010</marker>
<rawString>Amit Goyal, Ellen Riloff, and Hal DaumeIII. 2010. Automatically producing plot unit representations for narrative text. In Proceedings of EMNLP, pages 77–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yaw Gyamfi</author>
<author>Janyce Wiebe</author>
<author>Rada Mihalcea</author>
<author>Cem Akkaya</author>
</authors>
<title>Integrating knowledge for subjectivity sense labeling.</title>
<date>2009</date>
<booktitle>In Proceedings of NAACL HLT</booktitle>
<pages>10--18</pages>
<contexts>
<context position="9297" citStr="Gyamfi et al. (2009)" startWordPosition="1482" endWordPosition="1485">assifiers were applied to all WordNet senses to measure positive, negative, and objective scores. In extending their work (Esuli and Sebastiani, 2007), the PageRank algorithm is applied to rank senses in terms of how strongly they are positive or negative. In the graph, each sense is one node, and two nodes are connected when they contain the same words in their WordNet glosses. Moreover, a random-walk step is adopted to refine the scores in their recent work (Baccianella et al., 2010). In contrast, our approach uses WordNet relations and graph propagation in addition to gloss classification. Gyamfi et al. (2009) construct a classifier to label the subjectivity of word senses. The hierarchical structure and domain information in WordNet are exploited to define features in terms of similarity (using the LCS metric in Resnik (1995)) of target senses and a seed set of senses. Also, the similarity of glosses in WordNet is considered. Even though they investigated the hierarchical structure by LCS values, WordNet relations are not exploited directly. Su and Markert (2009) adopt a semi-supervised mincut method to recognize the subjectivity of word senses. To construct a graph, each node corresponds to one W</context>
<context position="30588" citStr="Gyamfi et al. (2009)" startWordPosition="5120" endWordPosition="5123">ifying +/-effect senses. 7.1 Model Comparison To provide evidence for our assumption that different models are needed for different information to maximize effectiveness, we compare the hybrid method with the supervised learning and the graph-based learning (GSSL) methods, each utilizing both WordNet relations and gloss information. Supervised Learning (onlySL): The gloss classifier is trained with word features and sentiment features for WordNet Gloss. To exploit WordNet relations in supervised learning, especially the hierarchical information, we use least common subsumer (LCS) values as in Gyamfi et al. (2009), which, recall, performs supervised learning of subjective/objective senses. The values are calculated as follows. For a target sense t and a seed set S, the maximum LCS value between a target sense and a member of the seed set is found as: Score(t, S) = maxs∈SLCS(t, s) With this LCS feature and the features described in Section 6, we run SVM on the same training and test data. For LCS values, the similarity using the information content proposed by Resnik (1995) is measured. WordNet Similarity9 package provides pre-computed pairwise similarity values for that. Table 6 shows results of onlySL</context>
</contexts>
<marker>Gyamfi, Wiebe, Mihalcea, Akkaya, 2009</marker>
<rawString>Yaw Gyamfi, Janyce Wiebe, Rada Mihalcea, and Cem Akkaya. 2009. Integrating knowledge for subjectivity sense labeling. In Proceedings of NAACL HLT 2009, pages 10–18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasileios Hatzivassiloglou</author>
<author>Kathleen R McKeown</author>
</authors>
<title>Predicting the semantic orientation of adjectives.</title>
<date>1997</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>174--181</pages>
<contexts>
<context position="7839" citStr="Hatzivassiloglou and McKeown (1997)" startWordPosition="1247" endWordPosition="1251">ied” +effect S: (v) purge (rid of impurities) “purge the water”; “purge your mind” +effect This is part of the WordNet output for the word purge. In the first sense, the polarity is -effect since it has a negative effect on the object, Deng Xizo Ping. However, the other cases have positive effect on the object. Moreover, although a word may not have both +effect and -effect senses, it may have mixtures of ((+effect or -effect) and Null). A purely word-based approach is blind to these cases. 3 Related Work Lexicons are widely used in sentiment analysis and opinion mining. Several works such as Hatzivassiloglou and McKeown (1997), Turney and Littman (2003), Kim and Hovy (2004), Strapparava and Valitutti (2004), and Peng and Park (2011) have tackled automatic lexicon expansion or acquistion. However, in most such work, the lexicons are word-level rather than sense-level. 5Called the goodFor/badFor corpus in that paper. 1182 For the related (but different) tasks of developing subjectivity, sentiment and connotation lexicons, some do take a sense-level approach. Esuli and Sebastiani (2006) construct SentiWordNet. They assume that terms with the same polarity tend to have similar glosses. So, they first expand a manually </context>
</contexts>
<marker>Hatzivassiloglou, McKeown, 1997</marker>
<rawString>Vasileios Hatzivassiloglou and Kathleen R. McKeown. 1997. Predicting the semantic orientation of adjectives. In Proceedings of ACL, pages 174–181.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun Seok Kang</author>
<author>Song Feng</author>
<author>Leman Akoglu</author>
<author>Yejin Choi</author>
</authors>
<title>Connotationwordnet: Learning connotation over the word+sense network.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd ACL,</booktitle>
<pages>15441554</pages>
<contexts>
<context position="5168" citStr="Kang et al., 2014" startWordPosition="811" endWordPosition="814">in, and benefit classes are +effect events. For example, baking a cake has a positive effect on the cake because it is created;3 increasing the tax rate has a positive effect on the tax rate; and comforting the child has a positive effect on the child. The antonymous classes of each are -effect events: destroying the building has a negative effect on the building; demand decreasing has a negative effect on demand; and killing Bill has a negative effect on Bill.4 While sentiment (Esuli and Sebastiani, 2006; Wilson et al., 2005; Su and Markert, 2009) and connotation lexicons (Feng et al., 2011; Kang et al., 2014) are related, sentiment, connotation, and +/-effects are not the same; a single event may have different sentiment and +/-effect polarities, for example. Consider the following example: perpetrate: S: (v) perpetrate, commit, pull (perform an act, usually with a negative connotation) “perpetrate a crime”; “pull a bank robbery” This sense of perpetuate has a negative connotation, and is an objective term in SentiWordNet. However, it has a positive effect on the object, a crime, since performing a crime brings it into existence. 3Deng et al. (2013) point out that +/-effect objects are not equival</context>
<context position="10529" citStr="Kang et al. (2014)" startWordPosition="1675" endWordPosition="1678">nd is connected to two classification nodes (one for subjectivity and another for objectivity) via a weighted edge that is assigned by a classifier. For this classifier, WordNet glosses, relations, and monosemous features are considered. Also, several WordNet relations (e.g., antonymy, similiar-to, direct hypernym, etc.) are used to connect two nodes. Although they make use of both WordNet glosses and relations, and gloss information is utilized for a classifier, this classifier is generated only for weighting edges between sense nodes and classification nodes, not for classifying all senses. Kang et al. (2014) present a unified model that assigns connotation polarities to both words and senses. They formulate the induction process as collective inference over pairwise-Markov Random Fields, and apply loopy belief propagation for inference. Their approach relies on selectional preferences of connotative predicates; the polarity of a connotation predicate suggests the polarity of its arguments. We have not discovered an analogous type of predicate for the problem we address. Goyal et al. (2010) generate a lexicon of patient polarity verbs (PPVs) that impart positive or negative states on their patient</context>
</contexts>
<marker>Kang, Feng, Akoglu, Choi, 2014</marker>
<rawString>Jun Seok Kang, Song Feng, Leman Akoglu, and Yejin Choi. 2014. Connotationwordnet: Learning connotation over the word+sense network. In Proceedings of the 52nd ACL, page 15441554.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Soo-Min Kim</author>
<author>Eduard Hovy</author>
</authors>
<title>Determining the sentiment of opinions.</title>
<date>2004</date>
<booktitle>In Proceedings of 20th COLING,</booktitle>
<pages>1367--1373</pages>
<contexts>
<context position="7887" citStr="Kim and Hovy (2004)" startWordPosition="1256" endWordPosition="1259"> “purge your mind” +effect This is part of the WordNet output for the word purge. In the first sense, the polarity is -effect since it has a negative effect on the object, Deng Xizo Ping. However, the other cases have positive effect on the object. Moreover, although a word may not have both +effect and -effect senses, it may have mixtures of ((+effect or -effect) and Null). A purely word-based approach is blind to these cases. 3 Related Work Lexicons are widely used in sentiment analysis and opinion mining. Several works such as Hatzivassiloglou and McKeown (1997), Turney and Littman (2003), Kim and Hovy (2004), Strapparava and Valitutti (2004), and Peng and Park (2011) have tackled automatic lexicon expansion or acquistion. However, in most such work, the lexicons are word-level rather than sense-level. 5Called the goodFor/badFor corpus in that paper. 1182 For the related (but different) tasks of developing subjectivity, sentiment and connotation lexicons, some do take a sense-level approach. Esuli and Sebastiani (2006) construct SentiWordNet. They assume that terms with the same polarity tend to have similar glosses. So, they first expand a manually selected seed set of senses using WordNet lexica</context>
</contexts>
<marker>Kim, Hovy, 2004</marker>
<rawString>Soo-Min Kim and Eduard Hovy. 2004. Determining the sentiment of opinions. In Proceedings of 20th COLING, pages 1367–1373.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
<author>Richard Beckwith</author>
<author>Christiane Fellbaum</author>
<author>Derek Gross</author>
<author>Katherine Miller</author>
</authors>
<title>Wordnet: An on-line lexical database.</title>
<date>1990</date>
<journal>International Journal of Lexicography,</journal>
<volume>13</volume>
<issue>4</issue>
<contexts>
<context position="16324" citStr="Miller et al., 1990" startWordPosition="2674" endWordPosition="2677"> model, and as a training set by the gloss classifer. Table 1 shows the distribution of the data. In total, there are 258 +effect senses, 487 -effect senses, and 880 Null senses. To avoid too large a bias toward the Null class,7 we randomly chose half (i.e., the Null set contains 440 senses). Half of each set is used as seed and training data, and the other half is used for evaluation. +effect -effect Null # annotated data 258 487 880 # Seed/TrainSet 129 243 220 # TestSet 129 244 220 Table 1: Distribution of annotated data. 5 Graph-based Semi-Supervised Learning for WordNet Relations WordNet (Miller et al., 1990) is organized by semantic relations such as hypernymy, troponymy, grouping, and so on. These semantic relations can be used to build a network. Since the most frequently encoded relation is the super-subordinate relation, most verb senses are arranged into hierarchies; verb senses towards the bottom of the graph express increasingly specific manner. Thus, by following this hierarchical information, we hypothesized that +/-effect polarity tends to propagate. We use a graph-based semi-supervised learning (GSSL) method to carry out the label propagation. 5.1 Graph Formulation We formulate a graph</context>
</contexts>
<marker>Miller, Beckwith, Fellbaum, Gross, Miller, 1990</marker>
<rawString>George A. Miller, Richard Beckwith, Christiane Fellbaum, Derek Gross, and Katherine Miller. 1990. Wordnet: An on-line lexical database. International Journal of Lexicography, 13(4):235–312.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Peng</author>
<author>Dae Hoon Park</author>
</authors>
<title>Generate adjective sentiment dictionary for social media sentiment analysis using constrained nonnegative matrix factorization.</title>
<date>2011</date>
<booktitle>In Proceedings of ICWSM.</booktitle>
<contexts>
<context position="7947" citStr="Peng and Park (2011)" startWordPosition="1266" endWordPosition="1269">ut for the word purge. In the first sense, the polarity is -effect since it has a negative effect on the object, Deng Xizo Ping. However, the other cases have positive effect on the object. Moreover, although a word may not have both +effect and -effect senses, it may have mixtures of ((+effect or -effect) and Null). A purely word-based approach is blind to these cases. 3 Related Work Lexicons are widely used in sentiment analysis and opinion mining. Several works such as Hatzivassiloglou and McKeown (1997), Turney and Littman (2003), Kim and Hovy (2004), Strapparava and Valitutti (2004), and Peng and Park (2011) have tackled automatic lexicon expansion or acquistion. However, in most such work, the lexicons are word-level rather than sense-level. 5Called the goodFor/badFor corpus in that paper. 1182 For the related (but different) tasks of developing subjectivity, sentiment and connotation lexicons, some do take a sense-level approach. Esuli and Sebastiani (2006) construct SentiWordNet. They assume that terms with the same polarity tend to have similar glosses. So, they first expand a manually selected seed set of senses using WordNet lexical relations such as also-see and direct antonymy and train t</context>
</contexts>
<marker>Peng, Park, 2011</marker>
<rawString>Wei Peng and Dae Hoon Park. 2011. Generate adjective sentiment dictionary for social media sentiment analysis using constrained nonnegative matrix factorization. In Proceedings of ICWSM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Using information content to evaluate semantic similarity.</title>
<date>1995</date>
<booktitle>In Proceedings of 14th IJCAI,</booktitle>
<pages>448--453</pages>
<contexts>
<context position="9518" citStr="Resnik (1995)" startWordPosition="1521" endWordPosition="1522">ey are positive or negative. In the graph, each sense is one node, and two nodes are connected when they contain the same words in their WordNet glosses. Moreover, a random-walk step is adopted to refine the scores in their recent work (Baccianella et al., 2010). In contrast, our approach uses WordNet relations and graph propagation in addition to gloss classification. Gyamfi et al. (2009) construct a classifier to label the subjectivity of word senses. The hierarchical structure and domain information in WordNet are exploited to define features in terms of similarity (using the LCS metric in Resnik (1995)) of target senses and a seed set of senses. Also, the similarity of glosses in WordNet is considered. Even though they investigated the hierarchical structure by LCS values, WordNet relations are not exploited directly. Su and Markert (2009) adopt a semi-supervised mincut method to recognize the subjectivity of word senses. To construct a graph, each node corresponds to one WordNet sense and is connected to two classification nodes (one for subjectivity and another for objectivity) via a weighted edge that is assigned by a classifier. For this classifier, WordNet glosses, relations, and monos</context>
<context position="31056" citStr="Resnik (1995)" startWordPosition="5204" endWordPosition="5205">t relations in supervised learning, especially the hierarchical information, we use least common subsumer (LCS) values as in Gyamfi et al. (2009), which, recall, performs supervised learning of subjective/objective senses. The values are calculated as follows. For a target sense t and a seed set S, the maximum LCS value between a target sense and a member of the seed set is found as: Score(t, S) = maxs∈SLCS(t, s) With this LCS feature and the features described in Section 6, we run SVM on the same training and test data. For LCS values, the similarity using the information content proposed by Resnik (1995) is measured. WordNet Similarity9 package provides pre-computed pairwise similarity values for that. Table 6 shows results of onlySL. Compared to Table 4, while +effect and Null classes show a slight improvement, the performance is degraded for -effect. This means that the added feature is rather harmful to -effect. Even though the hierarchical feature is very helpful to expand +/-effect, it is not helpful for onlySL since SVM cannot capture propagation according to the hierarchy. Graph-based Learning (onlyGraph): In Section 5, the graph is constructed by using WordNet relations. To apply Word</context>
</contexts>
<marker>Resnik, 1995</marker>
<rawString>Philip Resnik. 1995. Using information content to evaluate semantic similarity. In Proceedings of 14th IJCAI, pages 448–453.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Riloff</author>
<author>Ashequl Qadir</author>
<author>Prafulla Surve</author>
<author>Lalindra De Silva</author>
<author>Nathan Gilbert</author>
<author>Ruihong Huang</author>
</authors>
<title>Sarcasm as contrast between a positive sentiment and negative situation.</title>
<date>2013</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>704--714</pages>
<marker>Riloff, Qadir, Surve, De Silva, Gilbert, Huang, 2013</marker>
<rawString>Ellen Riloff, Ashequl Qadir, Prafulla Surve, Lalindra De Silva, Nathan Gilbert, and Ruihong Huang. 2013. Sarcasm as contrast between a positive sentiment and negative situation. In Proceedings of EMNLP, pages 704–714.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carlo Strapparava</author>
<author>Alessandro Valitutti</author>
</authors>
<title>Wordnet-affect: An affective extension of wordnet.</title>
<date>2004</date>
<booktitle>In Proceedings of 4th LREC,</booktitle>
<pages>1083--1086</pages>
<contexts>
<context position="7921" citStr="Strapparava and Valitutti (2004)" startWordPosition="1260" endWordPosition="1264">ffect This is part of the WordNet output for the word purge. In the first sense, the polarity is -effect since it has a negative effect on the object, Deng Xizo Ping. However, the other cases have positive effect on the object. Moreover, although a word may not have both +effect and -effect senses, it may have mixtures of ((+effect or -effect) and Null). A purely word-based approach is blind to these cases. 3 Related Work Lexicons are widely used in sentiment analysis and opinion mining. Several works such as Hatzivassiloglou and McKeown (1997), Turney and Littman (2003), Kim and Hovy (2004), Strapparava and Valitutti (2004), and Peng and Park (2011) have tackled automatic lexicon expansion or acquistion. However, in most such work, the lexicons are word-level rather than sense-level. 5Called the goodFor/badFor corpus in that paper. 1182 For the related (but different) tasks of developing subjectivity, sentiment and connotation lexicons, some do take a sense-level approach. Esuli and Sebastiani (2006) construct SentiWordNet. They assume that terms with the same polarity tend to have similar glosses. So, they first expand a manually selected seed set of senses using WordNet lexical relations such as also-see and d</context>
</contexts>
<marker>Strapparava, Valitutti, 2004</marker>
<rawString>Carlo Strapparava and Alessandro Valitutti. 2004. Wordnet-affect: An affective extension of wordnet. In Proceedings of 4th LREC, pages 1083–1086.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fangzhong Su</author>
<author>Katja Markert</author>
</authors>
<title>Subjectivity recognition on word senses via semi-supervised mincuts.</title>
<date>2009</date>
<booktitle>In Proceedings of NAACL HLT</booktitle>
<pages>1--9</pages>
<contexts>
<context position="5104" citStr="Su and Markert, 2009" startWordPosition="800" endWordPosition="803">jury (Anand and Reschke, 2010; Deng et al., 2013). The creation, gain, and benefit classes are +effect events. For example, baking a cake has a positive effect on the cake because it is created;3 increasing the tax rate has a positive effect on the tax rate; and comforting the child has a positive effect on the child. The antonymous classes of each are -effect events: destroying the building has a negative effect on the building; demand decreasing has a negative effect on demand; and killing Bill has a negative effect on Bill.4 While sentiment (Esuli and Sebastiani, 2006; Wilson et al., 2005; Su and Markert, 2009) and connotation lexicons (Feng et al., 2011; Kang et al., 2014) are related, sentiment, connotation, and +/-effects are not the same; a single event may have different sentiment and +/-effect polarities, for example. Consider the following example: perpetrate: S: (v) perpetrate, commit, pull (perform an act, usually with a negative connotation) “perpetrate a crime”; “pull a bank robbery” This sense of perpetuate has a negative connotation, and is an objective term in SentiWordNet. However, it has a positive effect on the object, a crime, since performing a crime brings it into existence. 3Den</context>
<context position="9760" citStr="Su and Markert (2009)" startWordPosition="1559" endWordPosition="1562">k (Baccianella et al., 2010). In contrast, our approach uses WordNet relations and graph propagation in addition to gloss classification. Gyamfi et al. (2009) construct a classifier to label the subjectivity of word senses. The hierarchical structure and domain information in WordNet are exploited to define features in terms of similarity (using the LCS metric in Resnik (1995)) of target senses and a seed set of senses. Also, the similarity of glosses in WordNet is considered. Even though they investigated the hierarchical structure by LCS values, WordNet relations are not exploited directly. Su and Markert (2009) adopt a semi-supervised mincut method to recognize the subjectivity of word senses. To construct a graph, each node corresponds to one WordNet sense and is connected to two classification nodes (one for subjectivity and another for objectivity) via a weighted edge that is assigned by a classifier. For this classifier, WordNet glosses, relations, and monosemous features are considered. Also, several WordNet relations (e.g., antonymy, similiar-to, direct hypernym, etc.) are used to connect two nodes. Although they make use of both WordNet glosses and relations, and gloss information is utilized</context>
</contexts>
<marker>Su, Markert, 2009</marker>
<rawString>Fangzhong Su and Katja Markert. 2009. Subjectivity recognition on word senses via semi-supervised mincuts. In Proceedings of NAACL HLT 2009, pages 1–9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simon Tong</author>
<author>Daphne Koller</author>
</authors>
<title>Support vector machin active learning with applications to text classification.</title>
<date>2001</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>2--45</pages>
<contexts>
<context position="35527" citStr="Tong and Koller, 2001" startWordPosition="5946" endWordPosition="5949"> +effect and -effect classes as more seeds are added, mainly due to improvements in recall. The evaluation on the fixed set is also useful in the annotation process because it trades off +/-effect vs. Null annotations. If the new manual annotations were biased, in that they incorrectly label Null senses as +/-effect, then the f-measure results would instead degrade on the fixed TestSet, since the system is created each time using the increased seed set. We now consider the accuracy of the system on the newly labeled annotated data in Step 2. Note that our method is similar to Active Learning (Tong and Koller, 2001), in that both automatically identify which unlabeled instances the human should annotate next. However, in active learning, the goal is to find instances that are difficult for a supervised learning system. In our case, the goal is to find needles in the haystack of WordNet senses. In Step 3, we add the newly labeled senses to the seed set, enabling the model to find unlabeled senses close to the new seeds when the system is rerun for the next iteration. We assess the system’s accuracy on the newly labeled data by comparing the system’s labels with the human’s new labels. Accuracy for +effect</context>
</contexts>
<marker>Tong, Koller, 2001</marker>
<rawString>Simon Tong and Daphne Koller. 2001. Support vector machin active learning with applications to text classification. Journal of Machine Learning Research, 2:45–66.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Turney</author>
<author>Michael Littman</author>
</authors>
<title>Measuring praise and criticism: Inference of semantic orientation from association.</title>
<date>2003</date>
<journal>ACM Transactions on Information Systems,</journal>
<volume>21</volume>
<issue>4</issue>
<contexts>
<context position="7866" citStr="Turney and Littman (2003)" startWordPosition="1252" endWordPosition="1255">urities) “purge the water”; “purge your mind” +effect This is part of the WordNet output for the word purge. In the first sense, the polarity is -effect since it has a negative effect on the object, Deng Xizo Ping. However, the other cases have positive effect on the object. Moreover, although a word may not have both +effect and -effect senses, it may have mixtures of ((+effect or -effect) and Null). A purely word-based approach is blind to these cases. 3 Related Work Lexicons are widely used in sentiment analysis and opinion mining. Several works such as Hatzivassiloglou and McKeown (1997), Turney and Littman (2003), Kim and Hovy (2004), Strapparava and Valitutti (2004), and Peng and Park (2011) have tackled automatic lexicon expansion or acquistion. However, in most such work, the lexicons are word-level rather than sense-level. 5Called the goodFor/badFor corpus in that paper. 1182 For the related (but different) tasks of developing subjectivity, sentiment and connotation lexicons, some do take a sense-level approach. Esuli and Sebastiani (2006) construct SentiWordNet. They assume that terms with the same polarity tend to have similar glosses. So, they first expand a manually selected seed set of senses</context>
</contexts>
<marker>Turney, Littman, 2003</marker>
<rawString>Peter Turney and Michael Littman. 2003. Measuring praise and criticism: Inference of semantic orientation from association. ACM Transactions on Information Systems, 21(4):315–346.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
</authors>
<title>Recognizing contextual polarity in phraselevel sentiment analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of HLTEMNLP,</booktitle>
<pages>347--354</pages>
<marker>Wilson, Wiebe, 2005</marker>
<rawString>Theresa Wilson, Janyce Wiebe, , and Paul Hoffmann. 2005. Recognizing contextual polarity in phraselevel sentiment analysis. In Proceedings of HLTEMNLP, pages 347–354.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dengyong Zhou</author>
<author>Olivier Bousquet</author>
<author>Thomas Navin Lal</author>
<author>Jason Weston</author>
<author>Bernhard Scholkopf</author>
</authors>
<title>Learning with local and global consistency.</title>
<date>2004</date>
<booktitle>Advances in Neural Information Processing Systems,</booktitle>
<pages>16--321</pages>
<contexts>
<context position="19066" citStr="Zhou et al. (2004)" startWordPosition="3161" endWordPosition="3164">nected by this relation are co-extensive, we can assume that both are the same type of event. The synonym relation is not used because it is already defined in senses (i.e., each node in the graph is a synset), and the antonym relation is also not applied since the weight matrix should be non-negative. The weight value of all edges is 1.0. 5.2 Label Propagation Given a constructed graph, the label inference (or prediction) task is to propagate the seed labels to the unlabeled nodes. One of the classic GSSL label propagation methods is the local and global consistency (LGC) method suggested by Zhou et al. (2004). The LGC method is a graph transduction algorithm which is sufficiently smooth with respect to the intrinsic structure revealed by known labeled and unlabeled data. The cost function typically involves a tradeoff between the smoothness of the predicted labels over the entire graph and the accuracy of the predicted labels in fitting the given labeled nodes XL. LGC fits in a univariate regularization framework, where the output matrix is treated as the only variable in optimization, and the optimal solutions can be easily obtained by solving a linear system. Thus, we adopt the LGC method in thi</context>
</contexts>
<marker>Zhou, Bousquet, Lal, Weston, Scholkopf, 2004</marker>
<rawString>Dengyong Zhou, Olivier Bousquet, Thomas Navin Lal, Jason Weston, and Bernhard Scholkopf. 2004. Learning with local and global consistency. Advances in Neural Information Processing Systems, 16:321–329.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fernando Ziga</author>
<author>Seppo Kittil</author>
</authors>
<title>Benefactives and malefactives, Typological perspectives and case studies.</title>
<date>2010</date>
<publisher>John Benjamins Publishing.</publisher>
<contexts>
<context position="6006" citStr="Ziga and Kittil, 2010" startWordPosition="949" endWordPosition="952"> commit, pull (perform an act, usually with a negative connotation) “perpetrate a crime”; “pull a bank robbery” This sense of perpetuate has a negative connotation, and is an objective term in SentiWordNet. However, it has a positive effect on the object, a crime, since performing a crime brings it into existence. 3Deng et al. (2013) point out that +/-effect objects are not equivalent to benefactive/malefactive semantic roles. An example they give is She baked a cake for me: a cake is the object of the +effect event baked as just noted, while me is the filler of its benefactive semantic role (Ziga and Kittil, 2010). 4Their annotation manual, which gives additional cases, is available with the annotated data at http://mpqa.cs.pitt.edu/. As we mentioned, the +/-effect ambiguity cannot be avoided in a word-level lexicon. In the +/-effect corpus of Deng et al. (2013),5 +/-effect events and their agents and objects are annotated at the word level. In that corpus, 1,411 +/-effect instances are annotated; 196 different +effect words and 286 different -effect words appear in these instances. Among them, 10 words appear in both +effect and -effect instances, accounting for 9.07% of all annotated instances. They </context>
</contexts>
<marker>Ziga, Kittil, 2010</marker>
<rawString>Fernando Ziga and Seppo Kittil. 2010. Benefactives and malefactives, Typological perspectives and case studies. John Benjamins Publishing.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
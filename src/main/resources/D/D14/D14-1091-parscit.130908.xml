<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003216">
<title confidence="0.953195">
Syllable weight encodes mostly the same information for English word
segmentation as dictionary stress
</title>
<author confidence="0.998208">
John K Pate Mark Johnson
</author>
<affiliation confidence="0.9951765">
Centre for Language Technology
Macquarie University
</affiliation>
<address confidence="0.667752">
Sydney, NSW, Australia
</address>
<email confidence="0.998633">
{john.pate,mark.johnson}@mq.edu.au
</email>
<sectionHeader confidence="0.994794" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998823">
Stress is a useful cue for English word
segmentation. A wide range of computa-
tional models have found that stress cues
enable a 2-10% improvement in segmen-
tation accuracy, depending on the kind of
model, by using input that has been anno-
tated with stress using a pronouncing dic-
tionary. However, stress is neither invari-
ably produced nor unambiguously iden-
tifiable in real speech. Heavy syllables,
i.e. those with long vowels or syllable
codas, attract stress in English. We de-
vise Adaptor Grammar word segmentation
models that exploit either stress, or sylla-
ble weight, or both, and evaluate the util-
ity of syllable weight as a cue to word
boundaries. Our results suggest that sylla-
ble weight encodes largely the same infor-
mation for word segmentation in English
that annotated dictionary stress does.
</bodyText>
<sectionHeader confidence="0.998784" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999988431034483">
One of the first skills a child must develop in the
course of language acquisition is the ability to seg-
ment speech into words. Stress has long been
recognized as a useful cue for English word seg-
mentation, following the observation that words
in English are predominantly stress-initial (Cutler
and Carter, 1987), together with the result that 9-
month-old English-learning infants prefer stress-
initial stimuli (Jusczyk et al., 1993). A range of
statistical (Doyle and Levy, 2013; Christiansen et
al., 1998; B¨orschinger and Johnson, 2014) and
rule-based (Yang, 2004; Lignos and Yang, 2010)
models have used stress information to improve
word segmentation. However, that work uses
stress-marked input prepared by marking vowels
that are listed as stressed in a pronouncing dic-
tionary. This pre-processing step glosses over the
fact that stress identification itself involves a non-
trival learning problem, since stress has many pos-
sible phonetic reflexes and no known invariants
(Campbell and Beckman, 1997; Fry, 1955; Fry,
1958). One known strong correlate of stress in
English is syllable weight: heavy syllables, which
end in a consonant or have a long vowel, at-
tract stress in English. We present experiments
with Bayesian Adaptor Grammars (Johnson et al.,
2007) that suggest syllable weight encodes largely
the same information for word segmentation that
dictionary stress information does.
Specifically, we modify the Adaptor
Grammar word segmentation model of
B¨orschinger and Johnson (2014) to compare
the utility of syllable weight and stress cues for
finding word boundaries, both individually and in
combination. We describe how a shortcoming of
Adaptor Grammars prevents us from comparing
stress and weight cues in combination with the full
range of phonotactic cues for word segmentation,
and design two experiments to work around this
limitation. The first experiment uses grammars
that provide parallel analyses for syllable weight
and stress, and learns initial/non-initial phonotac-
tic distinctions. In this first experiment, syllable
weight cues are actually more useful than stress
cues at larger input sizes. The second experiment
focuses on incorporating phonotactic cues for
typical word-final consonant clusters (such as
inflectional morphemes), at the expense of parallel
structures. In this second experiment, weight cues
merely match stress cues at larger input sizes,
and the learning curve for the combined weight-
and-stress grammar follows almost perfectly with
the stress-only grammar. This second experiment
suggests that the advantage of weight over stress
in the first experiment was purely due to poor
modeling of word-final consonant clusters by
the stress-only grammar, not weight per se. All
together, these results indicate that syllable weight
</bodyText>
<page confidence="0.975629">
844
</page>
<note confidence="0.9102755">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 844–853,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.9997998">
is highly redundant with dictionary-based stress
for the purposes of English word segmentation;
in fact, in our experiments, there is no detectable
difference between relying on syllable weight and
relying on dictionary stress.
</bodyText>
<sectionHeader confidence="0.972761" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.99997783908046">
Stress is the perception that some syllables are
more prominent than others, and reflects a com-
plex, language-specific interaction between acous-
tic cues (such as loudness and duration), and
phonological patterns (such as syllable shapes).
The details on how stress is assigned, produced,
and perceived vary greatly across languages.
Three aspects of the English stress system are
relevant for this paper. First, although English
stress can shift in different contexts (Liberman and
Prince, 1977), such as from the first syllable of
‘fourteen’ in isolation to the second syllable when
followed by a stressed syllable, it is largely stable
across different tokens of a given word. Second,
most words in English end up being stress-initial
on a type and token basis. Third, heavy syllables
(those with a long vowel or a consonant coda) at-
tract stress in English.
There is experimental evidence that English-
learning infants prefer stress-initial words from
around the age of seven months (Jusczyk et al.,
1993; Juszcyk et al., 1999; Jusczyk et al., 1993;
Thiessen and Saffran, 2003). A variety of com-
putational models have subsequently been devel-
oped that take stress-annotated input and use this
regularity to improve segmentation accuracy. The
earliest Simple Recurrent Network (SRN) mod-
eling experiments of Christiansen et al. (1998)
and Christiansen and Curtin (1999) found that
stress improved word segmentation from about
39% to 43% token f-score (see Evaluation). Ryt-
ting et al. (2010) applied the SRN model to prob-
ability distributions over phones obtained from a
speech recognition system, and found that the en-
tropy of the probability distribution over phones,
as a proxy to local hyperarticulation and hence a
stress cue, improved token f-score from about 16%
to 23%. In a deterministic approach using pre-
syllabified input, Yang (2004), with follow-ups in
Lignos and Yang (2010) and Lignos (2011; 2012),
showed that a ‘Unique Stress Constraint’ (USC),
or assuming each word has at most one stressed
syllable, leads to an improvement of about 2.5%
boundary f-score.
Among explicitly probabilistic models,
Doyle and Levy (2013) incorporated stress into
Goldwater et al.’s (2009) Bigram model. They
did this by modifying the base distribution over
lexical forms to generate not simply phone strings
but a sequence of syllables that may or may
not be stressed. The resulting model can learn
that some sequences of syllables (in particular,
sequences that start with a stressed syllable)
are more likely than others. However, observed
stress improved token f-score by only 1%.
B¨orschinger and Johnson (2014) used Adaptor
Grammars (Johnson et al., 2007), a generalization
of Goldwater et al.’s (2009) Bigram model that
will be described shortly, and found a clearer
4-10% advantage in token f-score, depending on
the amount of training data.
Together, the experimental and computational
results suggest that infants in fact pay attention
to stress, and that stress carries useful information
for segmenting words in running speech. How-
ever, stress identification is itself a non-trivial
task, as stress has many highly variable, context-
sensitive, and optional phonetic reflexes. How-
ever, one strong phonological cue in English is
syllable weight: heavy syllables attract stress.
Heavy syllables, in turn, are syllables with a
coda and/or a long vowel, which, in English,
are tense vowels. Turk et al. (1995) replicated
the Jusczyk et al. (1993) finding that English-
learning infants prefer stress-initial stimuli (using
non-words), and then examined how stress inter-
acted with syllable weight. They found that sylla-
ble weight was not a necessary condition to trig-
ger the preference: infants preferred stress-initial
stimuli even if the initial syllable was light. How-
ever, they also found that infants most strongly
preferred stimuli whose first syllable was both
stressed and heavy: infants preferred stress-initial
and heavy-initial stimuli to stress-initial and light-
initial stimuli. This result suggests that infants are
sensitive to syllable weight in determining typical
stress and rythmic patterns in their language.
</bodyText>
<subsectionHeader confidence="0.991858">
2.1 Models
</subsectionHeader>
<bodyText confidence="0.998873">
We will adopt the Adaptor Grammar framework
used by B¨orschinger and Johnson (2014) to ex-
plore the utility of syllable weight as a cue
to word segmentation by way of its covariance
with stress. Adaptor Grammars are Probabilis-
tic Context Free Grammars (PCFGs) with a spe-
</bodyText>
<page confidence="0.995877">
845
</page>
<figure confidence="0.927493833333333">
Syll SyllIF SyllIF
Onset Rhyme OnsetI RhymeIF OnsetI RhymeF
k
k
k Nucleus
Coda
NucleusI
CodaF
NucleusF
CodaF
X is X is X is
(a) Basic syllable. (b) Mono-syllable with initial Rhyme. (c) Mono-syllable with final Rhyme.
</figure>
<figureCaption confidence="0.9966245">
Figure 1: Different ways to incorporate phonotactics. It is not possible to capture word-final codas and
word initial rhymes in monosyllabic words with factors the size of a PCFG rule.
</figureCaption>
<bodyText confidence="0.998558153846154">
cial set of adapted non-terminal nodes. We un-
derline adapted non-terminals (X) to distinguish
them from non-adapted non-terminals (Y). While
a vanilla PCFG can only directly model regular-
ities that are expressed by a single re-write rule,
an Adaptor Grammar model caches entire subtrees
that are rooted at adapted non-terminals. Adaptor
Grammars can thus learn the internal structure of
words, such as syllables, syllable onsets, and syl-
lable rhymes, while still learning entire words as
well.
In Adaptor Grammars, parameters are associ-
ated with PCFG rules. While this has been a useful
factorization in previous work, it makes it difficult
to integrate syllable weight and syllable stress in
a linguistically natural way. A syllable is typically
analyzed as having an optional onset followed by a
rhyme, with the rhyme rewriting to a nucleus (the
vowel) followed by an optional coda, as in Fig-
ure 1a. We expect stress and syllable weight to be
useful primarily because initial syllables tend to be
different from non-initial syllables. However, dis-
tinguishing final from non-final codas should be
useful as well, due to the frequency of suffixes in
English, and the importance of edge phenomena in
phonology more generally (Brent and Cartwright,
1996). These principles come into conflict when
modeling monosyllabic words. If we say that a
monosyllable is an Initial and Final SyllIF, and
has an initial Onset and an initial Rhyme, as in
Figure 1b, then we can learn the initial/non-initial
generalization about stressed or heavy rhymes at
the expense of the generalization about final and
non-final codas. If we say that a monosyllable is
an initial onset with a final rhyme, the reverse oc-
curs: we can learn the final/non-final coda gen-
eralization at the expense of the initial/non-initial
regularities. If we split the symbols further, we’d
generalize even less: we’d essentially have to learn
the initial/non-initial patterns separately for mono-
syllables and polysyllables.
The most direct solution would introduce fac-
tors that are ‘smaller’ than a single PCFG rule. Es-
sentially, we would compute the score of a PCFG
rule in terms of multiple features of its right-hand
side, rather than a single ‘one-hot’ feature identi-
fying the expansion. We left this direction for fu-
ture work and instead carried out two experiments
using Adaptor Grammars that were designed to
work around this limitation.
Our first experiment focuses on modeling
the initial/non-initial distinction, leaving the
final/non-final coda distinction unmodeled. The
models in this experiment assume parallel struc-
tures for syllable weight and stress, and focus on
providing the most direct comparison between syl-
lable weight and stress with a strictly initial/non-
initial distinction. This first experiment shows that
observing dictionary stress is better early in learn-
ing, but that modeling syllable weight is better
later in learning. However, it is possible that sylla-
ble weight was more useful because modeling syl-
lable weight involves modeling the characteristics
of codas; the advantage may not have been due to
weight per se but due to having learned something
about the effects of suffixes on final codas.
Our second experiment focuses on modeling
some aspects of final codas at the expense of main-
taining a rigid parallelism in the structures for syl-
lable weight and stress. The models in this exper-
iment split only those symbols that are necessary
to bring stress or weight patterns into the expres-
sive power of the model, and focus on comparing
richer models of syllable weight and stress that
account for inital/internal/final distinctions. This
second experiment shows that observing dictio-
nary stress is better early in learning, and that
modeling syllable weight merely catches up to
</bodyText>
<page confidence="0.890962">
846
</page>
<equation confidence="0.9995335">
Sentence → Collocations3+ (1)
Collocations3 → Collocations2+ (2)
Collocations2 → Collocation+ (3)
Collocation → Word+ (4)
</equation>
<figureCaption confidence="0.988507">
Figure 2: Three levels of collocation; symbols fol-
lowed by + may occur one or more times.
</figureCaption>
<bodyText confidence="0.996721416666666">
stress without surpassing it. Moreover, a com-
bined stress-and-weight model does no better than
a stress model, suggesting that the weight gram-
mar’s contribution is fully redundant, for the pur-
poses of word segmentation, with the stress obser-
vations.
Together, these experiments suggest that sylla-
ble weight eventually encodes everything about
word segmentation that dictionary stress does, and
that any advantage that syllable weight has over
observing dictionary stress is entirely redundant
with knowledge of word-final codas.
</bodyText>
<sectionHeader confidence="0.999769" genericHeader="method">
3 Experiments
</sectionHeader>
<subsectionHeader confidence="0.999952">
3.1 Adaptor Grammars
</subsectionHeader>
<bodyText confidence="0.9946495">
We follow B¨orschinger and Johnson (2014) in us-
ing a 3-level collocation Adaptor Grammar, as in-
troduced by Johnson and Goldwater (2009) and
presented in Figure 2, as the backbone for all
models, including the baseline. A 3-level collo-
cation grammar assumes that words are grouped
into collocations of words that tend to appear with
each other, and that the collocations themselves
are grouped into larger collocations, up to three
levels of collocations. This collocational struc-
ture allows the model to capture strong word-
to-word dependencies without having to group
frequently-occuring word sequences into a single,
incorrect, undersegmented ‘word’ as the unigram
model tends to do (Johnson and Goldwater, 2009)
Word rewrites in different ways in Experiment I
and Experiment II, which will be explained in the
relevant experiment section.
</bodyText>
<subsectionHeader confidence="0.988196">
3.2 Experimental Set-up
</subsectionHeader>
<bodyText confidence="0.999965511111111">
We applied the same experimental set-up used by
B¨orschinger and Johnson (2014), to their dataset,
as described below. To understand how different
modeling assumptions interact with corpus size,
we train on prefixes of each corpus with increas-
ing input size: 100, 200, 500, 1,000, 2,000, 5,000,
and 10,000 utterances. Inference closely fol-
lowed B¨orschinger and Johnson (2014) and John-
son and Goldwater (2009). We set our hyperpa-
rameters to encourage onset maximization. The
hyperparameter for syllable nodes to rewrite to
an onset followed by a rhyme was 10, and the
hyperparameter for syllable nodes to rewrite to a
rhyme only was 1. Similarly, the hyperparame-
ter for rhyme nodes to include a coda was 1, and
the hyperparameter for rhyme nodes to exclude
the coda was 10. All other hyperparameters spec-
ified vague priors. We ran eight chains of each
model for 1,000 iterations, collecting 20 samples
with a lag of 10 iterations between samples and a
burn-in of 800 iterations. We used the same batch-
initialization and table-label resampling to encour-
age the model to mix.
After gathering the samples, we used them to
perform a single minimum Bayes risk decoding
of a separate, held-out test set. This test set was
constructed by taking the last 1,000 utterances of
each corpus. We use a common test-set instead
of just evaluating on the training data to ensure
that performance figures are comparable across in-
put sizes; when we see learning curves slope up-
ward, we can be confident that the increase is due
to learning rather than easier evaluation sets.
We measured our models’ performance with the
usual token f-score metric (Brent, 1999), the har-
monic mean of how many proposed word tokens
are correct (token precision) and how many of the
actual word tokens are recovered (token recall).
For example, a model may propose “the in side”
when the true segmentation is “the inside.” This
segmentation would have a token precision of 13,
since one of three predicted words matches the
true word token (even though the other predicted
words are valid word types), and a token recall of
2, since it correctly recovered one of two words,
</bodyText>
<equation confidence="0.514233">
1
</equation>
<bodyText confidence="0.536271">
yield a token f-score of 0.4.
</bodyText>
<subsectionHeader confidence="0.971897">
3.3 Dataset
</subsectionHeader>
<bodyText confidence="0.9999215">
We evaluated on a dataset drawn from the Alex
portion of the Providence corpus (Demuth et al.,
2006). This dataset contains 17,948 utterances
with 72,859 word tokens directed to one child
from the age of 16 months to 41 months. We used
a version of this dataset that contained annota-
tions of primary stress that B¨orschinger and John-
son (2014) added to this input using an extended
</bodyText>
<page confidence="0.968311">
847
</page>
<figure confidence="0.997810212121212">
RhymeI Vowel (Coda)
Rhyme Vowel (Coda)
(a) Baseline grammar
RhymeI HeavyRhymeS
RhymeI HeavyRhymeU
RhymeI LightRhymeS
RhymeI LightRhymeU
Rhyme HeavyRhymeS
Rhyme HeavyRhymeU
Rhyme LightRhymeS
Rhyme LightRhymeU
HeavyRhymeS LongVowel Stress
HeavyRhymeS LongVowel Stress Coda
HeavyRhymeU LongVowel
HeavyRhymeU LongVowel Coda
LightRhymeS ShortVowel Stress
LightRhymeU ShortVowel
RhymeI HeavyRhyme
RhymeI LightRhyme
Rhyme HeavyRhyme
Rhyme LightRhyme
HeavyRhyme LongVowel
HeavyRhyme Vowel Coda
LightRhyme ShortVowel
(a) Weight-sensitive grammar
RhymeI RhymeS
RhymeI RhymeU
Rhyme RhymeS
Rhyme RhymeU
RhymeS Vowel Stress (Coda)
RhymeU Vowel (Coda)
(b) Stress-sensitive grammar
(d) Combined grammar
</figure>
<figureCaption confidence="0.999746">
Figure 3: Experiment I Grammars
</figureCaption>
<bodyText confidence="0.9999688">
version of CMUDict (cmu, 2008).1 The mean
number of syllables per word token was 1.2, and
only three word tokens had more than five sylla-
bles. Of the 40,323 word tokens with a stressed
syllable, 27,258 were monosyllabic. Of the
13,065 polysyllabic word tokens with a stressed
syllable, 9,931 were stress-initial. Turning to the
32, 536 word tokens with no stress (i.e., the func-
tion words), all but 23 were monosyllabic (the 23
were primarily contractions, such as “couldn’t”).
</bodyText>
<subsectionHeader confidence="0.983941">
3.4 Experiment I: Parallel Structures
</subsectionHeader>
<bodyText confidence="0.999986375">
The goal of this first experiment is to provide the
most direct comparison possible between gram-
mars that attend to stress cues and grammars that
attend to syllable weight cues. As these are both
hypothesized to be useful by way of an initial/non-
initial distinction, we defined a word to be an ini-
tial syllable SyllI followed by zero to three sylla-
bles, and syllables to consist of an optional onset
</bodyText>
<footnote confidence="0.83621325">
1This dataset and these Adap-
tor Grammar models are available at:
http://web.science.mq.edu.au/˜jpate/stress/
and a rhyme:
</footnote>
<equation confidence="0.946105333333333">
Word SyllI (Syll)10,31 (5)
SyllI (OnsetI) RhymeI (6)
Syll (Onset) Rhyme (7)
</equation>
<bodyText confidence="0.998445842105263">
In the baseline grammar, presented in Figure 3c,
rhymes rewrite to a vowel followed by an optional
consonant coda. Rhymes then rewrite to be heavy
or light in the weight grammar, as in Figure 3a, to
be stressed or unstressed in the stress grammar, as
in Figure 3b. In the combination grammar, rhymes
rewrite to be heavy or light and stressed or un-
stressed, as in Figure 3d. LongVowel and Short-
Vowel both re-write to all vowels. An additional
grammar that restricted them to rewrite to long and
short vowels, respectively, led to virtually identi-
cal performance, suggesting that vowel quantity
can be learned for the purposes of word segmenta-
tion from distributional cues. We will also present
evidence that the model did manage to learn most
of the contrast.
Figure 4 presents learning curves for the gram-
mars in this parallel structured comparison. We
see that observing stress without modeling weight
</bodyText>
<page confidence="0.976399">
848
</page>
<figure confidence="0.9991645">
Token F−Score 0.9 noweight:nostress
0.8 noweight:stress
0.7 weight:nostress
0.6 weight:stress
100 1000 10000
Number of utterances
</figure>
<figureCaption confidence="0.9908095">
Figure 4: Learning curves on the Alex corpus for Experiment I grammars with parallel distinctions
between Stressed/Unstressed and Heavy/Light syllable rhymes.
</figureCaption>
<figure confidence="0.974417333333333">
Vowel counts by quantity
1 10 100 1000 7000
LongVowel ShortVowel Vowel
</figure>
<figureCaption confidence="0.9380576">
Figure 5: Heatmap of learned vowels in the Ex-
periment I weight-only grammar. Each cell cor-
responds to the count of a particular vowel being
analyzed as one of the three vowel types. Diph-
thongs are rarely ShortVowel.
</figureCaption>
<bodyText confidence="0.996525115384615">
outperforms both the baseline and the weight-only
grammar early in learning. The weight-only gram-
mar rapidly improves in performance at larger
training data sizes, increasing its advantage over
the baseline, while the advantage of the stress-
only grammar slows and appears to disappear at
the largest training data size. At 10,000 utterances,
the improvement of the weight-only grammar over
the stress-only grammar is significant according to
an independent samples t-test (t = 7.2, p &lt; 0.001,
14 degrees of freedom). This pattern suggests that
annotated dictionary stress is easy to take advan-
tage of at low data sizes, but that, with sufficient
data, syllable weight can provide even more in-
formation about word boundaries. The best over-
all performance early in learning is obtained by
the combined grammar, suggesting that syllable
weight and dictionary stress provide information
about word segmentation that is not redundant.
An examination of the final segmentation sug-
gests that the weight grammar has learned that
initial syllables tend to be heavy. Specifically,
across eight runs, 98.1% of RhymeI symbols
rewrote to HeavyRhyme, whereas only 54.5% of
Rhyme symbols (i.e. non-initial rhymes) rewrote
to HeavyRhyme.
</bodyText>
<figure confidence="0.9973358125">
Vowel
uW
oW
aW
æ
0i
ei
ai
3`
a
U
A
ii
0
E
I
</figure>
<page confidence="0.994639">
849
</page>
<table confidence="0.9990098">
Model Mean TF Std. Dev.
noweight:nostress 0.830 0.005
noweight:stress 0.831 0.008
weight:nostress 0.861 0.008
weight:stress 0.861 0.008
</table>
<tableCaption confidence="0.8767045">
Table 1: Segmentation Token F-score for Experi-
ment I at 10,000 utterances across eight runs.
</tableCaption>
<bodyText confidence="0.999594777777778">
We also examined the final segmentation to see
well the model learned the distinction between
long vowels and short vowels. Figure 5 presents a
heatmap, with colors on a log-scale, showing how
many times each vowel label rewrote to each pos-
sible vowel in the (translated to IPA). Although the
quantity generalisations are not perfect, we do see
a general trend where ShortVowel rarely rewrites
to diphthongs.
</bodyText>
<subsectionHeader confidence="0.763015">
3.5 Experiment II: Word-final Codas
</subsectionHeader>
<bodyText confidence="0.999802212121212">
Experiment I suggested that, under a ba-
sic initial/non-initial distinction, syllable weight
eventually encodes more information about word
boundaries than does dictionary stress. This is
a surprising result, since we initially investigated
syllable weight as a noisy proxy for dictionary
stress. One possible source of the ‘extra’ advan-
tage that the syllable weight grammar exhibited
has to do with the importance of word-final codas,
which can encode word-final morphemes in En-
glish (Brent and Cartwright, 1996). Even though
the grammars did not explicitly model them, the
weight grammar could implicitly capture a bias for
or against having a coda in non-initial position,
while the stress grammar could not. This is be-
cause most word tokens are one or two syllables,
and only one of the two rhyme types of the weight
grammar included a coda. Thus, the HeavyRhyme
symbol could simultaneously capture the most im-
portant aspects of both stress and coda constraints.
To see if the extra advantage of the syllable
weight grammar can be attributed to the influence
of word-final codas, we formulated a set of gram-
mars that model word-final codas and also can
learn stress and/or syllable weight patterns. These
grammars are more similar in structure to the ones
that B¨orschinger and Johnson (2014) used. For the
baseline and weight grammar, we again defined
words to consist of up to four syllables with an ini-
tial SyllI syllable, but this time distinguished final
syllables SyllF in polysyllabic words. The non-
stress grammars use the following rules for pro-
ducing syllables:
</bodyText>
<equation confidence="0.9690485">
Word SyllIF (8)
Word SyllI (Syll){0,2} SyllF (9)
SyllIF (OnsetI) RhymeI (10)
SyllI (OnsetI) RhymeI (11)
Syll (Onset) Rhyme (12)
SyllF (Onset) RhymeF (13)
</equation>
<bodyText confidence="0.996849756097561">
For the stress grammar, we followed
B¨orschinger and Johnson (2014) in distin-
guishing stressed and unstressed syllables, rather
than simply stressed rhymes as in Experiment I,
to allow the model to learn likely stress patterns
at the word level. A word can consist of up to
four syllables, and any syllable and any number
of syllables may be stressed, as in Figure 6a.
The baseline grammar is similar to the previous
one, except it distinguishes word-final codas, as
in Figure 6b. The weight grammar, presented in
Figure 6c, rewrites rhymes to a nucleus followed
by an optional coda and distinguishes nuclei in
open syllables according to their position in the
word. The stress grammar, presented in Figure 6d,
is the all-stress-patterns model (without the unique
stress constraint) B¨orschinger and Johnson (2014).
This grammar introduces additional distinctions at
the syllable level to learn likely stress patterns,
and distinguishes final from non-final codas. The
combined model is identical to the stress model,
except Vowel non-terminals in closed and word-
internal syllables are replaced with Nucleus non-
terminals, and Vowel non-terminals in word-inital
(-final) open syllables are replaced with NucleusI
(NucleusF) non-terminals.
To summarize, the stress models distinguish
stressed and unstressed syllables in initial, final,
and internal position. The weight models distin-
guish the vowels of initial open syllables, the vow-
els of final open syllables, and other vowels, al-
lowing them to take advantage of an important cue
from syllable weight for word segmentation: if an
initial vowel is open, it should usually be long.
Figure 7 shows segmentation performance on
the Alex corpus with these more complete models.
While the performance of the weight grammars is
virtually unchanged compared to Figure 4, the two
grammars that do not model syllable weight im-
prove dramatically. This result supports our pro-
posal that much of the advantage of the weight
</bodyText>
<page confidence="0.982462">
850
</page>
<figure confidence="0.99818996">
Word → {SyllUIF|SyllSIF}
Word → {SyllUI|SyllSI} {SyllU|SyllS}{0,2} {SyllUF|SyllSF}
(a) The all-patterns stress model
SyllSIF → OnsetI RhymeSF
SyllUIF → OnsetI RhymeUF
SyllSI → Onset RhymeS
SyllUI → Onset RhymeU
SyllSF → Onset RhymeSF
SyllUF → Onset RhymeUF
RhymeSI → Vowel Stress (Coda)
RhymeUI → Vowel (Coda)
RhymeS → Vowel Stress (Coda)
RhymeU → Vowel (Coda)
RhymeSF → Vowel Stress (CodaF)
RhymeUF → Vowel (CodaF)
Rhyme → Vowel (Coda)
RhymeF → Vowel (CodaF)
(b) Baseline grammar
RhymeI → NucleusI
RhymeI → Nucleus Coda
Rhyme → Nucleus (Coda)
RhymeF → NucleusF
RhymeF → Nucleus CodaF
(c) Weight-sensitive grammar
(d) Stress-sensitive grammar
</figure>
<figureCaption confidence="0.971177">
Figure 6: Experiment II Grammars.
</figureCaption>
<figure confidence="0.998719272727273">
noweight:nostress
noweight:stress
weight:nostress
weight:stress
Token F−Score
0.9
0.8
0.7
0.6
100 1000 10000
Number of utterances
</figure>
<figureCaption confidence="0.9903385">
Figure 7: Learning curves on the Alex corpus for Experiment II grammars with word-final phonotactics
that exploit Stress and Weight.
</figureCaption>
<page confidence="0.965499">
851
</page>
<table confidence="0.997287">
Model Mean TF Std. Dev.
noweight:nostress 0.846 0.007
noweight:stress 0.880 0.005
weight:nostress 0.865 0.011
weight:stress 0.875 0.005
</table>
<tableCaption confidence="0.995836">
Table 2: Segmentation Token F-score for Experi-
</tableCaption>
<bodyText confidence="0.808004545454545">
ment II at 10,000 utterances across eight runs.
grammars over stress in Experiment I was due to
modeling of word-final coda phonotactics.
Table 2 presents token f-score at 10,000 train-
ing utterances averaged across eight runs, along
with the standard deviation in f-score. We see that
the noweight:nostress grammar is several standard
deviations than the grammars that model sylla-
ble weight and/or stress, while the syllable weight
and/or stress grammars exhibit a high degree of
overlap.
</bodyText>
<sectionHeader confidence="0.999505" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999995783783784">
We have presented computational modeling exper-
iments that suggest that syllable weight (eventu-
ally) encodes nearly everything about word seg-
mentation that dictionary stress does. Indeed,
our experiments did not find a persistent advan-
tage to observing stress over modeling syllable
weight. While it is possible that a different mod-
eling approach might find such a persistent advan-
tage, this advantage could not provide more than
13% absolute F-score. This result suggests that
children may be able to learn and exploit impor-
tant rhythm cues to word boundaries purely on
the basis of segmental input. However, this result
also suggests that annotating input with dictionary
stress has missed important aspects of the role
of stress in word segmentation. As mentioned,
Turk et al. (1995) found that infants preferred ini-
tial light syllables to be stressed. Such a prefer-
ence obviously cannot be learned by attending to
syllable weight alone, so infants who have learned
weight distinctions must also be sensitive to non-
segmental acoustic correlates to stress. There was
no long-term advantage to observing stress in ad-
dition to attending to syllable weight in our mod-
els, however, suggesting that annotated dictionary
stress does not capture the relevant non-segmental
phonetic detail. More modeling is necessary to as-
sess the non-segmental phonetic features that dis-
tinguish stressed light syllables from unstressed
light syllables.
This investigation also highlighted a weakness
of current Adaptor Grammar models: the ‘small-
est’ factors are the size of one PCFG rule. Allow-
ing further factorizations, perhaps using feature
functions of a rule’s right-hand side, would allow
models to capture finer-grained distinctions with-
out fully splitting the symbols that are involved.
</bodyText>
<sectionHeader confidence="0.99885" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999719">
Benjamin B¨orschinger and Mark Johnson. 2014. Ex-
ploring the role of stress in Bayesian word segmen-
tation using Adaptor Grammars. Transactions of the
ACL, 2:93–104.
Michael R Brent and Timothy A Cartwright. 1996.
Distributional regularity and phonotactic constraints
are useful for segmentation. Cognition, 61:93–125.
Michael Brent. 1999. An efficient, probabilistically
sound algorithm for segmentation and word discov-
ery. Machine Learning, 34:71–105.
Nick Campbell and Mary Beckman. 1997. Stress,
prominence, and spectral tilt. In Proceedings of an
ESCA workshop, pages 67–70, Athens, Greece.
Morten H. Christiansen and Suzanne L Curtin. 1999.
The power of statistical learning: No need for alge-
braic rules. In Proceedings of the 21st annual con-
ference of the Cognitive Science Society.
Morten H. Christiansen, Joseph Allen, and Mark S.
Seidenberg. 1998. Learning to segment speech
using multiple cues: A connectionist model. Lan-
guage and Cognitive Processes, 13:221—268.
2008. The CMU pronouncing dictionary.
http://www.speech.cs.cmu.edu/
cgi-bin/cmudict.
Anne Cutler and David M Carter. 1987. The predom-
inance of strong initial syllables in the English vo-
cabulary. Computer Speech &amp; Language, 2(3):133–
142.
Katherine Demuth, Jennifer Culbertson, and Jennifer
Alter. 2006. Word-minimality, epenthesis, and coda
licensing in the acquisition of English. Language
and Speech, 49:137–174.
Gabriel Doyle and Roger Levy. 2013. Combining mul-
tiple information types in Bayesian word segmenta-
tion. In Proceedings of NAACL 2013, pages 117–
126. Association for Computational Linguistics.
D B Fry. 1955. Duration and intensity as physical cor-
relates of linguistic stress. J. Acoust. Soc. of Am.,
27:765–768.
D B Fry. 1958. Experiments in the perception of stress.
Language and Speech, 1:126–152.
</reference>
<page confidence="0.985977">
852
</page>
<reference confidence="0.978800649122807">
Sharon Goldwater, Thomas L. Griffiths, and Mark
Johnson. 2009. A Bayesian framework for word
segmentation: Exploring the effects of context.
Cognition, 112(1):21–54.
Charles Yang. 2004. Universal grammar, statistics or
both? Trends in Cognitive Science, 8(10):451–456.
Mark Johnson and Sharon Goldwater. 2009. Im-
proving nonparametric Bayesian inference: exper-
iments on unsupervised word segmentation with
adaptor grammars. In Proceedings of Human Lan-
guage Technologies: The 2009 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, pages 317–325. As-
sociation for Computational Linguistics.
Mark Johnson, Thomas L Griffiths, and Sharon Gold-
water. 2007. Adaptor grammars: A framework for
specifying compositional nonparametric Bayesian
models. In B Schoelkopf, J Platt, and T Hoffmann,
editors, Advances in Neural Information Processing
Systems, volume 19. The MIT Press.
Peter W Jusczyk, Anne Cutler, and Nancy J Redanz.
1993. Infants’ preference for the predominant stress
patterns of English words. Child Development,
64(3):675–687.
Peter W Juszcyk, Derek M Houston, and Mary New-
some. 1999. The beginnings of word segmentation
in English-learning infants. Cognitive Psychology,
39(3–4):159–207.
Mark Liberman and Alan Prince. 1977. On stress and
linguistic rhythm. Linguistic Inquiry, 8(2):249–336,
Spring.
Constantine Lignos and Charles Yang. 2010. Reces-
sion segmentation: simpler online word segmenta-
tion using limited resources. In Proceedings of ACL
2010, pages 88–97. Association for Computational
Linguistics.
Constantine Lignos. 2011. Modeling infant word seg-
mentation. In Proceedings of the fifteenth confer-
ence on computational natural language learning,
pages 29–38. Association for Computational Lin-
guistics.
Constantine Lignos. 2012. Infant word segmentation:
An incremental, integrated model. In Proceedings
of the West Coast Conference on Formal Linguistics
30.
C Anton Rytting, Chris Brew, and Eric Fosler-Lussier.
2010. Segmenting words from natural speech: sub-
segmental variation in segmental cues. Journal of
Child Language, 37(3):513–543.
Erik D Thiessen and Jenny R Saffran. 2003. When
cues collide: use of stress and statistical cues to word
boundaries by 7-to-9-month-old infants. Develop-
mental Psychology, 39(4):706–716.
Alice Turk, Peter W Jusczyk, and Louann Gerken.
1995. Do English-learning infants use syllable
weight to determine stress? Language and Speech,
38(2):143–158.
</reference>
<page confidence="0.999334">
853
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.299764">
<title confidence="0.997967">Syllable weight encodes mostly the same information for English segmentation as dictionary stress</title>
<author confidence="0.999931">John K Pate Mark Johnson</author>
<affiliation confidence="0.914462">Centre for Language</affiliation>
<title confidence="0.378898">Macquarie</title>
<author confidence="0.752678">NSW Sydney</author>
<abstract confidence="0.998992952380953">Stress is a useful cue for English word segmentation. A wide range of computational models have found that stress cues enable a 2-10% improvement in segmentation accuracy, depending on the kind of model, by using input that has been annotated with stress using a pronouncing dictionary. However, stress is neither invariably produced nor unambiguously identifiable in real speech. Heavy syllables, i.e. those with long vowels or syllable codas, attract stress in English. We devise Adaptor Grammar word segmentation models that exploit either stress, or syllable weight, or both, and evaluate the utility of syllable weight as a cue to word boundaries. Our results suggest that syllable weight encodes largely the same information for word segmentation in English that annotated dictionary stress does.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Benjamin B¨orschinger</author>
<author>Mark Johnson</author>
</authors>
<title>Exploring the role of stress in Bayesian word segmentation using Adaptor Grammars.</title>
<date>2014</date>
<journal>Transactions of the ACL,</journal>
<pages>2--93</pages>
<marker>B¨orschinger, Johnson, 2014</marker>
<rawString>Benjamin B¨orschinger and Mark Johnson. 2014. Exploring the role of stress in Bayesian word segmentation using Adaptor Grammars. Transactions of the ACL, 2:93–104.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael R Brent</author>
<author>Timothy A Cartwright</author>
</authors>
<title>Distributional regularity and phonotactic constraints are useful for segmentation.</title>
<date>1996</date>
<journal>Cognition,</journal>
<pages>61--93</pages>
<contexts>
<context position="10349" citStr="Brent and Cartwright, 1996" startWordPosition="1592" endWordPosition="1595">es it difficult to integrate syllable weight and syllable stress in a linguistically natural way. A syllable is typically analyzed as having an optional onset followed by a rhyme, with the rhyme rewriting to a nucleus (the vowel) followed by an optional coda, as in Figure 1a. We expect stress and syllable weight to be useful primarily because initial syllables tend to be different from non-initial syllables. However, distinguishing final from non-final codas should be useful as well, due to the frequency of suffixes in English, and the importance of edge phenomena in phonology more generally (Brent and Cartwright, 1996). These principles come into conflict when modeling monosyllabic words. If we say that a monosyllable is an Initial and Final SyllIF, and has an initial Onset and an initial Rhyme, as in Figure 1b, then we can learn the initial/non-initial generalization about stressed or heavy rhymes at the expense of the generalization about final and non-final codas. If we say that a monosyllable is an initial onset with a final rhyme, the reverse occurs: we can learn the final/non-final coda generalization at the expense of the initial/non-initial regularities. If we split the symbols further, we’d general</context>
<context position="22853" citStr="Brent and Cartwright, 1996" startWordPosition="3570" endWordPosition="3573">ect, we do see a general trend where ShortVowel rarely rewrites to diphthongs. 3.5 Experiment II: Word-final Codas Experiment I suggested that, under a basic initial/non-initial distinction, syllable weight eventually encodes more information about word boundaries than does dictionary stress. This is a surprising result, since we initially investigated syllable weight as a noisy proxy for dictionary stress. One possible source of the ‘extra’ advantage that the syllable weight grammar exhibited has to do with the importance of word-final codas, which can encode word-final morphemes in English (Brent and Cartwright, 1996). Even though the grammars did not explicitly model them, the weight grammar could implicitly capture a bias for or against having a coda in non-initial position, while the stress grammar could not. This is because most word tokens are one or two syllables, and only one of the two rhyme types of the weight grammar included a coda. Thus, the HeavyRhyme symbol could simultaneously capture the most important aspects of both stress and coda constraints. To see if the extra advantage of the syllable weight grammar can be attributed to the influence of word-final codas, we formulated a set of gramma</context>
</contexts>
<marker>Brent, Cartwright, 1996</marker>
<rawString>Michael R Brent and Timothy A Cartwright. 1996. Distributional regularity and phonotactic constraints are useful for segmentation. Cognition, 61:93–125.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Brent</author>
</authors>
<title>An efficient, probabilistically sound algorithm for segmentation and word discovery.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<pages>34--71</pages>
<contexts>
<context position="16156" citStr="Brent, 1999" startWordPosition="2516" endWordPosition="2517"> resampling to encourage the model to mix. After gathering the samples, we used them to perform a single minimum Bayes risk decoding of a separate, held-out test set. This test set was constructed by taking the last 1,000 utterances of each corpus. We use a common test-set instead of just evaluating on the training data to ensure that performance figures are comparable across input sizes; when we see learning curves slope upward, we can be confident that the increase is due to learning rather than easier evaluation sets. We measured our models’ performance with the usual token f-score metric (Brent, 1999), the harmonic mean of how many proposed word tokens are correct (token precision) and how many of the actual word tokens are recovered (token recall). For example, a model may propose “the in side” when the true segmentation is “the inside.” This segmentation would have a token precision of 13, since one of three predicted words matches the true word token (even though the other predicted words are valid word types), and a token recall of 2, since it correctly recovered one of two words, 1 yield a token f-score of 0.4. 3.3 Dataset We evaluated on a dataset drawn from the Alex portion of the P</context>
</contexts>
<marker>Brent, 1999</marker>
<rawString>Michael Brent. 1999. An efficient, probabilistically sound algorithm for segmentation and word discovery. Machine Learning, 34:71–105.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nick Campbell</author>
<author>Mary Beckman</author>
</authors>
<title>Stress, prominence, and spectral tilt.</title>
<date>1997</date>
<booktitle>In Proceedings of an ESCA workshop,</booktitle>
<pages>67--70</pages>
<location>Athens, Greece.</location>
<contexts>
<context position="2071" citStr="Campbell and Beckman, 1997" startWordPosition="316" endWordPosition="319">ts prefer stressinitial stimuli (Jusczyk et al., 1993). A range of statistical (Doyle and Levy, 2013; Christiansen et al., 1998; B¨orschinger and Johnson, 2014) and rule-based (Yang, 2004; Lignos and Yang, 2010) models have used stress information to improve word segmentation. However, that work uses stress-marked input prepared by marking vowels that are listed as stressed in a pronouncing dictionary. This pre-processing step glosses over the fact that stress identification itself involves a nontrival learning problem, since stress has many possible phonetic reflexes and no known invariants (Campbell and Beckman, 1997; Fry, 1955; Fry, 1958). One known strong correlate of stress in English is syllable weight: heavy syllables, which end in a consonant or have a long vowel, attract stress in English. We present experiments with Bayesian Adaptor Grammars (Johnson et al., 2007) that suggest syllable weight encodes largely the same information for word segmentation that dictionary stress information does. Specifically, we modify the Adaptor Grammar word segmentation model of B¨orschinger and Johnson (2014) to compare the utility of syllable weight and stress cues for finding word boundaries, both individually an</context>
</contexts>
<marker>Campbell, Beckman, 1997</marker>
<rawString>Nick Campbell and Mary Beckman. 1997. Stress, prominence, and spectral tilt. In Proceedings of an ESCA workshop, pages 67–70, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Morten H Christiansen</author>
<author>Suzanne L Curtin</author>
</authors>
<title>The power of statistical learning: No need for algebraic rules.</title>
<date>1999</date>
<booktitle>In Proceedings of the 21st annual conference of the Cognitive Science Society.</booktitle>
<contexts>
<context position="5652" citStr="Christiansen and Curtin (1999)" startWordPosition="853" endWordPosition="856">on a type and token basis. Third, heavy syllables (those with a long vowel or a consonant coda) attract stress in English. There is experimental evidence that Englishlearning infants prefer stress-initial words from around the age of seven months (Jusczyk et al., 1993; Juszcyk et al., 1999; Jusczyk et al., 1993; Thiessen and Saffran, 2003). A variety of computational models have subsequently been developed that take stress-annotated input and use this regularity to improve segmentation accuracy. The earliest Simple Recurrent Network (SRN) modeling experiments of Christiansen et al. (1998) and Christiansen and Curtin (1999) found that stress improved word segmentation from about 39% to 43% token f-score (see Evaluation). Rytting et al. (2010) applied the SRN model to probability distributions over phones obtained from a speech recognition system, and found that the entropy of the probability distribution over phones, as a proxy to local hyperarticulation and hence a stress cue, improved token f-score from about 16% to 23%. In a deterministic approach using presyllabified input, Yang (2004), with follow-ups in Lignos and Yang (2010) and Lignos (2011; 2012), showed that a ‘Unique Stress Constraint’ (USC), or assum</context>
</contexts>
<marker>Christiansen, Curtin, 1999</marker>
<rawString>Morten H. Christiansen and Suzanne L Curtin. 1999. The power of statistical learning: No need for algebraic rules. In Proceedings of the 21st annual conference of the Cognitive Science Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Morten H Christiansen</author>
<author>Joseph Allen</author>
<author>Mark S Seidenberg</author>
</authors>
<title>Learning to segment speech using multiple cues: A connectionist model.</title>
<date>1998</date>
<booktitle>Language and Cognitive Processes,</booktitle>
<pages>13--221</pages>
<contexts>
<context position="1572" citStr="Christiansen et al., 1998" startWordPosition="242" endWordPosition="245">s largely the same information for word segmentation in English that annotated dictionary stress does. 1 Introduction One of the first skills a child must develop in the course of language acquisition is the ability to segment speech into words. Stress has long been recognized as a useful cue for English word segmentation, following the observation that words in English are predominantly stress-initial (Cutler and Carter, 1987), together with the result that 9- month-old English-learning infants prefer stressinitial stimuli (Jusczyk et al., 1993). A range of statistical (Doyle and Levy, 2013; Christiansen et al., 1998; B¨orschinger and Johnson, 2014) and rule-based (Yang, 2004; Lignos and Yang, 2010) models have used stress information to improve word segmentation. However, that work uses stress-marked input prepared by marking vowels that are listed as stressed in a pronouncing dictionary. This pre-processing step glosses over the fact that stress identification itself involves a nontrival learning problem, since stress has many possible phonetic reflexes and no known invariants (Campbell and Beckman, 1997; Fry, 1955; Fry, 1958). One known strong correlate of stress in English is syllable weight: heavy sy</context>
<context position="5617" citStr="Christiansen et al. (1998)" startWordPosition="848" endWordPosition="851">sh end up being stress-initial on a type and token basis. Third, heavy syllables (those with a long vowel or a consonant coda) attract stress in English. There is experimental evidence that Englishlearning infants prefer stress-initial words from around the age of seven months (Jusczyk et al., 1993; Juszcyk et al., 1999; Jusczyk et al., 1993; Thiessen and Saffran, 2003). A variety of computational models have subsequently been developed that take stress-annotated input and use this regularity to improve segmentation accuracy. The earliest Simple Recurrent Network (SRN) modeling experiments of Christiansen et al. (1998) and Christiansen and Curtin (1999) found that stress improved word segmentation from about 39% to 43% token f-score (see Evaluation). Rytting et al. (2010) applied the SRN model to probability distributions over phones obtained from a speech recognition system, and found that the entropy of the probability distribution over phones, as a proxy to local hyperarticulation and hence a stress cue, improved token f-score from about 16% to 23%. In a deterministic approach using presyllabified input, Yang (2004), with follow-ups in Lignos and Yang (2010) and Lignos (2011; 2012), showed that a ‘Unique</context>
</contexts>
<marker>Christiansen, Allen, Seidenberg, 1998</marker>
<rawString>Morten H. Christiansen, Joseph Allen, and Mark S. Seidenberg. 1998. Learning to segment speech using multiple cues: A connectionist model. Language and Cognitive Processes, 13:221—268.</rawString>
</citation>
<citation valid="true">
<title>The CMU pronouncing dictionary.</title>
<date>2008</date>
<note>http://www.speech.cs.cmu.edu/ cgi-bin/cmudict.</note>
<marker>2008</marker>
<rawString>2008. The CMU pronouncing dictionary. http://www.speech.cs.cmu.edu/ cgi-bin/cmudict.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anne Cutler</author>
<author>David M Carter</author>
</authors>
<title>The predominance of strong initial syllables in the English vocabulary.</title>
<date>1987</date>
<journal>Computer Speech &amp; Language,</journal>
<volume>2</volume>
<issue>3</issue>
<pages>142</pages>
<contexts>
<context position="1378" citStr="Cutler and Carter, 1987" startWordPosition="213" endWordPosition="216">entation models that exploit either stress, or syllable weight, or both, and evaluate the utility of syllable weight as a cue to word boundaries. Our results suggest that syllable weight encodes largely the same information for word segmentation in English that annotated dictionary stress does. 1 Introduction One of the first skills a child must develop in the course of language acquisition is the ability to segment speech into words. Stress has long been recognized as a useful cue for English word segmentation, following the observation that words in English are predominantly stress-initial (Cutler and Carter, 1987), together with the result that 9- month-old English-learning infants prefer stressinitial stimuli (Jusczyk et al., 1993). A range of statistical (Doyle and Levy, 2013; Christiansen et al., 1998; B¨orschinger and Johnson, 2014) and rule-based (Yang, 2004; Lignos and Yang, 2010) models have used stress information to improve word segmentation. However, that work uses stress-marked input prepared by marking vowels that are listed as stressed in a pronouncing dictionary. This pre-processing step glosses over the fact that stress identification itself involves a nontrival learning problem, since s</context>
</contexts>
<marker>Cutler, Carter, 1987</marker>
<rawString>Anne Cutler and David M Carter. 1987. The predominance of strong initial syllables in the English vocabulary. Computer Speech &amp; Language, 2(3):133– 142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katherine Demuth</author>
<author>Jennifer Culbertson</author>
<author>Jennifer Alter</author>
</authors>
<title>Word-minimality, epenthesis, and coda licensing in the acquisition of English. Language and Speech,</title>
<date>2006</date>
<pages>49--137</pages>
<contexts>
<context position="16794" citStr="Demuth et al., 2006" startWordPosition="2627" endWordPosition="2630">n of how many proposed word tokens are correct (token precision) and how many of the actual word tokens are recovered (token recall). For example, a model may propose “the in side” when the true segmentation is “the inside.” This segmentation would have a token precision of 13, since one of three predicted words matches the true word token (even though the other predicted words are valid word types), and a token recall of 2, since it correctly recovered one of two words, 1 yield a token f-score of 0.4. 3.3 Dataset We evaluated on a dataset drawn from the Alex portion of the Providence corpus (Demuth et al., 2006). This dataset contains 17,948 utterances with 72,859 word tokens directed to one child from the age of 16 months to 41 months. We used a version of this dataset that contained annotations of primary stress that B¨orschinger and Johnson (2014) added to this input using an extended 847 RhymeI Vowel (Coda) Rhyme Vowel (Coda) (a) Baseline grammar RhymeI HeavyRhymeS RhymeI HeavyRhymeU RhymeI LightRhymeS RhymeI LightRhymeU Rhyme HeavyRhymeS Rhyme HeavyRhymeU Rhyme LightRhymeS Rhyme LightRhymeU HeavyRhymeS LongVowel Stress HeavyRhymeS LongVowel Stress Coda HeavyRhymeU LongVowel HeavyRhymeU LongVowel</context>
</contexts>
<marker>Demuth, Culbertson, Alter, 2006</marker>
<rawString>Katherine Demuth, Jennifer Culbertson, and Jennifer Alter. 2006. Word-minimality, epenthesis, and coda licensing in the acquisition of English. Language and Speech, 49:137–174.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gabriel Doyle</author>
<author>Roger Levy</author>
</authors>
<title>Combining multiple information types in Bayesian word segmentation.</title>
<date>2013</date>
<booktitle>In Proceedings of NAACL 2013,</booktitle>
<pages>117--126</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1545" citStr="Doyle and Levy, 2013" startWordPosition="238" endWordPosition="241">syllable weight encodes largely the same information for word segmentation in English that annotated dictionary stress does. 1 Introduction One of the first skills a child must develop in the course of language acquisition is the ability to segment speech into words. Stress has long been recognized as a useful cue for English word segmentation, following the observation that words in English are predominantly stress-initial (Cutler and Carter, 1987), together with the result that 9- month-old English-learning infants prefer stressinitial stimuli (Jusczyk et al., 1993). A range of statistical (Doyle and Levy, 2013; Christiansen et al., 1998; B¨orschinger and Johnson, 2014) and rule-based (Yang, 2004; Lignos and Yang, 2010) models have used stress information to improve word segmentation. However, that work uses stress-marked input prepared by marking vowels that are listed as stressed in a pronouncing dictionary. This pre-processing step glosses over the fact that stress identification itself involves a nontrival learning problem, since stress has many possible phonetic reflexes and no known invariants (Campbell and Beckman, 1997; Fry, 1955; Fry, 1958). One known strong correlate of stress in English i</context>
<context position="6417" citStr="Doyle and Levy (2013)" startWordPosition="975" endWordPosition="978">to probability distributions over phones obtained from a speech recognition system, and found that the entropy of the probability distribution over phones, as a proxy to local hyperarticulation and hence a stress cue, improved token f-score from about 16% to 23%. In a deterministic approach using presyllabified input, Yang (2004), with follow-ups in Lignos and Yang (2010) and Lignos (2011; 2012), showed that a ‘Unique Stress Constraint’ (USC), or assuming each word has at most one stressed syllable, leads to an improvement of about 2.5% boundary f-score. Among explicitly probabilistic models, Doyle and Levy (2013) incorporated stress into Goldwater et al.’s (2009) Bigram model. They did this by modifying the base distribution over lexical forms to generate not simply phone strings but a sequence of syllables that may or may not be stressed. The resulting model can learn that some sequences of syllables (in particular, sequences that start with a stressed syllable) are more likely than others. However, observed stress improved token f-score by only 1%. B¨orschinger and Johnson (2014) used Adaptor Grammars (Johnson et al., 2007), a generalization of Goldwater et al.’s (2009) Bigram model that will be des</context>
</contexts>
<marker>Doyle, Levy, 2013</marker>
<rawString>Gabriel Doyle and Roger Levy. 2013. Combining multiple information types in Bayesian word segmentation. In Proceedings of NAACL 2013, pages 117– 126. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D B Fry</author>
</authors>
<title>Duration and intensity as physical correlates of linguistic stress.</title>
<date>1955</date>
<journal>J. Acoust. Soc. of Am.,</journal>
<pages>27--765</pages>
<contexts>
<context position="2082" citStr="Fry, 1955" startWordPosition="320" endWordPosition="321">uli (Jusczyk et al., 1993). A range of statistical (Doyle and Levy, 2013; Christiansen et al., 1998; B¨orschinger and Johnson, 2014) and rule-based (Yang, 2004; Lignos and Yang, 2010) models have used stress information to improve word segmentation. However, that work uses stress-marked input prepared by marking vowels that are listed as stressed in a pronouncing dictionary. This pre-processing step glosses over the fact that stress identification itself involves a nontrival learning problem, since stress has many possible phonetic reflexes and no known invariants (Campbell and Beckman, 1997; Fry, 1955; Fry, 1958). One known strong correlate of stress in English is syllable weight: heavy syllables, which end in a consonant or have a long vowel, attract stress in English. We present experiments with Bayesian Adaptor Grammars (Johnson et al., 2007) that suggest syllable weight encodes largely the same information for word segmentation that dictionary stress information does. Specifically, we modify the Adaptor Grammar word segmentation model of B¨orschinger and Johnson (2014) to compare the utility of syllable weight and stress cues for finding word boundaries, both individually and in combin</context>
</contexts>
<marker>Fry, 1955</marker>
<rawString>D B Fry. 1955. Duration and intensity as physical correlates of linguistic stress. J. Acoust. Soc. of Am., 27:765–768.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D B Fry</author>
</authors>
<title>Experiments in the perception of stress. Language and Speech,</title>
<date>1958</date>
<pages>1--126</pages>
<contexts>
<context position="2094" citStr="Fry, 1958" startWordPosition="322" endWordPosition="323">k et al., 1993). A range of statistical (Doyle and Levy, 2013; Christiansen et al., 1998; B¨orschinger and Johnson, 2014) and rule-based (Yang, 2004; Lignos and Yang, 2010) models have used stress information to improve word segmentation. However, that work uses stress-marked input prepared by marking vowels that are listed as stressed in a pronouncing dictionary. This pre-processing step glosses over the fact that stress identification itself involves a nontrival learning problem, since stress has many possible phonetic reflexes and no known invariants (Campbell and Beckman, 1997; Fry, 1955; Fry, 1958). One known strong correlate of stress in English is syllable weight: heavy syllables, which end in a consonant or have a long vowel, attract stress in English. We present experiments with Bayesian Adaptor Grammars (Johnson et al., 2007) that suggest syllable weight encodes largely the same information for word segmentation that dictionary stress information does. Specifically, we modify the Adaptor Grammar word segmentation model of B¨orschinger and Johnson (2014) to compare the utility of syllable weight and stress cues for finding word boundaries, both individually and in combination. We de</context>
</contexts>
<marker>Fry, 1958</marker>
<rawString>D B Fry. 1958. Experiments in the perception of stress. Language and Speech, 1:126–152.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>Thomas L Griffiths</author>
<author>Mark Johnson</author>
</authors>
<title>A Bayesian framework for word segmentation: Exploring the effects of context.</title>
<date>2009</date>
<journal>Cognition,</journal>
<volume>112</volume>
<issue>1</issue>
<marker>Goldwater, Griffiths, Johnson, 2009</marker>
<rawString>Sharon Goldwater, Thomas L. Griffiths, and Mark Johnson. 2009. A Bayesian framework for word segmentation: Exploring the effects of context. Cognition, 112(1):21–54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Yang</author>
</authors>
<title>Universal grammar, statistics or both?</title>
<date>2004</date>
<journal>Trends in Cognitive Science,</journal>
<volume>8</volume>
<issue>10</issue>
<contexts>
<context position="1632" citStr="Yang, 2004" startWordPosition="252" endWordPosition="253">ted dictionary stress does. 1 Introduction One of the first skills a child must develop in the course of language acquisition is the ability to segment speech into words. Stress has long been recognized as a useful cue for English word segmentation, following the observation that words in English are predominantly stress-initial (Cutler and Carter, 1987), together with the result that 9- month-old English-learning infants prefer stressinitial stimuli (Jusczyk et al., 1993). A range of statistical (Doyle and Levy, 2013; Christiansen et al., 1998; B¨orschinger and Johnson, 2014) and rule-based (Yang, 2004; Lignos and Yang, 2010) models have used stress information to improve word segmentation. However, that work uses stress-marked input prepared by marking vowels that are listed as stressed in a pronouncing dictionary. This pre-processing step glosses over the fact that stress identification itself involves a nontrival learning problem, since stress has many possible phonetic reflexes and no known invariants (Campbell and Beckman, 1997; Fry, 1955; Fry, 1958). One known strong correlate of stress in English is syllable weight: heavy syllables, which end in a consonant or have a long vowel, attr</context>
<context position="6127" citStr="Yang (2004)" startWordPosition="932" endWordPosition="933">uracy. The earliest Simple Recurrent Network (SRN) modeling experiments of Christiansen et al. (1998) and Christiansen and Curtin (1999) found that stress improved word segmentation from about 39% to 43% token f-score (see Evaluation). Rytting et al. (2010) applied the SRN model to probability distributions over phones obtained from a speech recognition system, and found that the entropy of the probability distribution over phones, as a proxy to local hyperarticulation and hence a stress cue, improved token f-score from about 16% to 23%. In a deterministic approach using presyllabified input, Yang (2004), with follow-ups in Lignos and Yang (2010) and Lignos (2011; 2012), showed that a ‘Unique Stress Constraint’ (USC), or assuming each word has at most one stressed syllable, leads to an improvement of about 2.5% boundary f-score. Among explicitly probabilistic models, Doyle and Levy (2013) incorporated stress into Goldwater et al.’s (2009) Bigram model. They did this by modifying the base distribution over lexical forms to generate not simply phone strings but a sequence of syllables that may or may not be stressed. The resulting model can learn that some sequences of syllables (in particular,</context>
</contexts>
<marker>Yang, 2004</marker>
<rawString>Charles Yang. 2004. Universal grammar, statistics or both? Trends in Cognitive Science, 8(10):451–456.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
<author>Sharon Goldwater</author>
</authors>
<title>Improving nonparametric Bayesian inference: experiments on unsupervised word segmentation with adaptor grammars.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>317--325</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="13783" citStr="Johnson and Goldwater (2009)" startWordPosition="2130" endWordPosition="2133">del does no better than a stress model, suggesting that the weight grammar’s contribution is fully redundant, for the purposes of word segmentation, with the stress observations. Together, these experiments suggest that syllable weight eventually encodes everything about word segmentation that dictionary stress does, and that any advantage that syllable weight has over observing dictionary stress is entirely redundant with knowledge of word-final codas. 3 Experiments 3.1 Adaptor Grammars We follow B¨orschinger and Johnson (2014) in using a 3-level collocation Adaptor Grammar, as introduced by Johnson and Goldwater (2009) and presented in Figure 2, as the backbone for all models, including the baseline. A 3-level collocation grammar assumes that words are grouped into collocations of words that tend to appear with each other, and that the collocations themselves are grouped into larger collocations, up to three levels of collocations. This collocational structure allows the model to capture strong wordto-word dependencies without having to group frequently-occuring word sequences into a single, incorrect, undersegmented ‘word’ as the unigram model tends to do (Johnson and Goldwater, 2009) Word rewrites in diff</context>
</contexts>
<marker>Johnson, Goldwater, 2009</marker>
<rawString>Mark Johnson and Sharon Goldwater. 2009. Improving nonparametric Bayesian inference: experiments on unsupervised word segmentation with adaptor grammars. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 317–325. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
<author>Thomas L Griffiths</author>
<author>Sharon Goldwater</author>
</authors>
<title>Adaptor grammars: A framework for specifying compositional nonparametric Bayesian models.</title>
<date>2007</date>
<booktitle>Advances in Neural Information Processing Systems,</booktitle>
<volume>19</volume>
<editor>In B Schoelkopf, J Platt, and T Hoffmann, editors,</editor>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="2331" citStr="Johnson et al., 2007" startWordPosition="359" endWordPosition="362">egmentation. However, that work uses stress-marked input prepared by marking vowels that are listed as stressed in a pronouncing dictionary. This pre-processing step glosses over the fact that stress identification itself involves a nontrival learning problem, since stress has many possible phonetic reflexes and no known invariants (Campbell and Beckman, 1997; Fry, 1955; Fry, 1958). One known strong correlate of stress in English is syllable weight: heavy syllables, which end in a consonant or have a long vowel, attract stress in English. We present experiments with Bayesian Adaptor Grammars (Johnson et al., 2007) that suggest syllable weight encodes largely the same information for word segmentation that dictionary stress information does. Specifically, we modify the Adaptor Grammar word segmentation model of B¨orschinger and Johnson (2014) to compare the utility of syllable weight and stress cues for finding word boundaries, both individually and in combination. We describe how a shortcoming of Adaptor Grammars prevents us from comparing stress and weight cues in combination with the full range of phonotactic cues for word segmentation, and design two experiments to work around this limitation. The f</context>
<context position="6940" citStr="Johnson et al., 2007" startWordPosition="1057" endWordPosition="1060">ement of about 2.5% boundary f-score. Among explicitly probabilistic models, Doyle and Levy (2013) incorporated stress into Goldwater et al.’s (2009) Bigram model. They did this by modifying the base distribution over lexical forms to generate not simply phone strings but a sequence of syllables that may or may not be stressed. The resulting model can learn that some sequences of syllables (in particular, sequences that start with a stressed syllable) are more likely than others. However, observed stress improved token f-score by only 1%. B¨orschinger and Johnson (2014) used Adaptor Grammars (Johnson et al., 2007), a generalization of Goldwater et al.’s (2009) Bigram model that will be described shortly, and found a clearer 4-10% advantage in token f-score, depending on the amount of training data. Together, the experimental and computational results suggest that infants in fact pay attention to stress, and that stress carries useful information for segmenting words in running speech. However, stress identification is itself a non-trivial task, as stress has many highly variable, contextsensitive, and optional phonetic reflexes. However, one strong phonological cue in English is syllable weight: heavy </context>
</contexts>
<marker>Johnson, Griffiths, Goldwater, 2007</marker>
<rawString>Mark Johnson, Thomas L Griffiths, and Sharon Goldwater. 2007. Adaptor grammars: A framework for specifying compositional nonparametric Bayesian models. In B Schoelkopf, J Platt, and T Hoffmann, editors, Advances in Neural Information Processing Systems, volume 19. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter W Jusczyk</author>
<author>Anne Cutler</author>
<author>Nancy J Redanz</author>
</authors>
<title>Infants’ preference for the predominant stress patterns of English words.</title>
<date>1993</date>
<journal>Child Development,</journal>
<volume>64</volume>
<issue>3</issue>
<contexts>
<context position="1499" citStr="Jusczyk et al., 1993" startWordPosition="230" endWordPosition="233">e to word boundaries. Our results suggest that syllable weight encodes largely the same information for word segmentation in English that annotated dictionary stress does. 1 Introduction One of the first skills a child must develop in the course of language acquisition is the ability to segment speech into words. Stress has long been recognized as a useful cue for English word segmentation, following the observation that words in English are predominantly stress-initial (Cutler and Carter, 1987), together with the result that 9- month-old English-learning infants prefer stressinitial stimuli (Jusczyk et al., 1993). A range of statistical (Doyle and Levy, 2013; Christiansen et al., 1998; B¨orschinger and Johnson, 2014) and rule-based (Yang, 2004; Lignos and Yang, 2010) models have used stress information to improve word segmentation. However, that work uses stress-marked input prepared by marking vowels that are listed as stressed in a pronouncing dictionary. This pre-processing step glosses over the fact that stress identification itself involves a nontrival learning problem, since stress has many possible phonetic reflexes and no known invariants (Campbell and Beckman, 1997; Fry, 1955; Fry, 1958). One</context>
<context position="5290" citStr="Jusczyk et al., 1993" startWordPosition="799" endWordPosition="802">ant for this paper. First, although English stress can shift in different contexts (Liberman and Prince, 1977), such as from the first syllable of ‘fourteen’ in isolation to the second syllable when followed by a stressed syllable, it is largely stable across different tokens of a given word. Second, most words in English end up being stress-initial on a type and token basis. Third, heavy syllables (those with a long vowel or a consonant coda) attract stress in English. There is experimental evidence that Englishlearning infants prefer stress-initial words from around the age of seven months (Jusczyk et al., 1993; Juszcyk et al., 1999; Jusczyk et al., 1993; Thiessen and Saffran, 2003). A variety of computational models have subsequently been developed that take stress-annotated input and use this regularity to improve segmentation accuracy. The earliest Simple Recurrent Network (SRN) modeling experiments of Christiansen et al. (1998) and Christiansen and Curtin (1999) found that stress improved word segmentation from about 39% to 43% token f-score (see Evaluation). Rytting et al. (2010) applied the SRN model to probability distributions over phones obtained from a speech recognition system, and found </context>
<context position="7731" citStr="Jusczyk et al. (1993)" startWordPosition="1179" endWordPosition="1182"> of training data. Together, the experimental and computational results suggest that infants in fact pay attention to stress, and that stress carries useful information for segmenting words in running speech. However, stress identification is itself a non-trivial task, as stress has many highly variable, contextsensitive, and optional phonetic reflexes. However, one strong phonological cue in English is syllable weight: heavy syllables attract stress. Heavy syllables, in turn, are syllables with a coda and/or a long vowel, which, in English, are tense vowels. Turk et al. (1995) replicated the Jusczyk et al. (1993) finding that Englishlearning infants prefer stress-initial stimuli (using non-words), and then examined how stress interacted with syllable weight. They found that syllable weight was not a necessary condition to trigger the preference: infants preferred stress-initial stimuli even if the initial syllable was light. However, they also found that infants most strongly preferred stimuli whose first syllable was both stressed and heavy: infants preferred stress-initial and heavy-initial stimuli to stress-initial and lightinitial stimuli. This result suggests that infants are sensitive to syllabl</context>
</contexts>
<marker>Jusczyk, Cutler, Redanz, 1993</marker>
<rawString>Peter W Jusczyk, Anne Cutler, and Nancy J Redanz. 1993. Infants’ preference for the predominant stress patterns of English words. Child Development, 64(3):675–687.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter W Juszcyk</author>
<author>Derek M Houston</author>
<author>Mary Newsome</author>
</authors>
<title>The beginnings of word segmentation in English-learning infants.</title>
<date>1999</date>
<pages>39--3</pages>
<publisher>Cognitive Psychology,</publisher>
<contexts>
<context position="5312" citStr="Juszcyk et al., 1999" startWordPosition="803" endWordPosition="806">rst, although English stress can shift in different contexts (Liberman and Prince, 1977), such as from the first syllable of ‘fourteen’ in isolation to the second syllable when followed by a stressed syllable, it is largely stable across different tokens of a given word. Second, most words in English end up being stress-initial on a type and token basis. Third, heavy syllables (those with a long vowel or a consonant coda) attract stress in English. There is experimental evidence that Englishlearning infants prefer stress-initial words from around the age of seven months (Jusczyk et al., 1993; Juszcyk et al., 1999; Jusczyk et al., 1993; Thiessen and Saffran, 2003). A variety of computational models have subsequently been developed that take stress-annotated input and use this regularity to improve segmentation accuracy. The earliest Simple Recurrent Network (SRN) modeling experiments of Christiansen et al. (1998) and Christiansen and Curtin (1999) found that stress improved word segmentation from about 39% to 43% token f-score (see Evaluation). Rytting et al. (2010) applied the SRN model to probability distributions over phones obtained from a speech recognition system, and found that the entropy of th</context>
</contexts>
<marker>Juszcyk, Houston, Newsome, 1999</marker>
<rawString>Peter W Juszcyk, Derek M Houston, and Mary Newsome. 1999. The beginnings of word segmentation in English-learning infants. Cognitive Psychology, 39(3–4):159–207.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Liberman</author>
<author>Alan Prince</author>
</authors>
<title>On stress and linguistic rhythm.</title>
<date>1977</date>
<journal>Linguistic Inquiry,</journal>
<volume>8</volume>
<issue>2</issue>
<publisher>Spring.</publisher>
<contexts>
<context position="4780" citStr="Liberman and Prince, 1977" startWordPosition="714" endWordPosition="717">r experiments, there is no detectable difference between relying on syllable weight and relying on dictionary stress. 2 Background Stress is the perception that some syllables are more prominent than others, and reflects a complex, language-specific interaction between acoustic cues (such as loudness and duration), and phonological patterns (such as syllable shapes). The details on how stress is assigned, produced, and perceived vary greatly across languages. Three aspects of the English stress system are relevant for this paper. First, although English stress can shift in different contexts (Liberman and Prince, 1977), such as from the first syllable of ‘fourteen’ in isolation to the second syllable when followed by a stressed syllable, it is largely stable across different tokens of a given word. Second, most words in English end up being stress-initial on a type and token basis. Third, heavy syllables (those with a long vowel or a consonant coda) attract stress in English. There is experimental evidence that Englishlearning infants prefer stress-initial words from around the age of seven months (Jusczyk et al., 1993; Juszcyk et al., 1999; Jusczyk et al., 1993; Thiessen and Saffran, 2003). A variety of co</context>
</contexts>
<marker>Liberman, Prince, 1977</marker>
<rawString>Mark Liberman and Alan Prince. 1977. On stress and linguistic rhythm. Linguistic Inquiry, 8(2):249–336, Spring.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Constantine Lignos</author>
<author>Charles Yang</author>
</authors>
<title>Recession segmentation: simpler online word segmentation using limited resources.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL 2010,</booktitle>
<pages>88--97</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1656" citStr="Lignos and Yang, 2010" startWordPosition="254" endWordPosition="257">ry stress does. 1 Introduction One of the first skills a child must develop in the course of language acquisition is the ability to segment speech into words. Stress has long been recognized as a useful cue for English word segmentation, following the observation that words in English are predominantly stress-initial (Cutler and Carter, 1987), together with the result that 9- month-old English-learning infants prefer stressinitial stimuli (Jusczyk et al., 1993). A range of statistical (Doyle and Levy, 2013; Christiansen et al., 1998; B¨orschinger and Johnson, 2014) and rule-based (Yang, 2004; Lignos and Yang, 2010) models have used stress information to improve word segmentation. However, that work uses stress-marked input prepared by marking vowels that are listed as stressed in a pronouncing dictionary. This pre-processing step glosses over the fact that stress identification itself involves a nontrival learning problem, since stress has many possible phonetic reflexes and no known invariants (Campbell and Beckman, 1997; Fry, 1955; Fry, 1958). One known strong correlate of stress in English is syllable weight: heavy syllables, which end in a consonant or have a long vowel, attract stress in English. W</context>
<context position="6170" citStr="Lignos and Yang (2010)" startWordPosition="937" endWordPosition="940">rent Network (SRN) modeling experiments of Christiansen et al. (1998) and Christiansen and Curtin (1999) found that stress improved word segmentation from about 39% to 43% token f-score (see Evaluation). Rytting et al. (2010) applied the SRN model to probability distributions over phones obtained from a speech recognition system, and found that the entropy of the probability distribution over phones, as a proxy to local hyperarticulation and hence a stress cue, improved token f-score from about 16% to 23%. In a deterministic approach using presyllabified input, Yang (2004), with follow-ups in Lignos and Yang (2010) and Lignos (2011; 2012), showed that a ‘Unique Stress Constraint’ (USC), or assuming each word has at most one stressed syllable, leads to an improvement of about 2.5% boundary f-score. Among explicitly probabilistic models, Doyle and Levy (2013) incorporated stress into Goldwater et al.’s (2009) Bigram model. They did this by modifying the base distribution over lexical forms to generate not simply phone strings but a sequence of syllables that may or may not be stressed. The resulting model can learn that some sequences of syllables (in particular, sequences that start with a stressed sylla</context>
</contexts>
<marker>Lignos, Yang, 2010</marker>
<rawString>Constantine Lignos and Charles Yang. 2010. Recession segmentation: simpler online word segmentation using limited resources. In Proceedings of ACL 2010, pages 88–97. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Constantine Lignos</author>
</authors>
<title>Modeling infant word segmentation.</title>
<date>2011</date>
<booktitle>In Proceedings of the fifteenth conference on computational natural language learning,</booktitle>
<pages>29--38</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6187" citStr="Lignos (2011" startWordPosition="942" endWordPosition="943"> experiments of Christiansen et al. (1998) and Christiansen and Curtin (1999) found that stress improved word segmentation from about 39% to 43% token f-score (see Evaluation). Rytting et al. (2010) applied the SRN model to probability distributions over phones obtained from a speech recognition system, and found that the entropy of the probability distribution over phones, as a proxy to local hyperarticulation and hence a stress cue, improved token f-score from about 16% to 23%. In a deterministic approach using presyllabified input, Yang (2004), with follow-ups in Lignos and Yang (2010) and Lignos (2011; 2012), showed that a ‘Unique Stress Constraint’ (USC), or assuming each word has at most one stressed syllable, leads to an improvement of about 2.5% boundary f-score. Among explicitly probabilistic models, Doyle and Levy (2013) incorporated stress into Goldwater et al.’s (2009) Bigram model. They did this by modifying the base distribution over lexical forms to generate not simply phone strings but a sequence of syllables that may or may not be stressed. The resulting model can learn that some sequences of syllables (in particular, sequences that start with a stressed syllable) are more lik</context>
</contexts>
<marker>Lignos, 2011</marker>
<rawString>Constantine Lignos. 2011. Modeling infant word segmentation. In Proceedings of the fifteenth conference on computational natural language learning, pages 29–38. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Constantine Lignos</author>
</authors>
<title>Infant word segmentation: An incremental, integrated model.</title>
<date>2012</date>
<booktitle>In Proceedings of the West Coast Conference on Formal Linguistics 30.</booktitle>
<marker>Lignos, 2012</marker>
<rawString>Constantine Lignos. 2012. Infant word segmentation: An incremental, integrated model. In Proceedings of the West Coast Conference on Formal Linguistics 30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Anton Rytting</author>
<author>Chris Brew</author>
<author>Eric Fosler-Lussier</author>
</authors>
<title>Segmenting words from natural speech: subsegmental variation in segmental cues.</title>
<date>2010</date>
<journal>Journal of Child Language,</journal>
<volume>37</volume>
<issue>3</issue>
<contexts>
<context position="5773" citStr="Rytting et al. (2010)" startWordPosition="872" endWordPosition="876">s experimental evidence that Englishlearning infants prefer stress-initial words from around the age of seven months (Jusczyk et al., 1993; Juszcyk et al., 1999; Jusczyk et al., 1993; Thiessen and Saffran, 2003). A variety of computational models have subsequently been developed that take stress-annotated input and use this regularity to improve segmentation accuracy. The earliest Simple Recurrent Network (SRN) modeling experiments of Christiansen et al. (1998) and Christiansen and Curtin (1999) found that stress improved word segmentation from about 39% to 43% token f-score (see Evaluation). Rytting et al. (2010) applied the SRN model to probability distributions over phones obtained from a speech recognition system, and found that the entropy of the probability distribution over phones, as a proxy to local hyperarticulation and hence a stress cue, improved token f-score from about 16% to 23%. In a deterministic approach using presyllabified input, Yang (2004), with follow-ups in Lignos and Yang (2010) and Lignos (2011; 2012), showed that a ‘Unique Stress Constraint’ (USC), or assuming each word has at most one stressed syllable, leads to an improvement of about 2.5% boundary f-score. Among explicitly</context>
</contexts>
<marker>Rytting, Brew, Fosler-Lussier, 2010</marker>
<rawString>C Anton Rytting, Chris Brew, and Eric Fosler-Lussier. 2010. Segmenting words from natural speech: subsegmental variation in segmental cues. Journal of Child Language, 37(3):513–543.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik D Thiessen</author>
<author>Jenny R Saffran</author>
</authors>
<title>When cues collide: use of stress and statistical cues to word boundaries by 7-to-9-month-old infants.</title>
<date>2003</date>
<journal>Developmental Psychology,</journal>
<volume>39</volume>
<issue>4</issue>
<contexts>
<context position="5363" citStr="Thiessen and Saffran, 2003" startWordPosition="811" endWordPosition="814">fferent contexts (Liberman and Prince, 1977), such as from the first syllable of ‘fourteen’ in isolation to the second syllable when followed by a stressed syllable, it is largely stable across different tokens of a given word. Second, most words in English end up being stress-initial on a type and token basis. Third, heavy syllables (those with a long vowel or a consonant coda) attract stress in English. There is experimental evidence that Englishlearning infants prefer stress-initial words from around the age of seven months (Jusczyk et al., 1993; Juszcyk et al., 1999; Jusczyk et al., 1993; Thiessen and Saffran, 2003). A variety of computational models have subsequently been developed that take stress-annotated input and use this regularity to improve segmentation accuracy. The earliest Simple Recurrent Network (SRN) modeling experiments of Christiansen et al. (1998) and Christiansen and Curtin (1999) found that stress improved word segmentation from about 39% to 43% token f-score (see Evaluation). Rytting et al. (2010) applied the SRN model to probability distributions over phones obtained from a speech recognition system, and found that the entropy of the probability distribution over phones, as a proxy </context>
</contexts>
<marker>Thiessen, Saffran, 2003</marker>
<rawString>Erik D Thiessen and Jenny R Saffran. 2003. When cues collide: use of stress and statistical cues to word boundaries by 7-to-9-month-old infants. Developmental Psychology, 39(4):706–716.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alice Turk</author>
<author>Peter W Jusczyk</author>
<author>Louann Gerken</author>
</authors>
<title>Do English-learning infants use syllable weight to determine stress?</title>
<date>1995</date>
<journal>Language and Speech,</journal>
<volume>38</volume>
<issue>2</issue>
<contexts>
<context position="7694" citStr="Turk et al. (1995)" startWordPosition="1173" endWordPosition="1176">n f-score, depending on the amount of training data. Together, the experimental and computational results suggest that infants in fact pay attention to stress, and that stress carries useful information for segmenting words in running speech. However, stress identification is itself a non-trivial task, as stress has many highly variable, contextsensitive, and optional phonetic reflexes. However, one strong phonological cue in English is syllable weight: heavy syllables attract stress. Heavy syllables, in turn, are syllables with a coda and/or a long vowel, which, in English, are tense vowels. Turk et al. (1995) replicated the Jusczyk et al. (1993) finding that Englishlearning infants prefer stress-initial stimuli (using non-words), and then examined how stress interacted with syllable weight. They found that syllable weight was not a necessary condition to trigger the preference: infants preferred stress-initial stimuli even if the initial syllable was light. However, they also found that infants most strongly preferred stimuli whose first syllable was both stressed and heavy: infants preferred stress-initial and heavy-initial stimuli to stress-initial and lightinitial stimuli. This result suggests </context>
<context position="28448" citStr="Turk et al. (1995)" startWordPosition="4447" endWordPosition="4450">tress does. Indeed, our experiments did not find a persistent advantage to observing stress over modeling syllable weight. While it is possible that a different modeling approach might find such a persistent advantage, this advantage could not provide more than 13% absolute F-score. This result suggests that children may be able to learn and exploit important rhythm cues to word boundaries purely on the basis of segmental input. However, this result also suggests that annotating input with dictionary stress has missed important aspects of the role of stress in word segmentation. As mentioned, Turk et al. (1995) found that infants preferred initial light syllables to be stressed. Such a preference obviously cannot be learned by attending to syllable weight alone, so infants who have learned weight distinctions must also be sensitive to nonsegmental acoustic correlates to stress. There was no long-term advantage to observing stress in addition to attending to syllable weight in our models, however, suggesting that annotated dictionary stress does not capture the relevant non-segmental phonetic detail. More modeling is necessary to assess the non-segmental phonetic features that distinguish stressed li</context>
</contexts>
<marker>Turk, Jusczyk, Gerken, 1995</marker>
<rawString>Alice Turk, Peter W Jusczyk, and Louann Gerken. 1995. Do English-learning infants use syllable weight to determine stress? Language and Speech, 38(2):143–158.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
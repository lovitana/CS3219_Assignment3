<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001377">
<title confidence="0.99863">
Citation-Enhanced Keyphrase Extraction from Research Papers:
A Supervised Approach
</title>
<author confidence="0.99565">
Cornelia Caragea&apos;, Florin Bulgarov&apos;, Andreea Godea&apos;, Sujatha Das Gollapalli2
</author>
<affiliation confidence="0.9805365">
&apos;Computer Science and Engineering, University of North Texas, TX, USA
2Institute for Infocomm Research, A*STAR, Singapore
</affiliation>
<email confidence="0.9728505">
ccaragea@unt.edu, FlorinBulgarov@my.unt.edu,
AndreeaGodea@my.unt.edu, gsdas@cse.psu.edu
</email>
<sectionHeader confidence="0.99383" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999939666666667">
Given the large amounts of online textual
documents available these days, e.g., news
articles, weblogs, and scientific papers, ef-
fective methods for extracting keyphrases,
which provide a high-level topic descrip-
tion of a document, are greatly needed. In
this paper, we propose a supervised model
for keyphrase extraction from research pa-
pers, which are embedded in citation net-
works. To this end, we design novel fea-
tures based on citation network informa-
tion and use them in conjunction with tra-
ditional features for keyphrase extraction
to obtain remarkable improvements in per-
formance over strong baselines.
</bodyText>
<sectionHeader confidence="0.998991" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999531126984127">
Keyphrase extraction is the problem of automat-
ically extracting important phrases or concepts
(i.e., the essence) of a document. Keyphrases
provide a high-level topic description of a docu-
ment and are shown to be rich sources of informa-
tion for many applications such as document clas-
sification, clustering, recommendation, indexing,
searching, and summarization (Jones and Stave-
ley, 1999; Zha, 2002; Hammouda et al., 2005;
Pudota et al., 2010; Turney, 2003). Despite the
fact that keyphrase extraction has been widely re-
searched in the natural language processing com-
munity, its performance is still far from being sat-
isfactory (Hasan and Ng, 2014).
Many previous approaches to keyphrase extrac-
tion generally used only the textual content of
a target document to extract keyphrases (Hulth,
2003; Mihalcea and Tarau, 2004; Liu et al., 2010).
Recently, Wan and Xiao (2008) proposed a model
that incorporates a local neighborhood of a doc-
ument. However, their neighborhood is limited
to textually-similar documents, where the cosine
similarity between the tf-idf vectors of documents
is used to compute their similarity. We posit
that, in addition to a document’s textual content
and textually-similar neighbors, other informative
neighborhoods exist that have the potential to im-
prove keyphrase extraction. For example, in a
scholarly domain, research papers are not isolated.
Rather, they are highly inter-connected in giant ci-
tation networks, in which papers cite or are cited
by other papers. In a citation network, information
flows from one paper to another via the citation re-
lation (Shi et al., 2010). This information flow and
the influence of one paper on another are specifi-
cally captured by means of citation contexts, i.e.,
short text segments surrounding a citation’s men-
tion. These contexts are not arbitrary, but they
serve as brief summaries of a cited paper. Figure
1 illustrates this idea using a small citation net-
work of a paper by Rendle et al. (2010) that cites
(Zimdars et al., 2001), (Hu et al., 2008), (Pan and
Scholz, 2009) and (Shani et al., 2005) and is cited
by (Cheng et al., 2013). The citation mentions
and citation contexts are shown with a dashed line.
Note the high overlap between the words in con-
texts and those in the title and abstract (shown in
bold) and the author-annotated keywords.
One question that can be raised is the following:
Can we effectively exploit information available
in large inter-linked document networks in order
to improve the performance of keyphrase extrac-
tion? The research that we describe in this paper
addresses specifically this question using citation
networks of research papers as a case study. Ex-
tracting keyphrases that can accurately “represent”
research papers is crucial to dealing with the large
numbers of research papers published during these
“big data” times. The importance of keyphrase ex-
traction from research papers is also emphasized
by the recent SemEval 2010 Shared Task on this
topic (Kim et al., 2010; Kim et al., 2013).
Our contributions. We present a supervised
</bodyText>
<page confidence="0.931673">
1435
</page>
<note confidence="0.9908875">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1435–1446,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</note>
<figureCaption confidence="0.999875">
Figure 1: A small citation network corresponding to a paper by Rendle et al. (2010).
</figureCaption>
<bodyText confidence="0.99993925">
approach to keyphrase extraction from research
papers that, in addition to the information con-
tained in a paper itself, effectively incorporates,
in the learned models, information from the pa-
per’s local neighborhood available in citation net-
works. To this end, we design novel features for
keyphrase extraction based on citation context in-
formation and use them in conjunction with tradi-
tional features in a supervised probabilistic frame-
work. We show empirically that the proposed
models substantially outperform strong baselines
on two datasets of research papers compiled from
two machine learning conferences: the World
Wide Web and Knowledge Discovery from Data.
The rest of the paper is organized as follows:
We summarize closely related work in Section 2.
The supervised classification for keyphrase extrac-
tion is discussed in Section 3. Experiments and re-
sults are presented in Section 4, followed by con-
clusions and future directions of our work.
</bodyText>
<sectionHeader confidence="0.999833" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999938755555555">
Many approaches to keyphrase extraction have
been proposed in the literature along two lines of
research: supervised and unsupervised, using dif-
ferent types of documents including scientific ab-
stracts, newswire documents, meeting transcripts,
and webpages (Frank et al., 1999; Hulth, 2003;
Nguyen and Kan, 2007; Liu et al., 2009; Marujo
et al., 2013; Mihalcea and Tarau, 2004).
In the supervised line of research, keyphrase
extraction is formulated as a binary classification
problem, where candidate phrases are classified as
either positive (i.e., keyphrases) or negative (i.e.,
non-keyphrases) (Frank et al., 1999; Turney, 2000;
Hulth, 2003). Different feature sets and classifica-
tion algorithms gave rise to different models. For
example, Hulth (2003) used four different features
in conjunction with a bagging technique. These
features are: term frequency, collection frequency,
the relative position of the first occurrence and the
part-of-speech tag of a term. Frank et al. (1999)
developed a system called KEA that used only
two features: tf-idf (term frequency-inverse doc-
ument frequency) of a phrase and the distance of
a phrase from the beginning of a document (i.e.,
its relative position) and used them as input to
Naive Bayes. Nguyen and Kan (2007) extended
KEA to include features such as the distribution
of keyphrases among different sections of a re-
search paper, and the acronym status of a term. In
contrast to these works, we propose novel features
extracted from the local neighborhoods of docu-
ments available in interlinked document networks.
Medelyan et al. (2009) extended KEA as well to
integrate information from Wikipedia. In contrast,
we used only information intrinsic to our data. En-
hancing our models with Wikipedia information
would be an interesting future direction to pursue.
In the unsupervised line of research, keyphrase
extraction is formulated as a ranking problem,
where keyphrases are ranked using their tf (Barker
and Cornacchia, 2000), tf-idf (Zhang et al., 2007;
Lee and Kim, 2008; Liu et al., 2009; Tonella et al.,
2003), and term informativeness (Wu and Giles,
2013; Rennie and Jaakkola, 2005; Kireyev, 2009)
(among others). The ranking based on tf-idf has
</bodyText>
<page confidence="0.986829">
1436
</page>
<bodyText confidence="0.999924976470588">
been shown to work well in practice (Liu et al.,
2009; Hasan and Ng, 2010) despite its simplicity.
Frantzi et al. (1998) combined linguistics and sta-
tistical information to extract technical terms from
documents in digital libraries. Graph-based al-
gorithms and centrality measures are also widely
used in unsupervised models. A word graph is
built for each document such that nodes corre-
spond to words and edges correspond to word as-
sociation patterns. Nodes are then ranked using
graph centrality measures such as PageRank and
its variants (Mihalcea and Tarau, 2004; Wan and
Xiao, 2008; Liu et al., 2010; Zhao et al., 2011),
HITS scores (Litvak and Last, 2008), as well as
node degree and betweenness (Boudin, 2013; Xie,
2005). Wan and Xiao (2008) were the first to
consider modeling a local neighborhood of a tar-
get document in addition to the document itself,
and applied this approach to news articles on the
Web. Their local neighborhood consists of textu-
ally similar documents, and did not capture infor-
mation contained in document networks.
Using terms from citation contexts of scientific
papers is not a new idea. It was used before in
various applications. For example, Ritchie et al.
(2006) used a combination of terms from citation
contexts and existing index terms of a paper to
improve indexing of cited papers. Citation con-
texts were also used to improve the performance of
citation recommendation systems (Kataria et al.,
2010; He et al., 2010) and to study author influ-
ence (Kataria et al., 2011). This idea of using
terms from citation contexts resembles the anal-
ysis of hyperlinks and the graph structure of the
Web, which are instrumental in Web search (Man-
ning et al., 2008). Many current Web search en-
gines build on the intuition that the anchor text
pointing to a page is a good descriptor of its con-
tent, and thus use anchor text terms as additional
index terms for a target webpage. The use of links
and anchor text was thoroughly researched for IR
tasks (Koolen and Kamps, 2010), broadening a
user’s search (Chakrabarti et al., 1998), query re-
finement (Kraft and Zien, 2004), and enriching
document representations (Metzler et al., 2009).
Moreover, citation contexts were used for scien-
tific paper summarization (Abu-Jbara and Radev,
2011; Qazvinian et al., 2010; Qazvinian and
Radev, 2008; Mei and Zhai, 2008; Lehnert et al.,
1990; Nakov et al., 2004). Among these, proba-
bly the most similar to our work is the work by
Qazvinian et al. (2010), where a set of important
keyphrases is extracted first from the citation con-
texts in which the paper to be summarized is cited
by other papers and then the “best” subset of sen-
tences that contain such keyphrases is returned as
the summary. However, keyphrases in (Qazvinian
et al., 2010) are extracted using frequent n-grams
in a language model framework, whereas in our
work, we propose a supervised approach to a dif-
ferent task: keyphrase extraction. Mei and Zhai
(2008) used information from citation contexts to
determine what sentences of a paper are of high
impact (as measured by the influence of a target
paper on further studies of similar or related top-
ics). These sentences constitute the impact-based
summary of the paper.
Despite the use of citation contexts and anchor
text in many IR and NLP tasks, to our knowl-
edge, we are the first to propose the incorporation
of information available in citation networks for
keyphrase extraction. In our recent work (Gol-
lapalli and Caragea, 2014), we designed a fully
unsupervised graph-based algorithm that incorpo-
rates evidence from multiple sources (citation con-
texts as well as document content) in a flexible
manner to score keywords. In the current work,
we present a supervised approach to keyphrase ex-
traction from research papers that are embedded in
large citation networks, and propose novel features
that show improvement over strong supervised and
unsupervised baselines. To our knowledge, fea-
tures extracted from citation contexts have not
been used before for keyphrase extraction in a su-
pervised learning framework.
</bodyText>
<sectionHeader confidence="0.988814" genericHeader="method">
3 Problem Characterization
</sectionHeader>
<bodyText confidence="0.998459692307692">
In citation networks, in addition to the informa-
tion contained in a paper itself, citing and cited
papers capture different aspects (e.g., topicality,
domain of study, algorithms used) about the tar-
get paper (Teufel et al., 2006), with citation con-
texts playing an instrumental role. A citation con-
text is defined as a window of n words surround-
ing a citation mention. We conjecture that cita-
tion contexts, which act as brief summaries about a
cited paper, provide additional clues in extracting
keyphrases for a target paper. These clues give rise
to the unique design of our model, called citation-
enhanced keyphrase extraction (CeKE).
</bodyText>
<subsectionHeader confidence="0.998802">
3.1 Citation-enhanced Keyphrase Extraction
</subsectionHeader>
<bodyText confidence="0.9965095">
Our proposed citation-enhanced keyphrase extrac-
tion (CeKE) model is a supervised binary classifi-
</bodyText>
<page confidence="0.998943">
1437
</page>
<tableCaption confidence="0.999549">
Table 1: The list of features used in our model.
</tableCaption>
<bodyText confidence="0.999925166666667">
cation model, built on a combination of novel fea-
tures that capture information from citation con-
texts and existing features from previous works.
The features are described in §3.1.1. CeKE classi-
fies candidate phrases as keyphrases (i.e., positive)
or non-keyphrases (i.e., negative) using Naive
Bayes classifiers. Positive examples for train-
ing correspond to manually annotated keyphrases
from the training research papers, whereas nega-
tive examples correspond to the remaining candi-
date phrases from these papers. The generation of
candidate phrases is explained in §3.2.
Note that Naive Bayes classifies a phrase as a
keyphrase if the probability of the phrase belong-
ing to the positive class is greater than 0.5. How-
ever, the default threshold of 0.5 can be varied to
allow only high-confidence (e.g., 0.9 confidence)
phrases to be classified as keyphrases.
</bodyText>
<sectionHeader confidence="0.572439" genericHeader="method">
3.1.1 Features
</sectionHeader>
<bodyText confidence="0.999954907407408">
We consider the following features in our model,
which are shown in Table 1. They are divided
into three categories: (1) Existing features for
keyphrase extraction include: tf-idf, i.e., the term
frequency - inverse document frequency of a can-
didate phrase, computed for each target paper;
This feature was used in KEA (Frank et al., 1999);
relative position, i.e., the position of the first oc-
currence of a phrase normalized by the length (in
the number of tokens) of the target paper; POS,
i.e., a phrase’s part-of-speech tag. If a phrase is
composed by more than one term, then the POS
will contain the tags of all terms. The relative posi-
tion was used in both KEA and Hulth (2003), and
POS was used in Hulth; (2) Novel features - Cita-
tion Network Based include: inCited and inCiting,
i.e., boolean features that are true if the candidate
phrase occurs in cited and citing contexts, respec-
tively. We differentiate between cited and citing
contexts for a paper: let d be a target paper and C
be a citation network such that d E C. A cited con-
text for d is a context in which d is cited by some
paper di in C. A citing context for d is a context
in which d is citing some paper dj in C. If a paper
is cited in multiple contexts by another paper, the
contexts are aggregated into a single one; citation
tf-idf, i.e., the tf-idf score of each phrase computed
from the citation contexts; (3) Novel features - Ex-
tend Other Existing Features include: first position
of a candidate phrase, i.e., the distance of the first
occurrence of a phrase from the beginning of a pa-
per; this is similar to relative position except that
it does not consider the length of a paper; tf-idf-
Over, i.e., a boolean feature, which is true if the
tf-idf of a candidate phrase is greater than a thresh-
old θ, and firstPosUnder, also a boolean feature,
which is true if the distance of the first occurrence
of a phrase from the beginning of a target paper is
below some value Q. This feature is similar to the
feature is-in-title, used previously in the literature
(Litvak and Last, 2008; Jiang et al., 2009). Both
tf-idf and citation tf-idf features showed better re-
sults when each tf was divided by the maximum tf
values from the target paper or citation contexts.
The tf-idf features have high values for phrases
that are frequent in a paper or citation contexts,
but are less frequent in collection and have low
values for phrases with high collection frequency.
We computed the idf component from each col-
lection used in experiments. Phrases that occur in
cited and citing contexts as well as early in a paper
are likely to be keyphrases since: (1) they capture
some aspect about the target paper and (2) authors
start to describe their problem upfront.
</bodyText>
<subsectionHeader confidence="0.999774">
3.2 Generating Candidate Phrases
</subsectionHeader>
<bodyText confidence="0.99418024">
We generate candidate phrases from the textual
content of a target paper by applying parts-of-
Feature Name Description
Novel features - Extensions of Existing Features
first position the distance of the first occurrence of
a phrase from the beginning of a paper
tf-idf-Over tf-idf larger than a threshold θ
firstPosUnder the distance of the first occurrence of a
phrase from the beginning of a paper is
below some value β
Existing features for keyphrase extraction
term frequency * inverse document
frequency, computed from a target
paper; used in KEA
relativePos the position of the first occurrence of a
phrase divided by the total number of
tokens; used in KEA and Hulth’s methods
POS the part-of-speech tag of the phrase;
used in Hulth’s methods
Novel features - Citation Network Based
tf-idf
inCited if the phrase occurs in cited contexts
inCiting if the phrase occurs in citing contexts
citation tf-idf the tf-idf value of the phrase, computed
from the aggregated citation contexts
</bodyText>
<page confidence="0.958424">
1438
</page>
<table confidence="0.998924">
Dataset Num. (#) Average Average Average #uni- #bi- #tri-
Papers Cited Ctx. Citing Ctx. Keyphrases grams grams grams
WWW 425 15.45 18.78 4.87 680 1036 247
KDD 365 12.69 19.74 4.03 363 853 189
</table>
<tableCaption confidence="0.998862">
Table 2: A summary of our datasets.
</tableCaption>
<bodyText confidence="0.999892157894737">
speech filters. Consistent with previous works
(Hulth, 2003; Mihalcea and Tarau, 2004; Liu
et al., 2010; Wan and Xiao, 2008), only nouns
and adjectives are retained to form candidate
phrases. The generation process consists of two
steps. First, using the NLP Stanford part of speech
tagger, we preprocess each document and keep
only the nouns and adjectives corresponding to
{NN, NNS, NNP, NNPS, JJ}. We apply the
Porter stemmer on every word. The position of
each word is kept consistent with the initial state
of the document before any word removal is made.
Second, words extracted in the first step that
have contiguous positions in a document are con-
catenated into n-grams. We used unigrams, bi-
grams, and trigrams (n = 1, 2, 3) as candidate
phrases for classification. Similar to Wan and Xiao
(2008), we eliminated phrases that end with an ad-
jective and the unigrams that are adjectives.
</bodyText>
<sectionHeader confidence="0.995674" genericHeader="evaluation">
4 Experiments and Results
</sectionHeader>
<bodyText confidence="0.999829">
In this section, we first describe our datasets and
then present experimental design and results.
</bodyText>
<subsectionHeader confidence="0.974066">
4.1 Datasets
</subsectionHeader>
<bodyText confidence="0.999959111111111">
In order to test the performance of our proposed
approach, we built our own datasets since citation-
enhanced evaluation benchmarks are not available
for keyphrase extraction tasks. In particular, we
compiled two datasets consisting of research pa-
pers from two top-tier machine learning confer-
ences: World Wide Web (WWW) and Knowledge
Discovery and Data Mining (KDD). Our choice
for WWW and KDD was motivated by the avail-
ability of author-input keywords for each paper,
which we used as gold-standard for evaluation.
Using the CiteSeer&amp;quot; digital library1, we re-
trieved the papers published in WWW and KDD
(available in CiteSeer&amp;quot;), and their citation network
information, i.e., their cited and citing contexts.
Since our goal is to study the impact of citation
network information on extracting keyphrases, a
paper was considered for analysis if it had at least
</bodyText>
<footnote confidence="0.906078">
1http://citeseerx.ist.psu.edu/
</footnote>
<bodyText confidence="0.9998625">
one cited and one citing context. For each paper,
we used: the title and abstract (referred to as the
target paper) and its citation contexts. The rea-
son for not considering the entire text of a paper
is that scientific papers contain details, e.g., dis-
cussion of results, experimental design, notation,
that do not provide additional benefits for extract-
ing keyphrases. Hence, similar to (Hulth, 2003;
Mihalcea and Tarau, 2004; Liu et al., 2009), we
did not use the entire text of a paper. However, ex-
tracting keyphrases from sections such as “intro-
duction” or “conclusion” needs further attention.
From the pdf of each paper, we extracted the
author-input keyphrases. An analysis of these
keyphrases revealed that generally authors de-
scribe their work using, almost half of the time,
bigrams, followed by unigrams and only rarely us-
ing trigrams (or higher n-grams). A summary of
our datasets that contains the number of papers,
the average number of cited and citing contexts
per paper, the average number of keyphrases per
paper, and the number of unigrams, bigrams and
trigrams, in each collection, is shown in Table 2.
Consistent with previous works (Frank et al.,
1999; Hulth, 2003), the positive and negative ex-
amples in our datasets correspond to candidate
phrases that consist of up to three tokens. The
positive examples are candidate phrases that have
a match in the author-input keyphrases, whereas
negative examples correspond to the remaining
candidate phrases.
Context lengths. In CiteSeer&amp;quot;, citation con-
texts have about 50 words on each side of a citation
mention. A previous study by Ritchie et al. (2008)
shows that a fixed window length of about 100
words around a citation mention is generally effec-
tive for information retrieval tasks. For this reason,
we used the contexts provided by CiteSeer&amp;quot; di-
rectly. However, in future, it would be interesting
to incorporate in our models more sophisticated
approaches to identifying the text that is relevant
to a target citation (Abu-Jbara and Radev, 2012;
Teufel, 1999) and study the influence of context
lengths on the quality of extracted keyphrase.
</bodyText>
<page confidence="0.945367">
1439
</page>
<table confidence="0.9996018">
WWW KDD
Method Precision Recall F1-score Precision Recall F1-score
Citation - Enhanced (CeKE) 0.227 0.386 0.284 0.213 0.413 0.280
Hulth - n-gram with tags 0.165 0.107 0.129 0.206 0.151 0.172
KEA 0.210 0.146 0.168 0.178 0.124 0.145
</table>
<tableCaption confidence="0.999857">
Table 3: The comparison of CeKE with supervised approaches on WWW and KDD collections.
</tableCaption>
<subsectionHeader confidence="0.940838">
4.2 Experimental Design
</subsectionHeader>
<bodyText confidence="0.9985055">
Our experiments are designed around the follow-
ing research questions:
</bodyText>
<listItem confidence="0.691015">
1. How does the performance of citation-
enhanced keyphrase extraction (CeKE) com-
</listItem>
<bodyText confidence="0.96733175">
pare with the performance of existing super-
vised models that use only information intrin-
sic to the data and what are the most informa-
tive features for classification? We compared
CeKE’s performance with that of classifiers
trained on KEA features only and Hulth’s
features only and present a ranking of fea-
tures based on information gain.
</bodyText>
<listItem confidence="0.973465285714286">
2. How do supervised models that integrate ci-
tation network information compare with re-
cent unsupervised models? Since recent un-
supervised approaches are becoming compet-
itive with supervised approaches (Hasan and
Ng, 2014), we also compared CeKE with
unsupervised ranking of candidate phrases
by TF-IDF, TextRank (Mihalcea and Ta-
rau, 2004) and ExpandRank (Wan and Xiao,
2008). For unsupervised, we considered top
5 and top 10 ranked phrases when computing
“@5” and “@10” measures.
3. How well does our proposed model perform
in the absence of either cited or citing con-
</listItem>
<bodyText confidence="0.990374">
texts? Since newly published scientific pa-
pers are not cited by many other papers, e.g.,
due to their recency, no cited contexts are
available. We studied the quality of predicted
keyphrases when either cited or citing con-
texts are missing. For this, we compared
the performance of models trained using both
cited and citing contexts with that of models
that use either cited or citing contexts.
Evaluation metrics. To evaluate the perfor-
mance of CeKE, we used the following metrics:
precision, recall and F1-score for the positive class
since correct identification of keyphrases is of
most interest. These metrics were widely used in
previous works (Hulth, 2003; Mihalcea and Tarau,
2004; Wan and Xiao, 2008; Hasan and Ng, 2010).
The reported values are averaged in 10-fold cross-
validation experiments, where folds were created
at document level and candidate phrases were ex-
tracted from the documents in each fold to form
the training and test sets. In all experiments, we
used Naive Bayes and their Weka implementa-
tion2. However, any probabilistic classifier that re-
turns a posterior probability of the class given an
example, can be used with our features.
The θ parameter was set to the (title and ab-
stract) tf-idf averaged over the entire collection,
while Q was set to 20. These values were esti-
mated on a validation set sampled from training.
</bodyText>
<subsectionHeader confidence="0.946885">
4.3 Results and Discussion
</subsectionHeader>
<bodyText confidence="0.999913291666667">
The impact of citation network information on the
keyphrase extraction task. Table 3 shows the re-
sults of the comparison of CeKE with two su-
pervised approaches, KEA and Hulth’s approach.
The features used in KEA are the tf-idf and the
relative position of a candidate phrase, whereas
those used in Hulth’s approach are tf, cf (i.e., col-
lection frequency), relative position and POS tags.
CeKE is trained using all features from Table 1.
Among the three methods for candidate phrase
formation proposed in Hulth (2003), i.e., n-grams,
NP-chunks, and POS Tag Patterns, our Hulth’s im-
plementation is based on n-grams since this gives
the best results among all methods (see (Hulth,
2003) for more details). In addition, the n-grams
method is the most similar to our candidate phrase
generation and that used in Frank et al. (1999).
As can be seen from Table 3, CeKE outperforms
KEA and Hulth’s approach in terms of all perfor-
mance measures on both WWW and KDD, with
a substantial improvement in recall over both ap-
proaches. For example, on WWW, CeKE achieves
a recall of 0.386 compared to 0.146 and 0.107 re-
call achieved by KEA and Hulth’s, respectively.
</bodyText>
<footnote confidence="0.973873">
2http://www.cs.waikato.ac.nz/ml/weka/
</footnote>
<page confidence="0.810416">
1440
</page>
<table confidence="0.999898846153846">
WWW KDD
Method Precision Recall F1-score Precision Recall F1-score
Citation - Enhanced (CeKE) 0.227 0.386 0.284 0.213 0.413 0.280
TF-IDF - Top 5 0.089 0.100 0.094 0.083 0.102 0.092
TF-IDF - Top 10 0.075 0.169 0.104 0.080 0.203 0.115
TextRank - Top 5 0.058 0.071 0.062 0.051 0.065 0.056
TextRank - Top 10 0.062 0.133 0.081 0.053 0.127 0.072
ExpandRank - 1 neigh. - Top 5 0.088 0.109 0.095 0.077 0.103 0.086
ExpandRank - 1 neigh. - Top 10 0.078 0.165 0.101 0.071 0.177 0.098
ExpandRank - 5 neigh. - Top 5 0.093 0.113 0.100 0.080 0.108 0.090
ExpandRank - 5 neigh. - Top 10 0.080 0.172 0.104 0.068 0.172 0.095
ExpandRank - 10 neigh. -Top 5 0.094 0.113 0.100 0.077 0.103 0.086
ExpandRank - 10 neigh. - Top 10 0.076 0.162 0.099 0.065 0.164 0.091
</table>
<tableCaption confidence="0.996337">
Table 5: The comparison of CeKE with unsupervised approaches on WWW and KDD collections.
</tableCaption>
<table confidence="0.883822">
Rank Feature IG Score
1 abstract tf-idf 0.0234
2 first position 0.0188
</table>
<figure confidence="0.711010571428571">
3 citation tf-idf 0.0177
4 relativePos 0.0154
5 firstPosUnder 0.0148
6 inCiting 0.0129
7 inCited 0.0098
8 POS 0.0085
9 tf-idf-Over 0.0078
</figure>
<tableCaption confidence="0.945399">
Table 4: Feature ranking by Info Gain on WWW.
</tableCaption>
<bodyText confidence="0.999759470588235">
Although there are only small variations from
KEA to Hulth’s approach, KEA performs better
on WWW, but worse on KDD compared with
Hulth’s approach. In contrast, CeKE shows con-
sistent improvement over the two approaches on
both datasets, hence, effectively making use of the
information available in the citation network.
In order to understand the importance of our
features, we ranked them based on Information
Gain (IG), which determines how informative a
feature is with respect to the class variable. Table
4 shows the features ranked in decreasing order of
their IG scores for WWW. As can be seen from
the table, tf-idf and citation tf-idf are both highly
ranked, first and third, respectively, illustrating
that they contain significant information in pre-
dicting keyphrases. The first position of a phrase
is also of great impact. This is consistent with the
fact that almost half of the identified keywords and
about 20% of the annotated keyphrases appear in
title. Similar ranking is obtained on KDD.
The comparison of CeKE with unsupervised
state-of-the-art models. Table 5 shows the re-
sults of the comparison of CeKE with three unsu-
pervised ranking approaches: TF-IDF (Tonella et
al., 2003), TextRank (Mihalcea and Tarau, 2004),
and ExpandRank (Wan and Xiao, 2008). TF-IDF
and TextRank use information only from the target
paper, whereas ExpandRank uses a small textual
neighborhood in addition to the target paper. Note
that, for all unsupervised methods, we used Porter
stemmer and the same candidate phrase generation
as in CeKE, as explained in §3.2.
For TF-IDF, we first tokenized the target paper
and computed the score for each word, and then
formed phrases and summed up the score of every
word within a phrase. For TextRank, we built an
undirected graph for each paper, where the nodes
correspond to words in the target paper and edges
are drawn between two words that occur next to
each other in the text, i.e., the window size is 2.
For ExpandRank, we built an undirected graph
for each paper and its local textual neighborhood.
Again, nodes correspond to words in the target pa-
per and its textually similar papers and edges are
drawn between two words that occur within a win-
dow of 10 words from each other in the text, i.e.,
the window size is 10. We performed experiments
with 1, 5, and 10 textually-similar neighbors. For
TextRank and ExpandRank, we summed up the
scores of words within a phrase as in TF-IDF.
</bodyText>
<page confidence="0.973887">
1441
</page>
<table confidence="0.9991576">
WWW KDD
Method Precision Recall F1-score Precision Recall F1-score
CeKE - Both contexts 0.227 0.386 0.284 0.213 0.413 0.280
CeKE - Only cited contexts 0.222 0.286 0.247 0.192 0.300 0.233
CeKE - Only citing contexts 0.203 0.342 0.253 0.195 0.351 0.250
</table>
<tableCaption confidence="0.999535">
Table 6: Results of CeKE using both contexts and using with only cited or citing contexts.
</tableCaption>
<bodyText confidence="0.999864093023256">
For each unsupervised method, we computed
results for top 5 and top 10 ranked phrases. As
can be seen from Table 5, CeKE substantially out-
performs all the other methods for our domain of
study, i.e., papers from WWW and KDD, illustrat-
ing again that the citation network of a paper con-
tains important information that can show remark-
able benefits for keyphrase extraction. Among all
unsupervised methods, ExpandRank with fewer
textual similar neighbor (one or five) performs the
best. This is generally consistent with the results
shown in (Wan and Xiao, 2008) for news articles.
The effect of cited and citing contexts informa-
tion on models’ performance. Table 6 shows the
precision, recall and F-score values for some vari-
ations of our method when: (1) all the citation con-
texts for a paper are used, (2) only cited contexts
are used, (3) only citing contexts are used. The
motivation behind this experiment was to deter-
mine how well the proposed model would perform
on newly published research papers that have not
accumulated citations yet. As shown in the table,
there is no substantial difference in terms of preci-
sion between CeKE models that use only cited or
only citing contexts, although the recall is substan-
tially higher for the case when only citing contexts
are used, for both WWW and KDD. The CeKE
that uses both citing and cited contexts achieves
a substantially higher recall and only a slightly
higher precision compared with the cases when
only one context type is available. The fact that
the citing context information provides a slight im-
provement in performance over cited contexts is
consistent with the intuition that when citing a pa-
per y, an author generally summarizes the main
ideas from y using important words from a target
paper x, making the citing contexts to have higher
overlap with words from x. In turn, a paper z that
cites x may use paraphrasing to summarize ideas
from x with words more similar to those from z.
Note that the results of all above experiments
are statistically significant at p-values ≥ 0.05, us-
ing a paired t-test on F1-scores.
</bodyText>
<subsectionHeader confidence="0.998473">
4.4 Anecdotal Evidence
</subsectionHeader>
<bodyText confidence="0.999975024390244">
In order to check the transferability of our pro-
posed approach to other research fields, e.g., nat-
ural language processing, it would be interesting
to use our trained classifiers on WWW and KDD
collections and evaluate them on new collections
such as NLP related collections. Since NLP col-
lections annotated with keyphrases are not avail-
able, we show anecdotal evidence for only one pa-
per. We selected for this task an award winning pa-
per published in the EMNLP conference. The pa-
per’s title is ”Unsupervised semantic parsing” and
has won the Best Paper Award in the year 2009
(Poon and Domingos, 2009). In order for our al-
gorithm to work, we gathered from the Web (using
Google Scholar) all the cited and citing contexts
that were available (49 cited contexts and 30 cit-
ing contexts). We manually annotated the target
paper with keyphrases. The title, abstract and all
the contexts were POS tagged using the NLP Stan-
ford tool. We then trained a classifier on the fea-
tures shown in Table 1, on both WWW and KDD
datasets combined. The trained classifier was used
to make predictions, which were compared against
the manually annotated keyphrases. The results
are shown in Figure 2, which displays the title and
abstract of the paper and the predicted keyphrases.
Candidate phrases that are predicted as keyphrases
are marked in red bold, those predicted as non-
keyphrases are shown in black, while the filtered
out words are shown in light gray.
We tuned our classifier trained on WWW and
KDD to return as keyphrases only those that had
an extremely high probability to be keyphrases.
Specifically, we used a threshold of 0.985. The
probability of each returned keyphrase (which is
above 0.985) is shown in the upper right corner
of a keyphrase. Human annotated keyphrases are
marked in italic, under the figure. There is a clear
match between the predictions and the human an-
notations. It is also possible to extract more or
less keyphrases simply by adjusting the threshold
</bodyText>
<page confidence="0.985961">
1442
</page>
<bodyText confidence="0.881110444444444">
Unsupervised Semantic Parsing0.997
We present the first unsupervised approach to the problem of learning a semantic parser1.000, using
Markov logic0.991 . Our USP system0.985 transforms dependency trees into quasi-logical forms, recur-
sively induces lambda forms from these, and clusters them to abstract away syntactic variations of the
same meaning. The MAP semantic parse1.000 of a sentence is obtained by recursively assigning its
parts to lambda-form clusters and composing them. We evaluate our approach by using it to extract a
knowledge base from biomedical abstracts and answer questions. USP1.000 substantially outperforms
TextRunner, DIRT and an informed baseline on both precision and recall on this task.
Human annotated labels: unsupervised semantic parsing, Markov logic, USP system
</bodyText>
<figureCaption confidence="0.918172">
Figure 2: The title and abstract of an EMNLP paper by Poon and Domingos (2009) and human annotated
</figureCaption>
<bodyText confidence="0.977021833333333">
keyphrases for the paper. Black words represent candidate phrases. Red bold words represent predicted
keyphrases. The numbers above predicted keyphrases are probabilities for the positive class assignment.
on the probability output by Naive Bayes. For ex-
ample, if we decrease the threshold to 0.920 the
following phrases would be added to the returned
set of keyphrases: dependency trees, quasi-logical
forms and unsupervised approach.
Another interesting aspect is the frequency of
occurrence of the predicted keyphrases in the cited
and citing contexts. Table 7 shows the term-
frequency of every predicted keyphrase within the
citation network. For example, the phrase seman-
tic parser appears in 29 cited contexts and 26 cit-
ing contexts. The reason for the higher cited con-
text frequency is not necessarily due to impor-
tance, but could be due to the larger number of
cited vs. citing contexts for this paper (49 vs. 30).
The high rate of keyphrases within the citation net-
work validates our assumption of the importance
of citation networks for keyphrase extraction.
Finally, we performed the same experiment
with Hulth’s and KEA methods. While the clas-
sifier trained on Hulth’s features did not identify
any keyphrases, KEA managed to identify several
good ones (e.g., USP, semantic parser), but left
out some important ones (e.g., Markov logic, un-
supervised). Moreover, the keyphrases predicted
by KEA have a lower confidence. For this reason,
lowering the probability threshold would result in
selecting other bad keyphrases.
</bodyText>
<subsectionHeader confidence="0.993483">
4.5 Error analysis
</subsectionHeader>
<bodyText confidence="0.999204">
We performed an error analysis and found that
candidate phrases are predicted as keyphrases
(FPs), although they do not appear in gold stan-
dard, i.e., the set of author-input keyphrases, in
cases when: 1) a more general terms is used to
describe an important concept of a document, e.g.,
</bodyText>
<table confidence="0.967102166666666">
Keyphrase #cited c. #citing c.
semantic parser 29 26
USP 31 10
Markov logic 15 10
unsupervised semantic parsing 12 1
USP system 3 2
</table>
<tableCaption confidence="0.967636">
Table 7: Frequency of the predicted keyphrases in
cited / citing contexts.
</tableCaption>
<bodyText confidence="0.99918737037037">
co-authorship prediction represented as link pre-
diction or Twitter platform represented as social
media; 2) an important concept is omitted (either
intentionally or forgetfully) from the set of author-
input keyphrases.
Hence, while we believe that authors are the
best keyphrase annotators for their own work,
there are cases when important keyphrases are
overlooked or expressed in different ways, possi-
bly due to the human subjective nature in choosing
important keyphrases that describe a document.
To this end, a limitation of our model is the use of
a single gold standard keyphrase annotation. In fu-
ture, we plan to acquire several human keyphrase
annotation sets for our datasets and test the perfor-
mance of the proposed approach on these annota-
tion sets, independently and in combination.
Keyphrases that appear in gold standard are
predicted as non-keyphrases (FNs) when: 1) a
keyphrase is infrequent in abstract; 2) its distance
from the beginning of a document is large; 3) does
not occur or occurs only rarely in a document’s
citation contexts, either citing or cited contexts.
Examples of FNs are model/algorithm/approach
names, e.g., random walks, that appear in sen-
tences such as: “In this paper, we model the prob-
lem [· · ·] by using random walks.” Although such
</bodyText>
<page confidence="0.970252">
1443
</page>
<bodyText confidence="0.9997381">
a sentence may appear further away from the be-
ginning of an abstract, it contains significant in-
formation from the point of view of keyphrase
extraction. The design of patters such as &lt;
by using $model &gt; or &lt; uses $model &gt; could
lead to improved classification performance.
Further investigation of FPs and FNs will be
considered in future work. We believe that a bet-
ter understanding of errors has the potential to ad-
vance state-of-the-art for keyphrase extraction.
</bodyText>
<sectionHeader confidence="0.970851" genericHeader="conclusions">
5 Conclusion and Future Directions
</sectionHeader>
<bodyText confidence="0.999970096774194">
In this paper, we presented a supervised classifi-
cation model for keyphrase extraction from scien-
tific research papers that are embedded in citation
networks. More precisely, we designed novel fea-
tures that take into account citation network in-
formation for building supervised models for the
classification of candidate phrases as keyphrases
or non-keyphrases. The results of our experi-
ments show that the proposed supervised model
trained on a combination of citation-based features
and existing features for keyphrase extraction per-
forms substantially better compared with state-of-
the-art supervised and unsupervised models.
Although we illustrated the benefits of leverag-
ing inter-linked document networks for keyphrase
extraction from scientific documents, the proposed
model can be extended to other types of docu-
ments such as webpages, emails, and weblogs. For
example, the anchor text on hyperlinks in weblogs
can be seen as the “citation context”.
Another aspect of future work would be the
use of external sources to better identify candi-
date phrases. For example, the use of Wikipedia
was studied before to check if the concept behind
a phrase has its own Wikipedia page (Medelyan
et al., 2009). Furthermore, since citations occur
in all sciences, extensions of the proposed model
to other domains, e.g., Biology and Chemistry,
and other applications, e.g., document summariza-
tion, similar to Mihalcea and Tarau (2004) and
Qazvinian et al. (2010), are of particular interest.
</bodyText>
<sectionHeader confidence="0.989775" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99997325">
We are grateful to Dr. C. Lee Giles for the
CiteSeerX data, which allowed the generation of
citation graphs. We also thank Kishore Nep-
palli and Juan Fern´andez-Ramizer for their help
with various dataset construction tasks. We very
much appreciate the constructive feedback from
our anonymous reviewers. This research was
supported in part by NSF awards #1353418 and
#1423337 to Cornelia Caragea. Any opinions,
findings, and conclusions expressed here are those
of the authors and do not necessarily reflect the
views of NSF.
</bodyText>
<sectionHeader confidence="0.989372" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998635319148936">
Amjad Abu-Jbara and Dragomir Radev. 2011. Co-
herent citation-based summarization of scientific pa-
pers. In Proc. of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies, HLT ’11, pages 500–509.
Amjad Abu-Jbara and Dragomir Radev. 2012. Ref-
erence scope identification in citing sentences. In
Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
NAACL HLT ’12, pages 80–90.
Ken Barker and Nadia Cornacchia. 2000. Using Noun
Phrase Heads to Extract Document Keyphrases. In
Proceedings of the 13th Biennial Conference of the
Canadian Society on Computational Studies of Intel-
ligence: Advances in Artificial Intelligence, AI ’00,
pages 40–52, London, UK, UK. Springer-Verlag.
Florian Boudin. 2013. A comparison of centrality
measures for graph-based keyphrase extraction. In
Proc. of IJCNLP, pages 834–838, Nagoya, Japan.
Soumen Chakrabarti, Byron Dom, Prabhakar Ragha-
van, Sridhar Rajagopalan, David Gibson, and Jon
Kleinberg. 1998. Automatic resource compilation
by analyzing hyperlink structure and associated text.
Comput. Netw. ISDNSyst., 30(1-7):65–74, April.
Chen Cheng, Haiqin Yang, Michael R. Lyu, and Irwin
King. 2013. Where you like to go next: Succes-
sive point-of-interest recommendation. In Proc. of
IJCAI’13, pages 2605–2611, Beijing, China.
Eibe Frank, Gordon W. Paynter, Ian H. Witten, Carl
Gutwin, and Craig G. Nevill-Manning. 1999.
Domain-specific keyphrase extraction. In Proceed-
ings of the 16th International Joint Conference on
Artificial Intelligence - Volume 2, IJCAI’99, pages
668–673, Stockholm, Sweden.
Katerina T. Frantzi, Sophia Ananiadou, and Jun-ichi
Tsujii. 1998. The c-value/nc-value method of au-
tomatic recognition for multi-word terms. In Proc.
of ECDL ’98, pages 585–604.
Sujatha Das Gollapalli and Cornelia Caragea. 2014.
Extracting keyphrases from research papers using
citation networks. In Proceedings of the 28th
AAAI Conference on Artificial Intelligence (AAAI-
14), Qu´ebec City, Qu´ebec, Canada.
Khaled M. Hammouda, Diego N. Matute, and Mo-
hamed S. Kamel. 2005. Corephrase: Keyphrase ex-
traction for document clustering. In Proc. of the 4th
</reference>
<page confidence="0.947789">
1444
</page>
<reference confidence="0.999344574074074">
International Conference on Machine Learning and
Data Mining in Pattern Recognition, MLDM’05,
pages 265–274, Leipzig, Germany.
Kazi Saidul Hasan and Vincent Ng. 2010. Conun-
drums in Unsupervised Keyphrase Extraction: Mak-
ing Sense of the State-of-the-Art. In Proceedings
of the 23rd International Conference on Computa-
tional Linguistics, pages 365–373.
Kazi Saidul Hasan and Vincent Ng. 2014. Automatic
keyphrase extraction: A survey of the state of the art.
In Proc. of the 52nd Annual Meeting of the Associa-
tion for Computational Linguistics (ACL).
Qi He, Jian Pei, Daniel Kifer, Prasenjit Mitra, and Lee
Giles. 2010. Context-aware citation recommenda-
tion. In Proc. of WWW ’10, pages 421–430, Raleigh,
North Carolina, USA.
Yifan Hu, Yehuda Koren, and Chris Volinsky.
2008. Collaborative filtering for implicit feedback
datasets. In Proc. of the 8th IEEE Intl. Conference
on Data Mining, ICDM ’08, pages 263–272.
Anette Hulth. 2003. Improved Automatic Keyword
Extraction Given More Linguistic Knowledge. In
Proceedings of the 2003 conference on Empirical
methods in natural language processing, EMNLP
’03, pages 216–223.
Xin Jiang, Yunhua Hu, and Hang Li. 2009. A Ranking
Approach to Keyphrase Extraction. In Proceedings
of the 32nd international ACM SIGIR conference on
Research and development in information retrieval,
pages 756–757. ACM.
Steve Jones and Mark S. Staveley. 1999. Phrasier:
A system for interactive document retrieval using
keyphrases. In Proceedings of SIGIR ’99, pages
160–167, Berkeley, California, USA.
Saurabh Kataria, Prasenjit Mitra, and Sumit Bhatia.
2010. Utilizing context in generative bayesian mod-
els for linked corpus. In In Proc. ofAAAI ’10, pages
1340–1345, Atlanta, Georgia, USA.
Saurabh Kataria, Prasenjit Mitra, Cornelia Caragea,
and C. Lee Giles. 2011. Context sensitive topic
models for author influence in document networks.
In Proceedings of IJCAI’11, pages 2274–2280,
Barcelona, Catalonia, Spain.
Su Nam Kim, Olena Medelyan, Min-Yen Kan, and
Timothy Baldwin. 2010. SemEval-2010 Task 5:
Automatic Keyphrase Extraction from Scientific Ar-
ticles. In Proceedings of the 5th International Work-
shop on Semantic Evaluation, SemEval ’10, pages
21–26, Los Angeles, California.
Su Nam Kim, Olena Medelyan, Min-Yen Kan, and
Timothy Baldwin. 2013. Automatic keyphrase
extraction from scientific articles. Language Re-
sources and Evaluation, Springer, 47(3):723–742.
Kirill Kireyev. 2009. Semantic-based estimation of
term informativeness. In Proc. of NAACL ’09, pages
530–538, Boulder, Colorado.
Marijn Koolen and Jaap Kamps. 2010. The impor-
tance of anchor text for ad hoc search revisited. In
Proceedings of SIGIR ’10, pages 122–129, Geneva,
Switzerland.
Reiner Kraft and Jason Zien. 2004. Mining anchor text
for query refinement. In Proceedings of the 13th In-
ternational Conference on World Wide Web, WWW
’04, pages 666–674, New York, NY, USA. ACM.
Sungjick Lee and Han-joon Kim. 2008. News Key-
word Extraction for Topic Tracking. In Proceedings
of the 2008 Fourth International Conference on Net-
worked Computing and Advanced Information Man-
agement - Volume 02, NCM ’08, pages 554–559,
Washington, DC, USA. IEEE Computer Society.
Wendy Lehnert, Claire Cardie, and Ellen Rilofl. 1990.
Analyzing research papers using citation sentences.
In Proceedings of the 12th Annual Conference of the
Cognitive Science Society, pages 511–518.
Marina Litvak and Mark Last. 2008. Graph-
Based Keyword Extraction for Single-Document
Summarization. In Proceedings of the Workshop
on Multi-source Multilingual Information Extrac-
tion and Summarization, MMIES ’08, pages 17–24,
Manchester, United Kingdom.
Feifan Liu, Deana Pennell, Fei Liu, and Yang Liu.
2009. Unsupervised Approaches for Automatic
Keyword Extraction Using Meeting Transcripts. In
Proceedings of NAACL ’09, pages 620–628, Boul-
der, Colorado.
Zhiyuan Liu, Wenyi Huang, Yabin Zheng, and
Maosong Sun. 2010. Automatic Keyphrase Ex-
traction via Topic Decomposition. In Proceedings
of EMNLP ’10, pages 366–376, Cambridge, Mas-
sachusetts.
Christopher D. Manning, Prabhakar Raghavan, and
Hinrich Sch¨utze. 2008. Introduction to Information
Retrieval. Cambridge University Press, New York,
NY, USA.
Lu´ıs Marujo, Ricardo Ribeiro, David Martins
de Matos, Jo˜ao Paulo Neto, Anatole Gershman, and
Jaime G. Carbonell. 2013. Key phrase extraction of
lightly filtered broadcast news. CoRR.
Olena Medelyan, Eibe Frank, and Ian H. Witten.
2009. Human-competitive tagging using automatic
keyphrase extraction. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing: Volume 3 - Volume 3, EMNLP
’09, pages 1318–1327, Singapore.
Qiaozhu Mei and ChengXiang Zhai. 2008. Generat-
ing impact-based summaries for scientific literature.
In Proceedings of ACL-08: HLT, pages 816–824,
Columbus, Ohio.
</reference>
<page confidence="0.825118">
1445
</page>
<reference confidence="0.999495055555555">
Donald Metzler, Jasmine Novak, Hang Cui, and Srihari
Reddy. 2009. Building enriched document repre-
sentations using aggregated anchor text. In Proc. of
SIGIR ’09, pages 219–226, Boston, MA, USA.
Rada Mihalcea and Paul Tarau. 2004. TextRank:
Bringing Order into Texts. In Proceedings of
EMNLP 2004, pages 404–411, Barcelona, Spain.
Preslav I. Nakov, Ariel S. Schwartz, and Marti A.
Hearst. 2004. Citances: Citation sentences for se-
mantic analysis of bioscience text. In SIGIR Work-
shop on Search and Discovery in Bioinformatics.
Thuy Dung Nguyen and Min-Yen Kan. 2007.
Keyphrase Extraction in Scientific Publications. In
Proc. of the Intl. Conf. on Asian digital libraries,
ICADL’07, pages 317–326, Hanoi, Vietnam.
Rong Pan and Martin Scholz. 2009. Mind the gaps:
Weighting the unknown in large-scale one-class col-
laborative filtering. In Proceedings of KDD ’09,
pages 667–676, Paris, France.
Hoifung Poon and Pedro Domingos. 2009. Unsuper-
vised semantic parsing. In Proc. of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing, EMNLP ’09, pages 1–10, Singapore.
Nirmala Pudota, Antonina Dattolo, Andrea Baruzzo,
Felice Ferrara, and Carlo Tasso. 2010. Auto-
matic keyphrase extraction and ontology mining for
content-based tag recommendation. International
Journal of Intelligent Systems.
Vahed Qazvinian and Dragomir R. Radev. 2008. Sci-
entific paper summarization using citation summary
networks. In Proc. of the 22nd Intl. Conference
on Computational Linguistics, COLING ’08, pages
689–696, Manchester, United Kingdom.
Vahed Qazvinian, Dragomir R. Radev, and Arzucan
¨Ozg¨ur. 2010. Citation summarization through
keyphrase extraction. In Proceedings of the 23rd In-
ternational Conference on Computational Linguis-
tics, COLING ’10, pages 895–903.
Steffen Rendle, Christoph Freudenthaler, and Lars
Schmidt-Thieme. 2010. Factorizing personalized
markov chains for next-basket recommendation. In
WWW ’10, pages 811–820, Raleigh, North Carolina.
Jason D. M. Rennie and Tommi Jaakkola. 2005. Using
Term Informativeness for Named Entity Detection.
In Proc. of SIGIR ’05, pages 353–360.
Anna Ritchie, Simone Teufel, and Stephen Robertson.
2006. How to find better index terms through cita-
tions. In Proc. of the Workshop on How Can Compu-
tational Linguistics Improve Information Retrieval?,
CLIIR ’06, pages 25–32, Sydney, Australia.
Anna Ritchie, Stephen Robertson, and Simone Teufel.
2008. Comparing citation contexts for information
retrieval. In Proc. of CIKM ’08, pages 213–222,
Napa Valley, California, USA.
Guy Shani, David Heckerman, and Ronen I. Brafman.
2005. An mdp-based recommender system. J.
Mach. Learn. Res., 6:1265–1295, December.
Xiaolin Shi, Jure Leskovec, and Daniel A. McFarland.
2010. Citing for high impact. In Proceedings of the
10th Annual Joint Conference on Digital Libraries,
JCDL ’10, pages 49–58, Gold Coast, Queensland,
Australia.
S. Teufel, A. Siddharthan, and D. Tidhar. 2006. Auto-
matic classification of citation function. In Proceed-
ings of EMNLP-06.
S. Teufel. 1999. Argumentative Zoning: Information
Extraction from Scientific Text. Ph.D. thesis, Uni-
versity of Edinburgh.
Paolo Tonella, Filippo Ricca, Emanuele Pianta, and
Christian Girardi. 2003. Using Keyword Extrac-
tion for Web Site Clustering. In Web Site Evolution,
2003. Theme: Architecture. Proceedings. Fifth IEEE
International Workshop on, pages 41–48.
Peter D. Turney. 2000. Learning algorithms for
keyphrase extraction. Inf. Retr., 2.
Peter D. Turney. 2003. Coherent Keyphrase Extraction
via Web Mining. In Proceedings of the 18th inter-
national joint conference on Artificial intelligence,
IJCAI’03, pages 434–439, Acapulco, Mexico.
Xiaojun Wan and Jianguo Xiao. 2008. Single Doc-
ument Keyphrase Extraction Using Neighborhood
Knowledge. In Proceedings of AAAI ’08, pages
855–860, Chicago, Illinois.
Zhaohui Wu and Lee C. Giles. 2013. Measuring
term informativeness in context. In Proceedings of
NAACL ’13, pages 259–269, Atlanta, Georgia.
Zhuli Xie. 2005. Centrality Measures in Text Mining:
Prediction of Noun Phrases that Appear in Abstracts.
In Proceedings of the ACL Student Research Work-
shop, pages 103–108, Ann Arbor, Michigan.
Hongyuan Zha. 2002. Generic summarization and
keyphrase extraction using mutual reinforcement
principle and sentence clustering. In SIGIR.
Yongzheng Zhang, Evangelos Milios, and Nur Zincir-
Heywood. 2007. A Comparative Study on Key
Phrase Extraction Methods in Automatic Web Site
Summarization. Journal of Digital Information
Management, 5(5):323.
Wayne Xin Zhao, Jing Jiang, Jing He, Yang Song,
Palakorn Achananuparp, Ee-Peng Lim, and Xiaom-
ing Li. 2011. Topical Keyphrase Extraction from
Twitter. In Proceedings of HLT ’11, pages 379–388,
Portland, Oregon.
Andrew Zimdars, David Maxwell Chickering, and
Christopher Meek. 2001. Using temporal data for
making recommendations. In Proceedings of the
17th Conference in Uncertainty in Artificial Intelli-
gence, UAI ’01, pages 580–588.
</reference>
<page confidence="0.992847">
1446
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.482200">
<title confidence="0.9973435">Citation-Enhanced Keyphrase Extraction from Research A Supervised Approach</title>
<author confidence="0.978738">Florin Andreea Sujatha Das</author>
<affiliation confidence="0.747376">Science and Engineering, University of North Texas, TX, for Infocomm Research, A*STAR,</affiliation>
<email confidence="0.974623">ccaragea@unt.edu,AndreeaGodea@my.unt.edu,gsdas@cse.psu.edu</email>
<abstract confidence="0.998835875">Given the large amounts of online textual documents available these days, e.g., news weblogs, and scientific papers, effor extracting keyphrases, which provide a high-level topic description of a document, are greatly needed. In this paper, we propose a supervised model for keyphrase extraction from research papers, which are embedded in citation networks. To this end, we design novel features based on citation network information and use them in conjunction with traditional features for keyphrase extraction to obtain remarkable improvements in performance over strong baselines.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Amjad Abu-Jbara</author>
<author>Dragomir Radev</author>
</authors>
<title>Coherent citation-based summarization of scientific papers.</title>
<date>2011</date>
<booktitle>In Proc. of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, HLT ’11,</booktitle>
<pages>500--509</pages>
<contexts>
<context position="9791" citStr="Abu-Jbara and Radev, 2011" startWordPosition="1533" endWordPosition="1536">ch are instrumental in Web search (Manning et al., 2008). Many current Web search engines build on the intuition that the anchor text pointing to a page is a good descriptor of its content, and thus use anchor text terms as additional index terms for a target webpage. The use of links and anchor text was thoroughly researched for IR tasks (Koolen and Kamps, 2010), broadening a user’s search (Chakrabarti et al., 1998), query refinement (Kraft and Zien, 2004), and enriching document representations (Metzler et al., 2009). Moreover, citation contexts were used for scientific paper summarization (Abu-Jbara and Radev, 2011; Qazvinian et al., 2010; Qazvinian and Radev, 2008; Mei and Zhai, 2008; Lehnert et al., 1990; Nakov et al., 2004). Among these, probably the most similar to our work is the work by Qazvinian et al. (2010), where a set of important keyphrases is extracted first from the citation contexts in which the paper to be summarized is cited by other papers and then the “best” subset of sentences that contain such keyphrases is returned as the summary. However, keyphrases in (Qazvinian et al., 2010) are extracted using frequent n-grams in a language model framework, whereas in our work, we propose a sup</context>
</contexts>
<marker>Abu-Jbara, Radev, 2011</marker>
<rawString>Amjad Abu-Jbara and Dragomir Radev. 2011. Coherent citation-based summarization of scientific papers. In Proc. of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, HLT ’11, pages 500–509.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amjad Abu-Jbara</author>
<author>Dragomir Radev</author>
</authors>
<title>Reference scope identification in citing sentences.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL HLT ’12,</booktitle>
<pages>80--90</pages>
<contexts>
<context position="21236" citStr="Abu-Jbara and Radev, 2012" startWordPosition="3430" endWordPosition="3433">yphrases, whereas negative examples correspond to the remaining candidate phrases. Context lengths. In CiteSeer&amp;quot;, citation contexts have about 50 words on each side of a citation mention. A previous study by Ritchie et al. (2008) shows that a fixed window length of about 100 words around a citation mention is generally effective for information retrieval tasks. For this reason, we used the contexts provided by CiteSeer&amp;quot; directly. However, in future, it would be interesting to incorporate in our models more sophisticated approaches to identifying the text that is relevant to a target citation (Abu-Jbara and Radev, 2012; Teufel, 1999) and study the influence of context lengths on the quality of extracted keyphrase. 1439 WWW KDD Method Precision Recall F1-score Precision Recall F1-score Citation - Enhanced (CeKE) 0.227 0.386 0.284 0.213 0.413 0.280 Hulth - n-gram with tags 0.165 0.107 0.129 0.206 0.151 0.172 KEA 0.210 0.146 0.168 0.178 0.124 0.145 Table 3: The comparison of CeKE with supervised approaches on WWW and KDD collections. 4.2 Experimental Design Our experiments are designed around the following research questions: 1. How does the performance of citationenhanced keyphrase extraction (CeKE) compare w</context>
</contexts>
<marker>Abu-Jbara, Radev, 2012</marker>
<rawString>Amjad Abu-Jbara and Dragomir Radev. 2012. Reference scope identification in citing sentences. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL HLT ’12, pages 80–90.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ken Barker</author>
<author>Nadia Cornacchia</author>
</authors>
<title>Using Noun Phrase Heads to Extract Document Keyphrases.</title>
<date>2000</date>
<booktitle>In Proceedings of the 13th Biennial Conference of the Canadian Society on Computational Studies of Intelligence: Advances in Artificial Intelligence, AI ’00,</booktitle>
<pages>40--52</pages>
<publisher>UK. Springer-Verlag.</publisher>
<location>London, UK,</location>
<contexts>
<context position="7296" citStr="Barker and Cornacchia, 2000" startWordPosition="1116" endWordPosition="1119"> sections of a research paper, and the acronym status of a term. In contrast to these works, we propose novel features extracted from the local neighborhoods of documents available in interlinked document networks. Medelyan et al. (2009) extended KEA as well to integrate information from Wikipedia. In contrast, we used only information intrinsic to our data. Enhancing our models with Wikipedia information would be an interesting future direction to pursue. In the unsupervised line of research, keyphrase extraction is formulated as a ranking problem, where keyphrases are ranked using their tf (Barker and Cornacchia, 2000), tf-idf (Zhang et al., 2007; Lee and Kim, 2008; Liu et al., 2009; Tonella et al., 2003), and term informativeness (Wu and Giles, 2013; Rennie and Jaakkola, 2005; Kireyev, 2009) (among others). The ranking based on tf-idf has 1436 been shown to work well in practice (Liu et al., 2009; Hasan and Ng, 2010) despite its simplicity. Frantzi et al. (1998) combined linguistics and statistical information to extract technical terms from documents in digital libraries. Graph-based algorithms and centrality measures are also widely used in unsupervised models. A word graph is built for each document suc</context>
</contexts>
<marker>Barker, Cornacchia, 2000</marker>
<rawString>Ken Barker and Nadia Cornacchia. 2000. Using Noun Phrase Heads to Extract Document Keyphrases. In Proceedings of the 13th Biennial Conference of the Canadian Society on Computational Studies of Intelligence: Advances in Artificial Intelligence, AI ’00, pages 40–52, London, UK, UK. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Florian Boudin</author>
</authors>
<title>A comparison of centrality measures for graph-based keyphrase extraction.</title>
<date>2013</date>
<booktitle>In Proc. of IJCNLP,</booktitle>
<pages>834--838</pages>
<location>Nagoya, Japan.</location>
<contexts>
<context position="8242" citStr="Boudin, 2013" startWordPosition="1275" endWordPosition="1276">tzi et al. (1998) combined linguistics and statistical information to extract technical terms from documents in digital libraries. Graph-based algorithms and centrality measures are also widely used in unsupervised models. A word graph is built for each document such that nodes correspond to words and edges correspond to word association patterns. Nodes are then ranked using graph centrality measures such as PageRank and its variants (Mihalcea and Tarau, 2004; Wan and Xiao, 2008; Liu et al., 2010; Zhao et al., 2011), HITS scores (Litvak and Last, 2008), as well as node degree and betweenness (Boudin, 2013; Xie, 2005). Wan and Xiao (2008) were the first to consider modeling a local neighborhood of a target document in addition to the document itself, and applied this approach to news articles on the Web. Their local neighborhood consists of textually similar documents, and did not capture information contained in document networks. Using terms from citation contexts of scientific papers is not a new idea. It was used before in various applications. For example, Ritchie et al. (2006) used a combination of terms from citation contexts and existing index terms of a paper to improve indexing of cit</context>
</contexts>
<marker>Boudin, 2013</marker>
<rawString>Florian Boudin. 2013. A comparison of centrality measures for graph-based keyphrase extraction. In Proc. of IJCNLP, pages 834–838, Nagoya, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Soumen Chakrabarti</author>
<author>Byron Dom</author>
<author>Prabhakar Raghavan</author>
<author>Sridhar Rajagopalan</author>
<author>David Gibson</author>
<author>Jon Kleinberg</author>
</authors>
<title>Automatic resource compilation by analyzing hyperlink structure and associated text.</title>
<date>1998</date>
<journal>Comput. Netw. ISDNSyst.,</journal>
<pages>30--1</pages>
<contexts>
<context position="9586" citStr="Chakrabarti et al., 1998" startWordPosition="1504" endWordPosition="1507">l., 2010; He et al., 2010) and to study author influence (Kataria et al., 2011). This idea of using terms from citation contexts resembles the analysis of hyperlinks and the graph structure of the Web, which are instrumental in Web search (Manning et al., 2008). Many current Web search engines build on the intuition that the anchor text pointing to a page is a good descriptor of its content, and thus use anchor text terms as additional index terms for a target webpage. The use of links and anchor text was thoroughly researched for IR tasks (Koolen and Kamps, 2010), broadening a user’s search (Chakrabarti et al., 1998), query refinement (Kraft and Zien, 2004), and enriching document representations (Metzler et al., 2009). Moreover, citation contexts were used for scientific paper summarization (Abu-Jbara and Radev, 2011; Qazvinian et al., 2010; Qazvinian and Radev, 2008; Mei and Zhai, 2008; Lehnert et al., 1990; Nakov et al., 2004). Among these, probably the most similar to our work is the work by Qazvinian et al. (2010), where a set of important keyphrases is extracted first from the citation contexts in which the paper to be summarized is cited by other papers and then the “best” subset of sentences that </context>
</contexts>
<marker>Chakrabarti, Dom, Raghavan, Rajagopalan, Gibson, Kleinberg, 1998</marker>
<rawString>Soumen Chakrabarti, Byron Dom, Prabhakar Raghavan, Sridhar Rajagopalan, David Gibson, and Jon Kleinberg. 1998. Automatic resource compilation by analyzing hyperlink structure and associated text. Comput. Netw. ISDNSyst., 30(1-7):65–74, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chen Cheng</author>
<author>Haiqin Yang</author>
<author>Michael R Lyu</author>
<author>Irwin King</author>
</authors>
<title>Where you like to go next: Successive point-of-interest recommendation.</title>
<date>2013</date>
<booktitle>In Proc. of IJCAI’13,</booktitle>
<pages>2605--2611</pages>
<location>Beijing, China.</location>
<contexts>
<context position="3116" citStr="Cheng et al., 2013" startWordPosition="471" endWordPosition="474">rs. In a citation network, information flows from one paper to another via the citation relation (Shi et al., 2010). This information flow and the influence of one paper on another are specifically captured by means of citation contexts, i.e., short text segments surrounding a citation’s mention. These contexts are not arbitrary, but they serve as brief summaries of a cited paper. Figure 1 illustrates this idea using a small citation network of a paper by Rendle et al. (2010) that cites (Zimdars et al., 2001), (Hu et al., 2008), (Pan and Scholz, 2009) and (Shani et al., 2005) and is cited by (Cheng et al., 2013). The citation mentions and citation contexts are shown with a dashed line. Note the high overlap between the words in contexts and those in the title and abstract (shown in bold) and the author-annotated keywords. One question that can be raised is the following: Can we effectively exploit information available in large inter-linked document networks in order to improve the performance of keyphrase extraction? The research that we describe in this paper addresses specifically this question using citation networks of research papers as a case study. Extracting keyphrases that can accurately “r</context>
</contexts>
<marker>Cheng, Yang, Lyu, King, 2013</marker>
<rawString>Chen Cheng, Haiqin Yang, Michael R. Lyu, and Irwin King. 2013. Where you like to go next: Successive point-of-interest recommendation. In Proc. of IJCAI’13, pages 2605–2611, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eibe Frank</author>
<author>Gordon W Paynter</author>
<author>Ian H Witten</author>
<author>Carl Gutwin</author>
<author>Craig G Nevill-Manning</author>
</authors>
<title>Domain-specific keyphrase extraction.</title>
<date>1999</date>
<booktitle>In Proceedings of the 16th International Joint Conference on Artificial Intelligence - Volume 2, IJCAI’99,</booktitle>
<pages>668--673</pages>
<location>Stockholm,</location>
<contexts>
<context position="5593" citStr="Frank et al., 1999" startWordPosition="852" endWordPosition="855"> Web and Knowledge Discovery from Data. The rest of the paper is organized as follows: We summarize closely related work in Section 2. The supervised classification for keyphrase extraction is discussed in Section 3. Experiments and results are presented in Section 4, followed by conclusions and future directions of our work. 2 Related Work Many approaches to keyphrase extraction have been proposed in the literature along two lines of research: supervised and unsupervised, using different types of documents including scientific abstracts, newswire documents, meeting transcripts, and webpages (Frank et al., 1999; Hulth, 2003; Nguyen and Kan, 2007; Liu et al., 2009; Marujo et al., 2013; Mihalcea and Tarau, 2004). In the supervised line of research, keyphrase extraction is formulated as a binary classification problem, where candidate phrases are classified as either positive (i.e., keyphrases) or negative (i.e., non-keyphrases) (Frank et al., 1999; Turney, 2000; Hulth, 2003). Different feature sets and classification algorithms gave rise to different models. For example, Hulth (2003) used four different features in conjunction with a bagging technique. These features are: term frequency, collection fr</context>
<context position="13656" citStr="Frank et al., 1999" startWordPosition="2153" endWordPosition="2156">lassifies a phrase as a keyphrase if the probability of the phrase belonging to the positive class is greater than 0.5. However, the default threshold of 0.5 can be varied to allow only high-confidence (e.g., 0.9 confidence) phrases to be classified as keyphrases. 3.1.1 Features We consider the following features in our model, which are shown in Table 1. They are divided into three categories: (1) Existing features for keyphrase extraction include: tf-idf, i.e., the term frequency - inverse document frequency of a candidate phrase, computed for each target paper; This feature was used in KEA (Frank et al., 1999); relative position, i.e., the position of the first occurrence of a phrase normalized by the length (in the number of tokens) of the target paper; POS, i.e., a phrase’s part-of-speech tag. If a phrase is composed by more than one term, then the POS will contain the tags of all terms. The relative position was used in both KEA and Hulth (2003), and POS was used in Hulth; (2) Novel features - Citation Network Based include: inCited and inCiting, i.e., boolean features that are true if the candidate phrase occurs in cited and citing contexts, respectively. We differentiate between cited and citi</context>
<context position="20392" citStr="Frank et al., 1999" startWordPosition="3295" endWordPosition="3298">n” or “conclusion” needs further attention. From the pdf of each paper, we extracted the author-input keyphrases. An analysis of these keyphrases revealed that generally authors describe their work using, almost half of the time, bigrams, followed by unigrams and only rarely using trigrams (or higher n-grams). A summary of our datasets that contains the number of papers, the average number of cited and citing contexts per paper, the average number of keyphrases per paper, and the number of unigrams, bigrams and trigrams, in each collection, is shown in Table 2. Consistent with previous works (Frank et al., 1999; Hulth, 2003), the positive and negative examples in our datasets correspond to candidate phrases that consist of up to three tokens. The positive examples are candidate phrases that have a match in the author-input keyphrases, whereas negative examples correspond to the remaining candidate phrases. Context lengths. In CiteSeer&amp;quot;, citation contexts have about 50 words on each side of a citation mention. A previous study by Ritchie et al. (2008) shows that a fixed window length of about 100 words around a citation mention is generally effective for information retrieval tasks. For this reason, </context>
<context position="24943" citStr="Frank et al. (1999)" startWordPosition="4035" endWordPosition="4038"> KEA are the tf-idf and the relative position of a candidate phrase, whereas those used in Hulth’s approach are tf, cf (i.e., collection frequency), relative position and POS tags. CeKE is trained using all features from Table 1. Among the three methods for candidate phrase formation proposed in Hulth (2003), i.e., n-grams, NP-chunks, and POS Tag Patterns, our Hulth’s implementation is based on n-grams since this gives the best results among all methods (see (Hulth, 2003) for more details). In addition, the n-grams method is the most similar to our candidate phrase generation and that used in Frank et al. (1999). As can be seen from Table 3, CeKE outperforms KEA and Hulth’s approach in terms of all performance measures on both WWW and KDD, with a substantial improvement in recall over both approaches. For example, on WWW, CeKE achieves a recall of 0.386 compared to 0.146 and 0.107 recall achieved by KEA and Hulth’s, respectively. 2http://www.cs.waikato.ac.nz/ml/weka/ 1440 WWW KDD Method Precision Recall F1-score Precision Recall F1-score Citation - Enhanced (CeKE) 0.227 0.386 0.284 0.213 0.413 0.280 TF-IDF - Top 5 0.089 0.100 0.094 0.083 0.102 0.092 TF-IDF - Top 10 0.075 0.169 0.104 0.080 0.203 0.115</context>
</contexts>
<marker>Frank, Paynter, Witten, Gutwin, Nevill-Manning, 1999</marker>
<rawString>Eibe Frank, Gordon W. Paynter, Ian H. Witten, Carl Gutwin, and Craig G. Nevill-Manning. 1999. Domain-specific keyphrase extraction. In Proceedings of the 16th International Joint Conference on Artificial Intelligence - Volume 2, IJCAI’99, pages 668–673, Stockholm, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katerina T Frantzi</author>
<author>Sophia Ananiadou</author>
<author>Jun-ichi Tsujii</author>
</authors>
<title>The c-value/nc-value method of automatic recognition for multi-word terms.</title>
<date>1998</date>
<booktitle>In Proc. of ECDL ’98,</booktitle>
<pages>585--604</pages>
<contexts>
<context position="7647" citStr="Frantzi et al. (1998)" startWordPosition="1177" endWordPosition="1180">a. Enhancing our models with Wikipedia information would be an interesting future direction to pursue. In the unsupervised line of research, keyphrase extraction is formulated as a ranking problem, where keyphrases are ranked using their tf (Barker and Cornacchia, 2000), tf-idf (Zhang et al., 2007; Lee and Kim, 2008; Liu et al., 2009; Tonella et al., 2003), and term informativeness (Wu and Giles, 2013; Rennie and Jaakkola, 2005; Kireyev, 2009) (among others). The ranking based on tf-idf has 1436 been shown to work well in practice (Liu et al., 2009; Hasan and Ng, 2010) despite its simplicity. Frantzi et al. (1998) combined linguistics and statistical information to extract technical terms from documents in digital libraries. Graph-based algorithms and centrality measures are also widely used in unsupervised models. A word graph is built for each document such that nodes correspond to words and edges correspond to word association patterns. Nodes are then ranked using graph centrality measures such as PageRank and its variants (Mihalcea and Tarau, 2004; Wan and Xiao, 2008; Liu et al., 2010; Zhao et al., 2011), HITS scores (Litvak and Last, 2008), as well as node degree and betweenness (Boudin, 2013; Xie</context>
</contexts>
<marker>Frantzi, Ananiadou, Tsujii, 1998</marker>
<rawString>Katerina T. Frantzi, Sophia Ananiadou, and Jun-ichi Tsujii. 1998. The c-value/nc-value method of automatic recognition for multi-word terms. In Proc. of ECDL ’98, pages 585–604.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sujatha Das Gollapalli</author>
<author>Cornelia Caragea</author>
</authors>
<title>Extracting keyphrases from research papers using citation networks.</title>
<date>2014</date>
<booktitle>In Proceedings of the 28th AAAI Conference on Artificial Intelligence (AAAI14),</booktitle>
<location>Qu´ebec City, Qu´ebec, Canada.</location>
<contexts>
<context position="10997" citStr="Gollapalli and Caragea, 2014" startWordPosition="1738" endWordPosition="1742"> work, we propose a supervised approach to a different task: keyphrase extraction. Mei and Zhai (2008) used information from citation contexts to determine what sentences of a paper are of high impact (as measured by the influence of a target paper on further studies of similar or related topics). These sentences constitute the impact-based summary of the paper. Despite the use of citation contexts and anchor text in many IR and NLP tasks, to our knowledge, we are the first to propose the incorporation of information available in citation networks for keyphrase extraction. In our recent work (Gollapalli and Caragea, 2014), we designed a fully unsupervised graph-based algorithm that incorporates evidence from multiple sources (citation contexts as well as document content) in a flexible manner to score keywords. In the current work, we present a supervised approach to keyphrase extraction from research papers that are embedded in large citation networks, and propose novel features that show improvement over strong supervised and unsupervised baselines. To our knowledge, features extracted from citation contexts have not been used before for keyphrase extraction in a supervised learning framework. 3 Problem Char</context>
</contexts>
<marker>Gollapalli, Caragea, 2014</marker>
<rawString>Sujatha Das Gollapalli and Cornelia Caragea. 2014. Extracting keyphrases from research papers using citation networks. In Proceedings of the 28th AAAI Conference on Artificial Intelligence (AAAI14), Qu´ebec City, Qu´ebec, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Khaled M Hammouda</author>
<author>Diego N Matute</author>
<author>Mohamed S Kamel</author>
</authors>
<title>Corephrase: Keyphrase extraction for document clustering.</title>
<date>2005</date>
<booktitle>In Proc. of the 4th</booktitle>
<contexts>
<context position="1428" citStr="Hammouda et al., 2005" startWordPosition="196" endWordPosition="199"> on citation network information and use them in conjunction with traditional features for keyphrase extraction to obtain remarkable improvements in performance over strong baselines. 1 Introduction Keyphrase extraction is the problem of automatically extracting important phrases or concepts (i.e., the essence) of a document. Keyphrases provide a high-level topic description of a document and are shown to be rich sources of information for many applications such as document classification, clustering, recommendation, indexing, searching, and summarization (Jones and Staveley, 1999; Zha, 2002; Hammouda et al., 2005; Pudota et al., 2010; Turney, 2003). Despite the fact that keyphrase extraction has been widely researched in the natural language processing community, its performance is still far from being satisfactory (Hasan and Ng, 2014). Many previous approaches to keyphrase extraction generally used only the textual content of a target document to extract keyphrases (Hulth, 2003; Mihalcea and Tarau, 2004; Liu et al., 2010). Recently, Wan and Xiao (2008) proposed a model that incorporates a local neighborhood of a document. However, their neighborhood is limited to textually-similar documents, where th</context>
</contexts>
<marker>Hammouda, Matute, Kamel, 2005</marker>
<rawString>Khaled M. Hammouda, Diego N. Matute, and Mohamed S. Kamel. 2005. Corephrase: Keyphrase extraction for document clustering. In Proc. of the 4th</rawString>
</citation>
<citation valid="false">
<booktitle>International Conference on Machine Learning and Data Mining in Pattern Recognition, MLDM’05,</booktitle>
<pages>265--274</pages>
<location>Leipzig, Germany.</location>
<marker></marker>
<rawString>International Conference on Machine Learning and Data Mining in Pattern Recognition, MLDM’05, pages 265–274, Leipzig, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kazi Saidul Hasan</author>
<author>Vincent Ng</author>
</authors>
<title>Conundrums in Unsupervised Keyphrase Extraction: Making Sense of the State-of-the-Art.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics,</booktitle>
<pages>365--373</pages>
<contexts>
<context position="7601" citStr="Hasan and Ng, 2010" startWordPosition="1170" endWordPosition="1173">e used only information intrinsic to our data. Enhancing our models with Wikipedia information would be an interesting future direction to pursue. In the unsupervised line of research, keyphrase extraction is formulated as a ranking problem, where keyphrases are ranked using their tf (Barker and Cornacchia, 2000), tf-idf (Zhang et al., 2007; Lee and Kim, 2008; Liu et al., 2009; Tonella et al., 2003), and term informativeness (Wu and Giles, 2013; Rennie and Jaakkola, 2005; Kireyev, 2009) (among others). The ranking based on tf-idf has 1436 been shown to work well in practice (Liu et al., 2009; Hasan and Ng, 2010) despite its simplicity. Frantzi et al. (1998) combined linguistics and statistical information to extract technical terms from documents in digital libraries. Graph-based algorithms and centrality measures are also widely used in unsupervised models. A word graph is built for each document such that nodes correspond to words and edges correspond to word association patterns. Nodes are then ranked using graph centrality measures such as PageRank and its variants (Mihalcea and Tarau, 2004; Wan and Xiao, 2008; Liu et al., 2010; Zhao et al., 2011), HITS scores (Litvak and Last, 2008), as well as </context>
<context position="23468" citStr="Hasan and Ng, 2010" startWordPosition="3788" endWordPosition="3791">no cited contexts are available. We studied the quality of predicted keyphrases when either cited or citing contexts are missing. For this, we compared the performance of models trained using both cited and citing contexts with that of models that use either cited or citing contexts. Evaluation metrics. To evaluate the performance of CeKE, we used the following metrics: precision, recall and F1-score for the positive class since correct identification of keyphrases is of most interest. These metrics were widely used in previous works (Hulth, 2003; Mihalcea and Tarau, 2004; Wan and Xiao, 2008; Hasan and Ng, 2010). The reported values are averaged in 10-fold crossvalidation experiments, where folds were created at document level and candidate phrases were extracted from the documents in each fold to form the training and test sets. In all experiments, we used Naive Bayes and their Weka implementation2. However, any probabilistic classifier that returns a posterior probability of the class given an example, can be used with our features. The θ parameter was set to the (title and abstract) tf-idf averaged over the entire collection, while Q was set to 20. These values were estimated on a validation set s</context>
</contexts>
<marker>Hasan, Ng, 2010</marker>
<rawString>Kazi Saidul Hasan and Vincent Ng. 2010. Conundrums in Unsupervised Keyphrase Extraction: Making Sense of the State-of-the-Art. In Proceedings of the 23rd International Conference on Computational Linguistics, pages 365–373.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kazi Saidul Hasan</author>
<author>Vincent Ng</author>
</authors>
<title>Automatic keyphrase extraction: A survey of the state of the art.</title>
<date>2014</date>
<booktitle>In Proc. of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="1655" citStr="Hasan and Ng, 2014" startWordPosition="233" endWordPosition="236">oblem of automatically extracting important phrases or concepts (i.e., the essence) of a document. Keyphrases provide a high-level topic description of a document and are shown to be rich sources of information for many applications such as document classification, clustering, recommendation, indexing, searching, and summarization (Jones and Staveley, 1999; Zha, 2002; Hammouda et al., 2005; Pudota et al., 2010; Turney, 2003). Despite the fact that keyphrase extraction has been widely researched in the natural language processing community, its performance is still far from being satisfactory (Hasan and Ng, 2014). Many previous approaches to keyphrase extraction generally used only the textual content of a target document to extract keyphrases (Hulth, 2003; Mihalcea and Tarau, 2004; Liu et al., 2010). Recently, Wan and Xiao (2008) proposed a model that incorporates a local neighborhood of a document. However, their neighborhood is limited to textually-similar documents, where the cosine similarity between the tf-idf vectors of documents is used to compute their similarity. We posit that, in addition to a document’s textual content and textually-similar neighbors, other informative neighborhoods exist </context>
<context position="22391" citStr="Hasan and Ng, 2014" startWordPosition="3611" endWordPosition="3614">rmance of citationenhanced keyphrase extraction (CeKE) compare with the performance of existing supervised models that use only information intrinsic to the data and what are the most informative features for classification? We compared CeKE’s performance with that of classifiers trained on KEA features only and Hulth’s features only and present a ranking of features based on information gain. 2. How do supervised models that integrate citation network information compare with recent unsupervised models? Since recent unsupervised approaches are becoming competitive with supervised approaches (Hasan and Ng, 2014), we also compared CeKE with unsupervised ranking of candidate phrases by TF-IDF, TextRank (Mihalcea and Tarau, 2004) and ExpandRank (Wan and Xiao, 2008). For unsupervised, we considered top 5 and top 10 ranked phrases when computing “@5” and “@10” measures. 3. How well does our proposed model perform in the absence of either cited or citing contexts? Since newly published scientific papers are not cited by many other papers, e.g., due to their recency, no cited contexts are available. We studied the quality of predicted keyphrases when either cited or citing contexts are missing. For this, we</context>
</contexts>
<marker>Hasan, Ng, 2014</marker>
<rawString>Kazi Saidul Hasan and Vincent Ng. 2014. Automatic keyphrase extraction: A survey of the state of the art. In Proc. of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qi He</author>
<author>Jian Pei</author>
<author>Daniel Kifer</author>
<author>Prasenjit Mitra</author>
<author>Lee Giles</author>
</authors>
<title>Context-aware citation recommendation.</title>
<date>2010</date>
<booktitle>In Proc. of WWW ’10,</booktitle>
<pages>421--430</pages>
<location>Raleigh, North Carolina, USA.</location>
<contexts>
<context position="8987" citStr="He et al., 2010" startWordPosition="1397" endWordPosition="1400">e document itself, and applied this approach to news articles on the Web. Their local neighborhood consists of textually similar documents, and did not capture information contained in document networks. Using terms from citation contexts of scientific papers is not a new idea. It was used before in various applications. For example, Ritchie et al. (2006) used a combination of terms from citation contexts and existing index terms of a paper to improve indexing of cited papers. Citation contexts were also used to improve the performance of citation recommendation systems (Kataria et al., 2010; He et al., 2010) and to study author influence (Kataria et al., 2011). This idea of using terms from citation contexts resembles the analysis of hyperlinks and the graph structure of the Web, which are instrumental in Web search (Manning et al., 2008). Many current Web search engines build on the intuition that the anchor text pointing to a page is a good descriptor of its content, and thus use anchor text terms as additional index terms for a target webpage. The use of links and anchor text was thoroughly researched for IR tasks (Koolen and Kamps, 2010), broadening a user’s search (Chakrabarti et al., 1998),</context>
</contexts>
<marker>He, Pei, Kifer, Mitra, Giles, 2010</marker>
<rawString>Qi He, Jian Pei, Daniel Kifer, Prasenjit Mitra, and Lee Giles. 2010. Context-aware citation recommendation. In Proc. of WWW ’10, pages 421–430, Raleigh, North Carolina, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yifan Hu</author>
<author>Yehuda Koren</author>
<author>Chris Volinsky</author>
</authors>
<title>Collaborative filtering for implicit feedback datasets.</title>
<date>2008</date>
<booktitle>In Proc. of the 8th IEEE Intl. Conference on Data Mining, ICDM ’08,</booktitle>
<pages>263--272</pages>
<contexts>
<context position="3030" citStr="Hu et al., 2008" startWordPosition="454" endWordPosition="457">nnected in giant citation networks, in which papers cite or are cited by other papers. In a citation network, information flows from one paper to another via the citation relation (Shi et al., 2010). This information flow and the influence of one paper on another are specifically captured by means of citation contexts, i.e., short text segments surrounding a citation’s mention. These contexts are not arbitrary, but they serve as brief summaries of a cited paper. Figure 1 illustrates this idea using a small citation network of a paper by Rendle et al. (2010) that cites (Zimdars et al., 2001), (Hu et al., 2008), (Pan and Scholz, 2009) and (Shani et al., 2005) and is cited by (Cheng et al., 2013). The citation mentions and citation contexts are shown with a dashed line. Note the high overlap between the words in contexts and those in the title and abstract (shown in bold) and the author-annotated keywords. One question that can be raised is the following: Can we effectively exploit information available in large inter-linked document networks in order to improve the performance of keyphrase extraction? The research that we describe in this paper addresses specifically this question using citation net</context>
</contexts>
<marker>Hu, Koren, Volinsky, 2008</marker>
<rawString>Yifan Hu, Yehuda Koren, and Chris Volinsky. 2008. Collaborative filtering for implicit feedback datasets. In Proc. of the 8th IEEE Intl. Conference on Data Mining, ICDM ’08, pages 263–272.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anette Hulth</author>
</authors>
<title>Improved Automatic Keyword Extraction Given More Linguistic Knowledge.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 conference on Empirical methods in natural language processing, EMNLP ’03,</booktitle>
<pages>216--223</pages>
<contexts>
<context position="1801" citStr="Hulth, 2003" startWordPosition="257" endWordPosition="258">document and are shown to be rich sources of information for many applications such as document classification, clustering, recommendation, indexing, searching, and summarization (Jones and Staveley, 1999; Zha, 2002; Hammouda et al., 2005; Pudota et al., 2010; Turney, 2003). Despite the fact that keyphrase extraction has been widely researched in the natural language processing community, its performance is still far from being satisfactory (Hasan and Ng, 2014). Many previous approaches to keyphrase extraction generally used only the textual content of a target document to extract keyphrases (Hulth, 2003; Mihalcea and Tarau, 2004; Liu et al., 2010). Recently, Wan and Xiao (2008) proposed a model that incorporates a local neighborhood of a document. However, their neighborhood is limited to textually-similar documents, where the cosine similarity between the tf-idf vectors of documents is used to compute their similarity. We posit that, in addition to a document’s textual content and textually-similar neighbors, other informative neighborhoods exist that have the potential to improve keyphrase extraction. For example, in a scholarly domain, research papers are not isolated. Rather, they are hi</context>
<context position="5606" citStr="Hulth, 2003" startWordPosition="856" endWordPosition="857">iscovery from Data. The rest of the paper is organized as follows: We summarize closely related work in Section 2. The supervised classification for keyphrase extraction is discussed in Section 3. Experiments and results are presented in Section 4, followed by conclusions and future directions of our work. 2 Related Work Many approaches to keyphrase extraction have been proposed in the literature along two lines of research: supervised and unsupervised, using different types of documents including scientific abstracts, newswire documents, meeting transcripts, and webpages (Frank et al., 1999; Hulth, 2003; Nguyen and Kan, 2007; Liu et al., 2009; Marujo et al., 2013; Mihalcea and Tarau, 2004). In the supervised line of research, keyphrase extraction is formulated as a binary classification problem, where candidate phrases are classified as either positive (i.e., keyphrases) or negative (i.e., non-keyphrases) (Frank et al., 1999; Turney, 2000; Hulth, 2003). Different feature sets and classification algorithms gave rise to different models. For example, Hulth (2003) used four different features in conjunction with a bagging technique. These features are: term frequency, collection frequency, the </context>
<context position="14001" citStr="Hulth (2003)" startWordPosition="2219" endWordPosition="2220">e 1. They are divided into three categories: (1) Existing features for keyphrase extraction include: tf-idf, i.e., the term frequency - inverse document frequency of a candidate phrase, computed for each target paper; This feature was used in KEA (Frank et al., 1999); relative position, i.e., the position of the first occurrence of a phrase normalized by the length (in the number of tokens) of the target paper; POS, i.e., a phrase’s part-of-speech tag. If a phrase is composed by more than one term, then the POS will contain the tags of all terms. The relative position was used in both KEA and Hulth (2003), and POS was used in Hulth; (2) Novel features - Citation Network Based include: inCited and inCiting, i.e., boolean features that are true if the candidate phrase occurs in cited and citing contexts, respectively. We differentiate between cited and citing contexts for a paper: let d be a target paper and C be a citation network such that d E C. A cited context for d is a context in which d is cited by some paper di in C. A citing context for d is a context in which d is citing some paper dj in C. If a paper is cited in multiple contexts by another paper, the contexts are aggregated into a si</context>
<context position="17358" citStr="Hulth, 2003" startWordPosition="2803" endWordPosition="2804"> Hulth’s methods POS the part-of-speech tag of the phrase; used in Hulth’s methods Novel features - Citation Network Based tf-idf inCited if the phrase occurs in cited contexts inCiting if the phrase occurs in citing contexts citation tf-idf the tf-idf value of the phrase, computed from the aggregated citation contexts 1438 Dataset Num. (#) Average Average Average #uni- #bi- #triPapers Cited Ctx. Citing Ctx. Keyphrases grams grams grams WWW 425 15.45 18.78 4.87 680 1036 247 KDD 365 12.69 19.74 4.03 363 853 189 Table 2: A summary of our datasets. speech filters. Consistent with previous works (Hulth, 2003; Mihalcea and Tarau, 2004; Liu et al., 2010; Wan and Xiao, 2008), only nouns and adjectives are retained to form candidate phrases. The generation process consists of two steps. First, using the NLP Stanford part of speech tagger, we preprocess each document and keep only the nouns and adjectives corresponding to {NN, NNS, NNP, NNPS, JJ}. We apply the Porter stemmer on every word. The position of each word is kept consistent with the initial state of the document before any word removal is made. Second, words extracted in the first step that have contiguous positions in a document are concate</context>
<context position="19619" citStr="Hulth, 2003" startWordPosition="3169" endWordPosition="3170"> cited and citing contexts. Since our goal is to study the impact of citation network information on extracting keyphrases, a paper was considered for analysis if it had at least 1http://citeseerx.ist.psu.edu/ one cited and one citing context. For each paper, we used: the title and abstract (referred to as the target paper) and its citation contexts. The reason for not considering the entire text of a paper is that scientific papers contain details, e.g., discussion of results, experimental design, notation, that do not provide additional benefits for extracting keyphrases. Hence, similar to (Hulth, 2003; Mihalcea and Tarau, 2004; Liu et al., 2009), we did not use the entire text of a paper. However, extracting keyphrases from sections such as “introduction” or “conclusion” needs further attention. From the pdf of each paper, we extracted the author-input keyphrases. An analysis of these keyphrases revealed that generally authors describe their work using, almost half of the time, bigrams, followed by unigrams and only rarely using trigrams (or higher n-grams). A summary of our datasets that contains the number of papers, the average number of cited and citing contexts per paper, the average </context>
<context position="23401" citStr="Hulth, 2003" startWordPosition="3778" endWordPosition="3779">ot cited by many other papers, e.g., due to their recency, no cited contexts are available. We studied the quality of predicted keyphrases when either cited or citing contexts are missing. For this, we compared the performance of models trained using both cited and citing contexts with that of models that use either cited or citing contexts. Evaluation metrics. To evaluate the performance of CeKE, we used the following metrics: precision, recall and F1-score for the positive class since correct identification of keyphrases is of most interest. These metrics were widely used in previous works (Hulth, 2003; Mihalcea and Tarau, 2004; Wan and Xiao, 2008; Hasan and Ng, 2010). The reported values are averaged in 10-fold crossvalidation experiments, where folds were created at document level and candidate phrases were extracted from the documents in each fold to form the training and test sets. In all experiments, we used Naive Bayes and their Weka implementation2. However, any probabilistic classifier that returns a posterior probability of the class given an example, can be used with our features. The θ parameter was set to the (title and abstract) tf-idf averaged over the entire collection, while</context>
<context position="24633" citStr="Hulth (2003)" startWordPosition="3985" endWordPosition="3986">ese values were estimated on a validation set sampled from training. 4.3 Results and Discussion The impact of citation network information on the keyphrase extraction task. Table 3 shows the results of the comparison of CeKE with two supervised approaches, KEA and Hulth’s approach. The features used in KEA are the tf-idf and the relative position of a candidate phrase, whereas those used in Hulth’s approach are tf, cf (i.e., collection frequency), relative position and POS tags. CeKE is trained using all features from Table 1. Among the three methods for candidate phrase formation proposed in Hulth (2003), i.e., n-grams, NP-chunks, and POS Tag Patterns, our Hulth’s implementation is based on n-grams since this gives the best results among all methods (see (Hulth, 2003) for more details). In addition, the n-grams method is the most similar to our candidate phrase generation and that used in Frank et al. (1999). As can be seen from Table 3, CeKE outperforms KEA and Hulth’s approach in terms of all performance measures on both WWW and KDD, with a substantial improvement in recall over both approaches. For example, on WWW, CeKE achieves a recall of 0.386 compared to 0.146 and 0.107 recall achieved</context>
</contexts>
<marker>Hulth, 2003</marker>
<rawString>Anette Hulth. 2003. Improved Automatic Keyword Extraction Given More Linguistic Knowledge. In Proceedings of the 2003 conference on Empirical methods in natural language processing, EMNLP ’03, pages 216–223.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xin Jiang</author>
<author>Yunhua Hu</author>
<author>Hang Li</author>
</authors>
<title>A Ranking Approach to Keyphrase Extraction.</title>
<date>2009</date>
<booktitle>In Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>756--757</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="15397" citStr="Jiang et al., 2009" startWordPosition="2476" endWordPosition="2479"> of a candidate phrase, i.e., the distance of the first occurrence of a phrase from the beginning of a paper; this is similar to relative position except that it does not consider the length of a paper; tf-idfOver, i.e., a boolean feature, which is true if the tf-idf of a candidate phrase is greater than a threshold θ, and firstPosUnder, also a boolean feature, which is true if the distance of the first occurrence of a phrase from the beginning of a target paper is below some value Q. This feature is similar to the feature is-in-title, used previously in the literature (Litvak and Last, 2008; Jiang et al., 2009). Both tf-idf and citation tf-idf features showed better results when each tf was divided by the maximum tf values from the target paper or citation contexts. The tf-idf features have high values for phrases that are frequent in a paper or citation contexts, but are less frequent in collection and have low values for phrases with high collection frequency. We computed the idf component from each collection used in experiments. Phrases that occur in cited and citing contexts as well as early in a paper are likely to be keyphrases since: (1) they capture some aspect about the target paper and (2</context>
</contexts>
<marker>Jiang, Hu, Li, 2009</marker>
<rawString>Xin Jiang, Yunhua Hu, and Hang Li. 2009. A Ranking Approach to Keyphrase Extraction. In Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval, pages 756–757. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steve Jones</author>
<author>Mark S Staveley</author>
</authors>
<title>Phrasier: A system for interactive document retrieval using keyphrases.</title>
<date>1999</date>
<booktitle>In Proceedings of SIGIR ’99,</booktitle>
<pages>160--167</pages>
<location>Berkeley, California, USA.</location>
<contexts>
<context position="1394" citStr="Jones and Staveley, 1999" startWordPosition="189" endWordPosition="193">s end, we design novel features based on citation network information and use them in conjunction with traditional features for keyphrase extraction to obtain remarkable improvements in performance over strong baselines. 1 Introduction Keyphrase extraction is the problem of automatically extracting important phrases or concepts (i.e., the essence) of a document. Keyphrases provide a high-level topic description of a document and are shown to be rich sources of information for many applications such as document classification, clustering, recommendation, indexing, searching, and summarization (Jones and Staveley, 1999; Zha, 2002; Hammouda et al., 2005; Pudota et al., 2010; Turney, 2003). Despite the fact that keyphrase extraction has been widely researched in the natural language processing community, its performance is still far from being satisfactory (Hasan and Ng, 2014). Many previous approaches to keyphrase extraction generally used only the textual content of a target document to extract keyphrases (Hulth, 2003; Mihalcea and Tarau, 2004; Liu et al., 2010). Recently, Wan and Xiao (2008) proposed a model that incorporates a local neighborhood of a document. However, their neighborhood is limited to tex</context>
</contexts>
<marker>Jones, Staveley, 1999</marker>
<rawString>Steve Jones and Mark S. Staveley. 1999. Phrasier: A system for interactive document retrieval using keyphrases. In Proceedings of SIGIR ’99, pages 160–167, Berkeley, California, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saurabh Kataria</author>
<author>Prasenjit Mitra</author>
<author>Sumit Bhatia</author>
</authors>
<title>Utilizing context in generative bayesian models for linked corpus. In</title>
<date>2010</date>
<booktitle>In Proc. ofAAAI ’10,</booktitle>
<pages>1340--1345</pages>
<location>Atlanta, Georgia, USA.</location>
<contexts>
<context position="8969" citStr="Kataria et al., 2010" startWordPosition="1393" endWordPosition="1396">ment in addition to the document itself, and applied this approach to news articles on the Web. Their local neighborhood consists of textually similar documents, and did not capture information contained in document networks. Using terms from citation contexts of scientific papers is not a new idea. It was used before in various applications. For example, Ritchie et al. (2006) used a combination of terms from citation contexts and existing index terms of a paper to improve indexing of cited papers. Citation contexts were also used to improve the performance of citation recommendation systems (Kataria et al., 2010; He et al., 2010) and to study author influence (Kataria et al., 2011). This idea of using terms from citation contexts resembles the analysis of hyperlinks and the graph structure of the Web, which are instrumental in Web search (Manning et al., 2008). Many current Web search engines build on the intuition that the anchor text pointing to a page is a good descriptor of its content, and thus use anchor text terms as additional index terms for a target webpage. The use of links and anchor text was thoroughly researched for IR tasks (Koolen and Kamps, 2010), broadening a user’s search (Chakraba</context>
</contexts>
<marker>Kataria, Mitra, Bhatia, 2010</marker>
<rawString>Saurabh Kataria, Prasenjit Mitra, and Sumit Bhatia. 2010. Utilizing context in generative bayesian models for linked corpus. In In Proc. ofAAAI ’10, pages 1340–1345, Atlanta, Georgia, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saurabh Kataria</author>
<author>Prasenjit Mitra</author>
<author>Cornelia Caragea</author>
<author>C Lee Giles</author>
</authors>
<title>Context sensitive topic models for author influence in document networks.</title>
<date>2011</date>
<booktitle>In Proceedings of IJCAI’11,</booktitle>
<pages>2274--2280</pages>
<location>Barcelona, Catalonia,</location>
<contexts>
<context position="9040" citStr="Kataria et al., 2011" startWordPosition="1407" endWordPosition="1410">news articles on the Web. Their local neighborhood consists of textually similar documents, and did not capture information contained in document networks. Using terms from citation contexts of scientific papers is not a new idea. It was used before in various applications. For example, Ritchie et al. (2006) used a combination of terms from citation contexts and existing index terms of a paper to improve indexing of cited papers. Citation contexts were also used to improve the performance of citation recommendation systems (Kataria et al., 2010; He et al., 2010) and to study author influence (Kataria et al., 2011). This idea of using terms from citation contexts resembles the analysis of hyperlinks and the graph structure of the Web, which are instrumental in Web search (Manning et al., 2008). Many current Web search engines build on the intuition that the anchor text pointing to a page is a good descriptor of its content, and thus use anchor text terms as additional index terms for a target webpage. The use of links and anchor text was thoroughly researched for IR tasks (Koolen and Kamps, 2010), broadening a user’s search (Chakrabarti et al., 1998), query refinement (Kraft and Zien, 2004), and enrichi</context>
</contexts>
<marker>Kataria, Mitra, Caragea, Giles, 2011</marker>
<rawString>Saurabh Kataria, Prasenjit Mitra, Cornelia Caragea, and C. Lee Giles. 2011. Context sensitive topic models for author influence in document networks. In Proceedings of IJCAI’11, pages 2274–2280, Barcelona, Catalonia, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Su Nam Kim</author>
<author>Olena Medelyan</author>
<author>Min-Yen Kan</author>
<author>Timothy Baldwin</author>
</authors>
<title>SemEval-2010 Task 5: Automatic Keyphrase Extraction from Scientific Articles.</title>
<date>2010</date>
<booktitle>In Proceedings of the 5th International Workshop on Semantic Evaluation, SemEval ’10,</booktitle>
<pages>21--26</pages>
<location>Los Angeles, California.</location>
<contexts>
<context position="3996" citStr="Kim et al., 2010" startWordPosition="611" endWordPosition="614"> Can we effectively exploit information available in large inter-linked document networks in order to improve the performance of keyphrase extraction? The research that we describe in this paper addresses specifically this question using citation networks of research papers as a case study. Extracting keyphrases that can accurately “represent” research papers is crucial to dealing with the large numbers of research papers published during these “big data” times. The importance of keyphrase extraction from research papers is also emphasized by the recent SemEval 2010 Shared Task on this topic (Kim et al., 2010; Kim et al., 2013). Our contributions. We present a supervised 1435 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1435–1446, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics Figure 1: A small citation network corresponding to a paper by Rendle et al. (2010). approach to keyphrase extraction from research papers that, in addition to the information contained in a paper itself, effectively incorporates, in the learned models, information from the paper’s local neighborhood available in citation networks. T</context>
</contexts>
<marker>Kim, Medelyan, Kan, Baldwin, 2010</marker>
<rawString>Su Nam Kim, Olena Medelyan, Min-Yen Kan, and Timothy Baldwin. 2010. SemEval-2010 Task 5: Automatic Keyphrase Extraction from Scientific Articles. In Proceedings of the 5th International Workshop on Semantic Evaluation, SemEval ’10, pages 21–26, Los Angeles, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Su Nam Kim</author>
<author>Olena Medelyan</author>
<author>Min-Yen Kan</author>
<author>Timothy Baldwin</author>
</authors>
<title>Automatic keyphrase extraction from scientific articles. Language Resources and Evaluation,</title>
<date>2013</date>
<pages>47--3</pages>
<publisher>Springer,</publisher>
<contexts>
<context position="4015" citStr="Kim et al., 2013" startWordPosition="615" endWordPosition="618">y exploit information available in large inter-linked document networks in order to improve the performance of keyphrase extraction? The research that we describe in this paper addresses specifically this question using citation networks of research papers as a case study. Extracting keyphrases that can accurately “represent” research papers is crucial to dealing with the large numbers of research papers published during these “big data” times. The importance of keyphrase extraction from research papers is also emphasized by the recent SemEval 2010 Shared Task on this topic (Kim et al., 2010; Kim et al., 2013). Our contributions. We present a supervised 1435 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1435–1446, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics Figure 1: A small citation network corresponding to a paper by Rendle et al. (2010). approach to keyphrase extraction from research papers that, in addition to the information contained in a paper itself, effectively incorporates, in the learned models, information from the paper’s local neighborhood available in citation networks. To this end, we desi</context>
</contexts>
<marker>Kim, Medelyan, Kan, Baldwin, 2013</marker>
<rawString>Su Nam Kim, Olena Medelyan, Min-Yen Kan, and Timothy Baldwin. 2013. Automatic keyphrase extraction from scientific articles. Language Resources and Evaluation, Springer, 47(3):723–742.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kirill Kireyev</author>
</authors>
<title>Semantic-based estimation of term informativeness.</title>
<date>2009</date>
<booktitle>In Proc. of NAACL ’09,</booktitle>
<pages>530--538</pages>
<location>Boulder, Colorado.</location>
<contexts>
<context position="7473" citStr="Kireyev, 2009" startWordPosition="1148" endWordPosition="1149">nked document networks. Medelyan et al. (2009) extended KEA as well to integrate information from Wikipedia. In contrast, we used only information intrinsic to our data. Enhancing our models with Wikipedia information would be an interesting future direction to pursue. In the unsupervised line of research, keyphrase extraction is formulated as a ranking problem, where keyphrases are ranked using their tf (Barker and Cornacchia, 2000), tf-idf (Zhang et al., 2007; Lee and Kim, 2008; Liu et al., 2009; Tonella et al., 2003), and term informativeness (Wu and Giles, 2013; Rennie and Jaakkola, 2005; Kireyev, 2009) (among others). The ranking based on tf-idf has 1436 been shown to work well in practice (Liu et al., 2009; Hasan and Ng, 2010) despite its simplicity. Frantzi et al. (1998) combined linguistics and statistical information to extract technical terms from documents in digital libraries. Graph-based algorithms and centrality measures are also widely used in unsupervised models. A word graph is built for each document such that nodes correspond to words and edges correspond to word association patterns. Nodes are then ranked using graph centrality measures such as PageRank and its variants (Miha</context>
</contexts>
<marker>Kireyev, 2009</marker>
<rawString>Kirill Kireyev. 2009. Semantic-based estimation of term informativeness. In Proc. of NAACL ’09, pages 530–538, Boulder, Colorado.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marijn Koolen</author>
<author>Jaap Kamps</author>
</authors>
<title>The importance of anchor text for ad hoc search revisited.</title>
<date>2010</date>
<booktitle>In Proceedings of SIGIR ’10,</booktitle>
<pages>122--129</pages>
<location>Geneva, Switzerland.</location>
<contexts>
<context position="9531" citStr="Koolen and Kamps, 2010" startWordPosition="1496" endWordPosition="1499">ance of citation recommendation systems (Kataria et al., 2010; He et al., 2010) and to study author influence (Kataria et al., 2011). This idea of using terms from citation contexts resembles the analysis of hyperlinks and the graph structure of the Web, which are instrumental in Web search (Manning et al., 2008). Many current Web search engines build on the intuition that the anchor text pointing to a page is a good descriptor of its content, and thus use anchor text terms as additional index terms for a target webpage. The use of links and anchor text was thoroughly researched for IR tasks (Koolen and Kamps, 2010), broadening a user’s search (Chakrabarti et al., 1998), query refinement (Kraft and Zien, 2004), and enriching document representations (Metzler et al., 2009). Moreover, citation contexts were used for scientific paper summarization (Abu-Jbara and Radev, 2011; Qazvinian et al., 2010; Qazvinian and Radev, 2008; Mei and Zhai, 2008; Lehnert et al., 1990; Nakov et al., 2004). Among these, probably the most similar to our work is the work by Qazvinian et al. (2010), where a set of important keyphrases is extracted first from the citation contexts in which the paper to be summarized is cited by oth</context>
</contexts>
<marker>Koolen, Kamps, 2010</marker>
<rawString>Marijn Koolen and Jaap Kamps. 2010. The importance of anchor text for ad hoc search revisited. In Proceedings of SIGIR ’10, pages 122–129, Geneva, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reiner Kraft</author>
<author>Jason Zien</author>
</authors>
<title>Mining anchor text for query refinement.</title>
<date>2004</date>
<booktitle>In Proceedings of the 13th International Conference on World Wide Web, WWW ’04,</booktitle>
<pages>666--674</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="9627" citStr="Kraft and Zien, 2004" startWordPosition="1511" endWordPosition="1514">r influence (Kataria et al., 2011). This idea of using terms from citation contexts resembles the analysis of hyperlinks and the graph structure of the Web, which are instrumental in Web search (Manning et al., 2008). Many current Web search engines build on the intuition that the anchor text pointing to a page is a good descriptor of its content, and thus use anchor text terms as additional index terms for a target webpage. The use of links and anchor text was thoroughly researched for IR tasks (Koolen and Kamps, 2010), broadening a user’s search (Chakrabarti et al., 1998), query refinement (Kraft and Zien, 2004), and enriching document representations (Metzler et al., 2009). Moreover, citation contexts were used for scientific paper summarization (Abu-Jbara and Radev, 2011; Qazvinian et al., 2010; Qazvinian and Radev, 2008; Mei and Zhai, 2008; Lehnert et al., 1990; Nakov et al., 2004). Among these, probably the most similar to our work is the work by Qazvinian et al. (2010), where a set of important keyphrases is extracted first from the citation contexts in which the paper to be summarized is cited by other papers and then the “best” subset of sentences that contain such keyphrases is returned as th</context>
</contexts>
<marker>Kraft, Zien, 2004</marker>
<rawString>Reiner Kraft and Jason Zien. 2004. Mining anchor text for query refinement. In Proceedings of the 13th International Conference on World Wide Web, WWW ’04, pages 666–674, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sungjick Lee</author>
<author>Han-joon Kim</author>
</authors>
<title>News Keyword Extraction for Topic Tracking.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Fourth International Conference on Networked Computing and Advanced Information Management - Volume 02, NCM ’08,</booktitle>
<pages>554--559</pages>
<publisher>IEEE Computer Society.</publisher>
<location>Washington, DC, USA.</location>
<contexts>
<context position="7343" citStr="Lee and Kim, 2008" startWordPosition="1125" endWordPosition="1128"> term. In contrast to these works, we propose novel features extracted from the local neighborhoods of documents available in interlinked document networks. Medelyan et al. (2009) extended KEA as well to integrate information from Wikipedia. In contrast, we used only information intrinsic to our data. Enhancing our models with Wikipedia information would be an interesting future direction to pursue. In the unsupervised line of research, keyphrase extraction is formulated as a ranking problem, where keyphrases are ranked using their tf (Barker and Cornacchia, 2000), tf-idf (Zhang et al., 2007; Lee and Kim, 2008; Liu et al., 2009; Tonella et al., 2003), and term informativeness (Wu and Giles, 2013; Rennie and Jaakkola, 2005; Kireyev, 2009) (among others). The ranking based on tf-idf has 1436 been shown to work well in practice (Liu et al., 2009; Hasan and Ng, 2010) despite its simplicity. Frantzi et al. (1998) combined linguistics and statistical information to extract technical terms from documents in digital libraries. Graph-based algorithms and centrality measures are also widely used in unsupervised models. A word graph is built for each document such that nodes correspond to words and edges corr</context>
</contexts>
<marker>Lee, Kim, 2008</marker>
<rawString>Sungjick Lee and Han-joon Kim. 2008. News Keyword Extraction for Topic Tracking. In Proceedings of the 2008 Fourth International Conference on Networked Computing and Advanced Information Management - Volume 02, NCM ’08, pages 554–559, Washington, DC, USA. IEEE Computer Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wendy Lehnert</author>
<author>Claire Cardie</author>
<author>Ellen Rilofl</author>
</authors>
<title>Analyzing research papers using citation sentences.</title>
<date>1990</date>
<booktitle>In Proceedings of the 12th Annual Conference of the Cognitive Science Society,</booktitle>
<pages>511--518</pages>
<contexts>
<context position="9884" citStr="Lehnert et al., 1990" startWordPosition="1549" endWordPosition="1552"> the intuition that the anchor text pointing to a page is a good descriptor of its content, and thus use anchor text terms as additional index terms for a target webpage. The use of links and anchor text was thoroughly researched for IR tasks (Koolen and Kamps, 2010), broadening a user’s search (Chakrabarti et al., 1998), query refinement (Kraft and Zien, 2004), and enriching document representations (Metzler et al., 2009). Moreover, citation contexts were used for scientific paper summarization (Abu-Jbara and Radev, 2011; Qazvinian et al., 2010; Qazvinian and Radev, 2008; Mei and Zhai, 2008; Lehnert et al., 1990; Nakov et al., 2004). Among these, probably the most similar to our work is the work by Qazvinian et al. (2010), where a set of important keyphrases is extracted first from the citation contexts in which the paper to be summarized is cited by other papers and then the “best” subset of sentences that contain such keyphrases is returned as the summary. However, keyphrases in (Qazvinian et al., 2010) are extracted using frequent n-grams in a language model framework, whereas in our work, we propose a supervised approach to a different task: keyphrase extraction. Mei and Zhai (2008) used informat</context>
</contexts>
<marker>Lehnert, Cardie, Rilofl, 1990</marker>
<rawString>Wendy Lehnert, Claire Cardie, and Ellen Rilofl. 1990. Analyzing research papers using citation sentences. In Proceedings of the 12th Annual Conference of the Cognitive Science Society, pages 511–518.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marina Litvak</author>
<author>Mark Last</author>
</authors>
<title>GraphBased Keyword Extraction for Single-Document Summarization.</title>
<date>2008</date>
<booktitle>In Proceedings of the Workshop on Multi-source Multilingual Information Extraction and Summarization, MMIES ’08,</booktitle>
<pages>17--24</pages>
<location>Manchester, United Kingdom.</location>
<contexts>
<context position="8188" citStr="Litvak and Last, 2008" startWordPosition="1264" endWordPosition="1267">u et al., 2009; Hasan and Ng, 2010) despite its simplicity. Frantzi et al. (1998) combined linguistics and statistical information to extract technical terms from documents in digital libraries. Graph-based algorithms and centrality measures are also widely used in unsupervised models. A word graph is built for each document such that nodes correspond to words and edges correspond to word association patterns. Nodes are then ranked using graph centrality measures such as PageRank and its variants (Mihalcea and Tarau, 2004; Wan and Xiao, 2008; Liu et al., 2010; Zhao et al., 2011), HITS scores (Litvak and Last, 2008), as well as node degree and betweenness (Boudin, 2013; Xie, 2005). Wan and Xiao (2008) were the first to consider modeling a local neighborhood of a target document in addition to the document itself, and applied this approach to news articles on the Web. Their local neighborhood consists of textually similar documents, and did not capture information contained in document networks. Using terms from citation contexts of scientific papers is not a new idea. It was used before in various applications. For example, Ritchie et al. (2006) used a combination of terms from citation contexts and exis</context>
<context position="15376" citStr="Litvak and Last, 2008" startWordPosition="2472" endWordPosition="2475">include: first position of a candidate phrase, i.e., the distance of the first occurrence of a phrase from the beginning of a paper; this is similar to relative position except that it does not consider the length of a paper; tf-idfOver, i.e., a boolean feature, which is true if the tf-idf of a candidate phrase is greater than a threshold θ, and firstPosUnder, also a boolean feature, which is true if the distance of the first occurrence of a phrase from the beginning of a target paper is below some value Q. This feature is similar to the feature is-in-title, used previously in the literature (Litvak and Last, 2008; Jiang et al., 2009). Both tf-idf and citation tf-idf features showed better results when each tf was divided by the maximum tf values from the target paper or citation contexts. The tf-idf features have high values for phrases that are frequent in a paper or citation contexts, but are less frequent in collection and have low values for phrases with high collection frequency. We computed the idf component from each collection used in experiments. Phrases that occur in cited and citing contexts as well as early in a paper are likely to be keyphrases since: (1) they capture some aspect about th</context>
</contexts>
<marker>Litvak, Last, 2008</marker>
<rawString>Marina Litvak and Mark Last. 2008. GraphBased Keyword Extraction for Single-Document Summarization. In Proceedings of the Workshop on Multi-source Multilingual Information Extraction and Summarization, MMIES ’08, pages 17–24, Manchester, United Kingdom.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Feifan Liu</author>
<author>Deana Pennell</author>
<author>Fei Liu</author>
<author>Yang Liu</author>
</authors>
<title>Unsupervised Approaches for Automatic Keyword Extraction Using Meeting Transcripts.</title>
<date>2009</date>
<booktitle>In Proceedings of NAACL ’09,</booktitle>
<pages>620--628</pages>
<location>Boulder, Colorado.</location>
<contexts>
<context position="5646" citStr="Liu et al., 2009" startWordPosition="862" endWordPosition="865"> paper is organized as follows: We summarize closely related work in Section 2. The supervised classification for keyphrase extraction is discussed in Section 3. Experiments and results are presented in Section 4, followed by conclusions and future directions of our work. 2 Related Work Many approaches to keyphrase extraction have been proposed in the literature along two lines of research: supervised and unsupervised, using different types of documents including scientific abstracts, newswire documents, meeting transcripts, and webpages (Frank et al., 1999; Hulth, 2003; Nguyen and Kan, 2007; Liu et al., 2009; Marujo et al., 2013; Mihalcea and Tarau, 2004). In the supervised line of research, keyphrase extraction is formulated as a binary classification problem, where candidate phrases are classified as either positive (i.e., keyphrases) or negative (i.e., non-keyphrases) (Frank et al., 1999; Turney, 2000; Hulth, 2003). Different feature sets and classification algorithms gave rise to different models. For example, Hulth (2003) used four different features in conjunction with a bagging technique. These features are: term frequency, collection frequency, the relative position of the first occurrenc</context>
<context position="7361" citStr="Liu et al., 2009" startWordPosition="1129" endWordPosition="1132">to these works, we propose novel features extracted from the local neighborhoods of documents available in interlinked document networks. Medelyan et al. (2009) extended KEA as well to integrate information from Wikipedia. In contrast, we used only information intrinsic to our data. Enhancing our models with Wikipedia information would be an interesting future direction to pursue. In the unsupervised line of research, keyphrase extraction is formulated as a ranking problem, where keyphrases are ranked using their tf (Barker and Cornacchia, 2000), tf-idf (Zhang et al., 2007; Lee and Kim, 2008; Liu et al., 2009; Tonella et al., 2003), and term informativeness (Wu and Giles, 2013; Rennie and Jaakkola, 2005; Kireyev, 2009) (among others). The ranking based on tf-idf has 1436 been shown to work well in practice (Liu et al., 2009; Hasan and Ng, 2010) despite its simplicity. Frantzi et al. (1998) combined linguistics and statistical information to extract technical terms from documents in digital libraries. Graph-based algorithms and centrality measures are also widely used in unsupervised models. A word graph is built for each document such that nodes correspond to words and edges correspond to word ass</context>
<context position="19664" citStr="Liu et al., 2009" startWordPosition="3175" endWordPosition="3178">oal is to study the impact of citation network information on extracting keyphrases, a paper was considered for analysis if it had at least 1http://citeseerx.ist.psu.edu/ one cited and one citing context. For each paper, we used: the title and abstract (referred to as the target paper) and its citation contexts. The reason for not considering the entire text of a paper is that scientific papers contain details, e.g., discussion of results, experimental design, notation, that do not provide additional benefits for extracting keyphrases. Hence, similar to (Hulth, 2003; Mihalcea and Tarau, 2004; Liu et al., 2009), we did not use the entire text of a paper. However, extracting keyphrases from sections such as “introduction” or “conclusion” needs further attention. From the pdf of each paper, we extracted the author-input keyphrases. An analysis of these keyphrases revealed that generally authors describe their work using, almost half of the time, bigrams, followed by unigrams and only rarely using trigrams (or higher n-grams). A summary of our datasets that contains the number of papers, the average number of cited and citing contexts per paper, the average number of keyphrases per paper, and the numbe</context>
</contexts>
<marker>Liu, Pennell, Liu, Liu, 2009</marker>
<rawString>Feifan Liu, Deana Pennell, Fei Liu, and Yang Liu. 2009. Unsupervised Approaches for Automatic Keyword Extraction Using Meeting Transcripts. In Proceedings of NAACL ’09, pages 620–628, Boulder, Colorado.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhiyuan Liu</author>
<author>Wenyi Huang</author>
<author>Yabin Zheng</author>
<author>Maosong Sun</author>
</authors>
<title>Automatic Keyphrase Extraction via Topic Decomposition.</title>
<date>2010</date>
<booktitle>In Proceedings of EMNLP ’10,</booktitle>
<pages>366--376</pages>
<location>Cambridge, Massachusetts.</location>
<contexts>
<context position="1846" citStr="Liu et al., 2010" startWordPosition="263" endWordPosition="266">es of information for many applications such as document classification, clustering, recommendation, indexing, searching, and summarization (Jones and Staveley, 1999; Zha, 2002; Hammouda et al., 2005; Pudota et al., 2010; Turney, 2003). Despite the fact that keyphrase extraction has been widely researched in the natural language processing community, its performance is still far from being satisfactory (Hasan and Ng, 2014). Many previous approaches to keyphrase extraction generally used only the textual content of a target document to extract keyphrases (Hulth, 2003; Mihalcea and Tarau, 2004; Liu et al., 2010). Recently, Wan and Xiao (2008) proposed a model that incorporates a local neighborhood of a document. However, their neighborhood is limited to textually-similar documents, where the cosine similarity between the tf-idf vectors of documents is used to compute their similarity. We posit that, in addition to a document’s textual content and textually-similar neighbors, other informative neighborhoods exist that have the potential to improve keyphrase extraction. For example, in a scholarly domain, research papers are not isolated. Rather, they are highly inter-connected in giant citation networ</context>
<context position="8131" citStr="Liu et al., 2010" startWordPosition="1254" endWordPosition="1257">df has 1436 been shown to work well in practice (Liu et al., 2009; Hasan and Ng, 2010) despite its simplicity. Frantzi et al. (1998) combined linguistics and statistical information to extract technical terms from documents in digital libraries. Graph-based algorithms and centrality measures are also widely used in unsupervised models. A word graph is built for each document such that nodes correspond to words and edges correspond to word association patterns. Nodes are then ranked using graph centrality measures such as PageRank and its variants (Mihalcea and Tarau, 2004; Wan and Xiao, 2008; Liu et al., 2010; Zhao et al., 2011), HITS scores (Litvak and Last, 2008), as well as node degree and betweenness (Boudin, 2013; Xie, 2005). Wan and Xiao (2008) were the first to consider modeling a local neighborhood of a target document in addition to the document itself, and applied this approach to news articles on the Web. Their local neighborhood consists of textually similar documents, and did not capture information contained in document networks. Using terms from citation contexts of scientific papers is not a new idea. It was used before in various applications. For example, Ritchie et al. (2006) us</context>
<context position="17402" citStr="Liu et al., 2010" startWordPosition="2809" endWordPosition="2812"> tag of the phrase; used in Hulth’s methods Novel features - Citation Network Based tf-idf inCited if the phrase occurs in cited contexts inCiting if the phrase occurs in citing contexts citation tf-idf the tf-idf value of the phrase, computed from the aggregated citation contexts 1438 Dataset Num. (#) Average Average Average #uni- #bi- #triPapers Cited Ctx. Citing Ctx. Keyphrases grams grams grams WWW 425 15.45 18.78 4.87 680 1036 247 KDD 365 12.69 19.74 4.03 363 853 189 Table 2: A summary of our datasets. speech filters. Consistent with previous works (Hulth, 2003; Mihalcea and Tarau, 2004; Liu et al., 2010; Wan and Xiao, 2008), only nouns and adjectives are retained to form candidate phrases. The generation process consists of two steps. First, using the NLP Stanford part of speech tagger, we preprocess each document and keep only the nouns and adjectives corresponding to {NN, NNS, NNP, NNPS, JJ}. We apply the Porter stemmer on every word. The position of each word is kept consistent with the initial state of the document before any word removal is made. Second, words extracted in the first step that have contiguous positions in a document are concatenated into n-grams. We used unigrams, bigram</context>
</contexts>
<marker>Liu, Huang, Zheng, Sun, 2010</marker>
<rawString>Zhiyuan Liu, Wenyi Huang, Yabin Zheng, and Maosong Sun. 2010. Automatic Keyphrase Extraction via Topic Decomposition. In Proceedings of EMNLP ’10, pages 366–376, Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Prabhakar Raghavan</author>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Introduction to Information Retrieval.</title>
<date>2008</date>
<publisher>Cambridge University Press,</publisher>
<location>New York, NY, USA.</location>
<marker>Manning, Raghavan, Sch¨utze, 2008</marker>
<rawString>Christopher D. Manning, Prabhakar Raghavan, and Hinrich Sch¨utze. 2008. Introduction to Information Retrieval. Cambridge University Press, New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lu´ıs Marujo</author>
<author>Ricardo Ribeiro</author>
<author>David Martins de Matos</author>
<author>Jo˜ao Paulo Neto</author>
<author>Anatole Gershman</author>
<author>Jaime G Carbonell</author>
</authors>
<title>Key phrase extraction of lightly filtered broadcast news.</title>
<date>2013</date>
<publisher>CoRR.</publisher>
<marker>Marujo, Ribeiro, de Matos, Neto, Gershman, Carbonell, 2013</marker>
<rawString>Lu´ıs Marujo, Ricardo Ribeiro, David Martins de Matos, Jo˜ao Paulo Neto, Anatole Gershman, and Jaime G. Carbonell. 2013. Key phrase extraction of lightly filtered broadcast news. CoRR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olena Medelyan</author>
<author>Eibe Frank</author>
<author>Ian H Witten</author>
</authors>
<title>Human-competitive tagging using automatic keyphrase extraction.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 3 - Volume 3, EMNLP ’09,</booktitle>
<pages>1318--1327</pages>
<contexts>
<context position="6905" citStr="Medelyan et al. (2009)" startWordPosition="1057" endWordPosition="1060">m. Frank et al. (1999) developed a system called KEA that used only two features: tf-idf (term frequency-inverse document frequency) of a phrase and the distance of a phrase from the beginning of a document (i.e., its relative position) and used them as input to Naive Bayes. Nguyen and Kan (2007) extended KEA to include features such as the distribution of keyphrases among different sections of a research paper, and the acronym status of a term. In contrast to these works, we propose novel features extracted from the local neighborhoods of documents available in interlinked document networks. Medelyan et al. (2009) extended KEA as well to integrate information from Wikipedia. In contrast, we used only information intrinsic to our data. Enhancing our models with Wikipedia information would be an interesting future direction to pursue. In the unsupervised line of research, keyphrase extraction is formulated as a ranking problem, where keyphrases are ranked using their tf (Barker and Cornacchia, 2000), tf-idf (Zhang et al., 2007; Lee and Kim, 2008; Liu et al., 2009; Tonella et al., 2003), and term informativeness (Wu and Giles, 2013; Rennie and Jaakkola, 2005; Kireyev, 2009) (among others). The ranking bas</context>
<context position="39195" citStr="Medelyan et al., 2009" startWordPosition="6388" endWordPosition="6391">fthe-art supervised and unsupervised models. Although we illustrated the benefits of leveraging inter-linked document networks for keyphrase extraction from scientific documents, the proposed model can be extended to other types of documents such as webpages, emails, and weblogs. For example, the anchor text on hyperlinks in weblogs can be seen as the “citation context”. Another aspect of future work would be the use of external sources to better identify candidate phrases. For example, the use of Wikipedia was studied before to check if the concept behind a phrase has its own Wikipedia page (Medelyan et al., 2009). Furthermore, since citations occur in all sciences, extensions of the proposed model to other domains, e.g., Biology and Chemistry, and other applications, e.g., document summarization, similar to Mihalcea and Tarau (2004) and Qazvinian et al. (2010), are of particular interest. Acknowledgments We are grateful to Dr. C. Lee Giles for the CiteSeerX data, which allowed the generation of citation graphs. We also thank Kishore Neppalli and Juan Fern´andez-Ramizer for their help with various dataset construction tasks. We very much appreciate the constructive feedback from our anonymous reviewers</context>
</contexts>
<marker>Medelyan, Frank, Witten, 2009</marker>
<rawString>Olena Medelyan, Eibe Frank, and Ian H. Witten. 2009. Human-competitive tagging using automatic keyphrase extraction. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 3 - Volume 3, EMNLP ’09, pages 1318–1327, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qiaozhu Mei</author>
<author>ChengXiang Zhai</author>
</authors>
<title>Generating impact-based summaries for scientific literature.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>816--824</pages>
<location>Columbus, Ohio.</location>
<contexts>
<context position="9862" citStr="Mei and Zhai, 2008" startWordPosition="1545" endWordPosition="1548">rch engines build on the intuition that the anchor text pointing to a page is a good descriptor of its content, and thus use anchor text terms as additional index terms for a target webpage. The use of links and anchor text was thoroughly researched for IR tasks (Koolen and Kamps, 2010), broadening a user’s search (Chakrabarti et al., 1998), query refinement (Kraft and Zien, 2004), and enriching document representations (Metzler et al., 2009). Moreover, citation contexts were used for scientific paper summarization (Abu-Jbara and Radev, 2011; Qazvinian et al., 2010; Qazvinian and Radev, 2008; Mei and Zhai, 2008; Lehnert et al., 1990; Nakov et al., 2004). Among these, probably the most similar to our work is the work by Qazvinian et al. (2010), where a set of important keyphrases is extracted first from the citation contexts in which the paper to be summarized is cited by other papers and then the “best” subset of sentences that contain such keyphrases is returned as the summary. However, keyphrases in (Qazvinian et al., 2010) are extracted using frequent n-grams in a language model framework, whereas in our work, we propose a supervised approach to a different task: keyphrase extraction. Mei and Zha</context>
</contexts>
<marker>Mei, Zhai, 2008</marker>
<rawString>Qiaozhu Mei and ChengXiang Zhai. 2008. Generating impact-based summaries for scientific literature. In Proceedings of ACL-08: HLT, pages 816–824, Columbus, Ohio.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donald Metzler</author>
<author>Jasmine Novak</author>
<author>Hang Cui</author>
<author>Srihari Reddy</author>
</authors>
<title>Building enriched document representations using aggregated anchor text.</title>
<date>2009</date>
<booktitle>In Proc. of SIGIR ’09,</booktitle>
<pages>219--226</pages>
<location>Boston, MA, USA.</location>
<contexts>
<context position="9690" citStr="Metzler et al., 2009" startWordPosition="1519" endWordPosition="1522">om citation contexts resembles the analysis of hyperlinks and the graph structure of the Web, which are instrumental in Web search (Manning et al., 2008). Many current Web search engines build on the intuition that the anchor text pointing to a page is a good descriptor of its content, and thus use anchor text terms as additional index terms for a target webpage. The use of links and anchor text was thoroughly researched for IR tasks (Koolen and Kamps, 2010), broadening a user’s search (Chakrabarti et al., 1998), query refinement (Kraft and Zien, 2004), and enriching document representations (Metzler et al., 2009). Moreover, citation contexts were used for scientific paper summarization (Abu-Jbara and Radev, 2011; Qazvinian et al., 2010; Qazvinian and Radev, 2008; Mei and Zhai, 2008; Lehnert et al., 1990; Nakov et al., 2004). Among these, probably the most similar to our work is the work by Qazvinian et al. (2010), where a set of important keyphrases is extracted first from the citation contexts in which the paper to be summarized is cited by other papers and then the “best” subset of sentences that contain such keyphrases is returned as the summary. However, keyphrases in (Qazvinian et al., 2010) are </context>
</contexts>
<marker>Metzler, Novak, Cui, Reddy, 2009</marker>
<rawString>Donald Metzler, Jasmine Novak, Hang Cui, and Srihari Reddy. 2009. Building enriched document representations using aggregated anchor text. In Proc. of SIGIR ’09, pages 219–226, Boston, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Paul Tarau</author>
</authors>
<title>TextRank: Bringing Order into Texts.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP 2004,</booktitle>
<pages>404--411</pages>
<location>Barcelona,</location>
<contexts>
<context position="1827" citStr="Mihalcea and Tarau, 2004" startWordPosition="259" endWordPosition="262">are shown to be rich sources of information for many applications such as document classification, clustering, recommendation, indexing, searching, and summarization (Jones and Staveley, 1999; Zha, 2002; Hammouda et al., 2005; Pudota et al., 2010; Turney, 2003). Despite the fact that keyphrase extraction has been widely researched in the natural language processing community, its performance is still far from being satisfactory (Hasan and Ng, 2014). Many previous approaches to keyphrase extraction generally used only the textual content of a target document to extract keyphrases (Hulth, 2003; Mihalcea and Tarau, 2004; Liu et al., 2010). Recently, Wan and Xiao (2008) proposed a model that incorporates a local neighborhood of a document. However, their neighborhood is limited to textually-similar documents, where the cosine similarity between the tf-idf vectors of documents is used to compute their similarity. We posit that, in addition to a document’s textual content and textually-similar neighbors, other informative neighborhoods exist that have the potential to improve keyphrase extraction. For example, in a scholarly domain, research papers are not isolated. Rather, they are highly inter-connected in gi</context>
<context position="5694" citStr="Mihalcea and Tarau, 2004" startWordPosition="870" endWordPosition="873">arize closely related work in Section 2. The supervised classification for keyphrase extraction is discussed in Section 3. Experiments and results are presented in Section 4, followed by conclusions and future directions of our work. 2 Related Work Many approaches to keyphrase extraction have been proposed in the literature along two lines of research: supervised and unsupervised, using different types of documents including scientific abstracts, newswire documents, meeting transcripts, and webpages (Frank et al., 1999; Hulth, 2003; Nguyen and Kan, 2007; Liu et al., 2009; Marujo et al., 2013; Mihalcea and Tarau, 2004). In the supervised line of research, keyphrase extraction is formulated as a binary classification problem, where candidate phrases are classified as either positive (i.e., keyphrases) or negative (i.e., non-keyphrases) (Frank et al., 1999; Turney, 2000; Hulth, 2003). Different feature sets and classification algorithms gave rise to different models. For example, Hulth (2003) used four different features in conjunction with a bagging technique. These features are: term frequency, collection frequency, the relative position of the first occurrence and the part-of-speech tag of a term. Frank et</context>
<context position="8093" citStr="Mihalcea and Tarau, 2004" startWordPosition="1246" endWordPosition="1249">009) (among others). The ranking based on tf-idf has 1436 been shown to work well in practice (Liu et al., 2009; Hasan and Ng, 2010) despite its simplicity. Frantzi et al. (1998) combined linguistics and statistical information to extract technical terms from documents in digital libraries. Graph-based algorithms and centrality measures are also widely used in unsupervised models. A word graph is built for each document such that nodes correspond to words and edges correspond to word association patterns. Nodes are then ranked using graph centrality measures such as PageRank and its variants (Mihalcea and Tarau, 2004; Wan and Xiao, 2008; Liu et al., 2010; Zhao et al., 2011), HITS scores (Litvak and Last, 2008), as well as node degree and betweenness (Boudin, 2013; Xie, 2005). Wan and Xiao (2008) were the first to consider modeling a local neighborhood of a target document in addition to the document itself, and applied this approach to news articles on the Web. Their local neighborhood consists of textually similar documents, and did not capture information contained in document networks. Using terms from citation contexts of scientific papers is not a new idea. It was used before in various applications.</context>
<context position="17384" citStr="Mihalcea and Tarau, 2004" startWordPosition="2805" endWordPosition="2808">ods POS the part-of-speech tag of the phrase; used in Hulth’s methods Novel features - Citation Network Based tf-idf inCited if the phrase occurs in cited contexts inCiting if the phrase occurs in citing contexts citation tf-idf the tf-idf value of the phrase, computed from the aggregated citation contexts 1438 Dataset Num. (#) Average Average Average #uni- #bi- #triPapers Cited Ctx. Citing Ctx. Keyphrases grams grams grams WWW 425 15.45 18.78 4.87 680 1036 247 KDD 365 12.69 19.74 4.03 363 853 189 Table 2: A summary of our datasets. speech filters. Consistent with previous works (Hulth, 2003; Mihalcea and Tarau, 2004; Liu et al., 2010; Wan and Xiao, 2008), only nouns and adjectives are retained to form candidate phrases. The generation process consists of two steps. First, using the NLP Stanford part of speech tagger, we preprocess each document and keep only the nouns and adjectives corresponding to {NN, NNS, NNP, NNPS, JJ}. We apply the Porter stemmer on every word. The position of each word is kept consistent with the initial state of the document before any word removal is made. Second, words extracted in the first step that have contiguous positions in a document are concatenated into n-grams. We use</context>
<context position="19645" citStr="Mihalcea and Tarau, 2004" startWordPosition="3171" endWordPosition="3174">ting contexts. Since our goal is to study the impact of citation network information on extracting keyphrases, a paper was considered for analysis if it had at least 1http://citeseerx.ist.psu.edu/ one cited and one citing context. For each paper, we used: the title and abstract (referred to as the target paper) and its citation contexts. The reason for not considering the entire text of a paper is that scientific papers contain details, e.g., discussion of results, experimental design, notation, that do not provide additional benefits for extracting keyphrases. Hence, similar to (Hulth, 2003; Mihalcea and Tarau, 2004; Liu et al., 2009), we did not use the entire text of a paper. However, extracting keyphrases from sections such as “introduction” or “conclusion” needs further attention. From the pdf of each paper, we extracted the author-input keyphrases. An analysis of these keyphrases revealed that generally authors describe their work using, almost half of the time, bigrams, followed by unigrams and only rarely using trigrams (or higher n-grams). A summary of our datasets that contains the number of papers, the average number of cited and citing contexts per paper, the average number of keyphrases per p</context>
<context position="22508" citStr="Mihalcea and Tarau, 2004" startWordPosition="3628" endWordPosition="3632">ls that use only information intrinsic to the data and what are the most informative features for classification? We compared CeKE’s performance with that of classifiers trained on KEA features only and Hulth’s features only and present a ranking of features based on information gain. 2. How do supervised models that integrate citation network information compare with recent unsupervised models? Since recent unsupervised approaches are becoming competitive with supervised approaches (Hasan and Ng, 2014), we also compared CeKE with unsupervised ranking of candidate phrases by TF-IDF, TextRank (Mihalcea and Tarau, 2004) and ExpandRank (Wan and Xiao, 2008). For unsupervised, we considered top 5 and top 10 ranked phrases when computing “@5” and “@10” measures. 3. How well does our proposed model perform in the absence of either cited or citing contexts? Since newly published scientific papers are not cited by many other papers, e.g., due to their recency, no cited contexts are available. We studied the quality of predicted keyphrases when either cited or citing contexts are missing. For this, we compared the performance of models trained using both cited and citing contexts with that of models that use either </context>
<context position="27632" citStr="Mihalcea and Tarau, 2004" startWordPosition="4487" endWordPosition="4490">able, tf-idf and citation tf-idf are both highly ranked, first and third, respectively, illustrating that they contain significant information in predicting keyphrases. The first position of a phrase is also of great impact. This is consistent with the fact that almost half of the identified keywords and about 20% of the annotated keyphrases appear in title. Similar ranking is obtained on KDD. The comparison of CeKE with unsupervised state-of-the-art models. Table 5 shows the results of the comparison of CeKE with three unsupervised ranking approaches: TF-IDF (Tonella et al., 2003), TextRank (Mihalcea and Tarau, 2004), and ExpandRank (Wan and Xiao, 2008). TF-IDF and TextRank use information only from the target paper, whereas ExpandRank uses a small textual neighborhood in addition to the target paper. Note that, for all unsupervised methods, we used Porter stemmer and the same candidate phrase generation as in CeKE, as explained in §3.2. For TF-IDF, we first tokenized the target paper and computed the score for each word, and then formed phrases and summed up the score of every word within a phrase. For TextRank, we built an undirected graph for each paper, where the nodes correspond to words in the targe</context>
<context position="39419" citStr="Mihalcea and Tarau (2004)" startWordPosition="6420" endWordPosition="6423">r types of documents such as webpages, emails, and weblogs. For example, the anchor text on hyperlinks in weblogs can be seen as the “citation context”. Another aspect of future work would be the use of external sources to better identify candidate phrases. For example, the use of Wikipedia was studied before to check if the concept behind a phrase has its own Wikipedia page (Medelyan et al., 2009). Furthermore, since citations occur in all sciences, extensions of the proposed model to other domains, e.g., Biology and Chemistry, and other applications, e.g., document summarization, similar to Mihalcea and Tarau (2004) and Qazvinian et al. (2010), are of particular interest. Acknowledgments We are grateful to Dr. C. Lee Giles for the CiteSeerX data, which allowed the generation of citation graphs. We also thank Kishore Neppalli and Juan Fern´andez-Ramizer for their help with various dataset construction tasks. We very much appreciate the constructive feedback from our anonymous reviewers. This research was supported in part by NSF awards #1353418 and #1423337 to Cornelia Caragea. Any opinions, findings, and conclusions expressed here are those of the authors and do not necessarily reflect the views of NSF. </context>
</contexts>
<marker>Mihalcea, Tarau, 2004</marker>
<rawString>Rada Mihalcea and Paul Tarau. 2004. TextRank: Bringing Order into Texts. In Proceedings of EMNLP 2004, pages 404–411, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Preslav I Nakov</author>
<author>Ariel S Schwartz</author>
<author>Marti A Hearst</author>
</authors>
<title>Citances: Citation sentences for semantic analysis of bioscience text.</title>
<date>2004</date>
<booktitle>In SIGIR Workshop on Search and Discovery in Bioinformatics.</booktitle>
<contexts>
<context position="9905" citStr="Nakov et al., 2004" startWordPosition="1553" endWordPosition="1556">e anchor text pointing to a page is a good descriptor of its content, and thus use anchor text terms as additional index terms for a target webpage. The use of links and anchor text was thoroughly researched for IR tasks (Koolen and Kamps, 2010), broadening a user’s search (Chakrabarti et al., 1998), query refinement (Kraft and Zien, 2004), and enriching document representations (Metzler et al., 2009). Moreover, citation contexts were used for scientific paper summarization (Abu-Jbara and Radev, 2011; Qazvinian et al., 2010; Qazvinian and Radev, 2008; Mei and Zhai, 2008; Lehnert et al., 1990; Nakov et al., 2004). Among these, probably the most similar to our work is the work by Qazvinian et al. (2010), where a set of important keyphrases is extracted first from the citation contexts in which the paper to be summarized is cited by other papers and then the “best” subset of sentences that contain such keyphrases is returned as the summary. However, keyphrases in (Qazvinian et al., 2010) are extracted using frequent n-grams in a language model framework, whereas in our work, we propose a supervised approach to a different task: keyphrase extraction. Mei and Zhai (2008) used information from citation con</context>
</contexts>
<marker>Nakov, Schwartz, Hearst, 2004</marker>
<rawString>Preslav I. Nakov, Ariel S. Schwartz, and Marti A. Hearst. 2004. Citances: Citation sentences for semantic analysis of bioscience text. In SIGIR Workshop on Search and Discovery in Bioinformatics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thuy Dung Nguyen</author>
<author>Min-Yen Kan</author>
</authors>
<title>Keyphrase Extraction in Scientific Publications.</title>
<date>2007</date>
<booktitle>In Proc. of the Intl. Conf. on Asian digital libraries, ICADL’07,</booktitle>
<pages>317--326</pages>
<location>Hanoi, Vietnam.</location>
<contexts>
<context position="5628" citStr="Nguyen and Kan, 2007" startWordPosition="858" endWordPosition="861"> Data. The rest of the paper is organized as follows: We summarize closely related work in Section 2. The supervised classification for keyphrase extraction is discussed in Section 3. Experiments and results are presented in Section 4, followed by conclusions and future directions of our work. 2 Related Work Many approaches to keyphrase extraction have been proposed in the literature along two lines of research: supervised and unsupervised, using different types of documents including scientific abstracts, newswire documents, meeting transcripts, and webpages (Frank et al., 1999; Hulth, 2003; Nguyen and Kan, 2007; Liu et al., 2009; Marujo et al., 2013; Mihalcea and Tarau, 2004). In the supervised line of research, keyphrase extraction is formulated as a binary classification problem, where candidate phrases are classified as either positive (i.e., keyphrases) or negative (i.e., non-keyphrases) (Frank et al., 1999; Turney, 2000; Hulth, 2003). Different feature sets and classification algorithms gave rise to different models. For example, Hulth (2003) used four different features in conjunction with a bagging technique. These features are: term frequency, collection frequency, the relative position of t</context>
</contexts>
<marker>Nguyen, Kan, 2007</marker>
<rawString>Thuy Dung Nguyen and Min-Yen Kan. 2007. Keyphrase Extraction in Scientific Publications. In Proc. of the Intl. Conf. on Asian digital libraries, ICADL’07, pages 317–326, Hanoi, Vietnam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rong Pan</author>
<author>Martin Scholz</author>
</authors>
<title>Mind the gaps: Weighting the unknown in large-scale one-class collaborative filtering.</title>
<date>2009</date>
<booktitle>In Proceedings of KDD ’09,</booktitle>
<pages>667--676</pages>
<location>Paris, France.</location>
<contexts>
<context position="3054" citStr="Pan and Scholz, 2009" startWordPosition="458" endWordPosition="461">tation networks, in which papers cite or are cited by other papers. In a citation network, information flows from one paper to another via the citation relation (Shi et al., 2010). This information flow and the influence of one paper on another are specifically captured by means of citation contexts, i.e., short text segments surrounding a citation’s mention. These contexts are not arbitrary, but they serve as brief summaries of a cited paper. Figure 1 illustrates this idea using a small citation network of a paper by Rendle et al. (2010) that cites (Zimdars et al., 2001), (Hu et al., 2008), (Pan and Scholz, 2009) and (Shani et al., 2005) and is cited by (Cheng et al., 2013). The citation mentions and citation contexts are shown with a dashed line. Note the high overlap between the words in contexts and those in the title and abstract (shown in bold) and the author-annotated keywords. One question that can be raised is the following: Can we effectively exploit information available in large inter-linked document networks in order to improve the performance of keyphrase extraction? The research that we describe in this paper addresses specifically this question using citation networks of research papers</context>
</contexts>
<marker>Pan, Scholz, 2009</marker>
<rawString>Rong Pan and Martin Scholz. 2009. Mind the gaps: Weighting the unknown in large-scale one-class collaborative filtering. In Proceedings of KDD ’09, pages 667–676, Paris, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hoifung Poon</author>
<author>Pedro Domingos</author>
</authors>
<title>Unsupervised semantic parsing.</title>
<date>2009</date>
<booktitle>In Proc. of the 2009 Conference on Empirical Methods in Natural Language Processing, EMNLP ’09,</booktitle>
<pages>1--10</pages>
<contexts>
<context position="31888" citStr="Poon and Domingos, 2009" startWordPosition="5217" endWordPosition="5220">.4 Anecdotal Evidence In order to check the transferability of our proposed approach to other research fields, e.g., natural language processing, it would be interesting to use our trained classifiers on WWW and KDD collections and evaluate them on new collections such as NLP related collections. Since NLP collections annotated with keyphrases are not available, we show anecdotal evidence for only one paper. We selected for this task an award winning paper published in the EMNLP conference. The paper’s title is ”Unsupervised semantic parsing” and has won the Best Paper Award in the year 2009 (Poon and Domingos, 2009). In order for our algorithm to work, we gathered from the Web (using Google Scholar) all the cited and citing contexts that were available (49 cited contexts and 30 citing contexts). We manually annotated the target paper with keyphrases. The title, abstract and all the contexts were POS tagged using the NLP Stanford tool. We then trained a classifier on the features shown in Table 1, on both WWW and KDD datasets combined. The trained classifier was used to make predictions, which were compared against the manually annotated keyphrases. The results are shown in Figure 2, which displays the ti</context>
<context position="34140" citStr="Poon and Domingos (2009)" startWordPosition="5580" endWordPosition="5583">mbda forms from these, and clusters them to abstract away syntactic variations of the same meaning. The MAP semantic parse1.000 of a sentence is obtained by recursively assigning its parts to lambda-form clusters and composing them. We evaluate our approach by using it to extract a knowledge base from biomedical abstracts and answer questions. USP1.000 substantially outperforms TextRunner, DIRT and an informed baseline on both precision and recall on this task. Human annotated labels: unsupervised semantic parsing, Markov logic, USP system Figure 2: The title and abstract of an EMNLP paper by Poon and Domingos (2009) and human annotated keyphrases for the paper. Black words represent candidate phrases. Red bold words represent predicted keyphrases. The numbers above predicted keyphrases are probabilities for the positive class assignment. on the probability output by Naive Bayes. For example, if we decrease the threshold to 0.920 the following phrases would be added to the returned set of keyphrases: dependency trees, quasi-logical forms and unsupervised approach. Another interesting aspect is the frequency of occurrence of the predicted keyphrases in the cited and citing contexts. Table 7 shows the termf</context>
</contexts>
<marker>Poon, Domingos, 2009</marker>
<rawString>Hoifung Poon and Pedro Domingos. 2009. Unsupervised semantic parsing. In Proc. of the 2009 Conference on Empirical Methods in Natural Language Processing, EMNLP ’09, pages 1–10, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nirmala Pudota</author>
<author>Antonina Dattolo</author>
<author>Andrea Baruzzo</author>
<author>Felice Ferrara</author>
<author>Carlo Tasso</author>
</authors>
<title>Automatic keyphrase extraction and ontology mining for content-based tag recommendation.</title>
<date>2010</date>
<journal>International Journal of Intelligent Systems.</journal>
<contexts>
<context position="1449" citStr="Pudota et al., 2010" startWordPosition="200" endWordPosition="203">formation and use them in conjunction with traditional features for keyphrase extraction to obtain remarkable improvements in performance over strong baselines. 1 Introduction Keyphrase extraction is the problem of automatically extracting important phrases or concepts (i.e., the essence) of a document. Keyphrases provide a high-level topic description of a document and are shown to be rich sources of information for many applications such as document classification, clustering, recommendation, indexing, searching, and summarization (Jones and Staveley, 1999; Zha, 2002; Hammouda et al., 2005; Pudota et al., 2010; Turney, 2003). Despite the fact that keyphrase extraction has been widely researched in the natural language processing community, its performance is still far from being satisfactory (Hasan and Ng, 2014). Many previous approaches to keyphrase extraction generally used only the textual content of a target document to extract keyphrases (Hulth, 2003; Mihalcea and Tarau, 2004; Liu et al., 2010). Recently, Wan and Xiao (2008) proposed a model that incorporates a local neighborhood of a document. However, their neighborhood is limited to textually-similar documents, where the cosine similarity b</context>
</contexts>
<marker>Pudota, Dattolo, Baruzzo, Ferrara, Tasso, 2010</marker>
<rawString>Nirmala Pudota, Antonina Dattolo, Andrea Baruzzo, Felice Ferrara, and Carlo Tasso. 2010. Automatic keyphrase extraction and ontology mining for content-based tag recommendation. International Journal of Intelligent Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vahed Qazvinian</author>
<author>Dragomir R Radev</author>
</authors>
<title>Scientific paper summarization using citation summary networks.</title>
<date>2008</date>
<booktitle>In Proc. of the 22nd Intl. Conference on Computational Linguistics, COLING ’08,</booktitle>
<pages>689--696</pages>
<location>Manchester, United Kingdom.</location>
<contexts>
<context position="9842" citStr="Qazvinian and Radev, 2008" startWordPosition="1541" endWordPosition="1544">2008). Many current Web search engines build on the intuition that the anchor text pointing to a page is a good descriptor of its content, and thus use anchor text terms as additional index terms for a target webpage. The use of links and anchor text was thoroughly researched for IR tasks (Koolen and Kamps, 2010), broadening a user’s search (Chakrabarti et al., 1998), query refinement (Kraft and Zien, 2004), and enriching document representations (Metzler et al., 2009). Moreover, citation contexts were used for scientific paper summarization (Abu-Jbara and Radev, 2011; Qazvinian et al., 2010; Qazvinian and Radev, 2008; Mei and Zhai, 2008; Lehnert et al., 1990; Nakov et al., 2004). Among these, probably the most similar to our work is the work by Qazvinian et al. (2010), where a set of important keyphrases is extracted first from the citation contexts in which the paper to be summarized is cited by other papers and then the “best” subset of sentences that contain such keyphrases is returned as the summary. However, keyphrases in (Qazvinian et al., 2010) are extracted using frequent n-grams in a language model framework, whereas in our work, we propose a supervised approach to a different task: keyphrase ext</context>
</contexts>
<marker>Qazvinian, Radev, 2008</marker>
<rawString>Vahed Qazvinian and Dragomir R. Radev. 2008. Scientific paper summarization using citation summary networks. In Proc. of the 22nd Intl. Conference on Computational Linguistics, COLING ’08, pages 689–696, Manchester, United Kingdom.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vahed Qazvinian</author>
<author>Dragomir R Radev</author>
<author>Arzucan ¨Ozg¨ur</author>
</authors>
<title>Citation summarization through keyphrase extraction.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics, COLING ’10,</booktitle>
<pages>895--903</pages>
<marker>Qazvinian, Radev, ¨Ozg¨ur, 2010</marker>
<rawString>Vahed Qazvinian, Dragomir R. Radev, and Arzucan ¨Ozg¨ur. 2010. Citation summarization through keyphrase extraction. In Proceedings of the 23rd International Conference on Computational Linguistics, COLING ’10, pages 895–903.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steffen Rendle</author>
<author>Christoph Freudenthaler</author>
<author>Lars Schmidt-Thieme</author>
</authors>
<title>Factorizing personalized markov chains for next-basket recommendation.</title>
<date>2010</date>
<booktitle>In WWW ’10,</booktitle>
<pages>811--820</pages>
<location>Raleigh, North Carolina.</location>
<contexts>
<context position="2977" citStr="Rendle et al. (2010)" startWordPosition="444" endWordPosition="447">papers are not isolated. Rather, they are highly inter-connected in giant citation networks, in which papers cite or are cited by other papers. In a citation network, information flows from one paper to another via the citation relation (Shi et al., 2010). This information flow and the influence of one paper on another are specifically captured by means of citation contexts, i.e., short text segments surrounding a citation’s mention. These contexts are not arbitrary, but they serve as brief summaries of a cited paper. Figure 1 illustrates this idea using a small citation network of a paper by Rendle et al. (2010) that cites (Zimdars et al., 2001), (Hu et al., 2008), (Pan and Scholz, 2009) and (Shani et al., 2005) and is cited by (Cheng et al., 2013). The citation mentions and citation contexts are shown with a dashed line. Note the high overlap between the words in contexts and those in the title and abstract (shown in bold) and the author-annotated keywords. One question that can be raised is the following: Can we effectively exploit information available in large inter-linked document networks in order to improve the performance of keyphrase extraction? The research that we describe in this paper ad</context>
<context position="4344" citStr="Rendle et al. (2010)" startWordPosition="662" endWordPosition="665">” research papers is crucial to dealing with the large numbers of research papers published during these “big data” times. The importance of keyphrase extraction from research papers is also emphasized by the recent SemEval 2010 Shared Task on this topic (Kim et al., 2010; Kim et al., 2013). Our contributions. We present a supervised 1435 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1435–1446, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics Figure 1: A small citation network corresponding to a paper by Rendle et al. (2010). approach to keyphrase extraction from research papers that, in addition to the information contained in a paper itself, effectively incorporates, in the learned models, information from the paper’s local neighborhood available in citation networks. To this end, we design novel features for keyphrase extraction based on citation context information and use them in conjunction with traditional features in a supervised probabilistic framework. We show empirically that the proposed models substantially outperform strong baselines on two datasets of research papers compiled from two machine learn</context>
</contexts>
<marker>Rendle, Freudenthaler, Schmidt-Thieme, 2010</marker>
<rawString>Steffen Rendle, Christoph Freudenthaler, and Lars Schmidt-Thieme. 2010. Factorizing personalized markov chains for next-basket recommendation. In WWW ’10, pages 811–820, Raleigh, North Carolina.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason D M Rennie</author>
<author>Tommi Jaakkola</author>
</authors>
<title>Using Term Informativeness for Named Entity Detection.</title>
<date>2005</date>
<booktitle>In Proc. of SIGIR ’05,</booktitle>
<pages>353--360</pages>
<contexts>
<context position="7457" citStr="Rennie and Jaakkola, 2005" startWordPosition="1144" endWordPosition="1147">uments available in interlinked document networks. Medelyan et al. (2009) extended KEA as well to integrate information from Wikipedia. In contrast, we used only information intrinsic to our data. Enhancing our models with Wikipedia information would be an interesting future direction to pursue. In the unsupervised line of research, keyphrase extraction is formulated as a ranking problem, where keyphrases are ranked using their tf (Barker and Cornacchia, 2000), tf-idf (Zhang et al., 2007; Lee and Kim, 2008; Liu et al., 2009; Tonella et al., 2003), and term informativeness (Wu and Giles, 2013; Rennie and Jaakkola, 2005; Kireyev, 2009) (among others). The ranking based on tf-idf has 1436 been shown to work well in practice (Liu et al., 2009; Hasan and Ng, 2010) despite its simplicity. Frantzi et al. (1998) combined linguistics and statistical information to extract technical terms from documents in digital libraries. Graph-based algorithms and centrality measures are also widely used in unsupervised models. A word graph is built for each document such that nodes correspond to words and edges correspond to word association patterns. Nodes are then ranked using graph centrality measures such as PageRank and it</context>
</contexts>
<marker>Rennie, Jaakkola, 2005</marker>
<rawString>Jason D. M. Rennie and Tommi Jaakkola. 2005. Using Term Informativeness for Named Entity Detection. In Proc. of SIGIR ’05, pages 353–360.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anna Ritchie</author>
<author>Simone Teufel</author>
<author>Stephen Robertson</author>
</authors>
<title>How to find better index terms through citations.</title>
<date>2006</date>
<booktitle>In Proc. of the Workshop on How Can Computational Linguistics Improve Information Retrieval?, CLIIR ’06,</booktitle>
<pages>25--32</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="8728" citStr="Ritchie et al. (2006)" startWordPosition="1354" endWordPosition="1357">, 2008; Liu et al., 2010; Zhao et al., 2011), HITS scores (Litvak and Last, 2008), as well as node degree and betweenness (Boudin, 2013; Xie, 2005). Wan and Xiao (2008) were the first to consider modeling a local neighborhood of a target document in addition to the document itself, and applied this approach to news articles on the Web. Their local neighborhood consists of textually similar documents, and did not capture information contained in document networks. Using terms from citation contexts of scientific papers is not a new idea. It was used before in various applications. For example, Ritchie et al. (2006) used a combination of terms from citation contexts and existing index terms of a paper to improve indexing of cited papers. Citation contexts were also used to improve the performance of citation recommendation systems (Kataria et al., 2010; He et al., 2010) and to study author influence (Kataria et al., 2011). This idea of using terms from citation contexts resembles the analysis of hyperlinks and the graph structure of the Web, which are instrumental in Web search (Manning et al., 2008). Many current Web search engines build on the intuition that the anchor text pointing to a page is a good</context>
</contexts>
<marker>Ritchie, Teufel, Robertson, 2006</marker>
<rawString>Anna Ritchie, Simone Teufel, and Stephen Robertson. 2006. How to find better index terms through citations. In Proc. of the Workshop on How Can Computational Linguistics Improve Information Retrieval?, CLIIR ’06, pages 25–32, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anna Ritchie</author>
<author>Stephen Robertson</author>
<author>Simone Teufel</author>
</authors>
<title>Comparing citation contexts for information retrieval.</title>
<date>2008</date>
<booktitle>In Proc. of CIKM ’08,</booktitle>
<pages>213--222</pages>
<location>Napa Valley, California, USA.</location>
<contexts>
<context position="20840" citStr="Ritchie et al. (2008)" startWordPosition="3366" endWordPosition="3369">number of keyphrases per paper, and the number of unigrams, bigrams and trigrams, in each collection, is shown in Table 2. Consistent with previous works (Frank et al., 1999; Hulth, 2003), the positive and negative examples in our datasets correspond to candidate phrases that consist of up to three tokens. The positive examples are candidate phrases that have a match in the author-input keyphrases, whereas negative examples correspond to the remaining candidate phrases. Context lengths. In CiteSeer&amp;quot;, citation contexts have about 50 words on each side of a citation mention. A previous study by Ritchie et al. (2008) shows that a fixed window length of about 100 words around a citation mention is generally effective for information retrieval tasks. For this reason, we used the contexts provided by CiteSeer&amp;quot; directly. However, in future, it would be interesting to incorporate in our models more sophisticated approaches to identifying the text that is relevant to a target citation (Abu-Jbara and Radev, 2012; Teufel, 1999) and study the influence of context lengths on the quality of extracted keyphrase. 1439 WWW KDD Method Precision Recall F1-score Precision Recall F1-score Citation - Enhanced (CeKE) 0.227 0</context>
</contexts>
<marker>Ritchie, Robertson, Teufel, 2008</marker>
<rawString>Anna Ritchie, Stephen Robertson, and Simone Teufel. 2008. Comparing citation contexts for information retrieval. In Proc. of CIKM ’08, pages 213–222, Napa Valley, California, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guy Shani</author>
<author>David Heckerman</author>
<author>Ronen I Brafman</author>
</authors>
<title>An mdp-based recommender system.</title>
<date>2005</date>
<journal>J. Mach. Learn. Res.,</journal>
<pages>6--1265</pages>
<contexts>
<context position="3079" citStr="Shani et al., 2005" startWordPosition="463" endWordPosition="466">apers cite or are cited by other papers. In a citation network, information flows from one paper to another via the citation relation (Shi et al., 2010). This information flow and the influence of one paper on another are specifically captured by means of citation contexts, i.e., short text segments surrounding a citation’s mention. These contexts are not arbitrary, but they serve as brief summaries of a cited paper. Figure 1 illustrates this idea using a small citation network of a paper by Rendle et al. (2010) that cites (Zimdars et al., 2001), (Hu et al., 2008), (Pan and Scholz, 2009) and (Shani et al., 2005) and is cited by (Cheng et al., 2013). The citation mentions and citation contexts are shown with a dashed line. Note the high overlap between the words in contexts and those in the title and abstract (shown in bold) and the author-annotated keywords. One question that can be raised is the following: Can we effectively exploit information available in large inter-linked document networks in order to improve the performance of keyphrase extraction? The research that we describe in this paper addresses specifically this question using citation networks of research papers as a case study. Extract</context>
</contexts>
<marker>Shani, Heckerman, Brafman, 2005</marker>
<rawString>Guy Shani, David Heckerman, and Ronen I. Brafman. 2005. An mdp-based recommender system. J. Mach. Learn. Res., 6:1265–1295, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaolin Shi</author>
<author>Jure Leskovec</author>
<author>Daniel A McFarland</author>
</authors>
<title>Citing for high impact.</title>
<date>2010</date>
<booktitle>In Proceedings of the 10th Annual Joint Conference on Digital Libraries, JCDL ’10,</booktitle>
<pages>49--58</pages>
<location>Gold Coast, Queensland, Australia.</location>
<contexts>
<context position="2612" citStr="Shi et al., 2010" startWordPosition="381" endWordPosition="384">ually-similar documents, where the cosine similarity between the tf-idf vectors of documents is used to compute their similarity. We posit that, in addition to a document’s textual content and textually-similar neighbors, other informative neighborhoods exist that have the potential to improve keyphrase extraction. For example, in a scholarly domain, research papers are not isolated. Rather, they are highly inter-connected in giant citation networks, in which papers cite or are cited by other papers. In a citation network, information flows from one paper to another via the citation relation (Shi et al., 2010). This information flow and the influence of one paper on another are specifically captured by means of citation contexts, i.e., short text segments surrounding a citation’s mention. These contexts are not arbitrary, but they serve as brief summaries of a cited paper. Figure 1 illustrates this idea using a small citation network of a paper by Rendle et al. (2010) that cites (Zimdars et al., 2001), (Hu et al., 2008), (Pan and Scholz, 2009) and (Shani et al., 2005) and is cited by (Cheng et al., 2013). The citation mentions and citation contexts are shown with a dashed line. Note the high overla</context>
</contexts>
<marker>Shi, Leskovec, McFarland, 2010</marker>
<rawString>Xiaolin Shi, Jure Leskovec, and Daniel A. McFarland. 2010. Citing for high impact. In Proceedings of the 10th Annual Joint Conference on Digital Libraries, JCDL ’10, pages 49–58, Gold Coast, Queensland, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Teufel</author>
<author>A Siddharthan</author>
<author>D Tidhar</author>
</authors>
<title>Automatic classification of citation function.</title>
<date>2006</date>
<booktitle>In Proceedings of EMNLP-06.</booktitle>
<contexts>
<context position="11839" citStr="Teufel et al., 2006" startWordPosition="1867" endWordPosition="1870">ent a supervised approach to keyphrase extraction from research papers that are embedded in large citation networks, and propose novel features that show improvement over strong supervised and unsupervised baselines. To our knowledge, features extracted from citation contexts have not been used before for keyphrase extraction in a supervised learning framework. 3 Problem Characterization In citation networks, in addition to the information contained in a paper itself, citing and cited papers capture different aspects (e.g., topicality, domain of study, algorithms used) about the target paper (Teufel et al., 2006), with citation contexts playing an instrumental role. A citation context is defined as a window of n words surrounding a citation mention. We conjecture that citation contexts, which act as brief summaries about a cited paper, provide additional clues in extracting keyphrases for a target paper. These clues give rise to the unique design of our model, called citationenhanced keyphrase extraction (CeKE). 3.1 Citation-enhanced Keyphrase Extraction Our proposed citation-enhanced keyphrase extraction (CeKE) model is a supervised binary classifi1437 Table 1: The list of features used in our model.</context>
</contexts>
<marker>Teufel, Siddharthan, Tidhar, 2006</marker>
<rawString>S. Teufel, A. Siddharthan, and D. Tidhar. 2006. Automatic classification of citation function. In Proceedings of EMNLP-06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Teufel</author>
</authors>
<title>Argumentative Zoning: Information Extraction from Scientific Text.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Edinburgh.</institution>
<contexts>
<context position="21251" citStr="Teufel, 1999" startWordPosition="3434" endWordPosition="3435">examples correspond to the remaining candidate phrases. Context lengths. In CiteSeer&amp;quot;, citation contexts have about 50 words on each side of a citation mention. A previous study by Ritchie et al. (2008) shows that a fixed window length of about 100 words around a citation mention is generally effective for information retrieval tasks. For this reason, we used the contexts provided by CiteSeer&amp;quot; directly. However, in future, it would be interesting to incorporate in our models more sophisticated approaches to identifying the text that is relevant to a target citation (Abu-Jbara and Radev, 2012; Teufel, 1999) and study the influence of context lengths on the quality of extracted keyphrase. 1439 WWW KDD Method Precision Recall F1-score Precision Recall F1-score Citation - Enhanced (CeKE) 0.227 0.386 0.284 0.213 0.413 0.280 Hulth - n-gram with tags 0.165 0.107 0.129 0.206 0.151 0.172 KEA 0.210 0.146 0.168 0.178 0.124 0.145 Table 3: The comparison of CeKE with supervised approaches on WWW and KDD collections. 4.2 Experimental Design Our experiments are designed around the following research questions: 1. How does the performance of citationenhanced keyphrase extraction (CeKE) compare with the perform</context>
</contexts>
<marker>Teufel, 1999</marker>
<rawString>S. Teufel. 1999. Argumentative Zoning: Information Extraction from Scientific Text. Ph.D. thesis, University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paolo Tonella</author>
<author>Filippo Ricca</author>
<author>Emanuele Pianta</author>
<author>Christian Girardi</author>
</authors>
<title>Using Keyword Extraction for Web Site Clustering.</title>
<date>2003</date>
<booktitle>In Web Site Evolution, 2003. Theme: Architecture. Proceedings. Fifth IEEE International Workshop on,</booktitle>
<pages>41--48</pages>
<contexts>
<context position="7384" citStr="Tonella et al., 2003" startWordPosition="1133" endWordPosition="1136"> propose novel features extracted from the local neighborhoods of documents available in interlinked document networks. Medelyan et al. (2009) extended KEA as well to integrate information from Wikipedia. In contrast, we used only information intrinsic to our data. Enhancing our models with Wikipedia information would be an interesting future direction to pursue. In the unsupervised line of research, keyphrase extraction is formulated as a ranking problem, where keyphrases are ranked using their tf (Barker and Cornacchia, 2000), tf-idf (Zhang et al., 2007; Lee and Kim, 2008; Liu et al., 2009; Tonella et al., 2003), and term informativeness (Wu and Giles, 2013; Rennie and Jaakkola, 2005; Kireyev, 2009) (among others). The ranking based on tf-idf has 1436 been shown to work well in practice (Liu et al., 2009; Hasan and Ng, 2010) despite its simplicity. Frantzi et al. (1998) combined linguistics and statistical information to extract technical terms from documents in digital libraries. Graph-based algorithms and centrality measures are also widely used in unsupervised models. A word graph is built for each document such that nodes correspond to words and edges correspond to word association patterns. Node</context>
<context position="27595" citStr="Tonella et al., 2003" startWordPosition="4482" endWordPosition="4485">or WWW. As can be seen from the table, tf-idf and citation tf-idf are both highly ranked, first and third, respectively, illustrating that they contain significant information in predicting keyphrases. The first position of a phrase is also of great impact. This is consistent with the fact that almost half of the identified keywords and about 20% of the annotated keyphrases appear in title. Similar ranking is obtained on KDD. The comparison of CeKE with unsupervised state-of-the-art models. Table 5 shows the results of the comparison of CeKE with three unsupervised ranking approaches: TF-IDF (Tonella et al., 2003), TextRank (Mihalcea and Tarau, 2004), and ExpandRank (Wan and Xiao, 2008). TF-IDF and TextRank use information only from the target paper, whereas ExpandRank uses a small textual neighborhood in addition to the target paper. Note that, for all unsupervised methods, we used Porter stemmer and the same candidate phrase generation as in CeKE, as explained in §3.2. For TF-IDF, we first tokenized the target paper and computed the score for each word, and then formed phrases and summed up the score of every word within a phrase. For TextRank, we built an undirected graph for each paper, where the n</context>
</contexts>
<marker>Tonella, Ricca, Pianta, Girardi, 2003</marker>
<rawString>Paolo Tonella, Filippo Ricca, Emanuele Pianta, and Christian Girardi. 2003. Using Keyword Extraction for Web Site Clustering. In Web Site Evolution, 2003. Theme: Architecture. Proceedings. Fifth IEEE International Workshop on, pages 41–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>Learning algorithms for keyphrase extraction.</title>
<date>2000</date>
<journal>Inf. Retr.,</journal>
<volume>2</volume>
<contexts>
<context position="5948" citStr="Turney, 2000" startWordPosition="907" endWordPosition="908"> keyphrase extraction have been proposed in the literature along two lines of research: supervised and unsupervised, using different types of documents including scientific abstracts, newswire documents, meeting transcripts, and webpages (Frank et al., 1999; Hulth, 2003; Nguyen and Kan, 2007; Liu et al., 2009; Marujo et al., 2013; Mihalcea and Tarau, 2004). In the supervised line of research, keyphrase extraction is formulated as a binary classification problem, where candidate phrases are classified as either positive (i.e., keyphrases) or negative (i.e., non-keyphrases) (Frank et al., 1999; Turney, 2000; Hulth, 2003). Different feature sets and classification algorithms gave rise to different models. For example, Hulth (2003) used four different features in conjunction with a bagging technique. These features are: term frequency, collection frequency, the relative position of the first occurrence and the part-of-speech tag of a term. Frank et al. (1999) developed a system called KEA that used only two features: tf-idf (term frequency-inverse document frequency) of a phrase and the distance of a phrase from the beginning of a document (i.e., its relative position) and used them as input to Na</context>
</contexts>
<marker>Turney, 2000</marker>
<rawString>Peter D. Turney. 2000. Learning algorithms for keyphrase extraction. Inf. Retr., 2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>Coherent Keyphrase Extraction via Web Mining.</title>
<date>2003</date>
<booktitle>In Proceedings of the 18th international joint conference on Artificial intelligence, IJCAI’03,</booktitle>
<pages>434--439</pages>
<location>Acapulco, Mexico.</location>
<contexts>
<context position="1464" citStr="Turney, 2003" startWordPosition="204" endWordPosition="205">m in conjunction with traditional features for keyphrase extraction to obtain remarkable improvements in performance over strong baselines. 1 Introduction Keyphrase extraction is the problem of automatically extracting important phrases or concepts (i.e., the essence) of a document. Keyphrases provide a high-level topic description of a document and are shown to be rich sources of information for many applications such as document classification, clustering, recommendation, indexing, searching, and summarization (Jones and Staveley, 1999; Zha, 2002; Hammouda et al., 2005; Pudota et al., 2010; Turney, 2003). Despite the fact that keyphrase extraction has been widely researched in the natural language processing community, its performance is still far from being satisfactory (Hasan and Ng, 2014). Many previous approaches to keyphrase extraction generally used only the textual content of a target document to extract keyphrases (Hulth, 2003; Mihalcea and Tarau, 2004; Liu et al., 2010). Recently, Wan and Xiao (2008) proposed a model that incorporates a local neighborhood of a document. However, their neighborhood is limited to textually-similar documents, where the cosine similarity between the tf-i</context>
</contexts>
<marker>Turney, 2003</marker>
<rawString>Peter D. Turney. 2003. Coherent Keyphrase Extraction via Web Mining. In Proceedings of the 18th international joint conference on Artificial intelligence, IJCAI’03, pages 434–439, Acapulco, Mexico.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaojun Wan</author>
<author>Jianguo Xiao</author>
</authors>
<title>Single Document Keyphrase Extraction Using Neighborhood Knowledge.</title>
<date>2008</date>
<booktitle>In Proceedings of AAAI ’08,</booktitle>
<pages>855--860</pages>
<location>Chicago, Illinois.</location>
<contexts>
<context position="1877" citStr="Wan and Xiao (2008)" startWordPosition="268" endWordPosition="271">plications such as document classification, clustering, recommendation, indexing, searching, and summarization (Jones and Staveley, 1999; Zha, 2002; Hammouda et al., 2005; Pudota et al., 2010; Turney, 2003). Despite the fact that keyphrase extraction has been widely researched in the natural language processing community, its performance is still far from being satisfactory (Hasan and Ng, 2014). Many previous approaches to keyphrase extraction generally used only the textual content of a target document to extract keyphrases (Hulth, 2003; Mihalcea and Tarau, 2004; Liu et al., 2010). Recently, Wan and Xiao (2008) proposed a model that incorporates a local neighborhood of a document. However, their neighborhood is limited to textually-similar documents, where the cosine similarity between the tf-idf vectors of documents is used to compute their similarity. We posit that, in addition to a document’s textual content and textually-similar neighbors, other informative neighborhoods exist that have the potential to improve keyphrase extraction. For example, in a scholarly domain, research papers are not isolated. Rather, they are highly inter-connected in giant citation networks, in which papers cite or are</context>
<context position="8113" citStr="Wan and Xiao, 2008" startWordPosition="1250" endWordPosition="1253">anking based on tf-idf has 1436 been shown to work well in practice (Liu et al., 2009; Hasan and Ng, 2010) despite its simplicity. Frantzi et al. (1998) combined linguistics and statistical information to extract technical terms from documents in digital libraries. Graph-based algorithms and centrality measures are also widely used in unsupervised models. A word graph is built for each document such that nodes correspond to words and edges correspond to word association patterns. Nodes are then ranked using graph centrality measures such as PageRank and its variants (Mihalcea and Tarau, 2004; Wan and Xiao, 2008; Liu et al., 2010; Zhao et al., 2011), HITS scores (Litvak and Last, 2008), as well as node degree and betweenness (Boudin, 2013; Xie, 2005). Wan and Xiao (2008) were the first to consider modeling a local neighborhood of a target document in addition to the document itself, and applied this approach to news articles on the Web. Their local neighborhood consists of textually similar documents, and did not capture information contained in document networks. Using terms from citation contexts of scientific papers is not a new idea. It was used before in various applications. For example, Ritchi</context>
<context position="17423" citStr="Wan and Xiao, 2008" startWordPosition="2813" endWordPosition="2816">; used in Hulth’s methods Novel features - Citation Network Based tf-idf inCited if the phrase occurs in cited contexts inCiting if the phrase occurs in citing contexts citation tf-idf the tf-idf value of the phrase, computed from the aggregated citation contexts 1438 Dataset Num. (#) Average Average Average #uni- #bi- #triPapers Cited Ctx. Citing Ctx. Keyphrases grams grams grams WWW 425 15.45 18.78 4.87 680 1036 247 KDD 365 12.69 19.74 4.03 363 853 189 Table 2: A summary of our datasets. speech filters. Consistent with previous works (Hulth, 2003; Mihalcea and Tarau, 2004; Liu et al., 2010; Wan and Xiao, 2008), only nouns and adjectives are retained to form candidate phrases. The generation process consists of two steps. First, using the NLP Stanford part of speech tagger, we preprocess each document and keep only the nouns and adjectives corresponding to {NN, NNS, NNP, NNPS, JJ}. We apply the Porter stemmer on every word. The position of each word is kept consistent with the initial state of the document before any word removal is made. Second, words extracted in the first step that have contiguous positions in a document are concatenated into n-grams. We used unigrams, bigrams, and trigrams (n = </context>
<context position="22544" citStr="Wan and Xiao, 2008" startWordPosition="3635" endWordPosition="3638">the data and what are the most informative features for classification? We compared CeKE’s performance with that of classifiers trained on KEA features only and Hulth’s features only and present a ranking of features based on information gain. 2. How do supervised models that integrate citation network information compare with recent unsupervised models? Since recent unsupervised approaches are becoming competitive with supervised approaches (Hasan and Ng, 2014), we also compared CeKE with unsupervised ranking of candidate phrases by TF-IDF, TextRank (Mihalcea and Tarau, 2004) and ExpandRank (Wan and Xiao, 2008). For unsupervised, we considered top 5 and top 10 ranked phrases when computing “@5” and “@10” measures. 3. How well does our proposed model perform in the absence of either cited or citing contexts? Since newly published scientific papers are not cited by many other papers, e.g., due to their recency, no cited contexts are available. We studied the quality of predicted keyphrases when either cited or citing contexts are missing. For this, we compared the performance of models trained using both cited and citing contexts with that of models that use either cited or citing contexts. Evaluation</context>
<context position="27669" citStr="Wan and Xiao, 2008" startWordPosition="4493" endWordPosition="4496">ighly ranked, first and third, respectively, illustrating that they contain significant information in predicting keyphrases. The first position of a phrase is also of great impact. This is consistent with the fact that almost half of the identified keywords and about 20% of the annotated keyphrases appear in title. Similar ranking is obtained on KDD. The comparison of CeKE with unsupervised state-of-the-art models. Table 5 shows the results of the comparison of CeKE with three unsupervised ranking approaches: TF-IDF (Tonella et al., 2003), TextRank (Mihalcea and Tarau, 2004), and ExpandRank (Wan and Xiao, 2008). TF-IDF and TextRank use information only from the target paper, whereas ExpandRank uses a small textual neighborhood in addition to the target paper. Note that, for all unsupervised methods, we used Porter stemmer and the same candidate phrase generation as in CeKE, as explained in §3.2. For TF-IDF, we first tokenized the target paper and computed the score for each word, and then formed phrases and summed up the score of every word within a phrase. For TextRank, we built an undirected graph for each paper, where the nodes correspond to words in the target paper and edges are drawn between t</context>
<context position="29736" citStr="Wan and Xiao, 2008" startWordPosition="4847" endWordPosition="4850">both contexts and using with only cited or citing contexts. For each unsupervised method, we computed results for top 5 and top 10 ranked phrases. As can be seen from Table 5, CeKE substantially outperforms all the other methods for our domain of study, i.e., papers from WWW and KDD, illustrating again that the citation network of a paper contains important information that can show remarkable benefits for keyphrase extraction. Among all unsupervised methods, ExpandRank with fewer textual similar neighbor (one or five) performs the best. This is generally consistent with the results shown in (Wan and Xiao, 2008) for news articles. The effect of cited and citing contexts information on models’ performance. Table 6 shows the precision, recall and F-score values for some variations of our method when: (1) all the citation contexts for a paper are used, (2) only cited contexts are used, (3) only citing contexts are used. The motivation behind this experiment was to determine how well the proposed model would perform on newly published research papers that have not accumulated citations yet. As shown in the table, there is no substantial difference in terms of precision between CeKE models that use only c</context>
</contexts>
<marker>Wan, Xiao, 2008</marker>
<rawString>Xiaojun Wan and Jianguo Xiao. 2008. Single Document Keyphrase Extraction Using Neighborhood Knowledge. In Proceedings of AAAI ’08, pages 855–860, Chicago, Illinois.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhaohui Wu</author>
<author>Lee C Giles</author>
</authors>
<title>Measuring term informativeness in context.</title>
<date>2013</date>
<booktitle>In Proceedings of NAACL ’13,</booktitle>
<pages>259--269</pages>
<location>Atlanta,</location>
<contexts>
<context position="7430" citStr="Wu and Giles, 2013" startWordPosition="1140" endWordPosition="1143">neighborhoods of documents available in interlinked document networks. Medelyan et al. (2009) extended KEA as well to integrate information from Wikipedia. In contrast, we used only information intrinsic to our data. Enhancing our models with Wikipedia information would be an interesting future direction to pursue. In the unsupervised line of research, keyphrase extraction is formulated as a ranking problem, where keyphrases are ranked using their tf (Barker and Cornacchia, 2000), tf-idf (Zhang et al., 2007; Lee and Kim, 2008; Liu et al., 2009; Tonella et al., 2003), and term informativeness (Wu and Giles, 2013; Rennie and Jaakkola, 2005; Kireyev, 2009) (among others). The ranking based on tf-idf has 1436 been shown to work well in practice (Liu et al., 2009; Hasan and Ng, 2010) despite its simplicity. Frantzi et al. (1998) combined linguistics and statistical information to extract technical terms from documents in digital libraries. Graph-based algorithms and centrality measures are also widely used in unsupervised models. A word graph is built for each document such that nodes correspond to words and edges correspond to word association patterns. Nodes are then ranked using graph centrality measu</context>
</contexts>
<marker>Wu, Giles, 2013</marker>
<rawString>Zhaohui Wu and Lee C. Giles. 2013. Measuring term informativeness in context. In Proceedings of NAACL ’13, pages 259–269, Atlanta, Georgia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhuli Xie</author>
</authors>
<title>Centrality Measures in Text Mining: Prediction of Noun Phrases that Appear in Abstracts.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL Student Research Workshop,</booktitle>
<pages>103--108</pages>
<location>Ann Arbor, Michigan.</location>
<contexts>
<context position="8254" citStr="Xie, 2005" startWordPosition="1277" endWordPosition="1278">98) combined linguistics and statistical information to extract technical terms from documents in digital libraries. Graph-based algorithms and centrality measures are also widely used in unsupervised models. A word graph is built for each document such that nodes correspond to words and edges correspond to word association patterns. Nodes are then ranked using graph centrality measures such as PageRank and its variants (Mihalcea and Tarau, 2004; Wan and Xiao, 2008; Liu et al., 2010; Zhao et al., 2011), HITS scores (Litvak and Last, 2008), as well as node degree and betweenness (Boudin, 2013; Xie, 2005). Wan and Xiao (2008) were the first to consider modeling a local neighborhood of a target document in addition to the document itself, and applied this approach to news articles on the Web. Their local neighborhood consists of textually similar documents, and did not capture information contained in document networks. Using terms from citation contexts of scientific papers is not a new idea. It was used before in various applications. For example, Ritchie et al. (2006) used a combination of terms from citation contexts and existing index terms of a paper to improve indexing of cited papers. C</context>
</contexts>
<marker>Xie, 2005</marker>
<rawString>Zhuli Xie. 2005. Centrality Measures in Text Mining: Prediction of Noun Phrases that Appear in Abstracts. In Proceedings of the ACL Student Research Workshop, pages 103–108, Ann Arbor, Michigan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hongyuan Zha</author>
</authors>
<title>Generic summarization and keyphrase extraction using mutual reinforcement principle and sentence clustering.</title>
<date>2002</date>
<booktitle>In SIGIR.</booktitle>
<contexts>
<context position="1405" citStr="Zha, 2002" startWordPosition="194" endWordPosition="195">tures based on citation network information and use them in conjunction with traditional features for keyphrase extraction to obtain remarkable improvements in performance over strong baselines. 1 Introduction Keyphrase extraction is the problem of automatically extracting important phrases or concepts (i.e., the essence) of a document. Keyphrases provide a high-level topic description of a document and are shown to be rich sources of information for many applications such as document classification, clustering, recommendation, indexing, searching, and summarization (Jones and Staveley, 1999; Zha, 2002; Hammouda et al., 2005; Pudota et al., 2010; Turney, 2003). Despite the fact that keyphrase extraction has been widely researched in the natural language processing community, its performance is still far from being satisfactory (Hasan and Ng, 2014). Many previous approaches to keyphrase extraction generally used only the textual content of a target document to extract keyphrases (Hulth, 2003; Mihalcea and Tarau, 2004; Liu et al., 2010). Recently, Wan and Xiao (2008) proposed a model that incorporates a local neighborhood of a document. However, their neighborhood is limited to textually-simi</context>
</contexts>
<marker>Zha, 2002</marker>
<rawString>Hongyuan Zha. 2002. Generic summarization and keyphrase extraction using mutual reinforcement principle and sentence clustering. In SIGIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yongzheng Zhang</author>
<author>Evangelos Milios</author>
<author>Nur ZincirHeywood</author>
</authors>
<title>A Comparative Study on Key Phrase Extraction Methods in Automatic Web Site Summarization.</title>
<date>2007</date>
<journal>Journal of Digital Information Management,</journal>
<volume>5</volume>
<issue>5</issue>
<contexts>
<context position="7324" citStr="Zhang et al., 2007" startWordPosition="1121" endWordPosition="1124"> acronym status of a term. In contrast to these works, we propose novel features extracted from the local neighborhoods of documents available in interlinked document networks. Medelyan et al. (2009) extended KEA as well to integrate information from Wikipedia. In contrast, we used only information intrinsic to our data. Enhancing our models with Wikipedia information would be an interesting future direction to pursue. In the unsupervised line of research, keyphrase extraction is formulated as a ranking problem, where keyphrases are ranked using their tf (Barker and Cornacchia, 2000), tf-idf (Zhang et al., 2007; Lee and Kim, 2008; Liu et al., 2009; Tonella et al., 2003), and term informativeness (Wu and Giles, 2013; Rennie and Jaakkola, 2005; Kireyev, 2009) (among others). The ranking based on tf-idf has 1436 been shown to work well in practice (Liu et al., 2009; Hasan and Ng, 2010) despite its simplicity. Frantzi et al. (1998) combined linguistics and statistical information to extract technical terms from documents in digital libraries. Graph-based algorithms and centrality measures are also widely used in unsupervised models. A word graph is built for each document such that nodes correspond to w</context>
</contexts>
<marker>Zhang, Milios, ZincirHeywood, 2007</marker>
<rawString>Yongzheng Zhang, Evangelos Milios, and Nur ZincirHeywood. 2007. A Comparative Study on Key Phrase Extraction Methods in Automatic Web Site Summarization. Journal of Digital Information Management, 5(5):323.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wayne Xin Zhao</author>
<author>Jing Jiang</author>
<author>Jing He</author>
</authors>
<title>Yang Song, Palakorn Achananuparp, Ee-Peng Lim, and Xiaoming Li.</title>
<date>2011</date>
<booktitle>In Proceedings of HLT ’11,</booktitle>
<pages>379--388</pages>
<location>Portland, Oregon.</location>
<contexts>
<context position="8151" citStr="Zhao et al., 2011" startWordPosition="1258" endWordPosition="1261">hown to work well in practice (Liu et al., 2009; Hasan and Ng, 2010) despite its simplicity. Frantzi et al. (1998) combined linguistics and statistical information to extract technical terms from documents in digital libraries. Graph-based algorithms and centrality measures are also widely used in unsupervised models. A word graph is built for each document such that nodes correspond to words and edges correspond to word association patterns. Nodes are then ranked using graph centrality measures such as PageRank and its variants (Mihalcea and Tarau, 2004; Wan and Xiao, 2008; Liu et al., 2010; Zhao et al., 2011), HITS scores (Litvak and Last, 2008), as well as node degree and betweenness (Boudin, 2013; Xie, 2005). Wan and Xiao (2008) were the first to consider modeling a local neighborhood of a target document in addition to the document itself, and applied this approach to news articles on the Web. Their local neighborhood consists of textually similar documents, and did not capture information contained in document networks. Using terms from citation contexts of scientific papers is not a new idea. It was used before in various applications. For example, Ritchie et al. (2006) used a combination of </context>
</contexts>
<marker>Zhao, Jiang, He, 2011</marker>
<rawString>Wayne Xin Zhao, Jing Jiang, Jing He, Yang Song, Palakorn Achananuparp, Ee-Peng Lim, and Xiaoming Li. 2011. Topical Keyphrase Extraction from Twitter. In Proceedings of HLT ’11, pages 379–388, Portland, Oregon.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Zimdars</author>
<author>David Maxwell Chickering</author>
<author>Christopher Meek</author>
</authors>
<title>Using temporal data for making recommendations.</title>
<date>2001</date>
<booktitle>In Proceedings of the 17th Conference in Uncertainty in Artificial Intelligence, UAI ’01,</booktitle>
<pages>580--588</pages>
<contexts>
<context position="3011" citStr="Zimdars et al., 2001" startWordPosition="450" endWordPosition="453">they are highly inter-connected in giant citation networks, in which papers cite or are cited by other papers. In a citation network, information flows from one paper to another via the citation relation (Shi et al., 2010). This information flow and the influence of one paper on another are specifically captured by means of citation contexts, i.e., short text segments surrounding a citation’s mention. These contexts are not arbitrary, but they serve as brief summaries of a cited paper. Figure 1 illustrates this idea using a small citation network of a paper by Rendle et al. (2010) that cites (Zimdars et al., 2001), (Hu et al., 2008), (Pan and Scholz, 2009) and (Shani et al., 2005) and is cited by (Cheng et al., 2013). The citation mentions and citation contexts are shown with a dashed line. Note the high overlap between the words in contexts and those in the title and abstract (shown in bold) and the author-annotated keywords. One question that can be raised is the following: Can we effectively exploit information available in large inter-linked document networks in order to improve the performance of keyphrase extraction? The research that we describe in this paper addresses specifically this question</context>
</contexts>
<marker>Zimdars, Chickering, Meek, 2001</marker>
<rawString>Andrew Zimdars, David Maxwell Chickering, and Christopher Meek. 2001. Using temporal data for making recommendations. In Proceedings of the 17th Conference in Uncertainty in Artificial Intelligence, UAI ’01, pages 580–588.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
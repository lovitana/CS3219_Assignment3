<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000023">
<title confidence="0.782492">
Learning to Solve Arithmetic Word Problems with Verb Categorization
</title>
<author confidence="0.587953">
Mohammad Javad Hosseini1, Hannaneh Hajishirzi1, Oren Etzioni2, and Nate Kushman3
</author>
<email confidence="0.703207">
1{hosseini, hannaneh}@washington.edu, 2OrenE@allenai.org, 3nkushman@csail.mit.edu
</email>
<affiliation confidence="0.856866">
1University of Washington, 2Allen Institute for AI, 3Massachusetts Institute of Technology
</affiliation>
<sectionHeader confidence="0.977395" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999693947368421">
This paper presents a novel approach to
learning to solve simple arithmetic word
problems. Our system, ARIS, analyzes
each of the sentences in the problem state-
ment to identify the relevant variables and
their values. ARIS then maps this infor-
mation into an equation that represents
the problem, and enables its (trivial) so-
lution as shown in Figure 1. The pa-
per analyzes the arithmetic-word problems
“genre”, identifying seven categories of
verbs used in such problems. ARIS learns
to categorize verbs with 81.2% accuracy,
and is able to solve 77.7% of the problems
in a corpus of standard primary school test
questions. We report the first learning re-
sults on this task without reliance on pre-
defined templates and make our data pub-
licly available.1
</bodyText>
<sectionHeader confidence="0.999134" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999932384615385">
Designing algorithms to automatically solve math
and science problems is a long-standing AI chal-
lenge (Feigenbaum and Feldman, 1963). For NLP,
mathematical word problems are particularly at-
tractive because the text is concise and relatively
straightforward, while the semantics reduces to
simple equations.
Arithmetic word problems begin by describing
a partial world state, followed by simple updates
or elaborations and end with a quantitative ques-
tion. For a child, the language understanding part
is trivial, but the reasoning may be challenging;
for our system, the opposite is true. ARIS needs to
</bodyText>
<footnote confidence="0.9884295">
1Our data is available at https://www.cs.
washington.edu/nlp/arithmetic.
</footnote>
<table confidence="0.353785230769231">
Arithmetic word Problem
Liz had 9 black kittens. She gave some of her kittens to
Joan. Joan now has 11 kittens. Liz has 5 kittens left and 3
have spots. How many kittens did Joan get?
State Transition
s1 s2 Liz Joan
Liz
N: 9 give N: 9-L1 N: J0+L1
E: Kitten E: Kitten E: Kitten
A: Black A: Black A: Black
Liz gave some of her kittens to Joan.
Equation: 9 − x = 5
Solution: x = 4 kittens
</table>
<figureCaption confidence="0.999446">
Figure 1: Example problem and solution.
</figureCaption>
<bodyText confidence="0.999959434782609">
make sense of multiple sentences, as shown in Fig-
ure 2, without a priori restrictions on the syntax or
vocabulary used to describe the problem. Figure
1 shows an example where ARIS is asked to infer
how many kittens Joan received based on facts and
constraints expressed in the text, and represented
by the state diagram and corresponding equation.
While the equation is trivial, the text could have
involved assembling toy aircraft, collecting coins,
eating cookies, or just about any activity involving
changes in the quantities of discrete objects.
This paper investigates the task of learning to
solve such problems by mapping the verbs in the
problem text into categories that describe their im-
pact on the world state. While the verbs category
is crucial (e.g., what happens if “give” is replaced
by “receive” in Figure 1?), some elements of the
problem are irrelevant. For instance, the fact that
three kittens have spots is immaterial to the solu-
tion. Thus, ARIS has to determine what informa-
tion is relevant to solving the problem.
To abstract from the problem text, ARIS maps
the text to a state representation which consists of
</bodyText>
<page confidence="0.982323">
523
</page>
<note confidence="0.910631">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 523–533,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999871457142857">
a set of entities, their containers, attributes, quan-
tities, and relations. A problem text is split into
fragments where each fragment corresponds to an
observation or an update of the quantity of an en-
tity in one or two containers. For example in Fig-
ure 1, the sentence “Liz has 5 kittens left and 3
have spots” has two fragments of “Liz has 5 kit-
tens left” and “3 have spots”.
The verb in each sentence is associated with one
or two containers, and ARIS has to classify each
verb in a sentence into one of seven categories
that describe the impact of the verb on the con-
tainers (Table 1). ARIS learns this classifier based
on training data as described in section 4.2.
To evaluate ARIS, we compiled a corpus of
about 400 arithmetic (addition and subtraction)
word problems and utilized cross validation to
both train ARIS and evaluate its performance
over this corpus. We compare its performance
to the template-based learning method developed
independently and concurrently by Kushman et
al. (2014). We find that our approach is much
more robust to domain diversity between the train-
ing and test sets.
Our contributions are three-fold: (a) We present
ARIS, a novel, fully automated method that learns
to solve arithmetic word problems; (b) We intro-
duce a method to automatically categorize verbs
for sentences from simple, easy-to-obtain train-
ing data; our results refine verb senses in Word-
Net (Miller, 1995) for arithmetic word problems;
(c) We introduce a corpus of arithmetic word prob-
lems, and report on a series of experiments show-
ing high efficacy in solving addition and subtrac-
tion problems based on verb categorization.
</bodyText>
<sectionHeader confidence="0.999833" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999959576923077">
Understanding semantics of a natural language
text has been the focus of many researchers in nat-
ural language processing (NLP). Recent work fo-
cus on learning to align text with meaning repre-
sentations in specific, controlled domains. A few
methods (Zettlemoyer and Collins, 2005; Ge and
Mooney, 2006) use an expensive supervision in
the form of manually annotated formal representa-
tions for every sentence in the training data. More
recent work (Eisenstein et al., 2009; Kate and
Mooney, 2007; Goldwasser and Roth, 2011; Poon
and Domingos, 2009; Goldwasser et al., 2011;
Kushman and Barzilay, 2013) reduce the amount
of required supervision in mapping sentences to
meaning representations while taking advantage
of special properties of the domains. Our method,
on the other hand, requires small, easy-to-obtain
training data in the form of verb categories that
are shared among many different problem types.
Our work is also closely related to the grounded
language acquisition research (Snyder and Barzi-
lay, 2007; Branavan et al., 2009; Branavan et al.,
2012; Vogel and Jurafsky, 2010; Chen et al., 2010;
Hajishirzi et al., 2011; Chambers and Jurafsky,
2009; Liang et al., 2009; Bordes et al., 2010)
where the goal is to align a text into underlying en-
tities and events of an environment. These meth-
ods interact with an environment to obtain super-
vision from the real events and entities in the envi-
ronment. Our method, on the other hand, grounds
the problem into world state transitions by learn-
ing to predict verb categories in sentences. In addi-
tion, our method combines the representations of
individual sentences into a coherent whole to form
the equations. This is in contrast with the previous
work that study each sentence in isolation from the
other sentences.
Previous work on studying math word and logic
problems uses manually aligned meaning repre-
sentations or domain knowledge where the seman-
tics for all the words is provided (Lev, 2007; Lev
et al., 2004). Most recently, Kushman et al. (2014)
introduced an algorithm that learns to align al-
gebra problems to equations through the use of
templates. This method applies to broad range of
math problems, including multiplication, division,
and simultaneous equations, while ARIS only han-
dles arithmetic problems (addition and subtrac-
tion). However, our empirical results show that
for the problems it handles, ARIS is much more
robust to diversity in the problem types between
the training and test data.
</bodyText>
<sectionHeader confidence="0.991037" genericHeader="method">
3 Arithmetic Problem Representation
</sectionHeader>
<bodyText confidence="0.988075272727273">
We address solving arithmetic word problems that
include addition and subtraction. A problem text
is split into fragments where each fragment is rep-
resented as a transition between two world states
in which the quantities of entities are updated or
observed (Figure 2). We refer to these fragments
as sentences. We represent the world state as a tu-
ple (E, C, R) consisting of entities E, containers
C, and relations R among entities, containers, at-
tributes, and quantities.
Entities: An entity is a mention in the text corre-
</bodyText>
<page confidence="0.996511">
524
</page>
<table confidence="0.993857891891892">
Liz had 9 black kittens. She gave some of her kittens to Joan. Joan has now 11 kittens. Liz has 5 kitten s5 Liz
left and 3 has spots. How many kittens did Joan get?
N: 5
E: Kitten
A: Black
s2 Liz Liz s4 Liz
s3
N: 9-L1 N: 9-L1 N: 5
Joan
s1 E: Kitten E: Kitten E: Kitten
Liz
N: 11
E: Kitten
A: Black
A: Black A: Black A: Black
N: 9
s0
Liz had 9 E: Kitten She gave some Joan has now Liz has 5 And 3 has
A: Black of her kittens to Joan Joan Joan
kitten left
unknown
black kittens 11 kittens spots
Joan N: J0+L1 N: 11 N: 11
N:3
E: Kitten
E: Kitten E: Kitten E: Kitten
A: Black A: Black A: Black
There are 42 walnut trees and 12 orange trees currently in the park. Park workers cut down 13 walnut trees that were
How trees be in the the finished?
damaged. many walnut will park when workers are s2 N: 42-13 N: 12
s1 Park Park E: tree E: tree
A: walnut A: orange
N: 42 N: 12
s0 E: tree E: tree Park workers cut down 13 walnut trees N: W0-13
There are 42 walnut trees and 12
orange trees currently in the park. A: walnut A: orange that were damaged Workers E: tree
A: walnut
</table>
<figureCaption confidence="0.996426">
Figure 2: A figure sketching different steps of our method — a sequence of states.
</figureCaption>
<bodyText confidence="0.999390848484849">
sponding to an object whose quantity is observed
or is changing throughout the problem. For in-
stance, kitten and tree are entities in Fig-
ure 2. In addition, every entity has attributes that
modify the entity. For instance, black is an at-
tribute of kittens, and walnut is an attribute
of tree (more details on attributes in section 4.1).
Relations describing attributes are invariant to the
state changes. For instance kittens stay black
throughout the problem of Figure 1.
Containers: A container is a mention in the
text representing a set of entities. For instance,
Liz, Joan, park, and workers are containers
in Figure 2. Containers usually correspond to the
person possessing entities or a location contain-
ing entities. For example, in the sentence “There
are 43 blue marbles in the basket. John found 32
marbles.”, basket and John are containers of
marbles.
Quantities: Containers include entities with their
corresponding quantities in a particular world
state. Quantities can be known numbers (e.g. 9),
unknown variables (e.g. L1), or numerical expres-
sions over unknown quantities and numbers (e.g.
9−L1). For instance, in state 2 of Figure 2, the nu-
merical expression corresponding to Liz is 9−L1
and corresponding to Joan is J0 + L1, where J0
is a variable representing the number of kittens
that Joan has started with.
Hereinafter, we will refer to a generic entity as
e, container as c, number as num, attribute as a.
We represent the relation between a container, an
entity, and a number in the form of a quantity ex-
</bodyText>
<table confidence="0.996976">
Category Example
Observation There were 28 bales of hay in the barn.
Positive Joan went to 4 football games this year.
Negative John lost 3 of the violet balloons.
Positive Mike’s dad borrowed 7 nickels from
Transfer
Mike.
Negative Jason placed 131 erasers in the drawer.
Transfer
Construct Karen added 1/4 of a cup of walnuts to a
batch of trail mix.
Destroy The rabbits ate 4 of Dan’s potatoes.
</table>
<tableCaption confidence="0.978932666666667">
Table 1: Examples for different verb categories in sen-
tences. Entities are underlined; containers are italic, and
verbs are bolded.
</tableCaption>
<bodyText confidence="0.999870666666667">
pression N(c,e). Figure 2 shows the quantity
relations in different world states.
State transitions: Sentences depict progression
of the world state (Figure 2) in the form of ob-
servations of updates of quantities. We assume
that every sentence w consists of a verb v, an en-
tity e, a quantity num (might be unknown), one
or two containers c1, c2, and attributes a. The
presence of the second container, c2, will be dic-
tated by the category of the verb, as we discuss
below. Sentences abstract transitions (st → st+1)
between states in the form of an algebraic opera-
tion of addition or subtraction. For every sentence,
we model the state transition according to the verb
category and containers in the sentence. There are
three verb categories for sentences with one con-
tainer: Observation: the quantity is initialized in
the container, Positive: the quantity is increased
in the container, and Negative: the quantity is de-
creased in the container. Moreover, there are four
categories for sentences with two containers: Pos-
</bodyText>
<page confidence="0.990255">
525
</page>
<bodyText confidence="0.998516916666667">
itive transfer: the quantity is transferred from the
second container to the first one, Negative trans-
fer: the quantity is transferred from the first con-
tainer to the second one, Construct: the quantity
is increased for both containers, and Destroy: the
quantity is decreased for both containers.
Figure 2 shows how the state transitions are
determined by the verb categories. The sen-
tence “Liz has 9 black kittens” initializes the
quantity of kittens in the container Liz
to 9. In addition, the sentence “She gave
some of her kittens to Joan.” shows the
negative transfer of L1 kittens from Liz to
Joan represented as N(Liz,kitten)=9-L1
and N(Joan,kitten)=J0 + L1.
Given a math word problem, ARIS grounds the
world state into entities (e.g., kitten), contain-
ers (e.g., Liz), attributes (e.g., black), and quan-
tities (e.g., 9) (Section 4.1). In addition, ARIS
learns state transitions by classifying verb cate-
gories in sentences (Section 4.2). Finally, from the
world state and transitions, it generates an arith-
metic equation which can be solved to generate the
numeric answer to the word problem.
</bodyText>
<sectionHeader confidence="0.992961" genericHeader="method">
4 Our Method
</sectionHeader>
<bodyText confidence="0.9999825">
In this section we describe how ARIS maps an
arithmetic word problem into an equation (Fig-
ure 2). ARIS consists of three main steps (Fig-
ure 3): (1) grounding the problem into entities and
containers, (2) training a model to classify verb
categories in sentences, and (3) solving the prob-
lem by updating the world states with the learned
verb categories and forming equations.
</bodyText>
<subsectionHeader confidence="0.999568">
4.1 Grounding into Entities and Containers
</subsectionHeader>
<bodyText confidence="0.999980772727273">
ARIS automatically identifies entities, attributes,
containers, and quantities corresponding to every
sentence fragment (details in Figure 3 step 1). For
every problem, this module returns a sequence of
sentence fragments (w1,... , wT, wx) where every
wt consists of a verb vt, an entity et, its quantity
numt, its attributes at, and up to two containers
ct1, ct2. wx corresponds to the question sentence
inquiring about an unknown entity. ARIS applies
the Stanford dependency parser, named entity rec-
ognizer and coreference resolution system to the
problem text (de Marneffe et al., 2006; Finkel et
al., 2005; Raghunathan et al., 2010). It uses the
predicted coreference relationships to replace pro-
nouns (including possessive pronouns) with their
coreferenent links. The named entity recognition
output is used to identify numbers and people.
Entities: Entities are references to some object
whose quantity is observed or changing through-
out the problem. So to determine the set of
entities, we define h as the set of noun types
which have a dependent number (in the depen-
dency parse) somewhere in the problem text. The
set of entities is then defined as all noun phrases
which are headed by a noun type in h. For in-
stance kitten in the first sentence of Figure 1
is an entity because it is modified by the number
9, while kitten in the second sentence of Fig-
ure 1 is an entity because kitten was modified
by a number in the first sentence. Every number
in the text is associated with one entity. Num-
bers which are dependents of a noun are associ-
ated with its entity. Bare numbers (not dependent
on a noun) are associated with the previous entity
in the text. The entity in the last sentence is identi-
fied as the question entity ex . Finally, ARIS splits
the problem text into T + 1 sentence fragments
(w1, .. . wT, wx) such that each fragment contains
a single entity and it’s containers. For simplicity
we refer to these fragments as a sentences.
Containers: Each entity is associated with one
or two container noun phrases using the algorithm
described in in Figure 3 step 1c. As we saw earlier
with numbers, arithmetic problems often include
sentences with missing information. For example
in Figure 2, the second container in the the sen-
tence “Park workers had to cut down 13 walnut
trees that were damaged.” is not explicitly men-
tioned. To handle this missing information, we
use the circumscription assumption (McCarthy,
1980). The circumscription assumption formal-
izes the commonsense assumption that things are
as expected unless otherwise specified. In this set-
ting, we assume that the set of containers are fixed
in a problem. Thus if the container(s) for a given
entity cannot be identified they are set to the con-
tainer(s) for the previous entity with the same head
word. For example in Figure 2 we know from the
previous sentence that trees were in the park.
Therefore, we assume that the unmentioned con-
tainer is the park.
Attributes: ARIS selects attributes A as modifiers
for every entity from the dependency parser (de-
tails in Figure 3 step 1a). For example black is
an attribute of the entity kitten and is an ad-
jective modifier in the parser. These attributes are
</bodyText>
<page confidence="0.985993">
526
</page>
<listItem confidence="0.765789842105263">
1. Grounding into entities and containers: for every problem p in dataset (Section 4.1)
(a) (e1, ... , eT, ex)p +— extract all entities and the question entity
i. Extract all numbers and noun phrases (NP).
ii. h +— all noun types which appear with a number as a dependant (in the dependency parse tree) somewhere
in the problem text.
iii. et +— all NPs which are headed by a noun type in h.
iv. numt +— the dependant number of et if one exists. Bare numbers (not directly dependant on any noun
phrase) are associated with the previous entity in the text. All other numt are set to unknown.
v. ex +— the last identified entity.
vi. at +— adjective and noun modifiers of et. Update implicit attributes using the previously observed attributes.
vii. vt +— the verb with the shortest path to et in the dependency parse tree.
(b) (w1, ... , wT, wx)p +— split the problem text into fragments based on the entities and verbs
(c) (ct1, ct2, ... , cT1, cT2, cx)p +— the list of containers for each entity
i. ct1 +— the subject of wt.
If wt contains There is/are, ct1 is the first adverb of place to the verb.
ii. ct2 +— An NP that is direct object of the verb. If not found, ct2 is the object of the first adverbial phrase of
the verb.
iii. Circumscription assumption: When ct1 or ct2 are not found, they are set to the previous containers.
2. Training for sentence categorization (Section 4.2)
</listItem>
<figure confidence="0.815361785714286">
(a) instances1, instances2 +— 0
(b) for every sentence wt E (w1, ... , wT, wx)p in the training set:
i. featurest +— extract features (similarity based, WordNet based, structural) (Section 4.2.1)
ii. lt1, lt2 +— determine labels for containers ct1 and ct2 based on the verb category of wt.
iii. append (featurest, lt,1), (featurest, lt,2) to instances1, instances2.
(c) M1, M2 +— train two SVMs for instances1, instances2
3. Solving: for every problem p in the test set (Section 4.3)
(a) Identifying verb categories in sentences
i. for every sentence wt E (w1, ... , wT, wx)p:
A. featurest +— extract features (similarity based, WordNet based, structural).
B. lt1, lt2 +— classify wt for both containers ct1 and ct2 using models M1, M2.
(b) State progression: Form (s0, ... ,sT) (Section 4.3.1)
i. s0 +— null.
ii. for t E (1, ... , T): st +— progress(st_1, wt).
A. if et = ex and at = ax:
if wt is an observation: Nt(ct1, et) = numt.
else: update Nt(ct1, et) and Nt(ct2, et) given verb categories lt1, lt2.
B. copy Nt_1(c, e) to Nt(c, e) for all other (c, e) pairs.
(c) Forming equations and solution (Section 4.3.2)
i. Mark each wt that matches with wx if:
a) ct1 matches with cx and verb categories are equal or verbs are similar.
b) ct2 matches with cx and the verbs are in opposite categories.
ii. x +— the unknown quantity if wx matches with a sentence introducing an unknown number
iii. If the question asks about an unknown variable x or a start variable (wx contains “begin” or “start”):
For some container c, find two states st (quantity expression contains x) and st+1 (quantity is a known
number). Then, form an equation for x: Nt(c, ex) = Nt+1(c, ex).
iv. else: form equation as x = Nt(cx, ex).
v. Solve the equation and return the absolute value of x.
</figure>
<figureCaption confidence="0.980191">
Figure 3: ARIS: a method for solving arithmetic word problems.
</figureCaption>
<bodyText confidence="0.999511454545455">
used to prune the irrelevant information in pro-
gressing world states.
Arithmetic problems usually include sentences
with no attributes for the entities. For example,
the attribute black has not been explicitly men-
tioned for the kitten in the second sentence. In
particular, ARIS updates an implicit attribute using
the previously observed attribute. For example, in
“Joan went to 4 football games this year. She went
to 9 games last year.”, ARIS assigns football as
an attribute of the game in both sentences.
</bodyText>
<subsectionHeader confidence="0.878558">
4.2 Training for Verb Categories
</subsectionHeader>
<bodyText confidence="0.999924454545455">
This step involves training a model to identify verb
categories for sentences. This entails predicting
one label (increasing, decreasing) for each (verb,
container) pair in the sentence. Each possible set-
ting of these binary labels corresponds to one of
the seven verb categories discussed earlier. For ex-
ample, if c1 is increasing and c2 is decreasing this
is a positive transfer verb.
Our dataset includes word problems from dif-
ferent domains (more details in Section 5.2). Each
verb in our dataset is labeled with one of the 7 cat-
</bodyText>
<page confidence="0.985223">
527
</page>
<bodyText confidence="0.999037838709677">
egories from Table 1.
For training, we compile a list of sentences from
all the problems in the dataset and split sentences
into training and test sets in two settings. In the
first setting no instance from the same domain
appears in the training and test sets in order to
study the robustness of our method to new prob-
lem types. In the second setting no verb is re-
peated in the training and test sets in order to study
how well our method predicts categories of unseen
verbs.
For every sentence wt in the problems, we build
two data instances, (wt, c1) and (wt, c2), where c1
and c2 are containers extracted from the sentence.
For every instance in the training data, we assign
training labels using the verb categories of the sen-
tences instead of labeling every sentence individu-
ally. The verb can be increasing or decreasing cor-
responding to every container in the sentence. For
positive (negative) and construction (destruction)
verbs, both instances are labeled positive (nega-
tive). For transfer positive (negative) verbs, the
first instance is labeled positive (negative) and the
second instance is labeled negative (positive). For
observation verbs, both instances are labeled pos-
itive. We assume that the observation verbs are
known (total of 5 verbs). Finally, we train Support
Vector Machines given the extracted features and
training labels explained above (Figure 3 step 2).
In the following, we describe the features used for
training.
</bodyText>
<sectionHeader confidence="0.592016" genericHeader="method">
4.2.1 Features
</sectionHeader>
<bodyText confidence="0.999952872727273">
There are three sets of features: similarity based,
Wordnet-based, and structural features. The first
two sets of features focus on the verb and the third
set focuses on the dependency structure of the sen-
tence. All of our features are unlexicalized. This
allows ARIS to handle verbs in the test questions
which are completely different from those seen in
the training data.
Similarity-based Features: For every instance
(w, c), the feature vector includes similarity be-
tween the verb of the sentence w and a list of seed
verbs. The list of seed verbs is automatically se-
lected from a set V containing the 2000 most com-
mon English verbs using E1 regularized feature se-
lection technique. We select a small set of seed
verbs to avoid dominating the other feature types
(structural and WordNet-based features).
The goal is to automatically select verbs from
V that are most discriminative for each of the 7
verb categories in Table 1. We define 7 classifi-
cation tasks: “Is a verb a member of each cate-
gory?” Then, we select the three most represen-
tative verbs for each category. To do so, we ran-
domly select a set of 65 verbs Vl, from all the verbs
in our dataset (118 in total) and manually anno-
tate the verb categories. For every classification
task, the feature vector X includes the similarity
scores (Equation 1) between the verb v and all the
verbs in the V. We train an E1 regularized regres-
sion model (Park and Hastie, 2007) over the fea-
ture vector X to learn each category individually.
The number of original (similarity based) features
in X is relatively large, but E1 regularization pro-
vides a sparse weight vector. ARIS then selects the
three most common verbs (without replacement)
among the features (verbs) with non-zero weights.
This accounts for 21 total seed verbs to be used for
the main classification task. We find that in prac-
tice using this selection technique leads to better
performance than using either all the verbs in V or
using just the 65 randomly selected verbs.
Our method computes the similarity between
two verbs v1 and v2 from the similarity between all
the senses (from WordNet) of these verbs (Equa-
tion 1). We compute the similarity between two
senses using linear similarity (Lin, 1998). The
similarity between two synsets sv1 and sv2 are pe-
nalized according to the order of each sense for the
corresponding verb. Intuitively, if a synset appears
earlier in the set of synsets of a verb, it is more
likely to be considered as the correct meaning.
Therefore, later occurrences of a synset should re-
sult in reduced similarity scores. The similarity
between two verbs v1 and v2 is the maximum sim-
ilarity between two synsets of the verbs:
</bodyText>
<equation confidence="0.9998965">
sim(v1, v2) = max lin-sim(sv1, sv2) (1)
sv:synsets(v) log(p1 + p2)
</equation>
<bodyText confidence="0.995003363636364">
where sv1, sv2 are two synsets, p1, p2 are the posi-
tion of each synset match, and lin-sim is the linear
similarity. Our experiments show better perfor-
mance using linear similarity compared to other
common similarity metrics (e.g., WordNet path
similarity and Resnik similarity (Resnik, 1995)).
WordNet-based Features: We use WordNet
verb categories in the feature vector. For each
part of speech in WordNet, the synsets are or-
ganized into different categories. There are
15 categories for verbs. Some examples in-
</bodyText>
<page confidence="0.99523">
528
</page>
<bodyText confidence="0.9998777">
clude “verb.communication”, “verb.possession”,
and “verb.creation”. In addition, WordNet in-
cludes the frequency measure f,sv indicating how
often the sense sv has appeared in a reference cor-
pus. For each category i, we define the feature fz
as the ratio of the frequency of the sense svz over
the total frequency of the verb i.e., fz = f,svi /f,v.
Structural Features: For structural features, we
use the dependency relations between the verb and
the sentence elements since they can be a good
proxy of the sentence structure. ARIS uses a bi-
nary vector including 35 dependency relations be-
tween the verb and other elements. For example,
in the sentence “Joan picked 2 apples from the ap-
ple tree”, the dependency between (‘picked’ and
‘tree’) and (‘picked’ and ‘apples’) are depicted as
‘prep-from’ and ‘dobj’ relations in the dependency
parser, respectively. In addition, we include the
length of the path in the dependency parse from
the entity to the verb.
</bodyText>
<subsectionHeader confidence="0.999905">
4.3 Solving the Problem
</subsectionHeader>
<bodyText confidence="0.999621">
So far, ARIS grounds every problem into entities,
containers, and attributes, and learns verb cate-
gories in sentences. Solving the problem consists
of two main steps: (1) progressing states based on
verb categories in sentences and (2) forming the
equation.
</bodyText>
<subsubsectionHeader confidence="0.678466">
4.3.1 State Progression with Verb Categories
</subsubsectionHeader>
<bodyText confidence="0.999835479166667">
This step (Figure 3 step 3b) involves forming
states (s1, ... , sT) by updating quantities in every
container using learned verb categories (Figure 3
step 3a). ARIS initializes s0 to an empty state. It
then iteratively updates the state st by progressing
the state st−1 given the sentence wt with the verb
v, entity e, number num, and containers c1 and c2.
For a given sentence t, ARIS attempts to match
et and ct to entities and categories in st−1. An
entity/category is matched if has the same head
word and same set of attributes as an existing en-
tity/category. If an entity or category cannot be
matching to one in st−1, then a new one is created
in st.
The progress subroutine prunes the irrelevant
sentences by checking if the entity e and its at-
tributes a agree with the question entity ex and its
attributes ax in the question. For example both
game entities agree with the question entity in the
problem “Joan went to 4 football games this year.
She went to 9 games last year. How many football
games did Joan go?”. The first entity has an ex-
plicit football attribute, and the second entity
has been assigned the same attribute (Section 4.1).
Even if the question asks about games without
mentioning football, the two sentences will
match the question. Note that the second sentence
would have not been matched if there was an ex-
plicit mention of the ‘basketball game’ in the sec-
ond sentence.
For the matched entities, ARIS initializes or up-
dates the values of the containers c1, c2 in the state
st. ARIS uses the learned verb categories in sen-
tences (Section 4.2) to update the values of con-
tainers. For an observation sentence wt, the value
of c1 in the state st is assigned to the observed
quantity num. For other sentence types, if the
container c does not match to a container the pre-
vious state, its value is initialized with a start vari-
able C0. For example, the container Joan is ini-
tialized with J0 at the state s1 (Figure 2). Other-
wise, the values of c1 and c2 are updated according
to the verb category in the sentence. For instance,
if the verb category in the sentence is a positive
transfer then Nt(c1, e) = Nt−1(c1, e) − num and
Nt(c2, e) = Nt−1(c2, e) + num where Nt(c, e)
represents the quantity of e in the container c at
state st (Figure 2).
</bodyText>
<subsectionHeader confidence="0.984639">
4.3.2 Forming Equations and Solution
</subsectionHeader>
<bodyText confidence="0.999962260869565">
The question entity ex can match either to an en-
tity in the final state, or to some unknown gener-
ated during the state progression. Concretely, the
question sentence wx asks about the quantity x of
the entity ex in a container cx at a particular state
su or a transition after the sentence wu (Figure 3
step 3c).
To determine if ex matches to an unknown vari-
able, we define a matching subroutine between
the question sentence wx and every sentence wt
to check entities, containers, and verbs (Figure 3
step 3(c)i). We consider two cases. 1) When
wx contains the words “begin”, or “start”, the un-
known variable is about the initial value of an en-
tity, and it is set to the start variable of the con-
tainer cx (Figure 3 step 3(c)iii). For example, in
“Bob had balloons. He gave 9 to his friends. He
now has 4 balloons. How many balloons did he
have to start with?”, the unknown variable is set to
the start variable B0. 2) When the question verb
is not one of the defined set of observation verbs,
ARIS attempts to match ex with an unknown in-
troduced by one of the state transitions (Figure 3
</bodyText>
<page confidence="0.996509">
529
</page>
<bodyText confidence="0.999576037037037">
step 3(c)iii). For example, the second sentence
in Figure 1 introduces an unknown variable over
kittens. The matching subroutine matches this
entity with the question entity since the question
container, i.e. Joan, matches with the second
container and verb categories are complementary.
In order to solve for the unknown variable x,
ARIS searches through consecutive states st and
st+1, where in st, the quantity of ex for a container
c is an expression over x, and in st+1, the quan-
tity is a known number for a container matched
to c. It then forms an equation by comparing the
quantities for containers matched between the two
states. In the previous example, the equation will
be B0 − 9 = 4 by comparing states s2 and s3,
where the numerical expression over balloons
is B0−9 in the state s2, and the quantity is a known
number in the state s3.
When neither of the two above cases apply,
ARIS matches ex to an entity in the final state,
sT and returns its quantity, (Figure 3 step 3(c)iv).
In the football example of the previous sec-
tion, the equation will be x = Nt(cx, ex), where
Nt(cx, ex) is the quantity in the final state.
Finally, the equation will be solved for the un-
known variable x and the absolute value of the un-
known variable is returned.
</bodyText>
<sectionHeader confidence="0.998687" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.9999796">
To experimentally evaluate our method we build
a dataset of arithmetic word problems along with
their correct solutions. We test our method on the
accuracy of solving arithmetic word problems and
identifying verb categories in sentences.
</bodyText>
<subsectionHeader confidence="0.99325">
5.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.991733466666667">
Datasets: We compiled three diverse datasets
MA1, MA2, IXL (Table 2) of Arithmetic word
problems on addition and subtraction for third,
fourth, and fifth graders. These datasets have sim-
ilar problem types, but have different characteris-
tics. Problem types include combinations of ad-
ditions, subtractions, one unknown equations, and
U.S. money word problems. Problems in MA2 in-
clude more irrelevant information compared to the
other two datasets, and IXL includes more infor-
mation gaps. In total, they include 395 problems,
13,632 words, 118 verbs, and 1,483 sentences.
Tasks and Baselines: We evaluate ARIS on two
tasks: 1) solving arithmetic word problems in the
three datasets and 2) classifying verb categories in
</bodyText>
<table confidence="0.99726975">
Source #Tests Avg.# Sentences
MA1 math-aids.com 134 3.5
IXL ixl.com 140 3.36
MA2 math-aids.com 121 4.48
</table>
<tableCaption confidence="0.962393">
Table 2: Properties of the datasets.
</tableCaption>
<table confidence="0.99995">
MA1 IXL MA2 Total
3-fold Cross validation
ARIS 83.6 75.0 74.4 77.7
ARIS2 83.9 75.4+ 69.8+ 76.5+
KAZB 89.6 51.1 51.2 64.0
Majority 45.5 71.4 23.7 48.9
Gold sentence categorization
Gold ARIS 94.0 77.1 81.0 84.0
</table>
<tableCaption confidence="0.873756777777778">
Table 3: Accuracy of solving arithmetic word problems in
three datasets MA1, IXL, and MA2. This table compares
our method, ARIS, ARIS2 with the state-of-the-art KAZB. All
methods are trained on two (out of three) datasets and tested
on the other one. ARIS2 is trained when no verb is repeated
in the training and test sets. Gold ARIS uses gold verb cat-
egories. The improvement of ARIS (boldfaced) and ARIS2
(denoted by +) are significant over KAZB and the majority
baseline with P &lt; 0.05.
</tableCaption>
<bodyText confidence="0.999813736842106">
sentences. We use the percentage of correct an-
swers to the problems as the evaluation metric for
the first task and accuracy as the evaluation metric
for the second task. We use Weka’s SVM (Wit-
ten et al., 1999) with default parameters for clas-
sification which is trained with verb categories in
sentences (as described in Section 4.2).
For the first task, we compare ARIS with
KAZB (Kushman et al., 2014), majority baseline,
ARIS2, and Gold ARIS. KAZB requires training
data in the form of equation systems and numeri-
cal answers to the problems. The majority base-
line classifies every instance as increasing. In
ARIS2 (a variant of ARIS) the system is trained in
a way that no verb is repeated in the training and
test sets. Gold ARIS uses the ground-truth sen-
tence categories instead of predicted ones. For the
second task, we compare ARIS with a baseline that
uses WordNet verb senses.
</bodyText>
<subsectionHeader confidence="0.916829">
5.2 Results
</subsectionHeader>
<bodyText confidence="0.999977666666667">
We evaluate ARIS in solving arithmetic word
problems in the three datasets and then evaluate its
ability in classifying verb categories in sentences.
</bodyText>
<subsectionHeader confidence="0.988886">
5.2.1 Solving Arithmetic Problems
</subsectionHeader>
<bodyText confidence="0.999843333333333">
Table 3 shows the accuracy of ARIS in solv-
ing problems in each dataset (when trained on
the other two datasets).Table 3 shows that ARIS
</bodyText>
<page confidence="0.988005">
530
</page>
<bodyText confidence="0.999964595744681">
significantly outperforms KAZB and the major-
ity baseline. As expected, ARIS shows a larger
gain on the two more complex datasets MA2 and
IXL; our method shows promising results in deal-
ing with irrelevant information (dataset MA2) and
information gaps (dataset IXL). This is because
ARIS learns to classify verb categories in sen-
tences and does not require observing similar pat-
terns/templates in the training data. Therefore,
ARIS is more robust to differences between the
training and test datasets and can generalize across
different dataset types. As discussed in the ex-
perimental setup, the datasets have mathematically
similar problems, but differ in the natural language
properties such as in the sentence length and irrel-
evant information (Table 2).
Table 3 also shows that the sentence categoriza-
tion is performed with high accuracy even if the
problem types and also the verbs are different. In
particular, there are a total of 118 verbs among
which 64 verbs belong to MA datasets and 54 are
new to IXL. To further study this, we train our
method ARIS2 in which no verb can be repeated
in the training and test sets. ARIS2 still signifi-
cantly outperforms KAZB. In addition, we observe
only a slight change in accuracy between ARIS
and ARIS2.
To further understand our method, we study the
effect of verb categorization in sentences in solv-
ing problems. Table 3 shows the results of Gold
ARIS in solving arithmetic word problems with
gold sentence categorizations. In addition, com-
paring ARIS with Gold ARIS suggests that our
method is able to reliably identify verb categories
in sentences.
We also perform an experiment where we pool
all of the problems in the three datasets and
randomly choose 3 folds for the data (instead
of putting each original dataset into it’s own
fold). We compare our method with KAZBin
this scenario. In this setting, our method’s accu-
racy is 79.5% while KAZB’s accuracy is 81.8%.
As expected, our method’s performance has not
changed significantly from the previous setting,
while KAZB’s performance significantly improves
because of the reduced diversity between the train-
ing and test sets in this scenario.
</bodyText>
<subsectionHeader confidence="0.477119">
5.2.2 Sentence Categorization
</subsectionHeader>
<bodyText confidence="0.999058674418604">
Table 4 compares accuracy scores of sentence
categorization for our method with different fea-
tures, a baseline that uses WordNet verb senses,
and the majority baseline that assigns every (verb,
container) pair as increasing. Similar to ARIS2,
we randomly split verbs into three equal folds
and assign the corresponding sentences to each
fold. No verb is shared between training and test
sets. We then directly evaluate the accuracy of
the SVM’s verb categorization (explained in Sec-
tion 4.2). This table shows that ARIS performs
well in classifying sentence categories even with
new verbs in the test set. This suggests that our
method can generalize well to predict verb cate-
gories for unseen verbs.
Table 4 also details the performance of four
variants of our method that ablate various features
of ARIS. The table shows that similarity, contex-
tual, and WordNet features are all important to
the performance of ARIS in verb categorization,
whereas the WordNet features are less important
for solving the problems. In addition, it shows that
similarity features play more important roles. We
also performed another experiment to study the ef-
fect of the proposed feature selection method for
similarity-based features. The accuracy of ARIS
in classifying sentence categories is 69.7% when
we use all the verbs in V in the similarity feature
vector. This shows that our feature selection algo-
rithm for selecting seed verbs is important towards
categorizing verbs.
Finally, Table 4 shows that our method signif-
icantly outperforms the baseline that only uses
WordNet verb sense. An interesting observation
is that the majority baseline in fact outperforms
WordNet verb senses in verb categorization, but
is significantly worse in solving arithmetic word
problems. In addition, we evaluate the accuracy
of predicting only verb categories by assigning the
verb label according to the majority of its labels
in the sentence categories. The accuracy of verb
categories is 78.2% confirming that ARIS is able
to successfully categorize verbs.
</bodyText>
<subsectionHeader confidence="0.446228">
5.2.3 Error Analysis
</subsectionHeader>
<bodyText confidence="0.9999733">
We analyzed all 63 errors of Gold ARIS and
present our findings in Table 5. There are five ma-
jor classes of errors. In the first category, some in-
formation is not mentioned explicitly and should
be entailed. For example, ‘washing cars’ is the
source of ‘making money’. Despite the improve-
ments that come from ARIS, a large portion of the
errors can still be attributed to irrelevant informa-
tion. For example, ‘short’ is not a ‘toy’. The third
category refers to errors that require knowledge
</bodyText>
<page confidence="0.991654">
531
</page>
<table confidence="0.999790142857143">
Categorization Solution
ARIS 81.2+ 76.5+
No similarity features 68.8 65.4
No WordNet features 75.3 78.0+
No structural features 75.5 72.4+
Baseline (WordNet) 67.8 68.4
Majority Baseline 73.4 48.9
</table>
<tableCaption confidence="0.975747428571429">
Table 4: Ablation study and baseline comparisons: this ta-
ble reports the accuracy of verb categorization in sentences
and solutions for ARIS with ablating features. It also pro-
vides comparisons to WordNet and majority baselines. The
improvement of ARIS (boldfaced) and ablations denoted by +
are statistically significant over the baselines (with p &lt; 0.05)
for both tasks.
</tableCaption>
<table confidence="0.999342705882353">
Error type Example
Entailment, Last week Tom had $74. He washed cars
Implicit over the weekend and now has $86. How
Action (26%) much money did he make washing cars?
Irrelevant Tom bought a skateboard for $9.46, and
Information spent $9.56 on marbles. Tom also spent
(19%) $14.50 on shorts. In total, how much did
Tom spend on toys?
Set Comple- Sara’s school played 12 games this year.
tion (13%) They won 4 games. How many games did
they lose?
Parsing Sally had 27 Pokemon cards. Dan gave
Issues (21%) her 41 new Pokemon cards. How many
Pokemon cards does Sally have now?
Others (21%) In March it rained 0.81 inches. It rained
0.35 inches less in April than in March.
How much did it rain in April?
</table>
<tableCaption confidence="0.99118">
Table 5: Examples of different error categories and relative
frequencies. The cause of error is bolded.
</tableCaption>
<bodyText confidence="0.9991245">
about set completions. For example, the ‘played’
games can be split into ‘win’ and ‘lost’ games.
Finally, parsing and coreference mistakes are an-
other source of errors for ARIS.
</bodyText>
<sectionHeader confidence="0.994858" genericHeader="conclusions">
6 Discussions and Conclusion
</sectionHeader>
<bodyText confidence="0.999653743589744">
In this paper we introduce ARIS, a method for
solving arithmetic word problems. ARIS learns
to predict verb categories in sentences using syn-
tactic and (shallow) semantic features from small,
easy-to-obtain training data. ARIS grounds the
world state into entities, sets, quantities, attributes,
and their relations and takes advantage of the cir-
cumscription assumption and successfully fills in
the information gaps. Finally, ARIS makes use
of attributes and discards irrelevant information in
the problems. Together these provide a new rep-
resentation and a learning algorithm for solving
arithmetic word problems.
This paper is one step toward building a sys-
tem that can solve any math and logic word
problem. Our empirical evaluations show that
our method outperforms a template-based learn-
ing method (developed recently by Kushman et al.
(2014)) on solving addition and subtraction prob-
lems with diversity between the training and test
sets. In particular, our method generalizes bet-
ter to data from different domains because ARIS
only relies on learning verb categories which al-
leviates the need for equation templates for arith-
metic problems. In this paper, we have focused
on addition and subtraction problems. However,
KAZB can deal with more general types of prob-
lems such as multiplication, division, and simulta-
neous equations.
We have observed a complementary behavior
between our method and that of Kushman et al.
This suggests a hybrid approach that can bene-
fit from the strengths of both methods while be-
ing applicable to more general problems while ro-
bust to the errors specific to each. In addition, we
plan to focus on incrementally collecting domain
knowledge to deal with missing information gaps.
Another possible direction is to improve parsing
and coreference resolution.
</bodyText>
<sectionHeader confidence="0.997761" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9999714">
The research was supported by the Allen Institute
for AI, and grants from the NSF (IIS-1352249)
and UW-RRF (65-2775). We thank Ben Hixon
and the anonymous reviewers for helpful com-
ments and the feedback on the work.
</bodyText>
<sectionHeader confidence="0.999261" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99900895">
Antoine Bordes, Nicolas Usunier, and Jason Weston. 2010.
Label ranking under ambiguous supervision for learning
semantic correspondences. In Proc. International Confer-
ence on Machine Learning (ICML).
SRK Branavan, Harr Chen, Luke S. Zettlemoyer, and Regina
Barzilay. 2009. Reinforcement learning for mapping in-
structions to actions. In Proc. of the Annual Meeting of the
Association for Computational Linguistics and the Inter-
national Joint Conference on Natural Language Process-
ing of the AFNLP (ACL-AFNLP).
SRK Branavan, Nate Kushman, Tao Lei, and Regina Barzi-
lay. 2012. Learning high-level planning from text. In
Proc. of the Annual Meeting of the Association for Com-
putational Linguistics (ACL).
Nathanael Chambers and Dan Jurafsky. 2009. Unsupervised
learning of narrative schemas and their participants. In
Proc. of the Annual Meeting of the Association for Com-
putational Linguistics and the International Joint Con-
ference on Natural Language Processing of the AFNLP
(ACL-AFNLP).
</reference>
<page confidence="0.970732">
532
</page>
<reference confidence="0.998476309278351">
David Chen, Joohyun Kim, and Raymond Mooney. 2010.
Training a multilingual sportscaster: Using perceptual
context to learn language. Journal of Artificial Intelli-
gence Research, 37.
Marie-Catherine de Marneffe, Bill MacCartney, and Christo-
pher D. Manning. 2006. Generating typed dependency
parses from phrase structure parses. In Proc. Language
Resources and Evaluation Conference (LREC).
Jacob Eisenstein, James Clarke, Dan Goldwasser, and Dan
Roth. 2009. Reading to learn: Constructing features
from semantic abstracts. In Proc. Conference on Empiri-
cal Methods in Natural Language Processing (EMNLP).
Edward A. Feigenbaum and Julian Feldman, editors. 1963.
Computers and Thought. McGraw Hill, New York.
Jenny Rose Finkel, Trond Grenager, and Christopher Man-
ning. 2005. Incorporating non-local information into
information extraction systems by gibbs sampling. In
Proc. of the Annual Meeting of the Association for Com-
putational Linguistics (ACL).
Ruifang Ge and Raymond J. Mooney. 2006. Discriminative
reranking for semantic parsing. In Proc. of the Annual
Meeting of the Association for Computational Linguistics
(ACL).
Dan Goldwasser and Dan Roth. 2011. Learning from natural
instructions. In Proceedings of International Joint Con-
ference on Artificial Intelligence (IJCAI).
Dan Goldwasser, Roi Reichart, James Clarke, and Dan Roth.
2011. Confidence driven unsupervised semantic parsing.
In Proc. of the Annual Meeting of the Association for Com-
putational Linguistics (ACL).
Hannaneh Hajishirzi, Julia Hockenmaier, Erik T. Mueller,
and Eyal Amir. 2011. Reasoning about robocup soccer
narratives. In Proc. Conference on Uncertainty in Artifi-
cial Intelligence (UAI).
Rohit J. Kate and Raymond J. Mooney. 2007. Learn-
ing language semantics from ambiguous supervision. In
Proc. Conference of the Association for the Advancement
of Artificial Intelligence (AAAI).
Nate Kushman and Regina Barzilay. 2013. Using seman-
tic unification to generate regular expressions from natu-
ral language. In Proceeding of the Annual Meeting of the
North American Chapter of the Association for Computa-
tional Linguistics.
Nate Kushman, Yoav Artzi, Luke Zettlemoyer, and Regina
Barzilay. 2014. Learning to automatically solve algebra
word problems. In Proc. of the Annual Meeting of the
Association for Computational Linguistics (ACL).
Iddo Lev, Bill MacCartney, Christopher D. Manning, , and
Roger Levy. 2004. Solving logic puzzles: From robust
processing to precise semantics. In Workshop on Text
Meaning and Interpretation at Association for Computa-
tional Linguistics (ACL).
Iddo Lev. 2007. Packed Computation of Exact Meaning Rep-
resentations. Ph.D. thesis, CS, Stanford University.
Percy Liang, Michael I. Jordan, and Dan Klein. 2009. Learn-
ing semantic correspondences with less supervision. In
Proc. of the Annual Meeting of the Association for Com-
putational Linguistics and the International Joint Con-
ference on Natural Language Processing of the AFNLP
(ACL-AFNLP).
Dekang Lin. 1998. An information-theoretic definition of
similarity. In Proc. International Conference on Machine
Learning (ICML).
John McCarthy. 1980. Circumscription—a form of non-
monotonic reasoning. Artificial Intelligence, 13.
George A Miller. 1995. Wordnet: a lexical database for en-
glish. Communications of the ACM, 38.
Mee Young Park and Trevor Hastie. 2007. L1-regularization
path algorithm for generalized linear models. Journal of
the Royal Statistical Society: Series B (Statistical Method-
ology), 69.
Hoifung Poon and Pedro Domingos. 2009. Unsupervised se-
mantic parsing. In Proc. Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP).
Karthik Raghunathan, Heeyoung Lee, Sudarshan Rangara-
jan, Nathanael Chambers, Mihai Surdeanu, Dan Jurafsky,
and Christopher Manning. 2010. A multi-pass sieve for
coreference resolution. In Proc. Conference on Empirical
Methods in Natural Language Processing (EMNLP).
Philip Resnik. 1995. Using information content to evaluate
semantic similarity in a taxonomy. In International joint
conference on Artificial intelligence (IJCAI).
Benjamin Snyder and Regina Barzilay. 2007. Database-text
alignment via structured multilabel classification. In Pro-
ceedings of International Joint Conference on Artificial
Intelligence (IJCAI).
Adam Vogel and Daniel Jurafsky. 2010. Learning to follow
navigational directions. In Proc. of the Annual Meeting of
the Association for Computational Linguistics (ACL).
Ian H Witten, Eibe Frank, Leonard E Trigg, Mark A Hall, Ge-
offrey Holmes, and Sally Jo Cunningham. 1999. Weka:
Practical machine learning tools and techniques with java
implementations.
Luke S. Zettlemoyer and Michael Collins. 2005. Learning
to map sentences to logical form: Structured classification
with probabilistic categorial grammars. In Proc. Confer-
ence on Uncertainty in Artificial Intelligence (UAI).
</reference>
<page confidence="0.99893">
533
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.528687">
<title confidence="0.999344">Learning to Solve Arithmetic Word Problems with Verb Categorization</title>
<author confidence="0.973271">Javad Hannaneh Oren</author>
<author confidence="0.973271">Nate</author>
<affiliation confidence="0.832849">of Washington, Institute for AI, Institute of Technology</affiliation>
<abstract confidence="0.980633105263158">This paper presents a novel approach to learning to solve simple arithmetic word Our system, analyzes each of the sentences in the problem statement to identify the relevant variables and values. maps this information into an equation that represents the problem, and enables its (trivial) solution as shown in Figure 1. The paper analyzes the arithmetic-word problems “genre”, identifying seven categories of used in such problems. to categorize verbs with 81.2% accuracy, and is able to solve 77.7% of the problems in a corpus of standard primary school test questions. We report the first learning results on this task without reliance on predefined templates and make our data pub-</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Antoine Bordes</author>
<author>Nicolas Usunier</author>
<author>Jason Weston</author>
</authors>
<title>Label ranking under ambiguous supervision for learning semantic correspondences.</title>
<date>2010</date>
<booktitle>In Proc. International Conference on Machine Learning (ICML).</booktitle>
<contexts>
<context position="6385" citStr="Bordes et al., 2010" startWordPosition="1028" endWordPosition="1031">d Barzilay, 2013) reduce the amount of required supervision in mapping sentences to meaning representations while taking advantage of special properties of the domains. Our method, on the other hand, requires small, easy-to-obtain training data in the form of verb categories that are shared among many different problem types. Our work is also closely related to the grounded language acquisition research (Snyder and Barzilay, 2007; Branavan et al., 2009; Branavan et al., 2012; Vogel and Jurafsky, 2010; Chen et al., 2010; Hajishirzi et al., 2011; Chambers and Jurafsky, 2009; Liang et al., 2009; Bordes et al., 2010) where the goal is to align a text into underlying entities and events of an environment. These methods interact with an environment to obtain supervision from the real events and entities in the environment. Our method, on the other hand, grounds the problem into world state transitions by learning to predict verb categories in sentences. In addition, our method combines the representations of individual sentences into a coherent whole to form the equations. This is in contrast with the previous work that study each sentence in isolation from the other sentences. Previous work on studying mat</context>
</contexts>
<marker>Bordes, Usunier, Weston, 2010</marker>
<rawString>Antoine Bordes, Nicolas Usunier, and Jason Weston. 2010. Label ranking under ambiguous supervision for learning semantic correspondences. In Proc. International Conference on Machine Learning (ICML).</rawString>
</citation>
<citation valid="true">
<authors>
<author>SRK Branavan</author>
<author>Harr Chen</author>
<author>Luke S Zettlemoyer</author>
<author>Regina Barzilay</author>
</authors>
<title>Reinforcement learning for mapping instructions to actions.</title>
<date>2009</date>
<booktitle>In Proc. of the Annual Meeting of the Association for Computational Linguistics and the International Joint Conference on Natural Language Processing of the AFNLP (ACL-AFNLP).</booktitle>
<contexts>
<context position="6221" citStr="Branavan et al., 2009" startWordPosition="1000" endWordPosition="1003">ining data. More recent work (Eisenstein et al., 2009; Kate and Mooney, 2007; Goldwasser and Roth, 2011; Poon and Domingos, 2009; Goldwasser et al., 2011; Kushman and Barzilay, 2013) reduce the amount of required supervision in mapping sentences to meaning representations while taking advantage of special properties of the domains. Our method, on the other hand, requires small, easy-to-obtain training data in the form of verb categories that are shared among many different problem types. Our work is also closely related to the grounded language acquisition research (Snyder and Barzilay, 2007; Branavan et al., 2009; Branavan et al., 2012; Vogel and Jurafsky, 2010; Chen et al., 2010; Hajishirzi et al., 2011; Chambers and Jurafsky, 2009; Liang et al., 2009; Bordes et al., 2010) where the goal is to align a text into underlying entities and events of an environment. These methods interact with an environment to obtain supervision from the real events and entities in the environment. Our method, on the other hand, grounds the problem into world state transitions by learning to predict verb categories in sentences. In addition, our method combines the representations of individual sentences into a coherent w</context>
</contexts>
<marker>Branavan, Chen, Zettlemoyer, Barzilay, 2009</marker>
<rawString>SRK Branavan, Harr Chen, Luke S. Zettlemoyer, and Regina Barzilay. 2009. Reinforcement learning for mapping instructions to actions. In Proc. of the Annual Meeting of the Association for Computational Linguistics and the International Joint Conference on Natural Language Processing of the AFNLP (ACL-AFNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>SRK Branavan</author>
<author>Nate Kushman</author>
<author>Tao Lei</author>
<author>Regina Barzilay</author>
</authors>
<title>Learning high-level planning from text.</title>
<date>2012</date>
<booktitle>In Proc. of the Annual Meeting of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="6244" citStr="Branavan et al., 2012" startWordPosition="1004" endWordPosition="1007"> work (Eisenstein et al., 2009; Kate and Mooney, 2007; Goldwasser and Roth, 2011; Poon and Domingos, 2009; Goldwasser et al., 2011; Kushman and Barzilay, 2013) reduce the amount of required supervision in mapping sentences to meaning representations while taking advantage of special properties of the domains. Our method, on the other hand, requires small, easy-to-obtain training data in the form of verb categories that are shared among many different problem types. Our work is also closely related to the grounded language acquisition research (Snyder and Barzilay, 2007; Branavan et al., 2009; Branavan et al., 2012; Vogel and Jurafsky, 2010; Chen et al., 2010; Hajishirzi et al., 2011; Chambers and Jurafsky, 2009; Liang et al., 2009; Bordes et al., 2010) where the goal is to align a text into underlying entities and events of an environment. These methods interact with an environment to obtain supervision from the real events and entities in the environment. Our method, on the other hand, grounds the problem into world state transitions by learning to predict verb categories in sentences. In addition, our method combines the representations of individual sentences into a coherent whole to form the equati</context>
</contexts>
<marker>Branavan, Kushman, Lei, Barzilay, 2012</marker>
<rawString>SRK Branavan, Nate Kushman, Tao Lei, and Regina Barzilay. 2012. Learning high-level planning from text. In Proc. of the Annual Meeting of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nathanael Chambers</author>
<author>Dan Jurafsky</author>
</authors>
<title>Unsupervised learning of narrative schemas and their participants.</title>
<date>2009</date>
<booktitle>In Proc. of the Annual Meeting of the Association for Computational Linguistics and the International Joint Conference on Natural Language Processing of the AFNLP (ACL-AFNLP).</booktitle>
<contexts>
<context position="6343" citStr="Chambers and Jurafsky, 2009" startWordPosition="1020" endWordPosition="1023">mingos, 2009; Goldwasser et al., 2011; Kushman and Barzilay, 2013) reduce the amount of required supervision in mapping sentences to meaning representations while taking advantage of special properties of the domains. Our method, on the other hand, requires small, easy-to-obtain training data in the form of verb categories that are shared among many different problem types. Our work is also closely related to the grounded language acquisition research (Snyder and Barzilay, 2007; Branavan et al., 2009; Branavan et al., 2012; Vogel and Jurafsky, 2010; Chen et al., 2010; Hajishirzi et al., 2011; Chambers and Jurafsky, 2009; Liang et al., 2009; Bordes et al., 2010) where the goal is to align a text into underlying entities and events of an environment. These methods interact with an environment to obtain supervision from the real events and entities in the environment. Our method, on the other hand, grounds the problem into world state transitions by learning to predict verb categories in sentences. In addition, our method combines the representations of individual sentences into a coherent whole to form the equations. This is in contrast with the previous work that study each sentence in isolation from the othe</context>
</contexts>
<marker>Chambers, Jurafsky, 2009</marker>
<rawString>Nathanael Chambers and Dan Jurafsky. 2009. Unsupervised learning of narrative schemas and their participants. In Proc. of the Annual Meeting of the Association for Computational Linguistics and the International Joint Conference on Natural Language Processing of the AFNLP (ACL-AFNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chen</author>
<author>Joohyun Kim</author>
<author>Raymond Mooney</author>
</authors>
<title>Training a multilingual sportscaster: Using perceptual context to learn language.</title>
<date>2010</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<volume>37</volume>
<contexts>
<context position="6289" citStr="Chen et al., 2010" startWordPosition="1012" endWordPosition="1015">2007; Goldwasser and Roth, 2011; Poon and Domingos, 2009; Goldwasser et al., 2011; Kushman and Barzilay, 2013) reduce the amount of required supervision in mapping sentences to meaning representations while taking advantage of special properties of the domains. Our method, on the other hand, requires small, easy-to-obtain training data in the form of verb categories that are shared among many different problem types. Our work is also closely related to the grounded language acquisition research (Snyder and Barzilay, 2007; Branavan et al., 2009; Branavan et al., 2012; Vogel and Jurafsky, 2010; Chen et al., 2010; Hajishirzi et al., 2011; Chambers and Jurafsky, 2009; Liang et al., 2009; Bordes et al., 2010) where the goal is to align a text into underlying entities and events of an environment. These methods interact with an environment to obtain supervision from the real events and entities in the environment. Our method, on the other hand, grounds the problem into world state transitions by learning to predict verb categories in sentences. In addition, our method combines the representations of individual sentences into a coherent whole to form the equations. This is in contrast with the previous wo</context>
</contexts>
<marker>Chen, Kim, Mooney, 2010</marker>
<rawString>David Chen, Joohyun Kim, and Raymond Mooney. 2010. Training a multilingual sportscaster: Using perceptual context to learn language. Journal of Artificial Intelligence Research, 37.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Bill MacCartney</author>
<author>Christopher D Manning</author>
</authors>
<title>Generating typed dependency parses from phrase structure parses.</title>
<date>2006</date>
<booktitle>In Proc. Language Resources and Evaluation Conference (LREC).</booktitle>
<marker>de Marneffe, MacCartney, Manning, 2006</marker>
<rawString>Marie-Catherine de Marneffe, Bill MacCartney, and Christopher D. Manning. 2006. Generating typed dependency parses from phrase structure parses. In Proc. Language Resources and Evaluation Conference (LREC).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Eisenstein</author>
<author>James Clarke</author>
<author>Dan Goldwasser</author>
<author>Dan Roth</author>
</authors>
<title>Reading to learn: Constructing features from semantic abstracts.</title>
<date>2009</date>
<booktitle>In Proc. Conference on Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="5653" citStr="Eisenstein et al., 2009" startWordPosition="913" endWordPosition="916">ms, and report on a series of experiments showing high efficacy in solving addition and subtraction problems based on verb categorization. 2 Related Work Understanding semantics of a natural language text has been the focus of many researchers in natural language processing (NLP). Recent work focus on learning to align text with meaning representations in specific, controlled domains. A few methods (Zettlemoyer and Collins, 2005; Ge and Mooney, 2006) use an expensive supervision in the form of manually annotated formal representations for every sentence in the training data. More recent work (Eisenstein et al., 2009; Kate and Mooney, 2007; Goldwasser and Roth, 2011; Poon and Domingos, 2009; Goldwasser et al., 2011; Kushman and Barzilay, 2013) reduce the amount of required supervision in mapping sentences to meaning representations while taking advantage of special properties of the domains. Our method, on the other hand, requires small, easy-to-obtain training data in the form of verb categories that are shared among many different problem types. Our work is also closely related to the grounded language acquisition research (Snyder and Barzilay, 2007; Branavan et al., 2009; Branavan et al., 2012; Vogel a</context>
</contexts>
<marker>Eisenstein, Clarke, Goldwasser, Roth, 2009</marker>
<rawString>Jacob Eisenstein, James Clarke, Dan Goldwasser, and Dan Roth. 2009. Reading to learn: Constructing features from semantic abstracts. In Proc. Conference on Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="false">
<authors>
<author>Edward A Feigenbaum</author>
<author>Julian Feldman</author>
</authors>
<publisher>McGraw Hill,</publisher>
<location>New York.</location>
<marker>Feigenbaum, Feldman, </marker>
<rawString>Edward A. Feigenbaum and Julian Feldman, editors. 1963. Computers and Thought. McGraw Hill, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Trond Grenager</author>
<author>Christopher Manning</author>
</authors>
<title>Incorporating non-local information into information extraction systems by gibbs sampling.</title>
<date>2005</date>
<booktitle>In Proc. of the Annual Meeting of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="14590" citStr="Finkel et al., 2005" startWordPosition="2437" endWordPosition="2440">ontainers ARIS automatically identifies entities, attributes, containers, and quantities corresponding to every sentence fragment (details in Figure 3 step 1). For every problem, this module returns a sequence of sentence fragments (w1,... , wT, wx) where every wt consists of a verb vt, an entity et, its quantity numt, its attributes at, and up to two containers ct1, ct2. wx corresponds to the question sentence inquiring about an unknown entity. ARIS applies the Stanford dependency parser, named entity recognizer and coreference resolution system to the problem text (de Marneffe et al., 2006; Finkel et al., 2005; Raghunathan et al., 2010). It uses the predicted coreference relationships to replace pronouns (including possessive pronouns) with their coreferenent links. The named entity recognition output is used to identify numbers and people. Entities: Entities are references to some object whose quantity is observed or changing throughout the problem. So to determine the set of entities, we define h as the set of noun types which have a dependent number (in the dependency parse) somewhere in the problem text. The set of entities is then defined as all noun phrases which are headed by a noun type in </context>
</contexts>
<marker>Finkel, Grenager, Manning, 2005</marker>
<rawString>Jenny Rose Finkel, Trond Grenager, and Christopher Manning. 2005. Incorporating non-local information into information extraction systems by gibbs sampling. In Proc. of the Annual Meeting of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruifang Ge</author>
<author>Raymond J Mooney</author>
</authors>
<title>Discriminative reranking for semantic parsing.</title>
<date>2006</date>
<booktitle>In Proc. of the Annual Meeting of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="5484" citStr="Ge and Mooney, 2006" startWordPosition="886" endWordPosition="889">sy-to-obtain training data; our results refine verb senses in WordNet (Miller, 1995) for arithmetic word problems; (c) We introduce a corpus of arithmetic word problems, and report on a series of experiments showing high efficacy in solving addition and subtraction problems based on verb categorization. 2 Related Work Understanding semantics of a natural language text has been the focus of many researchers in natural language processing (NLP). Recent work focus on learning to align text with meaning representations in specific, controlled domains. A few methods (Zettlemoyer and Collins, 2005; Ge and Mooney, 2006) use an expensive supervision in the form of manually annotated formal representations for every sentence in the training data. More recent work (Eisenstein et al., 2009; Kate and Mooney, 2007; Goldwasser and Roth, 2011; Poon and Domingos, 2009; Goldwasser et al., 2011; Kushman and Barzilay, 2013) reduce the amount of required supervision in mapping sentences to meaning representations while taking advantage of special properties of the domains. Our method, on the other hand, requires small, easy-to-obtain training data in the form of verb categories that are shared among many different proble</context>
</contexts>
<marker>Ge, Mooney, 2006</marker>
<rawString>Ruifang Ge and Raymond J. Mooney. 2006. Discriminative reranking for semantic parsing. In Proc. of the Annual Meeting of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Goldwasser</author>
<author>Dan Roth</author>
</authors>
<title>Learning from natural instructions.</title>
<date>2011</date>
<booktitle>In Proceedings of International Joint Conference on Artificial Intelligence (IJCAI).</booktitle>
<contexts>
<context position="5703" citStr="Goldwasser and Roth, 2011" startWordPosition="921" endWordPosition="924">g high efficacy in solving addition and subtraction problems based on verb categorization. 2 Related Work Understanding semantics of a natural language text has been the focus of many researchers in natural language processing (NLP). Recent work focus on learning to align text with meaning representations in specific, controlled domains. A few methods (Zettlemoyer and Collins, 2005; Ge and Mooney, 2006) use an expensive supervision in the form of manually annotated formal representations for every sentence in the training data. More recent work (Eisenstein et al., 2009; Kate and Mooney, 2007; Goldwasser and Roth, 2011; Poon and Domingos, 2009; Goldwasser et al., 2011; Kushman and Barzilay, 2013) reduce the amount of required supervision in mapping sentences to meaning representations while taking advantage of special properties of the domains. Our method, on the other hand, requires small, easy-to-obtain training data in the form of verb categories that are shared among many different problem types. Our work is also closely related to the grounded language acquisition research (Snyder and Barzilay, 2007; Branavan et al., 2009; Branavan et al., 2012; Vogel and Jurafsky, 2010; Chen et al., 2010; Hajishirzi e</context>
</contexts>
<marker>Goldwasser, Roth, 2011</marker>
<rawString>Dan Goldwasser and Dan Roth. 2011. Learning from natural instructions. In Proceedings of International Joint Conference on Artificial Intelligence (IJCAI).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Goldwasser</author>
<author>Roi Reichart</author>
<author>James Clarke</author>
<author>Dan Roth</author>
</authors>
<title>Confidence driven unsupervised semantic parsing.</title>
<date>2011</date>
<booktitle>In Proc. of the Annual Meeting of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="5753" citStr="Goldwasser et al., 2011" startWordPosition="929" endWordPosition="932">problems based on verb categorization. 2 Related Work Understanding semantics of a natural language text has been the focus of many researchers in natural language processing (NLP). Recent work focus on learning to align text with meaning representations in specific, controlled domains. A few methods (Zettlemoyer and Collins, 2005; Ge and Mooney, 2006) use an expensive supervision in the form of manually annotated formal representations for every sentence in the training data. More recent work (Eisenstein et al., 2009; Kate and Mooney, 2007; Goldwasser and Roth, 2011; Poon and Domingos, 2009; Goldwasser et al., 2011; Kushman and Barzilay, 2013) reduce the amount of required supervision in mapping sentences to meaning representations while taking advantage of special properties of the domains. Our method, on the other hand, requires small, easy-to-obtain training data in the form of verb categories that are shared among many different problem types. Our work is also closely related to the grounded language acquisition research (Snyder and Barzilay, 2007; Branavan et al., 2009; Branavan et al., 2012; Vogel and Jurafsky, 2010; Chen et al., 2010; Hajishirzi et al., 2011; Chambers and Jurafsky, 2009; Liang et</context>
</contexts>
<marker>Goldwasser, Reichart, Clarke, Roth, 2011</marker>
<rawString>Dan Goldwasser, Roi Reichart, James Clarke, and Dan Roth. 2011. Confidence driven unsupervised semantic parsing. In Proc. of the Annual Meeting of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hannaneh Hajishirzi</author>
<author>Julia Hockenmaier</author>
<author>Erik T Mueller</author>
<author>Eyal Amir</author>
</authors>
<title>Reasoning about robocup soccer narratives.</title>
<date>2011</date>
<booktitle>In Proc. Conference on Uncertainty in Artificial Intelligence (UAI).</booktitle>
<contexts>
<context position="6314" citStr="Hajishirzi et al., 2011" startWordPosition="1016" endWordPosition="1019">d Roth, 2011; Poon and Domingos, 2009; Goldwasser et al., 2011; Kushman and Barzilay, 2013) reduce the amount of required supervision in mapping sentences to meaning representations while taking advantage of special properties of the domains. Our method, on the other hand, requires small, easy-to-obtain training data in the form of verb categories that are shared among many different problem types. Our work is also closely related to the grounded language acquisition research (Snyder and Barzilay, 2007; Branavan et al., 2009; Branavan et al., 2012; Vogel and Jurafsky, 2010; Chen et al., 2010; Hajishirzi et al., 2011; Chambers and Jurafsky, 2009; Liang et al., 2009; Bordes et al., 2010) where the goal is to align a text into underlying entities and events of an environment. These methods interact with an environment to obtain supervision from the real events and entities in the environment. Our method, on the other hand, grounds the problem into world state transitions by learning to predict verb categories in sentences. In addition, our method combines the representations of individual sentences into a coherent whole to form the equations. This is in contrast with the previous work that study each senten</context>
</contexts>
<marker>Hajishirzi, Hockenmaier, Mueller, Amir, 2011</marker>
<rawString>Hannaneh Hajishirzi, Julia Hockenmaier, Erik T. Mueller, and Eyal Amir. 2011. Reasoning about robocup soccer narratives. In Proc. Conference on Uncertainty in Artificial Intelligence (UAI).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rohit J Kate</author>
<author>Raymond J Mooney</author>
</authors>
<title>Learning language semantics from ambiguous supervision.</title>
<date>2007</date>
<booktitle>In Proc. Conference of the Association for the Advancement of Artificial Intelligence (AAAI).</booktitle>
<contexts>
<context position="5676" citStr="Kate and Mooney, 2007" startWordPosition="917" endWordPosition="920">s of experiments showing high efficacy in solving addition and subtraction problems based on verb categorization. 2 Related Work Understanding semantics of a natural language text has been the focus of many researchers in natural language processing (NLP). Recent work focus on learning to align text with meaning representations in specific, controlled domains. A few methods (Zettlemoyer and Collins, 2005; Ge and Mooney, 2006) use an expensive supervision in the form of manually annotated formal representations for every sentence in the training data. More recent work (Eisenstein et al., 2009; Kate and Mooney, 2007; Goldwasser and Roth, 2011; Poon and Domingos, 2009; Goldwasser et al., 2011; Kushman and Barzilay, 2013) reduce the amount of required supervision in mapping sentences to meaning representations while taking advantage of special properties of the domains. Our method, on the other hand, requires small, easy-to-obtain training data in the form of verb categories that are shared among many different problem types. Our work is also closely related to the grounded language acquisition research (Snyder and Barzilay, 2007; Branavan et al., 2009; Branavan et al., 2012; Vogel and Jurafsky, 2010; Chen</context>
</contexts>
<marker>Kate, Mooney, 2007</marker>
<rawString>Rohit J. Kate and Raymond J. Mooney. 2007. Learning language semantics from ambiguous supervision. In Proc. Conference of the Association for the Advancement of Artificial Intelligence (AAAI).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nate Kushman</author>
<author>Regina Barzilay</author>
</authors>
<title>Using semantic unification to generate regular expressions from natural language.</title>
<date>2013</date>
<booktitle>In Proceeding of the Annual Meeting of the North American Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="5782" citStr="Kushman and Barzilay, 2013" startWordPosition="933" endWordPosition="936">tegorization. 2 Related Work Understanding semantics of a natural language text has been the focus of many researchers in natural language processing (NLP). Recent work focus on learning to align text with meaning representations in specific, controlled domains. A few methods (Zettlemoyer and Collins, 2005; Ge and Mooney, 2006) use an expensive supervision in the form of manually annotated formal representations for every sentence in the training data. More recent work (Eisenstein et al., 2009; Kate and Mooney, 2007; Goldwasser and Roth, 2011; Poon and Domingos, 2009; Goldwasser et al., 2011; Kushman and Barzilay, 2013) reduce the amount of required supervision in mapping sentences to meaning representations while taking advantage of special properties of the domains. Our method, on the other hand, requires small, easy-to-obtain training data in the form of verb categories that are shared among many different problem types. Our work is also closely related to the grounded language acquisition research (Snyder and Barzilay, 2007; Branavan et al., 2009; Branavan et al., 2012; Vogel and Jurafsky, 2010; Chen et al., 2010; Hajishirzi et al., 2011; Chambers and Jurafsky, 2009; Liang et al., 2009; Bordes et al., 20</context>
</contexts>
<marker>Kushman, Barzilay, 2013</marker>
<rawString>Nate Kushman and Regina Barzilay. 2013. Using semantic unification to generate regular expressions from natural language. In Proceeding of the Annual Meeting of the North American Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nate Kushman</author>
<author>Yoav Artzi</author>
<author>Luke Zettlemoyer</author>
<author>Regina Barzilay</author>
</authors>
<title>Learning to automatically solve algebra word problems.</title>
<date>2014</date>
<booktitle>In Proc. of the Annual Meeting of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="4537" citStr="Kushman et al. (2014)" startWordPosition="732" endWordPosition="735">he verb in each sentence is associated with one or two containers, and ARIS has to classify each verb in a sentence into one of seven categories that describe the impact of the verb on the containers (Table 1). ARIS learns this classifier based on training data as described in section 4.2. To evaluate ARIS, we compiled a corpus of about 400 arithmetic (addition and subtraction) word problems and utilized cross validation to both train ARIS and evaluate its performance over this corpus. We compare its performance to the template-based learning method developed independently and concurrently by Kushman et al. (2014). We find that our approach is much more robust to domain diversity between the training and test sets. Our contributions are three-fold: (a) We present ARIS, a novel, fully automated method that learns to solve arithmetic word problems; (b) We introduce a method to automatically categorize verbs for sentences from simple, easy-to-obtain training data; our results refine verb senses in WordNet (Miller, 1995) for arithmetic word problems; (c) We introduce a corpus of arithmetic word problems, and report on a series of experiments showing high efficacy in solving addition and subtraction problem</context>
<context position="7194" citStr="Kushman et al. (2014)" startWordPosition="1165" endWordPosition="1168"> in the environment. Our method, on the other hand, grounds the problem into world state transitions by learning to predict verb categories in sentences. In addition, our method combines the representations of individual sentences into a coherent whole to form the equations. This is in contrast with the previous work that study each sentence in isolation from the other sentences. Previous work on studying math word and logic problems uses manually aligned meaning representations or domain knowledge where the semantics for all the words is provided (Lev, 2007; Lev et al., 2004). Most recently, Kushman et al. (2014) introduced an algorithm that learns to align algebra problems to equations through the use of templates. This method applies to broad range of math problems, including multiplication, division, and simultaneous equations, while ARIS only handles arithmetic problems (addition and subtraction). However, our empirical results show that for the problems it handles, ARIS is much more robust to diversity in the problem types between the training and test data. 3 Arithmetic Problem Representation We address solving arithmetic word problems that include addition and subtraction. A problem text is spl</context>
<context position="34362" citStr="Kushman et al., 2014" startWordPosition="5877" endWordPosition="5880"> no verb is repeated in the training and test sets. Gold ARIS uses gold verb categories. The improvement of ARIS (boldfaced) and ARIS2 (denoted by +) are significant over KAZB and the majority baseline with P &lt; 0.05. sentences. We use the percentage of correct answers to the problems as the evaluation metric for the first task and accuracy as the evaluation metric for the second task. We use Weka’s SVM (Witten et al., 1999) with default parameters for classification which is trained with verb categories in sentences (as described in Section 4.2). For the first task, we compare ARIS with KAZB (Kushman et al., 2014), majority baseline, ARIS2, and Gold ARIS. KAZB requires training data in the form of equation systems and numerical answers to the problems. The majority baseline classifies every instance as increasing. In ARIS2 (a variant of ARIS) the system is trained in a way that no verb is repeated in the training and test sets. Gold ARIS uses the ground-truth sentence categories instead of predicted ones. For the second task, we compare ARIS with a baseline that uses WordNet verb senses. 5.2 Results We evaluate ARIS in solving arithmetic word problems in the three datasets and then evaluate its ability</context>
<context position="42339" citStr="Kushman et al. (2014)" startWordPosition="7173" endWordPosition="7176">grounds the world state into entities, sets, quantities, attributes, and their relations and takes advantage of the circumscription assumption and successfully fills in the information gaps. Finally, ARIS makes use of attributes and discards irrelevant information in the problems. Together these provide a new representation and a learning algorithm for solving arithmetic word problems. This paper is one step toward building a system that can solve any math and logic word problem. Our empirical evaluations show that our method outperforms a template-based learning method (developed recently by Kushman et al. (2014)) on solving addition and subtraction problems with diversity between the training and test sets. In particular, our method generalizes better to data from different domains because ARIS only relies on learning verb categories which alleviates the need for equation templates for arithmetic problems. In this paper, we have focused on addition and subtraction problems. However, KAZB can deal with more general types of problems such as multiplication, division, and simultaneous equations. We have observed a complementary behavior between our method and that of Kushman et al. This suggests a hybri</context>
</contexts>
<marker>Kushman, Artzi, Zettlemoyer, Barzilay, 2014</marker>
<rawString>Nate Kushman, Yoav Artzi, Luke Zettlemoyer, and Regina Barzilay. 2014. Learning to automatically solve algebra word problems. In Proc. of the Annual Meeting of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Iddo Lev</author>
<author>Bill MacCartney</author>
<author>Christopher D Manning</author>
</authors>
<title>Solving logic puzzles: From robust processing to precise semantics.</title>
<date>2004</date>
<booktitle>In Workshop on Text Meaning and Interpretation at Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="7156" citStr="Lev et al., 2004" startWordPosition="1159" endWordPosition="1162"> from the real events and entities in the environment. Our method, on the other hand, grounds the problem into world state transitions by learning to predict verb categories in sentences. In addition, our method combines the representations of individual sentences into a coherent whole to form the equations. This is in contrast with the previous work that study each sentence in isolation from the other sentences. Previous work on studying math word and logic problems uses manually aligned meaning representations or domain knowledge where the semantics for all the words is provided (Lev, 2007; Lev et al., 2004). Most recently, Kushman et al. (2014) introduced an algorithm that learns to align algebra problems to equations through the use of templates. This method applies to broad range of math problems, including multiplication, division, and simultaneous equations, while ARIS only handles arithmetic problems (addition and subtraction). However, our empirical results show that for the problems it handles, ARIS is much more robust to diversity in the problem types between the training and test data. 3 Arithmetic Problem Representation We address solving arithmetic word problems that include addition </context>
</contexts>
<marker>Lev, MacCartney, Manning, 2004</marker>
<rawString>Iddo Lev, Bill MacCartney, Christopher D. Manning, , and Roger Levy. 2004. Solving logic puzzles: From robust processing to precise semantics. In Workshop on Text Meaning and Interpretation at Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Iddo Lev</author>
</authors>
<title>Packed Computation of Exact Meaning Representations.</title>
<date>2007</date>
<tech>Ph.D. thesis,</tech>
<institution>CS, Stanford University.</institution>
<contexts>
<context position="7137" citStr="Lev, 2007" startWordPosition="1157" endWordPosition="1158">supervision from the real events and entities in the environment. Our method, on the other hand, grounds the problem into world state transitions by learning to predict verb categories in sentences. In addition, our method combines the representations of individual sentences into a coherent whole to form the equations. This is in contrast with the previous work that study each sentence in isolation from the other sentences. Previous work on studying math word and logic problems uses manually aligned meaning representations or domain knowledge where the semantics for all the words is provided (Lev, 2007; Lev et al., 2004). Most recently, Kushman et al. (2014) introduced an algorithm that learns to align algebra problems to equations through the use of templates. This method applies to broad range of math problems, including multiplication, division, and simultaneous equations, while ARIS only handles arithmetic problems (addition and subtraction). However, our empirical results show that for the problems it handles, ARIS is much more robust to diversity in the problem types between the training and test data. 3 Arithmetic Problem Representation We address solving arithmetic word problems tha</context>
</contexts>
<marker>Lev, 2007</marker>
<rawString>Iddo Lev. 2007. Packed Computation of Exact Meaning Representations. Ph.D. thesis, CS, Stanford University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Michael I Jordan</author>
<author>Dan Klein</author>
</authors>
<title>Learning semantic correspondences with less supervision.</title>
<date>2009</date>
<booktitle>In Proc. of the Annual Meeting of the Association for Computational Linguistics and the International Joint Conference on Natural Language Processing of the AFNLP (ACL-AFNLP).</booktitle>
<contexts>
<context position="6363" citStr="Liang et al., 2009" startWordPosition="1024" endWordPosition="1027">l., 2011; Kushman and Barzilay, 2013) reduce the amount of required supervision in mapping sentences to meaning representations while taking advantage of special properties of the domains. Our method, on the other hand, requires small, easy-to-obtain training data in the form of verb categories that are shared among many different problem types. Our work is also closely related to the grounded language acquisition research (Snyder and Barzilay, 2007; Branavan et al., 2009; Branavan et al., 2012; Vogel and Jurafsky, 2010; Chen et al., 2010; Hajishirzi et al., 2011; Chambers and Jurafsky, 2009; Liang et al., 2009; Bordes et al., 2010) where the goal is to align a text into underlying entities and events of an environment. These methods interact with an environment to obtain supervision from the real events and entities in the environment. Our method, on the other hand, grounds the problem into world state transitions by learning to predict verb categories in sentences. In addition, our method combines the representations of individual sentences into a coherent whole to form the equations. This is in contrast with the previous work that study each sentence in isolation from the other sentences. Previou</context>
</contexts>
<marker>Liang, Jordan, Klein, 2009</marker>
<rawString>Percy Liang, Michael I. Jordan, and Dan Klein. 2009. Learning semantic correspondences with less supervision. In Proc. of the Annual Meeting of the Association for Computational Linguistics and the International Joint Conference on Natural Language Processing of the AFNLP (ACL-AFNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>An information-theoretic definition of similarity.</title>
<date>1998</date>
<booktitle>In Proc. International Conference on Machine Learning (ICML).</booktitle>
<contexts>
<context position="25152" citStr="Lin, 1998" startWordPosition="4279" endWordPosition="4280">ght vector. ARIS then selects the three most common verbs (without replacement) among the features (verbs) with non-zero weights. This accounts for 21 total seed verbs to be used for the main classification task. We find that in practice using this selection technique leads to better performance than using either all the verbs in V or using just the 65 randomly selected verbs. Our method computes the similarity between two verbs v1 and v2 from the similarity between all the senses (from WordNet) of these verbs (Equation 1). We compute the similarity between two senses using linear similarity (Lin, 1998). The similarity between two synsets sv1 and sv2 are penalized according to the order of each sense for the corresponding verb. Intuitively, if a synset appears earlier in the set of synsets of a verb, it is more likely to be considered as the correct meaning. Therefore, later occurrences of a synset should result in reduced similarity scores. The similarity between two verbs v1 and v2 is the maximum similarity between two synsets of the verbs: sim(v1, v2) = max lin-sim(sv1, sv2) (1) sv:synsets(v) log(p1 + p2) where sv1, sv2 are two synsets, p1, p2 are the position of each synset match, and li</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. An information-theoretic definition of similarity. In Proc. International Conference on Machine Learning (ICML).</rawString>
</citation>
<citation valid="true">
<authors>
<author>John McCarthy</author>
</authors>
<title>Circumscription—a form of nonmonotonic reasoning.</title>
<date>1980</date>
<journal>Artificial Intelligence,</journal>
<volume>13</volume>
<contexts>
<context position="16416" citStr="McCarthy, 1980" startWordPosition="2757" endWordPosition="2758">) such that each fragment contains a single entity and it’s containers. For simplicity we refer to these fragments as a sentences. Containers: Each entity is associated with one or two container noun phrases using the algorithm described in in Figure 3 step 1c. As we saw earlier with numbers, arithmetic problems often include sentences with missing information. For example in Figure 2, the second container in the the sentence “Park workers had to cut down 13 walnut trees that were damaged.” is not explicitly mentioned. To handle this missing information, we use the circumscription assumption (McCarthy, 1980). The circumscription assumption formalizes the commonsense assumption that things are as expected unless otherwise specified. In this setting, we assume that the set of containers are fixed in a problem. Thus if the container(s) for a given entity cannot be identified they are set to the container(s) for the previous entity with the same head word. For example in Figure 2 we know from the previous sentence that trees were in the park. Therefore, we assume that the unmentioned container is the park. Attributes: ARIS selects attributes A as modifiers for every entity from the dependency parser </context>
</contexts>
<marker>McCarthy, 1980</marker>
<rawString>John McCarthy. 1980. Circumscription—a form of nonmonotonic reasoning. Artificial Intelligence, 13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>Wordnet: a lexical database for english.</title>
<date>1995</date>
<journal>Communications of the ACM,</journal>
<volume>38</volume>
<contexts>
<context position="4948" citStr="Miller, 1995" startWordPosition="801" endWordPosition="802">ation to both train ARIS and evaluate its performance over this corpus. We compare its performance to the template-based learning method developed independently and concurrently by Kushman et al. (2014). We find that our approach is much more robust to domain diversity between the training and test sets. Our contributions are three-fold: (a) We present ARIS, a novel, fully automated method that learns to solve arithmetic word problems; (b) We introduce a method to automatically categorize verbs for sentences from simple, easy-to-obtain training data; our results refine verb senses in WordNet (Miller, 1995) for arithmetic word problems; (c) We introduce a corpus of arithmetic word problems, and report on a series of experiments showing high efficacy in solving addition and subtraction problems based on verb categorization. 2 Related Work Understanding semantics of a natural language text has been the focus of many researchers in natural language processing (NLP). Recent work focus on learning to align text with meaning representations in specific, controlled domains. A few methods (Zettlemoyer and Collins, 2005; Ge and Mooney, 2006) use an expensive supervision in the form of manually annotated </context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>George A Miller. 1995. Wordnet: a lexical database for english. Communications of the ACM, 38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mee Young Park</author>
<author>Trevor Hastie</author>
</authors>
<title>L1-regularization path algorithm for generalized linear models.</title>
<date>2007</date>
<journal>Journal of the Royal Statistical Society: Series B (Statistical Methodology),</journal>
<volume>69</volume>
<contexts>
<context position="24358" citStr="Park and Hastie, 2007" startWordPosition="4145" endWordPosition="4148"> The goal is to automatically select verbs from V that are most discriminative for each of the 7 verb categories in Table 1. We define 7 classification tasks: “Is a verb a member of each category?” Then, we select the three most representative verbs for each category. To do so, we randomly select a set of 65 verbs Vl, from all the verbs in our dataset (118 in total) and manually annotate the verb categories. For every classification task, the feature vector X includes the similarity scores (Equation 1) between the verb v and all the verbs in the V. We train an E1 regularized regression model (Park and Hastie, 2007) over the feature vector X to learn each category individually. The number of original (similarity based) features in X is relatively large, but E1 regularization provides a sparse weight vector. ARIS then selects the three most common verbs (without replacement) among the features (verbs) with non-zero weights. This accounts for 21 total seed verbs to be used for the main classification task. We find that in practice using this selection technique leads to better performance than using either all the verbs in V or using just the 65 randomly selected verbs. Our method computes the similarity b</context>
</contexts>
<marker>Park, Hastie, 2007</marker>
<rawString>Mee Young Park and Trevor Hastie. 2007. L1-regularization path algorithm for generalized linear models. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 69.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hoifung Poon</author>
<author>Pedro Domingos</author>
</authors>
<title>Unsupervised semantic parsing.</title>
<date>2009</date>
<booktitle>In Proc. Conference on Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="5728" citStr="Poon and Domingos, 2009" startWordPosition="925" endWordPosition="928">addition and subtraction problems based on verb categorization. 2 Related Work Understanding semantics of a natural language text has been the focus of many researchers in natural language processing (NLP). Recent work focus on learning to align text with meaning representations in specific, controlled domains. A few methods (Zettlemoyer and Collins, 2005; Ge and Mooney, 2006) use an expensive supervision in the form of manually annotated formal representations for every sentence in the training data. More recent work (Eisenstein et al., 2009; Kate and Mooney, 2007; Goldwasser and Roth, 2011; Poon and Domingos, 2009; Goldwasser et al., 2011; Kushman and Barzilay, 2013) reduce the amount of required supervision in mapping sentences to meaning representations while taking advantage of special properties of the domains. Our method, on the other hand, requires small, easy-to-obtain training data in the form of verb categories that are shared among many different problem types. Our work is also closely related to the grounded language acquisition research (Snyder and Barzilay, 2007; Branavan et al., 2009; Branavan et al., 2012; Vogel and Jurafsky, 2010; Chen et al., 2010; Hajishirzi et al., 2011; Chambers and</context>
</contexts>
<marker>Poon, Domingos, 2009</marker>
<rawString>Hoifung Poon and Pedro Domingos. 2009. Unsupervised semantic parsing. In Proc. Conference on Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karthik Raghunathan</author>
<author>Heeyoung Lee</author>
<author>Sudarshan Rangarajan</author>
<author>Nathanael Chambers</author>
<author>Mihai Surdeanu</author>
<author>Dan Jurafsky</author>
<author>Christopher Manning</author>
</authors>
<title>A multi-pass sieve for coreference resolution.</title>
<date>2010</date>
<booktitle>In Proc. Conference on Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="14617" citStr="Raghunathan et al., 2010" startWordPosition="2441" endWordPosition="2444">tically identifies entities, attributes, containers, and quantities corresponding to every sentence fragment (details in Figure 3 step 1). For every problem, this module returns a sequence of sentence fragments (w1,... , wT, wx) where every wt consists of a verb vt, an entity et, its quantity numt, its attributes at, and up to two containers ct1, ct2. wx corresponds to the question sentence inquiring about an unknown entity. ARIS applies the Stanford dependency parser, named entity recognizer and coreference resolution system to the problem text (de Marneffe et al., 2006; Finkel et al., 2005; Raghunathan et al., 2010). It uses the predicted coreference relationships to replace pronouns (including possessive pronouns) with their coreferenent links. The named entity recognition output is used to identify numbers and people. Entities: Entities are references to some object whose quantity is observed or changing throughout the problem. So to determine the set of entities, we define h as the set of noun types which have a dependent number (in the dependency parse) somewhere in the problem text. The set of entities is then defined as all noun phrases which are headed by a noun type in h. For instance kitten in t</context>
</contexts>
<marker>Raghunathan, Lee, Rangarajan, Chambers, Surdeanu, Jurafsky, Manning, 2010</marker>
<rawString>Karthik Raghunathan, Heeyoung Lee, Sudarshan Rangarajan, Nathanael Chambers, Mihai Surdeanu, Dan Jurafsky, and Christopher Manning. 2010. A multi-pass sieve for coreference resolution. In Proc. Conference on Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Using information content to evaluate semantic similarity in a taxonomy.</title>
<date>1995</date>
<booktitle>In International joint conference on Artificial intelligence (IJCAI).</booktitle>
<contexts>
<context position="25959" citStr="Resnik, 1995" startWordPosition="4415" endWordPosition="4416">a verb, it is more likely to be considered as the correct meaning. Therefore, later occurrences of a synset should result in reduced similarity scores. The similarity between two verbs v1 and v2 is the maximum similarity between two synsets of the verbs: sim(v1, v2) = max lin-sim(sv1, sv2) (1) sv:synsets(v) log(p1 + p2) where sv1, sv2 are two synsets, p1, p2 are the position of each synset match, and lin-sim is the linear similarity. Our experiments show better performance using linear similarity compared to other common similarity metrics (e.g., WordNet path similarity and Resnik similarity (Resnik, 1995)). WordNet-based Features: We use WordNet verb categories in the feature vector. For each part of speech in WordNet, the synsets are organized into different categories. There are 15 categories for verbs. Some examples in528 clude “verb.communication”, “verb.possession”, and “verb.creation”. In addition, WordNet includes the frequency measure f,sv indicating how often the sense sv has appeared in a reference corpus. For each category i, we define the feature fz as the ratio of the frequency of the sense svz over the total frequency of the verb i.e., fz = f,svi /f,v. Structural Features: For st</context>
</contexts>
<marker>Resnik, 1995</marker>
<rawString>Philip Resnik. 1995. Using information content to evaluate semantic similarity in a taxonomy. In International joint conference on Artificial intelligence (IJCAI).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Snyder</author>
<author>Regina Barzilay</author>
</authors>
<title>Database-text alignment via structured multilabel classification.</title>
<date>2007</date>
<booktitle>In Proceedings of International Joint Conference on Artificial Intelligence (IJCAI).</booktitle>
<contexts>
<context position="6198" citStr="Snyder and Barzilay, 2007" startWordPosition="995" endWordPosition="999">r every sentence in the training data. More recent work (Eisenstein et al., 2009; Kate and Mooney, 2007; Goldwasser and Roth, 2011; Poon and Domingos, 2009; Goldwasser et al., 2011; Kushman and Barzilay, 2013) reduce the amount of required supervision in mapping sentences to meaning representations while taking advantage of special properties of the domains. Our method, on the other hand, requires small, easy-to-obtain training data in the form of verb categories that are shared among many different problem types. Our work is also closely related to the grounded language acquisition research (Snyder and Barzilay, 2007; Branavan et al., 2009; Branavan et al., 2012; Vogel and Jurafsky, 2010; Chen et al., 2010; Hajishirzi et al., 2011; Chambers and Jurafsky, 2009; Liang et al., 2009; Bordes et al., 2010) where the goal is to align a text into underlying entities and events of an environment. These methods interact with an environment to obtain supervision from the real events and entities in the environment. Our method, on the other hand, grounds the problem into world state transitions by learning to predict verb categories in sentences. In addition, our method combines the representations of individual sent</context>
</contexts>
<marker>Snyder, Barzilay, 2007</marker>
<rawString>Benjamin Snyder and Regina Barzilay. 2007. Database-text alignment via structured multilabel classification. In Proceedings of International Joint Conference on Artificial Intelligence (IJCAI).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Vogel</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Learning to follow navigational directions.</title>
<date>2010</date>
<booktitle>In Proc. of the Annual Meeting of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="6270" citStr="Vogel and Jurafsky, 2010" startWordPosition="1008" endWordPosition="1011">., 2009; Kate and Mooney, 2007; Goldwasser and Roth, 2011; Poon and Domingos, 2009; Goldwasser et al., 2011; Kushman and Barzilay, 2013) reduce the amount of required supervision in mapping sentences to meaning representations while taking advantage of special properties of the domains. Our method, on the other hand, requires small, easy-to-obtain training data in the form of verb categories that are shared among many different problem types. Our work is also closely related to the grounded language acquisition research (Snyder and Barzilay, 2007; Branavan et al., 2009; Branavan et al., 2012; Vogel and Jurafsky, 2010; Chen et al., 2010; Hajishirzi et al., 2011; Chambers and Jurafsky, 2009; Liang et al., 2009; Bordes et al., 2010) where the goal is to align a text into underlying entities and events of an environment. These methods interact with an environment to obtain supervision from the real events and entities in the environment. Our method, on the other hand, grounds the problem into world state transitions by learning to predict verb categories in sentences. In addition, our method combines the representations of individual sentences into a coherent whole to form the equations. This is in contrast w</context>
</contexts>
<marker>Vogel, Jurafsky, 2010</marker>
<rawString>Adam Vogel and Daniel Jurafsky. 2010. Learning to follow navigational directions. In Proc. of the Annual Meeting of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian H Witten</author>
<author>Eibe Frank</author>
<author>Leonard E Trigg</author>
<author>Mark A Hall</author>
<author>Geoffrey Holmes</author>
<author>Sally Jo Cunningham</author>
</authors>
<title>Weka: Practical machine learning tools and techniques with java implementations.</title>
<date>1999</date>
<contexts>
<context position="34168" citStr="Witten et al., 1999" startWordPosition="5844" endWordPosition="5848">L, and MA2. This table compares our method, ARIS, ARIS2 with the state-of-the-art KAZB. All methods are trained on two (out of three) datasets and tested on the other one. ARIS2 is trained when no verb is repeated in the training and test sets. Gold ARIS uses gold verb categories. The improvement of ARIS (boldfaced) and ARIS2 (denoted by +) are significant over KAZB and the majority baseline with P &lt; 0.05. sentences. We use the percentage of correct answers to the problems as the evaluation metric for the first task and accuracy as the evaluation metric for the second task. We use Weka’s SVM (Witten et al., 1999) with default parameters for classification which is trained with verb categories in sentences (as described in Section 4.2). For the first task, we compare ARIS with KAZB (Kushman et al., 2014), majority baseline, ARIS2, and Gold ARIS. KAZB requires training data in the form of equation systems and numerical answers to the problems. The majority baseline classifies every instance as increasing. In ARIS2 (a variant of ARIS) the system is trained in a way that no verb is repeated in the training and test sets. Gold ARIS uses the ground-truth sentence categories instead of predicted ones. For th</context>
</contexts>
<marker>Witten, Frank, Trigg, Hall, Holmes, Cunningham, 1999</marker>
<rawString>Ian H Witten, Eibe Frank, Leonard E Trigg, Mark A Hall, Geoffrey Holmes, and Sally Jo Cunningham. 1999. Weka: Practical machine learning tools and techniques with java implementations.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luke S Zettlemoyer</author>
<author>Michael Collins</author>
</authors>
<title>Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars.</title>
<date>2005</date>
<booktitle>In Proc. Conference on Uncertainty in Artificial Intelligence (UAI).</booktitle>
<contexts>
<context position="5462" citStr="Zettlemoyer and Collins, 2005" startWordPosition="882" endWordPosition="885">s for sentences from simple, easy-to-obtain training data; our results refine verb senses in WordNet (Miller, 1995) for arithmetic word problems; (c) We introduce a corpus of arithmetic word problems, and report on a series of experiments showing high efficacy in solving addition and subtraction problems based on verb categorization. 2 Related Work Understanding semantics of a natural language text has been the focus of many researchers in natural language processing (NLP). Recent work focus on learning to align text with meaning representations in specific, controlled domains. A few methods (Zettlemoyer and Collins, 2005; Ge and Mooney, 2006) use an expensive supervision in the form of manually annotated formal representations for every sentence in the training data. More recent work (Eisenstein et al., 2009; Kate and Mooney, 2007; Goldwasser and Roth, 2011; Poon and Domingos, 2009; Goldwasser et al., 2011; Kushman and Barzilay, 2013) reduce the amount of required supervision in mapping sentences to meaning representations while taking advantage of special properties of the domains. Our method, on the other hand, requires small, easy-to-obtain training data in the form of verb categories that are shared among</context>
</contexts>
<marker>Zettlemoyer, Collins, 2005</marker>
<rawString>Luke S. Zettlemoyer and Michael Collins. 2005. Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars. In Proc. Conference on Uncertainty in Artificial Intelligence (UAI).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
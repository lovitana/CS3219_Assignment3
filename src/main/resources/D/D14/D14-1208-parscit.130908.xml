<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.994712">
Noisy Or-based model for Relation Extraction using Distant Supervision
</title>
<author confidence="0.992674">
Ajay Nagesh1,2,3
</author>
<affiliation confidence="0.966555">
1IITB-Monash Research Academy
</affiliation>
<email confidence="0.916379">
ajaynagesh@cse.iitb.ac.in
</email>
<author confidence="0.979966">
Gholamreza Haffari Ganesh Ramakrishnan
</author>
<affiliation confidence="0.989452">
2Faculty of IT, Monash University 3Dept. of CSE, IIT Bombay
</affiliation>
<email confidence="0.989728">
gholamreza.haffari@monash.edu ganesh@cse.iitb.ac.in
</email>
<sectionHeader confidence="0.993661" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999864809523809">
Distant supervision, a paradigm of rela-
tion extraction where training data is cre-
ated by aligning facts in a database with a
large unannotated corpus, is an attractive
approach for training relation extractors.
Various models are proposed in recent lit-
erature to align the facts in the database
to their mentions in the corpus. In this
paper, we discuss and critically analyse a
popular alignment strategy called the “at
least one” heuristic. We provide a sim-
ple, yet effective relaxation to this strat-
egy. We formulate the inference proce-
dures in training as integer linear program-
ming (ILP) problems and implement the
relaxation to the “at least one ” heuris-
tic via a soft constraint in this formulation.
Empirically, we demonstrate that this sim-
ple strategy leads to a better performance
under certain settings over the existing ap-
proaches.
</bodyText>
<sectionHeader confidence="0.998994" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999644">
Although supervised approaches to relation ex-
traction (GuoDong et al., 2005; Surdeanu and Cia-
ramita, 2007) achieve very high accuracies, they
do not scale as they are data intensive and the cost
of creating annotated data is quite high. To alle-
viate this problem, Mintz et al. (2009) proposed
relation extraction in the paradigm of distant su-
pervision. In this approach, given a database of
facts (e.g. Freebase1) and an unannotated docu-
ment collection, the goal is to heuristically align
the facts in the database to the sentences in the
corpus which contain the entities mentioned in the
fact. This is done to create weakly labeled train-
ing data to train a classifier for relation extraction.
The underlying assumption is that all mentions of
</bodyText>
<footnote confidence="0.944546">
1www.freebase.com
</footnote>
<bodyText confidence="0.99808665">
an entity pair2 (i.e. sentences containing the en-
tity pair) in the corpus express the same relation
as stated in the database.
The above assumption is a weak one and is
often violated in natural language text. For in-
stance, the entity pair, (Barack Obama, United
States) participate in more than one relation:
citizenOf, presidentOf, bornIn and every men-
tion expresses either one of these fixed set of rela-
tions or none of them.
Consequently, a number of models have been
proposed in literature to provide better heuristics
for the mapping between the entity pair in the
database and its mentions in the sentences of the
corpus. Riedel et al. (2010) tightens the assump-
tion of distant supervision in the following man-
ner: “Given a pair of entities and their mentions in
sentences from a corpus, at least one of the men-
tions express the relation given in the database”.
In other words, it models the problem as that of
multi-instance (mentions) single-label (relation)
learning. Following this, Hoffmann et al. (2011)
and Surdeanu et al. (2012) propose models that
consider the mapping as that of multi-instance
multi-label learning. The instances are the men-
tions of the entity pair in the sentences of the cor-
pus and the entity pair can participate in more than
one relation.
Although, these models work very well in prac-
tice, they have a number of shortcomings. One
of them is the possibility that during the align-
ment, a fact in the database might not have an in-
stantiation in the corpus. For instance, if our cor-
pus only contains documents from the years 2000
to 2005, the fact presidentOf(Barack Obama,
United States) will not be present in the corpus.
In such cases, the distant supervision assumption
fails to provide a mapping for the fact in the cor-
pus.
In this paper, we address this situation with a
</bodyText>
<footnote confidence="0.923686">
2In this paper we restrict ourselves to binary relations
</footnote>
<page confidence="0.83314">
1937
</page>
<bodyText confidence="0.9423897">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1937–1941,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
noisy-or model (Srinivas, 2013) in training the re-
lation extractor by relaxing the “at least one” as-
sumption discussed above. Our contributions in
this paper are as follows: (i) We formulate the in-
ference procedures in the training algorithm as in-
teger linear programming (ILP) problems, (ii) We
introduce a soft-constraint in the ILP objective to
model noisy-or in training, and (iii) Empirically,
our algorithm performs better than Hoffmann et
al. (2011) procedure under certain settings on two
benchmark datasets.
Our paper is organized as follows. In Section 2,
we discuss our methodology. We review the ap-
proach of Hoffmann et al. (2011) and explain our
modifications to it. In Section 3, we discuss re-
lated work. In Section 4, we discuss the experi-
mental setup and our preliminary results. We con-
clude in Section 4.
</bodyText>
<sectionHeader confidence="0.996295" genericHeader="introduction">
2 Methodology
</sectionHeader>
<bodyText confidence="0.99988625">
Our work extends the work of Hoffmann et al.
(2011). So, we recapitulate Hoffmann’s model in
the following subsection. Following which our ad-
ditions to this model is explained in detail.
</bodyText>
<subsectionHeader confidence="0.792129">
Hoffmann’s model
</subsectionHeader>
<bodyText confidence="0.905161452380952">
Hoffmann et al. (2011) present a multi-instance
multi-label model for relation extraction through
distant supervision. In this model, a pair of enti-
ties have multiple mentions (sentence containing
the entity pair) in the corpus. An entity pair can
have one or more relation labels (obtained from
the database).
Objective function
Consider an entity pair (e1, e2) denoted by the in-
dex i. The set of sentences containing the entity
pair is denoted xi and the set of relation labels for
the entity pair from the database is denoted by yi.
The mention-level labels are denoted by the latent
variable z (there is one variable zj for each sen-
tence j).
To learn the parameters 0, the training objective
to maximize is the likelihood of the facts observed
in the database conditioned on the sentences in the
text corpus.
The expression Pr(yi, z|xi) for a given entity
pair is defined by two types of factors in the factor
graph. They are extract factors for each mention
and mention factors between a relation label and
all the mentions.
The extract factors capture the local signal for
each mention and consists of a bunch of lexical
and syntactic features like POS tags, dependency
path between the entities and so on (Mintz et al.,
2009).
The mention factors capture the dependency be-
tween relation label and its mentions. Here, the at
least one assumption that was discussed in Section
1 is modeled. It is implemented as a simple deter-
ministic OR operator as given below:
finention(yr, z) =1 1
0
weighted edge-cover problem which can
be
solved exactly, although Hoffmann et al. (2011)
provide an approximate solution.
Algorithm 1: Hoffmann et al. (2011) : Train-
ing
</bodyText>
<subsectionHeader confidence="0.694916">
Input
</subsectionHeader>
<bodyText confidence="0.831938">
i) E: set of sentences, ii) E: set of entities
mentioned in the sentences, iii) R: set of
relation labels, iv) D: database of facts
Output: Extraction model: O
begin
</bodyText>
<equation confidence="0.932050052631579">
fort
1
T ; /*
:
←
to
training iterations */
do
for i
← 1 to N ; /* No. of entity pairs */
do
Pr(y, z Ixi; O)
if y! = yi then
z* =arg max
z
=
Onew
Oold+Φ(xi,z*)−Φ(xi, Z)
end
</equation>
<bodyText confidence="0.9447005">
Our additions to Hoffmann’s model
Training algorithm
The learning algorithm is a perceptron-style
parameter update scheme with 2 modifications:
i) online learning ii) Viterbi approximation. The
inference is shown to reduce to the well-known
</bodyText>
<equation confidence="0.607037">
Pr(zlyi, xi; O)
</equation>
<bodyText confidence="0.98170375">
In the training algorithm described above, there
are two MAP inference procedures. Our con-
tri
butions in this space is two-fold. Firstly, we
</bodyText>
<equation confidence="0.959890833333333">
0∗ = arg max
jl Pr(yi|xi; 0)
θ i
= arg max
rl 1: Pr(yi, z|xi; 0)
θ i z
</equation>
<figure confidence="0.674546555555556">
1
if
yr is true n∃i : zi = r
otherwise
Y,
z
=arg max
,
y,z
</figure>
<page confidence="0.940545">
1938
</page>
<bodyText confidence="0.992433714285714">
have formulated these as ILP problems. As a re-
sult of this, the approximate inference therein is
replaced by an exact inference procedure. Sec-
ondly, we replace the deterministic-or by a noisy-
or which provides a soft-constraint instead of the
hard-constraint of Hoffmann. (“at least one” as-
sumption)
</bodyText>
<figure confidence="0.94712832">
ILP formulations
Some notations:
good match.
zrE P
isji
( mX
j
i!
X
not be a very zj
s.t 1. i = 1 ∀j
i∈{R,nil}
i ≤ yi ∀j, ∀i
3. yi zj
i + ei ∀i
∈
l
i
{
i
}
∈
i
R
j=1
</figure>
<listItem confidence="0.988543444444444">
❑ zji : The mention variable zj (or jth sen-
tence) taking the relation value i
❑ sji : Score for zj taking the value of i. Scores
are computed from the extract factors
❑ yi : relation label being i
❑ m : number of mentions (sentences) for the
given entity pair
❑ R: total number of relation labels (excluding
the nil label)
</listItem>
<subsectionHeader confidence="0.739111">
Deterministic OR
</subsectionHeader>
<bodyText confidence="0.9713492">
The following is the ILP formulation for the exact
inference arg max Pr(y, z|xi) in the model based
on the deterministic-or:
where zj
i ∈ {0, 1}, yi ∈ {0, 1}, ei ∈ {0, 1}
can come from different domains and there might
call this new term
and it is a binary variable to
model noise. Through this term we encourage at
least one type of configuration but will not disal-
low aconfiguration that does not conform to this.
Essentially, the consequence of this is to allow the
case where a fact is present in the database but is
Ei
tiated in the text.
</bodyText>
<sectionHeader confidence="0.994301" genericHeader="related work">
3 Related Work
</sectionHeader>
<equation confidence="0.84667725">
( Xmi)
X h
max zjisji
Z,Y
j=1 i∈{R,nil}
s.t 1. X i = 1 ∀j
i∈{R,nil}
zj
i ≤ yi ∀j, ∀i
3. yi ≤ Xm i ∀i
j=1
zj
</equation>
<bodyText confidence="0.7126482">
where zj
i ∈ {0, 1}, yi ∈ {0, 1}
an
ILP allows us to easily add more constraints to
it.
</bodyText>
<sectionHeader confidence="0.72623" genericHeader="method">
OR
</sectionHeader>
<bodyText confidence="0.904845227272728">
As acase-study, we add the noisy-or soft-
constraint in the above objective function. The
idea is to model the situation where a fact is
present in the database but it is not instan
Noisy
tiated in
the text. This is a common scenario, as the facts
populated in the database and the text of the corpus
Our work is inspired fr
om previous works
like Roth and tau Yih (2004). The use of ILP
for this problem facilitates easy incorporation of
different constraints and to the best of our knowl-
edge, has not been investigated by the community.
publicly available
distantly supervised
slot-filling
(Surdeanu et al., 2011) an
Stanford’s
system3
d
Hoffmann et al. (2011) code-base4.
</bodyText>
<footnote confidence="0.897266">
3http://nlp.stanford.edu/software/
mimlre.shtml
4http://www.cs.washington.edu/ai/
raphaelh/mr/
2. zj
</footnote>
<bodyText confidence="0.9999564">
The first constraint restricts a mention to have
only one label. The second and third constraints
impose the at least one assumption. This is the
same formulation as Hoffmann but expressed as
an ILP problem. However, posing the inference as
</bodyText>
<sectionHeader confidence="0.759314" genericHeader="method">
2. zj
</sectionHeader>
<bodyText confidence="0.994270375">
In the above formulation, the objective function
is augmented with a soft penalty. Also the third
constraint is modified with this penalty term. We
not instan
Relation Extraction in the paradigm of distant su-
pervision was introduced by Craven and Kum-
lien (1999). They used a biological database as
the source of distant supervision to discover rela-
tions between biological entities. The progression
of models for information extraction using distant
supervision was presented in Section 1.
Surdeanu et al. (2012) discuss a noisy-or
method for combining the scores of various sen-
tence level models to rank a relation during evalu-
ation. In our approach, we introduce the noisy-or
mechanism in the training phase of the algorithm.
</bodyText>
<sectionHeader confidence="0.999699" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.979823">
The experimental runs were carried out using the
</bodyText>
<page confidence="0.998134">
1939
</page>
<tableCaption confidence="0.99742">
Table 1 : Highest F1 point in P/R curve : KBP Dataset
</tableCaption>
<table confidence="0.999493">
Precision Recall F1
Hoffmann 0.306451619 0.197916672 0.2405063349
MIMLRE 0.28061223 0.286458343 0.2835051518
Noisy-OR 0.297002733 0.189236104 0.2311770916
Hoffmann-ilp 0.293010741 0.189236104 0.2299577976
</table>
<subsectionHeader confidence="0.579972">
Datasets and Evaluation
</subsectionHeader>
<bodyText confidence="0.999972153846154">
We report results on two standard datasets used as
benchmarks by the community namely KBP and
Riedel datasets. A complete description of these
datasets is provided in Surdeanu et al. (2012).
The evaluation setup and module is the same
as that described in Surdeanu et al. (2012). We
also use the same set of features used by the var-
ious systems in the package to ensure that the ap-
proaches are comparable. As in previous work, we
report precision/recall (P/R) graphs to evaluate the
various techniques.
We used the publicly available lp solve pack-
age5 to solve our inference problems.
</bodyText>
<subsectionHeader confidence="0.637986">
Performance of ILP
</subsectionHeader>
<bodyText confidence="0.999871444444445">
Use of ILP raises concerns about performance as
it is NP-hard. In our problem we solve a separate
ILP for every entity pair. The number of variables
is limited by the number of mentions for the given
entity pair. Empirically, on the KBP dataset (larger
of the two datasets), Hoffmann takes around 1hr
to run. Our ILP formulation takes around 8.5hrs.
However, MIMLRE algorithm (EM-based) takes
around 23hrs to converge.
</bodyText>
<sectionHeader confidence="0.767378" genericHeader="evaluation">
Results
</sectionHeader>
<bodyText confidence="0.999993153846154">
We would primarily like to highlight two settings
on which we report the P/R curves and contrast
it with Hoffmann et al. (2011). Firstly, we re-
place the approximate inference in that work with
our ILP-based exact inference; we call this set-
ting the hoffmann-ilp. Secondly, we replace the
deterministic-or in the model with a noisy-or, and
call this setting the noisy-or. We further compare
our approach with Surdeanu et al. (2012). The
P/R curves for the various techniques on the two
datasets are shown in Figures 1 and 2.
We further report the highest F1 point in the P/R
curve for both the datasets in Tables 1 and 2.
</bodyText>
<sectionHeader confidence="0.985563" genericHeader="discussions">
Discussion
</sectionHeader>
<bodyText confidence="0.9904795">
We would like to discuss the results in the above
two scenarios.
</bodyText>
<footnote confidence="0.964461">
5http://lpsolve.sourceforge.net/5.5/
</footnote>
<figureCaption confidence="0.9999345">
Figure 1: Results: KBP dataset
Figure 2: Results: Riedel dataset
</figureCaption>
<listItem confidence="0.761702">
1. Performance of hoffmann-ilp
</listItem>
<bodyText confidence="0.999762277777778">
On the KBP dataset, we observe that
hoffmann-ilp has higher precision in the
range of 0.05 to 0.1 at lower recall (0 to 0.04).
In other parts of the curve it is very close to
the baseline (although hoffmann’s algorithm
is slightly better). In Table 1, we notice that
recall of hoffmann-ilp is lower in comparison
with hoffmann’s algorithm.
On the Riedel dataset, we observe that
hoffmann-ilp has better precision (0.15 to
0.2) than MIMLRE within recall of 0.1.
At recall &gt; 0.1, precision drops drastically.
This is because, hoffmann-ilp predicts signif-
icantly more nil labels. However, nil labels
are not part of the label-set in the P/R curves
reported in the community. In Table 2, we see
that hoffmann-ilp has higher precision (0.04)
compared to Hoffmann’s algorithm.
</bodyText>
<listItem confidence="0.374039">
2. Performance of noisy-or
</listItem>
<figure confidence="0.999055357142857">
0.7
0.6
precision
0.5
0.4
0.3
0.2
hoffmann
noisyOr
hoffmann_ilp
mimlre
0 0.05 0.1 0.15 0.2 0.25 0.3
recall
0 0.05 0.1 0.15 0.2 0.25 0.3
recall
precision
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
hoffmann
noisyOr
hoffmann_ilp
mimlre
</figure>
<page confidence="0.944479">
1940
</page>
<tableCaption confidence="0.998014">
Table 2 : Highest F1 point in P/R curve : Riedel Dataset
</tableCaption>
<table confidence="0.9965506">
Precision Recall F1
Hoffmann 0.32054795 0.24049332 0.27480916
MIMLRE 0.28061223 0.28645834 0.28350515
Noisy-OR 0.317 0.18139774 0.23075178
Hoffmann-ilp 0.36701337 0.12692702 0.18862161
</table>
<bodyText confidence="0.991495615384616">
In Figure 1 we see that there is a big jump
in precision (around 0.4) of noisy-or com-
pared to Hoffmann’s model in most parts of
the curve on the KBP dataset. However, in
Figure 2 (Riedel dataset), we do not see such
a trend. Although, we do perform better than
MIMLRE (Surdeanu et al., 2012) (precision
&gt; 0.15 for recall &lt; 0.15).
On both datasets, noisy-or has higher preci-
sion than MIMLRE, as seen from Tables 1
and 2. However, the recall reduces. More in-
vestigation in this direction is part of future
work.
</bodyText>
<sectionHeader confidence="0.999301" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.99957555">
In this paper we described an important addition to
Hoffmann’s model by the use of the noisy-or soft
constraint to further relax the at least one assump-
tion. Since we posed the inference procedures in
Hoffmann using ILP, we could easily add this con-
straint during the training and inference.
Empirically, we showed that the resulting P/R
curves have a significant performance boost over
Hoffmann’s algorithm as a result of this newly
added constraint. Although our system has a lower
recall when compared to MIMLRE (Surdeanu et
al., 2012), it performs competitively w.r.t the pre-
cision at low recall.
As part of immediate future work, we would
like to improve the system recall. Our ILP for-
mulation provides a good framework to add new
type of constraints to the problem. In the future,
we would like to experiment with other constraints
like modeling the selectional preferences of entity
types.
</bodyText>
<sectionHeader confidence="0.999469" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9998218">
Mark Craven and Johan Kumlien. 1999. Constructing
biological knowledge bases by extracting informa-
tion from text sources. In Proceedings of the Sev-
enth International Conference on Intelligent Systems
for Molecular Biology, pages 77–86. AAAI Press.
Zhou GuoDong, Su Jian, Zhang Jie, and Zhang Min.
2005. Exploring various knowledge in relation ex-
traction. In Proceedings of the 43rd Annual Meeting
on Association for Computational Linguistics, ACL
’05, pages 427–434, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Raphael Hoffmann, Congle Zhang, Xiao Ling,
Luke Zettlemoyer, and Daniel S. Weld. 2011.
Knowledge-based weak supervision for information
extraction of overlapping relations. In Proceed-
ings of the 49th Annual Meeting of the Association
for Computational Linguistics: Human Language
Technologies - Volume 1, HLT ’11, pages 541–550,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Mike Mintz, Steven Bills, Rion Snow, and Dan Ju-
rafsky. 2009. Distant supervision for relation ex-
traction without labeled data. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP: Vol-
ume 2 - Volume 2, ACL ’09, pages 1003–1011,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Sebastian Riedel, Limin Yao, and Andrew McCal-
lum. 2010. Modeling relations and their men-
tions without labeled text. In Proceedings of the
2010 European conference on Machine learning
and knowledge discovery in databases: Part III,
ECML PKDD’10, pages 148–163, Berlin, Heidel-
berg. Springer-Verlag.
Dan Roth and Wen tau Yih. 2004. A linear program-
ming formulation for global inference in natural lan-
guage tasks. In In Proceedings of CoNLL-2004,
pages 1–8.
Sampath Srinivas. 2013. A generalization of the noisy-
or model. CoRR, abs/1303.1479.
Mihai Surdeanu and Massimiliano Ciaramita. 2007.
Robust information extraction with perceptrons. In
Proceedings of the NIST 2007 Automatic Content
Extraction Workshop (ACE07), March.
Mihai Surdeanu, Sonal Gupta, John Bauer, David Mc-
Closky, Angel X. Chang, Valentin I. Spitkovsky,
and Christopher D. Manning. 2011. Stanford’s
distantly-supervised slot-filling system. In Proceed-
ings of the Fourth Text Analysis Conference (TAC
2011), Gaithersburg, Maryland, USA, November.
Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati,
and Christopher D. Manning. 2012. Multi-instance
multi-label learning for relation extraction. In Pro-
ceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning, EMNLP-
CoNLL ’12, pages 455–465, Stroudsburg, PA, USA.
Association for Computational Linguistics.
</reference>
<page confidence="0.994683">
1941
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.493781">
<title confidence="0.883914333333333">Noisy Or-based model for Relation Extraction using Distant Supervision Research Academy ajaynagesh@cse.iitb.ac.in</title>
<author confidence="0.999173">Gholamreza Haffari Ganesh Ramakrishnan</author>
<affiliation confidence="0.831152">of IT, Monash University of CSE, IIT Bombay</affiliation>
<email confidence="0.940687">gholamreza.haffari@monash.eduganesh@cse.iitb.ac.in</email>
<abstract confidence="0.993014954545454">a paradigm of relation extraction where training data is created by aligning facts in a database with a large unannotated corpus, is an attractive approach for training relation extractors. Various models are proposed in recent literature to align the facts in the database to their mentions in the corpus. In this paper, we discuss and critically analyse a alignment strategy called the one” We provide a simple, yet effective relaxation to this strategy. We formulate the inference procedures in training as integer linear programproblems and implement the to the least one ” heurissoft constraint in this formulation. Empirically, we demonstrate that this simple strategy leads to a better performance under certain settings over the existing approaches.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Mark Craven</author>
<author>Johan Kumlien</author>
</authors>
<title>Constructing biological knowledge bases by extracting information from text sources.</title>
<date>1999</date>
<booktitle>In Proceedings of the Seventh International Conference on Intelligent Systems for Molecular Biology,</booktitle>
<pages>77--86</pages>
<publisher>AAAI Press.</publisher>
<contexts>
<context position="10419" citStr="Craven and Kumlien (1999)" startWordPosition="1795" endWordPosition="1799">11) code-base4. 3http://nlp.stanford.edu/software/ mimlre.shtml 4http://www.cs.washington.edu/ai/ raphaelh/mr/ 2. zj The first constraint restricts a mention to have only one label. The second and third constraints impose the at least one assumption. This is the same formulation as Hoffmann but expressed as an ILP problem. However, posing the inference as 2. zj In the above formulation, the objective function is augmented with a soft penalty. Also the third constraint is modified with this penalty term. We not instan Relation Extraction in the paradigm of distant supervision was introduced by Craven and Kumlien (1999). They used a biological database as the source of distant supervision to discover relations between biological entities. The progression of models for information extraction using distant supervision was presented in Section 1. Surdeanu et al. (2012) discuss a noisy-or method for combining the scores of various sentence level models to rank a relation during evaluation. In our approach, we introduce the noisy-or mechanism in the training phase of the algorithm. 4 Experiments The experimental runs were carried out using the 1939 Table 1 : Highest F1 point in P/R curve : KBP Dataset Precision R</context>
</contexts>
<marker>Craven, Kumlien, 1999</marker>
<rawString>Mark Craven and Johan Kumlien. 1999. Constructing biological knowledge bases by extracting information from text sources. In Proceedings of the Seventh International Conference on Intelligent Systems for Molecular Biology, pages 77–86. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhou GuoDong</author>
<author>Su Jian</author>
<author>Zhang Jie</author>
<author>Zhang Min</author>
</authors>
<title>Exploring various knowledge in relation extraction.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL ’05,</booktitle>
<pages>427--434</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1238" citStr="GuoDong et al., 2005" startWordPosition="180" endWordPosition="183">n the corpus. In this paper, we discuss and critically analyse a popular alignment strategy called the “at least one” heuristic. We provide a simple, yet effective relaxation to this strategy. We formulate the inference procedures in training as integer linear programming (ILP) problems and implement the relaxation to the “at least one ” heuristic via a soft constraint in this formulation. Empirically, we demonstrate that this simple strategy leads to a better performance under certain settings over the existing approaches. 1 Introduction Although supervised approaches to relation extraction (GuoDong et al., 2005; Surdeanu and Ciaramita, 2007) achieve very high accuracies, they do not scale as they are data intensive and the cost of creating annotated data is quite high. To alleviate this problem, Mintz et al. (2009) proposed relation extraction in the paradigm of distant supervision. In this approach, given a database of facts (e.g. Freebase1) and an unannotated document collection, the goal is to heuristically align the facts in the database to the sentences in the corpus which contain the entities mentioned in the fact. This is done to create weakly labeled training data to train a classifier for r</context>
</contexts>
<marker>GuoDong, Jian, Jie, Min, 2005</marker>
<rawString>Zhou GuoDong, Su Jian, Zhang Jie, and Zhang Min. 2005. Exploring various knowledge in relation extraction. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL ’05, pages 427–434, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raphael Hoffmann</author>
<author>Congle Zhang</author>
<author>Xiao Ling</author>
<author>Luke Zettlemoyer</author>
<author>Daniel S Weld</author>
</authors>
<title>Knowledge-based weak supervision for information extraction of overlapping relations.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1, HLT ’11,</booktitle>
<pages>541--550</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="2942" citStr="Hoffmann et al. (2011)" startWordPosition="465" endWordPosition="468"> set of relations or none of them. Consequently, a number of models have been proposed in literature to provide better heuristics for the mapping between the entity pair in the database and its mentions in the sentences of the corpus. Riedel et al. (2010) tightens the assumption of distant supervision in the following manner: “Given a pair of entities and their mentions in sentences from a corpus, at least one of the mentions express the relation given in the database”. In other words, it models the problem as that of multi-instance (mentions) single-label (relation) learning. Following this, Hoffmann et al. (2011) and Surdeanu et al. (2012) propose models that consider the mapping as that of multi-instance multi-label learning. The instances are the mentions of the entity pair in the sentences of the corpus and the entity pair can participate in more than one relation. Although, these models work very well in practice, they have a number of shortcomings. One of them is the possibility that during the alignment, a fact in the database might not have an instantiation in the corpus. For instance, if our corpus only contains documents from the years 2000 to 2005, the fact presidentOf(Barack Obama, United S</context>
<context position="4451" citStr="Hoffmann et al. (2011)" startWordPosition="713" endWordPosition="716">ence on Empirical Methods in Natural Language Processing (EMNLP), pages 1937–1941, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics noisy-or model (Srinivas, 2013) in training the relation extractor by relaxing the “at least one” assumption discussed above. Our contributions in this paper are as follows: (i) We formulate the inference procedures in the training algorithm as integer linear programming (ILP) problems, (ii) We introduce a soft-constraint in the ILP objective to model noisy-or in training, and (iii) Empirically, our algorithm performs better than Hoffmann et al. (2011) procedure under certain settings on two benchmark datasets. Our paper is organized as follows. In Section 2, we discuss our methodology. We review the approach of Hoffmann et al. (2011) and explain our modifications to it. In Section 3, we discuss related work. In Section 4, we discuss the experimental setup and our preliminary results. We conclude in Section 4. 2 Methodology Our work extends the work of Hoffmann et al. (2011). So, we recapitulate Hoffmann’s model in the following subsection. Following which our additions to this model is explained in detail. Hoffmann’s model Hoffmann et al. </context>
<context position="6615" citStr="Hoffmann et al. (2011)" startWordPosition="1081" endWordPosition="1084">tion and mention factors between a relation label and all the mentions. The extract factors capture the local signal for each mention and consists of a bunch of lexical and syntactic features like POS tags, dependency path between the entities and so on (Mintz et al., 2009). The mention factors capture the dependency between relation label and its mentions. Here, the at least one assumption that was discussed in Section 1 is modeled. It is implemented as a simple deterministic OR operator as given below: finention(yr, z) =1 1 0 weighted edge-cover problem which can be solved exactly, although Hoffmann et al. (2011) provide an approximate solution. Algorithm 1: Hoffmann et al. (2011) : Training Input i) E: set of sentences, ii) E: set of entities mentioned in the sentences, iii) R: set of relation labels, iv) D: database of facts Output: Extraction model: O begin fort 1 T ; /* : ← to training iterations */ do for i ← 1 to N ; /* No. of entity pairs */ do Pr(y, z Ixi; O) if y! = yi then z* =arg max z = Onew Oold+Φ(xi,z*)−Φ(xi, Z) end Our additions to Hoffmann’s model Training algorithm The learning algorithm is a perceptron-style parameter update scheme with 2 modifications: i) online learning ii) Viterbi</context>
<context position="9797" citStr="Hoffmann et al. (2011)" startWordPosition="1704" endWordPosition="1707">ftconstraint in the above objective function. The idea is to model the situation where a fact is present in the database but it is not instan Noisy tiated in the text. This is a common scenario, as the facts populated in the database and the text of the corpus Our work is inspired fr om previous works like Roth and tau Yih (2004). The use of ILP for this problem facilitates easy incorporation of different constraints and to the best of our knowledge, has not been investigated by the community. publicly available distantly supervised slot-filling (Surdeanu et al., 2011) an Stanford’s system3 d Hoffmann et al. (2011) code-base4. 3http://nlp.stanford.edu/software/ mimlre.shtml 4http://www.cs.washington.edu/ai/ raphaelh/mr/ 2. zj The first constraint restricts a mention to have only one label. The second and third constraints impose the at least one assumption. This is the same formulation as Hoffmann but expressed as an ILP problem. However, posing the inference as 2. zj In the above formulation, the objective function is augmented with a soft penalty. Also the third constraint is modified with this penalty term. We not instan Relation Extraction in the paradigm of distant supervision was introduced by Cra</context>
<context position="12395" citStr="Hoffmann et al. (2011)" startWordPosition="2111" endWordPosition="2114"> solve package5 to solve our inference problems. Performance of ILP Use of ILP raises concerns about performance as it is NP-hard. In our problem we solve a separate ILP for every entity pair. The number of variables is limited by the number of mentions for the given entity pair. Empirically, on the KBP dataset (larger of the two datasets), Hoffmann takes around 1hr to run. Our ILP formulation takes around 8.5hrs. However, MIMLRE algorithm (EM-based) takes around 23hrs to converge. Results We would primarily like to highlight two settings on which we report the P/R curves and contrast it with Hoffmann et al. (2011). Firstly, we replace the approximate inference in that work with our ILP-based exact inference; we call this setting the hoffmann-ilp. Secondly, we replace the deterministic-or in the model with a noisy-or, and call this setting the noisy-or. We further compare our approach with Surdeanu et al. (2012). The P/R curves for the various techniques on the two datasets are shown in Figures 1 and 2. We further report the highest F1 point in the P/R curve for both the datasets in Tables 1 and 2. Discussion We would like to discuss the results in the above two scenarios. 5http://lpsolve.sourceforge.ne</context>
</contexts>
<marker>Hoffmann, Zhang, Ling, Zettlemoyer, Weld, 2011</marker>
<rawString>Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke Zettlemoyer, and Daniel S. Weld. 2011. Knowledge-based weak supervision for information extraction of overlapping relations. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1, HLT ’11, pages 541–550, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Mike Mintz</author>
<author>Steven Bills</author>
<author>Rion Snow</author>
<author>Dan Jurafsky</author>
</authors>
<title>Distant supervision for relation extraction without labeled data.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 2 - Volume 2, ACL ’09,</booktitle>
<pages>1003--1011</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1446" citStr="Mintz et al. (2009)" startWordPosition="217" endWordPosition="220"> inference procedures in training as integer linear programming (ILP) problems and implement the relaxation to the “at least one ” heuristic via a soft constraint in this formulation. Empirically, we demonstrate that this simple strategy leads to a better performance under certain settings over the existing approaches. 1 Introduction Although supervised approaches to relation extraction (GuoDong et al., 2005; Surdeanu and Ciaramita, 2007) achieve very high accuracies, they do not scale as they are data intensive and the cost of creating annotated data is quite high. To alleviate this problem, Mintz et al. (2009) proposed relation extraction in the paradigm of distant supervision. In this approach, given a database of facts (e.g. Freebase1) and an unannotated document collection, the goal is to heuristically align the facts in the database to the sentences in the corpus which contain the entities mentioned in the fact. This is done to create weakly labeled training data to train a classifier for relation extraction. The underlying assumption is that all mentions of 1www.freebase.com an entity pair2 (i.e. sentences containing the entity pair) in the corpus express the same relation as stated in the dat</context>
<context position="6267" citStr="Mintz et al., 2009" startWordPosition="1023" endWordPosition="1026">ariable zj for each sentence j). To learn the parameters 0, the training objective to maximize is the likelihood of the facts observed in the database conditioned on the sentences in the text corpus. The expression Pr(yi, z|xi) for a given entity pair is defined by two types of factors in the factor graph. They are extract factors for each mention and mention factors between a relation label and all the mentions. The extract factors capture the local signal for each mention and consists of a bunch of lexical and syntactic features like POS tags, dependency path between the entities and so on (Mintz et al., 2009). The mention factors capture the dependency between relation label and its mentions. Here, the at least one assumption that was discussed in Section 1 is modeled. It is implemented as a simple deterministic OR operator as given below: finention(yr, z) =1 1 0 weighted edge-cover problem which can be solved exactly, although Hoffmann et al. (2011) provide an approximate solution. Algorithm 1: Hoffmann et al. (2011) : Training Input i) E: set of sentences, ii) E: set of entities mentioned in the sentences, iii) R: set of relation labels, iv) D: database of facts Output: Extraction model: O begin</context>
</contexts>
<marker>Mintz, Bills, Snow, Jurafsky, 2009</marker>
<rawString>Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky. 2009. Distant supervision for relation extraction without labeled data. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 2 - Volume 2, ACL ’09, pages 1003–1011, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Riedel</author>
<author>Limin Yao</author>
<author>Andrew McCallum</author>
</authors>
<title>Modeling relations and their mentions without labeled text.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 European conference on Machine learning and knowledge discovery in databases: Part III, ECML PKDD’10,</booktitle>
<pages>148--163</pages>
<publisher>Springer-Verlag.</publisher>
<location>Berlin, Heidelberg.</location>
<contexts>
<context position="2575" citStr="Riedel et al. (2010)" startWordPosition="405" endWordPosition="408">es containing the entity pair) in the corpus express the same relation as stated in the database. The above assumption is a weak one and is often violated in natural language text. For instance, the entity pair, (Barack Obama, United States) participate in more than one relation: citizenOf, presidentOf, bornIn and every mention expresses either one of these fixed set of relations or none of them. Consequently, a number of models have been proposed in literature to provide better heuristics for the mapping between the entity pair in the database and its mentions in the sentences of the corpus. Riedel et al. (2010) tightens the assumption of distant supervision in the following manner: “Given a pair of entities and their mentions in sentences from a corpus, at least one of the mentions express the relation given in the database”. In other words, it models the problem as that of multi-instance (mentions) single-label (relation) learning. Following this, Hoffmann et al. (2011) and Surdeanu et al. (2012) propose models that consider the mapping as that of multi-instance multi-label learning. The instances are the mentions of the entity pair in the sentences of the corpus and the entity pair can participate</context>
</contexts>
<marker>Riedel, Yao, McCallum, 2010</marker>
<rawString>Sebastian Riedel, Limin Yao, and Andrew McCallum. 2010. Modeling relations and their mentions without labeled text. In Proceedings of the 2010 European conference on Machine learning and knowledge discovery in databases: Part III, ECML PKDD’10, pages 148–163, Berlin, Heidelberg. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Roth</author>
<author>Wen tau Yih</author>
</authors>
<title>A linear programming formulation for global inference in natural language tasks. In</title>
<date>2004</date>
<booktitle>In Proceedings of CoNLL-2004,</booktitle>
<pages>1--8</pages>
<marker>Roth, Yih, 2004</marker>
<rawString>Dan Roth and Wen tau Yih. 2004. A linear programming formulation for global inference in natural language tasks. In In Proceedings of CoNLL-2004, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sampath Srinivas</author>
</authors>
<title>A generalization of the noisyor model.</title>
<date>2013</date>
<location>CoRR, abs/1303.1479.</location>
<contexts>
<context position="4026" citStr="Srinivas, 2013" startWordPosition="647" endWordPosition="648">orpus. For instance, if our corpus only contains documents from the years 2000 to 2005, the fact presidentOf(Barack Obama, United States) will not be present in the corpus. In such cases, the distant supervision assumption fails to provide a mapping for the fact in the corpus. In this paper, we address this situation with a 2In this paper we restrict ourselves to binary relations 1937 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1937–1941, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics noisy-or model (Srinivas, 2013) in training the relation extractor by relaxing the “at least one” assumption discussed above. Our contributions in this paper are as follows: (i) We formulate the inference procedures in the training algorithm as integer linear programming (ILP) problems, (ii) We introduce a soft-constraint in the ILP objective to model noisy-or in training, and (iii) Empirically, our algorithm performs better than Hoffmann et al. (2011) procedure under certain settings on two benchmark datasets. Our paper is organized as follows. In Section 2, we discuss our methodology. We review the approach of Hoffmann et</context>
</contexts>
<marker>Srinivas, 2013</marker>
<rawString>Sampath Srinivas. 2013. A generalization of the noisyor model. CoRR, abs/1303.1479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Massimiliano Ciaramita</author>
</authors>
<title>Robust information extraction with perceptrons.</title>
<date>2007</date>
<booktitle>In Proceedings of the NIST 2007 Automatic Content Extraction Workshop (ACE07),</booktitle>
<contexts>
<context position="1269" citStr="Surdeanu and Ciaramita, 2007" startWordPosition="184" endWordPosition="188">paper, we discuss and critically analyse a popular alignment strategy called the “at least one” heuristic. We provide a simple, yet effective relaxation to this strategy. We formulate the inference procedures in training as integer linear programming (ILP) problems and implement the relaxation to the “at least one ” heuristic via a soft constraint in this formulation. Empirically, we demonstrate that this simple strategy leads to a better performance under certain settings over the existing approaches. 1 Introduction Although supervised approaches to relation extraction (GuoDong et al., 2005; Surdeanu and Ciaramita, 2007) achieve very high accuracies, they do not scale as they are data intensive and the cost of creating annotated data is quite high. To alleviate this problem, Mintz et al. (2009) proposed relation extraction in the paradigm of distant supervision. In this approach, given a database of facts (e.g. Freebase1) and an unannotated document collection, the goal is to heuristically align the facts in the database to the sentences in the corpus which contain the entities mentioned in the fact. This is done to create weakly labeled training data to train a classifier for relation extraction. The underly</context>
</contexts>
<marker>Surdeanu, Ciaramita, 2007</marker>
<rawString>Mihai Surdeanu and Massimiliano Ciaramita. 2007. Robust information extraction with perceptrons. In Proceedings of the NIST 2007 Automatic Content Extraction Workshop (ACE07), March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Sonal Gupta</author>
<author>John Bauer</author>
<author>David McClosky</author>
<author>Angel X Chang</author>
<author>Valentin I Spitkovsky</author>
<author>Christopher D Manning</author>
</authors>
<title>Stanford’s distantly-supervised slot-filling system.</title>
<date>2011</date>
<booktitle>In Proceedings of the Fourth Text Analysis Conference (TAC 2011),</booktitle>
<location>Gaithersburg, Maryland, USA,</location>
<contexts>
<context position="9750" citStr="Surdeanu et al., 2011" startWordPosition="1696" endWordPosition="1699">o it. OR As acase-study, we add the noisy-or softconstraint in the above objective function. The idea is to model the situation where a fact is present in the database but it is not instan Noisy tiated in the text. This is a common scenario, as the facts populated in the database and the text of the corpus Our work is inspired fr om previous works like Roth and tau Yih (2004). The use of ILP for this problem facilitates easy incorporation of different constraints and to the best of our knowledge, has not been investigated by the community. publicly available distantly supervised slot-filling (Surdeanu et al., 2011) an Stanford’s system3 d Hoffmann et al. (2011) code-base4. 3http://nlp.stanford.edu/software/ mimlre.shtml 4http://www.cs.washington.edu/ai/ raphaelh/mr/ 2. zj The first constraint restricts a mention to have only one label. The second and third constraints impose the at least one assumption. This is the same formulation as Hoffmann but expressed as an ILP problem. However, posing the inference as 2. zj In the above formulation, the objective function is augmented with a soft penalty. Also the third constraint is modified with this penalty term. We not instan Relation Extraction in the paradi</context>
</contexts>
<marker>Surdeanu, Gupta, Bauer, McClosky, Chang, Spitkovsky, Manning, 2011</marker>
<rawString>Mihai Surdeanu, Sonal Gupta, John Bauer, David McClosky, Angel X. Chang, Valentin I. Spitkovsky, and Christopher D. Manning. 2011. Stanford’s distantly-supervised slot-filling system. In Proceedings of the Fourth Text Analysis Conference (TAC 2011), Gaithersburg, Maryland, USA, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Julie Tibshirani</author>
<author>Ramesh Nallapati</author>
<author>Christopher D Manning</author>
</authors>
<title>Multi-instance multi-label learning for relation extraction.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLPCoNLL ’12,</booktitle>
<pages>455--465</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="2969" citStr="Surdeanu et al. (2012)" startWordPosition="470" endWordPosition="473">f them. Consequently, a number of models have been proposed in literature to provide better heuristics for the mapping between the entity pair in the database and its mentions in the sentences of the corpus. Riedel et al. (2010) tightens the assumption of distant supervision in the following manner: “Given a pair of entities and their mentions in sentences from a corpus, at least one of the mentions express the relation given in the database”. In other words, it models the problem as that of multi-instance (mentions) single-label (relation) learning. Following this, Hoffmann et al. (2011) and Surdeanu et al. (2012) propose models that consider the mapping as that of multi-instance multi-label learning. The instances are the mentions of the entity pair in the sentences of the corpus and the entity pair can participate in more than one relation. Although, these models work very well in practice, they have a number of shortcomings. One of them is the possibility that during the alignment, a fact in the database might not have an instantiation in the corpus. For instance, if our corpus only contains documents from the years 2000 to 2005, the fact presidentOf(Barack Obama, United States) will not be present </context>
<context position="10670" citStr="Surdeanu et al. (2012)" startWordPosition="1833" endWordPosition="1836">s is the same formulation as Hoffmann but expressed as an ILP problem. However, posing the inference as 2. zj In the above formulation, the objective function is augmented with a soft penalty. Also the third constraint is modified with this penalty term. We not instan Relation Extraction in the paradigm of distant supervision was introduced by Craven and Kumlien (1999). They used a biological database as the source of distant supervision to discover relations between biological entities. The progression of models for information extraction using distant supervision was presented in Section 1. Surdeanu et al. (2012) discuss a noisy-or method for combining the scores of various sentence level models to rank a relation during evaluation. In our approach, we introduce the noisy-or mechanism in the training phase of the algorithm. 4 Experiments The experimental runs were carried out using the 1939 Table 1 : Highest F1 point in P/R curve : KBP Dataset Precision Recall F1 Hoffmann 0.306451619 0.197916672 0.2405063349 MIMLRE 0.28061223 0.286458343 0.2835051518 Noisy-OR 0.297002733 0.189236104 0.2311770916 Hoffmann-ilp 0.293010741 0.189236104 0.2299577976 Datasets and Evaluation We report results on two standard</context>
<context position="12698" citStr="Surdeanu et al. (2012)" startWordPosition="2160" endWordPosition="2163">P dataset (larger of the two datasets), Hoffmann takes around 1hr to run. Our ILP formulation takes around 8.5hrs. However, MIMLRE algorithm (EM-based) takes around 23hrs to converge. Results We would primarily like to highlight two settings on which we report the P/R curves and contrast it with Hoffmann et al. (2011). Firstly, we replace the approximate inference in that work with our ILP-based exact inference; we call this setting the hoffmann-ilp. Secondly, we replace the deterministic-or in the model with a noisy-or, and call this setting the noisy-or. We further compare our approach with Surdeanu et al. (2012). The P/R curves for the various techniques on the two datasets are shown in Figures 1 and 2. We further report the highest F1 point in the P/R curve for both the datasets in Tables 1 and 2. Discussion We would like to discuss the results in the above two scenarios. 5http://lpsolve.sourceforge.net/5.5/ Figure 1: Results: KBP dataset Figure 2: Results: Riedel dataset 1. Performance of hoffmann-ilp On the KBP dataset, we observe that hoffmann-ilp has higher precision in the range of 0.05 to 0.1 at lower recall (0 to 0.04). In other parts of the curve it is very close to the baseline (although ho</context>
<context position="14656" citStr="Surdeanu et al., 2012" startWordPosition="2486" endWordPosition="2489">call precision 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 hoffmann noisyOr hoffmann_ilp mimlre 1940 Table 2 : Highest F1 point in P/R curve : Riedel Dataset Precision Recall F1 Hoffmann 0.32054795 0.24049332 0.27480916 MIMLRE 0.28061223 0.28645834 0.28350515 Noisy-OR 0.317 0.18139774 0.23075178 Hoffmann-ilp 0.36701337 0.12692702 0.18862161 In Figure 1 we see that there is a big jump in precision (around 0.4) of noisy-or compared to Hoffmann’s model in most parts of the curve on the KBP dataset. However, in Figure 2 (Riedel dataset), we do not see such a trend. Although, we do perform better than MIMLRE (Surdeanu et al., 2012) (precision &gt; 0.15 for recall &lt; 0.15). On both datasets, noisy-or has higher precision than MIMLRE, as seen from Tables 1 and 2. However, the recall reduces. More investigation in this direction is part of future work. 5 Conclusion In this paper we described an important addition to Hoffmann’s model by the use of the noisy-or soft constraint to further relax the at least one assumption. Since we posed the inference procedures in Hoffmann using ILP, we could easily add this constraint during the training and inference. Empirically, we showed that the resulting P/R curves have a significant perf</context>
</contexts>
<marker>Surdeanu, Tibshirani, Nallapati, Manning, 2012</marker>
<rawString>Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati, and Christopher D. Manning. 2012. Multi-instance multi-label learning for relation extraction. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLPCoNLL ’12, pages 455–465, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
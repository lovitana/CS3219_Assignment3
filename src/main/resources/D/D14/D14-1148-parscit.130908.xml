<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9766335">
Using Structured Events to Predict Stock Price Movement:
An Empirical Investigation
</title>
<author confidence="0.928177">
Xiao Ding†∗, Yue Zhang‡, Ting Liu†, Junwen Duan††Research Center for Social Computing and Information Retrieval
</author>
<affiliation confidence="0.932489">
Harbin Institute of Technology, China
</affiliation>
<email confidence="0.673868">
{xding, tliu, jwduan}@ir.hit.edu.cn
</email>
<affiliation confidence="0.993368">
‡Singapore University of Technology and Design
</affiliation>
<email confidence="0.976476">
yue zhang@sutd.edu.sg
</email>
<sectionHeader confidence="0.993961" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999946259259259">
It has been shown that news events influ-
ence the trends of stock price movements.
However, previous work on news-driven
stock market prediction rely on shallow
features (such as bags-of-words, named
entities and noun phrases), which do not
capture structured entity-relation informa-
tion, and hence cannot represent complete
and exact events. Recent advances in
Open Information Extraction (Open IE)
techniques enable the extraction of struc-
tured events from web-scale data. We
propose to adapt Open IE technology for
event-based stock price movement pre-
diction, extracting structured events from
large-scale public news without manual
efforts. Both linear and nonlinear mod-
els are employed to empirically investigate
the hidden and complex relationships be-
tween events and the stock market. Large-
scale experiments show that the accuracy
of S&amp;P 500 index prediction is 60%, and
that of individual stock prediction can be
over 70%. Our event-based system out-
performs bags-of-words-based baselines,
and previously reported systems trained on
S&amp;P 500 stock historical data.
</bodyText>
<sectionHeader confidence="0.998878" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9489605">
Predicting stock price movements is of clear in-
terest to investors, public companies and govern-
ments. There has been a debate on whether the
market can be predicted. The Random Walk The-
ory (Malkiel, 1973) hypothesizes that prices are
determined randomly and hence it is impossible to
outperform the market. However, with advances
of AI, it has been shown empirically that stock
This work was done while the first author was visiting
Singapore University of Technology and Design
</bodyText>
<figureCaption confidence="0.8635645">
Figure 1: Example news for Apple Inc. and
Google Inc.
</figureCaption>
<bodyText confidence="0.999916875">
price movement is predictable (Bondt and Thaler,
1985; Jegadeesh, 1990; Lo and MacKinlay, 1990;
Jegadeesh and Titman, 1993). Recent work (Das
and Chen, 2007; Tetlock, 2007; Tetlock et al.,
2008; Si et al., 2013; Xie et al., 2013; Wang and
Hua, 2014) has applied Natural Language Process-
ing (NLP) techniques to help analyze the effect of
web texts on stock market prediction, finding that
events reported in financial news are important ev-
idence to stock price movement prediction.
As news events affect human decisions and the
volatility of stock prices is influenced by human
trading, it is reasonable to say that events can influ-
ence the stock market. Figure 1 shows two pieces
of financial news about Apple Inc. and Google
Inc., respectively. Shares of Apple Inc. fell as trad-
ing began in New York on Thursday morning, the
day after its former CEO Steve Jobs passed away.
Google’s stock fell after grim earnings came out.
Accurate extraction of events from financial news
may play an important role in stock market pre-
diction. However, previous work represents news
documents mainly using simple features, such as
bags-of-words, noun phrases, and named entities
(Lavrenko et al., 2000; Kogan et al., 2009; Luss
and d’Aspremont, 2012; Schumaker and Chen,
2009). With these unstructured features, it is dif-
ficult to capture key events embedded in financial
news, and even more difficult to model the impact
of events on stock market prediction. For exam-
ple, representing the event “Apple has sued Sam-
sung Electronics for copying ‘the look and feel’
</bodyText>
<page confidence="0.927765">
1415
</page>
<note confidence="0.897204">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1415–1425,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999936714285714">
of its iPad tablet and iPhone smartphone.” using
term-level features {“Apple”, “sued”, “Samsung”,
“Electronics”, “copying”, ...I alone, it can be dif-
ficult to accurately predict the stock price move-
ments of Apple Inc. and Samsung Inc., respec-
tively, as the unstructured terms cannot indicate
the actor and object of the event.
In this paper, we propose using structured in-
formation to represent events, and develop a pre-
diction model to analyze the relationship between
events and the stock market. The problem is im-
portant because it provides insights into under-
standing the underlying mechanisms of the influ-
ence of events on the stock market. There are two
main challenges to this method. On the one hand,
how to obtain structured event information from
large-scale news streams is a challenging problem.
We propose to apply Open Information Extraction
techniques (Open IE; Banko et al. (2007); Et-
zioni et al. (2011); Fader et al. (2011)), which
do not require predefined event types or manu-
ally labeled corpora. Subsequently, two ontolo-
gies (i.e. VerbNet and WordNet) are used to gen-
eralize structured event features in order to reduce
their sparseness. On the other hand, the problem
of accurately predicting stock price movement us-
ing structured events is challenging, since events
and the stock market can have complex relations,
which can be influenced by hidden factors. In ad-
dition to the commonly used linear models, we
build a deep neural network model, which takes
structured events as input and learn the potential
relationships between events and the stock market.
Experiments on large-scale financial news
datasets from Reuters1 (106,521 documents)
and Bloomberg2 (447,145 documents) show that
events are better features for stock market predic-
tion than bags-of-words. In addition, deep neu-
ral networks achieve better performance than lin-
ear models. The accuracy of S&amp;P 500 index pre-
diction by our approach outperforms previous sys-
tems, and the accuracy of individual stock predic-
tion can be over 70% on the large-scale data.
Our system can be regarded as one step towards
building an expert system that exploits rich knowl-
edge for stock market prediction. Our results are
helpful for automatically mining stock price re-
lated news events, and for improving the accuracy
of algorithm trading systems.
</bodyText>
<footnote confidence="0.608134">
lhttp://www.reuters.com/
zhttp://www.bloomberg.com/
</footnote>
<sectionHeader confidence="0.956473" genericHeader="introduction">
2 Method
</sectionHeader>
<subsectionHeader confidence="0.996191">
2.1 Event Representation
</subsectionHeader>
<bodyText confidence="0.998642972972973">
We follow the work of Kim (1993) and design a
structured representation scheme that allows us to
extract events and generalize them. Kim defines
an event as a tuple (OZ, P, T), where OZ C O is
a set of objects, P is a relation over the objects
and T is a time interval. We propose a representa-
tion that further structures the event to have roles
in addition to relations. Each event is composed
of an action P, an actor O1 that conducted the
action, and an object O2 on which the action was
performed. Formally, an event is represented as
E = (O1, P, O2, T), where P is the action, O1
is the actor, O2 is the object and T is the timestamp
(T is mainly used for aligning stock data with
news data). For example, the event “Sep 3, 2013
- Microsoft agrees to buy Nokia’s mobile phone
business for $7.2 billion.” is modeled as: (Actor =
Microsoft, Action = buy, Object = Nokia’s mobile
phone business, Time = Sep 3, 2013).
Previous work on stock market prediction rep-
resents events as a set of individual terms (Fung
et al., 2002; Fung et al., 2003; Hayo and Ku-
tan, 2004; Feldman et al., 2011). For example,
“Microsoft agrees to buy Nokia’s mobile phone
business for $7.2 billion.” can be represented by
{“Microsoft”, “agrees”, “buy”, “Nokia’s”, “mo-
bile”, ...I and “Oracle has filed suit against Google
over its ever-more-popular mobile operating sys-
tem, Android.” can be represented by {“Oracle”,
“has”, “filed”, “suit”, “against”, “Google”, ...I.
However, terms alone might fail to accurately pre-
dict the stock price movement of Microsoft, Nokia,
Oracle and Google, because they cannot indicate
the actor and object of the event. To our knowl-
edge, no effort has been reported in the literature
to empirically investigate structured event repre-
sentations for stock market prediction.
</bodyText>
<subsectionHeader confidence="0.999694">
2.2 Event Extraction
</subsectionHeader>
<bodyText confidence="0.9999632">
A main contribution of our work is to extract and
use structured events instead of bags-of-words in
prediction models. However, structured event ex-
traction can be a costly task, requiring predefined
event types and manual event templates (Ji and Gr-
ishman, 2008; Li et al., 2013). Partly due to this,
the bags-of-words-based document representation
has been the mainstream method for a long time.
To tackle this issue, we resort to Open IE, extract-
ing event tuples from wide-coverage data with-
</bodyText>
<page confidence="0.978305">
1416
</page>
<listItem confidence="0.711683179487179">
out requiring any human input (e.g. templates).
Our system is based on the system of Fader et al.
(2011) and the work of Ding et al. (2013); it does
not require predefined target event types and la-
beled training examples. Given a natural language
sentence obtained from news texts, the following
procedure is used to extract structured events:
1. Event Phrase Extraction. We extract the
predicate verb P of a sentence based on
the dependency parser of Zhang and Clark
(2011), and then find the longest sequence of
words Pv, such that Pv starts at P and satis-
fies the syntactic and lexical constraints pro-
posed by Fader et al. (2011). The content of
these two constraints are as follows:
• Syntactic constraint: every multi-word
event phrase must begin with a verb, end
with a preposition, and be a contiguous
sequence of words in the sentence.
• Lexical constraint: an event phrase
should appear with at least a minimal
number of distinct argument pairs in a
large corpus.
2. Argument Extraction. For each event
phrase Pv identified in the step above, we find
the nearest noun phrase O1 to the left of Pv
in the sentence, and O1 should contain the
subject of the sentence (if it does not contain
the subject of Pv, we find the second near-
est noun phrase). Analogously, we find the
nearest noun phrase O2 to the right of Pv in
the sentence, and O2 should contain the ob-
ject of the sentence (if it does not contain the
object of Pv, we find the second nearest noun
phrase).
An example of the extraction algorithm is as fol-
lows. Consider the sentence,
Instant view: Private sector adds 114,000 jobs
in July: ADP.
</listItem>
<bodyText confidence="0.999873">
The predicate verb is identified as “adds”, and
its subject and object “sector” and “jobs”, respec-
tively. The structured event is extracted as (Private
sector, adds, 114,000 jobs).
</bodyText>
<subsectionHeader confidence="0.99949">
2.3 Event Generalization
</subsectionHeader>
<bodyText confidence="0.999855">
Our goal is to train a model that is able to make
predictions based on various expressions of the
same event. For example, “Microsoft swallows
Nokia’s phone business for $7.2 billion” and “Mi-
crosoft purchases Nokia’s phone business” report
the same event. To improve the accuracy of our
prediction model, we should endow the event ex-
traction algorithm with generalization capacity.
To this end, we leverage knowledge from two
well-known ontologies, WordNet (Miller, 1995)
and VerbNet (Kipper et al., 2006). The pro-
cess of event generalization consists of two steps.
First, we construct a morphological analysis tool
based on the WordNet stemmer to extract lemma
forms of inflected words. For example, in “In-
stant view: Private sector adds 114,000 jobs in
July.”, the words “adds” and “jobs” are trans-
formed to “add” and “job”, respectively. Second,
we generalize each verb to its class name in Verb-
Net. For example, “add” belongs to the multi-
ply class. After generalization, the event (Private
sector, adds, 114,000 jobs) becomes (private sec-
tor, multiply class, 114,000 job). Similar methods
on event generalization have been investigated in
Open IE based event causal prediction (Radinsky
and Horvitz, 2013).
</bodyText>
<subsectionHeader confidence="0.983706">
2.4 Prediction Models
</subsectionHeader>
<listItem confidence="0.658092">
1. Linear model. Most previous work uses linear
models to predict the stock market (Fung et al.,
2002; Luss and d’Aspremont, 2012; Schumaker
and Chen, 2009; Kogan et al., 2009; Das and
Chen, 2007; Xie et al., 2013). To make direct com-
parisons, this paper constructs a linear prediction
model by using Support Vector Machines (SVMs),
a state-of-the-art classification model. Given a
training set (d1, y1), (d2, y2), ..., (dN, yN),
where n E [1, N], dn is a news document and
yi E {+1, −11 is the output class. dn can be
news titles, news contents or both. The output
Class +1 represents that the stock price will in-
crease the next day/week/month, and the output
Class -1 represents that the stock price will de-
crease the next day/week/month. The features
can be bag-of-words features or structured event
features. By SVMs, y = arg max{Class +
1, Class − 11 is determined by the linear func-
tion w · 4i(dn, yn), where w is the feature weight
vector, and 4i(dn, yn) is a function that maps dn
into a M-dimensional feature space. Feature tem-
plates will be discussed in the next subsection.
2. Nonlinear model. Intuitively, the relationship
between events and the stock market may be more
complex than linear, due to hidden and indirect
</listItem>
<page confidence="0.996568">
1417
</page>
<figureCaption confidence="0.921828">
Figure 2: Structure of the deep neural network
model
</figureCaption>
<bodyText confidence="0.9995296">
relationships. We exploit a deep neural network
model, the hidden layers of which is useful for
learning such hidden relationships. The structure
of the model with two hidden layers is illustrated
in Figure 2. In all layers, the sigmoid activation
function σ is used.
Let the values of the neurons of the output layer
be ycls (cls E 1+1, −1}), its input be netcls, and
y2 be the value vector of the neurons of the last
hidden layer; then:
</bodyText>
<equation confidence="0.993221">
ycls = f(netcls) = σ(wcls &apos; y2) (1)
</equation>
<bodyText confidence="0.999978333333333">
where wcls is the weight vector between the neu-
ron cls of the output layer and the neurons of the
last hidden layer. In addition,
</bodyText>
<equation confidence="0.9982455">
y2k = σ(w2k &apos; y1) (k E [1, |y2|]) (2)
y1j = σ(w1j &apos; `)(dn)) (j E [1, |y1|])
</equation>
<bodyText confidence="0.975881058823529">
Here y1 is the value vector of the neu-
rons of the first hidden layer,
(w2k1, w2k2, ..., w2k|y1|), k E [1, |y2|] and
w1j = (w1j1, w1j2, ..., w1jM), j E [1, |y1|].
w2kj is the weight between the kth neuron of
the last hidden layer and the jth neuron of the
first hidden layer; w1jm is the weight between
the jth neuron of the first hidden layer and the
mth neuron of the input layer m E [1, M]; dn
is a news document and 4(dn) maps dn into a
M-dimensional features space. News documents
and features used in the nonlinear model are the
same as those in the linear model, which will be
introduced in details in the next subsection. The
standard back-propagation algorithm (Rumelhart
et al., 1985) is used for supervised training of the
neural network.
</bodyText>
<table confidence="0.53669025">
train dev test
number of 1425 178 179
instances
number of 54776 6457 6593
events
time inter- 02/10/2006 19/06/2012 22/02/2013
val - - -
18/16/2012 21/02/2013 21/11/2013
</table>
<tableCaption confidence="0.997781">
Table 1: Dataset splitting
</tableCaption>
<subsectionHeader confidence="0.996629">
2.5 Feature Representation
</subsectionHeader>
<bodyText confidence="0.999715">
In this paper, we use the same features (i.e. docu-
ment representations) in the linear and nonlinear
prediction models, including bags-of-words and
structured events.
</bodyText>
<listItem confidence="0.988327">
(1) Bag-of-words features. We use the clas-
</listItem>
<bodyText confidence="0.902937777777778">
sic “TFIDF” score for bag-of-words features. Let
L be the vocabulary size derived from the train-
ing data (introduced in the next section), and
freq(tl) denote the number of occurrences of
the lth word in the vocabulary in document d.
TFl = 1
|d|freq(tl), bl E [1, L], where |d |is
the number of words in the document d (stop
words are removed). TFIDFl = 1
</bodyText>
<equation confidence="0.999071">
|d|freq(tl) x
log( �
</equation>
<bodyText confidence="0.990152285714286">
|{d:����(t`)&gt;0}|), where N is the number of
documents in the training set. The feature vector
4) can be represented as 4) = (ϕ1, ϕ2, ..., ϕM) =
(TFIDF1, TFIDF2, ..., TFIDFi12 ). The TFIDF
feature representation has been used by most pre-
vious studies on stock market prediction (Kogan et
al., 2009; Luss and d’Aspremont, 2012).
</bodyText>
<listItem confidence="0.913128571428571">
(2) Event features. We represent an event
tuple (O1, P, O2, T) by the combination of
elements (except for T) (O1, P, O2, O1 + P,
P + O2, O1 + P + O2). For example, the
event tuple (Microsoft, buy, Nokia’s mobile phone
business) can be represented as (#arg1=Microsoft,
#action=get class, #arg2=Nokia’s mobile phone
</listItem>
<bodyText confidence="0.995117857142857">
business, #arg1 action=Microsoft get class,
#action arg2=get class Nokia’s mobile phone
business, #arg1 action arg2=Microsoft get class
Nokia’s mobile phone business). Structured
events are more sparse than words, and we reduce
sparseness by two means. First, verb classes
(Section 2.3) are used instead of verbs for P. For
example, “get class” is used instead of the verb
“buy”. Second, we use back-off features, such
as O1 + P (“Microsoft get class”) and P + O2
(“get class Nokia’s mobile phone business”), to
address the sparseness of O1 and O2. Note that the
order of O1 and O2 is important for our task since
they indicate the actor and object, respectively.
</bodyText>
<figure confidence="0.989871574074074">
Class +1
The polarity of the stock
price movement is
positive
Class -1
The polarity of the stock
price movement is
negative
...
...
...
Output
Layer
Hidden
Layers
Input
Layer
ʔ1
ʔ2 ʔ3 ʔM
News documents
w2k =
1418
0.14
0.12
0.1
0.08
MCC
0.06
0.04
0.02
0
1day 1week 1month
Time span
(a) Accuarcy
1day 1week 1month
Time span
(b) MCC
bow+svm
bow+deep neural network
event+svm
event+deep neural network
bow+svm
bow+deep neural network
event+svm
event+deep neural network
Accuracy 0.6
0.59
0.58
0.57
0.56
0.55
0.54
0.53
0.52
</figure>
<figureCaption confidence="0.999547">
Figure 3: Overall development experiment results
</figureCaption>
<sectionHeader confidence="0.998954" genericHeader="background">
3 Experiments
</sectionHeader>
<bodyText confidence="0.99996775">
Our experiments are carried out on three differ-
ent time intervals: short term (1 day), medium
term (1 week) and long term (1 month). We test
the influence of events on predicting the polarity
of stock change for each time interval, comparing
the event-based news representation with bag-of-
words-based news representations, and the deep
neural network model with the SVM model.
</bodyText>
<subsectionHeader confidence="0.995421">
3.1 Data Description
</subsectionHeader>
<bodyText confidence="0.968889057142857">
We use publicly available financial news from
Reuters and Bloomberg over the period from Oc-
tober 2006 to November 2013. This time span
witnesses a severe economic downturn in 2007-
2010, followed by a modest recovery in 2011-
2013. There are 106,521 documents in total
from Reuters News and 447,145 from Bloomberg
News. News titles and contents are extracted from
HTML. The timestamps of the news are also ex-
tracted, for alignment with stock price informa-
tion. The data size is larger than most previous
work in the literature.
We mainly focus on predicting the change of the
Standard &amp; Poor’s 500 stock (S&amp;P 500) index3,
obtaining indices and stock price data from Yahoo
Finance. To justify the effectiveness of our predic-
tion model, we also predict price movements of
fifteen individual shares from different sectors in
S&amp;P 500. We automatically align 1,782 instances
of daily trading data with news titles and contents
from the previous day/the day a week before the
stock price data/the day a month before the stock
price data, 4/5 of which are used as the training
3Standard &amp; Poor’s 500 is a stock market index based
on the market capitalizations of 500 large companies having
common stock listed on the NYSE or NASDAQ.
data, 1/10 for development testing and 1/10 for
testing. As shown in Table 1, the training, devel-
opment and test set are split temporally, with the
data from 02/10/2006 to 18/16/2012 for training,
the data from 19/06/2012 to 21/02/2013 for de-
velopment testing, and the data from 22/02/2013
to 21/11/2013 for testing. There are about 54,776
events in the training set, 6,457 events in the de-
velopment set and 6,593 events in the test set.
</bodyText>
<subsectionHeader confidence="0.995415">
3.2 Evaluation Metrics
</subsectionHeader>
<bodyText confidence="0.999802823529412">
We use two assessment metrics. First, a standard
and intuitive approach to measuring the perfor-
mance of classifiers is accuracy. However, this
measure is very sensitive to data skew: when a
class has an overwhelmingly high frequency, the
accuracy can be high using a classifier that makes
prediction on the majority class. Previous work
(Xie et al., 2013) uses an additional evaluation
metric, which relies on the Matthews Correlation
Cofficient (MCC) to avoid bias due to data skew
(our data are rather large and not severely skewed,
but we also use MCC for comparison with previ-
ous work). MCC is a single summary value that
incorporates all 4 cells of a 2*2 confusion matrix
(True Positive, False Positive, True Negative and
False Negative, respectively). Given TP, TN, FP
and FN:
</bodyText>
<equation confidence="0.8082212">
MCC =
TP ·TN−FP ·FN
√
(TP+FP)(TP+FN)(TN +FP)(TN +FN)
(3)
</equation>
<subsectionHeader confidence="0.983022">
3.3 Overall Development Results
</subsectionHeader>
<bodyText confidence="0.999758333333333">
We evaluate our four prediction methods (i.e.
SVM with bag-of-word features (bow), deep neu-
ral network with bag-of-word features (bow),
</bodyText>
<page confidence="0.944617">
1419
</page>
<table confidence="0.46783974137931">
All News
Company News
Sector News
Google Inc.
1 day 1 week 1 month
1 layer Accuracy 58.94% 57.73% 55.76%
MCC 0.1249 0.0916 0.0731
2 layers Accuracy 59.60% 57.73% 56.19%
MCC 0.1683 0.1215 0.0875
Acc
67.86%
MCC
0.4642
Acc
61.17%
MCC
0.2301
Acc
55.70%
MCC
0.1135
Boeing Company
68.75%
0.4339
57.14%
0.1585
56.04%
0.1605
All News
Company News
MCC
Acc
MCC
Acc
MCC
Sector News
Acc
Table 2: Different numbers of hidden layers
Company News
Acc
MCC
70.45%
0.4679
MCC
Acc
MCC
0.1605
56.04%
0.2703
title content content + bloomberg
title title + title
Acc 59.60% 54.65% 56.83% 59.64%
MCC 0.1683 0.0627 0.0852 0.1758
Wal-Mart Stores
Sector News
Acc
62.03%
All News
</table>
<tableCaption confidence="0.999335">
Table 3: Different amounts of data Table 4: Individual stock prediction results
</tableCaption>
<bodyText confidence="0.998956">
SVM with event features and deep neural network
with event features) on three time intervals (i.e.
1 day, 1 week and 1 month, respectively) on the
development dataset, and show the results in Fig-
ure 3. We find that:
</bodyText>
<listItem confidence="0.7980953">
(1) Structured event is a better choice for rep-
resenting news documents. Given the same pre-
diction model (SVM or deep neural network), the
event-based method achieves consistently better
performance than the bag-of-words-based method
over all three time intervals. This is likely due
to the following two reasons. First, being an ex-
traction of predicate-argument structures, events
carry the most essential information of the docu-
ment. In contrast, bag-of-words can contain more
</listItem>
<bodyText confidence="0.868069">
irrelevant information. Second, structured events
can directly give the actor and object of the action,
which is important for predicting stock market.
(2) The deep neural network model achieves
better performance than the SVM model, partly by
learning hidden relationships between structured
events and stock prices. We give analysis to these
relationships in the next section.
</bodyText>
<listItem confidence="0.8888618125">
(3) Event information is a good indicator for
short-term volatility of stock prices. As shown in
Figure 3, the performance of daily prediction is
better than weekly and monthly prediction. Our
experimental results confirm the conclusion of
Tetlock, Saar-Tsechansky, and Macskassy (2008)
that there is a one-day delay between the price
response and the information embedded in the
news. In addition, we find that some events may
cause immediate changes of stock prices. For ex-
ample, former Microsoft CEO Steve Ballmer an-
nounced he would step down within 12 months
on 23/08/2013. Within an hour, Microsoft shares
jumped as much as 9 percent. This fact indicates
that it may be possible to predict stock price move-
ment on a shorter time interval than one day. How-
</listItem>
<bodyText confidence="0.99823">
ever, we cannot access fine-grained stock price
historical data, and this investigation will be left
as future work.
</bodyText>
<subsectionHeader confidence="0.747970666666667">
3.4 Experiments with Different Numbers of
Hidden Layers of the Deep Neural
Network Model
</subsectionHeader>
<bodyText confidence="0.999928181818182">
Cybenko (1989) states that when every processing
element utilizes the sigmoid activation function,
one hidden layer is enough to solve any discrim-
inant classification problem, and two hidden lay-
ers are capable to parse arbitrary output functions
of input pattern. Here we conduct a development
experiment by different number of hidden layers
for the deep neural network model. As shown in
Table 2, the performance of two hidden layers is
better than one hidden layer, which is consistent
with the experimental results of Sharda and De-
len (2006) on the task of movie box-office predic-
tion. It indicates that more hidden layers can ex-
plain more complex relations (Bengio, 2009). In-
tuitively, three or more hidden layers may achieve
better performance. However, three hidden lay-
ers mean that we construct a five-layer deep neu-
ral network, which is difficult to train (Bengio et
al., 1994). We did not obtain improved accuracies
using three hidden layers, due to diminishing gra-
dients. A deep investigation of this problem is out
of the scope of this paper.
</bodyText>
<subsectionHeader confidence="0.749912">
3.5 Experiments with Different Amounts of
Data
</subsectionHeader>
<bodyText confidence="0.999958285714286">
We conduct a development experiment by extract-
ing news titles and contents from Reuters and
Bloomberg, respectively. While titles can give the
central information about the news, contents may
provide some background knowledge or details.
Radinsky et al. (2012) argued that news titles are
more helpful for prediction compared to news con-
</bodyText>
<page confidence="0.942838">
1420
</page>
<figure confidence="0.99987956">
0.75
0.7
Accuracy
0.65
0.6
0.55
0.5
SanDisk
Actavis Gannett
0.5
0.45
0.4
0.35
MCC
0.3
0.25
0.2
0.15
k
0.1
Hershey
Actavis
Gannett
SanDis
Starbucks
Visa
individual stock
Symantec
Mattel
Wal-Mart
BoeinGgle
Nike
Qualcomm
Apache
Avon
Hershey
Starbucks
Avon
Visa
Symantec
Mattel
Nike
Apache
Qualcomm
Wal-MartGoogle
Boeing
individual stock
0 100 200 300 400 500 0 100 200 300 400 500
Company Ranking Company Ranking
(a) Accuarcy (b) MCC
</figure>
<figureCaption confidence="0.999973">
Figure 4: Individual stock prediction experiment results
</figureCaption>
<bodyText confidence="0.999984285714286">
tents, and this paper mainly uses titles. Here we
design a comparative experiment to analyze the ef-
fectiveness of news titles and contents. First, we
use Reuters news to compare the effectiveness of
news titles and contents, and then add Bloomberg
news titles to investigate whether the amounts of
data matters. Table 3 shows that using only news
titles achieves the best performance. A likely rea-
son is that we may extract some irrelevant events
from news contents.
With the additional Bloomberg data, the results
are not dramatically improved. This is intuitively
because most events are reported by both Reuters
news and Bloomberg news. We randomly se-
lect about 9,000 pieces of news documents from
Reuters and Bloomberg and check the daily over-
lap manually, finding that about 60% of the news
are reported by both Reuters and Bloomberg. The
overlap of important news (news related to S&amp;P
500 companies) is 80% and the overlap of unim-
portant news is 40%.
</bodyText>
<subsectionHeader confidence="0.991836">
3.6 Individual Stock Prediction
</subsectionHeader>
<bodyText confidence="0.996375361702128">
In addition to predicting the S&amp;P 500 index, we
also investigate the effectiveness of our approach
on the problem of individual stock prediction us-
ing the development dataset. We select three well-
known companies, Google Inc., Boeing Company
and Wal-Mart Stores from three different sec-
tors (i.e. Information Technology, Industrials and
Consumer Staples, respectively) classified by the
Global Industry Classification Standard (GICS).
We use company news, sector news and all news to
predict individual stock price movement, respec-
tively. The experimental results are listed in Ta-
ble 4.
The result of individual stock prediction by us-
ing only company news dramatically outperforms
the result of S&amp;P 500 index prediction. The main
reason is that company-related events can directly
affect the volatility of company shares. There is
a strong correlation between company events and
company shares. Table 4 also shows that the result
of individual stock prediction by using sector news
or all news does not achieve a good performance,
probably because there are many irrelevant events
in all news, which would reduce the performance
of our prediction model.
The fact that the accuracy of these well-known
stocks are higher than the index may be because
there is relatively more news events dedicated to
the relevant companies. To gain a better under-
standing of the behavior of the model on more
individual stocks, we randomly select 15 compa-
nies (i.e. Google Inc., Boeing Company, Wal-Mart
Stores, Nike Inc., QUALCOMM Inc., Apache Cor-
poration, Starbucks Corp., Avon Products, Visa
Inc., Symantec Corp., The Hershey Company,
Mattel Inc., Actavis plc, Gannett Co. and SanDisk
Corporation) from S&amp;P 500 companies. More
specifically, according to the Fortune ranking of
S&amp;P 500 companies4, we divide the ranked list
into five parts, and randomly select three compa-
nies from each part. The experimental results are
shown in Figure 4. We find that:
(1) All 15 individual stocks can be predicted
with accuracies above 50%, while 60% of the
stocks can be predicted with accuracies above
60%. It shows that the amount of company-related
events has strong relationship with the volatility of
</bodyText>
<footnote confidence="0.9742386">
4http://money.cnn.com/magazines/fortune/fortune500/.
The amount of company-related news is correlated to the
fortune ranking of companies. However, we find that the
trade volume does not have such a correlation with the
ranking.
</footnote>
<page confidence="0.931804">
1421
</page>
<table confidence="0.9991168">
S&amp;P 500 Index Prediction Individual Stock Prediction
Google Inc. Boeing Company Wal-Mart Stores
Accuracy MCC Accuracy MCC Accuracy MCC Accuracy MCC
dev 59.60% 0.1683 67.86% 0.4642 68.75% 0.4339 70.45% 0.4679
test 58.94% 0.1649 66.97% 0.4435 68.03% 0.4018 69.87% 0.4456
</table>
<tableCaption confidence="0.998967">
Table 5: Final experimental results on the test dataset
</tableCaption>
<bodyText confidence="0.975892857142857">
company shares.
(2) With decreasing company fortune rankings,
the accuracy and MCC decrease. This is mainly
because there is not as much daily news about low-
ranking companies, and hence one cannot extract
enough structured events to predict the volatility
of these individual stocks.
</bodyText>
<subsectionHeader confidence="0.994007">
3.7 Final Results
</subsectionHeader>
<bodyText confidence="0.999849">
The final experimental results on the test dataset
are shown in Table 5 (as space is limited, we show
the results on the time interval of one day only).
The experimental results on the development and
test datasets are consistent, which indicate that our
approach has good robustness. The following con-
clusions obtained from development experiments
also hold on the test dataset:
</bodyText>
<listItem confidence="0.862915352941176">
(1) Structured events are more useful represen-
tations compared to bags-of-words for the task of
stock market prediction.
(2) A deep neural network model can be more
accurate on predicting the stock market compared
to the linear model.
(3) Our approach can achieve stable experiment
results on S&amp;P 500 index prediction and individ-
ual stock prediction over a large amount of data
(eight years of stock prices and more than 550,000
pieces of news).
(4) The quality of information is more impor-
tant than the quantity of information on the task
of stock market prediction. That is to say that the
most relevant information (i.e. news title vs news
content, individual company news vs all news) is
better than more, but less relevant information.
</listItem>
<subsectionHeader confidence="0.998066">
3.8 Analysis and Discussion
</subsectionHeader>
<bodyText confidence="0.999459285714286">
We use Figure 5 to demonstrate our analysis to
the development experimental result of Google
Inc. stock prediction, which directly shows the
relationship between structured events and the
stock market. The links between each layer show
the magnitudes of feature weights in the model
learned using the training set.
</bodyText>
<footnote confidence="0.357209">
Three events, (Google, says bought stake in,
China’s XunLei), (Google, reveals stake in, Chi-
</footnote>
<note confidence="0.343052">
ʔ1 ʔ2 03 ʔ4 ʔ5 ʔ6 07 ʔ8 0M
</note>
<tableCaption confidence="0.971552166666667">
ʔ 1: (Google, says bought stake in, China’s XunLei)
ʔ 4: (Google, reveals stake in, Chinese social website)
ʔ 6: (Capgemini, partners, Google apps software)
ʔ 2: (Oracle, sues, Google)
ʔ 5: (Google map, break, privacy law)
ʔ 8: (Google, may pull out of, China)
</tableCaption>
<figureCaption confidence="0.964047">
Figure 5: Prediction of Google Inc. (we only show
structured event features since backoff features are
less informative)
</figureCaption>
<bodyText confidence="0.999825714285714">
nese social website) and (Capgemini, partners,
Google apps software), have the highest link
weights to the first hidden node (from the left).
These three events indicate that Google constantly
makes new partners and expands its business area.
The first hidden node has high-weight links to
Class +1, showing that Google’s positive coopera-
tion can lead to the rise of its stock price.
Three other events, (Oracle, sues, Google),
(Google map, break, privacy law) and (Google,
may pull out of, China), have high-weight links
to the second hidden node. These three events
show that Google was suffering questions and
challenges, which could affect its reputation and
further pull down its earnings. Correspondingly,
the second hidden node has high-weight links to
Class -1. These suggest that our method can au-
tomatically and directly reveal complex relation-
ships between structured events and the stock mar-
ket, which is very useful for investors, and can fa-
cilitate the research of stock market prediction.
Note that the event features used in our predic-
tion model are generalized based on the algorithm
introduced in Section 2.5. Therefore, though a
specific event in the development test set might
have never happened, its generalized form can be
found in the training set. For example, “Google
acquired social marketing company Wildfire In-
</bodyText>
<figure confidence="0.956893">
...
...
</figure>
<page confidence="0.9853">
1422
</page>
<bodyText confidence="0.992357777777778">
teractive” is not in the training data, but “Google
get class” (“get” is the class name of “acquire”
and “buy” in VerbNet) can indeed be found in the
training set, such as “Google bought stake in Xun-
Lei” on 04/01/2007. Hence although the full spe-
cific event feature does not fire, its back-offs fire
for a correct prediction. For simplicity of showing
the event, we did not include back-off features in
Figure 5.
</bodyText>
<sectionHeader confidence="0.999958" genericHeader="related work">
4 Related Work
</sectionHeader>
<bodyText confidence="0.999977234567901">
Stock market prediction has attracted a great deal
of attention across the fields of finance, computer
science and other research communities in the
past. The literature of stock market prediction
was initiated by economists (Keynes, 1937). Sub-
sequently, the influential theory of Efficient Mar-
ket Hypothesis (EMH) (Fama, 1965) was estab-
lished, which states that the price of a security re-
flects all of the information available and that ev-
eryone has a certain degree of access to the infor-
mation. EMH had a significant impact on security
investment, and can serve as the theoretical basis
of event-based stock price movement prediction.
Various studies have found that financial news
can dramatically affect the share price of a se-
curity (Chan, 2003; Tetlock et al., 2008). Cul-
ter et al. (1998) was one of the first to investi-
gate the relationship between news coverage and
stock prices, since which empirical text analysis
technology has been widely used across numerous
disciplines (Lavrenko et al., 2000; Kogan et al.,
2009; Luss and d’Aspremont, 2012). These stud-
ies primarily use bags-of-words to represent finan-
cial news documents. However, as Schumaker and
Chen (2009) and Xie et al. (2013) point out, bag-
of-words features are not the best choice for pre-
dicting stock prices. Schumaker and Chen (2009)
extract noun phrases and named entities to aug-
ment bags-of-words. Xie et al. (2013) explore a
rich feature space that relies on frame semantic
parsing. Wang et al. (2014) use the same fea-
tures as Xie et al. (2013), but they perform non-
parametric kernel density estimation to smooth out
the distribution of features. These can be regarded
as extensions to the bag-of-word method. The
drawback of these approaches, as discussed in the
introduction, is that they do not directly model
events, which have structured information.
There has been efforts to model events more di-
rectly (Fung et al., 2002; Hayo and Kutan, 2005;
Feldman et al., 2011). Fung, Yu, and Lam (2002)
use a normalized word vector-space to model
event. Feldman et al. (2011) extract 9 prede-
fined categories of events based on heuristic rules.
There are two main problems with these efforts.
First, they cannot extract structured event (e.g. the
actor of the event and the object of the event). Sec-
ond, Feldman et al. (2011) can obtain only lim-
ited categories of events, and hence the scalabil-
ity of their work is not strong. In contrast, we
extract structured events by leveraging Open In-
formation Extraction technology (Open IE; Yates
et al. (2007); Etzioni et al. (2011); Faber et al.
(2011)) without predefined event types, which can
effectively solve the two problems above.
Apart from events, sentiment analysis is another
perspective to the problem of stock prediction
(Das and Chen, 2007; Tetlock, 2007; Tetlock et
al., 2008; Bollen et al., 2011; Si et al., 2013). Tet-
lock (2007) examines how qualitative information
(i.e. the fraction of negative words in a particular
news column) is incorporated in aggregate market
valuations. Tetlock, Saar-Tsechansky, and Mac-
skassy (2008) extend that analysis to address the
impact of negative words in all Wall Street Joural
(WSJ) and Dow Jones News Services (DJNS) sto-
ries about individual S&amp;P500 firms from 1980 to
2004. Bollen and Zeng (2011) study whether the
large-scale collective emotion on Twitter is cor-
related with the volatility of Dow Jones Indus-
trial Average (DJIA). From the experimental re-
sults, they find that changes of the public mood
match shifts in the DJIA values that occur 3 to 4
days later. Sentiment-analysis-based stock mar-
ket prediction focuses on investigating the influ-
ence of subjective emotion. However, this paper
puts emphasis on the relationship between objec-
tive events and the stock price movement, and is
orthogonal to the study of subjectivity. As a result,
our model can be combined with the sentiment-
analysis-based method.
</bodyText>
<sectionHeader confidence="0.999183" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999948375">
In this paper, we have presented a framework for
event-based stock price movement prediction. We
extracted structured events from large-scale news
based on Open IE technology and employed both
linear and nonlinear models to empirically investi-
gate the complex relationships between events and
the stock market. Experimental results showed
that events-based document representations are
</bodyText>
<page confidence="0.936311">
1423
</page>
<bodyText confidence="0.9998766">
better than bags-of-words-based methods, and
deep neural networks can model the hidden and in-
directed relationship between events and the stock
market. For further comparisons, we freely release
our data at http://ir.hit.edu.cn/∼xding/data.
</bodyText>
<sectionHeader confidence="0.99547" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999953916666667">
We thank the anonymous reviewers for their
constructive comments, and gratefully acknowl-
edge the support of the National Basic Re-
search Program (973 Program) of China via Grant
2014CB340503, the National Natural Science
Foundation of China (NSFC) via Grant 61133012
and 61202277, the Singapore Ministry of Educa-
tion (MOE) AcRF Tier 2 grant T2MOE201301
and SRG ISTD 2012 038 from Singapore Univer-
sity of Technology and Design. We are very grate-
ful to Ji Ma for providing an implementation of the
neural network algorithm.
</bodyText>
<sectionHeader confidence="0.997132" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999799702380952">
Michele Banko, Michael J Cafarella, Stephen Soder-
land, Matthew Broadhead, and Oren Etzioni. 2007.
Open information extraction for the web. In IJCAI,
volume 7, pages 2670–2676.
Yoshua Bengio, Patrice Simard, and Paolo Frasconi.
1994. Learning long-term dependencies with gra-
dient descent is difficult. Neural Networks, IEEE
Transactions on, 5(2):157–166.
Yoshua Bengio. 2009. Learning deep architectures for
ai. Foundations and trends ⃝R in Machine Learning,
2(1):1–127.
Johan Bollen, Huina Mao, and Xiaojun Zeng. 2011.
Twitter mood predicts the stock market. Journal of
Computational Science, 2(1):1–8.
Werner FM Bondt and Richard Thaler. 1985. Does
the stock market overreact? The Journal offinance,
40(3):793–805.
Wesley S Chan. 2003. Stock price reaction to news and
no-news: Drift and reversal after headlines. Journal
of Financial Economics, 70(2):223–260.
David M Cutler, James M Poterba, and Lawrence H
Summers. 1998. What moves stock prices? Bern-
stein, Peter L. and Frank L. Fabozzi, pages 56–63.
George Cybenko. 1989. Approximation by superposi-
tions of a sigmoidal function. Mathematics of con-
trol, signals and systems, 2(4):303–314.
Sanjiv R Das and Mike Y Chen. 2007. Yahoo! for
amazon: Sentiment extraction from small talk on the
web. Management Science, 53(9):1375–1388.
Xiao Ding, Bing Qin, and Ting Liu. 2013. Building
chinese event type paradigm based on trigger clus-
tering. In Proc. of IJCNLP, pages 311–319, Octo-
ber.
Oren Etzioni, Anthony Fader, Janara Christensen,
Stephen Soderland, and Mausam Mausam. 2011.
Open information extraction: The second genera-
tion. In Proceedings of the Twenty-Second inter-
national joint conference on Artificial Intelligence-
Volume Volume One, pages 3–10. AAAI Press.
Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying relations for open information ex-
traction. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
pages 1535–1545. Association for Computational
Linguistics.
Eugene F Fama. 1965. The behavior of stock-market
prices. The journal of Business, 38(1):34–105.
Ronen Feldman, Benjamin Rosenfeld, Roy Bar-Haim,
and Moshe Fresko. 2011. The stock sonarsentiment
analysis of stocks based on a hybrid approach. In
Twenty-Third IAAI Conference.
Gabriel Pui Cheong Fung, Jeffrey Xu Yu, and Wai
Lam. 2002. News sensitive stock trend prediction.
In Advances in Knowledge Discovery and Data Min-
ing, pages 481–493. Springer.
Bernd Hayo and Ali M Kutan. 2005. The impact of
news, oil prices, and global market developments
on russian financial markets1. Economics of Tran-
sition, 13(2):373–393.
Narasimhan Jegadeesh and Sheridan Titman. 1993.
Returns to buying winners and selling losers: Im-
plications for stock market efficiency. The Journal
of Finance, 48(1):65–91.
Narasimhan Jegadeesh. 1990. Evidence of predictable
behavior of security returns. The Journal of Fi-
nance, 45(3):881–898.
Heng Ji and Ralph Grishman. 2008. Refining event ex-
traction through cross-document inference. In ACL,
pages 254–262.
John Maynard Keynes. 1937. The general theory of
employment. The Quarterly Journal of Economics,
51(2):209–223.
Jaegwon Kim. 1993. Supervenience and mind: Se-
lected philosophical essays. Cambridge University
Press.
Karin Kipper, Anna Korhonen, Neville Ryant, and
Martha Palmer. 2006. Extending verbnet with novel
verb classes. In Proceedings of LREC, volume 2006,
page 1.
Shimon Kogan, Dimitry Levin, Bryan R. Routledge,
Jacob S. Sagi, and Noah A. Smith. 2009. Predicting
risk from financial reports with regression. In Proc.
NAACL, pages 272–280, Boulder, Colorado, June.
Association for Computational Linguistics.
</reference>
<page confidence="0.882994">
1424
</page>
<reference confidence="0.99986225">
Victor Lavrenko, Matt Schmill, Dawn Lawrie, Paul
Ogilvie, David Jensen, and James Allan. 2000.
Mining of concurrent text and time series. In KDD-
2000 Workshop on Text Mining, pages 37–44.
Qi Li, Heng Ji, and Liang Huang. 2013. Joint event
extraction via structured prediction with global fea-
tures. In Proc. of ACL (Volume 1: Long Papers),
pages 73–82, August.
Andrew W Lo and Archie Craig MacKinlay. 1990.
When are contrarian profits due to stock mar-
ket overreaction? Review of Financial studies,
3(2):175–205.
Ronny Luss and Alexandre d’Aspremont. 2012.
Predicting abnormal returns from news using text
classification. Quantitative Finance, pp.1–14,
doi:10.1080/14697688.2012.672762.
Burton G. Malkiel. 1973. A Random Walk Down Wall
Street. W. W. Norton, New York.
George A Miller. 1995. Wordnet: a lexical
database for english. Communications of the ACM,
38(11):39–41.
Kira Radinsky and Eric Horvitz. 2013. Mining the
web to predict future events. In Proceedings of the
sixth ACM international conference on Web search
and data mining, pages 255–264. ACM.
Kira Radinsky, Sagie Davidovich, and Shaul
Markovitch. 2012. Learning causality for
news events prediction. In Proc. of WWW, pages
909–918. ACM.
David E Rumelhart, Geoffrey E Hinton, and Ronald J
Williams. 1985. Learning internal representations
by error propagation. Technical report, DTIC Doc-
ument.
Robert P Schumaker and Hsinchun Chen. 2009.
Textual analysis of stock market prediction using
breaking financial news: The azfin text system.
ACM Transactions on Information Systems (TOIS),
27(2):12.
Ramesh Sharda and Dursun Delen. 2006. Predict-
ing box-office success of motion pictures with neu-
ral networks. Expert Systems with Applications,
30(2):243–254.
Jianfeng Si, Arjun Mukherjee, Bing Liu, Qing Li,
Huayi Li, and Xiaotie Deng. 2013. Exploiting
topic based twitter sentiment for stock prediction. In
Proc. of ACL (Volume 2: Short Papers), pages 24–
29, Sofia, Bulgaria, August. Association for Compu-
tational Linguistics.
Paul C Tetlock, Maytal Saar-Tsechansky, and Sofus
Macskassy. 2008. More than words: Quantifying
language to measure firms’ fundamentals. The Jour-
nal of Finance, 63(3):1437–1467.
Paul C Tetlock. 2007. Giving content to investor sen-
timent: The role of media in the stock market. The
Journal of Finance, 62(3):1139–1168.
William Yang Wang and Zhenhao Hua. 2014. A
semiparametric gaussian copula regression model
for predicting financial risks from earnings calls. In
Proc. of ACL, June.
Boyi Xie, Rebecca J. Passonneau, Leon Wu, and
Germ´an G. Creamer. 2013. Semantic frames to pre-
dict stock price movement. In Proc. ofACL (Volume
1: Long Papers), pages 873–883, August.
Alexander Yates, Michael Cafarella, Michele Banko,
Oren Etzioni, Matthew Broadhead, and Stephen
Soderland. 2007. Textrunner: open information ex-
traction on the web. In Proc. ofNAACL: Demonstra-
tions, pages 25–26. Association for Computational
Linguistics.
Yue Zhang and Stephen Clark. 2011. Syntactic pro-
cessing using the generalized perceptron and beam
search. Computational Linguistics, 37(1):105–151.
</reference>
<page confidence="0.992513">
1425
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.117156">
<title confidence="0.897511333333333">Using Structured Events to Predict Stock Price An Empirical Investigation Yue Ting Junwen Center for Social Computing and Information</title>
<affiliation confidence="0.785725">Harbin Institute of Technology, tliu, University of Technology and</affiliation>
<abstract confidence="0.979595206896552">yue zhang@sutd.edu.sg Abstract It has been shown that news events influence the trends of stock price movements. However, previous work on news-driven stock market prediction rely on shallow features (such as bags-of-words, named entities and noun phrases), which do not capture structured entity-relation information, and hence cannot represent complete and exact events. Recent advances in Open Information Extraction (Open IE) techniques enable the extraction of structured events from web-scale data. We propose to adapt Open IE technology for event-based stock price movement prediction, extracting structured events from large-scale public news without manual efforts. Both linear and nonlinear models are employed to empirically investigate the hidden and complex relationships between events and the stock market. Largescale experiments show that the accuracy of S&amp;P 500 index prediction is 60%, and that of individual stock prediction can be over 70%. Our event-based system outperforms bags-of-words-based baselines, and previously reported systems trained on S&amp;P 500 stock historical data.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Michele Banko</author>
<author>Michael J Cafarella</author>
<author>Stephen Soderland</author>
<author>Matthew Broadhead</author>
<author>Oren Etzioni</author>
</authors>
<title>Open information extraction for the web. In</title>
<date>2007</date>
<booktitle>IJCAI,</booktitle>
<volume>7</volume>
<pages>2670--2676</pages>
<contexts>
<context position="4613" citStr="Banko et al. (2007)" startWordPosition="713" endWordPosition="716">annot indicate the actor and object of the event. In this paper, we propose using structured information to represent events, and develop a prediction model to analyze the relationship between events and the stock market. The problem is important because it provides insights into understanding the underlying mechanisms of the influence of events on the stock market. There are two main challenges to this method. On the one hand, how to obtain structured event information from large-scale news streams is a challenging problem. We propose to apply Open Information Extraction techniques (Open IE; Banko et al. (2007); Etzioni et al. (2011); Fader et al. (2011)), which do not require predefined event types or manually labeled corpora. Subsequently, two ontologies (i.e. VerbNet and WordNet) are used to generalize structured event features in order to reduce their sparseness. On the other hand, the problem of accurately predicting stock price movement using structured events is challenging, since events and the stock market can have complex relations, which can be influenced by hidden factors. In addition to the commonly used linear models, we build a deep neural network model, which takes structured events </context>
</contexts>
<marker>Banko, Cafarella, Soderland, Broadhead, Etzioni, 2007</marker>
<rawString>Michele Banko, Michael J Cafarella, Stephen Soderland, Matthew Broadhead, and Oren Etzioni. 2007. Open information extraction for the web. In IJCAI, volume 7, pages 2670–2676.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>Patrice Simard</author>
<author>Paolo Frasconi</author>
</authors>
<title>Learning long-term dependencies with gradient descent is difficult. Neural Networks,</title>
<date>1994</date>
<journal>IEEE Transactions on,</journal>
<volume>5</volume>
<issue>2</issue>
<contexts>
<context position="23637" citStr="Bengio et al., 1994" startWordPosition="3913" endWordPosition="3916">ere we conduct a development experiment by different number of hidden layers for the deep neural network model. As shown in Table 2, the performance of two hidden layers is better than one hidden layer, which is consistent with the experimental results of Sharda and Delen (2006) on the task of movie box-office prediction. It indicates that more hidden layers can explain more complex relations (Bengio, 2009). Intuitively, three or more hidden layers may achieve better performance. However, three hidden layers mean that we construct a five-layer deep neural network, which is difficult to train (Bengio et al., 1994). We did not obtain improved accuracies using three hidden layers, due to diminishing gradients. A deep investigation of this problem is out of the scope of this paper. 3.5 Experiments with Different Amounts of Data We conduct a development experiment by extracting news titles and contents from Reuters and Bloomberg, respectively. While titles can give the central information about the news, contents may provide some background knowledge or details. Radinsky et al. (2012) argued that news titles are more helpful for prediction compared to news con1420 0.75 0.7 Accuracy 0.65 0.6 0.55 0.5 SanDis</context>
</contexts>
<marker>Bengio, Simard, Frasconi, 1994</marker>
<rawString>Yoshua Bengio, Patrice Simard, and Paolo Frasconi. 1994. Learning long-term dependencies with gradient descent is difficult. Neural Networks, IEEE Transactions on, 5(2):157–166.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
</authors>
<title>Learning deep architectures for ai. Foundations and trends ⃝R</title>
<date>2009</date>
<booktitle>in Machine Learning,</booktitle>
<volume>2</volume>
<issue>1</issue>
<contexts>
<context position="23427" citStr="Bengio, 2009" startWordPosition="3880" endWordPosition="3881">zes the sigmoid activation function, one hidden layer is enough to solve any discriminant classification problem, and two hidden layers are capable to parse arbitrary output functions of input pattern. Here we conduct a development experiment by different number of hidden layers for the deep neural network model. As shown in Table 2, the performance of two hidden layers is better than one hidden layer, which is consistent with the experimental results of Sharda and Delen (2006) on the task of movie box-office prediction. It indicates that more hidden layers can explain more complex relations (Bengio, 2009). Intuitively, three or more hidden layers may achieve better performance. However, three hidden layers mean that we construct a five-layer deep neural network, which is difficult to train (Bengio et al., 1994). We did not obtain improved accuracies using three hidden layers, due to diminishing gradients. A deep investigation of this problem is out of the scope of this paper. 3.5 Experiments with Different Amounts of Data We conduct a development experiment by extracting news titles and contents from Reuters and Bloomberg, respectively. While titles can give the central information about the n</context>
</contexts>
<marker>Bengio, 2009</marker>
<rawString>Yoshua Bengio. 2009. Learning deep architectures for ai. Foundations and trends ⃝R in Machine Learning, 2(1):1–127.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johan Bollen</author>
<author>Huina Mao</author>
<author>Xiaojun Zeng</author>
</authors>
<title>Twitter mood predicts the stock market.</title>
<date>2011</date>
<journal>Journal of Computational Science,</journal>
<volume>2</volume>
<issue>1</issue>
<contexts>
<context position="35283" citStr="Bollen et al., 2011" startWordPosition="5798" endWordPosition="5801">t (e.g. the actor of the event and the object of the event). Second, Feldman et al. (2011) can obtain only limited categories of events, and hence the scalability of their work is not strong. In contrast, we extract structured events by leveraging Open Information Extraction technology (Open IE; Yates et al. (2007); Etzioni et al. (2011); Faber et al. (2011)) without predefined event types, which can effectively solve the two problems above. Apart from events, sentiment analysis is another perspective to the problem of stock prediction (Das and Chen, 2007; Tetlock, 2007; Tetlock et al., 2008; Bollen et al., 2011; Si et al., 2013). Tetlock (2007) examines how qualitative information (i.e. the fraction of negative words in a particular news column) is incorporated in aggregate market valuations. Tetlock, Saar-Tsechansky, and Macskassy (2008) extend that analysis to address the impact of negative words in all Wall Street Joural (WSJ) and Dow Jones News Services (DJNS) stories about individual S&amp;P500 firms from 1980 to 2004. Bollen and Zeng (2011) study whether the large-scale collective emotion on Twitter is correlated with the volatility of Dow Jones Industrial Average (DJIA). From the experimental res</context>
</contexts>
<marker>Bollen, Mao, Zeng, 2011</marker>
<rawString>Johan Bollen, Huina Mao, and Xiaojun Zeng. 2011. Twitter mood predicts the stock market. Journal of Computational Science, 2(1):1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Werner FM Bondt</author>
<author>Richard Thaler</author>
</authors>
<title>Does the stock market overreact?</title>
<date>1985</date>
<journal>The Journal offinance,</journal>
<volume>40</volume>
<issue>3</issue>
<contexts>
<context position="2019" citStr="Bondt and Thaler, 1985" startWordPosition="296" endWordPosition="299">ock historical data. 1 Introduction Predicting stock price movements is of clear interest to investors, public companies and governments. There has been a debate on whether the market can be predicted. The Random Walk Theory (Malkiel, 1973) hypothesizes that prices are determined randomly and hence it is impossible to outperform the market. However, with advances of AI, it has been shown empirically that stock This work was done while the first author was visiting Singapore University of Technology and Design Figure 1: Example news for Apple Inc. and Google Inc. price movement is predictable (Bondt and Thaler, 1985; Jegadeesh, 1990; Lo and MacKinlay, 1990; Jegadeesh and Titman, 1993). Recent work (Das and Chen, 2007; Tetlock, 2007; Tetlock et al., 2008; Si et al., 2013; Xie et al., 2013; Wang and Hua, 2014) has applied Natural Language Processing (NLP) techniques to help analyze the effect of web texts on stock market prediction, finding that events reported in financial news are important evidence to stock price movement prediction. As news events affect human decisions and the volatility of stock prices is influenced by human trading, it is reasonable to say that events can influence the stock market.</context>
</contexts>
<marker>Bondt, Thaler, 1985</marker>
<rawString>Werner FM Bondt and Richard Thaler. 1985. Does the stock market overreact? The Journal offinance, 40(3):793–805.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wesley S Chan</author>
</authors>
<title>Stock price reaction to news and no-news: Drift and reversal after headlines.</title>
<date>2003</date>
<journal>Journal of Financial Economics,</journal>
<volume>70</volume>
<issue>2</issue>
<contexts>
<context position="33205" citStr="Chan, 2003" startWordPosition="5454" endWordPosition="5455">s in the past. The literature of stock market prediction was initiated by economists (Keynes, 1937). Subsequently, the influential theory of Efficient Market Hypothesis (EMH) (Fama, 1965) was established, which states that the price of a security reflects all of the information available and that everyone has a certain degree of access to the information. EMH had a significant impact on security investment, and can serve as the theoretical basis of event-based stock price movement prediction. Various studies have found that financial news can dramatically affect the share price of a security (Chan, 2003; Tetlock et al., 2008). Culter et al. (1998) was one of the first to investigate the relationship between news coverage and stock prices, since which empirical text analysis technology has been widely used across numerous disciplines (Lavrenko et al., 2000; Kogan et al., 2009; Luss and d’Aspremont, 2012). These studies primarily use bags-of-words to represent financial news documents. However, as Schumaker and Chen (2009) and Xie et al. (2013) point out, bagof-words features are not the best choice for predicting stock prices. Schumaker and Chen (2009) extract noun phrases and named entities </context>
</contexts>
<marker>Chan, 2003</marker>
<rawString>Wesley S Chan. 2003. Stock price reaction to news and no-news: Drift and reversal after headlines. Journal of Financial Economics, 70(2):223–260.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Cutler</author>
<author>James M Poterba</author>
<author>Lawrence H Summers</author>
</authors>
<title>What moves stock prices?</title>
<date>1998</date>
<pages>56--63</pages>
<marker>Cutler, Poterba, Summers, 1998</marker>
<rawString>David M Cutler, James M Poterba, and Lawrence H Summers. 1998. What moves stock prices? Bernstein, Peter L. and Frank L. Fabozzi, pages 56–63.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Cybenko</author>
</authors>
<title>Approximation by superpositions of a sigmoidal function. Mathematics of control, signals and systems,</title>
<date>1989</date>
<pages>2--4</pages>
<contexts>
<context position="22766" citStr="Cybenko (1989)" startWordPosition="3772" endWordPosition="3773">bedded in the news. In addition, we find that some events may cause immediate changes of stock prices. For example, former Microsoft CEO Steve Ballmer announced he would step down within 12 months on 23/08/2013. Within an hour, Microsoft shares jumped as much as 9 percent. This fact indicates that it may be possible to predict stock price movement on a shorter time interval than one day. However, we cannot access fine-grained stock price historical data, and this investigation will be left as future work. 3.4 Experiments with Different Numbers of Hidden Layers of the Deep Neural Network Model Cybenko (1989) states that when every processing element utilizes the sigmoid activation function, one hidden layer is enough to solve any discriminant classification problem, and two hidden layers are capable to parse arbitrary output functions of input pattern. Here we conduct a development experiment by different number of hidden layers for the deep neural network model. As shown in Table 2, the performance of two hidden layers is better than one hidden layer, which is consistent with the experimental results of Sharda and Delen (2006) on the task of movie box-office prediction. It indicates that more hi</context>
</contexts>
<marker>Cybenko, 1989</marker>
<rawString>George Cybenko. 1989. Approximation by superpositions of a sigmoidal function. Mathematics of control, signals and systems, 2(4):303–314.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanjiv R Das</author>
<author>Mike Y Chen</author>
</authors>
<title>Yahoo! for amazon: Sentiment extraction from small talk on the web.</title>
<date>2007</date>
<journal>Management Science,</journal>
<volume>53</volume>
<issue>9</issue>
<contexts>
<context position="2122" citStr="Das and Chen, 2007" startWordPosition="312" endWordPosition="315">lic companies and governments. There has been a debate on whether the market can be predicted. The Random Walk Theory (Malkiel, 1973) hypothesizes that prices are determined randomly and hence it is impossible to outperform the market. However, with advances of AI, it has been shown empirically that stock This work was done while the first author was visiting Singapore University of Technology and Design Figure 1: Example news for Apple Inc. and Google Inc. price movement is predictable (Bondt and Thaler, 1985; Jegadeesh, 1990; Lo and MacKinlay, 1990; Jegadeesh and Titman, 1993). Recent work (Das and Chen, 2007; Tetlock, 2007; Tetlock et al., 2008; Si et al., 2013; Xie et al., 2013; Wang and Hua, 2014) has applied Natural Language Processing (NLP) techniques to help analyze the effect of web texts on stock market prediction, finding that events reported in financial news are important evidence to stock price movement prediction. As news events affect human decisions and the volatility of stock prices is influenced by human trading, it is reasonable to say that events can influence the stock market. Figure 1 shows two pieces of financial news about Apple Inc. and Google Inc., respectively. Shares of </context>
<context position="11662" citStr="Das and Chen, 2007" startWordPosition="1888" endWordPosition="1891">“add” and “job”, respectively. Second, we generalize each verb to its class name in VerbNet. For example, “add” belongs to the multiply class. After generalization, the event (Private sector, adds, 114,000 jobs) becomes (private sector, multiply class, 114,000 job). Similar methods on event generalization have been investigated in Open IE based event causal prediction (Radinsky and Horvitz, 2013). 2.4 Prediction Models 1. Linear model. Most previous work uses linear models to predict the stock market (Fung et al., 2002; Luss and d’Aspremont, 2012; Schumaker and Chen, 2009; Kogan et al., 2009; Das and Chen, 2007; Xie et al., 2013). To make direct comparisons, this paper constructs a linear prediction model by using Support Vector Machines (SVMs), a state-of-the-art classification model. Given a training set (d1, y1), (d2, y2), ..., (dN, yN), where n E [1, N], dn is a news document and yi E {+1, −11 is the output class. dn can be news titles, news contents or both. The output Class +1 represents that the stock price will increase the next day/week/month, and the output Class -1 represents that the stock price will decrease the next day/week/month. The features can be bag-of-words features or structure</context>
<context position="35225" citStr="Das and Chen, 2007" startWordPosition="5788" endWordPosition="5791">these efforts. First, they cannot extract structured event (e.g. the actor of the event and the object of the event). Second, Feldman et al. (2011) can obtain only limited categories of events, and hence the scalability of their work is not strong. In contrast, we extract structured events by leveraging Open Information Extraction technology (Open IE; Yates et al. (2007); Etzioni et al. (2011); Faber et al. (2011)) without predefined event types, which can effectively solve the two problems above. Apart from events, sentiment analysis is another perspective to the problem of stock prediction (Das and Chen, 2007; Tetlock, 2007; Tetlock et al., 2008; Bollen et al., 2011; Si et al., 2013). Tetlock (2007) examines how qualitative information (i.e. the fraction of negative words in a particular news column) is incorporated in aggregate market valuations. Tetlock, Saar-Tsechansky, and Macskassy (2008) extend that analysis to address the impact of negative words in all Wall Street Joural (WSJ) and Dow Jones News Services (DJNS) stories about individual S&amp;P500 firms from 1980 to 2004. Bollen and Zeng (2011) study whether the large-scale collective emotion on Twitter is correlated with the volatility of Dow </context>
</contexts>
<marker>Das, Chen, 2007</marker>
<rawString>Sanjiv R Das and Mike Y Chen. 2007. Yahoo! for amazon: Sentiment extraction from small talk on the web. Management Science, 53(9):1375–1388.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiao Ding</author>
<author>Bing Qin</author>
<author>Ting Liu</author>
</authors>
<title>Building chinese event type paradigm based on trigger clustering.</title>
<date>2013</date>
<booktitle>In Proc. of IJCNLP,</booktitle>
<pages>311--319</pages>
<contexts>
<context position="8557" citStr="Ding et al. (2013)" startWordPosition="1368" endWordPosition="1371">n of our work is to extract and use structured events instead of bags-of-words in prediction models. However, structured event extraction can be a costly task, requiring predefined event types and manual event templates (Ji and Grishman, 2008; Li et al., 2013). Partly due to this, the bags-of-words-based document representation has been the mainstream method for a long time. To tackle this issue, we resort to Open IE, extracting event tuples from wide-coverage data with1416 out requiring any human input (e.g. templates). Our system is based on the system of Fader et al. (2011) and the work of Ding et al. (2013); it does not require predefined target event types and labeled training examples. Given a natural language sentence obtained from news texts, the following procedure is used to extract structured events: 1. Event Phrase Extraction. We extract the predicate verb P of a sentence based on the dependency parser of Zhang and Clark (2011), and then find the longest sequence of words Pv, such that Pv starts at P and satisfies the syntactic and lexical constraints proposed by Fader et al. (2011). The content of these two constraints are as follows: • Syntactic constraint: every multi-word event phras</context>
</contexts>
<marker>Ding, Qin, Liu, 2013</marker>
<rawString>Xiao Ding, Bing Qin, and Ting Liu. 2013. Building chinese event type paradigm based on trigger clustering. In Proc. of IJCNLP, pages 311–319, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oren Etzioni</author>
<author>Anthony Fader</author>
<author>Janara Christensen</author>
<author>Stephen Soderland</author>
<author>Mausam Mausam</author>
</authors>
<title>Open information extraction: The second generation.</title>
<date>2011</date>
<booktitle>In Proceedings of the Twenty-Second international joint conference on Artificial IntelligenceVolume Volume One,</booktitle>
<pages>3--10</pages>
<publisher>AAAI Press.</publisher>
<contexts>
<context position="4636" citStr="Etzioni et al. (2011)" startWordPosition="717" endWordPosition="721">tor and object of the event. In this paper, we propose using structured information to represent events, and develop a prediction model to analyze the relationship between events and the stock market. The problem is important because it provides insights into understanding the underlying mechanisms of the influence of events on the stock market. There are two main challenges to this method. On the one hand, how to obtain structured event information from large-scale news streams is a challenging problem. We propose to apply Open Information Extraction techniques (Open IE; Banko et al. (2007); Etzioni et al. (2011); Fader et al. (2011)), which do not require predefined event types or manually labeled corpora. Subsequently, two ontologies (i.e. VerbNet and WordNet) are used to generalize structured event features in order to reduce their sparseness. On the other hand, the problem of accurately predicting stock price movement using structured events is challenging, since events and the stock market can have complex relations, which can be influenced by hidden factors. In addition to the commonly used linear models, we build a deep neural network model, which takes structured events as input and learn the </context>
<context position="35003" citStr="Etzioni et al. (2011)" startWordPosition="5754" endWordPosition="5757">5; Feldman et al., 2011). Fung, Yu, and Lam (2002) use a normalized word vector-space to model event. Feldman et al. (2011) extract 9 predefined categories of events based on heuristic rules. There are two main problems with these efforts. First, they cannot extract structured event (e.g. the actor of the event and the object of the event). Second, Feldman et al. (2011) can obtain only limited categories of events, and hence the scalability of their work is not strong. In contrast, we extract structured events by leveraging Open Information Extraction technology (Open IE; Yates et al. (2007); Etzioni et al. (2011); Faber et al. (2011)) without predefined event types, which can effectively solve the two problems above. Apart from events, sentiment analysis is another perspective to the problem of stock prediction (Das and Chen, 2007; Tetlock, 2007; Tetlock et al., 2008; Bollen et al., 2011; Si et al., 2013). Tetlock (2007) examines how qualitative information (i.e. the fraction of negative words in a particular news column) is incorporated in aggregate market valuations. Tetlock, Saar-Tsechansky, and Macskassy (2008) extend that analysis to address the impact of negative words in all Wall Street Joural </context>
</contexts>
<marker>Etzioni, Fader, Christensen, Soderland, Mausam, 2011</marker>
<rawString>Oren Etzioni, Anthony Fader, Janara Christensen, Stephen Soderland, and Mausam Mausam. 2011. Open information extraction: The second generation. In Proceedings of the Twenty-Second international joint conference on Artificial IntelligenceVolume Volume One, pages 3–10. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anthony Fader</author>
<author>Stephen Soderland</author>
<author>Oren Etzioni</author>
</authors>
<title>Identifying relations for open information extraction.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1535--1545</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4657" citStr="Fader et al. (2011)" startWordPosition="722" endWordPosition="725">vent. In this paper, we propose using structured information to represent events, and develop a prediction model to analyze the relationship between events and the stock market. The problem is important because it provides insights into understanding the underlying mechanisms of the influence of events on the stock market. There are two main challenges to this method. On the one hand, how to obtain structured event information from large-scale news streams is a challenging problem. We propose to apply Open Information Extraction techniques (Open IE; Banko et al. (2007); Etzioni et al. (2011); Fader et al. (2011)), which do not require predefined event types or manually labeled corpora. Subsequently, two ontologies (i.e. VerbNet and WordNet) are used to generalize structured event features in order to reduce their sparseness. On the other hand, the problem of accurately predicting stock price movement using structured events is challenging, since events and the stock market can have complex relations, which can be influenced by hidden factors. In addition to the commonly used linear models, we build a deep neural network model, which takes structured events as input and learn the potential relationshi</context>
<context position="8522" citStr="Fader et al. (2011)" startWordPosition="1360" endWordPosition="1363"> Event Extraction A main contribution of our work is to extract and use structured events instead of bags-of-words in prediction models. However, structured event extraction can be a costly task, requiring predefined event types and manual event templates (Ji and Grishman, 2008; Li et al., 2013). Partly due to this, the bags-of-words-based document representation has been the mainstream method for a long time. To tackle this issue, we resort to Open IE, extracting event tuples from wide-coverage data with1416 out requiring any human input (e.g. templates). Our system is based on the system of Fader et al. (2011) and the work of Ding et al. (2013); it does not require predefined target event types and labeled training examples. Given a natural language sentence obtained from news texts, the following procedure is used to extract structured events: 1. Event Phrase Extraction. We extract the predicate verb P of a sentence based on the dependency parser of Zhang and Clark (2011), and then find the longest sequence of words Pv, such that Pv starts at P and satisfies the syntactic and lexical constraints proposed by Fader et al. (2011). The content of these two constraints are as follows: • Syntactic const</context>
</contexts>
<marker>Fader, Soderland, Etzioni, 2011</marker>
<rawString>Anthony Fader, Stephen Soderland, and Oren Etzioni. 2011. Identifying relations for open information extraction. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1535–1545. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene F Fama</author>
</authors>
<title>The behavior of stock-market prices.</title>
<date>1965</date>
<journal>The journal of Business,</journal>
<volume>38</volume>
<issue>1</issue>
<contexts>
<context position="32782" citStr="Fama, 1965" startWordPosition="5382" endWordPosition="5383">n the training set, such as “Google bought stake in XunLei” on 04/01/2007. Hence although the full specific event feature does not fire, its back-offs fire for a correct prediction. For simplicity of showing the event, we did not include back-off features in Figure 5. 4 Related Work Stock market prediction has attracted a great deal of attention across the fields of finance, computer science and other research communities in the past. The literature of stock market prediction was initiated by economists (Keynes, 1937). Subsequently, the influential theory of Efficient Market Hypothesis (EMH) (Fama, 1965) was established, which states that the price of a security reflects all of the information available and that everyone has a certain degree of access to the information. EMH had a significant impact on security investment, and can serve as the theoretical basis of event-based stock price movement prediction. Various studies have found that financial news can dramatically affect the share price of a security (Chan, 2003; Tetlock et al., 2008). Culter et al. (1998) was one of the first to investigate the relationship between news coverage and stock prices, since which empirical text analysis te</context>
</contexts>
<marker>Fama, 1965</marker>
<rawString>Eugene F Fama. 1965. The behavior of stock-market prices. The journal of Business, 38(1):34–105.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronen Feldman</author>
<author>Benjamin Rosenfeld</author>
<author>Roy Bar-Haim</author>
<author>Moshe Fresko</author>
</authors>
<title>The stock sonarsentiment analysis of stocks based on a hybrid approach.</title>
<date>2011</date>
<booktitle>In Twenty-Third IAAI Conference.</booktitle>
<contexts>
<context position="7208" citStr="Feldman et al., 2011" startWordPosition="1154" endWordPosition="1157">n which the action was performed. Formally, an event is represented as E = (O1, P, O2, T), where P is the action, O1 is the actor, O2 is the object and T is the timestamp (T is mainly used for aligning stock data with news data). For example, the event “Sep 3, 2013 - Microsoft agrees to buy Nokia’s mobile phone business for $7.2 billion.” is modeled as: (Actor = Microsoft, Action = buy, Object = Nokia’s mobile phone business, Time = Sep 3, 2013). Previous work on stock market prediction represents events as a set of individual terms (Fung et al., 2002; Fung et al., 2003; Hayo and Kutan, 2004; Feldman et al., 2011). For example, “Microsoft agrees to buy Nokia’s mobile phone business for $7.2 billion.” can be represented by {“Microsoft”, “agrees”, “buy”, “Nokia’s”, “mobile”, ...I and “Oracle has filed suit against Google over its ever-more-popular mobile operating system, Android.” can be represented by {“Oracle”, “has”, “filed”, “suit”, “against”, “Google”, ...I. However, terms alone might fail to accurately predict the stock price movement of Microsoft, Nokia, Oracle and Google, because they cannot indicate the actor and object of the event. To our knowledge, no effort has been reported in the literatu</context>
<context position="34406" citStr="Feldman et al., 2011" startWordPosition="5652" endWordPosition="5655"> and named entities to augment bags-of-words. Xie et al. (2013) explore a rich feature space that relies on frame semantic parsing. Wang et al. (2014) use the same features as Xie et al. (2013), but they perform nonparametric kernel density estimation to smooth out the distribution of features. These can be regarded as extensions to the bag-of-word method. The drawback of these approaches, as discussed in the introduction, is that they do not directly model events, which have structured information. There has been efforts to model events more directly (Fung et al., 2002; Hayo and Kutan, 2005; Feldman et al., 2011). Fung, Yu, and Lam (2002) use a normalized word vector-space to model event. Feldman et al. (2011) extract 9 predefined categories of events based on heuristic rules. There are two main problems with these efforts. First, they cannot extract structured event (e.g. the actor of the event and the object of the event). Second, Feldman et al. (2011) can obtain only limited categories of events, and hence the scalability of their work is not strong. In contrast, we extract structured events by leveraging Open Information Extraction technology (Open IE; Yates et al. (2007); Etzioni et al. (2011); F</context>
</contexts>
<marker>Feldman, Rosenfeld, Bar-Haim, Fresko, 2011</marker>
<rawString>Ronen Feldman, Benjamin Rosenfeld, Roy Bar-Haim, and Moshe Fresko. 2011. The stock sonarsentiment analysis of stocks based on a hybrid approach. In Twenty-Third IAAI Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gabriel Pui Cheong Fung</author>
<author>Jeffrey Xu Yu</author>
<author>Wai Lam</author>
</authors>
<title>News sensitive stock trend prediction.</title>
<date>2002</date>
<booktitle>In Advances in Knowledge Discovery and Data Mining,</booktitle>
<pages>481--493</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="7144" citStr="Fung et al., 2002" startWordPosition="1141" endWordPosition="1144">P, an actor O1 that conducted the action, and an object O2 on which the action was performed. Formally, an event is represented as E = (O1, P, O2, T), where P is the action, O1 is the actor, O2 is the object and T is the timestamp (T is mainly used for aligning stock data with news data). For example, the event “Sep 3, 2013 - Microsoft agrees to buy Nokia’s mobile phone business for $7.2 billion.” is modeled as: (Actor = Microsoft, Action = buy, Object = Nokia’s mobile phone business, Time = Sep 3, 2013). Previous work on stock market prediction represents events as a set of individual terms (Fung et al., 2002; Fung et al., 2003; Hayo and Kutan, 2004; Feldman et al., 2011). For example, “Microsoft agrees to buy Nokia’s mobile phone business for $7.2 billion.” can be represented by {“Microsoft”, “agrees”, “buy”, “Nokia’s”, “mobile”, ...I and “Oracle has filed suit against Google over its ever-more-popular mobile operating system, Android.” can be represented by {“Oracle”, “has”, “filed”, “suit”, “against”, “Google”, ...I. However, terms alone might fail to accurately predict the stock price movement of Microsoft, Nokia, Oracle and Google, because they cannot indicate the actor and object of the even</context>
<context position="11568" citStr="Fung et al., 2002" startWordPosition="1872" endWordPosition="1875">: Private sector adds 114,000 jobs in July.”, the words “adds” and “jobs” are transformed to “add” and “job”, respectively. Second, we generalize each verb to its class name in VerbNet. For example, “add” belongs to the multiply class. After generalization, the event (Private sector, adds, 114,000 jobs) becomes (private sector, multiply class, 114,000 job). Similar methods on event generalization have been investigated in Open IE based event causal prediction (Radinsky and Horvitz, 2013). 2.4 Prediction Models 1. Linear model. Most previous work uses linear models to predict the stock market (Fung et al., 2002; Luss and d’Aspremont, 2012; Schumaker and Chen, 2009; Kogan et al., 2009; Das and Chen, 2007; Xie et al., 2013). To make direct comparisons, this paper constructs a linear prediction model by using Support Vector Machines (SVMs), a state-of-the-art classification model. Given a training set (d1, y1), (d2, y2), ..., (dN, yN), where n E [1, N], dn is a news document and yi E {+1, −11 is the output class. dn can be news titles, news contents or both. The output Class +1 represents that the stock price will increase the next day/week/month, and the output Class -1 represents that the stock price</context>
<context position="34361" citStr="Fung et al., 2002" startWordPosition="5644" endWordPosition="5647">aker and Chen (2009) extract noun phrases and named entities to augment bags-of-words. Xie et al. (2013) explore a rich feature space that relies on frame semantic parsing. Wang et al. (2014) use the same features as Xie et al. (2013), but they perform nonparametric kernel density estimation to smooth out the distribution of features. These can be regarded as extensions to the bag-of-word method. The drawback of these approaches, as discussed in the introduction, is that they do not directly model events, which have structured information. There has been efforts to model events more directly (Fung et al., 2002; Hayo and Kutan, 2005; Feldman et al., 2011). Fung, Yu, and Lam (2002) use a normalized word vector-space to model event. Feldman et al. (2011) extract 9 predefined categories of events based on heuristic rules. There are two main problems with these efforts. First, they cannot extract structured event (e.g. the actor of the event and the object of the event). Second, Feldman et al. (2011) can obtain only limited categories of events, and hence the scalability of their work is not strong. In contrast, we extract structured events by leveraging Open Information Extraction technology (Open IE; </context>
</contexts>
<marker>Fung, Yu, Lam, 2002</marker>
<rawString>Gabriel Pui Cheong Fung, Jeffrey Xu Yu, and Wai Lam. 2002. News sensitive stock trend prediction. In Advances in Knowledge Discovery and Data Mining, pages 481–493. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernd Hayo</author>
<author>Ali M Kutan</author>
</authors>
<title>The impact of news, oil prices, and global market developments on russian financial markets1.</title>
<date>2005</date>
<journal>Economics of Transition,</journal>
<volume>13</volume>
<issue>2</issue>
<contexts>
<context position="34383" citStr="Hayo and Kutan, 2005" startWordPosition="5648" endWordPosition="5651">) extract noun phrases and named entities to augment bags-of-words. Xie et al. (2013) explore a rich feature space that relies on frame semantic parsing. Wang et al. (2014) use the same features as Xie et al. (2013), but they perform nonparametric kernel density estimation to smooth out the distribution of features. These can be regarded as extensions to the bag-of-word method. The drawback of these approaches, as discussed in the introduction, is that they do not directly model events, which have structured information. There has been efforts to model events more directly (Fung et al., 2002; Hayo and Kutan, 2005; Feldman et al., 2011). Fung, Yu, and Lam (2002) use a normalized word vector-space to model event. Feldman et al. (2011) extract 9 predefined categories of events based on heuristic rules. There are two main problems with these efforts. First, they cannot extract structured event (e.g. the actor of the event and the object of the event). Second, Feldman et al. (2011) can obtain only limited categories of events, and hence the scalability of their work is not strong. In contrast, we extract structured events by leveraging Open Information Extraction technology (Open IE; Yates et al. (2007); E</context>
</contexts>
<marker>Hayo, Kutan, 2005</marker>
<rawString>Bernd Hayo and Ali M Kutan. 2005. The impact of news, oil prices, and global market developments on russian financial markets1. Economics of Transition, 13(2):373–393.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Narasimhan Jegadeesh</author>
<author>Sheridan Titman</author>
</authors>
<title>Returns to buying winners and selling losers: Implications for stock market efficiency.</title>
<date>1993</date>
<journal>The Journal of Finance,</journal>
<volume>48</volume>
<issue>1</issue>
<contexts>
<context position="2089" citStr="Jegadeesh and Titman, 1993" startWordPosition="306" endWordPosition="309">nts is of clear interest to investors, public companies and governments. There has been a debate on whether the market can be predicted. The Random Walk Theory (Malkiel, 1973) hypothesizes that prices are determined randomly and hence it is impossible to outperform the market. However, with advances of AI, it has been shown empirically that stock This work was done while the first author was visiting Singapore University of Technology and Design Figure 1: Example news for Apple Inc. and Google Inc. price movement is predictable (Bondt and Thaler, 1985; Jegadeesh, 1990; Lo and MacKinlay, 1990; Jegadeesh and Titman, 1993). Recent work (Das and Chen, 2007; Tetlock, 2007; Tetlock et al., 2008; Si et al., 2013; Xie et al., 2013; Wang and Hua, 2014) has applied Natural Language Processing (NLP) techniques to help analyze the effect of web texts on stock market prediction, finding that events reported in financial news are important evidence to stock price movement prediction. As news events affect human decisions and the volatility of stock prices is influenced by human trading, it is reasonable to say that events can influence the stock market. Figure 1 shows two pieces of financial news about Apple Inc. and Goog</context>
</contexts>
<marker>Jegadeesh, Titman, 1993</marker>
<rawString>Narasimhan Jegadeesh and Sheridan Titman. 1993. Returns to buying winners and selling losers: Implications for stock market efficiency. The Journal of Finance, 48(1):65–91.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Narasimhan Jegadeesh</author>
</authors>
<title>Evidence of predictable behavior of security returns.</title>
<date>1990</date>
<journal>The Journal of Finance,</journal>
<volume>45</volume>
<issue>3</issue>
<contexts>
<context position="2036" citStr="Jegadeesh, 1990" startWordPosition="300" endWordPosition="301">ntroduction Predicting stock price movements is of clear interest to investors, public companies and governments. There has been a debate on whether the market can be predicted. The Random Walk Theory (Malkiel, 1973) hypothesizes that prices are determined randomly and hence it is impossible to outperform the market. However, with advances of AI, it has been shown empirically that stock This work was done while the first author was visiting Singapore University of Technology and Design Figure 1: Example news for Apple Inc. and Google Inc. price movement is predictable (Bondt and Thaler, 1985; Jegadeesh, 1990; Lo and MacKinlay, 1990; Jegadeesh and Titman, 1993). Recent work (Das and Chen, 2007; Tetlock, 2007; Tetlock et al., 2008; Si et al., 2013; Xie et al., 2013; Wang and Hua, 2014) has applied Natural Language Processing (NLP) techniques to help analyze the effect of web texts on stock market prediction, finding that events reported in financial news are important evidence to stock price movement prediction. As news events affect human decisions and the volatility of stock prices is influenced by human trading, it is reasonable to say that events can influence the stock market. Figure 1 shows t</context>
</contexts>
<marker>Jegadeesh, 1990</marker>
<rawString>Narasimhan Jegadeesh. 1990. Evidence of predictable behavior of security returns. The Journal of Finance, 45(3):881–898.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heng Ji</author>
<author>Ralph Grishman</author>
</authors>
<title>Refining event extraction through cross-document inference.</title>
<date>2008</date>
<booktitle>In ACL,</booktitle>
<pages>254--262</pages>
<contexts>
<context position="8181" citStr="Ji and Grishman, 2008" startWordPosition="1301" endWordPosition="1305">.I. However, terms alone might fail to accurately predict the stock price movement of Microsoft, Nokia, Oracle and Google, because they cannot indicate the actor and object of the event. To our knowledge, no effort has been reported in the literature to empirically investigate structured event representations for stock market prediction. 2.2 Event Extraction A main contribution of our work is to extract and use structured events instead of bags-of-words in prediction models. However, structured event extraction can be a costly task, requiring predefined event types and manual event templates (Ji and Grishman, 2008; Li et al., 2013). Partly due to this, the bags-of-words-based document representation has been the mainstream method for a long time. To tackle this issue, we resort to Open IE, extracting event tuples from wide-coverage data with1416 out requiring any human input (e.g. templates). Our system is based on the system of Fader et al. (2011) and the work of Ding et al. (2013); it does not require predefined target event types and labeled training examples. Given a natural language sentence obtained from news texts, the following procedure is used to extract structured events: 1. Event Phrase Ext</context>
</contexts>
<marker>Ji, Grishman, 2008</marker>
<rawString>Heng Ji and Ralph Grishman. 2008. Refining event extraction through cross-document inference. In ACL, pages 254–262.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Maynard Keynes</author>
</authors>
<title>The general theory of employment.</title>
<date>1937</date>
<journal>The Quarterly Journal of Economics,</journal>
<volume>51</volume>
<issue>2</issue>
<contexts>
<context position="32694" citStr="Keynes, 1937" startWordPosition="5369" endWordPosition="5370">t class” (“get” is the class name of “acquire” and “buy” in VerbNet) can indeed be found in the training set, such as “Google bought stake in XunLei” on 04/01/2007. Hence although the full specific event feature does not fire, its back-offs fire for a correct prediction. For simplicity of showing the event, we did not include back-off features in Figure 5. 4 Related Work Stock market prediction has attracted a great deal of attention across the fields of finance, computer science and other research communities in the past. The literature of stock market prediction was initiated by economists (Keynes, 1937). Subsequently, the influential theory of Efficient Market Hypothesis (EMH) (Fama, 1965) was established, which states that the price of a security reflects all of the information available and that everyone has a certain degree of access to the information. EMH had a significant impact on security investment, and can serve as the theoretical basis of event-based stock price movement prediction. Various studies have found that financial news can dramatically affect the share price of a security (Chan, 2003; Tetlock et al., 2008). Culter et al. (1998) was one of the first to investigate the rel</context>
</contexts>
<marker>Keynes, 1937</marker>
<rawString>John Maynard Keynes. 1937. The general theory of employment. The Quarterly Journal of Economics, 51(2):209–223.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jaegwon Kim</author>
</authors>
<title>Supervenience and mind: Selected philosophical essays.</title>
<date>1993</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="6151" citStr="Kim (1993)" startWordPosition="955" endWordPosition="956">ve better performance than linear models. The accuracy of S&amp;P 500 index prediction by our approach outperforms previous systems, and the accuracy of individual stock prediction can be over 70% on the large-scale data. Our system can be regarded as one step towards building an expert system that exploits rich knowledge for stock market prediction. Our results are helpful for automatically mining stock price related news events, and for improving the accuracy of algorithm trading systems. lhttp://www.reuters.com/ zhttp://www.bloomberg.com/ 2 Method 2.1 Event Representation We follow the work of Kim (1993) and design a structured representation scheme that allows us to extract events and generalize them. Kim defines an event as a tuple (OZ, P, T), where OZ C O is a set of objects, P is a relation over the objects and T is a time interval. We propose a representation that further structures the event to have roles in addition to relations. Each event is composed of an action P, an actor O1 that conducted the action, and an object O2 on which the action was performed. Formally, an event is represented as E = (O1, P, O2, T), where P is the action, O1 is the actor, O2 is the object and T is the tim</context>
</contexts>
<marker>Kim, 1993</marker>
<rawString>Jaegwon Kim. 1993. Supervenience and mind: Selected philosophical essays. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karin Kipper</author>
<author>Anna Korhonen</author>
<author>Neville Ryant</author>
<author>Martha Palmer</author>
</authors>
<title>Extending verbnet with novel verb classes.</title>
<date>2006</date>
<booktitle>In Proceedings of LREC,</booktitle>
<volume>volume</volume>
<pages>1</pages>
<contexts>
<context position="10739" citStr="Kipper et al., 2006" startWordPosition="1739" endWordPosition="1742"> respectively. The structured event is extracted as (Private sector, adds, 114,000 jobs). 2.3 Event Generalization Our goal is to train a model that is able to make predictions based on various expressions of the same event. For example, “Microsoft swallows Nokia’s phone business for $7.2 billion” and “Microsoft purchases Nokia’s phone business” report the same event. To improve the accuracy of our prediction model, we should endow the event extraction algorithm with generalization capacity. To this end, we leverage knowledge from two well-known ontologies, WordNet (Miller, 1995) and VerbNet (Kipper et al., 2006). The process of event generalization consists of two steps. First, we construct a morphological analysis tool based on the WordNet stemmer to extract lemma forms of inflected words. For example, in “Instant view: Private sector adds 114,000 jobs in July.”, the words “adds” and “jobs” are transformed to “add” and “job”, respectively. Second, we generalize each verb to its class name in VerbNet. For example, “add” belongs to the multiply class. After generalization, the event (Private sector, adds, 114,000 jobs) becomes (private sector, multiply class, 114,000 job). Similar methods on event gen</context>
</contexts>
<marker>Kipper, Korhonen, Ryant, Palmer, 2006</marker>
<rawString>Karin Kipper, Anna Korhonen, Neville Ryant, and Martha Palmer. 2006. Extending verbnet with novel verb classes. In Proceedings of LREC, volume 2006, page 1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shimon Kogan</author>
<author>Dimitry Levin</author>
<author>Bryan R Routledge</author>
<author>Jacob S Sagi</author>
<author>Noah A Smith</author>
</authors>
<title>Predicting risk from financial reports with regression.</title>
<date>2009</date>
<booktitle>In Proc. NAACL,</booktitle>
<pages>272--280</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Boulder, Colorado,</location>
<contexts>
<context position="3173" citStr="Kogan et al., 2009" startWordPosition="487" endWordPosition="490">is reasonable to say that events can influence the stock market. Figure 1 shows two pieces of financial news about Apple Inc. and Google Inc., respectively. Shares of Apple Inc. fell as trading began in New York on Thursday morning, the day after its former CEO Steve Jobs passed away. Google’s stock fell after grim earnings came out. Accurate extraction of events from financial news may play an important role in stock market prediction. However, previous work represents news documents mainly using simple features, such as bags-of-words, noun phrases, and named entities (Lavrenko et al., 2000; Kogan et al., 2009; Luss and d’Aspremont, 2012; Schumaker and Chen, 2009). With these unstructured features, it is difficult to capture key events embedded in financial news, and even more difficult to model the impact of events on stock market prediction. For example, representing the event “Apple has sued Samsung Electronics for copying ‘the look and feel’ 1415 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1415–1425, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics of its iPad tablet and iPhone smartphone.” using term-le</context>
<context position="11642" citStr="Kogan et al., 2009" startWordPosition="1884" endWordPosition="1887"> are transformed to “add” and “job”, respectively. Second, we generalize each verb to its class name in VerbNet. For example, “add” belongs to the multiply class. After generalization, the event (Private sector, adds, 114,000 jobs) becomes (private sector, multiply class, 114,000 job). Similar methods on event generalization have been investigated in Open IE based event causal prediction (Radinsky and Horvitz, 2013). 2.4 Prediction Models 1. Linear model. Most previous work uses linear models to predict the stock market (Fung et al., 2002; Luss and d’Aspremont, 2012; Schumaker and Chen, 2009; Kogan et al., 2009; Das and Chen, 2007; Xie et al., 2013). To make direct comparisons, this paper constructs a linear prediction model by using Support Vector Machines (SVMs), a state-of-the-art classification model. Given a training set (d1, y1), (d2, y2), ..., (dN, yN), where n E [1, N], dn is a news document and yi E {+1, −11 is the output class. dn can be news titles, news contents or both. The output Class +1 represents that the stock price will increase the next day/week/month, and the output Class -1 represents that the stock price will decrease the next day/week/month. The features can be bag-of-words f</context>
<context position="15290" citStr="Kogan et al., 2009" startWordPosition="2538" endWordPosition="2541">he vocabulary size derived from the training data (introduced in the next section), and freq(tl) denote the number of occurrences of the lth word in the vocabulary in document d. TFl = 1 |d|freq(tl), bl E [1, L], where |d |is the number of words in the document d (stop words are removed). TFIDFl = 1 |d|freq(tl) x log( � |{d:����(t`)&gt;0}|), where N is the number of documents in the training set. The feature vector 4) can be represented as 4) = (ϕ1, ϕ2, ..., ϕM) = (TFIDF1, TFIDF2, ..., TFIDFi12 ). The TFIDF feature representation has been used by most previous studies on stock market prediction (Kogan et al., 2009; Luss and d’Aspremont, 2012). (2) Event features. We represent an event tuple (O1, P, O2, T) by the combination of elements (except for T) (O1, P, O2, O1 + P, P + O2, O1 + P + O2). For example, the event tuple (Microsoft, buy, Nokia’s mobile phone business) can be represented as (#arg1=Microsoft, #action=get class, #arg2=Nokia’s mobile phone business, #arg1 action=Microsoft get class, #action arg2=get class Nokia’s mobile phone business, #arg1 action arg2=Microsoft get class Nokia’s mobile phone business). Structured events are more sparse than words, and we reduce sparseness by two means. Fi</context>
<context position="33482" citStr="Kogan et al., 2009" startWordPosition="5498" endWordPosition="5501">formation available and that everyone has a certain degree of access to the information. EMH had a significant impact on security investment, and can serve as the theoretical basis of event-based stock price movement prediction. Various studies have found that financial news can dramatically affect the share price of a security (Chan, 2003; Tetlock et al., 2008). Culter et al. (1998) was one of the first to investigate the relationship between news coverage and stock prices, since which empirical text analysis technology has been widely used across numerous disciplines (Lavrenko et al., 2000; Kogan et al., 2009; Luss and d’Aspremont, 2012). These studies primarily use bags-of-words to represent financial news documents. However, as Schumaker and Chen (2009) and Xie et al. (2013) point out, bagof-words features are not the best choice for predicting stock prices. Schumaker and Chen (2009) extract noun phrases and named entities to augment bags-of-words. Xie et al. (2013) explore a rich feature space that relies on frame semantic parsing. Wang et al. (2014) use the same features as Xie et al. (2013), but they perform nonparametric kernel density estimation to smooth out the distribution of features. T</context>
</contexts>
<marker>Kogan, Levin, Routledge, Sagi, Smith, 2009</marker>
<rawString>Shimon Kogan, Dimitry Levin, Bryan R. Routledge, Jacob S. Sagi, and Noah A. Smith. 2009. Predicting risk from financial reports with regression. In Proc. NAACL, pages 272–280, Boulder, Colorado, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Victor Lavrenko</author>
<author>Matt Schmill</author>
<author>Dawn Lawrie</author>
<author>Paul Ogilvie</author>
<author>David Jensen</author>
<author>James Allan</author>
</authors>
<title>Mining of concurrent text and time series.</title>
<date>2000</date>
<booktitle>In KDD2000 Workshop on Text Mining,</booktitle>
<pages>37--44</pages>
<contexts>
<context position="3153" citStr="Lavrenko et al., 2000" startWordPosition="483" endWordPosition="486">d by human trading, it is reasonable to say that events can influence the stock market. Figure 1 shows two pieces of financial news about Apple Inc. and Google Inc., respectively. Shares of Apple Inc. fell as trading began in New York on Thursday morning, the day after its former CEO Steve Jobs passed away. Google’s stock fell after grim earnings came out. Accurate extraction of events from financial news may play an important role in stock market prediction. However, previous work represents news documents mainly using simple features, such as bags-of-words, noun phrases, and named entities (Lavrenko et al., 2000; Kogan et al., 2009; Luss and d’Aspremont, 2012; Schumaker and Chen, 2009). With these unstructured features, it is difficult to capture key events embedded in financial news, and even more difficult to model the impact of events on stock market prediction. For example, representing the event “Apple has sued Samsung Electronics for copying ‘the look and feel’ 1415 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1415–1425, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics of its iPad tablet and iPhone smartp</context>
<context position="33462" citStr="Lavrenko et al., 2000" startWordPosition="5494" endWordPosition="5497"> reflects all of the information available and that everyone has a certain degree of access to the information. EMH had a significant impact on security investment, and can serve as the theoretical basis of event-based stock price movement prediction. Various studies have found that financial news can dramatically affect the share price of a security (Chan, 2003; Tetlock et al., 2008). Culter et al. (1998) was one of the first to investigate the relationship between news coverage and stock prices, since which empirical text analysis technology has been widely used across numerous disciplines (Lavrenko et al., 2000; Kogan et al., 2009; Luss and d’Aspremont, 2012). These studies primarily use bags-of-words to represent financial news documents. However, as Schumaker and Chen (2009) and Xie et al. (2013) point out, bagof-words features are not the best choice for predicting stock prices. Schumaker and Chen (2009) extract noun phrases and named entities to augment bags-of-words. Xie et al. (2013) explore a rich feature space that relies on frame semantic parsing. Wang et al. (2014) use the same features as Xie et al. (2013), but they perform nonparametric kernel density estimation to smooth out the distrib</context>
</contexts>
<marker>Lavrenko, Schmill, Lawrie, Ogilvie, Jensen, Allan, 2000</marker>
<rawString>Victor Lavrenko, Matt Schmill, Dawn Lawrie, Paul Ogilvie, David Jensen, and James Allan. 2000. Mining of concurrent text and time series. In KDD2000 Workshop on Text Mining, pages 37–44.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qi Li</author>
<author>Heng Ji</author>
<author>Liang Huang</author>
</authors>
<title>Joint event extraction via structured prediction with global features.</title>
<date>2013</date>
<booktitle>In Proc. of ACL (Volume 1: Long Papers),</booktitle>
<pages>73--82</pages>
<contexts>
<context position="8199" citStr="Li et al., 2013" startWordPosition="1306" endWordPosition="1309">e might fail to accurately predict the stock price movement of Microsoft, Nokia, Oracle and Google, because they cannot indicate the actor and object of the event. To our knowledge, no effort has been reported in the literature to empirically investigate structured event representations for stock market prediction. 2.2 Event Extraction A main contribution of our work is to extract and use structured events instead of bags-of-words in prediction models. However, structured event extraction can be a costly task, requiring predefined event types and manual event templates (Ji and Grishman, 2008; Li et al., 2013). Partly due to this, the bags-of-words-based document representation has been the mainstream method for a long time. To tackle this issue, we resort to Open IE, extracting event tuples from wide-coverage data with1416 out requiring any human input (e.g. templates). Our system is based on the system of Fader et al. (2011) and the work of Ding et al. (2013); it does not require predefined target event types and labeled training examples. Given a natural language sentence obtained from news texts, the following procedure is used to extract structured events: 1. Event Phrase Extraction. We extrac</context>
</contexts>
<marker>Li, Ji, Huang, 2013</marker>
<rawString>Qi Li, Heng Ji, and Liang Huang. 2013. Joint event extraction via structured prediction with global features. In Proc. of ACL (Volume 1: Long Papers), pages 73–82, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew W Lo</author>
<author>Archie Craig MacKinlay</author>
</authors>
<title>When are contrarian profits due to stock market overreaction? Review of Financial studies,</title>
<date>1990</date>
<pages>3--2</pages>
<contexts>
<context position="2060" citStr="Lo and MacKinlay, 1990" startWordPosition="302" endWordPosition="305">cting stock price movements is of clear interest to investors, public companies and governments. There has been a debate on whether the market can be predicted. The Random Walk Theory (Malkiel, 1973) hypothesizes that prices are determined randomly and hence it is impossible to outperform the market. However, with advances of AI, it has been shown empirically that stock This work was done while the first author was visiting Singapore University of Technology and Design Figure 1: Example news for Apple Inc. and Google Inc. price movement is predictable (Bondt and Thaler, 1985; Jegadeesh, 1990; Lo and MacKinlay, 1990; Jegadeesh and Titman, 1993). Recent work (Das and Chen, 2007; Tetlock, 2007; Tetlock et al., 2008; Si et al., 2013; Xie et al., 2013; Wang and Hua, 2014) has applied Natural Language Processing (NLP) techniques to help analyze the effect of web texts on stock market prediction, finding that events reported in financial news are important evidence to stock price movement prediction. As news events affect human decisions and the volatility of stock prices is influenced by human trading, it is reasonable to say that events can influence the stock market. Figure 1 shows two pieces of financial n</context>
</contexts>
<marker>Lo, MacKinlay, 1990</marker>
<rawString>Andrew W Lo and Archie Craig MacKinlay. 1990. When are contrarian profits due to stock market overreaction? Review of Financial studies, 3(2):175–205.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronny Luss</author>
<author>Alexandre d’Aspremont</author>
</authors>
<title>Predicting abnormal returns from news using text classification. Quantitative Finance,</title>
<date>2012</date>
<pages>1--14</pages>
<marker>Luss, d’Aspremont, 2012</marker>
<rawString>Ronny Luss and Alexandre d’Aspremont. 2012. Predicting abnormal returns from news using text classification. Quantitative Finance, pp.1–14, doi:10.1080/14697688.2012.672762.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Burton G Malkiel</author>
</authors>
<title>A Random Walk Down Wall Street.</title>
<date>1973</date>
<publisher>W. W. Norton,</publisher>
<location>New York.</location>
<contexts>
<context position="1637" citStr="Malkiel, 1973" startWordPosition="237" endWordPosition="238">s are employed to empirically investigate the hidden and complex relationships between events and the stock market. Largescale experiments show that the accuracy of S&amp;P 500 index prediction is 60%, and that of individual stock prediction can be over 70%. Our event-based system outperforms bags-of-words-based baselines, and previously reported systems trained on S&amp;P 500 stock historical data. 1 Introduction Predicting stock price movements is of clear interest to investors, public companies and governments. There has been a debate on whether the market can be predicted. The Random Walk Theory (Malkiel, 1973) hypothesizes that prices are determined randomly and hence it is impossible to outperform the market. However, with advances of AI, it has been shown empirically that stock This work was done while the first author was visiting Singapore University of Technology and Design Figure 1: Example news for Apple Inc. and Google Inc. price movement is predictable (Bondt and Thaler, 1985; Jegadeesh, 1990; Lo and MacKinlay, 1990; Jegadeesh and Titman, 1993). Recent work (Das and Chen, 2007; Tetlock, 2007; Tetlock et al., 2008; Si et al., 2013; Xie et al., 2013; Wang and Hua, 2014) has applied Natural L</context>
</contexts>
<marker>Malkiel, 1973</marker>
<rawString>Burton G. Malkiel. 1973. A Random Walk Down Wall Street. W. W. Norton, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>Wordnet: a lexical database for english.</title>
<date>1995</date>
<journal>Communications of the ACM,</journal>
<volume>38</volume>
<issue>11</issue>
<contexts>
<context position="10705" citStr="Miller, 1995" startWordPosition="1735" endWordPosition="1736">object “sector” and “jobs”, respectively. The structured event is extracted as (Private sector, adds, 114,000 jobs). 2.3 Event Generalization Our goal is to train a model that is able to make predictions based on various expressions of the same event. For example, “Microsoft swallows Nokia’s phone business for $7.2 billion” and “Microsoft purchases Nokia’s phone business” report the same event. To improve the accuracy of our prediction model, we should endow the event extraction algorithm with generalization capacity. To this end, we leverage knowledge from two well-known ontologies, WordNet (Miller, 1995) and VerbNet (Kipper et al., 2006). The process of event generalization consists of two steps. First, we construct a morphological analysis tool based on the WordNet stemmer to extract lemma forms of inflected words. For example, in “Instant view: Private sector adds 114,000 jobs in July.”, the words “adds” and “jobs” are transformed to “add” and “job”, respectively. Second, we generalize each verb to its class name in VerbNet. For example, “add” belongs to the multiply class. After generalization, the event (Private sector, adds, 114,000 jobs) becomes (private sector, multiply class, 114,000 </context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>George A Miller. 1995. Wordnet: a lexical database for english. Communications of the ACM, 38(11):39–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kira Radinsky</author>
<author>Eric Horvitz</author>
</authors>
<title>Mining the web to predict future events.</title>
<date>2013</date>
<booktitle>In Proceedings of the sixth ACM international conference on Web search and data mining,</booktitle>
<pages>255--264</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="11443" citStr="Radinsky and Horvitz, 2013" startWordPosition="1851" endWordPosition="1854">uct a morphological analysis tool based on the WordNet stemmer to extract lemma forms of inflected words. For example, in “Instant view: Private sector adds 114,000 jobs in July.”, the words “adds” and “jobs” are transformed to “add” and “job”, respectively. Second, we generalize each verb to its class name in VerbNet. For example, “add” belongs to the multiply class. After generalization, the event (Private sector, adds, 114,000 jobs) becomes (private sector, multiply class, 114,000 job). Similar methods on event generalization have been investigated in Open IE based event causal prediction (Radinsky and Horvitz, 2013). 2.4 Prediction Models 1. Linear model. Most previous work uses linear models to predict the stock market (Fung et al., 2002; Luss and d’Aspremont, 2012; Schumaker and Chen, 2009; Kogan et al., 2009; Das and Chen, 2007; Xie et al., 2013). To make direct comparisons, this paper constructs a linear prediction model by using Support Vector Machines (SVMs), a state-of-the-art classification model. Given a training set (d1, y1), (d2, y2), ..., (dN, yN), where n E [1, N], dn is a news document and yi E {+1, −11 is the output class. dn can be news titles, news contents or both. The output Class +1 r</context>
</contexts>
<marker>Radinsky, Horvitz, 2013</marker>
<rawString>Kira Radinsky and Eric Horvitz. 2013. Mining the web to predict future events. In Proceedings of the sixth ACM international conference on Web search and data mining, pages 255–264. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kira Radinsky</author>
<author>Sagie Davidovich</author>
<author>Shaul Markovitch</author>
</authors>
<title>Learning causality for news events prediction.</title>
<date>2012</date>
<booktitle>In Proc. of WWW,</booktitle>
<pages>909--918</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="24113" citStr="Radinsky et al. (2012)" startWordPosition="3988" endWordPosition="3991">rformance. However, three hidden layers mean that we construct a five-layer deep neural network, which is difficult to train (Bengio et al., 1994). We did not obtain improved accuracies using three hidden layers, due to diminishing gradients. A deep investigation of this problem is out of the scope of this paper. 3.5 Experiments with Different Amounts of Data We conduct a development experiment by extracting news titles and contents from Reuters and Bloomberg, respectively. While titles can give the central information about the news, contents may provide some background knowledge or details. Radinsky et al. (2012) argued that news titles are more helpful for prediction compared to news con1420 0.75 0.7 Accuracy 0.65 0.6 0.55 0.5 SanDisk Actavis Gannett 0.5 0.45 0.4 0.35 MCC 0.3 0.25 0.2 0.15 k 0.1 Hershey Actavis Gannett SanDis Starbucks Visa individual stock Symantec Mattel Wal-Mart BoeinGgle Nike Qualcomm Apache Avon Hershey Starbucks Avon Visa Symantec Mattel Nike Apache Qualcomm Wal-MartGoogle Boeing individual stock 0 100 200 300 400 500 0 100 200 300 400 500 Company Ranking Company Ranking (a) Accuarcy (b) MCC Figure 4: Individual stock prediction experiment results tents, and this paper mainly u</context>
</contexts>
<marker>Radinsky, Davidovich, Markovitch, 2012</marker>
<rawString>Kira Radinsky, Sagie Davidovich, and Shaul Markovitch. 2012. Learning causality for news events prediction. In Proc. of WWW, pages 909–918. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David E Rumelhart</author>
<author>Geoffrey E Hinton</author>
<author>Ronald J Williams</author>
</authors>
<title>Learning internal representations by error propagation.</title>
<date>1985</date>
<tech>Technical report, DTIC Document.</tech>
<contexts>
<context position="14130" citStr="Rumelhart et al., 1985" startWordPosition="2342" endWordPosition="2345">(w2k1, w2k2, ..., w2k|y1|), k E [1, |y2|] and w1j = (w1j1, w1j2, ..., w1jM), j E [1, |y1|]. w2kj is the weight between the kth neuron of the last hidden layer and the jth neuron of the first hidden layer; w1jm is the weight between the jth neuron of the first hidden layer and the mth neuron of the input layer m E [1, M]; dn is a news document and 4(dn) maps dn into a M-dimensional features space. News documents and features used in the nonlinear model are the same as those in the linear model, which will be introduced in details in the next subsection. The standard back-propagation algorithm (Rumelhart et al., 1985) is used for supervised training of the neural network. train dev test number of 1425 178 179 instances number of 54776 6457 6593 events time inter- 02/10/2006 19/06/2012 22/02/2013 val - - - 18/16/2012 21/02/2013 21/11/2013 Table 1: Dataset splitting 2.5 Feature Representation In this paper, we use the same features (i.e. document representations) in the linear and nonlinear prediction models, including bags-of-words and structured events. (1) Bag-of-words features. We use the classic “TFIDF” score for bag-of-words features. Let L be the vocabulary size derived from the training data (introdu</context>
</contexts>
<marker>Rumelhart, Hinton, Williams, 1985</marker>
<rawString>David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. 1985. Learning internal representations by error propagation. Technical report, DTIC Document.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert P Schumaker</author>
<author>Hsinchun Chen</author>
</authors>
<title>Textual analysis of stock market prediction using breaking financial news: The azfin text system.</title>
<date>2009</date>
<journal>ACM Transactions on Information Systems (TOIS),</journal>
<volume>27</volume>
<issue>2</issue>
<contexts>
<context position="3228" citStr="Schumaker and Chen, 2009" startWordPosition="495" endWordPosition="498">he stock market. Figure 1 shows two pieces of financial news about Apple Inc. and Google Inc., respectively. Shares of Apple Inc. fell as trading began in New York on Thursday morning, the day after its former CEO Steve Jobs passed away. Google’s stock fell after grim earnings came out. Accurate extraction of events from financial news may play an important role in stock market prediction. However, previous work represents news documents mainly using simple features, such as bags-of-words, noun phrases, and named entities (Lavrenko et al., 2000; Kogan et al., 2009; Luss and d’Aspremont, 2012; Schumaker and Chen, 2009). With these unstructured features, it is difficult to capture key events embedded in financial news, and even more difficult to model the impact of events on stock market prediction. For example, representing the event “Apple has sued Samsung Electronics for copying ‘the look and feel’ 1415 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1415–1425, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics of its iPad tablet and iPhone smartphone.” using term-level features {“Apple”, “sued”, “Samsung”, “Electronics”</context>
<context position="11622" citStr="Schumaker and Chen, 2009" startWordPosition="1880" endWordPosition="1883">he words “adds” and “jobs” are transformed to “add” and “job”, respectively. Second, we generalize each verb to its class name in VerbNet. For example, “add” belongs to the multiply class. After generalization, the event (Private sector, adds, 114,000 jobs) becomes (private sector, multiply class, 114,000 job). Similar methods on event generalization have been investigated in Open IE based event causal prediction (Radinsky and Horvitz, 2013). 2.4 Prediction Models 1. Linear model. Most previous work uses linear models to predict the stock market (Fung et al., 2002; Luss and d’Aspremont, 2012; Schumaker and Chen, 2009; Kogan et al., 2009; Das and Chen, 2007; Xie et al., 2013). To make direct comparisons, this paper constructs a linear prediction model by using Support Vector Machines (SVMs), a state-of-the-art classification model. Given a training set (d1, y1), (d2, y2), ..., (dN, yN), where n E [1, N], dn is a news document and yi E {+1, −11 is the output class. dn can be news titles, news contents or both. The output Class +1 represents that the stock price will increase the next day/week/month, and the output Class -1 represents that the stock price will decrease the next day/week/month. The features c</context>
<context position="33631" citStr="Schumaker and Chen (2009)" startWordPosition="5520" endWordPosition="5523">and can serve as the theoretical basis of event-based stock price movement prediction. Various studies have found that financial news can dramatically affect the share price of a security (Chan, 2003; Tetlock et al., 2008). Culter et al. (1998) was one of the first to investigate the relationship between news coverage and stock prices, since which empirical text analysis technology has been widely used across numerous disciplines (Lavrenko et al., 2000; Kogan et al., 2009; Luss and d’Aspremont, 2012). These studies primarily use bags-of-words to represent financial news documents. However, as Schumaker and Chen (2009) and Xie et al. (2013) point out, bagof-words features are not the best choice for predicting stock prices. Schumaker and Chen (2009) extract noun phrases and named entities to augment bags-of-words. Xie et al. (2013) explore a rich feature space that relies on frame semantic parsing. Wang et al. (2014) use the same features as Xie et al. (2013), but they perform nonparametric kernel density estimation to smooth out the distribution of features. These can be regarded as extensions to the bag-of-word method. The drawback of these approaches, as discussed in the introduction, is that they do not</context>
</contexts>
<marker>Schumaker, Chen, 2009</marker>
<rawString>Robert P Schumaker and Hsinchun Chen. 2009. Textual analysis of stock market prediction using breaking financial news: The azfin text system. ACM Transactions on Information Systems (TOIS), 27(2):12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ramesh Sharda</author>
<author>Dursun Delen</author>
</authors>
<title>Predicting box-office success of motion pictures with neural networks.</title>
<date>2006</date>
<booktitle>Expert Systems with Applications,</booktitle>
<volume>30</volume>
<issue>2</issue>
<contexts>
<context position="23296" citStr="Sharda and Delen (2006)" startWordPosition="3855" endWordPosition="3859">ments with Different Numbers of Hidden Layers of the Deep Neural Network Model Cybenko (1989) states that when every processing element utilizes the sigmoid activation function, one hidden layer is enough to solve any discriminant classification problem, and two hidden layers are capable to parse arbitrary output functions of input pattern. Here we conduct a development experiment by different number of hidden layers for the deep neural network model. As shown in Table 2, the performance of two hidden layers is better than one hidden layer, which is consistent with the experimental results of Sharda and Delen (2006) on the task of movie box-office prediction. It indicates that more hidden layers can explain more complex relations (Bengio, 2009). Intuitively, three or more hidden layers may achieve better performance. However, three hidden layers mean that we construct a five-layer deep neural network, which is difficult to train (Bengio et al., 1994). We did not obtain improved accuracies using three hidden layers, due to diminishing gradients. A deep investigation of this problem is out of the scope of this paper. 3.5 Experiments with Different Amounts of Data We conduct a development experiment by extr</context>
</contexts>
<marker>Sharda, Delen, 2006</marker>
<rawString>Ramesh Sharda and Dursun Delen. 2006. Predicting box-office success of motion pictures with neural networks. Expert Systems with Applications, 30(2):243–254.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianfeng Si</author>
<author>Arjun Mukherjee</author>
<author>Bing Liu</author>
<author>Qing Li</author>
<author>Huayi Li</author>
<author>Xiaotie Deng</author>
</authors>
<title>Exploiting topic based twitter sentiment for stock prediction.</title>
<date>2013</date>
<booktitle>In Proc. of ACL (Volume 2: Short Papers),</booktitle>
<pages>24--29</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="2176" citStr="Si et al., 2013" startWordPosition="322" endWordPosition="325"> whether the market can be predicted. The Random Walk Theory (Malkiel, 1973) hypothesizes that prices are determined randomly and hence it is impossible to outperform the market. However, with advances of AI, it has been shown empirically that stock This work was done while the first author was visiting Singapore University of Technology and Design Figure 1: Example news for Apple Inc. and Google Inc. price movement is predictable (Bondt and Thaler, 1985; Jegadeesh, 1990; Lo and MacKinlay, 1990; Jegadeesh and Titman, 1993). Recent work (Das and Chen, 2007; Tetlock, 2007; Tetlock et al., 2008; Si et al., 2013; Xie et al., 2013; Wang and Hua, 2014) has applied Natural Language Processing (NLP) techniques to help analyze the effect of web texts on stock market prediction, finding that events reported in financial news are important evidence to stock price movement prediction. As news events affect human decisions and the volatility of stock prices is influenced by human trading, it is reasonable to say that events can influence the stock market. Figure 1 shows two pieces of financial news about Apple Inc. and Google Inc., respectively. Shares of Apple Inc. fell as trading began in New York on Thursd</context>
<context position="35301" citStr="Si et al., 2013" startWordPosition="5802" endWordPosition="5805">the event and the object of the event). Second, Feldman et al. (2011) can obtain only limited categories of events, and hence the scalability of their work is not strong. In contrast, we extract structured events by leveraging Open Information Extraction technology (Open IE; Yates et al. (2007); Etzioni et al. (2011); Faber et al. (2011)) without predefined event types, which can effectively solve the two problems above. Apart from events, sentiment analysis is another perspective to the problem of stock prediction (Das and Chen, 2007; Tetlock, 2007; Tetlock et al., 2008; Bollen et al., 2011; Si et al., 2013). Tetlock (2007) examines how qualitative information (i.e. the fraction of negative words in a particular news column) is incorporated in aggregate market valuations. Tetlock, Saar-Tsechansky, and Macskassy (2008) extend that analysis to address the impact of negative words in all Wall Street Joural (WSJ) and Dow Jones News Services (DJNS) stories about individual S&amp;P500 firms from 1980 to 2004. Bollen and Zeng (2011) study whether the large-scale collective emotion on Twitter is correlated with the volatility of Dow Jones Industrial Average (DJIA). From the experimental results, they find th</context>
</contexts>
<marker>Si, Mukherjee, Liu, Li, Li, Deng, 2013</marker>
<rawString>Jianfeng Si, Arjun Mukherjee, Bing Liu, Qing Li, Huayi Li, and Xiaotie Deng. 2013. Exploiting topic based twitter sentiment for stock prediction. In Proc. of ACL (Volume 2: Short Papers), pages 24– 29, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul C Tetlock</author>
<author>Maytal Saar-Tsechansky</author>
<author>Sofus Macskassy</author>
</authors>
<title>More than words: Quantifying language to measure firms’ fundamentals.</title>
<date>2008</date>
<journal>The Journal of Finance,</journal>
<volume>63</volume>
<issue>3</issue>
<contexts>
<context position="2159" citStr="Tetlock et al., 2008" startWordPosition="318" endWordPosition="321">e has been a debate on whether the market can be predicted. The Random Walk Theory (Malkiel, 1973) hypothesizes that prices are determined randomly and hence it is impossible to outperform the market. However, with advances of AI, it has been shown empirically that stock This work was done while the first author was visiting Singapore University of Technology and Design Figure 1: Example news for Apple Inc. and Google Inc. price movement is predictable (Bondt and Thaler, 1985; Jegadeesh, 1990; Lo and MacKinlay, 1990; Jegadeesh and Titman, 1993). Recent work (Das and Chen, 2007; Tetlock, 2007; Tetlock et al., 2008; Si et al., 2013; Xie et al., 2013; Wang and Hua, 2014) has applied Natural Language Processing (NLP) techniques to help analyze the effect of web texts on stock market prediction, finding that events reported in financial news are important evidence to stock price movement prediction. As news events affect human decisions and the volatility of stock prices is influenced by human trading, it is reasonable to say that events can influence the stock market. Figure 1 shows two pieces of financial news about Apple Inc. and Google Inc., respectively. Shares of Apple Inc. fell as trading began in N</context>
<context position="33228" citStr="Tetlock et al., 2008" startWordPosition="5456" endWordPosition="5459">t. The literature of stock market prediction was initiated by economists (Keynes, 1937). Subsequently, the influential theory of Efficient Market Hypothesis (EMH) (Fama, 1965) was established, which states that the price of a security reflects all of the information available and that everyone has a certain degree of access to the information. EMH had a significant impact on security investment, and can serve as the theoretical basis of event-based stock price movement prediction. Various studies have found that financial news can dramatically affect the share price of a security (Chan, 2003; Tetlock et al., 2008). Culter et al. (1998) was one of the first to investigate the relationship between news coverage and stock prices, since which empirical text analysis technology has been widely used across numerous disciplines (Lavrenko et al., 2000; Kogan et al., 2009; Luss and d’Aspremont, 2012). These studies primarily use bags-of-words to represent financial news documents. However, as Schumaker and Chen (2009) and Xie et al. (2013) point out, bagof-words features are not the best choice for predicting stock prices. Schumaker and Chen (2009) extract noun phrases and named entities to augment bags-of-word</context>
<context position="35262" citStr="Tetlock et al., 2008" startWordPosition="5794" endWordPosition="5797">xtract structured event (e.g. the actor of the event and the object of the event). Second, Feldman et al. (2011) can obtain only limited categories of events, and hence the scalability of their work is not strong. In contrast, we extract structured events by leveraging Open Information Extraction technology (Open IE; Yates et al. (2007); Etzioni et al. (2011); Faber et al. (2011)) without predefined event types, which can effectively solve the two problems above. Apart from events, sentiment analysis is another perspective to the problem of stock prediction (Das and Chen, 2007; Tetlock, 2007; Tetlock et al., 2008; Bollen et al., 2011; Si et al., 2013). Tetlock (2007) examines how qualitative information (i.e. the fraction of negative words in a particular news column) is incorporated in aggregate market valuations. Tetlock, Saar-Tsechansky, and Macskassy (2008) extend that analysis to address the impact of negative words in all Wall Street Joural (WSJ) and Dow Jones News Services (DJNS) stories about individual S&amp;P500 firms from 1980 to 2004. Bollen and Zeng (2011) study whether the large-scale collective emotion on Twitter is correlated with the volatility of Dow Jones Industrial Average (DJIA). From</context>
</contexts>
<marker>Tetlock, Saar-Tsechansky, Macskassy, 2008</marker>
<rawString>Paul C Tetlock, Maytal Saar-Tsechansky, and Sofus Macskassy. 2008. More than words: Quantifying language to measure firms’ fundamentals. The Journal of Finance, 63(3):1437–1467.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul C Tetlock</author>
</authors>
<title>Giving content to investor sentiment: The role of media in the stock market.</title>
<date>2007</date>
<journal>The Journal of Finance,</journal>
<volume>62</volume>
<issue>3</issue>
<contexts>
<context position="2137" citStr="Tetlock, 2007" startWordPosition="316" endWordPosition="317">vernments. There has been a debate on whether the market can be predicted. The Random Walk Theory (Malkiel, 1973) hypothesizes that prices are determined randomly and hence it is impossible to outperform the market. However, with advances of AI, it has been shown empirically that stock This work was done while the first author was visiting Singapore University of Technology and Design Figure 1: Example news for Apple Inc. and Google Inc. price movement is predictable (Bondt and Thaler, 1985; Jegadeesh, 1990; Lo and MacKinlay, 1990; Jegadeesh and Titman, 1993). Recent work (Das and Chen, 2007; Tetlock, 2007; Tetlock et al., 2008; Si et al., 2013; Xie et al., 2013; Wang and Hua, 2014) has applied Natural Language Processing (NLP) techniques to help analyze the effect of web texts on stock market prediction, finding that events reported in financial news are important evidence to stock price movement prediction. As news events affect human decisions and the volatility of stock prices is influenced by human trading, it is reasonable to say that events can influence the stock market. Figure 1 shows two pieces of financial news about Apple Inc. and Google Inc., respectively. Shares of Apple Inc. fell</context>
<context position="35240" citStr="Tetlock, 2007" startWordPosition="5792" endWordPosition="5793">, they cannot extract structured event (e.g. the actor of the event and the object of the event). Second, Feldman et al. (2011) can obtain only limited categories of events, and hence the scalability of their work is not strong. In contrast, we extract structured events by leveraging Open Information Extraction technology (Open IE; Yates et al. (2007); Etzioni et al. (2011); Faber et al. (2011)) without predefined event types, which can effectively solve the two problems above. Apart from events, sentiment analysis is another perspective to the problem of stock prediction (Das and Chen, 2007; Tetlock, 2007; Tetlock et al., 2008; Bollen et al., 2011; Si et al., 2013). Tetlock (2007) examines how qualitative information (i.e. the fraction of negative words in a particular news column) is incorporated in aggregate market valuations. Tetlock, Saar-Tsechansky, and Macskassy (2008) extend that analysis to address the impact of negative words in all Wall Street Joural (WSJ) and Dow Jones News Services (DJNS) stories about individual S&amp;P500 firms from 1980 to 2004. Bollen and Zeng (2011) study whether the large-scale collective emotion on Twitter is correlated with the volatility of Dow Jones Industria</context>
</contexts>
<marker>Tetlock, 2007</marker>
<rawString>Paul C Tetlock. 2007. Giving content to investor sentiment: The role of media in the stock market. The Journal of Finance, 62(3):1139–1168.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Yang Wang</author>
<author>Zhenhao Hua</author>
</authors>
<title>A semiparametric gaussian copula regression model for predicting financial risks from earnings calls.</title>
<date>2014</date>
<booktitle>In Proc. of ACL,</booktitle>
<contexts>
<context position="2215" citStr="Wang and Hua, 2014" startWordPosition="330" endWordPosition="333">d. The Random Walk Theory (Malkiel, 1973) hypothesizes that prices are determined randomly and hence it is impossible to outperform the market. However, with advances of AI, it has been shown empirically that stock This work was done while the first author was visiting Singapore University of Technology and Design Figure 1: Example news for Apple Inc. and Google Inc. price movement is predictable (Bondt and Thaler, 1985; Jegadeesh, 1990; Lo and MacKinlay, 1990; Jegadeesh and Titman, 1993). Recent work (Das and Chen, 2007; Tetlock, 2007; Tetlock et al., 2008; Si et al., 2013; Xie et al., 2013; Wang and Hua, 2014) has applied Natural Language Processing (NLP) techniques to help analyze the effect of web texts on stock market prediction, finding that events reported in financial news are important evidence to stock price movement prediction. As news events affect human decisions and the volatility of stock prices is influenced by human trading, it is reasonable to say that events can influence the stock market. Figure 1 shows two pieces of financial news about Apple Inc. and Google Inc., respectively. Shares of Apple Inc. fell as trading began in New York on Thursday morning, the day after its former CE</context>
</contexts>
<marker>Wang, Hua, 2014</marker>
<rawString>William Yang Wang and Zhenhao Hua. 2014. A semiparametric gaussian copula regression model for predicting financial risks from earnings calls. In Proc. of ACL, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Boyi Xie</author>
<author>Rebecca J Passonneau</author>
<author>Leon Wu</author>
<author>Germ´an G Creamer</author>
</authors>
<title>Semantic frames to predict stock price movement.</title>
<date>2013</date>
<booktitle>In Proc. ofACL (Volume 1: Long Papers),</booktitle>
<pages>873--883</pages>
<contexts>
<context position="2194" citStr="Xie et al., 2013" startWordPosition="326" endWordPosition="329">et can be predicted. The Random Walk Theory (Malkiel, 1973) hypothesizes that prices are determined randomly and hence it is impossible to outperform the market. However, with advances of AI, it has been shown empirically that stock This work was done while the first author was visiting Singapore University of Technology and Design Figure 1: Example news for Apple Inc. and Google Inc. price movement is predictable (Bondt and Thaler, 1985; Jegadeesh, 1990; Lo and MacKinlay, 1990; Jegadeesh and Titman, 1993). Recent work (Das and Chen, 2007; Tetlock, 2007; Tetlock et al., 2008; Si et al., 2013; Xie et al., 2013; Wang and Hua, 2014) has applied Natural Language Processing (NLP) techniques to help analyze the effect of web texts on stock market prediction, finding that events reported in financial news are important evidence to stock price movement prediction. As news events affect human decisions and the volatility of stock prices is influenced by human trading, it is reasonable to say that events can influence the stock market. Figure 1 shows two pieces of financial news about Apple Inc. and Google Inc., respectively. Shares of Apple Inc. fell as trading began in New York on Thursday morning, the da</context>
<context position="11681" citStr="Xie et al., 2013" startWordPosition="1892" endWordPosition="1895">pectively. Second, we generalize each verb to its class name in VerbNet. For example, “add” belongs to the multiply class. After generalization, the event (Private sector, adds, 114,000 jobs) becomes (private sector, multiply class, 114,000 job). Similar methods on event generalization have been investigated in Open IE based event causal prediction (Radinsky and Horvitz, 2013). 2.4 Prediction Models 1. Linear model. Most previous work uses linear models to predict the stock market (Fung et al., 2002; Luss and d’Aspremont, 2012; Schumaker and Chen, 2009; Kogan et al., 2009; Das and Chen, 2007; Xie et al., 2013). To make direct comparisons, this paper constructs a linear prediction model by using Support Vector Machines (SVMs), a state-of-the-art classification model. Given a training set (d1, y1), (d2, y2), ..., (dN, yN), where n E [1, N], dn is a news document and yi E {+1, −11 is the output class. dn can be news titles, news contents or both. The output Class +1 represents that the stock price will increase the next day/week/month, and the output Class -1 represents that the stock price will decrease the next day/week/month. The features can be bag-of-words features or structured event features. B</context>
<context position="19321" citStr="Xie et al., 2013" startWordPosition="3212" endWordPosition="3215">e data from 19/06/2012 to 21/02/2013 for development testing, and the data from 22/02/2013 to 21/11/2013 for testing. There are about 54,776 events in the training set, 6,457 events in the development set and 6,593 events in the test set. 3.2 Evaluation Metrics We use two assessment metrics. First, a standard and intuitive approach to measuring the performance of classifiers is accuracy. However, this measure is very sensitive to data skew: when a class has an overwhelmingly high frequency, the accuracy can be high using a classifier that makes prediction on the majority class. Previous work (Xie et al., 2013) uses an additional evaluation metric, which relies on the Matthews Correlation Cofficient (MCC) to avoid bias due to data skew (our data are rather large and not severely skewed, but we also use MCC for comparison with previous work). MCC is a single summary value that incorporates all 4 cells of a 2*2 confusion matrix (True Positive, False Positive, True Negative and False Negative, respectively). Given TP, TN, FP and FN: MCC = TP ·TN−FP ·FN √ (TP+FP)(TP+FN)(TN +FP)(TN +FN) (3) 3.3 Overall Development Results We evaluate our four prediction methods (i.e. SVM with bag-of-word features (bow), </context>
<context position="33653" citStr="Xie et al. (2013)" startWordPosition="5525" endWordPosition="5528">al basis of event-based stock price movement prediction. Various studies have found that financial news can dramatically affect the share price of a security (Chan, 2003; Tetlock et al., 2008). Culter et al. (1998) was one of the first to investigate the relationship between news coverage and stock prices, since which empirical text analysis technology has been widely used across numerous disciplines (Lavrenko et al., 2000; Kogan et al., 2009; Luss and d’Aspremont, 2012). These studies primarily use bags-of-words to represent financial news documents. However, as Schumaker and Chen (2009) and Xie et al. (2013) point out, bagof-words features are not the best choice for predicting stock prices. Schumaker and Chen (2009) extract noun phrases and named entities to augment bags-of-words. Xie et al. (2013) explore a rich feature space that relies on frame semantic parsing. Wang et al. (2014) use the same features as Xie et al. (2013), but they perform nonparametric kernel density estimation to smooth out the distribution of features. These can be regarded as extensions to the bag-of-word method. The drawback of these approaches, as discussed in the introduction, is that they do not directly model events</context>
</contexts>
<marker>Xie, Passonneau, Wu, Creamer, 2013</marker>
<rawString>Boyi Xie, Rebecca J. Passonneau, Leon Wu, and Germ´an G. Creamer. 2013. Semantic frames to predict stock price movement. In Proc. ofACL (Volume 1: Long Papers), pages 873–883, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Yates</author>
<author>Michael Cafarella</author>
<author>Michele Banko</author>
<author>Oren Etzioni</author>
<author>Matthew Broadhead</author>
<author>Stephen Soderland</author>
</authors>
<title>Textrunner: open information extraction on the web. In</title>
<date>2007</date>
<booktitle>Proc. ofNAACL: Demonstrations,</booktitle>
<pages>25--26</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="34980" citStr="Yates et al. (2007)" startWordPosition="5750" endWordPosition="5753">; Hayo and Kutan, 2005; Feldman et al., 2011). Fung, Yu, and Lam (2002) use a normalized word vector-space to model event. Feldman et al. (2011) extract 9 predefined categories of events based on heuristic rules. There are two main problems with these efforts. First, they cannot extract structured event (e.g. the actor of the event and the object of the event). Second, Feldman et al. (2011) can obtain only limited categories of events, and hence the scalability of their work is not strong. In contrast, we extract structured events by leveraging Open Information Extraction technology (Open IE; Yates et al. (2007); Etzioni et al. (2011); Faber et al. (2011)) without predefined event types, which can effectively solve the two problems above. Apart from events, sentiment analysis is another perspective to the problem of stock prediction (Das and Chen, 2007; Tetlock, 2007; Tetlock et al., 2008; Bollen et al., 2011; Si et al., 2013). Tetlock (2007) examines how qualitative information (i.e. the fraction of negative words in a particular news column) is incorporated in aggregate market valuations. Tetlock, Saar-Tsechansky, and Macskassy (2008) extend that analysis to address the impact of negative words in </context>
</contexts>
<marker>Yates, Cafarella, Banko, Etzioni, Broadhead, Soderland, 2007</marker>
<rawString>Alexander Yates, Michael Cafarella, Michele Banko, Oren Etzioni, Matthew Broadhead, and Stephen Soderland. 2007. Textrunner: open information extraction on the web. In Proc. ofNAACL: Demonstrations, pages 25–26. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>Syntactic processing using the generalized perceptron and beam search.</title>
<date>2011</date>
<journal>Computational Linguistics,</journal>
<volume>37</volume>
<issue>1</issue>
<contexts>
<context position="8892" citStr="Zhang and Clark (2011)" startWordPosition="1422" endWordPosition="1425">as been the mainstream method for a long time. To tackle this issue, we resort to Open IE, extracting event tuples from wide-coverage data with1416 out requiring any human input (e.g. templates). Our system is based on the system of Fader et al. (2011) and the work of Ding et al. (2013); it does not require predefined target event types and labeled training examples. Given a natural language sentence obtained from news texts, the following procedure is used to extract structured events: 1. Event Phrase Extraction. We extract the predicate verb P of a sentence based on the dependency parser of Zhang and Clark (2011), and then find the longest sequence of words Pv, such that Pv starts at P and satisfies the syntactic and lexical constraints proposed by Fader et al. (2011). The content of these two constraints are as follows: • Syntactic constraint: every multi-word event phrase must begin with a verb, end with a preposition, and be a contiguous sequence of words in the sentence. • Lexical constraint: an event phrase should appear with at least a minimal number of distinct argument pairs in a large corpus. 2. Argument Extraction. For each event phrase Pv identified in the step above, we find the nearest no</context>
</contexts>
<marker>Zhang, Clark, 2011</marker>
<rawString>Yue Zhang and Stephen Clark. 2011. Syntactic processing using the generalized perceptron and beam search. Computational Linguistics, 37(1):105–151.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
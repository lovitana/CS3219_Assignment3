<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000379">
<title confidence="0.995495">
Modeling Biological Processes for Reading Comprehension
</title>
<author confidence="0.964859">
Jonathan Berant∗, Vivek Srikumar∗, Pei-Chun Chen, Brad Huang and Christopher D. Manning
</author>
<affiliation confidence="0.854159">
Stanford University, Stanford
</affiliation>
<author confidence="0.850014">
Abby Vander Linden and Brittany Harding
</author>
<affiliation confidence="0.996115">
University of Washington, Seattle
</affiliation>
<sectionHeader confidence="0.9788" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998196">
Machine reading calls for programs that
read and understand text, but most current
work only attempts to extract facts from
redundant web-scale corpora. In this pa-
per, we focus on a new reading compre-
hension task that requires complex reason-
ing over a single document. The input is
a paragraph describing a biological pro-
cess, and the goal is to answer questions
that require an understanding of the re-
lations between entities and events in the
process. To answer the questions, we first
predict a rich structure representing the
process in the paragraph. Then, we map
the question to a formal query, which is
executed against the predicted structure.
We demonstrate that answering questions
via predicted structures substantially im-
proves accuracy over baselines that use
shallower representations.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9829495">
The goal of machine reading is to develop pro-
grams that read text to learn about the world
and make decisions based on accumulated knowl-
edge. Work in this field has focused mostly on
macro-reading, i.e., processing large text collec-
tions and extracting knowledge bases of facts (Et-
zioni et al., 2006; Carlson et al., 2010; Fader et al.,
2011). Such methods rely on redundancy, and are
thus suitable for answering common factoid ques-
tions which have ample evidence in text (Fader et
al., 2013). However, reading a single document
(micro-reading) to answer comprehension ques-
tions that require deep reasoning is currently be-
yond the scope of state-of-the-art systems.
In this paper, we introduce a task where given
a paragraph describing a process, the goal is to
∗Both authors equally contributed to the paper.
answer reading comprehension questions that test
understanding of the underlying structure. In par-
ticular, we consider processes in biology text-
books such as this excerpt and the question that
follows:
“. .. Water is split, providing a source of elec-
trons and protons (hydrogen ions, H+) and giv-
ing off O2 as a by-product. Light absorbed by
chlorophyll drives a transfer of the electrons
and hydrogen ions from water to an acceptor
called NADP+ ...”
</bodyText>
<figure confidence="0.981171333333333">
Q What can the splitting of water lead to?
a Light absorption
b Transfer of ions
</figure>
<bodyText confidence="0.999930846153846">
This excerpt describes a process in which a com-
plex set of events and entities are related to one
another. A system trying to answer this ques-
tion must extract a rich structure spanning multi-
ple sentences and reason that water splitting com-
bined with light absorption leads to transfer of
ions. Note that shallow methods, which rely on
lexical overlap or text proximity, will fail. Indeed,
both answers are covered by the paragraph and the
wrong answer is closer in the text to the question.
We propose a novel method that tackles this
challenging problem (see Figure 1). First, we train
a supervised structure predictor that learns to ex-
tract entities, events and their relations describing
the biological process. This is a difficult prob-
lem because events have complex interactions that
span multiple sentences. Then, treating this struc-
ture as a small knowledge-base, we map ques-
tions to formal queries that are executed against
the structure to provide the answer.
Micro-reading is an important aspect of natural
language understanding (Richardson et al., 2013;
Kushman et al., 2014). In this work, we focus
specifically on modeling processes, where events
and entities relate to one another through com-
plex interactions. While we work in the biology
</bodyText>
<page confidence="0.97953">
1499
</page>
<note confidence="0.65492675">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1499–1510,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
“... Water is split, providing a source of elec-
trons and protons (hydrogen ions, H+) and
giving off O2 as a by-product. Light ab-
sorbed by chlorophyll drives a transfer of
the electrons and hydrogen ions from water
to an acceptor called NADP+ ...”
</note>
<figure confidence="0.998667916666667">
Q What can the splitting of water lead to?
a Light absorption
b Transfer of ions
Step 2
Step 1
water
water
water
THEME split absorb (CAUSE|ENABLE)+ THEME
(CAUSE|ENABLE)+
THEME THEME
split transfer
THEME
Step 3: Answer = b
ENABLE CAUSE
split
transfer
absorb
THEME
THEME
ions
light
light
ions
</figure>
<figureCaption confidence="0.9954912">
Figure 1: An overview of our reading comprehension system. First, we predict a structure from the input paragraph (the
top right portion shows a partial structure skipping some arguments for brevity). Circles denote events, squares denote argu-
ments, solid arrows represent event-event relations, and dashed arrows represent event-argument relations. Second, we map
the question paired with each answer into a query that will be answered using the structure. The bottom right shows the query
representation. Last, the two queries are executed against the structure, and a final answer is returned.
</figureCaption>
<bodyText confidence="0.911490333333333">
domain, processes are abundant in domains such
as chemistry, economics, manufacturing, and even
everyday events like shopping or cooking, and our
model can be applied to these domains as well.
The contributions of this paper are:
1. We propose a reading comprehension task
which requires deep reasoning over struc-
tures that represent complex relations be-
tween multiple events and entities.
2. We present PROCESSBANK, a new dataset
consisting of descriptions of biological pro-
cesses, fully-annotated with rich process
structures, and accompanied by multiple-
choice questions.
3. We present a novel method for answer-
ing questions, by predicting process struc-
tures and mapping questions to queries. We
demonstrate that by predicting structures we
can improve reading comprehension accu-
racy over baselines that do not exploit the un-
derlying structure.
The data and code for this paper are avail-
able at http://www-nlp.stanford.edu/
software/bioprocess.
</bodyText>
<sectionHeader confidence="0.862841" genericHeader="method">
2 Task Definition and Setup
</sectionHeader>
<bodyText confidence="0.99987652173913">
This section describes the reading comprehension
task we address and the accompanying dataset.
We will use the example in Figure 1 as our run-
ning example throughout the paper.
Our goal is to tackle a complex reading com-
prehension setting that centers on understanding
the underlying meaning of a process description.
We target a multiple-choice setting in which each
input consists of a paragraph of text describing a
biological process, a question, and two possible
answers. The goal is to identify the correct answer
using the text (Figure 1, left). We used the 148
paragraphs from the textbook Biology (Campbell
and Reece, 2005) that were manually identified by
Scaria et al. (2013). We extended this set to 200
paragraphs by including additional paragraphs that
describe biological processes. Each paragraph in
the collection represents a single biological pro-
cess and describes a set of events, their partici-
pants and their interactions.
Because we target understanding of paragraph
meaning, we use the following desiderata for
building the corpus of questions and answers:
</bodyText>
<listItem confidence="0.9917567">
1. The questions should focus on the events and
entities participating in the process described
in the paragraph, and answering the questions
should require reasoning about the relations
between those events and entities.
2. Both answers should have similar lexical
overlap with the paragraph. Moreover, names
of entities and events in the question and an-
swers should appear as in the paragraph and
not using synonyms. This is to ensure that the
</listItem>
<bodyText confidence="0.60036">
task revolves around reading comprehension
rather than lexical variability.1
A biologist created the question-answer part of
</bodyText>
<footnote confidence="0.9994905">
1Lexical variability is an important problem in NLP, but
is not the focus of this task.
</footnote>
<page confidence="0.989477">
1500
</page>
<bodyText confidence="0.999366466666667">
the corpus comprising of 585 questions spread
over the 200 paragraphs. A second annotator val-
idated 326 randomly chosen questions and agreed
on the correct answer with the first annotator in
98.1% of cases. We provide the annotation guide-
lines in the supplementary material.
Figure 1 (left) shows an excerpt of a paragraph
describing a process and an example of a ques-
tion based on it. In general, questions test an un-
derstanding of the interactions between multiple
events (such as causality, inhibition, temporal or-
dering), or between events and entities (i.e., roles
of entities in events), and require complex reason-
ing about chains of event-event and event-entity
relations.
</bodyText>
<sectionHeader confidence="0.96819" genericHeader="method">
3 The Structure of Processes
</sectionHeader>
<bodyText confidence="0.997903541666667">
A natural first step for answering reading compre-
hension questions is to identify a structured rep-
resentation of the text. In this section, we define
this structure. We broadly follow the definition of
Scaria et al. (2013), but modify important aspects,
highlighted at the end of this section.
A paragraph describing a process is a sequence
of tokens that describes events, entities and their
relations (see Figure 1, top right). A process is
a directed graph (T , A, £tt, £ta), where the nodes
T are labeled event triggers, the nodes A are ar-
guments, £tt are labeled edges describing event-
event relations, and £ta are labeled edges from
triggers to arguments denoting semantic roles (see
Figure 1 top right for a partial structure of the run-
ning example). The goal of process extraction is
to generate the process graph given the input para-
graph.
Triggers and arguments A trigger is a token
span denoting the occurrence of an event. In Fig-
ure 1, split, absorbed and transfer are event trig-
gers. In rare cases, a trigger denotes the non-
occurrence of an event. For example, in “sym-
patric speciation can occur when gene flow is
blocked”, sympatric speciation occurs if gene flow
does not happen. Thus, nodes in T are labeled as
either a T-YES or T-NO to distinguish triggers of
events that occur from triggers of events that do
not occur. Arguments are token spans denoting
entities that participate in the process (such as wa-
ter, light and ions in Figure 1).
Semantic roles The edges £ta from triggers
to arguments are labeled by the semantic roles
AGENT, THEME, SOURCE, DESTINATION, LO-
CATION, RESULT, and OTHER for all other roles.
Our running example shows three THEME seman-
tic roles for the three triggers. For brevity, the fig-
ure does not show the RESULT of the event split,
namely, both source of electrons and protons (hy-
drogen ions, H+) and O2.
Event-event relations The directed edges £tt
between triggers are labeled by one of eight pos-
sible event-event relations. These relations are
central to answering reading comprehension ques-
tions, which test understanding of the depen-
dencies and causal relations between the process
events. We first define three relations that express
a dependency between two event triggers u and v.
</bodyText>
<listItem confidence="0.978696625">
1. CAUSE denotes that u starts before v, and if
u happens then v happens (Figure 1).
2. ENABLE denotes that u creates conditions
necessary for the occurrence of v. This
means that u starts before v and v can only
happen if u happens (Figure 1).2
3. PREVENT denotes that u starts before v and
if u happens, then v does not happen.
</listItem>
<bodyText confidence="0.999357304347826">
In processes, events sometimes depend on more
than one other event. For example, in Figure 1
(right top) transfer of ions depends on both water
splitting as well as light absorption. Conversely,
in Figure 2, the shifting event results in either one
of two events but not both. To express both con-
junctions and disjunctions of related events we
add the relations CAUSE-OR, ENABLE-OR and
PREVENT-OR, which express disjunctions, while
the default CAUSE, ENABLE, and PREVENT ex-
press conjunction (Compare the CAUSE-OR rela-
tions in Figure 2 with the relations in Figure 1).
We define the SUPER relation to denote that
event u is part of event v. (In Figure 2, slip-
page is a sub-event of replication.) Last, we use
the event coreference relation SAME to denote two
event mentions referring to the same event.
Notice that the assignments of relation labels in-
teract across different pairs of events. As an ex-
ample, if event u causes event v, then v can not
cause u. Our inference algorithm uses such struc-
tural constraints when predicting process structure
(Section 4).
</bodyText>
<footnote confidence="0.99830875">
2In this work, we do not distinguish causation from facil-
itation, where u can help v but is not absolutely required. We
instructed the annotators to ignore the inherent uncertainty in
these cases and use CAUSE.
</footnote>
<page confidence="0.988621">
1501
</page>
<figureCaption confidence="0.99499">
Figure 2: Partial example of a process, as annotated in our dataset.
</figureCaption>
<table confidence="0.994336">
Avg Min Max
# of triggers 7.0 2 18
# of arguments 11.3 1 36
# of relation 7.9 1 37
</table>
<tableCaption confidence="0.8599825">
Table 1: Statistics of triggers, arguments and rela-
tions over the 200 annotated paragraphs.
</tableCaption>
<bodyText confidence="0.999765205882353">
Three biologists annotated the same 200 para-
graphs described in Section 2 using the brat anno-
tation tool (Stenetorp et al., 2012). For each para-
graph, one annotator annotated the process, and
a second validated its correctness. Importantly,
the questions and answers were authored sepa-
rately by a different annotator, thus ensuring that
the questions and answers are independent from
the annotated structures. Table 1 gives statistics
over the dataset. The annotation guidelines are in-
cluded in the supplementary material.
Relation to Scaria et al. (2013) Scaria et al.
(2013) also defined processes as graphs where
nodes are events and edges describe event-event
relations. Our definition differs in a few important
aspects.
First, the set of event-event relations in that
work included temporal relations in addition to
causal ones. In this work, we posit that because
events in a process are inter-related, causal depen-
dencies are sufficient to capture the relevant tem-
poral ordering between them. Figure 1 illustrates
this phenomenon, where the temporal ordering be-
tween the events of water splitting and light ab-
sorption is unspecified. It does not matter whether
one happens before, during, or after the other. Fur-
thermore, the incoming causal links to transfer im-
ply that the event should happen after splitting and
absorption.
A second difference is that Scaria et al. (2013)
do not include disjunctions and conjunctions of
events in their formulation. Last, Scaria et al.
(2013) predict only relations given input triggers,
while we predict a full process structure.
</bodyText>
<sectionHeader confidence="0.997472" genericHeader="method">
4 Predicting Process Structures
</sectionHeader>
<bodyText confidence="0.99988875">
We now describe the first step of our algorithm.
Given an input paragraph we predict events, their
arguments and event-event relations (Figure 1,
top). We decompose this into three sub-problems:
</bodyText>
<listItem confidence="0.9343760625">
1. Labeling trigger candidates using a multi-
class classifier (Section 4.1).
2. For each trigger, identifying an over-
complete set of possible arguments, using a
classifier tuned for high recall (Section 4.2).
3. Jointly assigning argument labels and rela-
tion labels for all trigger pairs (Section 4.3).
The event-event relations CAUSE, ENABLE,
CAUSE-OR and ENABLE-OR, form a semantic
cluster: If (u, v) is labeled by one of these, then
the occurrence of v depends on the occurrence of
u. Since our dataset is small, we share statistics by
collapsing all four labels to a single ENABLE la-
bel. Similarly, we collapse the PREVENT and
PREVENT-OR labels, overall reducing the number
of relations to four.
</listItem>
<bodyText confidence="0.970746666666667">
For brevity, in what follows we only provide
a flavor of the features we extract, and refer the
reader to the supplementary material for details.
</bodyText>
<subsectionHeader confidence="0.999373">
4.1 Predicting Event Triggers
</subsectionHeader>
<bodyText confidence="0.999798727272727">
The first step is to identify the events in the pro-
cess. We model the trigger detector as a multi-
class classifier that labels all content words in
the paragraph as one of T-YES, T-NO or NOT-
TRIGGER (Recall that a word can trigger an event
that occurred, an event that did not occur, or not
be a trigger at all). For simplicity, we model trig-
gers as single words, but in the gold annotation
about 14% are phrases (such as gene flow). Thus,
we evaluate trigger prediction by taking heads of
gold phrases. To train the classifier, we extract
</bodyText>
<page confidence="0.9808">
1502
</page>
<bodyText confidence="0.999907285714286">
the lemma and POS tag of the word and adja-
cent words, dependency path to the root, POS
tag of children and parent in the dependency tree,
and clustering features from WordNet (Fellbaum,
1998), Nomlex (Macleod et al., 1998), Levin verb
classes (Levin, 1993), and a list of biological pro-
cesses compiled from Wikipedia.
</bodyText>
<subsectionHeader confidence="0.998591">
4.2 Filtering Argument Candidates
</subsectionHeader>
<bodyText confidence="0.9999630625">
Labeling trigger-argument edges is similar to se-
mantic role labeling. Following the standard ap-
proach (Punyakanok et al., 2008), for each trigger
we collect all constituents in the same sentence to
build an over-complete set of plausible candidate
arguments. This set is pruned with a binary classi-
fier that is tuned for high recall (akin to the argu-
ment identifier in SRL systems). On the develop-
ment set we filter more than half of the argument
candidates, while achieving more than 99% recall.
This classifier is trained using argument identifica-
tion features from Punyakanok et al. (2008).
At the end of this step, each trigger has a set of
candidate arguments which will be labeled during
joint inference. In further discussion, the argument
candidates for trigger t are denoted by At.
</bodyText>
<subsectionHeader confidence="0.99994">
4.3 Predicting Arguments and Relations
</subsectionHeader>
<bodyText confidence="0.999942652173913">
Given the output of the trigger classifier, our goal
is to jointly predict event-argument and event-
event relations. We model this as an integer linear
program (ILP) instance described below. We first
describe the inference setup assuming a model that
scores inference decisions and defer description of
learning to Section 4.4. The ILP has two types of
decision variables: arguments and relations.
Argument variables These variables capture
the decision that a candidate argument a, belong-
ing to the set At of argument candidates, takes a
label A (from Section 3). We denote the Boolean
variables by yt,a,A, which are assigned a score
bt,a,A by the model. We include an additional label
NULL-ARG, indicating that the candidate is not an
argument for the trigger.
Event-event relation variables These variables
capture the decision that a pair of triggers t1 and
t2 are connected by a directed edge (t1, t2) labeled
by the relation R. We denote these variables by
zt1,t2,R, which are associated with a score ct1,t2,R.
Again, we introduce a label NULL-REL to indicate
triggers that are not connected by an edge.
</bodyText>
<subsectionHeader confidence="0.988453">
Name Description
</subsectionHeader>
<bodyText confidence="0.985173285714286">
Every argument candidate and trigger pair has ex-
actly one label.
Two arguments of the same trigger cannot overlap.
The SAME relation is symmetric. All other rela-
tions are anti-symmetric, i.e., for any relation la-
bel other than SAME, at most one of (t;, tj) or
(tj, t;) can take that label and the other is assigned
the label NULL-REL.
Every trigger can have no more than two arguments
with the same label.
The same span of text can not be an argument for
more than two triggers.
The triggers must form a connected graph, framed
as flow constraints as in Magnanti and Wolsey
(1995) and Martins et al. (2009).
If the same span of text is an argument of two trig-
gers, then the triggers must be connected by a rela-
tion that is not NULL-REL. This ensures that trig-
gers that share arguments are related.
For any trigger, at most one outgoing edge can be
labeled SUPER.
</bodyText>
<tableCaption confidence="0.913665">
Table 2: Constraints for joint inference.
</tableCaption>
<bodyText confidence="0.999524230769231">
Formulation Given the two sets of variables,
the objective of inference is to find a global as-
signment that maximizes the score. That is, the
objective can be stated as follows:
Here, y and z refer to all the argument and rela-
tion variables respectively.
Clearly, all possible assignments to the infer-
ence variables are not feasible and there are both
structural as well as prior knowledge constraints
over the output space. Table 2 states the con-
straints we include, which are expressed as linear
inequalities over output variables using standard
techniques (e.g., (Roth and Yih, 2004)).
</bodyText>
<subsectionHeader confidence="0.986553">
4.4 Learning in the Joint Model
</subsectionHeader>
<bodyText confidence="0.999978875">
We train both the trigger classifier and the argu-
ment identifier using L2-regularized logistic re-
gression. For the joint model, we use a linear
model for the scoring functions, and train jointly
using the structured averaged perceptron algo-
rithm (Collins, 2002).
Since argument labeling is similar to semantic
role labeling (SRL), we extract standard SRL fea-
tures given the trigger and argument from the syn-
tactic tree for the corresponding sentence. In ad-
dition, we add features extracted from an off-the-
shelf SRL system. We also include all feature con-
junctions. For event relations, we include the fea-
tures described in Scaria et al. (2013), as well as
context features for both triggers, and the depen-
dency path between them, if one exists.
</bodyText>
<figure confidence="0.99705875">
Unique labels
Argument overlap
Relation symmetry
Max arguments per
trigger
Max triggers per ar-
gument
Connectivity
Shared arguments
Unique parent
�max �bt,a,A &apos; yt,a,A + ct1,t2,R &apos; zt1,t2,R
y,z t,a∈At,A t1,t2,R
</figure>
<page confidence="0.818331">
1503
</page>
<sectionHeader confidence="0.844739" genericHeader="method">
5 Question Answering via Structures
</sectionHeader>
<bodyText confidence="0.999977727272727">
This section describes our question answering sys-
tem that, given a process structure, a question and
two answers, chooses the correct answer (steps 2
and 3 in Figure 1).
Our strategy is to treat the process structure as
a small knowledge-base. We map each answer
along with the question into a structured query that
we compare against the structure. The query can
prove either the correctness or incorrectness of the
answer being considered. That is, either we get a
valid match for an answer (proving that the cor-
responding answer is correct), or we get a refu-
tation in the form of a contradicted causal chain
(thus proving that the other answer is correct).
This is similar to theorem proving approaches sug-
gested in the past for factoid question answering
(Moldovan et al., 2003).
The rest of this section is divided into three
parts: Section 5.1 defines the queries we use, Sec-
tion 5.2 describes a rule-based algorithm for con-
verting a question and an answer into a query and
finally, 5.3 describes the overall algorithm.
</bodyText>
<subsectionHeader confidence="0.998522">
5.1 Queries over Processes
</subsectionHeader>
<bodyText confidence="0.999819333333333">
We model a query as a directed graph path with
regular expressions over edge labels. The bot-
tom right portion of Figure 1 shows examples of
queries for our running example. In general, given
a question and one of the answer candidates, one
end of the path is populated by a trigger/argument
found in the question and the other is populated
with a trigger/ argument from the answer.
We define a query to consist of three parts:
</bodyText>
<listItem confidence="0.999691">
1. A regular expression over relation labels, de-
scribing permissible paths,
2. A source trigger/argument node, and
3. A target trigger/argument node.
</listItem>
<bodyText confidence="0.99982575">
For example, the bottom query in Figure 1 looks
for paths labeled with CAUSE or ENABLE edges
from the event split to the event transfer.
Note that the representation of questions as di-
rected paths is a modeling choice and did not influ-
ence the authoring of the questions. Indeed, while
most questions do fit this model, there are rare
cases that require a more complex query structure.
</bodyText>
<subsectionHeader confidence="0.998075">
5.2 Query Generation
</subsectionHeader>
<bodyText confidence="0.999863924528302">
Mapping a question and an answer into a query
involves identifying the components of the query
listed above. We do this in two phases: (1) In the
alignment phase, we align triggers and arguments
in the question and answer to the process structure
to give us candidate source and target nodes. (2)
In the query construction phase, we identify the
regular expression and the direction of the query
using the question, the answer and the alignment.
We identify three broad categories of QA pairs
(see Table 3) that can be identified using simple
lexical rules: (a) Dependency questions ask which
event or argument depends on another event or ar-
gument, (b) Temporal questions ask about tempo-
ral ordering of events, and (c) True-false questions
ask whether some fact is true. Below, we describe
the two phases of query generation primarily in the
context of dependency questions with a brief dis-
cussion about temporal and true-false questions at
the end of the section.
Alignment Phase We align triggers in the struc-
ture to the question and the answer by matching
lemmas or nominalizations. In case of multiple
matches, we use the context to disambiguate and
resolve ties using the highest matching candidate
in the syntactic dependency tree.
We align arguments in the question and the an-
swer in a similar manner. Since arguments are
typically several words long, we prefer maximal
spans. Additionally, if a question (or an answer)
contains an aligned trigger, we prefer to align
words to its arguments.
Query Construction Phase We construct a
query using the aligned question and answer trig-
gers/arguments. We will explain query construc-
tion using our running example (reproduced as the
dependency question in Table 3).
First, we identify the source and the target of
the query. We select either the source or the tar-
get to be a question node and populate the other
end of the query path with an answer node. To
make the choice between source or target for the
question node, we use the main verb in the ques-
tion, its voice and relative position of the question
word with respect to the main verb. In our exam-
ple, the main verb lead to is in active voice and the
question word what is not in subject position. This
places the trigger from the question as the source
of the query path (see both queries in the bottom
right portion of the running example). In contrast,
had the verb been require, the trigger would be the
target of the query. We construct two verb clusters
that indicate query direction using a small seed set
</bodyText>
<page confidence="0.992888">
1504
</page>
<figure confidence="0.997657833333333">
Example
Q: What can the splitting of water lead to?
a: Light absorption
b: Transfer of ions
Q: What is the correct order of events?
a: PDGF binds to tyrosine kinases, then cells divide, then wound healing
b: Cells divide, then PDGF binds to tyrosine kinases, then wound healing
Q: Cdk associates with MPF to become cyclin
a: True
b: False
Type
Dependency
Temporal
True-False
# (%)
407 (69.57%)
57 (9.74%)
121 (20.68%)
</figure>
<tableCaption confidence="0.899358">
Table 3: Examples and statistics for each of the three coarse types of questions.
</tableCaption>
<figure confidence="0.989410384615384">
Is main verb trigger?
Yes No
Condition
Regular Exp.
(ENABLEISUPER)+
(ENABLEISUPER)
(ENABLE|SUPER)&apos;PREVENT(ENABLE|SUPER)&apos;
default
DIRECT
PREVENT
Condition Regular Exp.
Wh- word subjective? AGENT
Wh- word object? THEME
</figure>
<figureCaption confidence="0.998897166666667">
Figure 3: Rules for determining the regular expressions for queries concerning two triggers. In each table, the condition
column decides the regular expression to be chosen. In the left table, we make the choice based on the path from the root to
the Wh- word in the question. In the right table, if the word directly modifies the main trigger, the DIRECT regular expression
is chosen. If the main verb in the question is in the synset of prevent, inhibit, stop or prohibit, we select the PREVENT regular
expression. Otherwise, the default one is chosen. We omit the relation label SAME from the expressions, but allow going
through any number of edges labeled by SAME when matching expressions to the structure.
</figureCaption>
<bodyText confidence="0.999002965517241">
that we expand using WordNet.
The final step in constructing the query is to
identify the regular expression for the path con-
necting the source and the target. Due to paucity
of data, we do not map a question and an answer
to arbitrary regular expressions. Instead, we con-
struct a small set of regular expressions, and build
a rule-based system that selects one. We used the
training set to construct the regular expressions
and we found that they answer most questions (see
Section 6.4). We determine the regular expression
based on whether the main verb in the sentence is
a trigger and whether the source and target of the
path are triggers or arguments. Figure 3 shows the
possible regular expressions and the procedure for
choosing one when both the source and target are
triggers. If either of them are argument nodes, we
append the appropriate semantic role to the regu-
lar expression, based on whether the argument is
the source or the target of the path (or both).
True-false questions are treated similarly, ex-
cept that both source and target are chosen from
the question. For temporal questions, we seek to
identify the ordering of events in the answers. We
use the keywords first, then, or simultaneously to
identify the implied order in the answer. We use
the regular expression SUPER+ for questions ask-
ing about simultaneous events and ENABLE+ for
those asking about sequential events.
</bodyText>
<subsectionHeader confidence="0.999485">
5.3 Answering Questions
</subsectionHeader>
<bodyText confidence="0.9999864">
We match the query of an answer to the process
structure to identify the answer. In case of a match,
the corresponding answer is chosen. The matching
path can be thought of as a proof for the answer.
If neither query matches the graph (or both do),
we check if either answer contradicts the struc-
ture. To do so, we find an undirected path from
the source to the target. In the event of a match, if
the matching path traverses any ENABLE edge in
the incorrect direction, we treat this as a refutation
for the corresponding answer and select the other
one. In our running example, in addition to the
valid path for the second query, for the first query
we see that there is an undirected path from split
to absorb through transfer that matches the first
query. This tells us that light absorption cannot
be the answer because it is not along a causal path
from split.
Finally, if none of the queries results in a match,
we look for any unlabeled path between the source
and the target, before backing off to a dependency-
based proximity baseline described in Section 6.
When there are multiple aligning nodes in the
question and answer, we look for any proof or
refutation before backing off to the baselines.
</bodyText>
<page confidence="0.993422">
1505
</page>
<sectionHeader confidence="0.995495" genericHeader="method">
6 Empirical Evaluation
</sectionHeader>
<bodyText confidence="0.9999775">
In this section we aim to empirically evaluate
whether we can improve reading comprehension
accuracy by predicting process structures. We first
provide details of the experimental setup.
</bodyText>
<subsectionHeader confidence="0.99187">
6.1 Experimental setup
</subsectionHeader>
<bodyText confidence="0.993658384615385">
We used 150 processes (435 questions) for train-
ing and 50 processes (150 questions) as the test
set. For development, we randomly split the train-
ing set 10 times (80%/20%), and tuned hyper-
parameters by maximizing average accuracy on
question answering. We preprocessed the para-
graphs with the Stanford CoreNLP pipeline ver-
sion 3.4 (Manning et al., 2014) and Illinois SRL
(Punyakanok et al., 2008; Clarke et al., 2012). We
used the Gurobi optimization package3 for infer-
ence.
We compare our system PROREAD to baselines
that do not have access to the process structure:
</bodyText>
<listItem confidence="0.985405708333333">
1. BOW: For each answer, we compute the
proportion of content word lemmas covered
by the paragraph and choose the one with
higher coverage. For true-false questions, we
compute the coverage of the question state-
ment, and answer “True” if it is higher than a
threshold tuned on the development set.
2. TEXTPROX: For dependency questions, we
align content word lemmas in both the ques-
tion and answer against the text and select the
answer whose aligned tokens are closer to the
aligned tokens of the question. For tempo-
ral questions, we return the answer for which
the order of events is identical to their order
in the paragraph. For true-false questions, we
return “True” if the number of bigrams from
the question covered in the text is higher than
a threshold tuned on the development set.
3. SYNTPROX: For dependency questions, we
use proximity as in TEXTPROX, except that
distance is measured using dependency tree
edges. To support multiple sentences we con-
nect roots of adjacent sentences with bidi-
rectional edges. For temporal questions this
</listItem>
<bodyText confidence="0.68827">
baseline is identical to TEXTPROX. For true-
false questions, we compute the number of
dependency tree edges in the question state-
ment covered by edges in the paragraph (an
edge has a source lemma, relation, and target
lemma), and answer “True” if the coverage is
</bodyText>
<footnote confidence="0.941844">
3http://www.gurobi.com/
</footnote>
<table confidence="0.985649571428571">
Method Depen. Temp. True- All
false
PROREAD 68.1 80.0 55.6 66.7
SYNTPROX 61.9 70.0 48.1 60.0
TEXTPROX 58.4 70.0 33.3 54.7
BOW 47.8 40.0 44.4 46.7
GOLD 77.9 80.0 70.4 76.7
</table>
<tableCaption confidence="0.977751333333333">
Table 4: Reading comprehension test set accuracy. The All
column shows overall accuracy across all questions. The first
three columns show accuracy for each coarse type.
</tableCaption>
<bodyText confidence="0.9931685">
higher than a threshold tuned on the training
set.
To separate the contribution of process struc-
tures from the performance of our structure pre-
dictor, we also run our QA system given manually
annotated gold standard structures (GOLD).4
</bodyText>
<subsectionHeader confidence="0.999544">
6.2 Reading Comprehension Task
</subsectionHeader>
<bodyText confidence="0.999939966666667">
We evaluate our system using accuracy, i.e., the
proportion of questions answered correctly. Ta-
ble 4 presents test set results, where we break
down questions by their coarse-type.
PROREAD improves accuracy compared to the
best baseline by 6.7 absolute points (last column).
Most of the gain is due to improvement on de-
pendency questions, which are the most common
question type. The performance of BOW indicates
that lexical coverage alone does not distinguish the
correct answer from the wrong answer. In fact,
guessing the answer with higher lexical overlap
results in performance that is slightly lower than
random. Text proximity and syntactic proximity
provide a stronger cue, but exploiting predicted
process structures substantially outperforms these
baselines.
Examining results using gold information high-
lights the importance of process structures inde-
pendently of the structure predictor. Results of
GOLD demonstrate that given gold structures we
can obtain a dramatic improvement of almost 17
points compared to the baselines, using our sim-
ple deterministic QA system.
Results on true-false questions are low for
PROREAD and all the baselines. True-false ques-
tions are harder for two main reasons. First, in
dependency and temporal questions, we create a
query for both answers, and can find a proof or
a refutation for either one of them. In true-false
</bodyText>
<footnote confidence="0.996999">
4We also ran an experiment where gold triggers are
given and arguments and relations are predicted. We found
that this results in slightly higher performance compared to
PROREAD.
</footnote>
<page confidence="0.942423">
1506
</page>
<table confidence="0.9999355">
Precision Recall Fl
Triggers 75.4 73.9 74.6
Arguments 43.4 34.4 38.3
Relations 27.0 22.5 24.6
</table>
<tableCaption confidence="0.98651">
Table 5: Structured prediction test set results.
</tableCaption>
<figure confidence="0.867613666666667">
Reason
Alignment
Missing from annotation
Entity coreference
Missing regular expression
Lexical variability
Error in predicted structure
GOLD
PROREAD
35%
25%
20%
10%
5%
5%
15%
10%
10%
10%
55%
Other
</figure>
<bodyText confidence="0.9639432">
questions we must determine given a single state-
ment whether it holds. Second, an analysis of true-
false questions reveals that they focus less on re-
lations between events and entities in the process,
and require modeling lexical variability.5
</bodyText>
<subsectionHeader confidence="0.999256">
6.3 Structure Prediction Task
</subsectionHeader>
<bodyText confidence="0.999964088235294">
Our evaluation demonstrates that gold structures
improve accuracy substantially more than pre-
dicted structures. To examine this, we now di-
rectly evaluate the structure predictor by com-
paring micro-average precision, recall and Fi be-
tween predicted and gold structures (Table 5).
While performance for trigger identification is
reasonable, performance on argument and relation
prediction is low. This explains the higher perfor-
mance obtained in reading comprehension given
gold structures. Note that errors in trigger predic-
tion propagate to argument and relation prediction
– a relation cannot be predicted correctly if either
one of the related triggers is not previously identi-
fied. One reason for low performance is the small
size of the dataset. Thus, training process predic-
tors with less supervision is an important direction
for future work. Furthermore, the task of process
prediction is inherently difficult, because often re-
lations are expressed only indirectly in text. For
example, in Figure 1 the relation between water
splitting and transfer of ions is only recoverable
by understanding that water provides the ions that
need to be transferred.
Nevertheless, we find that questions can often
be answered correctly even if the structure con-
tains some errors. For example, the gold structure
for the sentence “Some ... radioisotopes have
long half-lives, allowing ... ”, contains the trigger
long half-lives, while we predict have as a trigger
and long half-lives as an argument. This is good
enough to answer questions related to this part of
the structure correctly, and overall, to improve per-
formance using predicted structures.
</bodyText>
<footnote confidence="0.986723">
5The low performance of TEXTPROX and SYNTPROX on
true-false questions can also be attributed to the fact that we
tuned a threshold parameter on the training set, and this did
not generalize well to the test set.
</footnote>
<tableCaption confidence="0.9973995">
Table 6: Error analysis results. An explanation of the vari-
ous categories are in the body of the paper.
</tableCaption>
<subsectionHeader confidence="0.864905">
6.4 Error Analysis
</subsectionHeader>
<bodyText confidence="0.9999124">
This section presents the results of an analysis of
20 sampled errors of GOLD (gold structures), and
20 errors of PROREAD (predicted structures). We
have categorized the primary reason for error in
Table 6.
As expected, the main problem when using pre-
dicted structures, is structure errors which account
for more than half of the errors.
Errors in GOLD are distributed across various
categories, which we briefly describe. Alignment
errors occur due to multiple words aligning to mul-
tiple triggers and arguments. For example, in the
question “What is the result of gases being pro-
duced in the lysosome?”, the answer “engulfed
pathogens are poisoned” is incorrectly aligned to
the trigger engulfed rather than to poisoned.
Another reason for errors are cases where ques-
tions are asked about parts of the paragraph that
are missing from annotation. This is possible since
questions were authored independently of struc-
ture annotation. Two other causes for errors are
entity coreference errors, where a referent for an
entity is missing from the structure, and lexical
variability, where the author of questions uses
names for triggers or arguments that are missing
from the paragraph, and so alignment fails.
Last, in 10% of the cases in GOLD we found
that the answer could not be retrieved using the set
of regular expressions that are currently used by
our QA system.
</bodyText>
<sectionHeader confidence="0.99966" genericHeader="discussions">
7 Discussion
</sectionHeader>
<bodyText confidence="0.997113875">
This work touches on several strands of work in
NLP including information extraction, semantic
role labeling, semantic parsing and reading com-
prehension.
Event and relation extraction have been studied
via the ACE data (Doddington et al., 2004) and
related work. The BioNLP shared tasks (Kim et
al., 2009; Kim et al., 2011; Riedel and McCal-
</bodyText>
<page confidence="0.98042">
1507
</page>
<bodyText confidence="0.999842797101449">
lum, 2011) focused on biomedical data to extract
events and their arguments. Event-event relations
have been mostly studied from the perspective of
temporal ordering; e.g., (Chambers and Jurafsky,
2008; Yoshikawa et al., 2009; Do et al., 2012; Mc-
Closky and Manning, 2012). The process struc-
ture predicted in this work differs from these lines
of work in two important ways: First, we predict
events, arguments and their interactions from mul-
tiple sentences, while most earlier work focused
on one or two of these components. Second, we
model processes, and thus target causal relations
between events, rather than temporal order only.
Our semantic role annotation is similar to ex-
isting SRL schemes such as PropBank (Palmer et
al., 2005), FrameNet (Ruppenhofer et al., 2006)
and BioProp (Chou et al., 2006). However, in con-
trast to PropBank and FrameNet, we do not allow
all verbs to trigger events and instead let the an-
notators decide on biologically important triggers,
which are not restricted to verbs (unlike BioProp,
where 30 pre-specified verbs were selected for an-
notation). Like PropBank and BioProp, the argu-
ment labels are not trigger specific.
Mapping questions to queries is effectively a se-
mantic parsing task. In recent years, several lines
of work addressed semantic parsing using vari-
ous formalisms and levels of supervision (Zettle-
moyer and Collins, 2005; Wong and Mooney,
2006; Clarke et al., 2010; Berant et al., 2013).
In particular, Krishnamurthy and Kollar (2013)
learned to map natural language utterances to ref-
erents in an image by constructing a KB from the
image and then mapping the utterance to a query
over the KB. This is analogous to our process of
constructing a process structure and performing
QA by querying that structure. In our work, we
parse questions into graph-based queries, suitable
for modeling processes, using a rule-based heuris-
tic. Training a statistical semantic parser that will
replace the QA system is an interesting direction
for future research.
Multiple choice reading comprehension tests
are a natural choice for evaluating machine read-
ing. Hirschman et al. (1999) presented a bag-of-
words approach to retrieving sentences for read-
ing comprehension. Richardson et al. (2013) re-
cently released the MCTest reading comprehen-
sion dataset that examines understanding of fic-
tional stories. Their work shares our goal of ad-
vancing micro-reading, but they do not focus on
process understanding.
Developing programs that perform deep reason-
ing over complex descriptions of processes is an
important step on the road to fulfilling the higher
goals of machine reading. In this paper, we present
an end-to-end system for reading comprehen-
sion of paragraphs which describe biological pro-
cesses. This is, to the best of our knowledge, the
first system to both predict a rich structured rep-
resentation that includes entities, events and their
relations, and utilize this structure for answering
reading comprehension questions. We also created
a new dataset, PROCESSBANK, which contains
200 paragraphs that are both fully-annotated with
process structure, as well as accompanied by ques-
tions. We empirically demonstrated that model-
ing biological processes can substantially improve
reading comprehension accuracy in this domain.
</bodyText>
<sectionHeader confidence="0.998561" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999944">
The authors would like to thank Luke
Amuchastegui for authoring the multiple-choice
questions, and also the anonymous reviewers for
their constructive feedback. We thank the Allen
Institute for Artificial Intelligence for assistance
in funding this work.
</bodyText>
<sectionHeader confidence="0.998573" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999880695652174">
Jonathan Berant, Andrew Chou, Roy Frostig, and Percy
Liang. 2013. Semantic parsing on Freebase from
question-answer pairs. In Proceedings of EMNLP.
Neil Campbell and Jane Reece. 2005. Biology. Ben-
jamin Cummings.
Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr
Settles, Estevam R. Hruschka Jr., and Tom M.
Mitchell. 2010. Toward an architecture for never-
ending language learning. In Proceedings of AAAI.
Nathanael Chambers and Daniel Jurafsky. 2008.
Jointly combining implicit constraints improves
temporal ordering. In Proceedings of EMNLP.
Wen-Chi Chou, Richard Tzong-Han Tsai, Ying-Shan
Su, Wei Ku, Ting-Yi Sung, and Wen-Lian Hsu.
2006. A semi-automatic method for annotating a
biomedical proposition bank. In Proceedings of the
Workshop on Frontiers in Linguistically Annotated
Corpora 2006, July.
James Clarke, Dan Goldwasser, Ming-Wei Chang, and
Dan Roth. 2010. Driving semantic parsing from the
world’s response. In Proceedings of CoNLL.
James Clarke, Vivek Srikumar, Mark Sammons, and
Dan Roth. 2012. An NLP Curator (or: How I
</reference>
<page confidence="0.822654">
1508
</page>
<reference confidence="0.999528201923077">
Learned to Stop Worrying and Love NLP Pipelines).
In Proceedings of LREC.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of ACL.
Quang Do, Wei Lu, and Dan Roth. 2012. Joint infer-
ence for event timeline construction. In Proceedings
of EMNLP-CoNLL.
George R. Doddington, Alexis Mitchell, Mark A. Przy-
bocki, Lance A. Ramshaw, Stephanie Strassel, and
Ralph M. Weischedel. 2004. The Automatic Con-
tent Extraction (ACE) Program-Tasks, Data, and
Evaluation. In Proceedings of LREC.
Oren Etzioni, Michele Banko, and Michael J. Cafarella.
2006. Machine reading. In Proceedings of AAAI.
Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying relations for open information ex-
traction. In Proceedings of EMNLP.
Anthony Fader, Luke Zettlemoyer, and Oren Etzioni.
2013. Paraphrase-Driven Learning for Open Ques-
tion Answering. In Proceedings of ACL.
Christiane Fellbaum, editor. 1998. WordNet: An elec-
tronic lexical database. MIT Press.
Lynette Hirschman, Marc Light, Eric Breck, and
John D. Burger. 1999. Deep read: A reading com-
prehension system. In Proceedings of ACL.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Junichi Tsujii. 2009. Overview of
BioNLP 09 shared task on event extraction. In Pro-
ceedings of BioNLP.
Jin-Dong Kim, Sampo Pyysalo, Tomoko Ohta, Robert
Bossy, and Junichi Tsujii. 2011. Overview of
BioNLP shared task 2011. In Proceedings of
BioNLP.
Jayant Krishnamurthy and Thomas Kollar. 2013.
Jointly learning to parse and perceive: Connect-
ing natural language to the physical world. TACL,
1:193–206.
Nate Kushman, Yoav Artzi, Luke Zettlemoyer, and
Regina Barzilay. 2014. Learning to automatically
solve algebra word problems. In Proceedings of
ACL.
Beth Levin. 1993. English verb classes and alter-
nations: A preliminary investigation. University of
Chicago Press.
Catherine Macleod, Ralph Grishman, Adam Meyers,
Leslie Barrett, and Ruth Reeves. 1998. Nomlex: A
lexicon of nominalizations. In Proceedings of EU-
RALEX.
Thomas L. Magnanti and Laurence A. Wolsey. 1995.
Optimal trees. Handbooks in operations research
and management science, 7:503–615.
Christopher D. Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven J. Bethard, and David Mc-
Closky. 2014. The Stanford CoreNLP natural lan-
guage processing toolkit. In Proceedings of ACL:
System Demonstrations.
Andr´e L. Martins, Noah A. Smith, and Eric P. Xing.
2009. Concise integer linear programming formu-
lations for dependency parsing. In Proceedings of
ACL/IJCNLP.
David McClosky and Christopher D. Manning. 2012.
Learning constraints for consistent timeline extrac-
tion. In Proceedings of EMNLP-CoNLL.
Dan Moldovan, Christine Clark, Sanda Harabagiu, and
Steve Maiorano. 2003. Cogex: A logic prover
for question answering. In Proceedings of NAACL-
HLT.
Martha Palmer, Paul Kingsbury, and Daniel Gildea.
2005. The proposition bank: An annotated cor-
pus of semantic roles. Computational Linguistics,
31(1):71–106.
Vasin Punyakanok, Dan Roth, and Wen-tau Yih. 2008.
The importance of syntactic parsing and inference in
semantic role labeling. Computational Linguistics,
34(2).
Matthew Richardson, Christopher JC Burges, and Erin
Renshaw. 2013. MCTest: A challenge dataset for
the open-domain machine comprehension of text. In
Proceedings of EMNLP.
Sebastian Riedel and Andrew McCallum. 2011. Fast
and robust joint models for biomedical event extrac-
tion. In Proceedings of EMNLP.
Dan Roth and Wen-tau Yih. 2004. A linear program-
ming formulation for global inference in natural lan-
guage tasks. In Proceedings of CoNLL.
Josef Ruppenhofer, Michael Ellsworth, Miriam RL
Petruck, Christopher R. Johnson, and Jan Schef-
fczyk. 2006. FrameNet II: Extended theory and
practice. Berkeley FrameNet Release, 1.
Aju Thalappillil Scaria, Jonathan Berant, Mengqiu
Wang, Peter Clark, Justin Lewis, Brittany Harding,
and Christopher D. Manning. 2013. Learning bi-
ological processes with global constraints. In Pro-
ceedings of EMNLP.
Pontus Stenetorp, Sampo Pyysalo, Goran Topi´c,
Tomoko Ohta, Sophia Ananiadou, and Jun’ichi Tsu-
jii. 2012. Brat: a web-based tool for NLP-assisted
text annotation. In Proceedings of the demonstra-
tions at EACL.
Yuk Wah Wong and Raymond J. Mooney. 2006.
Learning for semantic parsing with statistical ma-
chine translation. In Proceedings of HLT-NAACL.
</reference>
<page confidence="0.890897">
1509
</page>
<reference confidence="0.998134625">
Katsumasa Yoshikawa, Sebastian Riedel, Masayuki
Asahara, and Yuji Matsumoto. 2009. Jointly identi-
fying temporal relations with Markov logic. In Pro-
ceedings of ACL/IJCNLP.
Luke S. Zettlemoyer and Michael Collins. 2005.
Learning to map sentences to logical form: Struc-
tured classification with probabilistic categorial
grammars. In Proceedings of UAI.
</reference>
<page confidence="0.989286">
1510
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.736281">
<title confidence="0.99958">Modeling Biological Processes for Reading Comprehension</title>
<author confidence="0.929404">Vivek Pei-Chun Chen</author>
<author confidence="0.929404">Brad Huang</author>
<author confidence="0.929404">D Christopher</author>
<affiliation confidence="0.969463">Stanford University, Stanford</affiliation>
<author confidence="0.872095">Abby Vander Linden</author>
<author confidence="0.872095">Brittany</author>
<affiliation confidence="0.902284">University of Washington, Seattle</affiliation>
<abstract confidence="0.999370523809524">Machine reading calls for programs that read and understand text, but most current work only attempts to extract facts from redundant web-scale corpora. In this paper, we focus on a new reading comprehension task that requires complex reasoning over a single document. The input is a paragraph describing a biological process, and the goal is to answer questions that require an understanding of the relations between entities and events in the process. To answer the questions, we first predict a rich structure representing the process in the paragraph. Then, we map the question to a formal query, which is executed against the predicted structure. We demonstrate that answering questions via predicted structures substantially improves accuracy over baselines that use shallower representations.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jonathan Berant</author>
<author>Andrew Chou</author>
<author>Roy Frostig</author>
<author>Percy Liang</author>
</authors>
<title>Semantic parsing on Freebase from question-answer pairs.</title>
<date>2013</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="39251" citStr="Berant et al., 2013" startWordPosition="6463" endWordPosition="6466"> contrast to PropBank and FrameNet, we do not allow all verbs to trigger events and instead let the annotators decide on biologically important triggers, which are not restricted to verbs (unlike BioProp, where 30 pre-specified verbs were selected for annotation). Like PropBank and BioProp, the argument labels are not trigger specific. Mapping questions to queries is effectively a semantic parsing task. In recent years, several lines of work addressed semantic parsing using various formalisms and levels of supervision (Zettlemoyer and Collins, 2005; Wong and Mooney, 2006; Clarke et al., 2010; Berant et al., 2013). In particular, Krishnamurthy and Kollar (2013) learned to map natural language utterances to referents in an image by constructing a KB from the image and then mapping the utterance to a query over the KB. This is analogous to our process of constructing a process structure and performing QA by querying that structure. In our work, we parse questions into graph-based queries, suitable for modeling processes, using a rule-based heuristic. Training a statistical semantic parser that will replace the QA system is an interesting direction for future research. Multiple choice reading comprehensio</context>
</contexts>
<marker>Berant, Chou, Frostig, Liang, 2013</marker>
<rawString>Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. 2013. Semantic parsing on Freebase from question-answer pairs. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Neil Campbell</author>
<author>Jane Reece</author>
</authors>
<date>2005</date>
<location>Biology. Benjamin Cummings.</location>
<contexts>
<context position="6611" citStr="Campbell and Reece, 2005" startWordPosition="1046" endWordPosition="1049">ection describes the reading comprehension task we address and the accompanying dataset. We will use the example in Figure 1 as our running example throughout the paper. Our goal is to tackle a complex reading comprehension setting that centers on understanding the underlying meaning of a process description. We target a multiple-choice setting in which each input consists of a paragraph of text describing a biological process, a question, and two possible answers. The goal is to identify the correct answer using the text (Figure 1, left). We used the 148 paragraphs from the textbook Biology (Campbell and Reece, 2005) that were manually identified by Scaria et al. (2013). We extended this set to 200 paragraphs by including additional paragraphs that describe biological processes. Each paragraph in the collection represents a single biological process and describes a set of events, their participants and their interactions. Because we target understanding of paragraph meaning, we use the following desiderata for building the corpus of questions and answers: 1. The questions should focus on the events and entities participating in the process described in the paragraph, and answering the questions should req</context>
</contexts>
<marker>Campbell, Reece, 2005</marker>
<rawString>Neil Campbell and Jane Reece. 2005. Biology. Benjamin Cummings.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Carlson</author>
<author>Justin Betteridge</author>
<author>Bryan Kisiel</author>
<author>Burr Settles</author>
<author>Estevam R Hruschka Jr</author>
<author>Tom M Mitchell</author>
</authors>
<title>Toward an architecture for neverending language learning.</title>
<date>2010</date>
<booktitle>In Proceedings of AAAI.</booktitle>
<contexts>
<context position="1393" citStr="Carlson et al., 2010" startWordPosition="213" endWordPosition="216">resenting the process in the paragraph. Then, we map the question to a formal query, which is executed against the predicted structure. We demonstrate that answering questions via predicted structures substantially improves accuracy over baselines that use shallower representations. 1 Introduction The goal of machine reading is to develop programs that read text to learn about the world and make decisions based on accumulated knowledge. Work in this field has focused mostly on macro-reading, i.e., processing large text collections and extracting knowledge bases of facts (Etzioni et al., 2006; Carlson et al., 2010; Fader et al., 2011). Such methods rely on redundancy, and are thus suitable for answering common factoid questions which have ample evidence in text (Fader et al., 2013). However, reading a single document (micro-reading) to answer comprehension questions that require deep reasoning is currently beyond the scope of state-of-the-art systems. In this paper, we introduce a task where given a paragraph describing a process, the goal is to ∗Both authors equally contributed to the paper. answer reading comprehension questions that test understanding of the underlying structure. In particular, we c</context>
</contexts>
<marker>Carlson, Betteridge, Kisiel, Settles, Jr, Mitchell, 2010</marker>
<rawString>Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr Settles, Estevam R. Hruschka Jr., and Tom M. Mitchell. 2010. Toward an architecture for neverending language learning. In Proceedings of AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nathanael Chambers</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Jointly combining implicit constraints improves temporal ordering.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="38013" citStr="Chambers and Jurafsky, 2008" startWordPosition="6260" endWordPosition="6263">et of regular expressions that are currently used by our QA system. 7 Discussion This work touches on several strands of work in NLP including information extraction, semantic role labeling, semantic parsing and reading comprehension. Event and relation extraction have been studied via the ACE data (Doddington et al., 2004) and related work. The BioNLP shared tasks (Kim et al., 2009; Kim et al., 2011; Riedel and McCal1507 lum, 2011) focused on biomedical data to extract events and their arguments. Event-event relations have been mostly studied from the perspective of temporal ordering; e.g., (Chambers and Jurafsky, 2008; Yoshikawa et al., 2009; Do et al., 2012; McClosky and Manning, 2012). The process structure predicted in this work differs from these lines of work in two important ways: First, we predict events, arguments and their interactions from multiple sentences, while most earlier work focused on one or two of these components. Second, we model processes, and thus target causal relations between events, rather than temporal order only. Our semantic role annotation is similar to existing SRL schemes such as PropBank (Palmer et al., 2005), FrameNet (Ruppenhofer et al., 2006) and BioProp (Chou et al., </context>
</contexts>
<marker>Chambers, Jurafsky, 2008</marker>
<rawString>Nathanael Chambers and Daniel Jurafsky. 2008. Jointly combining implicit constraints improves temporal ordering. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wen-Chi Chou</author>
<author>Richard Tzong-Han Tsai</author>
<author>Ying-Shan Su</author>
<author>Wei Ku</author>
<author>Ting-Yi Sung</author>
<author>Wen-Lian Hsu</author>
</authors>
<title>A semi-automatic method for annotating a biomedical proposition bank.</title>
<date>2006</date>
<booktitle>In Proceedings of the Workshop on Frontiers in Linguistically Annotated Corpora</booktitle>
<contexts>
<context position="38618" citStr="Chou et al., 2006" startWordPosition="6360" endWordPosition="6363">urafsky, 2008; Yoshikawa et al., 2009; Do et al., 2012; McClosky and Manning, 2012). The process structure predicted in this work differs from these lines of work in two important ways: First, we predict events, arguments and their interactions from multiple sentences, while most earlier work focused on one or two of these components. Second, we model processes, and thus target causal relations between events, rather than temporal order only. Our semantic role annotation is similar to existing SRL schemes such as PropBank (Palmer et al., 2005), FrameNet (Ruppenhofer et al., 2006) and BioProp (Chou et al., 2006). However, in contrast to PropBank and FrameNet, we do not allow all verbs to trigger events and instead let the annotators decide on biologically important triggers, which are not restricted to verbs (unlike BioProp, where 30 pre-specified verbs were selected for annotation). Like PropBank and BioProp, the argument labels are not trigger specific. Mapping questions to queries is effectively a semantic parsing task. In recent years, several lines of work addressed semantic parsing using various formalisms and levels of supervision (Zettlemoyer and Collins, 2005; Wong and Mooney, 2006; Clarke e</context>
</contexts>
<marker>Chou, Tsai, Su, Ku, Sung, Hsu, 2006</marker>
<rawString>Wen-Chi Chou, Richard Tzong-Han Tsai, Ying-Shan Su, Wei Ku, Ting-Yi Sung, and Wen-Lian Hsu. 2006. A semi-automatic method for annotating a biomedical proposition bank. In Proceedings of the Workshop on Frontiers in Linguistically Annotated Corpora 2006, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Clarke</author>
<author>Dan Goldwasser</author>
<author>Ming-Wei Chang</author>
<author>Dan Roth</author>
</authors>
<title>Driving semantic parsing from the world’s response.</title>
<date>2010</date>
<booktitle>In Proceedings of CoNLL.</booktitle>
<contexts>
<context position="39229" citStr="Clarke et al., 2010" startWordPosition="6459" endWordPosition="6462">., 2006). However, in contrast to PropBank and FrameNet, we do not allow all verbs to trigger events and instead let the annotators decide on biologically important triggers, which are not restricted to verbs (unlike BioProp, where 30 pre-specified verbs were selected for annotation). Like PropBank and BioProp, the argument labels are not trigger specific. Mapping questions to queries is effectively a semantic parsing task. In recent years, several lines of work addressed semantic parsing using various formalisms and levels of supervision (Zettlemoyer and Collins, 2005; Wong and Mooney, 2006; Clarke et al., 2010; Berant et al., 2013). In particular, Krishnamurthy and Kollar (2013) learned to map natural language utterances to referents in an image by constructing a KB from the image and then mapping the utterance to a query over the KB. This is analogous to our process of constructing a process structure and performing QA by querying that structure. In our work, we parse questions into graph-based queries, suitable for modeling processes, using a rule-based heuristic. Training a statistical semantic parser that will replace the QA system is an interesting direction for future research. Multiple choic</context>
</contexts>
<marker>Clarke, Goldwasser, Chang, Roth, 2010</marker>
<rawString>James Clarke, Dan Goldwasser, Ming-Wei Chang, and Dan Roth. 2010. Driving semantic parsing from the world’s response. In Proceedings of CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Clarke</author>
<author>Vivek Srikumar</author>
<author>Mark Sammons</author>
<author>Dan Roth</author>
</authors>
<title>An NLP Curator (or: How I Learned to Stop Worrying and Love NLP Pipelines).</title>
<date>2012</date>
<booktitle>In Proceedings of LREC.</booktitle>
<contexts>
<context position="29849" citStr="Clarke et al., 2012" startWordPosition="4955" endWordPosition="4958"> section we aim to empirically evaluate whether we can improve reading comprehension accuracy by predicting process structures. We first provide details of the experimental setup. 6.1 Experimental setup We used 150 processes (435 questions) for training and 50 processes (150 questions) as the test set. For development, we randomly split the training set 10 times (80%/20%), and tuned hyperparameters by maximizing average accuracy on question answering. We preprocessed the paragraphs with the Stanford CoreNLP pipeline version 3.4 (Manning et al., 2014) and Illinois SRL (Punyakanok et al., 2008; Clarke et al., 2012). We used the Gurobi optimization package3 for inference. We compare our system PROREAD to baselines that do not have access to the process structure: 1. BOW: For each answer, we compute the proportion of content word lemmas covered by the paragraph and choose the one with higher coverage. For true-false questions, we compute the coverage of the question statement, and answer “True” if it is higher than a threshold tuned on the development set. 2. TEXTPROX: For dependency questions, we align content word lemmas in both the question and answer against the text and select the answer whose aligne</context>
</contexts>
<marker>Clarke, Srikumar, Sammons, Roth, 2012</marker>
<rawString>James Clarke, Vivek Srikumar, Mark Sammons, and Dan Roth. 2012. An NLP Curator (or: How I Learned to Stop Worrying and Love NLP Pipelines). In Proceedings of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="19836" citStr="Collins, 2002" startWordPosition="3254" endWordPosition="3255">possible assignments to the inference variables are not feasible and there are both structural as well as prior knowledge constraints over the output space. Table 2 states the constraints we include, which are expressed as linear inequalities over output variables using standard techniques (e.g., (Roth and Yih, 2004)). 4.4 Learning in the Joint Model We train both the trigger classifier and the argument identifier using L2-regularized logistic regression. For the joint model, we use a linear model for the scoring functions, and train jointly using the structured averaged perceptron algorithm (Collins, 2002). Since argument labeling is similar to semantic role labeling (SRL), we extract standard SRL features given the trigger and argument from the syntactic tree for the corresponding sentence. In addition, we add features extracted from an off-theshelf SRL system. We also include all feature conjunctions. For event relations, we include the features described in Scaria et al. (2013), as well as context features for both triggers, and the dependency path between them, if one exists. Unique labels Argument overlap Relation symmetry Max arguments per trigger Max triggers per argument Connectivity Sh</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Quang Do</author>
<author>Wei Lu</author>
<author>Dan Roth</author>
</authors>
<title>Joint inference for event timeline construction.</title>
<date>2012</date>
<booktitle>In Proceedings of EMNLP-CoNLL.</booktitle>
<contexts>
<context position="38054" citStr="Do et al., 2012" startWordPosition="6268" endWordPosition="6271">our QA system. 7 Discussion This work touches on several strands of work in NLP including information extraction, semantic role labeling, semantic parsing and reading comprehension. Event and relation extraction have been studied via the ACE data (Doddington et al., 2004) and related work. The BioNLP shared tasks (Kim et al., 2009; Kim et al., 2011; Riedel and McCal1507 lum, 2011) focused on biomedical data to extract events and their arguments. Event-event relations have been mostly studied from the perspective of temporal ordering; e.g., (Chambers and Jurafsky, 2008; Yoshikawa et al., 2009; Do et al., 2012; McClosky and Manning, 2012). The process structure predicted in this work differs from these lines of work in two important ways: First, we predict events, arguments and their interactions from multiple sentences, while most earlier work focused on one or two of these components. Second, we model processes, and thus target causal relations between events, rather than temporal order only. Our semantic role annotation is similar to existing SRL schemes such as PropBank (Palmer et al., 2005), FrameNet (Ruppenhofer et al., 2006) and BioProp (Chou et al., 2006). However, in contrast to PropBank a</context>
</contexts>
<marker>Do, Lu, Roth, 2012</marker>
<rawString>Quang Do, Wei Lu, and Dan Roth. 2012. Joint inference for event timeline construction. In Proceedings of EMNLP-CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George R Doddington</author>
<author>Alexis Mitchell</author>
<author>Mark A Przybocki</author>
<author>Lance A Ramshaw</author>
<author>Stephanie Strassel</author>
<author>Ralph M Weischedel</author>
</authors>
<title>The Automatic Content Extraction (ACE) Program-Tasks, Data, and Evaluation.</title>
<date>2004</date>
<booktitle>In Proceedings of LREC.</booktitle>
<contexts>
<context position="37711" citStr="Doddington et al., 2004" startWordPosition="6212" endWordPosition="6215">eferent for an entity is missing from the structure, and lexical variability, where the author of questions uses names for triggers or arguments that are missing from the paragraph, and so alignment fails. Last, in 10% of the cases in GOLD we found that the answer could not be retrieved using the set of regular expressions that are currently used by our QA system. 7 Discussion This work touches on several strands of work in NLP including information extraction, semantic role labeling, semantic parsing and reading comprehension. Event and relation extraction have been studied via the ACE data (Doddington et al., 2004) and related work. The BioNLP shared tasks (Kim et al., 2009; Kim et al., 2011; Riedel and McCal1507 lum, 2011) focused on biomedical data to extract events and their arguments. Event-event relations have been mostly studied from the perspective of temporal ordering; e.g., (Chambers and Jurafsky, 2008; Yoshikawa et al., 2009; Do et al., 2012; McClosky and Manning, 2012). The process structure predicted in this work differs from these lines of work in two important ways: First, we predict events, arguments and their interactions from multiple sentences, while most earlier work focused on one or</context>
</contexts>
<marker>Doddington, Mitchell, Przybocki, Ramshaw, Strassel, Weischedel, 2004</marker>
<rawString>George R. Doddington, Alexis Mitchell, Mark A. Przybocki, Lance A. Ramshaw, Stephanie Strassel, and Ralph M. Weischedel. 2004. The Automatic Content Extraction (ACE) Program-Tasks, Data, and Evaluation. In Proceedings of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oren Etzioni</author>
<author>Michele Banko</author>
<author>Michael J Cafarella</author>
</authors>
<title>Machine reading.</title>
<date>2006</date>
<booktitle>In Proceedings of AAAI.</booktitle>
<contexts>
<context position="1371" citStr="Etzioni et al., 2006" startWordPosition="208" endWordPosition="212">t a rich structure representing the process in the paragraph. Then, we map the question to a formal query, which is executed against the predicted structure. We demonstrate that answering questions via predicted structures substantially improves accuracy over baselines that use shallower representations. 1 Introduction The goal of machine reading is to develop programs that read text to learn about the world and make decisions based on accumulated knowledge. Work in this field has focused mostly on macro-reading, i.e., processing large text collections and extracting knowledge bases of facts (Etzioni et al., 2006; Carlson et al., 2010; Fader et al., 2011). Such methods rely on redundancy, and are thus suitable for answering common factoid questions which have ample evidence in text (Fader et al., 2013). However, reading a single document (micro-reading) to answer comprehension questions that require deep reasoning is currently beyond the scope of state-of-the-art systems. In this paper, we introduce a task where given a paragraph describing a process, the goal is to ∗Both authors equally contributed to the paper. answer reading comprehension questions that test understanding of the underlying structur</context>
</contexts>
<marker>Etzioni, Banko, Cafarella, 2006</marker>
<rawString>Oren Etzioni, Michele Banko, and Michael J. Cafarella. 2006. Machine reading. In Proceedings of AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anthony Fader</author>
<author>Stephen Soderland</author>
<author>Oren Etzioni</author>
</authors>
<title>Identifying relations for open information extraction.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="1414" citStr="Fader et al., 2011" startWordPosition="217" endWordPosition="220">in the paragraph. Then, we map the question to a formal query, which is executed against the predicted structure. We demonstrate that answering questions via predicted structures substantially improves accuracy over baselines that use shallower representations. 1 Introduction The goal of machine reading is to develop programs that read text to learn about the world and make decisions based on accumulated knowledge. Work in this field has focused mostly on macro-reading, i.e., processing large text collections and extracting knowledge bases of facts (Etzioni et al., 2006; Carlson et al., 2010; Fader et al., 2011). Such methods rely on redundancy, and are thus suitable for answering common factoid questions which have ample evidence in text (Fader et al., 2013). However, reading a single document (micro-reading) to answer comprehension questions that require deep reasoning is currently beyond the scope of state-of-the-art systems. In this paper, we introduce a task where given a paragraph describing a process, the goal is to ∗Both authors equally contributed to the paper. answer reading comprehension questions that test understanding of the underlying structure. In particular, we consider processes in </context>
</contexts>
<marker>Fader, Soderland, Etzioni, 2011</marker>
<rawString>Anthony Fader, Stephen Soderland, and Oren Etzioni. 2011. Identifying relations for open information extraction. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anthony Fader</author>
<author>Luke Zettlemoyer</author>
<author>Oren Etzioni</author>
</authors>
<title>Paraphrase-Driven Learning for Open Question Answering.</title>
<date>2013</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="1564" citStr="Fader et al., 2013" startWordPosition="242" endWordPosition="245">ons via predicted structures substantially improves accuracy over baselines that use shallower representations. 1 Introduction The goal of machine reading is to develop programs that read text to learn about the world and make decisions based on accumulated knowledge. Work in this field has focused mostly on macro-reading, i.e., processing large text collections and extracting knowledge bases of facts (Etzioni et al., 2006; Carlson et al., 2010; Fader et al., 2011). Such methods rely on redundancy, and are thus suitable for answering common factoid questions which have ample evidence in text (Fader et al., 2013). However, reading a single document (micro-reading) to answer comprehension questions that require deep reasoning is currently beyond the scope of state-of-the-art systems. In this paper, we introduce a task where given a paragraph describing a process, the goal is to ∗Both authors equally contributed to the paper. answer reading comprehension questions that test understanding of the underlying structure. In particular, we consider processes in biology textbooks such as this excerpt and the question that follows: “. .. Water is split, providing a source of electrons and protons (hydrogen ions</context>
</contexts>
<marker>Fader, Zettlemoyer, Etzioni, 2013</marker>
<rawString>Anthony Fader, Luke Zettlemoyer, and Oren Etzioni. 2013. Paraphrase-Driven Learning for Open Question Answering. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<title>WordNet: An electronic lexical database.</title>
<date>1998</date>
<editor>Christiane Fellbaum, editor.</editor>
<publisher>MIT Press.</publisher>
<marker>1998</marker>
<rawString>Christiane Fellbaum, editor. 1998. WordNet: An electronic lexical database. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lynette Hirschman</author>
<author>Marc Light</author>
<author>Eric Breck</author>
<author>John D Burger</author>
</authors>
<title>Deep read: A reading comprehension system.</title>
<date>1999</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="39935" citStr="Hirschman et al. (1999)" startWordPosition="6571" endWordPosition="6574">ap natural language utterances to referents in an image by constructing a KB from the image and then mapping the utterance to a query over the KB. This is analogous to our process of constructing a process structure and performing QA by querying that structure. In our work, we parse questions into graph-based queries, suitable for modeling processes, using a rule-based heuristic. Training a statistical semantic parser that will replace the QA system is an interesting direction for future research. Multiple choice reading comprehension tests are a natural choice for evaluating machine reading. Hirschman et al. (1999) presented a bag-ofwords approach to retrieving sentences for reading comprehension. Richardson et al. (2013) recently released the MCTest reading comprehension dataset that examines understanding of fictional stories. Their work shares our goal of advancing micro-reading, but they do not focus on process understanding. Developing programs that perform deep reasoning over complex descriptions of processes is an important step on the road to fulfilling the higher goals of machine reading. In this paper, we present an end-to-end system for reading comprehension of paragraphs which describe biolo</context>
</contexts>
<marker>Hirschman, Light, Breck, Burger, 1999</marker>
<rawString>Lynette Hirschman, Marc Light, Eric Breck, and John D. Burger. 1999. Deep read: A reading comprehension system. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jin-Dong Kim</author>
<author>Tomoko Ohta</author>
<author>Sampo Pyysalo</author>
<author>Yoshinobu Kano</author>
<author>Junichi Tsujii</author>
</authors>
<title>shared task on event extraction.</title>
<date>2009</date>
<journal>Overview of BioNLP</journal>
<booktitle>In Proceedings of BioNLP.</booktitle>
<volume>09</volume>
<contexts>
<context position="37771" citStr="Kim et al., 2009" startWordPosition="6223" endWordPosition="6226">iability, where the author of questions uses names for triggers or arguments that are missing from the paragraph, and so alignment fails. Last, in 10% of the cases in GOLD we found that the answer could not be retrieved using the set of regular expressions that are currently used by our QA system. 7 Discussion This work touches on several strands of work in NLP including information extraction, semantic role labeling, semantic parsing and reading comprehension. Event and relation extraction have been studied via the ACE data (Doddington et al., 2004) and related work. The BioNLP shared tasks (Kim et al., 2009; Kim et al., 2011; Riedel and McCal1507 lum, 2011) focused on biomedical data to extract events and their arguments. Event-event relations have been mostly studied from the perspective of temporal ordering; e.g., (Chambers and Jurafsky, 2008; Yoshikawa et al., 2009; Do et al., 2012; McClosky and Manning, 2012). The process structure predicted in this work differs from these lines of work in two important ways: First, we predict events, arguments and their interactions from multiple sentences, while most earlier work focused on one or two of these components. Second, we model processes, and th</context>
</contexts>
<marker>Kim, Ohta, Pyysalo, Kano, Tsujii, 2009</marker>
<rawString>Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshinobu Kano, and Junichi Tsujii. 2009. Overview of BioNLP 09 shared task on event extraction. In Proceedings of BioNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jin-Dong Kim</author>
<author>Sampo Pyysalo</author>
<author>Tomoko Ohta</author>
<author>Robert Bossy</author>
<author>Junichi Tsujii</author>
</authors>
<title>Overview of BioNLP shared task</title>
<date>2011</date>
<booktitle>In Proceedings of BioNLP.</booktitle>
<contexts>
<context position="37789" citStr="Kim et al., 2011" startWordPosition="6227" endWordPosition="6230">e author of questions uses names for triggers or arguments that are missing from the paragraph, and so alignment fails. Last, in 10% of the cases in GOLD we found that the answer could not be retrieved using the set of regular expressions that are currently used by our QA system. 7 Discussion This work touches on several strands of work in NLP including information extraction, semantic role labeling, semantic parsing and reading comprehension. Event and relation extraction have been studied via the ACE data (Doddington et al., 2004) and related work. The BioNLP shared tasks (Kim et al., 2009; Kim et al., 2011; Riedel and McCal1507 lum, 2011) focused on biomedical data to extract events and their arguments. Event-event relations have been mostly studied from the perspective of temporal ordering; e.g., (Chambers and Jurafsky, 2008; Yoshikawa et al., 2009; Do et al., 2012; McClosky and Manning, 2012). The process structure predicted in this work differs from these lines of work in two important ways: First, we predict events, arguments and their interactions from multiple sentences, while most earlier work focused on one or two of these components. Second, we model processes, and thus target causal r</context>
</contexts>
<marker>Kim, Pyysalo, Ohta, Bossy, Tsujii, 2011</marker>
<rawString>Jin-Dong Kim, Sampo Pyysalo, Tomoko Ohta, Robert Bossy, and Junichi Tsujii. 2011. Overview of BioNLP shared task 2011. In Proceedings of BioNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jayant Krishnamurthy</author>
<author>Thomas Kollar</author>
</authors>
<title>Jointly learning to parse and perceive: Connecting natural language to the physical world.</title>
<date>2013</date>
<journal>TACL,</journal>
<pages>1--193</pages>
<contexts>
<context position="39299" citStr="Krishnamurthy and Kollar (2013)" startWordPosition="6469" endWordPosition="6472">e do not allow all verbs to trigger events and instead let the annotators decide on biologically important triggers, which are not restricted to verbs (unlike BioProp, where 30 pre-specified verbs were selected for annotation). Like PropBank and BioProp, the argument labels are not trigger specific. Mapping questions to queries is effectively a semantic parsing task. In recent years, several lines of work addressed semantic parsing using various formalisms and levels of supervision (Zettlemoyer and Collins, 2005; Wong and Mooney, 2006; Clarke et al., 2010; Berant et al., 2013). In particular, Krishnamurthy and Kollar (2013) learned to map natural language utterances to referents in an image by constructing a KB from the image and then mapping the utterance to a query over the KB. This is analogous to our process of constructing a process structure and performing QA by querying that structure. In our work, we parse questions into graph-based queries, suitable for modeling processes, using a rule-based heuristic. Training a statistical semantic parser that will replace the QA system is an interesting direction for future research. Multiple choice reading comprehension tests are a natural choice for evaluating mach</context>
</contexts>
<marker>Krishnamurthy, Kollar, 2013</marker>
<rawString>Jayant Krishnamurthy and Thomas Kollar. 2013. Jointly learning to parse and perceive: Connecting natural language to the physical world. TACL, 1:193–206.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nate Kushman</author>
<author>Yoav Artzi</author>
<author>Luke Zettlemoyer</author>
<author>Regina Barzilay</author>
</authors>
<title>Learning to automatically solve algebra word problems.</title>
<date>2014</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="3501" citStr="Kushman et al., 2014" startWordPosition="560" endWordPosition="563">he text to the question. We propose a novel method that tackles this challenging problem (see Figure 1). First, we train a supervised structure predictor that learns to extract entities, events and their relations describing the biological process. This is a difficult problem because events have complex interactions that span multiple sentences. Then, treating this structure as a small knowledge-base, we map questions to formal queries that are executed against the structure to provide the answer. Micro-reading is an important aspect of natural language understanding (Richardson et al., 2013; Kushman et al., 2014). In this work, we focus specifically on modeling processes, where events and entities relate to one another through complex interactions. While we work in the biology 1499 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1499–1510, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics “... Water is split, providing a source of electrons and protons (hydrogen ions, H+) and giving off O2 as a by-product. Light absorbed by chlorophyll drives a transfer of the electrons and hydrogen ions from water to an acceptor ca</context>
</contexts>
<marker>Kushman, Artzi, Zettlemoyer, Barzilay, 2014</marker>
<rawString>Nate Kushman, Yoav Artzi, Luke Zettlemoyer, and Regina Barzilay. 2014. Learning to automatically solve algebra word problems. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beth Levin</author>
</authors>
<title>English verb classes and alternations: A preliminary investigation.</title>
<date>1993</date>
<publisher>University of Chicago Press.</publisher>
<contexts>
<context position="15996" citStr="Levin, 1993" startWordPosition="2612" endWordPosition="2613">OTTRIGGER (Recall that a word can trigger an event that occurred, an event that did not occur, or not be a trigger at all). For simplicity, we model triggers as single words, but in the gold annotation about 14% are phrases (such as gene flow). Thus, we evaluate trigger prediction by taking heads of gold phrases. To train the classifier, we extract 1502 the lemma and POS tag of the word and adjacent words, dependency path to the root, POS tag of children and parent in the dependency tree, and clustering features from WordNet (Fellbaum, 1998), Nomlex (Macleod et al., 1998), Levin verb classes (Levin, 1993), and a list of biological processes compiled from Wikipedia. 4.2 Filtering Argument Candidates Labeling trigger-argument edges is similar to semantic role labeling. Following the standard approach (Punyakanok et al., 2008), for each trigger we collect all constituents in the same sentence to build an over-complete set of plausible candidate arguments. This set is pruned with a binary classifier that is tuned for high recall (akin to the argument identifier in SRL systems). On the development set we filter more than half of the argument candidates, while achieving more than 99% recall. This cl</context>
</contexts>
<marker>Levin, 1993</marker>
<rawString>Beth Levin. 1993. English verb classes and alternations: A preliminary investigation. University of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Catherine Macleod</author>
<author>Ralph Grishman</author>
<author>Adam Meyers</author>
<author>Leslie Barrett</author>
<author>Ruth Reeves</author>
</authors>
<title>Nomlex: A lexicon of nominalizations.</title>
<date>1998</date>
<booktitle>In Proceedings of EURALEX.</booktitle>
<contexts>
<context position="15962" citStr="Macleod et al., 1998" startWordPosition="2605" endWordPosition="2608">in the paragraph as one of T-YES, T-NO or NOTTRIGGER (Recall that a word can trigger an event that occurred, an event that did not occur, or not be a trigger at all). For simplicity, we model triggers as single words, but in the gold annotation about 14% are phrases (such as gene flow). Thus, we evaluate trigger prediction by taking heads of gold phrases. To train the classifier, we extract 1502 the lemma and POS tag of the word and adjacent words, dependency path to the root, POS tag of children and parent in the dependency tree, and clustering features from WordNet (Fellbaum, 1998), Nomlex (Macleod et al., 1998), Levin verb classes (Levin, 1993), and a list of biological processes compiled from Wikipedia. 4.2 Filtering Argument Candidates Labeling trigger-argument edges is similar to semantic role labeling. Following the standard approach (Punyakanok et al., 2008), for each trigger we collect all constituents in the same sentence to build an over-complete set of plausible candidate arguments. This set is pruned with a binary classifier that is tuned for high recall (akin to the argument identifier in SRL systems). On the development set we filter more than half of the argument candidates, while achie</context>
</contexts>
<marker>Macleod, Grishman, Meyers, Barrett, Reeves, 1998</marker>
<rawString>Catherine Macleod, Ralph Grishman, Adam Meyers, Leslie Barrett, and Ruth Reeves. 1998. Nomlex: A lexicon of nominalizations. In Proceedings of EURALEX.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas L Magnanti</author>
<author>Laurence A Wolsey</author>
</authors>
<title>Optimal trees. Handbooks in operations research and management science,</title>
<date>1995</date>
<pages>7--503</pages>
<contexts>
<context position="18629" citStr="Magnanti and Wolsey (1995)" startWordPosition="3048" endWordPosition="3051">re not connected by an edge. Name Description Every argument candidate and trigger pair has exactly one label. Two arguments of the same trigger cannot overlap. The SAME relation is symmetric. All other relations are anti-symmetric, i.e., for any relation label other than SAME, at most one of (t;, tj) or (tj, t;) can take that label and the other is assigned the label NULL-REL. Every trigger can have no more than two arguments with the same label. The same span of text can not be an argument for more than two triggers. The triggers must form a connected graph, framed as flow constraints as in Magnanti and Wolsey (1995) and Martins et al. (2009). If the same span of text is an argument of two triggers, then the triggers must be connected by a relation that is not NULL-REL. This ensures that triggers that share arguments are related. For any trigger, at most one outgoing edge can be labeled SUPER. Table 2: Constraints for joint inference. Formulation Given the two sets of variables, the objective of inference is to find a global assignment that maximizes the score. That is, the objective can be stated as follows: Here, y and z refer to all the argument and relation variables respectively. Clearly, all possibl</context>
</contexts>
<marker>Magnanti, Wolsey, 1995</marker>
<rawString>Thomas L. Magnanti and Laurence A. Wolsey. 1995. Optimal trees. Handbooks in operations research and management science, 7:503–615.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Mihai Surdeanu</author>
<author>John Bauer</author>
<author>Jenny Finkel</author>
<author>Steven J Bethard</author>
<author>David McClosky</author>
</authors>
<title>The Stanford CoreNLP natural language processing toolkit.</title>
<date>2014</date>
<booktitle>In Proceedings of ACL: System Demonstrations.</booktitle>
<contexts>
<context position="29785" citStr="Manning et al., 2014" startWordPosition="4944" endWordPosition="4947">backing off to the baselines. 1505 6 Empirical Evaluation In this section we aim to empirically evaluate whether we can improve reading comprehension accuracy by predicting process structures. We first provide details of the experimental setup. 6.1 Experimental setup We used 150 processes (435 questions) for training and 50 processes (150 questions) as the test set. For development, we randomly split the training set 10 times (80%/20%), and tuned hyperparameters by maximizing average accuracy on question answering. We preprocessed the paragraphs with the Stanford CoreNLP pipeline version 3.4 (Manning et al., 2014) and Illinois SRL (Punyakanok et al., 2008; Clarke et al., 2012). We used the Gurobi optimization package3 for inference. We compare our system PROREAD to baselines that do not have access to the process structure: 1. BOW: For each answer, we compute the proportion of content word lemmas covered by the paragraph and choose the one with higher coverage. For true-false questions, we compute the coverage of the question statement, and answer “True” if it is higher than a threshold tuned on the development set. 2. TEXTPROX: For dependency questions, we align content word lemmas in both the questio</context>
</contexts>
<marker>Manning, Surdeanu, Bauer, Finkel, Bethard, McClosky, 2014</marker>
<rawString>Christopher D. Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven J. Bethard, and David McClosky. 2014. The Stanford CoreNLP natural language processing toolkit. In Proceedings of ACL: System Demonstrations.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andr´e L Martins</author>
<author>Noah A Smith</author>
<author>Eric P Xing</author>
</authors>
<title>Concise integer linear programming formulations for dependency parsing.</title>
<date>2009</date>
<booktitle>In Proceedings of ACL/IJCNLP.</booktitle>
<contexts>
<context position="18655" citStr="Martins et al. (2009)" startWordPosition="3053" endWordPosition="3056">me Description Every argument candidate and trigger pair has exactly one label. Two arguments of the same trigger cannot overlap. The SAME relation is symmetric. All other relations are anti-symmetric, i.e., for any relation label other than SAME, at most one of (t;, tj) or (tj, t;) can take that label and the other is assigned the label NULL-REL. Every trigger can have no more than two arguments with the same label. The same span of text can not be an argument for more than two triggers. The triggers must form a connected graph, framed as flow constraints as in Magnanti and Wolsey (1995) and Martins et al. (2009). If the same span of text is an argument of two triggers, then the triggers must be connected by a relation that is not NULL-REL. This ensures that triggers that share arguments are related. For any trigger, at most one outgoing edge can be labeled SUPER. Table 2: Constraints for joint inference. Formulation Given the two sets of variables, the objective of inference is to find a global assignment that maximizes the score. That is, the objective can be stated as follows: Here, y and z refer to all the argument and relation variables respectively. Clearly, all possible assignments to the infer</context>
</contexts>
<marker>Martins, Smith, Xing, 2009</marker>
<rawString>Andr´e L. Martins, Noah A. Smith, and Eric P. Xing. 2009. Concise integer linear programming formulations for dependency parsing. In Proceedings of ACL/IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McClosky</author>
<author>Christopher D Manning</author>
</authors>
<title>Learning constraints for consistent timeline extraction.</title>
<date>2012</date>
<booktitle>In Proceedings of EMNLP-CoNLL.</booktitle>
<contexts>
<context position="38083" citStr="McClosky and Manning, 2012" startWordPosition="6272" endWordPosition="6276">Discussion This work touches on several strands of work in NLP including information extraction, semantic role labeling, semantic parsing and reading comprehension. Event and relation extraction have been studied via the ACE data (Doddington et al., 2004) and related work. The BioNLP shared tasks (Kim et al., 2009; Kim et al., 2011; Riedel and McCal1507 lum, 2011) focused on biomedical data to extract events and their arguments. Event-event relations have been mostly studied from the perspective of temporal ordering; e.g., (Chambers and Jurafsky, 2008; Yoshikawa et al., 2009; Do et al., 2012; McClosky and Manning, 2012). The process structure predicted in this work differs from these lines of work in two important ways: First, we predict events, arguments and their interactions from multiple sentences, while most earlier work focused on one or two of these components. Second, we model processes, and thus target causal relations between events, rather than temporal order only. Our semantic role annotation is similar to existing SRL schemes such as PropBank (Palmer et al., 2005), FrameNet (Ruppenhofer et al., 2006) and BioProp (Chou et al., 2006). However, in contrast to PropBank and FrameNet, we do not allow </context>
</contexts>
<marker>McClosky, Manning, 2012</marker>
<rawString>David McClosky and Christopher D. Manning. 2012. Learning constraints for consistent timeline extraction. In Proceedings of EMNLP-CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Moldovan</author>
<author>Christine Clark</author>
<author>Sanda Harabagiu</author>
<author>Steve Maiorano</author>
</authors>
<title>Cogex: A logic prover for question answering.</title>
<date>2003</date>
<booktitle>In Proceedings of NAACLHLT.</booktitle>
<contexts>
<context position="21353" citStr="Moldovan et al., 2003" startWordPosition="3505" endWordPosition="3508">igure 1). Our strategy is to treat the process structure as a small knowledge-base. We map each answer along with the question into a structured query that we compare against the structure. The query can prove either the correctness or incorrectness of the answer being considered. That is, either we get a valid match for an answer (proving that the corresponding answer is correct), or we get a refutation in the form of a contradicted causal chain (thus proving that the other answer is correct). This is similar to theorem proving approaches suggested in the past for factoid question answering (Moldovan et al., 2003). The rest of this section is divided into three parts: Section 5.1 defines the queries we use, Section 5.2 describes a rule-based algorithm for converting a question and an answer into a query and finally, 5.3 describes the overall algorithm. 5.1 Queries over Processes We model a query as a directed graph path with regular expressions over edge labels. The bottom right portion of Figure 1 shows examples of queries for our running example. In general, given a question and one of the answer candidates, one end of the path is populated by a trigger/argument found in the question and the other is</context>
</contexts>
<marker>Moldovan, Clark, Harabagiu, Maiorano, 2003</marker>
<rawString>Dan Moldovan, Christine Clark, Sanda Harabagiu, and Steve Maiorano. 2003. Cogex: A logic prover for question answering. In Proceedings of NAACLHLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Paul Kingsbury</author>
<author>Daniel Gildea</author>
</authors>
<title>The proposition bank: An annotated corpus of semantic roles.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>1</issue>
<contexts>
<context position="38549" citStr="Palmer et al., 2005" startWordPosition="6349" endWordPosition="6352">tudied from the perspective of temporal ordering; e.g., (Chambers and Jurafsky, 2008; Yoshikawa et al., 2009; Do et al., 2012; McClosky and Manning, 2012). The process structure predicted in this work differs from these lines of work in two important ways: First, we predict events, arguments and their interactions from multiple sentences, while most earlier work focused on one or two of these components. Second, we model processes, and thus target causal relations between events, rather than temporal order only. Our semantic role annotation is similar to existing SRL schemes such as PropBank (Palmer et al., 2005), FrameNet (Ruppenhofer et al., 2006) and BioProp (Chou et al., 2006). However, in contrast to PropBank and FrameNet, we do not allow all verbs to trigger events and instead let the annotators decide on biologically important triggers, which are not restricted to verbs (unlike BioProp, where 30 pre-specified verbs were selected for annotation). Like PropBank and BioProp, the argument labels are not trigger specific. Mapping questions to queries is effectively a semantic parsing task. In recent years, several lines of work addressed semantic parsing using various formalisms and levels of superv</context>
</contexts>
<marker>Palmer, Kingsbury, Gildea, 2005</marker>
<rawString>Martha Palmer, Paul Kingsbury, and Daniel Gildea. 2005. The proposition bank: An annotated corpus of semantic roles. Computational Linguistics, 31(1):71–106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasin Punyakanok</author>
<author>Dan Roth</author>
<author>Wen-tau Yih</author>
</authors>
<title>The importance of syntactic parsing and inference in semantic role labeling.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>2</issue>
<contexts>
<context position="16219" citStr="Punyakanok et al., 2008" startWordPosition="2643" endWordPosition="2646"> are phrases (such as gene flow). Thus, we evaluate trigger prediction by taking heads of gold phrases. To train the classifier, we extract 1502 the lemma and POS tag of the word and adjacent words, dependency path to the root, POS tag of children and parent in the dependency tree, and clustering features from WordNet (Fellbaum, 1998), Nomlex (Macleod et al., 1998), Levin verb classes (Levin, 1993), and a list of biological processes compiled from Wikipedia. 4.2 Filtering Argument Candidates Labeling trigger-argument edges is similar to semantic role labeling. Following the standard approach (Punyakanok et al., 2008), for each trigger we collect all constituents in the same sentence to build an over-complete set of plausible candidate arguments. This set is pruned with a binary classifier that is tuned for high recall (akin to the argument identifier in SRL systems). On the development set we filter more than half of the argument candidates, while achieving more than 99% recall. This classifier is trained using argument identification features from Punyakanok et al. (2008). At the end of this step, each trigger has a set of candidate arguments which will be labeled during joint inference. In further discu</context>
<context position="29827" citStr="Punyakanok et al., 2008" startWordPosition="4951" endWordPosition="4954">irical Evaluation In this section we aim to empirically evaluate whether we can improve reading comprehension accuracy by predicting process structures. We first provide details of the experimental setup. 6.1 Experimental setup We used 150 processes (435 questions) for training and 50 processes (150 questions) as the test set. For development, we randomly split the training set 10 times (80%/20%), and tuned hyperparameters by maximizing average accuracy on question answering. We preprocessed the paragraphs with the Stanford CoreNLP pipeline version 3.4 (Manning et al., 2014) and Illinois SRL (Punyakanok et al., 2008; Clarke et al., 2012). We used the Gurobi optimization package3 for inference. We compare our system PROREAD to baselines that do not have access to the process structure: 1. BOW: For each answer, we compute the proportion of content word lemmas covered by the paragraph and choose the one with higher coverage. For true-false questions, we compute the coverage of the question statement, and answer “True” if it is higher than a threshold tuned on the development set. 2. TEXTPROX: For dependency questions, we align content word lemmas in both the question and answer against the text and select t</context>
</contexts>
<marker>Punyakanok, Roth, Yih, 2008</marker>
<rawString>Vasin Punyakanok, Dan Roth, and Wen-tau Yih. 2008. The importance of syntactic parsing and inference in semantic role labeling. Computational Linguistics, 34(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Richardson</author>
<author>Christopher JC Burges</author>
<author>Erin Renshaw</author>
</authors>
<title>MCTest: A challenge dataset for the open-domain machine comprehension of text.</title>
<date>2013</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="3478" citStr="Richardson et al., 2013" startWordPosition="556" endWordPosition="559">ong answer is closer in the text to the question. We propose a novel method that tackles this challenging problem (see Figure 1). First, we train a supervised structure predictor that learns to extract entities, events and their relations describing the biological process. This is a difficult problem because events have complex interactions that span multiple sentences. Then, treating this structure as a small knowledge-base, we map questions to formal queries that are executed against the structure to provide the answer. Micro-reading is an important aspect of natural language understanding (Richardson et al., 2013; Kushman et al., 2014). In this work, we focus specifically on modeling processes, where events and entities relate to one another through complex interactions. While we work in the biology 1499 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1499–1510, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics “... Water is split, providing a source of electrons and protons (hydrogen ions, H+) and giving off O2 as a by-product. Light absorbed by chlorophyll drives a transfer of the electrons and hydrogen ions from </context>
<context position="40044" citStr="Richardson et al. (2013)" startWordPosition="6587" endWordPosition="6590"> the utterance to a query over the KB. This is analogous to our process of constructing a process structure and performing QA by querying that structure. In our work, we parse questions into graph-based queries, suitable for modeling processes, using a rule-based heuristic. Training a statistical semantic parser that will replace the QA system is an interesting direction for future research. Multiple choice reading comprehension tests are a natural choice for evaluating machine reading. Hirschman et al. (1999) presented a bag-ofwords approach to retrieving sentences for reading comprehension. Richardson et al. (2013) recently released the MCTest reading comprehension dataset that examines understanding of fictional stories. Their work shares our goal of advancing micro-reading, but they do not focus on process understanding. Developing programs that perform deep reasoning over complex descriptions of processes is an important step on the road to fulfilling the higher goals of machine reading. In this paper, we present an end-to-end system for reading comprehension of paragraphs which describe biological processes. This is, to the best of our knowledge, the first system to both predict a rich structured re</context>
</contexts>
<marker>Richardson, Burges, Renshaw, 2013</marker>
<rawString>Matthew Richardson, Christopher JC Burges, and Erin Renshaw. 2013. MCTest: A challenge dataset for the open-domain machine comprehension of text. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Riedel</author>
<author>Andrew McCallum</author>
</authors>
<title>Fast and robust joint models for biomedical event extraction.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<marker>Riedel, McCallum, 2011</marker>
<rawString>Sebastian Riedel and Andrew McCallum. 2011. Fast and robust joint models for biomedical event extraction. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Roth</author>
<author>Wen-tau Yih</author>
</authors>
<title>A linear programming formulation for global inference in natural language tasks.</title>
<date>2004</date>
<booktitle>In Proceedings of CoNLL.</booktitle>
<contexts>
<context position="19540" citStr="Roth and Yih, 2004" startWordPosition="3205" endWordPosition="3208">Constraints for joint inference. Formulation Given the two sets of variables, the objective of inference is to find a global assignment that maximizes the score. That is, the objective can be stated as follows: Here, y and z refer to all the argument and relation variables respectively. Clearly, all possible assignments to the inference variables are not feasible and there are both structural as well as prior knowledge constraints over the output space. Table 2 states the constraints we include, which are expressed as linear inequalities over output variables using standard techniques (e.g., (Roth and Yih, 2004)). 4.4 Learning in the Joint Model We train both the trigger classifier and the argument identifier using L2-regularized logistic regression. For the joint model, we use a linear model for the scoring functions, and train jointly using the structured averaged perceptron algorithm (Collins, 2002). Since argument labeling is similar to semantic role labeling (SRL), we extract standard SRL features given the trigger and argument from the syntactic tree for the corresponding sentence. In addition, we add features extracted from an off-theshelf SRL system. We also include all feature conjunctions. </context>
</contexts>
<marker>Roth, Yih, 2004</marker>
<rawString>Dan Roth and Wen-tau Yih. 2004. A linear programming formulation for global inference in natural language tasks. In Proceedings of CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Josef Ruppenhofer</author>
<author>Michael Ellsworth</author>
<author>Miriam RL Petruck</author>
<author>Christopher R Johnson</author>
<author>Jan Scheffczyk</author>
</authors>
<title>FrameNet II: Extended theory and practice.</title>
<date>2006</date>
<journal>Berkeley FrameNet Release,</journal>
<volume>1</volume>
<contexts>
<context position="38586" citStr="Ruppenhofer et al., 2006" startWordPosition="6354" endWordPosition="6357">emporal ordering; e.g., (Chambers and Jurafsky, 2008; Yoshikawa et al., 2009; Do et al., 2012; McClosky and Manning, 2012). The process structure predicted in this work differs from these lines of work in two important ways: First, we predict events, arguments and their interactions from multiple sentences, while most earlier work focused on one or two of these components. Second, we model processes, and thus target causal relations between events, rather than temporal order only. Our semantic role annotation is similar to existing SRL schemes such as PropBank (Palmer et al., 2005), FrameNet (Ruppenhofer et al., 2006) and BioProp (Chou et al., 2006). However, in contrast to PropBank and FrameNet, we do not allow all verbs to trigger events and instead let the annotators decide on biologically important triggers, which are not restricted to verbs (unlike BioProp, where 30 pre-specified verbs were selected for annotation). Like PropBank and BioProp, the argument labels are not trigger specific. Mapping questions to queries is effectively a semantic parsing task. In recent years, several lines of work addressed semantic parsing using various formalisms and levels of supervision (Zettlemoyer and Collins, 2005;</context>
</contexts>
<marker>Ruppenhofer, Ellsworth, Petruck, Johnson, Scheffczyk, 2006</marker>
<rawString>Josef Ruppenhofer, Michael Ellsworth, Miriam RL Petruck, Christopher R. Johnson, and Jan Scheffczyk. 2006. FrameNet II: Extended theory and practice. Berkeley FrameNet Release, 1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aju Thalappillil Scaria</author>
<author>Jonathan Berant</author>
<author>Mengqiu Wang</author>
<author>Peter Clark</author>
<author>Justin Lewis</author>
<author>Brittany Harding</author>
<author>Christopher D Manning</author>
</authors>
<title>Learning biological processes with global constraints.</title>
<date>2013</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="6665" citStr="Scaria et al. (2013)" startWordPosition="1055" endWordPosition="1058">and the accompanying dataset. We will use the example in Figure 1 as our running example throughout the paper. Our goal is to tackle a complex reading comprehension setting that centers on understanding the underlying meaning of a process description. We target a multiple-choice setting in which each input consists of a paragraph of text describing a biological process, a question, and two possible answers. The goal is to identify the correct answer using the text (Figure 1, left). We used the 148 paragraphs from the textbook Biology (Campbell and Reece, 2005) that were manually identified by Scaria et al. (2013). We extended this set to 200 paragraphs by including additional paragraphs that describe biological processes. Each paragraph in the collection represents a single biological process and describes a set of events, their participants and their interactions. Because we target understanding of paragraph meaning, we use the following desiderata for building the corpus of questions and answers: 1. The questions should focus on the events and entities participating in the process described in the paragraph, and answering the questions should require reasoning about the relations between those event</context>
<context position="8654" citStr="Scaria et al. (2013)" startWordPosition="1369" endWordPosition="1372">a paragraph describing a process and an example of a question based on it. In general, questions test an understanding of the interactions between multiple events (such as causality, inhibition, temporal ordering), or between events and entities (i.e., roles of entities in events), and require complex reasoning about chains of event-event and event-entity relations. 3 The Structure of Processes A natural first step for answering reading comprehension questions is to identify a structured representation of the text. In this section, we define this structure. We broadly follow the definition of Scaria et al. (2013), but modify important aspects, highlighted at the end of this section. A paragraph describing a process is a sequence of tokens that describes events, entities and their relations (see Figure 1, top right). A process is a directed graph (T , A, £tt, £ta), where the nodes T are labeled event triggers, the nodes A are arguments, £tt are labeled edges describing eventevent relations, and £ta are labeled edges from triggers to arguments denoting semantic roles (see Figure 1 top right for a partial structure of the running example). The goal of process extraction is to generate the process graph g</context>
<context position="13074" citStr="Scaria et al. (2013)" startWordPosition="2126" endWordPosition="2129">gers, arguments and relations over the 200 annotated paragraphs. Three biologists annotated the same 200 paragraphs described in Section 2 using the brat annotation tool (Stenetorp et al., 2012). For each paragraph, one annotator annotated the process, and a second validated its correctness. Importantly, the questions and answers were authored separately by a different annotator, thus ensuring that the questions and answers are independent from the annotated structures. Table 1 gives statistics over the dataset. The annotation guidelines are included in the supplementary material. Relation to Scaria et al. (2013) Scaria et al. (2013) also defined processes as graphs where nodes are events and edges describe event-event relations. Our definition differs in a few important aspects. First, the set of event-event relations in that work included temporal relations in addition to causal ones. In this work, we posit that because events in a process are inter-related, causal dependencies are sufficient to capture the relevant temporal ordering between them. Figure 1 illustrates this phenomenon, where the temporal ordering between the events of water splitting and light absorption is unspecified. It does not m</context>
<context position="20218" citStr="Scaria et al. (2013)" startWordPosition="3316" endWordPosition="3319">er classifier and the argument identifier using L2-regularized logistic regression. For the joint model, we use a linear model for the scoring functions, and train jointly using the structured averaged perceptron algorithm (Collins, 2002). Since argument labeling is similar to semantic role labeling (SRL), we extract standard SRL features given the trigger and argument from the syntactic tree for the corresponding sentence. In addition, we add features extracted from an off-theshelf SRL system. We also include all feature conjunctions. For event relations, we include the features described in Scaria et al. (2013), as well as context features for both triggers, and the dependency path between them, if one exists. Unique labels Argument overlap Relation symmetry Max arguments per trigger Max triggers per argument Connectivity Shared arguments Unique parent �max �bt,a,A &apos; yt,a,A + ct1,t2,R &apos; zt1,t2,R y,z t,a∈At,A t1,t2,R 1503 5 Question Answering via Structures This section describes our question answering system that, given a process structure, a question and two answers, chooses the correct answer (steps 2 and 3 in Figure 1). Our strategy is to treat the process structure as a small knowledge-base. We </context>
</contexts>
<marker>Scaria, Berant, Wang, Clark, Lewis, Harding, Manning, 2013</marker>
<rawString>Aju Thalappillil Scaria, Jonathan Berant, Mengqiu Wang, Peter Clark, Justin Lewis, Brittany Harding, and Christopher D. Manning. 2013. Learning biological processes with global constraints. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pontus Stenetorp</author>
</authors>
<title>Sampo Pyysalo, Goran Topi´c, Tomoko Ohta, Sophia Ananiadou, and Jun’ichi Tsujii.</title>
<date>2012</date>
<booktitle>In Proceedings of the demonstrations at EACL.</booktitle>
<marker>Stenetorp, 2012</marker>
<rawString>Pontus Stenetorp, Sampo Pyysalo, Goran Topi´c, Tomoko Ohta, Sophia Ananiadou, and Jun’ichi Tsujii. 2012. Brat: a web-based tool for NLP-assisted text annotation. In Proceedings of the demonstrations at EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuk Wah Wong</author>
<author>Raymond J Mooney</author>
</authors>
<title>Learning for semantic parsing with statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of HLT-NAACL.</booktitle>
<contexts>
<context position="39208" citStr="Wong and Mooney, 2006" startWordPosition="6455" endWordPosition="6458">and BioProp (Chou et al., 2006). However, in contrast to PropBank and FrameNet, we do not allow all verbs to trigger events and instead let the annotators decide on biologically important triggers, which are not restricted to verbs (unlike BioProp, where 30 pre-specified verbs were selected for annotation). Like PropBank and BioProp, the argument labels are not trigger specific. Mapping questions to queries is effectively a semantic parsing task. In recent years, several lines of work addressed semantic parsing using various formalisms and levels of supervision (Zettlemoyer and Collins, 2005; Wong and Mooney, 2006; Clarke et al., 2010; Berant et al., 2013). In particular, Krishnamurthy and Kollar (2013) learned to map natural language utterances to referents in an image by constructing a KB from the image and then mapping the utterance to a query over the KB. This is analogous to our process of constructing a process structure and performing QA by querying that structure. In our work, we parse questions into graph-based queries, suitable for modeling processes, using a rule-based heuristic. Training a statistical semantic parser that will replace the QA system is an interesting direction for future res</context>
</contexts>
<marker>Wong, Mooney, 2006</marker>
<rawString>Yuk Wah Wong and Raymond J. Mooney. 2006. Learning for semantic parsing with statistical machine translation. In Proceedings of HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katsumasa Yoshikawa</author>
<author>Sebastian Riedel</author>
<author>Masayuki Asahara</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Jointly identifying temporal relations with Markov logic.</title>
<date>2009</date>
<booktitle>In Proceedings of ACL/IJCNLP.</booktitle>
<contexts>
<context position="38037" citStr="Yoshikawa et al., 2009" startWordPosition="6264" endWordPosition="6267">t are currently used by our QA system. 7 Discussion This work touches on several strands of work in NLP including information extraction, semantic role labeling, semantic parsing and reading comprehension. Event and relation extraction have been studied via the ACE data (Doddington et al., 2004) and related work. The BioNLP shared tasks (Kim et al., 2009; Kim et al., 2011; Riedel and McCal1507 lum, 2011) focused on biomedical data to extract events and their arguments. Event-event relations have been mostly studied from the perspective of temporal ordering; e.g., (Chambers and Jurafsky, 2008; Yoshikawa et al., 2009; Do et al., 2012; McClosky and Manning, 2012). The process structure predicted in this work differs from these lines of work in two important ways: First, we predict events, arguments and their interactions from multiple sentences, while most earlier work focused on one or two of these components. Second, we model processes, and thus target causal relations between events, rather than temporal order only. Our semantic role annotation is similar to existing SRL schemes such as PropBank (Palmer et al., 2005), FrameNet (Ruppenhofer et al., 2006) and BioProp (Chou et al., 2006). However, in contr</context>
</contexts>
<marker>Yoshikawa, Riedel, Asahara, Matsumoto, 2009</marker>
<rawString>Katsumasa Yoshikawa, Sebastian Riedel, Masayuki Asahara, and Yuji Matsumoto. 2009. Jointly identifying temporal relations with Markov logic. In Proceedings of ACL/IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luke S Zettlemoyer</author>
<author>Michael Collins</author>
</authors>
<title>Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars.</title>
<date>2005</date>
<booktitle>In Proceedings of UAI.</booktitle>
<contexts>
<context position="39185" citStr="Zettlemoyer and Collins, 2005" startWordPosition="6450" endWordPosition="6454">Net (Ruppenhofer et al., 2006) and BioProp (Chou et al., 2006). However, in contrast to PropBank and FrameNet, we do not allow all verbs to trigger events and instead let the annotators decide on biologically important triggers, which are not restricted to verbs (unlike BioProp, where 30 pre-specified verbs were selected for annotation). Like PropBank and BioProp, the argument labels are not trigger specific. Mapping questions to queries is effectively a semantic parsing task. In recent years, several lines of work addressed semantic parsing using various formalisms and levels of supervision (Zettlemoyer and Collins, 2005; Wong and Mooney, 2006; Clarke et al., 2010; Berant et al., 2013). In particular, Krishnamurthy and Kollar (2013) learned to map natural language utterances to referents in an image by constructing a KB from the image and then mapping the utterance to a query over the KB. This is analogous to our process of constructing a process structure and performing QA by querying that structure. In our work, we parse questions into graph-based queries, suitable for modeling processes, using a rule-based heuristic. Training a statistical semantic parser that will replace the QA system is an interesting d</context>
</contexts>
<marker>Zettlemoyer, Collins, 2005</marker>
<rawString>Luke S. Zettlemoyer and Michael Collins. 2005. Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars. In Proceedings of UAI.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000016">
<title confidence="0.992191">
Joint Relational Embeddings for Knowledge-based Question Answering
</title>
<author confidence="0.987844">
Min-Chul Yang† Nan Duan$ Ming Zhou$ Hae-Chang Rim††Dept. of Computer &amp; Radio Comms. Engineering, Korea University, Seoul, South Korea
</author>
<affiliation confidence="0.977148">
$Microsoft Research Asia, Beijing, China
</affiliation>
<email confidence="0.959161">
mcyang@nlp.korea.ac.kr
{nanduan, mingzhou}@microsoft.com
rim@nlp.korea.ac.kr
</email>
<sectionHeader confidence="0.993735" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999915823529412">
Transforming a natural language (NL)
question into a corresponding logical form
(LF) is central to the knowledge-based
question answering (KB-QA) task. Un-
like most previous methods that achieve
this goal based on mappings between lex-
icalized phrases and logical predicates,
this paper goes one step further and pro-
poses a novel embedding-based approach
that maps NL-questions into LFs for KB-
QA by leveraging semantic associations
between lexical representations and KB-
properties in the latent space. Experimen-
tal results demonstrate that our proposed
method outperforms three KB-QA base-
line methods on two publicly released QA
data sets.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99980075">
Knowledge-based question answering (KB-QA)
involves answering questions posed in natural
language (NL) using existing knowledge bases
(KBs). As most KBs are structured databases,
how to transform the input question into its corre-
sponding structured query for KB (KB-query) as
a logical form (LF), also known as semantic pars-
ing, is the central task for KB-QA systems. Pre-
vious works (Mooney, 2007; Liang et al., 2011;
Cai and Yates, 2013; Fader et al., 2013; Berant et
al., 2013; Bao et al., 2014) usually leveraged map-
pings between NL phrases and logical predicates
as lexical triggers to perform transformation tasks
in semantic parsing, but they had to deal with two
limitations: (i) as the meaning of a logical pred-
icate often has different natural language expres-
sion (NLE) forms, the lexical triggers extracted for
a predicate may at times are limited in size; (ii)
entities detected by the named entity recognition
(NER) component will be used to compose the
logical forms together with the logical predicates,
so their types should be consistent with the pred-
icates as well. However, most NER components
used in existing KB-QA systems are independent
from the NLE-to-predicate mapping procedure.
We present a novel embedding-based KB-QA
method that takes all the aforementioned lim-
itations into account, and maps NLE-to-entity
and NLE-to-predicate simultaneously using sim-
ple vector operations for structured query con-
struction. First, low-dimensional embeddings of
n-grams, entity types, and predicates are jointly
learned from an existing knowledge base and from
entries &lt;entitysubj, NL relation phrase, entityobj&gt;
that are mined from NL texts labeled as KB-
properties with weak supervision. Each such en-
try corresponds to an NL expression of a triple
&lt;entitysubj, predicate, entityobj&gt; in the KB. These
embeddings are used to measure the semantic as-
sociations between lexical phrases and two prop-
erties of the KB, entity type and logical predicate.
Next, given an NL-question, all possible struc-
tured queries as candidate LFs are generated and
then they are ranked by the similarity between the
embeddings of observed features (n-grams) in the
NL-question and the embeddings of logical fea-
tures in the structured queries. Last, answers are
retrieved from the KB using the selected LFs.
The contributions of this work are two-fold: (1)
as a smoothing technique, the low-dimensional
embeddings can alleviate the coverage issues of
lexical triggers; (2) our joint approach integrates
entity span selection and predicate mapping tasks
for KB-QA. For this we built independent entity
embeddings as the additional component, solving
the entity disambiguation problem.
</bodyText>
<sectionHeader confidence="0.997517" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.584342333333333">
Supervised semantic parsers (Zelle and Mooney,
1996; Zettlemoyer and Collins, 2005; Mooney,
2007) heavily rely on the &lt;sentence, semantic an-
</bodyText>
<page confidence="0.955333">
645
</page>
<bodyText confidence="0.971805921052632">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 645–650,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
notation&gt; pairs for lexical trigger extraction and
model training. Due to the data annotation re-
quirement, such methods are usually restricted to
specific domains, and struggle with the coverage
issue caused by the limited size of lexical triggers.
Studies on weakly supervised semantic parsers
have tried to reduce the amount of human supervi-
sion by using question-answer pairs (Liang et al.,
2011) or distant supervision (Krishnamurthy and
Mitchell, 2012) instead of full semantic annota-
tions. Still, for KB-QA, the question of how to
leverage KB-properties and analyze the question
structures remains.
Bordes et al. (2012) and Weston et al. (2013) de-
signed embedding models that connect free texts
with KBs using the relational learning method
(Weston et al., 2010). Their inputs are often
statement sentences which include subject and ob-
ject entities for a given predicate, whereas NL-
questions lack either a subject or object entity that
is the potential answer. Hence, we can only use
the information of a subject or object entity, which
leads to a different training instance generation
procedure and a different training criterion.
Recently, researchers have developed open do-
main systems based on large scale KBs such as
FREEBASE1 (Cai and Yates, 2013; Fader et al.,
2013; Berant et al., 2013; Kwiatkowski et al.,
2013; Bao et al., 2014; Berant and Liang, 2014;
Yao and Van Durme, 2014). Their semantic
parsers for Open QA are unified formal and scal-
able: they enable the NL-question to be mapped
into the appropriate logical form. Our method ob-
tains similar logical forms, but using only low-
dimensional embeddings of n-grams, entity types,
and predicates learned from texts and KB.
</bodyText>
<sectionHeader confidence="0.999337" genericHeader="method">
3 Setup
</sectionHeader>
<subsectionHeader confidence="0.99965">
3.1 Relational Components for KB-QA
</subsectionHeader>
<bodyText confidence="0.999843875">
Our method learns semantic mappings between
NLEs and the KB2 based on the paired relation-
ships of the following three components: C de-
notes a set of bag-of-words (or n-grams) as context
features (c) for NLEs that are the lexical represen-
tations of a logical predicate (p) in KB; T denotes
a set of entity types (t) in KB and each type can be
used as the abstract expression of a subject entity
</bodyText>
<footnote confidence="0.8941714">
1http://www.freebase.com
2For this paper, we used a large scale knowledge base that
contains 2.3B entities, 5.5K predicates, and 18B assertions.
A 16-machine cluster was used to host and serve the whole
data.
</footnote>
<bodyText confidence="0.999655705882353">
(s) that occurs in the input question; P denotes a
set of logical predicates (p) in KB, each of which
is the canonical form of different NLEs sharing an
identical meaning (bag-of-words; c).
Based on the components defined above, the
paired relationships are described as follows: T-
P can investigate the relationship between sub-
ject entity and logical predicate, as object entity
is always missing in KB-QA; C-T can scruti-
nize subject entity’s attributes for the entity span
selection such as its positional information and
relevant entity types to the given context, which
may solve the entity disambiguation problem in
KB-QA; C-P can leverage the semantic overlap
between question contexts (n-gram features) and
logical predicates, which is important for mapping
NL-questions to their corresponding predicates.
</bodyText>
<subsectionHeader confidence="0.995847">
3.2 NLE-KB Pair Extraction
</subsectionHeader>
<bodyText confidence="0.999922772727273">
This section describes how we extract the semantic
associated pairs of NLE-entries and KB-triples to
learn the relational embeddings (Section 4.1).
&lt;Relation Mention, Predicate&gt; Pair (MP)
Each relation mention denotes a lexical phrase
of an existing KB-predicate. Following informa-
tion extraction methods, such as PATTY (Nakas-
hole et al., 2012), we extracted the &lt;relation
mention, logical predicate&gt; pairs from English
WIKIPEDIA3, which is closely connected to our
KB, as follows: Given a KB-triple &lt;entitysubj,
logical predicate, entityobj&gt;, we extracted NLE-
entries &lt;entitysubj, relation mention, entityobj&gt;
where relation mention is the shortest path be-
tween entitysubj and entityobj in the dependency
tree of sentences. The assumption is that any re-
lation mention (m) in the NLE-entry containing
such entity pairs that occurred in the KB-triple is
likely to express the predicate (p) of that triple.
With obtaining high-quality MP pairs, we kept
only relation mentions that were highly associated
with a predicate measured by the scoring function:
</bodyText>
<equation confidence="0.693432">
S(m, p) = PMI(em; ep) + PMI(um; up) (1)
</equation>
<bodyText confidence="0.9998944">
where ex is the set of total pairs of both-side
entities of entry x (m or p) and ux is the set
of unique (distinct) pairs of both-side entities of
entry x. In this case, the both-side entities in-
dicate entitysubj and entityobj. For a frequency-
</bodyText>
<equation confidence="0.835049333333333">
based probability, PMI(x; y) = log P (x,y)
P (x)P (y)
3http://en.wikipedia.org/
646
(Church and Hanks, 1990) can be re-written as
PMI(x; y) = log |x � y|·C
</equation>
<bodyText confidence="0.999197058823529">
|x|·|y |, where C denotes the
total number of items shown in the corpus. The
function is partially derived from the support score
(Gerber and Ngonga Ngomo, 2011), but we fo-
cus on the correlation of shared entity pairs be-
tween relation mentions and predicates using the
PMI computation.
&lt;Question Pattern, Predicate&gt; Pair (2P)
Since WIKIPEDIA articles have no information to
leverage interrogative features which highly de-
pend on the object entity (answer), it is difficult to
distinguish some questions that are composed of
only different SW1H words, e.g., {When|Where}
was Barack Obama born? Hence, we used the
method of collecting question patterns with human
labeled predicates that are restricted by the set of
predicates used in MP (Bao et al., 2014).
</bodyText>
<sectionHeader confidence="0.993781" genericHeader="method">
4 Embedding-based KB-QA
</sectionHeader>
<bodyText confidence="0.99089425">
Our task is as follows. First, our model learns the
semantic associations of C-T, C-P, and T-P (Sec-
tion 3.1) based on NLE-KB pairs (Section 3.2),
and then predicts the semantic-related KB-query
which can directly find the answer to a given NL-
question.
For our feature space, given an NLE-KB pair,
the NLE (relation mention in MP or question
pattern in 2P) is decomposed into n-gram fea-
tures: C = {c  |c is a segment of NLE}, and
the KB-properties are represented by entity type
t of entitysubj and predicate p. Then we can ob-
tain a training triplet w = [C, t, p]. Each feature
(c E C, t E T , p E P) is encoded in the distributed
representation which is n-dimensional embedding
vectors (En): bx, x encode
</bodyText>
<listItem confidence="0.275119">
⇒ E(x) E En.
</listItem>
<bodyText confidence="0.995949588235294">
All n-gram features (C) for an NLE are merged
into one embedding vector to help speed up the
learning process: E(C) = E c∈C E(c)/|C|. This
feature representation is inspired by previous work
in embedding-based relation extraction (Weston et
al., 2013), but differs in the following ways: (1)
entity information is represented on a separate em-
bedding, but its positional information remains as
symbol (entity); (2) when the vectors are com-
bined, we use the average of each index to normal-
ize features.
For our joint relational approach, we focus on
the set of paired relationships R = {C-t, C-p, t-
p} that can be semantically leveraged. Formally,
these features are embedded into the same latent
space (En) and their semantic similarities can be
computed by a dot product operation:
</bodyText>
<equation confidence="0.980628">
Sim(a, b) = Sim(rab) = E(a)|E(b) (2)
</equation>
<bodyText confidence="0.9997364">
where rab denotes a paired relationship a-b (or (a,
b)) in the above set R. We believe that our joint re-
lational learning can smooth the surface (lexical)
features for semantic parsing using the aligned en-
tity and predicate.
</bodyText>
<subsectionHeader confidence="0.995761">
4.1 Joint Relational Embedding Learning
</subsectionHeader>
<bodyText confidence="0.999436333333333">
Our ranking-based relational learning is based on
a ranking loss (Weston et al., 2010) that supports
the idea that the similarity scores of observed pairs
in the training set (positive instances) should be
larger than those of any other pairs (negative in-
stances):
</bodyText>
<equation confidence="0.69592">
bi, by0 =� yi, Sim(xi, yi) &gt; 1+Sim(xi, y0) (3)
</equation>
<bodyText confidence="0.857258">
More precisely, for each triplet wi = [Ci, ti, pi]
obtained from an NLE-KB pair, the relationships
Ri = {Ci-ti, Ci-pi, ti-pi} are trained under the
soft ranking criterion, which conducts Stochastic
Gradient Descent (SGD). We thus aim to minimize
the following:
</bodyText>
<equation confidence="0.328581">
bi, by0 =� yi, max(0,1−Sim(xi, yi)+Sim(xi, y0))
(4)
</equation>
<bodyText confidence="0.999519625">
Our learning strategy is as follows. First, we ini-
tialize embedding space En by randomly giving
mean 0 and standard deviation 1/n to each vec-
tor. Then for each training triplet wi, we select the
negative pairs against positive pairs (Ci-ti, Ci-pi,
and ti-pi) in the triplet. Last, we make a stochastic
gradient step to minimize Equation 4 and update
En at each step.
</bodyText>
<subsectionHeader confidence="0.959151">
4.2 KB-QA using Embedding Models
</subsectionHeader>
<bodyText confidence="0.999985714285714">
Our goal for KB-QA is to translate a given NL-
question to a KB-query with the form &lt;subject
entity, predicate, ?&gt;, where ? denotes the an-
swer entity we are looking for. The decoding pro-
cess consists of two stages. The first stage in-
volves generating all possible KB-queries (Kq) for
an NL-question q. We first extract n-gram fea-
tures (Cq) from the NL-question q. Then for a
KB-query kq, we find all available entity types
(tq) of the identified subject entities (sq) using
the dictionary-based entity detection on the NL-
question q (all of spans can be candidate entities),
and assign all items of predicate set (P) as the can-
didate predicates (pq). Like the training triplets,
</bodyText>
<page confidence="0.949179">
647
</page>
<figure confidence="0.836494333333333">
q
ˆk(q)
Cq
</figure>
<bodyText confidence="0.739832">
where is the city of david?
</bodyText>
<note confidence="0.431355">
[The City of David, contained by, ?]
</note>
<table confidence="0.802252833333333">
n-grams of “where is (entity) ?”
location
contained by
# Entries Accuracy
MP pairs 291,585 89%
QP pairs 4,764 98%
</table>
<tableCaption confidence="0.921315">
Table 2: Statistics of NLE-KB pairs
</tableCaption>
<table confidence="0.6360585">
tq
pq
</table>
<tableCaption confidence="0.872531">
Table 1: The corresponding KB-query ˆk(q) for a
NL-question q and its decoding triplet wq.
</tableCaption>
<bodyText confidence="0.959717230769231">
we also represent the above features as the triplet
form wq� = [Cq�, tq�, pq�] which is directly linked to
a KB-query kq� = [sq�, pq�, ?]. The second stage
involves ranking candidate KB-queries based on
the similarity scores between the following paired
relationships from the triplet wq� : Rq � = {Cq�-tq�,
Cq�-pq�, tq�-pq�}. Unlike in the training step, the sim-
ilarities of Cq�-tq � and Cq�-pq� are computed by sum-
mation of all pairwise elements (each context em-
bedding E(c), not E(C), with each paired E(t) or
E(p)) for a more precise measurement. Since sim-
ilarites of Rq are calculated on different scales, we
normalize each value using Z-score (Z(x) = ms−µ)
</bodyText>
<equation confidence="0.97900675">
Q
(Kreyszig, 1979). The final score is measured by:
5imq2k(q, kq) = � Z(5im(r)) (5)
rERq
</equation>
<bodyText confidence="0.569114">
Then, given any NL-question q, we can predict the
corresponding KB-query ˆk(q):
</bodyText>
<equation confidence="0.9442385">
ˆk(q) = arg max 5imq2k(q, k) (6)
kEKq
</equation>
<bodyText confidence="0.999814652173913">
Last, we can retrieve an answer from the KB using
a structured query ˆk(q). Table 1 shows an example
of our decoding process.
Multi-related Question Some questions in-
clude two-subject entities, both of which are cru-
cial to understanding the question. For the ques-
tion who plays gandalf in the lord of the rings?
Gandalf (character) and The Lord Of The
Rings (film) are explicit entities that should be
joined to a pair of the two entities (implicit entity).
More precisely, the two entities can be combined
into one concatenated entity (character-in-film)
using our manual rule, which compares the possi-
ble pairs of entity types in the question with the
list of pre-defined entity type pairs that can be
merged into a concatenated entity. Our solution
enables a multi-related question to be transformed
to a single-related question which can be directly
translated to a KB-query. Then, the two entity
mentions are replaced with the symbol (entity)
(who play (entity) in (entity) ?). We re-
gard the result of this transformation as one of the
candidate KB-queries in the decoding step.
</bodyText>
<sectionHeader confidence="0.999392" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.99957017948718">
Experimental Setting We first performed pre-
processing, including lowercase transformation,
lemmatization and tokenization, on NLE-KB pairs
and evaluation data. We used 71,310 n-grams
(uni-, bi-, tri-), 990 entity types, and 660 predi-
cates as relational components shown in Section
3.1. The sum of these three numbers (72,960)
equals the size of the embeddings we are going
to learn. In Table 2, we evaluated the quality of
NLE-KB pairs (MP and QP) described in Sec-
tion 3.2. We can see that the quality of QP pairs is
good, mainly due to human efforts. Also, we ob-
tained MP pairs that have an acceptable quality
using threshold 3.0 for Equation 1, which lever-
ages the redundancy information in the large-scale
data (WIKIPEDIA). For our embedding learning,
we set the embedding dimension n to 100, the
learning rate (A) for SGD to 0.0001, and the it-
eration number to 30. To make the decoding
procedure computable, we kept only the popular
KB-entity in the dictionary to map different entity
mentions into a KB-entity.
We used two publicly released data sets for QA
evaluations: Free917 (Cai and Yates, 2013) in-
cludes the annotated lambda calculus forms for
each question, and covers 81 domains and 635
Freebase relations; WebQ. (Berant et al., 2013)
provides 5,810 question-answer pairs that are built
by collecting common questions from Web-query
logs and by manually labeling answers. We used
the previous three approaches (Cai and Yates,
2013; Berant et al., 2013; Bao et al., 2014) as our
baselines.
Experimental Results Table 3 reports the over-
all performances of our proposed KB-QA method
on the two evaluation data sets and compares them
with those of the three baselines. Note that we
did not re-implement the baseline systems, but just
borrowed the evaluation results reported in their
</bodyText>
<page confidence="0.995435">
648
</page>
<table confidence="0.9531398">
Methods Free917 WebQ.
Cai and Yates (2013) 59.00% N/A
Berant et al. (2013) 62.00% 31.40%
Bao et al. (2014) N/A 37.50%
Our method 71.38% 41.34%
</table>
<tableCaption confidence="0.99745">
Table 3: Accuracy on the evaluation data
</tableCaption>
<table confidence="0.9880396">
Free917 WebQ.
71.38% 41.34%
70.65% 40.55%
67.03% 38.44%
31.16% 19.24%
</table>
<tableCaption confidence="0.999864">
Table 4: Ablation of the relationship types
</tableCaption>
<bodyText confidence="0.999818271186441">
papers. Although the KB used by our system is
much larger than FREEBASE, we still think that
the experimental results are directly comparable
because we disallow all the entities that are not in-
cluded in FREEBASE.
Table 3 shows that our method outperforms the
baselines on both Free917 and WebQ. data sets.
We think that using the low-dimensional embed-
dings of n-grams rather than the lexical triggers
greatly improves the coverage issue. Unlike the
previous methods which perform entity disam-
biguation and predicate prediction separately, our
method jointly performs these two tasks. More
precisely, we consider the relationships C-T and
C-P simultaneously to rank candidate KB-queries.
In Table 1, the most independent NER in KB-QA
systems may detect David as the subject entity,
but our joint approach can predict the appropriate
subject entity The City of David by leveraging
not only the relationships with other components
but also other relationships at once. The syntax-
based (grammar formalism) approaches such as
Combinatory Categorial Grammar (CCG) may ex-
perience errors if a question has grammatical er-
rors. However, our bag-of-words model-based ap-
proach can handle any question as long as the
question contains keywords that can help in un-
derstanding it.
Table 4 shows the contributions of the relation-
ships (][R) between relational components C, T,
and P. For each row, we remove the similarity
from each of the relationship types described in
Section 3.1. We can see that the C-P relationship
plays a crucial role in translating NL-questions to
KB-queries, while the other two relationships are
slightly helpful.
Result Analysis Since the majority of questions
in WebQ. tend to be more natural and diverse, our
method cannot find the correct answers to many
questions. The errors can be caused by any of
the following reasons. First, some NLEs cannot
be easily linked to existing KB-predicates, mak-
ing it difficult to find the answer entity. Second,
some entities can be mentioned in several different
ways, e.g., nickname (shaq→Shaquille O’neal)
and family name (hitler→Adolf Hitler). Third, in
terms of KB coverage issues, we cannot detect the
entities that are unpopular. Last, feature represen-
tation for a question can fail when the question
consists of rare n-grams.
The two training sets shown in Section 3.2 are
complementary: QP pairs provide more oppor-
tunities for us to learn the semantic associations
between interrogative words and predicates. Such
resources are especially important for understand-
ing NL-questions, as most of them start with such
5W1H words; on the other hand, MP pairs en-
rich the semantic associations between context in-
formation (n-gram features) and predicates.
</bodyText>
<sectionHeader confidence="0.999233" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.931293192307692">
In this paper, we propose a novel method that
transforms NL-questions into their corresponding
logical forms using joint relational embeddings.
We also built a simple and robust KB-QA system
based on only the learned embeddings. Such em-
beddings learn the semantic associations between
natural language statements and KB-properties
from NLE-KB pairs that are automatically ex-
tracted from English WIKIPEDIA using KB-triples
with weak supervision. Then, we generate all pos-
sible structured queries derived from latent logical
features of the given NL-question, and rank them
based on the similarity scores between those re-
lational attributes. The experimental results show
that our method outperforms the latest three KB-
QA baseline systems. For our future work, we will
build concept-level context embeddings by lever-
aging latent meanings of NLEs rather than their
surface n-grams with the aligned logical features
on KB.
Acknowledgement This research was sup-
ported by the Next-Generation Information
Computing Development Program through the
National Research Foundation of Korea (NRF)
funded by the Ministry of Science, ICT &amp; Future
Planning (NRF-2012M3C4A7033344).
</bodyText>
<figure confidence="0.9561952">
Methods
Our method
w/o T-P
w/o C-T
w/o C-P
</figure>
<page confidence="0.993336">
649
</page>
<sectionHeader confidence="0.989355" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999648519230769">
Junwei Bao, Nan Duan, Ming Zhou, and Tiejun Zhao.
2014. Knowledge-based question answering as ma-
chine translation. Proceedings of the 52nd Annual
Meeting of the Association for Computational Lin-
guistics, pages 967–976. Association for Computa-
tional Linguistics.
Jonathan Berant and Percy Liang. 2014. Seman-
tic parsing via paraphrasing. Proceedings of the
52nd Annual Meeting of the Association for Com-
putational Linguistics, pages 1415–1425. Associa-
tion for Computational Linguistics.
Jonathan Berant, Andrew Chou, Roy Frostig, and Percy
Liang. 2013. Semantic parsing on Freebase from
question-answer pairs. In Proceedings of the 2013
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1533–1544, Seattle, Wash-
ington, USA, October. Association for Computa-
tional Linguistics.
Antoine Bordes, Xavier Glorot, Jason Weston, and
Yoshua Bengio. 2012. Joint learning of words
and meaning representations for open-text seman-
tic parsing. In In Proceedings of 15th International
Conference on Artificial Intelligence and Statistics.
Qingqing Cai and Alexander Yates. 2013. Large-scale
semantic parsing via schema matching and lexicon
extension. In Association for Computational Lin-
guistics (ACL), pages 423–433. The Association for
Computer Linguistics.
Kenneth Ward Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lexicog-
raphy. Comput. Linguist., 16(1):22–29, March.
Anthony Fader, Luke S. Zettlemoyer, and Oren Etzioni.
2013. Paraphrase-driven learning for open question
answering. In Association for Computational Lin-
guistics (ACL), pages 1608–1618. The Association
for Computer Linguistics.
Daniel Gerber and Axel-Cyrille Ngonga Ngomo. 2011.
Bootstrapping the linked data web. In 1st Workshop
on Web Scale Knowledge Extraction @ ISWC 2011.
E. Kreyszig. 1979. Advanced Engineering Mathemat-
ics. Wiley.
Jayant Krishnamurthy and Tom M. Mitchell. 2012.
Weakly supervised training of semantic parsers. In
Proceedings of the 2012 Joint Conference on Em-
pirical Methods in Natural Language Processing
and Computational Natural Language Learning,
EMNLP-CoNLL ’12, pages 754–765, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Tom Kwiatkowski, Eunsol Choi, Yoav Artzi, and Luke
Zettlemoyer. 2013. Scaling semantic parsers with
on-the-fly ontology matching. In Proceedings of
the 2013 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1545–1556, Seattle,
Washington, USA, October. Association for Compu-
tational Linguistics.
Percy Liang, Michael I. Jordan, and Dan Klein. 2011.
Learning dependency-based compositional seman-
tics. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies - Volume 1, HLT ’11,
pages 590–599, Stroudsburg, PA, USA. Association
for Computational Linguistics.
RaymondJ. Mooney. 2007. Learning for semantic
parsing. In Alexander Gelbukh, editor, Computa-
tional Linguistics and Intelligent Text Processing,
volume 4394 of Lecture Notes in Computer Science,
pages 311–324. Springer Berlin Heidelberg.
Ndapandula Nakashole, Gerhard Weikum, and Fabian
Suchanek. 2012. Patty: A taxonomy of relational
patterns with semantic types. In Proceedings of
the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, EMNLP-CoNLL ’12,
pages 1135–1145, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Jason Weston, Samy Bengio, and Nicolas Usunier.
2010. Large scale image annotation: Learning to
rank with joint word-image embeddings. Machine
Learning, 81(1):21–35, October.
Jason Weston, Antoine Bordes, Oksana Yakhnenko,
and Nicolas Usunier. 2013. Connecting language
and knowledge bases with embedding models for re-
lation extraction. In Proceedings of the 2013 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1366–1371, Seattle, Washington,
USA, October. Association for Computational Lin-
guistics.
Xuchen Yao and Benjamin Van Durme. 2014. Infor-
mation extraction over structured data: Question an-
swering with freebase. In Proceedings of the 52nd
Annual Meeting of the Association for Computa-
tional Linguistics, pages 956–966, Baltimore, Mary-
land, June. Association for Computational Linguis-
tics.
John M. Zelle and Raymond J. Mooney. 1996. Learn-
ing to parse database queries using inductive logic
programming. In Proceedings of the Thirteenth Na-
tional Conference on Artificial Intelligence - Volume
2, AAAI’96, pages 1050–1055. AAAI Press.
Luke S. Zettlemoyer and Michael Collins. 2005.
Learning to map sentences to logical form: Struc-
tured classification with probabilistic categorial
grammars. In UAI, pages 658–666. AUAI Press.
</reference>
<page confidence="0.997966">
650
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.706446">
<title confidence="0.964788">Joint Relational Embeddings for Knowledge-based Question Answering</title>
<author confidence="0.804411">Korea University Engineering</author>
<author confidence="0.804411">South Seoul</author>
<affiliation confidence="0.809343">Research Asia, Beijing,</affiliation>
<email confidence="0.991909">rim@nlp.korea.ac.kr</email>
<abstract confidence="0.999216388888889">Transforming a natural language (NL) question into a corresponding logical form (LF) is central to the knowledge-based question answering (KB-QA) task. Unlike most previous methods that achieve this goal based on mappings between lexicalized phrases and logical predicates, this paper goes one step further and proposes a novel embedding-based approach that maps NL-questions into LFs for KB- QA by leveraging semantic associations between lexical representations and KBproperties in the latent space. Experimental results demonstrate that our proposed method outperforms three KB-QA baseline methods on two publicly released QA data sets.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Junwei Bao</author>
<author>Nan Duan</author>
<author>Ming Zhou</author>
<author>Tiejun Zhao</author>
</authors>
<title>Knowledge-based question answering as machine translation.</title>
<date>2014</date>
<booktitle>Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>967--976</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1478" citStr="Bao et al., 2014" startWordPosition="211" endWordPosition="214">that our proposed method outperforms three KB-QA baseline methods on two publicly released QA data sets. 1 Introduction Knowledge-based question answering (KB-QA) involves answering questions posed in natural language (NL) using existing knowledge bases (KBs). As most KBs are structured databases, how to transform the input question into its corresponding structured query for KB (KB-query) as a logical form (LF), also known as semantic parsing, is the central task for KB-QA systems. Previous works (Mooney, 2007; Liang et al., 2011; Cai and Yates, 2013; Fader et al., 2013; Berant et al., 2013; Bao et al., 2014) usually leveraged mappings between NL phrases and logical predicates as lexical triggers to perform transformation tasks in semantic parsing, but they had to deal with two limitations: (i) as the meaning of a logical predicate often has different natural language expression (NLE) forms, the lexical triggers extracted for a predicate may at times are limited in size; (ii) entities detected by the named entity recognition (NER) component will be used to compose the logical forms together with the logical predicates, so their types should be consistent with the predicates as well. However, most </context>
<context position="5350" citStr="Bao et al., 2014" startWordPosition="805" endWordPosition="808">al learning method (Weston et al., 2010). Their inputs are often statement sentences which include subject and object entities for a given predicate, whereas NLquestions lack either a subject or object entity that is the potential answer. Hence, we can only use the information of a subject or object entity, which leads to a different training instance generation procedure and a different training criterion. Recently, researchers have developed open domain systems based on large scale KBs such as FREEBASE1 (Cai and Yates, 2013; Fader et al., 2013; Berant et al., 2013; Kwiatkowski et al., 2013; Bao et al., 2014; Berant and Liang, 2014; Yao and Van Durme, 2014). Their semantic parsers for Open QA are unified formal and scalable: they enable the NL-question to be mapped into the appropriate logical form. Our method obtains similar logical forms, but using only lowdimensional embeddings of n-grams, entity types, and predicates learned from texts and KB. 3 Setup 3.1 Relational Components for KB-QA Our method learns semantic mappings between NLEs and the KB2 based on the paired relationships of the following three components: C denotes a set of bag-of-words (or n-grams) as context features (c) for NLEs t</context>
<context position="9429" citStr="Bao et al., 2014" startWordPosition="1464" endWordPosition="1467">and Ngonga Ngomo, 2011), but we focus on the correlation of shared entity pairs between relation mentions and predicates using the PMI computation. &lt;Question Pattern, Predicate&gt; Pair (2P) Since WIKIPEDIA articles have no information to leverage interrogative features which highly depend on the object entity (answer), it is difficult to distinguish some questions that are composed of only different SW1H words, e.g., {When|Where} was Barack Obama born? Hence, we used the method of collecting question patterns with human labeled predicates that are restricted by the set of predicates used in MP (Bao et al., 2014). 4 Embedding-based KB-QA Our task is as follows. First, our model learns the semantic associations of C-T, C-P, and T-P (Section 3.1) based on NLE-KB pairs (Section 3.2), and then predicts the semantic-related KB-query which can directly find the answer to a given NLquestion. For our feature space, given an NLE-KB pair, the NLE (relation mention in MP or question pattern in 2P) is decomposed into n-gram features: C = {c |c is a segment of NLE}, and the KB-properties are represented by entity type t of entitysubj and predicate p. Then we can obtain a training triplet w = [C, t, p]. Each featur</context>
<context position="16734" citStr="Bao et al., 2014" startWordPosition="2710" endWordPosition="2713">0. To make the decoding procedure computable, we kept only the popular KB-entity in the dictionary to map different entity mentions into a KB-entity. We used two publicly released data sets for QA evaluations: Free917 (Cai and Yates, 2013) includes the annotated lambda calculus forms for each question, and covers 81 domains and 635 Freebase relations; WebQ. (Berant et al., 2013) provides 5,810 question-answer pairs that are built by collecting common questions from Web-query logs and by manually labeling answers. We used the previous three approaches (Cai and Yates, 2013; Berant et al., 2013; Bao et al., 2014) as our baselines. Experimental Results Table 3 reports the overall performances of our proposed KB-QA method on the two evaluation data sets and compares them with those of the three baselines. Note that we did not re-implement the baseline systems, but just borrowed the evaluation results reported in their 648 Methods Free917 WebQ. Cai and Yates (2013) 59.00% N/A Berant et al. (2013) 62.00% 31.40% Bao et al. (2014) N/A 37.50% Our method 71.38% 41.34% Table 3: Accuracy on the evaluation data Free917 WebQ. 71.38% 41.34% 70.65% 40.55% 67.03% 38.44% 31.16% 19.24% Table 4: Ablation of the relatio</context>
</contexts>
<marker>Bao, Duan, Zhou, Zhao, 2014</marker>
<rawString>Junwei Bao, Nan Duan, Ming Zhou, and Tiejun Zhao. 2014. Knowledge-based question answering as machine translation. Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 967–976. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Berant</author>
<author>Percy Liang</author>
</authors>
<title>Semantic parsing via paraphrasing.</title>
<date>2014</date>
<booktitle>Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1415--1425</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="5374" citStr="Berant and Liang, 2014" startWordPosition="809" endWordPosition="812"> (Weston et al., 2010). Their inputs are often statement sentences which include subject and object entities for a given predicate, whereas NLquestions lack either a subject or object entity that is the potential answer. Hence, we can only use the information of a subject or object entity, which leads to a different training instance generation procedure and a different training criterion. Recently, researchers have developed open domain systems based on large scale KBs such as FREEBASE1 (Cai and Yates, 2013; Fader et al., 2013; Berant et al., 2013; Kwiatkowski et al., 2013; Bao et al., 2014; Berant and Liang, 2014; Yao and Van Durme, 2014). Their semantic parsers for Open QA are unified formal and scalable: they enable the NL-question to be mapped into the appropriate logical form. Our method obtains similar logical forms, but using only lowdimensional embeddings of n-grams, entity types, and predicates learned from texts and KB. 3 Setup 3.1 Relational Components for KB-QA Our method learns semantic mappings between NLEs and the KB2 based on the paired relationships of the following three components: C denotes a set of bag-of-words (or n-grams) as context features (c) for NLEs that are the lexical repr</context>
</contexts>
<marker>Berant, Liang, 2014</marker>
<rawString>Jonathan Berant and Percy Liang. 2014. Semantic parsing via paraphrasing. Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1415–1425. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Berant</author>
<author>Andrew Chou</author>
<author>Roy Frostig</author>
<author>Percy Liang</author>
</authors>
<title>Semantic parsing on Freebase from question-answer pairs.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1533--1544</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Seattle, Washington, USA,</location>
<contexts>
<context position="1459" citStr="Berant et al., 2013" startWordPosition="207" endWordPosition="210"> results demonstrate that our proposed method outperforms three KB-QA baseline methods on two publicly released QA data sets. 1 Introduction Knowledge-based question answering (KB-QA) involves answering questions posed in natural language (NL) using existing knowledge bases (KBs). As most KBs are structured databases, how to transform the input question into its corresponding structured query for KB (KB-query) as a logical form (LF), also known as semantic parsing, is the central task for KB-QA systems. Previous works (Mooney, 2007; Liang et al., 2011; Cai and Yates, 2013; Fader et al., 2013; Berant et al., 2013; Bao et al., 2014) usually leveraged mappings between NL phrases and logical predicates as lexical triggers to perform transformation tasks in semantic parsing, but they had to deal with two limitations: (i) as the meaning of a logical predicate often has different natural language expression (NLE) forms, the lexical triggers extracted for a predicate may at times are limited in size; (ii) entities detected by the named entity recognition (NER) component will be used to compose the logical forms together with the logical predicates, so their types should be consistent with the predicates as w</context>
<context position="5306" citStr="Berant et al., 2013" startWordPosition="797" endWordPosition="800"> connect free texts with KBs using the relational learning method (Weston et al., 2010). Their inputs are often statement sentences which include subject and object entities for a given predicate, whereas NLquestions lack either a subject or object entity that is the potential answer. Hence, we can only use the information of a subject or object entity, which leads to a different training instance generation procedure and a different training criterion. Recently, researchers have developed open domain systems based on large scale KBs such as FREEBASE1 (Cai and Yates, 2013; Fader et al., 2013; Berant et al., 2013; Kwiatkowski et al., 2013; Bao et al., 2014; Berant and Liang, 2014; Yao and Van Durme, 2014). Their semantic parsers for Open QA are unified formal and scalable: they enable the NL-question to be mapped into the appropriate logical form. Our method obtains similar logical forms, but using only lowdimensional embeddings of n-grams, entity types, and predicates learned from texts and KB. 3 Setup 3.1 Relational Components for KB-QA Our method learns semantic mappings between NLEs and the KB2 based on the paired relationships of the following three components: C denotes a set of bag-of-words (or</context>
<context position="16498" citStr="Berant et al., 2013" startWordPosition="2673" endWordPosition="2676">3.0 for Equation 1, which leverages the redundancy information in the large-scale data (WIKIPEDIA). For our embedding learning, we set the embedding dimension n to 100, the learning rate (A) for SGD to 0.0001, and the iteration number to 30. To make the decoding procedure computable, we kept only the popular KB-entity in the dictionary to map different entity mentions into a KB-entity. We used two publicly released data sets for QA evaluations: Free917 (Cai and Yates, 2013) includes the annotated lambda calculus forms for each question, and covers 81 domains and 635 Freebase relations; WebQ. (Berant et al., 2013) provides 5,810 question-answer pairs that are built by collecting common questions from Web-query logs and by manually labeling answers. We used the previous three approaches (Cai and Yates, 2013; Berant et al., 2013; Bao et al., 2014) as our baselines. Experimental Results Table 3 reports the overall performances of our proposed KB-QA method on the two evaluation data sets and compares them with those of the three baselines. Note that we did not re-implement the baseline systems, but just borrowed the evaluation results reported in their 648 Methods Free917 WebQ. Cai and Yates (2013) 59.00% </context>
</contexts>
<marker>Berant, Chou, Frostig, Liang, 2013</marker>
<rawString>Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. 2013. Semantic parsing on Freebase from question-answer pairs. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1533–1544, Seattle, Washington, USA, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antoine Bordes</author>
<author>Xavier Glorot</author>
<author>Jason Weston</author>
<author>Yoshua Bengio</author>
</authors>
<title>Joint learning of words and meaning representations for open-text semantic parsing. In</title>
<date>2012</date>
<booktitle>In Proceedings of 15th International Conference on Artificial Intelligence and Statistics.</booktitle>
<contexts>
<context position="4631" citStr="Bordes et al. (2012)" startWordPosition="686" endWordPosition="689">ion&gt; pairs for lexical trigger extraction and model training. Due to the data annotation requirement, such methods are usually restricted to specific domains, and struggle with the coverage issue caused by the limited size of lexical triggers. Studies on weakly supervised semantic parsers have tried to reduce the amount of human supervision by using question-answer pairs (Liang et al., 2011) or distant supervision (Krishnamurthy and Mitchell, 2012) instead of full semantic annotations. Still, for KB-QA, the question of how to leverage KB-properties and analyze the question structures remains. Bordes et al. (2012) and Weston et al. (2013) designed embedding models that connect free texts with KBs using the relational learning method (Weston et al., 2010). Their inputs are often statement sentences which include subject and object entities for a given predicate, whereas NLquestions lack either a subject or object entity that is the potential answer. Hence, we can only use the information of a subject or object entity, which leads to a different training instance generation procedure and a different training criterion. Recently, researchers have developed open domain systems based on large scale KBs such</context>
</contexts>
<marker>Bordes, Glorot, Weston, Bengio, 2012</marker>
<rawString>Antoine Bordes, Xavier Glorot, Jason Weston, and Yoshua Bengio. 2012. Joint learning of words and meaning representations for open-text semantic parsing. In In Proceedings of 15th International Conference on Artificial Intelligence and Statistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qingqing Cai</author>
<author>Alexander Yates</author>
</authors>
<title>Large-scale semantic parsing via schema matching and lexicon extension.</title>
<date>2013</date>
<booktitle>In Association for Computational Linguistics (ACL),</booktitle>
<pages>423--433</pages>
<institution>The Association for Computer Linguistics.</institution>
<contexts>
<context position="1418" citStr="Cai and Yates, 2013" startWordPosition="199" endWordPosition="202">perties in the latent space. Experimental results demonstrate that our proposed method outperforms three KB-QA baseline methods on two publicly released QA data sets. 1 Introduction Knowledge-based question answering (KB-QA) involves answering questions posed in natural language (NL) using existing knowledge bases (KBs). As most KBs are structured databases, how to transform the input question into its corresponding structured query for KB (KB-query) as a logical form (LF), also known as semantic parsing, is the central task for KB-QA systems. Previous works (Mooney, 2007; Liang et al., 2011; Cai and Yates, 2013; Fader et al., 2013; Berant et al., 2013; Bao et al., 2014) usually leveraged mappings between NL phrases and logical predicates as lexical triggers to perform transformation tasks in semantic parsing, but they had to deal with two limitations: (i) as the meaning of a logical predicate often has different natural language expression (NLE) forms, the lexical triggers extracted for a predicate may at times are limited in size; (ii) entities detected by the named entity recognition (NER) component will be used to compose the logical forms together with the logical predicates, so their types shou</context>
<context position="5265" citStr="Cai and Yates, 2013" startWordPosition="789" endWordPosition="792">al. (2013) designed embedding models that connect free texts with KBs using the relational learning method (Weston et al., 2010). Their inputs are often statement sentences which include subject and object entities for a given predicate, whereas NLquestions lack either a subject or object entity that is the potential answer. Hence, we can only use the information of a subject or object entity, which leads to a different training instance generation procedure and a different training criterion. Recently, researchers have developed open domain systems based on large scale KBs such as FREEBASE1 (Cai and Yates, 2013; Fader et al., 2013; Berant et al., 2013; Kwiatkowski et al., 2013; Bao et al., 2014; Berant and Liang, 2014; Yao and Van Durme, 2014). Their semantic parsers for Open QA are unified formal and scalable: they enable the NL-question to be mapped into the appropriate logical form. Our method obtains similar logical forms, but using only lowdimensional embeddings of n-grams, entity types, and predicates learned from texts and KB. 3 Setup 3.1 Relational Components for KB-QA Our method learns semantic mappings between NLEs and the KB2 based on the paired relationships of the following three compon</context>
<context position="16356" citStr="Cai and Yates, 2013" startWordPosition="2650" endWordPosition="2653">that the quality of QP pairs is good, mainly due to human efforts. Also, we obtained MP pairs that have an acceptable quality using threshold 3.0 for Equation 1, which leverages the redundancy information in the large-scale data (WIKIPEDIA). For our embedding learning, we set the embedding dimension n to 100, the learning rate (A) for SGD to 0.0001, and the iteration number to 30. To make the decoding procedure computable, we kept only the popular KB-entity in the dictionary to map different entity mentions into a KB-entity. We used two publicly released data sets for QA evaluations: Free917 (Cai and Yates, 2013) includes the annotated lambda calculus forms for each question, and covers 81 domains and 635 Freebase relations; WebQ. (Berant et al., 2013) provides 5,810 question-answer pairs that are built by collecting common questions from Web-query logs and by manually labeling answers. We used the previous three approaches (Cai and Yates, 2013; Berant et al., 2013; Bao et al., 2014) as our baselines. Experimental Results Table 3 reports the overall performances of our proposed KB-QA method on the two evaluation data sets and compares them with those of the three baselines. Note that we did not re-imp</context>
</contexts>
<marker>Cai, Yates, 2013</marker>
<rawString>Qingqing Cai and Alexander Yates. 2013. Large-scale semantic parsing via schema matching and lexicon extension. In Association for Computational Linguistics (ACL), pages 423–433. The Association for Computer Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Ward Church</author>
<author>Patrick Hanks</author>
</authors>
<title>Word association norms, mutual information, and lexicography.</title>
<date>1990</date>
<journal>Comput. Linguist.,</journal>
<volume>16</volume>
<issue>1</issue>
<contexts>
<context position="8626" citStr="Church and Hanks, 1990" startWordPosition="1331" endWordPosition="1334">at occurred in the KB-triple is likely to express the predicate (p) of that triple. With obtaining high-quality MP pairs, we kept only relation mentions that were highly associated with a predicate measured by the scoring function: S(m, p) = PMI(em; ep) + PMI(um; up) (1) where ex is the set of total pairs of both-side entities of entry x (m or p) and ux is the set of unique (distinct) pairs of both-side entities of entry x. In this case, the both-side entities indicate entitysubj and entityobj. For a frequencybased probability, PMI(x; y) = log P (x,y) P (x)P (y) 3http://en.wikipedia.org/ 646 (Church and Hanks, 1990) can be re-written as PMI(x; y) = log |x � y|·C |x|·|y |, where C denotes the total number of items shown in the corpus. The function is partially derived from the support score (Gerber and Ngonga Ngomo, 2011), but we focus on the correlation of shared entity pairs between relation mentions and predicates using the PMI computation. &lt;Question Pattern, Predicate&gt; Pair (2P) Since WIKIPEDIA articles have no information to leverage interrogative features which highly depend on the object entity (answer), it is difficult to distinguish some questions that are composed of only different SW1H words, e</context>
</contexts>
<marker>Church, Hanks, 1990</marker>
<rawString>Kenneth Ward Church and Patrick Hanks. 1990. Word association norms, mutual information, and lexicography. Comput. Linguist., 16(1):22–29, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anthony Fader</author>
<author>Luke S Zettlemoyer</author>
<author>Oren Etzioni</author>
</authors>
<title>Paraphrase-driven learning for open question answering.</title>
<date>2013</date>
<booktitle>In Association for Computational Linguistics (ACL),</booktitle>
<pages>1608--1618</pages>
<institution>The Association for Computer Linguistics.</institution>
<contexts>
<context position="1438" citStr="Fader et al., 2013" startWordPosition="203" endWordPosition="206"> space. Experimental results demonstrate that our proposed method outperforms three KB-QA baseline methods on two publicly released QA data sets. 1 Introduction Knowledge-based question answering (KB-QA) involves answering questions posed in natural language (NL) using existing knowledge bases (KBs). As most KBs are structured databases, how to transform the input question into its corresponding structured query for KB (KB-query) as a logical form (LF), also known as semantic parsing, is the central task for KB-QA systems. Previous works (Mooney, 2007; Liang et al., 2011; Cai and Yates, 2013; Fader et al., 2013; Berant et al., 2013; Bao et al., 2014) usually leveraged mappings between NL phrases and logical predicates as lexical triggers to perform transformation tasks in semantic parsing, but they had to deal with two limitations: (i) as the meaning of a logical predicate often has different natural language expression (NLE) forms, the lexical triggers extracted for a predicate may at times are limited in size; (ii) entities detected by the named entity recognition (NER) component will be used to compose the logical forms together with the logical predicates, so their types should be consistent wit</context>
<context position="5285" citStr="Fader et al., 2013" startWordPosition="793" endWordPosition="796">mbedding models that connect free texts with KBs using the relational learning method (Weston et al., 2010). Their inputs are often statement sentences which include subject and object entities for a given predicate, whereas NLquestions lack either a subject or object entity that is the potential answer. Hence, we can only use the information of a subject or object entity, which leads to a different training instance generation procedure and a different training criterion. Recently, researchers have developed open domain systems based on large scale KBs such as FREEBASE1 (Cai and Yates, 2013; Fader et al., 2013; Berant et al., 2013; Kwiatkowski et al., 2013; Bao et al., 2014; Berant and Liang, 2014; Yao and Van Durme, 2014). Their semantic parsers for Open QA are unified formal and scalable: they enable the NL-question to be mapped into the appropriate logical form. Our method obtains similar logical forms, but using only lowdimensional embeddings of n-grams, entity types, and predicates learned from texts and KB. 3 Setup 3.1 Relational Components for KB-QA Our method learns semantic mappings between NLEs and the KB2 based on the paired relationships of the following three components: C denotes a se</context>
</contexts>
<marker>Fader, Zettlemoyer, Etzioni, 2013</marker>
<rawString>Anthony Fader, Luke S. Zettlemoyer, and Oren Etzioni. 2013. Paraphrase-driven learning for open question answering. In Association for Computational Linguistics (ACL), pages 1608–1618. The Association for Computer Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gerber</author>
<author>Axel-Cyrille Ngonga Ngomo</author>
</authors>
<title>Bootstrapping the linked data web.</title>
<date>2011</date>
<booktitle>In 1st Workshop on Web Scale Knowledge Extraction @ ISWC</booktitle>
<marker>Gerber, Ngomo, 2011</marker>
<rawString>Daniel Gerber and Axel-Cyrille Ngonga Ngomo. 2011. Bootstrapping the linked data web. In 1st Workshop on Web Scale Knowledge Extraction @ ISWC 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Kreyszig</author>
</authors>
<title>Advanced Engineering Mathematics.</title>
<date>1979</date>
<publisher>Wiley.</publisher>
<contexts>
<context position="13964" citStr="Kreyszig, 1979" startWordPosition="2252" endWordPosition="2253">�, tq�, pq�] which is directly linked to a KB-query kq� = [sq�, pq�, ?]. The second stage involves ranking candidate KB-queries based on the similarity scores between the following paired relationships from the triplet wq� : Rq � = {Cq�-tq�, Cq�-pq�, tq�-pq�}. Unlike in the training step, the similarities of Cq�-tq � and Cq�-pq� are computed by summation of all pairwise elements (each context embedding E(c), not E(C), with each paired E(t) or E(p)) for a more precise measurement. Since similarites of Rq are calculated on different scales, we normalize each value using Z-score (Z(x) = ms−µ) Q (Kreyszig, 1979). The final score is measured by: 5imq2k(q, kq) = � Z(5im(r)) (5) rERq Then, given any NL-question q, we can predict the corresponding KB-query ˆk(q): ˆk(q) = arg max 5imq2k(q, k) (6) kEKq Last, we can retrieve an answer from the KB using a structured query ˆk(q). Table 1 shows an example of our decoding process. Multi-related Question Some questions include two-subject entities, both of which are crucial to understanding the question. For the question who plays gandalf in the lord of the rings? Gandalf (character) and The Lord Of The Rings (film) are explicit entities that should be joined to</context>
</contexts>
<marker>Kreyszig, 1979</marker>
<rawString>E. Kreyszig. 1979. Advanced Engineering Mathematics. Wiley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jayant Krishnamurthy</author>
<author>Tom M Mitchell</author>
</authors>
<title>Weakly supervised training of semantic parsers.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLP-CoNLL ’12,</booktitle>
<pages>754--765</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="4463" citStr="Krishnamurthy and Mitchell, 2012" startWordPosition="660" endWordPosition="663"> 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 645–650, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics notation&gt; pairs for lexical trigger extraction and model training. Due to the data annotation requirement, such methods are usually restricted to specific domains, and struggle with the coverage issue caused by the limited size of lexical triggers. Studies on weakly supervised semantic parsers have tried to reduce the amount of human supervision by using question-answer pairs (Liang et al., 2011) or distant supervision (Krishnamurthy and Mitchell, 2012) instead of full semantic annotations. Still, for KB-QA, the question of how to leverage KB-properties and analyze the question structures remains. Bordes et al. (2012) and Weston et al. (2013) designed embedding models that connect free texts with KBs using the relational learning method (Weston et al., 2010). Their inputs are often statement sentences which include subject and object entities for a given predicate, whereas NLquestions lack either a subject or object entity that is the potential answer. Hence, we can only use the information of a subject or object entity, which leads to a dif</context>
</contexts>
<marker>Krishnamurthy, Mitchell, 2012</marker>
<rawString>Jayant Krishnamurthy and Tom M. Mitchell. 2012. Weakly supervised training of semantic parsers. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLP-CoNLL ’12, pages 754–765, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom Kwiatkowski</author>
<author>Eunsol Choi</author>
<author>Yoav Artzi</author>
<author>Luke Zettlemoyer</author>
</authors>
<title>Scaling semantic parsers with on-the-fly ontology matching.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1545--1556</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Seattle, Washington, USA,</location>
<contexts>
<context position="5332" citStr="Kwiatkowski et al., 2013" startWordPosition="801" endWordPosition="804">ith KBs using the relational learning method (Weston et al., 2010). Their inputs are often statement sentences which include subject and object entities for a given predicate, whereas NLquestions lack either a subject or object entity that is the potential answer. Hence, we can only use the information of a subject or object entity, which leads to a different training instance generation procedure and a different training criterion. Recently, researchers have developed open domain systems based on large scale KBs such as FREEBASE1 (Cai and Yates, 2013; Fader et al., 2013; Berant et al., 2013; Kwiatkowski et al., 2013; Bao et al., 2014; Berant and Liang, 2014; Yao and Van Durme, 2014). Their semantic parsers for Open QA are unified formal and scalable: they enable the NL-question to be mapped into the appropriate logical form. Our method obtains similar logical forms, but using only lowdimensional embeddings of n-grams, entity types, and predicates learned from texts and KB. 3 Setup 3.1 Relational Components for KB-QA Our method learns semantic mappings between NLEs and the KB2 based on the paired relationships of the following three components: C denotes a set of bag-of-words (or n-grams) as context featu</context>
</contexts>
<marker>Kwiatkowski, Choi, Artzi, Zettlemoyer, 2013</marker>
<rawString>Tom Kwiatkowski, Eunsol Choi, Yoav Artzi, and Luke Zettlemoyer. 2013. Scaling semantic parsers with on-the-fly ontology matching. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1545–1556, Seattle, Washington, USA, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Michael I Jordan</author>
<author>Dan Klein</author>
</authors>
<title>Learning dependency-based compositional semantics.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1, HLT ’11,</booktitle>
<pages>590--599</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1397" citStr="Liang et al., 2011" startWordPosition="195" endWordPosition="198">sentations and KBproperties in the latent space. Experimental results demonstrate that our proposed method outperforms three KB-QA baseline methods on two publicly released QA data sets. 1 Introduction Knowledge-based question answering (KB-QA) involves answering questions posed in natural language (NL) using existing knowledge bases (KBs). As most KBs are structured databases, how to transform the input question into its corresponding structured query for KB (KB-query) as a logical form (LF), also known as semantic parsing, is the central task for KB-QA systems. Previous works (Mooney, 2007; Liang et al., 2011; Cai and Yates, 2013; Fader et al., 2013; Berant et al., 2013; Bao et al., 2014) usually leveraged mappings between NL phrases and logical predicates as lexical triggers to perform transformation tasks in semantic parsing, but they had to deal with two limitations: (i) as the meaning of a logical predicate often has different natural language expression (NLE) forms, the lexical triggers extracted for a predicate may at times are limited in size; (ii) entities detected by the named entity recognition (NER) component will be used to compose the logical forms together with the logical predicates</context>
<context position="4405" citStr="Liang et al., 2011" startWordPosition="653" endWordPosition="656">&lt;sentence, semantic an645 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 645–650, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics notation&gt; pairs for lexical trigger extraction and model training. Due to the data annotation requirement, such methods are usually restricted to specific domains, and struggle with the coverage issue caused by the limited size of lexical triggers. Studies on weakly supervised semantic parsers have tried to reduce the amount of human supervision by using question-answer pairs (Liang et al., 2011) or distant supervision (Krishnamurthy and Mitchell, 2012) instead of full semantic annotations. Still, for KB-QA, the question of how to leverage KB-properties and analyze the question structures remains. Bordes et al. (2012) and Weston et al. (2013) designed embedding models that connect free texts with KBs using the relational learning method (Weston et al., 2010). Their inputs are often statement sentences which include subject and object entities for a given predicate, whereas NLquestions lack either a subject or object entity that is the potential answer. Hence, we can only use the infor</context>
</contexts>
<marker>Liang, Jordan, Klein, 2011</marker>
<rawString>Percy Liang, Michael I. Jordan, and Dan Klein. 2011. Learning dependency-based compositional semantics. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1, HLT ’11, pages 590–599, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mooney</author>
</authors>
<title>Learning for semantic parsing.</title>
<date>2007</date>
<booktitle>Computational Linguistics and Intelligent Text Processing,</booktitle>
<volume>4394</volume>
<pages>311--324</pages>
<editor>In Alexander Gelbukh, editor,</editor>
<publisher>Springer</publisher>
<location>Berlin Heidelberg.</location>
<contexts>
<context position="1377" citStr="Mooney, 2007" startWordPosition="193" endWordPosition="194"> lexical representations and KBproperties in the latent space. Experimental results demonstrate that our proposed method outperforms three KB-QA baseline methods on two publicly released QA data sets. 1 Introduction Knowledge-based question answering (KB-QA) involves answering questions posed in natural language (NL) using existing knowledge bases (KBs). As most KBs are structured databases, how to transform the input question into its corresponding structured query for KB (KB-query) as a logical form (LF), also known as semantic parsing, is the central task for KB-QA systems. Previous works (Mooney, 2007; Liang et al., 2011; Cai and Yates, 2013; Fader et al., 2013; Berant et al., 2013; Bao et al., 2014) usually leveraged mappings between NL phrases and logical predicates as lexical triggers to perform transformation tasks in semantic parsing, but they had to deal with two limitations: (i) as the meaning of a logical predicate often has different natural language expression (NLE) forms, the lexical triggers extracted for a predicate may at times are limited in size; (ii) entities detected by the named entity recognition (NER) component will be used to compose the logical forms together with th</context>
<context position="3765" citStr="Mooney, 2007" startWordPosition="560" endWordPosition="561">he embeddings of logical features in the structured queries. Last, answers are retrieved from the KB using the selected LFs. The contributions of this work are two-fold: (1) as a smoothing technique, the low-dimensional embeddings can alleviate the coverage issues of lexical triggers; (2) our joint approach integrates entity span selection and predicate mapping tasks for KB-QA. For this we built independent entity embeddings as the additional component, solving the entity disambiguation problem. 2 Related Work Supervised semantic parsers (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Mooney, 2007) heavily rely on the &lt;sentence, semantic an645 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 645–650, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics notation&gt; pairs for lexical trigger extraction and model training. Due to the data annotation requirement, such methods are usually restricted to specific domains, and struggle with the coverage issue caused by the limited size of lexical triggers. Studies on weakly supervised semantic parsers have tried to reduce the amount of human supervision by using qu</context>
</contexts>
<marker>Mooney, 2007</marker>
<rawString>RaymondJ. Mooney. 2007. Learning for semantic parsing. In Alexander Gelbukh, editor, Computational Linguistics and Intelligent Text Processing, volume 4394 of Lecture Notes in Computer Science, pages 311–324. Springer Berlin Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ndapandula Nakashole</author>
<author>Gerhard Weikum</author>
<author>Fabian Suchanek</author>
</authors>
<title>Patty: A taxonomy of relational patterns with semantic types.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLP-CoNLL ’12,</booktitle>
<pages>1135--1145</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="7527" citStr="Nakashole et al., 2012" startWordPosition="1152" endWordPosition="1156">ntext, which may solve the entity disambiguation problem in KB-QA; C-P can leverage the semantic overlap between question contexts (n-gram features) and logical predicates, which is important for mapping NL-questions to their corresponding predicates. 3.2 NLE-KB Pair Extraction This section describes how we extract the semantic associated pairs of NLE-entries and KB-triples to learn the relational embeddings (Section 4.1). &lt;Relation Mention, Predicate&gt; Pair (MP) Each relation mention denotes a lexical phrase of an existing KB-predicate. Following information extraction methods, such as PATTY (Nakashole et al., 2012), we extracted the &lt;relation mention, logical predicate&gt; pairs from English WIKIPEDIA3, which is closely connected to our KB, as follows: Given a KB-triple &lt;entitysubj, logical predicate, entityobj&gt;, we extracted NLEentries &lt;entitysubj, relation mention, entityobj&gt; where relation mention is the shortest path between entitysubj and entityobj in the dependency tree of sentences. The assumption is that any relation mention (m) in the NLE-entry containing such entity pairs that occurred in the KB-triple is likely to express the predicate (p) of that triple. With obtaining high-quality MP pairs, we</context>
</contexts>
<marker>Nakashole, Weikum, Suchanek, 2012</marker>
<rawString>Ndapandula Nakashole, Gerhard Weikum, and Fabian Suchanek. 2012. Patty: A taxonomy of relational patterns with semantic types. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLP-CoNLL ’12, pages 1135–1145, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Weston</author>
<author>Samy Bengio</author>
<author>Nicolas Usunier</author>
</authors>
<title>Large scale image annotation: Learning to rank with joint word-image embeddings.</title>
<date>2010</date>
<booktitle>Machine Learning,</booktitle>
<volume>81</volume>
<issue>1</issue>
<contexts>
<context position="4774" citStr="Weston et al., 2010" startWordPosition="710" endWordPosition="713">ecific domains, and struggle with the coverage issue caused by the limited size of lexical triggers. Studies on weakly supervised semantic parsers have tried to reduce the amount of human supervision by using question-answer pairs (Liang et al., 2011) or distant supervision (Krishnamurthy and Mitchell, 2012) instead of full semantic annotations. Still, for KB-QA, the question of how to leverage KB-properties and analyze the question structures remains. Bordes et al. (2012) and Weston et al. (2013) designed embedding models that connect free texts with KBs using the relational learning method (Weston et al., 2010). Their inputs are often statement sentences which include subject and object entities for a given predicate, whereas NLquestions lack either a subject or object entity that is the potential answer. Hence, we can only use the information of a subject or object entity, which leads to a different training instance generation procedure and a different training criterion. Recently, researchers have developed open domain systems based on large scale KBs such as FREEBASE1 (Cai and Yates, 2013; Fader et al., 2013; Berant et al., 2013; Kwiatkowski et al., 2013; Bao et al., 2014; Berant and Liang, 2014</context>
<context position="11341" citStr="Weston et al., 2010" startWordPosition="1797" endWordPosition="1800">he set of paired relationships R = {C-t, C-p, tp} that can be semantically leveraged. Formally, these features are embedded into the same latent space (En) and their semantic similarities can be computed by a dot product operation: Sim(a, b) = Sim(rab) = E(a)|E(b) (2) where rab denotes a paired relationship a-b (or (a, b)) in the above set R. We believe that our joint relational learning can smooth the surface (lexical) features for semantic parsing using the aligned entity and predicate. 4.1 Joint Relational Embedding Learning Our ranking-based relational learning is based on a ranking loss (Weston et al., 2010) that supports the idea that the similarity scores of observed pairs in the training set (positive instances) should be larger than those of any other pairs (negative instances): bi, by0 =� yi, Sim(xi, yi) &gt; 1+Sim(xi, y0) (3) More precisely, for each triplet wi = [Ci, ti, pi] obtained from an NLE-KB pair, the relationships Ri = {Ci-ti, Ci-pi, ti-pi} are trained under the soft ranking criterion, which conducts Stochastic Gradient Descent (SGD). We thus aim to minimize the following: bi, by0 =� yi, max(0,1−Sim(xi, yi)+Sim(xi, y0)) (4) Our learning strategy is as follows. First, we initialize emb</context>
</contexts>
<marker>Weston, Bengio, Usunier, 2010</marker>
<rawString>Jason Weston, Samy Bengio, and Nicolas Usunier. 2010. Large scale image annotation: Learning to rank with joint word-image embeddings. Machine Learning, 81(1):21–35, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Weston</author>
<author>Antoine Bordes</author>
<author>Oksana Yakhnenko</author>
<author>Nicolas Usunier</author>
</authors>
<title>Connecting language and knowledge bases with embedding models for relation extraction.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1366--1371</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Seattle, Washington, USA,</location>
<contexts>
<context position="4656" citStr="Weston et al. (2013)" startWordPosition="691" endWordPosition="694">igger extraction and model training. Due to the data annotation requirement, such methods are usually restricted to specific domains, and struggle with the coverage issue caused by the limited size of lexical triggers. Studies on weakly supervised semantic parsers have tried to reduce the amount of human supervision by using question-answer pairs (Liang et al., 2011) or distant supervision (Krishnamurthy and Mitchell, 2012) instead of full semantic annotations. Still, for KB-QA, the question of how to leverage KB-properties and analyze the question structures remains. Bordes et al. (2012) and Weston et al. (2013) designed embedding models that connect free texts with KBs using the relational learning method (Weston et al., 2010). Their inputs are often statement sentences which include subject and object entities for a given predicate, whereas NLquestions lack either a subject or object entity that is the potential answer. Hence, we can only use the information of a subject or object entity, which leads to a different training instance generation procedure and a different training criterion. Recently, researchers have developed open domain systems based on large scale KBs such as FREEBASE1 (Cai and Ya</context>
<context position="10423" citStr="Weston et al., 2013" startWordPosition="1643" endWordPosition="1646"> in 2P) is decomposed into n-gram features: C = {c |c is a segment of NLE}, and the KB-properties are represented by entity type t of entitysubj and predicate p. Then we can obtain a training triplet w = [C, t, p]. Each feature (c E C, t E T , p E P) is encoded in the distributed representation which is n-dimensional embedding vectors (En): bx, x encode ⇒ E(x) E En. All n-gram features (C) for an NLE are merged into one embedding vector to help speed up the learning process: E(C) = E c∈C E(c)/|C|. This feature representation is inspired by previous work in embedding-based relation extraction (Weston et al., 2013), but differs in the following ways: (1) entity information is represented on a separate embedding, but its positional information remains as symbol (entity); (2) when the vectors are combined, we use the average of each index to normalize features. For our joint relational approach, we focus on the set of paired relationships R = {C-t, C-p, tp} that can be semantically leveraged. Formally, these features are embedded into the same latent space (En) and their semantic similarities can be computed by a dot product operation: Sim(a, b) = Sim(rab) = E(a)|E(b) (2) where rab denotes a paired relati</context>
</contexts>
<marker>Weston, Bordes, Yakhnenko, Usunier, 2013</marker>
<rawString>Jason Weston, Antoine Bordes, Oksana Yakhnenko, and Nicolas Usunier. 2013. Connecting language and knowledge bases with embedding models for relation extraction. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1366–1371, Seattle, Washington, USA, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xuchen Yao</author>
<author>Benjamin Van Durme</author>
</authors>
<title>Information extraction over structured data: Question answering with freebase.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>956--966</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, Maryland,</location>
<marker>Yao, Van Durme, 2014</marker>
<rawString>Xuchen Yao and Benjamin Van Durme. 2014. Information extraction over structured data: Question answering with freebase. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 956–966, Baltimore, Maryland, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John M Zelle</author>
<author>Raymond J Mooney</author>
</authors>
<title>Learning to parse database queries using inductive logic programming.</title>
<date>1996</date>
<booktitle>In Proceedings of the Thirteenth National Conference on Artificial Intelligence - Volume 2, AAAI’96,</booktitle>
<pages>1050--1055</pages>
<publisher>AAAI Press.</publisher>
<contexts>
<context position="3719" citStr="Zelle and Mooney, 1996" startWordPosition="552" endWordPosition="555">of observed features (n-grams) in the NL-question and the embeddings of logical features in the structured queries. Last, answers are retrieved from the KB using the selected LFs. The contributions of this work are two-fold: (1) as a smoothing technique, the low-dimensional embeddings can alleviate the coverage issues of lexical triggers; (2) our joint approach integrates entity span selection and predicate mapping tasks for KB-QA. For this we built independent entity embeddings as the additional component, solving the entity disambiguation problem. 2 Related Work Supervised semantic parsers (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Mooney, 2007) heavily rely on the &lt;sentence, semantic an645 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 645–650, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics notation&gt; pairs for lexical trigger extraction and model training. Due to the data annotation requirement, such methods are usually restricted to specific domains, and struggle with the coverage issue caused by the limited size of lexical triggers. Studies on weakly supervised semantic parsers have tried to redu</context>
</contexts>
<marker>Zelle, Mooney, 1996</marker>
<rawString>John M. Zelle and Raymond J. Mooney. 1996. Learning to parse database queries using inductive logic programming. In Proceedings of the Thirteenth National Conference on Artificial Intelligence - Volume 2, AAAI’96, pages 1050–1055. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luke S Zettlemoyer</author>
<author>Michael Collins</author>
</authors>
<title>Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars.</title>
<date>2005</date>
<booktitle>In UAI,</booktitle>
<pages>658--666</pages>
<publisher>AUAI Press.</publisher>
<contexts>
<context position="3750" citStr="Zettlemoyer and Collins, 2005" startWordPosition="556" endWordPosition="559">grams) in the NL-question and the embeddings of logical features in the structured queries. Last, answers are retrieved from the KB using the selected LFs. The contributions of this work are two-fold: (1) as a smoothing technique, the low-dimensional embeddings can alleviate the coverage issues of lexical triggers; (2) our joint approach integrates entity span selection and predicate mapping tasks for KB-QA. For this we built independent entity embeddings as the additional component, solving the entity disambiguation problem. 2 Related Work Supervised semantic parsers (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Mooney, 2007) heavily rely on the &lt;sentence, semantic an645 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 645–650, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics notation&gt; pairs for lexical trigger extraction and model training. Due to the data annotation requirement, such methods are usually restricted to specific domains, and struggle with the coverage issue caused by the limited size of lexical triggers. Studies on weakly supervised semantic parsers have tried to reduce the amount of human supervis</context>
</contexts>
<marker>Zettlemoyer, Collins, 2005</marker>
<rawString>Luke S. Zettlemoyer and Michael Collins. 2005. Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars. In UAI, pages 658–666. AUAI Press.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000002">
<title confidence="0.999077">
Word Translation Prediction for Morphologically Rich Languages
with Bilingual Neural Networks
</title>
<author confidence="0.992688">
Ke Tran Arianna Bisazza Christof Monz
</author>
<affiliation confidence="0.997138">
Informatics Institute, University of Amsterdam
</affiliation>
<address confidence="0.924274">
Science Park 904, 1098 XH Amsterdam, The Netherlands
</address>
<email confidence="0.998617">
{m.k.tran,a.bisazza,c.monz}@uva.nl
</email>
<sectionHeader confidence="0.993882" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999957142857143">
Translating into morphologically rich lan-
guages is a particularly difficult problem
in machine translation due to the high de-
gree of inflectional ambiguity in the tar-
get language, often only poorly captured
by existing word translation models. We
present a general approach that exploits
source-side contexts of foreign words to
improve translation prediction accuracy.
Our approach is based on a probabilistic
neural network which does not require lin-
guistic annotation nor manual feature en-
gineering. We report significant improve-
ments in word translation prediction accu-
racy for three morphologically rich target
languages. In addition, preliminary results
for integrating our approach into a large-
scale English-Russian statistical machine
translation system show small but statisti-
cally significant improvements in transla-
tion quality.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99995">
The ability to make context-sensitive translation
decisions is one of the major strengths of phrase-
based SMT (PSMT). However, the way PSMT ex-
ploits source-language context has several limita-
tions as pointed out, for instance, by Quirk and
Menezes (2006) and Durrani et al. (2013). First,
the amount of context used to translate a given
input word depends on the phrase segmentation,
with hypotheses resulting from different segmen-
tations competing with one another. Another issue
is that, given a phrase segmentation, each source
phrase is translated independently from the oth-
ers, which can be problematic especially for short
phrases. As a result, the predictive translation of
a source phrase does not access useful linguistic
clues in the source sentence that are outside of the
scope of the phrase.
Lexical weighting tackles the problem of un-
reliable phrase probabilities, typically associated
with long phrases, but does not alleviate the prob-
lem of context segmentation. An important share
of the translation selection task is then left to the
language model (LM), which is certainly very ef-
fective but can only leverage target language con-
text. Moreover, decisions that are taken at early
decoding stages—such as the common practice
of retaining only top n translation options for
each source span—depend only on the translation
models and on the target context available in the
phrase.
Source context based translation models (Gim-
pel and Smith, 2008; Mauser et al., 2009; Jeong
et al., 2010; Haque et al., 2011) naturally ad-
dress these limitations. These models can ex-
ploit a boundless context of the input text, but
they assume that target words can be predicted in-
dependently from each other, which makes them
easy to integrate into state-of-the-art PSMT sys-
tems. Even though the independence assump-
tion is made on the target side, these models have
shown the benefits of utilizing source context, es-
pecially in translating into morphologically rich
languages. One drawback of previous research
on this topic, though, is that it relied on rich
sets of manually designed features, which in turn
required the availability of linguistic annotation
tools like POS taggers and syntactic parsers.
In this paper, we specifically focus on im-
proving the prediction accuracy for word transla-
tions. Achieving high levels of word translation
accuracy is particularly challenging for language
</bodyText>
<page confidence="0.926459">
1676
</page>
<note confidence="0.8978015">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1676–1688,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999894733333333">
pairs where the source language is morphologi-
cally poor, such as English, and the target lan-
guage is morphologically rich, such as Russian,
i.e., language pairs with a high degree of surface
realization ambiguity (Minkov et al., 2007). To
address this problem we propose a general ap-
proach based on bilingual neural networks (BNN)
exploiting source-side contextual information.
This paper makes a number of contributions:
Unlike previous approaches our models do not re-
quire any form of linguistic annotation (Minkov
et al., 2007; Kholy and Habash, 2012; Chahuneau
et al., 2013), nor do they require any feature en-
gineering (Gimpel and Smith, 2008). Moreover,
besides directly predicting fully inflected forms
as Jeong et al. (2010), our approach can also
model stem and suffix prediction explicitly. Pre-
diction accuracy is evaluated with respect to three
morphologically rich target languages (Bulgarian,
Czech, and Russian) showing that our approach
consistently yields substantial improvements over
a competitive baseline. We also show that these
improvements in prediction accuracy can be ben-
eficial in an end-to-end machine translation sce-
nario by integrating into a large-scale English-
Russian PSMT system. Finally, a detailed analysis
shows that our approach induces a positive bias on
phrase translation probabilities leading to a better
ranking of the translation options employed by the
decoder.
</bodyText>
<sectionHeader confidence="0.788086" genericHeader="introduction">
2 Lexical coverage of SMT models
</sectionHeader>
<bodyText confidence="0.99996005882353">
The first question we ask is whether translation
can be improved by a more accurate selection of
the translation options already existing in the SMT
models, as opposed to generating new options.
To answer this question we measure the lexical
coverage of a baseline PSMT system trained on
English-Russian.1 We choose this language pair
because of the morphological richness on the tar-
get side: Russian is characterized by a highly in-
flectional morphology with a particularly complex
nominal declension (six core cases, three genders
and two number categories). As suggested by
Green and DeNero (2012), we compute the re-
call of reference tokens in the set of target to-
kens that the decoder could produce in a trans-
lation of the source, that is the target tokens of
all phrase pairs that matched the input sentence
</bodyText>
<footnote confidence="0.419187">
1Training data and SMT setup are described in Section 6.
</footnote>
<bodyText confidence="0.999780466666667">
and that were actually used for decoding.2 We
call this the decoder’s lexical search space. Then,
we compare the reference/space recall against the
reference/MT-output recall: that is, the percent-
age of reference tokens that also appeared in the
1-best translation output by the SMT system. Re-
sults for the WMT12 benchmark are presented in
Table 1. From the first two rows, we see that only a
rather small part of the correct target tokens avail-
able to the decoder are actually produced in the
1-best MT output (50% against 86%). Although
our word-level analysis does not directly estimate
phrase-level coverage, these numbers suggest that
a large potential for translation improvement lies
in better lexical selection during decoding.
</bodyText>
<table confidence="0.62786">
Token recall:
reference/MT-search-space 86.0%
reference/MT-output 50.0%
stem-only reference/MT-output 12.3%
of which reachable 11.2%
</table>
<tableCaption confidence="0.9350695">
Table 1: Lexical coverage analysis of the baseline
SMT system (English-Russian wmt12).
</tableCaption>
<bodyText confidence="0.999860285714286">
To quantify the importance of morphology, we
count how many reference tokens matched the
MT output only at the stem level3 and for how
many of those the correct surface form existed
in the search space (reachable matches). These
two numbers represent the upper bound of the im-
provement achievable by a model only predicting
suffixes given the target stems. As shown in Ta-
ble 1, such a model could potentially increase the
reference/MT-output recall by 12.3% with genera-
tion of new inflected forms, and by 11.2% without.
Thus, also when it comes to morphology, gener-
ation seems to be of secondary importance com-
pared to better selection in our experimental setup.
</bodyText>
<sectionHeader confidence="0.946542" genericHeader="method">
3 Predicting word translations in context
</sectionHeader>
<bodyText confidence="0.999532714285714">
It is standard practice in PSMT to use word-
to-word translation probabilities as an additional
phrase score. More specifically, state-of-the-art
PSMT systems employ the maximum-likelihood
estimate of the context-independent probability
of a target word given its aligned source word
P(tj|si) for each word alignment link aij.
</bodyText>
<footnote confidence="0.996782666666666">
2This corresponds to the top 30 phrases sorted by
weighted phrase, lexical and LM probabilities, for each
source span. Koehn (2004) and our own experience suggest
that using more phrases has little or no impact on MT quality.
3Word segmentation for this analysis is performed by the
Russian Snowball stemmer, see also Section 5.3.
</footnote>
<page confidence="0.987661">
1677
</page>
<figure confidence="0.731341">
[KOHCTMTyL MOHHOCTb] [MHAMaHa 3aKOH] [.]
[the constitutionality of the] [indiana law] [.]
</figure>
<figureCaption confidence="0.91055325">
Figure 1: Fragment of English sentence and its in-
correct Russian translation produced by the base-
line SMT system. Square brackets indicate phrase
boundaries.
</figureCaption>
<bodyText confidence="0.999663066666667">
The main goal of our work is to improve the
estimation of such probabilities by exploiting the
context of si, which in turn we expect will re-
sult in better phrase translation selection. Figure
1 illustrates this idea: the translation of “law” in
this example has a wrong case—nominative in-
stead of genitive. Due to the rare word “Indi-
ana/Ha,qHaaa”, the target LM must backoff to the
bigram history and does not penalize this choice
sufficiently. However, a model that has access to
the word “of” in the near source context could bias
the translation of “law” to the correct case.
We then model P(tj|csi) with source context
csi defined as a fixed-length word sequence cen-
tered around si:
</bodyText>
<equation confidence="0.866277">
csi = si−k, ..., si, ..., si+k
</equation>
<bodyText confidence="0.99974775">
Our definition of context is similar to the n − 1
word history used in n-gram LMs. Similarly to
previous work in source context-sensitive trans-
lation modeling (Jeong et al., 2010; Chahuneau
et al., 2013), target words are predicted indepen-
dently from each other, which allows for an ef-
ficient decoding integration. We are particularly
interested in translating into morphologically rich
languages where source context can provide useful
information for the prediction of target translation,
for example, the gender of the subject in a source
sentence constrains the morphology of the transla-
tion of the source verb. Therefore, we integrate the
notions of stem and suffix directly into the model.
We assume the availability of a word segmenta-
tion function g that takes a target word t as in-
put and returns its stem and suffix: g(t) = (σ, µ).
Then, the conditional probability p(tj|csi) can be
decomposed into stem probability and suffix prob-
ability:
</bodyText>
<equation confidence="0.998237">
p(tj|csi) = p(σj|csi)p(µj|csi,σj) (1)
</equation>
<bodyText confidence="0.99986">
These two probabilities can be estimated sepa-
rately, which yields the two subtasks:
</bodyText>
<listItem confidence="0.990339333333333">
1. predict target stem σ given source context cs;
2. predict target suffix µ given source context cs
and target stem σ.
</listItem>
<bodyText confidence="0.999964454545454">
Based on the results of our analysis, we focus
on the selection of existing translation candidates.
We then restrict our prediction on a set of pos-
sible target candidates depending on the task in-
stead of considering all target words in the vocab-
ulary. More specifically, for each source word si,
our candidate generation function returns the set of
target words Ts = {t1, ... , tm} that were aligned
to si in the parallel training corpus, which in turn
corresponds to the set of target words that the SMT
system can produce for a given source. In practice,
we use a pruned version of Ts to speed up training
and reduce noise (see details in Section 5).
As for the morphological models, given Ts and
g, we can obtain Ls = {σ1,... , σk}, the set of
possible target stem translations of s, and MQ =
{µ1, ... , µl}, the set of possible suffixes for a tar-
get stem σ. The use of Ls, and MQ is similar to
stemming and inflection operations in (Toutanova
et al., 2008) while the set Ts is similar to the GEN
function in (Jeong et al., 2010).4
Our approach differs crucially from previous
work (Minkov et al., 2007; Chahuneau et al.,
2013) in that it does not require linguistic fea-
tures such as part-of-speech and syntactic tree on
the source side. The proposed models automati-
cally learn features that are relevant for each of the
modeled tasks, directly from word-aligned data.
To make the approach completely language inde-
pendent, the word segmentation function g can be
trained with an unsupervised segmentation tool.
The effects of using different word segmentation
techniques are discussed in Section 5.
</bodyText>
<sectionHeader confidence="0.8583195" genericHeader="method">
4 Bilingual neural networks for
translation prediction
</sectionHeader>
<bodyText confidence="0.999844">
Probabilistic neural network (NN), or continuous
space, language models have received increas-
ing attention over the last few years and have
been applied to several natural language process-
ing tasks (Bengio et al., 2003; Collobert and We-
ston, 2008; Socher et al., 2011; Socher et al.,
2012). Within statistical machine translation, they
</bodyText>
<footnote confidence="0.6495388">
4Note that our suffix generation function Mo is restricted
to the forms observed in the target monolingual data, but not
to those aligned to a source word s, which opens the possi-
bility of generating inflected forms that are missing from the
translation models. We leave this possibility to future work.
</footnote>
<page confidence="0.993353">
1678
</page>
<bodyText confidence="0.999930173913043">
have been used for monolingual target language
modeling (Schwenk et al., 2006; Le et al., 2011;
Duh et al., 2013; Vaswani et al., 2013), n-gram
translation modeling (Son et al., 2012), phrase
translation modeling (Schwenk, 2012; Zou et al.,
2013; Gao et al., 2014) and minimal translation
modeling (Hu et al., 2014). The recurrent neural
network LMs of Auli et al. (2013) are primarily
trained to predict target word sequences. However,
they also experiment with an additional input layer
representing source side context.
Our models differ from most previous work in
neural language modeling in that we predict a tar-
get translation given a source context while pre-
vious models predict the next word given a tar-
get word history. Unlike previous work in phrase
translation modeling with NNs, our models have
the advantage of accessing source context that can
fall outside the phrase boundaries.
We now describe our models in a general set-
ting, predicting target translations given a source
context, where target translations can be either
words, stems or suffixes.5
</bodyText>
<subsectionHeader confidence="0.977195">
4.1 Neural network architecture
</subsectionHeader>
<bodyText confidence="0.999851111111111">
Following a common approach in deep learning
for NLP (Bengio et al., 2003; Collobert and We-
ston, 2008), we represent each source word si by
a column vector rsi E Rd. Given a source con-
text csi = si−k, ..., si, ..., si+k of k words on the
left and k words on the right of si, the context rep-
resentation rcsi E R(2k+1)xd is obtained by con-
catenating the vector representations of all words
in csi:
</bodyText>
<equation confidence="0.533856">
rcsi = rsi−k O ... O rsi+k
</equation>
<bodyText confidence="0.9997666">
Our main BNN architecture for word or stem
prediction (Figure 2a) is a feed-forward neural
network (FFNN) with one hidden layer, a matrix
W1 E Rnx(2k+1)d connecting the input layer to
the hidden layer, a matrix W2 E R|Vt|xn connect-
ing the hidden layer to the output layer, and a bias
vector b2 E R|Vt |where |Vt |is the size of target
translations vocabulary. The target translation dis-
tribution PBNN(t|csi) for a given source context
csi is computed by a forward pass:
</bodyText>
<equation confidence="0.983852">
softmax (W2 φ(W1rcsi) + b2) (2)
</equation>
<bodyText confidence="0.9983775">
where φ is a nonlinearity (tanh, sigmoid or rec-
tified linear units). The parameters of the neural
</bodyText>
<footnote confidence="0.902398">
5The source code of our models is available at https:
//bitbucket.org/ketran
</footnote>
<bodyText confidence="0.9530645">
network are θ = {rsi, W1, W2, b2}.
The suffix prediction BNN is obtained by
adding the target stem representation rσ to the in-
put layer (see Figure 2b).
</bodyText>
<subsectionHeader confidence="0.998062">
4.2 Model variants
</subsectionHeader>
<bodyText confidence="0.99948504">
We encounter two major issues with FFNNs: (i)
They do not provide a natural mechanism to com-
pute word surface conditional probability p(t|cs)
given individual stem probability p(σ|cs) and suf-
fix probability p(µ|cs, σ), and (ii) FFNNs do not
provide the flexibility to capture long dependen-
cies among words if they lie outside the source
context window. Hence, we consider two BNN
variants: a log-bilinear model (LBL) and a con-
volutional neural network model (ConvNet). LBL
could potentially address (i) by factorizing target
representations into target stem and suffix repre-
sentations whereas ConvNets offer the advantage
of modeling variable input length (ii) (Kalchbren-
ner et al., 2014).
Log-bilinear model. The FFNN models stem
and suffix probabilities separately. A log-bilinear
model instead could directly model word predic-
tion through a factored representation of target
words, similarly to Botha and Blunsom (2014).
Thus, no probability mass would be wasted over
stem-suffix combinations that are not in the candi-
date generation function. The LBL model speci-
fies the conditional distribution for the word trans-
lation tj E Tsi given a source context csi:
</bodyText>
<equation confidence="0.993921333333333">
Pe(tj|csi) = exp (se (tj,csi)) (3)
E exp(se(t&amp;quot;csi))
t&apos; ET
</equation>
<bodyText confidence="0.999958333333333">
We use an additional set of word representations
qtj E Rn for target translations tj. The LBL
model computes a predictive representation qˆ of a
source context csi by taking a linear combination
of the source word representations rsi+m with the
position-dependent weight matrices Cm E Rnxd:
</bodyText>
<equation confidence="0.973616">
k
qˆ= E Cmrsi+m (4)
m=−k
</equation>
<bodyText confidence="0.999419666666667">
The score function se(tj, csi) measures the simi-
larity between the predictive representation qˆ and
the target representation qtj:
</bodyText>
<equation confidence="0.97022525">
se(tj, csi) = ˆqTqtj + bThqtj + btj (5)
s
j
i
</equation>
<page confidence="0.974573">
1679
</page>
<figure confidence="0.999698153846154">
(a) BNN for word prediction.
(b) BNN for suffix prediction.
P✓(t|c8.)
W2
0( )
W1
r8%—k r, rs%+k
P✓(it|a, c.J
W2
0( )
W1
rs%—k r, rs%+k
rQ
</figure>
<figureCaption confidence="0.992265">
Figure 2: Feed-forward BNN architectures for predicting target translations: (a) word model (similar to
stem model), and (b) suffix model with an additional vector representation rσ for target stems σ.
</figureCaption>
<bodyText confidence="0.998609928571429">
Here btj is the bias term associated with target
word tj. bh ∈ Rn are the representation bi-
ases. sθ(tj, csi) can be seen as the negative en-
ergy function of the target translation tj and its
context csi. The parameters of the model thus
are 0 = {rsi, Cm, qtj, bh, btj}. Our log-bilinear
model is a modification of the log-bilinear model
proposed for n-gram language modeling in (Mnih
and Hinton, 2007).
Convolutional neural network model. This
model (Figure 3) computes the predictive repre-
sentation qˆ by applying a sequence of 2k convo-
lutional layers {L1, ... , L2k}. The source context
csi is represented as a matrix mcsi ∈ Rdx(2k+1):
</bodyText>
<equation confidence="0.995255">
(6)
mcsi = �rsi−k; ... ; rsi+k �
</equation>
<bodyText confidence="0.99905">
neural network model are 0 = {rsi, mj, W}.
Here, we focus on a fixed length input, how-
ever convolutional neural networks may be used to
model variable length input (Kalchbrenner et al.,
2014; Kalchbrenner and Blunsom, 2013).
</bodyText>
<subsectionHeader confidence="0.996365">
4.3 Training
</subsectionHeader>
<bodyText confidence="0.9998946">
In training, for each example (t, cs), we maximize
the conditional probability Pθ(t|cs) of a correct
target label t. The contribution of the training ex-
ample (t, cs) to the gradient of the log conditional
probability is given by:
</bodyText>
<equation confidence="0.985482714285714">
∂ ∂
∂0 log Pθ(t|cs) = ∂0sθ(t|cs)
�−
t�ETs
Pθ(t�|cs)∂0∂ sθ(t&amp;quot; cs)
q�
rso rs, r12 rsi rs4 rs5 rss
</equation>
<figureCaption confidence="0.548722">
Figure 3: Convolutional neural network model.
Edges with the same color indicate the same ker-
nel weight matrix.
</figureCaption>
<bodyText confidence="0.999983272727273">
Each convolutional layer Li consists of a one-
dimensional filter mi ∈ Rdx2. Each row of mi
is convolved with the corresponding row in the
previous layer resulting in a weight matrix whose
number of columns decreases by one. Thus after
2k convolutional layers, the network transforms
the source context matrix mcsi to a feature vec-
tor qˆ ∈ Rd. A fully connected layer with weight
matrix W followed by a softmax layer are placed
after the last convolutional layer L2k to perform
classification. The parameters of the convolutional
Note that in the gradient, we do not sum over all
target translations T but a set of possible candi-
dates Ts of a source word s. In practice |Ts |≤ 200
with our pruning settings (see Section 5.1), thus
training time for one example does not depend on
the vocabulary size. Our training criterion can be
seen as a form of contrastive estimation (Smith
and Eisner, 2005), however we explicitly move the
probability mass from competing candidates to the
correct translation candidate, thus obtaining more
reliable estimates of the conditional probabilities.
The BNN parameters are initialized randomly
according to a zero-mean Gaussian. We regularize
the models with L2. As an alternative to the L2
regularizer, we also experiment with dropout (Hin-
ton et al., 2012), where the neurons are randomly
zeroed out with dropout rate p. This technique is
known to be useful in computer vision tasks but
has been rarely used in NLP tasks. In FFNN, we
use dropout after the hidden layer, while in Con-
vNet, dropout applies after the last convolutional
layer. The dropout rate p is set to 0.3 in our exper-
</bodyText>
<page confidence="0.955132">
1680
</page>
<bodyText confidence="0.976599818181818">
iments. We use rectified nonlinearities6 in FFNN
and after each convolutional layer in ConvNet. We
train our BNN models with the standard stochastic
gradient descent.
source word representations to 100 in all experi-
ments. The number of hidden units in our feed-
forward neural networks and the target translation
embedding size in LBL models are set to 200. All
models are trained for 10 iterations with learning
rate set to 0.001.
5 Evaluating word translation prediction
In this section, we assess the ability of our BNN
models to predict the correct translation of a word
in context. In addition to English-Russian, we also
consider translation prediction for Czech and Bul-
garian. As members of the Slavic language fam-
ily, Czech and Bulgarian are also characterized by
highly inflectional morphology. Czech, like Rus-
sian, displays a very rich nominal inflection with
as many as 14 declension paradigms. Bulgarian,
unlike Russian, is not affected by case distinctions
but is characterized by a definiteness suffix.
</bodyText>
<subsectionHeader confidence="0.912056">
5.1 Experimental setup
</subsectionHeader>
<bodyText confidence="0.9459195">
The following parallel corpora are used to train the
BNN models:
</bodyText>
<listItem confidence="0.968992166666667">
• English-Russian: WMT13 data (News Com-
mentary and Yandex corpora);
• English-Czech: CzEng 1.0 corpus (Bojar et
al., 2012) (Web Pages and News sections);
• English-Bulgarian: a mix of crawled news
data, TED talks and Europarl proceedings.
</listItem>
<bodyText confidence="0.998531705882353">
Detailed corpus statistics are given in Table 2. For
each language pair, accuracies are measured on a
held-out set of 10K parallel sentences.
To prepare the candidate generation function,
each dataset is first word-aligned with GIZA++,
then a bilingual lexicon with maximum-likelihood
probabilities (Pmle) is built from the symmetrized
alignment. After some frequency and signifi-
cance pruning,7 the top 200 translations sorted by
Pmle(t|s) · Pmle(s|t) are kept as candidate word
translations for each source word in the vocabu-
lary. Word alignments are also used to train the
BNN models: each alignment link constitutes a
training sample, with no special treatment of un-
aligned words and 1-to-many alignments.
The context window size k is set to 3 (cor-
responding to 7-gram) and the dimensionality of
</bodyText>
<footnote confidence="0.9872886">
6We find that using rectified linear units gives better re-
sults than sigmoid and tanh.
7Each lexicon is pruned with minimum word frequency 5,
minimum source-target word pair frequency 2, minimum log
odds ratio 10.
</footnote>
<table confidence="0.9996365">
En-Ru En-Cs En-Bg
Sentences 1M 1M 0.8M
Src. tokens 26.5M 19.2M 19.3M
Trg. tokens 24.7M 16.7M 18.9M
Src. T/T .0109 .0105 .0051
Trg. T/T .0247 .0163 .0104
</table>
<tableCaption confidence="0.931658">
Table 2: BNN training corpora statistics: number
of sentences, tokens, and type/token ratio (T/T).
</tableCaption>
<subsectionHeader confidence="0.593034">
5.2 Word, stem and suffix prediction
accuracy
</subsectionHeader>
<bodyText confidence="0.999974266666667">
We measure accuracy at top-n, i. e. the number
of times the correct translation was in the top n
candidates sorted by a model. For each subtask—
word, stem and suffix prediction—the BNN
model is compared to the context-independent
maximum-likelihood baseline Pmle(t|s) on which
the PSMT lexical weighting score is based. Note
that this is a more realistic baseline than the uni-
form models sometimes reported in the litera-
ture. The oracle corresponds to the percentage of
aligned source-target word pairs in the held-out set
that are covered by the candidate generation func-
tion. Out of the missing links, about 4% is due
to lexicon pruning. Results for all three language
pairs are presented in Table 3. In this series of
experiments, the morphological BNNs utilize un-
supervised segmentation models trained on each
target language following Lee et al. (2011).8
As shown in Table 3, the BNN models outper-
form the baseline by a large margin in all tasks and
languages. In particular, word prediction accuracy
at top-1 increases by +6.4%, +24.6% and +9.0%
absolute in English-Russian, English-Czech and
English-Bulgarian respectively, without the use of
any features based on linguistic annotation. While
the baseline and oracle differences among lan-
guages can be explained by different levels of
overlap between training and held-out set, we can-
not easily explain why the Czech BNN perfor-
mance is so much higher. When comparing the
</bodyText>
<footnote confidence="0.94868">
8We use the C++ implementation available at http://
groups.csail.mit.edu/rbg/code/morphsyn
</footnote>
<page confidence="0.802084">
1681
</page>
<table confidence="0.9967695">
Model En-Ru En-Cs En-Bg
Word prediction (%):
Baseline 33.0 / 50.1 42.0 / 59.9 47.9 / 66.0
Word BNN 39.4 / 56.6 66.6 / 81.4 56.9 / 74.0
+6.4 / +6.5 +24.6/+21.5 +9.0 / +8.0
Oracle 79.5 90.2 86.9
Stem prediction (%):
Baseline 40.7 / 58.2 46.1 / 64.3 51.9 / 70.1
Stem BNN 45.1 / 62.5 66.1 / 81.6 56.7 / 74.4
+4.4 / +4.3 +20.0/+17.3 +4.8 / +4.3
Suffix prediction (%):
Baseline 71.2 / 85.6 78.8 / 93.2 81.5 / 92.4
Suffix BNN 77.0 / 89.7 91.9 / 97.4 87.7 / 94.9
+5.8 / +4.1 +13.1 /+4.2 +6.2 / +2.5
</table>
<tableCaption confidence="0.965196333333333">
Table 3: BNN prediction accuracy (top-1/top-3)
compared to a context-independent maximum-
likelihood baseline.
</tableCaption>
<bodyText confidence="0.998878727272727">
three prediction subtasks, we find that word pre-
diction is the hardest task as expected. Stem pre-
diction accuracies are considerably higher than
word prediction accuracies in Russian, but almost
equal in the other two languages. Finally, base-
line accuracies for suffix prediction are by far
the highest, ranging between 71.2% and 81.5%,
which is primarily explained by a smaller num-
ber of candidates to choose from. Also on this
task, the BNN model achieves considerable gains
of +5.8%, +13.1% and +6.2% at top-1, without the
need of manual feature engineering.
From these figures, it is hard to predict whether
word BNNs or morphological BNNs will have a
better effect on SMT performance. On one hand,
the word-level BNN achieves the highest gain over
the MLE baseline. On the other, the stem- and
suffix-level BNNs provide two separate scoring
functions, whose weights can be directly tuned for
translation quality. A preliminary answer to this
question is given by the SMT experiments pre-
sented in Section 6.
</bodyText>
<subsectionHeader confidence="0.999394">
5.3 Effect of word segmentation
</subsectionHeader>
<bodyText confidence="0.999919875">
This section analyzes the effect of using different
segmentation techniques. We consider two super-
vised tagging methods that produce lemma and in-
flection tag for each token in a context-sensitive
manner: TreeTagger (Sharoff et al., 2008) for Rus-
sian and the Morce tagger (Spoustová et al., 2007)
for Czech.9 Finally, we employ the Russian Snow-
ball rule-based stemmer as a light-weight context-
</bodyText>
<footnote confidence="0.91881">
9Annotation included in the CzEng 1.0 corpus release.
</footnote>
<figureCaption confidence="0.877316333333333">
Figure 4: Effect of different word segmentation
techniques (U: unsupervised, S: supervised, R:
rule-based stemmer) on stem and suffix prediction
</figureCaption>
<bodyText confidence="0.93904375">
accuracy. The dark part of each bar stands for top-
1, the light one for top-3 accuracy.
insensitive segmentation technique.10
As shown in Figure 4, accuracies for both stem
and suffix prediction vary noticeably with the seg-
mentation used. However, higher stem accuracies
corresponds to lower suffix accuracies and vice
versa, which can be mainly due to a general pref-
erence of a tool to segment more or less than an-
other. In summary, the unsupervised segmentation
methods and the light-weight stemmer appear to
perform comparably to the supervised methods.
</bodyText>
<subsectionHeader confidence="0.996844">
5.4 Effect of training data size
</subsectionHeader>
<bodyText confidence="0.99946575">
We examine the predictive power of our models
with respect to the size of training data. Table 4
shows the accuracies of stem and suffix models
trained on 200K and 1M English-Russian sentence
pairs with unsupervised word segmentation. Sur-
prisingly, we observe only a minor loss when we
decrease the training data size, which suggests that
our models are robust even on a small data set.
</bodyText>
<table confidence="0.896013666666667">
# Train sent. Stem Acc. Suffix Acc.
1M 45.1 / 62.5 77.0 / 89.7
200K 44.6 / 61.8 75.7 / 88.6
</table>
<tableCaption confidence="0.942759">
Table 4: Accuracy at top-1/top-3 (%) of stem and
suffix BNNs with different training data sizes.
</tableCaption>
<subsectionHeader confidence="0.857121">
5.5 Fine-grained evaluation
</subsectionHeader>
<bodyText confidence="0.999941">
We evaluate the suffix BNN model at the part-of-
speech (POS) level. Table 5 provides suffix pre-
diction accuracy per POS for En-Ru. For this
analysis, Russian data is segmented by TreeTag-
</bodyText>
<footnote confidence="0.861367">
10http://snowball.tartarus.org/
algorithms/russian/stemmer.html
</footnote>
<page confidence="0.995172">
1682
</page>
<bodyText confidence="0.999786166666667">
ger. Additionally, we report the average number
of suffixes per stem given the part-of-speech.
Our results are consistent with the findings of
Chahuneau et al. (2013):11 the prediction of ad-
jectives is more difficult than that of other POS
while Russian verb prediction is relatively easier
in spite of the higher number of suffixes per stem.
These differences reflect the importance of source
versus target context features in the prediction of
the target inflection: For instance, adjectives agree
in gender with the nouns they modify, but this may
be only inferred from the target context.
</bodyText>
<table confidence="0.996800333333333">
POS A V N M P
Acc. (%) 49.6 61.9 62.8 84.5 64.4
|Mo |18.2 18.4 9.2 7.1 13.3
</table>
<tableCaption confidence="0.79281375">
Table 5: Suffix prediction accuracy at top-1 (%),
breakdown by category (A: adjectives, V: verbs,
N: nouns, M: numerals and P: pronouns). |Mσ|
denotes the average number of suffixes per stem.
</tableCaption>
<subsectionHeader confidence="0.872628">
5.6 Neural Network variants
</subsectionHeader>
<bodyText confidence="0.995018233333333">
Table 6 shows the stem and suffix accuracies of
BNN variants on English-Czech. Although none
of the variants outperform our main FFNN archi-
tecture, we observe similar performances by the
LBL on stem prediction, and by the ConvNet on
suffix prediction. This suggests that future work
could exploit their additional flexibilities (see Sec-
tion 4.2) to improve the BNN predictive power.
As for the low suffix accuracy by the LBL, it
can be explained by the absence of nonlinearity
transformation. Nonlinearity is important for the
suffix model where the prediction of target suf-
fix µj often does not depend linearly on si and
σj. The predictive representation of target stem
in the LBL stem model, however, mainly depends
on the source representation rsi through a position
dependent weight matrix C0. Thus, we observe a
smaller accuracy drop in the stem model than in
the suffix model. Conversely, the ConvNet per-
forms poorly on stem prediction because it cap-
tures the meaning of the whole source context in-
stead of emphasizing the importance of the source
word si as the main predictor of the target transla-
tion tj.
11Chahuneau et al. (2013) report an average accuracy of
63.1% for the prediction of A, V, N, M suffixes. When we
train our model on the same dataset (news-commentary) we
obtain a comparable result (64.7% vs 63.1%).
Unexpectedly, no improvement is obtained by
the use of dropout regularizer (see Section 4.3).
</bodyText>
<table confidence="0.9967586">
Model Stem Acc Suffix Acc
FFNN 66.1 / 81.6 91.9 / 97.4
FFNN+do 64.6 / 81.1 91.5 / 97.5
LBL 63.6 / 79.6 86.4 / 96.4
ConvNet+do 58.6 / 75.6 90.3 / 96.9
</table>
<tableCaption confidence="0.772126333333333">
Table 6: Accuracies at top-1/top-3 (%) of stem and
suffix models. +do indicates dropout instead of L2
regularizer. FFNN is our main architecture.
</tableCaption>
<sectionHeader confidence="0.988992" genericHeader="method">
6 SMT experiments
</sectionHeader>
<bodyText confidence="0.999972125">
While the main objective of this paper is to im-
prove prediction accuracy of word translations,
see Section 5, we are also interested in know-
ing to which extent these improvements carry over
within an end-to-end machine translation task. To
this end, we integrate our translation prediction
models described in Section 4 into our existing
English-Russian SMT system.
For each phrase pair matching the input, the
phrase BNN score PBNN-p is computed as follows:
where a is the word-level alignment of the phrase
pair (˜s, ˜t) and {ai} is the set of target positions
aligned to si. If a source-target link cannot be
scored by the BNN model, we give it a PBNN
probability of 1 and increment a separate count
feature ε. Note that the same phrase pair can get
different BNN scores if used in different source
side contexts.
Our baseline is an in-house phrase-based
(Koehn et al., 2003) statistical machine transla-
tion system very similar to Moses (Koehn et al.,
2007). All system runs use hierarchical lexicalized
reordering (Galley and Manning, 2008; Cherry
et al., 2012), distinguishing between monotone,
swap, and discontinuous reordering, all with re-
spect to left-to-right and right-to-left decoding.
Other features include linear distortion, bidirec-
tional lexical weighting (Koehn et al., 2003), word
and phrase penalties, and finally a word-level 5-
gram target LM trained on all available mono-
lingual data with modified Kneser-Ney smooth-
ing (Chen and Goodman, 1999). The distortion
</bodyText>
<equation confidence="0.973115111111111">
PBNN-p(˜s, ˜t, a) =
 |{ai } |Y_ PBNN(tj  |csi) if  |{ai} |&gt; 0
j∈{ail
Pmle(NULL|si)
1
otherwise
{
� |˜s|
i=1
</equation>
<page confidence="0.988932">
1683
</page>
<tableCaption confidence="0.9974875">
Table 7: SMT training and test data statistics. All
numbers refer to tokenized, lowercased data.
</tableCaption>
<table confidence="0.9997474375">
Corpus Lang. #Sent. #Tok.
EN 1.9M 48.9M
paral.train
RU 45.9M
Wiki dict. EN/RU 508K –
mono.train RU 21.0M 390M
WMT2012 3K 64K
EN
3K 56K
WMT2013
SMT system wmt12 (dev) wmt13 (test)
Baseline 24.7 18.9
+ stem/suff. BNN 25.1 19.3&apos;
Base+suffLM 24.5 19.2
+ word BNN 24.5 19.3
+ stem/suff. BNN 24.7 19.6A
</table>
<tableCaption confidence="0.981752">
Table 8: Effect of our BNN models on English-
Russian translation quality (BLEU[%]).
</tableCaption>
<bodyText confidence="0.999426666666667">
limit is set to 6 and for each source phrase the top
30 translation candidates are considered. When
translating into a morphologically rich language,
data sparsity issues in the target language become
particularly apparent. To compensate for this we
also experiment with a 5-gram suffix-based LM in
addition to the surface-based LM (Müller et al.,
2012; Bisazza and Monz, 2014).
The BNN models are integrated as additional
log-probability feature functions (log PBNN-p):
one feature for the word prediction model or two
features for the stem and suffix models respec-
tively, plus the penalty feature ε.
Table 7 shows the data used to train our English-
Russian SMT system. The feature weights for all
approaches were tuned by using pairwise rank-
ing optimization (Hopkins and May, 2011) on the
wmt12 benchmark (Callison-Burch et al., 2012).
During tuning, 14 PRO parameter estimation runs
are performed in parallel on different samples of
the n-best list after each decoder iteration. The
weights of the individual PRO runs are then av-
eraged and passed on to the next decoding itera-
tion. Performing weight estimation independently
for a number of samples corrects for some of the
instability that can be caused by individual sam-
ples. The wmt13 set (Bojar et al., 2013) was used
for testing. We use approximate randomization
(Noreen, 1989) to test for statistically significant
differences between runs (Riezler and Maxwell,
2005).
Translation quality is measured with case-
insensitive BLEU[%] using one reference trans-
lation. As shown in Table 8, statistically signif-
icant improvements over the respective baseline
(Baseline and Base+suffLM) are marked • at the
p &lt; .01 level. Integrating our bilingual neural net-
work approach into our SMT system yields small
but statistically significant improvements of 0.4
BLEU over a competitive baseline. We can also
see that it is beneficial to add a suffix-based lan-
guage model to the baseline system. The biggest
improvement is obtained by combining the suffix-
based language model and our BNN approach,
yielding 0.7 BLEU over a competitive, state-of-
the-art baseline, of which 0.4 BLEU are due to our
BNNs. Finally, one can see that the BNNs mod-
eling stems and suffixes separately perform bet-
ter than a BNN directly predicting fully inflected
forms.
To better understand the BNN effect on the
SMT system, we analyze the set of phrase pairs
that are employed by the decoder to translate each
sentence. This set is ranked by the weighted com-
bination of phrase translation and lexical weight-
ing scores, target language model score and, if
available, phrase BNN scores. As shown in Ta-
ble 9, the morphological BNN models have a pos-
itive effect on the decoder’s lexical search space
increasing the recall of reference tokens among
the top 1 and 3 phrase translation candidates. The
mean reciprocal rank (MRR) also improves from
0.655 to 0.662. Looking at the 1-best SMT output,
we observe a slight increase of reference/output
recall (50.0% to 50.7%), which is less than the in-
crease we observe for the top 1 translation candi-
dates (57.6% to 59.0%). One possible explanation
is that the new, more accurate translation distribu-
tions are overruled by other SMT model scores,
</bodyText>
<table confidence="0.440073125">
Token recall (wmt12): Baseline +BNN
reference/MT-search-space [top-1] 57.6% 59.0%
reference/MT-search-space [top-3] 70.7% 70.9%
reference/MT-search-space [top-30] 86.0% 85.0%
reference/MT-search-space [MRR] 0.655 0.662
reference/MT-output 50.0% 50.7%
stem-only reference/MT-output 12.3% 11.5%
of which reachable 11.2% 10.3%
</table>
<tableCaption confidence="0.858127">
Table 9: Target word coverage analysis of the
English-Russian SMT system before and after
adding the morphological BNN models.
</tableCaption>
<page confidence="0.993718">
1684
</page>
<bodyText confidence="0.999621714285714">
like the target LM, that are based on traditional
maximum-likelihood estimates. While the suffix-
based LMs proved beneficial in our experiments,
we speculate that higher gains could be obtained
by coupling our approach with a morphology-
aware neural LM like the one recently presented
by Botha and Blunsom (2014).
</bodyText>
<sectionHeader confidence="0.999774" genericHeader="method">
7 Related work
</sectionHeader>
<bodyText confidence="0.9999803">
While most relevant literature has been discussed
in earlier sections, the following approaches are
particularly related to ours: Minkov et al. (2007)
and Toutanova et al. (2008) address target inflec-
tion prediction with a log-linear model based on
rich morphological and syntactic features. Their
model exploits target context and is applied to
inflect the output of a stem-based SMT system,
whereas our models predict target words (or pairs
of stem-suffix) independently and are integrated
into decoding. Chahuneau et al. (2013) address
the same problem with another feature-rich dis-
criminative model that can be integrated in decod-
ing, like ours, but they also use it to inflect on-
the-fly stemmed phrases. It is not clear what part
of their SMT improvements is due to the gener-
ation of new phrases or to better scoring. Jeong
et al. (2010) predict surface word forms in con-
text, similarly to our word BNN, and integrate the
scores into the SMT system. Unlike us, they rely
on linguistic feature-rich log-linear models to do
that. Gimpel and Smith (2008) propose a similar
approach to directly predict phrases in context, in-
stead of words.
All those approaches employed features that
capture the global structure of source sentences,
like dependency relations. By contrast, our mod-
els access only local context in the source sen-
tence but they achieve accuracy gains comparably
to models that also use global sentence structure.
</bodyText>
<sectionHeader confidence="0.992289" genericHeader="conclusions">
8 Conclusions
</sectionHeader>
<bodyText confidence="0.99997996875">
We have proposed a general approach to predict
word translations in context using bilingual neu-
ral network architectures. Unlike previous NN ap-
proaches, we model word, stem and suffix dis-
tributions in the target language given context in
the source language. Instead of relying on man-
ually engineered features, our models automati-
cally learn abstract word representations and fea-
tures that are relevant for the modeled task directly
from word-aligned parallel data. Our preliminary
results with LBL and ConvNet architectures sug-
gest that potential improvement may be achieved
by factorizing target representations or by dynam-
ically modeling source context size. Evaluated
on three morphologically rich languages, our ap-
proach achieves considerable gains in word, stem
and suffix accuracy over a context-independent
maximum-likelihood baseline. Finally, we have
shown that the proposed BNN models can be
tightly integrated into a phrase-based SMT sys-
tem, resulting in small but statistically significant
BLEU improvement over a competitive, large-
scale English-Russian baseline.
Our analysis shows that the number of correct
target words occurring in highly scored phrase
translation candidates increases after integrating
the morphological BNNs. However, only few of
these end up in the 1-best translation output. Fu-
ture work will investigate the benefits of coupling
our BNN models with target language models that
also exploit abstract word representations, such as
Botha and Blunsom (2014) and Auli et al. (2013).
</bodyText>
<sectionHeader confidence="0.997475" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999787833333333">
This research was funded in part by the
Netherlands Organization for Scientific Research
(NWO) under project numbers 639.022.213 and
612.001.218. We would like to thank Ekaterina
Garmash for helping with the error analysis of the
English-Russian translations.
</bodyText>
<sectionHeader confidence="0.99843" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997268">
Michael Auli, Michel Galley, Chris Quirk, and Geof-
frey Zweig. 2013. Joint language and translation
modeling with recurrent neural networks. In Pro-
ceedings of the 2013 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1044–
1054, Seattle, Washington, USA, October.
Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. The Journal of Machine Learning Re-
search, 3:1137–1155.
Arianna Bisazza and Christof Monz. 2014. Class-
based language modeling for translating into mor-
phologically rich languages. In Proceedings of
COLING 2014, the 25th International Conference
on Computational Linguistics, pages 1918–1927,
Dublin, Ireland.
Ond&amp;quot;rej Bojar, Zden&amp;quot;ek Žabokrtský, Ond&amp;quot;rej Du&amp;quot;sek, Pe-
tra Galu&amp;quot;s&amp;quot;cáková, Martin Majli&amp;quot;s, David Mare&amp;quot;cek, Ji&amp;quot;rí
Mar&amp;quot;sík, Michal Novák, Martin Popel, and Ale&amp;quot;s Tam-
chyna. 2012. The joy of parallelism with czeng
</reference>
<page confidence="0.912303">
1685
</page>
<reference confidence="0.996181571428572">
1.0. In Proceedings of LREC2012, Istanbul, Turkey,
May. ELRA, European Language Resources Asso-
ciation.
Ondˇrej Bojar, Christian Buck, Chris Callison-Burch,
Christian Federmann, Barry Haddow, Philipp
Koehn, Christof Monz, Matt Post, Radu Soricut, and
Lucia Specia. 2013. Findings of the 2013 Work-
shop on Statistical Machine Translation. In Pro-
ceedings of the Eighth Workshop on Statistical Ma-
chine Translation, pages 1–44, Sofia, Bulgaria, Au-
gust. Association for Computational Linguistics.
Jan A. Botha and Phil Blunsom. 2014. Composi-
tional Morphology for Word Representations and
Language Modelling. In Proceedings of the 31st In-
ternational Conference on Machine Learning, Bei-
jing, China, June.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 workshop on statistical ma-
chine translation. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, pages
10–51, Montréal, Canada, June. Association for
Computational Linguistics.
Victor Chahuneau, Eva Schlinger, Noah A. Smith, and
Chris Dyer. 2013. Translating into morphologically
rich languages with synthetic phrases. In Proceed-
ings of the Conference on Empirical Methods in Nat-
ural Language Processing, pages 1677–1687, Seat-
tle, USA, October.
Stanley F. Chen and Joshua Goodman. 1999. An
empirical study of smoothing techniques for lan-
guage modeling. Computer Speech and Language,
4(13):359–393.
Colin Cherry, Robert C. Moore, and Chris Quirk. 2012.
On hierarchical re-ordering and permutation parsing
for phrase-based decoding. In Proceedings of the
Seventh Workshop on Statistical Machine Transla-
tion, pages 200–209, Montréal, Canada, June. Asso-
ciation for Computational Linguistics.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: deep
neural networks with multitask learning. In Pro-
ceedings of the 25th Annual International Confer-
ence on Machine Learning, volume 12, pages 2493–
2537.
Kevin Duh, Graham Neubig, Katsuhito Sudoh, and Ha-
jime Tsukada. 2013. Adaptation data selection us-
ing neural language models: Experiments in ma-
chine translation. In Proceedings of the 51st Annual
Meeting of the Association for Computational Lin-
guistics, pages 678–683, Sofia, Bulgaria, August.
Nadir Durrani, Alexander Fraser, and Helmut Schmid.
2013. Model with minimal translation units, but de-
code with phrases. In Proceedings of Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 1–11, Atlanta, Georgia, USA, June.
Michel Galley and Christopher D. Manning. 2008. A
simple and effective hierarchical phrase reordering
model. In EMNLP ’08: Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 848–856, Morristown, NJ, USA.
Association for Computational Linguistics.
Jianfeng Gao, Xiaodong He, Wen-tau Yih, and
Li Deng. 2014. Learning continuous phrase repre-
sentations for translation modeling. In Proceedings
of the 52nd Annual Meeting of the Association for
Computational Linguistics, pages 699–709. Associ-
ation for Computational Linguistics.
Kevin Gimpel and Noah A. Smith. 2008. Rich source-
side context for statistical machine translation. In
Proceedings of the Third Workshop on Statistical
Machine Translation, pages 9–17, Columbus, Ohio,
USA.
Spence Green and John DeNero. 2012. A class-based
agreement model for generating accurately inflected
translations. In Proceedings of the 50th Annual
Meeting of the Association for Computational Lin-
guistics, ACL ’12, pages 146–155, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Rejwanul Haque, Sudip Kumar Naskar, Antal Bosch,
and Andy Way. 2011. Integrating source-
language context into phrase-based statistical ma-
chine translation. Machine Translation, 25(3):239–
285, September.
Geoffrey E. Hinton, Nitish Srivastava, Alex
Krizhevsky, Ilya Sutskever, and Ruslan R. Salakhut-
dinov. 2012. Improving neural networks by
preventing co-adaptation of feature detectors. arXiv
preprint arXiv:1207.0580.
Mark Hopkins and Jonathan May. 2011. Tuning as
ranking. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1352–1362, Edinburgh, Scotland, UK.,
July. Association for Computational Linguistics.
Yuening Hu, Michael Auli, Qin Gao, and Jianfeng Gao.
2014. Minimum translation modeling with recur-
rent neural networks. In Proceedings of the 14th
Conference of the European Chapter of the Associ-
ation for Computational Linguistics, pages 20–29,
Gothenburg, Sweden, April. Association for Com-
putational Linguistics.
Minwoo Jeong, Kristina Toutanova, Hisami Suzuki,
and Chris Quirk. 2010. A discriminative lexicon
model for complex morphology. In The Ninth Con-
ference of the Association for Machine Translation
in the Americas.
Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent
continuous translation models. In Proceedings of
the 2013 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1700–1709, Seattle,
USA, October.
</reference>
<page confidence="0.923164">
1686
</page>
<reference confidence="0.9996097">
Nal Kalchbrenner, Edward Grefenstette, and Phil Blun-
som. 2014. A convolutional neural network for
modelling sentences. In Proceedings of the 52nd
Annual Meeting of the Association for Computa-
tional Linguistics, pages 655–665. Association for
Computational Linguistics.
Ahmed El Kholy and Nizar Habash. 2012. Translate,
predict or generate: Modeling rich morphology in
statistical machine translation. In Proceedings of
the 16th Conference of the European Association for
Machine Translation (EAMT).
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of HLT-NAACL 2003, pages 127–133, Ed-
monton, Canada.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open Source Toolkit for Statistical Machine Trans-
lation. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics Com-
panion Volume Proceedings of the Demo and Poster
Sessions, pages 177–180, Prague, Czech Republic.
Association for Computational Linguistics.
Philipp Koehn. 2004. Pharaoh: A beam search
decoder for phrase-based statistical machine trans-
lation models. In Robert E. Frederking and
Kathryn B. Taylor, editors, Proceedings of the 6th
Conference of the Association for Machine Transla-
tions in the Americas (AMTA 2004), pages 115–124.
Hai-Son Le, Ilya Oparin, Alexandre Allauzen, J Gau-
vain, and François Yvon. 2011. Structured output
layer neural network language model. In Proceed-
ings of Proceedings of ICASSP.
Yoong Keok Lee, Aria Haghighi, and Regina Barzilay.
2011. Modeling syntactic context improves mor-
phological segmentation. In Proceedings of the Fif-
teenth Conference on Computational Natural Lan-
guage Learning, pages 1–9, Portland, Oregon, USA,
June. Association for Computational Linguistics.
Arne Mauser, Saša Hasan, and Hermann Ney. 2009.
Extending statistical machine translation with dis-
criminative and trigger-based lexicon models. In
Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing: Volume
1 - Volume 1, EMNLP ’09, pages 210–218, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Einat Minkov, Kristina Toutanova, and Hisami Suzuki.
2007. Generating complex morphology for machine
translation. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
pages 128–135.
Andriy Mnih and Geoffrey E. Hinton. 2007. Three
new graphical models for statistical language mod-
elling. In Proceedings of the 24th International
Conference on Machine Learning, pages 641–648,
New York, NY, USA.
Thomas Müller, Hinrich Schütze, and Helmut Schmid.
2012. A comparative investigation of morphological
language modeling for the languages of the Euro-
pean Union. In Proceedings of the 2012 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, pages 386–395, Montréal, Canada,
June. Association for Computational Linguistics.
Eric W. Noreen. 1989. Computer Intensive Meth-
ods for Testing Hypotheses. An Introduction. Wiley-
Interscience.
Chris Quirk and Arul Menezes. 2006. Do we need
phrases? challenging the conventional wisdom in
statistical machine translation. In Proceedings of
the Human Language Technology Conference of the
NAACL, Main Conference, pages 9–16, New York
City, USA, June. Association for Computational
Linguistics.
Stefan Riezler and John T. Maxwell. 2005. On some
pitfalls in automatic evaluation and significance test-
ing for MT. In Proceedings of the ACL Workshop
on Intrinsic and Extrinsic Evaluation Measures for
Machine Translation and/or Summarization, pages
57–64, Ann Arbor, Michigan, June. Association for
Computational Linguistics.
Holger Schwenk, Daniel Dechelotte, and Jean-Luc
Gauvain. 2006. Continuous space language models
for statistical machine translation. In Proceedings
of the COLING/ACL 2006 Conference, pages 723–
730, Sydney, Australia, July. Association for Com-
putational Linguistics.
Holger Schwenk. 2012. Continuous space translation
models for phrase-based statistical machine transla-
tion. In Proceedings of COLING.
Serge Sharoff, Mikhail Kopotev, Tomaz Erjavec, Anna
Feldman, and Dagmar Divjak. 2008. Design-
ing and evaluating a russian tagset. In Pro-
ceedings of the Sixth International Conference on
Language Resources and Evaluation (LREC’08),
Marrakech, Morocco. European Language Re-
sources Association (ELRA). http://www.lrec-
conf.org/proceedings/lrec2008/.
Noah A. Smith and Jason Eisner. 2005. Contrastive
estimation: Training log-linear models on unlabeled
data. In Proceedings of the 43rd Annual Meeting on
Association for Computational Linguistics.
Richard Socher, Jeffrey Pennington, Eric H. Huang,
Andrew Y. Ng, and Christopher D. Manning. 2011.
Semi-supervised recursive autoencoders for predict-
ing sentiment distributions. In Proceedings of the
</reference>
<page confidence="0.826538">
1687
</page>
<reference confidence="0.999762341463415">
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 151–161, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Richard Socher, Brody Huval, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Semantic composi-
tionality through recursive matrix-vector spaces. In
Proceedings of the 2012 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning, pages
1201–1211. Association for Computational Linguis-
tics.
Le Hai Son, Alexandre Allauzen, and François Yvon.
2012. Continuous space translation models with
neural networks. In Proceedings of the 2012 Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 39–48, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Drahomíra Spoustová, Jan Hajiˇc, Jan Votrubec, Pavel
Krbec, and Pavel Kvˇetoˇn. 2007. The best of two
worlds: Cooperation of statistical and rule-based
taggers for czech. In Proceedings of the Work-
shop on Balto-Slavonic Natural Language Process-
ing, pages 67–74, Prague, Czech Republic, June.
Association for Computational Linguistics.
Kristina Toutanova, Hisami Suzuki, and Achim Ruopp.
2008. Applying morphology generation models to
machine translation. In Proceedings of the Associa-
tion for Computational Linguistics.
Ashish Vaswani, Yinggong Zhao, Victoria Fossum,
and David Chiang. 2013. Decoding with large-
scale neural language models improves translation.
In Proceedings of the 2013 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1387–1392, Seattle, October.
Will Y. Zou, Richard Socher, Daniel Cer, and Christo-
pher D. Manning. 2013. Bilingual word embed-
dings for phrase-based machine translation. In Pro-
ceedings of the 2013 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1393–
1398, Seattle, USA, October.
</reference>
<page confidence="0.993464">
1688
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.705405">
<title confidence="0.9972555">Word Translation Prediction for Morphologically Rich with Bilingual Neural Networks</title>
<author confidence="0.929644">Ke Tran Arianna Bisazza Christof</author>
<affiliation confidence="0.924053">Informatics Institute, University of</affiliation>
<address confidence="0.769093">Science Park 904, 1098 XH Amsterdam, The</address>
<email confidence="0.973393">m.k.tran@uva.nl</email>
<email confidence="0.973393">a.bisazza@uva.nl</email>
<email confidence="0.973393">c.monz@uva.nl</email>
<abstract confidence="0.998256181818182">Translating into morphologically rich languages is a particularly difficult problem in machine translation due to the high degree of inflectional ambiguity in the target language, often only poorly captured by existing word translation models. We present a general approach that exploits source-side contexts of foreign words to improve translation prediction accuracy. Our approach is based on a probabilistic neural network which does not require linguistic annotation nor manual feature engineering. We report significant improvements in word translation prediction accuracy for three morphologically rich target languages. In addition, preliminary results for integrating our approach into a largescale English-Russian statistical machine translation system show small but statistically significant improvements in translation quality.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Michael Auli</author>
<author>Michel Galley</author>
<author>Chris Quirk</author>
<author>Geoffrey Zweig</author>
</authors>
<title>Joint language and translation modeling with recurrent neural networks.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1044--1054</pages>
<location>Seattle, Washington, USA,</location>
<contexts>
<context position="13183" citStr="Auli et al. (2013)" startWordPosition="2092" endWordPosition="2095">the forms observed in the target monolingual data, but not to those aligned to a source word s, which opens the possibility of generating inflected forms that are missing from the translation models. We leave this possibility to future work. 1678 have been used for monolingual target language modeling (Schwenk et al., 2006; Le et al., 2011; Duh et al., 2013; Vaswani et al., 2013), n-gram translation modeling (Son et al., 2012), phrase translation modeling (Schwenk, 2012; Zou et al., 2013; Gao et al., 2014) and minimal translation modeling (Hu et al., 2014). The recurrent neural network LMs of Auli et al. (2013) are primarily trained to predict target word sequences. However, they also experiment with an additional input layer representing source side context. Our models differ from most previous work in neural language modeling in that we predict a target translation given a source context while previous models predict the next word given a target word history. Unlike previous work in phrase translation modeling with NNs, our models have the advantage of accessing source context that can fall outside the phrase boundaries. We now describe our models in a general setting, predicting target translatio</context>
</contexts>
<marker>Auli, Galley, Quirk, Zweig, 2013</marker>
<rawString>Michael Auli, Michel Galley, Chris Quirk, and Geoffrey Zweig. 2013. Joint language and translation modeling with recurrent neural networks. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1044– 1054, Seattle, Washington, USA, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>Réjean Ducharme</author>
<author>Pascal Vincent</author>
<author>Christian Janvin</author>
</authors>
<title>A neural probabilistic language model.</title>
<date>2003</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>3--1137</pages>
<contexts>
<context position="12385" citStr="Bengio et al., 2003" startWordPosition="1960" endWordPosition="1963">dels automatically learn features that are relevant for each of the modeled tasks, directly from word-aligned data. To make the approach completely language independent, the word segmentation function g can be trained with an unsupervised segmentation tool. The effects of using different word segmentation techniques are discussed in Section 5. 4 Bilingual neural networks for translation prediction Probabilistic neural network (NN), or continuous space, language models have received increasing attention over the last few years and have been applied to several natural language processing tasks (Bengio et al., 2003; Collobert and Weston, 2008; Socher et al., 2011; Socher et al., 2012). Within statistical machine translation, they 4Note that our suffix generation function Mo is restricted to the forms observed in the target monolingual data, but not to those aligned to a source word s, which opens the possibility of generating inflected forms that are missing from the translation models. We leave this possibility to future work. 1678 have been used for monolingual target language modeling (Schwenk et al., 2006; Le et al., 2011; Duh et al., 2013; Vaswani et al., 2013), n-gram translation modeling (Son et </context>
<context position="13982" citStr="Bengio et al., 2003" startWordPosition="2219" endWordPosition="2222">previous work in neural language modeling in that we predict a target translation given a source context while previous models predict the next word given a target word history. Unlike previous work in phrase translation modeling with NNs, our models have the advantage of accessing source context that can fall outside the phrase boundaries. We now describe our models in a general setting, predicting target translations given a source context, where target translations can be either words, stems or suffixes.5 4.1 Neural network architecture Following a common approach in deep learning for NLP (Bengio et al., 2003; Collobert and Weston, 2008), we represent each source word si by a column vector rsi E Rd. Given a source context csi = si−k, ..., si, ..., si+k of k words on the left and k words on the right of si, the context representation rcsi E R(2k+1)xd is obtained by concatenating the vector representations of all words in csi: rcsi = rsi−k O ... O rsi+k Our main BNN architecture for word or stem prediction (Figure 2a) is a feed-forward neural network (FFNN) with one hidden layer, a matrix W1 E Rnx(2k+1)d connecting the input layer to the hidden layer, a matrix W2 E R|Vt|xn connecting the hidden laye</context>
</contexts>
<marker>Bengio, Ducharme, Vincent, Janvin, 2003</marker>
<rawString>Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and Christian Janvin. 2003. A neural probabilistic language model. The Journal of Machine Learning Research, 3:1137–1155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arianna Bisazza</author>
<author>Christof Monz</author>
</authors>
<title>Classbased language modeling for translating into morphologically rich languages.</title>
<date>2014</date>
<booktitle>In Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics,</booktitle>
<pages>1918--1927</pages>
<location>Dublin, Ireland.</location>
<contexts>
<context position="33140" citStr="Bisazza and Monz, 2014" startWordPosition="5407" endWordPosition="5410"> WMT2013 SMT system wmt12 (dev) wmt13 (test) Baseline 24.7 18.9 + stem/suff. BNN 25.1 19.3&apos; Base+suffLM 24.5 19.2 + word BNN 24.5 19.3 + stem/suff. BNN 24.7 19.6A Table 8: Effect of our BNN models on EnglishRussian translation quality (BLEU[%]). limit is set to 6 and for each source phrase the top 30 translation candidates are considered. When translating into a morphologically rich language, data sparsity issues in the target language become particularly apparent. To compensate for this we also experiment with a 5-gram suffix-based LM in addition to the surface-based LM (Müller et al., 2012; Bisazza and Monz, 2014). The BNN models are integrated as additional log-probability feature functions (log PBNN-p): one feature for the word prediction model or two features for the stem and suffix models respectively, plus the penalty feature ε. Table 7 shows the data used to train our EnglishRussian SMT system. The feature weights for all approaches were tuned by using pairwise ranking optimization (Hopkins and May, 2011) on the wmt12 benchmark (Callison-Burch et al., 2012). During tuning, 14 PRO parameter estimation runs are performed in parallel on different samples of the n-best list after each decoder iterati</context>
</contexts>
<marker>Bisazza, Monz, 2014</marker>
<rawString>Arianna Bisazza and Christof Monz. 2014. Classbased language modeling for translating into morphologically rich languages. In Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics, pages 1918–1927, Dublin, Ireland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ondrej Bojar</author>
<author>Zdenek Žabokrtský</author>
<author>Ondrej Dusek</author>
<author>Petra Galuscáková</author>
<author>Martin Majlis</author>
<author>David Marecek</author>
<author>Jirí Marsík</author>
<author>Michal Novák</author>
<author>Martin Popel</author>
<author>Ales Tamchyna</author>
</authors>
<title>The joy of parallelism with czeng 1.0.</title>
<date>2012</date>
<journal>ELRA, European Language Resources Association.</journal>
<booktitle>In Proceedings of LREC2012,</booktitle>
<location>Istanbul, Turkey,</location>
<contexts>
<context position="21432" citStr="Bojar et al., 2012" startWordPosition="3480" endWordPosition="3483">h-Russian, we also consider translation prediction for Czech and Bulgarian. As members of the Slavic language family, Czech and Bulgarian are also characterized by highly inflectional morphology. Czech, like Russian, displays a very rich nominal inflection with as many as 14 declension paradigms. Bulgarian, unlike Russian, is not affected by case distinctions but is characterized by a definiteness suffix. 5.1 Experimental setup The following parallel corpora are used to train the BNN models: • English-Russian: WMT13 data (News Commentary and Yandex corpora); • English-Czech: CzEng 1.0 corpus (Bojar et al., 2012) (Web Pages and News sections); • English-Bulgarian: a mix of crawled news data, TED talks and Europarl proceedings. Detailed corpus statistics are given in Table 2. For each language pair, accuracies are measured on a held-out set of 10K parallel sentences. To prepare the candidate generation function, each dataset is first word-aligned with GIZA++, then a bilingual lexicon with maximum-likelihood probabilities (Pmle) is built from the symmetrized alignment. After some frequency and significance pruning,7 the top 200 translations sorted by Pmle(t|s) · Pmle(s|t) are kept as candidate word tran</context>
</contexts>
<marker>Bojar, Žabokrtský, Dusek, Galuscáková, Majlis, Marecek, Marsík, Novák, Popel, Tamchyna, 2012</marker>
<rawString>Ond&amp;quot;rej Bojar, Zden&amp;quot;ek Žabokrtský, Ond&amp;quot;rej Du&amp;quot;sek, Petra Galu&amp;quot;s&amp;quot;cáková, Martin Majli&amp;quot;s, David Mare&amp;quot;cek, Ji&amp;quot;rí Mar&amp;quot;sík, Michal Novák, Martin Popel, and Ale&amp;quot;s Tamchyna. 2012. The joy of parallelism with czeng 1.0. In Proceedings of LREC2012, Istanbul, Turkey, May. ELRA, European Language Resources Association.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ondˇrej Bojar</author>
<author>Christian Buck</author>
<author>Chris Callison-Burch</author>
<author>Christian Federmann</author>
<author>Barry Haddow</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Matt Post</author>
<author>Radu Soricut</author>
<author>Lucia Specia</author>
</authors>
<date>2013</date>
<booktitle>Findings of the 2013 Workshop on Statistical Machine Translation. In Proceedings of the Eighth Workshop on Statistical Machine Translation,</booktitle>
<pages>1--44</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="34027" citStr="Bojar et al., 2013" startWordPosition="5553" endWordPosition="5556">glishRussian SMT system. The feature weights for all approaches were tuned by using pairwise ranking optimization (Hopkins and May, 2011) on the wmt12 benchmark (Callison-Burch et al., 2012). During tuning, 14 PRO parameter estimation runs are performed in parallel on different samples of the n-best list after each decoder iteration. The weights of the individual PRO runs are then averaged and passed on to the next decoding iteration. Performing weight estimation independently for a number of samples corrects for some of the instability that can be caused by individual samples. The wmt13 set (Bojar et al., 2013) was used for testing. We use approximate randomization (Noreen, 1989) to test for statistically significant differences between runs (Riezler and Maxwell, 2005). Translation quality is measured with caseinsensitive BLEU[%] using one reference translation. As shown in Table 8, statistically significant improvements over the respective baseline (Baseline and Base+suffLM) are marked • at the p &lt; .01 level. Integrating our bilingual neural network approach into our SMT system yields small but statistically significant improvements of 0.4 BLEU over a competitive baseline. We can also see that it i</context>
</contexts>
<marker>Bojar, Buck, Callison-Burch, Federmann, Haddow, Koehn, Monz, Post, Soricut, Specia, 2013</marker>
<rawString>Ondˇrej Bojar, Christian Buck, Chris Callison-Burch, Christian Federmann, Barry Haddow, Philipp Koehn, Christof Monz, Matt Post, Radu Soricut, and Lucia Specia. 2013. Findings of the 2013 Workshop on Statistical Machine Translation. In Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 1–44, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan A Botha</author>
<author>Phil Blunsom</author>
</authors>
<title>Compositional Morphology for Word Representations and Language Modelling.</title>
<date>2014</date>
<booktitle>In Proceedings of the 31st International Conference on Machine Learning,</booktitle>
<location>Beijing, China,</location>
<contexts>
<context position="16103" citStr="Botha and Blunsom (2014)" startWordPosition="2578" endWordPosition="2581">g words if they lie outside the source context window. Hence, we consider two BNN variants: a log-bilinear model (LBL) and a convolutional neural network model (ConvNet). LBL could potentially address (i) by factorizing target representations into target stem and suffix representations whereas ConvNets offer the advantage of modeling variable input length (ii) (Kalchbrenner et al., 2014). Log-bilinear model. The FFNN models stem and suffix probabilities separately. A log-bilinear model instead could directly model word prediction through a factored representation of target words, similarly to Botha and Blunsom (2014). Thus, no probability mass would be wasted over stem-suffix combinations that are not in the candidate generation function. The LBL model specifies the conditional distribution for the word translation tj E Tsi given a source context csi: Pe(tj|csi) = exp (se (tj,csi)) (3) E exp(se(t&amp;quot;csi)) t&apos; ET We use an additional set of word representations qtj E Rn for target translations tj. The LBL model computes a predictive representation qˆ of a source context csi by taking a linear combination of the source word representations rsi+m with the position-dependent weight matrices Cm E Rnxd: k qˆ= E Cmr</context>
<context position="36732" citStr="Botha and Blunsom (2014)" startWordPosition="5972" endWordPosition="5975">earch-space [top-30] 86.0% 85.0% reference/MT-search-space [MRR] 0.655 0.662 reference/MT-output 50.0% 50.7% stem-only reference/MT-output 12.3% 11.5% of which reachable 11.2% 10.3% Table 9: Target word coverage analysis of the English-Russian SMT system before and after adding the morphological BNN models. 1684 like the target LM, that are based on traditional maximum-likelihood estimates. While the suffixbased LMs proved beneficial in our experiments, we speculate that higher gains could be obtained by coupling our approach with a morphologyaware neural LM like the one recently presented by Botha and Blunsom (2014). 7 Related work While most relevant literature has been discussed in earlier sections, the following approaches are particularly related to ours: Minkov et al. (2007) and Toutanova et al. (2008) address target inflection prediction with a log-linear model based on rich morphological and syntactic features. Their model exploits target context and is applied to inflect the output of a stem-based SMT system, whereas our models predict target words (or pairs of stem-suffix) independently and are integrated into decoding. Chahuneau et al. (2013) address the same problem with another feature-rich d</context>
</contexts>
<marker>Botha, Blunsom, 2014</marker>
<rawString>Jan A. Botha and Phil Blunsom. 2014. Compositional Morphology for Word Representations and Language Modelling. In Proceedings of the 31st International Conference on Machine Learning, Beijing, China, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Matt Post</author>
<author>Radu Soricut</author>
<author>Lucia Specia</author>
</authors>
<title>Findings of the 2012 workshop on statistical machine translation.</title>
<date>2012</date>
<booktitle>In Proceedings of the Seventh Workshop on Statistical Machine Translation,</booktitle>
<pages>10--51</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montréal, Canada,</location>
<contexts>
<context position="33598" citStr="Callison-Burch et al., 2012" startWordPosition="5481" endWordPosition="5484">ularly apparent. To compensate for this we also experiment with a 5-gram suffix-based LM in addition to the surface-based LM (Müller et al., 2012; Bisazza and Monz, 2014). The BNN models are integrated as additional log-probability feature functions (log PBNN-p): one feature for the word prediction model or two features for the stem and suffix models respectively, plus the penalty feature ε. Table 7 shows the data used to train our EnglishRussian SMT system. The feature weights for all approaches were tuned by using pairwise ranking optimization (Hopkins and May, 2011) on the wmt12 benchmark (Callison-Burch et al., 2012). During tuning, 14 PRO parameter estimation runs are performed in parallel on different samples of the n-best list after each decoder iteration. The weights of the individual PRO runs are then averaged and passed on to the next decoding iteration. Performing weight estimation independently for a number of samples corrects for some of the instability that can be caused by individual samples. The wmt13 set (Bojar et al., 2013) was used for testing. We use approximate randomization (Noreen, 1989) to test for statistically significant differences between runs (Riezler and Maxwell, 2005). Translat</context>
</contexts>
<marker>Callison-Burch, Koehn, Monz, Post, Soricut, Specia, 2012</marker>
<rawString>Chris Callison-Burch, Philipp Koehn, Christof Monz, Matt Post, Radu Soricut, and Lucia Specia. 2012. Findings of the 2012 workshop on statistical machine translation. In Proceedings of the Seventh Workshop on Statistical Machine Translation, pages 10–51, Montréal, Canada, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Victor Chahuneau</author>
<author>Eva Schlinger</author>
<author>Noah A Smith</author>
<author>Chris Dyer</author>
</authors>
<title>Translating into morphologically rich languages with synthetic phrases.</title>
<date>2013</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1677--1687</pages>
<location>Seattle, USA,</location>
<contexts>
<context position="4301" citStr="Chahuneau et al., 2013" startWordPosition="646" endWordPosition="649">14 Association for Computational Linguistics pairs where the source language is morphologically poor, such as English, and the target language is morphologically rich, such as Russian, i.e., language pairs with a high degree of surface realization ambiguity (Minkov et al., 2007). To address this problem we propose a general approach based on bilingual neural networks (BNN) exploiting source-side contextual information. This paper makes a number of contributions: Unlike previous approaches our models do not require any form of linguistic annotation (Minkov et al., 2007; Kholy and Habash, 2012; Chahuneau et al., 2013), nor do they require any feature engineering (Gimpel and Smith, 2008). Moreover, besides directly predicting fully inflected forms as Jeong et al. (2010), our approach can also model stem and suffix prediction explicitly. Prediction accuracy is evaluated with respect to three morphologically rich target languages (Bulgarian, Czech, and Russian) showing that our approach consistently yields substantial improvements over a competitive baseline. We also show that these improvements in prediction accuracy can be beneficial in an end-to-end machine translation scenario by integrating into a large-</context>
<context position="9522" citStr="Chahuneau et al., 2013" startWordPosition="1477" endWordPosition="1480">enitive. Due to the rare word “Indiana/Ha,qHaaa”, the target LM must backoff to the bigram history and does not penalize this choice sufficiently. However, a model that has access to the word “of” in the near source context could bias the translation of “law” to the correct case. We then model P(tj|csi) with source context csi defined as a fixed-length word sequence centered around si: csi = si−k, ..., si, ..., si+k Our definition of context is similar to the n − 1 word history used in n-gram LMs. Similarly to previous work in source context-sensitive translation modeling (Jeong et al., 2010; Chahuneau et al., 2013), target words are predicted independently from each other, which allows for an efficient decoding integration. We are particularly interested in translating into morphologically rich languages where source context can provide useful information for the prediction of target translation, for example, the gender of the subject in a source sentence constrains the morphology of the translation of the source verb. Therefore, we integrate the notions of stem and suffix directly into the model. We assume the availability of a word segmentation function g that takes a target word t as input and return</context>
<context position="11640" citStr="Chahuneau et al., 2013" startWordPosition="1846" endWordPosition="1849">stem can produce for a given source. In practice, we use a pruned version of Ts to speed up training and reduce noise (see details in Section 5). As for the morphological models, given Ts and g, we can obtain Ls = {σ1,... , σk}, the set of possible target stem translations of s, and MQ = {µ1, ... , µl}, the set of possible suffixes for a target stem σ. The use of Ls, and MQ is similar to stemming and inflection operations in (Toutanova et al., 2008) while the set Ts is similar to the GEN function in (Jeong et al., 2010).4 Our approach differs crucially from previous work (Minkov et al., 2007; Chahuneau et al., 2013) in that it does not require linguistic features such as part-of-speech and syntactic tree on the source side. The proposed models automatically learn features that are relevant for each of the modeled tasks, directly from word-aligned data. To make the approach completely language independent, the word segmentation function g can be trained with an unsupervised segmentation tool. The effects of using different word segmentation techniques are discussed in Section 5. 4 Bilingual neural networks for translation prediction Probabilistic neural network (NN), or continuous space, language models h</context>
<context position="28234" citStr="Chahuneau et al. (2013)" startWordPosition="4586" endWordPosition="4589">Stem Acc. Suffix Acc. 1M 45.1 / 62.5 77.0 / 89.7 200K 44.6 / 61.8 75.7 / 88.6 Table 4: Accuracy at top-1/top-3 (%) of stem and suffix BNNs with different training data sizes. 5.5 Fine-grained evaluation We evaluate the suffix BNN model at the part-ofspeech (POS) level. Table 5 provides suffix prediction accuracy per POS for En-Ru. For this analysis, Russian data is segmented by TreeTag10http://snowball.tartarus.org/ algorithms/russian/stemmer.html 1682 ger. Additionally, we report the average number of suffixes per stem given the part-of-speech. Our results are consistent with the findings of Chahuneau et al. (2013):11 the prediction of adjectives is more difficult than that of other POS while Russian verb prediction is relatively easier in spite of the higher number of suffixes per stem. These differences reflect the importance of source versus target context features in the prediction of the target inflection: For instance, adjectives agree in gender with the nouns they modify, but this may be only inferred from the target context. POS A V N M P Acc. (%) 49.6 61.9 62.8 84.5 64.4 |Mo |18.2 18.4 9.2 7.1 13.3 Table 5: Suffix prediction accuracy at top-1 (%), breakdown by category (A: adjectives, V: verbs,</context>
<context position="30096" citStr="Chahuneau et al. (2013)" startWordPosition="4900" endWordPosition="4903">nt for the suffix model where the prediction of target suffix µj often does not depend linearly on si and σj. The predictive representation of target stem in the LBL stem model, however, mainly depends on the source representation rsi through a position dependent weight matrix C0. Thus, we observe a smaller accuracy drop in the stem model than in the suffix model. Conversely, the ConvNet performs poorly on stem prediction because it captures the meaning of the whole source context instead of emphasizing the importance of the source word si as the main predictor of the target translation tj. 11Chahuneau et al. (2013) report an average accuracy of 63.1% for the prediction of A, V, N, M suffixes. When we train our model on the same dataset (news-commentary) we obtain a comparable result (64.7% vs 63.1%). Unexpectedly, no improvement is obtained by the use of dropout regularizer (see Section 4.3). Model Stem Acc Suffix Acc FFNN 66.1 / 81.6 91.9 / 97.4 FFNN+do 64.6 / 81.1 91.5 / 97.5 LBL 63.6 / 79.6 86.4 / 96.4 ConvNet+do 58.6 / 75.6 90.3 / 96.9 Table 6: Accuracies at top-1/top-3 (%) of stem and suffix models. +do indicates dropout instead of L2 regularizer. FFNN is our main architecture. 6 SMT experiments Wh</context>
<context position="37279" citStr="Chahuneau et al. (2013)" startWordPosition="6055" endWordPosition="6058">yaware neural LM like the one recently presented by Botha and Blunsom (2014). 7 Related work While most relevant literature has been discussed in earlier sections, the following approaches are particularly related to ours: Minkov et al. (2007) and Toutanova et al. (2008) address target inflection prediction with a log-linear model based on rich morphological and syntactic features. Their model exploits target context and is applied to inflect the output of a stem-based SMT system, whereas our models predict target words (or pairs of stem-suffix) independently and are integrated into decoding. Chahuneau et al. (2013) address the same problem with another feature-rich discriminative model that can be integrated in decoding, like ours, but they also use it to inflect onthe-fly stemmed phrases. It is not clear what part of their SMT improvements is due to the generation of new phrases or to better scoring. Jeong et al. (2010) predict surface word forms in context, similarly to our word BNN, and integrate the scores into the SMT system. Unlike us, they rely on linguistic feature-rich log-linear models to do that. Gimpel and Smith (2008) propose a similar approach to directly predict phrases in context, instea</context>
</contexts>
<marker>Chahuneau, Schlinger, Smith, Dyer, 2013</marker>
<rawString>Victor Chahuneau, Eva Schlinger, Noah A. Smith, and Chris Dyer. 2013. Translating into morphologically rich languages with synthetic phrases. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1677–1687, Seattle, USA, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Joshua Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling.</title>
<date>1999</date>
<journal>Computer Speech and Language,</journal>
<volume>4</volume>
<issue>13</issue>
<contexts>
<context position="32159" citStr="Chen and Goodman, 1999" startWordPosition="5242" endWordPosition="5245">se phrase-based (Koehn et al., 2003) statistical machine translation system very similar to Moses (Koehn et al., 2007). All system runs use hierarchical lexicalized reordering (Galley and Manning, 2008; Cherry et al., 2012), distinguishing between monotone, swap, and discontinuous reordering, all with respect to left-to-right and right-to-left decoding. Other features include linear distortion, bidirectional lexical weighting (Koehn et al., 2003), word and phrase penalties, and finally a word-level 5- gram target LM trained on all available monolingual data with modified Kneser-Ney smoothing (Chen and Goodman, 1999). The distortion PBNN-p(˜s, ˜t, a) = |{ai } |Y_ PBNN(tj |csi) if |{ai} |&gt; 0 j∈{ail Pmle(NULL|si) 1 otherwise { � |˜s| i=1 1683 Table 7: SMT training and test data statistics. All numbers refer to tokenized, lowercased data. Corpus Lang. #Sent. #Tok. EN 1.9M 48.9M paral.train RU 45.9M Wiki dict. EN/RU 508K – mono.train RU 21.0M 390M WMT2012 3K 64K EN 3K 56K WMT2013 SMT system wmt12 (dev) wmt13 (test) Baseline 24.7 18.9 + stem/suff. BNN 25.1 19.3&apos; Base+suffLM 24.5 19.2 + word BNN 24.5 19.3 + stem/suff. BNN 24.7 19.6A Table 8: Effect of our BNN models on EnglishRussian translation quality (BLEU[%</context>
</contexts>
<marker>Chen, Goodman, 1999</marker>
<rawString>Stanley F. Chen and Joshua Goodman. 1999. An empirical study of smoothing techniques for language modeling. Computer Speech and Language, 4(13):359–393.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Cherry</author>
<author>Robert C Moore</author>
<author>Chris Quirk</author>
</authors>
<title>On hierarchical re-ordering and permutation parsing for phrase-based decoding.</title>
<date>2012</date>
<booktitle>In Proceedings of the Seventh Workshop on Statistical Machine Translation,</booktitle>
<pages>200--209</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montréal, Canada,</location>
<contexts>
<context position="31759" citStr="Cherry et al., 2012" startWordPosition="5185" endWordPosition="5188">d as follows: where a is the word-level alignment of the phrase pair (˜s, ˜t) and {ai} is the set of target positions aligned to si. If a source-target link cannot be scored by the BNN model, we give it a PBNN probability of 1 and increment a separate count feature ε. Note that the same phrase pair can get different BNN scores if used in different source side contexts. Our baseline is an in-house phrase-based (Koehn et al., 2003) statistical machine translation system very similar to Moses (Koehn et al., 2007). All system runs use hierarchical lexicalized reordering (Galley and Manning, 2008; Cherry et al., 2012), distinguishing between monotone, swap, and discontinuous reordering, all with respect to left-to-right and right-to-left decoding. Other features include linear distortion, bidirectional lexical weighting (Koehn et al., 2003), word and phrase penalties, and finally a word-level 5- gram target LM trained on all available monolingual data with modified Kneser-Ney smoothing (Chen and Goodman, 1999). The distortion PBNN-p(˜s, ˜t, a) = |{ai } |Y_ PBNN(tj |csi) if |{ai} |&gt; 0 j∈{ail Pmle(NULL|si) 1 otherwise { � |˜s| i=1 1683 Table 7: SMT training and test data statistics. All numbers refer to toke</context>
</contexts>
<marker>Cherry, Moore, Quirk, 2012</marker>
<rawString>Colin Cherry, Robert C. Moore, and Chris Quirk. 2012. On hierarchical re-ordering and permutation parsing for phrase-based decoding. In Proceedings of the Seventh Workshop on Statistical Machine Translation, pages 200–209, Montréal, Canada, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
</authors>
<title>A unified architecture for natural language processing: deep neural networks with multitask learning.</title>
<date>2008</date>
<booktitle>In Proceedings of the 25th Annual International Conference on Machine Learning,</booktitle>
<volume>12</volume>
<pages>2493--2537</pages>
<contexts>
<context position="12413" citStr="Collobert and Weston, 2008" startWordPosition="1964" endWordPosition="1968">arn features that are relevant for each of the modeled tasks, directly from word-aligned data. To make the approach completely language independent, the word segmentation function g can be trained with an unsupervised segmentation tool. The effects of using different word segmentation techniques are discussed in Section 5. 4 Bilingual neural networks for translation prediction Probabilistic neural network (NN), or continuous space, language models have received increasing attention over the last few years and have been applied to several natural language processing tasks (Bengio et al., 2003; Collobert and Weston, 2008; Socher et al., 2011; Socher et al., 2012). Within statistical machine translation, they 4Note that our suffix generation function Mo is restricted to the forms observed in the target monolingual data, but not to those aligned to a source word s, which opens the possibility of generating inflected forms that are missing from the translation models. We leave this possibility to future work. 1678 have been used for monolingual target language modeling (Schwenk et al., 2006; Le et al., 2011; Duh et al., 2013; Vaswani et al., 2013), n-gram translation modeling (Son et al., 2012), phrase translati</context>
<context position="14011" citStr="Collobert and Weston, 2008" startWordPosition="2223" endWordPosition="2227">al language modeling in that we predict a target translation given a source context while previous models predict the next word given a target word history. Unlike previous work in phrase translation modeling with NNs, our models have the advantage of accessing source context that can fall outside the phrase boundaries. We now describe our models in a general setting, predicting target translations given a source context, where target translations can be either words, stems or suffixes.5 4.1 Neural network architecture Following a common approach in deep learning for NLP (Bengio et al., 2003; Collobert and Weston, 2008), we represent each source word si by a column vector rsi E Rd. Given a source context csi = si−k, ..., si, ..., si+k of k words on the left and k words on the right of si, the context representation rcsi E R(2k+1)xd is obtained by concatenating the vector representations of all words in csi: rcsi = rsi−k O ... O rsi+k Our main BNN architecture for word or stem prediction (Figure 2a) is a feed-forward neural network (FFNN) with one hidden layer, a matrix W1 E Rnx(2k+1)d connecting the input layer to the hidden layer, a matrix W2 E R|Vt|xn connecting the hidden layer to the output layer, and a </context>
</contexts>
<marker>Collobert, Weston, 2008</marker>
<rawString>Ronan Collobert and Jason Weston. 2008. A unified architecture for natural language processing: deep neural networks with multitask learning. In Proceedings of the 25th Annual International Conference on Machine Learning, volume 12, pages 2493– 2537.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Duh</author>
<author>Graham Neubig</author>
<author>Katsuhito Sudoh</author>
<author>Hajime Tsukada</author>
</authors>
<title>Adaptation data selection using neural language models: Experiments in machine translation.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>678--683</pages>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="12924" citStr="Duh et al., 2013" startWordPosition="2050" endWordPosition="2053">een applied to several natural language processing tasks (Bengio et al., 2003; Collobert and Weston, 2008; Socher et al., 2011; Socher et al., 2012). Within statistical machine translation, they 4Note that our suffix generation function Mo is restricted to the forms observed in the target monolingual data, but not to those aligned to a source word s, which opens the possibility of generating inflected forms that are missing from the translation models. We leave this possibility to future work. 1678 have been used for monolingual target language modeling (Schwenk et al., 2006; Le et al., 2011; Duh et al., 2013; Vaswani et al., 2013), n-gram translation modeling (Son et al., 2012), phrase translation modeling (Schwenk, 2012; Zou et al., 2013; Gao et al., 2014) and minimal translation modeling (Hu et al., 2014). The recurrent neural network LMs of Auli et al. (2013) are primarily trained to predict target word sequences. However, they also experiment with an additional input layer representing source side context. Our models differ from most previous work in neural language modeling in that we predict a target translation given a source context while previous models predict the next word given a targ</context>
</contexts>
<marker>Duh, Neubig, Sudoh, Tsukada, 2013</marker>
<rawString>Kevin Duh, Graham Neubig, Katsuhito Sudoh, and Hajime Tsukada. 2013. Adaptation data selection using neural language models: Experiments in machine translation. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 678–683, Sofia, Bulgaria, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nadir Durrani</author>
<author>Alexander Fraser</author>
<author>Helmut Schmid</author>
</authors>
<title>Model with minimal translation units, but decode with phrases.</title>
<date>2013</date>
<booktitle>In Proceedings of Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>1--11</pages>
<location>Atlanta, Georgia, USA,</location>
<contexts>
<context position="1410" citStr="Durrani et al. (2013)" startWordPosition="195" endWordPosition="198">significant improvements in word translation prediction accuracy for three morphologically rich target languages. In addition, preliminary results for integrating our approach into a largescale English-Russian statistical machine translation system show small but statistically significant improvements in translation quality. 1 Introduction The ability to make context-sensitive translation decisions is one of the major strengths of phrasebased SMT (PSMT). However, the way PSMT exploits source-language context has several limitations as pointed out, for instance, by Quirk and Menezes (2006) and Durrani et al. (2013). First, the amount of context used to translate a given input word depends on the phrase segmentation, with hypotheses resulting from different segmentations competing with one another. Another issue is that, given a phrase segmentation, each source phrase is translated independently from the others, which can be problematic especially for short phrases. As a result, the predictive translation of a source phrase does not access useful linguistic clues in the source sentence that are outside of the scope of the phrase. Lexical weighting tackles the problem of unreliable phrase probabilities, t</context>
</contexts>
<marker>Durrani, Fraser, Schmid, 2013</marker>
<rawString>Nadir Durrani, Alexander Fraser, and Helmut Schmid. 2013. Model with minimal translation units, but decode with phrases. In Proceedings of Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1–11, Atlanta, Georgia, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Christopher D Manning</author>
</authors>
<title>A simple and effective hierarchical phrase reordering model.</title>
<date>2008</date>
<booktitle>In EMNLP ’08: Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>848--856</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="31737" citStr="Galley and Manning, 2008" startWordPosition="5181" endWordPosition="5184">NN score PBNN-p is computed as follows: where a is the word-level alignment of the phrase pair (˜s, ˜t) and {ai} is the set of target positions aligned to si. If a source-target link cannot be scored by the BNN model, we give it a PBNN probability of 1 and increment a separate count feature ε. Note that the same phrase pair can get different BNN scores if used in different source side contexts. Our baseline is an in-house phrase-based (Koehn et al., 2003) statistical machine translation system very similar to Moses (Koehn et al., 2007). All system runs use hierarchical lexicalized reordering (Galley and Manning, 2008; Cherry et al., 2012), distinguishing between monotone, swap, and discontinuous reordering, all with respect to left-to-right and right-to-left decoding. Other features include linear distortion, bidirectional lexical weighting (Koehn et al., 2003), word and phrase penalties, and finally a word-level 5- gram target LM trained on all available monolingual data with modified Kneser-Ney smoothing (Chen and Goodman, 1999). The distortion PBNN-p(˜s, ˜t, a) = |{ai } |Y_ PBNN(tj |csi) if |{ai} |&gt; 0 j∈{ail Pmle(NULL|si) 1 otherwise { � |˜s| i=1 1683 Table 7: SMT training and test data statistics. All</context>
</contexts>
<marker>Galley, Manning, 2008</marker>
<rawString>Michel Galley and Christopher D. Manning. 2008. A simple and effective hierarchical phrase reordering model. In EMNLP ’08: Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 848–856, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianfeng Gao</author>
<author>Xiaodong He</author>
<author>Wen-tau Yih</author>
<author>Li Deng</author>
</authors>
<title>Learning continuous phrase representations for translation modeling.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>699--709</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="13076" citStr="Gao et al., 2014" startWordPosition="2074" endWordPosition="2077">ithin statistical machine translation, they 4Note that our suffix generation function Mo is restricted to the forms observed in the target monolingual data, but not to those aligned to a source word s, which opens the possibility of generating inflected forms that are missing from the translation models. We leave this possibility to future work. 1678 have been used for monolingual target language modeling (Schwenk et al., 2006; Le et al., 2011; Duh et al., 2013; Vaswani et al., 2013), n-gram translation modeling (Son et al., 2012), phrase translation modeling (Schwenk, 2012; Zou et al., 2013; Gao et al., 2014) and minimal translation modeling (Hu et al., 2014). The recurrent neural network LMs of Auli et al. (2013) are primarily trained to predict target word sequences. However, they also experiment with an additional input layer representing source side context. Our models differ from most previous work in neural language modeling in that we predict a target translation given a source context while previous models predict the next word given a target word history. Unlike previous work in phrase translation modeling with NNs, our models have the advantage of accessing source context that can fall o</context>
</contexts>
<marker>Gao, He, Yih, Deng, 2014</marker>
<rawString>Jianfeng Gao, Xiaodong He, Wen-tau Yih, and Li Deng. 2014. Learning continuous phrase representations for translation modeling. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 699–709. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Gimpel</author>
<author>Noah A Smith</author>
</authors>
<title>Rich sourceside context for statistical machine translation.</title>
<date>2008</date>
<booktitle>In Proceedings of the Third Workshop on Statistical Machine Translation,</booktitle>
<pages>9--17</pages>
<location>Columbus, Ohio, USA.</location>
<contexts>
<context position="2589" citStr="Gimpel and Smith, 2008" startWordPosition="380" endWordPosition="384">oblem of unreliable phrase probabilities, typically associated with long phrases, but does not alleviate the problem of context segmentation. An important share of the translation selection task is then left to the language model (LM), which is certainly very effective but can only leverage target language context. Moreover, decisions that are taken at early decoding stages—such as the common practice of retaining only top n translation options for each source span—depend only on the translation models and on the target context available in the phrase. Source context based translation models (Gimpel and Smith, 2008; Mauser et al., 2009; Jeong et al., 2010; Haque et al., 2011) naturally address these limitations. These models can exploit a boundless context of the input text, but they assume that target words can be predicted independently from each other, which makes them easy to integrate into state-of-the-art PSMT systems. Even though the independence assumption is made on the target side, these models have shown the benefits of utilizing source context, especially in translating into morphologically rich languages. One drawback of previous research on this topic, though, is that it relied on rich set</context>
<context position="4371" citStr="Gimpel and Smith, 2008" startWordPosition="658" endWordPosition="661">nguage is morphologically poor, such as English, and the target language is morphologically rich, such as Russian, i.e., language pairs with a high degree of surface realization ambiguity (Minkov et al., 2007). To address this problem we propose a general approach based on bilingual neural networks (BNN) exploiting source-side contextual information. This paper makes a number of contributions: Unlike previous approaches our models do not require any form of linguistic annotation (Minkov et al., 2007; Kholy and Habash, 2012; Chahuneau et al., 2013), nor do they require any feature engineering (Gimpel and Smith, 2008). Moreover, besides directly predicting fully inflected forms as Jeong et al. (2010), our approach can also model stem and suffix prediction explicitly. Prediction accuracy is evaluated with respect to three morphologically rich target languages (Bulgarian, Czech, and Russian) showing that our approach consistently yields substantial improvements over a competitive baseline. We also show that these improvements in prediction accuracy can be beneficial in an end-to-end machine translation scenario by integrating into a large-scale EnglishRussian PSMT system. Finally, a detailed analysis shows t</context>
<context position="37805" citStr="Gimpel and Smith (2008)" startWordPosition="6148" endWordPosition="6151">(or pairs of stem-suffix) independently and are integrated into decoding. Chahuneau et al. (2013) address the same problem with another feature-rich discriminative model that can be integrated in decoding, like ours, but they also use it to inflect onthe-fly stemmed phrases. It is not clear what part of their SMT improvements is due to the generation of new phrases or to better scoring. Jeong et al. (2010) predict surface word forms in context, similarly to our word BNN, and integrate the scores into the SMT system. Unlike us, they rely on linguistic feature-rich log-linear models to do that. Gimpel and Smith (2008) propose a similar approach to directly predict phrases in context, instead of words. All those approaches employed features that capture the global structure of source sentences, like dependency relations. By contrast, our models access only local context in the source sentence but they achieve accuracy gains comparably to models that also use global sentence structure. 8 Conclusions We have proposed a general approach to predict word translations in context using bilingual neural network architectures. Unlike previous NN approaches, we model word, stem and suffix distributions in the target </context>
</contexts>
<marker>Gimpel, Smith, 2008</marker>
<rawString>Kevin Gimpel and Noah A. Smith. 2008. Rich sourceside context for statistical machine translation. In Proceedings of the Third Workshop on Statistical Machine Translation, pages 9–17, Columbus, Ohio, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Spence Green</author>
<author>John DeNero</author>
</authors>
<title>A class-based agreement model for generating accurately inflected translations.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, ACL ’12,</booktitle>
<pages>146--155</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="5760" citStr="Green and DeNero (2012)" startWordPosition="868" endWordPosition="871"> coverage of SMT models The first question we ask is whether translation can be improved by a more accurate selection of the translation options already existing in the SMT models, as opposed to generating new options. To answer this question we measure the lexical coverage of a baseline PSMT system trained on English-Russian.1 We choose this language pair because of the morphological richness on the target side: Russian is characterized by a highly inflectional morphology with a particularly complex nominal declension (six core cases, three genders and two number categories). As suggested by Green and DeNero (2012), we compute the recall of reference tokens in the set of target tokens that the decoder could produce in a translation of the source, that is the target tokens of all phrase pairs that matched the input sentence 1Training data and SMT setup are described in Section 6. and that were actually used for decoding.2 We call this the decoder’s lexical search space. Then, we compare the reference/space recall against the reference/MT-output recall: that is, the percentage of reference tokens that also appeared in the 1-best translation output by the SMT system. Results for the WMT12 benchmark are pre</context>
</contexts>
<marker>Green, DeNero, 2012</marker>
<rawString>Spence Green and John DeNero. 2012. A class-based agreement model for generating accurately inflected translations. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, ACL ’12, pages 146–155, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rejwanul Haque</author>
<author>Sudip Kumar Naskar</author>
<author>Antal Bosch</author>
<author>Andy Way</author>
</authors>
<title>Integrating sourcelanguage context into phrase-based statistical machine translation.</title>
<date>2011</date>
<journal>Machine Translation,</journal>
<volume>25</volume>
<issue>3</issue>
<pages>285</pages>
<contexts>
<context position="2651" citStr="Haque et al., 2011" startWordPosition="393" endWordPosition="396">th long phrases, but does not alleviate the problem of context segmentation. An important share of the translation selection task is then left to the language model (LM), which is certainly very effective but can only leverage target language context. Moreover, decisions that are taken at early decoding stages—such as the common practice of retaining only top n translation options for each source span—depend only on the translation models and on the target context available in the phrase. Source context based translation models (Gimpel and Smith, 2008; Mauser et al., 2009; Jeong et al., 2010; Haque et al., 2011) naturally address these limitations. These models can exploit a boundless context of the input text, but they assume that target words can be predicted independently from each other, which makes them easy to integrate into state-of-the-art PSMT systems. Even though the independence assumption is made on the target side, these models have shown the benefits of utilizing source context, especially in translating into morphologically rich languages. One drawback of previous research on this topic, though, is that it relied on rich sets of manually designed features, which in turn required the av</context>
</contexts>
<marker>Haque, Naskar, Bosch, Way, 2011</marker>
<rawString>Rejwanul Haque, Sudip Kumar Naskar, Antal Bosch, and Andy Way. 2011. Integrating sourcelanguage context into phrase-based statistical machine translation. Machine Translation, 25(3):239– 285, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey E Hinton</author>
<author>Nitish Srivastava</author>
<author>Alex Krizhevsky</author>
<author>Ilya Sutskever</author>
<author>Ruslan R Salakhutdinov</author>
</authors>
<title>Improving neural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580.</title>
<date>2012</date>
<contexts>
<context position="19873" citStr="Hinton et al., 2012" startWordPosition="3222" endWordPosition="3226">|Ts |≤ 200 with our pruning settings (see Section 5.1), thus training time for one example does not depend on the vocabulary size. Our training criterion can be seen as a form of contrastive estimation (Smith and Eisner, 2005), however we explicitly move the probability mass from competing candidates to the correct translation candidate, thus obtaining more reliable estimates of the conditional probabilities. The BNN parameters are initialized randomly according to a zero-mean Gaussian. We regularize the models with L2. As an alternative to the L2 regularizer, we also experiment with dropout (Hinton et al., 2012), where the neurons are randomly zeroed out with dropout rate p. This technique is known to be useful in computer vision tasks but has been rarely used in NLP tasks. In FFNN, we use dropout after the hidden layer, while in ConvNet, dropout applies after the last convolutional layer. The dropout rate p is set to 0.3 in our exper1680 iments. We use rectified nonlinearities6 in FFNN and after each convolutional layer in ConvNet. We train our BNN models with the standard stochastic gradient descent. source word representations to 100 in all experiments. The number of hidden units in our feedforwar</context>
</contexts>
<marker>Hinton, Srivastava, Krizhevsky, Sutskever, Salakhutdinov, 2012</marker>
<rawString>Geoffrey E. Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R. Salakhutdinov. 2012. Improving neural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hopkins</author>
<author>Jonathan May</author>
</authors>
<title>Tuning as ranking.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1352--1362</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Edinburgh, Scotland, UK.,</location>
<contexts>
<context position="33545" citStr="Hopkins and May, 2011" startWordPosition="5473" endWordPosition="5476">ity issues in the target language become particularly apparent. To compensate for this we also experiment with a 5-gram suffix-based LM in addition to the surface-based LM (Müller et al., 2012; Bisazza and Monz, 2014). The BNN models are integrated as additional log-probability feature functions (log PBNN-p): one feature for the word prediction model or two features for the stem and suffix models respectively, plus the penalty feature ε. Table 7 shows the data used to train our EnglishRussian SMT system. The feature weights for all approaches were tuned by using pairwise ranking optimization (Hopkins and May, 2011) on the wmt12 benchmark (Callison-Burch et al., 2012). During tuning, 14 PRO parameter estimation runs are performed in parallel on different samples of the n-best list after each decoder iteration. The weights of the individual PRO runs are then averaged and passed on to the next decoding iteration. Performing weight estimation independently for a number of samples corrects for some of the instability that can be caused by individual samples. The wmt13 set (Bojar et al., 2013) was used for testing. We use approximate randomization (Noreen, 1989) to test for statistically significant differenc</context>
</contexts>
<marker>Hopkins, May, 2011</marker>
<rawString>Mark Hopkins and Jonathan May. 2011. Tuning as ranking. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1352–1362, Edinburgh, Scotland, UK., July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuening Hu</author>
<author>Michael Auli</author>
<author>Qin Gao</author>
<author>Jianfeng Gao</author>
</authors>
<title>Minimum translation modeling with recurrent neural networks.</title>
<date>2014</date>
<booktitle>In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>pages</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Gothenburg, Sweden,</location>
<contexts>
<context position="13127" citStr="Hu et al., 2014" startWordPosition="2082" endWordPosition="2085">at our suffix generation function Mo is restricted to the forms observed in the target monolingual data, but not to those aligned to a source word s, which opens the possibility of generating inflected forms that are missing from the translation models. We leave this possibility to future work. 1678 have been used for monolingual target language modeling (Schwenk et al., 2006; Le et al., 2011; Duh et al., 2013; Vaswani et al., 2013), n-gram translation modeling (Son et al., 2012), phrase translation modeling (Schwenk, 2012; Zou et al., 2013; Gao et al., 2014) and minimal translation modeling (Hu et al., 2014). The recurrent neural network LMs of Auli et al. (2013) are primarily trained to predict target word sequences. However, they also experiment with an additional input layer representing source side context. Our models differ from most previous work in neural language modeling in that we predict a target translation given a source context while previous models predict the next word given a target word history. Unlike previous work in phrase translation modeling with NNs, our models have the advantage of accessing source context that can fall outside the phrase boundaries. We now describe our m</context>
</contexts>
<marker>Hu, Auli, Gao, Gao, 2014</marker>
<rawString>Yuening Hu, Michael Auli, Qin Gao, and Jianfeng Gao. 2014. Minimum translation modeling with recurrent neural networks. In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 20–29, Gothenburg, Sweden, April. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minwoo Jeong</author>
<author>Kristina Toutanova</author>
<author>Hisami Suzuki</author>
<author>Chris Quirk</author>
</authors>
<title>A discriminative lexicon model for complex morphology.</title>
<date>2010</date>
<booktitle>In The Ninth Conference of the Association for Machine Translation in the Americas.</booktitle>
<contexts>
<context position="2630" citStr="Jeong et al., 2010" startWordPosition="389" endWordPosition="392">ically associated with long phrases, but does not alleviate the problem of context segmentation. An important share of the translation selection task is then left to the language model (LM), which is certainly very effective but can only leverage target language context. Moreover, decisions that are taken at early decoding stages—such as the common practice of retaining only top n translation options for each source span—depend only on the translation models and on the target context available in the phrase. Source context based translation models (Gimpel and Smith, 2008; Mauser et al., 2009; Jeong et al., 2010; Haque et al., 2011) naturally address these limitations. These models can exploit a boundless context of the input text, but they assume that target words can be predicted independently from each other, which makes them easy to integrate into state-of-the-art PSMT systems. Even though the independence assumption is made on the target side, these models have shown the benefits of utilizing source context, especially in translating into morphologically rich languages. One drawback of previous research on this topic, though, is that it relied on rich sets of manually designed features, which in</context>
<context position="4455" citStr="Jeong et al. (2010)" startWordPosition="670" endWordPosition="673">lly rich, such as Russian, i.e., language pairs with a high degree of surface realization ambiguity (Minkov et al., 2007). To address this problem we propose a general approach based on bilingual neural networks (BNN) exploiting source-side contextual information. This paper makes a number of contributions: Unlike previous approaches our models do not require any form of linguistic annotation (Minkov et al., 2007; Kholy and Habash, 2012; Chahuneau et al., 2013), nor do they require any feature engineering (Gimpel and Smith, 2008). Moreover, besides directly predicting fully inflected forms as Jeong et al. (2010), our approach can also model stem and suffix prediction explicitly. Prediction accuracy is evaluated with respect to three morphologically rich target languages (Bulgarian, Czech, and Russian) showing that our approach consistently yields substantial improvements over a competitive baseline. We also show that these improvements in prediction accuracy can be beneficial in an end-to-end machine translation scenario by integrating into a large-scale EnglishRussian PSMT system. Finally, a detailed analysis shows that our approach induces a positive bias on phrase translation probabilities leading</context>
<context position="9497" citStr="Jeong et al., 2010" startWordPosition="1473" endWordPosition="1476">inative instead of genitive. Due to the rare word “Indiana/Ha,qHaaa”, the target LM must backoff to the bigram history and does not penalize this choice sufficiently. However, a model that has access to the word “of” in the near source context could bias the translation of “law” to the correct case. We then model P(tj|csi) with source context csi defined as a fixed-length word sequence centered around si: csi = si−k, ..., si, ..., si+k Our definition of context is similar to the n − 1 word history used in n-gram LMs. Similarly to previous work in source context-sensitive translation modeling (Jeong et al., 2010; Chahuneau et al., 2013), target words are predicted independently from each other, which allows for an efficient decoding integration. We are particularly interested in translating into morphologically rich languages where source context can provide useful information for the prediction of target translation, for example, the gender of the subject in a source sentence constrains the morphology of the translation of the source verb. Therefore, we integrate the notions of stem and suffix directly into the model. We assume the availability of a word segmentation function g that takes a target w</context>
<context position="11542" citStr="Jeong et al., 2010" startWordPosition="1831" endWordPosition="1834">parallel training corpus, which in turn corresponds to the set of target words that the SMT system can produce for a given source. In practice, we use a pruned version of Ts to speed up training and reduce noise (see details in Section 5). As for the morphological models, given Ts and g, we can obtain Ls = {σ1,... , σk}, the set of possible target stem translations of s, and MQ = {µ1, ... , µl}, the set of possible suffixes for a target stem σ. The use of Ls, and MQ is similar to stemming and inflection operations in (Toutanova et al., 2008) while the set Ts is similar to the GEN function in (Jeong et al., 2010).4 Our approach differs crucially from previous work (Minkov et al., 2007; Chahuneau et al., 2013) in that it does not require linguistic features such as part-of-speech and syntactic tree on the source side. The proposed models automatically learn features that are relevant for each of the modeled tasks, directly from word-aligned data. To make the approach completely language independent, the word segmentation function g can be trained with an unsupervised segmentation tool. The effects of using different word segmentation techniques are discussed in Section 5. 4 Bilingual neural networks fo</context>
<context position="37591" citStr="Jeong et al. (2010)" startWordPosition="6112" endWordPosition="6115">log-linear model based on rich morphological and syntactic features. Their model exploits target context and is applied to inflect the output of a stem-based SMT system, whereas our models predict target words (or pairs of stem-suffix) independently and are integrated into decoding. Chahuneau et al. (2013) address the same problem with another feature-rich discriminative model that can be integrated in decoding, like ours, but they also use it to inflect onthe-fly stemmed phrases. It is not clear what part of their SMT improvements is due to the generation of new phrases or to better scoring. Jeong et al. (2010) predict surface word forms in context, similarly to our word BNN, and integrate the scores into the SMT system. Unlike us, they rely on linguistic feature-rich log-linear models to do that. Gimpel and Smith (2008) propose a similar approach to directly predict phrases in context, instead of words. All those approaches employed features that capture the global structure of source sentences, like dependency relations. By contrast, our models access only local context in the source sentence but they achieve accuracy gains comparably to models that also use global sentence structure. 8 Conclusion</context>
</contexts>
<marker>Jeong, Toutanova, Suzuki, Quirk, 2010</marker>
<rawString>Minwoo Jeong, Kristina Toutanova, Hisami Suzuki, and Chris Quirk. 2010. A discriminative lexicon model for complex morphology. In The Ninth Conference of the Association for Machine Translation in the Americas.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nal Kalchbrenner</author>
<author>Phil Blunsom</author>
</authors>
<title>Recurrent continuous translation models.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1700--1709</pages>
<location>Seattle, USA,</location>
<contexts>
<context position="18132" citStr="Kalchbrenner and Blunsom, 2013" startWordPosition="2929" endWordPosition="2932"> btj}. Our log-bilinear model is a modification of the log-bilinear model proposed for n-gram language modeling in (Mnih and Hinton, 2007). Convolutional neural network model. This model (Figure 3) computes the predictive representation qˆ by applying a sequence of 2k convolutional layers {L1, ... , L2k}. The source context csi is represented as a matrix mcsi ∈ Rdx(2k+1): (6) mcsi = �rsi−k; ... ; rsi+k � neural network model are 0 = {rsi, mj, W}. Here, we focus on a fixed length input, however convolutional neural networks may be used to model variable length input (Kalchbrenner et al., 2014; Kalchbrenner and Blunsom, 2013). 4.3 Training In training, for each example (t, cs), we maximize the conditional probability Pθ(t|cs) of a correct target label t. The contribution of the training example (t, cs) to the gradient of the log conditional probability is given by: ∂ ∂ ∂0 log Pθ(t|cs) = ∂0sθ(t|cs) �− t�ETs Pθ(t�|cs)∂0∂ sθ(t&amp;quot; cs) q� rso rs, r12 rsi rs4 rs5 rss Figure 3: Convolutional neural network model. Edges with the same color indicate the same kernel weight matrix. Each convolutional layer Li consists of a onedimensional filter mi ∈ Rdx2. Each row of mi is convolved with the corresponding row in the previous l</context>
</contexts>
<marker>Kalchbrenner, Blunsom, 2013</marker>
<rawString>Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent continuous translation models. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1700–1709, Seattle, USA, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nal Kalchbrenner</author>
<author>Edward Grefenstette</author>
<author>Phil Blunsom</author>
</authors>
<title>A convolutional neural network for modelling sentences.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>655--665</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="15869" citStr="Kalchbrenner et al., 2014" startWordPosition="2544" endWordPosition="2548">ide a natural mechanism to compute word surface conditional probability p(t|cs) given individual stem probability p(σ|cs) and suffix probability p(µ|cs, σ), and (ii) FFNNs do not provide the flexibility to capture long dependencies among words if they lie outside the source context window. Hence, we consider two BNN variants: a log-bilinear model (LBL) and a convolutional neural network model (ConvNet). LBL could potentially address (i) by factorizing target representations into target stem and suffix representations whereas ConvNets offer the advantage of modeling variable input length (ii) (Kalchbrenner et al., 2014). Log-bilinear model. The FFNN models stem and suffix probabilities separately. A log-bilinear model instead could directly model word prediction through a factored representation of target words, similarly to Botha and Blunsom (2014). Thus, no probability mass would be wasted over stem-suffix combinations that are not in the candidate generation function. The LBL model specifies the conditional distribution for the word translation tj E Tsi given a source context csi: Pe(tj|csi) = exp (se (tj,csi)) (3) E exp(se(t&amp;quot;csi)) t&apos; ET We use an additional set of word representations qtj E Rn for target</context>
<context position="18099" citStr="Kalchbrenner et al., 2014" startWordPosition="2925" endWordPosition="2928"> are 0 = {rsi, Cm, qtj, bh, btj}. Our log-bilinear model is a modification of the log-bilinear model proposed for n-gram language modeling in (Mnih and Hinton, 2007). Convolutional neural network model. This model (Figure 3) computes the predictive representation qˆ by applying a sequence of 2k convolutional layers {L1, ... , L2k}. The source context csi is represented as a matrix mcsi ∈ Rdx(2k+1): (6) mcsi = �rsi−k; ... ; rsi+k � neural network model are 0 = {rsi, mj, W}. Here, we focus on a fixed length input, however convolutional neural networks may be used to model variable length input (Kalchbrenner et al., 2014; Kalchbrenner and Blunsom, 2013). 4.3 Training In training, for each example (t, cs), we maximize the conditional probability Pθ(t|cs) of a correct target label t. The contribution of the training example (t, cs) to the gradient of the log conditional probability is given by: ∂ ∂ ∂0 log Pθ(t|cs) = ∂0sθ(t|cs) �− t�ETs Pθ(t�|cs)∂0∂ sθ(t&amp;quot; cs) q� rso rs, r12 rsi rs4 rs5 rss Figure 3: Convolutional neural network model. Edges with the same color indicate the same kernel weight matrix. Each convolutional layer Li consists of a onedimensional filter mi ∈ Rdx2. Each row of mi is convolved with the co</context>
</contexts>
<marker>Kalchbrenner, Grefenstette, Blunsom, 2014</marker>
<rawString>Nal Kalchbrenner, Edward Grefenstette, and Phil Blunsom. 2014. A convolutional neural network for modelling sentences. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 655–665. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ahmed El Kholy</author>
<author>Nizar Habash</author>
</authors>
<title>Translate, predict or generate: Modeling rich morphology in statistical machine translation.</title>
<date>2012</date>
<booktitle>In Proceedings of the 16th Conference of the European Association for Machine Translation (EAMT).</booktitle>
<marker>El Kholy, Habash, 2012</marker>
<rawString>Ahmed El Kholy and Nizar Habash. 2012. Translate, predict or generate: Modeling rich morphology in statistical machine translation. In Proceedings of the 16th Conference of the European Association for Machine Translation (EAMT).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT-NAACL 2003,</booktitle>
<pages>127--133</pages>
<location>Edmonton, Canada.</location>
<contexts>
<context position="31572" citStr="Koehn et al., 2003" startWordPosition="5157" endWordPosition="5160">ate our translation prediction models described in Section 4 into our existing English-Russian SMT system. For each phrase pair matching the input, the phrase BNN score PBNN-p is computed as follows: where a is the word-level alignment of the phrase pair (˜s, ˜t) and {ai} is the set of target positions aligned to si. If a source-target link cannot be scored by the BNN model, we give it a PBNN probability of 1 and increment a separate count feature ε. Note that the same phrase pair can get different BNN scores if used in different source side contexts. Our baseline is an in-house phrase-based (Koehn et al., 2003) statistical machine translation system very similar to Moses (Koehn et al., 2007). All system runs use hierarchical lexicalized reordering (Galley and Manning, 2008; Cherry et al., 2012), distinguishing between monotone, swap, and discontinuous reordering, all with respect to left-to-right and right-to-left decoding. Other features include linear distortion, bidirectional lexical weighting (Koehn et al., 2003), word and phrase penalties, and finally a word-level 5- gram target LM trained on all available monolingual data with modified Kneser-Ney smoothing (Chen and Goodman, 1999). The distort</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of HLT-NAACL 2003, pages 127–133, Edmonton, Canada.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
<author>Chris Dyer</author>
<author>Ondrej Bojar</author>
<author>Alexandra Constantin</author>
<author>Evan Herbst</author>
</authors>
<title>Moses: Open Source Toolkit for Statistical Machine Translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions,</booktitle>
<pages>177--180</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="31654" citStr="Koehn et al., 2007" startWordPosition="5170" endWordPosition="5173">lish-Russian SMT system. For each phrase pair matching the input, the phrase BNN score PBNN-p is computed as follows: where a is the word-level alignment of the phrase pair (˜s, ˜t) and {ai} is the set of target positions aligned to si. If a source-target link cannot be scored by the BNN model, we give it a PBNN probability of 1 and increment a separate count feature ε. Note that the same phrase pair can get different BNN scores if used in different source side contexts. Our baseline is an in-house phrase-based (Koehn et al., 2003) statistical machine translation system very similar to Moses (Koehn et al., 2007). All system runs use hierarchical lexicalized reordering (Galley and Manning, 2008; Cherry et al., 2012), distinguishing between monotone, swap, and discontinuous reordering, all with respect to left-to-right and right-to-left decoding. Other features include linear distortion, bidirectional lexical weighting (Koehn et al., 2003), word and phrase penalties, and finally a word-level 5- gram target LM trained on all available monolingual data with modified Kneser-Ney smoothing (Chen and Goodman, 1999). The distortion PBNN-p(˜s, ˜t, a) = |{ai } |Y_ PBNN(tj |csi) if |{ai} |&gt; 0 j∈{ail Pmle(NULL|si</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, Herbst, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open Source Toolkit for Statistical Machine Translation. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions, pages 177–180, Prague, Czech Republic. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Pharaoh: A beam search decoder for phrase-based statistical machine translation models.</title>
<date>2004</date>
<booktitle>Proceedings of the 6th Conference of the Association for Machine Translations in the Americas (AMTA 2004),</booktitle>
<pages>115--124</pages>
<editor>In Robert E. Frederking and Kathryn B. Taylor, editors,</editor>
<contexts>
<context position="8147" citStr="Koehn (2004)" startWordPosition="1248" endWordPosition="1249">orphology, generation seems to be of secondary importance compared to better selection in our experimental setup. 3 Predicting word translations in context It is standard practice in PSMT to use wordto-word translation probabilities as an additional phrase score. More specifically, state-of-the-art PSMT systems employ the maximum-likelihood estimate of the context-independent probability of a target word given its aligned source word P(tj|si) for each word alignment link aij. 2This corresponds to the top 30 phrases sorted by weighted phrase, lexical and LM probabilities, for each source span. Koehn (2004) and our own experience suggest that using more phrases has little or no impact on MT quality. 3Word segmentation for this analysis is performed by the Russian Snowball stemmer, see also Section 5.3. 1677 [KOHCTMTyL MOHHOCTb] [MHAMaHa 3aKOH] [.] [the constitutionality of the] [indiana law] [.] Figure 1: Fragment of English sentence and its incorrect Russian translation produced by the baseline SMT system. Square brackets indicate phrase boundaries. The main goal of our work is to improve the estimation of such probabilities by exploiting the context of si, which in turn we expect will result i</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Pharaoh: A beam search decoder for phrase-based statistical machine translation models. In Robert E. Frederking and Kathryn B. Taylor, editors, Proceedings of the 6th Conference of the Association for Machine Translations in the Americas (AMTA 2004), pages 115–124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai-Son Le</author>
<author>Ilya Oparin</author>
<author>Alexandre Allauzen</author>
<author>J Gauvain</author>
<author>François Yvon</author>
</authors>
<title>Structured output layer neural network language model.</title>
<date>2011</date>
<booktitle>In Proceedings of Proceedings of ICASSP.</booktitle>
<contexts>
<context position="12906" citStr="Le et al., 2011" startWordPosition="2046" endWordPosition="2049"> years and have been applied to several natural language processing tasks (Bengio et al., 2003; Collobert and Weston, 2008; Socher et al., 2011; Socher et al., 2012). Within statistical machine translation, they 4Note that our suffix generation function Mo is restricted to the forms observed in the target monolingual data, but not to those aligned to a source word s, which opens the possibility of generating inflected forms that are missing from the translation models. We leave this possibility to future work. 1678 have been used for monolingual target language modeling (Schwenk et al., 2006; Le et al., 2011; Duh et al., 2013; Vaswani et al., 2013), n-gram translation modeling (Son et al., 2012), phrase translation modeling (Schwenk, 2012; Zou et al., 2013; Gao et al., 2014) and minimal translation modeling (Hu et al., 2014). The recurrent neural network LMs of Auli et al. (2013) are primarily trained to predict target word sequences. However, they also experiment with an additional input layer representing source side context. Our models differ from most previous work in neural language modeling in that we predict a target translation given a source context while previous models predict the next</context>
</contexts>
<marker>Le, Oparin, Allauzen, Gauvain, Yvon, 2011</marker>
<rawString>Hai-Son Le, Ilya Oparin, Alexandre Allauzen, J Gauvain, and François Yvon. 2011. Structured output layer neural network language model. In Proceedings of Proceedings of ICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoong Keok Lee</author>
<author>Aria Haghighi</author>
<author>Regina Barzilay</author>
</authors>
<title>Modeling syntactic context improves morphological segmentation.</title>
<date>2011</date>
<booktitle>In Proceedings of the Fifteenth Conference on Computational Natural Language Learning,</booktitle>
<pages>1--9</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="23718" citStr="Lee et al. (2011)" startWordPosition="3846" endWordPosition="3849">kelihood baseline Pmle(t|s) on which the PSMT lexical weighting score is based. Note that this is a more realistic baseline than the uniform models sometimes reported in the literature. The oracle corresponds to the percentage of aligned source-target word pairs in the held-out set that are covered by the candidate generation function. Out of the missing links, about 4% is due to lexicon pruning. Results for all three language pairs are presented in Table 3. In this series of experiments, the morphological BNNs utilize unsupervised segmentation models trained on each target language following Lee et al. (2011).8 As shown in Table 3, the BNN models outperform the baseline by a large margin in all tasks and languages. In particular, word prediction accuracy at top-1 increases by +6.4%, +24.6% and +9.0% absolute in English-Russian, English-Czech and English-Bulgarian respectively, without the use of any features based on linguistic annotation. While the baseline and oracle differences among languages can be explained by different levels of overlap between training and held-out set, we cannot easily explain why the Czech BNN performance is so much higher. When comparing the 8We use the C++ implementati</context>
</contexts>
<marker>Lee, Haghighi, Barzilay, 2011</marker>
<rawString>Yoong Keok Lee, Aria Haghighi, and Regina Barzilay. 2011. Modeling syntactic context improves morphological segmentation. In Proceedings of the Fifteenth Conference on Computational Natural Language Learning, pages 1–9, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arne Mauser</author>
<author>Saša Hasan</author>
<author>Hermann Ney</author>
</authors>
<title>Extending statistical machine translation with discriminative and trigger-based lexicon models.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 1 - Volume 1, EMNLP ’09,</booktitle>
<pages>210--218</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="2610" citStr="Mauser et al., 2009" startWordPosition="385" endWordPosition="388">se probabilities, typically associated with long phrases, but does not alleviate the problem of context segmentation. An important share of the translation selection task is then left to the language model (LM), which is certainly very effective but can only leverage target language context. Moreover, decisions that are taken at early decoding stages—such as the common practice of retaining only top n translation options for each source span—depend only on the translation models and on the target context available in the phrase. Source context based translation models (Gimpel and Smith, 2008; Mauser et al., 2009; Jeong et al., 2010; Haque et al., 2011) naturally address these limitations. These models can exploit a boundless context of the input text, but they assume that target words can be predicted independently from each other, which makes them easy to integrate into state-of-the-art PSMT systems. Even though the independence assumption is made on the target side, these models have shown the benefits of utilizing source context, especially in translating into morphologically rich languages. One drawback of previous research on this topic, though, is that it relied on rich sets of manually designe</context>
</contexts>
<marker>Mauser, Hasan, Ney, 2009</marker>
<rawString>Arne Mauser, Saša Hasan, and Hermann Ney. 2009. Extending statistical machine translation with discriminative and trigger-based lexicon models. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 1 - Volume 1, EMNLP ’09, pages 210–218, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Einat Minkov</author>
<author>Kristina Toutanova</author>
<author>Hisami Suzuki</author>
</authors>
<title>Generating complex morphology for machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>128--135</pages>
<contexts>
<context position="3957" citStr="Minkov et al., 2007" startWordPosition="593" endWordPosition="596">his paper, we specifically focus on improving the prediction accuracy for word translations. Achieving high levels of word translation accuracy is particularly challenging for language 1676 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1676–1688, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics pairs where the source language is morphologically poor, such as English, and the target language is morphologically rich, such as Russian, i.e., language pairs with a high degree of surface realization ambiguity (Minkov et al., 2007). To address this problem we propose a general approach based on bilingual neural networks (BNN) exploiting source-side contextual information. This paper makes a number of contributions: Unlike previous approaches our models do not require any form of linguistic annotation (Minkov et al., 2007; Kholy and Habash, 2012; Chahuneau et al., 2013), nor do they require any feature engineering (Gimpel and Smith, 2008). Moreover, besides directly predicting fully inflected forms as Jeong et al. (2010), our approach can also model stem and suffix prediction explicitly. Prediction accuracy is evaluated </context>
<context position="11615" citStr="Minkov et al., 2007" startWordPosition="1842" endWordPosition="1845">words that the SMT system can produce for a given source. In practice, we use a pruned version of Ts to speed up training and reduce noise (see details in Section 5). As for the morphological models, given Ts and g, we can obtain Ls = {σ1,... , σk}, the set of possible target stem translations of s, and MQ = {µ1, ... , µl}, the set of possible suffixes for a target stem σ. The use of Ls, and MQ is similar to stemming and inflection operations in (Toutanova et al., 2008) while the set Ts is similar to the GEN function in (Jeong et al., 2010).4 Our approach differs crucially from previous work (Minkov et al., 2007; Chahuneau et al., 2013) in that it does not require linguistic features such as part-of-speech and syntactic tree on the source side. The proposed models automatically learn features that are relevant for each of the modeled tasks, directly from word-aligned data. To make the approach completely language independent, the word segmentation function g can be trained with an unsupervised segmentation tool. The effects of using different word segmentation techniques are discussed in Section 5. 4 Bilingual neural networks for translation prediction Probabilistic neural network (NN), or continuous</context>
<context position="36899" citStr="Minkov et al. (2007)" startWordPosition="5997" endWordPosition="6000">1.2% 10.3% Table 9: Target word coverage analysis of the English-Russian SMT system before and after adding the morphological BNN models. 1684 like the target LM, that are based on traditional maximum-likelihood estimates. While the suffixbased LMs proved beneficial in our experiments, we speculate that higher gains could be obtained by coupling our approach with a morphologyaware neural LM like the one recently presented by Botha and Blunsom (2014). 7 Related work While most relevant literature has been discussed in earlier sections, the following approaches are particularly related to ours: Minkov et al. (2007) and Toutanova et al. (2008) address target inflection prediction with a log-linear model based on rich morphological and syntactic features. Their model exploits target context and is applied to inflect the output of a stem-based SMT system, whereas our models predict target words (or pairs of stem-suffix) independently and are integrated into decoding. Chahuneau et al. (2013) address the same problem with another feature-rich discriminative model that can be integrated in decoding, like ours, but they also use it to inflect onthe-fly stemmed phrases. It is not clear what part of their SMT im</context>
</contexts>
<marker>Minkov, Toutanova, Suzuki, 2007</marker>
<rawString>Einat Minkov, Kristina Toutanova, and Hisami Suzuki. 2007. Generating complex morphology for machine translation. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 128–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andriy Mnih</author>
<author>Geoffrey E Hinton</author>
</authors>
<title>Three new graphical models for statistical language modelling.</title>
<date>2007</date>
<booktitle>In Proceedings of the 24th International Conference on Machine Learning,</booktitle>
<pages>641--648</pages>
<location>New York, NY, USA.</location>
<contexts>
<context position="17639" citStr="Mnih and Hinton, 2007" startWordPosition="2844" endWordPosition="2847">1 rs%—k r, rs%+k rQ Figure 2: Feed-forward BNN architectures for predicting target translations: (a) word model (similar to stem model), and (b) suffix model with an additional vector representation rσ for target stems σ. Here btj is the bias term associated with target word tj. bh ∈ Rn are the representation biases. sθ(tj, csi) can be seen as the negative energy function of the target translation tj and its context csi. The parameters of the model thus are 0 = {rsi, Cm, qtj, bh, btj}. Our log-bilinear model is a modification of the log-bilinear model proposed for n-gram language modeling in (Mnih and Hinton, 2007). Convolutional neural network model. This model (Figure 3) computes the predictive representation qˆ by applying a sequence of 2k convolutional layers {L1, ... , L2k}. The source context csi is represented as a matrix mcsi ∈ Rdx(2k+1): (6) mcsi = �rsi−k; ... ; rsi+k � neural network model are 0 = {rsi, mj, W}. Here, we focus on a fixed length input, however convolutional neural networks may be used to model variable length input (Kalchbrenner et al., 2014; Kalchbrenner and Blunsom, 2013). 4.3 Training In training, for each example (t, cs), we maximize the conditional probability Pθ(t|cs) of a</context>
</contexts>
<marker>Mnih, Hinton, 2007</marker>
<rawString>Andriy Mnih and Geoffrey E. Hinton. 2007. Three new graphical models for statistical language modelling. In Proceedings of the 24th International Conference on Machine Learning, pages 641–648, New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Müller</author>
<author>Hinrich Schütze</author>
<author>Helmut Schmid</author>
</authors>
<title>A comparative investigation of morphological language modeling for the languages of the European Union.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>386--395</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montréal, Canada,</location>
<contexts>
<context position="33115" citStr="Müller et al., 2012" startWordPosition="5403" endWordPosition="5406">2012 3K 64K EN 3K 56K WMT2013 SMT system wmt12 (dev) wmt13 (test) Baseline 24.7 18.9 + stem/suff. BNN 25.1 19.3&apos; Base+suffLM 24.5 19.2 + word BNN 24.5 19.3 + stem/suff. BNN 24.7 19.6A Table 8: Effect of our BNN models on EnglishRussian translation quality (BLEU[%]). limit is set to 6 and for each source phrase the top 30 translation candidates are considered. When translating into a morphologically rich language, data sparsity issues in the target language become particularly apparent. To compensate for this we also experiment with a 5-gram suffix-based LM in addition to the surface-based LM (Müller et al., 2012; Bisazza and Monz, 2014). The BNN models are integrated as additional log-probability feature functions (log PBNN-p): one feature for the word prediction model or two features for the stem and suffix models respectively, plus the penalty feature ε. Table 7 shows the data used to train our EnglishRussian SMT system. The feature weights for all approaches were tuned by using pairwise ranking optimization (Hopkins and May, 2011) on the wmt12 benchmark (Callison-Burch et al., 2012). During tuning, 14 PRO parameter estimation runs are performed in parallel on different samples of the n-best list a</context>
</contexts>
<marker>Müller, Schütze, Schmid, 2012</marker>
<rawString>Thomas Müller, Hinrich Schütze, and Helmut Schmid. 2012. A comparative investigation of morphological language modeling for the languages of the European Union. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 386–395, Montréal, Canada, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric W Noreen</author>
</authors>
<title>Computer Intensive Methods for Testing Hypotheses. An Introduction.</title>
<date>1989</date>
<publisher>WileyInterscience.</publisher>
<contexts>
<context position="34097" citStr="Noreen, 1989" startWordPosition="5565" endWordPosition="5566">y using pairwise ranking optimization (Hopkins and May, 2011) on the wmt12 benchmark (Callison-Burch et al., 2012). During tuning, 14 PRO parameter estimation runs are performed in parallel on different samples of the n-best list after each decoder iteration. The weights of the individual PRO runs are then averaged and passed on to the next decoding iteration. Performing weight estimation independently for a number of samples corrects for some of the instability that can be caused by individual samples. The wmt13 set (Bojar et al., 2013) was used for testing. We use approximate randomization (Noreen, 1989) to test for statistically significant differences between runs (Riezler and Maxwell, 2005). Translation quality is measured with caseinsensitive BLEU[%] using one reference translation. As shown in Table 8, statistically significant improvements over the respective baseline (Baseline and Base+suffLM) are marked • at the p &lt; .01 level. Integrating our bilingual neural network approach into our SMT system yields small but statistically significant improvements of 0.4 BLEU over a competitive baseline. We can also see that it is beneficial to add a suffix-based language model to the baseline syst</context>
</contexts>
<marker>Noreen, 1989</marker>
<rawString>Eric W. Noreen. 1989. Computer Intensive Methods for Testing Hypotheses. An Introduction. WileyInterscience.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Quirk</author>
<author>Arul Menezes</author>
</authors>
<title>Do we need phrases? challenging the conventional wisdom in statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference,</booktitle>
<pages>9--16</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>New York City, USA,</location>
<contexts>
<context position="1384" citStr="Quirk and Menezes (2006)" startWordPosition="190" endWordPosition="193">ature engineering. We report significant improvements in word translation prediction accuracy for three morphologically rich target languages. In addition, preliminary results for integrating our approach into a largescale English-Russian statistical machine translation system show small but statistically significant improvements in translation quality. 1 Introduction The ability to make context-sensitive translation decisions is one of the major strengths of phrasebased SMT (PSMT). However, the way PSMT exploits source-language context has several limitations as pointed out, for instance, by Quirk and Menezes (2006) and Durrani et al. (2013). First, the amount of context used to translate a given input word depends on the phrase segmentation, with hypotheses resulting from different segmentations competing with one another. Another issue is that, given a phrase segmentation, each source phrase is translated independently from the others, which can be problematic especially for short phrases. As a result, the predictive translation of a source phrase does not access useful linguistic clues in the source sentence that are outside of the scope of the phrase. Lexical weighting tackles the problem of unreliab</context>
</contexts>
<marker>Quirk, Menezes, 2006</marker>
<rawString>Chris Quirk and Arul Menezes. 2006. Do we need phrases? challenging the conventional wisdom in statistical machine translation. In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference, pages 9–16, New York City, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Riezler</author>
<author>John T Maxwell</author>
</authors>
<title>On some pitfalls in automatic evaluation and significance testing for MT.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization,</booktitle>
<pages>57--64</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="34188" citStr="Riezler and Maxwell, 2005" startWordPosition="5575" endWordPosition="5578">chmark (Callison-Burch et al., 2012). During tuning, 14 PRO parameter estimation runs are performed in parallel on different samples of the n-best list after each decoder iteration. The weights of the individual PRO runs are then averaged and passed on to the next decoding iteration. Performing weight estimation independently for a number of samples corrects for some of the instability that can be caused by individual samples. The wmt13 set (Bojar et al., 2013) was used for testing. We use approximate randomization (Noreen, 1989) to test for statistically significant differences between runs (Riezler and Maxwell, 2005). Translation quality is measured with caseinsensitive BLEU[%] using one reference translation. As shown in Table 8, statistically significant improvements over the respective baseline (Baseline and Base+suffLM) are marked • at the p &lt; .01 level. Integrating our bilingual neural network approach into our SMT system yields small but statistically significant improvements of 0.4 BLEU over a competitive baseline. We can also see that it is beneficial to add a suffix-based language model to the baseline system. The biggest improvement is obtained by combining the suffixbased language model and our</context>
</contexts>
<marker>Riezler, Maxwell, 2005</marker>
<rawString>Stefan Riezler and John T. Maxwell. 2005. On some pitfalls in automatic evaluation and significance testing for MT. In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, pages 57–64, Ann Arbor, Michigan, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Holger Schwenk</author>
<author>Daniel Dechelotte</author>
<author>Jean-Luc Gauvain</author>
</authors>
<title>Continuous space language models for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the COLING/ACL 2006 Conference,</booktitle>
<pages>723--730</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sydney, Australia,</location>
<contexts>
<context position="12889" citStr="Schwenk et al., 2006" startWordPosition="2042" endWordPosition="2045">tion over the last few years and have been applied to several natural language processing tasks (Bengio et al., 2003; Collobert and Weston, 2008; Socher et al., 2011; Socher et al., 2012). Within statistical machine translation, they 4Note that our suffix generation function Mo is restricted to the forms observed in the target monolingual data, but not to those aligned to a source word s, which opens the possibility of generating inflected forms that are missing from the translation models. We leave this possibility to future work. 1678 have been used for monolingual target language modeling (Schwenk et al., 2006; Le et al., 2011; Duh et al., 2013; Vaswani et al., 2013), n-gram translation modeling (Son et al., 2012), phrase translation modeling (Schwenk, 2012; Zou et al., 2013; Gao et al., 2014) and minimal translation modeling (Hu et al., 2014). The recurrent neural network LMs of Auli et al. (2013) are primarily trained to predict target word sequences. However, they also experiment with an additional input layer representing source side context. Our models differ from most previous work in neural language modeling in that we predict a target translation given a source context while previous models</context>
</contexts>
<marker>Schwenk, Dechelotte, Gauvain, 2006</marker>
<rawString>Holger Schwenk, Daniel Dechelotte, and Jean-Luc Gauvain. 2006. Continuous space language models for statistical machine translation. In Proceedings of the COLING/ACL 2006 Conference, pages 723– 730, Sydney, Australia, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Holger Schwenk</author>
</authors>
<title>Continuous space translation models for phrase-based statistical machine translation.</title>
<date>2012</date>
<booktitle>In Proceedings of COLING.</booktitle>
<contexts>
<context position="13039" citStr="Schwenk, 2012" startWordPosition="2068" endWordPosition="2069">l., 2011; Socher et al., 2012). Within statistical machine translation, they 4Note that our suffix generation function Mo is restricted to the forms observed in the target monolingual data, but not to those aligned to a source word s, which opens the possibility of generating inflected forms that are missing from the translation models. We leave this possibility to future work. 1678 have been used for monolingual target language modeling (Schwenk et al., 2006; Le et al., 2011; Duh et al., 2013; Vaswani et al., 2013), n-gram translation modeling (Son et al., 2012), phrase translation modeling (Schwenk, 2012; Zou et al., 2013; Gao et al., 2014) and minimal translation modeling (Hu et al., 2014). The recurrent neural network LMs of Auli et al. (2013) are primarily trained to predict target word sequences. However, they also experiment with an additional input layer representing source side context. Our models differ from most previous work in neural language modeling in that we predict a target translation given a source context while previous models predict the next word given a target word history. Unlike previous work in phrase translation modeling with NNs, our models have the advantage of acc</context>
</contexts>
<marker>Schwenk, 2012</marker>
<rawString>Holger Schwenk. 2012. Continuous space translation models for phrase-based statistical machine translation. In Proceedings of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Serge Sharoff</author>
<author>Mikhail Kopotev</author>
<author>Tomaz Erjavec</author>
<author>Anna Feldman</author>
<author>Dagmar Divjak</author>
</authors>
<title>Designing and evaluating a russian tagset.</title>
<date>2008</date>
<journal>European Language Resources Association</journal>
<booktitle>In Proceedings of the Sixth International Conference on Language Resources and Evaluation (LREC’08),</booktitle>
<location>Marrakech,</location>
<contexts>
<context position="26267" citStr="Sharoff et al., 2008" startWordPosition="4269" endWordPosition="4272">er effect on SMT performance. On one hand, the word-level BNN achieves the highest gain over the MLE baseline. On the other, the stem- and suffix-level BNNs provide two separate scoring functions, whose weights can be directly tuned for translation quality. A preliminary answer to this question is given by the SMT experiments presented in Section 6. 5.3 Effect of word segmentation This section analyzes the effect of using different segmentation techniques. We consider two supervised tagging methods that produce lemma and inflection tag for each token in a context-sensitive manner: TreeTagger (Sharoff et al., 2008) for Russian and the Morce tagger (Spoustová et al., 2007) for Czech.9 Finally, we employ the Russian Snowball rule-based stemmer as a light-weight context9Annotation included in the CzEng 1.0 corpus release. Figure 4: Effect of different word segmentation techniques (U: unsupervised, S: supervised, R: rule-based stemmer) on stem and suffix prediction accuracy. The dark part of each bar stands for top1, the light one for top-3 accuracy. insensitive segmentation technique.10 As shown in Figure 4, accuracies for both stem and suffix prediction vary noticeably with the segmentation used. However,</context>
</contexts>
<marker>Sharoff, Kopotev, Erjavec, Feldman, Divjak, 2008</marker>
<rawString>Serge Sharoff, Mikhail Kopotev, Tomaz Erjavec, Anna Feldman, and Dagmar Divjak. 2008. Designing and evaluating a russian tagset. In Proceedings of the Sixth International Conference on Language Resources and Evaluation (LREC’08), Marrakech, Morocco. European Language Resources Association (ELRA). http://www.lrecconf.org/proceedings/lrec2008/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noah A Smith</author>
<author>Jason Eisner</author>
</authors>
<title>Contrastive estimation: Training log-linear models on unlabeled data.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics.</booktitle>
<contexts>
<context position="19479" citStr="Smith and Eisner, 2005" startWordPosition="3165" endWordPosition="3168">transforms the source context matrix mcsi to a feature vector qˆ ∈ Rd. A fully connected layer with weight matrix W followed by a softmax layer are placed after the last convolutional layer L2k to perform classification. The parameters of the convolutional Note that in the gradient, we do not sum over all target translations T but a set of possible candidates Ts of a source word s. In practice |Ts |≤ 200 with our pruning settings (see Section 5.1), thus training time for one example does not depend on the vocabulary size. Our training criterion can be seen as a form of contrastive estimation (Smith and Eisner, 2005), however we explicitly move the probability mass from competing candidates to the correct translation candidate, thus obtaining more reliable estimates of the conditional probabilities. The BNN parameters are initialized randomly according to a zero-mean Gaussian. We regularize the models with L2. As an alternative to the L2 regularizer, we also experiment with dropout (Hinton et al., 2012), where the neurons are randomly zeroed out with dropout rate p. This technique is known to be useful in computer vision tasks but has been rarely used in NLP tasks. In FFNN, we use dropout after the hidden</context>
</contexts>
<marker>Smith, Eisner, 2005</marker>
<rawString>Noah A. Smith and Jason Eisner. 2005. Contrastive estimation: Training log-linear models on unlabeled data. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Jeffrey Pennington</author>
<author>Eric H Huang</author>
<author>Andrew Y Ng</author>
<author>Christopher D Manning</author>
</authors>
<title>Semi-supervised recursive autoencoders for predicting sentiment distributions.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>151--161</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="12434" citStr="Socher et al., 2011" startWordPosition="1969" endWordPosition="1972">nt for each of the modeled tasks, directly from word-aligned data. To make the approach completely language independent, the word segmentation function g can be trained with an unsupervised segmentation tool. The effects of using different word segmentation techniques are discussed in Section 5. 4 Bilingual neural networks for translation prediction Probabilistic neural network (NN), or continuous space, language models have received increasing attention over the last few years and have been applied to several natural language processing tasks (Bengio et al., 2003; Collobert and Weston, 2008; Socher et al., 2011; Socher et al., 2012). Within statistical machine translation, they 4Note that our suffix generation function Mo is restricted to the forms observed in the target monolingual data, but not to those aligned to a source word s, which opens the possibility of generating inflected forms that are missing from the translation models. We leave this possibility to future work. 1678 have been used for monolingual target language modeling (Schwenk et al., 2006; Le et al., 2011; Duh et al., 2013; Vaswani et al., 2013), n-gram translation modeling (Son et al., 2012), phrase translation modeling (Schwenk,</context>
</contexts>
<marker>Socher, Pennington, Huang, Ng, Manning, 2011</marker>
<rawString>Richard Socher, Jeffrey Pennington, Eric H. Huang, Andrew Y. Ng, and Christopher D. Manning. 2011. Semi-supervised recursive autoencoders for predicting sentiment distributions. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 151–161, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Brody Huval</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Semantic compositionality through recursive matrix-vector spaces.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>1201--1211</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="12456" citStr="Socher et al., 2012" startWordPosition="1973" endWordPosition="1976">deled tasks, directly from word-aligned data. To make the approach completely language independent, the word segmentation function g can be trained with an unsupervised segmentation tool. The effects of using different word segmentation techniques are discussed in Section 5. 4 Bilingual neural networks for translation prediction Probabilistic neural network (NN), or continuous space, language models have received increasing attention over the last few years and have been applied to several natural language processing tasks (Bengio et al., 2003; Collobert and Weston, 2008; Socher et al., 2011; Socher et al., 2012). Within statistical machine translation, they 4Note that our suffix generation function Mo is restricted to the forms observed in the target monolingual data, but not to those aligned to a source word s, which opens the possibility of generating inflected forms that are missing from the translation models. We leave this possibility to future work. 1678 have been used for monolingual target language modeling (Schwenk et al., 2006; Le et al., 2011; Duh et al., 2013; Vaswani et al., 2013), n-gram translation modeling (Son et al., 2012), phrase translation modeling (Schwenk, 2012; Zou et al., 201</context>
</contexts>
<marker>Socher, Huval, Manning, Ng, 2012</marker>
<rawString>Richard Socher, Brody Huval, Christopher D. Manning, and Andrew Y. Ng. 2012. Semantic compositionality through recursive matrix-vector spaces. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1201–1211. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Le Hai Son</author>
<author>Alexandre Allauzen</author>
<author>François Yvon</author>
</authors>
<title>Continuous space translation models with neural networks.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>39--48</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="12995" citStr="Son et al., 2012" startWordPosition="2061" endWordPosition="2064">., 2003; Collobert and Weston, 2008; Socher et al., 2011; Socher et al., 2012). Within statistical machine translation, they 4Note that our suffix generation function Mo is restricted to the forms observed in the target monolingual data, but not to those aligned to a source word s, which opens the possibility of generating inflected forms that are missing from the translation models. We leave this possibility to future work. 1678 have been used for monolingual target language modeling (Schwenk et al., 2006; Le et al., 2011; Duh et al., 2013; Vaswani et al., 2013), n-gram translation modeling (Son et al., 2012), phrase translation modeling (Schwenk, 2012; Zou et al., 2013; Gao et al., 2014) and minimal translation modeling (Hu et al., 2014). The recurrent neural network LMs of Auli et al. (2013) are primarily trained to predict target word sequences. However, they also experiment with an additional input layer representing source side context. Our models differ from most previous work in neural language modeling in that we predict a target translation given a source context while previous models predict the next word given a target word history. Unlike previous work in phrase translation modeling wi</context>
</contexts>
<marker>Son, Allauzen, Yvon, 2012</marker>
<rawString>Le Hai Son, Alexandre Allauzen, and François Yvon. 2012. Continuous space translation models with neural networks. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 39–48, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Drahomíra Spoustová</author>
<author>Jan Hajiˇc</author>
<author>Jan Votrubec</author>
<author>Pavel Krbec</author>
<author>Pavel Kvˇetoˇn</author>
</authors>
<title>The best of two worlds: Cooperation of statistical and rule-based taggers for czech.</title>
<date>2007</date>
<booktitle>In Proceedings of the Workshop on Balto-Slavonic Natural Language Processing,</booktitle>
<pages>67--74</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<marker>Spoustová, Hajiˇc, Votrubec, Krbec, Kvˇetoˇn, 2007</marker>
<rawString>Drahomíra Spoustová, Jan Hajiˇc, Jan Votrubec, Pavel Krbec, and Pavel Kvˇetoˇn. 2007. The best of two worlds: Cooperation of statistical and rule-based taggers for czech. In Proceedings of the Workshop on Balto-Slavonic Natural Language Processing, pages 67–74, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Hisami Suzuki</author>
<author>Achim Ruopp</author>
</authors>
<title>Applying morphology generation models to machine translation.</title>
<date>2008</date>
<booktitle>In Proceedings of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="11470" citStr="Toutanova et al., 2008" startWordPosition="1816" endWordPosition="1819"> the set of target words Ts = {t1, ... , tm} that were aligned to si in the parallel training corpus, which in turn corresponds to the set of target words that the SMT system can produce for a given source. In practice, we use a pruned version of Ts to speed up training and reduce noise (see details in Section 5). As for the morphological models, given Ts and g, we can obtain Ls = {σ1,... , σk}, the set of possible target stem translations of s, and MQ = {µ1, ... , µl}, the set of possible suffixes for a target stem σ. The use of Ls, and MQ is similar to stemming and inflection operations in (Toutanova et al., 2008) while the set Ts is similar to the GEN function in (Jeong et al., 2010).4 Our approach differs crucially from previous work (Minkov et al., 2007; Chahuneau et al., 2013) in that it does not require linguistic features such as part-of-speech and syntactic tree on the source side. The proposed models automatically learn features that are relevant for each of the modeled tasks, directly from word-aligned data. To make the approach completely language independent, the word segmentation function g can be trained with an unsupervised segmentation tool. The effects of using different word segmentati</context>
<context position="36927" citStr="Toutanova et al. (2008)" startWordPosition="6002" endWordPosition="6005">t word coverage analysis of the English-Russian SMT system before and after adding the morphological BNN models. 1684 like the target LM, that are based on traditional maximum-likelihood estimates. While the suffixbased LMs proved beneficial in our experiments, we speculate that higher gains could be obtained by coupling our approach with a morphologyaware neural LM like the one recently presented by Botha and Blunsom (2014). 7 Related work While most relevant literature has been discussed in earlier sections, the following approaches are particularly related to ours: Minkov et al. (2007) and Toutanova et al. (2008) address target inflection prediction with a log-linear model based on rich morphological and syntactic features. Their model exploits target context and is applied to inflect the output of a stem-based SMT system, whereas our models predict target words (or pairs of stem-suffix) independently and are integrated into decoding. Chahuneau et al. (2013) address the same problem with another feature-rich discriminative model that can be integrated in decoding, like ours, but they also use it to inflect onthe-fly stemmed phrases. It is not clear what part of their SMT improvements is due to the gen</context>
</contexts>
<marker>Toutanova, Suzuki, Ruopp, 2008</marker>
<rawString>Kristina Toutanova, Hisami Suzuki, and Achim Ruopp. 2008. Applying morphology generation models to machine translation. In Proceedings of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ashish Vaswani</author>
<author>Yinggong Zhao</author>
<author>Victoria Fossum</author>
<author>David Chiang</author>
</authors>
<title>Decoding with largescale neural language models improves translation.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1387--1392</pages>
<location>Seattle,</location>
<contexts>
<context position="12947" citStr="Vaswani et al., 2013" startWordPosition="2054" endWordPosition="2057">eral natural language processing tasks (Bengio et al., 2003; Collobert and Weston, 2008; Socher et al., 2011; Socher et al., 2012). Within statistical machine translation, they 4Note that our suffix generation function Mo is restricted to the forms observed in the target monolingual data, but not to those aligned to a source word s, which opens the possibility of generating inflected forms that are missing from the translation models. We leave this possibility to future work. 1678 have been used for monolingual target language modeling (Schwenk et al., 2006; Le et al., 2011; Duh et al., 2013; Vaswani et al., 2013), n-gram translation modeling (Son et al., 2012), phrase translation modeling (Schwenk, 2012; Zou et al., 2013; Gao et al., 2014) and minimal translation modeling (Hu et al., 2014). The recurrent neural network LMs of Auli et al. (2013) are primarily trained to predict target word sequences. However, they also experiment with an additional input layer representing source side context. Our models differ from most previous work in neural language modeling in that we predict a target translation given a source context while previous models predict the next word given a target word history. Unlike</context>
</contexts>
<marker>Vaswani, Zhao, Fossum, Chiang, 2013</marker>
<rawString>Ashish Vaswani, Yinggong Zhao, Victoria Fossum, and David Chiang. 2013. Decoding with largescale neural language models improves translation. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1387–1392, Seattle, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Will Y Zou</author>
<author>Richard Socher</author>
<author>Daniel Cer</author>
<author>Christopher D Manning</author>
</authors>
<title>Bilingual word embeddings for phrase-based machine translation.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1393--1398</pages>
<location>Seattle, USA,</location>
<contexts>
<context position="13057" citStr="Zou et al., 2013" startWordPosition="2070" endWordPosition="2073">r et al., 2012). Within statistical machine translation, they 4Note that our suffix generation function Mo is restricted to the forms observed in the target monolingual data, but not to those aligned to a source word s, which opens the possibility of generating inflected forms that are missing from the translation models. We leave this possibility to future work. 1678 have been used for monolingual target language modeling (Schwenk et al., 2006; Le et al., 2011; Duh et al., 2013; Vaswani et al., 2013), n-gram translation modeling (Son et al., 2012), phrase translation modeling (Schwenk, 2012; Zou et al., 2013; Gao et al., 2014) and minimal translation modeling (Hu et al., 2014). The recurrent neural network LMs of Auli et al. (2013) are primarily trained to predict target word sequences. However, they also experiment with an additional input layer representing source side context. Our models differ from most previous work in neural language modeling in that we predict a target translation given a source context while previous models predict the next word given a target word history. Unlike previous work in phrase translation modeling with NNs, our models have the advantage of accessing source cont</context>
</contexts>
<marker>Zou, Socher, Cer, Manning, 2013</marker>
<rawString>Will Y. Zou, Richard Socher, Daniel Cer, and Christopher D. Manning. 2013. Bilingual word embeddings for phrase-based machine translation. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1393– 1398, Seattle, USA, October.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
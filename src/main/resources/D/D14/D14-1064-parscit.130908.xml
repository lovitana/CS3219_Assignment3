<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004521">
<title confidence="0.990062">
Learning to Translate: A Query-Specific Combination Approach for
Cross-Lingual Information Retrieval
</title>
<author confidence="0.802984">
Ferhan Ture
</author>
<affiliation confidence="0.701381">
Raytheon BBN Technologies
</affiliation>
<address confidence="0.952796">
10 Moulton St
Cambridge, MA, 02138 USA
</address>
<email confidence="0.99922">
fture@bbn.com
</email>
<sectionHeader confidence="0.997391" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999821695652174">
When documents and queries are pre-
sented in different languages, the com-
mon approach is to translate the query into
the document language. While there are
a variety of query translation approaches,
recent research suggests that combining
multiple methods into a single ”structured
query” is the most effective. In this pa-
per, we introduce a novel approach for
producing a unique combination recipe for
each query, as it has also been shown
that the optimal combination weights dif-
fer substantially across queries and other
task specifics. Our query-specific combi-
nation method generates statistically sig-
nificant improvements over other combi-
nation strategies presented in the litera-
ture, such as uniform and task-specific
weighting. An in-depth empirical anal-
ysis presents insights about the effect of
data size, domain differences, labeling and
tuning on the end performance of our ap-
proach.
</bodyText>
<sectionHeader confidence="0.999472" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999805133333333">
Cross-lingual information retrieval (CLIR) is a
special case of information retrieval (IR) in which
documents and queries are presented in different
languages. In order to overcome the language
barrier, the most commonly adopted method is
to translate queries into the document language.
Many methods have been introduced for translat-
ing queries for CLIR, ranging from word-by-word
dictionary lookups (Xu and Weischedel, 2005;
Darwish and Oard, 2003) to sophisticated use of
machine translation (MT) systems (Magdy and
Jones, 2011; Ma et al., 2012). Previous research
has shown that combining evidence from differ-
ent translation approaches is superior to any sin-
gle query translation method (Braschler, 2004;
</bodyText>
<note confidence="0.283756">
Elizabeth Boschee
Raytheon BBN Technologies
10 Moulton St
Cambridge, MA, 02138 USA
</note>
<email confidence="0.987144">
eboschee@bbn.com
</email>
<bodyText confidence="0.999626761904762">
Herbert et al., 2011). While there are numer-
ous combination-of-evidence techniques for both
mono-lingual and cross-lingual IR, recent work
suggests that there is no one-size-fits-all solution.
In fact, the optimal combination weights (i.e.,
weights assigned to each piece of evidence in a
linear combination) differ greatly across queries,
tasks, languages, and other variants (Ture et al.,
2012; Berger and Savoy, 2007).
In this paper, we introduce a novel method for
learning optimal combination weights when build-
ing a linear combination of existing query transla-
tion approaches. From standard query-document
relevance judgments we train a set of classifiers,
which produce a unique combination recipe for
each query, based on a large set of features ex-
tracted from the query and collection. Experi-
mental results show that the effectiveness of our
method is significantly higher than state-of-the-art
query translation methods and other combination
strategies.
</bodyText>
<sectionHeader confidence="0.999931" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999434277777778">
The earliest approaches to query translation for
CLIR used machine-readable bilingual dictio-
naries (Hull and Grefenstette, 1996; Balles-
teros and Croft, 1996), achieving around up to
60% of monolingual IR effectiveness. Xu and
Weischedel (2005) showed that effectiveness can
be increased to around 80% by weighting each
translation proportional to its rank in the dictio-
nary. The practice of weighting translation candi-
dates was later formulated as a “structured query”,
in which each query term is represented by a prob-
ability distribution over its translations in the doc-
ument language (Pirkola, 1998; Kwok, 1999; Dar-
wish and Oard, 2003). Our approach is based on
the structured query formulation.
Some of the earliest studies in IR discovered
that with different underlying models, the re-
trieved document set would vary substantially, al-
</bodyText>
<page confidence="0.981026">
589
</page>
<note confidence="0.9105165">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 589–599,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999940176470588">
though the effectiveness was similar (McGill et
al., 1979). Later studies showed that combining
different representations of the query and/or doc-
ument often produced superior output (Rajashekar
and Croft, 1995; Turtle and Croft, 1990; Fox,
1983). This intuitive idea was supported theoret-
ically by Pearl (1988), concluding that multiple
pieces of evidence estimates relevance more accu-
rately, but that the benefit strongly depends on the
quality and independence of each piece. Experi-
ments by Belkin et al. (1995) indicated the need to
properly weight each representation with respect
to its effectiveness. These so-called “combination-
of-evidence” techniques became more powerful
with the introduction of Indri, a probabilistic re-
trieval framework specifically designed for com-
bining multiple query and document representa-
tions (Metzler and Croft, 2005). Croft (2000) pro-
vides a detailed summary of earlier query combi-
nation approaches in IR, while Peters et al. (2012)
cites more recent related work.
The benefits of combination-of-evidence trans-
fer to the cross-lingual case especially well, since
the inherent ambiguity of translation readily pro-
vides a diverse set of representations. Most CLIR
approaches implement a post-retrieval merging
of ranked lists, each generated from different
query (Hiemstra et al., 2001; Savoy, 2001; Gey
et al., 2001; Chen and Gey, 2004) or docu-
ment (Lopez and Romary, 2009) representations,
also called “data fusion”. In contrast, we focus on
a pre-retrieval combination at the modeling stage,
so that a single complex query is used in retrieval,
instead of multiple simpler ones. Two advantages
of the former are easier implementation (since
the approach requires no changes to the modeling
side) and the possibly greater diversity that can be
achieved by having separate retrieval runs. How-
ever, each ranked list needs to be limited in size,
which might cause some potentially useful docu-
ments not to be considered in the combination at
all. Since the focus of this paper is on the model-
ing end of retrieval, pre-retrieval combination was
a more suitable choice, though we think that the
two approaches have complementary benefits.
The idea of combining query translations
before retrieval has been explored previously.
Braschler (2004) combines three translation ap-
proaches: output of an MT system, a novel trans-
lation approach based on a similarity thesaurus
built automatically from a comparable corpus,
and a dictionary-based translation. The main
reason that this combination does not provide
much benefit is due to the lower coverage of
the thesaurus-based and dictionary-based trans-
lation methods. A similar approach by Herbert
et al. (2011) uses Wikipedia to provide transla-
tions of certain phrases and entities, and combin-
ing that with the Google Translate MT sys-
tem yields statistically significant improvements
in English-to-German retrieval. More recently,
Ture et al. (2012) presented a more sophisti-
cated translation approach using the internal rep-
resentation of an MT system, and reported sta-
tistically significant improvements when a pre-
retrieval combination was performed.
All of the previously cited approaches either
use uniform weights for combination, or select
weights based on collection-level information.
However, as stated previously, numerous stud-
ies suggest that certain methods work better on
certain queries, collections, languages. In fact,
when weights are optimized separately on each
collection, they differ substantially across differ-
ent collections (Ture et al., 2012). For monolin-
gual retrieval, there has been a series of learning-
to-rank (LTR) papers that determine weights for
query concepts (Bendersky et al., 2011), such
that retrieval effectiveness is maximized. A re-
cent study extends this idea to the cross-lingual
case, by learning how to weight each translated
word for English-Persian CLIR (Azarbonyad et
al., 2013). In contrast, we extract translated word
weights from diverse and sophisticated translation
methods, then learn how to weight each trans-
lated structured query, We call this “learning-to-
translate” (LTT), which can be formulated as a
simpler learning problem. In CLIR, both LTR and
LTT are under-explored problems, with a common
goal of applying machine learning techniques to
improve query translation, yet with complemen-
tary benefits.
To our knowledge, there has been one prior LTT
approach: a classifier was trained to predict ef-
fectiveness of each query translation, using fea-
tures based on statistics of the query terms (Berger
and Savoy, 2007). Instead of weighting, the
translations with highest classifier scores were
concatenated, yielding statistically significant im-
provements over using the single-best translation
method. However, the translation methods ex-
plored in this paper are all based on one-best MT
</bodyText>
<page confidence="0.992987">
590
</page>
<bodyText confidence="0.9973605">
systems, making it difficult to draw strong conclu-
sions.
</bodyText>
<sectionHeader confidence="0.97804" genericHeader="method">
3 Query Translation
</sectionHeader>
<bodyText confidence="0.999927380952381">
The primary contribution of this paper is to show
how a diverse set of query translation (QT) meth-
ods can be combined effectively into a single
weighted structured query, with improved retrieval
effectiveness. While our approach can applied to
any set of translation methods, we focus on three
methods that have complementary strengths and
that have shown promise in CLIR: word-based
probabilistic translation, one-best MT, and n-best
probabilistic MT. We briefly present our imple-
mentation of each method; more details can be
found in earlier work (Darwish and Oard, 2003;
Ture et al., 2012).
Each QT method generates a representation of
the query in the document language. In the case of
word-based and n-best MT approaches, the repre-
sentation is a structured query itself, where each
query word is represented by a probability distri-
bution over translation alternatives. For one-best
MT, the query is represented by a bag of translated
words.
</bodyText>
<subsectionHeader confidence="0.995505">
3.1 One-Best MT
</subsectionHeader>
<bodyText confidence="0.996509">
A query translation approach that has become
more popular recently is to simply run the query
through an MT system, and use the best output as
the query:
</bodyText>
<equation confidence="0.990168">
t1t2 ... tl = MT(s1s2 ... sk) (1)
</equation>
<bodyText confidence="0.9996231">
where s = s1s2 ... sk is the query and t =
t1t2 ... tl is the translated query.
Since modern statistical MT systems generate
high-quality translations for many language pairs,
this one-best strategy works reasonably well for
retrieval and provides a competitive baseline. A
practical advantage of this approach is the ease of
implementation – one can simply use any MT in-
terface (e.g., Google Translate) as a black
box in their CLIR system.
</bodyText>
<subsectionHeader confidence="0.999882">
3.2 Probabilistic n-best MT
</subsectionHeader>
<bodyText confidence="0.999953208333333">
The top translation might sometimes be incorrect,
or might lack some of the alternative representa-
tions that are very useful in retrieval. Therefore,
considering the n highest scored translations (also
referred to as the n-best list in MT literature) has
become increasingly popular in CLIR approaches.
In order to benefit from the diversity amongst
the n-best translations, one can simply concate-
nate them together, forming a large list of query
terms. However, statistical MT systems also
assign probabilities to each translation, which
can be incorporated into the query representation
for better effectiveness, as suggested by Ture et
al. (2012).
In this approach, each of the top n transla-
tion candidates from the MT system are processed
one by one. For each translation candidate, the
MT system provides a translation probability, and
alignments between words in the query and its
translation. As we process each of the n transla-
tions, for each query word sz, we accumulate prob-
abilities on each translated word tzj aligned to sz.
Finally, we normalize the translation probabilities
to get Prnbest(tzj|sz).
</bodyText>
<subsectionHeader confidence="0.999534">
3.3 Word-based
</subsectionHeader>
<bodyText confidence="0.999996571428571">
One of the most widely used approaches in CLIR
is based on translating each query word sz in-
dependently, with probabilities assigned to each
translation candidate tzj. Translations are de-
rived automatically from a bilingual corpus using
statistical word alignment techniques, which are
used as part of the training of statistical MT sys-
tems (Brown et al., 1993). These probabilities can
be exploited for retrieval based on the technique
of Darwish and Oard (2003) for “projecting” text
into the document language. After cleaning up the
automatically learned translation probabilities (de-
tails omitted for space considerations), we end up
with the translation probabilities Prword(tzj|sz).
</bodyText>
<sectionHeader confidence="0.816332" genericHeader="method">
4 Combination of Evidence
</sectionHeader>
<bodyText confidence="0.99998675">
Once we have multiple ways to represent the query
q in the document language (QTz(q), i = 1... m),
it is possible to combine these “pieces of evi-
dence” into a single representation as follows:
</bodyText>
<equation confidence="0.9911545">
QT(q) = �m wz(q)QTz(q)
z=1
</equation>
<bodyText confidence="0.999209142857143">
and each combination-of-evidence approach dif-
fers by how the combination weights wz are com-
puted:
Uniform In this baseline method, we ignore any
information we have about the collection or query
and assign equal weights to each method (i.e.,
wz(q) = 1/m). In our case, this means a weight
</bodyText>
<page confidence="0.989384">
591
</page>
<bodyText confidence="0.9999569">
of 33.3% to each of the one-best, probabilistic n-
best, and word-based QT methods.
Task-specific We can optimize the combination
weights by overall effectiveness on a specific re-
trieval task. Given a query set and collection,
we perform a grid search on combination weights
(with a step interval of 0.1) and select the weights
that maximize retrieval effectiveness. The training
is performed in a leave-one-out manner: weights
for test query q are optimized on all queries except
for q.
Query-specific We propose a novel method to
compute combination weights specifically for
each query, resulting in a more customized op-
timization that can take into account how effec-
tiveness of each translation method varies across
queries.
In the remainder of this section, we describe
the details of our novel query-specific combina-
tion method.
</bodyText>
<subsectionHeader confidence="0.999486">
4.1 Overview of Query-Specific Combination
</subsectionHeader>
<bodyText confidence="0.999933722222222">
We present a novel approach for determining
query-specific combination weights by training a
classifier for each QT method. Prior to train-
ing the classifier, we first run retrieval using each
QT method, and evaluate the effectiveness of the
retrieved documents. The effectiveness of the
ith method on query q (i.e., fi(q)) is then con-
verted into a binary label (further described in
Section 4.2). Treating each query as a separate
instance, a classifier is trained for each method,
generating classifiers C1, ... , Cm. During re-
trieval (i.e., at test time), for each query q, each
trained classifier Ci is applied to the query, re-
sulting in a predicted label li(q) and the classi-
fier’s confidence in a positive label, Ci(q).1 These
values are then used to determine combination
weights w1(q), ... , wm(q) that are custom-fit for
the query.
</bodyText>
<subsectionHeader confidence="0.99498">
4.2 Labeling
</subsectionHeader>
<bodyText confidence="0.999845333333333">
First of all, we discard queries in which the dif-
ference between the best and worst performing
methods is small (specifically, the worst perform-
ing method scores at least k1% of the best per-
forming one). For such queries, generating fair
training labels is more difficult and therefore more
</bodyText>
<footnote confidence="0.476663">
1The confidence in a negative label is 1 − Ci(q).
</footnote>
<bodyText confidence="0.99993708">
likely to introduce noise into the process.2 More-
over, these are exactly the queries where choos-
ing optimal combination weights is less important
(since all methods perform relatively similarly), so
it is reasonable to exclude them from training. In
fact, a high number of such queries would indi-
cate lower potential for combination-of-evidence
approaches.
For each QT method i, we create training in-
stances per query, per retrieval task. Since our
goal is to select the best among existing methods,
the training label should reflect the effectiveness
of method i relative to other methods. A strategy
that we call best-by-measure assigns a label of 1
if the effectiveness of the ith method (i.e., fi(q)) is
at least k2% of the maximum effectiveness for that
query, and 0 otherwise. While this directly corre-
lates with retrieval effectiveness, labels might be
distributed in an unbalanced manner, which might
affect the training process negatively. A balanced
labeling requires sorting all training instances by
how much better the ith method is than other meth-
ods (maxi,=,4i(fi,(q)/fi(q))), and then assigning a
label of 1 to the lower half and 0 to the higher half.
This strategy is called best-by-rank.
</bodyText>
<subsectionHeader confidence="0.980057">
4.3 Features
</subsectionHeader>
<bodyText confidence="0.997654541666667">
We introduce a diverse set of features, in order to
train a robust classifier for predicting when each
QT method performs better and worse than others.
We split the feature set into four meaningful cate-
gories, so that we can measure the impact of each
subset separately:
Surface features These features do not require
a deep analysis of the query: (a) Number of words
in query and the translated query, (b) Type of
query that we automatically classify based on pre-
defined templates (e.g., fact question, cause-effect,
etc.), and (c) Number of stop words in the query
and the translated query.
Parse-based features These features are ex-
tracted from a deeper syntactic analysis of the
query text: (a) Number of related names found in
a named entity database, and (b) Existence of syn-
tactic constituents in query and its translation (e.g.,
“is there a VVB in the query parse tree”).
2We also experimented with including these queries with
a third label (e.g., “same”) and train a ternary classifier. Hav-
ing more labels requires more training data, which is not easy
to obtain for this task. Also, obtaining a balanced label dis-
tribution becomes even more difficult with three labels.
</bodyText>
<page confidence="0.987748">
592
</page>
<bodyText confidence="0.999869780487805">
Translation-based features These features
consist of statistics computed from the query and
its translation: (a) Number of query words that
were unaligned in at least half of the n-best query
translations, (b) Number of query words that were
aligned to multiple target words in at least half
of the n-best query translations, (c) Number of
query words that were self-aligned (i.e., target
word is exactly same string) in at least half of the
n-best query translations, (d) Average / Standard
deviation / Maximum / Minimum of entropy of
Prnbest of each query word, and (e) Average /
Standard deviation / Maximum / Minimum of
entropy of Prword of each query word.
Index-based features These features are based
on frequency statistics from a representative col-
lection:3 (a) Average / Standard deviation / Max-
imum / Minimum of document frequency (df) of
query words and their translations, (b) Average /
Standard deviation / Maximum / Minimum of term
frequency averaged across query words and their
translations, and (c) Sum / Maximum / Minimum
of total probability assigned to words that do not
appear in the collection (df = 0).
Additionally, the target language is a default
feature in all of our experiments. For each clas-
sification task, we train a separate classifier on
each subset of these four feature categories, so that
there are 16 different sets (including the empty
set). After we select which categories to pull fea-
tures from, we optionally perform feature selec-
tion to reduce the number of features by a pre-
defined percentage.
In our experimentation, we observed that
collection-based features were most useful for
classifying the one-best method, whereas parse-
based features were most discriminative for prob-
abilistic 10-best. For the word-based QT method,
the translation-based features were most effective
in our experiments. We further analyze the effect
of various features in Section 5.
</bodyText>
<subsectionHeader confidence="0.993855">
4.4 Training and Tuning Classifiers
</subsectionHeader>
<bodyText confidence="0.999988833333333">
The scikit-learn package was used for the
training pipeline (Pedregosa et al., 2012). Using
an established toolkit allowed us to experiment
with many options for classification, such as the
learner type (support vector machine, maximum
entropy, decision tree), feature set (16 subsets of
</bodyText>
<footnote confidence="0.7288">
3We used the BOLT collection in our experiments.
</footnote>
<bodyText confidence="0.99993434375">
the four categories described earlier) and two fea-
ture selection methods (recursive elimination or
selection based on univariate statistical tests). In
the end, we get 96 different parameter combina-
tions while training a classifier for a particular QT
method, resulting in the need for tuning — picking
the parameters that produce highest accuracy on a
representative tuning set.
Given that we have a set of queries for testing
purposes, there are few strategies for selecting a
training and tuning set. One approach is to apply a
leave-one-out strategy, so that a classifier is trained
and tuned on all but one of the test queries, and
then applied on the remaining query to predict its
label. We call this the fully-open setting.
In a more realistic scenario, there will not be
relevance judgments for the test queries, yet there
might be a small amount of labeled data similar to
the test task (e.g., different queries on same col-
lection) that can be utilized for tuning purposes,
and a larger set of training queries from different
collections. We call this the half-blind setting.
If testing in a new domain, queries of similar
type are not available for training and tuning pur-
poses. This is a more challenging scenario than
the previous two, yet it is important for real-world
applications. In order to demonstrate the effec-
tiveness of the training pipeline in this case, we
hold out test queries entirely, then train and tune
on queries from a completely different task (i.e.,
different queries and collection). We call this the
fully-blind setting.
</bodyText>
<subsectionHeader confidence="0.879852">
4.5 Retrieval
</subsectionHeader>
<bodyText confidence="0.999984615384615">
Once we have classifiers trained for all QT meth-
ods, we can apply them to a given query on-the-fly,
and compute query-specific combination weights.
One approach is hard weighting, putting all weight
onto a single method — when there are more than
one methods classified with label 1, we can ei-
ther pick one randomly or use the classifier con-
fidence value as a tie-breaker. An alternative is
soft weighting, where the weight of the ith method
can be computed either using classifier confidence
Ci (i.e., how confident the model is that the ith
method will perform well), precision on tuning set
precisioni (i.e., how precise the model is at its pre-
</bodyText>
<page confidence="0.994884">
593
</page>
<bodyText confidence="0.983412">
dictions for the ith method), or both:
</bodyText>
<equation confidence="0.99888425">
ws1
i (q) =Ci(q)
ws2
i (q) =precisioni(1) x li(q)
+(1−precisioni(0)) x (1 − li(q))
ws3
i (q) =precisioni(1) x Ci(q)
+(1−precisioni(0)) x (1 − Ci(q))
</equation>
<bodyText confidence="0.999805285714286">
The intuition behind all of these weighting
schemes is to produce a weight for each QT
method, by taking into account the confidence of
the classifier, and/or the precision of the classifier
on tuning instances.
The computed weights are normalized before
constructing the final query for retrieval:
</bodyText>
<equation confidence="0.96174">
wfinal
i (q) = wi(q)/Emj=1 wj(q)
</equation>
<bodyText confidence="0.999776714285714">
When compared empirically, we noticed that
soft weighting is more effective than hard weight-
ing, as the latter is more sensitive to classifier er-
rors. Among the three soft weighting functions,
differences were mostly negligible in our exper-
iments. Hence, we decided to use the simplest
weighting function ws1.
</bodyText>
<subsectionHeader confidence="0.91098">
4.6 Analytical Model
</subsectionHeader>
<bodyText confidence="0.99999024137931">
It is time-consuming to implement various
combination-of-evidence approaches and run re-
trieval experiments. Therefore, it is useful to have
an analytical model of the process that can pro-
vide a rough estimate of how fruitful it would be
to spend this effort, given certain details about the
task. The model we present in this section esti-
mates the effectiveness of combining QT methods
1... m on a query set Q, given (1) the effective-
ness of each method on Q and (2) error rate of
binary classifiers C1 ... Cm on Q. Using this for-
mulation, one can assess the benefit of combina-
tion without running retrieval, based only on er-
ror rates — this saves precious time during de-
velopment. Moreover, even without trained clas-
sifiers, this model can be used to estimate poten-
tial benefits by plugging in hypothetical error val-
ues. In other words, one can ask the question “If
I had classifiers with x% error on this query set,
what would be the benefit of using these classi-
fiers to combine QT methods?” before developing
any combination approach at all.
The analytical model considers a special case of
weighted combination: for each query q, we pick a
single QT method i = 1... m, for which the clas-
sifier predicts a label of 1. If there are more than
one such method, one of them is picked randomly.
This simplified version allows us to compute ex-
pected effectiveness for q as follows:
</bodyText>
<equation confidence="0.9903975">
E[f(q)] = � Pr(pick i|q)fi(q)
method i
</equation>
<bodyText confidence="0.998256454545455">
While fi(q) is an observed value (the effective-
ness of the ith method on query q), Pr(pick i|q)
needs to be estimated (the probability of selecting
the ith method). Since this depends on the pre-
dicted labels, we consider all possible scenarios
l = l1l2 ... lm, where each value is the prediction
of a classifier. For instance, “l=010” means that
classifiers C1 and C3 predicted a label of 0, while
C2 predicted a positive label. Marginalizing over
the 2m possible scenarios gives us the following
estimate:
</bodyText>
<equation confidence="0.9952525">
⎞Pr(l|q) ⎠x Pr(pick i|l, q)
⎞Pr(li|q) ⎠x Pr(i|l, q)
</equation>
<bodyText confidence="0.998086">
In the final step, we assumed that classifiers make
predictions independent of each other, which is
a desired property for successful combination.
Pr(li|q) can be estimated using classifier error
statistics:
</bodyText>
<equation confidence="0.979916666666667">
count(predicted = li, true = lq)
Pr(li|q) �
count(true = lq)
</equation>
<bodyText confidence="0.997769333333333">
where lq is the true label of q. If li = lq, this ex-
pression becomes the true positive or true negative
rate, depending on the value. Similarly, if li =� lq,
it is either the false positive or false negative rate.
Finally, the probability that the ith method is se-
lected in a particular scenario depends solely on
the predicted labels, since it is a random selection:
Pr(pick i|l) = li/Em j=1 lj
This concludes the derivation of the analytical
model of query evidence combination, which we
use in Section 5.1 to evaluate the effectiveness of
labeling approaches.
</bodyText>
<sectionHeader confidence="0.999238" genericHeader="evaluation">
5 Evaluation
</sectionHeader>
<bodyText confidence="0.953769333333333">
We evaluated our approach on four different CLIR
tasks: TREC 2002 English-Arabic CLIR, NTCIR-
8 English-Chinese Advanced Cross-Lingual Infor-
</bodyText>
<equation confidence="0.998076333333333">
Pr(pick i|q)
⎛ 1 . . . 1
= ⎝ � . . . �
⎛ l1=0 l„=0
= ⎝ 1 1
� �
l1=0 l„=0
�m
i=1
</equation>
<page confidence="0.993059">
594
</page>
<bodyText confidence="0.999800583333334">
mation Access (ACLIA), and two forum post re-
trieval tasks as part of the DARPA Broad Oper-
ational Language Technologies (BOLT) program:
English-Arabic (BOLTar) and English-Chinese
(BOLTch). The query language is English in all
cases, and we preprocess the queries using BBN’s
information extraction toolkit SERIF (Ramshaw et
al., 2011). State-of-the-art English-Arabic (En-
Ar) and English-Chinese (En-Ch) MT systems
were trained on parallel corpora released in NIST
OpenMT 2012, in addition to parallel forum data
collected as part of the BOLT program (10m En-
Ar words; 30m En-Ch words). From these data,
word alignments were learned with GIZA++ (Och
and Ney, 2003), using five iterations of each of
IBM Models 1–4 and HMM.
3-gram Chinese and 5-gram Arabic Kneser-Ney
language models were trained from the Gigaword
corpus (1b words each) and non-English side of
the training corpus. Chinese and English parallel
text were preprocessed through the Treebank Tok-
enizer,4 while no special treatment was performed
on Arabic.
For retrieval, we used Tndri, a state-of-the-
art probabilistic relevance model that supports
weighted query representations through operators
#combine and #weight (Metzler and Croft,
2005). A character-based index was built for
Chinese collections, whereas Arabic text was
stemmed using Lucene before indexing.5 En-
glish text was preprocessed by Tndri’s imple-
mentation of the Porter stemmer (Porter, 1997).
Statistics for each collection and query set are
summarized in Table 1.
Before performing any combination, we first
ran the three baseline QT methods individually
and evaluated the retrieved documents. Mean
average precision (MAP) was used to measure
retrieval effectiveness, which is a widely used
and stable metric, estimating the area under the
precision-recall curve. We set n = 10 for the
n-best probabilistic translation method. Baseline
scores are reported in Table 2. The average preci-
sion (AP) of each query in these tasks was used to
label the query and construct training data accord-
ingly.
In subsequent sections, we evaluate the effect of
several variants in the training pipeline.
</bodyText>
<footnote confidence="0.999958">
4http://www.cis.upenn.edu/˜treebank
5http://lucene.apache.org
</footnote>
<subsectionHeader confidence="0.985177">
5.1 Effect of Labeling
</subsectionHeader>
<bodyText confidence="0.999808063829788">
In Section 4.2, we introduced two ways to label
instances. In our evaluation, we set the free pa-
rameters k1 = k2 = 90, which filters out 33% of
queries from the training set of the BOLTar task;
this percentage is 29% in BOLTch, 44% in TREC,
and 27% in NTCIR.
Labeling determines which query translation
method is considered effective or not, which con-
sequently determines what the “learning problem”
is (since the objective of the classifier is to sep-
arate differently labeled instances). As a result,
there are two dimensions to consider when com-
paring labeling strategies. One is the accuracy of
the classifiers on held-out data, and the other is
how well the trained classifier reflects this accu-
racy when used in retrieval. To clarify the dis-
tinction, consider a case where every instance is
labeled 1. This generates a trivial learning prob-
lem with no test errors, yet this does not entail that
using these classifiers in retrieval will be more ef-
fective than other labeling strategies. If, even with
high classifier accuracy, the retrieval effectiveness
is low, that indicates a bad choice for labeling.
We can theoretically analyze how suitable each
labeling method is by applying the analytical
model to each CLIR task, setting parameters based
on a perfect classifier: true positive/negative rate
of 1 and false positive/negative rate of 0 (see Sec-
tion 4.6). Table 2 shows these results in the “Per-
fect” column, since these scores represent what
could be achieved if classifiers were trained to pre-
dict labels perfectly (no training or retrieval is ac-
tually performed). There are two values in each
row of the “Perfect” column, one for each labeling
strategy. In each row, we found these two values to
be statistically significantly higher than any of the
baseline scores. This shows that both labeling ap-
proaches have the potential to improve effective-
ness significantly.
We also made an empirical comparison of the
two labeling approaches by actually training clas-
sifiers with each labeling, and then using the clas-
sifiers to combine query translations in retrieval.
The “Trained” column in Table 2 shows the MAP
we get on each CLIR task (and average classifier
accuracies), using either labeling.6
Based on these results, we conclude that best-
</bodyText>
<footnote confidence="0.998047333333333">
6For a fair comparison, we fixed the train-tune setting to
fully-open, trained classifiers on the test collection and re-
ported leave-one-out accuracies.
</footnote>
<page confidence="0.990885">
595
</page>
<table confidence="0.991920125">
Lang Collection Topics MT Training data
Source Size (docs) Source (domain) Size (words)
Arabic TREC-02 383,872 50 OpenMT-12 (news/web) 10m
BOLT (forum)
Arabic BOLT 12,258,904 45
Chinese NTCIR-8 388,589 100 OpenMT-12 (news/web) 30m
BOLT (forum)
Chinese BOLT 6,693,951 45
</table>
<tableCaption confidence="0.99561">
Table 1: Summary of the CLIR tasks in our evaluations.
</tableCaption>
<table confidence="0.999825666666667">
Task Baseline Perfect Trained
one-best ten-best word measure rank measure rank
BOLTar 0.296 0.311 0.318 0.341 0.341 0.342 (74) 0.330 (72)
BOLTch 0.370 0.406 0.407 0.458 0.462 0.438 (68) 0.426 (60)
TREC 0.292 0.298 0.301 0.327 0.330 0.305 (59) 0.316 (59)
NTCIR 0.146 0.152 0.141 0.180 0.177 0.163 (56) 0.162 (61)
</table>
<tableCaption confidence="0.992143">
Table 2: Retrieval effectiveness of baseline QT methods is presented on the left side, and a comparison of
</tableCaption>
<bodyText confidence="0.968158545454545">
labeling strategies is provided on the right side. All numbers represent MAP values, except for classifier
accuracy shown in percentage values (in parantheses). Analytically computed values are shown in italics.
by-measure labeling is more useful in practice,
supported by typically higher accuracy and effec-
tiveness. Best-by-rank yields better results only
on TREC, but a closer look reveals that the in-
crease in MAP is due to only two outlier queries.
For BOLTar, on the other hand, retrieval with best-
by-measure labeling is more effective (statistically
significant) than best-by-rank; hence, the former is
used in remaining parts of our evaluation.
</bodyText>
<subsectionHeader confidence="0.999855">
5.2 Effect of Train-Tune Setting
</subsectionHeader>
<bodyText confidence="0.999916842105263">
In Section 4.4, we introduced three major train-
tune settings: fully-open, half-blind, and fully-
blind. In order to implement these settings, we
treat each of the three query sets (BOLT, TREC,
NTCIR) as a separate training dataset and experi-
ment with a variety of combinations.
For simplicity, let us demonstrate the variety of
experiments assuming the test collection is BOLT.
For the fully-open case, the default training data is
all of the BOLT queries (this training set is referred
to as b). Additionally, one can include queries
from TREC (referred to as t) and NTCIR (referred
to as n) into the training data. This gives us four
different training datasets for the fully-open case:
b, b + n, b + t, b + t + n. Similarly, each of the
half-blind and fully-blind settings can be applied
to three different training sets: For BOLT, these
are t, n, t + n.7 This results in ten different ex-
periments run for each task — in each experiment,
</bodyText>
<footnote confidence="0.742657">
7In the case of half-blind, b is split into two: 20% is used
for tuning and the remainder is used for testing.
</footnote>
<bodyText confidence="0.99986853125">
we train a classifier for each QT method, select the
best meta-parameters on the tuning set, and then
compute combination weights for retrieval using
the classifiers.
Each cell on the left side of Table 3 (under col-
umn “Query-specific Combination”) shows the re-
sults of the most effective experiment for a partic-
ular task and train-tune setting. Accuracy values
for classifiers varied widely across these experi-
ments. Still, even when accuracies dropped close
to or below 50% (i.e. random baseline), combined
retrieval was always more effective than any single
QT approach, which emphasizes the robustness of
our approach. For instance, in the fully-blind set-
ting for the NTCIR task, the individual classifiers
had accuracies of only 56%, 49%, and 44% but
MAP was 0.163, which is higher than the MAP of
any individual method for that collection (0.146,
0.152, or 0.141).
Another key observation in Table 3 is that
the domain effect (i.e., training and/or tuning on
queries similar to test queries) is only noticeable
on the two BOLT tasks. For NTCIR and TREC,
we do not observe a boost in MAP when queries
from the same task are included in training (i.e.,
fully-open setting). This can be explained by the
BOLT-centric nature of our system components:
the text analysis tool and MT systems are tuned
mainly for forum data, and the collection-based
features are extracted from BOLT. Due to this bias,
BOLT queries were most useful in our experi-
ments, supported by the fact that BOLT is always
</bodyText>
<page confidence="0.995811">
596
</page>
<table confidence="0.999363">
Task Query-specific Combination Uniform Task- Max
specific
fully-open half-blind fully-blind
BOLTar 0.342*† b 0.330 t+n 0.329 t+n 0.32412 0.3291 0.346
BOLTch 0.438*† b 0.428 n 0.426 t+n 0.4221 0.4311 0.466
TREC 0.321 b+t 0.324*† b+n 0.321 b+n 0.3141 0.3181 0.332
NTCIR 0.164* b+n 0.163 b+t 0.163 b 0.16213 0.16213 0.182
</table>
<tableCaption confidence="0.998347">
Table 3: A comparison of query combination approaches. For query-specific combination, MAP and
</tableCaption>
<bodyText confidence="0.955741727272727">
training data are shown for the most effective experiment of each train-tune setting. For each task, the
highest MAP achieved with our approach is shown in bold. Superscripts 1, 2, and 3 indicate statisti-
cally significant improvements over baseline methods one-best, probabilistic 10-best, and word-based,
whereas * indicates improvements over all three. Superscript † indicates results significantly better than
uniform and task-specific combination methods.
included in the train set when testing on TREC or
NTCIR (see lowest two rows in Table 3). Also,
when there is no domain effect (i.e., half-blind and
fully-blind ), more data yields higher effectiveness
in 6 out of 8 cases (see two right columns on the
left side of Table 3).
</bodyText>
<subsectionHeader confidence="0.996618">
5.3 Retrieval Effectiveness
</subsectionHeader>
<bodyText confidence="0.999971625">
In this section, we compare our novel query-
specific combination-of-evidence approach to the
baseline CLIR approaches, as well as comparable
combination methods (uniform and task-specific
combination) in terms of retrieval effectiveness.
Based on a randomized significance test (Smucker
et al., 2007), the best query-specific combina-
tion method (shown in boldface in Table 3) out-
performs all baseline QT methods in all tasks
with 95% confidence (indicated by superscript *
in Table 3). This is not the case for uniform or
task-specific query combination, which are statis-
tically indistinguishable from at least one of the
QT methods, depending on the task (indicated by
superscripts 1, 2, and 3 for one-best, probabilis-
tic 10-best, and word-based QT methods, respec-
tively). When we directly compare our query-
specific combination approach to other combina-
tion methods, the differences are statistically sig-
nificant for all tasks but NTCIR (indicated by su-
perscript †).
For reference, we also computed effectiveness
for a hypothetical system (denoted by “Max” in
Table 3) that could select the best QT method for
each query and use only that for retrieval. This is
not a strict upper bound, since correctly weight-
ing each method can produce better results, but
it is still a reasonable target for effectiveness. In
our experiments, Arabic retrieval runs were very
close to this target with our combination approach,
while the gap for Chinese is still substantial, which
is worth further exploration.
</bodyText>
<sectionHeader confidence="0.999387" genericHeader="conclusions">
6 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999908">
In this paper, we introduced a novel combination-
of-evidence approach for CLIR, which learns a
custom combination recipe for each query. We for-
mulate this as a set of binary classification prob-
lems, and show that trained classifiers can be used
to produce query-specific combination weights ef-
fectively. Our deep exploration of many variants
(e.g., labeling, training-tuning, weight computa-
tion, analytical formulation) and extensive empiri-
cal analysis on four different tasks provide insights
for future research on the under-studied problem
of combining translations for CLIR.
Our approach advances the state of the art of
CLIR, yielding higher effectiveness than three ad-
vanced query translation approaches, all based
on state-of-the-art MT systems. Furthermore, on
three of the four tasks, our combination strategy is
statistically significantly better than two compara-
ble combination techniques. Experimental results
also suggest that even a uniform combination of
query translations is consistently better than any
individual method. While it is known that com-
bining translations helps CLIR, we confirm this on
a set of modern CLIR tasks, including two target
languages and a variety of text domains.
Having a simple linear learning problem allows
us to train robust models with relatively simpler
features. Nevertheless, we are interested in ex-
perimenting with more sophisticated learning ap-
proaches. In terms of non-linear classifiers, our
experience with decision trees in this paper indi-
cated a higher tendency to overfit. In terms of
</bodyText>
<page confidence="0.991059">
597
</page>
<bodyText confidence="0.999965722222222">
combining queries in a non-linear fashion, our fu-
ture plans include integrating our approach into a
LTR framework, and directly optimize MAP. This
will also allow us to explore more complex fea-
tures extracted from query and document text, as
well as external sources.
Another possible future endeavor is to extend
these ideas to (i) other query translation ap-
proaches and (ii) document translation. While the
exact same problem can be formulated for learning
to translate documents effectively, a more compli-
cated infrastructure and longer running times are
two challenges that need to be considered.
Finally, we hope this to be a significant step to-
wards more context-dependent and robust CLIR
models, by taking advantage of modern translation
technologies, as well as machine learning tech-
niques.
</bodyText>
<sectionHeader confidence="0.999157" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999962888888889">
This work was supported by DARPA/I2O Con-
tract No. HR0011-12-C-0014 under the BOLT
program (Approved for Public Release, Distribu-
tion Unlimited). The views, opinions, and/or find-
ings contained in this article are those of the author
and should not be interpreted as representing the
official views or policies, either expressed or im-
plied, of the Defense Advanced Research Projects
Agency or the Department of Defense.
</bodyText>
<sectionHeader confidence="0.999135" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997277584415584">
Hosein Azarbonyad, Azadeh Shakery, and Heshaam
Faili. 2013. Exploiting multiple translation re-
sources for english-persian cross language informa-
tion retrieval. In Proceedings of the Cross-Language
Evaluation Forum on Cross-Language Information
Retrieval and Evaluation, CLEF ’13, pages 93–99.
Lisa Ballesteros and W. Bruce Croft. 1996. Dictionary
methods for cross-lingual information retrieval. In
Proceedings of the 7th International DEXA Confer-
ence on Database and Expert Systems Applications,
pages 791–801.
Nicholas J. Belkin, Paul Kantor, Edward A. Fox, and
Joseph A. Shaw. 1995. Combining the evidence
of multiple query representations for information
retrieval. Information Processing &amp; Management,
31(3):431–448, May.
Michael Bendersky, Donald Metzler, and W. Bruce
Croft. 2011. Parameterized concept weighting in
verbose queries. In Proceedings of the 34th Inter-
national ACM SIGIR Conference on Research and
Development in Information Retrieval, SIGIR ’11,
pages 605–614, New York, NY, USA. ACM.
Pierre-Yves Berger and Jacques Savoy. 2007. Se-
lecting automatically the best query translations. In
Large Scale Semantic Access to Content (Text, Im-
age, Video, and Sound), RIAO ’07, pages 287–300,
Paris, France, France. Le Centre de Hautes Etudes
Internationales D’Informatique Documentaire.
Martin Braschler. 2004. Combination approaches for
multilingual text retrieval. Information Retrieval,
7(1-2):183–204, January.
Peter F. Brown, Vincent J. Della Pietra, Stephen
A. Della Pietra, and Robert L. Mercer. 1993.
The mathematics of statistical machine translation:
parameter estimation. Computational Linguistics,
19(2):263–311.
Aitao Chen and Fredric C. Gey. 2004. Multilingual in-
formation retrieval using machine translation, rele-
vance feedback and decompounding. Inf. Retr., 7(1-
2):149–182, January.
W. Bruce Croft. 2000. Combining approaches to in-
formation retrieval. In W. Bruce Croft, editor, Ad-
vances in Information Retrieval, volume 7 of The
Information Retrieval Series, pages 1–36. Springer.
Kareem Darwish and Douglas W. Oard. 2003. Proba-
bilistic structured query methods. In Proceedings of
the 26th Annual International ACM SIGIR Confer-
ence on Research and Development in Informaion
Retrieval, SIGIR ’03, pages 338–344.
Edward A. Fox. 1983. Extending the Boolean
and Vector Space Models of Information Retrieval
with P-norm Queries and Multiple Concept Types.
Ph.D. thesis, Cornell University, Ithaca, NY, USA.
AAI8328584.
Fredric C. Gey, Hailing Jiang, Vivien Petras, and
Aitao Chen. 2001. Cross-language retrieval for
the clef collections - comparing multiple meth-
ods of retrieval. In Revised Papers from the
Workshop of Cross-Language Evaluation Forum on
Cross-Language Information Retrieval and Evalua-
tion, CLEF ’00, pages 116–128, London, UK, UK.
Springer-Verlag.
Benjamin Herbert, Gy¨orgy Szarvas, and Iryna
Gurevych. 2011. Combining query transla-
tion techniques to improve cross-language informa-
tion retrieval. In Proceedings of the 33rd Euro-
pean Conference on Advances in Information Re-
trieval, ECIR’11, pages 712–715, Berlin, Heidel-
berg. Springer-Verlag.
Djoerd Hiemstra, Wessel Kraaij, Ren´ee Pohlmann,
and Thijs Westerveld. 2001. Translation re-
sources, merging strategies, and relevance feedback
for cross-language information retrieval. In Revised
Papers from the Workshop of Cross-Language Eval-
uation Forum on Cross-Language Information Re-
trieval and Evaluation, CLEF ’00, pages 102–115,
London, UK, UK. Springer-Verlag.
</reference>
<page confidence="0.979509">
598
</page>
<reference confidence="0.999726881188119">
David A. Hull and Gregory Grefenstette. 1996. Query-
ing across languages: a dictionary-based approach
to multilingual information retrieval. In Proceed-
ings of the 19th Annual International ACM SIGIR
Conference on Research and Development in Infor-
mation Retrieval, SIGIR ’96, pages 49–57.
Kui-Lam Kwok. 1999. English-Chinese cross-
language retrieval based on a translation package.
In Workshop on Machine Translation for Cross Lan-
guage Information Retrieval, Machine Translation
Summit VII, pages 8–13.
Patrice Lopez and Laurent Romary. 2009. Patatras:
Retrieval model combination and regression mod-
els for prior art search. In Proceedings of the 10th
Cross-language Evaluation Forum Conference on
Multilingual Information Access Evaluation: Text
Retrieval Experiments, CLEF’09, pages 430–437,
Berlin, Heidelberg. Springer-Verlag.
Yanjun Ma, Jian-Yun Nie, Hua Wu, and Haifeng Wang.
2012. Opening machine translation black box for
cross-language information retrieval. In Information
Retrieval Technology, pages 467–476. Springer.
Walid Magdy and Gareth J. F. Jones. 2011. Should
MT systems be used as black boxes in CLIR? In
Proceedings of the 33rd European Conference on In-
formation Retrieval, ECIR ’11, pages 683–686.
Michael McGill, Matthew Koll, and Terry Noreault.
1979. An Evaluation of Factors Affecting Document
Ranking by Information Retrieval Systems. ERIC
reports. School of Information Studies, Syracuse
University.
Donald Metzler and W. Bruce Croft. 2005. A Markov
random field model for term dependencies. In Pro-
ceedings of the 28th Annual International ACM SI-
GIR Conference on Research and Development in
Information Retrieval, SIGIR ’05, pages 472–479.
Franz J. Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29(1):19–51.
Judea Pearl. 1988. Probabilistic Reasoning in In-
telligent Systems: Networks of Plausible Inference.
Morgan Kaufmann Publishers Inc., San Francisco,
CA, USA.
Fabian Pedregosa, Ga¨el Varoquaux, Alexandre Gram-
fort, Vincent Michel, Bertrand Thirion, Olivier
Grisel, Mathieu Blondel, Peter Prettenhofer, Ron
Weiss, Vincent Dubourg, Jake VanderPlas, Alexan-
dre Passos, David Cournapeau, Matthieu Brucher,
Matthieu Perrot, and Edouard Duchesnay. 2012.
Scikit-learn: Machine learning in python. CoRR,
abs/1201.0490.
Carol Peters, Martin Braschler, and Paul Clough. 2012.
Multilingual Information Retrieval - From Research
To Practice. Springer.
Ari Pirkola. 1998. The effects of query struc-
ture and dictionary-setups in dictionary-based cross-
language information retrieval. In Proceedings of
the 21st Annual International ACM SIGIR Confer-
ence on Research and Development in Information
Retrieval, SIGIR ’98, pages 55–63.
M. F. Porter. 1997. Readings in information retrieval.
chapter An Algorithm for Suffix Stripping, pages
313–316. Morgan Kaufmann Publishers Inc., San
Francisco, CA, USA.
T. B. Rajashekar and W. Bruce Croft. 1995. Com-
bining automatic and manual index representations
in probabilistic retrieval. J. Am. Soc. Inf. Sci.,
46(4):272–283, May.
Lance Ramshaw, Elizabeth Boschee, Marjorie Freed-
man, Jessica MacBride, Ralph Weischedel, and Alex
Zamanian. 2011. Serif language processing — ef-
fective trainable language understanding. In J. Olive
et al., editor, Handbook of Natural Language Pro-
cessing and Machine Translation: DARPA Global
Autonomous Language Exploitation, pages 626–
631. Springer.
Jacques Savoy. 2001. Report on CLEF-2001 exper-
iments: Effective combined query-translation ap-
proach. In CLEF, pages 27–43.
Mark D. Smucker, James Allan, and Ben Carterette.
2007. A comparison of statistical significance tests
for information retrieval evaluation. In Proceedings
of the 16th ACM conference on Conference on In-
formation and Knowledge Management, CIKM ’07,
pages 623–632.
Ferhan Ture, Jimmy Lin, and Douglas W. Oard.
2012. Combining statistical translation techniques
for cross-language information retrieval. In Pro-
ceedings of the 24th International Conference on
Computational Linguistics, COLING ’12, pages
2685–2702.
Howard Turtle and W. Bruce Croft. 1990. Inference
networks for document retrieval. In Proceedings of
the 13th Annual International ACM SIGIR Confer-
ence on Research and Development in Information
Retrieval, SIGIR ’90, pages 1–24, New York, NY,
USA. ACM.
Jinxi Xu and Ralph Weischedel. 2005. Empirical stud-
ies on the impact of lexical resources on CLIR per-
formance. Information Processing &amp; Management,
41(3):475–487, May.
</reference>
<page confidence="0.99874">
599
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.280069">
<title confidence="0.903996666666667">Learning to Translate: A Query-Specific Combination Approach Cross-Lingual Information Retrieval Ferhan Ture</title>
<author confidence="0.724005">Raytheon BBN</author>
<address confidence="0.8790055">10 Moulton Cambridge, MA, 02138</address>
<email confidence="0.999806">fture@bbn.com</email>
<abstract confidence="0.986113708333333">When documents and queries are presented in different languages, the common approach is to translate the query into the document language. While there are a variety of query translation approaches, recent research suggests that combining multiple methods into a single ”structured query” is the most effective. In this paper, we introduce a novel approach for producing a unique combination recipe for each query, as it has also been shown that the optimal combination weights differ substantially across queries and other task specifics. Our query-specific combination method generates statistically significant improvements over other combination strategies presented in the literature, such as uniform and task-specific weighting. An in-depth empirical analysis presents insights about the effect of data size, domain differences, labeling and tuning on the end performance of our approach.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Hosein Azarbonyad</author>
<author>Azadeh Shakery</author>
<author>Heshaam Faili</author>
</authors>
<title>Exploiting multiple translation resources for english-persian cross language information retrieval.</title>
<date>2013</date>
<booktitle>In Proceedings of the Cross-Language Evaluation Forum on Cross-Language Information Retrieval and Evaluation, CLEF ’13,</booktitle>
<pages>93--99</pages>
<contexts>
<context position="7835" citStr="Azarbonyad et al., 2013" startWordPosition="1169" endWordPosition="1172"> as stated previously, numerous studies suggest that certain methods work better on certain queries, collections, languages. In fact, when weights are optimized separately on each collection, they differ substantially across different collections (Ture et al., 2012). For monolingual retrieval, there has been a series of learningto-rank (LTR) papers that determine weights for query concepts (Bendersky et al., 2011), such that retrieval effectiveness is maximized. A recent study extends this idea to the cross-lingual case, by learning how to weight each translated word for English-Persian CLIR (Azarbonyad et al., 2013). In contrast, we extract translated word weights from diverse and sophisticated translation methods, then learn how to weight each translated structured query, We call this “learning-totranslate” (LTT), which can be formulated as a simpler learning problem. In CLIR, both LTR and LTT are under-explored problems, with a common goal of applying machine learning techniques to improve query translation, yet with complementary benefits. To our knowledge, there has been one prior LTT approach: a classifier was trained to predict effectiveness of each query translation, using features based on statis</context>
</contexts>
<marker>Azarbonyad, Shakery, Faili, 2013</marker>
<rawString>Hosein Azarbonyad, Azadeh Shakery, and Heshaam Faili. 2013. Exploiting multiple translation resources for english-persian cross language information retrieval. In Proceedings of the Cross-Language Evaluation Forum on Cross-Language Information Retrieval and Evaluation, CLEF ’13, pages 93–99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lisa Ballesteros</author>
<author>W Bruce Croft</author>
</authors>
<title>Dictionary methods for cross-lingual information retrieval.</title>
<date>1996</date>
<booktitle>In Proceedings of the 7th International DEXA Conference on Database and Expert Systems Applications,</booktitle>
<pages>791--801</pages>
<contexts>
<context position="3055" citStr="Ballesteros and Croft, 1996" startWordPosition="445" endWordPosition="449">en building a linear combination of existing query translation approaches. From standard query-document relevance judgments we train a set of classifiers, which produce a unique combination recipe for each query, based on a large set of features extracted from the query and collection. Experimental results show that the effectiveness of our method is significantly higher than state-of-the-art query translation methods and other combination strategies. 2 Related Work The earliest approaches to query translation for CLIR used machine-readable bilingual dictionaries (Hull and Grefenstette, 1996; Ballesteros and Croft, 1996), achieving around up to 60% of monolingual IR effectiveness. Xu and Weischedel (2005) showed that effectiveness can be increased to around 80% by weighting each translation proportional to its rank in the dictionary. The practice of weighting translation candidates was later formulated as a “structured query”, in which each query term is represented by a probability distribution over its translations in the document language (Pirkola, 1998; Kwok, 1999; Darwish and Oard, 2003). Our approach is based on the structured query formulation. Some of the earliest studies in IR discovered that with di</context>
</contexts>
<marker>Ballesteros, Croft, 1996</marker>
<rawString>Lisa Ballesteros and W. Bruce Croft. 1996. Dictionary methods for cross-lingual information retrieval. In Proceedings of the 7th International DEXA Conference on Database and Expert Systems Applications, pages 791–801.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicholas J Belkin</author>
<author>Paul Kantor</author>
<author>Edward A Fox</author>
<author>Joseph A Shaw</author>
</authors>
<title>Combining the evidence of multiple query representations for information retrieval.</title>
<date>1995</date>
<journal>Information Processing &amp; Management,</journal>
<volume>31</volume>
<issue>3</issue>
<contexts>
<context position="4448" citStr="Belkin et al. (1995)" startWordPosition="658" endWordPosition="661">(EMNLP), pages 589–599, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics though the effectiveness was similar (McGill et al., 1979). Later studies showed that combining different representations of the query and/or document often produced superior output (Rajashekar and Croft, 1995; Turtle and Croft, 1990; Fox, 1983). This intuitive idea was supported theoretically by Pearl (1988), concluding that multiple pieces of evidence estimates relevance more accurately, but that the benefit strongly depends on the quality and independence of each piece. Experiments by Belkin et al. (1995) indicated the need to properly weight each representation with respect to its effectiveness. These so-called “combinationof-evidence” techniques became more powerful with the introduction of Indri, a probabilistic retrieval framework specifically designed for combining multiple query and document representations (Metzler and Croft, 2005). Croft (2000) provides a detailed summary of earlier query combination approaches in IR, while Peters et al. (2012) cites more recent related work. The benefits of combination-of-evidence transfer to the cross-lingual case especially well, since the inherent </context>
</contexts>
<marker>Belkin, Kantor, Fox, Shaw, 1995</marker>
<rawString>Nicholas J. Belkin, Paul Kantor, Edward A. Fox, and Joseph A. Shaw. 1995. Combining the evidence of multiple query representations for information retrieval. Information Processing &amp; Management, 31(3):431–448, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Bendersky</author>
<author>Donald Metzler</author>
<author>W Bruce Croft</author>
</authors>
<title>Parameterized concept weighting in verbose queries.</title>
<date>2011</date>
<booktitle>In Proceedings of the 34th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’11,</booktitle>
<pages>605--614</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="7628" citStr="Bendersky et al., 2011" startWordPosition="1137" endWordPosition="1140">provements when a preretrieval combination was performed. All of the previously cited approaches either use uniform weights for combination, or select weights based on collection-level information. However, as stated previously, numerous studies suggest that certain methods work better on certain queries, collections, languages. In fact, when weights are optimized separately on each collection, they differ substantially across different collections (Ture et al., 2012). For monolingual retrieval, there has been a series of learningto-rank (LTR) papers that determine weights for query concepts (Bendersky et al., 2011), such that retrieval effectiveness is maximized. A recent study extends this idea to the cross-lingual case, by learning how to weight each translated word for English-Persian CLIR (Azarbonyad et al., 2013). In contrast, we extract translated word weights from diverse and sophisticated translation methods, then learn how to weight each translated structured query, We call this “learning-totranslate” (LTT), which can be formulated as a simpler learning problem. In CLIR, both LTR and LTT are under-explored problems, with a common goal of applying machine learning techniques to improve query tra</context>
</contexts>
<marker>Bendersky, Metzler, Croft, 2011</marker>
<rawString>Michael Bendersky, Donald Metzler, and W. Bruce Croft. 2011. Parameterized concept weighting in verbose queries. In Proceedings of the 34th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’11, pages 605–614, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pierre-Yves Berger</author>
<author>Jacques Savoy</author>
</authors>
<title>Selecting automatically the best query translations.</title>
<date>2007</date>
<booktitle>In Large Scale Semantic Access to Content (Text, Image, Video, and Sound), RIAO ’07,</booktitle>
<pages>287--300</pages>
<location>Paris, France,</location>
<contexts>
<context position="2339" citStr="Berger and Savoy, 2007" startWordPosition="340" endWordPosition="343"> different translation approaches is superior to any single query translation method (Braschler, 2004; Elizabeth Boschee Raytheon BBN Technologies 10 Moulton St Cambridge, MA, 02138 USA eboschee@bbn.com Herbert et al., 2011). While there are numerous combination-of-evidence techniques for both mono-lingual and cross-lingual IR, recent work suggests that there is no one-size-fits-all solution. In fact, the optimal combination weights (i.e., weights assigned to each piece of evidence in a linear combination) differ greatly across queries, tasks, languages, and other variants (Ture et al., 2012; Berger and Savoy, 2007). In this paper, we introduce a novel method for learning optimal combination weights when building a linear combination of existing query translation approaches. From standard query-document relevance judgments we train a set of classifiers, which produce a unique combination recipe for each query, based on a large set of features extracted from the query and collection. Experimental results show that the effectiveness of our method is significantly higher than state-of-the-art query translation methods and other combination strategies. 2 Related Work The earliest approaches to query translat</context>
<context position="8483" citStr="Berger and Savoy, 2007" startWordPosition="1270" endWordPosition="1273"> translated word weights from diverse and sophisticated translation methods, then learn how to weight each translated structured query, We call this “learning-totranslate” (LTT), which can be formulated as a simpler learning problem. In CLIR, both LTR and LTT are under-explored problems, with a common goal of applying machine learning techniques to improve query translation, yet with complementary benefits. To our knowledge, there has been one prior LTT approach: a classifier was trained to predict effectiveness of each query translation, using features based on statistics of the query terms (Berger and Savoy, 2007). Instead of weighting, the translations with highest classifier scores were concatenated, yielding statistically significant improvements over using the single-best translation method. However, the translation methods explored in this paper are all based on one-best MT 590 systems, making it difficult to draw strong conclusions. 3 Query Translation The primary contribution of this paper is to show how a diverse set of query translation (QT) methods can be combined effectively into a single weighted structured query, with improved retrieval effectiveness. While our approach can applied to any </context>
</contexts>
<marker>Berger, Savoy, 2007</marker>
<rawString>Pierre-Yves Berger and Jacques Savoy. 2007. Selecting automatically the best query translations. In Large Scale Semantic Access to Content (Text, Image, Video, and Sound), RIAO ’07, pages 287–300, Paris, France, France. Le Centre de Hautes Etudes Internationales D’Informatique Documentaire.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Braschler</author>
</authors>
<title>Combination approaches for multilingual text retrieval.</title>
<date>2004</date>
<journal>Information Retrieval,</journal>
<pages>7--1</pages>
<contexts>
<context position="1817" citStr="Braschler, 2004" startWordPosition="268" endWordPosition="269">n which documents and queries are presented in different languages. In order to overcome the language barrier, the most commonly adopted method is to translate queries into the document language. Many methods have been introduced for translating queries for CLIR, ranging from word-by-word dictionary lookups (Xu and Weischedel, 2005; Darwish and Oard, 2003) to sophisticated use of machine translation (MT) systems (Magdy and Jones, 2011; Ma et al., 2012). Previous research has shown that combining evidence from different translation approaches is superior to any single query translation method (Braschler, 2004; Elizabeth Boschee Raytheon BBN Technologies 10 Moulton St Cambridge, MA, 02138 USA eboschee@bbn.com Herbert et al., 2011). While there are numerous combination-of-evidence techniques for both mono-lingual and cross-lingual IR, recent work suggests that there is no one-size-fits-all solution. In fact, the optimal combination weights (i.e., weights assigned to each piece of evidence in a linear combination) differ greatly across queries, tasks, languages, and other variants (Ture et al., 2012; Berger and Savoy, 2007). In this paper, we introduce a novel method for learning optimal combination </context>
<context position="6209" citStr="Braschler (2004)" startWordPosition="930" endWordPosition="931">mplementation (since the approach requires no changes to the modeling side) and the possibly greater diversity that can be achieved by having separate retrieval runs. However, each ranked list needs to be limited in size, which might cause some potentially useful documents not to be considered in the combination at all. Since the focus of this paper is on the modeling end of retrieval, pre-retrieval combination was a more suitable choice, though we think that the two approaches have complementary benefits. The idea of combining query translations before retrieval has been explored previously. Braschler (2004) combines three translation approaches: output of an MT system, a novel translation approach based on a similarity thesaurus built automatically from a comparable corpus, and a dictionary-based translation. The main reason that this combination does not provide much benefit is due to the lower coverage of the thesaurus-based and dictionary-based translation methods. A similar approach by Herbert et al. (2011) uses Wikipedia to provide translations of certain phrases and entities, and combining that with the Google Translate MT system yields statistically significant improvements in English-to-</context>
</contexts>
<marker>Braschler, 2004</marker>
<rawString>Martin Braschler. 2004. Combination approaches for multilingual text retrieval. Information Retrieval, 7(1-2):183–204, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Vincent J Della Pietra</author>
<author>Stephen A Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="11943" citStr="Brown et al., 1993" startWordPosition="1824" endWordPosition="1827">he query and its translation. As we process each of the n translations, for each query word sz, we accumulate probabilities on each translated word tzj aligned to sz. Finally, we normalize the translation probabilities to get Prnbest(tzj|sz). 3.3 Word-based One of the most widely used approaches in CLIR is based on translating each query word sz independently, with probabilities assigned to each translation candidate tzj. Translations are derived automatically from a bilingual corpus using statistical word alignment techniques, which are used as part of the training of statistical MT systems (Brown et al., 1993). These probabilities can be exploited for retrieval based on the technique of Darwish and Oard (2003) for “projecting” text into the document language. After cleaning up the automatically learned translation probabilities (details omitted for space considerations), we end up with the translation probabilities Prword(tzj|sz). 4 Combination of Evidence Once we have multiple ways to represent the query q in the document language (QTz(q), i = 1... m), it is possible to combine these “pieces of evidence” into a single representation as follows: QT(q) = �m wz(q)QTz(q) z=1 and each combination-of-ev</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: parameter estimation. Computational Linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aitao Chen</author>
<author>Fredric C Gey</author>
</authors>
<title>Multilingual information retrieval using machine translation, relevance feedback and decompounding.</title>
<date>2004</date>
<journal>Inf. Retr.,</journal>
<pages>7--1</pages>
<contexts>
<context position="5307" citStr="Chen and Gey, 2004" startWordPosition="784" endWordPosition="787">fically designed for combining multiple query and document representations (Metzler and Croft, 2005). Croft (2000) provides a detailed summary of earlier query combination approaches in IR, while Peters et al. (2012) cites more recent related work. The benefits of combination-of-evidence transfer to the cross-lingual case especially well, since the inherent ambiguity of translation readily provides a diverse set of representations. Most CLIR approaches implement a post-retrieval merging of ranked lists, each generated from different query (Hiemstra et al., 2001; Savoy, 2001; Gey et al., 2001; Chen and Gey, 2004) or document (Lopez and Romary, 2009) representations, also called “data fusion”. In contrast, we focus on a pre-retrieval combination at the modeling stage, so that a single complex query is used in retrieval, instead of multiple simpler ones. Two advantages of the former are easier implementation (since the approach requires no changes to the modeling side) and the possibly greater diversity that can be achieved by having separate retrieval runs. However, each ranked list needs to be limited in size, which might cause some potentially useful documents not to be considered in the combination </context>
</contexts>
<marker>Chen, Gey, 2004</marker>
<rawString>Aitao Chen and Fredric C. Gey. 2004. Multilingual information retrieval using machine translation, relevance feedback and decompounding. Inf. Retr., 7(1-2):149–182, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Bruce Croft</author>
</authors>
<title>Combining approaches to information retrieval.</title>
<date>2000</date>
<booktitle>Advances in Information Retrieval,</booktitle>
<volume>7</volume>
<pages>1--36</pages>
<editor>In W. Bruce Croft, editor,</editor>
<publisher>Springer.</publisher>
<contexts>
<context position="4802" citStr="Croft (2000)" startWordPosition="708" endWordPosition="709">uitive idea was supported theoretically by Pearl (1988), concluding that multiple pieces of evidence estimates relevance more accurately, but that the benefit strongly depends on the quality and independence of each piece. Experiments by Belkin et al. (1995) indicated the need to properly weight each representation with respect to its effectiveness. These so-called “combinationof-evidence” techniques became more powerful with the introduction of Indri, a probabilistic retrieval framework specifically designed for combining multiple query and document representations (Metzler and Croft, 2005). Croft (2000) provides a detailed summary of earlier query combination approaches in IR, while Peters et al. (2012) cites more recent related work. The benefits of combination-of-evidence transfer to the cross-lingual case especially well, since the inherent ambiguity of translation readily provides a diverse set of representations. Most CLIR approaches implement a post-retrieval merging of ranked lists, each generated from different query (Hiemstra et al., 2001; Savoy, 2001; Gey et al., 2001; Chen and Gey, 2004) or document (Lopez and Romary, 2009) representations, also called “data fusion”. In contrast, </context>
</contexts>
<marker>Croft, 2000</marker>
<rawString>W. Bruce Croft. 2000. Combining approaches to information retrieval. In W. Bruce Croft, editor, Advances in Information Retrieval, volume 7 of The Information Retrieval Series, pages 1–36. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kareem Darwish</author>
<author>Douglas W Oard</author>
</authors>
<title>Probabilistic structured query methods.</title>
<date>2003</date>
<booktitle>In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Informaion Retrieval, SIGIR ’03,</booktitle>
<pages>338--344</pages>
<contexts>
<context position="1560" citStr="Darwish and Oard, 2003" startWordPosition="227" endWordPosition="230"> in-depth empirical analysis presents insights about the effect of data size, domain differences, labeling and tuning on the end performance of our approach. 1 Introduction Cross-lingual information retrieval (CLIR) is a special case of information retrieval (IR) in which documents and queries are presented in different languages. In order to overcome the language barrier, the most commonly adopted method is to translate queries into the document language. Many methods have been introduced for translating queries for CLIR, ranging from word-by-word dictionary lookups (Xu and Weischedel, 2005; Darwish and Oard, 2003) to sophisticated use of machine translation (MT) systems (Magdy and Jones, 2011; Ma et al., 2012). Previous research has shown that combining evidence from different translation approaches is superior to any single query translation method (Braschler, 2004; Elizabeth Boschee Raytheon BBN Technologies 10 Moulton St Cambridge, MA, 02138 USA eboschee@bbn.com Herbert et al., 2011). While there are numerous combination-of-evidence techniques for both mono-lingual and cross-lingual IR, recent work suggests that there is no one-size-fits-all solution. In fact, the optimal combination weights (i.e., </context>
<context position="3536" citStr="Darwish and Oard, 2003" startWordPosition="522" endWordPosition="526">proaches to query translation for CLIR used machine-readable bilingual dictionaries (Hull and Grefenstette, 1996; Ballesteros and Croft, 1996), achieving around up to 60% of monolingual IR effectiveness. Xu and Weischedel (2005) showed that effectiveness can be increased to around 80% by weighting each translation proportional to its rank in the dictionary. The practice of weighting translation candidates was later formulated as a “structured query”, in which each query term is represented by a probability distribution over its translations in the document language (Pirkola, 1998; Kwok, 1999; Darwish and Oard, 2003). Our approach is based on the structured query formulation. Some of the earliest studies in IR discovered that with different underlying models, the retrieved document set would vary substantially, al589 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 589–599, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics though the effectiveness was similar (McGill et al., 1979). Later studies showed that combining different representations of the query and/or document often produced superior output (Rajashekar and Cro</context>
<context position="9407" citStr="Darwish and Oard, 2003" startWordPosition="1410" endWordPosition="1413">t to draw strong conclusions. 3 Query Translation The primary contribution of this paper is to show how a diverse set of query translation (QT) methods can be combined effectively into a single weighted structured query, with improved retrieval effectiveness. While our approach can applied to any set of translation methods, we focus on three methods that have complementary strengths and that have shown promise in CLIR: word-based probabilistic translation, one-best MT, and n-best probabilistic MT. We briefly present our implementation of each method; more details can be found in earlier work (Darwish and Oard, 2003; Ture et al., 2012). Each QT method generates a representation of the query in the document language. In the case of word-based and n-best MT approaches, the representation is a structured query itself, where each query word is represented by a probability distribution over translation alternatives. For one-best MT, the query is represented by a bag of translated words. 3.1 One-Best MT A query translation approach that has become more popular recently is to simply run the query through an MT system, and use the best output as the query: t1t2 ... tl = MT(s1s2 ... sk) (1) where s = s1s2 ... sk </context>
<context position="12045" citStr="Darwish and Oard (2003)" startWordPosition="1840" endWordPosition="1843"> accumulate probabilities on each translated word tzj aligned to sz. Finally, we normalize the translation probabilities to get Prnbest(tzj|sz). 3.3 Word-based One of the most widely used approaches in CLIR is based on translating each query word sz independently, with probabilities assigned to each translation candidate tzj. Translations are derived automatically from a bilingual corpus using statistical word alignment techniques, which are used as part of the training of statistical MT systems (Brown et al., 1993). These probabilities can be exploited for retrieval based on the technique of Darwish and Oard (2003) for “projecting” text into the document language. After cleaning up the automatically learned translation probabilities (details omitted for space considerations), we end up with the translation probabilities Prword(tzj|sz). 4 Combination of Evidence Once we have multiple ways to represent the query q in the document language (QTz(q), i = 1... m), it is possible to combine these “pieces of evidence” into a single representation as follows: QT(q) = �m wz(q)QTz(q) z=1 and each combination-of-evidence approach differs by how the combination weights wz are computed: Uniform In this baseline metho</context>
</contexts>
<marker>Darwish, Oard, 2003</marker>
<rawString>Kareem Darwish and Douglas W. Oard. 2003. Probabilistic structured query methods. In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Informaion Retrieval, SIGIR ’03, pages 338–344.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward A Fox</author>
</authors>
<title>Extending the Boolean and Vector Space Models of Information Retrieval with P-norm Queries and Multiple Concept Types.</title>
<date>1983</date>
<tech>Ph.D. thesis,</tech>
<institution>Cornell University,</institution>
<location>Ithaca, NY, USA.</location>
<contexts>
<context position="4180" citStr="Fox, 1983" startWordPosition="618" endWordPosition="619">uctured query formulation. Some of the earliest studies in IR discovered that with different underlying models, the retrieved document set would vary substantially, al589 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 589–599, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics though the effectiveness was similar (McGill et al., 1979). Later studies showed that combining different representations of the query and/or document often produced superior output (Rajashekar and Croft, 1995; Turtle and Croft, 1990; Fox, 1983). This intuitive idea was supported theoretically by Pearl (1988), concluding that multiple pieces of evidence estimates relevance more accurately, but that the benefit strongly depends on the quality and independence of each piece. Experiments by Belkin et al. (1995) indicated the need to properly weight each representation with respect to its effectiveness. These so-called “combinationof-evidence” techniques became more powerful with the introduction of Indri, a probabilistic retrieval framework specifically designed for combining multiple query and document representations (Metzler and Crof</context>
</contexts>
<marker>Fox, 1983</marker>
<rawString>Edward A. Fox. 1983. Extending the Boolean and Vector Space Models of Information Retrieval with P-norm Queries and Multiple Concept Types. Ph.D. thesis, Cornell University, Ithaca, NY, USA. AAI8328584.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fredric C Gey</author>
<author>Hailing Jiang</author>
<author>Vivien Petras</author>
<author>Aitao Chen</author>
</authors>
<title>Cross-language retrieval for the clef collections - comparing multiple methods of retrieval.</title>
<date>2001</date>
<booktitle>In Revised Papers from the Workshop of Cross-Language Evaluation Forum on Cross-Language Information Retrieval and Evaluation, CLEF ’00,</booktitle>
<pages>116--128</pages>
<publisher>UK. Springer-Verlag.</publisher>
<location>London, UK,</location>
<contexts>
<context position="5286" citStr="Gey et al., 2001" startWordPosition="780" endWordPosition="783">al framework specifically designed for combining multiple query and document representations (Metzler and Croft, 2005). Croft (2000) provides a detailed summary of earlier query combination approaches in IR, while Peters et al. (2012) cites more recent related work. The benefits of combination-of-evidence transfer to the cross-lingual case especially well, since the inherent ambiguity of translation readily provides a diverse set of representations. Most CLIR approaches implement a post-retrieval merging of ranked lists, each generated from different query (Hiemstra et al., 2001; Savoy, 2001; Gey et al., 2001; Chen and Gey, 2004) or document (Lopez and Romary, 2009) representations, also called “data fusion”. In contrast, we focus on a pre-retrieval combination at the modeling stage, so that a single complex query is used in retrieval, instead of multiple simpler ones. Two advantages of the former are easier implementation (since the approach requires no changes to the modeling side) and the possibly greater diversity that can be achieved by having separate retrieval runs. However, each ranked list needs to be limited in size, which might cause some potentially useful documents not to be considere</context>
</contexts>
<marker>Gey, Jiang, Petras, Chen, 2001</marker>
<rawString>Fredric C. Gey, Hailing Jiang, Vivien Petras, and Aitao Chen. 2001. Cross-language retrieval for the clef collections - comparing multiple methods of retrieval. In Revised Papers from the Workshop of Cross-Language Evaluation Forum on Cross-Language Information Retrieval and Evaluation, CLEF ’00, pages 116–128, London, UK, UK. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Herbert</author>
<author>Gy¨orgy Szarvas</author>
<author>Iryna Gurevych</author>
</authors>
<title>Combining query translation techniques to improve cross-language information retrieval.</title>
<date>2011</date>
<booktitle>In Proceedings of the 33rd European Conference on Advances in Information Retrieval, ECIR’11,</booktitle>
<pages>712--715</pages>
<publisher>Springer-Verlag.</publisher>
<location>Berlin, Heidelberg.</location>
<contexts>
<context position="1940" citStr="Herbert et al., 2011" startWordPosition="283" endWordPosition="286">st commonly adopted method is to translate queries into the document language. Many methods have been introduced for translating queries for CLIR, ranging from word-by-word dictionary lookups (Xu and Weischedel, 2005; Darwish and Oard, 2003) to sophisticated use of machine translation (MT) systems (Magdy and Jones, 2011; Ma et al., 2012). Previous research has shown that combining evidence from different translation approaches is superior to any single query translation method (Braschler, 2004; Elizabeth Boschee Raytheon BBN Technologies 10 Moulton St Cambridge, MA, 02138 USA eboschee@bbn.com Herbert et al., 2011). While there are numerous combination-of-evidence techniques for both mono-lingual and cross-lingual IR, recent work suggests that there is no one-size-fits-all solution. In fact, the optimal combination weights (i.e., weights assigned to each piece of evidence in a linear combination) differ greatly across queries, tasks, languages, and other variants (Ture et al., 2012; Berger and Savoy, 2007). In this paper, we introduce a novel method for learning optimal combination weights when building a linear combination of existing query translation approaches. From standard query-document relevance</context>
<context position="6621" citStr="Herbert et al. (2011)" startWordPosition="991" endWordPosition="994">ination was a more suitable choice, though we think that the two approaches have complementary benefits. The idea of combining query translations before retrieval has been explored previously. Braschler (2004) combines three translation approaches: output of an MT system, a novel translation approach based on a similarity thesaurus built automatically from a comparable corpus, and a dictionary-based translation. The main reason that this combination does not provide much benefit is due to the lower coverage of the thesaurus-based and dictionary-based translation methods. A similar approach by Herbert et al. (2011) uses Wikipedia to provide translations of certain phrases and entities, and combining that with the Google Translate MT system yields statistically significant improvements in English-to-German retrieval. More recently, Ture et al. (2012) presented a more sophisticated translation approach using the internal representation of an MT system, and reported statistically significant improvements when a preretrieval combination was performed. All of the previously cited approaches either use uniform weights for combination, or select weights based on collection-level information. However, as stated</context>
</contexts>
<marker>Herbert, Szarvas, Gurevych, 2011</marker>
<rawString>Benjamin Herbert, Gy¨orgy Szarvas, and Iryna Gurevych. 2011. Combining query translation techniques to improve cross-language information retrieval. In Proceedings of the 33rd European Conference on Advances in Information Retrieval, ECIR’11, pages 712–715, Berlin, Heidelberg. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Djoerd Hiemstra</author>
<author>Wessel Kraaij</author>
<author>Ren´ee Pohlmann</author>
<author>Thijs Westerveld</author>
</authors>
<title>Translation resources, merging strategies, and relevance feedback for cross-language information retrieval.</title>
<date>2001</date>
<booktitle>In Revised Papers from the Workshop of Cross-Language Evaluation Forum on Cross-Language Information Retrieval and Evaluation, CLEF ’00,</booktitle>
<pages>102--115</pages>
<publisher>UK. Springer-Verlag.</publisher>
<location>London, UK,</location>
<contexts>
<context position="5255" citStr="Hiemstra et al., 2001" startWordPosition="774" endWordPosition="777">on of Indri, a probabilistic retrieval framework specifically designed for combining multiple query and document representations (Metzler and Croft, 2005). Croft (2000) provides a detailed summary of earlier query combination approaches in IR, while Peters et al. (2012) cites more recent related work. The benefits of combination-of-evidence transfer to the cross-lingual case especially well, since the inherent ambiguity of translation readily provides a diverse set of representations. Most CLIR approaches implement a post-retrieval merging of ranked lists, each generated from different query (Hiemstra et al., 2001; Savoy, 2001; Gey et al., 2001; Chen and Gey, 2004) or document (Lopez and Romary, 2009) representations, also called “data fusion”. In contrast, we focus on a pre-retrieval combination at the modeling stage, so that a single complex query is used in retrieval, instead of multiple simpler ones. Two advantages of the former are easier implementation (since the approach requires no changes to the modeling side) and the possibly greater diversity that can be achieved by having separate retrieval runs. However, each ranked list needs to be limited in size, which might cause some potentially usefu</context>
</contexts>
<marker>Hiemstra, Kraaij, Pohlmann, Westerveld, 2001</marker>
<rawString>Djoerd Hiemstra, Wessel Kraaij, Ren´ee Pohlmann, and Thijs Westerveld. 2001. Translation resources, merging strategies, and relevance feedback for cross-language information retrieval. In Revised Papers from the Workshop of Cross-Language Evaluation Forum on Cross-Language Information Retrieval and Evaluation, CLEF ’00, pages 102–115, London, UK, UK. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David A Hull</author>
<author>Gregory Grefenstette</author>
</authors>
<title>Querying across languages: a dictionary-based approach to multilingual information retrieval.</title>
<date>1996</date>
<booktitle>In Proceedings of the 19th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’96,</booktitle>
<pages>49--57</pages>
<contexts>
<context position="3025" citStr="Hull and Grefenstette, 1996" startWordPosition="441" endWordPosition="444">ptimal combination weights when building a linear combination of existing query translation approaches. From standard query-document relevance judgments we train a set of classifiers, which produce a unique combination recipe for each query, based on a large set of features extracted from the query and collection. Experimental results show that the effectiveness of our method is significantly higher than state-of-the-art query translation methods and other combination strategies. 2 Related Work The earliest approaches to query translation for CLIR used machine-readable bilingual dictionaries (Hull and Grefenstette, 1996; Ballesteros and Croft, 1996), achieving around up to 60% of monolingual IR effectiveness. Xu and Weischedel (2005) showed that effectiveness can be increased to around 80% by weighting each translation proportional to its rank in the dictionary. The practice of weighting translation candidates was later formulated as a “structured query”, in which each query term is represented by a probability distribution over its translations in the document language (Pirkola, 1998; Kwok, 1999; Darwish and Oard, 2003). Our approach is based on the structured query formulation. Some of the earliest studies</context>
</contexts>
<marker>Hull, Grefenstette, 1996</marker>
<rawString>David A. Hull and Gregory Grefenstette. 1996. Querying across languages: a dictionary-based approach to multilingual information retrieval. In Proceedings of the 19th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’96, pages 49–57.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kui-Lam Kwok</author>
</authors>
<title>English-Chinese crosslanguage retrieval based on a translation package.</title>
<date>1999</date>
<booktitle>In Workshop on Machine Translation for Cross Language Information Retrieval, Machine Translation Summit VII,</booktitle>
<pages>8--13</pages>
<contexts>
<context position="3511" citStr="Kwok, 1999" startWordPosition="520" endWordPosition="521"> earliest approaches to query translation for CLIR used machine-readable bilingual dictionaries (Hull and Grefenstette, 1996; Ballesteros and Croft, 1996), achieving around up to 60% of monolingual IR effectiveness. Xu and Weischedel (2005) showed that effectiveness can be increased to around 80% by weighting each translation proportional to its rank in the dictionary. The practice of weighting translation candidates was later formulated as a “structured query”, in which each query term is represented by a probability distribution over its translations in the document language (Pirkola, 1998; Kwok, 1999; Darwish and Oard, 2003). Our approach is based on the structured query formulation. Some of the earliest studies in IR discovered that with different underlying models, the retrieved document set would vary substantially, al589 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 589–599, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics though the effectiveness was similar (McGill et al., 1979). Later studies showed that combining different representations of the query and/or document often produced superior o</context>
</contexts>
<marker>Kwok, 1999</marker>
<rawString>Kui-Lam Kwok. 1999. English-Chinese crosslanguage retrieval based on a translation package. In Workshop on Machine Translation for Cross Language Information Retrieval, Machine Translation Summit VII, pages 8–13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrice Lopez</author>
<author>Laurent Romary</author>
</authors>
<title>Patatras: Retrieval model combination and regression models for prior art search.</title>
<date>2009</date>
<booktitle>In Proceedings of the 10th Cross-language Evaluation Forum Conference on Multilingual Information Access Evaluation: Text Retrieval Experiments, CLEF’09,</booktitle>
<pages>430--437</pages>
<publisher>Springer-Verlag.</publisher>
<location>Berlin, Heidelberg.</location>
<contexts>
<context position="5344" citStr="Lopez and Romary, 2009" startWordPosition="791" endWordPosition="794">ltiple query and document representations (Metzler and Croft, 2005). Croft (2000) provides a detailed summary of earlier query combination approaches in IR, while Peters et al. (2012) cites more recent related work. The benefits of combination-of-evidence transfer to the cross-lingual case especially well, since the inherent ambiguity of translation readily provides a diverse set of representations. Most CLIR approaches implement a post-retrieval merging of ranked lists, each generated from different query (Hiemstra et al., 2001; Savoy, 2001; Gey et al., 2001; Chen and Gey, 2004) or document (Lopez and Romary, 2009) representations, also called “data fusion”. In contrast, we focus on a pre-retrieval combination at the modeling stage, so that a single complex query is used in retrieval, instead of multiple simpler ones. Two advantages of the former are easier implementation (since the approach requires no changes to the modeling side) and the possibly greater diversity that can be achieved by having separate retrieval runs. However, each ranked list needs to be limited in size, which might cause some potentially useful documents not to be considered in the combination at all. Since the focus of this paper</context>
</contexts>
<marker>Lopez, Romary, 2009</marker>
<rawString>Patrice Lopez and Laurent Romary. 2009. Patatras: Retrieval model combination and regression models for prior art search. In Proceedings of the 10th Cross-language Evaluation Forum Conference on Multilingual Information Access Evaluation: Text Retrieval Experiments, CLEF’09, pages 430–437, Berlin, Heidelberg. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yanjun Ma</author>
<author>Jian-Yun Nie</author>
<author>Hua Wu</author>
<author>Haifeng Wang</author>
</authors>
<title>Opening machine translation black box for cross-language information retrieval.</title>
<date>2012</date>
<booktitle>In Information Retrieval Technology,</booktitle>
<pages>467--476</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="1658" citStr="Ma et al., 2012" startWordPosition="243" endWordPosition="246">g and tuning on the end performance of our approach. 1 Introduction Cross-lingual information retrieval (CLIR) is a special case of information retrieval (IR) in which documents and queries are presented in different languages. In order to overcome the language barrier, the most commonly adopted method is to translate queries into the document language. Many methods have been introduced for translating queries for CLIR, ranging from word-by-word dictionary lookups (Xu and Weischedel, 2005; Darwish and Oard, 2003) to sophisticated use of machine translation (MT) systems (Magdy and Jones, 2011; Ma et al., 2012). Previous research has shown that combining evidence from different translation approaches is superior to any single query translation method (Braschler, 2004; Elizabeth Boschee Raytheon BBN Technologies 10 Moulton St Cambridge, MA, 02138 USA eboschee@bbn.com Herbert et al., 2011). While there are numerous combination-of-evidence techniques for both mono-lingual and cross-lingual IR, recent work suggests that there is no one-size-fits-all solution. In fact, the optimal combination weights (i.e., weights assigned to each piece of evidence in a linear combination) differ greatly across queries,</context>
</contexts>
<marker>Ma, Nie, Wu, Wang, 2012</marker>
<rawString>Yanjun Ma, Jian-Yun Nie, Hua Wu, and Haifeng Wang. 2012. Opening machine translation black box for cross-language information retrieval. In Information Retrieval Technology, pages 467–476. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Walid Magdy</author>
<author>Gareth J F Jones</author>
</authors>
<title>Should MT systems be used as black boxes in CLIR?</title>
<date>2011</date>
<booktitle>In Proceedings of the 33rd European Conference on Information Retrieval, ECIR ’11,</booktitle>
<pages>683--686</pages>
<contexts>
<context position="1640" citStr="Magdy and Jones, 2011" startWordPosition="239" endWordPosition="242">in differences, labeling and tuning on the end performance of our approach. 1 Introduction Cross-lingual information retrieval (CLIR) is a special case of information retrieval (IR) in which documents and queries are presented in different languages. In order to overcome the language barrier, the most commonly adopted method is to translate queries into the document language. Many methods have been introduced for translating queries for CLIR, ranging from word-by-word dictionary lookups (Xu and Weischedel, 2005; Darwish and Oard, 2003) to sophisticated use of machine translation (MT) systems (Magdy and Jones, 2011; Ma et al., 2012). Previous research has shown that combining evidence from different translation approaches is superior to any single query translation method (Braschler, 2004; Elizabeth Boschee Raytheon BBN Technologies 10 Moulton St Cambridge, MA, 02138 USA eboschee@bbn.com Herbert et al., 2011). While there are numerous combination-of-evidence techniques for both mono-lingual and cross-lingual IR, recent work suggests that there is no one-size-fits-all solution. In fact, the optimal combination weights (i.e., weights assigned to each piece of evidence in a linear combination) differ great</context>
</contexts>
<marker>Magdy, Jones, 2011</marker>
<rawString>Walid Magdy and Gareth J. F. Jones. 2011. Should MT systems be used as black boxes in CLIR? In Proceedings of the 33rd European Conference on Information Retrieval, ECIR ’11, pages 683–686.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael McGill</author>
<author>Matthew Koll</author>
<author>Terry Noreault</author>
</authors>
<title>An Evaluation of Factors Affecting Document Ranking by Information Retrieval Systems. ERIC reports. School of Information Studies,</title>
<date>1979</date>
<location>Syracuse University.</location>
<contexts>
<context position="3993" citStr="McGill et al., 1979" startWordPosition="589" endWordPosition="592">ich each query term is represented by a probability distribution over its translations in the document language (Pirkola, 1998; Kwok, 1999; Darwish and Oard, 2003). Our approach is based on the structured query formulation. Some of the earliest studies in IR discovered that with different underlying models, the retrieved document set would vary substantially, al589 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 589–599, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics though the effectiveness was similar (McGill et al., 1979). Later studies showed that combining different representations of the query and/or document often produced superior output (Rajashekar and Croft, 1995; Turtle and Croft, 1990; Fox, 1983). This intuitive idea was supported theoretically by Pearl (1988), concluding that multiple pieces of evidence estimates relevance more accurately, but that the benefit strongly depends on the quality and independence of each piece. Experiments by Belkin et al. (1995) indicated the need to properly weight each representation with respect to its effectiveness. These so-called “combinationof-evidence” techniques</context>
</contexts>
<marker>McGill, Koll, Noreault, 1979</marker>
<rawString>Michael McGill, Matthew Koll, and Terry Noreault. 1979. An Evaluation of Factors Affecting Document Ranking by Information Retrieval Systems. ERIC reports. School of Information Studies, Syracuse University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donald Metzler</author>
<author>W Bruce Croft</author>
</authors>
<title>A Markov random field model for term dependencies.</title>
<date>2005</date>
<booktitle>In Proceedings of the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’05,</booktitle>
<pages>472--479</pages>
<contexts>
<context position="4788" citStr="Metzler and Croft, 2005" startWordPosition="704" endWordPosition="707">1990; Fox, 1983). This intuitive idea was supported theoretically by Pearl (1988), concluding that multiple pieces of evidence estimates relevance more accurately, but that the benefit strongly depends on the quality and independence of each piece. Experiments by Belkin et al. (1995) indicated the need to properly weight each representation with respect to its effectiveness. These so-called “combinationof-evidence” techniques became more powerful with the introduction of Indri, a probabilistic retrieval framework specifically designed for combining multiple query and document representations (Metzler and Croft, 2005). Croft (2000) provides a detailed summary of earlier query combination approaches in IR, while Peters et al. (2012) cites more recent related work. The benefits of combination-of-evidence transfer to the cross-lingual case especially well, since the inherent ambiguity of translation readily provides a diverse set of representations. Most CLIR approaches implement a post-retrieval merging of ranked lists, each generated from different query (Hiemstra et al., 2001; Savoy, 2001; Gey et al., 2001; Chen and Gey, 2004) or document (Lopez and Romary, 2009) representations, also called “data fusion”.</context>
<context position="26884" citStr="Metzler and Croft, 2005" startWordPosition="4294" endWordPosition="4297">ds). From these data, word alignments were learned with GIZA++ (Och and Ney, 2003), using five iterations of each of IBM Models 1–4 and HMM. 3-gram Chinese and 5-gram Arabic Kneser-Ney language models were trained from the Gigaword corpus (1b words each) and non-English side of the training corpus. Chinese and English parallel text were preprocessed through the Treebank Tokenizer,4 while no special treatment was performed on Arabic. For retrieval, we used Tndri, a state-of-theart probabilistic relevance model that supports weighted query representations through operators #combine and #weight (Metzler and Croft, 2005). A character-based index was built for Chinese collections, whereas Arabic text was stemmed using Lucene before indexing.5 English text was preprocessed by Tndri’s implementation of the Porter stemmer (Porter, 1997). Statistics for each collection and query set are summarized in Table 1. Before performing any combination, we first ran the three baseline QT methods individually and evaluated the retrieved documents. Mean average precision (MAP) was used to measure retrieval effectiveness, which is a widely used and stable metric, estimating the area under the precision-recall curve. We set n =</context>
</contexts>
<marker>Metzler, Croft, 2005</marker>
<rawString>Donald Metzler and W. Bruce Croft. 2005. A Markov random field model for term dependencies. In Proceedings of the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’05, pages 472–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="26342" citStr="Och and Ney, 2003" startWordPosition="4214" endWordPosition="4217">trieval tasks as part of the DARPA Broad Operational Language Technologies (BOLT) program: English-Arabic (BOLTar) and English-Chinese (BOLTch). The query language is English in all cases, and we preprocess the queries using BBN’s information extraction toolkit SERIF (Ramshaw et al., 2011). State-of-the-art English-Arabic (EnAr) and English-Chinese (En-Ch) MT systems were trained on parallel corpora released in NIST OpenMT 2012, in addition to parallel forum data collected as part of the BOLT program (10m EnAr words; 30m En-Ch words). From these data, word alignments were learned with GIZA++ (Och and Ney, 2003), using five iterations of each of IBM Models 1–4 and HMM. 3-gram Chinese and 5-gram Arabic Kneser-Ney language models were trained from the Gigaword corpus (1b words each) and non-English side of the training corpus. Chinese and English parallel text were preprocessed through the Treebank Tokenizer,4 while no special treatment was performed on Arabic. For retrieval, we used Tndri, a state-of-theart probabilistic relevance model that supports weighted query representations through operators #combine and #weight (Metzler and Croft, 2005). A character-based index was built for Chinese collection</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz J. Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Judea Pearl</author>
</authors>
<title>Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference.</title>
<date>1988</date>
<publisher>Morgan Kaufmann Publishers Inc.,</publisher>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="4245" citStr="Pearl (1988)" startWordPosition="628" endWordPosition="629">iscovered that with different underlying models, the retrieved document set would vary substantially, al589 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 589–599, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics though the effectiveness was similar (McGill et al., 1979). Later studies showed that combining different representations of the query and/or document often produced superior output (Rajashekar and Croft, 1995; Turtle and Croft, 1990; Fox, 1983). This intuitive idea was supported theoretically by Pearl (1988), concluding that multiple pieces of evidence estimates relevance more accurately, but that the benefit strongly depends on the quality and independence of each piece. Experiments by Belkin et al. (1995) indicated the need to properly weight each representation with respect to its effectiveness. These so-called “combinationof-evidence” techniques became more powerful with the introduction of Indri, a probabilistic retrieval framework specifically designed for combining multiple query and document representations (Metzler and Croft, 2005). Croft (2000) provides a detailed summary of earlier que</context>
</contexts>
<marker>Pearl, 1988</marker>
<rawString>Judea Pearl. 1988. Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabian Pedregosa</author>
<author>Alexandre Gramfort Ga¨el Varoquaux</author>
<author>Vincent Michel</author>
<author>Bertrand Thirion</author>
<author>Olivier Grisel</author>
<author>Mathieu Blondel</author>
<author>Peter Prettenhofer</author>
</authors>
<title>Scikit-learn: Machine learning in python.</title>
<date>2012</date>
<journal>CoRR,</journal>
<pages>1201--0490</pages>
<location>Ron Weiss, Vincent Dubourg, Jake VanderPlas, Alexandre Passos, David</location>
<marker>Pedregosa, Ga¨el Varoquaux, Michel, Thirion, Grisel, Blondel, Prettenhofer, 2012</marker>
<rawString>Fabian Pedregosa, Ga¨el Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake VanderPlas, Alexandre Passos, David Cournapeau, Matthieu Brucher, Matthieu Perrot, and Edouard Duchesnay. 2012. Scikit-learn: Machine learning in python. CoRR, abs/1201.0490.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carol Peters</author>
<author>Martin Braschler</author>
<author>Paul Clough</author>
</authors>
<title>Multilingual Information Retrieval - From Research To Practice.</title>
<date>2012</date>
<publisher>Springer.</publisher>
<contexts>
<context position="4904" citStr="Peters et al. (2012)" startWordPosition="724" endWordPosition="727">idence estimates relevance more accurately, but that the benefit strongly depends on the quality and independence of each piece. Experiments by Belkin et al. (1995) indicated the need to properly weight each representation with respect to its effectiveness. These so-called “combinationof-evidence” techniques became more powerful with the introduction of Indri, a probabilistic retrieval framework specifically designed for combining multiple query and document representations (Metzler and Croft, 2005). Croft (2000) provides a detailed summary of earlier query combination approaches in IR, while Peters et al. (2012) cites more recent related work. The benefits of combination-of-evidence transfer to the cross-lingual case especially well, since the inherent ambiguity of translation readily provides a diverse set of representations. Most CLIR approaches implement a post-retrieval merging of ranked lists, each generated from different query (Hiemstra et al., 2001; Savoy, 2001; Gey et al., 2001; Chen and Gey, 2004) or document (Lopez and Romary, 2009) representations, also called “data fusion”. In contrast, we focus on a pre-retrieval combination at the modeling stage, so that a single complex query is used </context>
</contexts>
<marker>Peters, Braschler, Clough, 2012</marker>
<rawString>Carol Peters, Martin Braschler, and Paul Clough. 2012. Multilingual Information Retrieval - From Research To Practice. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ari Pirkola</author>
</authors>
<title>The effects of query structure and dictionary-setups in dictionary-based crosslanguage information retrieval.</title>
<date>1998</date>
<booktitle>In Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’98,</booktitle>
<pages>55--63</pages>
<contexts>
<context position="3499" citStr="Pirkola, 1998" startWordPosition="518" endWordPosition="519">elated Work The earliest approaches to query translation for CLIR used machine-readable bilingual dictionaries (Hull and Grefenstette, 1996; Ballesteros and Croft, 1996), achieving around up to 60% of monolingual IR effectiveness. Xu and Weischedel (2005) showed that effectiveness can be increased to around 80% by weighting each translation proportional to its rank in the dictionary. The practice of weighting translation candidates was later formulated as a “structured query”, in which each query term is represented by a probability distribution over its translations in the document language (Pirkola, 1998; Kwok, 1999; Darwish and Oard, 2003). Our approach is based on the structured query formulation. Some of the earliest studies in IR discovered that with different underlying models, the retrieved document set would vary substantially, al589 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 589–599, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics though the effectiveness was similar (McGill et al., 1979). Later studies showed that combining different representations of the query and/or document often produce</context>
</contexts>
<marker>Pirkola, 1998</marker>
<rawString>Ari Pirkola. 1998. The effects of query structure and dictionary-setups in dictionary-based crosslanguage information retrieval. In Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’98, pages 55–63.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M F Porter</author>
</authors>
<title>Readings in information retrieval. chapter An Algorithm for Suffix Stripping,</title>
<date>1997</date>
<pages>313--316</pages>
<publisher>Morgan Kaufmann Publishers Inc.,</publisher>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="27100" citStr="Porter, 1997" startWordPosition="4328" endWordPosition="4329">word corpus (1b words each) and non-English side of the training corpus. Chinese and English parallel text were preprocessed through the Treebank Tokenizer,4 while no special treatment was performed on Arabic. For retrieval, we used Tndri, a state-of-theart probabilistic relevance model that supports weighted query representations through operators #combine and #weight (Metzler and Croft, 2005). A character-based index was built for Chinese collections, whereas Arabic text was stemmed using Lucene before indexing.5 English text was preprocessed by Tndri’s implementation of the Porter stemmer (Porter, 1997). Statistics for each collection and query set are summarized in Table 1. Before performing any combination, we first ran the three baseline QT methods individually and evaluated the retrieved documents. Mean average precision (MAP) was used to measure retrieval effectiveness, which is a widely used and stable metric, estimating the area under the precision-recall curve. We set n = 10 for the n-best probabilistic translation method. Baseline scores are reported in Table 2. The average precision (AP) of each query in these tasks was used to label the query and construct training data accordingl</context>
</contexts>
<marker>Porter, 1997</marker>
<rawString>M. F. Porter. 1997. Readings in information retrieval. chapter An Algorithm for Suffix Stripping, pages 313–316. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T B Rajashekar</author>
<author>W Bruce Croft</author>
</authors>
<title>Combining automatic and manual index representations in probabilistic retrieval.</title>
<date>1995</date>
<journal>J. Am. Soc. Inf. Sci.,</journal>
<volume>46</volume>
<issue>4</issue>
<contexts>
<context position="4144" citStr="Rajashekar and Croft, 1995" startWordPosition="610" endWordPosition="613">sh and Oard, 2003). Our approach is based on the structured query formulation. Some of the earliest studies in IR discovered that with different underlying models, the retrieved document set would vary substantially, al589 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 589–599, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics though the effectiveness was similar (McGill et al., 1979). Later studies showed that combining different representations of the query and/or document often produced superior output (Rajashekar and Croft, 1995; Turtle and Croft, 1990; Fox, 1983). This intuitive idea was supported theoretically by Pearl (1988), concluding that multiple pieces of evidence estimates relevance more accurately, but that the benefit strongly depends on the quality and independence of each piece. Experiments by Belkin et al. (1995) indicated the need to properly weight each representation with respect to its effectiveness. These so-called “combinationof-evidence” techniques became more powerful with the introduction of Indri, a probabilistic retrieval framework specifically designed for combining multiple query and docume</context>
</contexts>
<marker>Rajashekar, Croft, 1995</marker>
<rawString>T. B. Rajashekar and W. Bruce Croft. 1995. Combining automatic and manual index representations in probabilistic retrieval. J. Am. Soc. Inf. Sci., 46(4):272–283, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lance Ramshaw</author>
<author>Elizabeth Boschee</author>
<author>Marjorie Freedman</author>
<author>Jessica MacBride</author>
<author>Ralph Weischedel</author>
<author>Alex Zamanian</author>
</authors>
<title>Serif language processing — effective trainable language understanding.</title>
<date>2011</date>
<booktitle>Handbook of Natural Language Processing and Machine Translation: DARPA Global Autonomous Language Exploitation,</booktitle>
<pages>626--631</pages>
<editor>In J. Olive et al., editor,</editor>
<publisher>Springer.</publisher>
<contexts>
<context position="26014" citStr="Ramshaw et al., 2011" startWordPosition="4162" endWordPosition="4165"> evaluate the effectiveness of labeling approaches. 5 Evaluation We evaluated our approach on four different CLIR tasks: TREC 2002 English-Arabic CLIR, NTCIR8 English-Chinese Advanced Cross-Lingual InforPr(pick i|q) ⎛ 1 . . . 1 = ⎝ � . . . � ⎛ l1=0 l„=0 = ⎝ 1 1 � � l1=0 l„=0 �m i=1 594 mation Access (ACLIA), and two forum post retrieval tasks as part of the DARPA Broad Operational Language Technologies (BOLT) program: English-Arabic (BOLTar) and English-Chinese (BOLTch). The query language is English in all cases, and we preprocess the queries using BBN’s information extraction toolkit SERIF (Ramshaw et al., 2011). State-of-the-art English-Arabic (EnAr) and English-Chinese (En-Ch) MT systems were trained on parallel corpora released in NIST OpenMT 2012, in addition to parallel forum data collected as part of the BOLT program (10m EnAr words; 30m En-Ch words). From these data, word alignments were learned with GIZA++ (Och and Ney, 2003), using five iterations of each of IBM Models 1–4 and HMM. 3-gram Chinese and 5-gram Arabic Kneser-Ney language models were trained from the Gigaword corpus (1b words each) and non-English side of the training corpus. Chinese and English parallel text were preprocessed th</context>
</contexts>
<marker>Ramshaw, Boschee, Freedman, MacBride, Weischedel, Zamanian, 2011</marker>
<rawString>Lance Ramshaw, Elizabeth Boschee, Marjorie Freedman, Jessica MacBride, Ralph Weischedel, and Alex Zamanian. 2011. Serif language processing — effective trainable language understanding. In J. Olive et al., editor, Handbook of Natural Language Processing and Machine Translation: DARPA Global Autonomous Language Exploitation, pages 626– 631. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacques Savoy</author>
</authors>
<title>Report on CLEF-2001 experiments: Effective combined query-translation approach.</title>
<date>2001</date>
<booktitle>In CLEF,</booktitle>
<pages>27--43</pages>
<contexts>
<context position="5268" citStr="Savoy, 2001" startWordPosition="778" endWordPosition="779">istic retrieval framework specifically designed for combining multiple query and document representations (Metzler and Croft, 2005). Croft (2000) provides a detailed summary of earlier query combination approaches in IR, while Peters et al. (2012) cites more recent related work. The benefits of combination-of-evidence transfer to the cross-lingual case especially well, since the inherent ambiguity of translation readily provides a diverse set of representations. Most CLIR approaches implement a post-retrieval merging of ranked lists, each generated from different query (Hiemstra et al., 2001; Savoy, 2001; Gey et al., 2001; Chen and Gey, 2004) or document (Lopez and Romary, 2009) representations, also called “data fusion”. In contrast, we focus on a pre-retrieval combination at the modeling stage, so that a single complex query is used in retrieval, instead of multiple simpler ones. Two advantages of the former are easier implementation (since the approach requires no changes to the modeling side) and the possibly greater diversity that can be achieved by having separate retrieval runs. However, each ranked list needs to be limited in size, which might cause some potentially useful documents n</context>
</contexts>
<marker>Savoy, 2001</marker>
<rawString>Jacques Savoy. 2001. Report on CLEF-2001 experiments: Effective combined query-translation approach. In CLEF, pages 27–43.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark D Smucker</author>
<author>James Allan</author>
<author>Ben Carterette</author>
</authors>
<title>A comparison of statistical significance tests for information retrieval evaluation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 16th ACM conference on Conference on Information and Knowledge Management, CIKM ’07,</booktitle>
<pages>623--632</pages>
<contexts>
<context position="35747" citStr="Smucker et al., 2007" startWordPosition="5723" endWordPosition="5726">mbination methods. included in the train set when testing on TREC or NTCIR (see lowest two rows in Table 3). Also, when there is no domain effect (i.e., half-blind and fully-blind ), more data yields higher effectiveness in 6 out of 8 cases (see two right columns on the left side of Table 3). 5.3 Retrieval Effectiveness In this section, we compare our novel queryspecific combination-of-evidence approach to the baseline CLIR approaches, as well as comparable combination methods (uniform and task-specific combination) in terms of retrieval effectiveness. Based on a randomized significance test (Smucker et al., 2007), the best query-specific combination method (shown in boldface in Table 3) outperforms all baseline QT methods in all tasks with 95% confidence (indicated by superscript * in Table 3). This is not the case for uniform or task-specific query combination, which are statistically indistinguishable from at least one of the QT methods, depending on the task (indicated by superscripts 1, 2, and 3 for one-best, probabilistic 10-best, and word-based QT methods, respectively). When we directly compare our queryspecific combination approach to other combination methods, the differences are statisticall</context>
</contexts>
<marker>Smucker, Allan, Carterette, 2007</marker>
<rawString>Mark D. Smucker, James Allan, and Ben Carterette. 2007. A comparison of statistical significance tests for information retrieval evaluation. In Proceedings of the 16th ACM conference on Conference on Information and Knowledge Management, CIKM ’07, pages 623–632.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ferhan Ture</author>
<author>Jimmy Lin</author>
<author>Douglas W Oard</author>
</authors>
<title>Combining statistical translation techniques for cross-language information retrieval.</title>
<date>2012</date>
<booktitle>In Proceedings of the 24th International Conference on Computational Linguistics, COLING ’12,</booktitle>
<pages>2685--2702</pages>
<contexts>
<context position="2314" citStr="Ture et al., 2012" startWordPosition="336" endWordPosition="339">ining evidence from different translation approaches is superior to any single query translation method (Braschler, 2004; Elizabeth Boschee Raytheon BBN Technologies 10 Moulton St Cambridge, MA, 02138 USA eboschee@bbn.com Herbert et al., 2011). While there are numerous combination-of-evidence techniques for both mono-lingual and cross-lingual IR, recent work suggests that there is no one-size-fits-all solution. In fact, the optimal combination weights (i.e., weights assigned to each piece of evidence in a linear combination) differ greatly across queries, tasks, languages, and other variants (Ture et al., 2012; Berger and Savoy, 2007). In this paper, we introduce a novel method for learning optimal combination weights when building a linear combination of existing query translation approaches. From standard query-document relevance judgments we train a set of classifiers, which produce a unique combination recipe for each query, based on a large set of features extracted from the query and collection. Experimental results show that the effectiveness of our method is significantly higher than state-of-the-art query translation methods and other combination strategies. 2 Related Work The earliest app</context>
<context position="6860" citStr="Ture et al. (2012)" startWordPosition="1026" endWordPosition="1029">aches: output of an MT system, a novel translation approach based on a similarity thesaurus built automatically from a comparable corpus, and a dictionary-based translation. The main reason that this combination does not provide much benefit is due to the lower coverage of the thesaurus-based and dictionary-based translation methods. A similar approach by Herbert et al. (2011) uses Wikipedia to provide translations of certain phrases and entities, and combining that with the Google Translate MT system yields statistically significant improvements in English-to-German retrieval. More recently, Ture et al. (2012) presented a more sophisticated translation approach using the internal representation of an MT system, and reported statistically significant improvements when a preretrieval combination was performed. All of the previously cited approaches either use uniform weights for combination, or select weights based on collection-level information. However, as stated previously, numerous studies suggest that certain methods work better on certain queries, collections, languages. In fact, when weights are optimized separately on each collection, they differ substantially across different collections (T</context>
<context position="9427" citStr="Ture et al., 2012" startWordPosition="1414" endWordPosition="1417">ions. 3 Query Translation The primary contribution of this paper is to show how a diverse set of query translation (QT) methods can be combined effectively into a single weighted structured query, with improved retrieval effectiveness. While our approach can applied to any set of translation methods, we focus on three methods that have complementary strengths and that have shown promise in CLIR: word-based probabilistic translation, one-best MT, and n-best probabilistic MT. We briefly present our implementation of each method; more details can be found in earlier work (Darwish and Oard, 2003; Ture et al., 2012). Each QT method generates a representation of the query in the document language. In the case of word-based and n-best MT approaches, the representation is a structured query itself, where each query word is represented by a probability distribution over translation alternatives. For one-best MT, the query is represented by a bag of translated words. 3.1 One-Best MT A query translation approach that has become more popular recently is to simply run the query through an MT system, and use the best output as the query: t1t2 ... tl = MT(s1s2 ... sk) (1) where s = s1s2 ... sk is the query and t =</context>
<context position="11103" citStr="Ture et al. (2012)" startWordPosition="1688" endWordPosition="1691">imes be incorrect, or might lack some of the alternative representations that are very useful in retrieval. Therefore, considering the n highest scored translations (also referred to as the n-best list in MT literature) has become increasingly popular in CLIR approaches. In order to benefit from the diversity amongst the n-best translations, one can simply concatenate them together, forming a large list of query terms. However, statistical MT systems also assign probabilities to each translation, which can be incorporated into the query representation for better effectiveness, as suggested by Ture et al. (2012). In this approach, each of the top n translation candidates from the MT system are processed one by one. For each translation candidate, the MT system provides a translation probability, and alignments between words in the query and its translation. As we process each of the n translations, for each query word sz, we accumulate probabilities on each translated word tzj aligned to sz. Finally, we normalize the translation probabilities to get Prnbest(tzj|sz). 3.3 Word-based One of the most widely used approaches in CLIR is based on translating each query word sz independently, with probabiliti</context>
</contexts>
<marker>Ture, Lin, Oard, 2012</marker>
<rawString>Ferhan Ture, Jimmy Lin, and Douglas W. Oard. 2012. Combining statistical translation techniques for cross-language information retrieval. In Proceedings of the 24th International Conference on Computational Linguistics, COLING ’12, pages 2685–2702.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Howard Turtle</author>
<author>W Bruce Croft</author>
</authors>
<title>Inference networks for document retrieval.</title>
<date>1990</date>
<booktitle>In Proceedings of the 13th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’90,</booktitle>
<pages>1--24</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="4168" citStr="Turtle and Croft, 1990" startWordPosition="614" endWordPosition="617">oach is based on the structured query formulation. Some of the earliest studies in IR discovered that with different underlying models, the retrieved document set would vary substantially, al589 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 589–599, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics though the effectiveness was similar (McGill et al., 1979). Later studies showed that combining different representations of the query and/or document often produced superior output (Rajashekar and Croft, 1995; Turtle and Croft, 1990; Fox, 1983). This intuitive idea was supported theoretically by Pearl (1988), concluding that multiple pieces of evidence estimates relevance more accurately, but that the benefit strongly depends on the quality and independence of each piece. Experiments by Belkin et al. (1995) indicated the need to properly weight each representation with respect to its effectiveness. These so-called “combinationof-evidence” techniques became more powerful with the introduction of Indri, a probabilistic retrieval framework specifically designed for combining multiple query and document representations (Metz</context>
</contexts>
<marker>Turtle, Croft, 1990</marker>
<rawString>Howard Turtle and W. Bruce Croft. 1990. Inference networks for document retrieval. In Proceedings of the 13th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’90, pages 1–24, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jinxi Xu</author>
<author>Ralph Weischedel</author>
</authors>
<title>Empirical studies on the impact of lexical resources on CLIR performance.</title>
<date>2005</date>
<journal>Information Processing &amp; Management,</journal>
<volume>41</volume>
<issue>3</issue>
<contexts>
<context position="1535" citStr="Xu and Weischedel, 2005" startWordPosition="223" endWordPosition="226">sk-specific weighting. An in-depth empirical analysis presents insights about the effect of data size, domain differences, labeling and tuning on the end performance of our approach. 1 Introduction Cross-lingual information retrieval (CLIR) is a special case of information retrieval (IR) in which documents and queries are presented in different languages. In order to overcome the language barrier, the most commonly adopted method is to translate queries into the document language. Many methods have been introduced for translating queries for CLIR, ranging from word-by-word dictionary lookups (Xu and Weischedel, 2005; Darwish and Oard, 2003) to sophisticated use of machine translation (MT) systems (Magdy and Jones, 2011; Ma et al., 2012). Previous research has shown that combining evidence from different translation approaches is superior to any single query translation method (Braschler, 2004; Elizabeth Boschee Raytheon BBN Technologies 10 Moulton St Cambridge, MA, 02138 USA eboschee@bbn.com Herbert et al., 2011). While there are numerous combination-of-evidence techniques for both mono-lingual and cross-lingual IR, recent work suggests that there is no one-size-fits-all solution. In fact, the optimal co</context>
<context position="3141" citStr="Xu and Weischedel (2005)" startWordPosition="459" endWordPosition="462">uery-document relevance judgments we train a set of classifiers, which produce a unique combination recipe for each query, based on a large set of features extracted from the query and collection. Experimental results show that the effectiveness of our method is significantly higher than state-of-the-art query translation methods and other combination strategies. 2 Related Work The earliest approaches to query translation for CLIR used machine-readable bilingual dictionaries (Hull and Grefenstette, 1996; Ballesteros and Croft, 1996), achieving around up to 60% of monolingual IR effectiveness. Xu and Weischedel (2005) showed that effectiveness can be increased to around 80% by weighting each translation proportional to its rank in the dictionary. The practice of weighting translation candidates was later formulated as a “structured query”, in which each query term is represented by a probability distribution over its translations in the document language (Pirkola, 1998; Kwok, 1999; Darwish and Oard, 2003). Our approach is based on the structured query formulation. Some of the earliest studies in IR discovered that with different underlying models, the retrieved document set would vary substantially, al589 </context>
</contexts>
<marker>Xu, Weischedel, 2005</marker>
<rawString>Jinxi Xu and Ralph Weischedel. 2005. Empirical studies on the impact of lexical resources on CLIR performance. Information Processing &amp; Management, 41(3):475–487, May.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000036">
<title confidence="0.750296333333333">
A Polynomial-Time Dynamic Oracle
for Non-Projective Dependency Parsing
Carlos G´omez-Rodriguez
</title>
<author confidence="0.359561">
Departamento de
</author>
<affiliation confidence="0.487893">
Computaci´on
</affiliation>
<address confidence="0.722775">
Universidade da Coru˜na, Spain
</address>
<email confidence="0.99262">
cgomezr@udc.es
</email>
<author confidence="0.988706">
Francesco Sartorio
</author>
<affiliation confidence="0.985954">
Department of
Information Engineering
University of Padua, Italy
</affiliation>
<email confidence="0.994165">
sartorio@dei.unipd.it
</email>
<author confidence="0.988956">
Giorgio Satta
</author>
<affiliation confidence="0.986012">
Department of
Information Engineering
University of Padua, Italy
</affiliation>
<email confidence="0.995603">
satta@dei.unipd.it
</email>
<sectionHeader confidence="0.994709" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999926357142857">
The introduction of dynamic oracles has
considerably improved the accuracy of
greedy transition-based dependency pars-
ers, without sacrificing parsing efficiency.
However, this enhancement is limited to
projective parsing, and dynamic oracles
have not yet been implemented for pars-
ers supporting non-projectivity. In this
paper we introduce the first such oracle,
for a non-projective parser based on At-
tardi’s parser. We show that training with
this oracle improves parsing accuracy over
a conventional (static) oracle on a wide
range of datasets.
</bodyText>
<sectionHeader confidence="0.998428" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99991965625">
Greedy transition-based parsers for dependency
grammars have been pioneered by Yamada and
Matsumoto (2003) and Nivre (2003). These meth-
ods incrementally process the input sentence from
left to right, predicting the next parsing action,
called transition, on the basis of a compact rep-
resentation of the derivation history.
Greedy transition-based parsers can be very
efficient, allowing web-scale parsing with high
throughput. However, the accuracy of these meth-
ods still falls behind that of transition-based pars-
ers using beam-search, where the accuracy im-
provement is obtained at the cost of a decrease
in parsing efficiency; see for instance Zhang and
Nivre (2011), Huang and Sagae (2010), Choi and
McCallum (2013). As an alternative to beam-
search, recent research on transition-based parsing
has therefore explored possible ways of improving
accuracy at no extra cost in parsing efficiency.
The training of transition-based parsers relies
on a component called the parsing oracle, which
maps parser configurations to optimal transitions
with respect to a gold tree. A discriminative model
is then trained to simulate the oracle’s behavior,
and is later used for decoding. Traditionally, so-
called static oracles have been exploited in train-
ing, where a static oracle is defined only for con-
figurations that have been reached by computa-
tions with no mistake, and it returns a single ca-
nonical transition among those that are optimal.
Very recently, Goldberg and Nivre (2012),
Goldberg and Nivre (2013) and Goldberg et al.
(2014) showed that the accuracy of transition-
based parsers can be substantially improved using
dynamic oracles. A dynamic oracle returns the
set of all transitions that are optimal for a given
configuration, with respect to the gold tree, and
is well-defined and correct for every configuration
that is reachable by the parser.
Naive implementations of dynamic oracles run
in exponential time, since they need to simulate
all possible computations of the parser for the in-
put configuration. Polynomial-time implementa-
tions of dynamic oracles have been proposed by
the above mentioned authors for several project-
ive dependency parsers. To our knowledge, no
polynomial-time algorithm has been published for
transition-based parsers based on non-projective
dependency grammars.
In this paper we consider a restriction of a
transition-based, non-projective parser originally
presented by Attardi (2006). This restriction
was further investigated by Kuhlmann and Nivre
(2010) and Cohen et al. (2011). We provide an im-
plementation for a dynamic oracle for this parser
running in polynomial time.
We experimentally compare the parser trained
with the dynamic oracle to a baseline obtained
by training with a static oracle. Significant ac-
curacy improvements are achieved on many lan-
guages when using our dynamic oracle. To our
knowledge, these are the first experimental results
on non-projective parsing based on a dynamic or-
acle.
</bodyText>
<page confidence="0.938922">
917
</page>
<note confidence="0.9392865">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 917–927,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.97286" genericHeader="introduction">
2 Preliminary Definitions
</sectionHeader>
<bodyText confidence="0.9998372">
Transition-based dependency parsing was origin-
ally introduced by Yamada and Matsumoto (2003)
and Nivre (2003). In this section we briefly sum-
marize the notation we use for this framework and
introduce the notion of dynamic oracle.
</bodyText>
<subsectionHeader confidence="0.983805">
2.1 Transition-Based Dependency Parsing
</subsectionHeader>
<bodyText confidence="0.999887887096774">
We represent an input sentence as a string w =
w0 · · · w,,,, n &gt; 1, where each wz with i =� 0 is a
lexical symbol and w0 is a special symbol called
root. Set V„ = {i  |0 &lt; i &lt; n} denotes the sym-
bol occurrences in w. For i, j E V„ with i =� j,
we write i —* j to denote a grammatical depend-
ency of some unspecified type between wz and wj,
where wz is the head and wj is the dependent.
A dependency tree t for w is a directed tree
with node set V„ and with root node 0. An arc of t
is a pair (i, j), encoding a dependency i —* j; we
will often use the latter notation to denote arcs.
A transition-based dependency parser typically
uses a stack data structure to process the input
string from left to right, in a way very similar
to the classical push-down automaton for context-
free languages (Hopcroft et al., 2006). Each stack
element is a node from V„, representing the root
of a dependency tree spanning some portion of the
input w, and no internal state is used. At each step
the parser applies some transition that updates the
stack and/or consumes one symbol from the input.
Transitions may also construct new dependencies,
which are added to the current configuration of the
parser.
We represent the stack as an ordered sequence
σ = [hd, ... , h1], d &gt; 0, of nodes hz E V„, with
the topmost element placed at the right. When
d = 0, we have the empty stack σ = []. We use
the vertical bar to denote the append operator for
σ, and write σ = σ&apos;|h1 to indicate that h1 is the
topmost element of σ.
The portion of the input string still to be pro-
cessed by the parser is called the buffer. We
represent the buffer as an ordered sequence β =
[i, ... , n] of nodes from V„, with i the first ele-
ment of the buffer. We denote the empty buffer
as β = []. Again, we use the vertical bar to de-
note the append operator, and write β = i|β&apos; to
indicate that i is the first symbol occurrence of β;
consequently, we have β&apos; = [i + 1, ... , n].
In a transition-based parser, the parsing pro-
cess is defined through the technical notions of
configuration and transition. A configuration of
the parser relative to w is a triple c = (σ, β, A),
where σ and β are a stack and a buffer, respect-
ively, and A is the set of arcs that have been built
so far. A transition is a partial function map-
ping the set of parser configurations into itself.
Each transition-based parser is defined by means
of some finite inventory of transitions. We will
later introduce the specific inventory of transitions
for the parser that we investigate in this paper.
We use the symbol �- to denote the binary relation
formed by the union of all transitions of a parser.
With the notions of configuration and transition
in place, we can define a computation of the
parser on w as a sequence c0, c1, ... , cm, m &gt; 0,
of configurations relative to w, under the condition
that cz−1 �- cz for each i with 1 &lt; i &lt; m. We use
the reflexive and transitive closure of �-, written
�-∗, to represent computations.
</bodyText>
<subsectionHeader confidence="0.999726">
2.2 Configuration Loss and Dynamic Oracles
</subsectionHeader>
<bodyText confidence="0.998853709677419">
A transition-based dependency parser is a non-
deterministic device, meaning that a given con-
figuration can be mapped into several configur-
ations by the available transitions. However, in
several implementations the parser is associated
with a discriminative model that, on the basis of
some features of the current configuration, always
chooses a single transition. In other words, the
model is used to run the parser as a pseudo-de-
terministic device. The training of the discriminat-
ive model relies on a component called the parsing
oracle, which maps parser configurations to “op-
timal” transitions with respect to some reference
dependency tree, which we call the gold tree.
Traditionally, so-called static oracles have been
used which return a single, canonical transition
and they do so only for configurations that can
reach the gold tree, that is, configurations repres-
enting parsing histories with no mistake. In re-
cent work, Goldberg and Nivre (2012), Goldberg
and Nivre (2013) and Goldberg et al. (2014) have
introduced dynamic oracles, which return the set
of all transitions that are optimal with respect to
a gold tree, and are well-defined and correct for
every configuration that is reachable by the parser.
These authors have shown that the accuracy of
transition-based dependency parsers can be sub-
stantially improved if dynamic oracles are used in
place of static ones. In what follows, we provide
a mathematical definition of dynamic oracles, fol-
lowing Goldberg et al. (2014).
</bodyText>
<page confidence="0.962385">
918
</page>
<equation confidence="0.4423446">
(σ, k|β, A) `sh (σ|k, β, A)
(σ|i|j,β,A) `la (σ|j,β,A ∪ {j → i})
(σ|i|j,β,A) `ra (σ|i,β,A ∪ {i → j})
(σ|i|j|k, β, A) `la2 (σ|j|k, β, A ∪ {k → i})
(σ|i|j|k, β, A) `ra2 (σ|i|j, β, A ∪ {i → k})
</equation>
<figureCaption confidence="0.967747333333333">
Figure 1: Transitions of the non-projective parser.
Let t1 and t2 be dependency trees for w, with
arc sets A1 and A2, respectively. The loss of t1
</figureCaption>
<bodyText confidence="0.812751">
with respect to t2 is defined as
</bodyText>
<equation confidence="0.887191">
L(t1, t2) = |A1 \ A2 |. (1)
</equation>
<bodyText confidence="0.9861073">
Note that L(t1, t2) = L(t2, t1), since |A1 |= |A2|.
Furthermore L(t1, t2) = 0 if and only if t1 and t2
are the same tree.
Let c be a configuration of a transition-based
parser relative to w. Let also D(c) be the set of all
dependency trees that can be obtained in a com-
putation of the form c `* cf, where cf is a final
configuration, that is, a configuration that has con-
structed a dependency tree for w. We extend the
loss function in (1) to configurations by letting
</bodyText>
<equation confidence="0.993201">
L(c, t2) = min
t1ED(c)
</equation>
<bodyText confidence="0.9995096">
Let tG be the gold tree for w. Quantity L(c, tG)
can be used to define a dynamic oracle as follows.
For any transition `τ in the finite inventory of our
parser, we use the functional notation τ(c) = c&apos; in
place of c `τ c&apos;. We then let
</bodyText>
<equation confidence="0.996592">
oracle(c, tG) =
{τ  |L(τ(c), tG) − L(c, tG) = 0} . (3)
</equation>
<bodyText confidence="0.999908636363636">
In words, (3) provides the set of transitions that do
not increase the loss of c; we call these transitions
optimal for c.
A naive way of implementing (3) would be
to explicitly compute the set D(c) in (2), which
has exponential size. More interestingly, the im-
plementation of dynamic oracles proposed by the
above cited authors all run in polynomial time.
These oracles are all defined for projective pars-
ing. In this paper, we present a polynomial-time
oracle for a non-projective parser.
</bodyText>
<sectionHeader confidence="0.971822" genericHeader="method">
3 Non-Projective Dependency Parsing
</sectionHeader>
<bodyText confidence="0.999937698113207">
In this section we introduce a parser for non-
projective dependency grammars that is derived
from the transition-based parser originally presen-
ted by Attardi (2006), and was further investigated
by Kuhlmann and Nivre (2010) and Cohen et al.
(2011). Our definitions follow the framework in-
troduced in Section 2.1.
We start with some additional notation. Let t be
a dependency tree for w and let k be a node of t.
Consider the complete subtree t&apos; of t rooted at k,
that is, the subtree of t induced by k and all of the
descendants of k in t. The span of t&apos; is the sub-
sequence of tokens in w represented by the nodes
of t&apos;. Node k has gap-degree 0 if the span of t&apos;
forms a (contiguous) substring of w. A depend-
ency tree is called projective if all of its nodes
have gap-degree 0; a dependency tree which is not
projective is called non-projective.
Given w as input, the parser starts with the ini-
tial configuration ([], [0, ... , n], ∅), consisting of
an empty stack, a buffer with all the nodes repres-
enting the symbol occurrences in w, and an empty
set of constructed dependencies (arcs). The parser
stops when it reaches a final configuration of the
form ([0], [], A), consisting of a stack with only the
root node and of an empty buffer; in any such con-
figuration, set A always implicitly defines a valid
dependency tree (rooted in node 0).
The core of the parser consists of an invent-
ory of five transitions, defined in Figure 1. Each
transition is specified using the free variables σ,
β, A, i, j and k. As an example, the schema
(σ|i|j, β, A) `la (σ|j, β, A ∪ {j → i}) means that
if a configuration c matches the antecedent, then a
new configuration is obtained by instantiating the
variables in the consequent accordingly.
The transition `sh, called shift, reads a new
token from the input sentence by removing it from
the buffer and pushing it into the stack. Each
of the other transitions, collectively called reduce
transitions, has the effect of building a dependency
between two nodes in the stack, and then removing
the dependent node from the stack. The removal
of the dependent ensures that the output depend-
ency tree is built in a bottom-up order, collecting
all of the dependents of each node i before linking
i to its head.
The transition `la, called left-arc, creates a left-
ward arc where the topmost stack node is the
head and the second topmost node is the depend-
ent, and removes the latter from the stack. The
transition `ra, called right-arc, is defined sym-
metrically, so that the topmost stack node is at-
</bodyText>
<equation confidence="0.97859">
L(t1, t2) . (2)
</equation>
<page confidence="0.966054">
919
</page>
<figure confidence="0.963553">
stack length
6 6 6
c0 c1 ... cm
</figure>
<figureCaption confidence="0.9941265">
Figure 2: General form of the computations asso-
ciated with an item [h1, h2, h3].
</figureCaption>
<bodyText confidence="0.999958297297297">
tached as a dependent of the second topmost node.
The combination of the shift, left-arc and right-
arc transitions provides complete coverage of pro-
jective dependency trees, but no support for non-
projectivity, and corresponds to the so-called arc-
standard parser introduced by Nivre (2004).
Support for non-projective dependencies is
achieved by adding the transitions �la2 and �ra2,
which are variants of the left-arc and right-arc
transitions, respectively. These new transitions
create dependencies involving the first and the
third topmost nodes in the stack. The creation of
dependencies between non-adjacent stack nodes
might produce crossing arcs and is the key to the
construction of non-projective trees.
Recall that transitions are partial functions,
meaning that they might be undefined for some
configurations. Specifically, the shift transition is
only defined for configurations with a non-empty
buffer. Similarly, the left-arc and right-arc trans-
itions can only be applied if the length of the stack
is at least 2, while the transitions �la2 and �ra2 re-
quire at least 3 nodes in the stack.
Transitions �la2 and �ra2 were originally intro-
duced by Attardi (2006) together with other, more
complex transitions. The parser we define here
is therefore more restrictive than Attardi (2006),
meaning that it does not cover all the non-pro-
jective trees that can be processed by the ori-
ginal parser. However, the restricted parser has re-
cently attracted some research interest, as it covers
the vast majority of non-projective constructions
appearing in standard treebanks (Attardi, 2006;
Kuhlmann and Nivre, 2010), while keeping sim-
plicity and interesting properties like being com-
patible with polynomial-time dynamic program-
ming (Cohen et al., 2011).
</bodyText>
<sectionHeader confidence="0.876603" genericHeader="method">
4 Representation of Computations
</sectionHeader>
<bodyText confidence="0.997118666666667">
Our oracle algorithm exploits a dynamic program-
ming technique which, given an input string, com-
bines certain pieces of a computation of the parser
from Section 3 to obtain larger pieces. In order
to efficiently encode pieces of computations, we
borrow a representation proposed by Cohen et al.
(2011), which is introduced in this section.
Let w = a0 • • • an and Vw be specified as in
Section 2, and let w&apos; be some substring of w. (The
specification of w&apos; is not of our concern in this
section.) Let also h1, h2, h3 E Vw. We are inter-
ested in computations of the parser processing the
substring w&apos; and having the form c0, c1, ... , cm,
m &gt; 1, that satisfy both of the following condi-
tions, exemplified in Figure 2.
</bodyText>
<listItem confidence="0.99615725">
• For some sequence of nodes σ with |σ |&gt; 0,
the stack associated with c0 has the form σ|h1
and the stack associated with cm has the form
σ|h2|h3.
• For each intermediate configuration ci, 1 &lt;
i &lt; m − 1, the stack associated with ci has
the form σσi, where σi is a sequence of nodes
with |σi |&gt; 2.
</listItem>
<bodyText confidence="0.999824269230769">
An important property of the above definition
needs to be discussed here, which is at the heart of
the polynomial-time algorithm in the next section.
If in c0, c1, ... , cm we replace σ with a different
sequence σ&apos;, we obtain a valid computation for w&apos;
constructing exactly the same dependencies as the
original computation. To see this, let ci_1 rτi ci
for each i with 1 &lt; i &lt; m. Then rτ1 must be a
shift, otherwise |σ1 |&gt; 2 would be violated. Con-
sider now a transition rτi with 2 &lt; i &lt; m that
builds some dependency. From |σi |&gt; 2 we derive
|σi_1 |&gt; 3. We can easily check from Figure 1
that none of the nodes in σ can be involved in the
constructed dependency.
Intuitively, the above property asserts that the
sequence of transitions rτ1, �τ2, ... , rτm can be
applied to parse substring w&apos; independently of the
context σ. This suggests that we can group into
an equivalence class all the computations satisfy-
ing the conditions above, for different values of
σ. We indicate such class by means of the tuple
[h1, h2h3], called item. It is easy to see that each
item represents an exponential number of compu-
tations. In the next section we will show how we
can process items with the purpose of obtaining an
efficient computation for dynamic oracles.
</bodyText>
<figure confidence="0.987315222222222">
h 1
h1
minimum
stack length
at c 1 c
...
m
h3
h2
</figure>
<page confidence="0.947977">
920
</page>
<sectionHeader confidence="0.974527" genericHeader="method">
5 Dynamic Oracle Algorithm
</sectionHeader>
<bodyText confidence="0.9997478">
Our algorithm takes as input a gold tree tG for
string w and a parser configuration c = (Q, Q, A)
relative to w, specified as in Section 2. We assume
that tG can be parsed by the non-projective parser
of Section 3 starting from the initial configuration.
</bodyText>
<subsectionHeader confidence="0.991437">
5.1 Basic Idea
</subsectionHeader>
<bodyText confidence="0.999964096774193">
The algorithm consists of two separate stages, in-
formally discussed in what follows. In the first
stage we identify some tree fragments of tG that
can be constructed by the parser after reaching
configuration c, in a way that does not depend on
the content of Q. This means that these fragments
can be precomputed by looking only into Q. Fur-
thermore, since these fragments are subtrees of tG,
their computation has no effect on the overall loss
of a computation on w.
For each fragment t with the above properties,
we replace all the nodes in Q that are also nodes
of t with the root node of t itself. The result of the
first stage is therefore a new node sequence shorter
than Q, which we call the reduced buffer QR.
In the second stage of the algorithm we use a
variant of the tabular method developed by Co-
hen et al. (2011), which was originally designed
to simulate all computations of the parser in Sec-
tion 3 on an input string w. We run the above
method on the concatenation of the stack and the
reduced buffer, with some additional constraints
that restrict the search space in two respects. First,
we visit only those computations of the parser
that step through configuration c. Second, we
reach only those dependency trees that contain all
the tree fragments precomputed in the first stage.
We can show that such search space always con-
tains at least one dependency tree with the desired
loss, which we then retrieve performing a Viterbi
search.
</bodyText>
<subsectionHeader confidence="0.999766">
5.2 Preprocessing of the Buffer
</subsectionHeader>
<bodyText confidence="0.999407">
Let t be a complete subtree of tG, having root
node k in Q. Consider the following two condi-
tions, defined on t.
</bodyText>
<listItem confidence="0.961875">
• Bottom-up completeness: No arc i j in t
is such that i is a node in Q, i =� k, and j is a
node in Q.
• Zero gap-degree: The nodes of t that are in Q
form a (contiguous) substring of w.
</listItem>
<bodyText confidence="0.999494871794871">
We claim that if t satisfies the above conditions,
then we can safely reduce the nodes of t appearing
in Q, replacing them with node k. We only report
here an informal discussion of this claim, and omit
a formal proof.
As a first remark, recall that our parser imple-
ments a purely bottom-up strategy. This means
that after a tree has been constructed, all of its
nodes but the root are removed from the parser
configuration. Then the Bottom-up completeness
condition guarantees that if we remove from Q all
nodes of t but k, the nodes of t that are in Q can still
be processed in a way that does not affect the loss,
since their parent must be either k or a node that is
neither in Q nor in Q. Note that the nodes of t that
are neither in Q nor in Q are irrelevant to the pre-
computation of t from Q, since these nodes have
already been attached and are no longer available
to the parser.
As a second remark, the Zero gap-degree con-
dition guarantees that the span of t over the nodes
of Q is not interleaved by nodes that do not belong
to t. This is also an important requirement for the
precomputation of t from Q, since a tree fragment
having a discontinuous span over Q might not be
constructable independently of Q. More specific-
ally, parsing such fragment implies dealing with
the nodes in the discontinuities, and this might re-
quire transitions involving nodes from Q.
We can now use the sufficient condition above
to compute QR. We process Q from left to right.
For each node k, we can easily test the Bottom-up
completeness condition and the Zero gap-degree
condition for the complete subtree t of tG rooted
at k, and perform the reduction if both conditions
are satisfied. Note that in this process a node k
resulting from the reduction of t might in turn be
removed from Q if, at some later point, we reduce
a supertree of t.
</bodyText>
<subsectionHeader confidence="0.998651">
5.3 Computation of the Loss
</subsectionHeader>
<bodyText confidence="0.999110384615385">
We describe here our dynamic programming al-
gorithm for the computation of the loss of an in-
put configuration c. We start with some additional
notation. Let -y = QQR be the concatenation of Q
and QR, which we treat as a string of nodes. For
integers i with 0 &lt; i &lt; J-yJ − 1, we write -y[i] to
denote the (i + 1)-th node of -y. Let also E = JQJ.
Symbol E is used to mark the boundary between
the stack and the reduced buffer in -y, thus -y[i] with
i &lt; E is a node of Q, while -y[i] with i &gt; E is a node
of QR.
Algorithm 1 computes the loss of c by pro-
cessing the sequence -y in a way quite similar to the
</bodyText>
<page confidence="0.99026">
921
</page>
<bodyText confidence="0.99969068627451">
standard nested loop implementation of the CKY
parser for context-free grammars (Hopcroft et al.,
2006). The algorithm uses a two-dimensional ar-
ray T whose indexes range from 0 to |-y |= E +
|OR|, and only the cells T [i, j] with i &lt; j are
filled.
We view each T [i, j] as an association list
whose keys are items [h1, h2h3], defined in the
context of the substring -y[i] · · ·-y[j − 1] of -y; see
Section 4. The value stored at T [i, j]([h1, h2h3])
is the minimum loss contribution due to the com-
putations represented by [h1, h2h3]. For technical
reasons, we assume that our parser starts with a
symbol $ ∈6 Uw in the stack, denoting the bottom
of the stack.
We initialize the table by populating the cells
of the form T [i, i + 1] with information about
the trivial computations consisting of a single `sh
transition that shifts the node -y[i] into the stack.
These computations are known to have zero loss
contribution, because a `sh transition does not cre-
ate any arcs. In the case where the node -y[i] be-
longs to Q, i.e., i &lt; E, we assign loss contribution
0 to the entry T [i,i + 1]([-y[i − 1],-y[i − 1]-y[i]])
(line 3 of Algorithm 1), because -y[i] is shifted with
-y[i − 1] at the top of the stack. On the other hand,
if -y[i] is in O, i.e., i ≥ E, we assign loss contri-
bution 0 to several entries in T [i, i + 1] (line 6)
because, at the time -y[i] is shifted, the content of
the stack depends on the transitions executed be-
fore that point.
After the above initialization, we consider
pairs of contiguous substrings -y[i] · · · -y[k − 1] and
-y[k] · · · -y[j − 1] of -y. At each inner iteration
of the nested loops of lines 7-11 we update cell
T [i, j] based on the content of the cells T [i, k] and
T [k, j]. We do this through the procedure PRO-
CESSCELL(T, i, k, j), which considers all pairs
of keys [h1, h2h3] in T [i, k] and [h3, h4h5] in
T [k, j]. Note that we require the index h3 to match
between both items, meaning that their computa-
tions can be concatenated. In this way, for each
reduce transition T in our parser, we compute the
loss contribution for a new piece of computation
defined by concatenating a computation with min-
imum loss contribution in the first item and a com-
putation with minimum loss contribution in the
second item, followed by the transition T. The fact
that the new piece of computation can be repres-
ented by an item is exemplified in Figure 3 for the
case T = `ra2.
</bodyText>
<equation confidence="0.8619215">
h5
h1 h4
G G
c0 ... c +1
r
[h 1, h 4h 5]
</equation>
<figureCaption confidence="0.973915">
Figure 3: Concatenation of two computa-
tions/items and transition `ra2, resulting in a new
computation/item.
</figureCaption>
<bodyText confidence="0.807161857142857">
The computed loss contribution is used to up-
date the entry in T [i, j] corresponding to the item
associated with the new computation. Observe
how the loss contribution provided by the arc cre-
ated by T is computed by the SG function at lines
17, 20, 23 and 26, which is defined as:
� 0, if i → j is in tG;
</bodyText>
<equation confidence="0.9345835">
SG(i → j) =(4)
1, otherwise.
</equation>
<bodyText confidence="0.9998671">
We remark that the nature of our problem al-
lows us to apply several shortcuts and optimiza-
tions that would not be possible in a setting where
we actually needed to parse the string -y. First, the
range of variable i in the loop in line 8 starts at
max{0, E − d}, rather than at 0, because we do not
need to combine pairs of items originating from
nodes in Q below the topmost node, as the items
resulting from such combinations correspond to
computations that do not contain our input config-
uration c. Second, when we have set values for i
such that i+2 &lt; E, we can omit calling PROCESS-
CELL for values of the parameter k ranging from
i+2 to E−1, as those calls would use as their input
one of the items described above, which are not of
interest. Finally, when processing substrings that
are entirely in OR (i ≥ E) we can restrict the trans-
itions that we explore to those that generate arcs
that either are in the gold tree tG, or have a parent
node which is not present in -y (see conditions in
</bodyText>
<figure confidence="0.991628578947368">
G G G G G
... c c ...
m m
c0
cr
c +1
r
h5
h4
la : create arc
2
h5—h2 remove
h from stack
2
h5
h1 h2 h2 h2 h4
h3
h3
[h 1, h 2h 3] + [h 3, h 4h 5] + ˫la 2
</figure>
<page confidence="0.707535">
922
</page>
<bodyText confidence="0.324485">
Algorithm 1 Computation of the loss function
</bodyText>
<listItem confidence="0.930405214285714">
1: T [0, 1]([$, $0]) ← 0 &gt; shift node 0 on top of empty stack symbol $
2: for i ← 1 to f − 1 do
3: T [i, i + 1]([ry[i − 1], ry[i − 1]ry[i]]) ← 0 &gt; shift node ry[i] with ry[i − 1] on top of the stack
4: for i ← f to |ry |do
5: for h ← 0 to i − 1 do
6: T [i, i + 1]([ry[h], ry[h]ry[i]]) ← 0 &gt; shift node ry[i] with ry[h] on top of the stack
7: for d ← 2 to |ry |do &gt; consider substrings of length d
8: for i ← max{0, f − d} to |ry |− d do &gt; i = beginning of substring
9: j ← i + d &gt; j − 1 = end of substring
10: PROCESSCELL(T, i, i + 1, j) &gt; We omit the range k = i + 2 to max{i + 2, f} − 1
11: for k ← max{i + 2, f} to j do &gt; factorization of substring at k
12: PROCESSCELL(T, i, k, j)
13: return T [0, |ry|]([$, $0]) + Ei∈[0,e−1] Lc(a[i], tG)
14: procedure PROCESSCELL(T, i, k, j)
</listItem>
<equation confidence="0.920285785714286">
for each key [h1, h2h3]) defined in T [i, k] do
for each key [h3, h4h5]) defined in T [k, j] do &gt; h3 must match between the two entries
lossla ← T [i, k]([h1, h2h3]) +T[k,j]([h3, h4h5]) + 6G(h5 → h4)
if (i &lt; f) ∨ 6G(h5 → h4) = 0 ∨ (h5 ∈6 ry) then
T [i, j]([h1, h2h5]) ← min{lossla, T [i, j]([h1, h2h5])} &gt; cell update `la
lossra ← T [i, k]([h1, h2h3]) +T[k,j]([h3, h4h5]) + 6G(h4 → h5)
if (i &lt; f) ∨ 6G(h4 → h5) = 0 ∨ (h4 ∈6 ry) then
T [i, j]([h1, h2h4]) ← min{lossra, T [i, j]([h1, h2h4])} &gt; cell update `ra
lossla2 ← T [i, k]([h1, h2h3]) +T[k,j]([h3, h4h5]) + 6G(h5 → h2)
if (i &lt; f) ∨ 6G(h5 → h2) = 0 ∨ (h5 ∈6 ry) then
T [i, j]([h1, h4h5]) ← min{lossla2, T [i, j]([h1, h4h5])} &gt; cell update `la2
lossra2 ← T [i, k]([h1, h2h3]) +T[k,j]([h3, h4h5]) + 6G(h2 → h5)
if (i &lt; f) ∨ 6G(h2 → h5) = 0 ∨ (h2 ∈6 ry) then
T [i, j]([h1, h2h4]) ← min{lossra2, T [i, j]([h1, h2h4])} &gt; cell update `ra2
</equation>
<bodyText confidence="0.995193285714286">
lines 18, 21, 24, 27), because we know that incor-
rectly attaching a buffer node as a dependent of an-
other buffer node, when the correct head is avail-
able, can never be an optimal decision in terms of
loss.
Once we have filled the table T, the loss for
the input configuration c can be obtained from the
value of the entry T [0, |γ|]([$, $0]), representing
the minimum loss contribution among computa-
tions that reach the input configuration c and parse
the whole input string. To obtain the total loss,
we add to this value the loss contribution accu-
mulated by the dependency trees with root in the
stack σ of c. This is represented in Algorithm 1 as
</bodyText>
<equation confidence="0.654489">
E
</equation>
<bodyText confidence="0.99035375">
i∈[0,t−1] Lc(σ[i], tG), where Lc(σ[i], tG) is the
count of the descendants of σ[i] (the (i+1)-th ele-
ment of σ) that had been assigned the wrong head
by the parser with respect to tG.
</bodyText>
<subsectionHeader confidence="0.99642">
5.4 Sample Run
</subsectionHeader>
<bodyText confidence="0.999123741935484">
Consider the Czech sentence and the gold depend-
ency tree tG shown in Figure 4(a). Given the con-
figuration c = (σ, β, A) where σ = [0, 1, 3, 4],
β = [5, ... ,13] and A = {3 → 2}, we trace the
two stages of the algorithm.
Preprocessing of the buffer The complete sub-
tree rooted at node 7 satisfies the Bottom-up com-
pleteness and the Zero gap-degree conditions in
Section 5.2, so the nodes 5, ... ,12 in β can be
replaced with the root 7. Note that all the nodes in
the span 5,... ,12 have all their (gold) dependents
in that span, with the exception of the root 7, with
its dependent node 1 still in the stack. No other
reduction is possible, and we have βR = [7,13].
The corresponding fragment of tG is represented
in Figure 4(b).
Computation of the loss Let γ = σβR. Al-
gorithm 1 builds the two-dimensional array T in
Figure 4(c). Each cell T [i, j] contains an asso-
ciation list, whose (key:value) pairs map items to
their loss contribution. Figure 4(c) only shows the
pairs involved in the minimum-loss computation.
Lines 1-6 of Algorithm 1 initialize the cells in
the diagonal, T [0,1], ... , T [5,6]. The boundary
between stack and buffer is ` = 4, thus cells
T [0,1], T [1, 2], and T [2,3] contain only one ele-
ment, while T [3,4], T [4,5] and T [5,6] contain as
many as the previous elements in γ, although not
all of them are shown in the figure.
Lines 7-12 fill the superdiagonals until T [0, 6]
is reached. The cells T [0, 2], T [0, 3] and T [1, 3]
</bodyText>
<page confidence="0.995887">
923
</page>
<figure confidence="0.713210545454546">
-Root- V bˇeˇzn´em provozu vˇsak telefonn´ı linky nermaj´ı takivou kvalitu jako v laboratoˇri .
0 1 2 3 4 5 6 7 8 9 10 11 12 13
(a) Non-projective dependency tree from the Prague Dependency Treebank.
j i 1 2 3 4 5 [$,$ 6
0]:1
0 [$,$ 0]:0 0 0 ... [$,$ 0]:1
1 [0,0 1]:0 0 ... [0,0 4]:1 ...
2 [1,1 3]:0 [1,1 4]:1 [1,4 7]:1 ...
3 [3,3 4]:0 [3,4 7]:1 ...
4 [4,4 7]:0 ...
5 [0,0 13]:0
</figure>
<figureCaption confidence="0.7290225">
(c) Relevant portion of T computed by Algorithm 1, with the
loss of c in the yellow entry.
Figure 4: Example of loss computation given the sentence in (a) and considering a configuration c with
σ = [0, 1, 3, 4] and β = [5, ... ,13].
</figureCaption>
<figure confidence="0.9877655">
βR
(b) Fragment of dependency tree in (a) after buffer
reduction.
-Root- V provozu vˇsak nermaj´ı .
0 1 3 4 7 13
σ
</figure>
<bodyText confidence="0.999852764705882">
are left empty because ` = 4. Once T [0, 6]
is calculated, it contains only the entry with key
[$, $, 0], with the associated value 1 representing
the minimum number of wrong arcs that the pars-
ing algorithm has to build to reach a final con-
figuration from c. Then, Line 13 retrieves the
loss of the configuration, computed as the sum of
T [0, 6]([$, $, 0]) with the term Lc, representing the
erroneous arcs made before reaching c.
Note that in our example the loss of c is 1, even
though Lc = 0, meaning that there are no wrong
arcs in A. Indeed, given c, there is no single com-
putation that builds all the remaining arcs in tG.
This is reflected in T, where the path to reach the
item with minimum loss has to go through either
T [3,5] or T [2,4], which implies building the erro-
neous arc (w7 —* w3) or (w4 —* w3), respectively.
</bodyText>
<sectionHeader confidence="0.939733" genericHeader="method">
6 Computational Analysis
</sectionHeader>
<bodyText confidence="0.999583755555556">
The first stage of our algorithm can be easily im-
plemented in time O(|β ||tG|), where |tG |is the
number of nodes in tG, which is equal to the length
n of the input string.
For the worst-case complexity of the second
stage (Algorithm 1), note that the number
of cell updates made by calling PROCESS-
CELL(T, i, k, j) with k &lt; ` is O(|σ|3 |γ|2 |βR|).
This is because these updates can only be caused
by procedure calls on line 10 (as those on line 12
always set k &gt; `) and therefore the index k always
equals i + 1, while h2 must equal h1 because the
item [h1, h2h3] is one of the initial items created
on line 3. The variables i, h1 and h3 must index
nodes on the stack σ as they are bounded by k,
while j ranges over βR and h4 and h5 can refer to
nodes either on σ or on βR.
On the other hand, the number of cell updates
triggered by calls to PROCESSCELL such that k &gt;
` is O(|γ|4|βR|4), as they happen for four indices
referring to nodes of βR (k, j, h4, h5) and four
indices that can range over σ or βR (i, h1, h2, h3).
Putting everything together, we conclude that
the overall complexity of our algorithm is
O(|β ||tG |+ |σ|3 |γ|2 |βR |+ |γ|4 |βR|4).
In practice, quantities |σ|, |βR |and |γ |are signi-
ficantly smaller than n, providing reasonable train-
ing times as we will see in Section 7. For instance,
when measured on the Czech treebank, the aver-
age value of |σ |is 7.2, with a maximum of 87.
Even more interesting, the average value of |βR|
is 2.6, with a maximum of 23. Comparing this to
the average and maximum values of |β|, 11 and
192, respectively, we see that the buffer reduction
is crucial in reducing training time.
Note that, when expressed as a function of n,
our dynamic oracle has a worst-case time com-
plexity of O(n8). This is also the time complexity
of the dynamic programming algorithm of Cohen
et al. (2011) we started with, simulating all com-
putations of our parser. In contrast, the dynamic
oracle of Goldberg et al. (2014) for the projective
case achieves a time complexity of O(n3) from the
dynamic programming parser by Kuhlmann et al.
(2011) running in time O(n5).
</bodyText>
<page confidence="0.995698">
924
</page>
<bodyText confidence="0.999917">
The reason why we do not achieve any asymp-
totic improvement is that some helpful properties
that hold with projective trees are no longer satis-
fied in the non-projective case. In the projective
(arc-standard) case, subtrees that are in the buf-
fer can be completely reduced. As a consequence,
each oracle step always combines an inferred entry
in the table with either a node from the stack or a
node from the reduced buffer, asymptotically re-
ducing the time complexity. However, in the non-
projective (Attardi) case, subtrees in the buffer can
not always be completely reduced, for the reasons
mentioned in the second-to-last paragraph of Sec-
tion 5.2. As a consequence, the oracle needs to
make cell updates in a more general way, which
includes linking pairs of elements in the reduced
buffer or pairs of inferred entries in the table.
</bodyText>
<figure confidence="0.954952">
-Root- John was not as good for the job as Kate .
0 1 2 3 4 5 6 7 8 9 10 11
</figure>
<figureCaption confidence="0.996563">
Figure 5: Non-projective dependency tree adapted
from the Penn Treebank.
</figureCaption>
<bodyText confidence="0.999967526315789">
An example of why this is needed is provided
by the gold tree in Figure 5. Assume a config-
uration c = (σ, β, A) where σ = [0, 1, 2, 3, 4],
β = [5, ... ,11], and A = 0. It is easy to see that
the loss of c is greater than zero, since the gold tree
is not reachable from c: parsing the subtree rooted
at node 5 requires shifting 6 into the stack, and
this makes it impossible to build the arcs 2 -* 5
and 2 -* 6. However, if we reduced the subtree in
the buffer with root 5, we would incorrectly obtain
a loss of 0, as the resulting tree is parsable if we
start with Leh followed by ila and �ra2. Note that
there is no way of knowing whether it is safe to
reduce the subtree rooted at 5 without using non-
local information. For example, the arc 2 -* 6 is
crucial here: if 6 depended on 5 or 4 instead, the
loss would be zero. These complications are not
found in the projective case, allowing for the men-
tioned asymptotic improvement.
</bodyText>
<sectionHeader confidence="0.998215" genericHeader="evaluation">
7 Experimental Evaluation
</sectionHeader>
<bodyText confidence="0.999413666666667">
For comparability with previous work on dynamic
oracles, we follow the experimental settings repor-
ted by Goldberg et al. (2014) for their arc-standard
dynamic oracle. In particular, we use the same
training algorithm, features, and root node posi-
tion. However, we train the model for 20 itera-
</bodyText>
<table confidence="0.999648">
static dynamic
UAS LAS UAS LAS
Arabic 80.90 71.56 82.23 72.63
Basque 75.96 66.74 74.32 65.59
Catalan 90.55 85.20 89.94 84.96
Chinese 84.72 79.93 85.34 81.00
Czech 79.83 72.69 82.08 74.44
English 85.52 84.46 87.38 86.40
Greek 79.84 72.26 81.55 74.14
Hungarian 78.13 68.90 76.27 68.14
Italian 83.08 78.94 84.43 80.45
Turkish 79.57 69.44 79.41 70.32
Bulgarian 89.46 85.99 89.32 85.92
Danish 85.58 81.25 86.03 81.59
Dutch 79.05 75.69 80.13 77.22
German 88.34 86.48 88.86 86.94
Japanese 93.06 91.64 93.56 92.18
Portuguese 84.80 81.38 85.36 82.10
Slovene 76.33 68.43 78.20 70.22
Spanish 79.88 76.84 80.25 77.45
Swedish 87.26 82.77 87.24 82.49
PTB 89.55 87.18 90.47 88.18
</table>
<tableCaption confidence="0.998549">
Table 1: Unlabelled Attachment Score (UAS) and
</tableCaption>
<bodyText confidence="0.941993">
Labelled Attachment Score (LAS) using a static
and a dynamic oracle. Evaluation on CoNLL 2007
(first block) and CoNLL 2006 (second block) data-
sets is carried out including punctuation, evalu-
ation on the Penn Treebank excludes it.
tions rather than 15, as the increased search space
and spurious ambiguity of Attardi’s non-project-
ive parser implies that more iterations are required
to converge to a stable model. A more detailed
description of the experimental settings follows.
</bodyText>
<subsectionHeader confidence="0.979276">
7.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.999968285714286">
Training We train a global linear model using
the averaged perceptron algorithm and a labelled
version of the parser described in Section 3. We
perform on-line training using the oracle defined
in Section 5: at each parsing step, the model’s
weights are updated if the predicted transition res-
ults into an increase in configuration loss, but
the process continues by following the predicted
transition independently of the loss increase.
As our baseline we train the model using the
static oracle defined by (Cohen et al., 2012). This
oracle follows a canonical computation that cre-
ates arcs as soon as possible, and prioritizes the
ila transition over the �la2 transition in situations
</bodyText>
<page confidence="0.994771">
925
</page>
<bodyText confidence="0.999994869565218">
where both create a gold arc. The static oracle
is not able to deal with configurations that can-
not reach the gold dependency tree, so we con-
strain the training algorithm to follow the zero-loss
transition provided by the oracle.
While this version of Attardi’s parser has been
shown to cover the vast majority of non-projective
sentences in several treebanks (Attardi, 2006; Co-
hen et al., 2012), there still are some sentences
which are not parsable. These sentences are
skipped during training, but not during test and
evaluation of the model.
Datasets We evaluate the parser performance
over CoNLL 2006 and CoNLL 2007 datasets.
If a language is present in both datasets, we
use the latest version. We also include res-
ults over the Penn Treebank (PTB) (Marcus et
al., 1993) converted to Stanford basic dependen-
cies (De Marneffe et al., 2006). For the CoNLL
datasets we use the provided part-of-speech tags
and the standard training/test partition; for the
PTB we use automatically assigned tags, we train
on sections 2-21 and test on section 23.
</bodyText>
<subsectionHeader confidence="0.807373">
7.2 Results and Analysis
</subsectionHeader>
<bodyText confidence="0.999992264150944">
In Table 1 we report the unlabelled (UAS) and la-
belled (LAS) attachment scores for the static and
the dynamic oracles. Each figure is an average
over the accuracy provided by 5 models trained
with the same setup but using a different random
seed. The seed is only used to shuffle the sentences
in random order during each iteration of training.
Our results are consistent with the results re-
ported by Goldberg and Nivre (2013) and Gold-
berg et al. (2014). For most of the datasets, we
obtain a relevant improvement in both UAS and
LAS. For Dutch, Czech and German, we achieve
an error reduction of 5.2%, 11.2% and 4.5%, re-
spectively. Exceptions to this general trend are
Swedish and Bulgarian, where the accuracy differ-
ences are negligible, and the Basque, Catalan and
Hungarian datasets, where the performance actu-
ally decreases.
If instead of testing on the standard test sets we
use 10-fold cross-validation and average the res-
ulting accuracies, we obtain improvements for all
languages in Table 1 but Basque and Hungarian.
More specifically, measured (UAS, LAS) pairs for
Swedish are (86.85, 82.17) with dynamic oracle
against (86.6, 81.93) with static oracle; for Bul-
garian (88.42, 83.91) against (88.20, 83.55); and
for Catalan (88.33, 83.64) against (88.06, 83.13).
This suggests that the negligible or unfavourable
results in Table 1 for these languages are due to
statistical variability given the small size of the test
sets.
As for Basque, we measure (75.54, 67.58)
against (76.77, 68.20); similarly, for Hungarian
we measure (75.66, 67.66) against (77.22, 68.42).
Unfortunately, we have no explanation for these
performance decreases, in terms of the typology
of the non-projective patterns found in these two
datasets. Note that Goldberg et al. (2014) also
observed a performance decrease on the Basque
dataset in the projective case, although not on
Hungarian.
The parsing times measured in our experiments
for the static and the dynamic oracles are the same,
since the oracle algorithm is only used during the
training stage. Thus the reported improvements in
parsing accuracy come at no extra cost for parsing
time. In the training stage, the extra processing
needed to compute the loss and to explore paths
that do not lead to a gold tree made training about
4 times slower, on average, for the dynamic oracle
model. This confirms that our oracle algorithm is
fast enough to be of practical interest, in spite of its
relatively high worst-case asymptotic complexity.
</bodyText>
<sectionHeader confidence="0.99934" genericHeader="conclusions">
8 Conclusions
</sectionHeader>
<bodyText confidence="0.999966333333333">
We have presented what, to our knowledge, are
the first experimental results for a transition-based
non-projective parser trained with a dynamic or-
acle. We have also shown significant accuracy im-
provements on many languages over a static oracle
baseline.
The general picture that emerges from our ap-
proach is that dynamic programming algorithms
originally conceived for the simulation of trans-
ition-based parsers can effectively be used in the
development of polynomial-time algorithms for
dynamic oracles.
</bodyText>
<sectionHeader confidence="0.998822" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.998908">
The first author has been partially funded by Min-
isterio de Economia y Competitividad/FEDER
(Grant TIN2010-18552-C03-02) and by Xunta de
Galicia (Grant CN2012/008). The third author has
been partially supported by MIUR under project
PRIN No. 2010LYA9RH 006.
</bodyText>
<page confidence="0.997568">
926
</page>
<sectionHeader confidence="0.993866" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999735976744186">
Giuseppe Attardi. 2006. Experiments with a multil-
anguage non-projective dependency parser. In Pro-
ceedings of the Tenth Conference on Computational
Natural Language Learning (CoNLL), pages 166–
170, New York, USA.
Jinho D. Choi and Andrew McCallum. 2013.
Transition-based dependency parsing with selec-
tional branching. In Proceedings of the 51st An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 1052–
1062, Sofia, Bulgaria, August. Association for Com-
putational Linguistics.
Shay B. Cohen, Carlos G´omez-Rodr´ıguez, and Giorgio
Satta. 2011. Exact inference for generative prob-
abilistic non-projective dependency parsing. In Pro-
ceedings of the 2011 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1234–
1245, Edinburgh, Scotland, UK., July. Association
for Computational Linguistics.
Shay B. Cohen, Carlos G´omez-Rodr´ıguez, and Gior-
gio Satta. 2012. Elimination of spurious ambigu-
ity in transition-based dependency parsing. CoRR,
abs/1206.6735.
Marie-Catherine De Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of the 5th International Conference
on Language Resources and Evaluation (LREC),
volume 6, pages 449–454.
Yoav Goldberg and Joakim Nivre. 2012. A dynamic
oracle for arc-eager dependency parsing. In Proc. of
the 24th COLING, Mumbai, India.
Yoav Goldberg and Joakim Nivre. 2013. Training
deterministic parsers with non-deterministic oracles.
Transactions of the association for Computational
Linguistics, 1.
Yoav Goldberg, Francesco Sartorio, and Giorgio Satta.
2014. A tabular method for dynamic oracles in
transition-based parsing. Transactions of the Associ-
ation for Computational Linguistics, 2(April):119–
130.
John E. Hopcroft, Rajeev Motwani, and Jeffrey D. Ull-
man. 2006. Introduction to Automata Theory, Lan-
guages, and Computation (3rd Edition). Addison-
Wesley Longman Publishing Co., Inc., Boston, MA,
USA.
Liang Huang and Kenji Sagae. 2010. Dynamic pro-
gramming for linear-time incremental parsing. In
Proceedings of the 48th Annual Meeting of the As-
sociation for Computational Linguistics, July.
Marco Kuhlmann and Joakim Nivre. 2010. Transition-
based techniques for non-projective dependency
parsing. Northern European Journal of Language
Technology, 2(1):1–19.
Marco Kuhlmann, Carlos G´omez-Rodr´ıguez, and Gior-
gio Satta. 2011. Dynamic programming algorithms
for transition-based dependency parsers. In Pro-
ceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 673–682, Portland, Ore-
gon, USA, June. Association for Computational Lin-
guistics.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19(2):313–330.
Joakim Nivre. 2003. An efficient algorithm for pro-
jective dependency parsing. In Proceedings of the
Eighth International Workshop on Parsing Techno-
logies (IWPT), pages 149–160, Nancy, France.
Joakim Nivre. 2004. Incrementality in deterministic
dependency parsing. In Workshop on Incremental
Parsing: Bringing Engineering and Cognition To-
gether, pages 50–57, Barcelona, Spain.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Stat-
istical dependency analysis with support vector ma-
chines. In Proceedings of the 8th International
Workshop on Parsing Technologies (IWPT), pages
195–206.
Yue Zhang and Joakim Nivre. 2011. Transition-based
dependency parsing with rich non-local features. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 188–193, Portland, Ore-
gon, USA, June. Association for Computational Lin-
guistics.
</reference>
<page confidence="0.997128">
927
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.058617">
<title confidence="0.540625166666667">A Polynomial-Time Dynamic for Non-Projective Dependency Parsing Carlos Departamento da cgomezr@udc.es</title>
<author confidence="0.977823">Francesco</author>
<affiliation confidence="0.996052666666667">Department Information University of Padua,</affiliation>
<email confidence="0.991332">sartorio@dei.unipd.it</email>
<author confidence="0.999751">Giorgio Satta</author>
<affiliation confidence="0.995268">Department of Information Engineering</affiliation>
<address confidence="0.744298">University of Padua, Italy</address>
<email confidence="0.998545">satta@dei.unipd.it</email>
<abstract confidence="0.59585"></abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Giuseppe Attardi</author>
</authors>
<title>Experiments with a multilanguage non-projective dependency parser.</title>
<date>2006</date>
<booktitle>In Proceedings of the Tenth Conference on Computational Natural Language Learning (CoNLL),</booktitle>
<pages>166--170</pages>
<location>New York, USA.</location>
<contexts>
<context position="3369" citStr="Attardi (2006)" startWordPosition="488" endWordPosition="489">figuration that is reachable by the parser. Naive implementations of dynamic oracles run in exponential time, since they need to simulate all possible computations of the parser for the input configuration. Polynomial-time implementations of dynamic oracles have been proposed by the above mentioned authors for several projective dependency parsers. To our knowledge, no polynomial-time algorithm has been published for transition-based parsers based on non-projective dependency grammars. In this paper we consider a restriction of a transition-based, non-projective parser originally presented by Attardi (2006). This restriction was further investigated by Kuhlmann and Nivre (2010) and Cohen et al. (2011). We provide an implementation for a dynamic oracle for this parser running in polynomial time. We experimentally compare the parser trained with the dynamic oracle to a baseline obtained by training with a static oracle. Significant accuracy improvements are achieved on many languages when using our dynamic oracle. To our knowledge, these are the first experimental results on non-projective parsing based on a dynamic oracle. 917 Proceedings of the 2014 Conference on Empirical Methods in Natural Lan</context>
<context position="10760" citStr="Attardi (2006)" startWordPosition="1828" endWordPosition="1829">of c; we call these transitions optimal for c. A naive way of implementing (3) would be to explicitly compute the set D(c) in (2), which has exponential size. More interestingly, the implementation of dynamic oracles proposed by the above cited authors all run in polynomial time. These oracles are all defined for projective parsing. In this paper, we present a polynomial-time oracle for a non-projective parser. 3 Non-Projective Dependency Parsing In this section we introduce a parser for nonprojective dependency grammars that is derived from the transition-based parser originally presented by Attardi (2006), and was further investigated by Kuhlmann and Nivre (2010) and Cohen et al. (2011). Our definitions follow the framework introduced in Section 2.1. We start with some additional notation. Let t be a dependency tree for w and let k be a node of t. Consider the complete subtree t&apos; of t rooted at k, that is, the subtree of t induced by k and all of the descendants of k in t. The span of t&apos; is the subsequence of tokens in w represented by the nodes of t&apos;. Node k has gap-degree 0 if the span of t&apos; forms a (contiguous) substring of w. A dependency tree is called projective if all of its nodes have </context>
<context position="14429" citStr="Attardi (2006)" startWordPosition="2473" endWordPosition="2474">k. The creation of dependencies between non-adjacent stack nodes might produce crossing arcs and is the key to the construction of non-projective trees. Recall that transitions are partial functions, meaning that they might be undefined for some configurations. Specifically, the shift transition is only defined for configurations with a non-empty buffer. Similarly, the left-arc and right-arc transitions can only be applied if the length of the stack is at least 2, while the transitions �la2 and �ra2 require at least 3 nodes in the stack. Transitions �la2 and �ra2 were originally introduced by Attardi (2006) together with other, more complex transitions. The parser we define here is therefore more restrictive than Attardi (2006), meaning that it does not cover all the non-projective trees that can be processed by the original parser. However, the restricted parser has recently attracted some research interest, as it covers the vast majority of non-projective constructions appearing in standard treebanks (Attardi, 2006; Kuhlmann and Nivre, 2010), while keeping simplicity and interesting properties like being compatible with polynomial-time dynamic programming (Cohen et al., 2011). 4 Representation</context>
<context position="38182" citStr="Attardi, 2006" startWordPosition="7049" endWordPosition="7050">e we train the model using the static oracle defined by (Cohen et al., 2012). This oracle follows a canonical computation that creates arcs as soon as possible, and prioritizes the ila transition over the �la2 transition in situations 925 where both create a gold arc. The static oracle is not able to deal with configurations that cannot reach the gold dependency tree, so we constrain the training algorithm to follow the zero-loss transition provided by the oracle. While this version of Attardi’s parser has been shown to cover the vast majority of non-projective sentences in several treebanks (Attardi, 2006; Cohen et al., 2012), there still are some sentences which are not parsable. These sentences are skipped during training, but not during test and evaluation of the model. Datasets We evaluate the parser performance over CoNLL 2006 and CoNLL 2007 datasets. If a language is present in both datasets, we use the latest version. We also include results over the Penn Treebank (PTB) (Marcus et al., 1993) converted to Stanford basic dependencies (De Marneffe et al., 2006). For the CoNLL datasets we use the provided part-of-speech tags and the standard training/test partition; for the PTB we use autom</context>
</contexts>
<marker>Attardi, 2006</marker>
<rawString>Giuseppe Attardi. 2006. Experiments with a multilanguage non-projective dependency parser. In Proceedings of the Tenth Conference on Computational Natural Language Learning (CoNLL), pages 166– 170, New York, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jinho D Choi</author>
<author>Andrew McCallum</author>
</authors>
<title>Transition-based dependency parsing with selectional branching.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>1052--1062</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="1664" citStr="Choi and McCallum (2013)" startWordPosition="228" endWordPosition="231">umoto (2003) and Nivre (2003). These methods incrementally process the input sentence from left to right, predicting the next parsing action, called transition, on the basis of a compact representation of the derivation history. Greedy transition-based parsers can be very efficient, allowing web-scale parsing with high throughput. However, the accuracy of these methods still falls behind that of transition-based parsers using beam-search, where the accuracy improvement is obtained at the cost of a decrease in parsing efficiency; see for instance Zhang and Nivre (2011), Huang and Sagae (2010), Choi and McCallum (2013). As an alternative to beamsearch, recent research on transition-based parsing has therefore explored possible ways of improving accuracy at no extra cost in parsing efficiency. The training of transition-based parsers relies on a component called the parsing oracle, which maps parser configurations to optimal transitions with respect to a gold tree. A discriminative model is then trained to simulate the oracle’s behavior, and is later used for decoding. Traditionally, socalled static oracles have been exploited in training, where a static oracle is defined only for configurations that have be</context>
</contexts>
<marker>Choi, McCallum, 2013</marker>
<rawString>Jinho D. Choi and Andrew McCallum. 2013. Transition-based dependency parsing with selectional branching. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1052– 1062, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shay B Cohen</author>
<author>Carlos G´omez-Rodr´ıguez</author>
<author>Giorgio Satta</author>
</authors>
<title>Exact inference for generative probabilistic non-projective dependency parsing.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1234--1245</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Edinburgh, Scotland, UK.,</location>
<marker>Cohen, G´omez-Rodr´ıguez, Satta, 2011</marker>
<rawString>Shay B. Cohen, Carlos G´omez-Rodr´ıguez, and Giorgio Satta. 2011. Exact inference for generative probabilistic non-projective dependency parsing. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1234– 1245, Edinburgh, Scotland, UK., July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shay B Cohen</author>
<author>Carlos G´omez-Rodr´ıguez</author>
<author>Giorgio Satta</author>
</authors>
<title>Elimination of spurious ambiguity in transition-based dependency parsing.</title>
<date>2012</date>
<location>CoRR, abs/1206.6735.</location>
<marker>Cohen, G´omez-Rodr´ıguez, Satta, 2012</marker>
<rawString>Shay B. Cohen, Carlos G´omez-Rodr´ıguez, and Giorgio Satta. 2012. Elimination of spurious ambiguity in transition-based dependency parsing. CoRR, abs/1206.6735.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine De Marneffe</author>
<author>Bill MacCartney</author>
<author>Christopher D Manning</author>
</authors>
<title>Generating typed dependency parses from phrase structure parses.</title>
<date>2006</date>
<booktitle>In Proceedings of the 5th International Conference on Language Resources and Evaluation (LREC),</booktitle>
<volume>6</volume>
<pages>449--454</pages>
<marker>De Marneffe, MacCartney, Manning, 2006</marker>
<rawString>Marie-Catherine De Marneffe, Bill MacCartney, and Christopher D. Manning. 2006. Generating typed dependency parses from phrase structure parses. In Proceedings of the 5th International Conference on Language Resources and Evaluation (LREC), volume 6, pages 449–454.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Goldberg</author>
<author>Joakim Nivre</author>
</authors>
<title>A dynamic oracle for arc-eager dependency parsing.</title>
<date>2012</date>
<booktitle>In Proc. of the 24th COLING,</booktitle>
<location>Mumbai, India.</location>
<contexts>
<context position="2423" citStr="Goldberg and Nivre (2012)" startWordPosition="347" endWordPosition="350">acy at no extra cost in parsing efficiency. The training of transition-based parsers relies on a component called the parsing oracle, which maps parser configurations to optimal transitions with respect to a gold tree. A discriminative model is then trained to simulate the oracle’s behavior, and is later used for decoding. Traditionally, socalled static oracles have been exploited in training, where a static oracle is defined only for configurations that have been reached by computations with no mistake, and it returns a single canonical transition among those that are optimal. Very recently, Goldberg and Nivre (2012), Goldberg and Nivre (2013) and Goldberg et al. (2014) showed that the accuracy of transitionbased parsers can be substantially improved using dynamic oracles. A dynamic oracle returns the set of all transitions that are optimal for a given configuration, with respect to the gold tree, and is well-defined and correct for every configuration that is reachable by the parser. Naive implementations of dynamic oracles run in exponential time, since they need to simulate all possible computations of the parser for the input configuration. Polynomial-time implementations of dynamic oracles have been </context>
<context position="8350" citStr="Goldberg and Nivre (2012)" startWordPosition="1382" endWordPosition="1385">ays chooses a single transition. In other words, the model is used to run the parser as a pseudo-deterministic device. The training of the discriminative model relies on a component called the parsing oracle, which maps parser configurations to “optimal” transitions with respect to some reference dependency tree, which we call the gold tree. Traditionally, so-called static oracles have been used which return a single, canonical transition and they do so only for configurations that can reach the gold tree, that is, configurations representing parsing histories with no mistake. In recent work, Goldberg and Nivre (2012), Goldberg and Nivre (2013) and Goldberg et al. (2014) have introduced dynamic oracles, which return the set of all transitions that are optimal with respect to a gold tree, and are well-defined and correct for every configuration that is reachable by the parser. These authors have shown that the accuracy of transition-based dependency parsers can be substantially improved if dynamic oracles are used in place of static ones. In what follows, we provide a mathematical definition of dynamic oracles, following Goldberg et al. (2014). 918 (σ, k|β, A) `sh (σ|k, β, A) (σ|i|j,β,A) `la (σ|j,β,A ∪ {j →</context>
</contexts>
<marker>Goldberg, Nivre, 2012</marker>
<rawString>Yoav Goldberg and Joakim Nivre. 2012. A dynamic oracle for arc-eager dependency parsing. In Proc. of the 24th COLING, Mumbai, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Goldberg</author>
<author>Joakim Nivre</author>
</authors>
<title>Training deterministic parsers with non-deterministic oracles.</title>
<date>2013</date>
<journal>Transactions of the association for Computational Linguistics,</journal>
<volume>1</volume>
<contexts>
<context position="2450" citStr="Goldberg and Nivre (2013)" startWordPosition="351" endWordPosition="354">sing efficiency. The training of transition-based parsers relies on a component called the parsing oracle, which maps parser configurations to optimal transitions with respect to a gold tree. A discriminative model is then trained to simulate the oracle’s behavior, and is later used for decoding. Traditionally, socalled static oracles have been exploited in training, where a static oracle is defined only for configurations that have been reached by computations with no mistake, and it returns a single canonical transition among those that are optimal. Very recently, Goldberg and Nivre (2012), Goldberg and Nivre (2013) and Goldberg et al. (2014) showed that the accuracy of transitionbased parsers can be substantially improved using dynamic oracles. A dynamic oracle returns the set of all transitions that are optimal for a given configuration, with respect to the gold tree, and is well-defined and correct for every configuration that is reachable by the parser. Naive implementations of dynamic oracles run in exponential time, since they need to simulate all possible computations of the parser for the input configuration. Polynomial-time implementations of dynamic oracles have been proposed by the above menti</context>
<context position="8377" citStr="Goldberg and Nivre (2013)" startWordPosition="1386" endWordPosition="1389">tion. In other words, the model is used to run the parser as a pseudo-deterministic device. The training of the discriminative model relies on a component called the parsing oracle, which maps parser configurations to “optimal” transitions with respect to some reference dependency tree, which we call the gold tree. Traditionally, so-called static oracles have been used which return a single, canonical transition and they do so only for configurations that can reach the gold tree, that is, configurations representing parsing histories with no mistake. In recent work, Goldberg and Nivre (2012), Goldberg and Nivre (2013) and Goldberg et al. (2014) have introduced dynamic oracles, which return the set of all transitions that are optimal with respect to a gold tree, and are well-defined and correct for every configuration that is reachable by the parser. These authors have shown that the accuracy of transition-based dependency parsers can be substantially improved if dynamic oracles are used in place of static ones. In what follows, we provide a mathematical definition of dynamic oracles, following Goldberg et al. (2014). 918 (σ, k|β, A) `sh (σ|k, β, A) (σ|i|j,β,A) `la (σ|j,β,A ∪ {j → i}) (σ|i|j,β,A) `ra (σ|i,β</context>
<context position="39307" citStr="Goldberg and Nivre (2013)" startWordPosition="7239" endWordPosition="7242"> the provided part-of-speech tags and the standard training/test partition; for the PTB we use automatically assigned tags, we train on sections 2-21 and test on section 23. 7.2 Results and Analysis In Table 1 we report the unlabelled (UAS) and labelled (LAS) attachment scores for the static and the dynamic oracles. Each figure is an average over the accuracy provided by 5 models trained with the same setup but using a different random seed. The seed is only used to shuffle the sentences in random order during each iteration of training. Our results are consistent with the results reported by Goldberg and Nivre (2013) and Goldberg et al. (2014). For most of the datasets, we obtain a relevant improvement in both UAS and LAS. For Dutch, Czech and German, we achieve an error reduction of 5.2%, 11.2% and 4.5%, respectively. Exceptions to this general trend are Swedish and Bulgarian, where the accuracy differences are negligible, and the Basque, Catalan and Hungarian datasets, where the performance actually decreases. If instead of testing on the standard test sets we use 10-fold cross-validation and average the resulting accuracies, we obtain improvements for all languages in Table 1 but Basque and Hungarian. </context>
</contexts>
<marker>Goldberg, Nivre, 2013</marker>
<rawString>Yoav Goldberg and Joakim Nivre. 2013. Training deterministic parsers with non-deterministic oracles. Transactions of the association for Computational Linguistics, 1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Goldberg</author>
<author>Francesco Sartorio</author>
<author>Giorgio Satta</author>
</authors>
<title>A tabular method for dynamic oracles in transition-based parsing.</title>
<date>2014</date>
<journal>Transactions of the Association for Computational Linguistics,</journal>
<volume>2</volume>
<pages>130</pages>
<contexts>
<context position="2477" citStr="Goldberg et al. (2014)" startWordPosition="356" endWordPosition="359">of transition-based parsers relies on a component called the parsing oracle, which maps parser configurations to optimal transitions with respect to a gold tree. A discriminative model is then trained to simulate the oracle’s behavior, and is later used for decoding. Traditionally, socalled static oracles have been exploited in training, where a static oracle is defined only for configurations that have been reached by computations with no mistake, and it returns a single canonical transition among those that are optimal. Very recently, Goldberg and Nivre (2012), Goldberg and Nivre (2013) and Goldberg et al. (2014) showed that the accuracy of transitionbased parsers can be substantially improved using dynamic oracles. A dynamic oracle returns the set of all transitions that are optimal for a given configuration, with respect to the gold tree, and is well-defined and correct for every configuration that is reachable by the parser. Naive implementations of dynamic oracles run in exponential time, since they need to simulate all possible computations of the parser for the input configuration. Polynomial-time implementations of dynamic oracles have been proposed by the above mentioned authors for several pr</context>
<context position="8404" citStr="Goldberg et al. (2014)" startWordPosition="1391" endWordPosition="1394">l is used to run the parser as a pseudo-deterministic device. The training of the discriminative model relies on a component called the parsing oracle, which maps parser configurations to “optimal” transitions with respect to some reference dependency tree, which we call the gold tree. Traditionally, so-called static oracles have been used which return a single, canonical transition and they do so only for configurations that can reach the gold tree, that is, configurations representing parsing histories with no mistake. In recent work, Goldberg and Nivre (2012), Goldberg and Nivre (2013) and Goldberg et al. (2014) have introduced dynamic oracles, which return the set of all transitions that are optimal with respect to a gold tree, and are well-defined and correct for every configuration that is reachable by the parser. These authors have shown that the accuracy of transition-based dependency parsers can be substantially improved if dynamic oracles are used in place of static ones. In what follows, we provide a mathematical definition of dynamic oracles, following Goldberg et al. (2014). 918 (σ, k|β, A) `sh (σ|k, β, A) (σ|i|j,β,A) `la (σ|j,β,A ∪ {j → i}) (σ|i|j,β,A) `ra (σ|i,β,A ∪ {i → j}) (σ|i|j|k, β, </context>
<context position="33518" citStr="Goldberg et al. (2014)" startWordPosition="6238" endWordPosition="6241">Czech treebank, the average value of |σ |is 7.2, with a maximum of 87. Even more interesting, the average value of |βR| is 2.6, with a maximum of 23. Comparing this to the average and maximum values of |β|, 11 and 192, respectively, we see that the buffer reduction is crucial in reducing training time. Note that, when expressed as a function of n, our dynamic oracle has a worst-case time complexity of O(n8). This is also the time complexity of the dynamic programming algorithm of Cohen et al. (2011) we started with, simulating all computations of our parser. In contrast, the dynamic oracle of Goldberg et al. (2014) for the projective case achieves a time complexity of O(n3) from the dynamic programming parser by Kuhlmann et al. (2011) running in time O(n5). 924 The reason why we do not achieve any asymptotic improvement is that some helpful properties that hold with projective trees are no longer satisfied in the non-projective case. In the projective (arc-standard) case, subtrees that are in the buffer can be completely reduced. As a consequence, each oracle step always combines an inferred entry in the table with either a node from the stack or a node from the reduced buffer, asymptotically reducing t</context>
<context position="35738" citStr="Goldberg et al. (2014)" startWordPosition="6655" endWordPosition="6658">h root 5, we would incorrectly obtain a loss of 0, as the resulting tree is parsable if we start with Leh followed by ila and �ra2. Note that there is no way of knowing whether it is safe to reduce the subtree rooted at 5 without using nonlocal information. For example, the arc 2 -* 6 is crucial here: if 6 depended on 5 or 4 instead, the loss would be zero. These complications are not found in the projective case, allowing for the mentioned asymptotic improvement. 7 Experimental Evaluation For comparability with previous work on dynamic oracles, we follow the experimental settings reported by Goldberg et al. (2014) for their arc-standard dynamic oracle. In particular, we use the same training algorithm, features, and root node position. However, we train the model for 20 iterastatic dynamic UAS LAS UAS LAS Arabic 80.90 71.56 82.23 72.63 Basque 75.96 66.74 74.32 65.59 Catalan 90.55 85.20 89.94 84.96 Chinese 84.72 79.93 85.34 81.00 Czech 79.83 72.69 82.08 74.44 English 85.52 84.46 87.38 86.40 Greek 79.84 72.26 81.55 74.14 Hungarian 78.13 68.90 76.27 68.14 Italian 83.08 78.94 84.43 80.45 Turkish 79.57 69.44 79.41 70.32 Bulgarian 89.46 85.99 89.32 85.92 Danish 85.58 81.25 86.03 81.59 Dutch 79.05 75.69 80.13</context>
<context position="39334" citStr="Goldberg et al. (2014)" startWordPosition="7244" endWordPosition="7248">ags and the standard training/test partition; for the PTB we use automatically assigned tags, we train on sections 2-21 and test on section 23. 7.2 Results and Analysis In Table 1 we report the unlabelled (UAS) and labelled (LAS) attachment scores for the static and the dynamic oracles. Each figure is an average over the accuracy provided by 5 models trained with the same setup but using a different random seed. The seed is only used to shuffle the sentences in random order during each iteration of training. Our results are consistent with the results reported by Goldberg and Nivre (2013) and Goldberg et al. (2014). For most of the datasets, we obtain a relevant improvement in both UAS and LAS. For Dutch, Czech and German, we achieve an error reduction of 5.2%, 11.2% and 4.5%, respectively. Exceptions to this general trend are Swedish and Bulgarian, where the accuracy differences are negligible, and the Basque, Catalan and Hungarian datasets, where the performance actually decreases. If instead of testing on the standard test sets we use 10-fold cross-validation and average the resulting accuracies, we obtain improvements for all languages in Table 1 but Basque and Hungarian. More specifically, measured</context>
<context position="40644" citStr="Goldberg et al. (2014)" startWordPosition="7450" endWordPosition="7453">3) with static oracle; for Bulgarian (88.42, 83.91) against (88.20, 83.55); and for Catalan (88.33, 83.64) against (88.06, 83.13). This suggests that the negligible or unfavourable results in Table 1 for these languages are due to statistical variability given the small size of the test sets. As for Basque, we measure (75.54, 67.58) against (76.77, 68.20); similarly, for Hungarian we measure (75.66, 67.66) against (77.22, 68.42). Unfortunately, we have no explanation for these performance decreases, in terms of the typology of the non-projective patterns found in these two datasets. Note that Goldberg et al. (2014) also observed a performance decrease on the Basque dataset in the projective case, although not on Hungarian. The parsing times measured in our experiments for the static and the dynamic oracles are the same, since the oracle algorithm is only used during the training stage. Thus the reported improvements in parsing accuracy come at no extra cost for parsing time. In the training stage, the extra processing needed to compute the loss and to explore paths that do not lead to a gold tree made training about 4 times slower, on average, for the dynamic oracle model. This confirms that our oracle </context>
</contexts>
<marker>Goldberg, Sartorio, Satta, 2014</marker>
<rawString>Yoav Goldberg, Francesco Sartorio, and Giorgio Satta. 2014. A tabular method for dynamic oracles in transition-based parsing. Transactions of the Association for Computational Linguistics, 2(April):119– 130.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John E Hopcroft</author>
<author>Rajeev Motwani</author>
<author>Jeffrey D Ullman</author>
</authors>
<date>2006</date>
<booktitle>Introduction to Automata Theory, Languages, and Computation (3rd Edition).</booktitle>
<publisher>AddisonWesley Longman Publishing Co., Inc.,</publisher>
<location>Boston, MA, USA.</location>
<contexts>
<context position="5203" citStr="Hopcroft et al., 2006" startWordPosition="811" endWordPosition="814">symbol occurrences in w. For i, j E V„ with i =� j, we write i —* j to denote a grammatical dependency of some unspecified type between wz and wj, where wz is the head and wj is the dependent. A dependency tree t for w is a directed tree with node set V„ and with root node 0. An arc of t is a pair (i, j), encoding a dependency i —* j; we will often use the latter notation to denote arcs. A transition-based dependency parser typically uses a stack data structure to process the input string from left to right, in a way very similar to the classical push-down automaton for contextfree languages (Hopcroft et al., 2006). Each stack element is a node from V„, representing the root of a dependency tree spanning some portion of the input w, and no internal state is used. At each step the parser applies some transition that updates the stack and/or consumes one symbol from the input. Transitions may also construct new dependencies, which are added to the current configuration of the parser. We represent the stack as an ordered sequence σ = [hd, ... , h1], d &gt; 0, of nodes hz E V„, with the topmost element placed at the right. When d = 0, we have the empty stack σ = []. We use the vertical bar to denote the append</context>
<context position="22022" citStr="Hopcroft et al., 2006" startWordPosition="3894" endWordPosition="3897"> input configuration c. We start with some additional notation. Let -y = QQR be the concatenation of Q and QR, which we treat as a string of nodes. For integers i with 0 &lt; i &lt; J-yJ − 1, we write -y[i] to denote the (i + 1)-th node of -y. Let also E = JQJ. Symbol E is used to mark the boundary between the stack and the reduced buffer in -y, thus -y[i] with i &lt; E is a node of Q, while -y[i] with i &gt; E is a node of QR. Algorithm 1 computes the loss of c by processing the sequence -y in a way quite similar to the 921 standard nested loop implementation of the CKY parser for context-free grammars (Hopcroft et al., 2006). The algorithm uses a two-dimensional array T whose indexes range from 0 to |-y |= E + |OR|, and only the cells T [i, j] with i &lt; j are filled. We view each T [i, j] as an association list whose keys are items [h1, h2h3], defined in the context of the substring -y[i] · · ·-y[j − 1] of -y; see Section 4. The value stored at T [i, j]([h1, h2h3]) is the minimum loss contribution due to the computations represented by [h1, h2h3]. For technical reasons, we assume that our parser starts with a symbol $ ∈6 Uw in the stack, denoting the bottom of the stack. We initialize the table by populating the c</context>
</contexts>
<marker>Hopcroft, Motwani, Ullman, 2006</marker>
<rawString>John E. Hopcroft, Rajeev Motwani, and Jeffrey D. Ullman. 2006. Introduction to Automata Theory, Languages, and Computation (3rd Edition). AddisonWesley Longman Publishing Co., Inc., Boston, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>Kenji Sagae</author>
</authors>
<title>Dynamic programming for linear-time incremental parsing.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<contexts>
<context position="1638" citStr="Huang and Sagae (2010)" startWordPosition="224" endWordPosition="227">eered by Yamada and Matsumoto (2003) and Nivre (2003). These methods incrementally process the input sentence from left to right, predicting the next parsing action, called transition, on the basis of a compact representation of the derivation history. Greedy transition-based parsers can be very efficient, allowing web-scale parsing with high throughput. However, the accuracy of these methods still falls behind that of transition-based parsers using beam-search, where the accuracy improvement is obtained at the cost of a decrease in parsing efficiency; see for instance Zhang and Nivre (2011), Huang and Sagae (2010), Choi and McCallum (2013). As an alternative to beamsearch, recent research on transition-based parsing has therefore explored possible ways of improving accuracy at no extra cost in parsing efficiency. The training of transition-based parsers relies on a component called the parsing oracle, which maps parser configurations to optimal transitions with respect to a gold tree. A discriminative model is then trained to simulate the oracle’s behavior, and is later used for decoding. Traditionally, socalled static oracles have been exploited in training, where a static oracle is defined only for c</context>
</contexts>
<marker>Huang, Sagae, 2010</marker>
<rawString>Liang Huang and Kenji Sagae. 2010. Dynamic programming for linear-time incremental parsing. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Kuhlmann</author>
<author>Joakim Nivre</author>
</authors>
<title>Transitionbased techniques for non-projective dependency parsing.</title>
<date>2010</date>
<journal>Northern European Journal of Language Technology,</journal>
<volume>2</volume>
<issue>1</issue>
<contexts>
<context position="3441" citStr="Kuhlmann and Nivre (2010)" startWordPosition="496" endWordPosition="499">ions of dynamic oracles run in exponential time, since they need to simulate all possible computations of the parser for the input configuration. Polynomial-time implementations of dynamic oracles have been proposed by the above mentioned authors for several projective dependency parsers. To our knowledge, no polynomial-time algorithm has been published for transition-based parsers based on non-projective dependency grammars. In this paper we consider a restriction of a transition-based, non-projective parser originally presented by Attardi (2006). This restriction was further investigated by Kuhlmann and Nivre (2010) and Cohen et al. (2011). We provide an implementation for a dynamic oracle for this parser running in polynomial time. We experimentally compare the parser trained with the dynamic oracle to a baseline obtained by training with a static oracle. Significant accuracy improvements are achieved on many languages when using our dynamic oracle. To our knowledge, these are the first experimental results on non-projective parsing based on a dynamic oracle. 917 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 917–927, October 25-29, 2014, Doha, Qata</context>
<context position="10819" citStr="Kuhlmann and Nivre (2010)" startWordPosition="1835" endWordPosition="1838"> naive way of implementing (3) would be to explicitly compute the set D(c) in (2), which has exponential size. More interestingly, the implementation of dynamic oracles proposed by the above cited authors all run in polynomial time. These oracles are all defined for projective parsing. In this paper, we present a polynomial-time oracle for a non-projective parser. 3 Non-Projective Dependency Parsing In this section we introduce a parser for nonprojective dependency grammars that is derived from the transition-based parser originally presented by Attardi (2006), and was further investigated by Kuhlmann and Nivre (2010) and Cohen et al. (2011). Our definitions follow the framework introduced in Section 2.1. We start with some additional notation. Let t be a dependency tree for w and let k be a node of t. Consider the complete subtree t&apos; of t rooted at k, that is, the subtree of t induced by k and all of the descendants of k in t. The span of t&apos; is the subsequence of tokens in w represented by the nodes of t&apos;. Node k has gap-degree 0 if the span of t&apos; forms a (contiguous) substring of w. A dependency tree is called projective if all of its nodes have gap-degree 0; a dependency tree which is not projective is </context>
<context position="14874" citStr="Kuhlmann and Nivre, 2010" startWordPosition="2539" endWordPosition="2542">length of the stack is at least 2, while the transitions �la2 and �ra2 require at least 3 nodes in the stack. Transitions �la2 and �ra2 were originally introduced by Attardi (2006) together with other, more complex transitions. The parser we define here is therefore more restrictive than Attardi (2006), meaning that it does not cover all the non-projective trees that can be processed by the original parser. However, the restricted parser has recently attracted some research interest, as it covers the vast majority of non-projective constructions appearing in standard treebanks (Attardi, 2006; Kuhlmann and Nivre, 2010), while keeping simplicity and interesting properties like being compatible with polynomial-time dynamic programming (Cohen et al., 2011). 4 Representation of Computations Our oracle algorithm exploits a dynamic programming technique which, given an input string, combines certain pieces of a computation of the parser from Section 3 to obtain larger pieces. In order to efficiently encode pieces of computations, we borrow a representation proposed by Cohen et al. (2011), which is introduced in this section. Let w = a0 • • • an and Vw be specified as in Section 2, and let w&apos; be some substring of </context>
</contexts>
<marker>Kuhlmann, Nivre, 2010</marker>
<rawString>Marco Kuhlmann and Joakim Nivre. 2010. Transitionbased techniques for non-projective dependency parsing. Northern European Journal of Language Technology, 2(1):1–19.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Kuhlmann</author>
<author>Carlos G´omez-Rodr´ıguez</author>
<author>Giorgio Satta</author>
</authors>
<title>Dynamic programming algorithms for transition-based dependency parsers.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>673--682</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<marker>Kuhlmann, G´omez-Rodr´ıguez, Satta, 2011</marker>
<rawString>Marco Kuhlmann, Carlos G´omez-Rodr´ıguez, and Giorgio Satta. 2011. Dynamic programming algorithms for transition-based dependency parsers. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 673–682, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="38583" citStr="Marcus et al., 1993" startWordPosition="7116" endWordPosition="7119">ining algorithm to follow the zero-loss transition provided by the oracle. While this version of Attardi’s parser has been shown to cover the vast majority of non-projective sentences in several treebanks (Attardi, 2006; Cohen et al., 2012), there still are some sentences which are not parsable. These sentences are skipped during training, but not during test and evaluation of the model. Datasets We evaluate the parser performance over CoNLL 2006 and CoNLL 2007 datasets. If a language is present in both datasets, we use the latest version. We also include results over the Penn Treebank (PTB) (Marcus et al., 1993) converted to Stanford basic dependencies (De Marneffe et al., 2006). For the CoNLL datasets we use the provided part-of-speech tags and the standard training/test partition; for the PTB we use automatically assigned tags, we train on sections 2-21 and test on section 23. 7.2 Results and Analysis In Table 1 we report the unlabelled (UAS) and labelled (LAS) attachment scores for the static and the dynamic oracles. Each figure is an average over the accuracy provided by 5 models trained with the same setup but using a different random seed. The seed is only used to shuffle the sentences in rando</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
</authors>
<title>An efficient algorithm for projective dependency parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the Eighth International Workshop on Parsing Technologies (IWPT),</booktitle>
<pages>149--160</pages>
<location>Nancy, France.</location>
<contexts>
<context position="1069" citStr="Nivre (2003)" startWordPosition="138" endWordPosition="139"> of greedy transition-based dependency parsers, without sacrificing parsing efficiency. However, this enhancement is limited to projective parsing, and dynamic oracles have not yet been implemented for parsers supporting non-projectivity. In this paper we introduce the first such oracle, for a non-projective parser based on Attardi’s parser. We show that training with this oracle improves parsing accuracy over a conventional (static) oracle on a wide range of datasets. 1 Introduction Greedy transition-based parsers for dependency grammars have been pioneered by Yamada and Matsumoto (2003) and Nivre (2003). These methods incrementally process the input sentence from left to right, predicting the next parsing action, called transition, on the basis of a compact representation of the derivation history. Greedy transition-based parsers can be very efficient, allowing web-scale parsing with high throughput. However, the accuracy of these methods still falls behind that of transition-based parsers using beam-search, where the accuracy improvement is obtained at the cost of a decrease in parsing efficiency; see for instance Zhang and Nivre (2011), Huang and Sagae (2010), Choi and McCallum (2013). As </context>
<context position="4228" citStr="Nivre (2003)" startWordPosition="616" endWordPosition="617">ynamic oracle to a baseline obtained by training with a static oracle. Significant accuracy improvements are achieved on many languages when using our dynamic oracle. To our knowledge, these are the first experimental results on non-projective parsing based on a dynamic oracle. 917 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 917–927, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics 2 Preliminary Definitions Transition-based dependency parsing was originally introduced by Yamada and Matsumoto (2003) and Nivre (2003). In this section we briefly summarize the notation we use for this framework and introduce the notion of dynamic oracle. 2.1 Transition-Based Dependency Parsing We represent an input sentence as a string w = w0 · · · w,,,, n &gt; 1, where each wz with i =� 0 is a lexical symbol and w0 is a special symbol called root. Set V„ = {i |0 &lt; i &lt; n} denotes the symbol occurrences in w. For i, j E V„ with i =� j, we write i —* j to denote a grammatical dependency of some unspecified type between wz and wj, where wz is the head and wj is the dependent. A dependency tree t for w is a directed tree with node</context>
</contexts>
<marker>Nivre, 2003</marker>
<rawString>Joakim Nivre. 2003. An efficient algorithm for projective dependency parsing. In Proceedings of the Eighth International Workshop on Parsing Technologies (IWPT), pages 149–160, Nancy, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
</authors>
<title>Incrementality in deterministic dependency parsing.</title>
<date>2004</date>
<booktitle>In Workshop on Incremental Parsing: Bringing Engineering and Cognition Together,</booktitle>
<pages>50--57</pages>
<location>Barcelona,</location>
<contexts>
<context position="13543" citStr="Nivre (2004)" startWordPosition="2337" endWordPosition="2338">e head and the second topmost node is the dependent, and removes the latter from the stack. The transition `ra, called right-arc, is defined symmetrically, so that the topmost stack node is atL(t1, t2) . (2) 919 stack length 6 6 6 c0 c1 ... cm Figure 2: General form of the computations associated with an item [h1, h2, h3]. tached as a dependent of the second topmost node. The combination of the shift, left-arc and rightarc transitions provides complete coverage of projective dependency trees, but no support for nonprojectivity, and corresponds to the so-called arcstandard parser introduced by Nivre (2004). Support for non-projective dependencies is achieved by adding the transitions �la2 and �ra2, which are variants of the left-arc and right-arc transitions, respectively. These new transitions create dependencies involving the first and the third topmost nodes in the stack. The creation of dependencies between non-adjacent stack nodes might produce crossing arcs and is the key to the construction of non-projective trees. Recall that transitions are partial functions, meaning that they might be undefined for some configurations. Specifically, the shift transition is only defined for configurati</context>
</contexts>
<marker>Nivre, 2004</marker>
<rawString>Joakim Nivre. 2004. Incrementality in deterministic dependency parsing. In Workshop on Incremental Parsing: Bringing Engineering and Cognition Together, pages 50–57, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroyasu Yamada</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Statistical dependency analysis with support vector machines.</title>
<date>2003</date>
<booktitle>In Proceedings of the 8th International Workshop on Parsing Technologies (IWPT),</booktitle>
<pages>195--206</pages>
<contexts>
<context position="1052" citStr="Yamada and Matsumoto (2003)" startWordPosition="133" endWordPosition="136">nsiderably improved the accuracy of greedy transition-based dependency parsers, without sacrificing parsing efficiency. However, this enhancement is limited to projective parsing, and dynamic oracles have not yet been implemented for parsers supporting non-projectivity. In this paper we introduce the first such oracle, for a non-projective parser based on Attardi’s parser. We show that training with this oracle improves parsing accuracy over a conventional (static) oracle on a wide range of datasets. 1 Introduction Greedy transition-based parsers for dependency grammars have been pioneered by Yamada and Matsumoto (2003) and Nivre (2003). These methods incrementally process the input sentence from left to right, predicting the next parsing action, called transition, on the basis of a compact representation of the derivation history. Greedy transition-based parsers can be very efficient, allowing web-scale parsing with high throughput. However, the accuracy of these methods still falls behind that of transition-based parsers using beam-search, where the accuracy improvement is obtained at the cost of a decrease in parsing efficiency; see for instance Zhang and Nivre (2011), Huang and Sagae (2010), Choi and McC</context>
<context position="4211" citStr="Yamada and Matsumoto (2003)" startWordPosition="611" endWordPosition="614">re the parser trained with the dynamic oracle to a baseline obtained by training with a static oracle. Significant accuracy improvements are achieved on many languages when using our dynamic oracle. To our knowledge, these are the first experimental results on non-projective parsing based on a dynamic oracle. 917 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 917–927, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics 2 Preliminary Definitions Transition-based dependency parsing was originally introduced by Yamada and Matsumoto (2003) and Nivre (2003). In this section we briefly summarize the notation we use for this framework and introduce the notion of dynamic oracle. 2.1 Transition-Based Dependency Parsing We represent an input sentence as a string w = w0 · · · w,,,, n &gt; 1, where each wz with i =� 0 is a lexical symbol and w0 is a special symbol called root. Set V„ = {i |0 &lt; i &lt; n} denotes the symbol occurrences in w. For i, j E V„ with i =� j, we write i —* j to denote a grammatical dependency of some unspecified type between wz and wj, where wz is the head and wj is the dependent. A dependency tree t for w is a direct</context>
</contexts>
<marker>Yamada, Matsumoto, 2003</marker>
<rawString>Hiroyasu Yamada and Yuji Matsumoto. 2003. Statistical dependency analysis with support vector machines. In Proceedings of the 8th International Workshop on Parsing Technologies (IWPT), pages 195–206.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Joakim Nivre</author>
</authors>
<title>Transition-based dependency parsing with rich non-local features.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>188--193</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="1614" citStr="Zhang and Nivre (2011)" startWordPosition="220" endWordPosition="223"> grammars have been pioneered by Yamada and Matsumoto (2003) and Nivre (2003). These methods incrementally process the input sentence from left to right, predicting the next parsing action, called transition, on the basis of a compact representation of the derivation history. Greedy transition-based parsers can be very efficient, allowing web-scale parsing with high throughput. However, the accuracy of these methods still falls behind that of transition-based parsers using beam-search, where the accuracy improvement is obtained at the cost of a decrease in parsing efficiency; see for instance Zhang and Nivre (2011), Huang and Sagae (2010), Choi and McCallum (2013). As an alternative to beamsearch, recent research on transition-based parsing has therefore explored possible ways of improving accuracy at no extra cost in parsing efficiency. The training of transition-based parsers relies on a component called the parsing oracle, which maps parser configurations to optimal transitions with respect to a gold tree. A discriminative model is then trained to simulate the oracle’s behavior, and is later used for decoding. Traditionally, socalled static oracles have been exploited in training, where a static orac</context>
</contexts>
<marker>Zhang, Nivre, 2011</marker>
<rawString>Yue Zhang and Joakim Nivre. 2011. Transition-based dependency parsing with rich non-local features. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 188–193, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
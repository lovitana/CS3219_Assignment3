<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.993824">
Revisiting Embedding Features for Simple Semi-supervised Learning
</title>
<author confidence="0.990252">
Jiang Guo†, Wanxiang Che†, Haifeng Wang$, Ting Liu†*
</author>
<affiliation confidence="0.860845">
†Research Center for Social Computing and Information Retrieval
Harbin Institute of Technology, China
$Baidu Inc., Beijing, China
</affiliation>
<email confidence="0.8127425">
{jguo, car, tliu}@ir.hit.edu.cn
wanghaifeng@baidu.com
</email>
<sectionHeader confidence="0.981978" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99996308">
Recent work has shown success in us-
ing continuous word embeddings learned
from unlabeled data as features to improve
supervised NLP systems, which is re-
garded as a simple semi-supervised learn-
ing mechanism. However, fundamen-
tal problems on effectively incorporating
the word embedding features within the
framework of linear models remain. In
this study, we investigate and analyze three
different approaches, including a new pro-
posed distributional prototype approach,
for utilizing the embedding features. The
presented approaches can be integrated
into most of the classical linear models in
NLP. Experiments on the task of named
entity recognition show that each of the
proposed approaches can better utilize the
word embedding features, among which
the distributional prototype approach per-
forms the best. Moreover, the combination
of the approaches provides additive im-
provements, outperforming the dense and
continuous embedding features by nearly
2 points of F1 score.
</bodyText>
<sectionHeader confidence="0.992456" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.993276512195122">
Learning generalized representation of words is
an effective way of handling data sparsity caused
by high-dimensional lexical features in NLP sys-
tems, such as named entity recognition (NER)
and dependency parsing. As a typical low-
dimensional and generalized word representa-
tion, Brown clustering of words has been stud-
ied for a long time. For example, Liang (2005)
and Koo et al. (2008) used the Brown cluster
features for semi-supervised learning of various
NLP tasks and achieved significant improvements.
∗Email correspondence.
Recent research has focused on a special fam-
ily of word representations, named “word embed-
dings”. Word embeddings are conventionally de-
fined as dense, continuous, and low-dimensional
vector representations of words. Word embed-
dings can be learned from large-scale unlabeled
texts through context-predicting models (e.g., neu-
ral network language models) or spectral methods
(e.g., canonical correlation analysis) in an unsu-
pervised setting.
Compared with the so-called one-hot represen-
tation where each word is represented as a sparse
vector of the same size of the vocabulary and only
one dimension is on, word embedding preserves
rich linguistic regularities of words with each di-
mension hopefully representing a latent feature.
Similar words are expected to be distributed close
to one another in the embedding space. Conse-
quently, word embeddings can be beneficial for
a variety of NLP applications in different ways,
among which the most simple and general way is
to be fed as features to enhance existing supervised
NLP systems.
Previous work has demonstrated effectiveness
of the continuous word embedding features in sev-
eral tasks such as chunking and NER using gener-
alized linear models (Turian et al., 2010).1 How-
ever, there still remain two fundamental problems
that should be addressed:
</bodyText>
<listItem confidence="0.9153514">
• Are the continuous embedding features fit for
the generalized linear models that are most
widely adopted in NLP?
• How can the generalized linear models better
utilize the embedding features?
</listItem>
<bodyText confidence="0.933389">
According to the results provided by Turian et
1Generalized linear models refer to the models that de-
scribe the data as a combination of linear basis functions,
either directly in the input variables space or through some
transformation of the probability distributions (e.g., log-
linear models).
</bodyText>
<page confidence="0.460723">
110
</page>
<note confidence="0.9771435">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 110–120,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999863137254902">
al. (2010), the embedding features brought signif-
icantly less improvement than Brown clustering
features. This result is actually not reasonable be-
cause the expressing power of word embeddings
is theoretically stronger than clustering-based rep-
resentations which can be regarded as a kind of
one-hot representation but over a low-dimensional
vocabulary (Bengio et al., 2013).
Wang and Manning (2013) showed that linear
architectures perform better in high-dimensional
discrete feature space than non-linear ones,
whereas non-linear architectures are more effec-
tive in low-dimensional and continuous feature
space. Hence, the previous method that directly
uses the continuous word embeddings as features
in linear models (CRF) is inappropriate. Word
embeddings may be better utilized in the linear
modeling framework by smartly transforming the
embeddings to some relatively higher dimensional
and discrete representations.
Driven by this motivation, we present three
different approaches: binarization (Section 3.2),
clustering (Section 3.3) and a new proposed distri-
butional prototype method (Section 3.4) for better
incorporating the embeddings features. In the bi-
narization approach, we directly binarize the con-
tinuous word embeddings by dimension. In the
clustering approach, we cluster words based on
their embeddings and use the resulting word clus-
ter features instead. In the distributional prototype
approach, we derive task-specific features from
word embeddings by utilizing a set of automati-
cally extracted prototypes for each target label.
We carefully compare and analyze these ap-
proaches in the task of NER. Experimental results
are promising. With each of the three approaches,
we achieve higher performance than directly using
the continuous embedding features, among which
the distributional prototype approach performs the
best. Furthermore, by putting the most effective
two of these features together, we finally outper-
form the continuous embedding features by nearly
2 points of F1 Score (86.21% vs. 88.11%).
The major contribution of this paper is twofold.
(1) We investigate various approaches that can bet-
ter utilize word embeddings for semi-supervised
learning. (2) We propose a novel distributional
prototype approach that shows the great potential
of word embedding features. All the presented ap-
proaches can be easily integrated into most of the
classical linear NLP models.
</bodyText>
<sectionHeader confidence="0.9121845" genericHeader="introduction">
2 Semi-supervised Learning with Word
Embeddings
</sectionHeader>
<bodyText confidence="0.999983916666667">
Statistical modeling has achieved great success
in most NLP tasks. However, there still remain
some major unsolved problems and challenges,
among which the most widely concerned is the
data sparsity problem. Data sparsity in NLP is
mainly caused by two factors, namely, the lack
of labeled training data and the Zipf distribution
of words. On the one hand, large-scale labeled
training data are typically difficult to obtain, espe-
cially for structure prediction tasks, such as syn-
tactic parsing. Therefore, the supervised mod-
els can only see limited examples and thus make
biased estimation. On the other hand, the nat-
ural language words are Zipf distributed, which
means that most of the words appear a few times
or are completely absent in our texts. For these
low-frequency words, the corresponding parame-
ters usually cannot be fully trained.
More foundationally, the reason for the above
factors lies in the high-dimensional and sparse lex-
ical feature representation, which completely ig-
nores the similarity between features, especially
word features. To overcome this weakness, an ef-
fective way is to learn more generalized represen-
tations of words by exploiting the numerous un-
labeled data, in a semi-supervised manner. After
which, the generalized word representations can
be used as extra features to facilitate the super-
vised systems.
Liang (2005) learned Brown clusters of
words (Brown et al., 1992) from unlabeled data
and use them as features to promote the supervised
NER and Chinese word segmentation. Brown
clusters of words can be seen as a generalized
word representation distributed in a discrete and
low-dimensional vocabulary space. Contextually
similar words are grouped in the same cluster. The
Brown clustering of words was also adopted in de-
pendency parsing (Koo et al., 2008) and POS tag-
ging for online conversational text (Owoputi et al.,
2013), demonstrating significant improvements.
Recently, another kind of word representation
named “word embeddings” has been widely stud-
ied (Bengio et al., 2003; Mnih and Hinton, 2008).
Using word embeddings, we can evaluate the sim-
ilarity of two words straightforward by comput-
ing the dot-product of two numerical vectors in the
Hilbert space. Two similar words are expected to
</bodyText>
<page confidence="0.887061">
111
</page>
<bodyText confidence="0.999871681818182">
be distributed close to each other.2
Word embeddings can be useful as input to an
NLP model (mostly non-linear) or as additional
features to enhance existing systems. Collobert
et al. (2011) used word embeddings as input to a
deep neural network for multi-task learning. De-
spite of the effectiveness, such non-linear models
are hard to build and optimize. Besides, these ar-
chitectures are often specialized for a certain task
and not scalable to general tasks. A simple and
more general way is to feed word embeddings as
augmented features to an existing supervised sys-
tem, which is similar to the semi-supervised learn-
ing with Brown clusters.
As discussed in Section 1, Turian et al. (2010)
is the pioneering work on using word embedding
features for semi-supervised learning. However,
their approach cannot fully exploit the potential
of word embeddings. We revisit this problem
in this study and investigate three different ap-
proaches for better utilizing word embeddings in
semi-supervised learning.
</bodyText>
<sectionHeader confidence="0.98025" genericHeader="method">
3 Approaches for Utilizing Embedding
Features
</sectionHeader>
<subsectionHeader confidence="0.999866">
3.1 Word Embedding Training
</subsectionHeader>
<bodyText confidence="0.999791833333333">
In this paper, we will consider a context-
predicting model, more specifically, the Skip-gram
model (N ikolov et al., 2013a; N ikolov et al.,
2013b) for learning word embeddings, since it is
much more efficient as well as memory-saving
than other approaches.
Let’s denote the embedding matrix to be learned
by Cd×N, where N is the vocabulary size and d is
the dimension of word embeddings. Each column
of C represents the embedding of a word. The
Skip-gram model takes the current word w as in-
put, and predicts the probability distribution of its
context words within a fixed window size. Con-
cretely, w is first mapped to its embedding vw by
selecting the corresponding column vector of C
(or multiplying C with the one-hot vector of w).
The probability of its context word c is then com-
puted using a log-linear function:
</bodyText>
<equation confidence="0.9982985">
_ exp(v&gt;c vw)
P(c |w; θ) Ec&apos;∈V exp(vc&apos;&gt;vw) (1)
</equation>
<bodyText confidence="0.7858655">
where V is the vocabulary. The parameters θ are
vwi, vci for w, c E V and i = 1, ..., d. Then, the
2The term similar should be viewed depending on the spe-
cific task.
log-likelihood over the entire training dataset D
can be computed as:
</bodyText>
<equation confidence="0.9977415">
J(θ) = 1: log p(c|w; θ) (2)
(w,c)∈D
</equation>
<bodyText confidence="0.999377363636364">
The model can be trained by maximizing J(θ).
Here, we suppose that the word embeddings
have already been trained from large-scale unla-
beled texts. We will introduce various approaches
for utilizing the word embeddings as features for
semi-supervised learning. The main idea, as in-
troduced in Section 1, is to transform the continu-
ous word embeddings to some relatively higher di-
mensional and discrete representations. The direct
use of continuous embeddings as features (Turian
et al., 2010) will serve as our baseline setting.
</bodyText>
<subsectionHeader confidence="0.999967">
3.2 Binarization of Embeddings
</subsectionHeader>
<bodyText confidence="0.999910916666667">
One fairly natural approach for converting the
continuous-valued word embeddings to discrete
values is binarization by dimension.
Formally, we aim to convert the continuous-
valued embedding matrix Cd×N, to another matrix
Md×N which is discrete-valued. There are various
conversion functions. Here, we consider a sim-
ple one. For the ith dimension of the word em-
beddings, we divide the corresponding row vector
Ci into two halves for positive (Ci+) and nega-
tive (Ci−), respectively. The conversion function
is then defined as follows:
</bodyText>
<equation confidence="0.968378666666667">
Mij = φ(Cij) = { U+, if Cij ≥ mean(Ci+)
B−, if Cij G mean(Ci−)
0, otherwise
</equation>
<bodyText confidence="0.99989175">
where mean(v) is the mean value of vector v, U+
is a string feature which turns on when the value
(Cij) falls into the upper part of the positive list.
Similarly, B− refers to the bottom part of the neg-
ative list. The insight behind φ is that we only con-
sider the features with strong opinions (i.e., posi-
tive or negative) on each dimension and omit the
values close to zero.
</bodyText>
<subsectionHeader confidence="0.999929">
3.3 Clustering of Embeddings
</subsectionHeader>
<bodyText confidence="0.999925">
Yu et al. (2013) introduced clustering embeddings
to overcome the disadvantage that word embed-
dings are not suitable for linear models. They sug-
gested that the high-dimensional cluster features
make samples from different classes better sepa-
rated by linear models.
</bodyText>
<page confidence="0.653151">
112
</page>
<bodyText confidence="0.999991272727273">
In this study, we again investigate this ap-
proach. Concretely, each word is treated as a sin-
gle sample. The batch k-means clustering algo-
rithm (Sculley, 2010) is used,3 and each cluster
is represented as the mean of the embeddings of
words assigned to it. Similarities between words
and clusters are measured by Euclidean distance.
Moreover, different number of clusters n con-
tain information of different granularities. There-
fore, we combine the cluster features of different
ns to better utilize the embeddings.
</bodyText>
<subsectionHeader confidence="0.969894">
3.4 Distributional Prototype Features
</subsectionHeader>
<bodyText confidence="0.999993606060606">
We propose a novel kind of embedding features,
named distributional prototype features for su-
pervised models. This is mainly inspired by
prototype-driven learning (Haghighi and Klein,
2006) which was originally introduced as a pri-
marily unsupervised approach for sequence mod-
eling. In prototype-driven learning, a few pro-
totypical examples are specified for each target
label, which can be treated as an injection of
prior knowledge. This sparse prototype informa-
tion is then propagated across an unlabeled corpus
through distributional similarities.
The basic motivation of the distributional pro-
totype features is that similar words are supposed
to be tagged with the same label. This hypothesis
makes great sense in tasks such as NER and POS
tagging. For example, suppose Michael is a pro-
totype of the named entity (NE) type PER. Using
the distributional similarity, we could link similar
words to the same prototypes, so the word David
can be linked to Michael because the two words
have high similarity (exceeds a threshold). Using
this link feature, the model will push David closer
to PER.
To derive the distributional prototype features,
first, we need to construct a few canonical exam-
ples (prototypes) for each target annotation label.
We use the normalized pointwise mutual informa-
tion (NPMI) (Bouma, 2009) between the label and
word, which is a smoothing version of the standard
PMI, to decide the prototypes of each label. Given
the annotated training corpus, the NPMI between
a label and word is computed as follows:
</bodyText>
<equation confidence="0.89322">
λ(label, word)
λn(label, word) = (3)
− ln p(label, word)
3code.google.com/p/sofia-ml
</equation>
<table confidence="0.9929451">
NE Type Prototypes
B-PER Mark, Michael, David, Paul
I-PER Akram, Ahmed, Khan, Younis
B-ORG Reuters, U.N., Ajax, PSV
I-ORG Newsroom, Inc, Corp, Party
B-LOC U.S., Germany, Britain, Australia
I-LOC States, Republic, Africa, Lanka
B-MISC Russian, German, French, British
I-MISC Cup, Open, League, OPEN
O., ,, the, to
</table>
<tableCaption confidence="0.980838">
Table 1: Prototypes extracted from the CoNLL-
2003 NER training data using NPMI.
</tableCaption>
<bodyText confidence="0.964721">
where,
</bodyText>
<equation confidence="0.787071">
p(label, word)
(label, word) = ln p(label)p(word) (4)
</equation>
<bodyText confidence="0.992090333333333">
is the standard PMI.
For each target label l (e.g., PER, ORG, LOC),
we compute the NPMI of l and all words in the
vocabulary, and the top m words are chosen as the
prototypes of l. We should note that the proto-
types are extracted fully automatically, without in-
troducing additional human prior knowledge.
Table 1 shows the top four prototypes extracted
from the NER training corpus of CoNLL-2003
shared task (Tjong Kim Sang and De Meul-
der, 2003), which contains four NE types, namely,
PER, ORG, LOC, and MISC. Non-NEs are denoted
by O. We convert the original annotation to the
standard BIO-style. Thus, the final corpus con-
tains nine labels in total.
Next, we introduce the prototypes as features to
our supervised model. We denote the set of pro-
totypes for all target labels by Sp. For each proto-
type z ∈ Sp, we add a predicate proto = z, which
becomes active at each w if the distributional sim-
ilarity between z and w (DistSim(z, w)) is above
some threshold. DistSim(z, w) can be efficiently
calculated through the cosine similarity of the em-
beddings of z and w. Figure 1 gives an illustra-
tion of the distributional prototype features. Un-
like previous embedding features or Brown clus-
ters, the distributional prototype features are task-
specific because the prototypes of each label are
extracted from the training data.
Moreover, each prototype word is also its own
prototype (since a word has maximum similarity
to itself). Thus, if the prototype is closely related
to a label, all the words that are distributionally
</bodyText>
<page confidence="0.830699">
113
</page>
<figureCaption confidence="0.787376">
Figure 1: An example of distributional prototype
features for NER.
</figureCaption>
<bodyText confidence="0.9931385">
similar to that prototype are pushed towards that
label.
</bodyText>
<equation confidence="0.973076875">
y;  1 Y,
x i - 1
f x i y i
( , ) 
 word = Hague 
 proto = Britain B-LOC
 
 
</equation>
<sectionHeader confidence="0.927135" genericHeader="method">
4 Supervised Evaluation Task
</sectionHeader>
<bodyText confidence="0.999735142857143">
Various tasks can be considered to compare and
analyze the effectiveness of the above three ap-
proaches. In this study, we partly follow Turian
et al. (2010) and Yu et al. (2013), and take NER as
the supervised evaluation task.
NER identifies and classifies the named entities
such as the names of persons, locations, and orga-
nizations in text. The state-of-the-art systems typ-
ically treat NER as a sequence labeling problem,
where each word is tagged either as a BIO-style
NE or a non-NE category.
Here, we use the linear chain CRF model, which
is most widely used for sequence modeling in the
field of NLP. The CoNLL-2003 shared task dataset
from the Reuters, which was used by Turian et
al. (2010) and Yu et al. (2013), was chosen as
our evaluation dataset. The training set contains
14,987 sentences, the development set contains
3,466 sentences and is used for parameter tuning,
and the test set contains 3,684 sentences.
The baseline features are shown in Table 2.
</bodyText>
<subsectionHeader confidence="0.998517">
4.1 Embedding Feature Templates
</subsectionHeader>
<bodyText confidence="0.999540461538461">
In this section, we introduce the embedding fea-
tures to the baseline NER system, turning the su-
pervised approach into a semi-supervised one.
Dense embedding features. The dense con-
tinuous embedding features can be fed directly to
the CRF model. These embedding features can
be seen as heterogeneous features from the exist-
ing baseline features, which are discrete. There is
no effective way for dense embedding features to
be combined internally or with other discrete fea-
tures. So we only use the unigram embedding fea-
tures following Turian et al. (2010). Concretely,
the embedding feature template is:
</bodyText>
<listItem confidence="0.925044">
Baseline NER Feature Templates
00: wi+k, −2 ≤ k ≤ 2
1: wi+k ◦ wi+k+1, −2 ≤ k ≤ 1
2: ti+k,−2 ≤ k ≤ 2
3: ti+k ◦ ti+k+1, −2 ≤ k ≤ 1
4: chki+k, −2 ≤ k ≤ 2
5: chki+k ◦ chki+k+1, −2 ≤ k ≤ 1
6: Prefix(wi+k, l), −2 ≤ k ≤ 2,1 ≤ l ≤ 4
7: Suffix(wi+k, l), −2 ≤ k ≤ 2,1 ≤ l ≤ 4
</listItem>
<equation confidence="0.8592628">
8: Type(wi+k), −2 ≤ k ≤ 2
Unigram Features
yi ◦ 00 − 08
Bigram Features
yi−1 ◦ yi
</equation>
<bodyText confidence="0.76199525">
Table 2: Features used in the NER system. t is
the POS tag. chk is the chunking tag. Prefix
and Suffix are the first and last l characters of a
word. Type indicates if the word is all-capitalized,
is-capitalized, all-digits, etc.
• dei+k[d], −2 ≤ k ≤ 2, d ranges over the
dimensions of the dense word embedding de.
Binarized embedding features. The binarized
embedding feature template is similar to the dense
one. The only difference is that the feature val-
ues are discrete and we omit dimensions with zero
value. Therefore, the feature template becomes:
</bodyText>
<listItem confidence="0.714105666666667">
• bii+k[d], −2 ≤ k ≤ 2, where bii+k[d] =6 0,
d ranges over the dimensions of the binarized
vector bi of word embedding.
</listItem>
<bodyText confidence="0.999590454545455">
In this way, the dimension of the binarized em-
bedding feature space becomes 2 × d compared
with the originally d of the dense embeddings.
Compound cluster features. The advantage of
the cluster features is that they can be combined
internally or with other features to form compound
features, which can be more discriminative. Fur-
thermore, the number of resulting clusters n can
be tuned, and different ns indicate different granu-
larities. Concretely, the compound cluster feature
template for each specific n is:
</bodyText>
<listItem confidence="0.998789333333333">
• ci+k, −2 ≤ k ≤ 2.
• ci+k ◦ ci+k+1, −2 ≤ k ≤ 1.
• ci−1 ◦ ci+1.
</listItem>
<bodyText confidence="0.5673055">
Distributional prototype features. The set of
prototypes is again denoted by Sp, which is de-
</bodyText>
<figure confidence="0.783727090909091">
in
/IN
Hague
/NNP
O B-LOC
 
pos = NNP
 
proto = England
  ...  
114
</figure>
<bodyText confidence="0.998440875">
cided by selecting the top m (NPMI) words as pro-
totypes of each label, where m is tuned on the de-
velopment set. For each word wi in a sequence,
we compute the distributional similarity between
wi and each prototype in Sp and select the proto-
types zs that DistSim(z, w) &gt; 6. We set 6 = 0.5
without manual tuning. The distributional proto-
type feature template is then:
</bodyText>
<listItem confidence="0.9827335">
• {protoi+k=z  |DistSim(wi+k, z) &gt; 6 &amp; z E
Sp }, −2 &lt; k &lt; 2 .
</listItem>
<bodyText confidence="0.999952">
We only use the unigram features, since the
number of active distributional prototype features
varies for different words (positions). Hence,
these features cannot be combined effectively.
</bodyText>
<subsectionHeader confidence="0.994959">
4.2 Brown Clustering
</subsectionHeader>
<bodyText confidence="0.99998980952381">
Brown clustering has achieved great success in
various NLP applications. At most time, it
provides a strong baseline that is difficult to
beat (Turian et al., 2010). Consequently, in our
study, we conduct comparisons among the embed-
ding features and the Brown clustering features,
along with further investigations of their combina-
tion.
The Brown algorithm is a hierarchical cluster-
ing algorithm which optimizes a class-based bi-
gram language model defined on the word clus-
ters (Brown et al., 1992). The output of the Brown
algorithm is a binary tree, where each word is
uniquely identified by its path from the root. Thus
each word can be represented as a bit-string with
a specific length.
Following the setting of Owoputi et al. (2013),
we will use the prefix features of hierarchical clus-
ters to take advantage of the word similarity in dif-
ferent granularities. Concretely, the Brown cluster
feature template is:
</bodyText>
<listItem confidence="0.928161">
• bci+k, −2 &lt; k &lt; 2.
• prefix(bci+k, p), p E {2,4,6,...,16}, −2 &lt;
k &lt; 2. prefix takes the p-length prefix of
the Brown cluster coding bci+k.
</listItem>
<sectionHeader confidence="0.996832" genericHeader="method">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.996278">
5.1 Experimental Setting
</subsectionHeader>
<bodyText confidence="0.991971941176471">
We take the English Wikipedia until August 2012
as our unlabeled data to train the word embed-
dings.4 Little pre-processing is conducted for the
4download.wikimedia.org.
training of word embeddings. We remove para-
graphs that contain non-roman characters and all
MediaWiki markups. The resulting text is tok-
enized using the Stanford tokenizer,5 and every
word is converted to lowercase. The final dataset
contains about 30 million sentences and 1.52 bil-
lion words. We use a dictionary that contains
212,779 most common words (frequency &gt; 80) in
the dataset. An efficient open-source implementa-
tion of the Skip-gram model is adopted.6 We ap-
ply the negative sampling7 method for optimiza-
tion, and the asynchronous stochastic gradient de-
scent algorithm (Asynchronous SGD) for parallel
weight updating. In this study, we set the dimen-
sion of the word embeddings to 50. Higher di-
mension is supposed to bring more improvements
in semi-supervised learning, but its comparison is
beyond the scope of this paper.
For the cluster features, we tune the number
of clusters n from 500 to 3000 on the develop-
ment set, and finally use the combination of n =
500,1000,1500, 2000, 3000, which achieves the
best results. For the distributional prototype fea-
tures, we use a fixed number of prototype words
(m) for each target label. m is tuned on the devel-
opment set and is finally set to 40.
We induce 1,000 brown clusters of words, the
setting in prior work (Koo et al., 2008; Turian et
al., 2010). The training data of brown clustering is
the same with that of training word embeddings.
</bodyText>
<subsectionHeader confidence="0.786007">
5.2 Results
</subsectionHeader>
<bodyText confidence="0.8674807">
Table 3 shows the performances of NER on the
test dataset. Our baseline is slightly lower than
that of Turian et al. (2010), because they use
the BILOU encoding of NE types which outper-
forms BIO encoding (Ratinov and Roth, 2009).8
Nonetheless, our conclusions hold. As we can see,
all of the three approaches we investigate in this
study achieve better performance than the direct
use of the dense continuous embedding features.
To our surprise, even the binarized embedding
features (BinarizedEmb) outperform the continu-
ous version (DenseEmb). This provides clear evi-
dence that directly using the dense continuous em-
beddings as features in CRF indeed cannot fully
5nlp.stanford.edu/software/tokenizer.
shtml.
6code.google.com/p/word2vec/.
7More details are analyzed in (Goldberg and Levy, 2014).
8We use BIO encoding here in order to compare with most
of the reported benchmarks.
</bodyText>
<page confidence="0.638623">
115
</page>
<table confidence="0.998202235294118">
Setting F1
Baseline 83.43
+DenseEmb† 86.21
+BinarizedEmb 86.75
+ClusterEmb 86.90
+DistPrototype 87.44
+BinarizedEmb+ClusterEmb 87.56
+BinarizedEmb+DistPrototype 87.46
+ClusterEmb+DistPrototype 88.11
+Brown 87.49
+Brown+ClusterEmb 88.17
+Brown+DistPrototype 88.04
+Brown+ClusterEmb+DistPrototype 88.58
Finkel et al. (2005) 86.86
Krishnan and Manning (2006) 87.24
Ando and Zhang (2005) 89.31
Collobert et al. (2011) 88.67
</table>
<tableCaption confidence="0.99916">
Table 3: The performance of semi-supervised
</tableCaption>
<bodyText confidence="0.99879806329114">
NER on the CoNLL-2003 test data, using vari-
ous embedding features. † DenseEmb refers to the
method used by Turian et al. (2010), i.e., the direct
use of the dense and continuous embeddings.
exploit the potential of word embeddings. The
compound cluster features (ClusterEmb) also out-
perform the DenseEmb. The same result is also
shown in (Yu et al., 2013). Further, the distribu-
tional prototype features (DistPrototype) achieve
the best performance among the three approaches
(1.23% higher than DenseEmb).
We should note that the feature templates used
for BinarizedEmb and DistPrototype are merely
unigram features. However, for ClusterEmb, we
form more complex features by combining the
clusters of the context words. We also consider
different number of clusters n, to take advantage
of the different granularities. Consequently, the
dimension of the cluster features is much higher
than that of BinarizedEmb and DistPrototype.
We further combine the proposed features to see
if they are complementary to each other. As shown
in Table 3, the cluster and distributional prototype
features are the most complementary, whereas the
binarized embedding features seem to have large
overlap with the distributional prototype features.
By combining the cluster and distributional pro-
totype features, we further push the performance
to 88.11%, which is nearly two points higher than
the performance of the dense embedding features
(86.21%).9
We also compare the proposed features with
the Brown cluster features. As shown in Table 3,
the distributional prototype features alone achieve
comparable performance with the Brown clusters.
When the cluster and distributional prototype fea-
tures are used together, we outperform the Brown
clusters. This result is inspiring because we show
that the embedding features indeed have stronger
expressing power than the Brown clusters, as de-
sired. Finally, by combining the Brown cluster
features and the proposed embedding features, the
performance can be improved further (88.58%).
The binarized embedding features are not included
in the final compound features because they are al-
most overlapped with the distributional prototype
features in performance.
We also summarize some of the reported
benchmarks that utilize unlabeled data (with no
gazetteers used), including the Stanford NER tag-
ger (Finkel et al. (2005) and Krishnan and Man-
ning (2006)) with distributional similarity fea-
tures. Ando and Zhang (2005) use unlabeled data
for constructing auxiliary problems that are ex-
pected to capture a good feature representation of
the target problem. Collobert et al. (2011) adjust
the feature embeddings according to the specific
task in a deep neural network architecture. We
can see that both Ando and Zhang (2005) and Col-
lobert et al. (2011) learn task-specific lexical fea-
tures, which is similar to the proposed distribu-
tional prototype method in our study. We suggest
this to be the main reason for the superiority of
these methods.
Another advantage of the proposed discrete fea-
tures over the dense continuous features is tag-
ging efficiency. Table 4 shows the running time
using different kinds of embedding features. We
achieve a significant reduction of the tagging time
per sentence when using the discrete features. This
is mainly due to the dense/sparse battle. Al-
though the dense embedding features are low-
dimensional, the feature vector for each word is
much denser than in the sparse and discrete feature
space. Therefore, we actually need much more
computation during decoding. Similar results can
be observed in the comparison of the DistProto-
type and ClusterEmb features, since the density of
the DistPrototype features is higher. It is possible
</bodyText>
<table confidence="0.954092666666667">
9Statistical significant with p-value &lt; 0.001 by two-tailed
t-test.
116
Setting Time (ms) / sent
Baseline 1.04
+DenseEmb 4.75
+BinarizedEmb 1.25
+ClusterEmb 1.16
+DistPrototype 2.31
</table>
<tableCaption confidence="0.895637">
Table 4: Running time of different features on a
Intel(R) Xeon(R) E5620 2.40GHz machine.
</tableCaption>
<bodyText confidence="0.999729">
to accelerate the DistPrototype, by increasing the
threshold of DistSim(z, w). However, this is in-
deed an issue of trade-off between efficiency and
accuracy.
</bodyText>
<subsectionHeader confidence="0.998801">
5.3 Analysis
</subsectionHeader>
<bodyText confidence="0.9989768">
In this section, we conduct analyses to show the
reasons for the improvements.
very frequent words, while lower sparsity for mid-
frequent words. It indicates that for words that are
very rare or very frequent, BinarizedEmb just omit
most of the features. This is reasonable also for
the very frequent words, since they usually have
rich and diverse context distributions and their
embeddings cannot be well learned by our mod-
els (Huang et al., 2012).
</bodyText>
<figure confidence="0.960064785714286">
●
●
●
●
●
●
●
● ●
●
●
Sparsity
0.50 0.55 0.60 0.65 0.70
256 1k 4k 16k 64k
5.3.1 Rare words
</figure>
<bodyText confidence="0.99528065625">
As discussed by Turian et al. (2010), much of the
NER F1 is derived from decisions regarding rare
words. Therefore, in order to show that the three
proposed embedding features have stronger abil-
ity for handling rare words, we first conduct anal-
ysis for the tagging errors of words with differ-
ent frequency in the unlabeled data. We assign the
word frequencies to several buckets, and evaluate
the per-token errors that occurred in each bucket.
Results are shown in Figure 2. In most cases, all
three embedding features result in fewer errors on
rare words than the direct use of dense continuous
embedding features.
Interestingly, we find that for words that are
extremely rare (0–256), the binarized embedding
features incur significantly fewer errors than other
approaches. As we know, the embeddings for the
rare words are close to their initial value, because
they received few updates during training. Hence,
these words are not fully trained. In this case,
we would like to omit these features because their
embeddings are not even trustable. However, all
embedding features that we proposed except Bi-
narizedEmb are unable to handle this.
In order to see how much we have utilized
the embedding features in BinarizedEmb, we cal-
culate the sparsity of the binarized embedding
vectors, i.e., the ratio of zero values in each
vector (Section 3.2). As demonstrated in Fig-
ure 3, the sparsity-frequency curve has good prop-
erties: higher sparsity for very rare words and
Frequency of word in unlabeled data
</bodyText>
<figureCaption confidence="0.60028425">
Figure 3: Sparsity (with confidence interval) of the
binarized embedding vector w.r.t. word frequency
in the unlabeled data.
Figure 2(b) further supports our analysis. Bina-
</figureCaption>
<bodyText confidence="0.980338272727273">
rizedEmb also reduce much of the errors for the
highly frequent words (32k-64k).
As expected, the distributional prototype fea-
tures produce fewest errors in most cases. The
main reason is that the prototype features are task-
specific. The prototypes are extracted from the
training data and contained indicative information
of the target labels. By contrast, the other em-
bedding features are simply derived from general
word representations and are not specialized for
certain tasks, such as NER.
</bodyText>
<subsectionHeader confidence="0.619452">
5.3.2 Linear Separability
</subsectionHeader>
<bodyText confidence="0.999987">
Another reason for the superiority of the proposed
embedding features is that the high-dimensional
discrete features are more linear separable than
the low-dimensional continuous embeddings. To
verify the hypothesis, we further carry out experi-
ments to analyze the linear separability of the pro-
posed discrete embedding features against dense
continuous embeddings.
We formalize this problem as a binary classi-
fication task, to determine whether a word is an
NE or not (NE identification). The linear support
vector machine (SVM) is used to build the clas-
sifiers, using different embedding features respec-
</bodyText>
<figure confidence="0.979067210526316">
117
Frequency of word in unlabeled data
(a)
Frequency of word in unlabeled data
(b)
0−256 256−512 512−1k 1k−2k
number of per−token errors
0 50 100 150 200 250
DenseEmb
BinarizedEmb
ClusterEmb
DistPrototype
4k−8k 8k−16k 16k−32k 32k−64k
number of per−token errors
40 60 80 100 120
DenseEmb
BinarizedEmb
ClusterEmb
DistPrototype
</figure>
<figureCaption confidence="0.9545115">
Figure 2: The number of per-token errors w.r.t. word frequency in the unlabeled data. (a) For rare words
(frequency ≤ 2k). (b) For frequent words (frequency ≥ 4k).
</figureCaption>
<table confidence="0.999699333333333">
Setting Acc. #features
DenseEmb 95.46 250
BinarizedEmb 94.10 500
ClusterEmb 97.57 482,635
DistPrototype 96.09 1,700
DistPrototype-binary 96.82 4,530
</table>
<tableCaption confidence="0.983469">
Table 5: Performance of the NE/non-NE classi-
</tableCaption>
<bodyText confidence="0.993048">
fication on the CoNLL-2003 development dataset
using different embedding features.
tively. We use the LIBLINEAR tool (Fan et al.,
2008) as our SVM implementation. The penalty
parameter C is tuned from 0.1 to 1.0 on the devel-
opment dataset. The results are shown in Table 5.
As we can see, NEs and non-NEs can be better
separated using ClusterEmb or DistPrototype fea-
tures. However, the BinarizedEmb features per-
form worse than the direct use of word embedding
features. The reason might be inferred from the
third column of Table 5. As demonstrated in Wang
and Manning (2013), linear models are more ef-
fective in high-dimensional and discrete feature
space. The dimension of the BinarizedEmb fea-
tures remains small (500), which is merely twice
the DenseEmb. By contrast, feature dimensions
are much higher for ClusterEmb and DistProto-
type, leading to better linear separability and thus
can be better utilized by linear models.
We notice that the DistPrototype features per-
form significantly worse than ClusterEmb in NE
identification. As described in Section 3.4, in
previous experiments, we automatically extracted
prototypes for each label, and propagated the in-
formation via distributional similarities. Intu-
itively, the prototypes we used should be more ef-
fective in determining fine-grained NE types than
identifying whether a word is an NE. To verify
this, we extract new prototypes considering only
two labels, namely, NE and non-NE, using the
same metric in Section 3.4. As shown in the last
row of Table 5, higher performance is achieved.
</bodyText>
<sectionHeader confidence="0.999003" genericHeader="method">
6 Related Studies
</sectionHeader>
<bodyText confidence="0.999726083333333">
Semi-supervised learning with generalized word
representations is a simple and general way of im-
proving supervised NLP systems. One common
approach for inducing generalized word represen-
tations is to use clustering (e.g., Brown clustering)
(Miller et al., 2004; Liang, 2005; Koo et al., 2008;
Huang and Yates, 2009).
Aside from word clustering, word embeddings
have been widely studied. Bengio et al. (2003)
propose a feed-forward neural network based lan-
guage model (NNLM), which uses an embedding
layer to map each word to a dense continuous-
valued and low-dimensional vector (parameters),
and then use these vectors as the input to predict
the probability distribution of the next word. The
NNLM can be seen as a joint learning framework
for language modeling and word representations.
Alternative models for learning word embed-
dings are mostly inspired by the feed-forward
NNLM, including the Hierarchical Log-Bilinear
Model (Mnih and Hinton, 2008), the recurrent
neural network language model (Mikolov, 2012),
the C&amp;W model (Collobert et al., 2011), the log-
linear models such as the CBOW and the Skip-
</bodyText>
<page confidence="0.812489">
118
</page>
<bodyText confidence="0.997924105263158">
gram model (Mikolov et al., 2013a; Mikolov et
al., 2013b).
Aside from the NNLMs, word embeddings can
also be induced using spectral methods, such as
latent semantic analysis and canonical correlation
analysis (Dhillon et al., 2011). The spectral meth-
ods are generally faster but much more memory-
consuming than NNLMs.
There has been a plenty of work that exploits
word embeddings as features for semi-supervised
learning, most of which take the continuous fea-
tures directly in linear models (Turian et al., 2010;
Guo et al., 2014). Yu et al. (2013) propose com-
pound k-means cluster features based on word em-
beddings. They show that the high-dimensional
discrete cluster features can be better utilized by
linear models such as CRF. Wu et al. (2013) fur-
ther apply the cluster features to transition-based
dependency parsing.
</bodyText>
<sectionHeader confidence="0.964235" genericHeader="conclusions">
7 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999996444444444">
This paper revisits the problem of semi-supervised
learning with word embeddings. We present three
different approaches for a careful comparison and
analysis. Using any of the three embedding fea-
tures, we obtain higher performance than the di-
rect use of continuous embeddings, among which
the distributional prototype features perform the
best, showing the great potential of word embed-
dings. Moreover, the combination of the proposed
embedding features provides significant additive
improvements.
We give detailed analysis about the experimen-
tal results. Analysis on rare words and linear sep-
arability provides convincing explanations for the
performance of the embedding features.
For future work, we are exploring a novel and a
theoretically more sounding approach of introduc-
ing embedding kernel into the linear models.
</bodyText>
<sectionHeader confidence="0.995469" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9992983">
We are grateful to Mo Yu for the fruitful discus-
sion on the implementation of the cluster-based
embedding features. We also thank Ruiji Fu,
Meishan Zhang, Sendong Zhao and the anony-
mous reviewers for their insightful comments and
suggestions. This work was supported by the
National Key Basic Research Program of China
via grant 2014CB340503 and the National Natu-
ral Science Foundation of China (NSFC) via grant
61133012 and 61370164.
</bodyText>
<sectionHeader confidence="0.977926" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999145730769231">
Rie Kubota Ando and Tong Zhang. 2005. A high-
performance semi-supervised learning method for
text chunking. In Proceedings of the 43rd annual
meeting on association for computational linguis-
tics, pages 1–9. Association for Computational Lin-
guistics.
Yoshua Bengio, R. E. Jean Ducharme, Pascal Vincent,
and Christian Janvin. 2003. A neural probabilistic
language model. The Journal of Machine Learning
Research, 3(Feb):1137–1155.
Yoshua Bengio, Aaron Courville, and Pascal Vincent.
2013. Representation learning: A review and new
perspectives. Pattern Analysis and Machine Intelli-
gence, IEEE Transactions on, 35(8):1798–1828.
Gerlof Bouma. 2009. Normalized (pointwise) mutual
information in collocation extraction. Proceedings
of GSCL, pages 31–40.
Peter F Brown, Peter V Desouza, Robert L Mercer,
Vincent J Della Pietra, and Jenifer C Lai. 1992.
Class-based n-gram models of natural language.
Computational linguistics, 18(4):467–479.
Ronan Collobert, Jason Weston, L. E. On Bottou,
Michael Karlen, Koray Kavukcuoglu, and Pavel
Kuksa. 2011. Natural language processing (almost)
from scratch. The Journal of Machine Learning Re-
search, 12:2493–2537.
Paramveer S. Dhillon, Dean P. Foster, and Lyle H. Un-
gar. 2011. Multi-view learning of word embeddings
via cca. In NIPS, volume 24 of NIPS, pages 199–
207.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A
library for large linear classification. The Journal of
Machine Learning Research, 9:1871–1874.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs
sampling. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics,
pages 363–370. Association for Computational Lin-
guistics.
Yoav Goldberg and Omer Levy. 2014. word2vec ex-
plained: deriving mikolov et al.’s negative-sampling
word-embedding method. CoRR, abs/1402.3722.
Jiang Guo, Wanxiang Che, Haifeng Wang, and Ting
Liu. 2014. Learning sense-specific word embed-
dings by exploiting bilingual resources. In Pro-
ceedings of COLING 2014, the 25th International
Conference on Computational Linguistics: Techni-
cal Papers, pages 497–507, Dublin, Ireland, August.
Dublin City University and Association for Compu-
tational Linguistics.
</reference>
<page confidence="0.713129">
119
</page>
<reference confidence="0.999881618181818">
Aria Haghighi and Dan Klein. 2006. Prototype-driven
learning for sequence models. In Proceedings of
the main conference on Human Language Technol-
ogy Conference of the North American Chapter of
the Association of Computational Linguistics, pages
320–327. Association for Computational Linguis-
tics.
Fei Huang and Alexander Yates. 2009. Distribu-
tional representations for handling sparsity in super-
vised sequence-labeling. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on Natu-
ral Language Processing of the AFNLP: Volume 1-
Volume 1, Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the 4th Inter-
national Joint Conference on Natural Language Pro-
cessing of the AFNLP: Volume 1-Volume 1, pages
495–503.
Eric H. Huang, Richard Socher, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Improving word
representations via global context and multiple word
prototypes. In Proc. of the Annual Meeting of the
Association for Computational Linguistics (ACL),
Proc. of the Annual Meeting of the Association for
Computational Linguistics (ACL), pages 873–882,
Jeju Island, Korea. ACL.
Terry Koo, Xavier Carreras, and Michael Collins.
2008. Simple semi-supervised dependency pars-
ing. In Kathleen McKeown, Johanna D. Moore, Si-
mone Teufel, James Allan, and Sadaoki Furui, edi-
tors, Proc. of ACL-08: HLT, Proc. of ACL-08: HLT,
pages 595–603, Columbus, Ohio. ACL.
Vijay Krishnan and Christopher D Manning. 2006.
An effective two-stage model for exploiting non-
local dependencies in named entity recognition. In
Proceedings of the 21st International Conference
on Computational Linguistics and the 44th annual
meeting of the Association for Computational Lin-
guistics, pages 1121–1128. Association for Compu-
tational Linguistics.
Percy Liang. 2005. Semi-supervised learning for natu-
ral language. Master thesis, Massachusetts Institute
of Technology.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word repre-
sentations in vector space. In Proc. of Workshop at
ICLR, Proc. of Workshop at ICLR, Arizona.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S. Cor-
rado, and Jeff Dean. 2013b. Distributed representa-
tions of words and phrases and their compositional-
ity. In Proc. of the NIPS, Proc. of the NIPS, pages
3111–3119, Nevada. MIT Press.
Tomas Mikolov. 2012. Statistical Language Models
Based on Neural Networks. Ph. d. thesis, Brno Uni-
versity of Technology.
Scott Miller, Jethran Guinness, and Alex Zamanian.
2004. Name tagging with word clusters and discrim-
inative training. In HLT-NAACL, volume 4, pages
337–342.
Andriy Mnih and Geoffrey E. Hinton. 2008. A scal-
able hierarchical distributed language model. In
Proc. of the NIPS, Proc. of the NIPS, pages 1081–
1088, Vancouver. MIT Press.
Olutobi Owoputi, Brendan O’Connor, Chris Dyer,
Kevin Gimpel, Nathan Schneider, and Noah A
Smith. 2013. Improved part-of-speech tagging for
online conversational text with word clusters. In
Proceedings of NAACL-HLT, pages 380–390.
Lev Ratinov and Dan Roth. 2009. Design challenges
and misconceptions in named entity recognition. In
Proceedings of the Thirteenth Conference on Com-
putational Natural Language Learning, CoNLL ’09,
pages 147–155, Stroudsburg, PA, USA. Association
for Computational Linguistics.
D Sculley. 2010. Combined regression and ranking.
In Proceedings of the 16th ACM SIGKDD interna-
tional conference on Knowledge discovery and data
mining, pages 979–988. ACM.
Erik F Tjong Kim Sang and Fien De Meulder.
2003. Introduction to the conll-2003 shared task:
Language-independent named entity recognition. In
Proceedings of the seventh conference on Natural
language learning at HLT-NAACL 2003-Volume 4,
pages 142–147. Association for Computational Lin-
guistics.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In Jan Hajic, San-
dra Carberry, and Stephen Clark, editors, Proc. of
the Annual Meeting of the Association for Computa-
tional Linguistics (ACL), Proc. of the Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 384–394, Uppsala, Sweden. ACL.
Mengqiu Wang and Christopher D. Manning. 2013.
Effect of non-linear deep architecture in sequence la-
beling. In Proc. of the Sixth International Joint Con-
ference on Natural Language Processing, Proc. of
the Sixth International Joint Conference on Natural
Language Processing, pages 1285–1291, Nagoya,
Japan. Asian Federation of Natural Language Pro-
cessing.
Xianchao Wu, Jie Zhou, Yu Sun, Zhanyi Liu, Dian-
hai Yu, Hua Wu, and Haifeng Wang. 2013. Gener-
alization of words for chinese dependency parsing.
IWPT-2013, page 73.
Mo Yu, Tiejun Zhao, Daxiang Dong, Hao Tian, and Di-
anhai Yu. 2013. Compound embedding features for
semi-supervised learning. In Proc. of the NAACL-
HLT, Proc. of the NAACL-HLT, pages 563–568, At-
lanta. NAACL.
</reference>
<page confidence="0.821119">
120
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.636856">
<title confidence="0.999171">Revisiting Embedding Features for Simple Semi-supervised Learning</title>
<author confidence="0.860736">Wanxiang Haifeng Ting</author>
<affiliation confidence="0.9977325">Center for Social Computing and Information Harbin Institute of Technology,</affiliation>
<address confidence="0.961787">Inc., Beijing,</address>
<email confidence="0.912279">car,wanghaifeng@baidu.com</email>
<abstract confidence="0.996792038461538">Recent work has shown success in using continuous word embeddings learned from unlabeled data as features to improve supervised NLP systems, which is regarded as a simple semi-supervised learning mechanism. However, fundamental problems on effectively incorporating the word embedding features within the framework of linear models remain. In this study, we investigate and analyze three different approaches, including a new proprototype for utilizing the embedding features. The presented approaches can be integrated into most of the classical linear models in NLP. Experiments on the task of named entity recognition show that each of the proposed approaches can better utilize the word embedding features, among which prototype performs the best. Moreover, the combination of the approaches provides additive improvements, outperforming the dense and continuous embedding features by nearly 2 points of F1 score.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Rie Kubota Ando</author>
<author>Tong Zhang</author>
</authors>
<title>A highperformance semi-supervised learning method for text chunking.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd annual meeting on association for computational linguistics,</booktitle>
<pages>1--9</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="24926" citStr="Ando and Zhang (2005)" startWordPosition="4038" endWordPosition="4041">deed cannot fully 5nlp.stanford.edu/software/tokenizer. shtml. 6code.google.com/p/word2vec/. 7More details are analyzed in (Goldberg and Levy, 2014). 8We use BIO encoding here in order to compare with most of the reported benchmarks. 115 Setting F1 Baseline 83.43 +DenseEmb† 86.21 +BinarizedEmb 86.75 +ClusterEmb 86.90 +DistPrototype 87.44 +BinarizedEmb+ClusterEmb 87.56 +BinarizedEmb+DistPrototype 87.46 +ClusterEmb+DistPrototype 88.11 +Brown 87.49 +Brown+ClusterEmb 88.17 +Brown+DistPrototype 88.04 +Brown+ClusterEmb+DistPrototype 88.58 Finkel et al. (2005) 86.86 Krishnan and Manning (2006) 87.24 Ando and Zhang (2005) 89.31 Collobert et al. (2011) 88.67 Table 3: The performance of semi-supervised NER on the CoNLL-2003 test data, using various embedding features. † DenseEmb refers to the method used by Turian et al. (2010), i.e., the direct use of the dense and continuous embeddings. exploit the potential of word embeddings. The compound cluster features (ClusterEmb) also outperform the DenseEmb. The same result is also shown in (Yu et al., 2013). Further, the distributional prototype features (DistPrototype) achieve the best performance among the three approaches (1.23% higher than DenseEmb). We should not</context>
<context position="27454" citStr="Ando and Zhang (2005)" startWordPosition="4420" endWordPosition="4423">expressing power than the Brown clusters, as desired. Finally, by combining the Brown cluster features and the proposed embedding features, the performance can be improved further (88.58%). The binarized embedding features are not included in the final compound features because they are almost overlapped with the distributional prototype features in performance. We also summarize some of the reported benchmarks that utilize unlabeled data (with no gazetteers used), including the Stanford NER tagger (Finkel et al. (2005) and Krishnan and Manning (2006)) with distributional similarity features. Ando and Zhang (2005) use unlabeled data for constructing auxiliary problems that are expected to capture a good feature representation of the target problem. Collobert et al. (2011) adjust the feature embeddings according to the specific task in a deep neural network architecture. We can see that both Ando and Zhang (2005) and Collobert et al. (2011) learn task-specific lexical features, which is similar to the proposed distributional prototype method in our study. We suggest this to be the main reason for the superiority of these methods. Another advantage of the proposed discrete features over the dense continu</context>
</contexts>
<marker>Ando, Zhang, 2005</marker>
<rawString>Rie Kubota Ando and Tong Zhang. 2005. A highperformance semi-supervised learning method for text chunking. In Proceedings of the 43rd annual meeting on association for computational linguistics, pages 1–9. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>R E Jean Ducharme</author>
<author>Pascal Vincent</author>
<author>Christian Janvin</author>
</authors>
<title>A neural probabilistic language model.</title>
<date>2003</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>3--1137</pages>
<contexts>
<context position="8299" citStr="Bengio et al., 2003" startWordPosition="1234" endWordPosition="1237">2) from unlabeled data and use them as features to promote the supervised NER and Chinese word segmentation. Brown clusters of words can be seen as a generalized word representation distributed in a discrete and low-dimensional vocabulary space. Contextually similar words are grouped in the same cluster. The Brown clustering of words was also adopted in dependency parsing (Koo et al., 2008) and POS tagging for online conversational text (Owoputi et al., 2013), demonstrating significant improvements. Recently, another kind of word representation named “word embeddings” has been widely studied (Bengio et al., 2003; Mnih and Hinton, 2008). Using word embeddings, we can evaluate the similarity of two words straightforward by computing the dot-product of two numerical vectors in the Hilbert space. Two similar words are expected to 111 be distributed close to each other.2 Word embeddings can be useful as input to an NLP model (mostly non-linear) or as additional features to enhance existing systems. Collobert et al. (2011) used word embeddings as input to a deep neural network for multi-task learning. Despite of the effectiveness, such non-linear models are hard to build and optimize. Besides, these archit</context>
<context position="35149" citStr="Bengio et al. (2003)" startWordPosition="5647" endWordPosition="5650">ify this, we extract new prototypes considering only two labels, namely, NE and non-NE, using the same metric in Section 3.4. As shown in the last row of Table 5, higher performance is achieved. 6 Related Studies Semi-supervised learning with generalized word representations is a simple and general way of improving supervised NLP systems. One common approach for inducing generalized word representations is to use clustering (e.g., Brown clustering) (Miller et al., 2004; Liang, 2005; Koo et al., 2008; Huang and Yates, 2009). Aside from word clustering, word embeddings have been widely studied. Bengio et al. (2003) propose a feed-forward neural network based language model (NNLM), which uses an embedding layer to map each word to a dense continuousvalued and low-dimensional vector (parameters), and then use these vectors as the input to predict the probability distribution of the next word. The NNLM can be seen as a joint learning framework for language modeling and word representations. Alternative models for learning word embeddings are mostly inspired by the feed-forward NNLM, including the Hierarchical Log-Bilinear Model (Mnih and Hinton, 2008), the recurrent neural network language model (Mikolov, </context>
</contexts>
<marker>Bengio, Ducharme, Vincent, Janvin, 2003</marker>
<rawString>Yoshua Bengio, R. E. Jean Ducharme, Pascal Vincent, and Christian Janvin. 2003. A neural probabilistic language model. The Journal of Machine Learning Research, 3(Feb):1137–1155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>Aaron Courville</author>
<author>Pascal Vincent</author>
</authors>
<title>Representation learning: A review and new perspectives. Pattern Analysis and Machine Intelligence,</title>
<date>2013</date>
<journal>IEEE Transactions on,</journal>
<volume>35</volume>
<issue>8</issue>
<contexts>
<context position="4195" citStr="Bengio et al., 2013" startWordPosition="617" endWordPosition="620">ability distributions (e.g., loglinear models). 110 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 110–120, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics al. (2010), the embedding features brought significantly less improvement than Brown clustering features. This result is actually not reasonable because the expressing power of word embeddings is theoretically stronger than clustering-based representations which can be regarded as a kind of one-hot representation but over a low-dimensional vocabulary (Bengio et al., 2013). Wang and Manning (2013) showed that linear architectures perform better in high-dimensional discrete feature space than non-linear ones, whereas non-linear architectures are more effective in low-dimensional and continuous feature space. Hence, the previous method that directly uses the continuous word embeddings as features in linear models (CRF) is inappropriate. Word embeddings may be better utilized in the linear modeling framework by smartly transforming the embeddings to some relatively higher dimensional and discrete representations. Driven by this motivation, we present three differe</context>
</contexts>
<marker>Bengio, Courville, Vincent, 2013</marker>
<rawString>Yoshua Bengio, Aaron Courville, and Pascal Vincent. 2013. Representation learning: A review and new perspectives. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 35(8):1798–1828.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerlof Bouma</author>
</authors>
<title>Normalized (pointwise) mutual information in collocation extraction.</title>
<date>2009</date>
<booktitle>Proceedings of GSCL,</booktitle>
<pages>31--40</pages>
<contexts>
<context position="14449" citStr="Bouma, 2009" startWordPosition="2245" endWordPosition="2246">reat sense in tasks such as NER and POS tagging. For example, suppose Michael is a prototype of the named entity (NE) type PER. Using the distributional similarity, we could link similar words to the same prototypes, so the word David can be linked to Michael because the two words have high similarity (exceeds a threshold). Using this link feature, the model will push David closer to PER. To derive the distributional prototype features, first, we need to construct a few canonical examples (prototypes) for each target annotation label. We use the normalized pointwise mutual information (NPMI) (Bouma, 2009) between the label and word, which is a smoothing version of the standard PMI, to decide the prototypes of each label. Given the annotated training corpus, the NPMI between a label and word is computed as follows: λ(label, word) λn(label, word) = (3) − ln p(label, word) 3code.google.com/p/sofia-ml NE Type Prototypes B-PER Mark, Michael, David, Paul I-PER Akram, Ahmed, Khan, Younis B-ORG Reuters, U.N., Ajax, PSV I-ORG Newsroom, Inc, Corp, Party B-LOC U.S., Germany, Britain, Australia I-LOC States, Republic, Africa, Lanka B-MISC Russian, German, French, British I-MISC Cup, Open, League, OPEN O.,</context>
</contexts>
<marker>Bouma, 2009</marker>
<rawString>Gerlof Bouma. 2009. Normalized (pointwise) mutual information in collocation extraction. Proceedings of GSCL, pages 31–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Peter V Desouza</author>
<author>Robert L Mercer</author>
<author>Vincent J Della Pietra</author>
<author>Jenifer C Lai</author>
</authors>
<title>Class-based n-gram models of natural language.</title>
<date>1992</date>
<journal>Computational linguistics,</journal>
<volume>18</volume>
<issue>4</issue>
<contexts>
<context position="7682" citStr="Brown et al., 1992" startWordPosition="1139" endWordPosition="1142">rresponding parameters usually cannot be fully trained. More foundationally, the reason for the above factors lies in the high-dimensional and sparse lexical feature representation, which completely ignores the similarity between features, especially word features. To overcome this weakness, an effective way is to learn more generalized representations of words by exploiting the numerous unlabeled data, in a semi-supervised manner. After which, the generalized word representations can be used as extra features to facilitate the supervised systems. Liang (2005) learned Brown clusters of words (Brown et al., 1992) from unlabeled data and use them as features to promote the supervised NER and Chinese word segmentation. Brown clusters of words can be seen as a generalized word representation distributed in a discrete and low-dimensional vocabulary space. Contextually similar words are grouped in the same cluster. The Brown clustering of words was also adopted in dependency parsing (Koo et al., 2008) and POS tagging for online conversational text (Owoputi et al., 2013), demonstrating significant improvements. Recently, another kind of word representation named “word embeddings” has been widely studied (Be</context>
<context position="21478" citStr="Brown et al., 1992" startWordPosition="3494" endWordPosition="3497">atures varies for different words (positions). Hence, these features cannot be combined effectively. 4.2 Brown Clustering Brown clustering has achieved great success in various NLP applications. At most time, it provides a strong baseline that is difficult to beat (Turian et al., 2010). Consequently, in our study, we conduct comparisons among the embedding features and the Brown clustering features, along with further investigations of their combination. The Brown algorithm is a hierarchical clustering algorithm which optimizes a class-based bigram language model defined on the word clusters (Brown et al., 1992). The output of the Brown algorithm is a binary tree, where each word is uniquely identified by its path from the root. Thus each word can be represented as a bit-string with a specific length. Following the setting of Owoputi et al. (2013), we will use the prefix features of hierarchical clusters to take advantage of the word similarity in different granularities. Concretely, the Brown cluster feature template is: • bci+k, −2 &lt; k &lt; 2. • prefix(bci+k, p), p E {2,4,6,...,16}, −2 &lt; k &lt; 2. prefix takes the p-length prefix of the Brown cluster coding bci+k. 5 Experiments 5.1 Experimental Setting W</context>
</contexts>
<marker>Brown, Desouza, Mercer, Pietra, Lai, 1992</marker>
<rawString>Peter F Brown, Peter V Desouza, Robert L Mercer, Vincent J Della Pietra, and Jenifer C Lai. 1992. Class-based n-gram models of natural language. Computational linguistics, 18(4):467–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
<author>L E On Bottou</author>
<author>Michael Karlen</author>
<author>Koray Kavukcuoglu</author>
<author>Pavel Kuksa</author>
</authors>
<title>Natural language processing (almost) from scratch.</title>
<date>2011</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>12--2493</pages>
<contexts>
<context position="8712" citStr="Collobert et al. (2011)" startWordPosition="1302" endWordPosition="1305"> for online conversational text (Owoputi et al., 2013), demonstrating significant improvements. Recently, another kind of word representation named “word embeddings” has been widely studied (Bengio et al., 2003; Mnih and Hinton, 2008). Using word embeddings, we can evaluate the similarity of two words straightforward by computing the dot-product of two numerical vectors in the Hilbert space. Two similar words are expected to 111 be distributed close to each other.2 Word embeddings can be useful as input to an NLP model (mostly non-linear) or as additional features to enhance existing systems. Collobert et al. (2011) used word embeddings as input to a deep neural network for multi-task learning. Despite of the effectiveness, such non-linear models are hard to build and optimize. Besides, these architectures are often specialized for a certain task and not scalable to general tasks. A simple and more general way is to feed word embeddings as augmented features to an existing supervised system, which is similar to the semi-supervised learning with Brown clusters. As discussed in Section 1, Turian et al. (2010) is the pioneering work on using word embedding features for semi-supervised learning. However, the</context>
<context position="24956" citStr="Collobert et al. (2011)" startWordPosition="4043" endWordPosition="4046">ord.edu/software/tokenizer. shtml. 6code.google.com/p/word2vec/. 7More details are analyzed in (Goldberg and Levy, 2014). 8We use BIO encoding here in order to compare with most of the reported benchmarks. 115 Setting F1 Baseline 83.43 +DenseEmb† 86.21 +BinarizedEmb 86.75 +ClusterEmb 86.90 +DistPrototype 87.44 +BinarizedEmb+ClusterEmb 87.56 +BinarizedEmb+DistPrototype 87.46 +ClusterEmb+DistPrototype 88.11 +Brown 87.49 +Brown+ClusterEmb 88.17 +Brown+DistPrototype 88.04 +Brown+ClusterEmb+DistPrototype 88.58 Finkel et al. (2005) 86.86 Krishnan and Manning (2006) 87.24 Ando and Zhang (2005) 89.31 Collobert et al. (2011) 88.67 Table 3: The performance of semi-supervised NER on the CoNLL-2003 test data, using various embedding features. † DenseEmb refers to the method used by Turian et al. (2010), i.e., the direct use of the dense and continuous embeddings. exploit the potential of word embeddings. The compound cluster features (ClusterEmb) also outperform the DenseEmb. The same result is also shown in (Yu et al., 2013). Further, the distributional prototype features (DistPrototype) achieve the best performance among the three approaches (1.23% higher than DenseEmb). We should note that the feature templates u</context>
<context position="27615" citStr="Collobert et al. (2011)" startWordPosition="4445" endWordPosition="4448"> be improved further (88.58%). The binarized embedding features are not included in the final compound features because they are almost overlapped with the distributional prototype features in performance. We also summarize some of the reported benchmarks that utilize unlabeled data (with no gazetteers used), including the Stanford NER tagger (Finkel et al. (2005) and Krishnan and Manning (2006)) with distributional similarity features. Ando and Zhang (2005) use unlabeled data for constructing auxiliary problems that are expected to capture a good feature representation of the target problem. Collobert et al. (2011) adjust the feature embeddings according to the specific task in a deep neural network architecture. We can see that both Ando and Zhang (2005) and Collobert et al. (2011) learn task-specific lexical features, which is similar to the proposed distributional prototype method in our study. We suggest this to be the main reason for the superiority of these methods. Another advantage of the proposed discrete features over the dense continuous features is tagging efficiency. Table 4 shows the running time using different kinds of embedding features. We achieve a significant reduction of the tagging</context>
<context position="35794" citStr="Collobert et al., 2011" startWordPosition="5746" endWordPosition="5749">d neural network based language model (NNLM), which uses an embedding layer to map each word to a dense continuousvalued and low-dimensional vector (parameters), and then use these vectors as the input to predict the probability distribution of the next word. The NNLM can be seen as a joint learning framework for language modeling and word representations. Alternative models for learning word embeddings are mostly inspired by the feed-forward NNLM, including the Hierarchical Log-Bilinear Model (Mnih and Hinton, 2008), the recurrent neural network language model (Mikolov, 2012), the C&amp;W model (Collobert et al., 2011), the loglinear models such as the CBOW and the Skip118 gram model (Mikolov et al., 2013a; Mikolov et al., 2013b). Aside from the NNLMs, word embeddings can also be induced using spectral methods, such as latent semantic analysis and canonical correlation analysis (Dhillon et al., 2011). The spectral methods are generally faster but much more memoryconsuming than NNLMs. There has been a plenty of work that exploits word embeddings as features for semi-supervised learning, most of which take the continuous features directly in linear models (Turian et al., 2010; Guo et al., 2014). Yu et al. (20</context>
</contexts>
<marker>Collobert, Weston, Bottou, Karlen, Kavukcuoglu, Kuksa, 2011</marker>
<rawString>Ronan Collobert, Jason Weston, L. E. On Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural language processing (almost) from scratch. The Journal of Machine Learning Research, 12:2493–2537.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paramveer S Dhillon</author>
<author>Dean P Foster</author>
<author>Lyle H Ungar</author>
</authors>
<title>Multi-view learning of word embeddings via cca. In</title>
<date>2011</date>
<booktitle>NIPS,</booktitle>
<volume>24</volume>
<pages>199--207</pages>
<contexts>
<context position="36081" citStr="Dhillon et al., 2011" startWordPosition="5794" endWordPosition="5797">nt learning framework for language modeling and word representations. Alternative models for learning word embeddings are mostly inspired by the feed-forward NNLM, including the Hierarchical Log-Bilinear Model (Mnih and Hinton, 2008), the recurrent neural network language model (Mikolov, 2012), the C&amp;W model (Collobert et al., 2011), the loglinear models such as the CBOW and the Skip118 gram model (Mikolov et al., 2013a; Mikolov et al., 2013b). Aside from the NNLMs, word embeddings can also be induced using spectral methods, such as latent semantic analysis and canonical correlation analysis (Dhillon et al., 2011). The spectral methods are generally faster but much more memoryconsuming than NNLMs. There has been a plenty of work that exploits word embeddings as features for semi-supervised learning, most of which take the continuous features directly in linear models (Turian et al., 2010; Guo et al., 2014). Yu et al. (2013) propose compound k-means cluster features based on word embeddings. They show that the high-dimensional discrete cluster features can be better utilized by linear models such as CRF. Wu et al. (2013) further apply the cluster features to transition-based dependency parsing. 7 Conclu</context>
</contexts>
<marker>Dhillon, Foster, Ungar, 2011</marker>
<rawString>Paramveer S. Dhillon, Dean P. Foster, and Lyle H. Ungar. 2011. Multi-view learning of word embeddings via cca. In NIPS, volume 24 of NIPS, pages 199– 207.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rong-En Fan</author>
<author>Kai-Wei Chang</author>
<author>Cho-Jui Hsieh</author>
<author>XiangRui Wang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>Liblinear: A library for large linear classification.</title>
<date>2008</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>9--1871</pages>
<contexts>
<context position="33310" citStr="Fan et al., 2008" startWordPosition="5354" endWordPosition="5357">type 4k−8k 8k−16k 16k−32k 32k−64k number of per−token errors 40 60 80 100 120 DenseEmb BinarizedEmb ClusterEmb DistPrototype Figure 2: The number of per-token errors w.r.t. word frequency in the unlabeled data. (a) For rare words (frequency ≤ 2k). (b) For frequent words (frequency ≥ 4k). Setting Acc. #features DenseEmb 95.46 250 BinarizedEmb 94.10 500 ClusterEmb 97.57 482,635 DistPrototype 96.09 1,700 DistPrototype-binary 96.82 4,530 Table 5: Performance of the NE/non-NE classification on the CoNLL-2003 development dataset using different embedding features. tively. We use the LIBLINEAR tool (Fan et al., 2008) as our SVM implementation. The penalty parameter C is tuned from 0.1 to 1.0 on the development dataset. The results are shown in Table 5. As we can see, NEs and non-NEs can be better separated using ClusterEmb or DistPrototype features. However, the BinarizedEmb features perform worse than the direct use of word embedding features. The reason might be inferred from the third column of Table 5. As demonstrated in Wang and Manning (2013), linear models are more effective in high-dimensional and discrete feature space. The dimension of the BinarizedEmb features remains small (500), which is mere</context>
</contexts>
<marker>Fan, Chang, Hsieh, Wang, Lin, 2008</marker>
<rawString>Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, XiangRui Wang, and Chih-Jen Lin. 2008. Liblinear: A library for large linear classification. The Journal of Machine Learning Research, 9:1871–1874.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Trond Grenager</author>
<author>Christopher Manning</author>
</authors>
<title>Incorporating non-local information into information extraction systems by gibbs sampling.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>363--370</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="24864" citStr="Finkel et al. (2005)" startWordPosition="4028" endWordPosition="4031">y using the dense continuous embeddings as features in CRF indeed cannot fully 5nlp.stanford.edu/software/tokenizer. shtml. 6code.google.com/p/word2vec/. 7More details are analyzed in (Goldberg and Levy, 2014). 8We use BIO encoding here in order to compare with most of the reported benchmarks. 115 Setting F1 Baseline 83.43 +DenseEmb† 86.21 +BinarizedEmb 86.75 +ClusterEmb 86.90 +DistPrototype 87.44 +BinarizedEmb+ClusterEmb 87.56 +BinarizedEmb+DistPrototype 87.46 +ClusterEmb+DistPrototype 88.11 +Brown 87.49 +Brown+ClusterEmb 88.17 +Brown+DistPrototype 88.04 +Brown+ClusterEmb+DistPrototype 88.58 Finkel et al. (2005) 86.86 Krishnan and Manning (2006) 87.24 Ando and Zhang (2005) 89.31 Collobert et al. (2011) 88.67 Table 3: The performance of semi-supervised NER on the CoNLL-2003 test data, using various embedding features. † DenseEmb refers to the method used by Turian et al. (2010), i.e., the direct use of the dense and continuous embeddings. exploit the potential of word embeddings. The compound cluster features (ClusterEmb) also outperform the DenseEmb. The same result is also shown in (Yu et al., 2013). Further, the distributional prototype features (DistPrototype) achieve the best performance among th</context>
<context position="27358" citStr="Finkel et al. (2005)" startWordPosition="4405" endWordPosition="4408">ers. This result is inspiring because we show that the embedding features indeed have stronger expressing power than the Brown clusters, as desired. Finally, by combining the Brown cluster features and the proposed embedding features, the performance can be improved further (88.58%). The binarized embedding features are not included in the final compound features because they are almost overlapped with the distributional prototype features in performance. We also summarize some of the reported benchmarks that utilize unlabeled data (with no gazetteers used), including the Stanford NER tagger (Finkel et al. (2005) and Krishnan and Manning (2006)) with distributional similarity features. Ando and Zhang (2005) use unlabeled data for constructing auxiliary problems that are expected to capture a good feature representation of the target problem. Collobert et al. (2011) adjust the feature embeddings according to the specific task in a deep neural network architecture. We can see that both Ando and Zhang (2005) and Collobert et al. (2011) learn task-specific lexical features, which is similar to the proposed distributional prototype method in our study. We suggest this to be the main reason for the superior</context>
</contexts>
<marker>Finkel, Grenager, Manning, 2005</marker>
<rawString>Jenny Rose Finkel, Trond Grenager, and Christopher Manning. 2005. Incorporating non-local information into information extraction systems by gibbs sampling. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 363–370. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Goldberg</author>
<author>Omer Levy</author>
</authors>
<title>word2vec explained: deriving mikolov et al.’s negative-sampling word-embedding method.</title>
<date>2014</date>
<location>CoRR, abs/1402.3722.</location>
<contexts>
<context position="24453" citStr="Goldberg and Levy, 2014" startWordPosition="3982" endWordPosition="3985">s which outperforms BIO encoding (Ratinov and Roth, 2009).8 Nonetheless, our conclusions hold. As we can see, all of the three approaches we investigate in this study achieve better performance than the direct use of the dense continuous embedding features. To our surprise, even the binarized embedding features (BinarizedEmb) outperform the continuous version (DenseEmb). This provides clear evidence that directly using the dense continuous embeddings as features in CRF indeed cannot fully 5nlp.stanford.edu/software/tokenizer. shtml. 6code.google.com/p/word2vec/. 7More details are analyzed in (Goldberg and Levy, 2014). 8We use BIO encoding here in order to compare with most of the reported benchmarks. 115 Setting F1 Baseline 83.43 +DenseEmb† 86.21 +BinarizedEmb 86.75 +ClusterEmb 86.90 +DistPrototype 87.44 +BinarizedEmb+ClusterEmb 87.56 +BinarizedEmb+DistPrototype 87.46 +ClusterEmb+DistPrototype 88.11 +Brown 87.49 +Brown+ClusterEmb 88.17 +Brown+DistPrototype 88.04 +Brown+ClusterEmb+DistPrototype 88.58 Finkel et al. (2005) 86.86 Krishnan and Manning (2006) 87.24 Ando and Zhang (2005) 89.31 Collobert et al. (2011) 88.67 Table 3: The performance of semi-supervised NER on the CoNLL-2003 test data, using various</context>
</contexts>
<marker>Goldberg, Levy, 2014</marker>
<rawString>Yoav Goldberg and Omer Levy. 2014. word2vec explained: deriving mikolov et al.’s negative-sampling word-embedding method. CoRR, abs/1402.3722.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiang Guo</author>
<author>Wanxiang Che</author>
<author>Haifeng Wang</author>
<author>Ting Liu</author>
</authors>
<title>Learning sense-specific word embeddings by exploiting bilingual resources.</title>
<date>2014</date>
<booktitle>In Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,</booktitle>
<pages>497--507</pages>
<institution>Dublin City University and Association for Computational Linguistics.</institution>
<location>Dublin, Ireland,</location>
<contexts>
<context position="36379" citStr="Guo et al., 2014" startWordPosition="5844" endWordPosition="5847">W model (Collobert et al., 2011), the loglinear models such as the CBOW and the Skip118 gram model (Mikolov et al., 2013a; Mikolov et al., 2013b). Aside from the NNLMs, word embeddings can also be induced using spectral methods, such as latent semantic analysis and canonical correlation analysis (Dhillon et al., 2011). The spectral methods are generally faster but much more memoryconsuming than NNLMs. There has been a plenty of work that exploits word embeddings as features for semi-supervised learning, most of which take the continuous features directly in linear models (Turian et al., 2010; Guo et al., 2014). Yu et al. (2013) propose compound k-means cluster features based on word embeddings. They show that the high-dimensional discrete cluster features can be better utilized by linear models such as CRF. Wu et al. (2013) further apply the cluster features to transition-based dependency parsing. 7 Conclusion and Future Work This paper revisits the problem of semi-supervised learning with word embeddings. We present three different approaches for a careful comparison and analysis. Using any of the three embedding features, we obtain higher performance than the direct use of continuous embeddings, </context>
</contexts>
<marker>Guo, Che, Wang, Liu, 2014</marker>
<rawString>Jiang Guo, Wanxiang Che, Haifeng Wang, and Ting Liu. 2014. Learning sense-specific word embeddings by exploiting bilingual resources. In Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 497–507, Dublin, Ireland, August. Dublin City University and Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Dan Klein</author>
</authors>
<title>Prototype-driven learning for sequence models.</title>
<date>2006</date>
<booktitle>In Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics,</booktitle>
<pages>320--327</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="13321" citStr="Haghighi and Klein, 2006" startWordPosition="2063" endWordPosition="2066">means clustering algorithm (Sculley, 2010) is used,3 and each cluster is represented as the mean of the embeddings of words assigned to it. Similarities between words and clusters are measured by Euclidean distance. Moreover, different number of clusters n contain information of different granularities. Therefore, we combine the cluster features of different ns to better utilize the embeddings. 3.4 Distributional Prototype Features We propose a novel kind of embedding features, named distributional prototype features for supervised models. This is mainly inspired by prototype-driven learning (Haghighi and Klein, 2006) which was originally introduced as a primarily unsupervised approach for sequence modeling. In prototype-driven learning, a few prototypical examples are specified for each target label, which can be treated as an injection of prior knowledge. This sparse prototype information is then propagated across an unlabeled corpus through distributional similarities. The basic motivation of the distributional prototype features is that similar words are supposed to be tagged with the same label. This hypothesis makes great sense in tasks such as NER and POS tagging. For example, suppose Michael is a p</context>
</contexts>
<marker>Haghighi, Klein, 2006</marker>
<rawString>Aria Haghighi and Dan Klein. 2006. Prototype-driven learning for sequence models. In Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics, pages 320–327. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Fei Huang</author>
<author>Alexander Yates</author>
</authors>
<title>Distributional representations for handling sparsity in supervised sequence-labeling.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 1-Volume 1, Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume</booktitle>
<volume>1</volume>
<pages>495--503</pages>
<contexts>
<context position="35057" citStr="Huang and Yates, 2009" startWordPosition="5633" endWordPosition="5636">ffective in determining fine-grained NE types than identifying whether a word is an NE. To verify this, we extract new prototypes considering only two labels, namely, NE and non-NE, using the same metric in Section 3.4. As shown in the last row of Table 5, higher performance is achieved. 6 Related Studies Semi-supervised learning with generalized word representations is a simple and general way of improving supervised NLP systems. One common approach for inducing generalized word representations is to use clustering (e.g., Brown clustering) (Miller et al., 2004; Liang, 2005; Koo et al., 2008; Huang and Yates, 2009). Aside from word clustering, word embeddings have been widely studied. Bengio et al. (2003) propose a feed-forward neural network based language model (NNLM), which uses an embedding layer to map each word to a dense continuousvalued and low-dimensional vector (parameters), and then use these vectors as the input to predict the probability distribution of the next word. The NNLM can be seen as a joint learning framework for language modeling and word representations. Alternative models for learning word embeddings are mostly inspired by the feed-forward NNLM, including the Hierarchical Log-Bi</context>
</contexts>
<marker>Huang, Yates, 2009</marker>
<rawString>Fei Huang and Alexander Yates. 2009. Distributional representations for handling sparsity in supervised sequence-labeling. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 1-Volume 1, Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 1-Volume 1, pages 495–503.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric H Huang</author>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Improving word representations via global context and multiple word prototypes.</title>
<date>2012</date>
<booktitle>In Proc. of the Annual Meeting of the Association for Computational Linguistics (ACL), Proc. of the Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>873--882</pages>
<publisher>ACL.</publisher>
<location>Jeju Island,</location>
<contexts>
<context position="29595" citStr="Huang et al., 2012" startWordPosition="4762" endWordPosition="4765">to accelerate the DistPrototype, by increasing the threshold of DistSim(z, w). However, this is indeed an issue of trade-off between efficiency and accuracy. 5.3 Analysis In this section, we conduct analyses to show the reasons for the improvements. very frequent words, while lower sparsity for midfrequent words. It indicates that for words that are very rare or very frequent, BinarizedEmb just omit most of the features. This is reasonable also for the very frequent words, since they usually have rich and diverse context distributions and their embeddings cannot be well learned by our models (Huang et al., 2012). ● ● ● ● ● ● ● ● ● ● ● Sparsity 0.50 0.55 0.60 0.65 0.70 256 1k 4k 16k 64k 5.3.1 Rare words As discussed by Turian et al. (2010), much of the NER F1 is derived from decisions regarding rare words. Therefore, in order to show that the three proposed embedding features have stronger ability for handling rare words, we first conduct analysis for the tagging errors of words with different frequency in the unlabeled data. We assign the word frequencies to several buckets, and evaluate the per-token errors that occurred in each bucket. Results are shown in Figure 2. In most cases, all three embeddi</context>
</contexts>
<marker>Huang, Socher, Manning, Ng, 2012</marker>
<rawString>Eric H. Huang, Richard Socher, Christopher D. Manning, and Andrew Y. Ng. 2012. Improving word representations via global context and multiple word prototypes. In Proc. of the Annual Meeting of the Association for Computational Linguistics (ACL), Proc. of the Annual Meeting of the Association for Computational Linguistics (ACL), pages 873–882, Jeju Island, Korea. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Koo</author>
<author>Xavier Carreras</author>
<author>Michael Collins</author>
</authors>
<title>Simple semi-supervised dependency parsing.</title>
<date>2008</date>
<booktitle>Proc. of ACL-08: HLT, Proc. of ACL-08: HLT,</booktitle>
<pages>595--603</pages>
<editor>In Kathleen McKeown, Johanna D. Moore, Simone Teufel, James Allan, and Sadaoki Furui, editors,</editor>
<publisher>ACL.</publisher>
<location>Columbus, Ohio.</location>
<contexts>
<context position="1690" citStr="Koo et al. (2008)" startWordPosition="242" endWordPosition="245">tributional prototype approach performs the best. Moreover, the combination of the approaches provides additive improvements, outperforming the dense and continuous embedding features by nearly 2 points of F1 score. 1 Introduction Learning generalized representation of words is an effective way of handling data sparsity caused by high-dimensional lexical features in NLP systems, such as named entity recognition (NER) and dependency parsing. As a typical lowdimensional and generalized word representation, Brown clustering of words has been studied for a long time. For example, Liang (2005) and Koo et al. (2008) used the Brown cluster features for semi-supervised learning of various NLP tasks and achieved significant improvements. ∗Email correspondence. Recent research has focused on a special family of word representations, named “word embeddings”. Word embeddings are conventionally defined as dense, continuous, and low-dimensional vector representations of words. Word embeddings can be learned from large-scale unlabeled texts through context-predicting models (e.g., neural network language models) or spectral methods (e.g., canonical correlation analysis) in an unsupervised setting. Compared with t</context>
<context position="8073" citStr="Koo et al., 2008" startWordPosition="1201" endWordPosition="1204">labeled data, in a semi-supervised manner. After which, the generalized word representations can be used as extra features to facilitate the supervised systems. Liang (2005) learned Brown clusters of words (Brown et al., 1992) from unlabeled data and use them as features to promote the supervised NER and Chinese word segmentation. Brown clusters of words can be seen as a generalized word representation distributed in a discrete and low-dimensional vocabulary space. Contextually similar words are grouped in the same cluster. The Brown clustering of words was also adopted in dependency parsing (Koo et al., 2008) and POS tagging for online conversational text (Owoputi et al., 2013), demonstrating significant improvements. Recently, another kind of word representation named “word embeddings” has been widely studied (Bengio et al., 2003; Mnih and Hinton, 2008). Using word embeddings, we can evaluate the similarity of two words straightforward by computing the dot-product of two numerical vectors in the Hilbert space. Two similar words are expected to 111 be distributed close to each other.2 Word embeddings can be useful as input to an NLP model (mostly non-linear) or as additional features to enhance ex</context>
<context position="23533" citStr="Koo et al., 2008" startWordPosition="3842" endWordPosition="3845">word embeddings to 50. Higher dimension is supposed to bring more improvements in semi-supervised learning, but its comparison is beyond the scope of this paper. For the cluster features, we tune the number of clusters n from 500 to 3000 on the development set, and finally use the combination of n = 500,1000,1500, 2000, 3000, which achieves the best results. For the distributional prototype features, we use a fixed number of prototype words (m) for each target label. m is tuned on the development set and is finally set to 40. We induce 1,000 brown clusters of words, the setting in prior work (Koo et al., 2008; Turian et al., 2010). The training data of brown clustering is the same with that of training word embeddings. 5.2 Results Table 3 shows the performances of NER on the test dataset. Our baseline is slightly lower than that of Turian et al. (2010), because they use the BILOU encoding of NE types which outperforms BIO encoding (Ratinov and Roth, 2009).8 Nonetheless, our conclusions hold. As we can see, all of the three approaches we investigate in this study achieve better performance than the direct use of the dense continuous embedding features. To our surprise, even the binarized embedding </context>
<context position="35033" citStr="Koo et al., 2008" startWordPosition="5629" endWordPosition="5632">d should be more effective in determining fine-grained NE types than identifying whether a word is an NE. To verify this, we extract new prototypes considering only two labels, namely, NE and non-NE, using the same metric in Section 3.4. As shown in the last row of Table 5, higher performance is achieved. 6 Related Studies Semi-supervised learning with generalized word representations is a simple and general way of improving supervised NLP systems. One common approach for inducing generalized word representations is to use clustering (e.g., Brown clustering) (Miller et al., 2004; Liang, 2005; Koo et al., 2008; Huang and Yates, 2009). Aside from word clustering, word embeddings have been widely studied. Bengio et al. (2003) propose a feed-forward neural network based language model (NNLM), which uses an embedding layer to map each word to a dense continuousvalued and low-dimensional vector (parameters), and then use these vectors as the input to predict the probability distribution of the next word. The NNLM can be seen as a joint learning framework for language modeling and word representations. Alternative models for learning word embeddings are mostly inspired by the feed-forward NNLM, including</context>
</contexts>
<marker>Koo, Carreras, Collins, 2008</marker>
<rawString>Terry Koo, Xavier Carreras, and Michael Collins. 2008. Simple semi-supervised dependency parsing. In Kathleen McKeown, Johanna D. Moore, Simone Teufel, James Allan, and Sadaoki Furui, editors, Proc. of ACL-08: HLT, Proc. of ACL-08: HLT, pages 595–603, Columbus, Ohio. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vijay Krishnan</author>
<author>Christopher D Manning</author>
</authors>
<title>An effective two-stage model for exploiting nonlocal dependencies in named entity recognition.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics,</booktitle>
<pages>1121--1128</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="24898" citStr="Krishnan and Manning (2006)" startWordPosition="4033" endWordPosition="4036">s embeddings as features in CRF indeed cannot fully 5nlp.stanford.edu/software/tokenizer. shtml. 6code.google.com/p/word2vec/. 7More details are analyzed in (Goldberg and Levy, 2014). 8We use BIO encoding here in order to compare with most of the reported benchmarks. 115 Setting F1 Baseline 83.43 +DenseEmb† 86.21 +BinarizedEmb 86.75 +ClusterEmb 86.90 +DistPrototype 87.44 +BinarizedEmb+ClusterEmb 87.56 +BinarizedEmb+DistPrototype 87.46 +ClusterEmb+DistPrototype 88.11 +Brown 87.49 +Brown+ClusterEmb 88.17 +Brown+DistPrototype 88.04 +Brown+ClusterEmb+DistPrototype 88.58 Finkel et al. (2005) 86.86 Krishnan and Manning (2006) 87.24 Ando and Zhang (2005) 89.31 Collobert et al. (2011) 88.67 Table 3: The performance of semi-supervised NER on the CoNLL-2003 test data, using various embedding features. † DenseEmb refers to the method used by Turian et al. (2010), i.e., the direct use of the dense and continuous embeddings. exploit the potential of word embeddings. The compound cluster features (ClusterEmb) also outperform the DenseEmb. The same result is also shown in (Yu et al., 2013). Further, the distributional prototype features (DistPrototype) achieve the best performance among the three approaches (1.23% higher t</context>
<context position="27390" citStr="Krishnan and Manning (2006)" startWordPosition="4410" endWordPosition="4414">ring because we show that the embedding features indeed have stronger expressing power than the Brown clusters, as desired. Finally, by combining the Brown cluster features and the proposed embedding features, the performance can be improved further (88.58%). The binarized embedding features are not included in the final compound features because they are almost overlapped with the distributional prototype features in performance. We also summarize some of the reported benchmarks that utilize unlabeled data (with no gazetteers used), including the Stanford NER tagger (Finkel et al. (2005) and Krishnan and Manning (2006)) with distributional similarity features. Ando and Zhang (2005) use unlabeled data for constructing auxiliary problems that are expected to capture a good feature representation of the target problem. Collobert et al. (2011) adjust the feature embeddings according to the specific task in a deep neural network architecture. We can see that both Ando and Zhang (2005) and Collobert et al. (2011) learn task-specific lexical features, which is similar to the proposed distributional prototype method in our study. We suggest this to be the main reason for the superiority of these methods. Another ad</context>
</contexts>
<marker>Krishnan, Manning, 2006</marker>
<rawString>Vijay Krishnan and Christopher D Manning. 2006. An effective two-stage model for exploiting nonlocal dependencies in named entity recognition. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, pages 1121–1128. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
</authors>
<title>Semi-supervised learning for natural language. Master thesis,</title>
<date>2005</date>
<institution>Massachusetts Institute of Technology.</institution>
<contexts>
<context position="1668" citStr="Liang (2005)" startWordPosition="239" endWordPosition="240">ong which the distributional prototype approach performs the best. Moreover, the combination of the approaches provides additive improvements, outperforming the dense and continuous embedding features by nearly 2 points of F1 score. 1 Introduction Learning generalized representation of words is an effective way of handling data sparsity caused by high-dimensional lexical features in NLP systems, such as named entity recognition (NER) and dependency parsing. As a typical lowdimensional and generalized word representation, Brown clustering of words has been studied for a long time. For example, Liang (2005) and Koo et al. (2008) used the Brown cluster features for semi-supervised learning of various NLP tasks and achieved significant improvements. ∗Email correspondence. Recent research has focused on a special family of word representations, named “word embeddings”. Word embeddings are conventionally defined as dense, continuous, and low-dimensional vector representations of words. Word embeddings can be learned from large-scale unlabeled texts through context-predicting models (e.g., neural network language models) or spectral methods (e.g., canonical correlation analysis) in an unsupervised se</context>
<context position="7629" citStr="Liang (2005)" startWordPosition="1132" endWordPosition="1133">r texts. For these low-frequency words, the corresponding parameters usually cannot be fully trained. More foundationally, the reason for the above factors lies in the high-dimensional and sparse lexical feature representation, which completely ignores the similarity between features, especially word features. To overcome this weakness, an effective way is to learn more generalized representations of words by exploiting the numerous unlabeled data, in a semi-supervised manner. After which, the generalized word representations can be used as extra features to facilitate the supervised systems. Liang (2005) learned Brown clusters of words (Brown et al., 1992) from unlabeled data and use them as features to promote the supervised NER and Chinese word segmentation. Brown clusters of words can be seen as a generalized word representation distributed in a discrete and low-dimensional vocabulary space. Contextually similar words are grouped in the same cluster. The Brown clustering of words was also adopted in dependency parsing (Koo et al., 2008) and POS tagging for online conversational text (Owoputi et al., 2013), demonstrating significant improvements. Recently, another kind of word representatio</context>
<context position="35015" citStr="Liang, 2005" startWordPosition="5627" endWordPosition="5628">otypes we used should be more effective in determining fine-grained NE types than identifying whether a word is an NE. To verify this, we extract new prototypes considering only two labels, namely, NE and non-NE, using the same metric in Section 3.4. As shown in the last row of Table 5, higher performance is achieved. 6 Related Studies Semi-supervised learning with generalized word representations is a simple and general way of improving supervised NLP systems. One common approach for inducing generalized word representations is to use clustering (e.g., Brown clustering) (Miller et al., 2004; Liang, 2005; Koo et al., 2008; Huang and Yates, 2009). Aside from word clustering, word embeddings have been widely studied. Bengio et al. (2003) propose a feed-forward neural network based language model (NNLM), which uses an embedding layer to map each word to a dense continuousvalued and low-dimensional vector (parameters), and then use these vectors as the input to predict the probability distribution of the next word. The NNLM can be seen as a joint learning framework for language modeling and word representations. Alternative models for learning word embeddings are mostly inspired by the feed-forwa</context>
</contexts>
<marker>Liang, 2005</marker>
<rawString>Percy Liang. 2005. Semi-supervised learning for natural language. Master thesis, Massachusetts Institute of Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space.</title>
<date>2013</date>
<booktitle>In Proc. of Workshop at ICLR, Proc. of Workshop at ICLR,</booktitle>
<location>Arizona.</location>
<contexts>
<context position="35882" citStr="Mikolov et al., 2013" startWordPosition="5764" endWordPosition="5767">d to a dense continuousvalued and low-dimensional vector (parameters), and then use these vectors as the input to predict the probability distribution of the next word. The NNLM can be seen as a joint learning framework for language modeling and word representations. Alternative models for learning word embeddings are mostly inspired by the feed-forward NNLM, including the Hierarchical Log-Bilinear Model (Mnih and Hinton, 2008), the recurrent neural network language model (Mikolov, 2012), the C&amp;W model (Collobert et al., 2011), the loglinear models such as the CBOW and the Skip118 gram model (Mikolov et al., 2013a; Mikolov et al., 2013b). Aside from the NNLMs, word embeddings can also be induced using spectral methods, such as latent semantic analysis and canonical correlation analysis (Dhillon et al., 2011). The spectral methods are generally faster but much more memoryconsuming than NNLMs. There has been a plenty of work that exploits word embeddings as features for semi-supervised learning, most of which take the continuous features directly in linear models (Turian et al., 2010; Guo et al., 2014). Yu et al. (2013) propose compound k-means cluster features based on word embeddings. They show that t</context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013a. Efficient estimation of word representations in vector space. In Proc. of Workshop at ICLR, Proc. of Workshop at ICLR, Arizona.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Greg S Corrado</author>
<author>Jeff Dean</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality.</title>
<date>2013</date>
<booktitle>In Proc. of the NIPS, Proc. of the NIPS,</booktitle>
<pages>3111--3119</pages>
<publisher>MIT Press.</publisher>
<location>Nevada.</location>
<contexts>
<context position="35882" citStr="Mikolov et al., 2013" startWordPosition="5764" endWordPosition="5767">d to a dense continuousvalued and low-dimensional vector (parameters), and then use these vectors as the input to predict the probability distribution of the next word. The NNLM can be seen as a joint learning framework for language modeling and word representations. Alternative models for learning word embeddings are mostly inspired by the feed-forward NNLM, including the Hierarchical Log-Bilinear Model (Mnih and Hinton, 2008), the recurrent neural network language model (Mikolov, 2012), the C&amp;W model (Collobert et al., 2011), the loglinear models such as the CBOW and the Skip118 gram model (Mikolov et al., 2013a; Mikolov et al., 2013b). Aside from the NNLMs, word embeddings can also be induced using spectral methods, such as latent semantic analysis and canonical correlation analysis (Dhillon et al., 2011). The spectral methods are generally faster but much more memoryconsuming than NNLMs. There has been a plenty of work that exploits word embeddings as features for semi-supervised learning, most of which take the continuous features directly in linear models (Turian et al., 2010; Guo et al., 2014). Yu et al. (2013) propose compound k-means cluster features based on word embeddings. They show that t</context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S. Corrado, and Jeff Dean. 2013b. Distributed representations of words and phrases and their compositionality. In Proc. of the NIPS, Proc. of the NIPS, pages 3111–3119, Nevada. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
</authors>
<title>Statistical Language Models Based on Neural Networks.</title>
<date>2012</date>
<tech>Ph. d. thesis,</tech>
<institution>Brno University of Technology.</institution>
<contexts>
<context position="35754" citStr="Mikolov, 2012" startWordPosition="5741" endWordPosition="5742">l. (2003) propose a feed-forward neural network based language model (NNLM), which uses an embedding layer to map each word to a dense continuousvalued and low-dimensional vector (parameters), and then use these vectors as the input to predict the probability distribution of the next word. The NNLM can be seen as a joint learning framework for language modeling and word representations. Alternative models for learning word embeddings are mostly inspired by the feed-forward NNLM, including the Hierarchical Log-Bilinear Model (Mnih and Hinton, 2008), the recurrent neural network language model (Mikolov, 2012), the C&amp;W model (Collobert et al., 2011), the loglinear models such as the CBOW and the Skip118 gram model (Mikolov et al., 2013a; Mikolov et al., 2013b). Aside from the NNLMs, word embeddings can also be induced using spectral methods, such as latent semantic analysis and canonical correlation analysis (Dhillon et al., 2011). The spectral methods are generally faster but much more memoryconsuming than NNLMs. There has been a plenty of work that exploits word embeddings as features for semi-supervised learning, most of which take the continuous features directly in linear models (Turian et al.</context>
</contexts>
<marker>Mikolov, 2012</marker>
<rawString>Tomas Mikolov. 2012. Statistical Language Models Based on Neural Networks. Ph. d. thesis, Brno University of Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Miller</author>
<author>Jethran Guinness</author>
<author>Alex Zamanian</author>
</authors>
<title>Name tagging with word clusters and discriminative training.</title>
<date>2004</date>
<booktitle>In HLT-NAACL,</booktitle>
<volume>4</volume>
<pages>337--342</pages>
<contexts>
<context position="35002" citStr="Miller et al., 2004" startWordPosition="5623" endWordPosition="5626">Intuitively, the prototypes we used should be more effective in determining fine-grained NE types than identifying whether a word is an NE. To verify this, we extract new prototypes considering only two labels, namely, NE and non-NE, using the same metric in Section 3.4. As shown in the last row of Table 5, higher performance is achieved. 6 Related Studies Semi-supervised learning with generalized word representations is a simple and general way of improving supervised NLP systems. One common approach for inducing generalized word representations is to use clustering (e.g., Brown clustering) (Miller et al., 2004; Liang, 2005; Koo et al., 2008; Huang and Yates, 2009). Aside from word clustering, word embeddings have been widely studied. Bengio et al. (2003) propose a feed-forward neural network based language model (NNLM), which uses an embedding layer to map each word to a dense continuousvalued and low-dimensional vector (parameters), and then use these vectors as the input to predict the probability distribution of the next word. The NNLM can be seen as a joint learning framework for language modeling and word representations. Alternative models for learning word embeddings are mostly inspired by t</context>
</contexts>
<marker>Miller, Guinness, Zamanian, 2004</marker>
<rawString>Scott Miller, Jethran Guinness, and Alex Zamanian. 2004. Name tagging with word clusters and discriminative training. In HLT-NAACL, volume 4, pages 337–342.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andriy Mnih</author>
<author>Geoffrey E Hinton</author>
</authors>
<title>A scalable hierarchical distributed language model.</title>
<date>2008</date>
<booktitle>In Proc. of the NIPS, Proc. of the NIPS,</booktitle>
<pages>1081--1088</pages>
<publisher>MIT Press.</publisher>
<location>Vancouver.</location>
<contexts>
<context position="8323" citStr="Mnih and Hinton, 2008" startWordPosition="1238" endWordPosition="1241">a and use them as features to promote the supervised NER and Chinese word segmentation. Brown clusters of words can be seen as a generalized word representation distributed in a discrete and low-dimensional vocabulary space. Contextually similar words are grouped in the same cluster. The Brown clustering of words was also adopted in dependency parsing (Koo et al., 2008) and POS tagging for online conversational text (Owoputi et al., 2013), demonstrating significant improvements. Recently, another kind of word representation named “word embeddings” has been widely studied (Bengio et al., 2003; Mnih and Hinton, 2008). Using word embeddings, we can evaluate the similarity of two words straightforward by computing the dot-product of two numerical vectors in the Hilbert space. Two similar words are expected to 111 be distributed close to each other.2 Word embeddings can be useful as input to an NLP model (mostly non-linear) or as additional features to enhance existing systems. Collobert et al. (2011) used word embeddings as input to a deep neural network for multi-task learning. Despite of the effectiveness, such non-linear models are hard to build and optimize. Besides, these architectures are often specia</context>
<context position="35693" citStr="Mnih and Hinton, 2008" startWordPosition="5731" endWordPosition="5734">ord clustering, word embeddings have been widely studied. Bengio et al. (2003) propose a feed-forward neural network based language model (NNLM), which uses an embedding layer to map each word to a dense continuousvalued and low-dimensional vector (parameters), and then use these vectors as the input to predict the probability distribution of the next word. The NNLM can be seen as a joint learning framework for language modeling and word representations. Alternative models for learning word embeddings are mostly inspired by the feed-forward NNLM, including the Hierarchical Log-Bilinear Model (Mnih and Hinton, 2008), the recurrent neural network language model (Mikolov, 2012), the C&amp;W model (Collobert et al., 2011), the loglinear models such as the CBOW and the Skip118 gram model (Mikolov et al., 2013a; Mikolov et al., 2013b). Aside from the NNLMs, word embeddings can also be induced using spectral methods, such as latent semantic analysis and canonical correlation analysis (Dhillon et al., 2011). The spectral methods are generally faster but much more memoryconsuming than NNLMs. There has been a plenty of work that exploits word embeddings as features for semi-supervised learning, most of which take the</context>
</contexts>
<marker>Mnih, Hinton, 2008</marker>
<rawString>Andriy Mnih and Geoffrey E. Hinton. 2008. A scalable hierarchical distributed language model. In Proc. of the NIPS, Proc. of the NIPS, pages 1081– 1088, Vancouver. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olutobi Owoputi</author>
<author>Brendan O’Connor</author>
<author>Chris Dyer</author>
<author>Kevin Gimpel</author>
<author>Nathan Schneider</author>
<author>Noah A Smith</author>
</authors>
<title>Improved part-of-speech tagging for online conversational text with word clusters.</title>
<date>2013</date>
<booktitle>In Proceedings of NAACL-HLT,</booktitle>
<pages>380--390</pages>
<marker>Owoputi, O’Connor, Dyer, Gimpel, Schneider, Smith, 2013</marker>
<rawString>Olutobi Owoputi, Brendan O’Connor, Chris Dyer, Kevin Gimpel, Nathan Schneider, and Noah A Smith. 2013. Improved part-of-speech tagging for online conversational text with word clusters. In Proceedings of NAACL-HLT, pages 380–390.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lev Ratinov</author>
<author>Dan Roth</author>
</authors>
<title>Design challenges and misconceptions in named entity recognition.</title>
<date>2009</date>
<booktitle>In Proceedings of the Thirteenth Conference on Computational Natural Language Learning, CoNLL ’09,</booktitle>
<pages>147--155</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="23886" citStr="Ratinov and Roth, 2009" startWordPosition="3904" endWordPosition="3907"> best results. For the distributional prototype features, we use a fixed number of prototype words (m) for each target label. m is tuned on the development set and is finally set to 40. We induce 1,000 brown clusters of words, the setting in prior work (Koo et al., 2008; Turian et al., 2010). The training data of brown clustering is the same with that of training word embeddings. 5.2 Results Table 3 shows the performances of NER on the test dataset. Our baseline is slightly lower than that of Turian et al. (2010), because they use the BILOU encoding of NE types which outperforms BIO encoding (Ratinov and Roth, 2009).8 Nonetheless, our conclusions hold. As we can see, all of the three approaches we investigate in this study achieve better performance than the direct use of the dense continuous embedding features. To our surprise, even the binarized embedding features (BinarizedEmb) outperform the continuous version (DenseEmb). This provides clear evidence that directly using the dense continuous embeddings as features in CRF indeed cannot fully 5nlp.stanford.edu/software/tokenizer. shtml. 6code.google.com/p/word2vec/. 7More details are analyzed in (Goldberg and Levy, 2014). 8We use BIO encoding here in or</context>
</contexts>
<marker>Ratinov, Roth, 2009</marker>
<rawString>Lev Ratinov and Dan Roth. 2009. Design challenges and misconceptions in named entity recognition. In Proceedings of the Thirteenth Conference on Computational Natural Language Learning, CoNLL ’09, pages 147–155, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Sculley</author>
</authors>
<title>Combined regression and ranking.</title>
<date>2010</date>
<booktitle>In Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining,</booktitle>
<pages>979--988</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="12738" citStr="Sculley, 2010" startWordPosition="1979" endWordPosition="1980">insight behind φ is that we only consider the features with strong opinions (i.e., positive or negative) on each dimension and omit the values close to zero. 3.3 Clustering of Embeddings Yu et al. (2013) introduced clustering embeddings to overcome the disadvantage that word embeddings are not suitable for linear models. They suggested that the high-dimensional cluster features make samples from different classes better separated by linear models. 112 In this study, we again investigate this approach. Concretely, each word is treated as a single sample. The batch k-means clustering algorithm (Sculley, 2010) is used,3 and each cluster is represented as the mean of the embeddings of words assigned to it. Similarities between words and clusters are measured by Euclidean distance. Moreover, different number of clusters n contain information of different granularities. Therefore, we combine the cluster features of different ns to better utilize the embeddings. 3.4 Distributional Prototype Features We propose a novel kind of embedding features, named distributional prototype features for supervised models. This is mainly inspired by prototype-driven learning (Haghighi and Klein, 2006) which was origin</context>
</contexts>
<marker>Sculley, 2010</marker>
<rawString>D Sculley. 2010. Combined regression and ranking. In Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 979–988. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik F Tjong Kim Sang</author>
<author>Fien De Meulder</author>
</authors>
<title>Introduction to the conll-2003 shared task: Language-independent named entity recognition.</title>
<date>2003</date>
<booktitle>In Proceedings of the seventh conference on Natural language learning at HLT-NAACL 2003-Volume 4,</booktitle>
<pages>142--147</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Sang, De Meulder, 2003</marker>
<rawString>Erik F Tjong Kim Sang and Fien De Meulder. 2003. Introduction to the conll-2003 shared task: Language-independent named entity recognition. In Proceedings of the seventh conference on Natural language learning at HLT-NAACL 2003-Volume 4, pages 142–147. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>Lev Ratinov</author>
<author>Yoshua Bengio</author>
</authors>
<title>Word representations: a simple and general method for semi-supervised learning.</title>
<date>2010</date>
<booktitle>Proc. of the Annual Meeting of the Association for Computational Linguistics (ACL), Proc. of the Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>384--394</pages>
<editor>In Jan Hajic, Sandra Carberry, and Stephen Clark, editors,</editor>
<publisher>ACL.</publisher>
<location>Uppsala, Sweden.</location>
<contexts>
<context position="3051" citStr="Turian et al., 2010" startWordPosition="448" endWordPosition="451">on is on, word embedding preserves rich linguistic regularities of words with each dimension hopefully representing a latent feature. Similar words are expected to be distributed close to one another in the embedding space. Consequently, word embeddings can be beneficial for a variety of NLP applications in different ways, among which the most simple and general way is to be fed as features to enhance existing supervised NLP systems. Previous work has demonstrated effectiveness of the continuous word embedding features in several tasks such as chunking and NER using generalized linear models (Turian et al., 2010).1 However, there still remain two fundamental problems that should be addressed: • Are the continuous embedding features fit for the generalized linear models that are most widely adopted in NLP? • How can the generalized linear models better utilize the embedding features? According to the results provided by Turian et 1Generalized linear models refer to the models that describe the data as a combination of linear basis functions, either directly in the input variables space or through some transformation of the probability distributions (e.g., loglinear models). 110 Proceedings of the 2014 </context>
<context position="9213" citStr="Turian et al. (2010)" startWordPosition="1386" endWordPosition="1389">nput to an NLP model (mostly non-linear) or as additional features to enhance existing systems. Collobert et al. (2011) used word embeddings as input to a deep neural network for multi-task learning. Despite of the effectiveness, such non-linear models are hard to build and optimize. Besides, these architectures are often specialized for a certain task and not scalable to general tasks. A simple and more general way is to feed word embeddings as augmented features to an existing supervised system, which is similar to the semi-supervised learning with Brown clusters. As discussed in Section 1, Turian et al. (2010) is the pioneering work on using word embedding features for semi-supervised learning. However, their approach cannot fully exploit the potential of word embeddings. We revisit this problem in this study and investigate three different approaches for better utilizing word embeddings in semi-supervised learning. 3 Approaches for Utilizing Embedding Features 3.1 Word Embedding Training In this paper, we will consider a contextpredicting model, more specifically, the Skip-gram model (N ikolov et al., 2013a; N ikolov et al., 2013b) for learning word embeddings, since it is much more efficient as w</context>
<context position="11230" citStr="Turian et al., 2010" startWordPosition="1724" endWordPosition="1727"> on the specific task. log-likelihood over the entire training dataset D can be computed as: J(θ) = 1: log p(c|w; θ) (2) (w,c)∈D The model can be trained by maximizing J(θ). Here, we suppose that the word embeddings have already been trained from large-scale unlabeled texts. We will introduce various approaches for utilizing the word embeddings as features for semi-supervised learning. The main idea, as introduced in Section 1, is to transform the continuous word embeddings to some relatively higher dimensional and discrete representations. The direct use of continuous embeddings as features (Turian et al., 2010) will serve as our baseline setting. 3.2 Binarization of Embeddings One fairly natural approach for converting the continuous-valued word embeddings to discrete values is binarization by dimension. Formally, we aim to convert the continuousvalued embedding matrix Cd×N, to another matrix Md×N which is discrete-valued. There are various conversion functions. Here, we consider a simple one. For the ith dimension of the word embeddings, we divide the corresponding row vector Ci into two halves for positive (Ci+) and negative (Ci−), respectively. The conversion function is then defined as follows: </context>
<context position="17123" citStr="Turian et al. (2010)" startWordPosition="2706" endWordPosition="2709">m the training data. Moreover, each prototype word is also its own prototype (since a word has maximum similarity to itself). Thus, if the prototype is closely related to a label, all the words that are distributionally 113 Figure 1: An example of distributional prototype features for NER. similar to that prototype are pushed towards that label. y;  1 Y, x i - 1 f x i y i ( , )   word = Hague   proto = Britain B-LOC     4 Supervised Evaluation Task Various tasks can be considered to compare and analyze the effectiveness of the above three approaches. In this study, we partly follow Turian et al. (2010) and Yu et al. (2013), and take NER as the supervised evaluation task. NER identifies and classifies the named entities such as the names of persons, locations, and organizations in text. The state-of-the-art systems typically treat NER as a sequence labeling problem, where each word is tagged either as a BIO-style NE or a non-NE category. Here, we use the linear chain CRF model, which is most widely used for sequence modeling in the field of NLP. The CoNLL-2003 shared task dataset from the Reuters, which was used by Turian et al. (2010) and Yu et al. (2013), was chosen as our evaluation datas</context>
<context position="18524" citStr="Turian et al. (2010)" startWordPosition="2940" endWordPosition="2943">e features are shown in Table 2. 4.1 Embedding Feature Templates In this section, we introduce the embedding features to the baseline NER system, turning the supervised approach into a semi-supervised one. Dense embedding features. The dense continuous embedding features can be fed directly to the CRF model. These embedding features can be seen as heterogeneous features from the existing baseline features, which are discrete. There is no effective way for dense embedding features to be combined internally or with other discrete features. So we only use the unigram embedding features following Turian et al. (2010). Concretely, the embedding feature template is: Baseline NER Feature Templates 00: wi+k, −2 ≤ k ≤ 2 1: wi+k ◦ wi+k+1, −2 ≤ k ≤ 1 2: ti+k,−2 ≤ k ≤ 2 3: ti+k ◦ ti+k+1, −2 ≤ k ≤ 1 4: chki+k, −2 ≤ k ≤ 2 5: chki+k ◦ chki+k+1, −2 ≤ k ≤ 1 6: Prefix(wi+k, l), −2 ≤ k ≤ 2,1 ≤ l ≤ 4 7: Suffix(wi+k, l), −2 ≤ k ≤ 2,1 ≤ l ≤ 4 8: Type(wi+k), −2 ≤ k ≤ 2 Unigram Features yi ◦ 00 − 08 Bigram Features yi−1 ◦ yi Table 2: Features used in the NER system. t is the POS tag. chk is the chunking tag. Prefix and Suffix are the first and last l characters of a word. Type indicates if the word is all-capitalized, is-cap</context>
<context position="21145" citStr="Turian et al., 2010" startWordPosition="3442" endWordPosition="3445">ty between wi and each prototype in Sp and select the prototypes zs that DistSim(z, w) &gt; 6. We set 6 = 0.5 without manual tuning. The distributional prototype feature template is then: • {protoi+k=z |DistSim(wi+k, z) &gt; 6 &amp; z E Sp }, −2 &lt; k &lt; 2 . We only use the unigram features, since the number of active distributional prototype features varies for different words (positions). Hence, these features cannot be combined effectively. 4.2 Brown Clustering Brown clustering has achieved great success in various NLP applications. At most time, it provides a strong baseline that is difficult to beat (Turian et al., 2010). Consequently, in our study, we conduct comparisons among the embedding features and the Brown clustering features, along with further investigations of their combination. The Brown algorithm is a hierarchical clustering algorithm which optimizes a class-based bigram language model defined on the word clusters (Brown et al., 1992). The output of the Brown algorithm is a binary tree, where each word is uniquely identified by its path from the root. Thus each word can be represented as a bit-string with a specific length. Following the setting of Owoputi et al. (2013), we will use the prefix fe</context>
<context position="23555" citStr="Turian et al., 2010" startWordPosition="3846" endWordPosition="3849"> 50. Higher dimension is supposed to bring more improvements in semi-supervised learning, but its comparison is beyond the scope of this paper. For the cluster features, we tune the number of clusters n from 500 to 3000 on the development set, and finally use the combination of n = 500,1000,1500, 2000, 3000, which achieves the best results. For the distributional prototype features, we use a fixed number of prototype words (m) for each target label. m is tuned on the development set and is finally set to 40. We induce 1,000 brown clusters of words, the setting in prior work (Koo et al., 2008; Turian et al., 2010). The training data of brown clustering is the same with that of training word embeddings. 5.2 Results Table 3 shows the performances of NER on the test dataset. Our baseline is slightly lower than that of Turian et al. (2010), because they use the BILOU encoding of NE types which outperforms BIO encoding (Ratinov and Roth, 2009).8 Nonetheless, our conclusions hold. As we can see, all of the three approaches we investigate in this study achieve better performance than the direct use of the dense continuous embedding features. To our surprise, even the binarized embedding features (BinarizedEmb</context>
<context position="25134" citStr="Turian et al. (2010)" startWordPosition="4073" endWordPosition="4076">the reported benchmarks. 115 Setting F1 Baseline 83.43 +DenseEmb† 86.21 +BinarizedEmb 86.75 +ClusterEmb 86.90 +DistPrototype 87.44 +BinarizedEmb+ClusterEmb 87.56 +BinarizedEmb+DistPrototype 87.46 +ClusterEmb+DistPrototype 88.11 +Brown 87.49 +Brown+ClusterEmb 88.17 +Brown+DistPrototype 88.04 +Brown+ClusterEmb+DistPrototype 88.58 Finkel et al. (2005) 86.86 Krishnan and Manning (2006) 87.24 Ando and Zhang (2005) 89.31 Collobert et al. (2011) 88.67 Table 3: The performance of semi-supervised NER on the CoNLL-2003 test data, using various embedding features. † DenseEmb refers to the method used by Turian et al. (2010), i.e., the direct use of the dense and continuous embeddings. exploit the potential of word embeddings. The compound cluster features (ClusterEmb) also outperform the DenseEmb. The same result is also shown in (Yu et al., 2013). Further, the distributional prototype features (DistPrototype) achieve the best performance among the three approaches (1.23% higher than DenseEmb). We should note that the feature templates used for BinarizedEmb and DistPrototype are merely unigram features. However, for ClusterEmb, we form more complex features by combining the clusters of the context words. We also</context>
<context position="29724" citStr="Turian et al. (2010)" startWordPosition="4794" endWordPosition="4797">ween efficiency and accuracy. 5.3 Analysis In this section, we conduct analyses to show the reasons for the improvements. very frequent words, while lower sparsity for midfrequent words. It indicates that for words that are very rare or very frequent, BinarizedEmb just omit most of the features. This is reasonable also for the very frequent words, since they usually have rich and diverse context distributions and their embeddings cannot be well learned by our models (Huang et al., 2012). ● ● ● ● ● ● ● ● ● ● ● Sparsity 0.50 0.55 0.60 0.65 0.70 256 1k 4k 16k 64k 5.3.1 Rare words As discussed by Turian et al. (2010), much of the NER F1 is derived from decisions regarding rare words. Therefore, in order to show that the three proposed embedding features have stronger ability for handling rare words, we first conduct analysis for the tagging errors of words with different frequency in the unlabeled data. We assign the word frequencies to several buckets, and evaluate the per-token errors that occurred in each bucket. Results are shown in Figure 2. In most cases, all three embedding features result in fewer errors on rare words than the direct use of dense continuous embedding features. Interestingly, we fi</context>
<context position="36360" citStr="Turian et al., 2010" startWordPosition="5840" endWordPosition="5843">ikolov, 2012), the C&amp;W model (Collobert et al., 2011), the loglinear models such as the CBOW and the Skip118 gram model (Mikolov et al., 2013a; Mikolov et al., 2013b). Aside from the NNLMs, word embeddings can also be induced using spectral methods, such as latent semantic analysis and canonical correlation analysis (Dhillon et al., 2011). The spectral methods are generally faster but much more memoryconsuming than NNLMs. There has been a plenty of work that exploits word embeddings as features for semi-supervised learning, most of which take the continuous features directly in linear models (Turian et al., 2010; Guo et al., 2014). Yu et al. (2013) propose compound k-means cluster features based on word embeddings. They show that the high-dimensional discrete cluster features can be better utilized by linear models such as CRF. Wu et al. (2013) further apply the cluster features to transition-based dependency parsing. 7 Conclusion and Future Work This paper revisits the problem of semi-supervised learning with word embeddings. We present three different approaches for a careful comparison and analysis. Using any of the three embedding features, we obtain higher performance than the direct use of cont</context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: a simple and general method for semi-supervised learning. In Jan Hajic, Sandra Carberry, and Stephen Clark, editors, Proc. of the Annual Meeting of the Association for Computational Linguistics (ACL), Proc. of the Annual Meeting of the Association for Computational Linguistics (ACL), pages 384–394, Uppsala, Sweden. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mengqiu Wang</author>
<author>Christopher D Manning</author>
</authors>
<title>Effect of non-linear deep architecture in sequence labeling.</title>
<date>2013</date>
<journal>Asian Federation of Natural Language Processing.</journal>
<booktitle>In Proc. of the Sixth International Joint Conference on Natural Language Processing, Proc. of the Sixth International Joint Conference on Natural Language Processing,</booktitle>
<pages>1285--1291</pages>
<location>Nagoya,</location>
<marker>Wang, Manning, 2013</marker>
<rawString>Mengqiu Wang and Christopher D. Manning. 2013. Effect of non-linear deep architecture in sequence labeling. In Proc. of the Sixth International Joint Conference on Natural Language Processing, Proc. of the Sixth International Joint Conference on Natural Language Processing, pages 1285–1291, Nagoya, Japan. Asian Federation of Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xianchao Wu</author>
<author>Jie Zhou</author>
<author>Yu Sun</author>
<author>Zhanyi Liu</author>
<author>Dianhai Yu</author>
<author>Hua Wu</author>
<author>Haifeng Wang</author>
</authors>
<title>Generalization of words for chinese dependency parsing. IWPT-2013,</title>
<date>2013</date>
<pages>73</pages>
<contexts>
<context position="36597" citStr="Wu et al. (2013)" startWordPosition="5881" endWordPosition="5884"> methods, such as latent semantic analysis and canonical correlation analysis (Dhillon et al., 2011). The spectral methods are generally faster but much more memoryconsuming than NNLMs. There has been a plenty of work that exploits word embeddings as features for semi-supervised learning, most of which take the continuous features directly in linear models (Turian et al., 2010; Guo et al., 2014). Yu et al. (2013) propose compound k-means cluster features based on word embeddings. They show that the high-dimensional discrete cluster features can be better utilized by linear models such as CRF. Wu et al. (2013) further apply the cluster features to transition-based dependency parsing. 7 Conclusion and Future Work This paper revisits the problem of semi-supervised learning with word embeddings. We present three different approaches for a careful comparison and analysis. Using any of the three embedding features, we obtain higher performance than the direct use of continuous embeddings, among which the distributional prototype features perform the best, showing the great potential of word embeddings. Moreover, the combination of the proposed embedding features provides significant additive improvement</context>
</contexts>
<marker>Wu, Zhou, Sun, Liu, Yu, Wu, Wang, 2013</marker>
<rawString>Xianchao Wu, Jie Zhou, Yu Sun, Zhanyi Liu, Dianhai Yu, Hua Wu, and Haifeng Wang. 2013. Generalization of words for chinese dependency parsing. IWPT-2013, page 73.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mo Yu</author>
<author>Tiejun Zhao</author>
<author>Daxiang Dong</author>
<author>Hao Tian</author>
<author>Dianhai Yu</author>
</authors>
<title>Compound embedding features for semi-supervised learning.</title>
<date>2013</date>
<booktitle>In Proc. of the NAACLHLT, Proc. of the NAACL-HLT,</booktitle>
<pages>563--568</pages>
<location>Atlanta. NAACL.</location>
<contexts>
<context position="12327" citStr="Yu et al. (2013)" startWordPosition="1913" endWordPosition="1916">to two halves for positive (Ci+) and negative (Ci−), respectively. The conversion function is then defined as follows: Mij = φ(Cij) = { U+, if Cij ≥ mean(Ci+) B−, if Cij G mean(Ci−) 0, otherwise where mean(v) is the mean value of vector v, U+ is a string feature which turns on when the value (Cij) falls into the upper part of the positive list. Similarly, B− refers to the bottom part of the negative list. The insight behind φ is that we only consider the features with strong opinions (i.e., positive or negative) on each dimension and omit the values close to zero. 3.3 Clustering of Embeddings Yu et al. (2013) introduced clustering embeddings to overcome the disadvantage that word embeddings are not suitable for linear models. They suggested that the high-dimensional cluster features make samples from different classes better separated by linear models. 112 In this study, we again investigate this approach. Concretely, each word is treated as a single sample. The batch k-means clustering algorithm (Sculley, 2010) is used,3 and each cluster is represented as the mean of the embeddings of words assigned to it. Similarities between words and clusters are measured by Euclidean distance. Moreover, diffe</context>
<context position="17144" citStr="Yu et al. (2013)" startWordPosition="2711" endWordPosition="2714">over, each prototype word is also its own prototype (since a word has maximum similarity to itself). Thus, if the prototype is closely related to a label, all the words that are distributionally 113 Figure 1: An example of distributional prototype features for NER. similar to that prototype are pushed towards that label. y;  1 Y, x i - 1 f x i y i ( , )   word = Hague   proto = Britain B-LOC     4 Supervised Evaluation Task Various tasks can be considered to compare and analyze the effectiveness of the above three approaches. In this study, we partly follow Turian et al. (2010) and Yu et al. (2013), and take NER as the supervised evaluation task. NER identifies and classifies the named entities such as the names of persons, locations, and organizations in text. The state-of-the-art systems typically treat NER as a sequence labeling problem, where each word is tagged either as a BIO-style NE or a non-NE category. Here, we use the linear chain CRF model, which is most widely used for sequence modeling in the field of NLP. The CoNLL-2003 shared task dataset from the Reuters, which was used by Turian et al. (2010) and Yu et al. (2013), was chosen as our evaluation dataset. The training set </context>
<context position="25362" citStr="Yu et al., 2013" startWordPosition="4110" endWordPosition="4113">wn 87.49 +Brown+ClusterEmb 88.17 +Brown+DistPrototype 88.04 +Brown+ClusterEmb+DistPrototype 88.58 Finkel et al. (2005) 86.86 Krishnan and Manning (2006) 87.24 Ando and Zhang (2005) 89.31 Collobert et al. (2011) 88.67 Table 3: The performance of semi-supervised NER on the CoNLL-2003 test data, using various embedding features. † DenseEmb refers to the method used by Turian et al. (2010), i.e., the direct use of the dense and continuous embeddings. exploit the potential of word embeddings. The compound cluster features (ClusterEmb) also outperform the DenseEmb. The same result is also shown in (Yu et al., 2013). Further, the distributional prototype features (DistPrototype) achieve the best performance among the three approaches (1.23% higher than DenseEmb). We should note that the feature templates used for BinarizedEmb and DistPrototype are merely unigram features. However, for ClusterEmb, we form more complex features by combining the clusters of the context words. We also consider different number of clusters n, to take advantage of the different granularities. Consequently, the dimension of the cluster features is much higher than that of BinarizedEmb and DistPrototype. We further combine the p</context>
<context position="36397" citStr="Yu et al. (2013)" startWordPosition="5848" endWordPosition="5851">et al., 2011), the loglinear models such as the CBOW and the Skip118 gram model (Mikolov et al., 2013a; Mikolov et al., 2013b). Aside from the NNLMs, word embeddings can also be induced using spectral methods, such as latent semantic analysis and canonical correlation analysis (Dhillon et al., 2011). The spectral methods are generally faster but much more memoryconsuming than NNLMs. There has been a plenty of work that exploits word embeddings as features for semi-supervised learning, most of which take the continuous features directly in linear models (Turian et al., 2010; Guo et al., 2014). Yu et al. (2013) propose compound k-means cluster features based on word embeddings. They show that the high-dimensional discrete cluster features can be better utilized by linear models such as CRF. Wu et al. (2013) further apply the cluster features to transition-based dependency parsing. 7 Conclusion and Future Work This paper revisits the problem of semi-supervised learning with word embeddings. We present three different approaches for a careful comparison and analysis. Using any of the three embedding features, we obtain higher performance than the direct use of continuous embeddings, among which the di</context>
</contexts>
<marker>Yu, Zhao, Dong, Tian, Yu, 2013</marker>
<rawString>Mo Yu, Tiejun Zhao, Daxiang Dong, Hao Tian, and Dianhai Yu. 2013. Compound embedding features for semi-supervised learning. In Proc. of the NAACLHLT, Proc. of the NAACL-HLT, pages 563–568, Atlanta. NAACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
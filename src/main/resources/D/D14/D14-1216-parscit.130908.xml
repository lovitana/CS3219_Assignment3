<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003280">
<title confidence="0.9977535">
Classifying Idiomatic and Literal Expressions Using Topic Models and
Intensity of Emotions
</title>
<author confidence="0.998812">
Jing Peng &amp; Anna Feldman Ekaterina Vylomova
</author>
<affiliation confidence="0.970941333333333">
Computer Science/Linguistics Computer Science
Montclair State University Bauman State Technical University
Montclair, New Jersey, USA Moscow, Russia
</affiliation>
<email confidence="0.999187">
{pengj,feldmana}@mail.montclair.edu evylomova@gmail.com
</email>
<sectionHeader confidence="0.993907" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999994964285714">
We describe an algorithm for automatic clas-
sification of idiomatic and literal expressions.
Our starting point is that words in a given text
segment, such as a paragraph, that are high-
ranking representatives of a common topic of
discussion are less likely to be a part of an id-
iomatic expression. Our additional hypothesis
is that contexts in which idioms occur, typi-
cally, are more affective and therefore, we in-
corporate a simple analysis of the intensity of
the emotions expressed by the contexts. We
investigate the bag of words topic represen-
tation of one to three paragraphs containing
an expression that should be classified as id-
iomatic or literal (a target phrase). We ex-
tract topics from paragraphs containing idioms
and from paragraphs containing literals us-
ing an unsupervised clustering method, Latent
Dirichlet Allocation (LDA) (Blei et al., 2003).
Since idiomatic expressions exhibit the prop-
erty of non-compositionality, we assume that
they usually present different semantics than
the words used in the local topic. We treat
idioms as semantic outliers, and the identifi-
cation of a semantic shift as outlier detection.
Thus, this topic representation allows us to dif-
ferentiate idioms from literals using local se-
mantic contexts. Our results are encouraging.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99995762295082">
The definition of what is literal and figurative is still
object of debate. Ariel (2002) demonstrates that lit-
eral and non-literal meanings cannot always be distin-
guished from each other. Literal meaning is originally
assumed to be conventional, compositional, relatively
context independent, and truth conditional. The prob-
lem is that the boundary is not clear-cut, some figu-
rative expressions are compositional – metaphors and
many idioms; others are conventional – most of the id-
ioms. Idioms present great challenges for many Natu-
ral Language Processing (NLP) applications. They can
violate selection restrictions (Sporleder and Li, 2009)
as in push one’s luck under the assumption that only
concrete things can normally be pushed. Idioms can
disobey typical subcategorization constraints (e.g., in
line without a determiner before line), or change the
default assignments of semantic roles to syntactic cate-
gories (e.g., in X breaks something with Y, Y typically
is an instrument but for the idiom break the ice, it is
more likely to fill a patient role as in How to break the
ice with a stranger). In addition, many potentially id-
iomatic expressions can be used either literally or fig-
uratively, depending on the context. This presents a
great challenge for machine translation. For example,
a machine translation system must translate held fire
differently in Now, now, hold your fire until I’ve had a
chance to explain. Hold your fire, Bill. You’re too quick
to complain. and The sergeant told the soldiers to hold
their fire. Please hold your fire until I get out of the
way. In fact, we tested the last two examples using the
Google Translate engine and we got proper translations
of the two neither into Russian nor into Hebrew, Span-
ish, or Chinese. Most current translation systems rely
on large repositories of idioms. Unfortunately, these
systems are not capable to tell apart literal from figura-
tive usage of the same expression in context. Despite
the common perception that phrases that can be idioms
are mainly used in their idiomatic sense, Fazly et al.
(2009)’s analysis of 60 idioms has shown that close to
half of these also have a clear literal meaning; and of
those with a literal meaning, on average around 40% of
their usages are literal.
In this paper we describe an algorithm for automatic
classification of idiomatic and literal expressions. Our
starting point is that words in a given text segment,
such as a paragraph, that are high-ranking representa-
tives of a common topic of discussion are less likely
to be a part of an idiomatic expression. Our additional
hypothesis is that contexts in which idioms occur, typ-
ically, are more affective and therefore, we incorpo-
rate a simple analysis of the intensity of the emotions
expressed by the contexts. We investigate the bag of
words topic representation of one to three paragraphs
containing an expression that should be classified as
idiomatic or literal (a target phrase). We extract top-
ics from paragraphs containing idioms and from para-
graphs containing literals using an unsupervised clus-
tering method, Latent Dirichlet Allocation (LDA) (Blei
et al., 2003). Since idiomatic expressions exhibit the
property of non-compositionality, we assume that they
usually present different semantics than the words used
</bodyText>
<page confidence="0.990226">
2019
</page>
<note confidence="0.901123">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2019–2027,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999903538461538">
in the local topic. We treat idioms as semantic outliers,
and the identification of semantic shift as outlier detec-
tion. Thus, this topic representation allows us to differ-
entiate idioms from literals using the local semantics.
The paper is organized as follows. Section 2 briefly
describes previous approaches to idiom recognition or
classification. In Section 3 we describe our approach in
detail, including the hypothesis, the topic space repre-
sentation, and the proposed algorithm. After describing
the preprocessing procedure in Section 4, we turn to the
actual experiments in Sections 5 and 6. We then com-
pare our approach to other approaches (Section 7) and
discuss the results (Section 8).
</bodyText>
<sectionHeader confidence="0.996512" genericHeader="introduction">
2 Previous Work
</sectionHeader>
<bodyText confidence="0.999931408602151">
Previous approaches to idiom detection can be classi-
fied into two groups: 1) Type-based extraction, i.e., de-
tecting idioms at the type level; 2) token-based detec-
tion, i.e., detecting idioms in context. Type-based ex-
traction is based on the idea that idiomatic expressions
exhibit certain linguistic properties that can distinguish
them from literal expressions (Sag et al. (2002); Fa-
zly et al. (2009)), among many others, discuss various
properties of idioms. Some examples of such proper-
ties include 1) lexical fixedness: e.g., neither ‘shoot
the wind’ nor ‘hit the breeze’ are valid variations of
the idiom shoot the breeze and 2) syntactic fixedness:
e.g., The guy kicked the bucket is potentially idiomatic
whereas The bucket was kicked is not idiomatic any-
more; and of course, 3) non-compositionality. Thus,
some approaches look at the tendency for words to oc-
cur in one particular order, or a fixed pattern. Hearst
(1992) identifies lexico-syntactic patterns that occur
frequently, are recognizable with little or no precoded
knowledge, and indicate the lexical relation of interest.
Widdows and Dorow (2005) use Hearst’s concept of
lexicosyntactic patterns to extract idioms that consist
of fixed patterns between two nouns. Basically, their
technique works by finding patterns such as “thrills and
spills”, whose reversals (such as “spills and thrills”) are
never encountered.
While many idioms do have these properties, many
idioms fall on the continuum from being composi-
tional to being partly unanalyzable to completely non-
compositional (Cook et al. (2007)). Fazly et al. (2009);
Li and Sporleder (2010), among others, notice that
type-based approaches do not work on expressions that
can be interpreted idiomatically or literally depending
on the context and thus, an approach that considers to-
kens in context is more appropriate for the task of idiom
recognition.
A number of token-based approaches have been
discussed in the literature, both supervised (Katz
and Giesbrech (2006)), weakly supervised (Birke and
Sarkar (2006)) and unsupervised (Sporleder and Li
(2009); Fazly et al. (2009)). Fazly et al. (2009) de-
velop statistical measures for each linguistic property
of idiomatic expressions and use them both in a type-
based classification task and in a token identification
task, in which they distinguish idiomatic and literal us-
ages of potentially idiomatic expressions in context.
Sporleder and Li (2009) present a graph-based model
for representing the lexical cohesion of a discourse.
Nodes represent tokens in the discourse, which are con-
nected by edges whose value is determined by a seman-
tic relatedness function. They experiment with two dif-
ferent approaches to semantic relatedness: 1) Depen-
dency vectors, as described in Pado and Lapata (2007);
2) Normalized Google Distance (Cilibrasi and Vit´anyi
(2007)). Sporleder and Li (2009) show that this method
works better for larger contexts (greater than five para-
graphs). Li and Sporleder (2010) assume that literal
and figurative data are generated by two different Gaus-
sians, literal and non-literal and the detection is done by
comparing which Gaussian model has a higher prob-
ability to generate a specific instance. The approach
assumes that the target expressions are already known
and the goal is to determine whether this expression is
literal or figurative in a particular context. The impor-
tant insight of this method is that figurative language
in general exhibits less semantic cohesive ties with the
context than literal language.
Feldman and Peng (2013) describe several ap-
proaches to automatic idiom identification. One of
them is idiom recognition as outlier detection. They
apply principal component analysis for outlier detec-
tion – an approach that does not rely on costly an-
notated training data and is not limited to a specific
type of a syntactic construction, and is generally lan-
guage independent. The quantitative analysis provided
in their work shows that the outlier detection algorithm
performs better and seems promising. The qualitative
analysis also shows that their algorithm has to incor-
porate several important properties of the idioms: (1)
Idioms are relatively non-compositional, comparing to
literal expressions or other types of collocations. (2)
Idioms violate local cohesive ties, as a result, they are
semantically distant from the local topics. (3) While
not all semantic outliers are idioms, non-compositional
semantic outliers are likely to be idiomatic. (4) Id-
iomaticity is not a binary property. Idioms fall on the
continuum from being compositional to being partly
unanalyzable to completely non-compositional.
The approach described below is taking Feldman
and Peng (2013)’s original idea and is trying to address
(2) directly and (1) indirectly. Our approach is also
somewhat similar to Li and Sporleder (2010) because it
also relies on a list of potentially idiomatic expressions.
</bodyText>
<sectionHeader confidence="0.989839" genericHeader="method">
3 Our Hypothesis
</sectionHeader>
<bodyText confidence="0.998704666666667">
Similarly to Feldman and Peng (2013), out starting
point is that idioms are semantic outliers that violate
cohesive structure, especially in local contexts. How-
ever, our task is framed as supervised classification and
we rely on data annotated for idiomatic and literal ex-
pressions. We hypothesize that words in a given text
</bodyText>
<page confidence="0.981863">
2020
</page>
<bodyText confidence="0.9980465">
segment, such as a paragraph, that are high-ranking
representatives of a common topic of discussion are
less likely to be a part of an idiomatic expression in
the document.
</bodyText>
<subsectionHeader confidence="0.999404">
3.1 Topic Space Representation
</subsectionHeader>
<bodyText confidence="0.999970611111112">
Instead of the simple bag of words representation of a
target document (segment of three paragraphs that con-
tains a target phrase), we investigate the bag of words
topic representation for target documents. That is, we
extract topics from paragraphs containing idioms and
from paragraphs containing literals using an unsuper-
vised clustering method, Latent Dirichlet Allocation
(LDA) (Blei et al., 2003). The idea is that if the LDA
model is able to capture the semantics of a target docu-
ment, an idiomatic phrase will be a “semantic” outlier
of the themes. Thus, this topic representation will al-
low us to differentiate idioms from literals using the
semantics of the local context.
Let d = {w1, · · · , wN}t be a segment (document)
containing a target phrase, where N denotes the num-
ber of terms in a given corpus, and t represents trans-
pose. We first compute a set of m topics from d. We
denote this set by
</bodyText>
<equation confidence="0.997626">
T(d) = {t1, ··· , t,,,,},
</equation>
<bodyText confidence="0.999968865384616">
where ti = (w1, · · · , wk)t. Here wj represents a word
from a vocabulary of W words. Thus, we have two
representations for d: (1) d, represented by its original
terms, and (2) ˆd, represented by its topic terms. Two
corresponding term by document matrices will be de-
noted by MD and M ˆD, respectively, where D denotes
a set of documents. That is, MD represents the original
“text” term by document matrix, while MDˆ represents
the “topic” term by document matrix.
Figure 1 shows the potential benefit of topic space
representation. In the figure, text segments containing
target phrase “blow whistle” are projected on a two di-
mensional subspace. The left figure shows the projec-
tion in the “text” space, represented by the term by doc-
ument matrix MD. The middle figure shows the projec-
tion in the topic space, represented by M ˆD. The topic
space representation seems to provide a better separa-
tion.
We note that when learning topics from a small data
sample, learned topics can be less coherent and inter-
pretable, thus less useful. To address this issue, regu-
larized LDA has been proposed in the literature (New-
man et al., 2011). A key feature is to favor words that
exhibit short range dependencies for a given topic. We
can achieve a similar effect by placing restrictions on
the vocabulary. For example, when extracting topics
from segments containing idioms, we may restrict the
vocabulary to contain words from these segments only.
The middle and right figures in Figure 1 illustrate a case
in point. The middle figure shows a projection onto the
topic space that is computed with a restricted vocabu-
lary, while the right figure shows a projection when we
place no restriction on the vocabulary. That is, the vo-
cabulary includes terms from documents that contain
both idioms and literals.
Note that by computing M ˆD, the topic term by doc-
ument matrix, from the training data, we have created
a vocabulary, or a set of “features” (i.e., topic terms)
that is used to directly describe a query or test segment.
The main advantage is that topics are more accurate
when computed by LDA from a large collection of id-
iomatic or literal contexts. Thus, these topics capture
more accurately the semantic contexts in which the tar-
get idiomatic and literal expressions typically occur. If
a target query appears in a similar semantic context, the
topics will be able to describe this query as well. On the
other hand, one might similarly apply LDA to a given
query to extract query topics, and create the query vec-
tor from the query topics. The main disadvantage is
that LDA may not be able to extract topic terms that
match well with those in the training corpus, when ap-
plied to the query in isolation.
</bodyText>
<subsectionHeader confidence="0.996871">
3.2 Algorithm
</subsectionHeader>
<bodyText confidence="0.89568275">
The main steps of the proposed algorithm, called
TopSpace, are shown below.
Input: D = {d1, · · · , dk, dk+1, · · · , dn}: training
documents of k idioms and n − k literals.
</bodyText>
<equation confidence="0.708981">
Q = {q1, · · · , ql}: l query documents.
</equation>
<listItem confidence="0.999622133333333">
1. Let DicI be the vocabulary determined solely
from idioms {d1, · · · , dk}. Similarly, let DicL
be the vocabulary obtained from literals
{dk+1, ··· , dn}.
2. For a document di in {d1, · · · , dk}, apply LDA
to extract a set of m topics T(di) = {t1, · · · , t,,,,}
using DicI. For di ∈ {dk+1, · · · , dn}, DicL is
used.
3. Let Dˆ = { ˆd1, ··· , ˆdk, ˆdk+1, ··· , ˆdn} be the
resulting topic representation of D.
4. Compute the term by document matrix MDˆ from
ˆD, and let DicT and gw be the resulting
dictionary and global weight (idf), respectively.
5. Compute the term by document matrix MQ from
Q, using DicT and gw from the previous step.
</listItem>
<bodyText confidence="0.928489555555556">
Output: MDˆ and MQ
To summarize, after splitting our corpus (see section
4) into paragraphs and preprocessing it, we extract top-
ics from paragraphs containing idioms and from para-
graphs containing literals. We then compute a term by
document matrix, where terms are topic terms and doc-
uments are topics extracted from the paragraphs. Our
test data are represented as a term-by-document matrix
as well (See the details in section 5).
</bodyText>
<page confidence="0.945393">
2021
</page>
<figure confidence="0.993872424242424">
2D Text Space: Blow Whistle
2D Topic Space: Blow Whistle
2D Topic Space: Blow Whistle
−10
−12 −10 −8 −6 −4 −2 0 2 4 6 8
−20
−100 −80 −60 −40 −20 0 20
−5
−20 −15 −10 −5 0 5 10 15
Idioms
Literals
25
20
15
10
5
0
−5
100
80
60
40
20
0
20
15
10
5
0
Idioms
Literals
Idioms
Literals
</figure>
<figureCaption confidence="0.965828">
Figure 1: 2D projection of text segments containing “blow whistle.” Left panel: Original text space. Middle panel:
Topic space with restricted vocabulary. Right panel: Topic space with enlarged vocabulary.
</figureCaption>
<subsectionHeader confidence="0.992715">
3.3 Fisher Linear Discriminant Analysis
</subsectionHeader>
<bodyText confidence="0.9999628">
Once MDˆ and MQ are obtained, a classification rule
can be applied to predict idioms vs. literals. The ap-
proach we are taking in this work for classifying id-
ioms vs. literals is based on Fisher’s discriminant anal-
ysis (FDA) (Fukunaga, 1990). FDA often significantly
simplifies tasks such as regression and classification by
computing low-dimensional subspaces having statisti-
cally uncorrelated or discriminant variables. In lan-
guage analysis, statistically uncorrelate or discriminant
variables are extracted and utilized for description, de-
tection, and classification. Woods et al. (1986), for ex-
ample, use statistically uncorrelated variables for lan-
guage test scores. A group of subjects is scored on a
battery of language tests, where the subtests measure
different abilities such as vocabulary, grammar or read-
ing comprehension. Horvath (1985) analyzes speech
samples of Sydney speakers to determine the relative
occurrence of five different variants of each of five
vowels sounds. Using this data, the speakers cluster
according to such factors as gender, age, ethnicity and
socio-economic class.
A similar approach has been discussed in Peng et al.
(2010). FDA is a class of methods used in machine
learning to find the linear combination of features that
best separate two classes of events. FDA is closely
related to principal component analysis (PCA), where
a linear combination of features that best explains the
data. Discriminant analysis explicitly exploits class in-
formation in the data, while PCA does not.
Idiom classification based on discriminant analysis
has several advantages. First, as has been mentioned,
it does not make any assumption regarding data distri-
butions. Many statistical detection methods assume a
Gaussian distribution of normal data, which is far from
reality. Second, by using a few discriminants to de-
scribe data, discriminant analysis provides a compact
representation of the data, resulting in increased com-
putational efficiency and real time performance.
In FDA, within-class, between-class, and mixture
scatter matrices are used to formulate the criteria of
class separability. Consider a J class problem, where
m0 is the mean vector of all data, and mj is the mean
vector of jth class data. A within-class scatter ma-
trix characterizes the scatter of samples around their
respective class mean vector, and it is expressed by
</bodyText>
<equation confidence="0.944477">
(xji − mj)(xji − mj)t, (1)
</equation>
<bodyText confidence="0.997190666666667">
where lj is the size of the data in the jth class, pj
(Ej pj = 1) represents the proportion of the jth class
contribution, and t denotes the transpose operator. A
between-class scatter matrix characterizes the scatter of
the class means around the mixture mean m0. It is ex-
pressed by
</bodyText>
<equation confidence="0.788601">
pj(mj − m0)(mj − m0)t. (2)
</equation>
<bodyText confidence="0.999781333333333">
The mixture scatter matrix is the covariance matrix of
all samples, regardless of their class assignment, and it
is given by
</bodyText>
<equation confidence="0.9958154">
(xi − m0)(xi − m0)t = Sw + Sb. (3)
The Fisher criterion is used to find a projection matrix
W E Rq×d that maximizes
J(W) = |W tSbW|
|WtSwW|. (4)
</equation>
<bodyText confidence="0.999793285714286">
In order to determine the matrix W that maximizes
J(W), one can solve the generalized eigenvalue prob-
lem: Sbwi = λiSwwi. The eigenvectors corresponding
to the largest eigenvalues form the columns of W. For a
two class problem, it can be written in a simpler form:
Sww = m = m1 − m2, where m1 and m2 are the
means of the two classes.
</bodyText>
<sectionHeader confidence="0.983455" genericHeader="method">
4 Data preprocessing
</sectionHeader>
<subsectionHeader confidence="0.889759">
4.1 Verb-noun constructions
</subsectionHeader>
<bodyText confidence="0.999367">
For our experiments we use the British National Cor-
pus (BNC, Burnard (2000)) and a list of verb-noun con-
structions (VNCs) extracted from BNC by Fazly et al.
</bodyText>
<equation confidence="0.9855245">
pj
Sw =
lj
i=1
J
E
j=1
J
E
j=1
Sb =
�l
i=1
Sm =
</equation>
<page confidence="0.902458">
2022
</page>
<bodyText confidence="0.945871857142857">
(2009); Cook et al. (2008) and labeled as L (Literal),
I (Idioms), or Q (Unknown). The list contains only
those VNCs whose frequency was greater than 20 and
that occurred at least in one of two idiom dictionaries
(Cowie et al., 1983; Seaton and Macaulay, 2002). The
dataset consists of 2,984 VNC tokens. For our experi-
ments we only use VNCs that are annotated as I or L.
</bodyText>
<subsectionHeader confidence="0.977364">
4.2 Lemmatization
</subsectionHeader>
<bodyText confidence="0.999945142857143">
Instead of dealing with various forms of the same root,
we use lemmas provided by the BNC XML annotation,
so our corpus is lemmatized. We also apply the (modi-
fied) Google stop list before extracting the topics. The
reason we modified the stop list is that some function
words can potentially be idiom components (e.g., cer-
tain prepositions).
</bodyText>
<subsectionHeader confidence="0.991055">
4.3 Paragraphs
</subsectionHeader>
<bodyText confidence="0.99784475">
We use the original SGML annotation to extract para-
graghs from BNC. We only kept the paragraphs that
contained VNCs for our experiments. We experi-
mented with texts of one paragraph length (single para-
graph contexts) and of three-paragraph length (multi-
paragraph contexts). An example of multi-paragraph
contexts is shown below:
So, reluctantly, I joined Jack Hobbs in not rocking
the boat, reporting the play and the general uproar with
perhaps too much impartiality. My reports went to all
British newspapers, with special direct services by me
to India, South Africa and West Indies; even to King
George V in Buckingham Palace, who loved his cricket.
In other words, I was to some extent leading the British
public astray.
I regret I can shed little new light on the mystery of
who blew the whistle on the celebrated dressing-room
scene after Woodfull was hit. while he was lying on the
massage table after his innings waiting for a doctor,
Warner and Palairet called to express sympathy.
Most versions of Woodfull’s reply seem to agree that
he said. There are two teams out there on the oval.
One is playing cricket, the other is not. This game is
too good to be spoilt. It is time some people got out of
it. Warner and Palairet were too taken aback to reply.
They left the room in embarrassment.
Single paragraph contexts simply consist of the mid-
dle paragraph.
</bodyText>
<sectionHeader confidence="0.999572" genericHeader="method">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.85">
5.1 Methods
</subsectionHeader>
<bodyText confidence="0.999941714285714">
We have carried out an empirical study evaluating the
performance of the proposed algorithm. For compar-
ison, the following methods are evaluated. (1) The
proposed algorithm TopSpace (1), where the data are
represented in topic space. (2) TexSpace algorithm,
where the data are represented in original text space.
For each representation, two classification schemes are
applied: a) FDA (Eq. 4), followed by the nearest neigh-
bor rule. b) SVMs with Gaussian kernels (Cristianini
and Shawe-Taylor (2000)). For the nearest neighbor
rule, the number of nearest neighbors is set to rn/5],
where n denotes the number of training examples. For
SVMs, kernel width and soft margin parameters are set
to default values.
</bodyText>
<subsectionHeader confidence="0.998754">
5.2 Data Sets
</subsectionHeader>
<bodyText confidence="0.999854636363636">
The following data sets are used to evaluate the perfor-
mance of the proposed technique. These data sets have
enough examples from both idioms and literals to make
our results meaningful. On average, the training data is
6K word tokens. Our test data is of a similar size.
BlowWhistle: This data set has 78 examples, 27 of
which are idioms and the remaining 51 are literals. The
training data for BlowWhistle consist of 40 randomly
chosen examples (20 paragraphs containing idioms and
20 paragraphs containing literals). The remaining 38
examples (7 idiomatic and 31 literals) are used as test
data.
MakeScene: This data set has 50 examples, 30 of
which are paragraphs containing idioms and the re-
maining 20 are paragraphs containing literals. The
training data for MakeScene consist of 30 randomly
chosen examples, 15 of which are paragraphs contain-
ing make scene as an idiom and the rest 15 are para-
graphs containing make scene as a literal. The remain-
ing 20 examples (15 idiomatic paragraphs and 5 liter-
als) are used as test data.
LoseHead: This data set has 40 examples, 21 of
which are idioms and the remaining 19 are literals.
The training data for LoseHead consist of 30 randomly
chosen examples (15 idiomatic and 15 literal). The
remaining 10 examples (6 idiomatic and 4 literal) are
used as test data.
TakeHeart: This data set has 81 examples, 61 of
which are idioms and the remaining 20 are literals. The
training data for TakeHeart consist of 30 randomly
chosen examples (15 idiomatic and 15 literals). The
remaining 51 examples (46 idiomatic and 5 literals) are
used as test data.
</bodyText>
<subsectionHeader confidence="0.999593">
5.3 Adding affect
</subsectionHeader>
<bodyText confidence="0.9999915">
Nunberg et al. (1994) notice that “idioms are typically
used to imply a certain evaluation or affective stance
toward the things they denote”. Language users usu-
ally choose an idiom in non-neutral contexts. The situ-
ations that idioms describe can be positive or negative;
however, the polarity of the context is not as impor-
tant as the strength of the emotion expressed. So, we
decided to incorporate the knowledge about the emo-
tion strength into our algorithm. We use a database of
word norms collected by Warriner et al. (2013). This
database contains almost 14,000 English lemmas an-
notated with three components of emotions: valence
(the pleasantness of a stimulus), arousal (the intensity
of emotion provoked by a stimulus), and dominance
</bodyText>
<page confidence="0.994925">
2023
</page>
<tableCaption confidence="0.99977">
Table 1: Average accuracy of competing methods on four datasets in single paragraph contexts: A = Arousal
</tableCaption>
<table confidence="0.998882">
Model BlowWhistle LoseHead MakeScene TakeHeart
Prec Recall Acc Prec Recall Acc Prec Recall Acc Prec Recall Acc
FDA-Topics 0.44 0.40 0.79 0.70 0.90 0.70 0.82 0.97 0.81 0.91 0.97 0.89
FDA-Topics+A 0.51 0.51 0.75 0.78 0.68 0.66 0.80 0.99 0.80 0.93 0.84 0.80
FDA-Text 0.37 0.81 0.63 0.60 0.88 0.58 0.82 0.89 0.77 0.36 0.38 0.41
FDA-Text+A 0.42 0.49 0.76 0.64 0.92 0.63 0.83 0.95 0.82 0.75 0.53 0.53
SVMs-Topics 0.08 0.39 0.59 0.28 0.25 0.45 0.59 0.74 0.61 0.91 1.00 0.91
SVMs-Topics+A 0.06 0.21 0.69 0.38 0.18 0.44 0.53 0.40 0.44 0.91 1.00 0.91
SVMs-Text 0.08 0.39 0.59 0.36 0.60 0.52 0.23 0.30 0.40 0.42 0.16 0.22
SVMs-Text+A 0.15 0.51 0.60 0.31 0.38 0.48 0.37 0.40 0.45 0.95 0.48 0.50
</table>
<bodyText confidence="0.99930185">
(the degree of control exerted by a stimulus). These
components were elicited from human subjects via an
Amazon Mechanical Turk crowdsourced experiment.
We only used the arousal feature in our experiments
because we were interested in the intensity of the emo-
tion rather than its valence.
For a document d = {w1, · · · , wN}t, we calculate
the corresponding arousal value ai for each wi, ob-
taining dA = {a1, · · · , aN}t. Let mA be the aver-
age arousal value calculated over the entire training
data. The centered arousal value for a training docu-
ment is obtained by subtracting mA from dA, i.e., ¯dA =
dA −mA = {a1 −mA, · · · , aN −mA}t. Similarly, the
centered arousal value for a query is computed accord-
ing to ¯qA = qA − mA = {q1 − mA,··· , qN − mA}t.
That is, the training arousal mean is used to center both
training and query arousal values. The corresponding
arousal matrices for D, ˆD, and Q are AD, A D, AQ, re-
spectively. To incorporate the arousal feature, we sim-
ply compute
</bodyText>
<equation confidence="0.855201333333333">
OD = MD + AD, (5)
and
O D� = MD� + A D. (6)
</equation>
<bodyText confidence="0.9874405">
The arousal feature can be similarly incoporated into
query OQ = MQ + AQ.
</bodyText>
<sectionHeader confidence="0.999843" genericHeader="method">
6 Results
</sectionHeader>
<bodyText confidence="0.992389388888889">
Table 1 shows the average precision, recall, and ac-
curacy of the competing methods on the four data
sets over 10 runs in simple paragraph contexts. Table
2 shows the results for the multi-paragraph contexts.
Note that for single paragraph contexts, we chose two
topics, each having 10 terms. For multi-paragrah con-
texts, we had four topics, with 10 terms per topic. No
optimization was made for selecting the number of top-
ics as well as the number of terms per topic. In the
tables, the best performance in terms of the sum of pre-
cision, recall and accuracy is given in boldface.
The results show that the topic representation
achieved the best performance in 6 out of 8 cases. Fig-
ure 2 plots the overall aggregated performance in terms
of topic vs text representations across the entire data
sets, regardless of the classifiers used. Everything else
being equal, this clearly shows the advantage of topics
over simple text representation.
</bodyText>
<figureCaption confidence="0.859577">
Figure 2: Aggregated performance: Topic vs text rep-
resentations.
</figureCaption>
<bodyText confidence="0.99954735">
The arousal feature (Eqs 5 and 6) also improved the
overall performance, particularly in text representation
(Eq. 5). This can be seen in the top panel in Figure 3.
In fact, in 2/8 cases, text representation coupled with
the arousal feature achieved the best performance. One
possible explanation is that the LDA model already per-
formed “feature” selection (choosing topic terms), to
the extent possible. Thus, any additional information
such as arousal only provides marginal improvement
at the best (bottom panel in Figure 3). On the other
hand, original text represents “raw” features, whereby
arousal information helps provide better contexts, thus
improving overall performance.
Figure 4 shows a case in point: the average (sorted)
arousal values of idioms and literals of the target phrase
“lose head.” The upper panel plots arousal values in
the text space, while lower panel plots arousal values
in the topic space. The plot supports the results shown
in Tables 1 and 2, where the arousal feature generally
improves text representation.
</bodyText>
<sectionHeader confidence="0.698832" genericHeader="method">
7 Comparisons with other approaches
</sectionHeader>
<bodyText confidence="0.9997156">
Even though we used Fazly et al. (2009)’s dataset for
these experiments, the direct comparison with their
method is impossible here because our task is formu-
lated differently and we do not use the full dataset for
the experiments. Fazly et al. (2009)’s unsupervised
</bodyText>
<figure confidence="0.99709825">
Precision Recall Accuracy
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
Topics
Text
</figure>
<page confidence="0.992303">
2024
</page>
<tableCaption confidence="0.997813">
Table 2: Average accuracy of competing methods on four datasets in multiple paragraph contexts: A = Arousal
</tableCaption>
<table confidence="0.9993783">
Model BlowWhistle LoseHead MakeScene TakeHeart
Prec Recall Acc Prec Recall Acc Prec Recall Acc Prec Recall Acc
FDA-Topics 0.62 0.60 0.83 0.76 0.97 0.78 0.79 0.95 0.77 0.93 0.99 0.92
FDA-Topics+A 0.47 0.44 0.79 0.74 0.93 0.74 0.82 0.69 0.65 0.92 0.98 0.91
FDA-Text 0.65 0.43 0.84 0.72 0.73 0.65 0.79 0.95 0.77 0.46 0.40 0.42
FDA-Text+A 0.45 0.49 0.78 0.67 0.88 0.65 0.80 0.99 0.80 0.47 0.29 0.33
SVMs-Topics 0.07 0.40 0.56 0.60 0.83 0.61 0.46 0.57 0.55 0.90 1.00 0.90
SVMs-Topics+A 0.21 0.54 0.55 0.66 0.77 0.64 0.42 0.29 0.41 0.91 1.00 0.91
SVMs-Text 0.17 0.90 0.25 0.30 0.50 0.50 0.10 0.01 0.26 0.65 0.21 0.26
SVMs-Text+A 0.24 0.87 0.41 0.66 0.85 0.61 0.07 0.01 0.26 0.74 0.13 0.20
</table>
<figure confidence="0.976032944444444">
Te# Representation
0 5 10 15
Precision Recall Accuracy
Text
Text+A
Idioms
Literals
Arousal Values 0.2
0
−0.2
−0.4
−0.6
−0.8
−1
−1.2
−1.4
Te# Terms
Precision Recall Accuracy
</figure>
<figureCaption confidence="0.670181">
Figure 3: Aggregated performance: Text
vs. text+Arousal representations (top) and Top-
ics vs. Topics+Arousal representations (bottom).
</figureCaption>
<figure confidence="0.986256">
1.5
0 5 10 15
Topic Terms
</figure>
<figureCaption confidence="0.997416">
Figure 4: Average arousal values–Upper panel: Text
space. Lower panel: Topic space.
</figureCaption>
<figure confidence="0.999796074074074">
Topics+A
Topics
Topic Representation
Arousal Values 1
0.5
0
−0.5
−1
Idioms
Literals
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
</figure>
<bodyText confidence="0.99704475">
model that relies on the so-called canonical forms gives
72.4% (macro-)accuracy on the extraction of idiomatic
tokens when evaluated on their test data.
We cannot compare our method directly with the
other methods discussed in section 2 either because
each uses a different dataset or formulates the task
differently (detection vs. recognition vs. identifica-
tion). However, we can compare the method presented
here with Feldman and Peng (2013) who also experi-
ment with LDA, use similar data, and frame the prob-
lem as classification. Their goal, however, is to clas-
sify sentences as either idiomatic or literal. To obtain
a discriminant subspace, they train their model on a
small number of randomly selected idiomatic and non-
idiomatic sentences. They then project both the train-
ing and the test data on the chosen subspace and use
the three nearest neighbor (3NN) classifier to obtain
accuracy. The average accuracy they report is 80%.
Our method clearly outperforms the Feldman and Peng
(2013) approach (at least on the dataset we use).
</bodyText>
<sectionHeader confidence="0.993777" genericHeader="discussions">
8 Discussion and Conclusion
</sectionHeader>
<bodyText confidence="0.999953428571429">
We have described an algorithm for automatic classi-
fication of idiomatic and literal expressions. We have
investigated the bag of words topic representation for
target documents (segments of one or three paragraphs
that contains a target phrase). The approach definitely
outperforms the baseline model that is based on the
simple bag of words representation, but it also outper-
forms approaches previously discussed in the literature.
Our model captures the local semantics and thus is ca-
pable to identify semantic outliers (=idioms).
While we realize that the data set we use is small, the
results are encouraging. We notice that using 3 para-
graphs for local contexts improves the performance of
the classifiers. The reason is that some paragraphs are
</bodyText>
<page confidence="0.971194">
2025
</page>
<bodyText confidence="0.997489482758621">
relatively short. A larger context provides more related
terms, which gives LDA more opportunities to sample
these terms.
Idioms are also relatively non-compositional. While
we do not measure their non-compositionality in this
approach, we indirectly touch upon this property by hy-
pothesizing that non-compositional idiomatic expres-
sions are likely to be far from the local topics.
We feel that incorporating the intensity of emotion
expressed by the context into our model improves per-
formance, in particular, in text representation. When
we performed a qualitative analysis of the results try-
ing to determine the causes of false positives and neg-
atives, we noticed that there were quite a number of
cases that improved after incorporating the arousal fea-
ture into the model. For example, the FDA:topic classi-
fier labels ”blow the whistle” as literal in the following
context, but FDA:topics+A marks this expression as id-
iomatic (italicized words indicate words with relatively
high arousal values):
Peter thought it all out very carefully. He decided the wis-
est course was to pool all he had made over the last two years,
enabling Julian to purchase the lease of a high street property.
This would enable them to set up a business on a more set-
tled and permanent trading basis. Before long they opened a
grocery-cum-delicatessen in a good position as far as passing
trade was concerned. Peter’s investment was not misplaced.
The business did very well with the two lads greatly appreci-
ated locally for their hard work and quality of service. The
range of goods they were able to carry was welcomed in the
area, as well as lunchtime sandwich facilities which had pre-
viously been missing in the neighbourhood.
Success was the fruit of some three years’ strenuous work.
But it was more than a shock when Julian admitted to Pe-
ter that he had been running up huge debts with their bank.
Peter knew that Julian gambled, but he hadn’t expected him
to gamble to that level, and certainly not to use the shop as
security. With continual borrowing over two years, the bank
had blown the whistle. Everything was gone. Julian was
bankrupt. Even if they’d had a formal partnership, which
they didn’t, it would have made no difference. Peter lost all
he’d made, and with it his chance to help his parents and his
younger brother and sister, Toby and Laura.
Peter was heartbroken. His father had said all along: nei-
ther a lender nor a borrower. Peter had found out the hard
way. But as his mother observed, he was the same Peter, he’d
pick himself up somehow. Once again, Peter was resolute. He
made up his mind he’d never make the same mistake twice. It
wasn’t just the money or the hard work, though the waste of
that was difficult enough to accept. Peter had been working
a debt of love. He’d done all this for his parents, particularly
for his father, whose dedication to his children had always
impressed Peter and moved him deeply. And now it had all
come to nothing.
Therefore, we think that idioms have the tendency to
appear in more affective contexts; and we think that in-
corporating more sophisticated sentiment analysis into
our model will improve the results.
</bodyText>
<sectionHeader confidence="0.998844" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999108666666667">
This material is based upon work supported by the Na-
tional Science Foundation under Grant No. 1319846.
We also thank the anonymous reviewers for useful
comments. The third author thanks the Fulbright Foun-
dation for giving her an opportunity to conduct this re-
search at Montclair State University (MSU).
</bodyText>
<sectionHeader confidence="0.996335" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9993407">
Ariel, M. (2002). The demise of unique concept of
literal meaning. Journal of Pragmatics 34, 361–402.
Birke, J. and A. Sarkar (2006). A clustering approach
to the nearly unsupervised recognition of nonliteral
language. In Proceedings of the 11th Conference of
the European Chapter of the Association for Com-
putational Linguistics (EACL’06), Trento, Italy, pp.
329–226.
Blei, D., A. Ng, and M. Jordan (2003). Latent Dirich-
let Allocation. Journal of Machine Learning Re-
search 3, 993–1022.
Burnard, L. (2000). The British National Corpus Users
Reference Guide. Oxford University Computing Ser-
vices.
Cilibrasi, R. and P. M. B. Vit´anyi (2007). The
google similarity distance. IEEE Trans. Knowl. Data
Eng. 19(3), 370–383.
Cook, P., A. Fazly, and S. Stevenson (2007). Pulling
their weight: Exploiting syntactic forms for the auto-
matic identification of idiomatic expressions in con-
text. In Proceedings of the ACL 07 Workshop on A
Broader Perspective on Multiword Expressions, pp.
41–48.
Cook, P., A. Fazly, and S. Stevenson (2008, June). The
VNC-Tokens Dataset. In Proceedings of the LREC
Workshop: Towards a Shared Task for Multiword
Expressions (MWE 2008), Marrakech, Morocco.
Cowie, A. P., R. Mackin, and I. R. McCaig (1983). Ox-
ford Dictionary of Current Idiomatic English, Vol-
ume 2. Oxford University Press.
Cristianini, N. and J. Shawe-Taylor (2000). An In-
troduction to Support Vector Machines and other
kernel-based learning methods. Cambridge, UK:
Cambridge University Press.
Fazly, A., P. Cook, and S. Stevenson (2009). Unsu-
pervised Type and Token Identification of Idiomatic
Expressions. Computational Linguistics 35(1), 61–
103.
Feldman, A. and J. Peng (2013). Automatic detec-
tion of idiomatic clauses. In Computational Linguis-
tics and Intelligent Text Processing, pp. 435–446.
Springer.
Fukunaga, K. (1990). Introduction to statistical pattern
recognition. Academic Press.
Hearst, M. A. (1992). Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings of
the 14th Conference on Computational Linguistics
- Volume 2, COLING ’92, Stroudsburg, PA, USA,
pp. 539–545. Association for Computational Lin-
guistics.
</reference>
<page confidence="0.832211">
2026
</page>
<reference confidence="0.999831377358491">
Horvath, B. M. (1985). Variation in Australian English.
Cambridge: Cambridge University PRess.
Katz, G. and E. Giesbrech (2006). Automatic Iden-
tification of Non-compositional Multiword Expres-
sions using Latent Semantic Analysis. In Proceed-
ings of the ACL/COLING-06 Workshop on Multi-
word Expressions: Identifying and Exploiting Un-
derlying Properties, pp. 12–19.
Li, L. and C. Sporleder (2010). Using Gaussian Mix-
ture Models to Detect Figurative Language in Con-
text. In Proceedings of NAACL/HLT 2010.
Newman, D., E. V. Bonilla, and W. L. Buntine (2011).
Improving topic coherence with regularized topic
models. In NIPS, pp. 496–504.
Nunberg, G., I. A. Sag, and T. Wasow (1994). Idioms.
Language 70(3), 491–538.
Pado, S. and M. Lapata (2007). Dependency-based
construction of semantic space models. Computa-
tional Linguistics 33(2), 161–199.
Peng, J., A. Feldman, and L. Street (2010). Comput-
ing linear discriminants for idiomatic sentence de-
tection. Research in Computing Science, Special is-
sue: Natural Language Processing and its Applica-
tions 46, 17–28.
Sag, I. A., T. Baldwin, F. Bond, A. Copestake, and
D. Flickinger (2002). Multiword expressions: A
Pain in the Neck for NLP. In Proceedings of the
3rd International Conference on Intelligence Text
Processing and Computational Linguistics (CICLing
2002), Mexico City, Mexico, pp. 1–15.
Seaton, M. and A. Macaulay (Eds.) (2002). Collins
COBUILD Idioms Dictionary (second ed.). Harper-
Collins Publishers.
Sporleder, C. and L. Li (2009). Unsupervised Recogni-
tion of Literal and Non-literal Use of Idiomatic Ex-
pressions. In EACL ’09: Proceedings of the 12th
Conference of the European Chapter of the Associa-
tion for Computational Linguistics, Morristown, NJ,
USA, pp. 754–762. Association for Computational
Linguistics.
Warriner, A. B., V. Kuperman, and M. Brysbaert
(2013). Norms of valence, arousal, and dominance
for 13,915 english lemmas. Behavior Research
Methods 44(4).
Widdows, D. and B. Dorow (2005). Automatic extrac-
tion of idioms using graph analysis and asymmet-
ric lexicosyntactic patterns. In Proceedings of the
ACL-SIGLEX Workshop on Deep Lexical Acquisi-
tion, DeepLA ’05, Stroudsburg, PA, USA, pp. 48–
56. Association for Computational Linguistics.
Woods, A., P. Fletcher, and A. Hughes (1986). Statis-
tics in Language Studies. Cambridge: Cambridge
University Press.
</reference>
<page confidence="0.994422">
2027
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.486591">
<title confidence="0.9941505">Classifying Idiomatic and Literal Expressions Using Topic Models Intensity of Emotions</title>
<author confidence="0.999159">Jing Peng</author>
<author confidence="0.999159">Anna Feldman Ekaterina Vylomova</author>
<affiliation confidence="0.9993045">Computer Science/Linguistics Computer Science Montclair State University Bauman State Technical University</affiliation>
<address confidence="0.511674">Montclair, New Jersey, USA Moscow,</address>
<email confidence="0.999916">evylomova@gmail.com</email>
<abstract confidence="0.998714965517241">We describe an algorithm for automatic classification of idiomatic and literal expressions. Our starting point is that words in a given text segment, such as a paragraph, that are highranking representatives of a common topic of discussion are less likely to be a part of an idiomatic expression. Our additional hypothesis is that contexts in which idioms occur, typically, are more affective and therefore, we incorporate a simple analysis of the intensity of the emotions expressed by the contexts. We investigate the bag of words topic representation of one to three paragraphs containing an expression that should be classified as idiomatic or literal (a target phrase). We extract topics from paragraphs containing idioms and from paragraphs containing literals using an unsupervised clustering method, Latent Dirichlet Allocation (LDA) (Blei et al., 2003). Since idiomatic expressions exhibit the property of non-compositionality, we assume that they usually present different semantics than the words used in the local topic. We treat idioms as semantic outliers, and the identification of a semantic shift as outlier detection. Thus, this topic representation allows us to differentiate idioms from literals using local semantic contexts. Our results are encouraging.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Ariel</author>
</authors>
<title>The demise of unique concept of literal meaning.</title>
<date>2002</date>
<journal>Journal of Pragmatics</journal>
<volume>34</volume>
<pages>361--402</pages>
<contexts>
<context position="1728" citStr="Ariel (2002)" startWordPosition="257" endWordPosition="258">using an unsupervised clustering method, Latent Dirichlet Allocation (LDA) (Blei et al., 2003). Since idiomatic expressions exhibit the property of non-compositionality, we assume that they usually present different semantics than the words used in the local topic. We treat idioms as semantic outliers, and the identification of a semantic shift as outlier detection. Thus, this topic representation allows us to differentiate idioms from literals using local semantic contexts. Our results are encouraging. 1 Introduction The definition of what is literal and figurative is still object of debate. Ariel (2002) demonstrates that literal and non-literal meanings cannot always be distinguished from each other. Literal meaning is originally assumed to be conventional, compositional, relatively context independent, and truth conditional. The problem is that the boundary is not clear-cut, some figurative expressions are compositional – metaphors and many idioms; others are conventional – most of the idioms. Idioms present great challenges for many Natural Language Processing (NLP) applications. They can violate selection restrictions (Sporleder and Li, 2009) as in push one’s luck under the assumption tha</context>
</contexts>
<marker>Ariel, 2002</marker>
<rawString>Ariel, M. (2002). The demise of unique concept of literal meaning. Journal of Pragmatics 34, 361–402.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Birke</author>
<author>A Sarkar</author>
</authors>
<title>A clustering approach to the nearly unsupervised recognition of nonliteral language.</title>
<date>2006</date>
<booktitle>In Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics (EACL’06),</booktitle>
<pages>329--226</pages>
<location>Trento, Italy,</location>
<contexts>
<context position="7886" citStr="Birke and Sarkar (2006)" startWordPosition="1226" endWordPosition="1229">e properties, many idioms fall on the continuum from being compositional to being partly unanalyzable to completely noncompositional (Cook et al. (2007)). Fazly et al. (2009); Li and Sporleder (2010), among others, notice that type-based approaches do not work on expressions that can be interpreted idiomatically or literally depending on the context and thus, an approach that considers tokens in context is more appropriate for the task of idiom recognition. A number of token-based approaches have been discussed in the literature, both supervised (Katz and Giesbrech (2006)), weakly supervised (Birke and Sarkar (2006)) and unsupervised (Sporleder and Li (2009); Fazly et al. (2009)). Fazly et al. (2009) develop statistical measures for each linguistic property of idiomatic expressions and use them both in a typebased classification task and in a token identification task, in which they distinguish idiomatic and literal usages of potentially idiomatic expressions in context. Sporleder and Li (2009) present a graph-based model for representing the lexical cohesion of a discourse. Nodes represent tokens in the discourse, which are connected by edges whose value is determined by a semantic relatedness function.</context>
</contexts>
<marker>Birke, Sarkar, 2006</marker>
<rawString>Birke, J. and A. Sarkar (2006). A clustering approach to the nearly unsupervised recognition of nonliteral language. In Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics (EACL’06), Trento, Italy, pp. 329–226.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Blei</author>
<author>A Ng</author>
<author>M Jordan</author>
</authors>
<title>Latent Dirichlet Allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research</journal>
<volume>3</volume>
<pages>993--1022</pages>
<contexts>
<context position="1210" citStr="Blei et al., 2003" startWordPosition="175" endWordPosition="178"> less likely to be a part of an idiomatic expression. Our additional hypothesis is that contexts in which idioms occur, typically, are more affective and therefore, we incorporate a simple analysis of the intensity of the emotions expressed by the contexts. We investigate the bag of words topic representation of one to three paragraphs containing an expression that should be classified as idiomatic or literal (a target phrase). We extract topics from paragraphs containing idioms and from paragraphs containing literals using an unsupervised clustering method, Latent Dirichlet Allocation (LDA) (Blei et al., 2003). Since idiomatic expressions exhibit the property of non-compositionality, we assume that they usually present different semantics than the words used in the local topic. We treat idioms as semantic outliers, and the identification of a semantic shift as outlier detection. Thus, this topic representation allows us to differentiate idioms from literals using local semantic contexts. Our results are encouraging. 1 Introduction The definition of what is literal and figurative is still object of debate. Ariel (2002) demonstrates that literal and non-literal meanings cannot always be distinguished</context>
<context position="4780" citStr="Blei et al., 2003" startWordPosition="753" endWordPosition="756"> less likely to be a part of an idiomatic expression. Our additional hypothesis is that contexts in which idioms occur, typically, are more affective and therefore, we incorporate a simple analysis of the intensity of the emotions expressed by the contexts. We investigate the bag of words topic representation of one to three paragraphs containing an expression that should be classified as idiomatic or literal (a target phrase). We extract topics from paragraphs containing idioms and from paragraphs containing literals using an unsupervised clustering method, Latent Dirichlet Allocation (LDA) (Blei et al., 2003). Since idiomatic expressions exhibit the property of non-compositionality, we assume that they usually present different semantics than the words used 2019 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2019–2027, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics in the local topic. We treat idioms as semantic outliers, and the identification of semantic shift as outlier detection. Thus, this topic representation allows us to differentiate idioms from literals using the local semantics. The paper is organi</context>
<context position="11705" citStr="Blei et al., 2003" startWordPosition="1819" endWordPosition="1822">n text 2020 segment, such as a paragraph, that are high-ranking representatives of a common topic of discussion are less likely to be a part of an idiomatic expression in the document. 3.1 Topic Space Representation Instead of the simple bag of words representation of a target document (segment of three paragraphs that contains a target phrase), we investigate the bag of words topic representation for target documents. That is, we extract topics from paragraphs containing idioms and from paragraphs containing literals using an unsupervised clustering method, Latent Dirichlet Allocation (LDA) (Blei et al., 2003). The idea is that if the LDA model is able to capture the semantics of a target document, an idiomatic phrase will be a “semantic” outlier of the themes. Thus, this topic representation will allow us to differentiate idioms from literals using the semantics of the local context. Let d = {w1, · · · , wN}t be a segment (document) containing a target phrase, where N denotes the number of terms in a given corpus, and t represents transpose. We first compute a set of m topics from d. We denote this set by T(d) = {t1, ··· , t,,,,}, where ti = (w1, · · · , wk)t. Here wj represents a word from a voca</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>Blei, D., A. Ng, and M. Jordan (2003). Latent Dirichlet Allocation. Journal of Machine Learning Research 3, 993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Burnard</author>
</authors>
<title>The British National Corpus Users Reference Guide.</title>
<date>2000</date>
<institution>Oxford University Computing Services.</institution>
<contexts>
<context position="20208" citStr="Burnard (2000)" startWordPosition="3320" endWordPosition="3321">is given by (xi − m0)(xi − m0)t = Sw + Sb. (3) The Fisher criterion is used to find a projection matrix W E Rq×d that maximizes J(W) = |W tSbW| |WtSwW|. (4) In order to determine the matrix W that maximizes J(W), one can solve the generalized eigenvalue problem: Sbwi = λiSwwi. The eigenvectors corresponding to the largest eigenvalues form the columns of W. For a two class problem, it can be written in a simpler form: Sww = m = m1 − m2, where m1 and m2 are the means of the two classes. 4 Data preprocessing 4.1 Verb-noun constructions For our experiments we use the British National Corpus (BNC, Burnard (2000)) and a list of verb-noun constructions (VNCs) extracted from BNC by Fazly et al. pj Sw = lj i=1 J E j=1 J E j=1 Sb = �l i=1 Sm = 2022 (2009); Cook et al. (2008) and labeled as L (Literal), I (Idioms), or Q (Unknown). The list contains only those VNCs whose frequency was greater than 20 and that occurred at least in one of two idiom dictionaries (Cowie et al., 1983; Seaton and Macaulay, 2002). The dataset consists of 2,984 VNC tokens. For our experiments we only use VNCs that are annotated as I or L. 4.2 Lemmatization Instead of dealing with various forms of the same root, we use lemmas provid</context>
</contexts>
<marker>Burnard, 2000</marker>
<rawString>Burnard, L. (2000). The British National Corpus Users Reference Guide. Oxford University Computing Services.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Cilibrasi</author>
<author>P M B Vit´anyi</author>
</authors>
<title>The google similarity distance.</title>
<date>2007</date>
<journal>IEEE Trans. Knowl. Data Eng.</journal>
<volume>19</volume>
<issue>3</issue>
<pages>370--383</pages>
<marker>Cilibrasi, Vit´anyi, 2007</marker>
<rawString>Cilibrasi, R. and P. M. B. Vit´anyi (2007). The google similarity distance. IEEE Trans. Knowl. Data Eng. 19(3), 370–383.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Cook</author>
<author>A Fazly</author>
<author>S Stevenson</author>
</authors>
<title>Pulling their weight: Exploiting syntactic forms for the automatic identification of idiomatic expressions in context.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL 07 Workshop on A Broader Perspective on Multiword Expressions,</booktitle>
<pages>41--48</pages>
<contexts>
<context position="7415" citStr="Cook et al. (2007)" startWordPosition="1154" endWordPosition="1157">tactic patterns that occur frequently, are recognizable with little or no precoded knowledge, and indicate the lexical relation of interest. Widdows and Dorow (2005) use Hearst’s concept of lexicosyntactic patterns to extract idioms that consist of fixed patterns between two nouns. Basically, their technique works by finding patterns such as “thrills and spills”, whose reversals (such as “spills and thrills”) are never encountered. While many idioms do have these properties, many idioms fall on the continuum from being compositional to being partly unanalyzable to completely noncompositional (Cook et al. (2007)). Fazly et al. (2009); Li and Sporleder (2010), among others, notice that type-based approaches do not work on expressions that can be interpreted idiomatically or literally depending on the context and thus, an approach that considers tokens in context is more appropriate for the task of idiom recognition. A number of token-based approaches have been discussed in the literature, both supervised (Katz and Giesbrech (2006)), weakly supervised (Birke and Sarkar (2006)) and unsupervised (Sporleder and Li (2009); Fazly et al. (2009)). Fazly et al. (2009) develop statistical measures for each ling</context>
</contexts>
<marker>Cook, Fazly, Stevenson, 2007</marker>
<rawString>Cook, P., A. Fazly, and S. Stevenson (2007). Pulling their weight: Exploiting syntactic forms for the automatic identification of idiomatic expressions in context. In Proceedings of the ACL 07 Workshop on A Broader Perspective on Multiword Expressions, pp. 41–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Cook</author>
<author>A Fazly</author>
<author>S Stevenson</author>
</authors>
<title>The VNC-Tokens Dataset.</title>
<date>2008</date>
<booktitle>In Proceedings of the LREC Workshop: Towards a Shared Task for Multiword Expressions (MWE</booktitle>
<location>Marrakech, Morocco.</location>
<contexts>
<context position="20369" citStr="Cook et al. (2008)" startWordPosition="3356" endWordPosition="3359">In order to determine the matrix W that maximizes J(W), one can solve the generalized eigenvalue problem: Sbwi = λiSwwi. The eigenvectors corresponding to the largest eigenvalues form the columns of W. For a two class problem, it can be written in a simpler form: Sww = m = m1 − m2, where m1 and m2 are the means of the two classes. 4 Data preprocessing 4.1 Verb-noun constructions For our experiments we use the British National Corpus (BNC, Burnard (2000)) and a list of verb-noun constructions (VNCs) extracted from BNC by Fazly et al. pj Sw = lj i=1 J E j=1 J E j=1 Sb = �l i=1 Sm = 2022 (2009); Cook et al. (2008) and labeled as L (Literal), I (Idioms), or Q (Unknown). The list contains only those VNCs whose frequency was greater than 20 and that occurred at least in one of two idiom dictionaries (Cowie et al., 1983; Seaton and Macaulay, 2002). The dataset consists of 2,984 VNC tokens. For our experiments we only use VNCs that are annotated as I or L. 4.2 Lemmatization Instead of dealing with various forms of the same root, we use lemmas provided by the BNC XML annotation, so our corpus is lemmatized. We also apply the (modified) Google stop list before extracting the topics. The reason we modified the</context>
</contexts>
<marker>Cook, Fazly, Stevenson, 2008</marker>
<rawString>Cook, P., A. Fazly, and S. Stevenson (2008, June). The VNC-Tokens Dataset. In Proceedings of the LREC Workshop: Towards a Shared Task for Multiword Expressions (MWE 2008), Marrakech, Morocco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A P Cowie</author>
<author>R Mackin</author>
<author>I R McCaig</author>
</authors>
<date>1983</date>
<journal>Oxford Dictionary of Current Idiomatic English,</journal>
<volume>2</volume>
<publisher>Oxford University Press.</publisher>
<contexts>
<context position="20575" citStr="Cowie et al., 1983" startWordPosition="3393" endWordPosition="3396">a two class problem, it can be written in a simpler form: Sww = m = m1 − m2, where m1 and m2 are the means of the two classes. 4 Data preprocessing 4.1 Verb-noun constructions For our experiments we use the British National Corpus (BNC, Burnard (2000)) and a list of verb-noun constructions (VNCs) extracted from BNC by Fazly et al. pj Sw = lj i=1 J E j=1 J E j=1 Sb = �l i=1 Sm = 2022 (2009); Cook et al. (2008) and labeled as L (Literal), I (Idioms), or Q (Unknown). The list contains only those VNCs whose frequency was greater than 20 and that occurred at least in one of two idiom dictionaries (Cowie et al., 1983; Seaton and Macaulay, 2002). The dataset consists of 2,984 VNC tokens. For our experiments we only use VNCs that are annotated as I or L. 4.2 Lemmatization Instead of dealing with various forms of the same root, we use lemmas provided by the BNC XML annotation, so our corpus is lemmatized. We also apply the (modified) Google stop list before extracting the topics. The reason we modified the stop list is that some function words can potentially be idiom components (e.g., certain prepositions). 4.3 Paragraphs We use the original SGML annotation to extract paragraghs from BNC. We only kept the p</context>
</contexts>
<marker>Cowie, Mackin, McCaig, 1983</marker>
<rawString>Cowie, A. P., R. Mackin, and I. R. McCaig (1983). Oxford Dictionary of Current Idiomatic English, Volume 2. Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Cristianini</author>
<author>J Shawe-Taylor</author>
</authors>
<title>An Introduction to Support Vector Machines and other kernel-based learning methods.</title>
<date>2000</date>
<publisher>Cambridge University Press.</publisher>
<location>Cambridge, UK:</location>
<contexts>
<context position="22980" citStr="Cristianini and Shawe-Taylor (2000)" startWordPosition="3797" endWordPosition="3800">reply. They left the room in embarrassment. Single paragraph contexts simply consist of the middle paragraph. 5 Experiments 5.1 Methods We have carried out an empirical study evaluating the performance of the proposed algorithm. For comparison, the following methods are evaluated. (1) The proposed algorithm TopSpace (1), where the data are represented in topic space. (2) TexSpace algorithm, where the data are represented in original text space. For each representation, two classification schemes are applied: a) FDA (Eq. 4), followed by the nearest neighbor rule. b) SVMs with Gaussian kernels (Cristianini and Shawe-Taylor (2000)). For the nearest neighbor rule, the number of nearest neighbors is set to rn/5], where n denotes the number of training examples. For SVMs, kernel width and soft margin parameters are set to default values. 5.2 Data Sets The following data sets are used to evaluate the performance of the proposed technique. These data sets have enough examples from both idioms and literals to make our results meaningful. On average, the training data is 6K word tokens. Our test data is of a similar size. BlowWhistle: This data set has 78 examples, 27 of which are idioms and the remaining 51 are literals. The</context>
</contexts>
<marker>Cristianini, Shawe-Taylor, 2000</marker>
<rawString>Cristianini, N. and J. Shawe-Taylor (2000). An Introduction to Support Vector Machines and other kernel-based learning methods. Cambridge, UK: Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Fazly</author>
<author>P Cook</author>
<author>S Stevenson</author>
</authors>
<title>Unsupervised Type and Token Identification of Idiomatic Expressions.</title>
<date>2009</date>
<journal>Computational Linguistics</journal>
<volume>35</volume>
<issue>1</issue>
<pages>103</pages>
<contexts>
<context position="3718" citStr="Fazly et al. (2009)" startWordPosition="579" endWordPosition="582"> quick to complain. and The sergeant told the soldiers to hold their fire. Please hold your fire until I get out of the way. In fact, we tested the last two examples using the Google Translate engine and we got proper translations of the two neither into Russian nor into Hebrew, Spanish, or Chinese. Most current translation systems rely on large repositories of idioms. Unfortunately, these systems are not capable to tell apart literal from figurative usage of the same expression in context. Despite the common perception that phrases that can be idioms are mainly used in their idiomatic sense, Fazly et al. (2009)’s analysis of 60 idioms has shown that close to half of these also have a clear literal meaning; and of those with a literal meaning, on average around 40% of their usages are literal. In this paper we describe an algorithm for automatic classification of idiomatic and literal expressions. Our starting point is that words in a given text segment, such as a paragraph, that are high-ranking representatives of a common topic of discussion are less likely to be a part of an idiomatic expression. Our additional hypothesis is that contexts in which idioms occur, typically, are more affective and th</context>
<context position="6248" citStr="Fazly et al. (2009)" startWordPosition="974" endWordPosition="978">ribing the preprocessing procedure in Section 4, we turn to the actual experiments in Sections 5 and 6. We then compare our approach to other approaches (Section 7) and discuss the results (Section 8). 2 Previous Work Previous approaches to idiom detection can be classified into two groups: 1) Type-based extraction, i.e., detecting idioms at the type level; 2) token-based detection, i.e., detecting idioms in context. Type-based extraction is based on the idea that idiomatic expressions exhibit certain linguistic properties that can distinguish them from literal expressions (Sag et al. (2002); Fazly et al. (2009)), among many others, discuss various properties of idioms. Some examples of such properties include 1) lexical fixedness: e.g., neither ‘shoot the wind’ nor ‘hit the breeze’ are valid variations of the idiom shoot the breeze and 2) syntactic fixedness: e.g., The guy kicked the bucket is potentially idiomatic whereas The bucket was kicked is not idiomatic anymore; and of course, 3) non-compositionality. Thus, some approaches look at the tendency for words to occur in one particular order, or a fixed pattern. Hearst (1992) identifies lexico-syntactic patterns that occur frequently, are recogniz</context>
<context position="7950" citStr="Fazly et al. (2009)" startWordPosition="1236" endWordPosition="1239">onal to being partly unanalyzable to completely noncompositional (Cook et al. (2007)). Fazly et al. (2009); Li and Sporleder (2010), among others, notice that type-based approaches do not work on expressions that can be interpreted idiomatically or literally depending on the context and thus, an approach that considers tokens in context is more appropriate for the task of idiom recognition. A number of token-based approaches have been discussed in the literature, both supervised (Katz and Giesbrech (2006)), weakly supervised (Birke and Sarkar (2006)) and unsupervised (Sporleder and Li (2009); Fazly et al. (2009)). Fazly et al. (2009) develop statistical measures for each linguistic property of idiomatic expressions and use them both in a typebased classification task and in a token identification task, in which they distinguish idiomatic and literal usages of potentially idiomatic expressions in context. Sporleder and Li (2009) present a graph-based model for representing the lexical cohesion of a discourse. Nodes represent tokens in the discourse, which are connected by edges whose value is determined by a semantic relatedness function. They experiment with two different approaches to semantic relat</context>
<context position="29588" citStr="Fazly et al. (2009)" startWordPosition="4936" endWordPosition="4939">(bottom panel in Figure 3). On the other hand, original text represents “raw” features, whereby arousal information helps provide better contexts, thus improving overall performance. Figure 4 shows a case in point: the average (sorted) arousal values of idioms and literals of the target phrase “lose head.” The upper panel plots arousal values in the text space, while lower panel plots arousal values in the topic space. The plot supports the results shown in Tables 1 and 2, where the arousal feature generally improves text representation. 7 Comparisons with other approaches Even though we used Fazly et al. (2009)’s dataset for these experiments, the direct comparison with their method is impossible here because our task is formulated differently and we do not use the full dataset for the experiments. Fazly et al. (2009)’s unsupervised Precision Recall Accuracy 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 Topics Text 2024 Table 2: Average accuracy of competing methods on four datasets in multiple paragraph contexts: A = Arousal Model BlowWhistle LoseHead MakeScene TakeHeart Prec Recall Acc Prec Recall Acc Prec Recall Acc Prec Recall Acc FDA-Topics 0.62 0.60 0.83 0.76 0.97 0.78 0.79 0.95 0.77 0.93 0.99 0.92 FDA-To</context>
</contexts>
<marker>Fazly, Cook, Stevenson, 2009</marker>
<rawString>Fazly, A., P. Cook, and S. Stevenson (2009). Unsupervised Type and Token Identification of Idiomatic Expressions. Computational Linguistics 35(1), 61– 103.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Feldman</author>
<author>J Peng</author>
</authors>
<title>Automatic detection of idiomatic clauses.</title>
<date>2013</date>
<booktitle>In Computational Linguistics and Intelligent Text Processing,</booktitle>
<pages>435--446</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="9389" citStr="Feldman and Peng (2013)" startWordPosition="1462" endWordPosition="1465">eater than five paragraphs). Li and Sporleder (2010) assume that literal and figurative data are generated by two different Gaussians, literal and non-literal and the detection is done by comparing which Gaussian model has a higher probability to generate a specific instance. The approach assumes that the target expressions are already known and the goal is to determine whether this expression is literal or figurative in a particular context. The important insight of this method is that figurative language in general exhibits less semantic cohesive ties with the context than literal language. Feldman and Peng (2013) describe several approaches to automatic idiom identification. One of them is idiom recognition as outlier detection. They apply principal component analysis for outlier detection – an approach that does not rely on costly annotated training data and is not limited to a specific type of a syntactic construction, and is generally language independent. The quantitative analysis provided in their work shows that the outlier detection algorithm performs better and seems promising. The qualitative analysis also shows that their algorithm has to incorporate several important properties of the idiom</context>
<context position="10805" citStr="Feldman and Peng (2013)" startWordPosition="1678" endWordPosition="1681">nt from the local topics. (3) While not all semantic outliers are idioms, non-compositional semantic outliers are likely to be idiomatic. (4) Idiomaticity is not a binary property. Idioms fall on the continuum from being compositional to being partly unanalyzable to completely non-compositional. The approach described below is taking Feldman and Peng (2013)’s original idea and is trying to address (2) directly and (1) indirectly. Our approach is also somewhat similar to Li and Sporleder (2010) because it also relies on a list of potentially idiomatic expressions. 3 Our Hypothesis Similarly to Feldman and Peng (2013), out starting point is that idioms are semantic outliers that violate cohesive structure, especially in local contexts. However, our task is framed as supervised classification and we rely on data annotated for idiomatic and literal expressions. We hypothesize that words in a given text 2020 segment, such as a paragraph, that are high-ranking representatives of a common topic of discussion are less likely to be a part of an idiomatic expression in the document. 3.1 Topic Space Representation Instead of the simple bag of words representation of a target document (segment of three paragraphs th</context>
<context position="31691" citStr="Feldman and Peng (2013)" startWordPosition="5287" endWordPosition="5290">pace. Lower panel: Topic space. Topics+A Topics Topic Representation Arousal Values 1 0.5 0 −0.5 −1 Idioms Literals 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 model that relies on the so-called canonical forms gives 72.4% (macro-)accuracy on the extraction of idiomatic tokens when evaluated on their test data. We cannot compare our method directly with the other methods discussed in section 2 either because each uses a different dataset or formulates the task differently (detection vs. recognition vs. identification). However, we can compare the method presented here with Feldman and Peng (2013) who also experiment with LDA, use similar data, and frame the problem as classification. Their goal, however, is to classify sentences as either idiomatic or literal. To obtain a discriminant subspace, they train their model on a small number of randomly selected idiomatic and nonidiomatic sentences. They then project both the training and the test data on the chosen subspace and use the three nearest neighbor (3NN) classifier to obtain accuracy. The average accuracy they report is 80%. Our method clearly outperforms the Feldman and Peng (2013) approach (at least on the dataset we use). 8 Dis</context>
</contexts>
<marker>Feldman, Peng, 2013</marker>
<rawString>Feldman, A. and J. Peng (2013). Automatic detection of idiomatic clauses. In Computational Linguistics and Intelligent Text Processing, pp. 435–446. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Fukunaga</author>
</authors>
<title>Introduction to statistical pattern recognition.</title>
<date>1990</date>
<publisher>Academic Press.</publisher>
<contexts>
<context position="17013" citStr="Fukunaga, 1990" startWordPosition="2799" endWordPosition="2800">20 −5 −20 −15 −10 −5 0 5 10 15 Idioms Literals 25 20 15 10 5 0 −5 100 80 60 40 20 0 20 15 10 5 0 Idioms Literals Idioms Literals Figure 1: 2D projection of text segments containing “blow whistle.” Left panel: Original text space. Middle panel: Topic space with restricted vocabulary. Right panel: Topic space with enlarged vocabulary. 3.3 Fisher Linear Discriminant Analysis Once MDˆ and MQ are obtained, a classification rule can be applied to predict idioms vs. literals. The approach we are taking in this work for classifying idioms vs. literals is based on Fisher’s discriminant analysis (FDA) (Fukunaga, 1990). FDA often significantly simplifies tasks such as regression and classification by computing low-dimensional subspaces having statistically uncorrelated or discriminant variables. In language analysis, statistically uncorrelate or discriminant variables are extracted and utilized for description, detection, and classification. Woods et al. (1986), for example, use statistically uncorrelated variables for language test scores. A group of subjects is scored on a battery of language tests, where the subtests measure different abilities such as vocabulary, grammar or reading comprehension. Horvat</context>
</contexts>
<marker>Fukunaga, 1990</marker>
<rawString>Fukunaga, K. (1990). Introduction to statistical pattern recognition. Academic Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Hearst</author>
</authors>
<title>Automatic acquisition of hyponyms from large text corpora.</title>
<date>1992</date>
<booktitle>In Proceedings of the 14th Conference on Computational Linguistics - Volume 2, COLING ’92,</booktitle>
<pages>539--545</pages>
<institution>Association for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA,</location>
<contexts>
<context position="6775" citStr="Hearst (1992)" startWordPosition="1063" endWordPosition="1064"> can distinguish them from literal expressions (Sag et al. (2002); Fazly et al. (2009)), among many others, discuss various properties of idioms. Some examples of such properties include 1) lexical fixedness: e.g., neither ‘shoot the wind’ nor ‘hit the breeze’ are valid variations of the idiom shoot the breeze and 2) syntactic fixedness: e.g., The guy kicked the bucket is potentially idiomatic whereas The bucket was kicked is not idiomatic anymore; and of course, 3) non-compositionality. Thus, some approaches look at the tendency for words to occur in one particular order, or a fixed pattern. Hearst (1992) identifies lexico-syntactic patterns that occur frequently, are recognizable with little or no precoded knowledge, and indicate the lexical relation of interest. Widdows and Dorow (2005) use Hearst’s concept of lexicosyntactic patterns to extract idioms that consist of fixed patterns between two nouns. Basically, their technique works by finding patterns such as “thrills and spills”, whose reversals (such as “spills and thrills”) are never encountered. While many idioms do have these properties, many idioms fall on the continuum from being compositional to being partly unanalyzable to complet</context>
</contexts>
<marker>Hearst, 1992</marker>
<rawString>Hearst, M. A. (1992). Automatic acquisition of hyponyms from large text corpora. In Proceedings of the 14th Conference on Computational Linguistics - Volume 2, COLING ’92, Stroudsburg, PA, USA, pp. 539–545. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B M Horvath</author>
</authors>
<title>Variation in Australian English. Cambridge:</title>
<date>1985</date>
<publisher>Cambridge University PRess.</publisher>
<contexts>
<context position="17621" citStr="Horvath (1985)" startWordPosition="2883" endWordPosition="2884"> 1990). FDA often significantly simplifies tasks such as regression and classification by computing low-dimensional subspaces having statistically uncorrelated or discriminant variables. In language analysis, statistically uncorrelate or discriminant variables are extracted and utilized for description, detection, and classification. Woods et al. (1986), for example, use statistically uncorrelated variables for language test scores. A group of subjects is scored on a battery of language tests, where the subtests measure different abilities such as vocabulary, grammar or reading comprehension. Horvath (1985) analyzes speech samples of Sydney speakers to determine the relative occurrence of five different variants of each of five vowels sounds. Using this data, the speakers cluster according to such factors as gender, age, ethnicity and socio-economic class. A similar approach has been discussed in Peng et al. (2010). FDA is a class of methods used in machine learning to find the linear combination of features that best separate two classes of events. FDA is closely related to principal component analysis (PCA), where a linear combination of features that best explains the data. Discriminant analy</context>
</contexts>
<marker>Horvath, 1985</marker>
<rawString>Horvath, B. M. (1985). Variation in Australian English. Cambridge: Cambridge University PRess.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Katz</author>
<author>E Giesbrech</author>
</authors>
<title>Automatic Identification of Non-compositional Multiword Expressions using Latent Semantic Analysis.</title>
<date>2006</date>
<booktitle>In Proceedings of the ACL/COLING-06 Workshop on Multiword Expressions: Identifying and Exploiting Underlying Properties,</booktitle>
<pages>12--19</pages>
<contexts>
<context position="7841" citStr="Katz and Giesbrech (2006)" startWordPosition="1220" endWordPosition="1223">ver encountered. While many idioms do have these properties, many idioms fall on the continuum from being compositional to being partly unanalyzable to completely noncompositional (Cook et al. (2007)). Fazly et al. (2009); Li and Sporleder (2010), among others, notice that type-based approaches do not work on expressions that can be interpreted idiomatically or literally depending on the context and thus, an approach that considers tokens in context is more appropriate for the task of idiom recognition. A number of token-based approaches have been discussed in the literature, both supervised (Katz and Giesbrech (2006)), weakly supervised (Birke and Sarkar (2006)) and unsupervised (Sporleder and Li (2009); Fazly et al. (2009)). Fazly et al. (2009) develop statistical measures for each linguistic property of idiomatic expressions and use them both in a typebased classification task and in a token identification task, in which they distinguish idiomatic and literal usages of potentially idiomatic expressions in context. Sporleder and Li (2009) present a graph-based model for representing the lexical cohesion of a discourse. Nodes represent tokens in the discourse, which are connected by edges whose value is d</context>
</contexts>
<marker>Katz, Giesbrech, 2006</marker>
<rawString>Katz, G. and E. Giesbrech (2006). Automatic Identification of Non-compositional Multiword Expressions using Latent Semantic Analysis. In Proceedings of the ACL/COLING-06 Workshop on Multiword Expressions: Identifying and Exploiting Underlying Properties, pp. 12–19.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Li</author>
<author>C Sporleder</author>
</authors>
<title>Using Gaussian Mixture Models to Detect Figurative Language in Context.</title>
<date>2010</date>
<booktitle>In Proceedings of NAACL/HLT</booktitle>
<contexts>
<context position="7462" citStr="Li and Sporleder (2010)" startWordPosition="1162" endWordPosition="1165"> recognizable with little or no precoded knowledge, and indicate the lexical relation of interest. Widdows and Dorow (2005) use Hearst’s concept of lexicosyntactic patterns to extract idioms that consist of fixed patterns between two nouns. Basically, their technique works by finding patterns such as “thrills and spills”, whose reversals (such as “spills and thrills”) are never encountered. While many idioms do have these properties, many idioms fall on the continuum from being compositional to being partly unanalyzable to completely noncompositional (Cook et al. (2007)). Fazly et al. (2009); Li and Sporleder (2010), among others, notice that type-based approaches do not work on expressions that can be interpreted idiomatically or literally depending on the context and thus, an approach that considers tokens in context is more appropriate for the task of idiom recognition. A number of token-based approaches have been discussed in the literature, both supervised (Katz and Giesbrech (2006)), weakly supervised (Birke and Sarkar (2006)) and unsupervised (Sporleder and Li (2009); Fazly et al. (2009)). Fazly et al. (2009) develop statistical measures for each linguistic property of idiomatic expressions and us</context>
<context position="8818" citStr="Li and Sporleder (2010)" startWordPosition="1371" endWordPosition="1374">ages of potentially idiomatic expressions in context. Sporleder and Li (2009) present a graph-based model for representing the lexical cohesion of a discourse. Nodes represent tokens in the discourse, which are connected by edges whose value is determined by a semantic relatedness function. They experiment with two different approaches to semantic relatedness: 1) Dependency vectors, as described in Pado and Lapata (2007); 2) Normalized Google Distance (Cilibrasi and Vit´anyi (2007)). Sporleder and Li (2009) show that this method works better for larger contexts (greater than five paragraphs). Li and Sporleder (2010) assume that literal and figurative data are generated by two different Gaussians, literal and non-literal and the detection is done by comparing which Gaussian model has a higher probability to generate a specific instance. The approach assumes that the target expressions are already known and the goal is to determine whether this expression is literal or figurative in a particular context. The important insight of this method is that figurative language in general exhibits less semantic cohesive ties with the context than literal language. Feldman and Peng (2013) describe several approaches </context>
<context position="10680" citStr="Li and Sporleder (2010)" startWordPosition="1658" endWordPosition="1661"> expressions or other types of collocations. (2) Idioms violate local cohesive ties, as a result, they are semantically distant from the local topics. (3) While not all semantic outliers are idioms, non-compositional semantic outliers are likely to be idiomatic. (4) Idiomaticity is not a binary property. Idioms fall on the continuum from being compositional to being partly unanalyzable to completely non-compositional. The approach described below is taking Feldman and Peng (2013)’s original idea and is trying to address (2) directly and (1) indirectly. Our approach is also somewhat similar to Li and Sporleder (2010) because it also relies on a list of potentially idiomatic expressions. 3 Our Hypothesis Similarly to Feldman and Peng (2013), out starting point is that idioms are semantic outliers that violate cohesive structure, especially in local contexts. However, our task is framed as supervised classification and we rely on data annotated for idiomatic and literal expressions. We hypothesize that words in a given text 2020 segment, such as a paragraph, that are high-ranking representatives of a common topic of discussion are less likely to be a part of an idiomatic expression in the document. 3.1 Topi</context>
</contexts>
<marker>Li, Sporleder, 2010</marker>
<rawString>Li, L. and C. Sporleder (2010). Using Gaussian Mixture Models to Detect Figurative Language in Context. In Proceedings of NAACL/HLT 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Newman</author>
<author>E V Bonilla</author>
<author>W L Buntine</author>
</authors>
<title>Improving topic coherence with regularized topic models.</title>
<date>2011</date>
<booktitle>In NIPS,</booktitle>
<pages>496--504</pages>
<contexts>
<context position="13368" citStr="Newman et al., 2011" startWordPosition="2123" endWordPosition="2127"> space representation. In the figure, text segments containing target phrase “blow whistle” are projected on a two dimensional subspace. The left figure shows the projection in the “text” space, represented by the term by document matrix MD. The middle figure shows the projection in the topic space, represented by M ˆD. The topic space representation seems to provide a better separation. We note that when learning topics from a small data sample, learned topics can be less coherent and interpretable, thus less useful. To address this issue, regularized LDA has been proposed in the literature (Newman et al., 2011). A key feature is to favor words that exhibit short range dependencies for a given topic. We can achieve a similar effect by placing restrictions on the vocabulary. For example, when extracting topics from segments containing idioms, we may restrict the vocabulary to contain words from these segments only. The middle and right figures in Figure 1 illustrate a case in point. The middle figure shows a projection onto the topic space that is computed with a restricted vocabulary, while the right figure shows a projection when we place no restriction on the vocabulary. That is, the vocabulary inc</context>
</contexts>
<marker>Newman, Bonilla, Buntine, 2011</marker>
<rawString>Newman, D., E. V. Bonilla, and W. L. Buntine (2011). Improving topic coherence with regularized topic models. In NIPS, pp. 496–504.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Nunberg</author>
<author>I A Sag</author>
<author>T Wasow</author>
</authors>
<date>1994</date>
<journal>Idioms. Language</journal>
<volume>70</volume>
<issue>3</issue>
<pages>491--538</pages>
<contexts>
<context position="24834" citStr="Nunberg et al. (1994)" startWordPosition="4114" endWordPosition="4117">are used as test data. LoseHead: This data set has 40 examples, 21 of which are idioms and the remaining 19 are literals. The training data for LoseHead consist of 30 randomly chosen examples (15 idiomatic and 15 literal). The remaining 10 examples (6 idiomatic and 4 literal) are used as test data. TakeHeart: This data set has 81 examples, 61 of which are idioms and the remaining 20 are literals. The training data for TakeHeart consist of 30 randomly chosen examples (15 idiomatic and 15 literals). The remaining 51 examples (46 idiomatic and 5 literals) are used as test data. 5.3 Adding affect Nunberg et al. (1994) notice that “idioms are typically used to imply a certain evaluation or affective stance toward the things they denote”. Language users usually choose an idiom in non-neutral contexts. The situations that idioms describe can be positive or negative; however, the polarity of the context is not as important as the strength of the emotion expressed. So, we decided to incorporate the knowledge about the emotion strength into our algorithm. We use a database of word norms collected by Warriner et al. (2013). This database contains almost 14,000 English lemmas annotated with three components of emo</context>
</contexts>
<marker>Nunberg, Sag, Wasow, 1994</marker>
<rawString>Nunberg, G., I. A. Sag, and T. Wasow (1994). Idioms. Language 70(3), 491–538.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Pado</author>
<author>M Lapata</author>
</authors>
<title>Dependency-based construction of semantic space models.</title>
<date>2007</date>
<journal>Computational Linguistics</journal>
<volume>33</volume>
<issue>2</issue>
<pages>161--199</pages>
<contexts>
<context position="8619" citStr="Pado and Lapata (2007)" startWordPosition="1341" endWordPosition="1344">ures for each linguistic property of idiomatic expressions and use them both in a typebased classification task and in a token identification task, in which they distinguish idiomatic and literal usages of potentially idiomatic expressions in context. Sporleder and Li (2009) present a graph-based model for representing the lexical cohesion of a discourse. Nodes represent tokens in the discourse, which are connected by edges whose value is determined by a semantic relatedness function. They experiment with two different approaches to semantic relatedness: 1) Dependency vectors, as described in Pado and Lapata (2007); 2) Normalized Google Distance (Cilibrasi and Vit´anyi (2007)). Sporleder and Li (2009) show that this method works better for larger contexts (greater than five paragraphs). Li and Sporleder (2010) assume that literal and figurative data are generated by two different Gaussians, literal and non-literal and the detection is done by comparing which Gaussian model has a higher probability to generate a specific instance. The approach assumes that the target expressions are already known and the goal is to determine whether this expression is literal or figurative in a particular context. The im</context>
</contexts>
<marker>Pado, Lapata, 2007</marker>
<rawString>Pado, S. and M. Lapata (2007). Dependency-based construction of semantic space models. Computational Linguistics 33(2), 161–199.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Peng</author>
<author>A Feldman</author>
<author>L Street</author>
</authors>
<title>Computing linear discriminants for idiomatic sentence detection.</title>
<date>2010</date>
<journal>Research in Computing Science, Special issue: Natural Language Processing and its Applications</journal>
<volume>46</volume>
<pages>17--28</pages>
<contexts>
<context position="17935" citStr="Peng et al. (2010)" startWordPosition="2930" endWordPosition="2933">tion, and classification. Woods et al. (1986), for example, use statistically uncorrelated variables for language test scores. A group of subjects is scored on a battery of language tests, where the subtests measure different abilities such as vocabulary, grammar or reading comprehension. Horvath (1985) analyzes speech samples of Sydney speakers to determine the relative occurrence of five different variants of each of five vowels sounds. Using this data, the speakers cluster according to such factors as gender, age, ethnicity and socio-economic class. A similar approach has been discussed in Peng et al. (2010). FDA is a class of methods used in machine learning to find the linear combination of features that best separate two classes of events. FDA is closely related to principal component analysis (PCA), where a linear combination of features that best explains the data. Discriminant analysis explicitly exploits class information in the data, while PCA does not. Idiom classification based on discriminant analysis has several advantages. First, as has been mentioned, it does not make any assumption regarding data distributions. Many statistical detection methods assume a Gaussian distribution of no</context>
</contexts>
<marker>Peng, Feldman, Street, 2010</marker>
<rawString>Peng, J., A. Feldman, and L. Street (2010). Computing linear discriminants for idiomatic sentence detection. Research in Computing Science, Special issue: Natural Language Processing and its Applications 46, 17–28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I A Sag</author>
<author>T Baldwin</author>
<author>F Bond</author>
<author>A Copestake</author>
<author>D Flickinger</author>
</authors>
<title>Multiword expressions: A Pain in the Neck for NLP.</title>
<date>2002</date>
<booktitle>In Proceedings of the 3rd International Conference on Intelligence Text Processing and Computational Linguistics (CICLing 2002),</booktitle>
<pages>1--15</pages>
<location>Mexico City, Mexico,</location>
<contexts>
<context position="6227" citStr="Sag et al. (2002)" startWordPosition="970" endWordPosition="973">gorithm. After describing the preprocessing procedure in Section 4, we turn to the actual experiments in Sections 5 and 6. We then compare our approach to other approaches (Section 7) and discuss the results (Section 8). 2 Previous Work Previous approaches to idiom detection can be classified into two groups: 1) Type-based extraction, i.e., detecting idioms at the type level; 2) token-based detection, i.e., detecting idioms in context. Type-based extraction is based on the idea that idiomatic expressions exhibit certain linguistic properties that can distinguish them from literal expressions (Sag et al. (2002); Fazly et al. (2009)), among many others, discuss various properties of idioms. Some examples of such properties include 1) lexical fixedness: e.g., neither ‘shoot the wind’ nor ‘hit the breeze’ are valid variations of the idiom shoot the breeze and 2) syntactic fixedness: e.g., The guy kicked the bucket is potentially idiomatic whereas The bucket was kicked is not idiomatic anymore; and of course, 3) non-compositionality. Thus, some approaches look at the tendency for words to occur in one particular order, or a fixed pattern. Hearst (1992) identifies lexico-syntactic patterns that occur fre</context>
</contexts>
<marker>Sag, Baldwin, Bond, Copestake, Flickinger, 2002</marker>
<rawString>Sag, I. A., T. Baldwin, F. Bond, A. Copestake, and D. Flickinger (2002). Multiword expressions: A Pain in the Neck for NLP. In Proceedings of the 3rd International Conference on Intelligence Text Processing and Computational Linguistics (CICLing 2002), Mexico City, Mexico, pp. 1–15.</rawString>
</citation>
<citation valid="true">
<date>2002</date>
<booktitle>Collins COBUILD Idioms Dictionary</booktitle>
<editor>Seaton, M. and A. Macaulay (Eds.)</editor>
<publisher>HarperCollins Publishers.</publisher>
<contexts>
<context position="1728" citStr="(2002)" startWordPosition="258" endWordPosition="258">an unsupervised clustering method, Latent Dirichlet Allocation (LDA) (Blei et al., 2003). Since idiomatic expressions exhibit the property of non-compositionality, we assume that they usually present different semantics than the words used in the local topic. We treat idioms as semantic outliers, and the identification of a semantic shift as outlier detection. Thus, this topic representation allows us to differentiate idioms from literals using local semantic contexts. Our results are encouraging. 1 Introduction The definition of what is literal and figurative is still object of debate. Ariel (2002) demonstrates that literal and non-literal meanings cannot always be distinguished from each other. Literal meaning is originally assumed to be conventional, compositional, relatively context independent, and truth conditional. The problem is that the boundary is not clear-cut, some figurative expressions are compositional – metaphors and many idioms; others are conventional – most of the idioms. Idioms present great challenges for many Natural Language Processing (NLP) applications. They can violate selection restrictions (Sporleder and Li, 2009) as in push one’s luck under the assumption tha</context>
<context position="6227" citStr="(2002)" startWordPosition="973" endWordPosition="973">ter describing the preprocessing procedure in Section 4, we turn to the actual experiments in Sections 5 and 6. We then compare our approach to other approaches (Section 7) and discuss the results (Section 8). 2 Previous Work Previous approaches to idiom detection can be classified into two groups: 1) Type-based extraction, i.e., detecting idioms at the type level; 2) token-based detection, i.e., detecting idioms in context. Type-based extraction is based on the idea that idiomatic expressions exhibit certain linguistic properties that can distinguish them from literal expressions (Sag et al. (2002); Fazly et al. (2009)), among many others, discuss various properties of idioms. Some examples of such properties include 1) lexical fixedness: e.g., neither ‘shoot the wind’ nor ‘hit the breeze’ are valid variations of the idiom shoot the breeze and 2) syntactic fixedness: e.g., The guy kicked the bucket is potentially idiomatic whereas The bucket was kicked is not idiomatic anymore; and of course, 3) non-compositionality. Thus, some approaches look at the tendency for words to occur in one particular order, or a fixed pattern. Hearst (1992) identifies lexico-syntactic patterns that occur fre</context>
</contexts>
<marker>2002</marker>
<rawString>Seaton, M. and A. Macaulay (Eds.) (2002). Collins COBUILD Idioms Dictionary (second ed.). HarperCollins Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Sporleder</author>
<author>L Li</author>
</authors>
<title>Unsupervised Recognition of Literal and Non-literal Use of Idiomatic Expressions.</title>
<date>2009</date>
<booktitle>In EACL ’09: Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>754--762</pages>
<institution>Association for Computational Linguistics.</institution>
<location>Morristown, NJ, USA,</location>
<contexts>
<context position="2281" citStr="Sporleder and Li, 2009" startWordPosition="336" endWordPosition="339">what is literal and figurative is still object of debate. Ariel (2002) demonstrates that literal and non-literal meanings cannot always be distinguished from each other. Literal meaning is originally assumed to be conventional, compositional, relatively context independent, and truth conditional. The problem is that the boundary is not clear-cut, some figurative expressions are compositional – metaphors and many idioms; others are conventional – most of the idioms. Idioms present great challenges for many Natural Language Processing (NLP) applications. They can violate selection restrictions (Sporleder and Li, 2009) as in push one’s luck under the assumption that only concrete things can normally be pushed. Idioms can disobey typical subcategorization constraints (e.g., in line without a determiner before line), or change the default assignments of semantic roles to syntactic categories (e.g., in X breaks something with Y, Y typically is an instrument but for the idiom break the ice, it is more likely to fill a patient role as in How to break the ice with a stranger). In addition, many potentially idiomatic expressions can be used either literally or figuratively, depending on the context. This presents </context>
<context position="7929" citStr="Sporleder and Li (2009)" startWordPosition="1232" endWordPosition="1235">nuum from being compositional to being partly unanalyzable to completely noncompositional (Cook et al. (2007)). Fazly et al. (2009); Li and Sporleder (2010), among others, notice that type-based approaches do not work on expressions that can be interpreted idiomatically or literally depending on the context and thus, an approach that considers tokens in context is more appropriate for the task of idiom recognition. A number of token-based approaches have been discussed in the literature, both supervised (Katz and Giesbrech (2006)), weakly supervised (Birke and Sarkar (2006)) and unsupervised (Sporleder and Li (2009); Fazly et al. (2009)). Fazly et al. (2009) develop statistical measures for each linguistic property of idiomatic expressions and use them both in a typebased classification task and in a token identification task, in which they distinguish idiomatic and literal usages of potentially idiomatic expressions in context. Sporleder and Li (2009) present a graph-based model for representing the lexical cohesion of a discourse. Nodes represent tokens in the discourse, which are connected by edges whose value is determined by a semantic relatedness function. They experiment with two different approac</context>
</contexts>
<marker>Sporleder, Li, 2009</marker>
<rawString>Sporleder, C. and L. Li (2009). Unsupervised Recognition of Literal and Non-literal Use of Idiomatic Expressions. In EACL ’09: Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics, Morristown, NJ, USA, pp. 754–762. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A B Warriner</author>
<author>V Kuperman</author>
<author>M Brysbaert</author>
</authors>
<title>Norms of valence, arousal, and dominance for 13,915 english lemmas.</title>
<date>2013</date>
<journal>Behavior Research Methods</journal>
<volume>44</volume>
<issue>4</issue>
<contexts>
<context position="25342" citStr="Warriner et al. (2013)" startWordPosition="4200" endWordPosition="4203">remaining 51 examples (46 idiomatic and 5 literals) are used as test data. 5.3 Adding affect Nunberg et al. (1994) notice that “idioms are typically used to imply a certain evaluation or affective stance toward the things they denote”. Language users usually choose an idiom in non-neutral contexts. The situations that idioms describe can be positive or negative; however, the polarity of the context is not as important as the strength of the emotion expressed. So, we decided to incorporate the knowledge about the emotion strength into our algorithm. We use a database of word norms collected by Warriner et al. (2013). This database contains almost 14,000 English lemmas annotated with three components of emotions: valence (the pleasantness of a stimulus), arousal (the intensity of emotion provoked by a stimulus), and dominance 2023 Table 1: Average accuracy of competing methods on four datasets in single paragraph contexts: A = Arousal Model BlowWhistle LoseHead MakeScene TakeHeart Prec Recall Acc Prec Recall Acc Prec Recall Acc Prec Recall Acc FDA-Topics 0.44 0.40 0.79 0.70 0.90 0.70 0.82 0.97 0.81 0.91 0.97 0.89 FDA-Topics+A 0.51 0.51 0.75 0.78 0.68 0.66 0.80 0.99 0.80 0.93 0.84 0.80 FDA-Text 0.37 0.81 0</context>
</contexts>
<marker>Warriner, Kuperman, Brysbaert, 2013</marker>
<rawString>Warriner, A. B., V. Kuperman, and M. Brysbaert (2013). Norms of valence, arousal, and dominance for 13,915 english lemmas. Behavior Research Methods 44(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Widdows</author>
<author>B Dorow</author>
</authors>
<title>Automatic extraction of idioms using graph analysis and asymmetric lexicosyntactic patterns.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL-SIGLEX Workshop on Deep Lexical Acquisition, DeepLA ’05,</booktitle>
<pages>48--56</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA,</location>
<contexts>
<context position="6962" citStr="Widdows and Dorow (2005)" startWordPosition="1086" endWordPosition="1089">es include 1) lexical fixedness: e.g., neither ‘shoot the wind’ nor ‘hit the breeze’ are valid variations of the idiom shoot the breeze and 2) syntactic fixedness: e.g., The guy kicked the bucket is potentially idiomatic whereas The bucket was kicked is not idiomatic anymore; and of course, 3) non-compositionality. Thus, some approaches look at the tendency for words to occur in one particular order, or a fixed pattern. Hearst (1992) identifies lexico-syntactic patterns that occur frequently, are recognizable with little or no precoded knowledge, and indicate the lexical relation of interest. Widdows and Dorow (2005) use Hearst’s concept of lexicosyntactic patterns to extract idioms that consist of fixed patterns between two nouns. Basically, their technique works by finding patterns such as “thrills and spills”, whose reversals (such as “spills and thrills”) are never encountered. While many idioms do have these properties, many idioms fall on the continuum from being compositional to being partly unanalyzable to completely noncompositional (Cook et al. (2007)). Fazly et al. (2009); Li and Sporleder (2010), among others, notice that type-based approaches do not work on expressions that can be interpreted</context>
</contexts>
<marker>Widdows, Dorow, 2005</marker>
<rawString>Widdows, D. and B. Dorow (2005). Automatic extraction of idioms using graph analysis and asymmetric lexicosyntactic patterns. In Proceedings of the ACL-SIGLEX Workshop on Deep Lexical Acquisition, DeepLA ’05, Stroudsburg, PA, USA, pp. 48– 56. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Woods</author>
<author>P Fletcher</author>
<author>A Hughes</author>
</authors>
<title>Statistics in Language Studies. Cambridge:</title>
<date>1986</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="17362" citStr="Woods et al. (1986)" startWordPosition="2841" endWordPosition="2844"> Linear Discriminant Analysis Once MDˆ and MQ are obtained, a classification rule can be applied to predict idioms vs. literals. The approach we are taking in this work for classifying idioms vs. literals is based on Fisher’s discriminant analysis (FDA) (Fukunaga, 1990). FDA often significantly simplifies tasks such as regression and classification by computing low-dimensional subspaces having statistically uncorrelated or discriminant variables. In language analysis, statistically uncorrelate or discriminant variables are extracted and utilized for description, detection, and classification. Woods et al. (1986), for example, use statistically uncorrelated variables for language test scores. A group of subjects is scored on a battery of language tests, where the subtests measure different abilities such as vocabulary, grammar or reading comprehension. Horvath (1985) analyzes speech samples of Sydney speakers to determine the relative occurrence of five different variants of each of five vowels sounds. Using this data, the speakers cluster according to such factors as gender, age, ethnicity and socio-economic class. A similar approach has been discussed in Peng et al. (2010). FDA is a class of methods</context>
</contexts>
<marker>Woods, Fletcher, Hughes, 1986</marker>
<rawString>Woods, A., P. Fletcher, and A. Hughes (1986). Statistics in Language Studies. Cambridge: Cambridge University Press.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
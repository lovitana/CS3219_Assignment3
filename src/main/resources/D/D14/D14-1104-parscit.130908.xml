<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.015392">
<title confidence="0.983739">
Importance weighting and unsupervised domain adaptation
of POS taggers: a negative result
</title>
<author confidence="0.999455">
Barbara Plank, Anders Johannsen and Anders Søgaard
</author>
<affiliation confidence="0.9984895">
Center for Language Technology
University of Copenhagen, Denmark
</affiliation>
<address confidence="0.920944">
Njalsgade 140, DK-2300 Copenhagen S
</address>
<email confidence="0.998711">
bplank@cst.dk,ajohannsen@hum.ku.dk,soegaard@hum.ku.dk
</email>
<sectionHeader confidence="0.993887" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999910333333333">
Importance weighting is a generalization
of various statistical bias correction tech-
niques. While our labeled data in NLP is
heavily biased, importance weighting has
seen only few applications in NLP, most of
them relying on a small amount of labeled
target data. The publication bias toward
reporting positive results makes it hard to
say whether researchers have tried. This
paper presents a negative result on unsu-
pervised domain adaptation for POS tag-
ging. In this setup, we only have unlabeled
data and thus only indirect access to the
bias in emission and transition probabili-
ties. Moreover, most errors in POS tag-
ging are due to unseen words, and there,
importance weighting cannot help. We
present experiments with a wide variety of
weight functions, quantilizations, as well
as with randomly generated weights, to
support these claims.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.993297827586207">
Many NLP tasks rely on the availability of anno-
tated data. The majority of annotated data, how-
ever, is sampled from newswire corpora. The
performance of NLP systems, e.g., part-of-speech
(POS) tagger, parsers, relation extraction sys-
tems, etc., drops significantly when they are ap-
plied to data that departs from newswire conven-
tions. So while we can extract information, trans-
late and summarize newswire in major languages
with some success, we are much less successful
processing microblogs, chat, weblogs, answers,
emails or literature in a robust way. The main rea-
sons for the drops in accuracy have been attributed
to factors such as previously unseen words and bi-
grams, missing punctuation and capitalization, as
well as differences in the marginal distribution of
data (Blitzer et al., 2006; McClosky et al., 2008;
Søgaard and Haulrich, 2011).
The move from one domain to another (from a
source to a new target domain), say from newspa-
per articles to weblogs, results in a sample selec-
tion bias. Our training data is now biased, since
it is sampled from a related, but nevertheless dif-
ferent distribution. The problem of automatically
adjusting the model induced from source to a dif-
ferent target is referred to as domain adaptation.
Some researchers have studied domain adap-
tation scenarios, where small samples of labeled
data have been assumed to be available for the
target domains. This is usually an unrealistic as-
sumption, since even for major languages, small
samples are only available from a limited number
of domains, and in this work we focus on unsuper-
vised domain adaptation, assuming only unlabeled
target data is available.
Jiang and Zhai (2007), Foster et al. (2010; Plank
and Moschitti (2013) and Søgaard and Haulrich
(2011) have previously tried to use importance
weighting to correct sample bias in NLP. Im-
portance weighting means assigning a weight
to each training instance, reflecting its impor-
tance for modeling the target distribution. Im-
portance weighting is a generalization over post-
stratification (Smith, 1991) and importance sam-
pling (Smith et al., 1997) and can be used to cor-
rect bias in the labeled data.
Out of the four papers mentioned, only Søgaard
and Haulrich (2011) and Plank and Moschitti
(2013) considered an unsupervised domain adap-
tation scenario, obtaining mixed results. These
two papers assume covariate shift (Shimodaira,
2000), i.e., that there is only a bias in the marginal
distribution of the training data. Under this as-
sumption, we can correct the bias by applying a
weight function Pt(x)
Ps(x) to our training data points
(labeled sentences) and learn from the weighted
data. Of course this weight function cannot be
</bodyText>
<page confidence="0.93947">
968
</page>
<bodyText confidence="0.950830565217391">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 968–973,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
computed in general, but we can approximate it
in different ways.
In POS tagging, we typically factorize se-
quences into emission and transition probabilities.
Importance weighting can change emission prob-
abilities and transition probabilities by assigning
weights to sentences. For instance, if our corpus
consisted of three sequences: 1) a/A b/A, 2) a/A
b/B, and 3) a/A b/B, then P(B|A) = 2/3. If se-
quences two and three were down-weighted to 0.5,
then P(B|A) = 1/2.
However, this paper argues that importance
weighting cannot help adapting POS taggers to
new domains using only unlabeled target data. We
present three sources of evidence: (a) negative
results with the most obvious weight functions
across various English datasets, (b) negative re-
sults with randomly sampled weights, as well as
(c) an analysis of annotated data indicating that
there is little variation in emission and transition
probabilities across the various domains.
</bodyText>
<sectionHeader confidence="0.99942" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.995770195121951">
Most prior work on importance weighting use a
domain classifier, i.e., train a classifier to discrimi-
nate between source and target instances (Søgaard
and Haulrich, 2011; Plank and Moschitti, 2013)
(y E Is, t}). For instance, Søgaard and Haulrich
(2011) train a n-gram text classifier and Plank
and Moschitti (2013) a tree-kernel based clas-
sifier on relation extraction instances. In these
studies, Pˆ(t|x) is used as an approximation of
Pt(x) following Zadrozny (2004). In §3, we fol-
Ps(x),
low the approach of Søgaard and Haulrich (2011),
but consider a wider range of weight functions.
Others have proposed to use kernel mean match-
ing (Huang et al., 2007) or minimizing KL-
divergence (Sugiyama et al., 2007).
Jiang and Zhai (2007) use importance weight-
ing to select a subsample of the source data by
subsequently setting the weight of all selected data
points to 1, and 0 otherwise. However, they do
so by relying on a sequential model trained on
labeled target data. Our results indicate that the
covariate shift assumption fails to hold for cross-
domain POS tagging. While the marginal distri-
butions obviously do differ (since we can tell do-
mains apart without POS analysis), this is most
likely not the only difference. This might explain
the positive results obtained by Jiang and Zhai
(2007). We will come back to this in §4.
Cortes et al. (2010) show that importance
weighting potentially leads to over-fitting, but pro-
pose to use quantiles to obtain more robust weight
functions. The idea is to rank all weights and ob-
tain q quantiles. If a data point x is weighted by
w, and w lies in the ith quantile of the ranking
(i G q), x is weighted by the average weight of
data points in the ith quantile.
The weighted structured perceptron (§3) used in
the experiments below was recently used for a dif-
ferent problem, namely for correcting for bias in
annotations (Plank et al., 2014).
</bodyText>
<figureCaption confidence="0.99269">
Figure 1: Training epochs vs tagging accuracy for
the baseline model on the dev data.
</figureCaption>
<sectionHeader confidence="0.99721" genericHeader="method">
3 Experiments
</sectionHeader>
<subsectionHeader confidence="0.992933">
3.1 Data
</subsectionHeader>
<bodyText confidence="0.999993333333333">
We use the data made available in the SANCL
2012 Shared Task (Petrov and McDonald, 2012).
The training data is the OntoNotes 4.0 release
of the Wall Street Journal section of the Penn
Treebank, while the target domain evaluation data
comes from various sources, incl. Yahoo Answers,
user reviews, emails, weblogs and newsgroups.
For each target domain, we have both development
and test data.
</bodyText>
<subsectionHeader confidence="0.999535">
3.2 Model
</subsectionHeader>
<bodyText confidence="0.999833333333333">
In the weighted perceptron (Cavallanti et al.,
2006), we make the learning rate dependent on the
current instance xn, using the following update:
</bodyText>
<equation confidence="0.986241">
wi+1 &lt;--- wi + Qnα(yn − sign(wi - xn))xn (1)
</equation>
<bodyText confidence="0.99976975">
where Qn is the weight associated with xn. See
Huang et al. (2007) for similar notation.
We extend this idea straightforwardly to the
structured perceptron (Collins, 2002), for which
</bodyText>
<figure confidence="0.999390916666667">
0 5 10 15 20
92 93 94 95 96 97 98 99
0
0
0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 wsj
answers
reviews
emails
weblogs
newsgroups
</figure>
<page confidence="0.987836">
969
</page>
<table confidence="0.9984355">
System Answers Newsgroups Reviews Avg Emails Weblogs WSJ
Our system 91.08 91.57 91.59 91.41 87.97 92.19 97.32
SANCL12-2nd 90.99 92.32 90.65 91.32 – – 97.76
SANCL12-best 91.79 93.81 93.11 92.90 – – 97.29
SANCL12-last 88.24 89.70 88.15 88.70 – – 95.14
FLORS basic 91.17 92.41 92.25 88.67 91.37 97.11 91.94
</table>
<tableCaption confidence="0.999927">
Table 1: Tagging accuracies and comparison to prior work on the SANCL test sets (fine-grained POS).
</tableCaption>
<bodyText confidence="0.9998294">
we use an in-house implementation. We use
commonly used features, i.e., w,w−1, w−2,
w+1, w+2, digit, hyphen, capitalization, pre-
/suffix features, and Brown word clusters. The
model seems robust with respect to number
of training epochs, cf. Figure 1. Therefore
we fix the number of epochs to five and use
this setting in all our experiments. Our code
is available at: https://bitbucket.org/
bplank/importance-weighting-exp.
</bodyText>
<subsectionHeader confidence="0.997431">
3.3 Importance weighting
</subsectionHeader>
<bodyText confidence="0.999963590909091">
In our first set of experiments, we follow Søgaard
and Haulrich (2011) in using document classifiers
to obtain weights for the source instances. We
train a text classifier that discriminates the two
domains (source and target). For each sentence
in the source and target domain (the unlabeled
text that comes with the SANCL data), we mark
whether it comes from the source or target do-
main and train a binary classifier (logistic regres-
sion) to discriminate between the two. For ev-
ery sentence in the source we obtain its probabil-
ity for the target domain by doing 5-fold cross-
validation. While Søgaard and Haulrich (2011)
use only token-based features (word n-grams ≤
3), we here exploit a variety of features: word
token n-grams, and two generalizations: using
Brown clusters (estimated from the union of the
5 target domains), and Wiktionary tags (if a word
has multiple tags, we assign it the union of tags as
single tag; OOV words are marked as such).
The distributions of weights can be seen in the
upper half of Figure 2.
</bodyText>
<sectionHeader confidence="0.954212" genericHeader="method">
3.3.1 Results
</sectionHeader>
<bodyText confidence="0.998949833333333">
Table 1 shows that our baseline model achieves
state-of-the-art performance compared to
SANCL (Petrov and McDonald, 2012)1 and
FLORS (Schnabel and Sch¨utze, 2014). Our
results align well with the second best POS
tagger in the SANCL 2012 Shared Task. Note
</bodyText>
<footnote confidence="0.8865195">
1https://sites.google.com/site/sancl2012/home/
shared-task/results
</footnote>
<figureCaption confidence="0.998999">
Figure 2: Histogram of different weight functions.
</figureCaption>
<bodyText confidence="0.999258769230769">
that the best tagger in the shared task explicitly
used normalization and various other heuristics
to achieve better performance. In the rest of the
paper, we use the universal tag set part of the
SANCL data (Petrov et al., 2012).
Figure 3 presents our results on development
data for different importance weighting setups.
None of the above weight functions lead to signifi-
cant improvements on any of the datasets. We also
tried scaling and binning the weights, as suggested
by Cortes et al. (2010), but results kept fluctuating
around baseline performance, with no significant
improvements.
</bodyText>
<subsectionHeader confidence="0.983263">
3.4 Random weighting
</subsectionHeader>
<bodyText confidence="0.999937166666667">
Obviously, weight functions based on document
classifiers may simply not characterize the rele-
vant properties of the instances and hence lead to
bad re-weighting of the data. We consider three
random sampling strategies, namely sampling ran-
dom uniforms, random exponentials, and random
</bodyText>
<page confidence="0.992424">
970
</page>
<figureCaption confidence="0.9428365">
Figure 3: Results on development data for different weight functions, i.e., document classifiers trained
on a) raw tokens; b) tokens replaced by Wiktionary tags; c) tokens replaced by Brown cluster ids. The
weight was the raw pt(y|x) value, no scaling, no quantiles. Replacing only open-class tokens for b) and
c) gave similar or lower performance.
</figureCaption>
<table confidence="0.635432">
avg tag ambiguity OOV KL ρ
type token
1.09 1.41 11.5 0.0006 0.99
1.09 1.22 27.7 0.048 0.77
1.07 1.19 29.5 0.040 0.82
1.07 1.19 29.9 0.027 0.92
1.05 1.11 22.1 0.010 0.96
1.05 1.14 23.1 0.011 0.96
</table>
<bodyText confidence="0.991218777777778">
Zipfians and ran 500 samples for each. For these
experiments, we estimate significance cut-off lev-
els of tagging accuracies using the approximate
randomization test. To find the cut-off levels,
we randomly replace labels with gold labels until
the achieved accuracy significantly improves over
the baseline for more than 50% of the samples.
For each accuracy level, 50 random samples were
taken.
</bodyText>
<figureCaption confidence="0.6970346">
Figure 4: Random weight functions (500 runs
each) on test sets. Solid line is the baseline per-
formance, while the dashed line is the p-value cut-
off. From top: random, exponential and Zipfian
weighting. All runs fall below the cut-off.
</figureCaption>
<sectionHeader confidence="0.948462" genericHeader="method">
3.4.1 Results
</sectionHeader>
<bodyText confidence="0.999126714285714">
The dashed lines in Figure 4 show the p-value cut-
offs for positive results. We see that most random
weightings of data lead to slight drops in perfor-
mance or are around baseline performance, and no
weightings lead to significant improvements. Ran-
dom uniforms seem slightly better than exponen-
tials and Zipfians.
</bodyText>
<equation confidence="0.969110857142857">
domain (tokens)
wsj (train/test: 731k/39k)
answers (28k)
reviews (28k)
emails (28k)
weblogs (20k)
newsgroups (20k)
</equation>
<tableCaption confidence="0.8202465">
Table 2: Relevant statistics for our analysis (§4)
on the test sets: average tag ambiguity, out-of-
vocabulary rate, and KL-divergence and Pearson
correlation coefficient (ρ) on POS bigrams.
</tableCaption>
<sectionHeader confidence="0.989199" genericHeader="method">
4 Analysis
</sectionHeader>
<bodyText confidence="0.999938541666666">
Some differences between the gold-annotated
source domain data and the gold-annotated tar-
get data used for evaluation are presented in Ta-
ble 2. One important observation is the low ambi-
guity of word forms in the data. This makes the
room for improvement with importance weight-
ing smaller. Moreover, the KL divergencies over
POS bigrams are also very low. This tells us that
transition probabilities are also relatively constant
across domains, again suggesting limited room for
improvement for importance weighting.
Compared to this, we see much bigger differ-
ences in OOV rates. OOV rates do seem to explain
most of the performance drop across domains.
In order to verify this, we implemented a ver-
sion of our structured perceptron tagger with type-
constrained inference (T¨ackstr¨om et al., 2013).
This technique only improves performance on un-
seen words, but nevertheless we saw significant
improvements across all five domains (cf. Ta-
ble 3). This suggests that unseen words are a
more important problem than the marginal distri-
bution of data for unsupervised domain adaptation
of POS taggers.
</bodyText>
<figure confidence="0.999463875">
reviews
emails
weblogs
answers
● random
exp
zipf
newsgroups
94.2 94.4 94.6 94.8
94.2 94.6 95.0
94.4 94.8 95.2
● ● ●●
●
93.4 93.8
●
●
TA
● ● ● ●
● ● ● ● ●
● ● ● ● ●●
●
●
TA
●
AT
● ● ● ●
● ●
● ● ●
● ● ● ●
●●● ●
●
T
T
T
●
●
●
● ●
● ●
● ●
● ● ●
●
● ●
●
●
● ● ● ● ● ●
● ● ● ●
● ● ● ● ●
● ● ●
●
● ● ● ● ●
● ● ● ● ●
● ●
● ● ●
● ●
●
0 200 400
0 200 400
0 200 400
0 200 400
0 200 400
reviews
emails
weblogs
answers
newsgroups
94.2 94.6 95.0 95.4
93.2 93.6 94.0
94.0 94.4 94.8
94.2 94.6
0 200 400
0 200 400
0 200 400
0 200 400
0 200 400
0 50 100 150 0 50 100 150 0 50 100 150 0 50 100 150 0 50 100 150
93.2 93.6 94.0
93.0 93.4 93.8
92.5 93.0 93.5 94.0
answers
93.5 94.0 94.5
reviews
92.5 93.0 93.5 94.0
emails
94.0 94.5 95.0
weblogs
93.5 94.0 94.5 95.0
newsgroups
</figure>
<page confidence="0.985349">
971
</page>
<table confidence="0.924332">
ans rev email webl newsg
base 93.41 94.44 93.54 94.81 94.55
+type constr. 94.09† 94.85† 94.31† 95.99† 95.97†
p-val cut-off 93.90 94.85 94.10 95.3 95.10
</table>
<tableCaption confidence="0.760988">
Table 3: Results on the test sets by adding Wik-
tionary type constraints. †=p-value &lt; 0.001.
</tableCaption>
<bodyText confidence="0.999864142857143">
We also tried Jiang and Zhai’s subset selection
technique (§3.1 in Jiang and Zhai (2007)), which
assumes labeled training material for the target
domain. However, we did not see any improve-
ments. A possible explanation for these different
findings might be the following. Jiang and Zhai
(2007) use labeled target data to learn their weight-
ing model, i.e., in a supervised domain adaptation
scenario. This potentially leads to very different
weight functions. For example, let the source do-
main be 100 instances of a/A b/B and 100 in-
stances of b/B b/B, and the target domain be 100
instances of a/B a/B. Note that a domain classi-
fier would favor the first 100 sentences, but in an
HMM model induced from the labeled target data,
things look very different. If we apply Laplace
smoothing, the probability of a/A b/B accord-
ing to the target domain HMM model would be
- 8.9e−7, and the probability of b/B b/B would
be - 9e−5. Note also that this set-up does not as-
sume covariate shift.
</bodyText>
<sectionHeader confidence="0.998175" genericHeader="conclusions">
5 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.9999404">
Importance weighting, a generalization of various
statistical bias correction techniques, can poten-
tially correct bias in our labeled training data, but
this paper presented a negative result about impor-
tance weighting for unsupervised domain adapta-
tion of POS taggers. We first presented exper-
iments with a wide variety of weight functions,
quantilizations, as well as with randomly gener-
ated weights, none of which lead to significant im-
provements. Our analysis indicates that most er-
rors in POS tagging are due to unseen words, and
what remains seem to not be captured adequately
by unsupervised weight functions.
For future work we plan to extend this work to
further weight functions, data sets and NLP tasks.
</bodyText>
<sectionHeader confidence="0.996535" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.979243">
This research is funded by the ERC Starting Grant
LOWLANDS No. 313695.
</bodyText>
<sectionHeader confidence="0.994694" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99991393877551">
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In EMNLP.
Giovanni Cavallanti, Nicol`o Cesa-Bianchi, and Clau-
dio Gentile. 2006. Tracking the best hyperplane
with a simple budget perceptron. In COLT.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In EMNLP.
Corinna Cortes, Yishay Mansour, and Mehryar Mohri.
2010. Learning bounds for importance weighting.
In NIPS.
George Foster, Cyril Goutte, and Roland Kuhn. 2010.
Discriminative instance weighting for domain adap-
tation in statistical machine translation. In EMNLP.
Jiayuan Huang, Alexander Smola, Arthur Gretton,
Karsten Borgwardt, and Bernhard Sch¨olkopf. 2007.
Correcting sample bias by unlabeled data. In NIPS.
Jing Jiang and ChengXiang Zhai. 2007. Instance
weighting for domain adaptation in NLP. In ACL.
David McClosky, Eugene Charniak, and Mark John-
son. 2008. When is self-training effective for pars-
ing? In COLING.
Slav Petrov and Ryan McDonald. 2012. Overview of
the 2012 Shared Task on Parsing the Web. In Notes
of the First Workshop on Syntactic Analysis of Non-
Canonical Language (SANCL).
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012.
A universal part-of-speech tagset. In LREC.
Barbara Plank and Alessandro Moschitti. 2013. Em-
bedding semantic similarity in tree kernels for do-
main adaptation of relation extraction. In ACL.
Barbara Plank, Dirk Hovy, and Anders Søgaard. 2014.
Learning part-of-speech taggers with inter-annotator
agreement loss. In EACL.
Tobias Schnabel and Hinrich Sch¨utze. 2014. Flors:
Fast and simple domain adaptation for part-of-
speech tagging. TACL, 2:15–16.
Hidetoshi Shimodaira. 2000. Improving predictive in-
ference under covariate shift by weighting the log-
likelihood function. Journal of Statistical Planning
and Inference, 90:227–244.
Peter Smith, Mansoor Shafi, and Hongsheng Gao.
1997. Quick simulation: A review of importance
sampling techniques in communications systems.
IEEE Journal on Selected Areas in Communica-
tions, 15(4):597–613.
T.M.F. Smith. 1991. Post-stratification. The Statisti-
cian, 40:315–323.
</reference>
<page confidence="0.978496">
972
</page>
<reference confidence="0.996889714285714">
Anders Søgaard and Martin Haulrich. 2011.
Sentence-level instance-weighting for graph-based
and transition-based dependency parsing. In IWPT.
Masashi Sugiyama, Shinichi Nakajima, Hisashi
Kashima, Paul von B¨unau, and Motoaki Kawanabe.
2007. Direct importance estimation with model se-
lection and its application to covariate shift adapta-
tion. In NIPS.
Oscar T¨ackstr¨om, Dipanjan Das, Slav Petrov, Ryan
McDonald, and Joakim Nivre. 2013. Token and
type constraints for cross-lingual part-of-speech tag-
ging. TACL, 1:1–12.
Bianca Zadrozny. 2004. Learning and evaluating clas-
sifiers under sample selection bias. In ICML.
</reference>
<page confidence="0.999188">
973
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.893750">
<title confidence="0.9548815">Importance weighting and unsupervised domain of POS taggers: a negative result</title>
<author confidence="0.949606">Barbara Plank</author>
<author confidence="0.949606">Anders Johannsen</author>
<author confidence="0.949606">Anders</author>
<affiliation confidence="0.9991895">Center for Language University of Copenhagen,</affiliation>
<address confidence="0.994659">Njalsgade 140, DK-2300 Copenhagen</address>
<email confidence="0.994154">bplank@cst.dk,ajohannsen@hum.ku.dk,soegaard@hum.ku.dk</email>
<abstract confidence="0.99982">Importance weighting is a generalization of various statistical bias correction techniques. While our labeled data in NLP is heavily biased, importance weighting has seen only few applications in NLP, most of them relying on a small amount of labeled target data. The publication bias toward reporting positive results makes it hard to say whether researchers have tried. This paper presents a negative result on unsupervised domain adaptation for POS tag- In this setup, we only have data and thus only indirect access to the bias in emission and transition probabilities. Moreover, most errors in POS tagging are due to unseen words, and there, importance weighting cannot help. We present experiments with a wide variety of weight functions, quantilizations, as well as with randomly generated weights, to support these claims.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>John Blitzer</author>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
</authors>
<title>Domain adaptation with structural correspondence learning.</title>
<date>2006</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="1962" citStr="Blitzer et al., 2006" startWordPosition="294" endWordPosition="297">of-speech (POS) tagger, parsers, relation extraction systems, etc., drops significantly when they are applied to data that departs from newswire conventions. So while we can extract information, translate and summarize newswire in major languages with some success, we are much less successful processing microblogs, chat, weblogs, answers, emails or literature in a robust way. The main reasons for the drops in accuracy have been attributed to factors such as previously unseen words and bigrams, missing punctuation and capitalization, as well as differences in the marginal distribution of data (Blitzer et al., 2006; McClosky et al., 2008; Søgaard and Haulrich, 2011). The move from one domain to another (from a source to a new target domain), say from newspaper articles to weblogs, results in a sample selection bias. Our training data is now biased, since it is sampled from a related, but nevertheless different distribution. The problem of automatically adjusting the model induced from source to a different target is referred to as domain adaptation. Some researchers have studied domain adaptation scenarios, where small samples of labeled data have been assumed to be available for the target domains. Thi</context>
</contexts>
<marker>Blitzer, McDonald, Pereira, 2006</marker>
<rawString>John Blitzer, Ryan McDonald, and Fernando Pereira. 2006. Domain adaptation with structural correspondence learning. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Giovanni Cavallanti</author>
<author>Nicol`o Cesa-Bianchi</author>
<author>Claudio Gentile</author>
</authors>
<title>Tracking the best hyperplane with a simple budget perceptron.</title>
<date>2006</date>
<booktitle>In COLT.</booktitle>
<contexts>
<context position="7449" citStr="Cavallanti et al., 2006" startWordPosition="1194" endWordPosition="1197">or correcting for bias in annotations (Plank et al., 2014). Figure 1: Training epochs vs tagging accuracy for the baseline model on the dev data. 3 Experiments 3.1 Data We use the data made available in the SANCL 2012 Shared Task (Petrov and McDonald, 2012). The training data is the OntoNotes 4.0 release of the Wall Street Journal section of the Penn Treebank, while the target domain evaluation data comes from various sources, incl. Yahoo Answers, user reviews, emails, weblogs and newsgroups. For each target domain, we have both development and test data. 3.2 Model In the weighted perceptron (Cavallanti et al., 2006), we make the learning rate dependent on the current instance xn, using the following update: wi+1 &lt;--- wi + Qnα(yn − sign(wi - xn))xn (1) where Qn is the weight associated with xn. See Huang et al. (2007) for similar notation. We extend this idea straightforwardly to the structured perceptron (Collins, 2002), for which 0 5 10 15 20 92 93 94 95 96 97 98 99 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 wsj answers reviews emails weblogs newsgroups 969 System Answers Newsgroups Reviews Avg Emails Weblogs WSJ Our system 91.08 91.57 91.59 91.41 87.97 92.19 97.32 SANCL12-2nd 90.99 92.32 90.65 91.32 – –</context>
</contexts>
<marker>Cavallanti, Cesa-Bianchi, Gentile, 2006</marker>
<rawString>Giovanni Cavallanti, Nicol`o Cesa-Bianchi, and Claudio Gentile. 2006. Tracking the best hyperplane with a simple budget perceptron. In COLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="7759" citStr="Collins, 2002" startWordPosition="1248" endWordPosition="1249">reet Journal section of the Penn Treebank, while the target domain evaluation data comes from various sources, incl. Yahoo Answers, user reviews, emails, weblogs and newsgroups. For each target domain, we have both development and test data. 3.2 Model In the weighted perceptron (Cavallanti et al., 2006), we make the learning rate dependent on the current instance xn, using the following update: wi+1 &lt;--- wi + Qnα(yn − sign(wi - xn))xn (1) where Qn is the weight associated with xn. See Huang et al. (2007) for similar notation. We extend this idea straightforwardly to the structured perceptron (Collins, 2002), for which 0 5 10 15 20 92 93 94 95 96 97 98 99 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 wsj answers reviews emails weblogs newsgroups 969 System Answers Newsgroups Reviews Avg Emails Weblogs WSJ Our system 91.08 91.57 91.59 91.41 87.97 92.19 97.32 SANCL12-2nd 90.99 92.32 90.65 91.32 – – 97.76 SANCL12-best 91.79 93.81 93.11 92.90 – – 97.29 SANCL12-last 88.24 89.70 88.15 88.70 – – 95.14 FLORS basic 91.17 92.41 92.25 88.67 91.37 97.11 91.94 Table 1: Tagging accuracies and comparison to prior work on the SANCL test sets (fine-grained POS). we use an in-house implementation. We use commonly used</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Corinna Cortes</author>
<author>Yishay Mansour</author>
<author>Mehryar Mohri</author>
</authors>
<title>Learning bounds for importance weighting.</title>
<date>2010</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="6349" citStr="Cortes et al. (2010)" startWordPosition="1004" endWordPosition="1007"> use importance weighting to select a subsample of the source data by subsequently setting the weight of all selected data points to 1, and 0 otherwise. However, they do so by relying on a sequential model trained on labeled target data. Our results indicate that the covariate shift assumption fails to hold for crossdomain POS tagging. While the marginal distributions obviously do differ (since we can tell domains apart without POS analysis), this is most likely not the only difference. This might explain the positive results obtained by Jiang and Zhai (2007). We will come back to this in §4. Cortes et al. (2010) show that importance weighting potentially leads to over-fitting, but propose to use quantiles to obtain more robust weight functions. The idea is to rank all weights and obtain q quantiles. If a data point x is weighted by w, and w lies in the ith quantile of the ranking (i G q), x is weighted by the average weight of data points in the ith quantile. The weighted structured perceptron (§3) used in the experiments below was recently used for a different problem, namely for correcting for bias in annotations (Plank et al., 2014). Figure 1: Training epochs vs tagging accuracy for the baseline m</context>
<context position="10667" citStr="Cortes et al. (2010)" startWordPosition="1733" endWordPosition="1736">te 1https://sites.google.com/site/sancl2012/home/ shared-task/results Figure 2: Histogram of different weight functions. that the best tagger in the shared task explicitly used normalization and various other heuristics to achieve better performance. In the rest of the paper, we use the universal tag set part of the SANCL data (Petrov et al., 2012). Figure 3 presents our results on development data for different importance weighting setups. None of the above weight functions lead to significant improvements on any of the datasets. We also tried scaling and binning the weights, as suggested by Cortes et al. (2010), but results kept fluctuating around baseline performance, with no significant improvements. 3.4 Random weighting Obviously, weight functions based on document classifiers may simply not characterize the relevant properties of the instances and hence lead to bad re-weighting of the data. We consider three random sampling strategies, namely sampling random uniforms, random exponentials, and random 970 Figure 3: Results on development data for different weight functions, i.e., document classifiers trained on a) raw tokens; b) tokens replaced by Wiktionary tags; c) tokens replaced by Brown clust</context>
</contexts>
<marker>Cortes, Mansour, Mohri, 2010</marker>
<rawString>Corinna Cortes, Yishay Mansour, and Mehryar Mohri. 2010. Learning bounds for importance weighting. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Foster</author>
<author>Cyril Goutte</author>
<author>Roland Kuhn</author>
</authors>
<title>Discriminative instance weighting for domain adaptation in statistical machine translation.</title>
<date>2010</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="2854" citStr="Foster et al. (2010" startWordPosition="443" endWordPosition="446">, but nevertheless different distribution. The problem of automatically adjusting the model induced from source to a different target is referred to as domain adaptation. Some researchers have studied domain adaptation scenarios, where small samples of labeled data have been assumed to be available for the target domains. This is usually an unrealistic assumption, since even for major languages, small samples are only available from a limited number of domains, and in this work we focus on unsupervised domain adaptation, assuming only unlabeled target data is available. Jiang and Zhai (2007), Foster et al. (2010; Plank and Moschitti (2013) and Søgaard and Haulrich (2011) have previously tried to use importance weighting to correct sample bias in NLP. Importance weighting means assigning a weight to each training instance, reflecting its importance for modeling the target distribution. Importance weighting is a generalization over poststratification (Smith, 1991) and importance sampling (Smith et al., 1997) and can be used to correct bias in the labeled data. Out of the four papers mentioned, only Søgaard and Haulrich (2011) and Plank and Moschitti (2013) considered an unsupervised domain adaptation s</context>
</contexts>
<marker>Foster, Goutte, Kuhn, 2010</marker>
<rawString>George Foster, Cyril Goutte, and Roland Kuhn. 2010. Discriminative instance weighting for domain adaptation in statistical machine translation. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiayuan Huang</author>
<author>Alexander Smola</author>
<author>Arthur Gretton</author>
<author>Karsten Borgwardt</author>
<author>Bernhard Sch¨olkopf</author>
</authors>
<title>Correcting sample bias by unlabeled data.</title>
<date>2007</date>
<booktitle>In NIPS.</booktitle>
<marker>Huang, Smola, Gretton, Borgwardt, Sch¨olkopf, 2007</marker>
<rawString>Jiayuan Huang, Alexander Smola, Arthur Gretton, Karsten Borgwardt, and Bernhard Sch¨olkopf. 2007. Correcting sample bias by unlabeled data. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jing Jiang</author>
<author>ChengXiang Zhai</author>
</authors>
<title>Instance weighting for domain adaptation in NLP.</title>
<date>2007</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="2833" citStr="Jiang and Zhai (2007)" startWordPosition="439" endWordPosition="442"> sampled from a related, but nevertheless different distribution. The problem of automatically adjusting the model induced from source to a different target is referred to as domain adaptation. Some researchers have studied domain adaptation scenarios, where small samples of labeled data have been assumed to be available for the target domains. This is usually an unrealistic assumption, since even for major languages, small samples are only available from a limited number of domains, and in this work we focus on unsupervised domain adaptation, assuming only unlabeled target data is available. Jiang and Zhai (2007), Foster et al. (2010; Plank and Moschitti (2013) and Søgaard and Haulrich (2011) have previously tried to use importance weighting to correct sample bias in NLP. Importance weighting means assigning a weight to each training instance, reflecting its importance for modeling the target distribution. Importance weighting is a generalization over poststratification (Smith, 1991) and importance sampling (Smith et al., 1997) and can be used to correct bias in the labeled data. Out of the four papers mentioned, only Søgaard and Haulrich (2011) and Plank and Moschitti (2013) considered an unsupervise</context>
<context position="5729" citStr="Jiang and Zhai (2007)" startWordPosition="896" endWordPosition="899">een source and target instances (Søgaard and Haulrich, 2011; Plank and Moschitti, 2013) (y E Is, t}). For instance, Søgaard and Haulrich (2011) train a n-gram text classifier and Plank and Moschitti (2013) a tree-kernel based classifier on relation extraction instances. In these studies, Pˆ(t|x) is used as an approximation of Pt(x) following Zadrozny (2004). In §3, we folPs(x), low the approach of Søgaard and Haulrich (2011), but consider a wider range of weight functions. Others have proposed to use kernel mean matching (Huang et al., 2007) or minimizing KLdivergence (Sugiyama et al., 2007). Jiang and Zhai (2007) use importance weighting to select a subsample of the source data by subsequently setting the weight of all selected data points to 1, and 0 otherwise. However, they do so by relying on a sequential model trained on labeled target data. Our results indicate that the covariate shift assumption fails to hold for crossdomain POS tagging. While the marginal distributions obviously do differ (since we can tell domains apart without POS analysis), this is most likely not the only difference. This might explain the positive results obtained by Jiang and Zhai (2007). We will come back to this in §4. </context>
<context position="15073" citStr="Jiang and Zhai (2007)" startWordPosition="2531" endWordPosition="2534">.2 94.6 0 200 400 0 200 400 0 200 400 0 200 400 0 200 400 0 50 100 150 0 50 100 150 0 50 100 150 0 50 100 150 0 50 100 150 93.2 93.6 94.0 93.0 93.4 93.8 92.5 93.0 93.5 94.0 answers 93.5 94.0 94.5 reviews 92.5 93.0 93.5 94.0 emails 94.0 94.5 95.0 weblogs 93.5 94.0 94.5 95.0 newsgroups 971 ans rev email webl newsg base 93.41 94.44 93.54 94.81 94.55 +type constr. 94.09† 94.85† 94.31† 95.99† 95.97† p-val cut-off 93.90 94.85 94.10 95.3 95.10 Table 3: Results on the test sets by adding Wiktionary type constraints. †=p-value &lt; 0.001. We also tried Jiang and Zhai’s subset selection technique (§3.1 in Jiang and Zhai (2007)), which assumes labeled training material for the target domain. However, we did not see any improvements. A possible explanation for these different findings might be the following. Jiang and Zhai (2007) use labeled target data to learn their weighting model, i.e., in a supervised domain adaptation scenario. This potentially leads to very different weight functions. For example, let the source domain be 100 instances of a/A b/B and 100 instances of b/B b/B, and the target domain be 100 instances of a/B a/B. Note that a domain classifier would favor the first 100 sentences, but in an HMM mode</context>
</contexts>
<marker>Jiang, Zhai, 2007</marker>
<rawString>Jing Jiang and ChengXiang Zhai. 2007. Instance weighting for domain adaptation in NLP. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McClosky</author>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>When is self-training effective for parsing?</title>
<date>2008</date>
<booktitle>In COLING.</booktitle>
<contexts>
<context position="1985" citStr="McClosky et al., 2008" startWordPosition="298" endWordPosition="301">, parsers, relation extraction systems, etc., drops significantly when they are applied to data that departs from newswire conventions. So while we can extract information, translate and summarize newswire in major languages with some success, we are much less successful processing microblogs, chat, weblogs, answers, emails or literature in a robust way. The main reasons for the drops in accuracy have been attributed to factors such as previously unseen words and bigrams, missing punctuation and capitalization, as well as differences in the marginal distribution of data (Blitzer et al., 2006; McClosky et al., 2008; Søgaard and Haulrich, 2011). The move from one domain to another (from a source to a new target domain), say from newspaper articles to weblogs, results in a sample selection bias. Our training data is now biased, since it is sampled from a related, but nevertheless different distribution. The problem of automatically adjusting the model induced from source to a different target is referred to as domain adaptation. Some researchers have studied domain adaptation scenarios, where small samples of labeled data have been assumed to be available for the target domains. This is usually an unreali</context>
</contexts>
<marker>McClosky, Charniak, Johnson, 2008</marker>
<rawString>David McClosky, Eugene Charniak, and Mark Johnson. 2008. When is self-training effective for parsing? In COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Ryan McDonald</author>
</authors>
<date>2012</date>
<booktitle>Overview of the 2012 Shared Task on Parsing the Web. In Notes of the First Workshop on Syntactic Analysis of NonCanonical Language (SANCL).</booktitle>
<contexts>
<context position="7082" citStr="Petrov and McDonald, 2012" startWordPosition="1136" endWordPosition="1139">ore robust weight functions. The idea is to rank all weights and obtain q quantiles. If a data point x is weighted by w, and w lies in the ith quantile of the ranking (i G q), x is weighted by the average weight of data points in the ith quantile. The weighted structured perceptron (§3) used in the experiments below was recently used for a different problem, namely for correcting for bias in annotations (Plank et al., 2014). Figure 1: Training epochs vs tagging accuracy for the baseline model on the dev data. 3 Experiments 3.1 Data We use the data made available in the SANCL 2012 Shared Task (Petrov and McDonald, 2012). The training data is the OntoNotes 4.0 release of the Wall Street Journal section of the Penn Treebank, while the target domain evaluation data comes from various sources, incl. Yahoo Answers, user reviews, emails, weblogs and newsgroups. For each target domain, we have both development and test data. 3.2 Model In the weighted perceptron (Cavallanti et al., 2006), we make the learning rate dependent on the current instance xn, using the following update: wi+1 &lt;--- wi + Qnα(yn − sign(wi - xn))xn (1) where Qn is the weight associated with xn. See Huang et al. (2007) for similar notation. We ex</context>
<context position="9916" citStr="Petrov and McDonald, 2012" startWordPosition="1617" endWordPosition="1620">the target domain by doing 5-fold crossvalidation. While Søgaard and Haulrich (2011) use only token-based features (word n-grams ≤ 3), we here exploit a variety of features: word token n-grams, and two generalizations: using Brown clusters (estimated from the union of the 5 target domains), and Wiktionary tags (if a word has multiple tags, we assign it the union of tags as single tag; OOV words are marked as such). The distributions of weights can be seen in the upper half of Figure 2. 3.3.1 Results Table 1 shows that our baseline model achieves state-of-the-art performance compared to SANCL (Petrov and McDonald, 2012)1 and FLORS (Schnabel and Sch¨utze, 2014). Our results align well with the second best POS tagger in the SANCL 2012 Shared Task. Note 1https://sites.google.com/site/sancl2012/home/ shared-task/results Figure 2: Histogram of different weight functions. that the best tagger in the shared task explicitly used normalization and various other heuristics to achieve better performance. In the rest of the paper, we use the universal tag set part of the SANCL data (Petrov et al., 2012). Figure 3 presents our results on development data for different importance weighting setups. None of the above weight</context>
</contexts>
<marker>Petrov, McDonald, 2012</marker>
<rawString>Slav Petrov and Ryan McDonald. 2012. Overview of the 2012 Shared Task on Parsing the Web. In Notes of the First Workshop on Syntactic Analysis of NonCanonical Language (SANCL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dipanjan Das</author>
<author>Ryan McDonald</author>
</authors>
<title>A universal part-of-speech tagset.</title>
<date>2012</date>
<booktitle>In LREC.</booktitle>
<contexts>
<context position="10397" citStr="Petrov et al., 2012" startWordPosition="1689" endWordPosition="1692"> 2. 3.3.1 Results Table 1 shows that our baseline model achieves state-of-the-art performance compared to SANCL (Petrov and McDonald, 2012)1 and FLORS (Schnabel and Sch¨utze, 2014). Our results align well with the second best POS tagger in the SANCL 2012 Shared Task. Note 1https://sites.google.com/site/sancl2012/home/ shared-task/results Figure 2: Histogram of different weight functions. that the best tagger in the shared task explicitly used normalization and various other heuristics to achieve better performance. In the rest of the paper, we use the universal tag set part of the SANCL data (Petrov et al., 2012). Figure 3 presents our results on development data for different importance weighting setups. None of the above weight functions lead to significant improvements on any of the datasets. We also tried scaling and binning the weights, as suggested by Cortes et al. (2010), but results kept fluctuating around baseline performance, with no significant improvements. 3.4 Random weighting Obviously, weight functions based on document classifiers may simply not characterize the relevant properties of the instances and hence lead to bad re-weighting of the data. We consider three random sampling strate</context>
</contexts>
<marker>Petrov, Das, McDonald, 2012</marker>
<rawString>Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012. A universal part-of-speech tagset. In LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara Plank</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Embedding semantic similarity in tree kernels for domain adaptation of relation extraction.</title>
<date>2013</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="2882" citStr="Plank and Moschitti (2013)" startWordPosition="447" endWordPosition="450">fferent distribution. The problem of automatically adjusting the model induced from source to a different target is referred to as domain adaptation. Some researchers have studied domain adaptation scenarios, where small samples of labeled data have been assumed to be available for the target domains. This is usually an unrealistic assumption, since even for major languages, small samples are only available from a limited number of domains, and in this work we focus on unsupervised domain adaptation, assuming only unlabeled target data is available. Jiang and Zhai (2007), Foster et al. (2010; Plank and Moschitti (2013) and Søgaard and Haulrich (2011) have previously tried to use importance weighting to correct sample bias in NLP. Importance weighting means assigning a weight to each training instance, reflecting its importance for modeling the target distribution. Importance weighting is a generalization over poststratification (Smith, 1991) and importance sampling (Smith et al., 1997) and can be used to correct bias in the labeled data. Out of the four papers mentioned, only Søgaard and Haulrich (2011) and Plank and Moschitti (2013) considered an unsupervised domain adaptation scenario, obtaining mixed res</context>
<context position="5195" citStr="Plank and Moschitti, 2013" startWordPosition="807" endWordPosition="810">help adapting POS taggers to new domains using only unlabeled target data. We present three sources of evidence: (a) negative results with the most obvious weight functions across various English datasets, (b) negative results with randomly sampled weights, as well as (c) an analysis of annotated data indicating that there is little variation in emission and transition probabilities across the various domains. 2 Related work Most prior work on importance weighting use a domain classifier, i.e., train a classifier to discriminate between source and target instances (Søgaard and Haulrich, 2011; Plank and Moschitti, 2013) (y E Is, t}). For instance, Søgaard and Haulrich (2011) train a n-gram text classifier and Plank and Moschitti (2013) a tree-kernel based classifier on relation extraction instances. In these studies, Pˆ(t|x) is used as an approximation of Pt(x) following Zadrozny (2004). In §3, we folPs(x), low the approach of Søgaard and Haulrich (2011), but consider a wider range of weight functions. Others have proposed to use kernel mean matching (Huang et al., 2007) or minimizing KLdivergence (Sugiyama et al., 2007). Jiang and Zhai (2007) use importance weighting to select a subsample of the source data</context>
</contexts>
<marker>Plank, Moschitti, 2013</marker>
<rawString>Barbara Plank and Alessandro Moschitti. 2013. Embedding semantic similarity in tree kernels for domain adaptation of relation extraction. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara Plank</author>
<author>Dirk Hovy</author>
<author>Anders Søgaard</author>
</authors>
<title>Learning part-of-speech taggers with inter-annotator agreement loss.</title>
<date>2014</date>
<booktitle>In EACL.</booktitle>
<contexts>
<context position="6883" citStr="Plank et al., 2014" startWordPosition="1101" endWordPosition="1104">ained by Jiang and Zhai (2007). We will come back to this in §4. Cortes et al. (2010) show that importance weighting potentially leads to over-fitting, but propose to use quantiles to obtain more robust weight functions. The idea is to rank all weights and obtain q quantiles. If a data point x is weighted by w, and w lies in the ith quantile of the ranking (i G q), x is weighted by the average weight of data points in the ith quantile. The weighted structured perceptron (§3) used in the experiments below was recently used for a different problem, namely for correcting for bias in annotations (Plank et al., 2014). Figure 1: Training epochs vs tagging accuracy for the baseline model on the dev data. 3 Experiments 3.1 Data We use the data made available in the SANCL 2012 Shared Task (Petrov and McDonald, 2012). The training data is the OntoNotes 4.0 release of the Wall Street Journal section of the Penn Treebank, while the target domain evaluation data comes from various sources, incl. Yahoo Answers, user reviews, emails, weblogs and newsgroups. For each target domain, we have both development and test data. 3.2 Model In the weighted perceptron (Cavallanti et al., 2006), we make the learning rate depend</context>
</contexts>
<marker>Plank, Hovy, Søgaard, 2014</marker>
<rawString>Barbara Plank, Dirk Hovy, and Anders Søgaard. 2014. Learning part-of-speech taggers with inter-annotator agreement loss. In EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tobias Schnabel</author>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Flors: Fast and simple domain adaptation for part-ofspeech tagging.</title>
<date>2014</date>
<tech>TACL,</tech>
<pages>2--15</pages>
<marker>Schnabel, Sch¨utze, 2014</marker>
<rawString>Tobias Schnabel and Hinrich Sch¨utze. 2014. Flors: Fast and simple domain adaptation for part-ofspeech tagging. TACL, 2:15–16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hidetoshi Shimodaira</author>
</authors>
<title>Improving predictive inference under covariate shift by weighting the loglikelihood function.</title>
<date>2000</date>
<journal>Journal of Statistical Planning and Inference,</journal>
<pages>90--227</pages>
<contexts>
<context position="3546" citStr="Shimodaira, 2000" startWordPosition="552" endWordPosition="553">y tried to use importance weighting to correct sample bias in NLP. Importance weighting means assigning a weight to each training instance, reflecting its importance for modeling the target distribution. Importance weighting is a generalization over poststratification (Smith, 1991) and importance sampling (Smith et al., 1997) and can be used to correct bias in the labeled data. Out of the four papers mentioned, only Søgaard and Haulrich (2011) and Plank and Moschitti (2013) considered an unsupervised domain adaptation scenario, obtaining mixed results. These two papers assume covariate shift (Shimodaira, 2000), i.e., that there is only a bias in the marginal distribution of the training data. Under this assumption, we can correct the bias by applying a weight function Pt(x) Ps(x) to our training data points (labeled sentences) and learn from the weighted data. Of course this weight function cannot be 968 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 968–973, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics computed in general, but we can approximate it in different ways. In POS tagging, we typically factorize </context>
</contexts>
<marker>Shimodaira, 2000</marker>
<rawString>Hidetoshi Shimodaira. 2000. Improving predictive inference under covariate shift by weighting the loglikelihood function. Journal of Statistical Planning and Inference, 90:227–244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Smith</author>
<author>Mansoor Shafi</author>
<author>Hongsheng Gao</author>
</authors>
<title>Quick simulation: A review of importance sampling techniques in communications systems.</title>
<date>1997</date>
<journal>IEEE Journal on Selected Areas in Communications,</journal>
<volume>15</volume>
<issue>4</issue>
<contexts>
<context position="3256" citStr="Smith et al., 1997" startWordPosition="504" endWordPosition="507">ll samples are only available from a limited number of domains, and in this work we focus on unsupervised domain adaptation, assuming only unlabeled target data is available. Jiang and Zhai (2007), Foster et al. (2010; Plank and Moschitti (2013) and Søgaard and Haulrich (2011) have previously tried to use importance weighting to correct sample bias in NLP. Importance weighting means assigning a weight to each training instance, reflecting its importance for modeling the target distribution. Importance weighting is a generalization over poststratification (Smith, 1991) and importance sampling (Smith et al., 1997) and can be used to correct bias in the labeled data. Out of the four papers mentioned, only Søgaard and Haulrich (2011) and Plank and Moschitti (2013) considered an unsupervised domain adaptation scenario, obtaining mixed results. These two papers assume covariate shift (Shimodaira, 2000), i.e., that there is only a bias in the marginal distribution of the training data. Under this assumption, we can correct the bias by applying a weight function Pt(x) Ps(x) to our training data points (labeled sentences) and learn from the weighted data. Of course this weight function cannot be 968 Proceedin</context>
</contexts>
<marker>Smith, Shafi, Gao, 1997</marker>
<rawString>Peter Smith, Mansoor Shafi, and Hongsheng Gao. 1997. Quick simulation: A review of importance sampling techniques in communications systems. IEEE Journal on Selected Areas in Communications, 15(4):597–613.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T M F Smith</author>
</authors>
<date>1991</date>
<booktitle>Post-stratification. The Statistician,</booktitle>
<pages>40--315</pages>
<contexts>
<context position="3211" citStr="Smith, 1991" startWordPosition="498" endWordPosition="499">n, since even for major languages, small samples are only available from a limited number of domains, and in this work we focus on unsupervised domain adaptation, assuming only unlabeled target data is available. Jiang and Zhai (2007), Foster et al. (2010; Plank and Moschitti (2013) and Søgaard and Haulrich (2011) have previously tried to use importance weighting to correct sample bias in NLP. Importance weighting means assigning a weight to each training instance, reflecting its importance for modeling the target distribution. Importance weighting is a generalization over poststratification (Smith, 1991) and importance sampling (Smith et al., 1997) and can be used to correct bias in the labeled data. Out of the four papers mentioned, only Søgaard and Haulrich (2011) and Plank and Moschitti (2013) considered an unsupervised domain adaptation scenario, obtaining mixed results. These two papers assume covariate shift (Shimodaira, 2000), i.e., that there is only a bias in the marginal distribution of the training data. Under this assumption, we can correct the bias by applying a weight function Pt(x) Ps(x) to our training data points (labeled sentences) and learn from the weighted data. Of course</context>
</contexts>
<marker>Smith, 1991</marker>
<rawString>T.M.F. Smith. 1991. Post-stratification. The Statistician, 40:315–323.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anders Søgaard</author>
<author>Martin Haulrich</author>
</authors>
<title>Sentence-level instance-weighting for graph-based and transition-based dependency parsing.</title>
<date>2011</date>
<booktitle>In IWPT.</booktitle>
<contexts>
<context position="2014" citStr="Søgaard and Haulrich, 2011" startWordPosition="302" endWordPosition="305">raction systems, etc., drops significantly when they are applied to data that departs from newswire conventions. So while we can extract information, translate and summarize newswire in major languages with some success, we are much less successful processing microblogs, chat, weblogs, answers, emails or literature in a robust way. The main reasons for the drops in accuracy have been attributed to factors such as previously unseen words and bigrams, missing punctuation and capitalization, as well as differences in the marginal distribution of data (Blitzer et al., 2006; McClosky et al., 2008; Søgaard and Haulrich, 2011). The move from one domain to another (from a source to a new target domain), say from newspaper articles to weblogs, results in a sample selection bias. Our training data is now biased, since it is sampled from a related, but nevertheless different distribution. The problem of automatically adjusting the model induced from source to a different target is referred to as domain adaptation. Some researchers have studied domain adaptation scenarios, where small samples of labeled data have been assumed to be available for the target domains. This is usually an unrealistic assumption, since even f</context>
<context position="3376" citStr="Søgaard and Haulrich (2011)" startWordPosition="527" endWordPosition="530"> adaptation, assuming only unlabeled target data is available. Jiang and Zhai (2007), Foster et al. (2010; Plank and Moschitti (2013) and Søgaard and Haulrich (2011) have previously tried to use importance weighting to correct sample bias in NLP. Importance weighting means assigning a weight to each training instance, reflecting its importance for modeling the target distribution. Importance weighting is a generalization over poststratification (Smith, 1991) and importance sampling (Smith et al., 1997) and can be used to correct bias in the labeled data. Out of the four papers mentioned, only Søgaard and Haulrich (2011) and Plank and Moschitti (2013) considered an unsupervised domain adaptation scenario, obtaining mixed results. These two papers assume covariate shift (Shimodaira, 2000), i.e., that there is only a bias in the marginal distribution of the training data. Under this assumption, we can correct the bias by applying a weight function Pt(x) Ps(x) to our training data points (labeled sentences) and learn from the weighted data. Of course this weight function cannot be 968 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 968–973, October 25-29, 201</context>
<context position="5167" citStr="Søgaard and Haulrich, 2011" startWordPosition="803" endWordPosition="806">importance weighting cannot help adapting POS taggers to new domains using only unlabeled target data. We present three sources of evidence: (a) negative results with the most obvious weight functions across various English datasets, (b) negative results with randomly sampled weights, as well as (c) an analysis of annotated data indicating that there is little variation in emission and transition probabilities across the various domains. 2 Related work Most prior work on importance weighting use a domain classifier, i.e., train a classifier to discriminate between source and target instances (Søgaard and Haulrich, 2011; Plank and Moschitti, 2013) (y E Is, t}). For instance, Søgaard and Haulrich (2011) train a n-gram text classifier and Plank and Moschitti (2013) a tree-kernel based classifier on relation extraction instances. In these studies, Pˆ(t|x) is used as an approximation of Pt(x) following Zadrozny (2004). In §3, we folPs(x), low the approach of Søgaard and Haulrich (2011), but consider a wider range of weight functions. Others have proposed to use kernel mean matching (Huang et al., 2007) or minimizing KLdivergence (Sugiyama et al., 2007). Jiang and Zhai (2007) use importance weighting to select a </context>
<context position="8823" citStr="Søgaard and Haulrich (2011)" startWordPosition="1433" endWordPosition="1436">1 91.94 Table 1: Tagging accuracies and comparison to prior work on the SANCL test sets (fine-grained POS). we use an in-house implementation. We use commonly used features, i.e., w,w−1, w−2, w+1, w+2, digit, hyphen, capitalization, pre/suffix features, and Brown word clusters. The model seems robust with respect to number of training epochs, cf. Figure 1. Therefore we fix the number of epochs to five and use this setting in all our experiments. Our code is available at: https://bitbucket.org/ bplank/importance-weighting-exp. 3.3 Importance weighting In our first set of experiments, we follow Søgaard and Haulrich (2011) in using document classifiers to obtain weights for the source instances. We train a text classifier that discriminates the two domains (source and target). For each sentence in the source and target domain (the unlabeled text that comes with the SANCL data), we mark whether it comes from the source or target domain and train a binary classifier (logistic regression) to discriminate between the two. For every sentence in the source we obtain its probability for the target domain by doing 5-fold crossvalidation. While Søgaard and Haulrich (2011) use only token-based features (word n-grams ≤ 3)</context>
</contexts>
<marker>Søgaard, Haulrich, 2011</marker>
<rawString>Anders Søgaard and Martin Haulrich. 2011. Sentence-level instance-weighting for graph-based and transition-based dependency parsing. In IWPT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masashi Sugiyama</author>
<author>Shinichi Nakajima</author>
<author>Hisashi Kashima</author>
<author>Paul von B¨unau</author>
<author>Motoaki Kawanabe</author>
</authors>
<title>Direct importance estimation with model selection and its application to covariate shift adaptation.</title>
<date>2007</date>
<booktitle>In NIPS.</booktitle>
<marker>Sugiyama, Nakajima, Kashima, von B¨unau, Kawanabe, 2007</marker>
<rawString>Masashi Sugiyama, Shinichi Nakajima, Hisashi Kashima, Paul von B¨unau, and Motoaki Kawanabe. 2007. Direct importance estimation with model selection and its application to covariate shift adaptation. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oscar T¨ackstr¨om</author>
<author>Dipanjan Das</author>
<author>Slav Petrov</author>
<author>Ryan McDonald</author>
<author>Joakim Nivre</author>
</authors>
<title>Token and type constraints for cross-lingual part-of-speech tagging.</title>
<date>2013</date>
<tech>TACL,</tech>
<pages>1--1</pages>
<marker>T¨ackstr¨om, Das, Petrov, McDonald, Nivre, 2013</marker>
<rawString>Oscar T¨ackstr¨om, Dipanjan Das, Slav Petrov, Ryan McDonald, and Joakim Nivre. 2013. Token and type constraints for cross-lingual part-of-speech tagging. TACL, 1:1–12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bianca Zadrozny</author>
</authors>
<title>Learning and evaluating classifiers under sample selection bias.</title>
<date>2004</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="5467" citStr="Zadrozny (2004)" startWordPosition="852" endWordPosition="853">s of annotated data indicating that there is little variation in emission and transition probabilities across the various domains. 2 Related work Most prior work on importance weighting use a domain classifier, i.e., train a classifier to discriminate between source and target instances (Søgaard and Haulrich, 2011; Plank and Moschitti, 2013) (y E Is, t}). For instance, Søgaard and Haulrich (2011) train a n-gram text classifier and Plank and Moschitti (2013) a tree-kernel based classifier on relation extraction instances. In these studies, Pˆ(t|x) is used as an approximation of Pt(x) following Zadrozny (2004). In §3, we folPs(x), low the approach of Søgaard and Haulrich (2011), but consider a wider range of weight functions. Others have proposed to use kernel mean matching (Huang et al., 2007) or minimizing KLdivergence (Sugiyama et al., 2007). Jiang and Zhai (2007) use importance weighting to select a subsample of the source data by subsequently setting the weight of all selected data points to 1, and 0 otherwise. However, they do so by relying on a sequential model trained on labeled target data. Our results indicate that the covariate shift assumption fails to hold for crossdomain POS tagging. </context>
</contexts>
<marker>Zadrozny, 2004</marker>
<rawString>Bianca Zadrozny. 2004. Learning and evaluating classifiers under sample selection bias. In ICML.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
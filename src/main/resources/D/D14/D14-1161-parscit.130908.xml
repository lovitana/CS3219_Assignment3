<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000125">
<title confidence="0.9977365">
Word Semantic Representations using Bayesian Probabilistic Tensor
Factorization
</title>
<author confidence="0.996947">
Jingwei Zhang and Jeremy Salwen Michael Glass and Alfio Gliozzo
</author>
<affiliation confidence="0.993401">
Columbia University IBM T.J. Waston Research
Computer Science Yorktown Heights, NY 10598, USA
</affiliation>
<address confidence="0.935095">
New York, NY 10027, USA {mrglass,gliozzo}@us.ibm.com
</address>
<email confidence="0.999079">
{jz2541,jas2312}@columbia.edu
</email>
<sectionHeader confidence="0.993897" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999814454545454">
Many forms of word relatedness have been
developed, providing different perspec-
tives on word similarity. We introduce
a Bayesian probabilistic tensor factoriza-
tion model for synthesizing a single word
vector representation and per-perspective
linear transformations from any number
of word similarity matrices. The result-
ing word vectors, when combined with the
per-perspective linear transformation, ap-
proximately recreate while also regulariz-
ing and generalizing, each word similarity
perspective.
Our method can combine manually cre-
ated semantic resources with neural word
embeddings to separate synonyms and
antonyms, and is capable of generaliz-
ing to words outside the vocabulary of
any particular perspective. We evaluated
the word embeddings with GRE antonym
questions, the result achieves the state-of-
the-art performance.
</bodyText>
<sectionHeader confidence="0.999134" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999927517857143">
In recent years, vector space models (VSMs)
have been proved successful in solving various
NLP tasks including named entity recognition,
part-of-speech tagging, parsing, semantic role-
labeling and answering synonym or analogy ques-
tions (Turney et al., 2010; Collobert et al., 2011).
Also, VSMs are reported performing well on
tasks involving the measurement of word related-
ness (Turney et al., 2010). Many existing works
are distributional models, based on the Distribu-
tional Hypothesis, that words occurring in simi-
lar contexts tend to have similar meanings (Har-
ris, 1954). The limitation is that word vectors de-
veloped from distributional models cannot reveal
word relatedness if its information does not lie in
word distributions. For instance, they are believed
to have difficulty distinguishing antonyms from
synonyms, because the distribution of antonymous
words are close, since the context of antonymous
words are always similar to each other (Moham-
mad et al., 2013). Although some research claims
that in certain conditions there do exist differ-
ences between the contexts of different antony-
mous words (Scheible et al., 2013), the differences
are subtle enough that it can hardly be detected by
such language models, especially for rare words.
Another important class of lexical resource for
word relatedness is a lexicon, such as Word-
Net (Miller, 1995) or Roget’s Thesaurus (Kipfer,
2009). Manually producing or extending lexi-
cons is much more labor intensive than generat-
ing VSM word vectors using a corpus. Thus, lex-
icons are sparse with missing words and multi-
word terms as well as missing relationships be-
tween words. Considering the synonym / antonym
perspective as an example, WordNet answers less
than 40% percent of the the GRE antonym ques-
tions provided by Mohammad et al. (2008) di-
rectly. Moreover, binary entries in lexicons do not
indicate the degree of relatedness, such as the de-
gree of lexical contrast between happy and sad or
happy and depressed. The lack of such informa-
tion makes it less fruitful when adopted in NLP
applications.
In this work, we propose a Bayesian tensor fac-
torization model (BPTF) for synthesizing a com-
posite word vector representation by combining
multiple different sources of word relatedness.
The input is a set of word by word matrices, which
may be sparse, providing a number indicating the
presence or degree of relatedness. We treat word
relatedness matrices from different perspectives as
slices, forming a word relatedness tensor. Then the
composite word vectors can be efficiently obtained
by performing BPTF. Furthermore, given any two
words and any trained relatedness perspective, we
</bodyText>
<page confidence="0.946177">
1522
</page>
<note confidence="0.900137">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1522–1531,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999693216216216">
can create or recreate the pair-wise word related-
ness with regularization via per-perspective linear
transformation.
This method allows one set of word vectors to
represent word relatednesses from many different
perspectives (e.g. LSA for topic relatedness / cor-
pus occurrences, ISA relation and YAGO type) It
is able to bring the advantages from both word re-
latedness calculated by distributional models, and
manually created lexicons, since the former have
much more vocabulary coverage and many varia-
tions, while the latter covers word relatedness that
is hard to detect by distributional models. We can
use information from distributional perspectives to
create (if does not exist) or re-create (with regular-
ization) word relatedness from the lexicon’s per-
spective.
We evaluate our model on distinguishing syn-
onyms and antonyms. There are a number of re-
lated works (Lin and Zhao, 2003; Turney, 2008;
Mohammad et al., 2008; Mohammad et al., 2013;
Yih et al., 2012; Chang et al., 2013). A number of
sophisticated methods have been applied, produc-
ing competitive results using diverse approaches.
We use the GRE antonym questions (Mohammad
et al., 2008) as a benchmark, and answer these
questions by finding the most contrasting choice
according to the created or recreated synonym /
antonym word relatedness. The result achieves
state-of-the-art performance.
The rest of this paper is organized as fol-
lows. Section 2 describes the related work of
word vector representations, the BPTF model and
antonymy detection. Section 3 presents our BPTF
model and the sampling method. Section 4 shows
the experimental evaluation and results with Sec-
tion 5 providing conclusion and future work.
</bodyText>
<sectionHeader confidence="0.99994" genericHeader="related work">
2 Related Work
</sectionHeader>
<subsectionHeader confidence="0.99945">
2.1 Word Vector Representations
</subsectionHeader>
<bodyText confidence="0.999973777777778">
Vector space models of semantics have a long his-
tory as part of NLP technologies. One widely-
used method is deriving word vectors using la-
tent semantic analysis (LSA) (Deerwester et al.,
1990), for measuring word similarities. This pro-
vides a topic based perspective on word simi-
larity. In recent years, neural word embeddings
have proved very effective in improving various
NLP tasks (e.g. part-of-speech tagging, chunking,
named entity recognition and semantic role label-
ing) (Collobert et al., 2011). The proposed neural
models have a large number of variations, such as
feed-forward networks (Bengio et al., 2003), hi-
erarchical models (Mnih and Hinton, 2008), re-
current neural networks (Mikolov, 2012), and re-
cursive neural networks (Socher et al., 2011).
Mikolov et al. (2013) reported their vector-space
word representation is able to reveal linguistic
regularities and composite semantics using sim-
ple vector addition and subtraction. For example,
“King−Man+Woman” results in a vector very
close to “Queen”. Luong et al. (2013) proposed
a recursive neural networks model incorporating
morphological structure, and has better perfor-
mance for rare words.
Some non-VSM models1 also generate word
vector representations. Yih et al. (2012) apply po-
larity inducing latent semantic analysis (PILSA)
to a thesaurus to derive the embedding of words.
They treat each entry of a thesaurus as a docu-
ment giving synonyms positive term counts, and
antonyms negative term counts, and preform LSA
on the signed TF-IDF matrix In this way, syn-
onyms will have cosine similarities close to one
and antonyms close to minus one.
Chang et al. (2013) further introduced Multi-
Relational LSA (MRLSA), as as extension of
LSA, that performs Tucker decomposition over a
three-way tensor consisting of multiple relations
(document-term like matrix) between words as
slices, to capture lexical semantics. The purposes
of MRLSA and our model are similar, but the dif-
ferent factorization techniques offer different ad-
vantages. In MRLSA, the k-th slice of tensor W
is approximated by
</bodyText>
<equation confidence="0.849237">
W:,:,k ≈ X:,:,k = US:,:,kVT,
</equation>
<bodyText confidence="0.999405923076923">
where U and V are both for the same word list
but are not guaranteed (or necessarily desired) to
be the same. Thus, this model has the ability to
capture asymmetric relations, but this flexibility is
a detriment for symmetric relatedness. In order to
expand word relatedness coverage, MRLSA needs
to choose a pivot slice (e.g. the synonym slice),
thus there always must existence such a slice, and
the model performance depends on the quality of
this pivot slice. Also, while non-completeness is
a pervasive issue in manually created lexicons,
MRLSA is not flexible enough to treat the un-
known entries as missing. Instead it just sets them
</bodyText>
<footnote confidence="0.9811815">
1As defined by Turney et al. (2010), VSM must be derived
from event frequencies.
</footnote>
<page confidence="0.968554">
1523
</page>
<bodyText confidence="0.98155875">
to zero at the beginning and uses the pivot slice
to re-calculate them. In contrast, our method of
BPTF is well suited to symmetric relations with
many unknown relatedness entries.
</bodyText>
<subsectionHeader confidence="0.96646">
2.2 BPTF Model
</subsectionHeader>
<bodyText confidence="0.997666083333333">
Salakhutdinov and Mnih (2008) introduced a
Bayesian Probabilistic Matrix Factorization
(BPMF) model as a collaborative filtering algo-
rithm. Xiong et al. (2010) proposed a Bayesian
Probabilistic Tensor Factorization (BPTF) model
which further extended the original model to
incorporate temporal factors. They modeled latent
feature vector for users and items, both can be
trained efficiently using Markov chain Monte
Carlo methods, and they obtained competitive
results when applying their models on real-world
recommendation data sets.
</bodyText>
<subsectionHeader confidence="0.998738">
2.3 Antonomy Detection
</subsectionHeader>
<bodyText confidence="0.999978181818182">
There are a number of previous works in detect-
ing antonymy. Lin and Zhao (2003) identifies
antonyms by looking for pre-identified phrases in
corpus datasets. Turney (2008) proposed a su-
pervised classification method for handling analo-
gies, then apply it to antonyms by transforming
antonym pairs into analogy relations. Mohammad
et al. (Mohammad et al., 2008; Mohammad et
al., 2013) proposed empirical approaches consid-
ering corpus co-occurrence statistics and the struc-
ture of a published thesaurus. Based on the as-
sumption that the strongly related words of two
words in a contrasting pair are also often antony-
mous, they use affix patterns (e.g. “un-”, “in-” and
“im-”) and a thesaurus as seed sets to add con-
trast links between word categories. Their best
performance is achieved by further manually an-
notating contrasting adjacent categories. This ap-
proach relies on the Contrast Hypothesis, which
will increase false positives even with a carefully
designed methodology. Furthermore, while this
approach can expand contrast relationships in a
lexicon, out-of-vocabulary words still pose a sub-
stancial challenge.
Yih et al. (2012) and Chang et al. (2013) also
applied their vectors on antonymy detection, and
Yih et al. achieves the state-of-the-art performance
in answering GRE antonym questions. In addition
to the word vectors generated from PILSA, they
use morphology and k-nearest neighbors from dis-
tributional word vector spaces to derive the em-
beddings for out-of-vocabulary words. The latter
is problematic since both synonyms and antonyms
are distributionally similar. Their approach is two
stage: polarity inducing LSA from a manually
created thesaurus, then falling back to morphol-
ogy and distributional similarity when the lexicon
lacks coverage. In contrast, we focus on fusing
the information from thesauruses and automati-
cally induced word relatedness measures during
the word vector space creation. Then prediction
is done in a single stage, from the latent vectors
capturing all word relatedness perspectives and the
appropriate per-perspective transformation vector.
</bodyText>
<sectionHeader confidence="0.999607" genericHeader="method">
3 Methods
</sectionHeader>
<subsectionHeader confidence="0.999374">
3.1 The Bayesian Probabilistic Tensor
Factorization Model
</subsectionHeader>
<bodyText confidence="0.986530066666667">
Our model is a variation of the BPMF model
(Salakhutdinov and Mnih, 2008), and is similar
to the temporal BPTF model (Xiong et al., 2010).
To model word relatedness from multiple perspec-
tives, we denote the relatedness between word i
and word j from perspective k as Rk ij. Then we
can organize these similarities to form a three-way
tensor R E RN×N×lf.
Table 1 shows an example, the first slice of the
tensor is a N x N matrix consists of 1/-1 corre-
sponding to the synonym/antonym entries in the
Roget’s thesaurus, and the second slice is a N xN
matrix consists of the cosine similarity from neural
word embeddings created by Luong et al. (2013),
where N is the number of words in the vocabu-
lary. Note that in our model the entries missing
in Table 1a do not necessarily need to be treated
as zero. Here we use the indicator variable Ik ij
to denote if the entry Rk ij exists (Ikij = 1) or not
(Ikij = 0). If K = 1, the BPTF model becomes to
BPMF. Hence the key difference between BPTF
and BPMF is that the former combines multi-
ple complementary word relatedness perspectives,
while the later only smooths and generalizes over
one.
We assume the relatedness Rk ij to be Gaussian,
and can be expressed as the inner-product of three
D-dimensional latent vectors:
Rk ij|Vi, Vj, Pk — N(&lt; Vi, Vj, Pk &gt;, α−1),
where &lt; ·, ·, · &gt; is a generalization of dot product:
</bodyText>
<equation confidence="0.620939666666667">
D
&lt; Vi, Vj, Pk &gt;- V (d)Vj(d)Pkd),
d=1
</equation>
<page confidence="0.946738">
1524
</page>
<table confidence="0.940699307692308">
happy joyful lucky sad depressed
happy 1 1 -1 -1
joyful 1 -1
lucky 1 -1
sad -1 -1 -1 1
depressed -1 1
(a) The first slice: synonym &amp; antonym relatedness
happy joyful lucky sad depressed
happy .03 .61 .65 .13
joyful .03 .25 .18 .23
lucky .61 .25 .56 .31
sad .65 .18 .56 -.01
depressed .13 .23 .31 -.01
</table>
<tableCaption confidence="0.749511">
(b) The second slice: distributional similarity
Table 1: Word Relatedness Tensor
</tableCaption>
<bodyText confidence="0.921981666666667">
and α is the precision, the reciprocal of the vari-
ance. Vi and Vj are the latent vectors of word i and
word j, and Pk is the latent vector for perspective
k.
We follow a Bayesian approach, adding Gaus-
sian priors to the variables:
</bodyText>
<equation confidence="0.98960925">
Vi ∼ N(µV , A−1
V ),
Pi ∼ N(µP, A−1
P ),
</equation>
<bodyText confidence="0.9999052">
where µV and µP are D dimensional vectors and
AV and AP are D-by-D precision matrices.
Furthermore, we model the prior distribution of
hyper-parameters as conjugate priors (following
the model by (Xiong et al., 2010)):
</bodyText>
<equation confidence="0.9989431875">
p(α) = W(α |ˆW0, v0), Figure 1: The graphical model for BPTF.
W0, ν0
µ0
W0, ν0 ΛV µV µ0
· · · V% · · · V, · · ·
ΛP
µP
k = 1, ..., K
I%k,.7 = 1
Rk%3-
Pk
i, j = 1, ..., N
α
i =6 j
p(µV , AV ) = N(µV |µ0, (Q0AV )−1)W(AV |W0, v0),
p(µP, AP) = N(µP|µ0, (Q0AP)−1)W(AP |W0, v0),
</equation>
<bodyText confidence="0.999821388888889">
where W(W0, v0) is the Wishart distribution of
degree of freedom v and a D-by-D scale matrix
W, and ˆW0 is a 1-by-1 scale matrix for α. The
graphical model is shown in Figure 1 (with Q0 set
to 1). After choosing the hyper-priors, the only re-
maining parameter to tune is the dimension of the
latent vectors.
Due to the existence of prior distributions, our
model can capture the correlation between dif-
ferent perspectives during the factorization stage,
then create or re-create word relatedness using this
correlation for regularization and generalization.
This advantage is especially useful when such cor-
relation is too subtle to be captured by other meth- Where:
ods. On the other hand, if perspectives (let’s say k
and l) are actually unrelated, our model can handle
it as well by making Pk and Pl orthogonal to each
other.
</bodyText>
<subsectionHeader confidence="0.910541">
3.2 Inference
</subsectionHeader>
<bodyText confidence="0.999971">
To avoid calculating intractable distributions, we
use a numerical method to approximate the re-
sults. Here we use the Gibbs sampling algorithm
</bodyText>
<equation confidence="0.961079">
p(α|R, V, P) = W( ˆW0∗, ˆν0∗) (1)
</equation>
<bodyText confidence="0.946436777777778">
to perform the Markov chain Monte Carlo method.
When sampling a block of parameters, all other
parameters are fixed, and this procedure is re-
peated many times until convergence. The sam-
pling algorithm is shown in Algorithm 1.
With conjugate priors, and assuming Iki,i =
0, ∀i, k (we do not consider a word’s relatedness
to itself), the posterior distributions for each block
of parameters are:
</bodyText>
<equation confidence="0.970979823529412">
ˆν∗0 = ˆν0 + 2 N Ikij,
E E
k=1 i,j=1
(Wˆ0∗)−1 = Wˆ −1 2 N Ik ij(Rkij− &lt; Vi, Vj, Pk &gt;)2
0 + E E
k=1 i,j=1
1525
p(µV, AV |V) = N(µV |µ∗0, (β∗0AV)−1)W(AV |W0∗ , ν∗0 )
(2)
Where:
β0µ0 + N V¯
β0 + N , β∗0 = β0 + N, ν∗0 = ν0 + N,
(W0∗)−1 = W −1
0 + N S¯ + β0N
β0 + N (µ0 − V¯)(µ0 − V¯ )T ,
p(µP, AP |P) = N(µP |µ∗0, (β∗0AP)−1)W(AP |W0 ∗ , ν∗0)
(3)
</equation>
<bodyText confidence="0.533371">
Which has the same form as p(µV , AV |V).
</bodyText>
<equation confidence="0.982728272727273">
p(Vi|R, V¬i, P, µV , AV , α) = N(µ∗ i , (A∗i )−1) (4)
Where:
µi = (A∗
∗ i )−1(AV µV + α
Ik ijQjkQT jk,
Qjk = Vj O Pk
O is the element-wise product.
p(Pi|R, V, P¬i, µP, AP, α) = N(µ∗i , (A∗i )−1) (5)
Where:
µ∗k = (A∗k)−1(APµP + α XN IkijRkijXij),
i,j=1
</equation>
<table confidence="0.79908425">
Algorithm 1 Gibbs Sampling for BPTF
Initialize the parameters.
repeat
Sample the hyper-parameters α, µV , AV , µP,
AP (Equation 1, 2, 3)
for i = 1 to N do
Sample Vi (Equation 4)
end for
for k = 1 to 2 do
Sample Pk (Equation 5)
end for
until convergence
</table>
<bodyText confidence="0.994849769230769">
desired. In this situation, our model can generalize
word relatedness for the sparse perspective. For
example, assume perspective k has larger vocabu-
lary coverage Nk, while perspective l has a smaller
coverage Nl.
There are two options for using the high vocab-
ulary word relation matrix to generalize over the
perspective with lower coverage. The most direct
way simply considers the larger vocabulary in the
BPTF R E RNkxNkxK directly. A more efficient
method trains on a tensor using the smaller vocab-
ulary R E RNlxNlxK, then samples the Nk − Nl
word vectors using Equation 4.
</bodyText>
<subsectionHeader confidence="0.996223">
3.3 Predictions
</subsectionHeader>
<bodyText confidence="0.99984325">
With MCMC method, we can approximate the
word relatedness distribution easily by averaging
over a number of samples (instead of calculating
intractable marginal distribution):
</bodyText>
<equation confidence="0.996292612903226">
Vi,
(Vi − V¯)(Vi −
V¯)T
XN
i=1
V¯ =1
N
S¯ = 1
N
XN
i=1
µ0 =
∗
2
X
k=1
k k
Iij Rij Qjk),
N
X
j=1
2
X
k=1
A∗i = AV + α
N
X
j=1
A∗k = AP + α XN IkijXijXTij, p( 1 p( ˆRk ij�V m
Xij = Vi O Vj i,j=1 R M�R) Pz�M1: i , Vjm, Pkm , αm),
m=1
</equation>
<bodyText confidence="0.999699142857143">
The influence each perspective k has on the la-
tent word vectors is roughly propotional to the
number of non-empty entries nk = Ei,j Iki,j. If
one wants to adjust the weight of each slices, this
can easily achieved by adjusting (e.g. down sam-
pling) the number of entries of each slice sampled
at each iteration.
</bodyText>
<sectionHeader confidence="0.770391" genericHeader="method">
3.2.1 Out-of-Vocabulary words
</sectionHeader>
<bodyText confidence="0.999942375">
It often occurs that some of the perspectives have
greater word coverage than the others. For ex-
ample, hand-labeled word relatedness usually has
much less coverage than automatically acquired
similarities. Of course, it is typically for the hand-
labeled perspectives that the generalization is most
where m indicate parameters sampled from differ-
ent sampling iterations.
</bodyText>
<subsectionHeader confidence="0.968539">
3.4 Scalability
</subsectionHeader>
<bodyText confidence="0.99990925">
The time complexity of training our model is
roughly O(n x D2), where n is the number of ob-
served entries in the tensor. If one is only inter-
ested in creating and re-creating word relatedness
of one single slice rather than synthesizing word
vectors, then entries in other slices can be down-
sampled at every iteration to reduce the training
time. In our model, the vector length D is not
sensitive and does not necessarily need to be very
long. Xiong et al. (2010) reported in their collab-
orative filtering experiment D = 10 usually gives
satisfactory performance.
</bodyText>
<page confidence="0.994595">
1526
</page>
<sectionHeader confidence="0.998189" genericHeader="evaluation">
4 Experimental Evaluation
</sectionHeader>
<bodyText confidence="0.953287894736842">
In this section, we evaluate our model by answer-
ing antonym questions. This task is especially
suitable for evaluating our model since the perfor-
mance of straight-forward look-up from the the-
sauruses we considered is poor. There are two ma-
jor limitations:
1. The thesaurus usually only contains antonym
information for word pairs with a strong con-
trast.
2. The vocabulary of the antonym entries in the
thesaurus is limited, and does not contain
many words in the antonym questions.
On the other hand, distributional similarities can
be trained from large corpora and hence have a
large coverage for words. This implies that we can
treat the thesaurus data as the first slice, and the
distributional similarities as the second slice, then
use our model to create / recreate word relatedness
on the first slice to answer antonym questions.
</bodyText>
<subsectionHeader confidence="0.994411">
4.1 The GRE Antonym Questions
</subsectionHeader>
<bodyText confidence="0.998406833333333">
There are several publicly available test datasets
to measure the correctness of our word embed-
dings. In order to be able to compare with pre-
vious works, we follow the widely-used GRE test
dataset provided by (Mohammad et al., 2008),
which has a development set (consisting of 162
questions) and a test set (consisting of 950 ques-
tions). The GRE test is a good benchmark because
the words are relatively rare (19% of the words in
Mohammad’s test are not in the top 50,000 most
frequent words from Google Books (Goldberg and
Orwant, 2013)), thus it is hard to lookup answers
from a thesaurus directly with high recall. Below
is an example of the GRE antonym question:
adulterate: a. renounce b. forbid
c. purify d. criticize e. correct
The goal is to choose the most opposite word from
the target, here the correct answer is purify.
</bodyText>
<subsectionHeader confidence="0.992453">
4.2 Data Resources
</subsectionHeader>
<bodyText confidence="0.9998978">
In our tensor model, the first slice (k = 1) con-
sists of synonyms and antonyms from public the-
sauruses, and the second slice (k = 2) consists of
cosine similarities from neural word embeddings
(example in Table 1)
</bodyText>
<subsectionHeader confidence="0.648519">
4.2.1 Thesaurus
</subsectionHeader>
<bodyText confidence="0.9998545">
Two popular thesauruses used in other research are
the Macquarie Thesaurus and the Encarta The-
saurus. Unfortunately, their electronic versions
are not publicly available. In this work we use two
alternatives:
WordNet Words in WordNet (version 3.0) are
grouped into sense-disambiguated synonym sets
(synsets), and synsets have links between each
other to express conceptual relations. Previ-
ous works reported very different look-up perfor-
mance using WordNet (Mohammad et al., 2008;
Yih et al., 2012), we consider this difference
as different understanding of the WordNet struc-
ture. By extending “indirect antonyms” defined in
WordNet to nouns, verbs and adverbs that similar
words share the antonyms,we achieve a look-up
performance close to Yih et al. (2012). Using this
interpretation of WordNet synonym and antonym
structure we obtain a thesaurus containing 54,239
single-token words. Antonym entries are present
for 21,319 of them with 16.5 words per entry on
average, and 52,750 of them have synonym entries
with 11.7 words per entry on average.
Roget’s Only considering single-token words,
the Roget’s Thesaurus (Kipfer, 2009) contains
47,282 words. Antonym entries are present for
8,802 of them with 4.2 words per entry on av-
erage, and 22,575 of them have synonym entries
with 20.7 words per entry on average. Although
the Roget’s Thesaurus has a less coverage on both
vocabulary and antonym pairs, it has better look-
up precision in the GRE antonym questions.
</bodyText>
<subsectionHeader confidence="0.819338">
4.2.2 Distributional Similarities
</subsectionHeader>
<bodyText confidence="0.9999016875">
We use cosine similarity of the morphRNN word
representations2 provided by Luong et al. (2013)
as a distributional word relatedness perspective.
They used morphological structure in training re-
cursive neural networks and the learned mod-
els outperform previous works on word similarity
tasks, especially a task focused on rare words. The
vector space models were initialized from exist-
ing word embeddings trained on Wikipedia. We
use word embeddings adapted from Collobert et
al. (2011). This advantage complements the weak-
ness of the thesaurus perspective – that it has less
coverage on rare words. The word vector data con-
tains 138,218 words, and it covers 86.9% of the
words in the GRE antonym questions. Combining
the two perspectives, we can cover 99.8% of the
</bodyText>
<page confidence="0.978994">
1527
</page>
<table confidence="0.99996155">
Dev. Set Test Set F1
Prec. Rec. F1 Prec. Rec.
WordNet lookup 0.40 0.40 0.40 0.42 0.41 0.42
WordNet PILSA 0.63 0.62 0.62 0.60 0.60 0.60
WordNet MRLSA 0.66 0.65 0.65 0.61 0.59 0.60
Encarta lookup 0.65 0.61 0.63 0.61 0.56 0.59
Encarta PILSA 0.86 0.81 0.84 0.81 0.74 0.77
Encarta MRLSA 0.87 0.82 0.84 0.82 0.74 0.78
Encarta PILSA + S2Net + Emebed 0.88 0.87 0.87 0.81 0.80 0.81
W&amp;E MRLSA 0.88 0.85 0.87 0.81 0.77 0.79
WordNet lookup* 0.93 0.32 0.48 0.95 0.33 0.49
WordNet lookup 0.48 0.44 0.46 0.46 0.43 0.44
WordNet BPTF 0.63 0.63 0.63 0.63 0.62 0.62
Roget lookup* 1.00 0.35 0.52 0.99 0.31 0.47
Roget lookup 0.61 0.44 0.51 0.55 0.39 0.45
Roget BPTF 0.80 0.80 0.80 0.76 0.75 0.76
W&amp;R lookup* 1.00 0.48 0.64 0.98 0.45 0.62
W&amp;R lookup 0.62 0.54 0.58 0.59 0.51 0.55
W&amp;R BPMF 0.59 0.59 0.59 0.52 0.52 0.52
W&amp;R BPTF 0.88 0.88 0.88 0.82 0.82 0.82
</table>
<tableCaption confidence="0.95789">
Table 2: Development and test results on the GRE antonym questions. *Note: to allow comparison, in
</tableCaption>
<bodyText confidence="0.999374">
look-up we follow the approach used by (Yih et al., 2012): randomly guess an answer if the target word
is in the vocabulary while none of the choices are. Asterisk indicates the look-up results without random
guessing.
GRE antonym question words. Further using mor-
phology information from WordNet, the coverage
achieves 99.9%.
cording to the development set. We choose the
vector length D = 40, the burn-in period starting
from the 30th iterations, then averaging the relat-
edness over 200 runs. The hyper-priors used are
</bodyText>
<subsectionHeader confidence="0.896295">
4.3 Tests p0 = 0, v0 = ˆv0 = D, Q0 = 1 and W0 = ˆW0 = I
</subsectionHeader>
<bodyText confidence="0.99998862962963">
To answer the GRE questions, we calculate R1 for
word pair (i, j), where i is the target word and j
is one of the question’s candidates. The candidate
with the smallest similarity is then the predicted
answer. If a target word is missing in the vocabu-
lary, that question will not be answered, while if a
choice is missing, that choice will be ignored.
We first train on a tensor from a subset consist-
ing of words with antonym entries, then add all
other words using the out-of-vocabulary method
described in Section 3. During each iteration, ze-
ros are randomly added into the first slice to keep
the model from overfitting. In the meantime, the
second slice entries is randomly downsampled to
match the number of non-empty entries in the first
slice. This ensures each perspective has approxi-
mately equal influence on the latent word vectors.
We sample the parameters iteratively, and
choose the burn-in period and vector length D ac-
(not tuned). Note that Yih et al. (2012) use a vec-
tor length of 300, which means our embeddings
save considerable storage space and running time.
Our model usually takes less than 30 minutes to
meet the convergence criteria (on a machine with
an Intel Xeon E3-1230V2 @ 3.3GHz CPU ). In
contrast, the MRLSA requires about 3 hours for
tensor decomposition (Chang et al., 2013).
</bodyText>
<subsectionHeader confidence="0.906919">
4.4 Results
</subsectionHeader>
<bodyText confidence="0.99952375">
The results are summarized in Table 2. We list the
results of previous works (Yih et al., 2012; Chang
et al., 2013) at the top of the table, where the
best performance is achieved by PILSA on Encarta
with further discriminative training and embed-
ding. For comparison, we adopt the standard first
used by (Mohammad et al., 2008), where preci-
sion is the number of questions answered correctly
</bodyText>
<footnote confidence="0.99729">
2http://www-nlp.stanford.edu/ lmthang/morphoNLM/
</footnote>
<page confidence="0.989321">
1528
</page>
<figureCaption confidence="0.972649">
Figure 2: Convergence curves of BPMF and BPTF
</figureCaption>
<bodyText confidence="0.978914703125">
in training the W&amp;R dataset. MAE is the mean
absolute error over the synonym &amp; antonym slice
in the training tensor.
divided by the number of questions answered. Re-
call is the number of questions answered correctly
divided by the total number of questions. BPMF
(Bayesian Probabilistic Matrix Factorization) re-
sult is derived by only keeping the synonym &amp;
antonym slice in our BPTF model.
By using Roget’s and WordNet together, our
method increases the baseline look-up recall from
51% to 82% on the test set, while Yih’s method
increases the recall of Encarta from 56% to 80%.
This state-of-the-art performance is achieved with
the help of a neural network for fine tuning and
multiple schemes of out-of-vocabulary embed-
ding, while our method has inherent and straight-
forward “out-of-vocabulary embedding”. While
MRLSA, which has this character as well, only
has a recall 77% when combining WordNet and
Encarta together.
WordNet records less antonym relations for
nouns, verbs and adverbs, while the GRE antonym
questions has a large coverage of them. Al-
though by extending these antonym relations us-
ing the “indirect antonym” concept achieves better
look-up performance than Roget’s, in contrast, the
BPTF performance is actually much lower. This
implies Roget’s has better recording of antonym
relations. Mohammad et al. (2008) reproted a 23%
F-score look-up performance of WordNet which
support this claim as well. Combining WordNet
and Roget’s together can improve the look-up per-
formance further to 59% precision and 51% recall
(still not as good as Encarta look-up).
Notably, if we strictly follow our BPTF ap-
proach but only use the synonym &amp; antonym slice
(i.e. a matrix factorization model instead of ten-
sor factorization model), this single-slice model
BPMF has performance that is only slightly bet-
ter than look-up. Meanwhile Figure 1 shows the
convergence curves of BPMF and BPTF. BPMF
actually has lower MAE after convergence. Such
behavior is caused by overfitting of BPMF on the
training data. While known entries were recreated
well, empty entries were not filled correctly. On
the other hand, note that although our BPTF model
has a higher MAE, it has much better performance
in answering the GRE antonym questions. We in-
terpret this as the regularization and generalization
effect from other slice(s). Instead of focusing on
one-slice training data, our model fills the missing
entries with the help of inter-slice relations.
We also experimented with a linear metric
learning method over the generated word vectors
(to learn a metric matrix A to measure the word
relatedness via ViTAVj ) using L-BFGS. By op-
timizing the mean square error on the synonym
&amp; antonym slice, we can reduce 8% of the mean
square error on a held out test set, and improve
the F-score by roughly 0.5% (of a single iteration).
Although this method doesn’t give a significant
improvement, it is general and has the potential
to boost the performance in other scenarios.
</bodyText>
<sectionHeader confidence="0.998835" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999989608695652">
In this work, we propose a method to map words
into a metric space automatically using thesaurus
data, previous vector space models, or other word
relatedness matrices as input, which is capable
of handling out-of-vocabulary words of any par-
ticular perspective. This allows us to derive the
relatedness of any given word pair and any per-
spective by the embedded word vectors with per-
perspective linear transformation. We evaluated
the word embeddings with GRE antonym ques-
tions, and the result achieves the state-of-the-art
performance.
For future works, we will extend the model and
its applications in three main directions. First, in
this model we only use a three-way tensor with
two slices, while more relations may be able to
add into it directly. Possible additional perspec-
tive slices include LSA for topic relatedness, and
corpus occurrences in engineered or induced se-
mantic patterns.
Second, we will apply the method to other tasks
that require completing a word relatedness matrix.
We evaluated the performance of our model on
</bodyText>
<figure confidence="0.9916225">
BPMF
BPTF
20 40 60 80 100 120 140
Number of Iterations
2.0
1.5
1.0
0.5
0.0
2.5
</figure>
<page confidence="0.990467">
1529
</page>
<bodyText confidence="0.993565">
creating / recreating one perspective of word re-
latedness: antonymy. Perhaps using vectors gen-
erated from many kinds of perspectives would im-
prove the performance on other NLP tasks, such
as term matching employed by textual entailment
and machine translation metrics.
Third, if our model does learn the relation be-
tween semantic similarities and distributional sim-
ilarities, there may be fruitful information con-
tained in the vectors Vi and Pk that can be ex-
plored. One straight-forward idea is that the dot
product of perspective vectors Pk · Pl should be a
measurement of correlation between perspectives.
Also, a straightforward adaptation of our model
has the potential ability to capture asymmet-
ric word relatedness as well, by using a per-
perspective matrix instead of vector for the asym-
metric slices (i.e. use V T
</bodyText>
<equation confidence="0.90978975">
i AkVj instead of
EDd=1 V (d)
i P(d)
k V (d)
</equation>
<bodyText confidence="0.998105">
j for calculating word related-
ness, where Ak is a square matrix).
</bodyText>
<sectionHeader confidence="0.990645" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9959164">
We thank Christopher Kedzie for assisting the
Semantic Technologies in IBM Watson seminar
course in which this work has been carried out,
and Kai-Wei Chang for giving detailed explana-
tion of the evaluation method in his work.
</bodyText>
<sectionHeader confidence="0.998997" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999941662162162">
Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Re-
search, 3:1137–1155.
Kai-Wei Chang, Wen-tau Yih, and Christopher Meek.
2013. Multi-relational latent semantic analysis. In
EMNLP.
Ronan Collobert, Jason Weston, L´eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. The Journal of Machine Learning Re-
search, 12:2493–2537.
Scott C. Deerwester, Susan T. Dumais, George W. Fur-
nas, Thomas K. Landauer, and Richard A. Harsh-
man. 1990. Indexing by latent semantic analysis.
JASIS, 41(6):391–407.
Yoav Goldberg and Jon Orwant. 2013. A dataset of
syntactic-ngrams over time from a very large cor-
pus of english books. In Second Joint Conference
on Lexical and Computational Semantics (* SEM),
volume 1, pages 241–247.
Zellig Harris. 1954. Distributional structure. Word,
10(23):146–162.
Barbara Ann Kipfer. 2009. Roget’s 21st Century The-
saurus, Third Edition. Philip Lief Group.
Dekang Lin and Shaojun Zhao. 2003. Identifying syn-
onyms among distributionally similar words. In In
Proceedings of IJCAI-03, page 14921493.
Minh-Thang Luong, Richard Socher, and Christo-
pher D. Manning. 2013. Better word representa-
tions with recursive neural networks for morphol-
ogy. In CoNLL, Sofia, Bulgaria.
Tomas Mikolov, Wen tau Yih, and Geoffrey Zweig.
2013. Linguistic regularities in continuous space
word representations. In HLT-NAACL, pages 746–
751. The Association for Computational Linguistics.
Tom Mikolov. 2012. Statistical language models
based on neural networks. Ph.D. thesis, Ph. D. the-
sis, Brno University of Technology.
George A. Miller. 1995. Wordnet: A lexical database
for english. Commun. ACM, 38(11):39–41, Novem-
ber.
Andriy Mnih and Geoffrey E. Hinton. 2008. A scal-
able hierarchical distributed language model. In
NIPS, pages 1081–1088.
Saif Mohammad, Bonnie Dorr, and Graeme Hirst.
2008. Computing word-pair antonymy. In EMNLP,
pages 982–991. Association for Computational Lin-
guistics.
Saif Mohammad, Bonnie Dorr, Graeme Hirst, and Pe-
ter Turney. 2013. Computing lexical contrast. Com-
putational Linguistics, 39(3):555–590.
Ruslan Salakhutdinov and Andriy Mnih. 2008.
Bayesian probabilistic matrix factorization using
markov chain monte carlo. In ICML, pages 880–
887. ACM.
Silke Scheible, Sabine Schulte im Walde, and Sylvia
Springorum. 2013. Uncovering distributional dif-
ferences between synonyms and antonyms in a word
space model. International Joint Conference on
Natural Language Processing, pages 489–497.
Richard Socher, Cliff C. Lin, Andrew Y. Ng, and
Christopher D. Manning. 2011. Parsing natural
scenes and natural language with recursive neural
networks. In Proceedings of the 28th International
Conference on Machine Learning (ICML-11), pages
129–136.
Peter D. Turney, Patrick Pantel, et al. 2010. From
frequency to meaning: Vector space models of se-
mantics. Journal of artificial intelligence research,
37(1):141–188.
Peter D. Turney. 2008. A uniform approach to analo-
gies, synonyms, antonyms, and associations. Col-
ing, pages 905–912, August.
</reference>
<page confidence="0.749542">
1530
</page>
<reference confidence="0.997565444444445">
Liang Xiong, Xi Chen, Tzu-Kuo Huang, Jeff G.
Schneider, and Jaime G. Carbonell. 2010. Tempo-
ral collaborative filtering with bayesian probabilis-
tic tensor factorization. In SDM, volume 10, pages
211–222. SIAM.
Wen-tau Yih, Geoffrey Zweig, and John C. Platt.
2012. Polarity inducing latent semantic analysis. In
EMNLP-CoNLL, pages 1212–1222. Association for
Computational Linguistics.
</reference>
<page confidence="0.992732">
1531
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.701625">
<title confidence="0.9992115">Word Semantic Representations using Bayesian Probabilistic Tensor Factorization</title>
<author confidence="0.988752">Zhang Salwen Michael Glass Gliozzo</author>
<affiliation confidence="0.999748">Columbia University IBM T.J. Waston Research</affiliation>
<address confidence="0.874634">Computer Science Yorktown Heights, NY 10598, USA York, NY 10027, USA</address>
<abstract confidence="0.997714695652174">Many forms of word relatedness have been developed, providing different perspectives on word similarity. We introduce a Bayesian probabilistic tensor factorization model for synthesizing a single word vector representation and per-perspective linear transformations from any number of word similarity matrices. The resulting word vectors, when combined with the per-perspective linear transformation, approximately recreate while also regularizing and generalizing, each word similarity perspective. Our method can combine manually created semantic resources with neural word embeddings to separate synonyms and antonyms, and is capable of generalizing to words outside the vocabulary of any particular perspective. We evaluated the word embeddings with GRE antonym questions, the result achieves the state-ofthe-art performance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>R´ejean Ducharme</author>
<author>Pascal Vincent</author>
<author>Christian Jauvin</author>
</authors>
<title>A neural probabilistic language model.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--1137</pages>
<contexts>
<context position="6371" citStr="Bengio et al., 2003" startWordPosition="964" endWordPosition="967">ons Vector space models of semantics have a long history as part of NLP technologies. One widelyused method is deriving word vectors using latent semantic analysis (LSA) (Deerwester et al., 1990), for measuring word similarities. This provides a topic based perspective on word similarity. In recent years, neural word embeddings have proved very effective in improving various NLP tasks (e.g. part-of-speech tagging, chunking, named entity recognition and semantic role labeling) (Collobert et al., 2011). The proposed neural models have a large number of variations, such as feed-forward networks (Bengio et al., 2003), hierarchical models (Mnih and Hinton, 2008), recurrent neural networks (Mikolov, 2012), and recursive neural networks (Socher et al., 2011). Mikolov et al. (2013) reported their vector-space word representation is able to reveal linguistic regularities and composite semantics using simple vector addition and subtraction. For example, “King−Man+Woman” results in a vector very close to “Queen”. Luong et al. (2013) proposed a recursive neural networks model incorporating morphological structure, and has better performance for rare words. Some non-VSM models1 also generate word vector representa</context>
</contexts>
<marker>Bengio, Ducharme, Vincent, Jauvin, 2003</marker>
<rawString>Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and Christian Jauvin. 2003. A neural probabilistic language model. Journal of Machine Learning Research, 3:1137–1155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kai-Wei Chang</author>
<author>Wen-tau Yih</author>
<author>Christopher Meek</author>
</authors>
<title>Multi-relational latent semantic analysis.</title>
<date>2013</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="5010" citStr="Chang et al., 2013" startWordPosition="753" endWordPosition="756"> calculated by distributional models, and manually created lexicons, since the former have much more vocabulary coverage and many variations, while the latter covers word relatedness that is hard to detect by distributional models. We can use information from distributional perspectives to create (if does not exist) or re-create (with regularization) word relatedness from the lexicon’s perspective. We evaluate our model on distinguishing synonyms and antonyms. There are a number of related works (Lin and Zhao, 2003; Turney, 2008; Mohammad et al., 2008; Mohammad et al., 2013; Yih et al., 2012; Chang et al., 2013). A number of sophisticated methods have been applied, producing competitive results using diverse approaches. We use the GRE antonym questions (Mohammad et al., 2008) as a benchmark, and answer these questions by finding the most contrasting choice according to the created or recreated synonym / antonym word relatedness. The result achieves state-of-the-art performance. The rest of this paper is organized as follows. Section 2 describes the related work of word vector representations, the BPTF model and antonymy detection. Section 3 presents our BPTF model and the sampling method. Section 4 s</context>
<context position="7387" citStr="Chang et al. (2013)" startWordPosition="1123" endWordPosition="1126"> Luong et al. (2013) proposed a recursive neural networks model incorporating morphological structure, and has better performance for rare words. Some non-VSM models1 also generate word vector representations. Yih et al. (2012) apply polarity inducing latent semantic analysis (PILSA) to a thesaurus to derive the embedding of words. They treat each entry of a thesaurus as a document giving synonyms positive term counts, and antonyms negative term counts, and preform LSA on the signed TF-IDF matrix In this way, synonyms will have cosine similarities close to one and antonyms close to minus one. Chang et al. (2013) further introduced MultiRelational LSA (MRLSA), as as extension of LSA, that performs Tucker decomposition over a three-way tensor consisting of multiple relations (document-term like matrix) between words as slices, to capture lexical semantics. The purposes of MRLSA and our model are similar, but the different factorization techniques offer different advantages. In MRLSA, the k-th slice of tensor W is approximated by W:,:,k ≈ X:,:,k = US:,:,kVT, where U and V are both for the same word list but are not guaranteed (or necessarily desired) to be the same. Thus, this model has the ability to c</context>
<context position="10480" citStr="Chang et al. (2013)" startWordPosition="1606" endWordPosition="1609">ly related words of two words in a contrasting pair are also often antonymous, they use affix patterns (e.g. “un-”, “in-” and “im-”) and a thesaurus as seed sets to add contrast links between word categories. Their best performance is achieved by further manually annotating contrasting adjacent categories. This approach relies on the Contrast Hypothesis, which will increase false positives even with a carefully designed methodology. Furthermore, while this approach can expand contrast relationships in a lexicon, out-of-vocabulary words still pose a substancial challenge. Yih et al. (2012) and Chang et al. (2013) also applied their vectors on antonymy detection, and Yih et al. achieves the state-of-the-art performance in answering GRE antonym questions. In addition to the word vectors generated from PILSA, they use morphology and k-nearest neighbors from distributional word vector spaces to derive the embeddings for out-of-vocabulary words. The latter is problematic since both synonyms and antonyms are distributionally similar. Their approach is two stage: polarity inducing LSA from a manually created thesaurus, then falling back to morphology and distributional similarity when the lexicon lacks cover</context>
<context position="25690" citStr="Chang et al., 2013" startWordPosition="4313" endWordPosition="4316">d to match the number of non-empty entries in the first slice. This ensures each perspective has approximately equal influence on the latent word vectors. We sample the parameters iteratively, and choose the burn-in period and vector length D ac(not tuned). Note that Yih et al. (2012) use a vector length of 300, which means our embeddings save considerable storage space and running time. Our model usually takes less than 30 minutes to meet the convergence criteria (on a machine with an Intel Xeon E3-1230V2 @ 3.3GHz CPU ). In contrast, the MRLSA requires about 3 hours for tensor decomposition (Chang et al., 2013). 4.4 Results The results are summarized in Table 2. We list the results of previous works (Yih et al., 2012; Chang et al., 2013) at the top of the table, where the best performance is achieved by PILSA on Encarta with further discriminative training and embedding. For comparison, we adopt the standard first used by (Mohammad et al., 2008), where precision is the number of questions answered correctly 2http://www-nlp.stanford.edu/ lmthang/morphoNLM/ 1528 Figure 2: Convergence curves of BPMF and BPTF in training the W&amp;R dataset. MAE is the mean absolute error over the synonym &amp; antonym slice in</context>
</contexts>
<marker>Chang, Yih, Meek, 2013</marker>
<rawString>Kai-Wei Chang, Wen-tau Yih, and Christopher Meek. 2013. Multi-relational latent semantic analysis. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
<author>L´eon Bottou</author>
<author>Michael Karlen</author>
<author>Koray Kavukcuoglu</author>
<author>Pavel Kuksa</author>
</authors>
<title>Natural language processing (almost) from scratch.</title>
<date>2011</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>12--2493</pages>
<contexts>
<context position="1455" citStr="Collobert et al., 2011" startWordPosition="194" endWordPosition="197">r method can combine manually created semantic resources with neural word embeddings to separate synonyms and antonyms, and is capable of generalizing to words outside the vocabulary of any particular perspective. We evaluated the word embeddings with GRE antonym questions, the result achieves the state-ofthe-art performance. 1 Introduction In recent years, vector space models (VSMs) have been proved successful in solving various NLP tasks including named entity recognition, part-of-speech tagging, parsing, semantic rolelabeling and answering synonym or analogy questions (Turney et al., 2010; Collobert et al., 2011). Also, VSMs are reported performing well on tasks involving the measurement of word relatedness (Turney et al., 2010). Many existing works are distributional models, based on the Distributional Hypothesis, that words occurring in similar contexts tend to have similar meanings (Harris, 1954). The limitation is that word vectors developed from distributional models cannot reveal word relatedness if its information does not lie in word distributions. For instance, they are believed to have difficulty distinguishing antonyms from synonyms, because the distribution of antonymous words are close, s</context>
<context position="6256" citStr="Collobert et al., 2011" startWordPosition="946" endWordPosition="949">valuation and results with Section 5 providing conclusion and future work. 2 Related Work 2.1 Word Vector Representations Vector space models of semantics have a long history as part of NLP technologies. One widelyused method is deriving word vectors using latent semantic analysis (LSA) (Deerwester et al., 1990), for measuring word similarities. This provides a topic based perspective on word similarity. In recent years, neural word embeddings have proved very effective in improving various NLP tasks (e.g. part-of-speech tagging, chunking, named entity recognition and semantic role labeling) (Collobert et al., 2011). The proposed neural models have a large number of variations, such as feed-forward networks (Bengio et al., 2003), hierarchical models (Mnih and Hinton, 2008), recurrent neural networks (Mikolov, 2012), and recursive neural networks (Socher et al., 2011). Mikolov et al. (2013) reported their vector-space word representation is able to reveal linguistic regularities and composite semantics using simple vector addition and subtraction. For example, “King−Man+Woman” results in a vector very close to “Queen”. Luong et al. (2013) proposed a recursive neural networks model incorporating morphologi</context>
<context position="22586" citStr="Collobert et al. (2011)" startWordPosition="3757" endWordPosition="3760">rage on both vocabulary and antonym pairs, it has better lookup precision in the GRE antonym questions. 4.2.2 Distributional Similarities We use cosine similarity of the morphRNN word representations2 provided by Luong et al. (2013) as a distributional word relatedness perspective. They used morphological structure in training recursive neural networks and the learned models outperform previous works on word similarity tasks, especially a task focused on rare words. The vector space models were initialized from existing word embeddings trained on Wikipedia. We use word embeddings adapted from Collobert et al. (2011). This advantage complements the weakness of the thesaurus perspective – that it has less coverage on rare words. The word vector data contains 138,218 words, and it covers 86.9% of the words in the GRE antonym questions. Combining the two perspectives, we can cover 99.8% of the 1527 Dev. Set Test Set F1 Prec. Rec. F1 Prec. Rec. WordNet lookup 0.40 0.40 0.40 0.42 0.41 0.42 WordNet PILSA 0.63 0.62 0.62 0.60 0.60 0.60 WordNet MRLSA 0.66 0.65 0.65 0.61 0.59 0.60 Encarta lookup 0.65 0.61 0.63 0.61 0.56 0.59 Encarta PILSA 0.86 0.81 0.84 0.81 0.74 0.77 Encarta MRLSA 0.87 0.82 0.84 0.82 0.74 0.78 Enc</context>
</contexts>
<marker>Collobert, Weston, Bottou, Karlen, Kavukcuoglu, Kuksa, 2011</marker>
<rawString>Ronan Collobert, Jason Weston, L´eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural language processing (almost) from scratch. The Journal of Machine Learning Research, 12:2493–2537.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott C Deerwester</author>
<author>Susan T Dumais</author>
<author>George W Furnas</author>
<author>Thomas K Landauer</author>
<author>Richard A Harshman</author>
</authors>
<title>Indexing by latent semantic analysis.</title>
<date>1990</date>
<journal>JASIS,</journal>
<volume>41</volume>
<issue>6</issue>
<contexts>
<context position="5946" citStr="Deerwester et al., 1990" startWordPosition="900" endWordPosition="903">elatedness. The result achieves state-of-the-art performance. The rest of this paper is organized as follows. Section 2 describes the related work of word vector representations, the BPTF model and antonymy detection. Section 3 presents our BPTF model and the sampling method. Section 4 shows the experimental evaluation and results with Section 5 providing conclusion and future work. 2 Related Work 2.1 Word Vector Representations Vector space models of semantics have a long history as part of NLP technologies. One widelyused method is deriving word vectors using latent semantic analysis (LSA) (Deerwester et al., 1990), for measuring word similarities. This provides a topic based perspective on word similarity. In recent years, neural word embeddings have proved very effective in improving various NLP tasks (e.g. part-of-speech tagging, chunking, named entity recognition and semantic role labeling) (Collobert et al., 2011). The proposed neural models have a large number of variations, such as feed-forward networks (Bengio et al., 2003), hierarchical models (Mnih and Hinton, 2008), recurrent neural networks (Mikolov, 2012), and recursive neural networks (Socher et al., 2011). Mikolov et al. (2013) reported t</context>
</contexts>
<marker>Deerwester, Dumais, Furnas, Landauer, Harshman, 1990</marker>
<rawString>Scott C. Deerwester, Susan T. Dumais, George W. Furnas, Thomas K. Landauer, and Richard A. Harshman. 1990. Indexing by latent semantic analysis. JASIS, 41(6):391–407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Goldberg</author>
<author>Jon Orwant</author>
</authors>
<title>A dataset of syntactic-ngrams over time from a very large corpus of english books.</title>
<date>2013</date>
<booktitle>In Second Joint Conference on Lexical and Computational Semantics (* SEM),</booktitle>
<volume>1</volume>
<pages>241--247</pages>
<contexts>
<context position="20057" citStr="Goldberg and Orwant, 2013" startWordPosition="3356" endWordPosition="3359">elatedness on the first slice to answer antonym questions. 4.1 The GRE Antonym Questions There are several publicly available test datasets to measure the correctness of our word embeddings. In order to be able to compare with previous works, we follow the widely-used GRE test dataset provided by (Mohammad et al., 2008), which has a development set (consisting of 162 questions) and a test set (consisting of 950 questions). The GRE test is a good benchmark because the words are relatively rare (19% of the words in Mohammad’s test are not in the top 50,000 most frequent words from Google Books (Goldberg and Orwant, 2013)), thus it is hard to lookup answers from a thesaurus directly with high recall. Below is an example of the GRE antonym question: adulterate: a. renounce b. forbid c. purify d. criticize e. correct The goal is to choose the most opposite word from the target, here the correct answer is purify. 4.2 Data Resources In our tensor model, the first slice (k = 1) consists of synonyms and antonyms from public thesauruses, and the second slice (k = 2) consists of cosine similarities from neural word embeddings (example in Table 1) 4.2.1 Thesaurus Two popular thesauruses used in other research are the M</context>
</contexts>
<marker>Goldberg, Orwant, 2013</marker>
<rawString>Yoav Goldberg and Jon Orwant. 2013. A dataset of syntactic-ngrams over time from a very large corpus of english books. In Second Joint Conference on Lexical and Computational Semantics (* SEM), volume 1, pages 241–247.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zellig Harris</author>
</authors>
<date>1954</date>
<journal>Distributional structure. Word,</journal>
<volume>10</volume>
<issue>23</issue>
<contexts>
<context position="1747" citStr="Harris, 1954" startWordPosition="241" endWordPosition="243">te-ofthe-art performance. 1 Introduction In recent years, vector space models (VSMs) have been proved successful in solving various NLP tasks including named entity recognition, part-of-speech tagging, parsing, semantic rolelabeling and answering synonym or analogy questions (Turney et al., 2010; Collobert et al., 2011). Also, VSMs are reported performing well on tasks involving the measurement of word relatedness (Turney et al., 2010). Many existing works are distributional models, based on the Distributional Hypothesis, that words occurring in similar contexts tend to have similar meanings (Harris, 1954). The limitation is that word vectors developed from distributional models cannot reveal word relatedness if its information does not lie in word distributions. For instance, they are believed to have difficulty distinguishing antonyms from synonyms, because the distribution of antonymous words are close, since the context of antonymous words are always similar to each other (Mohammad et al., 2013). Although some research claims that in certain conditions there do exist differences between the contexts of different antonymous words (Scheible et al., 2013), the differences are subtle enough tha</context>
</contexts>
<marker>Harris, 1954</marker>
<rawString>Zellig Harris. 1954. Distributional structure. Word, 10(23):146–162.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara Ann Kipfer</author>
</authors>
<title>Roget’s 21st Century Thesaurus, Third Edition.</title>
<date>2009</date>
<publisher>Philip Lief Group.</publisher>
<contexts>
<context position="2572" citStr="Kipfer, 2009" startWordPosition="370" endWordPosition="371">distinguishing antonyms from synonyms, because the distribution of antonymous words are close, since the context of antonymous words are always similar to each other (Mohammad et al., 2013). Although some research claims that in certain conditions there do exist differences between the contexts of different antonymous words (Scheible et al., 2013), the differences are subtle enough that it can hardly be detected by such language models, especially for rare words. Another important class of lexical resource for word relatedness is a lexicon, such as WordNet (Miller, 1995) or Roget’s Thesaurus (Kipfer, 2009). Manually producing or extending lexicons is much more labor intensive than generating VSM word vectors using a corpus. Thus, lexicons are sparse with missing words and multiword terms as well as missing relationships between words. Considering the synonym / antonym perspective as an example, WordNet answers less than 40% percent of the the GRE antonym questions provided by Mohammad et al. (2008) directly. Moreover, binary entries in lexicons do not indicate the degree of relatedness, such as the degree of lexical contrast between happy and sad or happy and depressed. The lack of such informa</context>
<context position="21732" citStr="Kipfer, 2009" startWordPosition="3624" endWordPosition="3625">ifference as different understanding of the WordNet structure. By extending “indirect antonyms” defined in WordNet to nouns, verbs and adverbs that similar words share the antonyms,we achieve a look-up performance close to Yih et al. (2012). Using this interpretation of WordNet synonym and antonym structure we obtain a thesaurus containing 54,239 single-token words. Antonym entries are present for 21,319 of them with 16.5 words per entry on average, and 52,750 of them have synonym entries with 11.7 words per entry on average. Roget’s Only considering single-token words, the Roget’s Thesaurus (Kipfer, 2009) contains 47,282 words. Antonym entries are present for 8,802 of them with 4.2 words per entry on average, and 22,575 of them have synonym entries with 20.7 words per entry on average. Although the Roget’s Thesaurus has a less coverage on both vocabulary and antonym pairs, it has better lookup precision in the GRE antonym questions. 4.2.2 Distributional Similarities We use cosine similarity of the morphRNN word representations2 provided by Luong et al. (2013) as a distributional word relatedness perspective. They used morphological structure in training recursive neural networks and the learne</context>
</contexts>
<marker>Kipfer, 2009</marker>
<rawString>Barbara Ann Kipfer. 2009. Roget’s 21st Century Thesaurus, Third Edition. Philip Lief Group.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
<author>Shaojun Zhao</author>
</authors>
<title>Identifying synonyms among distributionally similar words. In</title>
<date>2003</date>
<booktitle>In Proceedings of IJCAI-03,</booktitle>
<pages>14921493</pages>
<contexts>
<context position="4911" citStr="Lin and Zhao, 2003" startWordPosition="735" endWordPosition="738">rrences, ISA relation and YAGO type) It is able to bring the advantages from both word relatedness calculated by distributional models, and manually created lexicons, since the former have much more vocabulary coverage and many variations, while the latter covers word relatedness that is hard to detect by distributional models. We can use information from distributional perspectives to create (if does not exist) or re-create (with regularization) word relatedness from the lexicon’s perspective. We evaluate our model on distinguishing synonyms and antonyms. There are a number of related works (Lin and Zhao, 2003; Turney, 2008; Mohammad et al., 2008; Mohammad et al., 2013; Yih et al., 2012; Chang et al., 2013). A number of sophisticated methods have been applied, producing competitive results using diverse approaches. We use the GRE antonym questions (Mohammad et al., 2008) as a benchmark, and answer these questions by finding the most contrasting choice according to the created or recreated synonym / antonym word relatedness. The result achieves state-of-the-art performance. The rest of this paper is organized as follows. Section 2 describes the related work of word vector representations, the BPTF m</context>
<context position="9400" citStr="Lin and Zhao (2003)" startWordPosition="1440" endWordPosition="1443">nih (2008) introduced a Bayesian Probabilistic Matrix Factorization (BPMF) model as a collaborative filtering algorithm. Xiong et al. (2010) proposed a Bayesian Probabilistic Tensor Factorization (BPTF) model which further extended the original model to incorporate temporal factors. They modeled latent feature vector for users and items, both can be trained efficiently using Markov chain Monte Carlo methods, and they obtained competitive results when applying their models on real-world recommendation data sets. 2.3 Antonomy Detection There are a number of previous works in detecting antonymy. Lin and Zhao (2003) identifies antonyms by looking for pre-identified phrases in corpus datasets. Turney (2008) proposed a supervised classification method for handling analogies, then apply it to antonyms by transforming antonym pairs into analogy relations. Mohammad et al. (Mohammad et al., 2008; Mohammad et al., 2013) proposed empirical approaches considering corpus co-occurrence statistics and the structure of a published thesaurus. Based on the assumption that the strongly related words of two words in a contrasting pair are also often antonymous, they use affix patterns (e.g. “un-”, “in-” and “im-”) and a </context>
</contexts>
<marker>Lin, Zhao, 2003</marker>
<rawString>Dekang Lin and Shaojun Zhao. 2003. Identifying synonyms among distributionally similar words. In In Proceedings of IJCAI-03, page 14921493.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minh-Thang Luong</author>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
</authors>
<title>Better word representations with recursive neural networks for morphology. In CoNLL,</title>
<date>2013</date>
<location>Sofia, Bulgaria.</location>
<contexts>
<context position="6788" citStr="Luong et al. (2013)" startWordPosition="1026" endWordPosition="1029">chunking, named entity recognition and semantic role labeling) (Collobert et al., 2011). The proposed neural models have a large number of variations, such as feed-forward networks (Bengio et al., 2003), hierarchical models (Mnih and Hinton, 2008), recurrent neural networks (Mikolov, 2012), and recursive neural networks (Socher et al., 2011). Mikolov et al. (2013) reported their vector-space word representation is able to reveal linguistic regularities and composite semantics using simple vector addition and subtraction. For example, “King−Man+Woman” results in a vector very close to “Queen”. Luong et al. (2013) proposed a recursive neural networks model incorporating morphological structure, and has better performance for rare words. Some non-VSM models1 also generate word vector representations. Yih et al. (2012) apply polarity inducing latent semantic analysis (PILSA) to a thesaurus to derive the embedding of words. They treat each entry of a thesaurus as a document giving synonyms positive term counts, and antonyms negative term counts, and preform LSA on the signed TF-IDF matrix In this way, synonyms will have cosine similarities close to one and antonyms close to minus one. Chang et al. (2013) </context>
<context position="12125" citStr="Luong et al. (2013)" startWordPosition="1866" endWordPosition="1869"> the BPMF model (Salakhutdinov and Mnih, 2008), and is similar to the temporal BPTF model (Xiong et al., 2010). To model word relatedness from multiple perspectives, we denote the relatedness between word i and word j from perspective k as Rk ij. Then we can organize these similarities to form a three-way tensor R E RN×N×lf. Table 1 shows an example, the first slice of the tensor is a N x N matrix consists of 1/-1 corresponding to the synonym/antonym entries in the Roget’s thesaurus, and the second slice is a N xN matrix consists of the cosine similarity from neural word embeddings created by Luong et al. (2013), where N is the number of words in the vocabulary. Note that in our model the entries missing in Table 1a do not necessarily need to be treated as zero. Here we use the indicator variable Ik ij to denote if the entry Rk ij exists (Ikij = 1) or not (Ikij = 0). If K = 1, the BPTF model becomes to BPMF. Hence the key difference between BPTF and BPMF is that the former combines multiple complementary word relatedness perspectives, while the later only smooths and generalizes over one. We assume the relatedness Rk ij to be Gaussian, and can be expressed as the inner-product of three D-dimensional </context>
<context position="22195" citStr="Luong et al. (2013)" startWordPosition="3698" endWordPosition="3701"> 52,750 of them have synonym entries with 11.7 words per entry on average. Roget’s Only considering single-token words, the Roget’s Thesaurus (Kipfer, 2009) contains 47,282 words. Antonym entries are present for 8,802 of them with 4.2 words per entry on average, and 22,575 of them have synonym entries with 20.7 words per entry on average. Although the Roget’s Thesaurus has a less coverage on both vocabulary and antonym pairs, it has better lookup precision in the GRE antonym questions. 4.2.2 Distributional Similarities We use cosine similarity of the morphRNN word representations2 provided by Luong et al. (2013) as a distributional word relatedness perspective. They used morphological structure in training recursive neural networks and the learned models outperform previous works on word similarity tasks, especially a task focused on rare words. The vector space models were initialized from existing word embeddings trained on Wikipedia. We use word embeddings adapted from Collobert et al. (2011). This advantage complements the weakness of the thesaurus perspective – that it has less coverage on rare words. The word vector data contains 138,218 words, and it covers 86.9% of the words in the GRE antony</context>
</contexts>
<marker>Luong, Socher, Manning, 2013</marker>
<rawString>Minh-Thang Luong, Richard Socher, and Christopher D. Manning. 2013. Better word representations with recursive neural networks for morphology. In CoNLL, Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Wen tau Yih</author>
<author>Geoffrey Zweig</author>
</authors>
<title>Linguistic regularities in continuous space word representations.</title>
<date>2013</date>
<booktitle>In HLT-NAACL,</booktitle>
<pages>746--751</pages>
<contexts>
<context position="6535" citStr="Mikolov et al. (2013)" startWordPosition="990" endWordPosition="993"> (LSA) (Deerwester et al., 1990), for measuring word similarities. This provides a topic based perspective on word similarity. In recent years, neural word embeddings have proved very effective in improving various NLP tasks (e.g. part-of-speech tagging, chunking, named entity recognition and semantic role labeling) (Collobert et al., 2011). The proposed neural models have a large number of variations, such as feed-forward networks (Bengio et al., 2003), hierarchical models (Mnih and Hinton, 2008), recurrent neural networks (Mikolov, 2012), and recursive neural networks (Socher et al., 2011). Mikolov et al. (2013) reported their vector-space word representation is able to reveal linguistic regularities and composite semantics using simple vector addition and subtraction. For example, “King−Man+Woman” results in a vector very close to “Queen”. Luong et al. (2013) proposed a recursive neural networks model incorporating morphological structure, and has better performance for rare words. Some non-VSM models1 also generate word vector representations. Yih et al. (2012) apply polarity inducing latent semantic analysis (PILSA) to a thesaurus to derive the embedding of words. They treat each entry of a thesau</context>
</contexts>
<marker>Mikolov, Yih, Zweig, 2013</marker>
<rawString>Tomas Mikolov, Wen tau Yih, and Geoffrey Zweig. 2013. Linguistic regularities in continuous space word representations. In HLT-NAACL, pages 746– 751. The Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom Mikolov</author>
</authors>
<title>Statistical language models based on neural networks.</title>
<date>2012</date>
<tech>Ph.D. thesis, Ph. D. thesis,</tech>
<institution>Brno University of Technology.</institution>
<contexts>
<context position="6459" citStr="Mikolov, 2012" startWordPosition="979" endWordPosition="980">lyused method is deriving word vectors using latent semantic analysis (LSA) (Deerwester et al., 1990), for measuring word similarities. This provides a topic based perspective on word similarity. In recent years, neural word embeddings have proved very effective in improving various NLP tasks (e.g. part-of-speech tagging, chunking, named entity recognition and semantic role labeling) (Collobert et al., 2011). The proposed neural models have a large number of variations, such as feed-forward networks (Bengio et al., 2003), hierarchical models (Mnih and Hinton, 2008), recurrent neural networks (Mikolov, 2012), and recursive neural networks (Socher et al., 2011). Mikolov et al. (2013) reported their vector-space word representation is able to reveal linguistic regularities and composite semantics using simple vector addition and subtraction. For example, “King−Man+Woman” results in a vector very close to “Queen”. Luong et al. (2013) proposed a recursive neural networks model incorporating morphological structure, and has better performance for rare words. Some non-VSM models1 also generate word vector representations. Yih et al. (2012) apply polarity inducing latent semantic analysis (PILSA) to a t</context>
</contexts>
<marker>Mikolov, 2012</marker>
<rawString>Tom Mikolov. 2012. Statistical language models based on neural networks. Ph.D. thesis, Ph. D. thesis, Brno University of Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>Wordnet: A lexical database for english.</title>
<date>1995</date>
<journal>Commun. ACM,</journal>
<volume>38</volume>
<issue>11</issue>
<contexts>
<context position="2536" citStr="Miller, 1995" startWordPosition="365" endWordPosition="366">hey are believed to have difficulty distinguishing antonyms from synonyms, because the distribution of antonymous words are close, since the context of antonymous words are always similar to each other (Mohammad et al., 2013). Although some research claims that in certain conditions there do exist differences between the contexts of different antonymous words (Scheible et al., 2013), the differences are subtle enough that it can hardly be detected by such language models, especially for rare words. Another important class of lexical resource for word relatedness is a lexicon, such as WordNet (Miller, 1995) or Roget’s Thesaurus (Kipfer, 2009). Manually producing or extending lexicons is much more labor intensive than generating VSM word vectors using a corpus. Thus, lexicons are sparse with missing words and multiword terms as well as missing relationships between words. Considering the synonym / antonym perspective as an example, WordNet answers less than 40% percent of the the GRE antonym questions provided by Mohammad et al. (2008) directly. Moreover, binary entries in lexicons do not indicate the degree of relatedness, such as the degree of lexical contrast between happy and sad or happy and</context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>George A. Miller. 1995. Wordnet: A lexical database for english. Commun. ACM, 38(11):39–41, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andriy Mnih</author>
<author>Geoffrey E Hinton</author>
</authors>
<title>A scalable hierarchical distributed language model.</title>
<date>2008</date>
<booktitle>In NIPS,</booktitle>
<pages>1081--1088</pages>
<contexts>
<context position="6416" citStr="Mnih and Hinton, 2008" startWordPosition="971" endWordPosition="974"> long history as part of NLP technologies. One widelyused method is deriving word vectors using latent semantic analysis (LSA) (Deerwester et al., 1990), for measuring word similarities. This provides a topic based perspective on word similarity. In recent years, neural word embeddings have proved very effective in improving various NLP tasks (e.g. part-of-speech tagging, chunking, named entity recognition and semantic role labeling) (Collobert et al., 2011). The proposed neural models have a large number of variations, such as feed-forward networks (Bengio et al., 2003), hierarchical models (Mnih and Hinton, 2008), recurrent neural networks (Mikolov, 2012), and recursive neural networks (Socher et al., 2011). Mikolov et al. (2013) reported their vector-space word representation is able to reveal linguistic regularities and composite semantics using simple vector addition and subtraction. For example, “King−Man+Woman” results in a vector very close to “Queen”. Luong et al. (2013) proposed a recursive neural networks model incorporating morphological structure, and has better performance for rare words. Some non-VSM models1 also generate word vector representations. Yih et al. (2012) apply polarity induc</context>
</contexts>
<marker>Mnih, Hinton, 2008</marker>
<rawString>Andriy Mnih and Geoffrey E. Hinton. 2008. A scalable hierarchical distributed language model. In NIPS, pages 1081–1088.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saif Mohammad</author>
<author>Bonnie Dorr</author>
<author>Graeme Hirst</author>
</authors>
<title>Computing word-pair antonymy.</title>
<date>2008</date>
<booktitle>In EMNLP,</booktitle>
<pages>982--991</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2972" citStr="Mohammad et al. (2008)" startWordPosition="436" endWordPosition="439">t can hardly be detected by such language models, especially for rare words. Another important class of lexical resource for word relatedness is a lexicon, such as WordNet (Miller, 1995) or Roget’s Thesaurus (Kipfer, 2009). Manually producing or extending lexicons is much more labor intensive than generating VSM word vectors using a corpus. Thus, lexicons are sparse with missing words and multiword terms as well as missing relationships between words. Considering the synonym / antonym perspective as an example, WordNet answers less than 40% percent of the the GRE antonym questions provided by Mohammad et al. (2008) directly. Moreover, binary entries in lexicons do not indicate the degree of relatedness, such as the degree of lexical contrast between happy and sad or happy and depressed. The lack of such information makes it less fruitful when adopted in NLP applications. In this work, we propose a Bayesian tensor factorization model (BPTF) for synthesizing a composite word vector representation by combining multiple different sources of word relatedness. The input is a set of word by word matrices, which may be sparse, providing a number indicating the presence or degree of relatedness. We treat word re</context>
<context position="4948" citStr="Mohammad et al., 2008" startWordPosition="741" endWordPosition="744">e) It is able to bring the advantages from both word relatedness calculated by distributional models, and manually created lexicons, since the former have much more vocabulary coverage and many variations, while the latter covers word relatedness that is hard to detect by distributional models. We can use information from distributional perspectives to create (if does not exist) or re-create (with regularization) word relatedness from the lexicon’s perspective. We evaluate our model on distinguishing synonyms and antonyms. There are a number of related works (Lin and Zhao, 2003; Turney, 2008; Mohammad et al., 2008; Mohammad et al., 2013; Yih et al., 2012; Chang et al., 2013). A number of sophisticated methods have been applied, producing competitive results using diverse approaches. We use the GRE antonym questions (Mohammad et al., 2008) as a benchmark, and answer these questions by finding the most contrasting choice according to the created or recreated synonym / antonym word relatedness. The result achieves state-of-the-art performance. The rest of this paper is organized as follows. Section 2 describes the related work of word vector representations, the BPTF model and antonymy detection. Section </context>
<context position="9679" citStr="Mohammad et al., 2008" startWordPosition="1481" endWordPosition="1484">ctors. They modeled latent feature vector for users and items, both can be trained efficiently using Markov chain Monte Carlo methods, and they obtained competitive results when applying their models on real-world recommendation data sets. 2.3 Antonomy Detection There are a number of previous works in detecting antonymy. Lin and Zhao (2003) identifies antonyms by looking for pre-identified phrases in corpus datasets. Turney (2008) proposed a supervised classification method for handling analogies, then apply it to antonyms by transforming antonym pairs into analogy relations. Mohammad et al. (Mohammad et al., 2008; Mohammad et al., 2013) proposed empirical approaches considering corpus co-occurrence statistics and the structure of a published thesaurus. Based on the assumption that the strongly related words of two words in a contrasting pair are also often antonymous, they use affix patterns (e.g. “un-”, “in-” and “im-”) and a thesaurus as seed sets to add contrast links between word categories. Their best performance is achieved by further manually annotating contrasting adjacent categories. This approach relies on the Contrast Hypothesis, which will increase false positives even with a carefully des</context>
<context position="19752" citStr="Mohammad et al., 2008" startWordPosition="3302" endWordPosition="3305">ns. On the other hand, distributional similarities can be trained from large corpora and hence have a large coverage for words. This implies that we can treat the thesaurus data as the first slice, and the distributional similarities as the second slice, then use our model to create / recreate word relatedness on the first slice to answer antonym questions. 4.1 The GRE Antonym Questions There are several publicly available test datasets to measure the correctness of our word embeddings. In order to be able to compare with previous works, we follow the widely-used GRE test dataset provided by (Mohammad et al., 2008), which has a development set (consisting of 162 questions) and a test set (consisting of 950 questions). The GRE test is a good benchmark because the words are relatively rare (19% of the words in Mohammad’s test are not in the top 50,000 most frequent words from Google Books (Goldberg and Orwant, 2013)), thus it is hard to lookup answers from a thesaurus directly with high recall. Below is an example of the GRE antonym question: adulterate: a. renounce b. forbid c. purify d. criticize e. correct The goal is to choose the most opposite word from the target, here the correct answer is purify. </context>
<context position="21080" citStr="Mohammad et al., 2008" startWordPosition="3522" endWordPosition="3525">ic thesauruses, and the second slice (k = 2) consists of cosine similarities from neural word embeddings (example in Table 1) 4.2.1 Thesaurus Two popular thesauruses used in other research are the Macquarie Thesaurus and the Encarta Thesaurus. Unfortunately, their electronic versions are not publicly available. In this work we use two alternatives: WordNet Words in WordNet (version 3.0) are grouped into sense-disambiguated synonym sets (synsets), and synsets have links between each other to express conceptual relations. Previous works reported very different look-up performance using WordNet (Mohammad et al., 2008; Yih et al., 2012), we consider this difference as different understanding of the WordNet structure. By extending “indirect antonyms” defined in WordNet to nouns, verbs and adverbs that similar words share the antonyms,we achieve a look-up performance close to Yih et al. (2012). Using this interpretation of WordNet synonym and antonym structure we obtain a thesaurus containing 54,239 single-token words. Antonym entries are present for 21,319 of them with 16.5 words per entry on average, and 52,750 of them have synonym entries with 11.7 words per entry on average. Roget’s Only considering sing</context>
<context position="26031" citStr="Mohammad et al., 2008" startWordPosition="4373" endWordPosition="4376">gs save considerable storage space and running time. Our model usually takes less than 30 minutes to meet the convergence criteria (on a machine with an Intel Xeon E3-1230V2 @ 3.3GHz CPU ). In contrast, the MRLSA requires about 3 hours for tensor decomposition (Chang et al., 2013). 4.4 Results The results are summarized in Table 2. We list the results of previous works (Yih et al., 2012; Chang et al., 2013) at the top of the table, where the best performance is achieved by PILSA on Encarta with further discriminative training and embedding. For comparison, we adopt the standard first used by (Mohammad et al., 2008), where precision is the number of questions answered correctly 2http://www-nlp.stanford.edu/ lmthang/morphoNLM/ 1528 Figure 2: Convergence curves of BPMF and BPTF in training the W&amp;R dataset. MAE is the mean absolute error over the synonym &amp; antonym slice in the training tensor. divided by the number of questions answered. Recall is the number of questions answered correctly divided by the total number of questions. BPMF (Bayesian Probabilistic Matrix Factorization) result is derived by only keeping the synonym &amp; antonym slice in our BPTF model. By using Roget’s and WordNet together, our meth</context>
<context position="27525" citStr="Mohammad et al. (2008)" startWordPosition="4605" endWordPosition="4608">abulary embedding, while our method has inherent and straightforward “out-of-vocabulary embedding”. While MRLSA, which has this character as well, only has a recall 77% when combining WordNet and Encarta together. WordNet records less antonym relations for nouns, verbs and adverbs, while the GRE antonym questions has a large coverage of them. Although by extending these antonym relations using the “indirect antonym” concept achieves better look-up performance than Roget’s, in contrast, the BPTF performance is actually much lower. This implies Roget’s has better recording of antonym relations. Mohammad et al. (2008) reproted a 23% F-score look-up performance of WordNet which support this claim as well. Combining WordNet and Roget’s together can improve the look-up performance further to 59% precision and 51% recall (still not as good as Encarta look-up). Notably, if we strictly follow our BPTF approach but only use the synonym &amp; antonym slice (i.e. a matrix factorization model instead of tensor factorization model), this single-slice model BPMF has performance that is only slightly better than look-up. Meanwhile Figure 1 shows the convergence curves of BPMF and BPTF. BPMF actually has lower MAE after con</context>
</contexts>
<marker>Mohammad, Dorr, Hirst, 2008</marker>
<rawString>Saif Mohammad, Bonnie Dorr, and Graeme Hirst. 2008. Computing word-pair antonymy. In EMNLP, pages 982–991. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saif Mohammad</author>
<author>Bonnie Dorr</author>
<author>Graeme Hirst</author>
<author>Peter Turney</author>
</authors>
<date>2013</date>
<booktitle>Computing lexical contrast. Computational Linguistics,</booktitle>
<pages>39--3</pages>
<contexts>
<context position="2148" citStr="Mohammad et al., 2013" startWordPosition="300" endWordPosition="304">ement of word relatedness (Turney et al., 2010). Many existing works are distributional models, based on the Distributional Hypothesis, that words occurring in similar contexts tend to have similar meanings (Harris, 1954). The limitation is that word vectors developed from distributional models cannot reveal word relatedness if its information does not lie in word distributions. For instance, they are believed to have difficulty distinguishing antonyms from synonyms, because the distribution of antonymous words are close, since the context of antonymous words are always similar to each other (Mohammad et al., 2013). Although some research claims that in certain conditions there do exist differences between the contexts of different antonymous words (Scheible et al., 2013), the differences are subtle enough that it can hardly be detected by such language models, especially for rare words. Another important class of lexical resource for word relatedness is a lexicon, such as WordNet (Miller, 1995) or Roget’s Thesaurus (Kipfer, 2009). Manually producing or extending lexicons is much more labor intensive than generating VSM word vectors using a corpus. Thus, lexicons are sparse with missing words and multiw</context>
<context position="4971" citStr="Mohammad et al., 2013" startWordPosition="745" endWordPosition="748">the advantages from both word relatedness calculated by distributional models, and manually created lexicons, since the former have much more vocabulary coverage and many variations, while the latter covers word relatedness that is hard to detect by distributional models. We can use information from distributional perspectives to create (if does not exist) or re-create (with regularization) word relatedness from the lexicon’s perspective. We evaluate our model on distinguishing synonyms and antonyms. There are a number of related works (Lin and Zhao, 2003; Turney, 2008; Mohammad et al., 2008; Mohammad et al., 2013; Yih et al., 2012; Chang et al., 2013). A number of sophisticated methods have been applied, producing competitive results using diverse approaches. We use the GRE antonym questions (Mohammad et al., 2008) as a benchmark, and answer these questions by finding the most contrasting choice according to the created or recreated synonym / antonym word relatedness. The result achieves state-of-the-art performance. The rest of this paper is organized as follows. Section 2 describes the related work of word vector representations, the BPTF model and antonymy detection. Section 3 presents our BPTF mod</context>
<context position="9703" citStr="Mohammad et al., 2013" startWordPosition="1485" endWordPosition="1488">ent feature vector for users and items, both can be trained efficiently using Markov chain Monte Carlo methods, and they obtained competitive results when applying their models on real-world recommendation data sets. 2.3 Antonomy Detection There are a number of previous works in detecting antonymy. Lin and Zhao (2003) identifies antonyms by looking for pre-identified phrases in corpus datasets. Turney (2008) proposed a supervised classification method for handling analogies, then apply it to antonyms by transforming antonym pairs into analogy relations. Mohammad et al. (Mohammad et al., 2008; Mohammad et al., 2013) proposed empirical approaches considering corpus co-occurrence statistics and the structure of a published thesaurus. Based on the assumption that the strongly related words of two words in a contrasting pair are also often antonymous, they use affix patterns (e.g. “un-”, “in-” and “im-”) and a thesaurus as seed sets to add contrast links between word categories. Their best performance is achieved by further manually annotating contrasting adjacent categories. This approach relies on the Contrast Hypothesis, which will increase false positives even with a carefully designed methodology. Furth</context>
</contexts>
<marker>Mohammad, Dorr, Hirst, Turney, 2013</marker>
<rawString>Saif Mohammad, Bonnie Dorr, Graeme Hirst, and Peter Turney. 2013. Computing lexical contrast. Computational Linguistics, 39(3):555–590.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruslan Salakhutdinov</author>
<author>Andriy Mnih</author>
</authors>
<title>Bayesian probabilistic matrix factorization using markov chain monte carlo.</title>
<date>2008</date>
<booktitle>In ICML,</booktitle>
<pages>880--887</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="8791" citStr="Salakhutdinov and Mnih (2008)" startWordPosition="1352" endWordPosition="1355">e.g. the synonym slice), thus there always must existence such a slice, and the model performance depends on the quality of this pivot slice. Also, while non-completeness is a pervasive issue in manually created lexicons, MRLSA is not flexible enough to treat the unknown entries as missing. Instead it just sets them 1As defined by Turney et al. (2010), VSM must be derived from event frequencies. 1523 to zero at the beginning and uses the pivot slice to re-calculate them. In contrast, our method of BPTF is well suited to symmetric relations with many unknown relatedness entries. 2.2 BPTF Model Salakhutdinov and Mnih (2008) introduced a Bayesian Probabilistic Matrix Factorization (BPMF) model as a collaborative filtering algorithm. Xiong et al. (2010) proposed a Bayesian Probabilistic Tensor Factorization (BPTF) model which further extended the original model to incorporate temporal factors. They modeled latent feature vector for users and items, both can be trained efficiently using Markov chain Monte Carlo methods, and they obtained competitive results when applying their models on real-world recommendation data sets. 2.3 Antonomy Detection There are a number of previous works in detecting antonymy. Lin and Zh</context>
<context position="11552" citStr="Salakhutdinov and Mnih, 2008" startWordPosition="1762" endWordPosition="1765">wo stage: polarity inducing LSA from a manually created thesaurus, then falling back to morphology and distributional similarity when the lexicon lacks coverage. In contrast, we focus on fusing the information from thesauruses and automatically induced word relatedness measures during the word vector space creation. Then prediction is done in a single stage, from the latent vectors capturing all word relatedness perspectives and the appropriate per-perspective transformation vector. 3 Methods 3.1 The Bayesian Probabilistic Tensor Factorization Model Our model is a variation of the BPMF model (Salakhutdinov and Mnih, 2008), and is similar to the temporal BPTF model (Xiong et al., 2010). To model word relatedness from multiple perspectives, we denote the relatedness between word i and word j from perspective k as Rk ij. Then we can organize these similarities to form a three-way tensor R E RN×N×lf. Table 1 shows an example, the first slice of the tensor is a N x N matrix consists of 1/-1 corresponding to the synonym/antonym entries in the Roget’s thesaurus, and the second slice is a N xN matrix consists of the cosine similarity from neural word embeddings created by Luong et al. (2013), where N is the number of </context>
</contexts>
<marker>Salakhutdinov, Mnih, 2008</marker>
<rawString>Ruslan Salakhutdinov and Andriy Mnih. 2008. Bayesian probabilistic matrix factorization using markov chain monte carlo. In ICML, pages 880– 887. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Silke Scheible</author>
<author>Sabine Schulte im Walde</author>
<author>Sylvia Springorum</author>
</authors>
<title>Uncovering distributional differences between synonyms and antonyms in a word space model.</title>
<date>2013</date>
<booktitle>International Joint Conference on Natural Language Processing,</booktitle>
<pages>489--497</pages>
<contexts>
<context position="2308" citStr="Scheible et al., 2013" startWordPosition="326" endWordPosition="329">similar contexts tend to have similar meanings (Harris, 1954). The limitation is that word vectors developed from distributional models cannot reveal word relatedness if its information does not lie in word distributions. For instance, they are believed to have difficulty distinguishing antonyms from synonyms, because the distribution of antonymous words are close, since the context of antonymous words are always similar to each other (Mohammad et al., 2013). Although some research claims that in certain conditions there do exist differences between the contexts of different antonymous words (Scheible et al., 2013), the differences are subtle enough that it can hardly be detected by such language models, especially for rare words. Another important class of lexical resource for word relatedness is a lexicon, such as WordNet (Miller, 1995) or Roget’s Thesaurus (Kipfer, 2009). Manually producing or extending lexicons is much more labor intensive than generating VSM word vectors using a corpus. Thus, lexicons are sparse with missing words and multiword terms as well as missing relationships between words. Considering the synonym / antonym perspective as an example, WordNet answers less than 40% percent of </context>
</contexts>
<marker>Scheible, Walde, Springorum, 2013</marker>
<rawString>Silke Scheible, Sabine Schulte im Walde, and Sylvia Springorum. 2013. Uncovering distributional differences between synonyms and antonyms in a word space model. International Joint Conference on Natural Language Processing, pages 489–497.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Cliff C Lin</author>
<author>Andrew Y Ng</author>
<author>Christopher D Manning</author>
</authors>
<title>Parsing natural scenes and natural language with recursive neural networks.</title>
<date>2011</date>
<booktitle>In Proceedings of the 28th International Conference on Machine Learning (ICML-11),</booktitle>
<pages>129--136</pages>
<contexts>
<context position="6512" citStr="Socher et al., 2011" startWordPosition="986" endWordPosition="989">tent semantic analysis (LSA) (Deerwester et al., 1990), for measuring word similarities. This provides a topic based perspective on word similarity. In recent years, neural word embeddings have proved very effective in improving various NLP tasks (e.g. part-of-speech tagging, chunking, named entity recognition and semantic role labeling) (Collobert et al., 2011). The proposed neural models have a large number of variations, such as feed-forward networks (Bengio et al., 2003), hierarchical models (Mnih and Hinton, 2008), recurrent neural networks (Mikolov, 2012), and recursive neural networks (Socher et al., 2011). Mikolov et al. (2013) reported their vector-space word representation is able to reveal linguistic regularities and composite semantics using simple vector addition and subtraction. For example, “King−Man+Woman” results in a vector very close to “Queen”. Luong et al. (2013) proposed a recursive neural networks model incorporating morphological structure, and has better performance for rare words. Some non-VSM models1 also generate word vector representations. Yih et al. (2012) apply polarity inducing latent semantic analysis (PILSA) to a thesaurus to derive the embedding of words. They treat</context>
</contexts>
<marker>Socher, Lin, Ng, Manning, 2011</marker>
<rawString>Richard Socher, Cliff C. Lin, Andrew Y. Ng, and Christopher D. Manning. 2011. Parsing natural scenes and natural language with recursive neural networks. In Proceedings of the 28th International Conference on Machine Learning (ICML-11), pages 129–136.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
<author>Patrick Pantel</author>
</authors>
<title>From frequency to meaning: Vector space models of semantics.</title>
<date>2010</date>
<journal>Journal of artificial intelligence research,</journal>
<volume>37</volume>
<issue>1</issue>
<marker>Turney, Pantel, 2010</marker>
<rawString>Peter D. Turney, Patrick Pantel, et al. 2010. From frequency to meaning: Vector space models of semantics. Journal of artificial intelligence research, 37(1):141–188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>A uniform approach to analogies, synonyms, antonyms, and associations.</title>
<date>2008</date>
<journal>Coling,</journal>
<pages>905--912</pages>
<contexts>
<context position="4925" citStr="Turney, 2008" startWordPosition="739" endWordPosition="740">n and YAGO type) It is able to bring the advantages from both word relatedness calculated by distributional models, and manually created lexicons, since the former have much more vocabulary coverage and many variations, while the latter covers word relatedness that is hard to detect by distributional models. We can use information from distributional perspectives to create (if does not exist) or re-create (with regularization) word relatedness from the lexicon’s perspective. We evaluate our model on distinguishing synonyms and antonyms. There are a number of related works (Lin and Zhao, 2003; Turney, 2008; Mohammad et al., 2008; Mohammad et al., 2013; Yih et al., 2012; Chang et al., 2013). A number of sophisticated methods have been applied, producing competitive results using diverse approaches. We use the GRE antonym questions (Mohammad et al., 2008) as a benchmark, and answer these questions by finding the most contrasting choice according to the created or recreated synonym / antonym word relatedness. The result achieves state-of-the-art performance. The rest of this paper is organized as follows. Section 2 describes the related work of word vector representations, the BPTF model and anton</context>
<context position="9492" citStr="Turney (2008)" startWordPosition="1454" endWordPosition="1455">e filtering algorithm. Xiong et al. (2010) proposed a Bayesian Probabilistic Tensor Factorization (BPTF) model which further extended the original model to incorporate temporal factors. They modeled latent feature vector for users and items, both can be trained efficiently using Markov chain Monte Carlo methods, and they obtained competitive results when applying their models on real-world recommendation data sets. 2.3 Antonomy Detection There are a number of previous works in detecting antonymy. Lin and Zhao (2003) identifies antonyms by looking for pre-identified phrases in corpus datasets. Turney (2008) proposed a supervised classification method for handling analogies, then apply it to antonyms by transforming antonym pairs into analogy relations. Mohammad et al. (Mohammad et al., 2008; Mohammad et al., 2013) proposed empirical approaches considering corpus co-occurrence statistics and the structure of a published thesaurus. Based on the assumption that the strongly related words of two words in a contrasting pair are also often antonymous, they use affix patterns (e.g. “un-”, “in-” and “im-”) and a thesaurus as seed sets to add contrast links between word categories. Their best performance</context>
</contexts>
<marker>Turney, 2008</marker>
<rawString>Peter D. Turney. 2008. A uniform approach to analogies, synonyms, antonyms, and associations. Coling, pages 905–912, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Xiong</author>
<author>Xi Chen</author>
<author>Tzu-Kuo Huang</author>
<author>Jeff G Schneider</author>
<author>Jaime G Carbonell</author>
</authors>
<title>Temporal collaborative filtering with bayesian probabilistic tensor factorization.</title>
<date>2010</date>
<booktitle>In SDM,</booktitle>
<volume>10</volume>
<pages>211--222</pages>
<publisher>SIAM.</publisher>
<contexts>
<context position="8921" citStr="Xiong et al. (2010)" startWordPosition="1370" endWordPosition="1373">. Also, while non-completeness is a pervasive issue in manually created lexicons, MRLSA is not flexible enough to treat the unknown entries as missing. Instead it just sets them 1As defined by Turney et al. (2010), VSM must be derived from event frequencies. 1523 to zero at the beginning and uses the pivot slice to re-calculate them. In contrast, our method of BPTF is well suited to symmetric relations with many unknown relatedness entries. 2.2 BPTF Model Salakhutdinov and Mnih (2008) introduced a Bayesian Probabilistic Matrix Factorization (BPMF) model as a collaborative filtering algorithm. Xiong et al. (2010) proposed a Bayesian Probabilistic Tensor Factorization (BPTF) model which further extended the original model to incorporate temporal factors. They modeled latent feature vector for users and items, both can be trained efficiently using Markov chain Monte Carlo methods, and they obtained competitive results when applying their models on real-world recommendation data sets. 2.3 Antonomy Detection There are a number of previous works in detecting antonymy. Lin and Zhao (2003) identifies antonyms by looking for pre-identified phrases in corpus datasets. Turney (2008) proposed a supervised classi</context>
<context position="11616" citStr="Xiong et al., 2010" startWordPosition="1774" endWordPosition="1777">lling back to morphology and distributional similarity when the lexicon lacks coverage. In contrast, we focus on fusing the information from thesauruses and automatically induced word relatedness measures during the word vector space creation. Then prediction is done in a single stage, from the latent vectors capturing all word relatedness perspectives and the appropriate per-perspective transformation vector. 3 Methods 3.1 The Bayesian Probabilistic Tensor Factorization Model Our model is a variation of the BPMF model (Salakhutdinov and Mnih, 2008), and is similar to the temporal BPTF model (Xiong et al., 2010). To model word relatedness from multiple perspectives, we denote the relatedness between word i and word j from perspective k as Rk ij. Then we can organize these similarities to form a three-way tensor R E RN×N×lf. Table 1 shows an example, the first slice of the tensor is a N x N matrix consists of 1/-1 corresponding to the synonym/antonym entries in the Roget’s thesaurus, and the second slice is a N xN matrix consists of the cosine similarity from neural word embeddings created by Luong et al. (2013), where N is the number of words in the vocabulary. Note that in our model the entries miss</context>
<context position="13750" citStr="Xiong et al., 2010" startWordPosition="2179" endWordPosition="2182">31 sad .65 .18 .56 -.01 depressed .13 .23 .31 -.01 (b) The second slice: distributional similarity Table 1: Word Relatedness Tensor and α is the precision, the reciprocal of the variance. Vi and Vj are the latent vectors of word i and word j, and Pk is the latent vector for perspective k. We follow a Bayesian approach, adding Gaussian priors to the variables: Vi ∼ N(µV , A−1 V ), Pi ∼ N(µP, A−1 P ), where µV and µP are D dimensional vectors and AV and AP are D-by-D precision matrices. Furthermore, we model the prior distribution of hyper-parameters as conjugate priors (following the model by (Xiong et al., 2010)): p(α) = W(α |ˆW0, v0), Figure 1: The graphical model for BPTF. W0, ν0 µ0 W0, ν0 ΛV µV µ0 · · · V% · · · V, · · · ΛP µP k = 1, ..., K I%k,.7 = 1 Rk%3- Pk i, j = 1, ..., N α i =6 j p(µV , AV ) = N(µV |µ0, (Q0AV )−1)W(AV |W0, v0), p(µP, AP) = N(µP|µ0, (Q0AP)−1)W(AP |W0, v0), where W(W0, v0) is the Wishart distribution of degree of freedom v and a D-by-D scale matrix W, and ˆW0 is a 1-by-1 scale matrix for α. The graphical model is shown in Figure 1 (with Q0 set to 1). After choosing the hyper-priors, the only remaining parameter to tune is the dimension of the latent vectors. Due to the existen</context>
<context position="18520" citStr="Xiong et al. (2010)" startWordPosition="3100" endWordPosition="3103">y for the handlabeled perspectives that the generalization is most where m indicate parameters sampled from different sampling iterations. 3.4 Scalability The time complexity of training our model is roughly O(n x D2), where n is the number of observed entries in the tensor. If one is only interested in creating and re-creating word relatedness of one single slice rather than synthesizing word vectors, then entries in other slices can be downsampled at every iteration to reduce the training time. In our model, the vector length D is not sensitive and does not necessarily need to be very long. Xiong et al. (2010) reported in their collaborative filtering experiment D = 10 usually gives satisfactory performance. 1526 4 Experimental Evaluation In this section, we evaluate our model by answering antonym questions. This task is especially suitable for evaluating our model since the performance of straight-forward look-up from the thesauruses we considered is poor. There are two major limitations: 1. The thesaurus usually only contains antonym information for word pairs with a strong contrast. 2. The vocabulary of the antonym entries in the thesaurus is limited, and does not contain many words in the anton</context>
</contexts>
<marker>Xiong, Chen, Huang, Schneider, Carbonell, 2010</marker>
<rawString>Liang Xiong, Xi Chen, Tzu-Kuo Huang, Jeff G. Schneider, and Jaime G. Carbonell. 2010. Temporal collaborative filtering with bayesian probabilistic tensor factorization. In SDM, volume 10, pages 211–222. SIAM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wen-tau Yih</author>
<author>Geoffrey Zweig</author>
<author>John C Platt</author>
</authors>
<title>Polarity inducing latent semantic analysis.</title>
<date>2012</date>
<booktitle>In EMNLP-CoNLL,</booktitle>
<pages>1212--1222</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4989" citStr="Yih et al., 2012" startWordPosition="749" endWordPosition="752">h word relatedness calculated by distributional models, and manually created lexicons, since the former have much more vocabulary coverage and many variations, while the latter covers word relatedness that is hard to detect by distributional models. We can use information from distributional perspectives to create (if does not exist) or re-create (with regularization) word relatedness from the lexicon’s perspective. We evaluate our model on distinguishing synonyms and antonyms. There are a number of related works (Lin and Zhao, 2003; Turney, 2008; Mohammad et al., 2008; Mohammad et al., 2013; Yih et al., 2012; Chang et al., 2013). A number of sophisticated methods have been applied, producing competitive results using diverse approaches. We use the GRE antonym questions (Mohammad et al., 2008) as a benchmark, and answer these questions by finding the most contrasting choice according to the created or recreated synonym / antonym word relatedness. The result achieves state-of-the-art performance. The rest of this paper is organized as follows. Section 2 describes the related work of word vector representations, the BPTF model and antonymy detection. Section 3 presents our BPTF model and the samplin</context>
<context position="6995" citStr="Yih et al. (2012)" startWordPosition="1055" endWordPosition="1058">rchical models (Mnih and Hinton, 2008), recurrent neural networks (Mikolov, 2012), and recursive neural networks (Socher et al., 2011). Mikolov et al. (2013) reported their vector-space word representation is able to reveal linguistic regularities and composite semantics using simple vector addition and subtraction. For example, “King−Man+Woman” results in a vector very close to “Queen”. Luong et al. (2013) proposed a recursive neural networks model incorporating morphological structure, and has better performance for rare words. Some non-VSM models1 also generate word vector representations. Yih et al. (2012) apply polarity inducing latent semantic analysis (PILSA) to a thesaurus to derive the embedding of words. They treat each entry of a thesaurus as a document giving synonyms positive term counts, and antonyms negative term counts, and preform LSA on the signed TF-IDF matrix In this way, synonyms will have cosine similarities close to one and antonyms close to minus one. Chang et al. (2013) further introduced MultiRelational LSA (MRLSA), as as extension of LSA, that performs Tucker decomposition over a three-way tensor consisting of multiple relations (document-term like matrix) between words a</context>
<context position="10456" citStr="Yih et al. (2012)" startWordPosition="1601" endWordPosition="1604">mption that the strongly related words of two words in a contrasting pair are also often antonymous, they use affix patterns (e.g. “un-”, “in-” and “im-”) and a thesaurus as seed sets to add contrast links between word categories. Their best performance is achieved by further manually annotating contrasting adjacent categories. This approach relies on the Contrast Hypothesis, which will increase false positives even with a carefully designed methodology. Furthermore, while this approach can expand contrast relationships in a lexicon, out-of-vocabulary words still pose a substancial challenge. Yih et al. (2012) and Chang et al. (2013) also applied their vectors on antonymy detection, and Yih et al. achieves the state-of-the-art performance in answering GRE antonym questions. In addition to the word vectors generated from PILSA, they use morphology and k-nearest neighbors from distributional word vector spaces to derive the embeddings for out-of-vocabulary words. The latter is problematic since both synonyms and antonyms are distributionally similar. Their approach is two stage: polarity inducing LSA from a manually created thesaurus, then falling back to morphology and distributional similarity when</context>
<context position="21099" citStr="Yih et al., 2012" startWordPosition="3526" endWordPosition="3529"> second slice (k = 2) consists of cosine similarities from neural word embeddings (example in Table 1) 4.2.1 Thesaurus Two popular thesauruses used in other research are the Macquarie Thesaurus and the Encarta Thesaurus. Unfortunately, their electronic versions are not publicly available. In this work we use two alternatives: WordNet Words in WordNet (version 3.0) are grouped into sense-disambiguated synonym sets (synsets), and synsets have links between each other to express conceptual relations. Previous works reported very different look-up performance using WordNet (Mohammad et al., 2008; Yih et al., 2012), we consider this difference as different understanding of the WordNet structure. By extending “indirect antonyms” defined in WordNet to nouns, verbs and adverbs that similar words share the antonyms,we achieve a look-up performance close to Yih et al. (2012). Using this interpretation of WordNet synonym and antonym structure we obtain a thesaurus containing 54,239 single-token words. Antonym entries are present for 21,319 of them with 16.5 words per entry on average, and 52,750 of them have synonym entries with 11.7 words per entry on average. Roget’s Only considering single-token words, the</context>
<context position="23863" citStr="Yih et al., 2012" startWordPosition="3992" endWordPosition="3995">E MRLSA 0.88 0.85 0.87 0.81 0.77 0.79 WordNet lookup* 0.93 0.32 0.48 0.95 0.33 0.49 WordNet lookup 0.48 0.44 0.46 0.46 0.43 0.44 WordNet BPTF 0.63 0.63 0.63 0.63 0.62 0.62 Roget lookup* 1.00 0.35 0.52 0.99 0.31 0.47 Roget lookup 0.61 0.44 0.51 0.55 0.39 0.45 Roget BPTF 0.80 0.80 0.80 0.76 0.75 0.76 W&amp;R lookup* 1.00 0.48 0.64 0.98 0.45 0.62 W&amp;R lookup 0.62 0.54 0.58 0.59 0.51 0.55 W&amp;R BPMF 0.59 0.59 0.59 0.52 0.52 0.52 W&amp;R BPTF 0.88 0.88 0.88 0.82 0.82 0.82 Table 2: Development and test results on the GRE antonym questions. *Note: to allow comparison, in look-up we follow the approach used by (Yih et al., 2012): randomly guess an answer if the target word is in the vocabulary while none of the choices are. Asterisk indicates the look-up results without random guessing. GRE antonym question words. Further using morphology information from WordNet, the coverage achieves 99.9%. cording to the development set. We choose the vector length D = 40, the burn-in period starting from the 30th iterations, then averaging the relatedness over 200 runs. The hyper-priors used are 4.3 Tests p0 = 0, v0 = ˆv0 = D, Q0 = 1 and W0 = ˆW0 = I To answer the GRE questions, we calculate R1 for word pair (i, j), where i is th</context>
<context position="25356" citStr="Yih et al. (2012)" startWordPosition="4255" endWordPosition="4258">irst train on a tensor from a subset consisting of words with antonym entries, then add all other words using the out-of-vocabulary method described in Section 3. During each iteration, zeros are randomly added into the first slice to keep the model from overfitting. In the meantime, the second slice entries is randomly downsampled to match the number of non-empty entries in the first slice. This ensures each perspective has approximately equal influence on the latent word vectors. We sample the parameters iteratively, and choose the burn-in period and vector length D ac(not tuned). Note that Yih et al. (2012) use a vector length of 300, which means our embeddings save considerable storage space and running time. Our model usually takes less than 30 minutes to meet the convergence criteria (on a machine with an Intel Xeon E3-1230V2 @ 3.3GHz CPU ). In contrast, the MRLSA requires about 3 hours for tensor decomposition (Chang et al., 2013). 4.4 Results The results are summarized in Table 2. We list the results of previous works (Yih et al., 2012; Chang et al., 2013) at the top of the table, where the best performance is achieved by PILSA on Encarta with further discriminative training and embedding. </context>
</contexts>
<marker>Yih, Zweig, Platt, 2012</marker>
<rawString>Wen-tau Yih, Geoffrey Zweig, and John C. Platt. 2012. Polarity inducing latent semantic analysis. In EMNLP-CoNLL, pages 1212–1222. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
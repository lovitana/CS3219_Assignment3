<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.024294">
<title confidence="0.9985055">
Improve Statistical Machine Translation with Context-Sensitive
Bilingual Semantic Embedding Model
</title>
<author confidence="0.9975075">
Haiyang Wu1 Daxiang Dong1 Wei He1 Xiaoguang Hu1 Dianhai Yu1
Hua Wu1 Haifeng Wang1 Ting Liu2
</author>
<affiliation confidence="0.876511">
1 Baidu Inc., No. 10, Shangdi 10th Street, Beijing, 100085, China
2 Harbin Institute of Technology, Harbin, China
wuhaiyang,dongdaxiang,hewei,huxiaoguang,yudianhai,
</affiliation>
<email confidence="0.86923">
wu hua,wanghaifeng@baidu.com
tliu@ir.hit.edu.cn
</email>
<sectionHeader confidence="0.997279" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998873">
We investigate how to improve bilingual
embedding which has been successfully
used as a feature in phrase-based sta-
tistical machine translation (SMT). De-
spite bilingual embedding’s success, the
contextual information, which is of criti-
cal importance to translation quality, was
ignored in previous work. To employ
the contextual information, we propose
a simple and memory-efficient model for
learning bilingual embedding, taking both
the source phrase and context around the
phrase into account. Bilingual translation
scores generated from our proposed bilin-
gual embedding model are used as features
in our SMT system. Experimental results
show that the proposed method achieves
significant improvements on large-scale
Chinese-English translation task.
</bodyText>
<sectionHeader confidence="0.999515" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999785666666667">
In Statistical Machine Translation (SMT) sys-
tem, it is difficult to determine the translation of
some phrases that have ambiguous meanings.For
example, the phrase “,4, jieguo” can be trans-
lated to either “results”, “eventually” or “fruit”,
depending on the context around it. There are two
reasons for the problem: First, the length of phrase
pairs is restricted due to the limitation of model
size and training data. Another reason is that SMT
systems often fail to use contextual information
in source sentence, therefore, phrase sense disam-
biguation highly depends on the language model
which is trained only on target corpus.
To solve this problem, we present to learn
context-sensitive bilingual semantic embedding.
Our methodology is to train a supervised model
where labels are automatically generated from
phrase-pairs. For each source phrase, the aligned
target phrase is marked as the positive label
whereas other phrases in our phrase table are
treated as negative labels. Different from previ-
ous work in bilingual embedding learning(Zou et
al., 2013; Gao et al., 2014), our framework is a
supervised model that utilizes contextual informa-
tion in source sentence as features and make use
of phrase pairs as weak labels. Bilingual seman-
tic embeddings are trained automatically from our
supervised learning task.
Our learned bilingual semantic embedding
model is used to measure the similarity of phrase
pairs which is treated as a feature in decoding. We
integrate our learned model into a phrase-based
translation system and experimental results indi-
cate that our system significantly outperform the
baseline system. On the NIST08 Chinese-English
translation task, we obtained 0.68 BLEU improve-
ment. We also test our proposed method on much
larger web dataset and obtain 0.49 BLEU im-
provement against the baseline.
</bodyText>
<sectionHeader confidence="0.99989" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999825857142857">
Using vectors to represent word meanings is
the essence of vector space models (VSM). The
representations capture words’ semantic and syn-
tactic information which can be used to measure
semantic similarities by computing distance be-
tween the vectors. Although most VSMs represent
one word with only one vector, they fail to cap-
ture homonymy and polysemy of word. Huang
et al. (2012) introduced global document context
and multiple word prototypes which distinguishes
and uses both local and global context via a joint
training objective. Much of the research focus
on the task of inducing representations for sin-
gle languages. Recently, a lot of progress has
</bodyText>
<page confidence="0.959343">
142
</page>
<bodyText confidence="0.954265826086956">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 142–146,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
been made at representation learning for bilin-
gual words. Bilingual word representations have
been presented by Peirsman and Pad´o (2010) and
Sumita (2000). Also unsupervised algorithms
such as LDA and LSA were used by Boyd-Graber
and Resnik (2010), Tam et al. (2007) and Zhao and
Xing (2006). Zou et al. (2013) learn bilingual em-
beddings utilizes word alignments and monolin-
gual embeddings result, Le et al. (2012) and Gao et
al. (2014) used continuous vector to represent the
source language or target language of each phrase,
and then computed translation probability using
vector distance. Vuli´c and Moens (2013) learned
bilingual vector spaces from non-parallel data in-
duced by using a seed lexicon. However, none
of these work considered the word sense disam-
biguation problem which Carpuat and Wu (2007)
proved it is useful for SMT. In this paper, we learn
bilingual semantic embeddings for source content
and target phrase, and incorporate it into a phrase-
based SMT system to improve translation quality.
</bodyText>
<sectionHeader confidence="0.8677" genericHeader="method">
3 Context-Sensitive Bilingual Semantic
</sectionHeader>
<subsectionHeader confidence="0.803164">
Embedding Model
</subsectionHeader>
<bodyText confidence="0.999995130434783">
We propose a simple and memory-efficient
model which embeds both contextual information
of source phrases and aligned phrases in target cor-
pus into low dimension. Our assumption is that
high frequent words are likely to have multiple
word senses; therefore, top frequent words are se-
lected in source corpus. We denote our selected
words as focused phrase. Our goal is to learn a
bilingual embedding model that can capture dis-
criminative contextual information for each fo-
cused phrase. To learn an effective context sensi-
tive bilingual embedding, we extract context fea-
tures nearby a focused phrase that will discrimi-
nate focused phrase’s target translation from other
possible candidates. Our task can be viewed as
a classification problem that each target phrase is
treated as a class. Since target phrases are usu-
ally in very high dimensional space, traditional
linear classification model is not suitable for our
problem. Therefore, we treat our problem as a
ranking problem that can handle large number of
classes and optimize the objectives with scalable
optimizer stochastic gradient descent.
</bodyText>
<subsectionHeader confidence="0.997668">
3.1 Bilingual Word Embedding
</subsectionHeader>
<bodyText confidence="0.999974428571429">
We apply a linear embedding model for bilin-
gual embedding learning. Cosine similarity be-
tween bilingual embedding representation is con-
sidered as score function. The score function
should be discriminative between target phrases
and other candidate phrases. Our score function
is in the form:
</bodyText>
<equation confidence="0.905323">
f(x, y; W, U) = cos(WT x, UTy) (1)
</equation>
<bodyText confidence="0.999906736842105">
where x is contextual feature vector in source sen-
tence, and y is the representation of target phrase,
W ∈ R|X|xk, U ∈ R|Y|xk are low rank ma-
trix. In our model, we allow y to be bag-of-words
representation. Our embedding model is memory-
efficient in that dimensionality of x and y can be
very large in practical setting. We use |X |and |Y|
means dimensionality of random variable x and y,
then traditional linear model such as max-entropy
model requires memory space of O(|X||Y|). Our
embedding model only requires O(k(|X |+ |Y|))
memory space that can handle large scale vocabu-
lary setting. To score a focused phrase and target
phrase pair with f(x, y), context features are ex-
tracted from nearby window of the focused phrase.
Target words are selected from phrase pairs. Given
a source sentence, embedding of a focused phrase
is estimated from WTx and target phrase embed-
ding can be obtained through UTy.
</bodyText>
<subsectionHeader confidence="0.999795">
3.2 Context Sensitive Features
</subsectionHeader>
<bodyText confidence="0.9999069">
Context of a focused phrase is extracted from
nearby window, and in our experiment we choose
window size of 6 as a focused phrase’s con-
text. Features are then extracted from the focused
phrase’s context. We demonstrate our feature
extraction and label generation process from the
Chinese-to-English example in figure 1. Window
size in this example is three. Position features
and Part-Of-Speech Tagging features are extracted
from the focused phrase’s context. The word fruit
</bodyText>
<figureCaption confidence="0.99634">
Figure 1: Feature extraction and label generation
</figureCaption>
<page confidence="0.993682">
143
</page>
<bodyText confidence="0.9999075">
is the aligned phrase of our focused phrase and is
treated as positive label. The phrase results is a
randomly selected phrase from phrase table results
of 04;F,. Note that feature window is not well de-
fined near the beginning or the end of a sentence.
To conquer this problem, we add special padding
word to the beginning and the end of a sentence to
augment sentence.
</bodyText>
<subsectionHeader confidence="0.998834">
3.3 Parameter Learning
</subsectionHeader>
<bodyText confidence="0.999904875">
To learn model parameter W and U, we ap-
ply a ranking scheme on candidates selected from
phrase table results of each focused phrase. In par-
ticular, given a focus phrase w, aligned phrase is
treated as positive label whereas phrases extracted
from other candidates in phrase table are treated
as negative label. A max-margin loss is applied in
this ranking setting.
</bodyText>
<equation confidence="0.999236">
I(O) = m m (δ − f(xi, yi; O) − f(xi, y0i; O))+
i=1
</equation>
<bodyText confidence="0.988299375">
(2)
Where f(xi, yi) is previously defined, O =
{W, U} and + means max-margin hinge loss. In
our implementation, a margin of δ = 0.15 is used
during training. Objectives are minimized through
stochastic gradient descent algorithm. For each
randomly selected training example, parameters
are updated through the following form:
</bodyText>
<equation confidence="0.975981333333333">
∂l(Θ)
Θ := Θ − α(3)
∂Θ
</equation>
<bodyText confidence="0.922748166666667">
where O = {W, U}. Given an instance with pos-
itive and negative label pair {x, y, y0}, gradients
of parameter W and U are as follows:
= qsx(WT x)T − pqs3x(UTy) (4)
= qsy(UTy)T − pqs3y(WT x) (5)
Where we set p = (WT x)T (UT y), q = 1
</bodyText>
<equation confidence="0.757351333333333">
||WT x||2
and s = 1
||UT y||2.
</equation>
<bodyText confidence="0.999844333333333">
To initialize our model param-
eters with strong semantic and syntactic informa-
tion, word vectors are pre-trained independently
on source and target corpus through word2vec
(Mikolov et al., 2013). And the pre-trained word
vectors are treated as initial parameters of our
model. The learned scoring function f(x, y) will
be used during decoding phase as a feature in log-
linear model which we will describe in detail later.
</bodyText>
<sectionHeader confidence="0.959939" genericHeader="method">
4 Integrating Bilingual Semantic
Embedding into Phrase-Based SMT
Architectures
</sectionHeader>
<bodyText confidence="0.999970714285714">
To incorporate the context-sensitive bilingual
embedding model into the state-of-the-art Phrase-
Based Translation model, we modify the decoding
so that context information is available on every
source phrase. For every phrase in a source sen-
tence, the following tasks are done at every node
in our decoder:
</bodyText>
<listItem confidence="0.997769">
• Get the focused phrase as well as its context in the
source sentence.
• Extract features from the focused phrase’s context.
• Get translation candidate extracted from phrase pairs of
the focused phrase.
• Compute scores for any pair of the focused phrase and
a candidate phrase.
</listItem>
<bodyText confidence="0.998363357142857">
We get the target sub-phrase using word align-
ment of phrase, and we treat NULL as a common
target word if there is no alignment for the focused
phrase. Finally we compute the matching score for
source content and target word using bilingual se-
mantic embedding model. If there are more than
one word in the focus phrase, then we add all score
together. A penalty value will be given if target is
not in translation candidate list. For each phrase in
a given SMT input sentence, the Bilingual Seman-
tic score can be used as an additional feature in
log-linear translation model, in combination with
other typical context-independent SMT bilexicon
probabilities.
</bodyText>
<sectionHeader confidence="0.999148" genericHeader="evaluation">
5 Experiment
</sectionHeader>
<bodyText confidence="0.999944375">
Our experiments are performed using an in-
house phrase-based system with a log-linear
framework. Our system includes a phrase trans-
lation model, an n-gram language model, a lexi-
calized reordering model, a word penalty model
and a phrase penalty model, which is similar to
Moses (Koehn et al., 2007). The evaluation metric
is BLEU (Papineni et al., 2002).
</bodyText>
<subsectionHeader confidence="0.996373">
5.1 Data set
</subsectionHeader>
<bodyText confidence="0.995299">
We test our approach on LDC corpus first. We
just use a subset of the data available for NIST
OpenMT08 task1 . The parallel training corpus
</bodyText>
<equation confidence="0.994568666666667">
1LDC2002E18, LDC2002L27, LDC2002T01,
LDC2003E07, LDC2003E14, LDC2004T07, LDC2005E83,
LDC2005T06, LDC2005T10, LDC2005T34, LDC2006E24,
LDC2006E26, LDC2006E34, LDC2006E86, LDC2006E92,
LDC2006E93, LDC2004T08(HK News, HK Hansards )
∂l(W, U)
∂W
∂l(W, U)
∂U
</equation>
<page confidence="0.995675">
144
</page>
<table confidence="0.9971808">
Method OpenMT08 WebData
BLEU BLEU
Our Baseline 26.24 29.32
LOC 26.78** 29.62*
LOC+POS 26.82** 29.81*
</table>
<tableCaption confidence="0.998616">
Table 1: Results of lowercase BLEU on NIST08
</tableCaption>
<bodyText confidence="0.987224842105263">
task. LOC is the location feature and POS is
the Part-of-Speech feature * or ** equals to sig-
nificantly better than our baseline(p &lt; 0.05 or
p &lt; 0.01, respectively)
contains 1.5M sentence pairs after we filter with
some simple heuristic rules, such as sentence be-
ing too long or containing messy codes. As mono-
lingual corpus, we use the XinHua portion of the
English GigaWord. In monolingual corpus we fil-
ter sentence if it contain more than 100 words
or contain messy codes, Finally, we get mono-
lingual corpus containing 369M words. In order
to test our approach on a more realistic scenario,
we train our models with web data. Sentence
pairs obtained from bilingual website and com-
parable webpage. Monolingual corpus is gained
from some large website such as WiKi. There are
50M sentence pairs and 10B words monolingual
corpus.
</bodyText>
<subsectionHeader confidence="0.982095">
5.2 Results and Analysis
</subsectionHeader>
<bodyText confidence="0.999194272727273">
For word alignment, we align all of the train-
ing data with GIZA++ (Och and Ney, 2003), us-
ing the grow-diag-final heuristic to improve recall.
For language model, we train a 5-gram modified
Kneser-Ney language model and use Minimum
Error Rate Training (Och, 2003) to tune the SMT.
For both OpenMT08 task and WebData task, we
use NIST06 as the tuning set, and use NIST08 as
the testing set. Our baseline system is a standard
phrase-based SMT system, and a language model
is trained with the target side of bilingual corpus.
Results on Chinese-English translation task are re-
ported in Table 1. Word position features and part-
of-speech tagging features are both useful for our
bilingual semantic embedding learning. Based on
our trained bilingual embedding model, we can
easily compute a translation score between any
bilingual phrase pair. We list some cases in table
2 to show that our bilingual embedding is context
sensitive.
Contextual features extracted from source sen-
tence are strong enough to discriminate different
</bodyText>
<table confidence="0.99971916">
Source Sentence 4 Nearest Neighbor from
bilingual embedding
���������� will be, can only, will, can
���������
A ; _(Investors
can only get down to
business in a stable so-
cial environment)
���������� skills, ability, abilities, tal-
��������� ent
���!�_(In compe-
titions, the Chinese Dis-
abled have shown ex-
traordinary athletic abil-
ities)
(åý„ê6¯ƒ7� fruit, outcome of, the out-
11 7 �E�kff-*_fFt come, result
� � �(In the natu-
ral environment of Costa
Rica, grapes do not nor-
mally yield fruit.)
� � � � � �it. — _% _(As in the end, eventually, as a
a result, Eastern District result, results
Council passed a pro-
posal)
</table>
<tableCaption confidence="0.987356">
Table 2: Top ranked focused phrases based on
bilingual semantic embedding
</tableCaption>
<bodyText confidence="0.799121">
word senses. And we also observe from the word
“&apos;--0Q4,&apos; jieguo” that Part-Of-Speech Tagging fea-
tures are effective in discriminating target phrases.
</bodyText>
<sectionHeader confidence="0.999266" genericHeader="conclusions">
6 Conlusion
</sectionHeader>
<bodyText confidence="0.9999815">
In this paper, we proposed a context-sensitive
bilingual semantic embedding model to improve
statistical machine translation. Contextual infor-
mation is used in our model for bilingual word
sense disambiguation. We integrated the bilingual
semantic model into the phrase-based SMT sys-
tem. Experimental results show that our method
achieves significant improvements over the base-
line on large scale Chinese-English translation
task. Our model is memory-efficient and practical
for industrial usage that training can be done on
large scale data set with large number of classes.
Prediction time is also negligible with regard to
SMT decoding phase. In the future, we will ex-
plore more features to refine the model and try to
utilize contextual information in target sentences.
</bodyText>
<sectionHeader confidence="0.998812" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99932425">
We thank the three anonymous reviewers for
their valuable comments, and Niu Gang and Wu
Xianchao for discussions. This paper is supported
by 973 program No. 2014CB340505.
</bodyText>
<page confidence="0.998536">
145
</page>
<sectionHeader confidence="0.996118" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998653388888889">
Jordan Boyd-Graber and Philip Resnik. 2010. Holis-
tic sentiment analysis across languages: Multilin-
gual supervised latent dirichlet allocation. In Pro-
ceedings of the 2010 Conference on Empirical Meth-
ods in Natural Language Processing, pages 45–55,
Cambridge, MA, October. Association for Compu-
tational Linguistics.
Marine Carpuat and Dekai Wu. 2007. Improving sta-
tistical machine translation using word sense disam-
biguation. In Proceedings of the 2007 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning (EMNLP-CoNLL), pages 61–72, Prague,
Czech Republic, June. Association for Computa-
tional Linguistics.
Jianfeng Gao, Xiaodong He, Wen-tau Yih, and
Li Deng. 2014. Learning continuous phrase rep-
resentations for translation modeling. In Proc. ACL.
Eric Huang, Richard Socher, Christopher Manning,
and Andrew Ng. 2012. Improving word represen-
tations via global context and multiple word proto-
types. In Proceedings of the 50th Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers), pages 873–882, Jeju Island,
Korea, July. Association for Computational Linguis-
tics.
Hai-Son Le, Alexandre Allauzen, and Franc¸ois Yvon.
2012. Continuous space translation models with
neural networks. In Proceedings of the 2012 Con-
ference of the North American Chapter of the As-
sociation for Computational Linguistics: Human
Language Technologies, pages 39–48, Montr´eal,
Canada, June. Association for Computational Lin-
guistics.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S.
Corrado, and Jeffrey Dean. 2013. Distributed rep-
resentations of words and phrases and their compo-
sitionality. In NIPS, pages 3111–3119.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. In Computational Linguistics, Volume 29,
Number 1, March 2003. Computational Linguistics,
March.
Franz Josef Och. 2003. Minimum error rate train-
ing in statistical machine translation. In Proceed-
ings of the 41st Annual Meeting of the Association
for Computational Linguistics, pages 160–167, Sap-
poro, Japan, July. Association for Computational
Linguistics.
Yves Peirsman and Sebastian Pad´o. 2010. Cross-
lingual induction of selectional preferences with
bilingual vector spaces. In Human Language Tech-
nologies: The 2010 Annual Conference of the North
American Chapter of the Association for Compu-
tational Linguistics, pages 921–929, Los Ange-
les, California, June. Association for Computational
Linguistics.
Eiichiro Sumita. 2000. Lexical transfer using a vector-
space model. In Proceedings of the 38th Annual
Meeting of the Association for Computational Lin-
guistics. Association for Computational Linguistics,
August.
Yik-Cheung Tam, Ian Lane, and Tanja Schultz. 2007.
Bilingual-lsa based lm adaptation for spoken lan-
guage translation. In Proceedings of the 45th An-
nual Meeting of the Association of Computational
Linguistics, pages 520–527, Prague, Czech Repub-
lic, June. Association for Computational Linguis-
tics.
Ivan Vuli´c and Marie-Francine Moens. 2013. Cross-
lingual semantic similarity of words as the similarity
of their semantic word responses. In Proceedings of
the 2013 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, pages 106–116, At-
lanta, Georgia, June. Association for Computational
Linguistics.
Bing Zhao and Eric P. Xing. 2006. Bitam: Bilingual
topic admixture models for word alignment. In Pro-
ceedings of the COLING/ACL 2006 Main Confer-
ence Poster Sessions, pages 969–976, Sydney, Aus-
tralia, July. Association for Computational Linguis-
tics.
Will Y. Zou, Richard Socher, Daniel Cer, and Christo-
pher D. Manning. 2013. Bilingual word embed-
dings for phrase-based machine translation. In Pro-
ceedings of the 2013 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1393–
1398, Seattle, Washington, USA, October. Associa-
tion for Computational Linguistics.
</reference>
<page confidence="0.998821">
146
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.208679">
<title confidence="0.845010333333333">Improve Statistical Machine Translation with Bilingual Semantic Embedding Model Daxiang Wei Xiaoguang Dianhai</title>
<author confidence="0.592208">Haifeng Ting</author>
<affiliation confidence="0.7481375">1Baidu Inc., No. 10, Shangdi 10th Street, Beijing, 100085, China 2Harbin Institute of Technology, Harbin,</affiliation>
<email confidence="0.81205">wutliu@ir.hit.edu.cn</email>
<abstract confidence="0.9989119">investigate how to improve has been successfully as a feature in phrase-based stamachine translation Despite bilingual embedding’s success, the which is of critical importance to translation quality, was ignored in previous work. To employ the contextual information, we propose a simple and memory-efficient model for learning bilingual embedding, taking both the source phrase and context around the phrase into account. Bilingual translation scores generated from our proposed bilingual embedding model are used as features in our SMT system. Experimental results show that the proposed method achieves significant improvements on large-scale Chinese-English translation task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jordan Boyd-Graber</author>
<author>Philip Resnik</author>
</authors>
<title>Holistic sentiment analysis across languages: Multilingual supervised latent dirichlet allocation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>45--55</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Cambridge, MA,</location>
<contexts>
<context position="4126" citStr="Boyd-Graber and Resnik (2010)" startWordPosition="609" endWordPosition="612">both local and global context via a joint training objective. Much of the research focus on the task of inducing representations for single languages. Recently, a lot of progress has 142 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 142–146, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics been made at representation learning for bilingual words. Bilingual word representations have been presented by Peirsman and Pad´o (2010) and Sumita (2000). Also unsupervised algorithms such as LDA and LSA were used by Boyd-Graber and Resnik (2010), Tam et al. (2007) and Zhao and Xing (2006). Zou et al. (2013) learn bilingual embeddings utilizes word alignments and monolingual embeddings result, Le et al. (2012) and Gao et al. (2014) used continuous vector to represent the source language or target language of each phrase, and then computed translation probability using vector distance. Vuli´c and Moens (2013) learned bilingual vector spaces from non-parallel data induced by using a seed lexicon. However, none of these work considered the word sense disambiguation problem which Carpuat and Wu (2007) proved it is useful for SMT. In this </context>
</contexts>
<marker>Boyd-Graber, Resnik, 2010</marker>
<rawString>Jordan Boyd-Graber and Philip Resnik. 2010. Holistic sentiment analysis across languages: Multilingual supervised latent dirichlet allocation. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 45–55, Cambridge, MA, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marine Carpuat</author>
<author>Dekai Wu</author>
</authors>
<title>Improving statistical machine translation using word sense disambiguation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</booktitle>
<pages>61--72</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="4688" citStr="Carpuat and Wu (2007)" startWordPosition="700" endWordPosition="703">h as LDA and LSA were used by Boyd-Graber and Resnik (2010), Tam et al. (2007) and Zhao and Xing (2006). Zou et al. (2013) learn bilingual embeddings utilizes word alignments and monolingual embeddings result, Le et al. (2012) and Gao et al. (2014) used continuous vector to represent the source language or target language of each phrase, and then computed translation probability using vector distance. Vuli´c and Moens (2013) learned bilingual vector spaces from non-parallel data induced by using a seed lexicon. However, none of these work considered the word sense disambiguation problem which Carpuat and Wu (2007) proved it is useful for SMT. In this paper, we learn bilingual semantic embeddings for source content and target phrase, and incorporate it into a phrasebased SMT system to improve translation quality. 3 Context-Sensitive Bilingual Semantic Embedding Model We propose a simple and memory-efficient model which embeds both contextual information of source phrases and aligned phrases in target corpus into low dimension. Our assumption is that high frequent words are likely to have multiple word senses; therefore, top frequent words are selected in source corpus. We denote our selected words as fo</context>
</contexts>
<marker>Carpuat, Wu, 2007</marker>
<rawString>Marine Carpuat and Dekai Wu. 2007. Improving statistical machine translation using word sense disambiguation. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 61–72, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianfeng Gao</author>
<author>Xiaodong He</author>
<author>Wen-tau Yih</author>
<author>Li Deng</author>
</authors>
<title>Learning continuous phrase representations for translation modeling. In</title>
<date>2014</date>
<booktitle>Proc. ACL.</booktitle>
<contexts>
<context position="2261" citStr="Gao et al., 2014" startWordPosition="322" endWordPosition="325"> to use contextual information in source sentence, therefore, phrase sense disambiguation highly depends on the language model which is trained only on target corpus. To solve this problem, we present to learn context-sensitive bilingual semantic embedding. Our methodology is to train a supervised model where labels are automatically generated from phrase-pairs. For each source phrase, the aligned target phrase is marked as the positive label whereas other phrases in our phrase table are treated as negative labels. Different from previous work in bilingual embedding learning(Zou et al., 2013; Gao et al., 2014), our framework is a supervised model that utilizes contextual information in source sentence as features and make use of phrase pairs as weak labels. Bilingual semantic embeddings are trained automatically from our supervised learning task. Our learned bilingual semantic embedding model is used to measure the similarity of phrase pairs which is treated as a feature in decoding. We integrate our learned model into a phrase-based translation system and experimental results indicate that our system significantly outperform the baseline system. On the NIST08 Chinese-English translation task, we o</context>
<context position="4315" citStr="Gao et al. (2014)" startWordPosition="643" endWordPosition="646"> the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 142–146, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics been made at representation learning for bilingual words. Bilingual word representations have been presented by Peirsman and Pad´o (2010) and Sumita (2000). Also unsupervised algorithms such as LDA and LSA were used by Boyd-Graber and Resnik (2010), Tam et al. (2007) and Zhao and Xing (2006). Zou et al. (2013) learn bilingual embeddings utilizes word alignments and monolingual embeddings result, Le et al. (2012) and Gao et al. (2014) used continuous vector to represent the source language or target language of each phrase, and then computed translation probability using vector distance. Vuli´c and Moens (2013) learned bilingual vector spaces from non-parallel data induced by using a seed lexicon. However, none of these work considered the word sense disambiguation problem which Carpuat and Wu (2007) proved it is useful for SMT. In this paper, we learn bilingual semantic embeddings for source content and target phrase, and incorporate it into a phrasebased SMT system to improve translation quality. 3 Context-Sensitive Bili</context>
</contexts>
<marker>Gao, He, Yih, Deng, 2014</marker>
<rawString>Jianfeng Gao, Xiaodong He, Wen-tau Yih, and Li Deng. 2014. Learning continuous phrase representations for translation modeling. In Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Huang</author>
<author>Richard Socher</author>
<author>Christopher Manning</author>
<author>Andrew Ng</author>
</authors>
<title>Improving word representations via global context and multiple word prototypes.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>873--882</pages>
<institution>Jeju Island, Korea, July. Association for Computational Linguistics.</institution>
<contexts>
<context position="3403" citStr="Huang et al. (2012)" startWordPosition="502" endWordPosition="505">orm the baseline system. On the NIST08 Chinese-English translation task, we obtained 0.68 BLEU improvement. We also test our proposed method on much larger web dataset and obtain 0.49 BLEU improvement against the baseline. 2 Related Work Using vectors to represent word meanings is the essence of vector space models (VSM). The representations capture words’ semantic and syntactic information which can be used to measure semantic similarities by computing distance between the vectors. Although most VSMs represent one word with only one vector, they fail to capture homonymy and polysemy of word. Huang et al. (2012) introduced global document context and multiple word prototypes which distinguishes and uses both local and global context via a joint training objective. Much of the research focus on the task of inducing representations for single languages. Recently, a lot of progress has 142 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 142–146, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics been made at representation learning for bilingual words. Bilingual word representations have been presented by Peirsman and </context>
</contexts>
<marker>Huang, Socher, Manning, Ng, 2012</marker>
<rawString>Eric Huang, Richard Socher, Christopher Manning, and Andrew Ng. 2012. Improving word representations via global context and multiple word prototypes. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 873–882, Jeju Island, Korea, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai-Son Le</author>
<author>Alexandre Allauzen</author>
<author>Franc¸ois Yvon</author>
</authors>
<title>Continuous space translation models with neural networks.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>39--48</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montr´eal, Canada,</location>
<contexts>
<context position="4293" citStr="Le et al. (2012)" startWordPosition="638" endWordPosition="641">as 142 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 142–146, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics been made at representation learning for bilingual words. Bilingual word representations have been presented by Peirsman and Pad´o (2010) and Sumita (2000). Also unsupervised algorithms such as LDA and LSA were used by Boyd-Graber and Resnik (2010), Tam et al. (2007) and Zhao and Xing (2006). Zou et al. (2013) learn bilingual embeddings utilizes word alignments and monolingual embeddings result, Le et al. (2012) and Gao et al. (2014) used continuous vector to represent the source language or target language of each phrase, and then computed translation probability using vector distance. Vuli´c and Moens (2013) learned bilingual vector spaces from non-parallel data induced by using a seed lexicon. However, none of these work considered the word sense disambiguation problem which Carpuat and Wu (2007) proved it is useful for SMT. In this paper, we learn bilingual semantic embeddings for source content and target phrase, and incorporate it into a phrasebased SMT system to improve translation quality. 3 </context>
</contexts>
<marker>Le, Allauzen, Yvon, 2012</marker>
<rawString>Hai-Son Le, Alexandre Allauzen, and Franc¸ois Yvon. 2012. Continuous space translation models with neural networks. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 39–48, Montr´eal, Canada, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Gregory S Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality.</title>
<date>2013</date>
<booktitle>In NIPS,</booktitle>
<pages>3111--3119</pages>
<contexts>
<context position="9483" citStr="Mikolov et al., 2013" startWordPosition="1504" endWordPosition="1507">hrough stochastic gradient descent algorithm. For each randomly selected training example, parameters are updated through the following form: ∂l(Θ) Θ := Θ − α(3) ∂Θ where O = {W, U}. Given an instance with positive and negative label pair {x, y, y0}, gradients of parameter W and U are as follows: = qsx(WT x)T − pqs3x(UTy) (4) = qsy(UTy)T − pqs3y(WT x) (5) Where we set p = (WT x)T (UT y), q = 1 ||WT x||2 and s = 1 ||UT y||2. To initialize our model parameters with strong semantic and syntactic information, word vectors are pre-trained independently on source and target corpus through word2vec (Mikolov et al., 2013). And the pre-trained word vectors are treated as initial parameters of our model. The learned scoring function f(x, y) will be used during decoding phase as a feature in loglinear model which we will describe in detail later. 4 Integrating Bilingual Semantic Embedding into Phrase-Based SMT Architectures To incorporate the context-sensitive bilingual embedding model into the state-of-the-art PhraseBased Translation model, we modify the decoding so that context information is available on every source phrase. For every phrase in a source sentence, the following tasks are done at every node in o</context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S. Corrado, and Jeffrey Dean. 2013. Distributed representations of words and phrases and their compositionality. In NIPS, pages 3111–3119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<booktitle>In Computational Linguistics, Volume 29, Number</booktitle>
<volume>1</volume>
<contexts>
<context position="12896" citStr="Och and Ney, 2003" startWordPosition="2062" endWordPosition="2065">us, we use the XinHua portion of the English GigaWord. In monolingual corpus we filter sentence if it contain more than 100 words or contain messy codes, Finally, we get monolingual corpus containing 369M words. In order to test our approach on a more realistic scenario, we train our models with web data. Sentence pairs obtained from bilingual website and comparable webpage. Monolingual corpus is gained from some large website such as WiKi. There are 50M sentence pairs and 10B words monolingual corpus. 5.2 Results and Analysis For word alignment, we align all of the training data with GIZA++ (Och and Ney, 2003), using the grow-diag-final heuristic to improve recall. For language model, we train a 5-gram modified Kneser-Ney language model and use Minimum Error Rate Training (Och, 2003) to tune the SMT. For both OpenMT08 task and WebData task, we use NIST06 as the tuning set, and use NIST08 as the testing set. Our baseline system is a standard phrase-based SMT system, and a language model is trained with the target side of bilingual corpus. Results on Chinese-English translation task are reported in Table 1. Word position features and partof-speech tagging features are both useful for our bilingual se</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. In Computational Linguistics, Volume 29, Number 1, March 2003. Computational Linguistics, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>160--167</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sapporo, Japan,</location>
<contexts>
<context position="13073" citStr="Och, 2003" startWordPosition="2091" endWordPosition="2092">pus containing 369M words. In order to test our approach on a more realistic scenario, we train our models with web data. Sentence pairs obtained from bilingual website and comparable webpage. Monolingual corpus is gained from some large website such as WiKi. There are 50M sentence pairs and 10B words monolingual corpus. 5.2 Results and Analysis For word alignment, we align all of the training data with GIZA++ (Och and Ney, 2003), using the grow-diag-final heuristic to improve recall. For language model, we train a 5-gram modified Kneser-Ney language model and use Minimum Error Rate Training (Och, 2003) to tune the SMT. For both OpenMT08 task and WebData task, we use NIST06 as the tuning set, and use NIST08 as the testing set. Our baseline system is a standard phrase-based SMT system, and a language model is trained with the target side of bilingual corpus. Results on Chinese-English translation task are reported in Table 1. Word position features and partof-speech tagging features are both useful for our bilingual semantic embedding learning. Based on our trained bilingual embedding model, we can easily compute a translation score between any bilingual phrase pair. We list some cases in tab</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 160–167, Sapporo, Japan, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yves Peirsman</author>
<author>Sebastian Pad´o</author>
</authors>
<title>Crosslingual induction of selectional preferences with bilingual vector spaces.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>921--929</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Los Angeles, California,</location>
<marker>Peirsman, Pad´o, 2010</marker>
<rawString>Yves Peirsman and Sebastian Pad´o. 2010. Crosslingual induction of selectional preferences with bilingual vector spaces. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 921–929, Los Angeles, California, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eiichiro Sumita</author>
</authors>
<title>Lexical transfer using a vectorspace model.</title>
<date>2000</date>
<booktitle>In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics,</booktitle>
<contexts>
<context position="4033" citStr="Sumita (2000)" startWordPosition="596" endWordPosition="597">l document context and multiple word prototypes which distinguishes and uses both local and global context via a joint training objective. Much of the research focus on the task of inducing representations for single languages. Recently, a lot of progress has 142 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 142–146, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics been made at representation learning for bilingual words. Bilingual word representations have been presented by Peirsman and Pad´o (2010) and Sumita (2000). Also unsupervised algorithms such as LDA and LSA were used by Boyd-Graber and Resnik (2010), Tam et al. (2007) and Zhao and Xing (2006). Zou et al. (2013) learn bilingual embeddings utilizes word alignments and monolingual embeddings result, Le et al. (2012) and Gao et al. (2014) used continuous vector to represent the source language or target language of each phrase, and then computed translation probability using vector distance. Vuli´c and Moens (2013) learned bilingual vector spaces from non-parallel data induced by using a seed lexicon. However, none of these work considered the word s</context>
</contexts>
<marker>Sumita, 2000</marker>
<rawString>Eiichiro Sumita. 2000. Lexical transfer using a vectorspace model. In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yik-Cheung Tam</author>
<author>Ian Lane</author>
<author>Tanja Schultz</author>
</authors>
<title>Bilingual-lsa based lm adaptation for spoken language translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>520--527</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="4145" citStr="Tam et al. (2007)" startWordPosition="613" endWordPosition="616">ia a joint training objective. Much of the research focus on the task of inducing representations for single languages. Recently, a lot of progress has 142 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 142–146, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics been made at representation learning for bilingual words. Bilingual word representations have been presented by Peirsman and Pad´o (2010) and Sumita (2000). Also unsupervised algorithms such as LDA and LSA were used by Boyd-Graber and Resnik (2010), Tam et al. (2007) and Zhao and Xing (2006). Zou et al. (2013) learn bilingual embeddings utilizes word alignments and monolingual embeddings result, Le et al. (2012) and Gao et al. (2014) used continuous vector to represent the source language or target language of each phrase, and then computed translation probability using vector distance. Vuli´c and Moens (2013) learned bilingual vector spaces from non-parallel data induced by using a seed lexicon. However, none of these work considered the word sense disambiguation problem which Carpuat and Wu (2007) proved it is useful for SMT. In this paper, we learn bil</context>
</contexts>
<marker>Tam, Lane, Schultz, 2007</marker>
<rawString>Yik-Cheung Tam, Ian Lane, and Tanja Schultz. 2007. Bilingual-lsa based lm adaptation for spoken language translation. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 520–527, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan Vuli´c</author>
<author>Marie-Francine Moens</author>
</authors>
<title>Crosslingual semantic similarity of words as the similarity of their semantic word responses.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>106--116</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Atlanta, Georgia,</location>
<marker>Vuli´c, Moens, 2013</marker>
<rawString>Ivan Vuli´c and Marie-Francine Moens. 2013. Crosslingual semantic similarity of words as the similarity of their semantic word responses. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 106–116, Atlanta, Georgia, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing Zhao</author>
<author>Eric P Xing</author>
</authors>
<title>Bitam: Bilingual topic admixture models for word alignment.</title>
<date>2006</date>
<booktitle>In Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions,</booktitle>
<pages>969--976</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sydney, Australia,</location>
<contexts>
<context position="4170" citStr="Zhao and Xing (2006)" startWordPosition="618" endWordPosition="621">jective. Much of the research focus on the task of inducing representations for single languages. Recently, a lot of progress has 142 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 142–146, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics been made at representation learning for bilingual words. Bilingual word representations have been presented by Peirsman and Pad´o (2010) and Sumita (2000). Also unsupervised algorithms such as LDA and LSA were used by Boyd-Graber and Resnik (2010), Tam et al. (2007) and Zhao and Xing (2006). Zou et al. (2013) learn bilingual embeddings utilizes word alignments and monolingual embeddings result, Le et al. (2012) and Gao et al. (2014) used continuous vector to represent the source language or target language of each phrase, and then computed translation probability using vector distance. Vuli´c and Moens (2013) learned bilingual vector spaces from non-parallel data induced by using a seed lexicon. However, none of these work considered the word sense disambiguation problem which Carpuat and Wu (2007) proved it is useful for SMT. In this paper, we learn bilingual semantic embedding</context>
</contexts>
<marker>Zhao, Xing, 2006</marker>
<rawString>Bing Zhao and Eric P. Xing. 2006. Bitam: Bilingual topic admixture models for word alignment. In Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 969–976, Sydney, Australia, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Will Y Zou</author>
<author>Richard Socher</author>
<author>Daniel Cer</author>
<author>Christopher D Manning</author>
</authors>
<title>Bilingual word embeddings for phrase-based machine translation.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1393--1398</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Seattle, Washington, USA,</location>
<contexts>
<context position="2242" citStr="Zou et al., 2013" startWordPosition="318" endWordPosition="321">systems often fail to use contextual information in source sentence, therefore, phrase sense disambiguation highly depends on the language model which is trained only on target corpus. To solve this problem, we present to learn context-sensitive bilingual semantic embedding. Our methodology is to train a supervised model where labels are automatically generated from phrase-pairs. For each source phrase, the aligned target phrase is marked as the positive label whereas other phrases in our phrase table are treated as negative labels. Different from previous work in bilingual embedding learning(Zou et al., 2013; Gao et al., 2014), our framework is a supervised model that utilizes contextual information in source sentence as features and make use of phrase pairs as weak labels. Bilingual semantic embeddings are trained automatically from our supervised learning task. Our learned bilingual semantic embedding model is used to measure the similarity of phrase pairs which is treated as a feature in decoding. We integrate our learned model into a phrase-based translation system and experimental results indicate that our system significantly outperform the baseline system. On the NIST08 Chinese-English tra</context>
<context position="4189" citStr="Zou et al. (2013)" startWordPosition="622" endWordPosition="625">esearch focus on the task of inducing representations for single languages. Recently, a lot of progress has 142 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 142–146, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics been made at representation learning for bilingual words. Bilingual word representations have been presented by Peirsman and Pad´o (2010) and Sumita (2000). Also unsupervised algorithms such as LDA and LSA were used by Boyd-Graber and Resnik (2010), Tam et al. (2007) and Zhao and Xing (2006). Zou et al. (2013) learn bilingual embeddings utilizes word alignments and monolingual embeddings result, Le et al. (2012) and Gao et al. (2014) used continuous vector to represent the source language or target language of each phrase, and then computed translation probability using vector distance. Vuli´c and Moens (2013) learned bilingual vector spaces from non-parallel data induced by using a seed lexicon. However, none of these work considered the word sense disambiguation problem which Carpuat and Wu (2007) proved it is useful for SMT. In this paper, we learn bilingual semantic embeddings for source conten</context>
</contexts>
<marker>Zou, Socher, Cer, Manning, 2013</marker>
<rawString>Will Y. Zou, Richard Socher, Daniel Cer, and Christopher D. Manning. 2013. Bilingual word embeddings for phrase-based machine translation. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1393– 1398, Seattle, Washington, USA, October. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
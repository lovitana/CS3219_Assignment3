<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.027928">
<title confidence="0.993981">
Non-linear Mapping for Improved Identification of 1300+ Languages
</title>
<author confidence="0.99383">
Ralf D. Brown
</author>
<affiliation confidence="0.816532">
Carnegie Mellon University Language Technologies Institute
5000 Forbes Avenue, Pittsburgh PA 15213 USA
</affiliation>
<email confidence="0.993909">
ralf@cs.cmu.edu
</email>
<sectionHeader confidence="0.996841" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.797119">
Non-linear mappings of the form
</bodyText>
<equation confidence="0.973495">
P(ngram)γ and log(1+τP(ngram))
log(1+τ)
</equation>
<bodyText confidence="0.999863142857143">
are applied to the n-gram probabilities
in five trainable open-source language
identifiers. The first mapping reduces
classification errors by 4.0% to 83.9%
over a test set of more than one million
65-character strings in 1366 languages,
and by 2.6% to 76.7% over a subset of 781
languages. The second mapping improves
four of the five identifiers by 10.6% to
83.8% on the larger corpus and 14.4% to
76.7% on the smaller corpus. The subset
corpus and the modified programs are
made freely available for download at
http://www.cs.cmu.edu/∼ralf/langid.html.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999821666666667">
Language identification, particularly of short
strings, is a task which is becoming quite impor-
tant as a preliminary step in much automated pro-
cessing of online data streams such as microblogs
(e.g. Twitter). In addition, an increasing num-
ber of languages are represented online, so it is
desireable that performance remain high as more
languages are added to the identifier.
In this paper, we stress-test five open-source
n-gram-based language identifiers by presenting
them with 65-character strings (about one printed
line of text in a book) in up to 1366 languages. We
then apply a simple modification to their scoring
algorithms which improves the classification ac-
curacy of all five of them, three quite dramatically.
</bodyText>
<sectionHeader confidence="0.9928" genericHeader="introduction">
2 Method
</sectionHeader>
<bodyText confidence="0.999916">
The selected modification to the scoring algorithm
is to apply a non-linear mapping which spreads
out the lower probability values while compact-
ing the higher ones. This low-end spreading of
values is the opposite of what one sees in a Zip-
fian distribution (Zipf, 1935), where the proba-
bilities of the most common items are the most
spread out while the less frequent items become
ever more crowded as there are increasing num-
bers of them in ever-smaller ranges. The hypoth-
esis is that regularizing the spacing between val-
ues will improve language-identification accuracy
by avoiding over-weighting frequent items (from
having higher probabilities in the training data and
also occurring more frequently in the test string).
Two functions were selected for experiments:
</bodyText>
<equation confidence="0.999941">
x = P(ngram)
gamma: y = xγ
loglike: y =
log(1 + 10τx)
log(1 + 10τ)
</equation>
<bodyText confidence="0.999974625">
The first simply raises the n-gram probabil-
ity to a non-unity power; this exponent is named
“gamma” as in image processing (Poynton, 1998).
The second mapping function is a normalized vari-
ant of the logarithm function; the normalization
provides fixed points at 0 and 1, as is the case for
gamma. Each of the functions gamma and loglike
has one tunable parameter, γ and T, respectively.
</bodyText>
<sectionHeader confidence="0.999964" genericHeader="related work">
3 Related Work
</sectionHeader>
<bodyText confidence="0.998436357142857">
Although n-gram statistics as a basis for language
identification has been in use for two decades since
Cavnar and Trenkle (1994) and Dunning (1994),
little work has been done on trying to optimize
the values used for those n-gram statistics. Where
some form of frequency mapping is used, it is of-
ten implicit (as in Cavnar and Trenkle’s use of
ranks instead of frequencies) and generally goes
unremarked as such.
Vogel and Tresner-Kirsch (2012) use the log-
arithm of the frequency for some experimental
runs, reporting that it improved accuracy in some
cases. Gebre et al (2013) used logarithmic term-
frequency scaling of words in an English-language
</bodyText>
<page confidence="0.962679">
627
</page>
<bodyText confidence="0.849424285714286">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 627–632,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
essay to classify the native language of the writer,
reporting an improvement from 82.36% accuracy
to 84.55% in conjunction with inverse document
frequency (IDF) weighting, and from 79.18% ac-
curacy to 80.82% without IDF.
</bodyText>
<sectionHeader confidence="0.999292" genericHeader="method">
4 Programs
</sectionHeader>
<subsectionHeader confidence="0.998312">
4.1 LangDetect
</subsectionHeader>
<bodyText confidence="0.999988409090909">
LangDetect, version 2011-09-13 (Shuyo,
2014), uses the Naive Bayes approach. Inputs are
split into a bag of character n-grams of length 1
through 3; each randomly-drawn n-gram’s prob-
ability in each of the trained models is multiplied
by the current score for that model. After 1000
n-grams, or when periodic renormalization into
a probability distribution detects that one model
has accumulated an overwhelming probability
mass, the iteration is terminated. After averaging
seven randomized iterations, each with a random
gaussian offset (mean 5×10−6, standard deviation
0.5 × 10−6) that is added to each probability prior
to multiplication (to avoid multiplication by zero),
the highest-scoring model is declared to be the
language of the input.
The mapping function is applied to the model’s
probability before adding the randomized off-
set. To work around the limitation of one model
per language code, disambiguating digits are ap-
pended to the language code during training and
removed from the output prior to scoring.
</bodyText>
<subsectionHeader confidence="0.94712">
4.2 libtextcat
</subsectionHeader>
<bodyText confidence="0.9999492">
libtextcat, version 2.2-9 (Hugueney, 2011),
is a C reimplementation of the Cavnar and Tren-
kle (1994) method. It compiles “fingerprints” con-
taining a ranked list of the 400 (by default) most
frequent 1- through 5-grams in the training data.
An unknown text is classified by forming its fin-
gerprint and comparing that fingerprint against the
trained fingerprints. A penalty is assigned based
on the number of positions by which each n-gram
differs between the input and the trained model;
n-grams which appear in only one of the two are
assigned the maximum penalty, equal to the size
of the fingerprints. The model with the lowest
penalty score is selected as the language of the in-
put.
For this work, the libtextcat source code
was modified to remove the hard-coded fingerprint
size of 400 n-grams. While adding the frequency
mapping, the code was discovered to also hard-
code the maximum distortion penalty at 400; this
was corrected to set the maximum penalty equal to
the maximum size of any loaded fingerprint.1
Score mapping was implemented by dividing
each penalty value by the maximum penalty to
produce a proportion, applying the mapping func-
tion, and then multiplying the result by the maxi-
mum penalty and rounding to an integer (to avoid
other code changes). Because there are only a lim-
ited number of possible penalties, a lookup table is
pre-computed, eliminating the impact on speed.
</bodyText>
<subsectionHeader confidence="0.976418">
4.3 mguesser
</subsectionHeader>
<bodyText confidence="0.999955041666667">
mguesser, version 0.4 (Barkov, 2008), is part of
the mnoGoSearch search engine. While its doc-
umentation indicates that it implements the Cav-
nar and Trenkle approach, its actual similarity
computation is very different. Each training and
test text is converted into a 4096-element hash ta-
ble by extracting byte n-grams of length 6 (trun-
cated at control characters and multiple consecu-
tive blanks), hashing each n-gram using CRC-32,
and incrementing the count for the corresponding
hash entry. The hash table entries are then nor-
malized to a mean of 0.0 and standard deviation
of 1.0, and the similarity is computed as the inner
(dot) product of the hash tables treated as vectors.
The trained model receiving the highest similarity
score against the input is declared the language of
the input.
Nonlinear mapping was added by inserting a
step just prior to the normalization of the hash ta-
ble. The counts in the table are converted to proba-
bilities by dividing by the sum of counts, the map-
ping is applied to that probability, and the result is
converted back into a count by multiplying by the
original sum of counts.
</bodyText>
<subsectionHeader confidence="0.990393">
4.4 whatlang
</subsectionHeader>
<bodyText confidence="0.984662230769231">
whatlang, version 1.24 (Brown, 2014a), is
the stand-alone identification program from LA-
Strings (Brown, 2013). It performs identifica-
tion by computing the inner product of byte tri-
grams through k-grams (k=6 by default and in
this work) between the input and the trained mod-
els; for speed, the computation is performed in-
crementally, adding the length-weighted probabil-
1The behavior observed by (Brown, 2013) of performance
rapidly degrading for fingerprints larger than 500 disappears
with this correction. It was an artifact of an increasing pro-
portion of n-grams present in the model receiving penalties
greater than n-grams absent from the model.
</bodyText>
<page confidence="0.994152">
628
</page>
<bodyText confidence="0.9999665">
ity of each n-gram as it is encountered in the in-
put. Models are formed by finding the highest-
frequency n-grams of the configured lengths, with
some filtering as described in (Brown, 2012).
</bodyText>
<subsectionHeader confidence="0.8707">
4.5 YALI
</subsectionHeader>
<bodyText confidence="0.9999526">
YALI (Yet Another Language Identifier) (Majlis,
2012) is an identifier written in Perl. It performs
minor text normalization by collapsing multiple
blanks into a single blank and removing leading
and trailing blanks from lines. Thereafter, it uses
a sliding window to generate byte n-grams of a
(configurable) fixed length, and sums the proba-
bilities for each n-gram in each trained model. As
with whatlang, this effectively computes the in-
ner products between the input and the models.
Mapping was added by applying the mapping
function to the model probabilities as they are
read in from disk. As with LangDetect, disam-
biguating digits were used to allow multiple mod-
els per language code.
</bodyText>
<sectionHeader confidence="0.99861" genericHeader="method">
5 Data
</sectionHeader>
<bodyText confidence="0.99982605">
The data used for the experiments described in
this paper comes predominantly from Bible trans-
lations, Wikipedia, and the Europarl corpus of Eu-
ropean parliamentary proceedings (Koehn, 2005).
The 1459 files of the training corpus generate 1483
models in 1368 languages. A number of train-
ing files generate models in both UTF-8 and ISO
8859-1, numerous languages have multiple train-
ing files in different writing systems, and several
have multiple files for different regional variants
(e.g. European and Brazilian Portugese).
The text for a language is split into training,
test, and possibly a disjoint development set. The
amount of text per language varies, with quartiles
of 1.19/1.47/2.22 million bytes. In general, ev-
ery thirtieth line of text is reserved for the test set;
some smaller languages reserve a higher propor-
tion. If more than 3.2 million bytes remain af-
ter reserving the test set, every thirtieth line of
the remaining text is reserved as a development
set. There are development sets for 220 languages.
The unreserved test is used for model training.
The test data is word-wrapped to 65 characters
or less, and wrapped lines shorter than 25 bytes
are excluded. Up to the first 1000 lines of wrapped
text are used for testing. One language with fewer
than 50 test strings is excluded from the test set, as
is the constructed language Klingon due to heavy
pollution with English. In total, the test files con-
tain 1,090,571 lines of text in 1366 languages.
Wikipedia text and many of the Bible transla-
tions are redistributable under Creative Commons
licenses, and have been packaged into the LTI
LangID Corpus (Brown, 2014b). This smaller
corpus contains 781 languages, 119 of them with
development sets, and a total of 649,589 lines in
the test files. The languages are a strict subset
of those in the larger corpus, but numerous lan-
guages have had Wikipedia text substituted for
non-redistributable Bible translations.
</bodyText>
<sectionHeader confidence="0.999739" genericHeader="method">
6 Experiments
</sectionHeader>
<bodyText confidence="0.999967058823529">
Using the data sets described in the previous sec-
tion, we ran a sweep of different gamma and tau
values for each language identifier to determine
their optimal values on both development and test
strings. Step sizes for γ were generally 0.1, while
those for τ were 1.0, with smaller steps near the
minima. Since it does not provide explicit con-
trol over model sizes, LangDetect was trained
on a maximum of 1,000,000 bytes per model, as
reported optimal in (Brown, 2013). The other pro-
grams were trained on a maximum of 2,500,000
bytes per model; libtextcat and whatlang
used default model sizes of 400 and 3500, respec-
tively, while mguesser was set to the previously-
reported 1500 n-grams per model. After some ex-
perimentation, YALI was set to use 5-grams, with
3500 n-grams per model to match whatlang.
</bodyText>
<sectionHeader confidence="0.999943" genericHeader="evaluation">
7 Results
</sectionHeader>
<bodyText confidence="0.999897888888889">
Tables 1 and 2 show the absolute performance and
relative percentage change in classification errors
for the five programs using the two mapping func-
tions, as well as the values of γ and τ at which the
fewest errors were made on the development set.
Overall, the smaller corpus performed worse due
to the greater percentage of Wikipedia texts, which
are polluted with words and phrases in other lan-
guages. In the test set, this occasionally causes
a correct identification as another language to be
scored as an error.
Figures 2 and 3 graph the classification error
rates (number of incorrectly-labeled strings di-
vided by total number of strings in the test set) in
percent for different values of γ. A gamma of 1.0
is the baseline condition. The dramatic improve-
ments in mguesser, whatlang and YALI are
quite evident, while the smaller but non-trivial im-
</bodyText>
<page confidence="0.998066">
629
</page>
<table confidence="0.999623857142857">
Program Error% gamma mapping loglike mapping
Error% A% -y Error% A% T
LangDet. 3.233 2.767 -14.4 0.80 2.889 -10.6 1.0
libtextcat 6.787 6.514 -4.0 2.20 – – –
mguesser 15.704 4.330 -72.4 0.39 4.177 -73.4 3.8
whatlang 13.309 2.136 -83.9 0.27 2.146 -83.8 4.5
YALI 9.883 2.313 -76.6 0.20 2.313 -76.6 8.0
</table>
<tableCaption confidence="0.98674">
Table 1: Language-identification accuracy on the 1366-language corpus. -y and T were tuned on the
220-language development set; only marginally better results can be achieved by tuning on the test set.
</tableCaption>
<table confidence="0.999901571428572">
Program Error% gamma mapping loglike mapping
Error% A% -y Error% A% T
LangDet. 3.603 3.093 -14.2 0.68 3.083 -14.4 2.3
libtextcat 6.693 6.521 -2.6 1.70 – – –
mguesser 14.200 4.936 -65.2 0.40 4.779 -66.3 3.7
whatlang 11.879 2.770 -76.7 0.14 2.772 -76.7 5.6
YALI 8.726 2.972 -65.9 0.09 2.989 -65.7 9.0
</table>
<tableCaption confidence="0.9517485">
Table 2: Language-identification accuracy on the 781-language corpus. -y and T were tuned on the 119-
language development set. libtextcat did not improve with the loglike mapping (see text).
</tableCaption>
<bodyText confidence="0.999524586206897">
provements in libtextcat are difficult to dis-
cern at this scale. Since libtextcat uses much
smaller models than the others by default, Figure
1 gives a closer look at its performance for larger
model sizes. As the models grow, the absolute
baseline performance improves, but the change
from gamma-correction decreases and the optimal
value of -y also decreases toward 1.0. This hints
that the implicit mapping of ranks either becomes
closer to optimal, or that gamma becomes less ef-
fective at correcting it. At a model size of 3000
n-grams, the baseline error rate is 2.465% while
the best performance is 2.457% at -y = 1.10.
That the best -y for libtextcat is greater
than 1.0 was not entirely unexpected. The power-
law distribution of n-gram frequencies implies
that the conversion from frequencies to ranks is
essentially logarithmic, and log n eventually be-
comes less than n&apos; for any c &gt; 0. The implication
of -y &gt; 1 is simply that the conversion to ranks
is too strong a correction, which must be partially
undone by the gamma mapping.
Figures 4 and 5 graph the error rates for differ-
ent values of T. On the graph, zero is the baseline
condition without mapping for comparison pur-
poses; the mapping function is not the identity for
T = 0. It can clearly be seen that libtextcat is
hurt by the loglike mapping, which never reduces
values, even with negative T. Using the inverse of
</bodyText>
<figure confidence="0.812688">
0.0 0.5 1.0 1.5 2.0
Gamma
</figure>
<figureCaption confidence="0.8718305">
Figure 1: libtextcat performance at different
fingerprint sizes. -y = 1 is the baseline.
</figureCaption>
<bodyText confidence="0.99970575">
the loglike mapping should improve performance,
but has not yet been tried. The other programs
show very similar behavior to their results with
gamma.
</bodyText>
<sectionHeader confidence="0.987455" genericHeader="conclusions">
8 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.9384325">
Non-linear mapping is shown to be effective at
improving the accuracy of five different language
</bodyText>
<figure confidence="0.986850611111111">
Error Rate (%)
9
8
5
4
3
2
7
6
n=400
n=500
n=2000
n=3000
630
Error Rate (%)
Error Rate (%)
0.0 0.5 1.0 1.5 2.0
Gamma
</figure>
<figureCaption confidence="0.952701">
Figure 2: Performance of the identifiers on the
1366-language corpus using the gamma mapping.
</figureCaption>
<figure confidence="0.9775675">
0.0 0.5 1.0 1.5 2.0
Gamma
</figure>
<figureCaption confidence="0.981152">
Figure 3: Performance of the identifiers on the
781-language corpus using the gamma mapping.
</figureCaption>
<figure confidence="0.9855205">
0.0 1.0 2.0 3.0 4.0 5.0 6.0 7.0 8.0
Tau
</figure>
<figureCaption confidence="0.9580785">
Figure 4: Performance of the identifiers on the
1366-language corpus using the loglike mapping.
</figureCaption>
<figure confidence="0.9756665">
0.0 1.0 2.0 3.0 4.0 5.0 6.0 7.0 8.0
Tau
</figure>
<figureCaption confidence="0.9810275">
Figure 5: Performance of the identifiers on the
781-language corpus using the loglike mapping.
</figureCaption>
<figure confidence="0.994692481481482">
16
14
12
10
8
4
2
6
mguesser (n=1500)
YALI (5gr, n=3500)
libtextcat (n=400)
LangDetect
whatlang (n=3500)
Error Rate (%)
16
14
12
10
8
4
2
6
mguesser (n=1500)
YALI (5gr, n=3500)
libtextcat (n=400)
LangDetect
whatlang (n=3500)
16
14
12
10
8
4
2
6
mguesser (n=1500)
YALI (5gr, n=3500)
libtextcat (n=400)
LangDetect
whatlang (n=3500)
Error Rate (%)
16
14
12
10
8
4
2
6
mguesser (n=1500)
YALI (5gr, n=3500)
libtextcat (n=400)
LangDetect
whatlang (n=3500)
</figure>
<bodyText confidence="0.999221">
identifier using four highly-divergent algorithms
for computing model scores from n-gram statis-
tics. Improvements range from small – 2.6% re-
duction in classification errors – to dramatic for
the three programs with the worst baselines – 65.2
to 76.7% reduction in errors on the smaller cor-
pus and 72.4 to 83.9% on the larger. While both
mappings have similar performance for four of the
programs, libtextcat only benefits from the
gamma mapping, as it can also reduce n-gram
scores, unlike the loglike mapping.
Training data, source code, and supple-
mentary information may be downloaded from
http://www.cs.cmu.edu/∼ralf/langid.html.
Future work includes modifying additional lan-
guage identifiers such as langid.py (Lui and
Baldwin, 2012) and VarClass (Zampieri and
Gebre, 2014), experimenting with other mapping
functions, and investigating the method’s efficacy
on pluricentric languages like those VarClass is
designed to identify.
</bodyText>
<page confidence="0.998167">
631
</page>
<sectionHeader confidence="0.995788" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999571086956522">
Alexander Barkov. 2008. mguesser ver-
sion 0.4. http://www.mnogosearch.org/guesser/-
mguesser-0.4.tar.gz (accessed 2014-08-19).
Ralf D. Brown. 2012. Finding and Identifying Text in
900+ Languages. Digital Investigation, 9:S34–S43.
Ralf D. Brown. 2013. Selecting and Weighting N-
Grams to Identify 1100 Languages. In Proceedings
of Text, Speech, and Discourse 2013, September.
Ralf Brown. 2014a. Language-Aware String Extractor,
August. https://sourceforge.net/projects/la-strings/
(accessed 2014-08-19).
Ralf D. Brown. 2014b. LTI LangID Corpus, Release
1. http://www.cs.cmu.edu/∼ralf/langid.html.
William B. Cavnar and John M. Trenkle. 1994. N-
Gram-Based Text Categorization. In Proceedings
of SDAIR-94, 3rd Annual Symposium on Document
Analysis and Information Retrieval, pages 161–175.
UNLV Publications/Reprographics, April.
Ted Dunning. 1994. Statistical Identification of Lan-
guage. Technical Report MCCS 94-273, New Mex-
ico State University.
Binyam Gebrekidan Gebre, Marcos Zampieri, Peter
Wittenburg, and Tom Heskes. 2013. Improving Na-
tive Language Identification with TF-IDF Weight-
ing. In Proceedings of the 8th NAACL Workshop on
Innovative Use of NLP for Building Educational Ap-
plications (BEA8).
Bernard Hugueney. 2011. libtextcat 2.2-
9: Faster Unicode-focused C++ reimplementation
of libtextcat. https://github.com/scientific-coder/-
libtextcat (accessed 2014-08-19).
Philipp Koehn. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. In Proceedings of
the Tenth Machine Translation Summit (MT Summix
X), pages 79–86.
Marco Lui and Timothy Baldwin. 2012. langid.py: An
Off-the-shelf Language Identification Tool. In Pro-
ceedings of the 50th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL-2012),
pages 25–30, July.
Martin Majlis. 2012. Yet Another Language Identi-
fier. In Proceedings of the Student Research Work-
shop at the 13th Conference of the European Chap-
ter of the Association for Computational Linguistics,
pages 46–54, Avignon, France, April. Association
for Computational Linguistics.
Charles Poynton. 1998. The Rehabilitation of
Gamma. In Human Vision and Electronic Imag-
ing III, Proceedings of SPIE/IS&amp;T Conference
3299, January. http://www.poynton.com/PDFs/-
Rehabilitation of gamma.pdf.
Nakatani Shuyo. 2014. Language Detection Li-
brary for Java, March. http://code.google.com/p/-
language-detection/ (accessed 2014-08-19).
John Vogel and David Tresner-Kirsch. 2012. Robust
Language Identification in Short, Noisy Texts: Im-
provements to LIGA. In Proceedings of the Third
International Workshop on Mining Ubiquitous and
Social Environments (MUSE 2012), pages 43–50,
September.
Marcos Zampieri and Binyam Gebrekidan Gebre.
2014. VarClass: An Open Source Language Iden-
tification Tool for Language Varieties. In Proceed-
ings of the Ninth International Language Resources
and Evaluation Conference (LREC 2014), Reyk-
javik, Iceland, May.
George Kingsley Zipf. 1935. The Psycho-biology of
Language: An Introduction to Dynamic Philology.
Houghton-Mifflin Co., Boston.
</reference>
<page confidence="0.997728">
632
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.701953">
<title confidence="0.999315">Non-linear Mapping for Improved Identification of 1300+ Languages</title>
<author confidence="0.999628">D Ralf</author>
<affiliation confidence="0.997419">Carnegie Mellon University Language Technologies</affiliation>
<address confidence="0.999968">5000 Forbes Avenue, Pittsburgh PA 15213</address>
<email confidence="0.998886">ralf@cs.cmu.edu</email>
<abstract confidence="0.9802404">Non-linear mappings of the form applied to the probabilities in five trainable open-source language identifiers. The first mapping reduces classification errors by 4.0% to 83.9% over a test set of more than one million 65-character strings in 1366 languages, and by 2.6% to 76.7% over a subset of 781 languages. The second mapping improves four of the five identifiers by 10.6% to 83.8% on the larger corpus and 14.4% to 76.7% on the smaller corpus. The subset corpus and the modified programs are made freely available for download at</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Alexander Barkov</author>
</authors>
<title>mguesser version 0.4.</title>
<date>2008</date>
<pages>2014--08</pages>
<note>http://www.mnogosearch.org/guesser/-mguesser-0.4.tar.gz (accessed</note>
<contexts>
<context position="6401" citStr="Barkov, 2008" startWordPosition="1016" endWordPosition="1017">g, the code was discovered to also hardcode the maximum distortion penalty at 400; this was corrected to set the maximum penalty equal to the maximum size of any loaded fingerprint.1 Score mapping was implemented by dividing each penalty value by the maximum penalty to produce a proportion, applying the mapping function, and then multiplying the result by the maximum penalty and rounding to an integer (to avoid other code changes). Because there are only a limited number of possible penalties, a lookup table is pre-computed, eliminating the impact on speed. 4.3 mguesser mguesser, version 0.4 (Barkov, 2008), is part of the mnoGoSearch search engine. While its documentation indicates that it implements the Cavnar and Trenkle approach, its actual similarity computation is very different. Each training and test text is converted into a 4096-element hash table by extracting byte n-grams of length 6 (truncated at control characters and multiple consecutive blanks), hashing each n-gram using CRC-32, and incrementing the count for the corresponding hash entry. The hash table entries are then normalized to a mean of 0.0 and standard deviation of 1.0, and the similarity is computed as the inner (dot) pro</context>
</contexts>
<marker>Barkov, 2008</marker>
<rawString>Alexander Barkov. 2008. mguesser version 0.4. http://www.mnogosearch.org/guesser/-mguesser-0.4.tar.gz (accessed 2014-08-19).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralf D Brown</author>
</authors>
<title>Finding and Identifying Text</title>
<date>2012</date>
<booktitle>in 900+ Languages. Digital Investigation, 9:S34–S43.</booktitle>
<contexts>
<context position="8338" citStr="Brown, 2012" startWordPosition="1337" endWordPosition="1338">work) between the input and the trained models; for speed, the computation is performed incrementally, adding the length-weighted probabil1The behavior observed by (Brown, 2013) of performance rapidly degrading for fingerprints larger than 500 disappears with this correction. It was an artifact of an increasing proportion of n-grams present in the model receiving penalties greater than n-grams absent from the model. 628 ity of each n-gram as it is encountered in the input. Models are formed by finding the highestfrequency n-grams of the configured lengths, with some filtering as described in (Brown, 2012). 4.5 YALI YALI (Yet Another Language Identifier) (Majlis, 2012) is an identifier written in Perl. It performs minor text normalization by collapsing multiple blanks into a single blank and removing leading and trailing blanks from lines. Thereafter, it uses a sliding window to generate byte n-grams of a (configurable) fixed length, and sums the probabilities for each n-gram in each trained model. As with whatlang, this effectively computes the inner products between the input and the models. Mapping was added by applying the mapping function to the model probabilities as they are read in from</context>
</contexts>
<marker>Brown, 2012</marker>
<rawString>Ralf D. Brown. 2012. Finding and Identifying Text in 900+ Languages. Digital Investigation, 9:S34–S43.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralf D Brown</author>
</authors>
<title>Selecting and Weighting NGrams to Identify 1100 Languages.</title>
<date>2013</date>
<booktitle>In Proceedings of Text, Speech, and Discourse</booktitle>
<contexts>
<context position="7605" citStr="Brown, 2013" startWordPosition="1218" endWordPosition="1219">dot) product of the hash tables treated as vectors. The trained model receiving the highest similarity score against the input is declared the language of the input. Nonlinear mapping was added by inserting a step just prior to the normalization of the hash table. The counts in the table are converted to probabilities by dividing by the sum of counts, the mapping is applied to that probability, and the result is converted back into a count by multiplying by the original sum of counts. 4.4 whatlang whatlang, version 1.24 (Brown, 2014a), is the stand-alone identification program from LAStrings (Brown, 2013). It performs identification by computing the inner product of byte trigrams through k-grams (k=6 by default and in this work) between the input and the trained models; for speed, the computation is performed incrementally, adding the length-weighted probabil1The behavior observed by (Brown, 2013) of performance rapidly degrading for fingerprints larger than 500 disappears with this correction. It was an artifact of an increasing proportion of n-grams present in the model receiving penalties greater than n-grams absent from the model. 628 ity of each n-gram as it is encountered in the input. M</context>
<context position="11468" citStr="Brown, 2013" startWordPosition="1854" endWordPosition="1855">ose in the larger corpus, but numerous languages have had Wikipedia text substituted for non-redistributable Bible translations. 6 Experiments Using the data sets described in the previous section, we ran a sweep of different gamma and tau values for each language identifier to determine their optimal values on both development and test strings. Step sizes for γ were generally 0.1, while those for τ were 1.0, with smaller steps near the minima. Since it does not provide explicit control over model sizes, LangDetect was trained on a maximum of 1,000,000 bytes per model, as reported optimal in (Brown, 2013). The other programs were trained on a maximum of 2,500,000 bytes per model; libtextcat and whatlang used default model sizes of 400 and 3500, respectively, while mguesser was set to the previouslyreported 1500 n-grams per model. After some experimentation, YALI was set to use 5-grams, with 3500 n-grams per model to match whatlang. 7 Results Tables 1 and 2 show the absolute performance and relative percentage change in classification errors for the five programs using the two mapping functions, as well as the values of γ and τ at which the fewest errors were made on the development set. Overal</context>
</contexts>
<marker>Brown, 2013</marker>
<rawString>Ralf D. Brown. 2013. Selecting and Weighting NGrams to Identify 1100 Languages. In Proceedings of Text, Speech, and Discourse 2013, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralf Brown</author>
</authors>
<title>Language-Aware String Extractor,</title>
<date>2014</date>
<pages>2014--08</pages>
<note>https://sourceforge.net/projects/la-strings/ (accessed</note>
<contexts>
<context position="7531" citStr="Brown, 2014" startWordPosition="1208" endWordPosition="1209"> standard deviation of 1.0, and the similarity is computed as the inner (dot) product of the hash tables treated as vectors. The trained model receiving the highest similarity score against the input is declared the language of the input. Nonlinear mapping was added by inserting a step just prior to the normalization of the hash table. The counts in the table are converted to probabilities by dividing by the sum of counts, the mapping is applied to that probability, and the result is converted back into a count by multiplying by the original sum of counts. 4.4 whatlang whatlang, version 1.24 (Brown, 2014a), is the stand-alone identification program from LAStrings (Brown, 2013). It performs identification by computing the inner product of byte trigrams through k-grams (k=6 by default and in this work) between the input and the trained models; for speed, the computation is performed incrementally, adding the length-weighted probabil1The behavior observed by (Brown, 2013) of performance rapidly degrading for fingerprints larger than 500 disappears with this correction. It was an artifact of an increasing proportion of n-grams present in the model receiving penalties greater than n-grams absent f</context>
<context position="10686" citStr="Brown, 2014" startWordPosition="1723" endWordPosition="1724">nreserved test is used for model training. The test data is word-wrapped to 65 characters or less, and wrapped lines shorter than 25 bytes are excluded. Up to the first 1000 lines of wrapped text are used for testing. One language with fewer than 50 test strings is excluded from the test set, as is the constructed language Klingon due to heavy pollution with English. In total, the test files contain 1,090,571 lines of text in 1366 languages. Wikipedia text and many of the Bible translations are redistributable under Creative Commons licenses, and have been packaged into the LTI LangID Corpus (Brown, 2014b). This smaller corpus contains 781 languages, 119 of them with development sets, and a total of 649,589 lines in the test files. The languages are a strict subset of those in the larger corpus, but numerous languages have had Wikipedia text substituted for non-redistributable Bible translations. 6 Experiments Using the data sets described in the previous section, we ran a sweep of different gamma and tau values for each language identifier to determine their optimal values on both development and test strings. Step sizes for γ were generally 0.1, while those for τ were 1.0, with smaller step</context>
</contexts>
<marker>Brown, 2014</marker>
<rawString>Ralf Brown. 2014a. Language-Aware String Extractor, August. https://sourceforge.net/projects/la-strings/ (accessed 2014-08-19).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralf D Brown</author>
</authors>
<date>2014</date>
<journal>LTI LangID Corpus, Release</journal>
<volume>1</volume>
<note>http://www.cs.cmu.edu/∼ralf/langid.html.</note>
<contexts>
<context position="7531" citStr="Brown, 2014" startWordPosition="1208" endWordPosition="1209"> standard deviation of 1.0, and the similarity is computed as the inner (dot) product of the hash tables treated as vectors. The trained model receiving the highest similarity score against the input is declared the language of the input. Nonlinear mapping was added by inserting a step just prior to the normalization of the hash table. The counts in the table are converted to probabilities by dividing by the sum of counts, the mapping is applied to that probability, and the result is converted back into a count by multiplying by the original sum of counts. 4.4 whatlang whatlang, version 1.24 (Brown, 2014a), is the stand-alone identification program from LAStrings (Brown, 2013). It performs identification by computing the inner product of byte trigrams through k-grams (k=6 by default and in this work) between the input and the trained models; for speed, the computation is performed incrementally, adding the length-weighted probabil1The behavior observed by (Brown, 2013) of performance rapidly degrading for fingerprints larger than 500 disappears with this correction. It was an artifact of an increasing proportion of n-grams present in the model receiving penalties greater than n-grams absent f</context>
<context position="10686" citStr="Brown, 2014" startWordPosition="1723" endWordPosition="1724">nreserved test is used for model training. The test data is word-wrapped to 65 characters or less, and wrapped lines shorter than 25 bytes are excluded. Up to the first 1000 lines of wrapped text are used for testing. One language with fewer than 50 test strings is excluded from the test set, as is the constructed language Klingon due to heavy pollution with English. In total, the test files contain 1,090,571 lines of text in 1366 languages. Wikipedia text and many of the Bible translations are redistributable under Creative Commons licenses, and have been packaged into the LTI LangID Corpus (Brown, 2014b). This smaller corpus contains 781 languages, 119 of them with development sets, and a total of 649,589 lines in the test files. The languages are a strict subset of those in the larger corpus, but numerous languages have had Wikipedia text substituted for non-redistributable Bible translations. 6 Experiments Using the data sets described in the previous section, we ran a sweep of different gamma and tau values for each language identifier to determine their optimal values on both development and test strings. Step sizes for γ were generally 0.1, while those for τ were 1.0, with smaller step</context>
</contexts>
<marker>Brown, 2014</marker>
<rawString>Ralf D. Brown. 2014b. LTI LangID Corpus, Release 1. http://www.cs.cmu.edu/∼ralf/langid.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William B Cavnar</author>
<author>John M Trenkle</author>
</authors>
<title>NGram-Based Text Categorization.</title>
<date>1994</date>
<booktitle>In Proceedings of SDAIR-94, 3rd Annual Symposium on Document Analysis and Information Retrieval,</booktitle>
<pages>161--175</pages>
<publisher>UNLV Publications/Reprographics,</publisher>
<contexts>
<context position="2952" citStr="Cavnar and Trenkle (1994)" startWordPosition="464" endWordPosition="467"> selected for experiments: x = P(ngram) gamma: y = xγ loglike: y = log(1 + 10τx) log(1 + 10τ) The first simply raises the n-gram probability to a non-unity power; this exponent is named “gamma” as in image processing (Poynton, 1998). The second mapping function is a normalized variant of the logarithm function; the normalization provides fixed points at 0 and 1, as is the case for gamma. Each of the functions gamma and loglike has one tunable parameter, γ and T, respectively. 3 Related Work Although n-gram statistics as a basis for language identification has been in use for two decades since Cavnar and Trenkle (1994) and Dunning (1994), little work has been done on trying to optimize the values used for those n-gram statistics. Where some form of frequency mapping is used, it is often implicit (as in Cavnar and Trenkle’s use of ranks instead of frequencies) and generally goes unremarked as such. Vogel and Tresner-Kirsch (2012) use the logarithm of the frequency for some experimental runs, reporting that it improved accuracy in some cases. Gebre et al (2013) used logarithmic termfrequency scaling of words in an English-language 627 Proceedings of the 2014 Conference on Empirical Methods in Natural Language</context>
<context position="5056" citStr="Cavnar and Trenkle (1994)" startWordPosition="789" endWordPosition="793">a random gaussian offset (mean 5×10−6, standard deviation 0.5 × 10−6) that is added to each probability prior to multiplication (to avoid multiplication by zero), the highest-scoring model is declared to be the language of the input. The mapping function is applied to the model’s probability before adding the randomized offset. To work around the limitation of one model per language code, disambiguating digits are appended to the language code during training and removed from the output prior to scoring. 4.2 libtextcat libtextcat, version 2.2-9 (Hugueney, 2011), is a C reimplementation of the Cavnar and Trenkle (1994) method. It compiles “fingerprints” containing a ranked list of the 400 (by default) most frequent 1- through 5-grams in the training data. An unknown text is classified by forming its fingerprint and comparing that fingerprint against the trained fingerprints. A penalty is assigned based on the number of positions by which each n-gram differs between the input and the trained model; n-grams which appear in only one of the two are assigned the maximum penalty, equal to the size of the fingerprints. The model with the lowest penalty score is selected as the language of the input. For this work,</context>
</contexts>
<marker>Cavnar, Trenkle, 1994</marker>
<rawString>William B. Cavnar and John M. Trenkle. 1994. NGram-Based Text Categorization. In Proceedings of SDAIR-94, 3rd Annual Symposium on Document Analysis and Information Retrieval, pages 161–175. UNLV Publications/Reprographics, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Dunning</author>
</authors>
<title>Statistical Identification of Language.</title>
<date>1994</date>
<tech>Technical Report MCCS 94-273,</tech>
<institution>New Mexico State University.</institution>
<contexts>
<context position="2971" citStr="Dunning (1994)" startWordPosition="469" endWordPosition="470"> P(ngram) gamma: y = xγ loglike: y = log(1 + 10τx) log(1 + 10τ) The first simply raises the n-gram probability to a non-unity power; this exponent is named “gamma” as in image processing (Poynton, 1998). The second mapping function is a normalized variant of the logarithm function; the normalization provides fixed points at 0 and 1, as is the case for gamma. Each of the functions gamma and loglike has one tunable parameter, γ and T, respectively. 3 Related Work Although n-gram statistics as a basis for language identification has been in use for two decades since Cavnar and Trenkle (1994) and Dunning (1994), little work has been done on trying to optimize the values used for those n-gram statistics. Where some form of frequency mapping is used, it is often implicit (as in Cavnar and Trenkle’s use of ranks instead of frequencies) and generally goes unremarked as such. Vogel and Tresner-Kirsch (2012) use the logarithm of the frequency for some experimental runs, reporting that it improved accuracy in some cases. Gebre et al (2013) used logarithmic termfrequency scaling of words in an English-language 627 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</context>
</contexts>
<marker>Dunning, 1994</marker>
<rawString>Ted Dunning. 1994. Statistical Identification of Language. Technical Report MCCS 94-273, New Mexico State University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Binyam Gebrekidan Gebre</author>
<author>Marcos Zampieri</author>
<author>Peter Wittenburg</author>
<author>Tom Heskes</author>
</authors>
<title>Improving Native Language Identification with TF-IDF Weighting.</title>
<date>2013</date>
<booktitle>In Proceedings of the 8th NAACL Workshop on Innovative Use of NLP for Building Educational Applications (BEA8).</booktitle>
<contexts>
<context position="3401" citStr="Gebre et al (2013)" startWordPosition="540" endWordPosition="543">, γ and T, respectively. 3 Related Work Although n-gram statistics as a basis for language identification has been in use for two decades since Cavnar and Trenkle (1994) and Dunning (1994), little work has been done on trying to optimize the values used for those n-gram statistics. Where some form of frequency mapping is used, it is often implicit (as in Cavnar and Trenkle’s use of ranks instead of frequencies) and generally goes unremarked as such. Vogel and Tresner-Kirsch (2012) use the logarithm of the frequency for some experimental runs, reporting that it improved accuracy in some cases. Gebre et al (2013) used logarithmic termfrequency scaling of words in an English-language 627 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 627–632, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics essay to classify the native language of the writer, reporting an improvement from 82.36% accuracy to 84.55% in conjunction with inverse document frequency (IDF) weighting, and from 79.18% accuracy to 80.82% without IDF. 4 Programs 4.1 LangDetect LangDetect, version 2011-09-13 (Shuyo, 2014), uses the Naive Bayes approach. Inputs</context>
</contexts>
<marker>Gebre, Zampieri, Wittenburg, Heskes, 2013</marker>
<rawString>Binyam Gebrekidan Gebre, Marcos Zampieri, Peter Wittenburg, and Tom Heskes. 2013. Improving Native Language Identification with TF-IDF Weighting. In Proceedings of the 8th NAACL Workshop on Innovative Use of NLP for Building Educational Applications (BEA8).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernard Hugueney</author>
</authors>
<title>libtextcat 2.2-9: Faster Unicode-focused C++ reimplementation of libtextcat.</title>
<date>2011</date>
<pages>2014--08</pages>
<note>https://github.com/scientific-coder/-libtextcat (accessed</note>
<contexts>
<context position="4998" citStr="Hugueney, 2011" startWordPosition="781" endWordPosition="782">veraging seven randomized iterations, each with a random gaussian offset (mean 5×10−6, standard deviation 0.5 × 10−6) that is added to each probability prior to multiplication (to avoid multiplication by zero), the highest-scoring model is declared to be the language of the input. The mapping function is applied to the model’s probability before adding the randomized offset. To work around the limitation of one model per language code, disambiguating digits are appended to the language code during training and removed from the output prior to scoring. 4.2 libtextcat libtextcat, version 2.2-9 (Hugueney, 2011), is a C reimplementation of the Cavnar and Trenkle (1994) method. It compiles “fingerprints” containing a ranked list of the 400 (by default) most frequent 1- through 5-grams in the training data. An unknown text is classified by forming its fingerprint and comparing that fingerprint against the trained fingerprints. A penalty is assigned based on the number of positions by which each n-gram differs between the input and the trained model; n-grams which appear in only one of the two are assigned the maximum penalty, equal to the size of the fingerprints. The model with the lowest penalty scor</context>
</contexts>
<marker>Hugueney, 2011</marker>
<rawString>Bernard Hugueney. 2011. libtextcat 2.2-9: Faster Unicode-focused C++ reimplementation of libtextcat. https://github.com/scientific-coder/-libtextcat (accessed 2014-08-19).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Europarl: A Parallel Corpus for Statistical Machine Translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the Tenth Machine Translation Summit (MT Summix X),</booktitle>
<pages>79--86</pages>
<contexts>
<context position="9237" citStr="Koehn, 2005" startWordPosition="1481" endWordPosition="1482"> byte n-grams of a (configurable) fixed length, and sums the probabilities for each n-gram in each trained model. As with whatlang, this effectively computes the inner products between the input and the models. Mapping was added by applying the mapping function to the model probabilities as they are read in from disk. As with LangDetect, disambiguating digits were used to allow multiple models per language code. 5 Data The data used for the experiments described in this paper comes predominantly from Bible translations, Wikipedia, and the Europarl corpus of European parliamentary proceedings (Koehn, 2005). The 1459 files of the training corpus generate 1483 models in 1368 languages. A number of training files generate models in both UTF-8 and ISO 8859-1, numerous languages have multiple training files in different writing systems, and several have multiple files for different regional variants (e.g. European and Brazilian Portugese). The text for a language is split into training, test, and possibly a disjoint development set. The amount of text per language varies, with quartiles of 1.19/1.47/2.22 million bytes. In general, every thirtieth line of text is reserved for the test set; some small</context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>Philipp Koehn. 2005. Europarl: A Parallel Corpus for Statistical Machine Translation. In Proceedings of the Tenth Machine Translation Summit (MT Summix X), pages 79–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Lui</author>
<author>Timothy Baldwin</author>
</authors>
<title>langid.py: An Off-the-shelf Language Identification Tool.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL-2012),</booktitle>
<pages>25--30</pages>
<marker>Lui, Baldwin, 2012</marker>
<rawString>Marco Lui and Timothy Baldwin. 2012. langid.py: An Off-the-shelf Language Identification Tool. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL-2012), pages 25–30, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Majlis</author>
</authors>
<title>Yet Another Language Identifier.</title>
<date>2012</date>
<booktitle>In Proceedings of the Student Research Workshop at the 13th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>46--54</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Avignon, France,</location>
<contexts>
<context position="8402" citStr="Majlis, 2012" startWordPosition="1346" endWordPosition="1347">computation is performed incrementally, adding the length-weighted probabil1The behavior observed by (Brown, 2013) of performance rapidly degrading for fingerprints larger than 500 disappears with this correction. It was an artifact of an increasing proportion of n-grams present in the model receiving penalties greater than n-grams absent from the model. 628 ity of each n-gram as it is encountered in the input. Models are formed by finding the highestfrequency n-grams of the configured lengths, with some filtering as described in (Brown, 2012). 4.5 YALI YALI (Yet Another Language Identifier) (Majlis, 2012) is an identifier written in Perl. It performs minor text normalization by collapsing multiple blanks into a single blank and removing leading and trailing blanks from lines. Thereafter, it uses a sliding window to generate byte n-grams of a (configurable) fixed length, and sums the probabilities for each n-gram in each trained model. As with whatlang, this effectively computes the inner products between the input and the models. Mapping was added by applying the mapping function to the model probabilities as they are read in from disk. As with LangDetect, disambiguating digits were used to al</context>
</contexts>
<marker>Majlis, 2012</marker>
<rawString>Martin Majlis. 2012. Yet Another Language Identifier. In Proceedings of the Student Research Workshop at the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 46–54, Avignon, France, April. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Poynton</author>
</authors>
<title>The Rehabilitation of Gamma.</title>
<date>1998</date>
<booktitle>In Human Vision and Electronic Imaging III, Proceedings of SPIE/IS&amp;T Conference 3299,</booktitle>
<note>http://www.poynton.com/PDFs/-Rehabilitation of gamma.pdf.</note>
<contexts>
<context position="2559" citStr="Poynton, 1998" startWordPosition="399" endWordPosition="400">equent items become ever more crowded as there are increasing numbers of them in ever-smaller ranges. The hypothesis is that regularizing the spacing between values will improve language-identification accuracy by avoiding over-weighting frequent items (from having higher probabilities in the training data and also occurring more frequently in the test string). Two functions were selected for experiments: x = P(ngram) gamma: y = xγ loglike: y = log(1 + 10τx) log(1 + 10τ) The first simply raises the n-gram probability to a non-unity power; this exponent is named “gamma” as in image processing (Poynton, 1998). The second mapping function is a normalized variant of the logarithm function; the normalization provides fixed points at 0 and 1, as is the case for gamma. Each of the functions gamma and loglike has one tunable parameter, γ and T, respectively. 3 Related Work Although n-gram statistics as a basis for language identification has been in use for two decades since Cavnar and Trenkle (1994) and Dunning (1994), little work has been done on trying to optimize the values used for those n-gram statistics. Where some form of frequency mapping is used, it is often implicit (as in Cavnar and Trenkle’</context>
</contexts>
<marker>Poynton, 1998</marker>
<rawString>Charles Poynton. 1998. The Rehabilitation of Gamma. In Human Vision and Electronic Imaging III, Proceedings of SPIE/IS&amp;T Conference 3299, January. http://www.poynton.com/PDFs/-Rehabilitation of gamma.pdf.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nakatani Shuyo</author>
</authors>
<title>Language Detection Library for Java,</title>
<date>2014</date>
<pages>2014--08</pages>
<note>http://code.google.com/p/-language-detection/ (accessed</note>
<contexts>
<context position="3962" citStr="Shuyo, 2014" startWordPosition="621" endWordPosition="622">improved accuracy in some cases. Gebre et al (2013) used logarithmic termfrequency scaling of words in an English-language 627 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 627–632, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics essay to classify the native language of the writer, reporting an improvement from 82.36% accuracy to 84.55% in conjunction with inverse document frequency (IDF) weighting, and from 79.18% accuracy to 80.82% without IDF. 4 Programs 4.1 LangDetect LangDetect, version 2011-09-13 (Shuyo, 2014), uses the Naive Bayes approach. Inputs are split into a bag of character n-grams of length 1 through 3; each randomly-drawn n-gram’s probability in each of the trained models is multiplied by the current score for that model. After 1000 n-grams, or when periodic renormalization into a probability distribution detects that one model has accumulated an overwhelming probability mass, the iteration is terminated. After averaging seven randomized iterations, each with a random gaussian offset (mean 5×10−6, standard deviation 0.5 × 10−6) that is added to each probability prior to multiplication (to</context>
</contexts>
<marker>Shuyo, 2014</marker>
<rawString>Nakatani Shuyo. 2014. Language Detection Library for Java, March. http://code.google.com/p/-language-detection/ (accessed 2014-08-19).</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Vogel</author>
<author>David Tresner-Kirsch</author>
</authors>
<title>Robust Language Identification in Short, Noisy Texts: Improvements to LIGA.</title>
<date>2012</date>
<booktitle>In Proceedings of the Third International Workshop on Mining Ubiquitous and Social Environments (MUSE 2012),</booktitle>
<pages>43--50</pages>
<contexts>
<context position="3268" citStr="Vogel and Tresner-Kirsch (2012)" startWordPosition="517" endWordPosition="520">n; the normalization provides fixed points at 0 and 1, as is the case for gamma. Each of the functions gamma and loglike has one tunable parameter, γ and T, respectively. 3 Related Work Although n-gram statistics as a basis for language identification has been in use for two decades since Cavnar and Trenkle (1994) and Dunning (1994), little work has been done on trying to optimize the values used for those n-gram statistics. Where some form of frequency mapping is used, it is often implicit (as in Cavnar and Trenkle’s use of ranks instead of frequencies) and generally goes unremarked as such. Vogel and Tresner-Kirsch (2012) use the logarithm of the frequency for some experimental runs, reporting that it improved accuracy in some cases. Gebre et al (2013) used logarithmic termfrequency scaling of words in an English-language 627 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 627–632, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics essay to classify the native language of the writer, reporting an improvement from 82.36% accuracy to 84.55% in conjunction with inverse document frequency (IDF) weighting, and from 79.18% accuracy</context>
</contexts>
<marker>Vogel, Tresner-Kirsch, 2012</marker>
<rawString>John Vogel and David Tresner-Kirsch. 2012. Robust Language Identification in Short, Noisy Texts: Improvements to LIGA. In Proceedings of the Third International Workshop on Mining Ubiquitous and Social Environments (MUSE 2012), pages 43–50, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marcos Zampieri</author>
<author>Binyam Gebrekidan Gebre</author>
</authors>
<title>VarClass: An Open Source Language Identification Tool for Language Varieties.</title>
<date>2014</date>
<booktitle>In Proceedings of the Ninth International Language Resources and Evaluation Conference (LREC 2014),</booktitle>
<location>Reykjavik, Iceland,</location>
<marker>Zampieri, Gebre, 2014</marker>
<rawString>Marcos Zampieri and Binyam Gebrekidan Gebre. 2014. VarClass: An Open Source Language Identification Tool for Language Varieties. In Proceedings of the Ninth International Language Resources and Evaluation Conference (LREC 2014), Reykjavik, Iceland, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Kingsley Zipf</author>
</authors>
<title>The Psycho-biology of Language: An Introduction to Dynamic Philology. Houghton-Mifflin Co.,</title>
<date>1935</date>
<location>Boston.</location>
<contexts>
<context position="1853" citStr="Zipf, 1935" startWordPosition="283" endWordPosition="284">we stress-test five open-source n-gram-based language identifiers by presenting them with 65-character strings (about one printed line of text in a book) in up to 1366 languages. We then apply a simple modification to their scoring algorithms which improves the classification accuracy of all five of them, three quite dramatically. 2 Method The selected modification to the scoring algorithm is to apply a non-linear mapping which spreads out the lower probability values while compacting the higher ones. This low-end spreading of values is the opposite of what one sees in a Zipfian distribution (Zipf, 1935), where the probabilities of the most common items are the most spread out while the less frequent items become ever more crowded as there are increasing numbers of them in ever-smaller ranges. The hypothesis is that regularizing the spacing between values will improve language-identification accuracy by avoiding over-weighting frequent items (from having higher probabilities in the training data and also occurring more frequently in the test string). Two functions were selected for experiments: x = P(ngram) gamma: y = xγ loglike: y = log(1 + 10τx) log(1 + 10τ) The first simply raises the n-gr</context>
</contexts>
<marker>Zipf, 1935</marker>
<rawString>George Kingsley Zipf. 1935. The Psycho-biology of Language: An Introduction to Dynamic Philology. Houghton-Mifflin Co., Boston.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004119">
<title confidence="0.997869">
Assessing the Impact of Translation Errors
on Machine Translation Quality with Mixed-effects Models
</title>
<author confidence="0.862904">
Marcello Federico, Matteo Negri, Luisa Bentivogli, Marco Turchi
</author>
<address confidence="0.587255">
FBK - Fondazione Bruno Kessler
Via Sommarive 18, 38123 Trento, Italy
</address>
<email confidence="0.998721">
{federico,negri,bentivogli,turchi}@fbk.eu
</email>
<sectionHeader confidence="0.997384" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999965052631579">
Learning from errors is a crucial aspect of
improving expertise. Based on this no-
tion, we discuss a robust statistical frame-
work for analysing the impact of different
error types on machine translation (MT)
output quality. Our approach is based on
linear mixed-effects models, which allow
the analysis of error-annotated MT out-
put taking into account the variability in-
herent to the specific experimental setting
from which the empirical observations are
drawn. Our experiments are carried out
on different language pairs involving Chi-
nese, Arabic and Russian as target lan-
guages. Interesting findings are reported,
concerning the impact of different error
types both at the level of human perception
of quality and with respect to performance
results measured with automatic metrics.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999106237288136">
The dominant statistical approach to machine
translation (MT) is based on learning from large
amounts of parallel data and tuning the result-
ing models on reference-based metrics that can
be computed automatically, such as BLEU (Pap-
ineni et al., 2001), METEOR (Banerjee and Lavie,
2005), TER (Snover et al., 2006), GTM (Turian
et al., 2003). Despite the steady progress in the
last two decades, especially for few well resourced
translation directions having English as target lan-
guage, this way to approach the problem is quickly
reaching a performance plateau. One reason is
that parallel data are a source of reliable informa-
tion but, alone, limit systems knowledge to ob-
served positive examples (i.e. how a sentence
should be translated) without explicitly modelling
any notion of error (i.e. how a sentence should
not be translated). Another reason is that, as a
development and evaluation criterion, automatic
metrics provide a holistic view of systems’ be-
haviour without identifying the specific issues of a
translation. Indeed, the global scores returned by
MT evaluation metrics depend on comparisons be-
tween translation hypotheses and reference trans-
lations, where the causes and the nature of the dif-
ferences between them are not identified.
To cope with these issues and define system
improvement priorities, the focus of MT evalua-
tion research is gradually shifting towards profil-
ing systems’ behaviour with respect to various ty-
pologies of errors (Vilar et al., 2006; Popovi´c and
Ney, 2011; Farr´us et al., 2012, inter alia). This
shift has enriched the traditional MT evaluation
framework with a new element, that is the actual
errors done by a system. Until now, most of the
research has focused on the relationship (i.e. the
correlation) between two elements of the frame-
work: humans and automatic evaluation metrics.
As a new element of the framework, which be-
comes a sort of “evaluation triangle”, the analy-
sis of error annotations opens interesting research
problems related to the relationships between: i)
error types and human perception of MT quality
and ii) error types and the sensitivity of automatic
metrics.
Besides motivating further investigation on met-
rics featuring high correlation with human judge-
ments (a well-established MT research sub-field,
which is out of the scope of this paper), connecting
the vertices of this triangle raises new challenging
questions such as:
(1) Which types of MT errors have the high-
est impact on human perception of translation
quality? Surprisingly, little prior work focused
on this side of the triangle. Error annotations
have been considered to highlight strengths and
weaknesses of MT engines or to investigate the
influence of different error types on post-editors’
work. However, the direct connection between er-
</bodyText>
<page confidence="0.859146">
1643
</page>
<note confidence="0.8971185">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1643–1653,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999638161290323">
rors and users’ preferences has been only partially
understood, mainly from a descriptive standpoint
and through rudimentary techniques unsuitable to
draw clear-cut conclusions or reliable inferences.
(2) To which types of errors are different MT
evaluation metrics more sensitive? This side of
the triangle has been even less explored. For in-
stance, little has been done to understand which
automatic metric is more suitable to assess sys-
tem improvements with respect to a specific issue
(e.g. word order or morphology) or to shed light
on the joint impact of different error types on per-
formance results calculated with different metrics.
To answer these questions, we propose a ro-
bust statistical framework to analyse the im-
pact of different error types, alone and in com-
bination, both on human perception of quality and
on MT evaluation metrics’ results. Our analysis
is carried out by employing linear mixed-effects
models, a generalization of linear regression mod-
els suited to model responses with fixed and ran-
dom effects. Experiments are performed on data
covering three translation directions (English to
Chinese, Arabic and Russian). For each direc-
tion, two automatic translations were collected for
around 400 sentences and were manually evalu-
ated by expert translators through absolute quality
judgements and error annotation.
Building on the advantages offered by linear
mixed-effects models, our main contributions in-
clude:
</bodyText>
<listItem confidence="0.968090888888889">
• A rigorous method, novel to MT error anal-
ysis research, to relate MT issues to human
preferences and MT metrics’ results;
• The application of such method to three
translation directions having English as
source and different languages as target;
• A number of findings, specific to each lan-
guage direction, which are out of the reach of
the few simpler methods proposed so far.
</listItem>
<bodyText confidence="0.9997945">
Overall, our study has clear practical implica-
tions for MT systems’ development and evalu-
ation. Indeed, the proposed statistical analysis
framework represents an ideal instrument to: i)
identify translation issues having the highest im-
pact on human perception of quality and ii) choose
the most appropriate evaluation metric to measure
progress towards their solution.
</bodyText>
<sectionHeader confidence="0.999786" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.99998006">
Error analysis, as a way to identify systems’ weak-
nesses and define priorities for their improvement,
is gaining increasing interest in the MT com-
munity (Popovi´c and Ney, 2011; Popovic et al.,
2013). Along this direction, the initial efforts to
develop error taxonomies covering different levels
of granularity (Flanagan, 1994; Vilar et al., 2006;
Farr´us Cabeceran et al., 2010; Stymne and Ahren-
berg, 2012; Lommel et al., 2014) have been re-
cently complemented by investigations on how to
exploit error annotations for diagnostic purposes.
Error annotations of sentences produced by differ-
ent MT systems, in different target languages and
domains, have been used to determine the qual-
ity of translations according to the amount of er-
rors encountered (Popovic et al., 2013), to design
new automatic metrics that take into considera-
tion human annotations (Popovic, 2012; Bojar et
al., 2013), and to train classifiers that can auto-
matic identify fine-grained errors in the MT output
(Popovi´c and Ney, 2011). The impact of edit op-
erations on post-editors’ productivity, which im-
plicitly connects the severity of different errors to
human activity, has also been studied (Temnikova,
2010; O’Brien, 2011; Blain et al., 2011), but
few attempts have been made to explicitly model
how fine-grained errors impact on human quality
judgements and automatic metrics.
Recently, the relation between different error
types, their frequency, and human quality judge-
ments has been investigated from a descriptive
standpoint in (Lommel et al., 2014; Popovi´c et al.,
2014). In both works, however, the underlying as-
sumption that the most frequent error has also the
largest impact on quality perception is not verified
(in general and, least of all, across language pairs,
domains, MT systems and post-editors). Another
limitation of the proposed (univariate) analysis lies
in the fact that it exclusively focuses on error types
taken in isolation. This simplification excludes the
possibility that humans, when assigning a global
quality score to a translation, may be influenced
not only by the error types but also by their inter-
action. The implications of such possibility call
for a multivariate analysis capable to model also
error interactions.
In (Kirchhoff et al., 2013), a statistically-
grounded approach based on conjoint analysis has
been used to investigate users’ reactions to dif-
ferent types of translation errors. According to
</bodyText>
<page confidence="0.990139">
1644
</page>
<bodyText confidence="0.999975081967213">
their results, word order is the most dispreferred
error type, and the count of the errors in a sen-
tence is not a good predictor of users’ prefer-
ences. Though more sophisticated than methods
based on rough error counts, the conjoint model
is bound to several constraints that limit its us-
ability. In particular, the application of conjoint
analysis in this context requires to: i) operate with
semi-automatically created (hence artificial) data
instead of real MT output, ii) manually define dif-
ferent levels of severity for each error type (e.g.
high/medium/low), and iii) limit the number of er-
ror types considered to avoid the explosion of all
possible combinations. Finally, the conjoint anal-
ysis framework is not able to explicitly model vari-
ance in the translated sentences, the human anno-
tators, and the SMT systems used to translate the
source sentences. Our claim is that avoiding any
possible bias introduced by these factors should be
a priority in the analysis of empirical observations
in a given experimental setting.
So far, the relation between errors and auto-
matic metrics has been analysed by measuring the
correlation between single or total error frequen-
cies and automatic scores (Popovi´c and Ney, 2011;
Farr´us et al., 2012). Using two different error tax-
onomies, both works show that the sum of the er-
rors has a high correlation with BLEU and TER
scores. Similar to the aforementioned works ad-
dressing the impact of MT errors on human per-
ception, these studies disregard error interactions,
and their possible impact on automatic scores.
To overcome these issues, we propose a ro-
bust statistic analysis framework based on mixed-
effects models, which have been successfully ap-
plied to several NLP problems such as sentiment
analysis (Greene and Resnik, 2009), automatic
speech recognition (Goldwater et al., 2010), and
spoken language translation (Ruiz and Federico,
2014). Despite their effectiveness, the use of
mixed-effects models in the MT field is rather re-
cent and limited to the analysis of human post-
editions (Green et al., 2013; L¨aubli et al., 2013).
In both studies, the goal was to evaluate the im-
pact of post-editing on the quality and productivity
of human translation assuming an ANOVA mixed
model for a between-subject design, in which hu-
man translators either post-edited or translated the
same texts. Our scenario is rather different as we
employ mixed models to measure the influence of
different MT error types - expressed as continu-
ous fixed effects - on quality judgements and auto-
matic quality metrics. Mixed models, having the
capability to absorb random variability due to the
specific experimental set-up, provide a robust mul-
tivariate method to efficiently analyse the impor-
tance of error types.
Finally, differently from all previous works, our
analysis is run on language pairs having English
as source and languages distant from English (in
term of morphology and word-order) as target.
</bodyText>
<sectionHeader confidence="0.999262" genericHeader="method">
3 Mixed-effects Models
</sectionHeader>
<bodyText confidence="0.999856236842105">
Mixed-effects models - or simply mixed models
- like any regression model, express the relation-
ship between a response variable and some co-
variates and/or contrast factors. They enhance
conventional models by complementing fixed ef-
fects with so-called random effects. Random ef-
fects are introduced to absorb random variability
inherent to the specific experimental setting from
which the observations are drawn. In general, ran-
dom effects correspond to covariates that are not -
or cannot be - exhaustively observed in an experi-
ment, e.g. the human annotators and the evaluated
systems. Hence, mixed models permit to elegantly
cope with experimental design aspects that hinder
the applicability of conventional regression mod-
els. These are, in particular, the use of repeated
and/or clustered observations that introduce corre-
lations in the response variable that clearly violate
the independence and homoscedasticity assump-
tions of conventional linear, ANOVA, and logis-
tic regression models. Significance testing with
mixed models is in general more powerful, i.e. less
prone to Type II Errors, and also permits to reduce
the chance of Type I Errors in within-subject de-
signs, which are prone to the “fallacy of language-
as-a-fixed-effect” (Clark, 1973).
Random effects can be directly associated to
the regression model parameters, as random in-
tercepts and random slopes, and have the same
form of the generic error component of the model,
i.e. normally distributed with zero mean and un-
known variance. As random effects introduce hid-
den variables, mixed models are trained with Ex-
pectation Maximization, while significance testing
is performed via likelihood-ratio (LR) tests.
In this work we employ mixed linear models to
measure the influence of different MT error types,
expressed as continuous fixed effects, on quality
</bodyText>
<page confidence="0.944896">
1645
</page>
<bodyText confidence="0.999823625">
judgements or on automatic quality metrics.1
We illustrate mixed linear models (Baayen et
al., 2008) by referring to our analysis, which ad-
dresses the relationships between a quality metric
(y) and different types of errors (e.g. A, B, and
C)2 observed at the sentence level. For the sake of
simplicity, we assume to have balanced repeated
observations for one single crossed effect. That is,
we have i E 11, ... , I} MT systems (our groups)
each of which translated the same j E 11, ... , J}
test sentences. Our response variable yij - a nu-
meric quality score - is computed on each (sen-
tence, system) pair, and we aim to investigate its
relationship with error statistics available for each
MT output, namely Aij, Bij and Cij. A (possible)
linear mixed model for our study would be:
</bodyText>
<equation confidence="0.9999235">
yij = β0 + β1 Aij + β2 Bij + β3 Cij + (1)
b0,i + b1,iAij + b2,iBij + b3,iCi + Eij
</equation>
<bodyText confidence="0.999321222222222">
The model is split into two lines on purpose. The
first line shows the fixed effect component, that is
intercept (β0) and slopes (β1, β2, β3) for each error
type. The second line specifies the random struc-
ture of the model, which includes random inter-
cept and slopes for each MT system and the resid-
ual error. Borrowing the notation from (Green
et al., 2013), we conveniently rewrite (1) in the
group-wise arranged matrix notation:
</bodyText>
<equation confidence="0.976488">
yi = xTi β + zTi bi + Ci (2)
</equation>
<bodyText confidence="0.999985">
where yi is the J x 1 vector of responses, xi is the
J xp design matrix of covariates (including the in-
tercept) with fixed coefficients β E Rp×1, z is the
random structure matrix defined by J x q covari-
ates with random coefficients bi E Rq×1, and ci is
the vector of residuals (in our example, p = 4 and
q = 4). By packing together vectors and matrices
indexed over groups i, we can rewrite the model
in a general form (Baayen et al., 2008), which can
represent any possible crossed-effects and random
structures defined over them allowing, at the same
time, for a compact model specification:
</bodyText>
<equation confidence="0.946664666666667">
y = XTβ + ZTb + c (3)
E ti
N(0,σ2I), b ti N(0, σ2E), b + E
</equation>
<bodyText confidence="0.999199583333333">
1Although mixed ordinal models (Tutz and Hennevogl,
1996) are in principle more appropriate to target quality
judgements, in our preliminary investigations mixed linear
models showed a significantly higher predictive power.
2Here, A, B and C represent three generic error classes.
Their actual number in a given experimental setting will de-
pend on the granularity of the reference error taxonomy.
where E is the relative variance-covariance q x q
matrix of the random effects (now q = 4I), σ2
is the variance of the per-observation term c, the
symbol + denotes independence of random vari-
ables, and N indicates the multivariate normal dis-
tribution. While b, σ, and E are estimated via max-
imum likelihood, the single random intercept and
slope values for each group are calculated subse-
quently. They are referred to as Best Linear Un-
biased Predictors (BLUPS) and, formally, are not
parameters of the model.
The significance of the contribution of each sin-
gle parameter (e.g. single entries of E) to the
goodness of fit can be tested via likelihood ratio.
In this way, both the fixed and random effect struc-
ture of the model can be investigated with respect
to its actual necessity to the model.
</bodyText>
<sectionHeader confidence="0.999225" genericHeader="method">
4 Dataset
</sectionHeader>
<bodyText confidence="0.999798684210526">
For our analysis we used a dataset that covers
three translation directions, corresponding to En-
glish to Chinese, Arabic, and Russian. An inter-
national organization provided us a set of English
sentences together with their translation produced
by two anonymous MT systems. For each evalu-
ation item (source sentence and two MT outputs)
three experts were asked to assign quality scores to
the MT outputs, and a fourth expert was asked to
annotate translation errors. The four experts, who
were all professional translators native in the ex-
amined target languages, were carefully trained to
get acquainted with the evaluation guidelines and
the annotation tool specifically developed for these
evaluation tasks (Girardi et al., 2014). The anno-
tation process was carried out in parallel by all an-
notators over one week, resulting in a final dataset
composed of 312 evaluation items for the ENZH
direction, 393 for ENAR, and 437 for ENRU.
</bodyText>
<subsectionHeader confidence="0.998999">
4.1 Quality Judgements
</subsectionHeader>
<bodyText confidence="0.999936181818182">
Quality judgements were collected by asking the
three experts to rate each automatic translation
according to a 1-5 Likert scale, where 1 means
“incomprehensible translation” and 5 means “per-
fect translation”. The distribution of the collected
annotations with respect to each quality score is
shown in Figure 1. As we can see, this distri-
bution reflects different levels of perceived qual-
ity across languages. ENZH, for instance, has the
highest number of low quality scores (1 and 2),
while ENRU has the highest number of high qual-
</bodyText>
<page confidence="0.991324">
1646
</page>
<figureCaption confidence="0.997843">
Figure 1: Distribution of quality scores.
Figure 2: Class specific inter-annotator agreement.
</figureCaption>
<figure confidence="0.999020608695652">
100% 5
80% 4
60% 3
40% 2
20%
0%
1
ENZH ENAR ENRU
FLEI55&apos; KAPPA
-­‐0.1
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
1 2 3 4 5
QUALITY SCORES
ENZH
ENAR
ENRU
</figure>
<bodyText confidence="0.987428266666667">
ity scores (4 and 5).
Table 1 shows the average of all the qual-
ity scores assigned by each annototator as well
as the average score obtained for each MT sys-
tem. These values demonstrate the variability
of annotators and systems. A particularly high
variability among human judges is observed for
the ENAR language direction (also reflected by
the inter-annotator agreement scores discussed be-
low), while ENZH shows the highest variability
between systems. As we will see in §5.1, we suc-
cessfully cope with this variability by considering
systems and annotators as random effects, which
allow the regression models to abstract from these
differences.
</bodyText>
<table confidence="0.99988725">
Ann1 Ann2 Ann3 Sys1 Sys2
ENZH 2.38 2.69 2.21 2.29 2.56
ENAR 2.76 2.77 1.84 2.39 2.53
ENRU 2.82 2.72 2.96 2.87 2.79
</table>
<tableCaption confidence="0.884297">
Table 1: Average quality scores per annotator and
per system.
</tableCaption>
<bodyText confidence="0.999442470588235">
Inter-annotator agreement was computed using
the Fleiss’ kappa coefficient (Fleiss, 1971), and re-
sulted in 22.70% for ENZH, 5.24% for ENAR, and
21.80% for ENRU. While for ENZH and ENRU
the results fall in the range of “fair” agreement
(Landis and Koch, 1977), for ENAR only “slight”
agreement is reached, reflecting the higher anno-
tators’ variability evidenced in Table 1.
A more fine-grained agreement analysis is pre-
sented in Figure 2, where the kappa values are
given for each score class. In general we no-
tice a lower agreement on the intermediate quality
scores, while annotators tend to agree on very bad
and, even more, on good translations. In partic-
ular, we see that the agreement for ENAR is sys-
tematically lower than the values measured for the
other languages on all the score classes.
</bodyText>
<subsectionHeader confidence="0.687485">
4.2 Error Annotation
</subsectionHeader>
<bodyText confidence="0.999909870967742">
This evaluation task was carried out by one ex-
pert for each language direction, who was asked to
identify the type of errors present in the MT output
and to mark their position in the text. Since the fo-
cus of our work is the analysis method rather than
the definition of an ideal error taxonomy, for the
difficult language directions addressed we opted
for the following general error classes, partially
overlapping with (Vilar et al., 2006): i) reordering
errors, ii) lexicon errors (including wrong lexical
choices and extra words), iii) missing words, iv)
morphology errors.
Figure 3 shows the distribution of the errors in
terms of affected tokens (words) for each error
type. Since token counts for Chinese are not word-
based but character-based, for readability purposes
the number of errors counted for Chinese trans-
lations have been divided by 2.5. Note also that
morphological errors annotated for ENZH involve
only 13 characters and thus are not visible in the
plot. The total number of errors amounts to 16,320
characters for ENZH, 4,926 words for ENAR, and
5,965 words for ENRU.
This distribution highlights some differences
between languages directions. For example, trans-
lations into Arabic and Russian present several
morphology errors, while word reordering is the
most frequent issue for translations into Chinese.
As we will see in §5.1, error frequency does not
give a direct indication of their impact on trasla-
tion quality judgements.
</bodyText>
<subsectionHeader confidence="0.994747">
4.3 Automatic Metrics
</subsectionHeader>
<bodyText confidence="0.9808838">
In our investigation we consider three popular au-
tomatic metrics: sentence-level BLEU (Lin and
Och, 2004), TER (Snover et al., 2006), and GTM
(Turian et al., 2003). We compute all automatic
scores by relying on a single reference and by
</bodyText>
<page confidence="0.990846">
1647
</page>
<figureCaption confidence="0.999958">
Figure 3: Distribution of error types.
</figureCaption>
<bodyText confidence="0.999799454545455">
means of standard packages. In particular, auto-
matic scores on Chinese are computed at the char-
acter level. Moreover, as we use metrics as re-
sponse variables for our regression models, we
compute all metrics at the sentence level. The
overall mean scores for all systems and languages
are reported in Table 2. Differences in systems’
performance can be observed for all language
pairs; as we will observe in §5.2 such variability
explains the effectiveness of considering the MT
systems as a random effect.
</bodyText>
<table confidence="0.9976396">
BLEU TER GTM
Sys1 Sys2 Sys1 Sys2 Sy1 Sys2
ENZH 27.95 44.11 64.52 48.13 62.15 72.30
ENAR 19.63 25.25 68.83 63.99 47.20 52.33
ENRU 27.10 31.07 60.89 54.41 53.74 56.41
</table>
<tableCaption confidence="0.996434">
Table 2: Overall automatic scores per system.
</tableCaption>
<sectionHeader confidence="0.999197" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999956166666667">
To assess the impact of translation errors on MT
quality we perform two sets of experiments. The
first set (§5.1) addresses the relation between er-
rors and human quality judgements. The sec-
ond set (§5.2) focuses on the relation between er-
rors and automatic metrics. In both cases, be-
fore measuring the impact of different errors on
the response variable (respectively quality judge-
ments and metrics), we validate the effectiveness
of mixed linear models by comparing their predic-
tion capability with other methods.
In all experiments, error counts of each category
were normalized into percentages with respect to
the sentence length and mapped in a logarithmic
scale. In this way, we basically assume that the
impact of errors tends to saturate above a given
threshold, hypothesis that also results in better fits
by our models.3 Notice that while the chosen log-
</bodyText>
<footnote confidence="0.925143">
3In other words, we assume that human sensitivity to er-
</footnote>
<bodyText confidence="0.994544428571429">
10 base is easy to interpret, linear models can im-
plicitly adjust it. Our analysis makes use of mixed
linear models incorporating, as fixed effects, the
four types of errors (lex, miss, morph and reo) and
their pairwise interactions (the product of the sin-
gle error log counts), while their random struc-
ture depends on each specific experiment. For
the experiments we rely on the R language (R
Core Team, 2013) implementation of linear mixed
model in the lme4 library (Bates et al., 2014).
We assess the quality of our mixed linear mod-
els (MLM) by comparing their prediction capabil-
ity with a sequence of simpler linear models in-
cluding only fixed effects. In particular, we built
five univariate models and two multivariate mod-
els. The univariate models use as covariates, re-
spectively, the sum of all error types (baseline),
and each of the four types of errors (lex, miss,
morph and reo). The two multivariate models in-
clude all the four error types, considering them
without interactions (FLM w/o Interact.) and with
interactions (FLM).
Prediction performance is computed in terms of
Mean Absolute Error (MAE),4 which we estimate
by averaging over 1,000 random splits of the data
in 90% training and 10% test. In particular, for the
human quality classes we pick the integer between
1-5 that is closest to the predicted value.
</bodyText>
<subsectionHeader confidence="0.935058">
5.1 Errors vs. Quality Judgements
</subsectionHeader>
<bodyText confidence="0.945416533333333">
The response variable we target in this experiment
is the quality score produced by human annotators.
Our measurements follow a typical within-subject
design in which all the 3 annotators are exposed
to the same conditions (levels of the independent
variables), corresponding in our case to perfectly
balanced observations from 2 MT systems and N
sentences. This setting results in repeated or clus-
tered observations (thus violating independence)
corresponding to groups which naturally identify
possible random effects,5 namely the annotators
(3 levels with 2xN observations each), the systems
(2 levels and 3xN observations each), and the sen-
rors follows a log-scale law: e.g. more sensitive to variations
in the interval [1-10] that in the interval [30-40].
</bodyText>
<footnote confidence="0.907244625">
4MAE is calculated as the average of the absolute errors
|fi − yi|, where fi is the prediction of the model and yi the
true value for the ith instance. As it is a measure of error,
lower MAE scores indicate that our predictions are closer to
the true values of each test instance.
5In all our experiments, random effects are limited to ran-
dom shifts since preliminary experiments also including ran-
dom slopes did not provide consistent results.
</footnote>
<figure confidence="0.992756357142857">
4000
3500
3000
2500
LEX
MISS
MORPH
REO
2000
1500
1000
500
0
ENZH ENAR ENRU
</figure>
<page confidence="0.952886">
1648
</page>
<table confidence="0.999564444444444">
Model ENZH ENAR ENRU
baseline 0.58 0.73 0.67
lex 0.67 0.78 0.72
miss 0.72 0.89 0.74
morph 0.72 0.89 0.74
reo 0.70 0.82 0.76
FLM w/o Interact. 0.59 0.77 0.65
FLM 0.57 0.72 0.63
MLM 0.53 0.61 0.61
</table>
<tableCaption confidence="0.931536">
Table 3: Prediction capability of human judge-
ments (MAE).
</tableCaption>
<bodyText confidence="0.996994828571429">
tences (N levels with 6 observations each). In prin-
ciple, such random effects permit to remove sys-
tematic biases of individual annotators, single sys-
tems and even single sentences, which are mod-
elled as random variables sampled from distinct
populations.
Table 3 shows a comparison of the prediction
capability of the mixed model6 with simpler ap-
proaches. While the good performance achieved
by our strong baseline cannot be outperformed
by separately counting the number of errors of a
single type, lower MAE results are obtained by
methods based on multivariate analysis. Among
them, FLM brings the first consistent improve-
ments over the baseline by considering error in-
teractions, while MLM leads to the lowest MAE
due to the addition of random effects. The impor-
tance of random effects is particularly evidenced
by ENAR (12 points below the baseline). Indeed,
as discussed in §4.1, for this language combina-
tion human annotators show the lowest agreement
score. This variability, which hides the smaller
differences in systems’ behaviour, demonstrates
the importance of accounting for the erratic fac-
tors that might influence empirical observations in
a given setting. The good performance achieved
by MLM, combined with their high descriptive
power,7 motivates their adoption in our study.
Concerning the analysis of error impact, Ta-
ble 4 shows the statistically significant coefficients
for the full-fledged MLM models for each trans-
lation direction. By default, all reported coeffi-
cients have p-values &lt; 10−4, while those marked
with • and o have respectively p-values &lt; 10−3
and &lt; 10−2. Slope coefficients basically show
</bodyText>
<footnote confidence="0.993638857142857">
6Note that the mixed model used in prediction does not in-
clude the random effect on sentences since the training sam-
ples do not guarantee sufficient observations for each test sen-
tence.
7Note that the strong baseline used for comparison is not
capable to describe the contribution of the different error
types.
</footnote>
<table confidence="0.998984333333333">
Error ENZH ENAR ENRU
Intercept 4.29 3.79• 4.21
lex -1.27 -0.96 -1.12
miss -1.76 -0.90 -1.30
morph -0.48◦ -0.83 -0.51
reo -1.01 -0.75 -0.18
lex:miss 1.00 0.39 0.68
lex:morph - 0.29 0.32
lex:reo 0.50 0.21 -
miss:morph - 0.35 -
miss:reo 0.54 0.33 -
morph:reo - 0.37 -
</table>
<tableCaption confidence="0.991132">
Table 4: Effect of translation errors on MT qual-
</tableCaption>
<bodyText confidence="0.986411868421053">
ity perception on all judged sentences. Reported
coefficients (Q) are all statistically significant with
p &lt; 10−4, except those marked with • (p &lt; 10−3),
and a (p &lt; 10−2).
the impact of different error types (alone and in
combination) on human quality scores. Those that
are not statistically significant are omitted as they
do not increase the fitting capability of our model.
As can be seen from the table, such impact varies
across the different language combinations. While
for ENZH and ENRU miss is the error having
the highest impact (highest decrement with respect
to the intercept), the most problematic error for
ENAR is lex. It is interesting to observe that pos-
itive values for error combinations indicate that
their combined impact is lower that the sum of the
impact of the single errors. For instance, while for
ENZH a one-step increment in lex and miss errors
would respectively cause a reduction in the human
judgement of 1.27 and 1.76, their occurrence in
the same sentence would be discounted by 1.00.
This would result in a global judgement of 2.26
(4.29 -1.27 -1.76 +1.00) instead of 1.26. While
for ENAR this phenomenon can be observed for
all error combinations, such discount effects are
not always significant for the other two language
pairs. The existence of discount effects of various
magnitude associated to the different error com-
binations is a novel finding made possible by the
adoption of mixed-effect models.
Another interesting observation is that, in con-
trast with the common belief that the most fre-
quent errors have the highest impact on human
quality judgements, our experiments do not re-
veal such strict correlation (at least for the exam-
ined language pairs). For instance, for ENZH and
ENRU the impact of miss errors is higher than the
impact of other more frequent issues.
</bodyText>
<page confidence="0.979801">
1649
</page>
<table confidence="0.9978208">
Model BLEU score ENZH TER ENRU ENZH GTM ENRU
ENZH ENAR ENRU ENAR ENAR
baseline 12.4 9.8 12.2 15.7 13.4 14.4 9.8 10.6 11.5
lex 12.9 10.4 13.0 16.3 13.8 14.9 9.7 10.9 12.1
miss 13.8 10.5 14.1 17.3 14.2 16.4 10.5 11.1 13.2
morph 13.9 10.3 13.6 17.5 13.8 16.3 10.5 10.9 13.1
reo 13.7 10.5 14.0 17.4 14.1 16.3 10.4 11.1 13.1
FLM w/o Interact. 12.9 9.9 12.2 16.3 13.5 14.4 9.7 10.7 11.7
FLM 12.3 9.7 12.1 15.6 13.4 14.3 9.4 10.6 11.6
MLM 10.8 9.5 12.0 14.7 13.0 14.2 8.9 10.5 11.6
</table>
<tableCaption confidence="0.9997">
Table 5: Prediction capability of BLEU score, TER and GTM (MAE).
</tableCaption>
<subsectionHeader confidence="0.997458">
5.2 Errors vs. Automatic Metrics
</subsectionHeader>
<bodyText confidence="0.99759550617284">
In this experiment, the response variable is an au-
tomatic metric which is computed on a sample of
MT outputs (which are again perfectly balanced
over systems and sentences) and a set of reference
translations. As no subjects are involved in the ex-
periment, random variability is assumed to come
from the involved systems, the tested sentences,
and the unknown missing link between the covari-
ates (error types) and the response variable which
is modelled by the residual noise. Notice that,
in this case, the random effect on the sentences
also incorporates in some sense the randomness
of the corresponding reference translations, which
are themselves representatives of larger samples.
The prediction capability of the mixed model,
in comparison with the simpler ones, is reported
in Table 5. Also in this case, the low MAE
achieved by the baseline is out of the reach of uni-
variate methods. Again, small improvements are
brought by FLM when considering error interac-
tions, whereas the most visible gains are achieved
by MLM due to their control of random effects.
This is more evident for some language combina-
tions and can be explained by the differences in
systems’ performance, a variability factor easily
absorbed by random effects. Indeed, the largest
MAE decrements over the baseline are always ob-
served for ENZH (for which the overall mean re-
sults reported in Table 2 show the largest dif-
ferences) and the smallest decrements relate to
language/metric combinations where systems’ be-
haviour is more similar (e.g. ENRU/GTM).
Concerning the analysis of error impact, Table
6 shows how different error types (alone and in
combination) influence performance results mea-
sured with automatic metrics. To ease interpre-
tation of the reported figures we also show Pear-
son and Spearman correlations of each set of coef-
ficients (excluding intercept estimates) with their
corresponding coefficients reported in Table 4. In
fact, our primary interest in this experiment is to
see which metrics show a sensitivity to specific er-
ror types similar to human perception. As we can
see, the coefficients for each metric significantly
vary depending on the language, for the simple
reason that also the distribution and co-occurrence
of errors vary significantly across the different lan-
guages and MT systems. Remarkably, for some
translation directions, some of the metrics show
a sensitivity to errors that is very similar to that
of human judges. In particular, BLEU for ENZH
and ENAR, and GTM for ENZH show a very high
correlation with the human sensitivity to transla-
tion errors, with Pearson correlation coefficient ≥
0.97. For ENRU, the best Pearson correlation is
instead achieved by TER (-0.78).
Besides these general observations, a closer
look at the reported scores brings additional find-
ings. In three cases (BLEU for ENZH, GTM for
ENZH and ENAR) the analysed metrics are most
sensitive to the same error type that has the high-
est influence on human judgements (according to
Table 4, these are miss for ENZH and ENRU, lex
for ENAR). On the contrary, in one case (TER for
ENZH) the analysed metric is insensitive to the er-
ror type (miss) which has the highest impact on hu-
man quality scores. From a practical point of view,
these remarks provide useful indications about the
appropriateness of each metric to highlight the de-
ficiencies of a specific system and to measure im-
provements targeting specific issues. As a rule of
thumb, for instance, to measure improvements of
an ENZH system with respect to missing words,
it would be more advisable to use BLEU or GTM
instead of TER.8
8Note that this conclusion holds for our data sample, in
which different types of errors co-occur and only one refer-
ence translation is available. In such conditions, our regres-
sion model shows that TER is not influenced by miss errors in
a statistically significant way. This does not mean that TER
is insensitive to missing words when occurring in isolation,
</bodyText>
<page confidence="0.955876">
1650
</page>
<table confidence="0.999536733333333">
Error BLEU score ENZH TER ENRU ENZH GTM ENRU
ENZH ENAR ENRU ENAR ENAR
Intercept 60.550 38.45◦ 51.73 32.410 52.25• 33.4• 83.57◦ 60.11• 75.38
lex -18.78 -9.25 -16.57 16.87 9.66 18.45 -13.63 -7.60 -16.13
miss -23.20 -10.41 -6.75 - - 8.24 -14.87 - -5.98
morph - -9.97 -12.65 - 8.90 11.41 - -6.60 -10.42
reo -13.27 -7.62 -10.57 14.44 9.81 6.39 -7.29 -5.50 -7.03
lex:miss 14.37 4.97◦ - - - - 8.24• - -
lex:morph - - 5.27• - - -5.22◦ - - 4.92
lex:reo 8.57 3.57◦ 5.40• -7.24◦ -4.35◦ - 5.46 3.22◦ 3.650
miss:morph - 4.44◦ - - - - - - -
miss:reo 6.74◦ - 4.30 - - -6.38◦ 5.07◦ - 4.71◦
morph:reo - 3.81• - - -4.97• - - 2.57◦ -
Pearson 0.98 0.97 0.70 -0.58 -0.78 -0.78 0.98 0.78 0.74
Spearman 0.97 0.91 0.73 -0.57 -0.59 -0.80 0.97 0.59 0.76
</table>
<tableCaption confidence="0.983017">
Table 6: Effect of translation errors on BLEU score, TER and GTM on all judged sentences and correla-
</tableCaption>
<bodyText confidence="0.973451076923077">
tion with their corresponding effects on human quality scores (from Table 4). Reported coefficients (Q)
are statistically significant with p G 10−4, except those marked with • (p G 10−3), , (p G 10−2) and
✷(p G 10−1).
Similar considerations also apply to the analysis
of the impact of error combinations. The same dis-
count effects that we noticed when analysing the
impact of errors’ co-occurrence on human percep-
tion (§5.1) are evidenced, with different degrees of
sensitivity, by the automatic metrics. While some
of them substantially reflect human response (e.g.
BLEU and GTM for ENZH), in some cases we
observe either the insensitivity to specific combi-
nations (mostly for ENAR), or a higher sensitivity
compared to the values measured for human as-
sessors (mostly for ENRU, where the impact of
miss:reo combinations is discounted - hence un-
derestimated - by all the metrics).
Despite such small differences, the coherence of
our results with previous findings (§5.1) suggests
the reliability of the applied method. Complet-
ing the picture along the side of the MT evalua-
tion triangle which connects error annotations and
automatic metrics, our findings contribute to shed
light on the existing relationships between transla-
tion errors, their interaction, and the sensitivity of
widely used automatic metrics.
</bodyText>
<sectionHeader confidence="0.999384" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.99994775">
We investigated the MT evaluation triangle (hav-
ing as corners automatic metrics, human quality
judgements and error annotations) along the two
less explored sides, namely: i) the relation be-
tween MT errors and human quality judgements
but that TER becomes less sensitive to such errors when they
co-occur with other types of errors. Overall, our experiments
show that when MT outputs contain more than one error type,
automatic metrics show different levels of sensitivity to each
specific error type.
and ii) the relation between MT errors and auto-
matic metrics. To this aim we employed a ro-
bust statistical analysis framework based on lin-
ear mixed-effects models (the first contribution of
the paper), which have a higher descriptive power
than simpler methods based on the raw count of
translation errors and are less artificial compared
to previous statistically-grounded approaches.
Working on three translation directions having
Chinese, Arabic and Russian as target (our second
contribution), we analysed error-annotated trans-
lations considering the impact of specific errors
(alone and in combination) and accounting for the
variability of the experimental set-up that origi-
nated our empirical observations. This led us to
interesting findings specific to each language pair
(third contribution). Concerning the relation be-
tween MT errors and quality judgements, we have
shown that: i) the frequency of errors of a given
type does not correlate with human preferences,
ii) errors having the highest impact can be pre-
cisely isolated and iii) the impact of error inter-
actions is often subject to measurable and previ-
ously unknown “discount” effects. Concerning the
relation between MT errors and automatic met-
rics (BLEU, TER and GTM), our analysis evi-
denced significant differences in the sensitivity of
each metric to different error types. Such differ-
ences provide useful indications about the most
appropriate metric to assess system improvements
with respect to specific weaknesses. If learning
from errors is a crucial aspect of improving exper-
tise, our method and the resulting empirical find-
ings represent a significant contribution towards a
</bodyText>
<page confidence="0.969639">
1651
</page>
<bodyText confidence="0.999691">
more informed approach to system development,
improvement and evaluation.
</bodyText>
<sectionHeader confidence="0.995581" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.997544">
This work has been partially supported by the EC-
funded project MateCat (ICT-2011.4.2-287688).
</bodyText>
<sectionHeader confidence="0.998787" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999318841584158">
Harald R. Baayen, Douglas J. Davidson, and Dou-
glas M. Bates. 2008. Mixed-effects modeling with
crossed random effects for subjects and items. Jour-
nal of memory and language, 59(4):390–412.
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An automatic metric for MT evaluation with im-
proved correlation with human judgments. In Pro-
ceedings of the ACL Workshop on Intrinsic and Ex-
trinsic Evaluation Measures for Machine Transla-
tion and/or Summarization, pages 65–72, Ann Ar-
bor, Michigan, June. Association for Computational
Linguistics.
Douglas Bates, Martin Maechler, Ben Bolker, and
Steven Walker, 2014. lme4: Linear mixed-effects
models using Eigen and S4. R package version 1.1-
6.
Fr´ed´eric Blain, Jean Senellart, Holger Schwenk, Mirko
Plitt, and Johann Roturier. 2011. Qualitative analy-
sis of post-editing for high quality machine transla-
tion. In Asia-Pacific Association for Machine Trans-
lation (AAMT), editor, Machine Translation Summit
XIII, Xiamen (China), 19-23 sept.
Ondˇrej Bojar, Christian Buck, Chris Callison-Burch,
Christian Federmann, Barry Haddow, Philipp
Koehn, Christof Monz, Matt Post, Radu Soricut, and
Lucia Specia. 2013. Findings of the 2013 Work-
shop on Statistical Machine Translation. In Pro-
ceedings of the Eighth Workshop on Statistical Ma-
chine Translation, pages 1–44, Sofia, Bulgaria, Au-
gust. Association for Computational Linguistics.
Herbert H. Clark. 1973. The language-as-fixed-effect
fallacy: A critique of language statistics in psycho-
logical research. Journal of verbal learning and ver-
bal behavior, 12(4):335–359.
Mireia Farris, Marta R. Costa-juss`a, and Maja
Popovi´c. 2012. Study and correlation analysis of
linguistic, perceptual, and automatic machine trans-
lation evaluations. J. Am. Soc. Inf. Sci. Technol.,
63(1):174–184, January.
Mireia Farris Cabeceran, Marta Ruiz Costa-Juss`a,
Jos´e Bernardo Mari˜no Acebal, Jos´e Adri´an
Rodr´ıguez Fonollosa, et al. 2010. Linguistic-based
evaluation criteria to identify statistical machine
translation errors. In Proceedings of the 14th
Annual Conference of the European Association for
Machine Translation (EAMT).
Mary Flanagan. 1994. Error classification for mt eval-
uation. In Technology Partnerships for Crossing the
Language Barrier: Proceedings of the First Confer-
ence of the Association for Machine Translation in
the Americas, pages 65–72.
Joseph L. Fleiss. 1971. Measuring nominal scale
agreement among many raters. Psychological Bul-
letin, 76(5).
Christian Girardi, Luisa Bentivogli, Mohammad Amin
Farajian, and Marcello Federico. 2014. Mt-equal:
a toolkit for human assessment of machine trans-
lation output. In Proceedings of COLING 2014,
the 25th International Conference on Computational
Linguistics: System Demonstrations, pages 120–
123, Dublin, Ireland, August. Dublin City Univer-
sity and Association for Computational Linguistics.
Sharon Goldwater, Daniel Jurafsky, and Christopher D.
Manning. 2010. Which words are hard to rec-
ognize? prosodic, lexical, and disfluency factors
that increase speech recognition error rates. Speech
Communication, 52(3):181–200.
Spence Green, Jeffrey Heer, and Christopher D. Man-
ning. 2013. The efficacy of human post-editing for
language translation. In Proceedings of the SIGCHI
Conference on Human Factors in Computing Sys-
tems, pages 439–448. ACM.
Stephan Greene and Philip Resnik. 2009. More than
words: Syntactic packaging and implicit sentiment.
In Proceedings of Human Language Technologies:
The 2009 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, NAACL ’09, pages 503–511, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Katrin Kirchhoff, Daniel Capurro, and Anne M. Turner.
2013. A conjoint analysis framework for evaluating
user preferences in machine translation. Machine
Translation, pages 1–17.
Richard J. Landis and Gary G. Koch. 1977. The mea-
surement of observer agreement for categorical data.
Biometrics, 33 (1):159–174.
Samuel L¨aubli, Mark Fishel, Gary Massey, Maureen
Ehrensberger-Dow, and Martin Volk. 2013. Assess-
ing Post-Editing Efficiency in a Realistic Translation
Environment. In Michel Simard Sharon O’Brien
and Lucia Specia (eds.), editors, Proceedings of MT
Summit XIV Workshop on Post-editing Technology
and Practice, pages 83–91, Nice, France.
Chin-Yew Lin and Franz Josef Och. 2004. Orange:
a method for evaluating automatic evaluation met-
rics for machine translation. In Proceedings of Col-
ing 2004, pages 501–507, Geneva, Switzerland, Aug
23–Aug 27. COLING.
Arle Lommel, Aljoscha Burchardt, Maja Popovi´c, Kim
Harris, Eleftherios Avramidis, and Hans Uszkoreit.
</reference>
<page confidence="0.850939">
1652
</page>
<reference confidence="0.99952908974359">
2014. Using a new analytic measure for the anno-
tation and analysis of mt errors on real data. In
Proceedings of the 17th Conference of the Euro-
pean Association for Machine Translation (EAMT),
Dubrovnik, Croatia, June.
Sharon O’Brien. 2011. Cognitive Explorations of
Translation. Bloomsbury Studies in Translation.
Bloomsbury Academic.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. Bleu: a method for automatic
evaluation of machine translation. Research Report
RC22176, IBM Research Division, Thomas J. Wat-
son Research Center.
Maja Popovi´c and Hermann Ney. 2011. Towards au-
tomatic error analysis of machine translation output.
Comput. Linguist., 37(4):657–688, December.
Maja Popovic, Eleftherios Avramidis, Aljoscha Bur-
chardt, Sabine Hunsicker, Sven Schmeier, Cindy
Tscherwinka, David Vilar, and Hans Uszkoreit.
2013. Learning from human judgments of machine
translation output. In Proceedings of the MT Summit
XIV. Proceedings of MT Summit XIV.
Maja Popovi´c, Arle Lommel, Aljoscha Burchardt,
Eleftherios Avramidis, and Hans Uszkoreit. 2014.
Relations between different types of post-editing op-
erations, cognitive effort and temporal effort. In
Proceedings of the 17th Conference of the Euro-
pean Association for Machine Translation (EAMT),
Dubrovnik, Croatia, June.
Maja Popovic. 2012. Class error rates for evaluation
of machine translation output. In Proceedings of the
Seventh Workshop on Statistical Machine Transla-
tion, pages 71–75, Montr´eal, Canada, June. Associ-
ation for Computational Linguistics.
R Core Team, 2013. R: A Language and Environment
for Statistical Computing. R Foundation for Statis-
tical Computing, Vienna, Austria.
Nick Ruiz and Marcello Federico. 2014. Assessing the
Impact of Speech Recognition Errors on Machine
Translation Quality. In 11th Conference of the As-
sociation for Machine Translation in the Americas
(AMTA), Vancouver, BC, Canada.
Matthew Snover, Bonnie Dorr, Rich Schwartz, Linnea
Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In 5th Conference of the Association for Machine
Translation in the Americas (AMTA), Boston, Mas-
sachusetts, August.
Sara Stymne and Lars Ahrenberg. 2012. On
the practice of error analysis for machine trans-
lation evaluation. In Nicoletta Calzolari (Con-
ference Chair), Khalid Choukri, Thierry Declerck,
Mehmet Uur Doan, Bente Maegaard, Joseph Mar-
iani, Jan Odijk, and Stelios Piperidis, editors, Pro-
ceedings of the Eight International Conference on
Language Resources and Evaluation (LREC’12), Is-
tanbul, Turkey, may. European Language Resources
Association (ELRA).
Irina Temnikova. 2010. Cognitive evaluation approach
for a controlled language post-editing experiment.
In Nicoletta Calzolari (Conference Chair), Khalid
Choukri, Bente Maegaard, Joseph Mariani, Jan
Odijk, Stelios Piperidis, Mike Rosner, and Daniel
Tapias, editors, Proceedings of the Seventh Interna-
tional Conference on Language Resources and Eval-
uation (LREC’10), Valletta, Malta, may. European
Language Resources Association (ELRA).
Joseph P. Turian, I. Dan Melamed, and Luke Shen.
2003. Evaluation of machine translation and its
evaluation. In Proceedings of the MT Summit IX.
Gerhard Tutz and Wolfgang Hennevogl. 1996. Ran-
dom effects in ordinal regression models. Computa-
tional Statistics &amp; Data Analysis, 22(5):537–557.
David Vilar, Jia Xu, Luis Fernando dHaro, and Her-
mann Ney. 2006. Error analysis of statistical ma-
chine translation output. In Proceedings of the Fifth
International Conference on Language Resources
and Evaluation (LREC’06), pages 697–702.
</reference>
<page confidence="0.977869">
1653
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.328467">
<title confidence="0.99914">Assessing the Impact of Translation on Machine Translation Quality with Mixed-effects Models</title>
<author confidence="0.819491">Marcello Federico</author>
<author confidence="0.819491">Matteo Negri</author>
<author confidence="0.819491">Luisa Bentivogli</author>
<author confidence="0.819491">Marco FBK</author>
<affiliation confidence="0.586361">Via Sommarive 18, 38123 Trento,</affiliation>
<abstract confidence="0.9998275">Learning from errors is a crucial aspect of improving expertise. Based on this notion, we discuss a robust statistical framework for analysing the impact of different error types on machine translation (MT) output quality. Our approach is based on linear mixed-effects models, which allow the analysis of error-annotated MT output taking into account the variability inherent to the specific experimental setting from which the empirical observations are drawn. Our experiments are carried out on different language pairs involving Chinese, Arabic and Russian as target languages. Interesting findings are reported, concerning the impact of different error types both at the level of human perception of quality and with respect to performance results measured with automatic metrics.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Harald R Baayen</author>
<author>Douglas J Davidson</author>
<author>Douglas M Bates</author>
</authors>
<title>Mixed-effects modeling with crossed random effects for subjects and items.</title>
<date>2008</date>
<journal>Journal of memory and language,</journal>
<pages>59--4</pages>
<contexts>
<context position="13597" citStr="Baayen et al., 2008" startWordPosition="2116" endWordPosition="2119">ion model parameters, as random intercepts and random slopes, and have the same form of the generic error component of the model, i.e. normally distributed with zero mean and unknown variance. As random effects introduce hidden variables, mixed models are trained with Expectation Maximization, while significance testing is performed via likelihood-ratio (LR) tests. In this work we employ mixed linear models to measure the influence of different MT error types, expressed as continuous fixed effects, on quality 1645 judgements or on automatic quality metrics.1 We illustrate mixed linear models (Baayen et al., 2008) by referring to our analysis, which addresses the relationships between a quality metric (y) and different types of errors (e.g. A, B, and C)2 observed at the sentence level. For the sake of simplicity, we assume to have balanced repeated observations for one single crossed effect. That is, we have i E 11, ... , I} MT systems (our groups) each of which translated the same j E 11, ... , J} test sentences. Our response variable yij - a numeric quality score - is computed on each (sentence, system) pair, and we aim to investigate its relationship with error statistics available for each MT outpu</context>
<context position="15264" citStr="Baayen et al., 2008" startWordPosition="2436" endWordPosition="2439">stem and the residual error. Borrowing the notation from (Green et al., 2013), we conveniently rewrite (1) in the group-wise arranged matrix notation: yi = xTi β + zTi bi + Ci (2) where yi is the J x 1 vector of responses, xi is the J xp design matrix of covariates (including the intercept) with fixed coefficients β E Rp×1, z is the random structure matrix defined by J x q covariates with random coefficients bi E Rq×1, and ci is the vector of residuals (in our example, p = 4 and q = 4). By packing together vectors and matrices indexed over groups i, we can rewrite the model in a general form (Baayen et al., 2008), which can represent any possible crossed-effects and random structures defined over them allowing, at the same time, for a compact model specification: y = XTβ + ZTb + c (3) E ti N(0,σ2I), b ti N(0, σ2E), b + E 1Although mixed ordinal models (Tutz and Hennevogl, 1996) are in principle more appropriate to target quality judgements, in our preliminary investigations mixed linear models showed a significantly higher predictive power. 2Here, A, B and C represent three generic error classes. Their actual number in a given experimental setting will depend on the granularity of the reference error </context>
</contexts>
<marker>Baayen, Davidson, Bates, 2008</marker>
<rawString>Harald R. Baayen, Douglas J. Davidson, and Douglas M. Bates. 2008. Mixed-effects modeling with crossed random effects for subjects and items. Journal of memory and language, 59(4):390–412.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satanjeev Banerjee</author>
<author>Alon Lavie</author>
</authors>
<title>METEOR: An automatic metric for MT evaluation with improved correlation with human judgments.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization,</booktitle>
<pages>65--72</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="1369" citStr="Banerjee and Lavie, 2005" startWordPosition="198" endWordPosition="201">. Our experiments are carried out on different language pairs involving Chinese, Arabic and Russian as target languages. Interesting findings are reported, concerning the impact of different error types both at the level of human perception of quality and with respect to performance results measured with automatic metrics. 1 Introduction The dominant statistical approach to machine translation (MT) is based on learning from large amounts of parallel data and tuning the resulting models on reference-based metrics that can be computed automatically, such as BLEU (Papineni et al., 2001), METEOR (Banerjee and Lavie, 2005), TER (Snover et al., 2006), GTM (Turian et al., 2003). Despite the steady progress in the last two decades, especially for few well resourced translation directions having English as target language, this way to approach the problem is quickly reaching a performance plateau. One reason is that parallel data are a source of reliable information but, alone, limit systems knowledge to observed positive examples (i.e. how a sentence should be translated) without explicitly modelling any notion of error (i.e. how a sentence should not be translated). Another reason is that, as a development and ev</context>
</contexts>
<marker>Banerjee, Lavie, 2005</marker>
<rawString>Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, pages 65–72, Ann Arbor, Michigan, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douglas Bates</author>
<author>Martin Maechler</author>
<author>Ben Bolker</author>
<author>Steven Walker</author>
</authors>
<title>lme4: Linear mixed-effects models using Eigen and S4. R package version 1.1-6.</title>
<date>2014</date>
<contexts>
<context position="23954" citStr="Bates et al., 2014" startWordPosition="3885" endWordPosition="3888">also results in better fits by our models.3 Notice that while the chosen log3In other words, we assume that human sensitivity to er10 base is easy to interpret, linear models can implicitly adjust it. Our analysis makes use of mixed linear models incorporating, as fixed effects, the four types of errors (lex, miss, morph and reo) and their pairwise interactions (the product of the single error log counts), while their random structure depends on each specific experiment. For the experiments we rely on the R language (R Core Team, 2013) implementation of linear mixed model in the lme4 library (Bates et al., 2014). We assess the quality of our mixed linear models (MLM) by comparing their prediction capability with a sequence of simpler linear models including only fixed effects. In particular, we built five univariate models and two multivariate models. The univariate models use as covariates, respectively, the sum of all error types (baseline), and each of the four types of errors (lex, miss, morph and reo). The two multivariate models include all the four error types, considering them without interactions (FLM w/o Interact.) and with interactions (FLM). Prediction performance is computed in terms of </context>
</contexts>
<marker>Bates, Maechler, Bolker, Walker, 2014</marker>
<rawString>Douglas Bates, Martin Maechler, Ben Bolker, and Steven Walker, 2014. lme4: Linear mixed-effects models using Eigen and S4. R package version 1.1-6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fr´ed´eric Blain</author>
<author>Jean Senellart</author>
<author>Holger Schwenk</author>
<author>Mirko Plitt</author>
<author>Johann Roturier</author>
</authors>
<title>Qualitative analysis of post-editing for high quality machine translation.</title>
<date>2011</date>
<booktitle>In Asia-Pacific Association for Machine Translation (AAMT), editor, Machine Translation Summit XIII,</booktitle>
<location>Xiamen</location>
<contexts>
<context position="7494" citStr="Blain et al., 2011" startWordPosition="1155" endWordPosition="1158">ems, in different target languages and domains, have been used to determine the quality of translations according to the amount of errors encountered (Popovic et al., 2013), to design new automatic metrics that take into consideration human annotations (Popovic, 2012; Bojar et al., 2013), and to train classifiers that can automatic identify fine-grained errors in the MT output (Popovi´c and Ney, 2011). The impact of edit operations on post-editors’ productivity, which implicitly connects the severity of different errors to human activity, has also been studied (Temnikova, 2010; O’Brien, 2011; Blain et al., 2011), but few attempts have been made to explicitly model how fine-grained errors impact on human quality judgements and automatic metrics. Recently, the relation between different error types, their frequency, and human quality judgements has been investigated from a descriptive standpoint in (Lommel et al., 2014; Popovi´c et al., 2014). In both works, however, the underlying assumption that the most frequent error has also the largest impact on quality perception is not verified (in general and, least of all, across language pairs, domains, MT systems and post-editors). Another limitation of the</context>
</contexts>
<marker>Blain, Senellart, Schwenk, Plitt, Roturier, 2011</marker>
<rawString>Fr´ed´eric Blain, Jean Senellart, Holger Schwenk, Mirko Plitt, and Johann Roturier. 2011. Qualitative analysis of post-editing for high quality machine translation. In Asia-Pacific Association for Machine Translation (AAMT), editor, Machine Translation Summit XIII, Xiamen (China), 19-23 sept.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ondˇrej Bojar</author>
<author>Christian Buck</author>
<author>Chris Callison-Burch</author>
<author>Christian Federmann</author>
<author>Barry Haddow</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Matt Post</author>
<author>Radu Soricut</author>
<author>Lucia Specia</author>
</authors>
<date>2013</date>
<booktitle>Findings of the 2013 Workshop on Statistical Machine Translation. In Proceedings of the Eighth Workshop on Statistical Machine Translation,</booktitle>
<pages>1--44</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="7163" citStr="Bojar et al., 2013" startWordPosition="1103" endWordPosition="1106"> covering different levels of granularity (Flanagan, 1994; Vilar et al., 2006; Farr´us Cabeceran et al., 2010; Stymne and Ahrenberg, 2012; Lommel et al., 2014) have been recently complemented by investigations on how to exploit error annotations for diagnostic purposes. Error annotations of sentences produced by different MT systems, in different target languages and domains, have been used to determine the quality of translations according to the amount of errors encountered (Popovic et al., 2013), to design new automatic metrics that take into consideration human annotations (Popovic, 2012; Bojar et al., 2013), and to train classifiers that can automatic identify fine-grained errors in the MT output (Popovi´c and Ney, 2011). The impact of edit operations on post-editors’ productivity, which implicitly connects the severity of different errors to human activity, has also been studied (Temnikova, 2010; O’Brien, 2011; Blain et al., 2011), but few attempts have been made to explicitly model how fine-grained errors impact on human quality judgements and automatic metrics. Recently, the relation between different error types, their frequency, and human quality judgements has been investigated from a desc</context>
</contexts>
<marker>Bojar, Buck, Callison-Burch, Federmann, Haddow, Koehn, Monz, Post, Soricut, Specia, 2013</marker>
<rawString>Ondˇrej Bojar, Christian Buck, Chris Callison-Burch, Christian Federmann, Barry Haddow, Philipp Koehn, Christof Monz, Matt Post, Radu Soricut, and Lucia Specia. 2013. Findings of the 2013 Workshop on Statistical Machine Translation. In Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 1–44, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Herbert H Clark</author>
</authors>
<title>The language-as-fixed-effect fallacy: A critique of language statistics in psychological research. Journal of verbal learning and verbal behavior,</title>
<date>1973</date>
<pages>12--4</pages>
<contexts>
<context position="12919" citStr="Clark, 1973" startWordPosition="2013" endWordPosition="2014">rimental design aspects that hinder the applicability of conventional regression models. These are, in particular, the use of repeated and/or clustered observations that introduce correlations in the response variable that clearly violate the independence and homoscedasticity assumptions of conventional linear, ANOVA, and logistic regression models. Significance testing with mixed models is in general more powerful, i.e. less prone to Type II Errors, and also permits to reduce the chance of Type I Errors in within-subject designs, which are prone to the “fallacy of languageas-a-fixed-effect” (Clark, 1973). Random effects can be directly associated to the regression model parameters, as random intercepts and random slopes, and have the same form of the generic error component of the model, i.e. normally distributed with zero mean and unknown variance. As random effects introduce hidden variables, mixed models are trained with Expectation Maximization, while significance testing is performed via likelihood-ratio (LR) tests. In this work we employ mixed linear models to measure the influence of different MT error types, expressed as continuous fixed effects, on quality 1645 judgements or on autom</context>
</contexts>
<marker>Clark, 1973</marker>
<rawString>Herbert H. Clark. 1973. The language-as-fixed-effect fallacy: A critique of language statistics in psychological research. Journal of verbal learning and verbal behavior, 12(4):335–359.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mireia Farris</author>
<author>Marta R Costa-juss`a</author>
<author>Maja Popovi´c</author>
</authors>
<title>Study and correlation analysis of linguistic, perceptual, and automatic machine translation evaluations.</title>
<date>2012</date>
<journal>J. Am. Soc. Inf. Sci. Technol.,</journal>
<volume>63</volume>
<issue>1</issue>
<marker>Farris, Costa-juss`a, Popovi´c, 2012</marker>
<rawString>Mireia Farris, Marta R. Costa-juss`a, and Maja Popovi´c. 2012. Study and correlation analysis of linguistic, perceptual, and automatic machine translation evaluations. J. Am. Soc. Inf. Sci. Technol., 63(1):174–184, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mireia Farris Cabeceran</author>
<author>Marta Ruiz Costa-Juss`a</author>
<author>Jos´e Bernardo Mari˜no Acebal</author>
<author>Jos´e Adri´an Rodr´ıguez Fonollosa</author>
</authors>
<title>Linguistic-based evaluation criteria to identify statistical machine translation errors.</title>
<date>2010</date>
<booktitle>In Proceedings of the 14th Annual Conference of the European Association for Machine Translation (EAMT).</booktitle>
<marker>Cabeceran, Costa-Juss`a, Acebal, Fonollosa, 2010</marker>
<rawString>Mireia Farris Cabeceran, Marta Ruiz Costa-Juss`a, Jos´e Bernardo Mari˜no Acebal, Jos´e Adri´an Rodr´ıguez Fonollosa, et al. 2010. Linguistic-based evaluation criteria to identify statistical machine translation errors. In Proceedings of the 14th Annual Conference of the European Association for Machine Translation (EAMT).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mary Flanagan</author>
</authors>
<title>Error classification for mt evaluation. In Technology Partnerships for Crossing the Language Barrier:</title>
<date>1994</date>
<booktitle>Proceedings of the First Conference of the Association for Machine Translation in the Americas,</booktitle>
<pages>65--72</pages>
<contexts>
<context position="6601" citStr="Flanagan, 1994" startWordPosition="1015" endWordPosition="1016">ndeed, the proposed statistical analysis framework represents an ideal instrument to: i) identify translation issues having the highest impact on human perception of quality and ii) choose the most appropriate evaluation metric to measure progress towards their solution. 2 Related Work Error analysis, as a way to identify systems’ weaknesses and define priorities for their improvement, is gaining increasing interest in the MT community (Popovi´c and Ney, 2011; Popovic et al., 2013). Along this direction, the initial efforts to develop error taxonomies covering different levels of granularity (Flanagan, 1994; Vilar et al., 2006; Farr´us Cabeceran et al., 2010; Stymne and Ahrenberg, 2012; Lommel et al., 2014) have been recently complemented by investigations on how to exploit error annotations for diagnostic purposes. Error annotations of sentences produced by different MT systems, in different target languages and domains, have been used to determine the quality of translations according to the amount of errors encountered (Popovic et al., 2013), to design new automatic metrics that take into consideration human annotations (Popovic, 2012; Bojar et al., 2013), and to train classifiers that can au</context>
</contexts>
<marker>Flanagan, 1994</marker>
<rawString>Mary Flanagan. 1994. Error classification for mt evaluation. In Technology Partnerships for Crossing the Language Barrier: Proceedings of the First Conference of the Association for Machine Translation in the Americas, pages 65–72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph L Fleiss</author>
</authors>
<title>Measuring nominal scale agreement among many raters.</title>
<date>1971</date>
<journal>Psychological Bulletin,</journal>
<volume>76</volume>
<issue>5</issue>
<contexts>
<context position="19328" citStr="Fleiss, 1971" startWordPosition="3116" endWordPosition="3117">R language direction (also reflected by the inter-annotator agreement scores discussed below), while ENZH shows the highest variability between systems. As we will see in §5.1, we successfully cope with this variability by considering systems and annotators as random effects, which allow the regression models to abstract from these differences. Ann1 Ann2 Ann3 Sys1 Sys2 ENZH 2.38 2.69 2.21 2.29 2.56 ENAR 2.76 2.77 1.84 2.39 2.53 ENRU 2.82 2.72 2.96 2.87 2.79 Table 1: Average quality scores per annotator and per system. Inter-annotator agreement was computed using the Fleiss’ kappa coefficient (Fleiss, 1971), and resulted in 22.70% for ENZH, 5.24% for ENAR, and 21.80% for ENRU. While for ENZH and ENRU the results fall in the range of “fair” agreement (Landis and Koch, 1977), for ENAR only “slight” agreement is reached, reflecting the higher annotators’ variability evidenced in Table 1. A more fine-grained agreement analysis is presented in Figure 2, where the kappa values are given for each score class. In general we notice a lower agreement on the intermediate quality scores, while annotators tend to agree on very bad and, even more, on good translations. In particular, we see that the agreement</context>
</contexts>
<marker>Fleiss, 1971</marker>
<rawString>Joseph L. Fleiss. 1971. Measuring nominal scale agreement among many raters. Psychological Bulletin, 76(5).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian Girardi</author>
<author>Luisa Bentivogli</author>
<author>Mohammad Amin Farajian</author>
<author>Marcello Federico</author>
</authors>
<title>Mt-equal: a toolkit for human assessment of machine translation output.</title>
<date>2014</date>
<booktitle>In Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: System Demonstrations,</booktitle>
<pages>120--123</pages>
<institution>Dublin City University and Association for Computational Linguistics.</institution>
<location>Dublin, Ireland,</location>
<contexts>
<context position="17413" citStr="Girardi et al., 2014" startWordPosition="2791" endWordPosition="2794">e, Arabic, and Russian. An international organization provided us a set of English sentences together with their translation produced by two anonymous MT systems. For each evaluation item (source sentence and two MT outputs) three experts were asked to assign quality scores to the MT outputs, and a fourth expert was asked to annotate translation errors. The four experts, who were all professional translators native in the examined target languages, were carefully trained to get acquainted with the evaluation guidelines and the annotation tool specifically developed for these evaluation tasks (Girardi et al., 2014). The annotation process was carried out in parallel by all annotators over one week, resulting in a final dataset composed of 312 evaluation items for the ENZH direction, 393 for ENAR, and 437 for ENRU. 4.1 Quality Judgements Quality judgements were collected by asking the three experts to rate each automatic translation according to a 1-5 Likert scale, where 1 means “incomprehensible translation” and 5 means “perfect translation”. The distribution of the collected annotations with respect to each quality score is shown in Figure 1. As we can see, this distribution reflects different levels o</context>
</contexts>
<marker>Girardi, Bentivogli, Farajian, Federico, 2014</marker>
<rawString>Christian Girardi, Luisa Bentivogli, Mohammad Amin Farajian, and Marcello Federico. 2014. Mt-equal: a toolkit for human assessment of machine translation output. In Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: System Demonstrations, pages 120– 123, Dublin, Ireland, August. Dublin City University and Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>Daniel Jurafsky</author>
<author>Christopher D Manning</author>
</authors>
<title>Which words are hard to recognize? prosodic, lexical, and disfluency factors that increase speech recognition error rates.</title>
<date>2010</date>
<journal>Speech Communication,</journal>
<volume>52</volume>
<issue>3</issue>
<contexts>
<context position="10538" citStr="Goldwater et al., 2010" startWordPosition="1639" endWordPosition="1642">nd Ney, 2011; Farr´us et al., 2012). Using two different error taxonomies, both works show that the sum of the errors has a high correlation with BLEU and TER scores. Similar to the aforementioned works addressing the impact of MT errors on human perception, these studies disregard error interactions, and their possible impact on automatic scores. To overcome these issues, we propose a robust statistic analysis framework based on mixedeffects models, which have been successfully applied to several NLP problems such as sentiment analysis (Greene and Resnik, 2009), automatic speech recognition (Goldwater et al., 2010), and spoken language translation (Ruiz and Federico, 2014). Despite their effectiveness, the use of mixed-effects models in the MT field is rather recent and limited to the analysis of human posteditions (Green et al., 2013; L¨aubli et al., 2013). In both studies, the goal was to evaluate the impact of post-editing on the quality and productivity of human translation assuming an ANOVA mixed model for a between-subject design, in which human translators either post-edited or translated the same texts. Our scenario is rather different as we employ mixed models to measure the influence of differ</context>
</contexts>
<marker>Goldwater, Jurafsky, Manning, 2010</marker>
<rawString>Sharon Goldwater, Daniel Jurafsky, and Christopher D. Manning. 2010. Which words are hard to recognize? prosodic, lexical, and disfluency factors that increase speech recognition error rates. Speech Communication, 52(3):181–200.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Spence Green</author>
<author>Jeffrey Heer</author>
<author>Christopher D Manning</author>
</authors>
<title>The efficacy of human post-editing for language translation.</title>
<date>2013</date>
<booktitle>In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems,</booktitle>
<pages>439--448</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="10762" citStr="Green et al., 2013" startWordPosition="1676" endWordPosition="1679">T errors on human perception, these studies disregard error interactions, and their possible impact on automatic scores. To overcome these issues, we propose a robust statistic analysis framework based on mixedeffects models, which have been successfully applied to several NLP problems such as sentiment analysis (Greene and Resnik, 2009), automatic speech recognition (Goldwater et al., 2010), and spoken language translation (Ruiz and Federico, 2014). Despite their effectiveness, the use of mixed-effects models in the MT field is rather recent and limited to the analysis of human posteditions (Green et al., 2013; L¨aubli et al., 2013). In both studies, the goal was to evaluate the impact of post-editing on the quality and productivity of human translation assuming an ANOVA mixed model for a between-subject design, in which human translators either post-edited or translated the same texts. Our scenario is rather different as we employ mixed models to measure the influence of different MT error types - expressed as continuous fixed effects - on quality judgements and automatic quality metrics. Mixed models, having the capability to absorb random variability due to the specific experimental set-up, prov</context>
<context position="14721" citStr="Green et al., 2013" startWordPosition="2328" endWordPosition="2331">r, and we aim to investigate its relationship with error statistics available for each MT output, namely Aij, Bij and Cij. A (possible) linear mixed model for our study would be: yij = β0 + β1 Aij + β2 Bij + β3 Cij + (1) b0,i + b1,iAij + b2,iBij + b3,iCi + Eij The model is split into two lines on purpose. The first line shows the fixed effect component, that is intercept (β0) and slopes (β1, β2, β3) for each error type. The second line specifies the random structure of the model, which includes random intercept and slopes for each MT system and the residual error. Borrowing the notation from (Green et al., 2013), we conveniently rewrite (1) in the group-wise arranged matrix notation: yi = xTi β + zTi bi + Ci (2) where yi is the J x 1 vector of responses, xi is the J xp design matrix of covariates (including the intercept) with fixed coefficients β E Rp×1, z is the random structure matrix defined by J x q covariates with random coefficients bi E Rq×1, and ci is the vector of residuals (in our example, p = 4 and q = 4). By packing together vectors and matrices indexed over groups i, we can rewrite the model in a general form (Baayen et al., 2008), which can represent any possible crossed-effects and ra</context>
</contexts>
<marker>Green, Heer, Manning, 2013</marker>
<rawString>Spence Green, Jeffrey Heer, and Christopher D. Manning. 2013. The efficacy of human post-editing for language translation. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, pages 439–448. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Greene</author>
<author>Philip Resnik</author>
</authors>
<title>More than words: Syntactic packaging and implicit sentiment.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, NAACL ’09,</booktitle>
<pages>503--511</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="10483" citStr="Greene and Resnik, 2009" startWordPosition="1632" endWordPosition="1635">total error frequencies and automatic scores (Popovi´c and Ney, 2011; Farr´us et al., 2012). Using two different error taxonomies, both works show that the sum of the errors has a high correlation with BLEU and TER scores. Similar to the aforementioned works addressing the impact of MT errors on human perception, these studies disregard error interactions, and their possible impact on automatic scores. To overcome these issues, we propose a robust statistic analysis framework based on mixedeffects models, which have been successfully applied to several NLP problems such as sentiment analysis (Greene and Resnik, 2009), automatic speech recognition (Goldwater et al., 2010), and spoken language translation (Ruiz and Federico, 2014). Despite their effectiveness, the use of mixed-effects models in the MT field is rather recent and limited to the analysis of human posteditions (Green et al., 2013; L¨aubli et al., 2013). In both studies, the goal was to evaluate the impact of post-editing on the quality and productivity of human translation assuming an ANOVA mixed model for a between-subject design, in which human translators either post-edited or translated the same texts. Our scenario is rather different as we</context>
</contexts>
<marker>Greene, Resnik, 2009</marker>
<rawString>Stephan Greene and Philip Resnik. 2009. More than words: Syntactic packaging and implicit sentiment. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, NAACL ’09, pages 503–511, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Kirchhoff</author>
<author>Daniel Capurro</author>
<author>Anne M Turner</author>
</authors>
<title>A conjoint analysis framework for evaluating user preferences in machine translation. Machine Translation,</title>
<date>2013</date>
<pages>1--17</pages>
<contexts>
<context position="8536" citStr="Kirchhoff et al., 2013" startWordPosition="1316" endWordPosition="1319"> also the largest impact on quality perception is not verified (in general and, least of all, across language pairs, domains, MT systems and post-editors). Another limitation of the proposed (univariate) analysis lies in the fact that it exclusively focuses on error types taken in isolation. This simplification excludes the possibility that humans, when assigning a global quality score to a translation, may be influenced not only by the error types but also by their interaction. The implications of such possibility call for a multivariate analysis capable to model also error interactions. In (Kirchhoff et al., 2013), a statisticallygrounded approach based on conjoint analysis has been used to investigate users’ reactions to different types of translation errors. According to 1644 their results, word order is the most dispreferred error type, and the count of the errors in a sentence is not a good predictor of users’ preferences. Though more sophisticated than methods based on rough error counts, the conjoint model is bound to several constraints that limit its usability. In particular, the application of conjoint analysis in this context requires to: i) operate with semi-automatically created (hence arti</context>
</contexts>
<marker>Kirchhoff, Capurro, Turner, 2013</marker>
<rawString>Katrin Kirchhoff, Daniel Capurro, and Anne M. Turner. 2013. A conjoint analysis framework for evaluating user preferences in machine translation. Machine Translation, pages 1–17.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard J Landis</author>
<author>Gary G Koch</author>
</authors>
<title>The measurement of observer agreement for categorical data.</title>
<date>1977</date>
<journal>Biometrics,</journal>
<volume>33</volume>
<pages>1--159</pages>
<contexts>
<context position="19497" citStr="Landis and Koch, 1977" startWordPosition="3146" endWordPosition="3149">ill see in §5.1, we successfully cope with this variability by considering systems and annotators as random effects, which allow the regression models to abstract from these differences. Ann1 Ann2 Ann3 Sys1 Sys2 ENZH 2.38 2.69 2.21 2.29 2.56 ENAR 2.76 2.77 1.84 2.39 2.53 ENRU 2.82 2.72 2.96 2.87 2.79 Table 1: Average quality scores per annotator and per system. Inter-annotator agreement was computed using the Fleiss’ kappa coefficient (Fleiss, 1971), and resulted in 22.70% for ENZH, 5.24% for ENAR, and 21.80% for ENRU. While for ENZH and ENRU the results fall in the range of “fair” agreement (Landis and Koch, 1977), for ENAR only “slight” agreement is reached, reflecting the higher annotators’ variability evidenced in Table 1. A more fine-grained agreement analysis is presented in Figure 2, where the kappa values are given for each score class. In general we notice a lower agreement on the intermediate quality scores, while annotators tend to agree on very bad and, even more, on good translations. In particular, we see that the agreement for ENAR is systematically lower than the values measured for the other languages on all the score classes. 4.2 Error Annotation This evaluation task was carried out by</context>
</contexts>
<marker>Landis, Koch, 1977</marker>
<rawString>Richard J. Landis and Gary G. Koch. 1977. The measurement of observer agreement for categorical data. Biometrics, 33 (1):159–174.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Samuel L¨aubli</author>
<author>Mark Fishel</author>
<author>Gary Massey</author>
<author>Maureen Ehrensberger-Dow</author>
<author>Martin Volk</author>
</authors>
<title>Assessing Post-Editing Efficiency in a Realistic Translation Environment.</title>
<date>2013</date>
<booktitle>Proceedings of MT Summit XIV Workshop on Post-editing Technology and Practice,</booktitle>
<pages>83--91</pages>
<editor>In Michel Simard Sharon O’Brien and Lucia Specia (eds.), editors,</editor>
<location>Nice, France.</location>
<marker>L¨aubli, Fishel, Massey, Ehrensberger-Dow, Volk, 2013</marker>
<rawString>Samuel L¨aubli, Mark Fishel, Gary Massey, Maureen Ehrensberger-Dow, and Martin Volk. 2013. Assessing Post-Editing Efficiency in a Realistic Translation Environment. In Michel Simard Sharon O’Brien and Lucia Specia (eds.), editors, Proceedings of MT Summit XIV Workshop on Post-editing Technology and Practice, pages 83–91, Nice, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
<author>Franz Josef Och</author>
</authors>
<title>Orange: a method for evaluating automatic evaluation metrics for machine translation.</title>
<date>2004</date>
<booktitle>In Proceedings of Coling</booktitle>
<pages>501--507</pages>
<publisher>COLING.</publisher>
<location>Geneva, Switzerland,</location>
<contexts>
<context position="21640" citStr="Lin and Och, 2004" startWordPosition="3496" endWordPosition="3499">n the plot. The total number of errors amounts to 16,320 characters for ENZH, 4,926 words for ENAR, and 5,965 words for ENRU. This distribution highlights some differences between languages directions. For example, translations into Arabic and Russian present several morphology errors, while word reordering is the most frequent issue for translations into Chinese. As we will see in §5.1, error frequency does not give a direct indication of their impact on traslation quality judgements. 4.3 Automatic Metrics In our investigation we consider three popular automatic metrics: sentence-level BLEU (Lin and Och, 2004), TER (Snover et al., 2006), and GTM (Turian et al., 2003). We compute all automatic scores by relying on a single reference and by 1647 Figure 3: Distribution of error types. means of standard packages. In particular, automatic scores on Chinese are computed at the character level. Moreover, as we use metrics as response variables for our regression models, we compute all metrics at the sentence level. The overall mean scores for all systems and languages are reported in Table 2. Differences in systems’ performance can be observed for all language pairs; as we will observe in §5.2 such variab</context>
</contexts>
<marker>Lin, Och, 2004</marker>
<rawString>Chin-Yew Lin and Franz Josef Och. 2004. Orange: a method for evaluating automatic evaluation metrics for machine translation. In Proceedings of Coling 2004, pages 501–507, Geneva, Switzerland, Aug 23–Aug 27. COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arle Lommel</author>
<author>Aljoscha Burchardt</author>
<author>Maja Popovi´c</author>
<author>Kim Harris</author>
<author>Eleftherios Avramidis</author>
<author>Hans Uszkoreit</author>
</authors>
<title>Using a new analytic measure for the annotation and analysis of mt errors on real data.</title>
<date>2014</date>
<booktitle>In Proceedings of the 17th Conference of the European Association for Machine Translation (EAMT),</booktitle>
<location>Dubrovnik, Croatia,</location>
<marker>Lommel, Burchardt, Popovi´c, Harris, Avramidis, Uszkoreit, 2014</marker>
<rawString>Arle Lommel, Aljoscha Burchardt, Maja Popovi´c, Kim Harris, Eleftherios Avramidis, and Hans Uszkoreit. 2014. Using a new analytic measure for the annotation and analysis of mt errors on real data. In Proceedings of the 17th Conference of the European Association for Machine Translation (EAMT), Dubrovnik, Croatia, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon O’Brien</author>
</authors>
<title>Cognitive Explorations of Translation. Bloomsbury Studies in Translation.</title>
<date>2011</date>
<publisher>Bloomsbury Academic.</publisher>
<marker>O’Brien, 2011</marker>
<rawString>Sharon O’Brien. 2011. Cognitive Explorations of Translation. Bloomsbury Studies in Translation. Bloomsbury Academic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2001</date>
<journal>Research Report RC22176, IBM Research Division, Thomas J. Watson Research Center.</journal>
<contexts>
<context position="1334" citStr="Papineni et al., 2001" startWordPosition="192" endWordPosition="196">empirical observations are drawn. Our experiments are carried out on different language pairs involving Chinese, Arabic and Russian as target languages. Interesting findings are reported, concerning the impact of different error types both at the level of human perception of quality and with respect to performance results measured with automatic metrics. 1 Introduction The dominant statistical approach to machine translation (MT) is based on learning from large amounts of parallel data and tuning the resulting models on reference-based metrics that can be computed automatically, such as BLEU (Papineni et al., 2001), METEOR (Banerjee and Lavie, 2005), TER (Snover et al., 2006), GTM (Turian et al., 2003). Despite the steady progress in the last two decades, especially for few well resourced translation directions having English as target language, this way to approach the problem is quickly reaching a performance plateau. One reason is that parallel data are a source of reliable information but, alone, limit systems knowledge to observed positive examples (i.e. how a sentence should be translated) without explicitly modelling any notion of error (i.e. how a sentence should not be translated). Another reas</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2001</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2001. Bleu: a method for automatic evaluation of machine translation. Research Report RC22176, IBM Research Division, Thomas J. Watson Research Center.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maja Popovi´c</author>
<author>Hermann Ney</author>
</authors>
<title>Towards automatic error analysis of machine translation output.</title>
<date>2011</date>
<journal>Comput. Linguist.,</journal>
<volume>37</volume>
<issue>4</issue>
<marker>Popovi´c, Ney, 2011</marker>
<rawString>Maja Popovi´c and Hermann Ney. 2011. Towards automatic error analysis of machine translation output. Comput. Linguist., 37(4):657–688, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maja Popovic</author>
<author>Eleftherios Avramidis</author>
<author>Aljoscha Burchardt</author>
<author>Sabine Hunsicker</author>
<author>Sven Schmeier</author>
<author>Cindy Tscherwinka</author>
<author>David Vilar</author>
<author>Hans Uszkoreit</author>
</authors>
<title>Learning from human judgments of machine translation output.</title>
<date>2013</date>
<booktitle>In Proceedings of the MT Summit XIV. Proceedings of MT Summit XIV.</booktitle>
<contexts>
<context position="6473" citStr="Popovic et al., 2013" startWordPosition="996" endWordPosition="999"> few simpler methods proposed so far. Overall, our study has clear practical implications for MT systems’ development and evaluation. Indeed, the proposed statistical analysis framework represents an ideal instrument to: i) identify translation issues having the highest impact on human perception of quality and ii) choose the most appropriate evaluation metric to measure progress towards their solution. 2 Related Work Error analysis, as a way to identify systems’ weaknesses and define priorities for their improvement, is gaining increasing interest in the MT community (Popovi´c and Ney, 2011; Popovic et al., 2013). Along this direction, the initial efforts to develop error taxonomies covering different levels of granularity (Flanagan, 1994; Vilar et al., 2006; Farr´us Cabeceran et al., 2010; Stymne and Ahrenberg, 2012; Lommel et al., 2014) have been recently complemented by investigations on how to exploit error annotations for diagnostic purposes. Error annotations of sentences produced by different MT systems, in different target languages and domains, have been used to determine the quality of translations according to the amount of errors encountered (Popovic et al., 2013), to design new automatic </context>
</contexts>
<marker>Popovic, Avramidis, Burchardt, Hunsicker, Schmeier, Tscherwinka, Vilar, Uszkoreit, 2013</marker>
<rawString>Maja Popovic, Eleftherios Avramidis, Aljoscha Burchardt, Sabine Hunsicker, Sven Schmeier, Cindy Tscherwinka, David Vilar, and Hans Uszkoreit. 2013. Learning from human judgments of machine translation output. In Proceedings of the MT Summit XIV. Proceedings of MT Summit XIV.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maja Popovi´c</author>
</authors>
<title>Arle Lommel, Aljoscha Burchardt, Eleftherios Avramidis, and Hans Uszkoreit.</title>
<date>2014</date>
<booktitle>In Proceedings of the 17th Conference of the European Association for Machine Translation (EAMT),</booktitle>
<location>Dubrovnik, Croatia,</location>
<marker>Popovi´c, 2014</marker>
<rawString>Maja Popovi´c, Arle Lommel, Aljoscha Burchardt, Eleftherios Avramidis, and Hans Uszkoreit. 2014. Relations between different types of post-editing operations, cognitive effort and temporal effort. In Proceedings of the 17th Conference of the European Association for Machine Translation (EAMT), Dubrovnik, Croatia, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maja Popovic</author>
</authors>
<title>Class error rates for evaluation of machine translation output.</title>
<date>2012</date>
<booktitle>In Proceedings of the Seventh Workshop on Statistical Machine Translation,</booktitle>
<pages>71--75</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montr´eal, Canada,</location>
<contexts>
<context position="7142" citStr="Popovic, 2012" startWordPosition="1101" endWordPosition="1102">rror taxonomies covering different levels of granularity (Flanagan, 1994; Vilar et al., 2006; Farr´us Cabeceran et al., 2010; Stymne and Ahrenberg, 2012; Lommel et al., 2014) have been recently complemented by investigations on how to exploit error annotations for diagnostic purposes. Error annotations of sentences produced by different MT systems, in different target languages and domains, have been used to determine the quality of translations according to the amount of errors encountered (Popovic et al., 2013), to design new automatic metrics that take into consideration human annotations (Popovic, 2012; Bojar et al., 2013), and to train classifiers that can automatic identify fine-grained errors in the MT output (Popovi´c and Ney, 2011). The impact of edit operations on post-editors’ productivity, which implicitly connects the severity of different errors to human activity, has also been studied (Temnikova, 2010; O’Brien, 2011; Blain et al., 2011), but few attempts have been made to explicitly model how fine-grained errors impact on human quality judgements and automatic metrics. Recently, the relation between different error types, their frequency, and human quality judgements has been inv</context>
</contexts>
<marker>Popovic, 2012</marker>
<rawString>Maja Popovic. 2012. Class error rates for evaluation of machine translation output. In Proceedings of the Seventh Workshop on Statistical Machine Translation, pages 71–75, Montr´eal, Canada, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Core Team</author>
</authors>
<title>R: A Language and Environment for Statistical Computing. R Foundation for Statistical Computing,</title>
<date>2013</date>
<location>Vienna, Austria.</location>
<contexts>
<context position="23876" citStr="Team, 2013" startWordPosition="3874" endWordPosition="3875"> of errors tends to saturate above a given threshold, hypothesis that also results in better fits by our models.3 Notice that while the chosen log3In other words, we assume that human sensitivity to er10 base is easy to interpret, linear models can implicitly adjust it. Our analysis makes use of mixed linear models incorporating, as fixed effects, the four types of errors (lex, miss, morph and reo) and their pairwise interactions (the product of the single error log counts), while their random structure depends on each specific experiment. For the experiments we rely on the R language (R Core Team, 2013) implementation of linear mixed model in the lme4 library (Bates et al., 2014). We assess the quality of our mixed linear models (MLM) by comparing their prediction capability with a sequence of simpler linear models including only fixed effects. In particular, we built five univariate models and two multivariate models. The univariate models use as covariates, respectively, the sum of all error types (baseline), and each of the four types of errors (lex, miss, morph and reo). The two multivariate models include all the four error types, considering them without interactions (FLM w/o Interact.</context>
</contexts>
<marker>Team, 2013</marker>
<rawString>R Core Team, 2013. R: A Language and Environment for Statistical Computing. R Foundation for Statistical Computing, Vienna, Austria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nick Ruiz</author>
<author>Marcello Federico</author>
</authors>
<title>Assessing the Impact of Speech Recognition Errors on Machine Translation Quality.</title>
<date>2014</date>
<booktitle>In 11th Conference of the Association for Machine Translation in the Americas (AMTA),</booktitle>
<location>Vancouver, BC,</location>
<contexts>
<context position="10597" citStr="Ruiz and Federico, 2014" startWordPosition="1647" endWordPosition="1650">rror taxonomies, both works show that the sum of the errors has a high correlation with BLEU and TER scores. Similar to the aforementioned works addressing the impact of MT errors on human perception, these studies disregard error interactions, and their possible impact on automatic scores. To overcome these issues, we propose a robust statistic analysis framework based on mixedeffects models, which have been successfully applied to several NLP problems such as sentiment analysis (Greene and Resnik, 2009), automatic speech recognition (Goldwater et al., 2010), and spoken language translation (Ruiz and Federico, 2014). Despite their effectiveness, the use of mixed-effects models in the MT field is rather recent and limited to the analysis of human posteditions (Green et al., 2013; L¨aubli et al., 2013). In both studies, the goal was to evaluate the impact of post-editing on the quality and productivity of human translation assuming an ANOVA mixed model for a between-subject design, in which human translators either post-edited or translated the same texts. Our scenario is rather different as we employ mixed models to measure the influence of different MT error types - expressed as continuous fixed effects </context>
</contexts>
<marker>Ruiz, Federico, 2014</marker>
<rawString>Nick Ruiz and Marcello Federico. 2014. Assessing the Impact of Speech Recognition Errors on Machine Translation Quality. In 11th Conference of the Association for Machine Translation in the Americas (AMTA), Vancouver, BC, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie Dorr</author>
<author>Rich Schwartz</author>
<author>Linnea Micciulla</author>
<author>John Makhoul</author>
</authors>
<title>A study of translation edit rate with targeted human annotation.</title>
<date>2006</date>
<booktitle>In 5th Conference of the Association for Machine Translation in the Americas (AMTA),</booktitle>
<location>Boston, Massachusetts,</location>
<contexts>
<context position="1396" citStr="Snover et al., 2006" startWordPosition="203" endWordPosition="206">t on different language pairs involving Chinese, Arabic and Russian as target languages. Interesting findings are reported, concerning the impact of different error types both at the level of human perception of quality and with respect to performance results measured with automatic metrics. 1 Introduction The dominant statistical approach to machine translation (MT) is based on learning from large amounts of parallel data and tuning the resulting models on reference-based metrics that can be computed automatically, such as BLEU (Papineni et al., 2001), METEOR (Banerjee and Lavie, 2005), TER (Snover et al., 2006), GTM (Turian et al., 2003). Despite the steady progress in the last two decades, especially for few well resourced translation directions having English as target language, this way to approach the problem is quickly reaching a performance plateau. One reason is that parallel data are a source of reliable information but, alone, limit systems knowledge to observed positive examples (i.e. how a sentence should be translated) without explicitly modelling any notion of error (i.e. how a sentence should not be translated). Another reason is that, as a development and evaluation criterion, automat</context>
<context position="21667" citStr="Snover et al., 2006" startWordPosition="3501" endWordPosition="3504">ber of errors amounts to 16,320 characters for ENZH, 4,926 words for ENAR, and 5,965 words for ENRU. This distribution highlights some differences between languages directions. For example, translations into Arabic and Russian present several morphology errors, while word reordering is the most frequent issue for translations into Chinese. As we will see in §5.1, error frequency does not give a direct indication of their impact on traslation quality judgements. 4.3 Automatic Metrics In our investigation we consider three popular automatic metrics: sentence-level BLEU (Lin and Och, 2004), TER (Snover et al., 2006), and GTM (Turian et al., 2003). We compute all automatic scores by relying on a single reference and by 1647 Figure 3: Distribution of error types. means of standard packages. In particular, automatic scores on Chinese are computed at the character level. Moreover, as we use metrics as response variables for our regression models, we compute all metrics at the sentence level. The overall mean scores for all systems and languages are reported in Table 2. Differences in systems’ performance can be observed for all language pairs; as we will observe in §5.2 such variability explains the effectiv</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>Matthew Snover, Bonnie Dorr, Rich Schwartz, Linnea Micciulla, and John Makhoul. 2006. A study of translation edit rate with targeted human annotation. In 5th Conference of the Association for Machine Translation in the Americas (AMTA), Boston, Massachusetts, August.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Sara Stymne</author>
<author>Lars Ahrenberg</author>
</authors>
<title>On the practice of error analysis for machine translation evaluation.</title>
<date>2012</date>
<booktitle>Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC’12),</booktitle>
<editor>In Nicoletta Calzolari (Conference Chair), Khalid Choukri, Thierry Declerck, Mehmet Uur Doan, Bente Maegaard, Joseph Mariani, Jan Odijk, and Stelios Piperidis, editors,</editor>
<location>Istanbul, Turkey,</location>
<contexts>
<context position="6681" citStr="Stymne and Ahrenberg, 2012" startWordPosition="1026" endWordPosition="1030">al instrument to: i) identify translation issues having the highest impact on human perception of quality and ii) choose the most appropriate evaluation metric to measure progress towards their solution. 2 Related Work Error analysis, as a way to identify systems’ weaknesses and define priorities for their improvement, is gaining increasing interest in the MT community (Popovi´c and Ney, 2011; Popovic et al., 2013). Along this direction, the initial efforts to develop error taxonomies covering different levels of granularity (Flanagan, 1994; Vilar et al., 2006; Farr´us Cabeceran et al., 2010; Stymne and Ahrenberg, 2012; Lommel et al., 2014) have been recently complemented by investigations on how to exploit error annotations for diagnostic purposes. Error annotations of sentences produced by different MT systems, in different target languages and domains, have been used to determine the quality of translations according to the amount of errors encountered (Popovic et al., 2013), to design new automatic metrics that take into consideration human annotations (Popovic, 2012; Bojar et al., 2013), and to train classifiers that can automatic identify fine-grained errors in the MT output (Popovi´c and Ney, 2011). </context>
</contexts>
<marker>Stymne, Ahrenberg, 2012</marker>
<rawString>Sara Stymne and Lars Ahrenberg. 2012. On the practice of error analysis for machine translation evaluation. In Nicoletta Calzolari (Conference Chair), Khalid Choukri, Thierry Declerck, Mehmet Uur Doan, Bente Maegaard, Joseph Mariani, Jan Odijk, and Stelios Piperidis, editors, Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC’12), Istanbul, Turkey, may. European Language Resources Association (ELRA).</rawString>
</citation>
<citation valid="false">
<authors>
<author>Irina Temnikova</author>
</authors>
<title>Cognitive evaluation approach for a controlled language post-editing experiment.</title>
<date>2010</date>
<booktitle>Proceedings of the Seventh International Conference on Language Resources and Evaluation (LREC’10),</booktitle>
<editor>In Nicoletta Calzolari (Conference Chair), Khalid Choukri, Bente Maegaard, Joseph Mariani, Jan Odijk, Stelios Piperidis, Mike Rosner, and Daniel Tapias, editors,</editor>
<location>Valletta, Malta,</location>
<contexts>
<context position="7458" citStr="Temnikova, 2010" startWordPosition="1151" endWordPosition="1152">es produced by different MT systems, in different target languages and domains, have been used to determine the quality of translations according to the amount of errors encountered (Popovic et al., 2013), to design new automatic metrics that take into consideration human annotations (Popovic, 2012; Bojar et al., 2013), and to train classifiers that can automatic identify fine-grained errors in the MT output (Popovi´c and Ney, 2011). The impact of edit operations on post-editors’ productivity, which implicitly connects the severity of different errors to human activity, has also been studied (Temnikova, 2010; O’Brien, 2011; Blain et al., 2011), but few attempts have been made to explicitly model how fine-grained errors impact on human quality judgements and automatic metrics. Recently, the relation between different error types, their frequency, and human quality judgements has been investigated from a descriptive standpoint in (Lommel et al., 2014; Popovi´c et al., 2014). In both works, however, the underlying assumption that the most frequent error has also the largest impact on quality perception is not verified (in general and, least of all, across language pairs, domains, MT systems and post</context>
</contexts>
<marker>Temnikova, 2010</marker>
<rawString>Irina Temnikova. 2010. Cognitive evaluation approach for a controlled language post-editing experiment. In Nicoletta Calzolari (Conference Chair), Khalid Choukri, Bente Maegaard, Joseph Mariani, Jan Odijk, Stelios Piperidis, Mike Rosner, and Daniel Tapias, editors, Proceedings of the Seventh International Conference on Language Resources and Evaluation (LREC’10), Valletta, Malta, may. European Language Resources Association (ELRA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph P Turian</author>
<author>I Dan Melamed</author>
<author>Luke Shen</author>
</authors>
<title>Evaluation of machine translation and its evaluation.</title>
<date>2003</date>
<booktitle>In Proceedings of the MT Summit IX.</booktitle>
<contexts>
<context position="1423" citStr="Turian et al., 2003" startWordPosition="208" endWordPosition="211">rs involving Chinese, Arabic and Russian as target languages. Interesting findings are reported, concerning the impact of different error types both at the level of human perception of quality and with respect to performance results measured with automatic metrics. 1 Introduction The dominant statistical approach to machine translation (MT) is based on learning from large amounts of parallel data and tuning the resulting models on reference-based metrics that can be computed automatically, such as BLEU (Papineni et al., 2001), METEOR (Banerjee and Lavie, 2005), TER (Snover et al., 2006), GTM (Turian et al., 2003). Despite the steady progress in the last two decades, especially for few well resourced translation directions having English as target language, this way to approach the problem is quickly reaching a performance plateau. One reason is that parallel data are a source of reliable information but, alone, limit systems knowledge to observed positive examples (i.e. how a sentence should be translated) without explicitly modelling any notion of error (i.e. how a sentence should not be translated). Another reason is that, as a development and evaluation criterion, automatic metrics provide a holist</context>
<context position="21698" citStr="Turian et al., 2003" startWordPosition="3507" endWordPosition="3510"> characters for ENZH, 4,926 words for ENAR, and 5,965 words for ENRU. This distribution highlights some differences between languages directions. For example, translations into Arabic and Russian present several morphology errors, while word reordering is the most frequent issue for translations into Chinese. As we will see in §5.1, error frequency does not give a direct indication of their impact on traslation quality judgements. 4.3 Automatic Metrics In our investigation we consider three popular automatic metrics: sentence-level BLEU (Lin and Och, 2004), TER (Snover et al., 2006), and GTM (Turian et al., 2003). We compute all automatic scores by relying on a single reference and by 1647 Figure 3: Distribution of error types. means of standard packages. In particular, automatic scores on Chinese are computed at the character level. Moreover, as we use metrics as response variables for our regression models, we compute all metrics at the sentence level. The overall mean scores for all systems and languages are reported in Table 2. Differences in systems’ performance can be observed for all language pairs; as we will observe in §5.2 such variability explains the effectiveness of considering the MT sys</context>
</contexts>
<marker>Turian, Melamed, Shen, 2003</marker>
<rawString>Joseph P. Turian, I. Dan Melamed, and Luke Shen. 2003. Evaluation of machine translation and its evaluation. In Proceedings of the MT Summit IX.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerhard Tutz</author>
<author>Wolfgang Hennevogl</author>
</authors>
<title>Random effects in ordinal regression models.</title>
<date>1996</date>
<journal>Computational Statistics &amp; Data Analysis,</journal>
<volume>22</volume>
<issue>5</issue>
<contexts>
<context position="15534" citStr="Tutz and Hennevogl, 1996" startWordPosition="2484" endWordPosition="2487"> (including the intercept) with fixed coefficients β E Rp×1, z is the random structure matrix defined by J x q covariates with random coefficients bi E Rq×1, and ci is the vector of residuals (in our example, p = 4 and q = 4). By packing together vectors and matrices indexed over groups i, we can rewrite the model in a general form (Baayen et al., 2008), which can represent any possible crossed-effects and random structures defined over them allowing, at the same time, for a compact model specification: y = XTβ + ZTb + c (3) E ti N(0,σ2I), b ti N(0, σ2E), b + E 1Although mixed ordinal models (Tutz and Hennevogl, 1996) are in principle more appropriate to target quality judgements, in our preliminary investigations mixed linear models showed a significantly higher predictive power. 2Here, A, B and C represent three generic error classes. Their actual number in a given experimental setting will depend on the granularity of the reference error taxonomy. where E is the relative variance-covariance q x q matrix of the random effects (now q = 4I), σ2 is the variance of the per-observation term c, the symbol + denotes independence of random variables, and N indicates the multivariate normal distribution. While b,</context>
</contexts>
<marker>Tutz, Hennevogl, 1996</marker>
<rawString>Gerhard Tutz and Wolfgang Hennevogl. 1996. Random effects in ordinal regression models. Computational Statistics &amp; Data Analysis, 22(5):537–557.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Vilar</author>
<author>Jia Xu</author>
<author>Luis Fernando dHaro</author>
<author>Hermann Ney</author>
</authors>
<title>Error analysis of statistical machine translation output.</title>
<date>2006</date>
<booktitle>In Proceedings of the Fifth International Conference on Language Resources and Evaluation (LREC’06),</booktitle>
<pages>697--702</pages>
<contexts>
<context position="2563" citStr="Vilar et al., 2006" startWordPosition="387" endWordPosition="390"> as a development and evaluation criterion, automatic metrics provide a holistic view of systems’ behaviour without identifying the specific issues of a translation. Indeed, the global scores returned by MT evaluation metrics depend on comparisons between translation hypotheses and reference translations, where the causes and the nature of the differences between them are not identified. To cope with these issues and define system improvement priorities, the focus of MT evaluation research is gradually shifting towards profiling systems’ behaviour with respect to various typologies of errors (Vilar et al., 2006; Popovi´c and Ney, 2011; Farr´us et al., 2012, inter alia). This shift has enriched the traditional MT evaluation framework with a new element, that is the actual errors done by a system. Until now, most of the research has focused on the relationship (i.e. the correlation) between two elements of the framework: humans and automatic evaluation metrics. As a new element of the framework, which becomes a sort of “evaluation triangle”, the analysis of error annotations opens interesting research problems related to the relationships between: i) error types and human perception of MT quality and </context>
<context position="6621" citStr="Vilar et al., 2006" startWordPosition="1017" endWordPosition="1020">sed statistical analysis framework represents an ideal instrument to: i) identify translation issues having the highest impact on human perception of quality and ii) choose the most appropriate evaluation metric to measure progress towards their solution. 2 Related Work Error analysis, as a way to identify systems’ weaknesses and define priorities for their improvement, is gaining increasing interest in the MT community (Popovi´c and Ney, 2011; Popovic et al., 2013). Along this direction, the initial efforts to develop error taxonomies covering different levels of granularity (Flanagan, 1994; Vilar et al., 2006; Farr´us Cabeceran et al., 2010; Stymne and Ahrenberg, 2012; Lommel et al., 2014) have been recently complemented by investigations on how to exploit error annotations for diagnostic purposes. Error annotations of sentences produced by different MT systems, in different target languages and domains, have been used to determine the quality of translations according to the amount of errors encountered (Popovic et al., 2013), to design new automatic metrics that take into consideration human annotations (Popovic, 2012; Bojar et al., 2013), and to train classifiers that can automatic identify fin</context>
<context position="20499" citStr="Vilar et al., 2006" startWordPosition="3318" endWordPosition="3321">ations. In particular, we see that the agreement for ENAR is systematically lower than the values measured for the other languages on all the score classes. 4.2 Error Annotation This evaluation task was carried out by one expert for each language direction, who was asked to identify the type of errors present in the MT output and to mark their position in the text. Since the focus of our work is the analysis method rather than the definition of an ideal error taxonomy, for the difficult language directions addressed we opted for the following general error classes, partially overlapping with (Vilar et al., 2006): i) reordering errors, ii) lexicon errors (including wrong lexical choices and extra words), iii) missing words, iv) morphology errors. Figure 3 shows the distribution of the errors in terms of affected tokens (words) for each error type. Since token counts for Chinese are not wordbased but character-based, for readability purposes the number of errors counted for Chinese translations have been divided by 2.5. Note also that morphological errors annotated for ENZH involve only 13 characters and thus are not visible in the plot. The total number of errors amounts to 16,320 characters for ENZH,</context>
</contexts>
<marker>Vilar, Xu, dHaro, Ney, 2006</marker>
<rawString>David Vilar, Jia Xu, Luis Fernando dHaro, and Hermann Ney. 2006. Error analysis of statistical machine translation output. In Proceedings of the Fifth International Conference on Language Resources and Evaluation (LREC’06), pages 697–702.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
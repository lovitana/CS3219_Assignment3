<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000002">
<title confidence="0.991646">
Semantic-Based Multilingual Document Clustering via Tensor Modeling
</title>
<author confidence="0.998624">
Salvatore Romeo, Andrea Tagarelli
</author>
<affiliation confidence="0.8920185">
DIMES, University of Calabria
Arcavacata di Rende, Italy
</affiliation>
<email confidence="0.9616425">
sromeo@dimes.unical.it
tagarelli@dimes.unical.it
</email>
<sectionHeader confidence="0.997195" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999961157894737">
A major challenge in document clustering re-
search arises from the growing amount of text
data written in different languages. Previ-
ous approaches depend on language-specific
solutions (e.g., bilingual dictionaries, sequen-
tial machine translation) to evaluate document
similarities, and the required transformations
may alter the original document semantics. To
cope with this issue we propose a new docu-
ment clustering approach for multilingual cor-
pora that (i) exploits a large-scale multilingual
knowledge base, (ii) takes advantage of the
multi-topic nature of the text documents, and
(iii) employs a tensor-based model to deal with
high dimensionality and sparseness. Results
have shown the significance of our approach
and its better performance w.r.t. classic docu-
ment clustering approaches, in both a balanced
and an unbalanced corpus evaluation.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999497208333333">
Document clustering research was initially focused on
the development of general purpose strategies to group
unstructured text data. Recent studies have started de-
veloping new methodologies and algorithms that take
into account both linguistic and topical characteristics,
where the former include the size of the text and the
type of language, and the latter focus on the commu-
nicative function and targets of the documents.
A major challenge in document clustering research
arises from the growing amount of text data that are
written in different languages, also due to the increased
popularity of a number of tools for collaboratively edit-
ing through contributors across the world. Multilingual
document clustering (MDC) aims to detect clusters in a
collection of texts written in different languages. This
can aid a variety of applications in cross-lingual infor-
mation retrieval, including statistical machine transla-
tion and corpora alignment.
Existing approaches to MDC can be divided in two
broad categories, depending on whether a parallel cor-
pus rather than a comparable corpus is used (Kumar et
al., 2011c). A parallel corpus is typically comprised
of documents with their related translations (Kim et
al., 2010). These translations are usually obtained
</bodyText>
<note confidence="0.69900625">
Dino Ienco
IRSTEA, UMR TETIS
Montpellier, France
LIRMM
</note>
<address confidence="0.336155">
Montpellier, France
</address>
<email confidence="0.874014">
dino.ienco@irstea.fr
</email>
<bodyText confidence="0.999641108695652">
through machine translation techniques based on a se-
lected anchor language. Conversely, a comparable cor-
pus is a collection of multilingual documents written
over the same set of classes (Ni et al., 2011; Yo-
gatama and Tanaka-Ishii, 2009) without any restric-
tion about translation or perfect correspondence be-
tween documents. To mine this kind of corpus, external
knowledge is employed to map concepts or terms from
a language to another (Kumar et al., 2011c; Kumar
et al., 2011a), which enables the extraction of cross-
lingual document correlations. In this case, a major
issue lies in the definition of a cross-lingual similarity
measure that can fit the extracted cross-lingual correla-
tions. Also, from a semi-supervised perspective, other
works attempt to define must-link constraints to de-
tect cross-lingual clusters (Yogatama and Tanaka-Ishii,
2009). This implies that, for each different dataset, the
set of constraints needs to be redefined; in general, the
final results can be negatively affected by the quantity
and the quality of involved constraints (Davidson et al.,
2006).
To the best of our knowledge, existing clustering ap-
proaches for comparable corpora are customized for a
small set (two or three) of languages (Montalvo et al.,
2007). Most of them are not generalizable to many
languages as they employ bilingual dictionaries and
the translation is performed sequentially considering
only pairs of languages. Therefore, the order in which
this process is done can seriously impact the results.
Another common drawback concerns the way most
of the recent approaches perform their analysis: the
various languages are analyzed independently of each
other (possibly by exploiting external knowledge like
Wikipedia to enrich documents (Kumar et al., 2011c;
Kumar et al., 2011a)), and then the language-specific
results are merged. This two-step analysis however
may fail in profitably exploiting cross-language infor-
mation from the multilingual corpus.
Contributions. We address the problem of MDC
by proposing a framework that features three key ele-
ments, namely: (1) to model documents over a unified
conceptual space, with the support of a large-scale mul-
tilingual knowledge base; (2) to decompose the mul-
tilingual documents into topically-cohesive segments;
and (3) to describe the multilingual corpus under a
multi-dimensional data structure.
</bodyText>
<page confidence="0.952963">
600
</page>
<note confidence="0.9112915">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 600–609,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999535631578948">
The first key element prevents loss of information
due to the translation of documents from different lan-
guages to a target one. It enables a conceptual represen-
tation of the documents in a language-independent way
preserving the content semantics. BabelNet (Navigli
and Ponzetto, 2012a) is used as multilingual knowl-
edge base. To the extent of our knowledge, this is the
first work in MDC that exploits BabelNet.
The second key element, document segmentation,
enables us to simplify the document representation
according to their multi-topic nature. Previous re-
search has demonstrated that a segment-based ap-
proach can significantly improve document clustering
performance (Tagarelli and Karypis, 2013). More-
over, the conceptual representation of the document
segments enables the grouping of linguistically dif-
ferent (portions of) documents into topically coherent
clusters.
The latter aspect is leveraged by the third key ele-
ment of our proposal, which relies on a tensor-based
model (Kolda and Bader, 2009) to effectively handle
the high dimensionality and sparseness in text. Ten-
sors are considered as a multi-linear generalization of
matrix factorizations, since all dimensions or modes
are retained thanks to multi-linear structures which can
produce meaningful components. The applicability of
tensor analysis has recently attracted growing atten-
tion in information retrieval and data mining, including
document clustering (e.g., (Liu et al., 2011; Romeo
et al., 2013)) and cross-lingual information retrieval
(e.g., (Chew et al., 2007)).
The rest of the paper is organized as follows. Sec-
tion 2 provides an overview of BabelNet and basic no-
tions on tensors. We describe our proposal in Section 3.
Data and experimental settings are described in Sec-
tion 4, while results are presented in Section 5. We
summarize our main findings in Section 6, finally Sec-
tion 7 concludes the paper.
</bodyText>
<sectionHeader confidence="0.986611" genericHeader="introduction">
2 Background
</sectionHeader>
<subsectionHeader confidence="0.9643">
2.1 BabelNet
</subsectionHeader>
<bodyText confidence="0.999947806451613">
BabelNet (Navigli and Ponzetto, 2012a) is a multilin-
gual semantic network obtained by linking Wikipedia
with WordNet, that is, the largest multilingual Web en-
cyclopedia and the most popular computational lex-
icon. The linking of the two knowledge bases was
performed through an automatic mapping of WordNet
synsets and Wikipages, harvesting multilingual lexi-
calization of the available concepts through human-
generated translations provided by the Wikipedia inter-
language links or through machine translation tech-
niques. The result is an encyclopedic dictionary con-
taining concepts and named entities lexicalized in 50
different languages.
Multilingual knowledge in BabelNet is represented
as a labeled directed graph in which nodes are concepts
or named entities and edges connect pairs of nodes
through a semantic relation. Each edge is labeled with a
relation type (is-a, part-of, etc.), while each node corre-
sponds to a BabelNet synset, i.e., a set of lexicalizations
of a concept in different languages.
BabelNet can be accessed and easily integrated into
applications by means of a Java API provided by the
toolkit described in (Navigli and Ponzetto, 2012b).
The toolkit also provides functionalities for graph-
based WSD in a multilingual context. Given an in-
put set of words, a semantic graph is built by looking
for related synset paths and by merging all them in a
unique graph. Once the semantic graph is built, the
graph nodes can be scored with a variety of algorithms.
Finally, this graph with scored nodes is used to rank the
input word senses by a graph-based approach.
</bodyText>
<subsectionHeader confidence="0.989579">
2.2 Tensor model representation
</subsectionHeader>
<bodyText confidence="0.992846">
A tensor is a multi-dimensional array T E
RI1xI2x···xIM .The number of dimensions M, also
known as ways or modes, is called order of the ten-
sor, so that a tensor with order M is also said a M-
way or M-order tensor. A higher-order tensor (i.e., a
tensor with order three or higher) is denoted by bold-
face calligraphic letters, e.g., T ; a matrix (2-way ten-
sor) is denoted by boldface capital letters, e.g., U;
a vector (1-way tensor) is denoted by boldface low-
ercase letters, e.g., v. The generic entry (i1, i2, i3)
of a third-order tensor T is denoted by ti1i2i3, with
i1 E [1..I1], i2 E [1..I2],i3 E [1..I3].
A one-dimensional fragment of tensor, defined by
varying one index and keeping the others fixed, is a
1-way tensor called fiber. A third-order tensor has
column, row and tube fibers. Analogously, a two-
dimensional fragment of tensor, defined by varying two
indices and keeping the rest fixed, is a 2-way tensor
called slice. A third-order tensor has horizontal, lateral
and frontal slices.
The mode-m matricization of a tensor T , denoted
by T(m), is obtained by arranging the mode-m fibers
as columns of a matrix. A third-order tensor T E
</bodyText>
<equation confidence="0.982730333333333">
RI1xI2xI3 is all-orthogonal ift t =
qq i1 i2 zliza zli2/j
Ei1i3 ti1αi3ti1Ni3 = Ei2i3 tai2i3tβi2i3 = 0 when-
ever α =� β. The mode-m product of a tensor T E
RI1xI2x···xIM with a matrix U E RJxIm, denoted by
T xm U, is a tensor of dimension I1 x ...Im_1 x
J x Im+1 x · · · x IM and can be expressed in terms
of matrix product as Y = T xm U, whose mode-m
matricization is Y(m) = UT(m).
</equation>
<sectionHeader confidence="0.999332" genericHeader="method">
3 Our Proposal
</sectionHeader>
<subsectionHeader confidence="0.9467915">
3.1 Multilingual Document Clustering
framework
</subsectionHeader>
<bodyText confidence="0.999698">
We are given a collection of multilingual documents
D = ULl=1 Dl, where each Dl = {dliJNl i=1 represents a
subset of documents written in the same language, with
N = E l=1 Nl = |D|. Our framework can be applied
to any multilingual document collection regardless of
the languages, and can deal with balanced as well as
</bodyText>
<page confidence="0.997151">
601
</page>
<bodyText confidence="0.773859">
Algorithm 1 SeMDocT (Segment-based MultiLingual
Document Clustering via Tensor Modeling)
Input: A collection of multilingual documents D, the num-
ber k of segment clusters, the number of tensorial com-
ponents r.
Output: A document clustering solution C over D.
</bodyText>
<listItem confidence="0.9506136">
1: Apply a text segmentation algorithm over each of the
documents in D to produce a collection of document seg-
ments S. /* Section 3.1.1 */
2: Represent S in either a bag-of-words (BoW) or a bag-of-
synsets (BoS) space. /* Section 3.1.2 */
3: Apply any document clustering algorithm on S to obtain
a segment clustering CS = {Csi }ki=1. /* Section 3.1.2 */
4: Represent CS in either a bag-of-words (BoW) or a bag-
of-synsets (BoS) space. /* Section 3.1.3 */
5: Model S as a third-order tensor T E W1xI2xI3, with
I1 = |D|, I2 = |F|, and I3 = k. /* Section 3.1.4 */
6: Decompose the tensor using a Truncated HOSVD.
/* Section 3.1.4 */
7: Apply a document clustering algorithm on the mode-1
factor matrix to obtain the final clusters of documents
</listItem>
<equation confidence="0.96244">
C = {Ci}Ki=1. /* Section 3.1.5 */
</equation>
<bodyText confidence="0.994169470588235">
unbalanced corpora. Therefore, no restriction is given
on both the number L of languages and the distribution
of documents over the languages (i.e., Ni Nj, with
i,j = 1..L, i =� j).
Real-world documents often span multiple topics.
We assume that each document in D is relatively long
to be comprised of smaller textual units, or segments,
each of which can be considered cohesive w.r.t. a topic
over the document. This represents a key aspect in
our framework as it enables the use of a tensor model
to conveniently address the multi-faceted nature of the
documents.
Our overall framework, named SeMDocT (Segment-
based MultiLingual Document Clustering via Tensor
Modeling), is shown in Algorithm 1. In the following,
we shall describe in details each of the steps involved
in SeMDocT.
</bodyText>
<subsectionHeader confidence="0.990157">
3.1.1 Computing within-document segments
</subsectionHeader>
<bodyText confidence="0.999981666666667">
Text segmentation is concerned with the fragmentation
of an input text into multi-paragraph, contiguous and
disjoint blocks that represent subtopics. Regardless of
the presence of logical structure clues in the document,
linguistic criteria (Beeferman et al., 1999) and statis-
tical similarity measures (Hearst, 1997; Choi et al.,
2001; Cristianini et al., 2001) have been mainly used to
detect subtopic boundaries between segments. A com-
mon assumption is that terms that discuss a subtopic
tend to co-occur locally, and a switch to a new subtopic
is detected by the ending of co-occurrence of a given
set of terms and the beginning of the co-occurrence of
another set of terms.
Our SeMDocT does not depend on a specific algo-
rithmic choice to perform text segmentation; in this
work, we refer to the classic TextTiling (Hearst, 1997),
which is the exemplary similarity-block-based method
for text segmentation.
</bodyText>
<subsectionHeader confidence="0.870851">
3.1.2 Inducing document segment clusters
</subsectionHeader>
<bodyText confidence="0.9999932">
The result of the previous step is a collection of doc-
ument segments, henceforth denoted as S. Each seg-
ment in S is represented as a vector of feature oc-
currences, where a feature can be either lexical or se-
mantic. This corresponds to two alternative represen-
tation models: the standard bag-of-words (henceforth
BoW), whereby features correspond to lemmatized,
non-stopword terms, and the obtained feature space
results from the union of the vocabularies of the dif-
ferent languages; and bag-of-synsets (henceforth BoS),
whereby features correspond to BabelNet synsets. We
shall devote Section 3.2 to a detailed description of our
proposed BoS representation.
The segment collection S is given in input to a doc-
ument clustering algorithm to produce a clustering of
the segments CS = {C3i }k1. The obtained clusters
of segments can be disjoint or overlapping. Again, our
SeMDocT is parametric to the clustering algorithm as
well; here, we resort to a state-of-the-art clustering al-
gorithm, namely Bisecting K-Means (Steinbach et al.,
2000), which is widely known to produce high-quality
(hard) clustering solutions in high-dimensional, large
datasets (Zhao and Karypis, 2004). Note however
that it requires as input the number of clusters. To
cope with this issue, we adopt the method described
in (Salvador and Chan, 2004), which explores how the
within-cluster cohesion changes by varying the number
of clusters. The number of clusters for which the slope
of the plot changes drastically is chosen as a suitable
value for the clustering algorithm.
</bodyText>
<subsectionHeader confidence="0.746461">
3.1.3 Segment-cluster based representation
</subsectionHeader>
<bodyText confidence="0.999988761904762">
Upon the segment clustering, each document is repre-
sented by its segments assigned to possibly multiple
segment clusters. Therefore, we derive a document-
feature matrix for each of the k segment clusters. The
features correspond either to the BoW or BoS model,
according to the choice made for the segment represen-
tation.
Let us denote with T the feature space for all seg-
ments in S. Given a segment cluster C3, the cor-
responding document-feature matrix is constructed as
follows. The representation of each document d E D
w.r.t. C3 is a vector of length |T |that results from the
sum of the feature vectors of the d’s segments belong-
ing to C3. Moreover, in order to weight the appearance
of a document in a cluster based on its segment-based
portion covered in the cluster, the document vector of d
w.r.t. C3 is finally obtained by multiplying the sum of
the segment-vectors by a scalar representing the portion
of d’s features that appear in the segments belonging to
C3. The document-feature matrix of C3 resulting from
the previous step is finally normalized by column.
</bodyText>
<subsectionHeader confidence="0.528252">
3.1.4 Tensor model and decomposition
</subsectionHeader>
<bodyText confidence="0.9961115">
The document-feature matrices corresponding to the k
segment-clusters are used to form a third-order tensor.
</bodyText>
<page confidence="0.979166">
602
</page>
<figure confidence="0.9860105">
features
(terms,synsets)
</figure>
<figureCaption confidence="0.816437666666667">
Figure 1: The third-order tensor model for the repre-
sentation of a multilingual document collection based
on segment clusters.
</figureCaption>
<bodyText confidence="0.999986">
Our third-order tensor model is built by arranging
as frontal slices the k segment-cluster matrices. The
resulting tensor will be of the form T E I
</bodyText>
<equation confidence="0.993843">
R x I2 × I3
</equation>
<bodyText confidence="0.991460090909091">
with I1 = |D|, I2 = |T|, and I3 = k. The proposed
tensor model is sketched in Fig. 1.
The resulting tensor is decomposed through a Trun-
cated Higher Order SVD (T-HOSVD) (Lathauwer et
al., 2000) in order to obtain a low-dimensional rep-
resentation of the segment-cluster-based representation
of the document collection. The T-HOSVD can be con-
sidered as an extension of the Truncated Singular Value
Decomposition (T-SVD) to the case of three or more
dimensions. For a third-order tensor T E RI1×I2×I3
the T-HOSVD is expressed as
</bodyText>
<equation confidence="0.875765833333333">
T ,: X X1 U(1) X2 U(2) X3 U(3)
where U(m) = [u(m)
1 u(m) 2... u(m)
rm ] E RIm×rm (m =
1, 2, 3) are orthogonal matrices, rm « Im, and the core
tensor X E Rr1×r2×r3 is an all-orthogonal and ordered
</equation>
<listItem confidence="0.771875571428572">
tensor. T-HOSVD can be computed in two steps:
1. For m E {1, 2, 3}, compute the unfolded ma-
trices T(m) from T and related standard SVD:
T(m) = U(m)S(m)V(m). The orthogonal matrix
U(m) contains the leading left singular vectors of
T(m).
2. Compute the core tensor X using the inversion
</listItem>
<equation confidence="0.94226">
formula: X = T X1 U(1)T X2 U(2)T X3 U(3)T.
</equation>
<bodyText confidence="0.999933727272727">
Note that, since T-HOSVD is computed by means of
3 standard matrix T-SVDs, its computational cost can
be reduced by using fast and efficient SVD algorithms.
Moreover, the ability of T-HOSVD in effectively cap-
turing the variation in each of the modes independently
from the other ones, is particularly important to alle-
viate the problem of concentration of distances, thus
making T-HOSVD well-suited to clustering purposes.
In this work, in order to obtain a final clustering so-
lution of the documents, we will consider the mode-1
factor matrix U(1) of the T-HOSVD.
</bodyText>
<subsectionHeader confidence="0.906382">
3.1.5 Document clustering
</subsectionHeader>
<bodyText confidence="0.9999875">
The mode-1 factor matrix is provided in input to a clus-
tering method to obtain a final organization of the doc-
uments into K clusters, i.e., C = {Ci}Ki=1. Note that
there is no principled relation between the number K of
final document clusters and k. However, K is expected
to reflect the number of topics of interest for the docu-
ment collection. Also, possibly but not necessarily, the
same clustering algorithm used for the segment cluster-
ing step (i.e., Bisecting K-Means) can be employed for
this step.
</bodyText>
<subsectionHeader confidence="0.999207">
3.2 Bag-of-synset representation
</subsectionHeader>
<bodyText confidence="0.999985219512195">
In the BoS model, our objective is to represent the doc-
ument segments in a conceptual feature space instead
of the traditional term space. Since we deal with mul-
tilingual documents, this task clearly relies on the mul-
tilingual lexical knowledge base functionalities of Ba-
belNet. Conceptual features will hence correspond to
BabelNet synsets.
The segment collection S is subject to a two-step
processing phase. In the first step, each segment is
broken down into a set of lemmatized and POS-tagged
sentences, in which each word is replaced with re-
lated lemma and associated POS-tag. Let us denote
with (w, POS(w)) a lemma and associated POS-tag
occurring in any sentence sen of the segment. In the
second step, a WSD method is applied to each pair
(w, POS(w)) to detect the most appropriate Babel-
Net synset Qw for (w, POS(w)) contextually to sen.
The WSD algorithm is carried out in such a way that
all words from all languages are disambiguated over
the same concept inventory, producing a language-
independent feature space for the whole multilingual
corpus. Each segment is finally modeled as a |13S|-
dimensional vector of BabelNet synset frequencies, be-
ing 13S the set of retrieved BabelNet synsets.
As previously discussed in Section 2.1, BabelNet
provides WSD algorithms for multilingual corpora.
More specifically, the authors in (Navigli and Ponzetto,
2012b) suggest to use the Degree algorithm (Navigli
and Lapata, 2010), as it showed to yield highly com-
petitive performance in a multilingual context as well.
Note that the Degree algorithm, given a semantic graph
for the input context, simply selects the sense of the tar-
get word with the highest vertex degree. Clearly, other
graph-based methods for (unsupervised) WSD, partic-
ularly PageRank-style methods (e.g., (Mihalcea et al.,
2004; Agirre and Soroa, 2009; Yeh et al., 2009; Tsat-
saronis et al., 2010)), can be plugged in to address the
multilingual WSD task based on BabelNet. An investi-
gation of the performance of existing WSD algorithms
for a multilingual context is however out of the scope
of this paper.
</bodyText>
<sectionHeader confidence="0.998197" genericHeader="method">
4 Evaluation Methodology
</sectionHeader>
<bodyText confidence="0.870668">
In order to evaluate our proposal we need a multilin-
gual comparable document collection with annotated
documents
</bodyText>
<page confidence="0.96695">
603
</page>
<table confidence="0.976163647058824">
RCV2 Topics English French Italian
Balanced Corpus
C15 - PERFORMANCE 850 850 850
C18 - OWNERSHIP CHANGES 850 850 850
E11 - ECONOMIC PERFORMANCE 850 850 850
E12 - MONETARY/ECONOMIC 850 850 850
M11 - EQUITY MARKETS 850 850 850
M13 - MONEY MARKETS 850 850 850
Total 5100 5 100 5 100
Unbalanced Corpus
C15 - PERFORMANCE 850 850 0
C18 - OWNERSHIP CHANGES 850 850 0
E11 - ECONOMIC PERFORMANCE 0 850 850
E12 - MONETARY/ECONOMIC 850 0 850
M11 - EQUITY MARKETS 0 850 850
M13 - MONEY MARKETS 850 0 850
Total 3 400 3 400 3 400
</table>
<tableCaption confidence="0.970227">
Table 1: Number of documents for each topic and lan-
guage.
</tableCaption>
<table confidence="0.964857">
Statistics Balanced Corpus Unbalanced Corpus
# of docs 15 300 10 200
# of terms 58 825 44 535
# of synsets 16 395 14 339
BoW Density 1.5E-3 2.0E-3
BoS Density 2.6E-3 3.1E-3
</table>
<tableCaption confidence="0.999466">
Table 2: Main characteristics of the corpora.
</tableCaption>
<bodyText confidence="0.9986996">
topics. For this reason, we used Reuters Corpus Volume
2 (RCV2), a multilingual corpus containing news arti-
cles in thirteen language.1 In the following, we present
the corpus characteristics and competing methods used
in our analysis.
</bodyText>
<subsectionHeader confidence="0.993967">
4.1 Data preparation
</subsectionHeader>
<bodyText confidence="0.99979284">
We consider a subset of the RCV2 corpus correspond-
ing to three languages: English, French and Italian.
It covers six different topics, i.e., different labels of
the RCV2 TOPICS field. Topics are chosen accord-
ing with their coverage in the different languages.
The language-specific documents were lemmatized and
POS-tagged through the Freeling library (Padr´o and
Stanilovsky, 2012) in order to obtain a suitable rep-
resentation for the WSD process.
To assess the robustness of our proposal, we de-
sign two different scenarios. The first (Balanced Cor-
pus) is characterized by a completely balanced dataset.
Each language covers all topics and for each pair lan-
guage/topic the same number of documents is selected.
The second scenario corresponds to an Unbalanced
Corpus. Starting from the balanced corpus, we re-
moved for each topic all the documents belonging to
one language. In this way, we obtained a corpus in
which each topic is covered by only two of the three
languages.
Main characteristics of both evaluation corpora are
reported in Table 1 and Table 2. In the latter table,
we report the number of documents, number of terms,
number of synsets and the dataset density for both
representations. To quantify the density of each cor-
</bodyText>
<footnote confidence="0.981316">
1http://trec.nist.gov/data/reuters/reuters.html
</footnote>
<table confidence="0.926657285714286">
RCV2 Topics English French Italian
C15 - PERFORMANCE 3.41 3.67 3.27
C18 - OWNERSHIP CHANGES 3.20 3.32 2.40
E11 - ECONOMIC PERFORMANCE 4.89 3.17 2.07
E12 - MONETARY/ECONOMIC 5.22 3.69 2.05
M11 - EQUITY MARKETS 4.29 2.94 2.15
M13 - MONEY MARKETS 3.31 3.12 2.10
</table>
<tableCaption confidence="0.9209665">
Table 3: Average number of document segments, for
each topic and language.
</tableCaption>
<table confidence="0.999915222222222">
English French Italian
RCV2 avg BoS avg BoW avg BoS avg BoW avg BoS avg BoW
Topics seg.leng. seg.leng. seg.leng. seg.leng. seg.leng. seg.leng.
C15 21.76 36.32 11.54 34.92 10.58 37.75
C18 20.94 36.87 10.94 35.62 11.24 41.20
E11 22.90 37.24 11.47 34.73 11.96 38.60
E12 22.70 37.70 11.50 37.44 12.59 43.63
M11 22.04 36.83 10.91 32.76 11.57 42.39
M13 22.22 36.97 11.34 34.75 11.72 39.36
</table>
<tableCaption confidence="0.9465275">
Table 4: Average length of document segment in the
BoW and BoS spaces, for each topic and language.
</tableCaption>
<bodyText confidence="0.999899871794872">
pus/representation combination, we counted the non-
zero entries of the induced document-synset matrix (al-
ternatively, document-term matrix) and we divided this
value by the size of such matrix. This number pro-
vides an estimate about the density/sparseness of each
dataset. Lower values indicate more sparse data. We
can note that BoS model yields more dense datasets for
both Balanced Corpus and Unbalanced Corpus.
As our proposal explicitly models document seg-
ments, we also report statistics, considering both topics
and languages, related to the average number of seg-
ments per document (Table 3), and the average length
of segments per document (Table 4). The latter statistic
is computed separately for BoW and BoS representa-
tions. We made this distinction because a term cannot
have a mapping to a synset, or it can be mapped to more
than one synset in the BoS space during the WSD pro-
cess (Section 3.2).
Looking at the average number of segments per doc-
ument in Table 3, it can be noted that English docu-
ments contain, for all topics, a larger number of seg-
ments. This means that English documents are gener-
ally richer than the ones in the other languages. Ital-
ian language corresponds to the smallest documents,
each of them containing between 2 and 3.2 segments
on average. A sharper difference appears in the MONE-
TARY/ECONOMIC topic for which English documents
contain 5.2 segments, while the Italian ones are com-
posed, on average, by only 2 segments.
Table 4 shows the average length of segments per
document for both space representations. Generally,
segments in the BoS representation are smaller than the
corresponding segments in the BoW space. More in de-
tail, if we consider the ratio between the segment length
in BoS and the one in BoW, this ratio is around 2/3 for
the English language, while for both French and Ital-
ian it varies between 1/4 and 1/3. This disequilibrium
is induced by the multilingual concept coverage of Ba-
belNet, as stated by its authors (Navigli and Ponzetto,
</bodyText>
<page confidence="0.995623">
604
</page>
<bodyText confidence="0.98923675">
2012a), (Navigli and Ponzetto, 2012b). In particular,
the WSD process tightly depends from the concept cov-
erage supplied from the language-specific knowledge
base.
</bodyText>
<subsectionHeader confidence="0.969605">
4.2 Competing methods and settings
</subsectionHeader>
<bodyText confidence="0.999987162162162">
We compare our SeMDocT with two standard ap-
proaches, namely Bisecting K-Means (Steinbach et al.,
2000), and Latent Semantic Analysis (LSA)-based doc-
ument clustering (for short, LSA). Given a number K
of desired clusters, Bisecting K-Means produces a K-
way clustering solution by performing a sequence of
K-1 repeated bisections based on standard K-Means
algorithm. This process continues until the number K
of clusters is found. LSA performs a decomposition of
the document collection matrix through Singular Value
Decomposition in order to extract a more concise and
descriptive representation of the documents. After this
step, Bisecting K-Means is applied over the new docu-
ment space to get the final document clustering.
All the three methods, SeMDocT, Bisecting K-
Means and LSA are coupled with either BoS or BoW
representation models. The comparison between BoS
and BoW representations allows us to evaluate the
presumed benefits that can be derived by exploiting
synsets instead of terms for the multilingual document
clustering task.
Both SeMDocT and LSA require the number of com-
ponents as input; as concerns specifically SeMDocT,
we varied r1 (cf. Section 3.1.4) from 2 to 30, with in-
crements of 2. To determine the number of segment
clusters k, we employed an automatic way as discussed
in Section 3.1.2. By varying k from 2 to 40, for Bal-
anced Corpus and Unbalanced Corpus, respectively,
the values of k obtained were 22 and 23 under BoS,
and 25 and 11 under BoW.
As concerns the step of text segmentation, TextTiling
requires the setting of some interdependent parameters,
particularly the size of the text unit to be compared and
the number of words in a token sequence. We used the
setting suggested in (Hearst, 1997) and also confirmed
in (Tagarelli and Karypis, 2013), i.e., 10 for the text
unit size and 20 for the token-sequence size.
</bodyText>
<subsectionHeader confidence="0.999719">
4.3 Assessment criteria
</subsectionHeader>
<bodyText confidence="0.9946802">
Performance of the different methods are evaluated us-
ing two standard clustering validation criteria, namely
F-Measure and Rand Index.
Given a document collection D, let P = {Γj}Hj=1
and C = {Ci}Ki=1 denote a reference classification
and a clustering solution for D, respectively. The lo-
cal precision and the local recall of a cluster Ci w.r.t.
a class Γj are defined as Pij = |Ci ∩ Γj|/|Ci |and
Rij = |Ci ∩ Γj|/|Γj|, respectively. F-Measure (FM) is
computed as follows (Steinbach et al., 2000):
</bodyText>
<equation confidence="0.7322255">
|Γj|
|D |maxi=1...K{Fij}
</equation>
<bodyText confidence="0.998836153846154">
where Fij = 2PijRij/(Pij + Rij).
Rand Index (RI) (Rand, 1971) measures the percent-
age of decisions that are correct, penalizing false pos-
itive and false negative decisions during clustering. It
takes into account the following quantities: TP, i.e., the
number of pairs of documents that are in the same clus-
ter in C and in the same class in P; TN, i.e., the number
of pairs of documents that are in different clusters in
C and in different classes in P; FN, i.e., the number of
pairs of documents that are in different clusters in C and
in the same class in P; and FP, i.e., the number of pairs
of documents that are in the same cluster in C and in
different classes in P. Rand Index is hence defined as:
</bodyText>
<equation confidence="0.999114">
TP + TN
RI = TP + TN + FP + FN
</equation>
<bodyText confidence="0.99984825">
Note that for each method, results were averaged
over 30 runs and the number of final document clusters
K was set equal to the number of topics in the docu-
ment collection (i.e., 6).
</bodyText>
<sectionHeader confidence="0.999973" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.9999844">
We present here our main experimental results. We first
provide a comparative evaluation of our SeMDocT with
the competing methods, on both balanced and unbal-
anced corpus evaluation cases. Then we provide a per
language analysis focusing on SeMDocT.
</bodyText>
<subsectionHeader confidence="0.997999">
5.1 Evaluation with competing methods
</subsectionHeader>
<bodyText confidence="0.999993925925926">
Evaluation on balanced corpus. Figure 2 shows FM
and RI results obtained by the various methods coupled
with the two document representations on the Balanced
Corpus. Several remarks stand out. First, the BoS
space positively influences the performance of all the
employed approaches. This is particularly evident for
Bisecting K-Means and LSA that clearly benefit from
this kind of representation. The former almost doubles
its performance in terms of FM and significantly im-
proves its result w.r.t. RI. LSA shows improvements
in both cases. SeMDocT-BoS generally outperforms
all the competitors for both FM and RI when the num-
ber of components is greater than 16. Note that, under
the BoW model, SeMDocT-BoW still outperforms the
other methods.
Evaluation on unbalanced corpus. Figure 3 reports
results for the Unbalanced Corpus. Also in this eval-
uation, the best performances for all the methods are
reached using the BoS representation. SeMDocT-BoS
shows similar behavior according to the two measures.
It always outperforms the competitors considering a
number of components greater than or equal to 12.
More precisely, SeMDocT-BoW obtains a gain of 0.047
and 0.103 in terms of FM and 0.006 and 0.058 in
terms of RI, w.r.t. LSA-BoW and Bisecting K-Means-
BoW, respectively. Similarly, SeMDocT-BoS obtains
improvements of 0.05 in terms of FM w.r.t. both BoS
</bodyText>
<equation confidence="0.7419055">
H
E
j=1
F=
</equation>
<page confidence="0.965885">
605
</page>
<figure confidence="0.9766335">
2 6 10 14 18 22 26 30
no. of components
no. of components
(b)
</figure>
<figureCaption confidence="0.91522925">
Figure 2: Average F-Measure (a) and Rand Index (b)
on the Balanced Corpus using BoW and BoS document
representation and varying the number of components
for both SeMDocT and LSA.
</figureCaption>
<bodyText confidence="0.994963666666667">
competitors, while in terms of RI the differences in per-
formance are 0.012 and 0.019 for LSA-BoS and Bisect-
ing K-Means-BoS, respectively.
</bodyText>
<subsectionHeader confidence="0.998233">
5.2 Per language evaluation of SeMDocT-BoS
</subsectionHeader>
<bodyText confidence="0.999905294117647">
Starting from the clustering solutions produced by
SeMDocT-BoS in both balanced and unbalanced cases,
for each language we extracted a language-specific pro-
jection of the clustering. After that, we computed the
clustering validation criteria according to language spe-
cific solutions to quantify how well the clustering result
fits each specific language. The results of this experi-
ment are reported in Fig. 4 and Fig. 5.
On the Balanced Corpus, SeMDocT-BoS shows
comparable performance for English and French docu-
ments, while it behaves slightly worse for Italian texts.
This trend is highlighted for both clustering evaluation
criteria. Inspecting the results for the Unbalanced Cor-
pus, we observe a different trend. Results obtained for
the English texts are generally better than the results
for the French and Italian documents. For this bench-
mark, SeMDocT-BoS obtains similar results for docu-
</bodyText>
<figure confidence="0.922092">
2 6 10 14 18 22 26 30
no. of components
(a)
2 6 10 14 18 22 26 30
no. of components
(b)
</figure>
<figureCaption confidence="0.82499025">
Figure 3: Average F-Measure (a) and Rand Index (b)
on the Unbalanced Corpus using BoW and BoS docu-
ment representation and varying the number of compo-
nents for both SeMDocT and LSA.
</figureCaption>
<table confidence="0.9995575">
Dataset Language BoW BoS avg # synsets
size size per term (0)
English 29999 12065 0.4021
Balanced French 17 826 5 310 0.2978
Italian 16951 4471 0.2637
English 19432 10387 0.5345
Unbalanced French 14439 4431 0.3068
Italian 14743 4012 0.2721
</table>
<tableCaption confidence="0.999351">
Table 5: Statistics by language.
</tableCaption>
<bodyText confidence="0.996822928571429">
ments written in French and in Italian.
We gained an insight into the above discussed perfor-
mance behaviors by computing some additional statis-
tics that we report in Table 5: for each language and
each dataset, the size of the term and synset dictionar-
ies and the average number of synsets per lemma (0)
we retrieved through BabelNet according to the related
corpus. More in detail, 0 is the ratio between the BoS
and the BoW dictionaries. This quantity roughly eval-
uates how many synsets are produced per term during
the multilingual WSD process (Section 3.2). As we can
observe, this value is always smaller than one, which
means that not all the terms have a corresponding map-
ping to a synset. The 0 ratio can explain the discrep-
</bodyText>
<figure confidence="0.991038418918919">
●
●
●
●
●
●
● SeMDocT−BoS
SeMDocT−BoW
LSA−BoS
LSA−BoW
BisKMeans−BoS
BisKMeans−BoW
(a)
2 6 10 14 18 22 26 30
Rand index
0.8
0.7
0.6
0.5
●
●
●SeMDocT−BoS
SeMDocT−BoW
LSA−BoS
●
● ●
● ●
● ● ● ● ● ● ● ●
LSA−BoW
BisKMeans−BoS
BisKMeans−BoW
●
●
●
●
●
●
●
● SeMDocT−BoS LSA−BoW
SeMDocT−BoW BisKMeans−BoS
LSA−BoS BisKMeans−BoW
●
●
●
● SeMDocT−BoS
● SeMDocT−BoW
LSA−BoS
LSA−BoW
BisKMeans−BoS
BisKMeans−BoW
F−measure 0.6
0.5
0.4
0.3
0.2
0.1
0.6
0.5
F−measure
0.4
0.3
0.2
0.8
0.7
Rand index
0.6
0.5
606
2 6 10 14 18 22 26 30
no. of components
(a)
2 6 10 14 18 22 26 30
no. of components
(b)
</figure>
<figureCaption confidence="0.990133">
Figure 4: Average F-Measure (a) and Rand Index (b)
for language specific solutions on the Balanced Corpus
obtained by SeMDocT-BoS.
</figureCaption>
<figure confidence="0.9928185">
no. of components
(a)
●
● ● ● ● ● ● ●● ● ●
● ●
●
●
● English Italian
French
2 6 10 14 18 22 26 30
no. of components
(b)
</figure>
<figureCaption confidence="0.850465333333333">
Figure 5: Average F-Measure (a) and Rand Index (b)
for language specific solutions on the Unbalanced Cor-
pus obtained by SeMDocT-BoS.
</figureCaption>
<figure confidence="0.999237321428572">
●
●
●
●
●
●
● ●
●
● ●
●
●
● English
French
Italian
●
●
●
● ●
●
● ● ● ●
●
●
Italian
●
●
● English
French
0.8 ●
0.7 ● ●
●
●
● ●
●
●
2 6 10 14 18 22 26 30
0.5
0.4
Italian
F−measure
0.6
●English
French
F−measure 0.7
Rand index 0.6
0.5
0.4
0.8
0.7
0.6
0.5
0.4
Rand index 0.8
0.7
0.6
0.5
0.4
</figure>
<bodyText confidence="0.999591529411765">
ancy in (language-specific) performances in the two
scenarios. In particular, the difference in the 0 statis-
tic between English and the other languages is more
evident for the Unbalanced Corpus (i.e., 0.23 between
English and French), while it is lower for the Balanced
Corpus (around 0.1). The relatively large gap in 0 be-
tween the first and the second language (respectively,
English and French) for the Unbalanced Corpus re-
duces the relative gap between the second and the third
languages (respectively, French and Italian) while this
trend is less marked for the Balanced Corpus as 0 range
is narrower. In summary, we can state that our frame-
work works well if BabelNet knowledge base provides
a good coverage of the terms in the analyzed language.
Experimental evidence shows that, if this condition is
met, SeMDocT-BoS provides better clustering results
w.r.t. the competing approaches.
</bodyText>
<subsectionHeader confidence="0.998384">
5.3 Runtime of tensor decomposition
</subsectionHeader>
<bodyText confidence="0.999893217391304">
As previously discussed, T-HOSVD of a third-order
tensor can be computed through three standard SVDs.
Furthermore, for clustering purposes, we considered
only the mode-1 factor matrix of the decomposition.
To compute the SVD, we used the svds() function of
MATLAB R2012b, which is based on an iterative algo-
rithm.2 Experiments were carried out on an Intel Core
I7-3610QM platform with 16GB DDR RAM.
Figure 6 shows the execution time of the SVD over
the mode-1 matricization of our tensor for the Balanced
Corpus, by varying the number of components, for both
BoW and BoS representation models. As it can be ob-
served, in both cases the runtime is linear in the number
of components. However, the SVD computation in the
BoS setting is one order of magnitude faster than time
performance in the BoW setting. This is mainly due to
a large difference in size between the feature spaces of
BoW and BoS (cf. Table 2), since the selected num-
ber of segment clusters (k) was nearly the same (25 for
BoW, and 22 for BoS). Therefore, by providing a more
compact feature space, BoS clearly allows for a much
less expensive SVD computation for our tensor decom-
position.
</bodyText>
<footnote confidence="0.953936">
2http://www.mathworks.it/it/help/matlab/ref/svds.html
</footnote>
<page confidence="0.994952">
607
</page>
<sectionHeader confidence="0.957118" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<figureCaption confidence="0.9446375">
Figure 6: Time performance of SVD over the mode-1
matricization of the Balanced Corpus tensor.
</figureCaption>
<sectionHeader confidence="0.999707" genericHeader="acknowledgments">
6 Discussion
</sectionHeader>
<bodyText confidence="0.999977884615385">
Our work paves the way for the use of a multilingual
knowledge base to deal with the multilingual document
clustering task. Here we sum up our main findings.
SeMDocT vs. LSA. LSA achieved its best results
for a number of components generally smaller than the
one for which SeMDocT obtained its maximum. This
is due to the initial information that the two methods
summarize. LSA tries to capture the variation of the
initial document-term (alternatively, document-synset)
matrix representing the texts in a lower space, whereas
SeMDocT does the same starting from a richer repre-
sentation of the documents (i.e., a third-order tensor
model). For this reason, SeMDocT tends to employ
relatively more components in order to summarize the
documents content; however, a number of components
between 16 and 30 is generally enough to ensure good
performance of SeMDocT. Moreover, in most cases,
the highest performance results by SeMDocT are better
than the highest performances of LSA.
BoS vs. BoW. Our results have highlighted the
better quality in multilingual clustering supplied by
synsets compared with the one provided by terms. BoS
produces a smaller representation space over which
documents are projected, but it is enough rich to well
capture the documents content. In particular, BoS ben-
efits from the WSD process that is able to discriminate
the same term w.r.t. the context in which it appears.
BabelNet. BabelNet is a recent project that supports
many different languages. As the intention of the au-
thors is to enrich this resource, in the future our frame-
work will benefit of this fact. Moreover, our framework
can deal with documents written in many different lan-
guages as they are represented through the same space;
the only constraint is related to the available language
support in BabelNet. On the other hand, we point out
that any other multilingual knowledge base and WSD
tools can in principle be integrated in our framework.
In this paper we proposed a new approach for multi-
lingual document clustering. Our key idea lies in the
combination of a tensor-based model with a bag-of-
synsets description, which enables a common space to
project multilingual document collections. We evalu-
ated our approach w.r.t. standard document clustering
methods, using both term and synset representations.
Results have shown the benefits deriving from the use
of a multilingual knowledge base in the analysis of
comparable corpora, and also shown the significance
of our approach in both a balanced and an unbalanced
corpus evaluation. Our tensor-based representation of
topically-segmented multilingual documents can also
be applied to cross-lingual information retrieval or mul-
tilingual document categorization.
</bodyText>
<sectionHeader confidence="0.999261" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998462972222222">
Eneko Agirre and Aitor Soroa. 2009. Personaliz-
ing PageRank for Word Sense Disambiguation. In
Proc. of the International Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics (EACL), pages 33–41.
Doug Beeferman, Adam L. Berger, and John D. Laf-
ferty. 1999. Statistical Models for Text Segmenta-
tion. Machine Learning, 34(1-3):177–210.
Peter A. Chew, Brett W. Bader, Tamara G. Kolda, and
Ahmed Abdelali. 2007. Cross-language information
retrieval using PARAFAC2. In Proc. of the ACM
SIGKDD International Conference on Knowledge
Discovery and Data Mining (KDD), pages 143–152.
Freddy Y. Y. Choi, Peter Wiemer-Hastings, and Jo-
hanna Moore. 2001. Latent Semantic Analysis for
Text Segmentation. In Proc. of the International
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 109–117.
Nello Cristianini, John Shawe-Taylor, and Huma
Lodhi. 2001. Latent Semantic Kernels. In Proc. of
the International Conference on Machine Learning
(ICML), pages 66–73.
Ian Davidson, Kiri Wagstaff, and Sugato Basu. 2006.
Measuring constraint-set utility for partitional clus-
tering algorithms. In Proc. of the European Confer-
ence on Principles and Practice of Knowledge Dis-
covery in Databases (PKDD), pages 115–126.
Dino Ienco, C´eline Robardet, Ruggero G. Pensa, and
Rosa Meo. 2013. Parameter-less co-clustering for
star-structured heterogeneous data. Data Mining
and Knowledge Discovery, 26(2):217–254.
Marti A. Hearst. 1997. TextTiling: Segmenting Text
into Multi-Paragraph Subtopic Passages. Computa-
tional Linguistics, 23(1):33–64.
Young-Min Kim, Massih-Reza Amini, Cyril Goutte,
and Patrick Gallinari. 2010. Multi-view clustering
</reference>
<figure confidence="0.991853833333333">
2 6 10 14 18 22 26 30
no. of components
time (sec.)
0 20 40 60 80 100 120
●
●
●
●Bag of Words
Bag of Synsets
●
● ●
● ●
●
●
●
●
●
●
</figure>
<page confidence="0.964165">
608
</page>
<reference confidence="0.999912098214286">
of multilingual documents. In Proc. of the ACM SI-
GIR International Conference on Research and De-
velopment in Information Retrieval (SIGIR), pages
821–822.
Tamara G. Kolda and Brett W. Bader. 2009. Tensor
Decompositions and Applications. SIAM Review,
51(3):455–500.
N. Kiran Kumar, G. S. K. Santosh, and Vasudeva
Varma. 2011. A language-independent approach to
identify the named entities in under-resourced lan-
guages and clustering multilingual documents. In
Proc. of the International Conference of the Cross-
Language Evaluation Forum (CLEF), pages 74–82.
N. Kiran Kumar, G. S. K. Santosh, and Vasudeva
Varma. 2011. Effectively mining Wikipedia for clus-
tering multilingual documents. In Proc. of the Inter-
national Conference on Applications ofNatural Lan-
guage to Information Systems (NLDB), pages 254–
257.
N. Kiran Kumar, G. S. K. Santosh, and Vasudeva
Varma. 2011. Multilingual document clustering us-
ing Wikipedia as external knowledge. In Proc. of the
Information Retrieval Facility Conference (IRFC),
pages 108–117.
Lieven De Lathauwer, Bart De Moor, and Joos Vande-
walle. 2000. A Multilinear Singular Value Decom-
position. SIAM Journal on Matrix Analysis and Ap-
plications, 21(4):1253–1278.
Xinhai Liu, Wolfgang Gl¨anzel, and Bart De Moor.
2011. Hybrid clustering of multi-view data via
Tucker-2 model and its application. Scientometrics,
88(3):819–839.
Rada Mihalcea, Paul Tarau, and Elizabeth Figa. 2004.
PageRank on Semantic Networks, with Application
to Word Sense Disambiguation. In Proc. of the In-
ternational Conference on Computational Linguis-
tics (COLING).
Soto Montalvo, Raquel Martfnez-Unanue, Arantza
Casillas, and Vfctor Fresno. 2007. Multilingual
news clustering: Feature translation vs. identifica-
tion of cognate named entities. Pattern Recognition
Letters, 28(16):2305–2311.
Roberto Navigli and Mirella Lapata. 2010. An Exper-
imental Study of Graph Connectivity for Unsuper-
vised Word Sense Disambiguation. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence,
32(4):678–692.
Roberto Navigli and Simone P. Ponzetto. 2012. Babel-
Net: The Automatic Construction, Evaluation and
Application of a Wide-Coverage Multilingual Se-
mantic Network. Artificial Intelligence, 193:217-
250.
Roberto Navigli and Simone P. Ponzetto. 2012. Mul-
tilingual WSD with Just a Few Lines of Code: The
BabelNet API. In Proc. of the System Demonstra-
tions of the Annual Meeting of the Association for
Computational Linguistics (ACL), pages 67–72.
Xiaochuan Ni, Jian-Tao Sun, Jian Hu, and Zheng Chen.
2011. Cross lingual text classification by mining
multilingual topics from Wikipedia. In Proc. of the
ACM International Conference on Web Search and
Web Data Mining (WSDM), pages 375–384.
Llufs Padr´o and Evgeny Stanilovsky. 2012. FreeL-
ing 3.0: Towards Wider Multilinguality. In Proc. of
the Language Resources and Evaluation Conference
(LREC).
William M. Rand. 1971. Objective criteria for the eval-
uation of clustering methods. Journal of the Ameri-
can Statistical Association, 66:846–850.
Salvatore Romeo, Andrea Tagarelli, Francesco Gullo,
and Sergio Greco. 2013. A Tensor-based Clustering
Approach for Multiple Document Classifications. In
Proc. of the International Conference on Pattern
Recognition Applications and Methods (ICPRAM),
pages 200–205.
Stan Salvador and Philip Chan. 2004. Determining the
Number of Clusters/Segments in Hierarchical Clus-
tering/Segmentation Algorithms. In Proc. of the In-
ternational Conference on Tools with Artificial Intel-
ligence (ICTAI), pages 576–584.
Michael Steinbach, George Karypis, and Vipin Kumar.
2000. A Comparison of Document Clustering Tech-
niques. In Proc. of the KDD Workshop on Text Min-
ing.
Andrea Tagarelli and George Karypis. 2013. A
segment-based approach to clustering multi-topic
documents. Knowledge and Information Systems,
34(3):563–595.
George Tsatsaronis, Iraklis Varlamis, and Kjetil
Nørv˚ag. 2010. SemanticRank: Ranking Keywords
and Sentences Using Semantic Graphs. In Proc. of
the International Conference on Computational Lin-
guistics (COLING), pages 1074–1082.
Chih-Ping Wei, Christopher C. Yang, and Chia-Min
Lin. 2008. A Latent Semantic Indexing-based Ap-
proach to Multilingual Document Clustering. Deci-
sion Support Systems, 45(3):606–620.
Eric Yeh, Daniel Ramage, Christopher D. Manning,
Eneko Agirre, and Aitor Soroa. 2009. WikiWalk:
Random walks on Wikipedia for Semantic Related-
ness. In Proc. of the ACL Workshop on Graph-based
Methods for Natural Language Processing, pages
41–49.
Dani Yogatama and Kumiko Tanaka-Ishii. 2009. Mul-
tilingual spectral clustering using document similar-
ity propagation. In Proc. of the International Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 871–879.
Ying Zhao and George Karypis. 2004. Empirical and
Theoretical Comparison of Selected Criterion Func-
tions for Document Clustering. Machine Learning,
55(3):311–331.
</reference>
<page confidence="0.998832">
609
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.935281">
<title confidence="0.999734">Semantic-Based Multilingual Document Clustering via Tensor Modeling</title>
<author confidence="0.999831">Salvatore Romeo</author>
<author confidence="0.999831">Andrea</author>
<affiliation confidence="0.9764115">DIMES, University of Arcavacata di Rende,</affiliation>
<email confidence="0.998331">tagarelli@dimes.unical.it</email>
<abstract confidence="0.99903625">A major challenge in document clustering research arises from the growing amount of text data written in different languages. Previous approaches depend on language-specific solutions (e.g., bilingual dictionaries, sequential machine translation) to evaluate document similarities, and the required transformations may alter the original document semantics. To cope with this issue we propose a new document clustering approach for multilingual corpora that (i) exploits a large-scale multilingual knowledge base, (ii) takes advantage of the multi-topic nature of the text documents, and (iii) employs a tensor-based model to deal with high dimensionality and sparseness. Results have shown the significance of our approach and its better performance w.r.t. classic document clustering approaches, in both a balanced and an unbalanced corpus evaluation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Aitor Soroa</author>
</authors>
<title>Personalizing PageRank for Word Sense Disambiguation.</title>
<date>2009</date>
<booktitle>In Proc. of the International Conference of the European Chapter of the Association for Computational Linguistics (EACL),</booktitle>
<pages>33--41</pages>
<contexts>
<context position="20384" citStr="Agirre and Soroa, 2009" startWordPosition="3279" endWordPosition="3282">ets. As previously discussed in Section 2.1, BabelNet provides WSD algorithms for multilingual corpora. More specifically, the authors in (Navigli and Ponzetto, 2012b) suggest to use the Degree algorithm (Navigli and Lapata, 2010), as it showed to yield highly competitive performance in a multilingual context as well. Note that the Degree algorithm, given a semantic graph for the input context, simply selects the sense of the target word with the highest vertex degree. Clearly, other graph-based methods for (unsupervised) WSD, particularly PageRank-style methods (e.g., (Mihalcea et al., 2004; Agirre and Soroa, 2009; Yeh et al., 2009; Tsatsaronis et al., 2010)), can be plugged in to address the multilingual WSD task based on BabelNet. An investigation of the performance of existing WSD algorithms for a multilingual context is however out of the scope of this paper. 4 Evaluation Methodology In order to evaluate our proposal we need a multilingual comparable document collection with annotated documents 603 RCV2 Topics English French Italian Balanced Corpus C15 - PERFORMANCE 850 850 850 C18 - OWNERSHIP CHANGES 850 850 850 E11 - ECONOMIC PERFORMANCE 850 850 850 E12 - MONETARY/ECONOMIC 850 850 850 M11 - EQUIT</context>
</contexts>
<marker>Agirre, Soroa, 2009</marker>
<rawString>Eneko Agirre and Aitor Soroa. 2009. Personalizing PageRank for Word Sense Disambiguation. In Proc. of the International Conference of the European Chapter of the Association for Computational Linguistics (EACL), pages 33–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Doug Beeferman</author>
<author>Adam L Berger</author>
<author>John D Lafferty</author>
</authors>
<title>Statistical Models for Text Segmentation.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<pages>34--1</pages>
<contexts>
<context position="12572" citStr="Beeferman et al., 1999" startWordPosition="1989" endWordPosition="1992">nables the use of a tensor model to conveniently address the multi-faceted nature of the documents. Our overall framework, named SeMDocT (Segmentbased MultiLingual Document Clustering via Tensor Modeling), is shown in Algorithm 1. In the following, we shall describe in details each of the steps involved in SeMDocT. 3.1.1 Computing within-document segments Text segmentation is concerned with the fragmentation of an input text into multi-paragraph, contiguous and disjoint blocks that represent subtopics. Regardless of the presence of logical structure clues in the document, linguistic criteria (Beeferman et al., 1999) and statistical similarity measures (Hearst, 1997; Choi et al., 2001; Cristianini et al., 2001) have been mainly used to detect subtopic boundaries between segments. A common assumption is that terms that discuss a subtopic tend to co-occur locally, and a switch to a new subtopic is detected by the ending of co-occurrence of a given set of terms and the beginning of the co-occurrence of another set of terms. Our SeMDocT does not depend on a specific algorithmic choice to perform text segmentation; in this work, we refer to the classic TextTiling (Hearst, 1997), which is the exemplary similari</context>
</contexts>
<marker>Beeferman, Berger, Lafferty, 1999</marker>
<rawString>Doug Beeferman, Adam L. Berger, and John D. Lafferty. 1999. Statistical Models for Text Segmentation. Machine Learning, 34(1-3):177–210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter A Chew</author>
<author>Brett W Bader</author>
<author>Tamara G Kolda</author>
<author>Ahmed Abdelali</author>
</authors>
<title>Cross-language information retrieval using PARAFAC2.</title>
<date>2007</date>
<booktitle>In Proc. of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD),</booktitle>
<pages>143--152</pages>
<contexts>
<context position="6551" citStr="Chew et al., 2007" startWordPosition="959" endWordPosition="962">lement of our proposal, which relies on a tensor-based model (Kolda and Bader, 2009) to effectively handle the high dimensionality and sparseness in text. Tensors are considered as a multi-linear generalization of matrix factorizations, since all dimensions or modes are retained thanks to multi-linear structures which can produce meaningful components. The applicability of tensor analysis has recently attracted growing attention in information retrieval and data mining, including document clustering (e.g., (Liu et al., 2011; Romeo et al., 2013)) and cross-lingual information retrieval (e.g., (Chew et al., 2007)). The rest of the paper is organized as follows. Section 2 provides an overview of BabelNet and basic notions on tensors. We describe our proposal in Section 3. Data and experimental settings are described in Section 4, while results are presented in Section 5. We summarize our main findings in Section 6, finally Section 7 concludes the paper. 2 Background 2.1 BabelNet BabelNet (Navigli and Ponzetto, 2012a) is a multilingual semantic network obtained by linking Wikipedia with WordNet, that is, the largest multilingual Web encyclopedia and the most popular computational lexicon. The linking of</context>
</contexts>
<marker>Chew, Bader, Kolda, Abdelali, 2007</marker>
<rawString>Peter A. Chew, Brett W. Bader, Tamara G. Kolda, and Ahmed Abdelali. 2007. Cross-language information retrieval using PARAFAC2. In Proc. of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD), pages 143–152.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Freddy Y Y Choi</author>
<author>Peter Wiemer-Hastings</author>
<author>Johanna Moore</author>
</authors>
<title>Latent Semantic Analysis for Text Segmentation.</title>
<date>2001</date>
<booktitle>In Proc. of the International Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>109--117</pages>
<contexts>
<context position="12641" citStr="Choi et al., 2001" startWordPosition="2000" endWordPosition="2003">nature of the documents. Our overall framework, named SeMDocT (Segmentbased MultiLingual Document Clustering via Tensor Modeling), is shown in Algorithm 1. In the following, we shall describe in details each of the steps involved in SeMDocT. 3.1.1 Computing within-document segments Text segmentation is concerned with the fragmentation of an input text into multi-paragraph, contiguous and disjoint blocks that represent subtopics. Regardless of the presence of logical structure clues in the document, linguistic criteria (Beeferman et al., 1999) and statistical similarity measures (Hearst, 1997; Choi et al., 2001; Cristianini et al., 2001) have been mainly used to detect subtopic boundaries between segments. A common assumption is that terms that discuss a subtopic tend to co-occur locally, and a switch to a new subtopic is detected by the ending of co-occurrence of a given set of terms and the beginning of the co-occurrence of another set of terms. Our SeMDocT does not depend on a specific algorithmic choice to perform text segmentation; in this work, we refer to the classic TextTiling (Hearst, 1997), which is the exemplary similarity-block-based method for text segmentation. 3.1.2 Inducing document </context>
</contexts>
<marker>Choi, Wiemer-Hastings, Moore, 2001</marker>
<rawString>Freddy Y. Y. Choi, Peter Wiemer-Hastings, and Johanna Moore. 2001. Latent Semantic Analysis for Text Segmentation. In Proc. of the International Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 109–117.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nello Cristianini</author>
<author>John Shawe-Taylor</author>
<author>Huma Lodhi</author>
</authors>
<title>Latent Semantic Kernels.</title>
<date>2001</date>
<booktitle>In Proc. of the International Conference on Machine Learning (ICML),</booktitle>
<pages>66--73</pages>
<contexts>
<context position="12668" citStr="Cristianini et al., 2001" startWordPosition="2004" endWordPosition="2007">ents. Our overall framework, named SeMDocT (Segmentbased MultiLingual Document Clustering via Tensor Modeling), is shown in Algorithm 1. In the following, we shall describe in details each of the steps involved in SeMDocT. 3.1.1 Computing within-document segments Text segmentation is concerned with the fragmentation of an input text into multi-paragraph, contiguous and disjoint blocks that represent subtopics. Regardless of the presence of logical structure clues in the document, linguistic criteria (Beeferman et al., 1999) and statistical similarity measures (Hearst, 1997; Choi et al., 2001; Cristianini et al., 2001) have been mainly used to detect subtopic boundaries between segments. A common assumption is that terms that discuss a subtopic tend to co-occur locally, and a switch to a new subtopic is detected by the ending of co-occurrence of a given set of terms and the beginning of the co-occurrence of another set of terms. Our SeMDocT does not depend on a specific algorithmic choice to perform text segmentation; in this work, we refer to the classic TextTiling (Hearst, 1997), which is the exemplary similarity-block-based method for text segmentation. 3.1.2 Inducing document segment clusters The result</context>
</contexts>
<marker>Cristianini, Shawe-Taylor, Lodhi, 2001</marker>
<rawString>Nello Cristianini, John Shawe-Taylor, and Huma Lodhi. 2001. Latent Semantic Kernels. In Proc. of the International Conference on Machine Learning (ICML), pages 66–73.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian Davidson</author>
<author>Kiri Wagstaff</author>
<author>Sugato Basu</author>
</authors>
<title>Measuring constraint-set utility for partitional clustering algorithms.</title>
<date>2006</date>
<booktitle>In Proc. of the European Conference on Principles and Practice of Knowledge Discovery in Databases (PKDD),</booktitle>
<pages>115--126</pages>
<contexts>
<context position="3531" citStr="Davidson et al., 2006" startWordPosition="516" endWordPosition="519">ar et al., 2011a), which enables the extraction of crosslingual document correlations. In this case, a major issue lies in the definition of a cross-lingual similarity measure that can fit the extracted cross-lingual correlations. Also, from a semi-supervised perspective, other works attempt to define must-link constraints to detect cross-lingual clusters (Yogatama and Tanaka-Ishii, 2009). This implies that, for each different dataset, the set of constraints needs to be redefined; in general, the final results can be negatively affected by the quantity and the quality of involved constraints (Davidson et al., 2006). To the best of our knowledge, existing clustering approaches for comparable corpora are customized for a small set (two or three) of languages (Montalvo et al., 2007). Most of them are not generalizable to many languages as they employ bilingual dictionaries and the translation is performed sequentially considering only pairs of languages. Therefore, the order in which this process is done can seriously impact the results. Another common drawback concerns the way most of the recent approaches perform their analysis: the various languages are analyzed independently of each other (possibly by </context>
</contexts>
<marker>Davidson, Wagstaff, Basu, 2006</marker>
<rawString>Ian Davidson, Kiri Wagstaff, and Sugato Basu. 2006. Measuring constraint-set utility for partitional clustering algorithms. In Proc. of the European Conference on Principles and Practice of Knowledge Discovery in Databases (PKDD), pages 115–126.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dino Ienco</author>
<author>C´eline Robardet</author>
<author>Ruggero G Pensa</author>
<author>Rosa Meo</author>
</authors>
<title>Parameter-less co-clustering for star-structured heterogeneous data. Data Mining and Knowledge Discovery,</title>
<date>2013</date>
<marker>Ienco, Robardet, Pensa, Meo, 2013</marker>
<rawString>Dino Ienco, C´eline Robardet, Ruggero G. Pensa, and Rosa Meo. 2013. Parameter-less co-clustering for star-structured heterogeneous data. Data Mining and Knowledge Discovery, 26(2):217–254.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti A Hearst</author>
</authors>
<title>TextTiling: Segmenting Text into Multi-Paragraph Subtopic Passages.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>1</issue>
<contexts>
<context position="12622" citStr="Hearst, 1997" startWordPosition="1998" endWordPosition="1999">multi-faceted nature of the documents. Our overall framework, named SeMDocT (Segmentbased MultiLingual Document Clustering via Tensor Modeling), is shown in Algorithm 1. In the following, we shall describe in details each of the steps involved in SeMDocT. 3.1.1 Computing within-document segments Text segmentation is concerned with the fragmentation of an input text into multi-paragraph, contiguous and disjoint blocks that represent subtopics. Regardless of the presence of logical structure clues in the document, linguistic criteria (Beeferman et al., 1999) and statistical similarity measures (Hearst, 1997; Choi et al., 2001; Cristianini et al., 2001) have been mainly used to detect subtopic boundaries between segments. A common assumption is that terms that discuss a subtopic tend to co-occur locally, and a switch to a new subtopic is detected by the ending of co-occurrence of a given set of terms and the beginning of the co-occurrence of another set of terms. Our SeMDocT does not depend on a specific algorithmic choice to perform text segmentation; in this work, we refer to the classic TextTiling (Hearst, 1997), which is the exemplary similarity-block-based method for text segmentation. 3.1.2</context>
<context position="27863" citStr="Hearst, 1997" startWordPosition="4531" endWordPosition="4532">pecifically SeMDocT, we varied r1 (cf. Section 3.1.4) from 2 to 30, with increments of 2. To determine the number of segment clusters k, we employed an automatic way as discussed in Section 3.1.2. By varying k from 2 to 40, for Balanced Corpus and Unbalanced Corpus, respectively, the values of k obtained were 22 and 23 under BoS, and 25 and 11 under BoW. As concerns the step of text segmentation, TextTiling requires the setting of some interdependent parameters, particularly the size of the text unit to be compared and the number of words in a token sequence. We used the setting suggested in (Hearst, 1997) and also confirmed in (Tagarelli and Karypis, 2013), i.e., 10 for the text unit size and 20 for the token-sequence size. 4.3 Assessment criteria Performance of the different methods are evaluated using two standard clustering validation criteria, namely F-Measure and Rand Index. Given a document collection D, let P = {Γj}Hj=1 and C = {Ci}Ki=1 denote a reference classification and a clustering solution for D, respectively. The local precision and the local recall of a cluster Ci w.r.t. a class Γj are defined as Pij = |Ci ∩ Γj|/|Ci |and Rij = |Ci ∩ Γj|/|Γj|, respectively. F-Measure (FM) is comp</context>
</contexts>
<marker>Hearst, 1997</marker>
<rawString>Marti A. Hearst. 1997. TextTiling: Segmenting Text into Multi-Paragraph Subtopic Passages. Computational Linguistics, 23(1):33–64.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Young-Min Kim</author>
<author>Massih-Reza Amini</author>
<author>Cyril Goutte</author>
<author>Patrick Gallinari</author>
</authors>
<title>Multi-view clustering of multilingual documents.</title>
<date>2010</date>
<booktitle>In Proc. of the ACM SIGIR International Conference on Research and Development in Information Retrieval (SIGIR),</booktitle>
<pages>821--822</pages>
<contexts>
<context position="2309" citStr="Kim et al., 2010" startWordPosition="332" endWordPosition="335">ber of tools for collaboratively editing through contributors across the world. Multilingual document clustering (MDC) aims to detect clusters in a collection of texts written in different languages. This can aid a variety of applications in cross-lingual information retrieval, including statistical machine translation and corpora alignment. Existing approaches to MDC can be divided in two broad categories, depending on whether a parallel corpus rather than a comparable corpus is used (Kumar et al., 2011c). A parallel corpus is typically comprised of documents with their related translations (Kim et al., 2010). These translations are usually obtained Dino Ienco IRSTEA, UMR TETIS Montpellier, France LIRMM Montpellier, France dino.ienco@irstea.fr through machine translation techniques based on a selected anchor language. Conversely, a comparable corpus is a collection of multilingual documents written over the same set of classes (Ni et al., 2011; Yogatama and Tanaka-Ishii, 2009) without any restriction about translation or perfect correspondence between documents. To mine this kind of corpus, external knowledge is employed to map concepts or terms from a language to another (Kumar et al., 2011c; Kum</context>
</contexts>
<marker>Kim, Amini, Goutte, Gallinari, 2010</marker>
<rawString>Young-Min Kim, Massih-Reza Amini, Cyril Goutte, and Patrick Gallinari. 2010. Multi-view clustering of multilingual documents. In Proc. of the ACM SIGIR International Conference on Research and Development in Information Retrieval (SIGIR), pages 821–822.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tamara G Kolda</author>
<author>Brett W Bader</author>
</authors>
<title>Tensor Decompositions and Applications.</title>
<date>2009</date>
<journal>SIAM Review,</journal>
<volume>51</volume>
<issue>3</issue>
<contexts>
<context position="6017" citStr="Kolda and Bader, 2009" startWordPosition="884" endWordPosition="887">C that exploits BabelNet. The second key element, document segmentation, enables us to simplify the document representation according to their multi-topic nature. Previous research has demonstrated that a segment-based approach can significantly improve document clustering performance (Tagarelli and Karypis, 2013). Moreover, the conceptual representation of the document segments enables the grouping of linguistically different (portions of) documents into topically coherent clusters. The latter aspect is leveraged by the third key element of our proposal, which relies on a tensor-based model (Kolda and Bader, 2009) to effectively handle the high dimensionality and sparseness in text. Tensors are considered as a multi-linear generalization of matrix factorizations, since all dimensions or modes are retained thanks to multi-linear structures which can produce meaningful components. The applicability of tensor analysis has recently attracted growing attention in information retrieval and data mining, including document clustering (e.g., (Liu et al., 2011; Romeo et al., 2013)) and cross-lingual information retrieval (e.g., (Chew et al., 2007)). The rest of the paper is organized as follows. Section 2 provid</context>
</contexts>
<marker>Kolda, Bader, 2009</marker>
<rawString>Tamara G. Kolda and Brett W. Bader. 2009. Tensor Decompositions and Applications. SIAM Review, 51(3):455–500.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Kiran Kumar</author>
<author>G S K Santosh</author>
<author>Vasudeva Varma</author>
</authors>
<title>A language-independent approach to identify the named entities in under-resourced languages and clustering multilingual documents.</title>
<date>2011</date>
<booktitle>In Proc. of the International Conference of the CrossLanguage Evaluation Forum (CLEF),</booktitle>
<pages>74--82</pages>
<contexts>
<context position="2201" citStr="Kumar et al., 2011" startWordPosition="316" endWordPosition="319">ng amount of text data that are written in different languages, also due to the increased popularity of a number of tools for collaboratively editing through contributors across the world. Multilingual document clustering (MDC) aims to detect clusters in a collection of texts written in different languages. This can aid a variety of applications in cross-lingual information retrieval, including statistical machine translation and corpora alignment. Existing approaches to MDC can be divided in two broad categories, depending on whether a parallel corpus rather than a comparable corpus is used (Kumar et al., 2011c). A parallel corpus is typically comprised of documents with their related translations (Kim et al., 2010). These translations are usually obtained Dino Ienco IRSTEA, UMR TETIS Montpellier, France LIRMM Montpellier, France dino.ienco@irstea.fr through machine translation techniques based on a selected anchor language. Conversely, a comparable corpus is a collection of multilingual documents written over the same set of classes (Ni et al., 2011; Yogatama and Tanaka-Ishii, 2009) without any restriction about translation or perfect correspondence between documents. To mine this kind of corpus, </context>
<context position="4215" citStr="Kumar et al., 2011" startWordPosition="620" endWordPosition="623"> comparable corpora are customized for a small set (two or three) of languages (Montalvo et al., 2007). Most of them are not generalizable to many languages as they employ bilingual dictionaries and the translation is performed sequentially considering only pairs of languages. Therefore, the order in which this process is done can seriously impact the results. Another common drawback concerns the way most of the recent approaches perform their analysis: the various languages are analyzed independently of each other (possibly by exploiting external knowledge like Wikipedia to enrich documents (Kumar et al., 2011c; Kumar et al., 2011a)), and then the language-specific results are merged. This two-step analysis however may fail in profitably exploiting cross-language information from the multilingual corpus. Contributions. We address the problem of MDC by proposing a framework that features three key elements, namely: (1) to model documents over a unified conceptual space, with the support of a large-scale multilingual knowledge base; (2) to decompose the multilingual documents into topically-cohesive segments; and (3) to describe the multilingual corpus under a multi-dimensional data structure. 600 Pr</context>
</contexts>
<marker>Kumar, Santosh, Varma, 2011</marker>
<rawString>N. Kiran Kumar, G. S. K. Santosh, and Vasudeva Varma. 2011. A language-independent approach to identify the named entities in under-resourced languages and clustering multilingual documents. In Proc. of the International Conference of the CrossLanguage Evaluation Forum (CLEF), pages 74–82.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Kiran Kumar</author>
<author>G S K Santosh</author>
<author>Vasudeva Varma</author>
</authors>
<title>Effectively mining Wikipedia for clustering multilingual documents.</title>
<date>2011</date>
<booktitle>In Proc. of the International Conference on Applications ofNatural Language to Information Systems (NLDB),</booktitle>
<pages>254--257</pages>
<contexts>
<context position="2201" citStr="Kumar et al., 2011" startWordPosition="316" endWordPosition="319">ng amount of text data that are written in different languages, also due to the increased popularity of a number of tools for collaboratively editing through contributors across the world. Multilingual document clustering (MDC) aims to detect clusters in a collection of texts written in different languages. This can aid a variety of applications in cross-lingual information retrieval, including statistical machine translation and corpora alignment. Existing approaches to MDC can be divided in two broad categories, depending on whether a parallel corpus rather than a comparable corpus is used (Kumar et al., 2011c). A parallel corpus is typically comprised of documents with their related translations (Kim et al., 2010). These translations are usually obtained Dino Ienco IRSTEA, UMR TETIS Montpellier, France LIRMM Montpellier, France dino.ienco@irstea.fr through machine translation techniques based on a selected anchor language. Conversely, a comparable corpus is a collection of multilingual documents written over the same set of classes (Ni et al., 2011; Yogatama and Tanaka-Ishii, 2009) without any restriction about translation or perfect correspondence between documents. To mine this kind of corpus, </context>
<context position="4215" citStr="Kumar et al., 2011" startWordPosition="620" endWordPosition="623"> comparable corpora are customized for a small set (two or three) of languages (Montalvo et al., 2007). Most of them are not generalizable to many languages as they employ bilingual dictionaries and the translation is performed sequentially considering only pairs of languages. Therefore, the order in which this process is done can seriously impact the results. Another common drawback concerns the way most of the recent approaches perform their analysis: the various languages are analyzed independently of each other (possibly by exploiting external knowledge like Wikipedia to enrich documents (Kumar et al., 2011c; Kumar et al., 2011a)), and then the language-specific results are merged. This two-step analysis however may fail in profitably exploiting cross-language information from the multilingual corpus. Contributions. We address the problem of MDC by proposing a framework that features three key elements, namely: (1) to model documents over a unified conceptual space, with the support of a large-scale multilingual knowledge base; (2) to decompose the multilingual documents into topically-cohesive segments; and (3) to describe the multilingual corpus under a multi-dimensional data structure. 600 Pr</context>
</contexts>
<marker>Kumar, Santosh, Varma, 2011</marker>
<rawString>N. Kiran Kumar, G. S. K. Santosh, and Vasudeva Varma. 2011. Effectively mining Wikipedia for clustering multilingual documents. In Proc. of the International Conference on Applications ofNatural Language to Information Systems (NLDB), pages 254– 257.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Kiran Kumar</author>
<author>G S K Santosh</author>
<author>Vasudeva Varma</author>
</authors>
<title>Multilingual document clustering using Wikipedia as external knowledge.</title>
<date>2011</date>
<booktitle>In Proc. of the Information Retrieval Facility Conference (IRFC),</booktitle>
<pages>108--117</pages>
<contexts>
<context position="2201" citStr="Kumar et al., 2011" startWordPosition="316" endWordPosition="319">ng amount of text data that are written in different languages, also due to the increased popularity of a number of tools for collaboratively editing through contributors across the world. Multilingual document clustering (MDC) aims to detect clusters in a collection of texts written in different languages. This can aid a variety of applications in cross-lingual information retrieval, including statistical machine translation and corpora alignment. Existing approaches to MDC can be divided in two broad categories, depending on whether a parallel corpus rather than a comparable corpus is used (Kumar et al., 2011c). A parallel corpus is typically comprised of documents with their related translations (Kim et al., 2010). These translations are usually obtained Dino Ienco IRSTEA, UMR TETIS Montpellier, France LIRMM Montpellier, France dino.ienco@irstea.fr through machine translation techniques based on a selected anchor language. Conversely, a comparable corpus is a collection of multilingual documents written over the same set of classes (Ni et al., 2011; Yogatama and Tanaka-Ishii, 2009) without any restriction about translation or perfect correspondence between documents. To mine this kind of corpus, </context>
<context position="4215" citStr="Kumar et al., 2011" startWordPosition="620" endWordPosition="623"> comparable corpora are customized for a small set (two or three) of languages (Montalvo et al., 2007). Most of them are not generalizable to many languages as they employ bilingual dictionaries and the translation is performed sequentially considering only pairs of languages. Therefore, the order in which this process is done can seriously impact the results. Another common drawback concerns the way most of the recent approaches perform their analysis: the various languages are analyzed independently of each other (possibly by exploiting external knowledge like Wikipedia to enrich documents (Kumar et al., 2011c; Kumar et al., 2011a)), and then the language-specific results are merged. This two-step analysis however may fail in profitably exploiting cross-language information from the multilingual corpus. Contributions. We address the problem of MDC by proposing a framework that features three key elements, namely: (1) to model documents over a unified conceptual space, with the support of a large-scale multilingual knowledge base; (2) to decompose the multilingual documents into topically-cohesive segments; and (3) to describe the multilingual corpus under a multi-dimensional data structure. 600 Pr</context>
</contexts>
<marker>Kumar, Santosh, Varma, 2011</marker>
<rawString>N. Kiran Kumar, G. S. K. Santosh, and Vasudeva Varma. 2011. Multilingual document clustering using Wikipedia as external knowledge. In Proc. of the Information Retrieval Facility Conference (IRFC), pages 108–117.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lieven De Lathauwer</author>
<author>Bart De Moor</author>
<author>Joos Vandewalle</author>
</authors>
<title>A Multilinear Singular Value Decomposition.</title>
<date>2000</date>
<journal>SIAM Journal on Matrix Analysis and Applications,</journal>
<volume>21</volume>
<issue>4</issue>
<marker>De Lathauwer, De Moor, Vandewalle, 2000</marker>
<rawString>Lieven De Lathauwer, Bart De Moor, and Joos Vandewalle. 2000. A Multilinear Singular Value Decomposition. SIAM Journal on Matrix Analysis and Applications, 21(4):1253–1278.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xinhai Liu</author>
<author>Wolfgang Gl¨anzel</author>
<author>Bart De Moor</author>
</authors>
<title>Hybrid clustering of multi-view data via Tucker-2 model and its application.</title>
<date>2011</date>
<journal>Scientometrics,</journal>
<volume>88</volume>
<issue>3</issue>
<marker>Liu, Gl¨anzel, De Moor, 2011</marker>
<rawString>Xinhai Liu, Wolfgang Gl¨anzel, and Bart De Moor. 2011. Hybrid clustering of multi-view data via Tucker-2 model and its application. Scientometrics, 88(3):819–839.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Paul Tarau</author>
<author>Elizabeth Figa</author>
</authors>
<title>PageRank on Semantic Networks, with Application to Word Sense Disambiguation.</title>
<date>2004</date>
<booktitle>In Proc. of the International Conference on Computational Linguistics (COLING).</booktitle>
<contexts>
<context position="20360" citStr="Mihalcea et al., 2004" startWordPosition="3275" endWordPosition="3278">retrieved BabelNet synsets. As previously discussed in Section 2.1, BabelNet provides WSD algorithms for multilingual corpora. More specifically, the authors in (Navigli and Ponzetto, 2012b) suggest to use the Degree algorithm (Navigli and Lapata, 2010), as it showed to yield highly competitive performance in a multilingual context as well. Note that the Degree algorithm, given a semantic graph for the input context, simply selects the sense of the target word with the highest vertex degree. Clearly, other graph-based methods for (unsupervised) WSD, particularly PageRank-style methods (e.g., (Mihalcea et al., 2004; Agirre and Soroa, 2009; Yeh et al., 2009; Tsatsaronis et al., 2010)), can be plugged in to address the multilingual WSD task based on BabelNet. An investigation of the performance of existing WSD algorithms for a multilingual context is however out of the scope of this paper. 4 Evaluation Methodology In order to evaluate our proposal we need a multilingual comparable document collection with annotated documents 603 RCV2 Topics English French Italian Balanced Corpus C15 - PERFORMANCE 850 850 850 C18 - OWNERSHIP CHANGES 850 850 850 E11 - ECONOMIC PERFORMANCE 850 850 850 E12 - MONETARY/ECONOMIC</context>
</contexts>
<marker>Mihalcea, Tarau, Figa, 2004</marker>
<rawString>Rada Mihalcea, Paul Tarau, and Elizabeth Figa. 2004. PageRank on Semantic Networks, with Application to Word Sense Disambiguation. In Proc. of the International Conference on Computational Linguistics (COLING).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Soto Montalvo</author>
<author>Raquel Martfnez-Unanue</author>
<author>Arantza Casillas</author>
<author>Vfctor Fresno</author>
</authors>
<title>Multilingual news clustering: Feature translation vs. identification of cognate named entities.</title>
<date>2007</date>
<journal>Pattern Recognition Letters,</journal>
<volume>28</volume>
<issue>16</issue>
<contexts>
<context position="3699" citStr="Montalvo et al., 2007" startWordPosition="544" endWordPosition="547">measure that can fit the extracted cross-lingual correlations. Also, from a semi-supervised perspective, other works attempt to define must-link constraints to detect cross-lingual clusters (Yogatama and Tanaka-Ishii, 2009). This implies that, for each different dataset, the set of constraints needs to be redefined; in general, the final results can be negatively affected by the quantity and the quality of involved constraints (Davidson et al., 2006). To the best of our knowledge, existing clustering approaches for comparable corpora are customized for a small set (two or three) of languages (Montalvo et al., 2007). Most of them are not generalizable to many languages as they employ bilingual dictionaries and the translation is performed sequentially considering only pairs of languages. Therefore, the order in which this process is done can seriously impact the results. Another common drawback concerns the way most of the recent approaches perform their analysis: the various languages are analyzed independently of each other (possibly by exploiting external knowledge like Wikipedia to enrich documents (Kumar et al., 2011c; Kumar et al., 2011a)), and then the language-specific results are merged. This tw</context>
</contexts>
<marker>Montalvo, Martfnez-Unanue, Casillas, Fresno, 2007</marker>
<rawString>Soto Montalvo, Raquel Martfnez-Unanue, Arantza Casillas, and Vfctor Fresno. 2007. Multilingual news clustering: Feature translation vs. identification of cognate named entities. Pattern Recognition Letters, 28(16):2305–2311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
<author>Mirella Lapata</author>
</authors>
<title>An Experimental Study of Graph Connectivity for Unsupervised Word Sense Disambiguation.</title>
<date>2010</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>32</volume>
<issue>4</issue>
<contexts>
<context position="19992" citStr="Navigli and Lapata, 2010" startWordPosition="3217" endWordPosition="3220">et Qw for (w, POS(w)) contextually to sen. The WSD algorithm is carried out in such a way that all words from all languages are disambiguated over the same concept inventory, producing a languageindependent feature space for the whole multilingual corpus. Each segment is finally modeled as a |13S|- dimensional vector of BabelNet synset frequencies, being 13S the set of retrieved BabelNet synsets. As previously discussed in Section 2.1, BabelNet provides WSD algorithms for multilingual corpora. More specifically, the authors in (Navigli and Ponzetto, 2012b) suggest to use the Degree algorithm (Navigli and Lapata, 2010), as it showed to yield highly competitive performance in a multilingual context as well. Note that the Degree algorithm, given a semantic graph for the input context, simply selects the sense of the target word with the highest vertex degree. Clearly, other graph-based methods for (unsupervised) WSD, particularly PageRank-style methods (e.g., (Mihalcea et al., 2004; Agirre and Soroa, 2009; Yeh et al., 2009; Tsatsaronis et al., 2010)), can be plugged in to address the multilingual WSD task based on BabelNet. An investigation of the performance of existing WSD algorithms for a multilingual cont</context>
</contexts>
<marker>Navigli, Lapata, 2010</marker>
<rawString>Roberto Navigli and Mirella Lapata. 2010. An Experimental Study of Graph Connectivity for Unsupervised Word Sense Disambiguation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 32(4):678–692.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
<author>Simone P Ponzetto</author>
</authors>
<title>BabelNet: The Automatic Construction, Evaluation and Application of a Wide-Coverage Multilingual Semantic Network.</title>
<date>2012</date>
<journal>Artificial Intelligence,</journal>
<pages>193--217</pages>
<contexts>
<context position="5292" citStr="Navigli and Ponzetto, 2012" startWordPosition="776" endWordPosition="779">multilingual documents into topically-cohesive segments; and (3) to describe the multilingual corpus under a multi-dimensional data structure. 600 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 600–609, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics The first key element prevents loss of information due to the translation of documents from different languages to a target one. It enables a conceptual representation of the documents in a language-independent way preserving the content semantics. BabelNet (Navigli and Ponzetto, 2012a) is used as multilingual knowledge base. To the extent of our knowledge, this is the first work in MDC that exploits BabelNet. The second key element, document segmentation, enables us to simplify the document representation according to their multi-topic nature. Previous research has demonstrated that a segment-based approach can significantly improve document clustering performance (Tagarelli and Karypis, 2013). Moreover, the conceptual representation of the document segments enables the grouping of linguistically different (portions of) documents into topically coherent clusters. The latt</context>
<context position="6960" citStr="Navigli and Ponzetto, 2012" startWordPosition="1030" endWordPosition="1033">tly attracted growing attention in information retrieval and data mining, including document clustering (e.g., (Liu et al., 2011; Romeo et al., 2013)) and cross-lingual information retrieval (e.g., (Chew et al., 2007)). The rest of the paper is organized as follows. Section 2 provides an overview of BabelNet and basic notions on tensors. We describe our proposal in Section 3. Data and experimental settings are described in Section 4, while results are presented in Section 5. We summarize our main findings in Section 6, finally Section 7 concludes the paper. 2 Background 2.1 BabelNet BabelNet (Navigli and Ponzetto, 2012a) is a multilingual semantic network obtained by linking Wikipedia with WordNet, that is, the largest multilingual Web encyclopedia and the most popular computational lexicon. The linking of the two knowledge bases was performed through an automatic mapping of WordNet synsets and Wikipages, harvesting multilingual lexicalization of the available concepts through humangenerated translations provided by the Wikipedia interlanguage links or through machine translation techniques. The result is an encyclopedic dictionary containing concepts and named entities lexicalized in 50 different languages</context>
<context position="19927" citStr="Navigli and Ponzetto, 2012" startWordPosition="3207" endWordPosition="3210">each pair (w, POS(w)) to detect the most appropriate BabelNet synset Qw for (w, POS(w)) contextually to sen. The WSD algorithm is carried out in such a way that all words from all languages are disambiguated over the same concept inventory, producing a languageindependent feature space for the whole multilingual corpus. Each segment is finally modeled as a |13S|- dimensional vector of BabelNet synset frequencies, being 13S the set of retrieved BabelNet synsets. As previously discussed in Section 2.1, BabelNet provides WSD algorithms for multilingual corpora. More specifically, the authors in (Navigli and Ponzetto, 2012b) suggest to use the Degree algorithm (Navigli and Lapata, 2010), as it showed to yield highly competitive performance in a multilingual context as well. Note that the Degree algorithm, given a semantic graph for the input context, simply selects the sense of the target word with the highest vertex degree. Clearly, other graph-based methods for (unsupervised) WSD, particularly PageRank-style methods (e.g., (Mihalcea et al., 2004; Agirre and Soroa, 2009; Yeh et al., 2009; Tsatsaronis et al., 2010)), can be plugged in to address the multilingual WSD task based on BabelNet. An investigation of t</context>
<context position="25969" citStr="Navigli and Ponzetto, 2012" startWordPosition="4223" endWordPosition="4226">es are composed, on average, by only 2 segments. Table 4 shows the average length of segments per document for both space representations. Generally, segments in the BoS representation are smaller than the corresponding segments in the BoW space. More in detail, if we consider the ratio between the segment length in BoS and the one in BoW, this ratio is around 2/3 for the English language, while for both French and Italian it varies between 1/4 and 1/3. This disequilibrium is induced by the multilingual concept coverage of BabelNet, as stated by its authors (Navigli and Ponzetto, 604 2012a), (Navigli and Ponzetto, 2012b). In particular, the WSD process tightly depends from the concept coverage supplied from the language-specific knowledge base. 4.2 Competing methods and settings We compare our SeMDocT with two standard approaches, namely Bisecting K-Means (Steinbach et al., 2000), and Latent Semantic Analysis (LSA)-based document clustering (for short, LSA). Given a number K of desired clusters, Bisecting K-Means produces a Kway clustering solution by performing a sequence of K-1 repeated bisections based on standard K-Means algorithm. This process continues until the number K of clusters is found. LSA perf</context>
</contexts>
<marker>Navigli, Ponzetto, 2012</marker>
<rawString>Roberto Navigli and Simone P. Ponzetto. 2012. BabelNet: The Automatic Construction, Evaluation and Application of a Wide-Coverage Multilingual Semantic Network. Artificial Intelligence, 193:217-250.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
<author>Simone P Ponzetto</author>
</authors>
<title>Multilingual WSD with Just a Few Lines of Code: The BabelNet API.</title>
<date>2012</date>
<booktitle>In Proc. of the System Demonstrations of the Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>67--72</pages>
<contexts>
<context position="5292" citStr="Navigli and Ponzetto, 2012" startWordPosition="776" endWordPosition="779">multilingual documents into topically-cohesive segments; and (3) to describe the multilingual corpus under a multi-dimensional data structure. 600 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 600–609, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics The first key element prevents loss of information due to the translation of documents from different languages to a target one. It enables a conceptual representation of the documents in a language-independent way preserving the content semantics. BabelNet (Navigli and Ponzetto, 2012a) is used as multilingual knowledge base. To the extent of our knowledge, this is the first work in MDC that exploits BabelNet. The second key element, document segmentation, enables us to simplify the document representation according to their multi-topic nature. Previous research has demonstrated that a segment-based approach can significantly improve document clustering performance (Tagarelli and Karypis, 2013). Moreover, the conceptual representation of the document segments enables the grouping of linguistically different (portions of) documents into topically coherent clusters. The latt</context>
<context position="6960" citStr="Navigli and Ponzetto, 2012" startWordPosition="1030" endWordPosition="1033">tly attracted growing attention in information retrieval and data mining, including document clustering (e.g., (Liu et al., 2011; Romeo et al., 2013)) and cross-lingual information retrieval (e.g., (Chew et al., 2007)). The rest of the paper is organized as follows. Section 2 provides an overview of BabelNet and basic notions on tensors. We describe our proposal in Section 3. Data and experimental settings are described in Section 4, while results are presented in Section 5. We summarize our main findings in Section 6, finally Section 7 concludes the paper. 2 Background 2.1 BabelNet BabelNet (Navigli and Ponzetto, 2012a) is a multilingual semantic network obtained by linking Wikipedia with WordNet, that is, the largest multilingual Web encyclopedia and the most popular computational lexicon. The linking of the two knowledge bases was performed through an automatic mapping of WordNet synsets and Wikipages, harvesting multilingual lexicalization of the available concepts through humangenerated translations provided by the Wikipedia interlanguage links or through machine translation techniques. The result is an encyclopedic dictionary containing concepts and named entities lexicalized in 50 different languages</context>
<context position="19927" citStr="Navigli and Ponzetto, 2012" startWordPosition="3207" endWordPosition="3210">each pair (w, POS(w)) to detect the most appropriate BabelNet synset Qw for (w, POS(w)) contextually to sen. The WSD algorithm is carried out in such a way that all words from all languages are disambiguated over the same concept inventory, producing a languageindependent feature space for the whole multilingual corpus. Each segment is finally modeled as a |13S|- dimensional vector of BabelNet synset frequencies, being 13S the set of retrieved BabelNet synsets. As previously discussed in Section 2.1, BabelNet provides WSD algorithms for multilingual corpora. More specifically, the authors in (Navigli and Ponzetto, 2012b) suggest to use the Degree algorithm (Navigli and Lapata, 2010), as it showed to yield highly competitive performance in a multilingual context as well. Note that the Degree algorithm, given a semantic graph for the input context, simply selects the sense of the target word with the highest vertex degree. Clearly, other graph-based methods for (unsupervised) WSD, particularly PageRank-style methods (e.g., (Mihalcea et al., 2004; Agirre and Soroa, 2009; Yeh et al., 2009; Tsatsaronis et al., 2010)), can be plugged in to address the multilingual WSD task based on BabelNet. An investigation of t</context>
<context position="25969" citStr="Navigli and Ponzetto, 2012" startWordPosition="4223" endWordPosition="4226">es are composed, on average, by only 2 segments. Table 4 shows the average length of segments per document for both space representations. Generally, segments in the BoS representation are smaller than the corresponding segments in the BoW space. More in detail, if we consider the ratio between the segment length in BoS and the one in BoW, this ratio is around 2/3 for the English language, while for both French and Italian it varies between 1/4 and 1/3. This disequilibrium is induced by the multilingual concept coverage of BabelNet, as stated by its authors (Navigli and Ponzetto, 604 2012a), (Navigli and Ponzetto, 2012b). In particular, the WSD process tightly depends from the concept coverage supplied from the language-specific knowledge base. 4.2 Competing methods and settings We compare our SeMDocT with two standard approaches, namely Bisecting K-Means (Steinbach et al., 2000), and Latent Semantic Analysis (LSA)-based document clustering (for short, LSA). Given a number K of desired clusters, Bisecting K-Means produces a Kway clustering solution by performing a sequence of K-1 repeated bisections based on standard K-Means algorithm. This process continues until the number K of clusters is found. LSA perf</context>
</contexts>
<marker>Navigli, Ponzetto, 2012</marker>
<rawString>Roberto Navigli and Simone P. Ponzetto. 2012. Multilingual WSD with Just a Few Lines of Code: The BabelNet API. In Proc. of the System Demonstrations of the Annual Meeting of the Association for Computational Linguistics (ACL), pages 67–72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaochuan Ni</author>
<author>Jian-Tao Sun</author>
<author>Jian Hu</author>
<author>Zheng Chen</author>
</authors>
<title>Cross lingual text classification by mining multilingual topics from Wikipedia.</title>
<date>2011</date>
<booktitle>In Proc. of the ACM International Conference on Web Search and Web Data Mining (WSDM),</booktitle>
<pages>375--384</pages>
<contexts>
<context position="2650" citStr="Ni et al., 2011" startWordPosition="381" endWordPosition="384"> Existing approaches to MDC can be divided in two broad categories, depending on whether a parallel corpus rather than a comparable corpus is used (Kumar et al., 2011c). A parallel corpus is typically comprised of documents with their related translations (Kim et al., 2010). These translations are usually obtained Dino Ienco IRSTEA, UMR TETIS Montpellier, France LIRMM Montpellier, France dino.ienco@irstea.fr through machine translation techniques based on a selected anchor language. Conversely, a comparable corpus is a collection of multilingual documents written over the same set of classes (Ni et al., 2011; Yogatama and Tanaka-Ishii, 2009) without any restriction about translation or perfect correspondence between documents. To mine this kind of corpus, external knowledge is employed to map concepts or terms from a language to another (Kumar et al., 2011c; Kumar et al., 2011a), which enables the extraction of crosslingual document correlations. In this case, a major issue lies in the definition of a cross-lingual similarity measure that can fit the extracted cross-lingual correlations. Also, from a semi-supervised perspective, other works attempt to define must-link constraints to detect cross-</context>
</contexts>
<marker>Ni, Sun, Hu, Chen, 2011</marker>
<rawString>Xiaochuan Ni, Jian-Tao Sun, Jian Hu, and Zheng Chen. 2011. Cross lingual text classification by mining multilingual topics from Wikipedia. In Proc. of the ACM International Conference on Web Search and Web Data Mining (WSDM), pages 375–384.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Llufs Padr´o</author>
<author>Evgeny Stanilovsky</author>
</authors>
<title>FreeLing 3.0: Towards Wider Multilinguality.</title>
<date>2012</date>
<booktitle>In Proc. of the Language Resources and Evaluation Conference (LREC).</booktitle>
<marker>Padr´o, Stanilovsky, 2012</marker>
<rawString>Llufs Padr´o and Evgeny Stanilovsky. 2012. FreeLing 3.0: Towards Wider Multilinguality. In Proc. of the Language Resources and Evaluation Conference (LREC).</rawString>
</citation>
<citation valid="true">
<authors>
<author>William M Rand</author>
</authors>
<title>Objective criteria for the evaluation of clustering methods.</title>
<date>1971</date>
<journal>Journal of the American Statistical Association,</journal>
<pages>66--846</pages>
<contexts>
<context position="28591" citStr="Rand, 1971" startWordPosition="4653" endWordPosition="4654">ize. 4.3 Assessment criteria Performance of the different methods are evaluated using two standard clustering validation criteria, namely F-Measure and Rand Index. Given a document collection D, let P = {Γj}Hj=1 and C = {Ci}Ki=1 denote a reference classification and a clustering solution for D, respectively. The local precision and the local recall of a cluster Ci w.r.t. a class Γj are defined as Pij = |Ci ∩ Γj|/|Ci |and Rij = |Ci ∩ Γj|/|Γj|, respectively. F-Measure (FM) is computed as follows (Steinbach et al., 2000): |Γj| |D |maxi=1...K{Fij} where Fij = 2PijRij/(Pij + Rij). Rand Index (RI) (Rand, 1971) measures the percentage of decisions that are correct, penalizing false positive and false negative decisions during clustering. It takes into account the following quantities: TP, i.e., the number of pairs of documents that are in the same cluster in C and in the same class in P; TN, i.e., the number of pairs of documents that are in different clusters in C and in different classes in P; FN, i.e., the number of pairs of documents that are in different clusters in C and in the same class in P; and FP, i.e., the number of pairs of documents that are in the same cluster in C and in different cl</context>
</contexts>
<marker>Rand, 1971</marker>
<rawString>William M. Rand. 1971. Objective criteria for the evaluation of clustering methods. Journal of the American Statistical Association, 66:846–850.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Salvatore Romeo</author>
<author>Andrea Tagarelli</author>
<author>Francesco Gullo</author>
<author>Sergio Greco</author>
</authors>
<title>A Tensor-based Clustering Approach for Multiple Document Classifications.</title>
<date>2013</date>
<booktitle>In Proc. of the International Conference on Pattern Recognition Applications and Methods (ICPRAM),</booktitle>
<pages>200--205</pages>
<contexts>
<context position="6483" citStr="Romeo et al., 2013" startWordPosition="950" endWordPosition="953"> coherent clusters. The latter aspect is leveraged by the third key element of our proposal, which relies on a tensor-based model (Kolda and Bader, 2009) to effectively handle the high dimensionality and sparseness in text. Tensors are considered as a multi-linear generalization of matrix factorizations, since all dimensions or modes are retained thanks to multi-linear structures which can produce meaningful components. The applicability of tensor analysis has recently attracted growing attention in information retrieval and data mining, including document clustering (e.g., (Liu et al., 2011; Romeo et al., 2013)) and cross-lingual information retrieval (e.g., (Chew et al., 2007)). The rest of the paper is organized as follows. Section 2 provides an overview of BabelNet and basic notions on tensors. We describe our proposal in Section 3. Data and experimental settings are described in Section 4, while results are presented in Section 5. We summarize our main findings in Section 6, finally Section 7 concludes the paper. 2 Background 2.1 BabelNet BabelNet (Navigli and Ponzetto, 2012a) is a multilingual semantic network obtained by linking Wikipedia with WordNet, that is, the largest multilingual Web enc</context>
</contexts>
<marker>Romeo, Tagarelli, Gullo, Greco, 2013</marker>
<rawString>Salvatore Romeo, Andrea Tagarelli, Francesco Gullo, and Sergio Greco. 2013. A Tensor-based Clustering Approach for Multiple Document Classifications. In Proc. of the International Conference on Pattern Recognition Applications and Methods (ICPRAM), pages 200–205.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stan Salvador</author>
<author>Philip Chan</author>
</authors>
<title>Determining the Number of Clusters/Segments in Hierarchical Clustering/Segmentation Algorithms.</title>
<date>2004</date>
<booktitle>In Proc. of the International Conference on Tools with Artificial Intelligence (ICTAI),</booktitle>
<pages>576--584</pages>
<contexts>
<context position="14579" citStr="Salvador and Chan, 2004" startWordPosition="2308" endWordPosition="2311">nput to a document clustering algorithm to produce a clustering of the segments CS = {C3i }k1. The obtained clusters of segments can be disjoint or overlapping. Again, our SeMDocT is parametric to the clustering algorithm as well; here, we resort to a state-of-the-art clustering algorithm, namely Bisecting K-Means (Steinbach et al., 2000), which is widely known to produce high-quality (hard) clustering solutions in high-dimensional, large datasets (Zhao and Karypis, 2004). Note however that it requires as input the number of clusters. To cope with this issue, we adopt the method described in (Salvador and Chan, 2004), which explores how the within-cluster cohesion changes by varying the number of clusters. The number of clusters for which the slope of the plot changes drastically is chosen as a suitable value for the clustering algorithm. 3.1.3 Segment-cluster based representation Upon the segment clustering, each document is represented by its segments assigned to possibly multiple segment clusters. Therefore, we derive a documentfeature matrix for each of the k segment clusters. The features correspond either to the BoW or BoS model, according to the choice made for the segment representation. Let us de</context>
</contexts>
<marker>Salvador, Chan, 2004</marker>
<rawString>Stan Salvador and Philip Chan. 2004. Determining the Number of Clusters/Segments in Hierarchical Clustering/Segmentation Algorithms. In Proc. of the International Conference on Tools with Artificial Intelligence (ICTAI), pages 576–584.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Steinbach</author>
<author>George Karypis</author>
<author>Vipin Kumar</author>
</authors>
<title>A Comparison of Document Clustering Techniques.</title>
<date>2000</date>
<booktitle>In Proc. of the KDD Workshop on Text Mining.</booktitle>
<contexts>
<context position="14295" citStr="Steinbach et al., 2000" startWordPosition="2264" endWordPosition="2267">sults from the union of the vocabularies of the different languages; and bag-of-synsets (henceforth BoS), whereby features correspond to BabelNet synsets. We shall devote Section 3.2 to a detailed description of our proposed BoS representation. The segment collection S is given in input to a document clustering algorithm to produce a clustering of the segments CS = {C3i }k1. The obtained clusters of segments can be disjoint or overlapping. Again, our SeMDocT is parametric to the clustering algorithm as well; here, we resort to a state-of-the-art clustering algorithm, namely Bisecting K-Means (Steinbach et al., 2000), which is widely known to produce high-quality (hard) clustering solutions in high-dimensional, large datasets (Zhao and Karypis, 2004). Note however that it requires as input the number of clusters. To cope with this issue, we adopt the method described in (Salvador and Chan, 2004), which explores how the within-cluster cohesion changes by varying the number of clusters. The number of clusters for which the slope of the plot changes drastically is chosen as a suitable value for the clustering algorithm. 3.1.3 Segment-cluster based representation Upon the segment clustering, each document is </context>
<context position="26235" citStr="Steinbach et al., 2000" startWordPosition="4262" endWordPosition="4265">consider the ratio between the segment length in BoS and the one in BoW, this ratio is around 2/3 for the English language, while for both French and Italian it varies between 1/4 and 1/3. This disequilibrium is induced by the multilingual concept coverage of BabelNet, as stated by its authors (Navigli and Ponzetto, 604 2012a), (Navigli and Ponzetto, 2012b). In particular, the WSD process tightly depends from the concept coverage supplied from the language-specific knowledge base. 4.2 Competing methods and settings We compare our SeMDocT with two standard approaches, namely Bisecting K-Means (Steinbach et al., 2000), and Latent Semantic Analysis (LSA)-based document clustering (for short, LSA). Given a number K of desired clusters, Bisecting K-Means produces a Kway clustering solution by performing a sequence of K-1 repeated bisections based on standard K-Means algorithm. This process continues until the number K of clusters is found. LSA performs a decomposition of the document collection matrix through Singular Value Decomposition in order to extract a more concise and descriptive representation of the documents. After this step, Bisecting K-Means is applied over the new document space to get the final</context>
<context position="28503" citStr="Steinbach et al., 2000" startWordPosition="4637" endWordPosition="4640">ed in (Tagarelli and Karypis, 2013), i.e., 10 for the text unit size and 20 for the token-sequence size. 4.3 Assessment criteria Performance of the different methods are evaluated using two standard clustering validation criteria, namely F-Measure and Rand Index. Given a document collection D, let P = {Γj}Hj=1 and C = {Ci}Ki=1 denote a reference classification and a clustering solution for D, respectively. The local precision and the local recall of a cluster Ci w.r.t. a class Γj are defined as Pij = |Ci ∩ Γj|/|Ci |and Rij = |Ci ∩ Γj|/|Γj|, respectively. F-Measure (FM) is computed as follows (Steinbach et al., 2000): |Γj| |D |maxi=1...K{Fij} where Fij = 2PijRij/(Pij + Rij). Rand Index (RI) (Rand, 1971) measures the percentage of decisions that are correct, penalizing false positive and false negative decisions during clustering. It takes into account the following quantities: TP, i.e., the number of pairs of documents that are in the same cluster in C and in the same class in P; TN, i.e., the number of pairs of documents that are in different clusters in C and in different classes in P; FN, i.e., the number of pairs of documents that are in different clusters in C and in the same class in P; and FP, i.e.</context>
</contexts>
<marker>Steinbach, Karypis, Kumar, 2000</marker>
<rawString>Michael Steinbach, George Karypis, and Vipin Kumar. 2000. A Comparison of Document Clustering Techniques. In Proc. of the KDD Workshop on Text Mining.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrea Tagarelli</author>
<author>George Karypis</author>
</authors>
<title>A segment-based approach to clustering multi-topic documents.</title>
<date>2013</date>
<journal>Knowledge and Information Systems,</journal>
<volume>34</volume>
<issue>3</issue>
<contexts>
<context position="5710" citStr="Tagarelli and Karypis, 2013" startWordPosition="837" endWordPosition="840">f documents from different languages to a target one. It enables a conceptual representation of the documents in a language-independent way preserving the content semantics. BabelNet (Navigli and Ponzetto, 2012a) is used as multilingual knowledge base. To the extent of our knowledge, this is the first work in MDC that exploits BabelNet. The second key element, document segmentation, enables us to simplify the document representation according to their multi-topic nature. Previous research has demonstrated that a segment-based approach can significantly improve document clustering performance (Tagarelli and Karypis, 2013). Moreover, the conceptual representation of the document segments enables the grouping of linguistically different (portions of) documents into topically coherent clusters. The latter aspect is leveraged by the third key element of our proposal, which relies on a tensor-based model (Kolda and Bader, 2009) to effectively handle the high dimensionality and sparseness in text. Tensors are considered as a multi-linear generalization of matrix factorizations, since all dimensions or modes are retained thanks to multi-linear structures which can produce meaningful components. The applicability of t</context>
<context position="27915" citStr="Tagarelli and Karypis, 2013" startWordPosition="4537" endWordPosition="4540">. Section 3.1.4) from 2 to 30, with increments of 2. To determine the number of segment clusters k, we employed an automatic way as discussed in Section 3.1.2. By varying k from 2 to 40, for Balanced Corpus and Unbalanced Corpus, respectively, the values of k obtained were 22 and 23 under BoS, and 25 and 11 under BoW. As concerns the step of text segmentation, TextTiling requires the setting of some interdependent parameters, particularly the size of the text unit to be compared and the number of words in a token sequence. We used the setting suggested in (Hearst, 1997) and also confirmed in (Tagarelli and Karypis, 2013), i.e., 10 for the text unit size and 20 for the token-sequence size. 4.3 Assessment criteria Performance of the different methods are evaluated using two standard clustering validation criteria, namely F-Measure and Rand Index. Given a document collection D, let P = {Γj}Hj=1 and C = {Ci}Ki=1 denote a reference classification and a clustering solution for D, respectively. The local precision and the local recall of a cluster Ci w.r.t. a class Γj are defined as Pij = |Ci ∩ Γj|/|Ci |and Rij = |Ci ∩ Γj|/|Γj|, respectively. F-Measure (FM) is computed as follows (Steinbach et al., 2000): |Γj| |D |m</context>
</contexts>
<marker>Tagarelli, Karypis, 2013</marker>
<rawString>Andrea Tagarelli and George Karypis. 2013. A segment-based approach to clustering multi-topic documents. Knowledge and Information Systems, 34(3):563–595.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Tsatsaronis</author>
<author>Iraklis Varlamis</author>
<author>Kjetil Nørv˚ag</author>
</authors>
<title>SemanticRank: Ranking Keywords and Sentences Using Semantic Graphs.</title>
<date>2010</date>
<booktitle>In Proc. of the International Conference on Computational Linguistics (COLING),</booktitle>
<pages>1074--1082</pages>
<marker>Tsatsaronis, Varlamis, Nørv˚ag, 2010</marker>
<rawString>George Tsatsaronis, Iraklis Varlamis, and Kjetil Nørv˚ag. 2010. SemanticRank: Ranking Keywords and Sentences Using Semantic Graphs. In Proc. of the International Conference on Computational Linguistics (COLING), pages 1074–1082.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chih-Ping Wei</author>
<author>Christopher C Yang</author>
<author>Chia-Min Lin</author>
</authors>
<title>A Latent Semantic Indexing-based Approach to Multilingual Document Clustering. Decision Support Systems,</title>
<date>2008</date>
<pages>45--3</pages>
<marker>Wei, Yang, Lin, 2008</marker>
<rawString>Chih-Ping Wei, Christopher C. Yang, and Chia-Min Lin. 2008. A Latent Semantic Indexing-based Approach to Multilingual Document Clustering. Decision Support Systems, 45(3):606–620.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Yeh</author>
<author>Daniel Ramage</author>
<author>Christopher D Manning</author>
<author>Eneko Agirre</author>
<author>Aitor Soroa</author>
</authors>
<title>WikiWalk: Random walks on Wikipedia for Semantic Relatedness.</title>
<date>2009</date>
<booktitle>In Proc. of the ACL Workshop on Graph-based Methods for Natural Language Processing,</booktitle>
<pages>41--49</pages>
<contexts>
<context position="20402" citStr="Yeh et al., 2009" startWordPosition="3283" endWordPosition="3286">ssed in Section 2.1, BabelNet provides WSD algorithms for multilingual corpora. More specifically, the authors in (Navigli and Ponzetto, 2012b) suggest to use the Degree algorithm (Navigli and Lapata, 2010), as it showed to yield highly competitive performance in a multilingual context as well. Note that the Degree algorithm, given a semantic graph for the input context, simply selects the sense of the target word with the highest vertex degree. Clearly, other graph-based methods for (unsupervised) WSD, particularly PageRank-style methods (e.g., (Mihalcea et al., 2004; Agirre and Soroa, 2009; Yeh et al., 2009; Tsatsaronis et al., 2010)), can be plugged in to address the multilingual WSD task based on BabelNet. An investigation of the performance of existing WSD algorithms for a multilingual context is however out of the scope of this paper. 4 Evaluation Methodology In order to evaluate our proposal we need a multilingual comparable document collection with annotated documents 603 RCV2 Topics English French Italian Balanced Corpus C15 - PERFORMANCE 850 850 850 C18 - OWNERSHIP CHANGES 850 850 850 E11 - ECONOMIC PERFORMANCE 850 850 850 E12 - MONETARY/ECONOMIC 850 850 850 M11 - EQUITY MARKETS 850 850 </context>
</contexts>
<marker>Yeh, Ramage, Manning, Agirre, Soroa, 2009</marker>
<rawString>Eric Yeh, Daniel Ramage, Christopher D. Manning, Eneko Agirre, and Aitor Soroa. 2009. WikiWalk: Random walks on Wikipedia for Semantic Relatedness. In Proc. of the ACL Workshop on Graph-based Methods for Natural Language Processing, pages 41–49.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dani Yogatama</author>
<author>Kumiko Tanaka-Ishii</author>
</authors>
<title>Multilingual spectral clustering using document similarity propagation.</title>
<date>2009</date>
<booktitle>In Proc. of the International Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>871--879</pages>
<contexts>
<context position="2684" citStr="Yogatama and Tanaka-Ishii, 2009" startWordPosition="385" endWordPosition="389">hes to MDC can be divided in two broad categories, depending on whether a parallel corpus rather than a comparable corpus is used (Kumar et al., 2011c). A parallel corpus is typically comprised of documents with their related translations (Kim et al., 2010). These translations are usually obtained Dino Ienco IRSTEA, UMR TETIS Montpellier, France LIRMM Montpellier, France dino.ienco@irstea.fr through machine translation techniques based on a selected anchor language. Conversely, a comparable corpus is a collection of multilingual documents written over the same set of classes (Ni et al., 2011; Yogatama and Tanaka-Ishii, 2009) without any restriction about translation or perfect correspondence between documents. To mine this kind of corpus, external knowledge is employed to map concepts or terms from a language to another (Kumar et al., 2011c; Kumar et al., 2011a), which enables the extraction of crosslingual document correlations. In this case, a major issue lies in the definition of a cross-lingual similarity measure that can fit the extracted cross-lingual correlations. Also, from a semi-supervised perspective, other works attempt to define must-link constraints to detect cross-lingual clusters (Yogatama and Tan</context>
</contexts>
<marker>Yogatama, Tanaka-Ishii, 2009</marker>
<rawString>Dani Yogatama and Kumiko Tanaka-Ishii. 2009. Multilingual spectral clustering using document similarity propagation. In Proc. of the International Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 871–879.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ying Zhao</author>
<author>George Karypis</author>
</authors>
<title>Empirical and Theoretical Comparison of Selected Criterion Functions for Document Clustering.</title>
<date>2004</date>
<booktitle>Machine Learning,</booktitle>
<volume>55</volume>
<issue>3</issue>
<contexts>
<context position="14431" citStr="Zhao and Karypis, 2004" startWordPosition="2282" endWordPosition="2285"> BabelNet synsets. We shall devote Section 3.2 to a detailed description of our proposed BoS representation. The segment collection S is given in input to a document clustering algorithm to produce a clustering of the segments CS = {C3i }k1. The obtained clusters of segments can be disjoint or overlapping. Again, our SeMDocT is parametric to the clustering algorithm as well; here, we resort to a state-of-the-art clustering algorithm, namely Bisecting K-Means (Steinbach et al., 2000), which is widely known to produce high-quality (hard) clustering solutions in high-dimensional, large datasets (Zhao and Karypis, 2004). Note however that it requires as input the number of clusters. To cope with this issue, we adopt the method described in (Salvador and Chan, 2004), which explores how the within-cluster cohesion changes by varying the number of clusters. The number of clusters for which the slope of the plot changes drastically is chosen as a suitable value for the clustering algorithm. 3.1.3 Segment-cluster based representation Upon the segment clustering, each document is represented by its segments assigned to possibly multiple segment clusters. Therefore, we derive a documentfeature matrix for each of th</context>
</contexts>
<marker>Zhao, Karypis, 2004</marker>
<rawString>Ying Zhao and George Karypis. 2004. Empirical and Theoretical Comparison of Selected Criterion Functions for Document Clustering. Machine Learning, 55(3):311–331.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.006985">
<title confidence="0.977984">
Joint Emotion Analysis via Multi-task Gaussian Processes
</title>
<author confidence="0.977658">
Daniel Beck† Trevor Cohn$ Lucia Specia††Department of Computer Science, University of Sheffield, United Kingdom
</author>
<email confidence="0.843365">
{debeck1,l.specia}@sheffield.ac.uk
</email>
<note confidence="0.902085">
$Computing and Information Systems, University of Melbourne, Australia
</note>
<email confidence="0.993921">
t.cohn@unimelb.edu.au
</email>
<sectionHeader confidence="0.993738" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999865083333333">
We propose a model for jointly predicting
multiple emotions in natural language sen-
tences. Our model is based on a low-rank
coregionalisation approach, which com-
bines a vector-valued Gaussian Process
with a rich parameterisation scheme. We
show that our approach is able to learn
correlations and anti-correlations between
emotions on a news headlines dataset. The
proposed model outperforms both single-
task baselines and other multi-task ap-
proaches.
</bodyText>
<sectionHeader confidence="0.998758" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998193113636364">
Multi-task learning (Caruana, 1997) has been
widely used in Natural Language Processing.
Most of these learning methods are aimed for Do-
main Adaptation (Daum´e III, 2007; Finkel and
Manning, 2009), where we hypothesize that we
can learn from multiple domains by assuming sim-
ilarities between them. A more recent use of
multi-task learning is to model annotator bias and
noise for datasets labelled by multiple annotators
(Cohn and Specia, 2013).
The settings mentioned above have one aspect
in common: they assume some degree of posi-
tive correlation between tasks. In Domain Adap-
tation, we assume that some “general”, domain-
independent knowledge exists in the data. For an-
notator noise modelling, we assume that a “ground
truth” exists and that annotations are some noisy
deviations from this truth. However, for some set-
tings these assumptions do not necessarily hold
and often tasks can be anti-correlated. For these
cases, we need to employ multi-task methods that
are able to learn these relations from data and
correctly employ them when making predictions,
avoiding negative knowledge transfer.
An example of a problem that shows this be-
haviour is Emotion Analysis, where the goal is to
automatically detect emotions in a text (Strappa-
rava and Mihalcea, 2008; Mihalcea and Strappa-
rava, 2012). This problem is closely related to
Opinion Mining (Pang and Lee, 2008), with sim-
ilar applications, but it is usually done at a more
fine-grained level and involves the prediction of a
set of labels (one for each emotion) instead of a
single label. While we expect some emotions to
have some degree of correlation, this is usually not
the case for all possible emotions. For instance, we
expect sadness and joy to be anti-correlated.
We propose a multi-task setting for Emotion
Analysis based on a vector-valued Gaussian Pro-
cess (GP) approach known as coregionalisation
( ´Alvarez et al., 2012). The idea is to combine a GP
with a low-rank matrix which encodes task corre-
lations. Our motivation to employ this model is
three-fold:
</bodyText>
<listItem confidence="0.997402666666667">
• Datasets for this task are scarce and small
so we hypothesize that a multi-task approach
will results in better models by allowing a
task to borrow statistical strength from other
tasks;
• The annotation scheme is subjective and very
fine-grained, and is therefore heavily prone to
bias and noise, both which can be modelled
easily using GPs;
• Finally, we also have the goal to learn a
model that shows sound and interpretable
correlations between emotions.
</listItem>
<sectionHeader confidence="0.998043" genericHeader="method">
2 Multi-task Gaussian Process
Regression
</sectionHeader>
<bodyText confidence="0.999010833333333">
Gaussian Processes (GPs) (Rasmussen and
Williams, 2006) are a Bayesian kernelised
framework considered the state-of-the-art for
regression. They have been recently used success-
fully for translation quality prediction (Cohn and
Specia, 2013; Beck et al., 2013; Shah et al., 2013)
</bodyText>
<page confidence="0.896516">
1798
</page>
<bodyText confidence="0.964708272727273">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1798–1803,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
and modelling text periodicities (Preotiuc-Pietro
and Cohn, 2013). In the following we give a
brief description on how GPs are applied in a
regression setting.
Given an input x, the GP regression assumes
that its output y is a noise corrupted version of a
latent function evaluation, y = f(x) + η, where
η �N(0, σ2n) is the added white noise and the
function f is drawn from a GP prior:
</bodyText>
<equation confidence="0.999002">
f(x) — 9P(µ(x), k(x, x0)), (1)
</equation>
<bodyText confidence="0.9999035">
where µ(x) is the mean function, which is usually
the 0 constant, and k(x, x0) is the kernel or co-
variance function, which describes the covariance
between values of f at locations x and x0.
To predict the value for an unseen input x∗, we
compute the Bayesian posterior, which can be cal-
culated analytically, resulting in a Gaussian distri-
bution over the output y∗:1
</bodyText>
<equation confidence="0.9996225">
y∗ — N(k∗(K + σnI)−1yT , (2)
k(x∗, x∗) — kT∗ (K + σnI)−1k∗),
</equation>
<bodyText confidence="0.999868166666667">
where K is the Gram matrix corre-
sponding to the covariance kernel evalu-
ated at every pair of training inputs and
k∗ = [(x1, x∗), (x2, x∗), ... , (xn, x∗)] is the
vector of kernel evaluations between the test input
and each training input.
</bodyText>
<subsectionHeader confidence="0.985792">
2.1 The Intrinsic Coregionalisation Model
</subsectionHeader>
<bodyText confidence="0.999456285714286">
By extending the GP regression framework to
vector-valued outputs we obtain the so-called
coregionalisation models. Specifically, we employ
a separable vector-valued kernel known as Intrin-
sic Coregionalisation Model (ICM) ( ´Alvarez et al.,
2012). Considering a set of D tasks, we define the
corresponding vector-valued kernel as:
</bodyText>
<equation confidence="0.926433">
k((x, d), (x0, d0)) = kdata(x, x0) x Bd,d&apos;, (3)
</equation>
<bodyText confidence="0.999695">
where kdata is a kernel on the input points (here
a Radial Basis Function, RBF), d and d0 are task
or metadata information for each input and B E
RD×D is the coregionalisation matrix, which en-
codes task covariances and is symmetric and posi-
tive semi-definite.
A key advantage of GP-based modelling is its
ability to learn hyperparameters directly from data
</bodyText>
<footnote confidence="0.9636165">
1We refer the reader to Rasmussen and Williams (2006,
Chap. 2) for an in-depth explanation of GP regression.
</footnote>
<bodyText confidence="0.948273555555556">
by maximising the marginal likelihood:
p(y|X, 0) = if
p(y|X, 0, f)p(f). (4)
This process is usually performed to learn the
noise variance and kernel hyperparameters, in-
cluding the coregionalisation matrix. In order to
do this, we need to consider how B is parame-
terised.
Cohn and Specia (2013) treat the diagonal val-
ues of B as hyperparameters, and as a conse-
quence are able to leverage the inter-task trans-
fer between each independent task and the global
“pooled” task. They however fix non-diagonal val-
ues to 1, which in practice is equivalent to assum-
ing equal correlation across tasks. This can be lim-
iting, in that this formulation cannot model anti-
correlations between tasks.
In this work we lift this restriction by adopting
a different parameterisation of B that allows the
learning of all task correlations. A straightforward
way to do that would be to consider every corre-
lation as an hyperparameter, but this can result in
a matrix which is not positive semi-definite (and
therefore, not a valid covariance matrix). To en-
sure this property, we follow the method proposed
by Bonilla et al. (2008), which decomposes B us-
ing Probabilistic Principal Component Analysis:
</bodyText>
<equation confidence="0.996553">
B = UAUT + diag(α), (5)
</equation>
<bodyText confidence="0.999993692307692">
where U is an D x R matrix containing the R
principal eigenvectors and A is a R x R diago-
nal matrix containing the corresponding eigenval-
ues. The choice of R defines the rank of UAUT,
which can be understood as the capacity of the
manifold with which we model the D tasks. The
vector α allows for each task to behave more or
less independently with respect to the global task.
The final rank of B depends on both terms in
Equation 5.
For numerical stability, we use the incomplete-
Cholesky decomposition over the matrix UAUT,
resulting in the following parameterisation for B:
</bodyText>
<equation confidence="0.997654">
B = ˜L˜LT + diag(α), (6)
</equation>
<bodyText confidence="0.9999095">
where L˜ is a D x R matrix. In this setting, we
treat all elements of L˜ as hyperparameters. Set-
ting a larger rank allows more flexibility in mod-
elling task correlations. However, a higher number
of hyperparameters may lead to overfitting prob-
lems or otherwise cause issues in optimisation due
</bodyText>
<page confidence="0.985892">
1799
</page>
<bodyText confidence="0.9936346">
to additional non-convexities in the log likelihood
objective. In our experiments we evaluate this be-
haviour empirically by testing a range of ranks for
each setting.
The low-rank model can subsume the ones pro-
posed by Cohn and Specia (2013) by fixing and
tying some of the hyperparameters:
Independent: fixing L˜ = 0 and α = 1;
Pooled: fixing L˜ = 1 and α = 0;
Combined: fixing L˜ = 1 and tying all compo-
nents of α;
Combined+: fixing L˜= 1.
These formulations allow us to easily replicate
their modelling approach, which we evaluate as
competitive baselines in our experiments.
</bodyText>
<sectionHeader confidence="0.999484" genericHeader="method">
3 Experimental Setup
</sectionHeader>
<bodyText confidence="0.9988695">
To address the feasibility of our approach, we pro-
pose a set of experiments with three goals in mind:
</bodyText>
<listItem confidence="0.998032">
• To find our whether the ICM is able to learn
sensible emotion correlations;
• To check if these correlations are able to im-
prove predictions for unseen texts;
• To investigate the behaviour of the ICM
</listItem>
<bodyText confidence="0.9832487">
model as we increase the training set size.
Dataset We use the dataset provided by the “Af-
fective Text” shared task in SemEval-2007 (Strap-
parava and Mihalcea, 2007), which is composed
of 1000 news headlines annotated in terms of six
emotions: Anger, Disgust, Fear, Joy, Sadness and
Surprise. For each emotion, a score between 0 and
100 is given, 0 meaning total lack of emotion and
100 maximum emotional load. We use 100 sen-
tences for training and the remaining 900 for test-
ing.
Model For all experiments, we use a Radial Ba-
sis Function (RBF) data kernel over a bag-of-
words feature representation. Words were down-
cased and lemmatized using the WordNet lemma-
tizer in the NLTK2 toolkit (Bird et al., 2009). We
then use the GPy toolkit3 to combine this kernel
with a coregionalisation model over the six emo-
tions, comparing a number of low-rank approxi-
mations.
</bodyText>
<footnote confidence="0.998046">
2http://www.nltk.org
3http://github.com/SheffieldML/GPy
</footnote>
<bodyText confidence="0.988198923076923">
Baselines and Evaluation We compare predic-
tion results with a set of single-task baselines: a
Support Vector Machine (SVM) using an RBF
kernel with hyperparameters optimised via cross-
validation and a single-task GP, optimised via like-
lihood maximisation. The SVM models were
trained using the Scikit-learn toolkit4 (Pedregosa
et al., 2011). We also compare our results against
the ones obtained by employing the “Combined”
and “Combined+” models proposed by Cohn and
Specia (2013). Following previous work in this
area, we use Pearson’s correlation coefficient as
evaluation metric.
</bodyText>
<sectionHeader confidence="0.999785" genericHeader="evaluation">
4 Results and Discussion
</sectionHeader>
<subsectionHeader confidence="0.947179">
4.1 Learned Task Correlations
</subsectionHeader>
<bodyText confidence="0.993530833333333">
Figure 1 shows the learned coregionalisation ma-
trix setting the initial rank as 1, reordering the
emotions to emphasize the learned structure. We
can see that the matrix follows a block structure,
clustering some of the emotions. This picture
shows two interesting behaviours:
</bodyText>
<listItem confidence="0.810721928571429">
• Sadness and fear are highly correlated. Anger
and disgust also correlate with them, al-
though to a lesser extent, and could be con-
sidered as belonging to the same cluster. We
can also see correlation between surprise and
joy. These are intuitively sound clusters
based on the polarity of these emotions.
• In addition to correlations, the model
learns anti-correlations, especially between
joy/surprise and the other emotions. We also
note that joy has the highest diagonal value,
meaning that it gives preference to indepen-
dent modelling (instead of pooling over the
remaining tasks).
</listItem>
<bodyText confidence="0.999284111111111">
Inspecting the eigenvalues of the learned ma-
trix allows us to empirically determine its result-
ing rank. In this case we find that the model has
learned a matrix of rank 3, which indicates that
our initial assumption of a rank 1 coregionalisa-
tion matrix may be too small in terms of modelling
capacity5. This suggests that a higher rank is
justified, although care must be taken due to the
local optima and overfitting issues cited in §2.1.
</bodyText>
<footnote confidence="0.994439333333333">
4http://scikit-learn.org
5The eigenvalues were 592, 62, 86, 4, 3 × 10−3 and 9 ×
10−5.
</footnote>
<page confidence="0.937091">
1800
</page>
<table confidence="0.999576857142857">
Anger Disgust Fear Joy Sadness Surprise All
SVM 0.3084 0.2135 0.3525 0.0905 0.3330 0.1148 0.2603
Single GP 0.1683 0.0035 0.3462 0.2035 0.3011 0.1599 0.3659
ICM GP (Combined) 0.2301 0.1230 0.2913 0.2202 0.2303 0.1744 0.3295
ICM GP (Combined+) 0.1539 0.1240 0.3438 0.2466 0.2850 0.2027 0.3723
ICM GP (Rank 1) 0.2133 0.1075 0.3623 0.2810 0.3137 0.2415 0.3988
ICM GP (Rank 5) 0.2542 0.1799 0.3727 0.2711 0.3157 0.2446 0.3957
</table>
<tableCaption confidence="0.814286">
Table 1: Prediction results in terms of Pearson’s correlation coefficient (higher is better). Boldface values
show the best performing model for each emotion. The scores for the “All” column were calculated over
the predictions for all emotions concatenated (instead of just averaging over the scores for each emotion).
</tableCaption>
<figureCaption confidence="0.9974915">
Figure 1: Heatmap showing a learned coregional-
isation matrix over the emotions.
</figureCaption>
<subsectionHeader confidence="0.99796">
4.2 Prediction Results
</subsectionHeader>
<bodyText confidence="0.9999907">
Table 1 shows the Pearson’s scores obtained in
our experiments. The low-rank models outper-
formed the baselines for the full task (predicting
all emotions) and for fear, joy and surprise sub-
tasks. The rank 5 models were also able to out-
perform all GP baselines for the remaining emo-
tions, but could not beat the SVM baseline. As
expected, the “Combined” and “Combined+” per-
formed worse than the low-rank models, probably
due to their inability to model anti-correlations.
</bodyText>
<subsectionHeader confidence="0.99817">
4.3 Error analysis
</subsectionHeader>
<bodyText confidence="0.976379925925926">
To check why SVM performs better than GPs for
some emotions, we analysed their gold-standard
score distributions. Figure 2 shows the smoothed
distributions for disgust and fear, comparing the
gold-standard scores to predictions from the SVM
and GP models. The distributions for the training
set follow similar shapes.
We can see that GP obtains better matching
score distributions in the case when the gold-
Figure 2: Test score distributions for disgust and
fear. For clarity, only scores between 0 and 50 are
shown. SVM performs better on disgust, while GP
performs better on fear.
standard scores are more spread over the full sup-
port of response values, i.e., [0,100]. Since our GP
model employs a Gaussian likelihood, it is effec-
tively minimising a squared-error loss. The SVM
model, on the other hand, uses hinge loss, which
is linear beyond the margin envelope constraints.
This affects the treatment of outlier points, which
attract quadratic cf. linear penalties for the GP
and SVM respectively. Therefore, when train-
ing scores are more uniformly distributed (which
is the case for fear), the GP model has to take the
high scores into account, resulting in broader cov-
erage of the full support. For disgust, the scores
are much more peaked near zero, favouring the
</bodyText>
<page confidence="0.981457">
1801
</page>
<bodyText confidence="0.999501857142857">
more narrow coverage of the SVM.
More importantly, Figure 2 also shows that both
SVM and GP predictions tend to exhibit a Gaus-
sian shape, while the true scores show an expo-
nential behaviour. This suggests that both mod-
els are making wrong prior assumptions about the
underlying score distribution. For SVMs, this is
a non-trivial issue to address, although it is much
easier for GPs, where we can use a different like-
lihood distribution, e.g., a Beta distribution to re-
flect that the outputs are only valid over a bounded
range. Note that non-Gaussian likelihoods mean
that exact inference is no longer tractable, due to
the lack of conjugacy between the prior and likeli-
hood. However a number of approximate infer-
ence methods are appropriate which are already
widely used in the GP literature for use with non-
Gaussian likelihoods, including expectation prop-
agation (Jyl¨anki et al., 2011), the Laplace approx-
imation (Williams and Barber, 1998) and Markov
Chain Monte Carlo sampling (Adams et al., 2009).
</bodyText>
<subsectionHeader confidence="0.999617">
4.4 Training Set Influence
</subsectionHeader>
<bodyText confidence="0.999986190476191">
We expect multi-task models to perform better for
smaller datasets, when compared to single-task
models. This stems from the fact that with small
datasets often there is more uncertainty associated
with each task, a problem which can be alleviated
using statistics from the other tasks. To measure
this behaviour, we performed an additional exper-
iment varying the size of the training sets, while
using 100 sentences for testing.
Figure 3 shows the scores obtained. As ex-
pected, for smaller datasets the single-task mod-
els are outperformed by ICM, but their perfor-
mance become equivalent as the training set size
increases. SVM performance tends to be slightly
worse for most sizes. To study why we obtained
an outlier for the single-task model with 200 sen-
tences, we inspected the prediction values. We
found that, in this case, predictions for joy, sur-
prise and disgust were all around the same value.6
For larger datasets, this effect disappears and the
single-task models yield good predictions.
</bodyText>
<sectionHeader confidence="0.998628" genericHeader="conclusions">
5 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.964048">
This paper proposed an multi-task approach for
Emotion Analysis that is able to learn correlations
</bodyText>
<footnote confidence="0.975982">
6Looking at the predictions for smaller datasets, we found
the same behaviour, but because the values found were near
the mean they did not hurt the Pearson’s score as much.
</footnote>
<figureCaption confidence="0.9822345">
Figure 3: Pearson’s correlation score according to
training set size (in number of sentences).
</figureCaption>
<bodyText confidence="0.984523916666667">
and anti-correlations between emotions. Our for-
mulation is based on a combination of a Gaussian
Process and a low-rank coregionalisation model,
using a richer parameterisation that allows the
learning of fine-grained task similarities. The pro-
posed model outperformed strong baselines when
applied to a news headline dataset.
As it was discussed in Section 4.3, we plan
to further explore the possibility of using non-
Gaussian likelihoods with the GP models. An-
other research avenue we intend to explore is to
employ multiple layers of metadata, similar to the
model proposed by Cohn and Specia (2013). An
example is to incorporate the dataset provided by
Snow et al. (2008), which provides multiple non-
expert emotion annotations for each sentence, ob-
tained via crowdsourcing. Finally, another possi-
ble extension comes from more advanced vector-
valued GP models, such as the linear model of
coregionalisation ( ´Alvarez et al., 2012) or hierar-
chical kernels (Hensman et al., 2013). These mod-
els can be specially useful when we want to em-
ploy multiple kernels to explain the relation be-
tween the input data and the labels.
</bodyText>
<sectionHeader confidence="0.995894" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9205986">
Daniel Beck was supported by funding from
CNPq/Brazil (No. 237999/2012-9). Dr.
Cohn is the recipient of an Australian Re-
search Council Future Fellowship (project number
FT130101105).
</bodyText>
<sectionHeader confidence="0.993986" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.965202">
Ryan Prescott Adams, Iain Murray, and David J. C.
MacKay. 2009. Tractable Nonparametric Bayesian
</reference>
<page confidence="0.966299">
1802
</page>
<reference confidence="0.998359936708861">
Inference in Poisson Processes with Gaussian Pro-
cess Intensities. In Proceedings ofICML, pages 1–8,
New York, New York, USA. ACM Press.
Mauricio A. ´Alvarez, Lorenzo Rosasco, and Neil D.
Lawrence. 2012. Kernels for Vector-Valued Func-
tions: a Review. Foundations and Trends in Ma-
chine Learning, pages 1–37.
Daniel Beck, Kashif Shah, Trevor Cohn, and Lucia
Specia. 2013. SHEF-Lite : When Less is More for
Translation Quality Estimation. In Proceedings of
WMT13, pages 337–342.
Steven Bird, Ewan Klein, and Edward Loper.
2009. Natural Language Processing with Python.
O’Reilly Media.
Edwin V. Bonilla, Kian Ming A. Chai, and Christopher
K. I. Williams. 2008. Multi-task Gaussian Process
Prediction. Advances in Neural Information Pro-
cessing Systems.
Rich Caruana. 1997. Multitask Learning. Machine
Learning, 28:41–75.
Trevor Cohn and Lucia Specia. 2013. Modelling
Annotator Bias with Multi-task Gaussian Processes:
An Application to Machine Translation Quality Es-
timation. In Proceedings of ACL.
Hal Daum´e III. 2007. Frustratingly easy domain adap-
tation. In Proceedings of ACL.
Jenny Rose Finkel and Christopher D. Manning. 2009.
Hierarchical Bayesian Domain Adaptation. In Pro-
ceedings of NAACL.
James Hensman, Neil D Lawrence, and Magnus Rat-
tray. 2013. Hierarchical Bayesian modelling of
gene expression time series across irregularly sam-
pled replicates and clusters. BMC Bioinformatics,
14:252.
Pasi Jyl¨anki, Jarno Vanhatalo, and Aki Vehtari. 2011.
Robust Gaussian Process Regression with a Student-
t Likelihood. Journal of Machine Learning Re-
search, 12:3227–3257.
Rada Mihalcea and Carlo Strapparava. 2012. Lyrics,
Music, and Emotions. In Proceedings of the Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, pages 590–599.
Bo Pang and Lillian Lee. 2008. Opinion Mining and
Sentiment Analysis. Foundations and Trends in In-
formation Retrieval, 2(1–2):1–135.
Fabian Pedregosa, Ga¨el Varoquaux, Alexandre Gram-
fort, Vincent Michel, Bertrand Thirion, Olivier
Grisel, Mathieu Blondel, Peter Prettenhofer, Ron
Weiss, Vincent Duborg, Jake Vanderplas, Alexan-
dre Passos, David Cournapeau, Matthieu Brucher,
Matthieu Perrot, and ´Edouard Duchesnay. 2011.
Scikit-learn: Machine learning in Python. Journal
of Machine Learning Research, 12:2825–2830.
Daniel Preotiuc-Pietro and Trevor Cohn. 2013. A tem-
poral model of text periodicities using Gaussian Pro-
cesses. In Proceedings of EMNLP.
Carl Edward Rasmussen and Christopher K. I.
Williams. 2006. Gaussian processes for machine
learning, volume 1. MIT Press Cambridge.
Kashif Shah, Trevor Cohn, and Lucia Specia. 2013.
An Investigation on the Effectiveness of Features for
Translation Quality Estimation. In Proceedings of
MT Summit XIV.
Rion Snow, Brendan O’Connor, Daniel Jurafsky, and
Andrew Y. Ng. 2008. Cheap and Fast - But
is it Good?: Evaluating Non-Expert Annotations
for Natural Language Tasks. In Proceedings of
EMNLP.
Carlo Strapparava and Rada Mihalcea. 2007.
SemEval-2007 Task 14 : Affective Text. In Pro-
ceedings of SEMEVAL.
Carlo Strapparava and Rada Mihalcea. 2008. Learning
to identify emotions in text. In Proceedings of the
2008 ACM Symposium on Applied Computing.
Christopher K. I. Williams and David Barber. 1998.
Bayesian Classification with Gaussian Processes.
IEEE Transactions on Pattern Analysis and Machine
Intelligence, 20(12):1342–1351.
</reference>
<page confidence="0.977878">
1803
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.843306">
<title confidence="0.9998">Joint Emotion Analysis via Multi-task Gaussian Processes</title>
<author confidence="0.999659">Trevor Lucia of Computer Science</author>
<author confidence="0.999659">University of Sheffield</author>
<author confidence="0.999659">United</author>
<affiliation confidence="0.998215">and Information Systems, University of Melbourne,</affiliation>
<email confidence="0.998119">t.cohn@unimelb.edu.au</email>
<abstract confidence="0.988094538461538">We propose a model for jointly predicting multiple emotions in natural language sentences. Our model is based on a low-rank coregionalisation approach, which combines a vector-valued Gaussian Process with a rich parameterisation scheme. We show that our approach is able to learn correlations and anti-correlations between emotions on a news headlines dataset. The proposed model outperforms both singletask baselines and other multi-task approaches.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ryan Prescott Adams</author>
<author>Iain Murray</author>
<author>David J C MacKay</author>
</authors>
<title>Tractable Nonparametric Bayesian Inference in Poisson Processes with Gaussian Process Intensities.</title>
<date>2009</date>
<booktitle>In Proceedings ofICML,</booktitle>
<pages>1--8</pages>
<publisher>ACM Press.</publisher>
<location>New York, New York, USA.</location>
<contexts>
<context position="15322" citStr="Adams et al., 2009" startWordPosition="2507" endWordPosition="2510">where we can use a different likelihood distribution, e.g., a Beta distribution to reflect that the outputs are only valid over a bounded range. Note that non-Gaussian likelihoods mean that exact inference is no longer tractable, due to the lack of conjugacy between the prior and likelihood. However a number of approximate inference methods are appropriate which are already widely used in the GP literature for use with nonGaussian likelihoods, including expectation propagation (Jyl¨anki et al., 2011), the Laplace approximation (Williams and Barber, 1998) and Markov Chain Monte Carlo sampling (Adams et al., 2009). 4.4 Training Set Influence We expect multi-task models to perform better for smaller datasets, when compared to single-task models. This stems from the fact that with small datasets often there is more uncertainty associated with each task, a problem which can be alleviated using statistics from the other tasks. To measure this behaviour, we performed an additional experiment varying the size of the training sets, while using 100 sentences for testing. Figure 3 shows the scores obtained. As expected, for smaller datasets the single-task models are outperformed by ICM, but their performance b</context>
</contexts>
<marker>Adams, Murray, MacKay, 2009</marker>
<rawString>Ryan Prescott Adams, Iain Murray, and David J. C. MacKay. 2009. Tractable Nonparametric Bayesian Inference in Poisson Processes with Gaussian Process Intensities. In Proceedings ofICML, pages 1–8, New York, New York, USA. ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mauricio A ´Alvarez</author>
<author>Lorenzo Rosasco</author>
<author>Neil D Lawrence</author>
</authors>
<title>Kernels for Vector-Valued Functions: a Review. Foundations and Trends</title>
<date>2012</date>
<booktitle>in Machine Learning,</booktitle>
<pages>1--37</pages>
<marker>´Alvarez, Rosasco, Lawrence, 2012</marker>
<rawString>Mauricio A. ´Alvarez, Lorenzo Rosasco, and Neil D. Lawrence. 2012. Kernels for Vector-Valued Functions: a Review. Foundations and Trends in Machine Learning, pages 1–37.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Beck</author>
<author>Kashif Shah</author>
<author>Trevor Cohn</author>
<author>Lucia Specia</author>
</authors>
<title>SHEF-Lite : When Less is More for Translation Quality Estimation.</title>
<date>2013</date>
<booktitle>In Proceedings of WMT13,</booktitle>
<pages>337--342</pages>
<contexts>
<context position="3561" citStr="Beck et al., 2013" startWordPosition="546" endWordPosition="549">borrow statistical strength from other tasks; • The annotation scheme is subjective and very fine-grained, and is therefore heavily prone to bias and noise, both which can be modelled easily using GPs; • Finally, we also have the goal to learn a model that shows sound and interpretable correlations between emotions. 2 Multi-task Gaussian Process Regression Gaussian Processes (GPs) (Rasmussen and Williams, 2006) are a Bayesian kernelised framework considered the state-of-the-art for regression. They have been recently used successfully for translation quality prediction (Cohn and Specia, 2013; Beck et al., 2013; Shah et al., 2013) 1798 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1798–1803, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics and modelling text periodicities (Preotiuc-Pietro and Cohn, 2013). In the following we give a brief description on how GPs are applied in a regression setting. Given an input x, the GP regression assumes that its output y is a noise corrupted version of a latent function evaluation, y = f(x) + η, where η �N(0, σ2n) is the added white noise and the function f is drawn from a G</context>
</contexts>
<marker>Beck, Shah, Cohn, Specia, 2013</marker>
<rawString>Daniel Beck, Kashif Shah, Trevor Cohn, and Lucia Specia. 2013. SHEF-Lite : When Less is More for Translation Quality Estimation. In Proceedings of WMT13, pages 337–342.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven Bird</author>
<author>Ewan Klein</author>
<author>Edward Loper</author>
</authors>
<date>2009</date>
<booktitle>Natural Language Processing with Python. O’Reilly Media.</booktitle>
<contexts>
<context position="9464" citStr="Bird et al., 2009" startWordPosition="1566" endWordPosition="1569">e “Affective Text” shared task in SemEval-2007 (Strapparava and Mihalcea, 2007), which is composed of 1000 news headlines annotated in terms of six emotions: Anger, Disgust, Fear, Joy, Sadness and Surprise. For each emotion, a score between 0 and 100 is given, 0 meaning total lack of emotion and 100 maximum emotional load. We use 100 sentences for training and the remaining 900 for testing. Model For all experiments, we use a Radial Basis Function (RBF) data kernel over a bag-ofwords feature representation. Words were downcased and lemmatized using the WordNet lemmatizer in the NLTK2 toolkit (Bird et al., 2009). We then use the GPy toolkit3 to combine this kernel with a coregionalisation model over the six emotions, comparing a number of low-rank approximations. 2http://www.nltk.org 3http://github.com/SheffieldML/GPy Baselines and Evaluation We compare prediction results with a set of single-task baselines: a Support Vector Machine (SVM) using an RBF kernel with hyperparameters optimised via crossvalidation and a single-task GP, optimised via likelihood maximisation. The SVM models were trained using the Scikit-learn toolkit4 (Pedregosa et al., 2011). We also compare our results against the ones obt</context>
</contexts>
<marker>Bird, Klein, Loper, 2009</marker>
<rawString>Steven Bird, Ewan Klein, and Edward Loper. 2009. Natural Language Processing with Python. O’Reilly Media.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edwin V Bonilla</author>
<author>Kian Ming A Chai</author>
<author>Christopher K I Williams</author>
</authors>
<date>2008</date>
<booktitle>Multi-task Gaussian Process Prediction. Advances in Neural Information Processing Systems.</booktitle>
<contexts>
<context position="6861" citStr="Bonilla et al. (2008)" startWordPosition="1105" endWordPosition="1108">ix non-diagonal values to 1, which in practice is equivalent to assuming equal correlation across tasks. This can be limiting, in that this formulation cannot model anticorrelations between tasks. In this work we lift this restriction by adopting a different parameterisation of B that allows the learning of all task correlations. A straightforward way to do that would be to consider every correlation as an hyperparameter, but this can result in a matrix which is not positive semi-definite (and therefore, not a valid covariance matrix). To ensure this property, we follow the method proposed by Bonilla et al. (2008), which decomposes B using Probabilistic Principal Component Analysis: B = UAUT + diag(α), (5) where U is an D x R matrix containing the R principal eigenvectors and A is a R x R diagonal matrix containing the corresponding eigenvalues. The choice of R defines the rank of UAUT, which can be understood as the capacity of the manifold with which we model the D tasks. The vector α allows for each task to behave more or less independently with respect to the global task. The final rank of B depends on both terms in Equation 5. For numerical stability, we use the incompleteCholesky decomposition ov</context>
</contexts>
<marker>Bonilla, Chai, Williams, 2008</marker>
<rawString>Edwin V. Bonilla, Kian Ming A. Chai, and Christopher K. I. Williams. 2008. Multi-task Gaussian Process Prediction. Advances in Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rich Caruana</author>
</authors>
<date>1997</date>
<booktitle>Multitask Learning. Machine Learning,</booktitle>
<pages>28--41</pages>
<contexts>
<context position="807" citStr="Caruana, 1997" startWordPosition="103" endWordPosition="104">ield.ac.uk $Computing and Information Systems, University of Melbourne, Australia t.cohn@unimelb.edu.au Abstract We propose a model for jointly predicting multiple emotions in natural language sentences. Our model is based on a low-rank coregionalisation approach, which combines a vector-valued Gaussian Process with a rich parameterisation scheme. We show that our approach is able to learn correlations and anti-correlations between emotions on a news headlines dataset. The proposed model outperforms both singletask baselines and other multi-task approaches. 1 Introduction Multi-task learning (Caruana, 1997) has been widely used in Natural Language Processing. Most of these learning methods are aimed for Domain Adaptation (Daum´e III, 2007; Finkel and Manning, 2009), where we hypothesize that we can learn from multiple domains by assuming similarities between them. A more recent use of multi-task learning is to model annotator bias and noise for datasets labelled by multiple annotators (Cohn and Specia, 2013). The settings mentioned above have one aspect in common: they assume some degree of positive correlation between tasks. In Domain Adaptation, we assume that some “general”, domainindependent</context>
</contexts>
<marker>Caruana, 1997</marker>
<rawString>Rich Caruana. 1997. Multitask Learning. Machine Learning, 28:41–75.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Cohn</author>
<author>Lucia Specia</author>
</authors>
<title>Modelling Annotator Bias with Multi-task Gaussian Processes: An Application to Machine Translation Quality Estimation.</title>
<date>2013</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="1216" citStr="Cohn and Specia, 2013" startWordPosition="167" endWordPosition="170">ions and anti-correlations between emotions on a news headlines dataset. The proposed model outperforms both singletask baselines and other multi-task approaches. 1 Introduction Multi-task learning (Caruana, 1997) has been widely used in Natural Language Processing. Most of these learning methods are aimed for Domain Adaptation (Daum´e III, 2007; Finkel and Manning, 2009), where we hypothesize that we can learn from multiple domains by assuming similarities between them. A more recent use of multi-task learning is to model annotator bias and noise for datasets labelled by multiple annotators (Cohn and Specia, 2013). The settings mentioned above have one aspect in common: they assume some degree of positive correlation between tasks. In Domain Adaptation, we assume that some “general”, domainindependent knowledge exists in the data. For annotator noise modelling, we assume that a “ground truth” exists and that annotations are some noisy deviations from this truth. However, for some settings these assumptions do not necessarily hold and often tasks can be anti-correlated. For these cases, we need to employ multi-task methods that are able to learn these relations from data and correctly employ them when m</context>
<context position="3542" citStr="Cohn and Specia, 2013" startWordPosition="542" endWordPosition="545"> by allowing a task to borrow statistical strength from other tasks; • The annotation scheme is subjective and very fine-grained, and is therefore heavily prone to bias and noise, both which can be modelled easily using GPs; • Finally, we also have the goal to learn a model that shows sound and interpretable correlations between emotions. 2 Multi-task Gaussian Process Regression Gaussian Processes (GPs) (Rasmussen and Williams, 2006) are a Bayesian kernelised framework considered the state-of-the-art for regression. They have been recently used successfully for translation quality prediction (Cohn and Specia, 2013; Beck et al., 2013; Shah et al., 2013) 1798 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1798–1803, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics and modelling text periodicities (Preotiuc-Pietro and Cohn, 2013). In the following we give a brief description on how GPs are applied in a regression setting. Given an input x, the GP regression assumes that its output y is a noise corrupted version of a latent function evaluation, y = f(x) + η, where η �N(0, σ2n) is the added white noise and the function </context>
<context position="6048" citStr="Cohn and Specia (2013)" startWordPosition="967" endWordPosition="970"> and B E RD×D is the coregionalisation matrix, which encodes task covariances and is symmetric and positive semi-definite. A key advantage of GP-based modelling is its ability to learn hyperparameters directly from data 1We refer the reader to Rasmussen and Williams (2006, Chap. 2) for an in-depth explanation of GP regression. by maximising the marginal likelihood: p(y|X, 0) = if p(y|X, 0, f)p(f). (4) This process is usually performed to learn the noise variance and kernel hyperparameters, including the coregionalisation matrix. In order to do this, we need to consider how B is parameterised. Cohn and Specia (2013) treat the diagonal values of B as hyperparameters, and as a consequence are able to leverage the inter-task transfer between each independent task and the global “pooled” task. They however fix non-diagonal values to 1, which in practice is equivalent to assuming equal correlation across tasks. This can be limiting, in that this formulation cannot model anticorrelations between tasks. In this work we lift this restriction by adopting a different parameterisation of B that allows the learning of all task correlations. A straightforward way to do that would be to consider every correlation as a</context>
<context position="8097" citStr="Cohn and Specia (2013)" startWordPosition="1323" endWordPosition="1326">UAUT, resulting in the following parameterisation for B: B = ˜L˜LT + diag(α), (6) where L˜ is a D x R matrix. In this setting, we treat all elements of L˜ as hyperparameters. Setting a larger rank allows more flexibility in modelling task correlations. However, a higher number of hyperparameters may lead to overfitting problems or otherwise cause issues in optimisation due 1799 to additional non-convexities in the log likelihood objective. In our experiments we evaluate this behaviour empirically by testing a range of ranks for each setting. The low-rank model can subsume the ones proposed by Cohn and Specia (2013) by fixing and tying some of the hyperparameters: Independent: fixing L˜ = 0 and α = 1; Pooled: fixing L˜ = 1 and α = 0; Combined: fixing L˜ = 1 and tying all components of α; Combined+: fixing L˜= 1. These formulations allow us to easily replicate their modelling approach, which we evaluate as competitive baselines in our experiments. 3 Experimental Setup To address the feasibility of our approach, we propose a set of experiments with three goals in mind: • To find our whether the ICM is able to learn sensible emotion correlations; • To check if these correlations are able to improve predicti</context>
<context position="10155" citStr="Cohn and Specia (2013)" startWordPosition="1667" endWordPosition="1670">alisation model over the six emotions, comparing a number of low-rank approximations. 2http://www.nltk.org 3http://github.com/SheffieldML/GPy Baselines and Evaluation We compare prediction results with a set of single-task baselines: a Support Vector Machine (SVM) using an RBF kernel with hyperparameters optimised via crossvalidation and a single-task GP, optimised via likelihood maximisation. The SVM models were trained using the Scikit-learn toolkit4 (Pedregosa et al., 2011). We also compare our results against the ones obtained by employing the “Combined” and “Combined+” models proposed by Cohn and Specia (2013). Following previous work in this area, we use Pearson’s correlation coefficient as evaluation metric. 4 Results and Discussion 4.1 Learned Task Correlations Figure 1 shows the learned coregionalisation matrix setting the initial rank as 1, reordering the emotions to emphasize the learned structure. We can see that the matrix follows a block structure, clustering some of the emotions. This picture shows two interesting behaviours: • Sadness and fear are highly correlated. Anger and disgust also correlate with them, although to a lesser extent, and could be considered as belonging to the same c</context>
<context position="17349" citStr="Cohn and Specia (2013)" startWordPosition="2832" endWordPosition="2835">number of sentences). and anti-correlations between emotions. Our formulation is based on a combination of a Gaussian Process and a low-rank coregionalisation model, using a richer parameterisation that allows the learning of fine-grained task similarities. The proposed model outperformed strong baselines when applied to a news headline dataset. As it was discussed in Section 4.3, we plan to further explore the possibility of using nonGaussian likelihoods with the GP models. Another research avenue we intend to explore is to employ multiple layers of metadata, similar to the model proposed by Cohn and Specia (2013). An example is to incorporate the dataset provided by Snow et al. (2008), which provides multiple nonexpert emotion annotations for each sentence, obtained via crowdsourcing. Finally, another possible extension comes from more advanced vectorvalued GP models, such as the linear model of coregionalisation ( ´Alvarez et al., 2012) or hierarchical kernels (Hensman et al., 2013). These models can be specially useful when we want to employ multiple kernels to explain the relation between the input data and the labels. Acknowledgements Daniel Beck was supported by funding from CNPq/Brazil (No. 2379</context>
</contexts>
<marker>Cohn, Specia, 2013</marker>
<rawString>Trevor Cohn and Lucia Specia. 2013. Modelling Annotator Bias with Multi-task Gaussian Processes: An Application to Machine Translation Quality Estimation. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
</authors>
<title>Frustratingly easy domain adaptation.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL.</booktitle>
<marker>Daum´e, 2007</marker>
<rawString>Hal Daum´e III. 2007. Frustratingly easy domain adaptation. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Christopher D Manning</author>
</authors>
<title>Hierarchical Bayesian Domain Adaptation.</title>
<date>2009</date>
<booktitle>In Proceedings of NAACL.</booktitle>
<contexts>
<context position="968" citStr="Finkel and Manning, 2009" startWordPosition="127" endWordPosition="130">ting multiple emotions in natural language sentences. Our model is based on a low-rank coregionalisation approach, which combines a vector-valued Gaussian Process with a rich parameterisation scheme. We show that our approach is able to learn correlations and anti-correlations between emotions on a news headlines dataset. The proposed model outperforms both singletask baselines and other multi-task approaches. 1 Introduction Multi-task learning (Caruana, 1997) has been widely used in Natural Language Processing. Most of these learning methods are aimed for Domain Adaptation (Daum´e III, 2007; Finkel and Manning, 2009), where we hypothesize that we can learn from multiple domains by assuming similarities between them. A more recent use of multi-task learning is to model annotator bias and noise for datasets labelled by multiple annotators (Cohn and Specia, 2013). The settings mentioned above have one aspect in common: they assume some degree of positive correlation between tasks. In Domain Adaptation, we assume that some “general”, domainindependent knowledge exists in the data. For annotator noise modelling, we assume that a “ground truth” exists and that annotations are some noisy deviations from this tru</context>
</contexts>
<marker>Finkel, Manning, 2009</marker>
<rawString>Jenny Rose Finkel and Christopher D. Manning. 2009. Hierarchical Bayesian Domain Adaptation. In Proceedings of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Hensman</author>
<author>Neil D Lawrence</author>
<author>Magnus Rattray</author>
</authors>
<title>Hierarchical Bayesian modelling of gene expression time series across irregularly sampled replicates and clusters.</title>
<date>2013</date>
<journal>BMC Bioinformatics,</journal>
<pages>14--252</pages>
<marker>Hensman, Lawrence, Rattray, 2013</marker>
<rawString>James Hensman, Neil D Lawrence, and Magnus Rattray. 2013. Hierarchical Bayesian modelling of gene expression time series across irregularly sampled replicates and clusters. BMC Bioinformatics, 14:252.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pasi Jyl¨anki</author>
<author>Jarno Vanhatalo</author>
<author>Aki Vehtari</author>
</authors>
<title>Robust Gaussian Process Regression with a Studentt Likelihood.</title>
<date>2011</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>12--3227</pages>
<marker>Jyl¨anki, Vanhatalo, Vehtari, 2011</marker>
<rawString>Pasi Jyl¨anki, Jarno Vanhatalo, and Aki Vehtari. 2011. Robust Gaussian Process Regression with a Studentt Likelihood. Journal of Machine Learning Research, 12:3227–3257.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Carlo Strapparava</author>
</authors>
<title>Lyrics, Music, and Emotions.</title>
<date>2012</date>
<booktitle>In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>590--599</pages>
<contexts>
<context position="2069" citStr="Mihalcea and Strapparava, 2012" startWordPosition="303" endWordPosition="307">For annotator noise modelling, we assume that a “ground truth” exists and that annotations are some noisy deviations from this truth. However, for some settings these assumptions do not necessarily hold and often tasks can be anti-correlated. For these cases, we need to employ multi-task methods that are able to learn these relations from data and correctly employ them when making predictions, avoiding negative knowledge transfer. An example of a problem that shows this behaviour is Emotion Analysis, where the goal is to automatically detect emotions in a text (Strapparava and Mihalcea, 2008; Mihalcea and Strapparava, 2012). This problem is closely related to Opinion Mining (Pang and Lee, 2008), with similar applications, but it is usually done at a more fine-grained level and involves the prediction of a set of labels (one for each emotion) instead of a single label. While we expect some emotions to have some degree of correlation, this is usually not the case for all possible emotions. For instance, we expect sadness and joy to be anti-correlated. We propose a multi-task setting for Emotion Analysis based on a vector-valued Gaussian Process (GP) approach known as coregionalisation ( ´Alvarez et al., 2012). The</context>
</contexts>
<marker>Mihalcea, Strapparava, 2012</marker>
<rawString>Rada Mihalcea and Carlo Strapparava. 2012. Lyrics, Music, and Emotions. In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 590–599.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Opinion Mining and Sentiment Analysis. Foundations and Trends in Information Retrieval,</title>
<date>2008</date>
<pages>2--1</pages>
<contexts>
<context position="2141" citStr="Pang and Lee, 2008" startWordPosition="316" endWordPosition="319">ations are some noisy deviations from this truth. However, for some settings these assumptions do not necessarily hold and often tasks can be anti-correlated. For these cases, we need to employ multi-task methods that are able to learn these relations from data and correctly employ them when making predictions, avoiding negative knowledge transfer. An example of a problem that shows this behaviour is Emotion Analysis, where the goal is to automatically detect emotions in a text (Strapparava and Mihalcea, 2008; Mihalcea and Strapparava, 2012). This problem is closely related to Opinion Mining (Pang and Lee, 2008), with similar applications, but it is usually done at a more fine-grained level and involves the prediction of a set of labels (one for each emotion) instead of a single label. While we expect some emotions to have some degree of correlation, this is usually not the case for all possible emotions. For instance, we expect sadness and joy to be anti-correlated. We propose a multi-task setting for Emotion Analysis based on a vector-valued Gaussian Process (GP) approach known as coregionalisation ( ´Alvarez et al., 2012). The idea is to combine a GP with a low-rank matrix which encodes task corre</context>
</contexts>
<marker>Pang, Lee, 2008</marker>
<rawString>Bo Pang and Lillian Lee. 2008. Opinion Mining and Sentiment Analysis. Foundations and Trends in Information Retrieval, 2(1–2):1–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabian Pedregosa</author>
<author>Alexandre Gramfort Ga¨el Varoquaux</author>
<author>Vincent Michel</author>
<author>Bertrand Thirion</author>
<author>Olivier Grisel</author>
<author>Mathieu Blondel</author>
<author>Peter Prettenhofer</author>
</authors>
<title>Scikit-learn: Machine learning in Python.</title>
<date>2011</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>12--2825</pages>
<location>Ron Weiss, Vincent Duborg, Jake Vanderplas, Alexandre Passos, David</location>
<marker>Pedregosa, Ga¨el Varoquaux, Michel, Thirion, Grisel, Blondel, Prettenhofer, 2011</marker>
<rawString>Fabian Pedregosa, Ga¨el Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Duborg, Jake Vanderplas, Alexandre Passos, David Cournapeau, Matthieu Brucher, Matthieu Perrot, and ´Edouard Duchesnay. 2011. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825–2830.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Preotiuc-Pietro</author>
<author>Trevor Cohn</author>
</authors>
<title>A temporal model of text periodicities using Gaussian Processes.</title>
<date>2013</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="3848" citStr="Preotiuc-Pietro and Cohn, 2013" startWordPosition="584" endWordPosition="587">interpretable correlations between emotions. 2 Multi-task Gaussian Process Regression Gaussian Processes (GPs) (Rasmussen and Williams, 2006) are a Bayesian kernelised framework considered the state-of-the-art for regression. They have been recently used successfully for translation quality prediction (Cohn and Specia, 2013; Beck et al., 2013; Shah et al., 2013) 1798 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1798–1803, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics and modelling text periodicities (Preotiuc-Pietro and Cohn, 2013). In the following we give a brief description on how GPs are applied in a regression setting. Given an input x, the GP regression assumes that its output y is a noise corrupted version of a latent function evaluation, y = f(x) + η, where η �N(0, σ2n) is the added white noise and the function f is drawn from a GP prior: f(x) — 9P(µ(x), k(x, x0)), (1) where µ(x) is the mean function, which is usually the 0 constant, and k(x, x0) is the kernel or covariance function, which describes the covariance between values of f at locations x and x0. To predict the value for an unseen input x∗, we compute </context>
</contexts>
<marker>Preotiuc-Pietro, Cohn, 2013</marker>
<rawString>Daniel Preotiuc-Pietro and Trevor Cohn. 2013. A temporal model of text periodicities using Gaussian Processes. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl Edward Rasmussen</author>
<author>Christopher K I Williams</author>
</authors>
<title>Gaussian processes for machine learning, volume 1.</title>
<date>2006</date>
<publisher>MIT Press</publisher>
<location>Cambridge.</location>
<contexts>
<context position="3358" citStr="Rasmussen and Williams, 2006" startWordPosition="517" endWordPosition="520">s task correlations. Our motivation to employ this model is three-fold: • Datasets for this task are scarce and small so we hypothesize that a multi-task approach will results in better models by allowing a task to borrow statistical strength from other tasks; • The annotation scheme is subjective and very fine-grained, and is therefore heavily prone to bias and noise, both which can be modelled easily using GPs; • Finally, we also have the goal to learn a model that shows sound and interpretable correlations between emotions. 2 Multi-task Gaussian Process Regression Gaussian Processes (GPs) (Rasmussen and Williams, 2006) are a Bayesian kernelised framework considered the state-of-the-art for regression. They have been recently used successfully for translation quality prediction (Cohn and Specia, 2013; Beck et al., 2013; Shah et al., 2013) 1798 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1798–1803, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics and modelling text periodicities (Preotiuc-Pietro and Cohn, 2013). In the following we give a brief description on how GPs are applied in a regression setting. Given an input </context>
<context position="5698" citStr="Rasmussen and Williams (2006" startWordPosition="909" endWordPosition="912">alued kernel known as Intrinsic Coregionalisation Model (ICM) ( ´Alvarez et al., 2012). Considering a set of D tasks, we define the corresponding vector-valued kernel as: k((x, d), (x0, d0)) = kdata(x, x0) x Bd,d&apos;, (3) where kdata is a kernel on the input points (here a Radial Basis Function, RBF), d and d0 are task or metadata information for each input and B E RD×D is the coregionalisation matrix, which encodes task covariances and is symmetric and positive semi-definite. A key advantage of GP-based modelling is its ability to learn hyperparameters directly from data 1We refer the reader to Rasmussen and Williams (2006, Chap. 2) for an in-depth explanation of GP regression. by maximising the marginal likelihood: p(y|X, 0) = if p(y|X, 0, f)p(f). (4) This process is usually performed to learn the noise variance and kernel hyperparameters, including the coregionalisation matrix. In order to do this, we need to consider how B is parameterised. Cohn and Specia (2013) treat the diagonal values of B as hyperparameters, and as a consequence are able to leverage the inter-task transfer between each independent task and the global “pooled” task. They however fix non-diagonal values to 1, which in practice is equivale</context>
</contexts>
<marker>Rasmussen, Williams, 2006</marker>
<rawString>Carl Edward Rasmussen and Christopher K. I. Williams. 2006. Gaussian processes for machine learning, volume 1. MIT Press Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kashif Shah</author>
<author>Trevor Cohn</author>
<author>Lucia Specia</author>
</authors>
<title>An Investigation on the Effectiveness of Features for Translation Quality Estimation.</title>
<date>2013</date>
<booktitle>In Proceedings of MT Summit XIV.</booktitle>
<contexts>
<context position="3581" citStr="Shah et al., 2013" startWordPosition="550" endWordPosition="553">strength from other tasks; • The annotation scheme is subjective and very fine-grained, and is therefore heavily prone to bias and noise, both which can be modelled easily using GPs; • Finally, we also have the goal to learn a model that shows sound and interpretable correlations between emotions. 2 Multi-task Gaussian Process Regression Gaussian Processes (GPs) (Rasmussen and Williams, 2006) are a Bayesian kernelised framework considered the state-of-the-art for regression. They have been recently used successfully for translation quality prediction (Cohn and Specia, 2013; Beck et al., 2013; Shah et al., 2013) 1798 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1798–1803, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics and modelling text periodicities (Preotiuc-Pietro and Cohn, 2013). In the following we give a brief description on how GPs are applied in a regression setting. Given an input x, the GP regression assumes that its output y is a noise corrupted version of a latent function evaluation, y = f(x) + η, where η �N(0, σ2n) is the added white noise and the function f is drawn from a GP prior: f(x) — 9P(µ</context>
</contexts>
<marker>Shah, Cohn, Specia, 2013</marker>
<rawString>Kashif Shah, Trevor Cohn, and Lucia Specia. 2013. An Investigation on the Effectiveness of Features for Translation Quality Estimation. In Proceedings of MT Summit XIV.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rion Snow</author>
<author>Brendan O’Connor</author>
<author>Daniel Jurafsky</author>
<author>Andrew Y Ng</author>
</authors>
<title>Cheap and Fast - But is it Good?: Evaluating Non-Expert Annotations for Natural Language Tasks.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<marker>Snow, O’Connor, Jurafsky, Ng, 2008</marker>
<rawString>Rion Snow, Brendan O’Connor, Daniel Jurafsky, and Andrew Y. Ng. 2008. Cheap and Fast - But is it Good?: Evaluating Non-Expert Annotations for Natural Language Tasks. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carlo Strapparava</author>
<author>Rada Mihalcea</author>
</authors>
<date>2007</date>
<booktitle>SemEval-2007 Task 14 : Affective Text. In Proceedings of SEMEVAL.</booktitle>
<contexts>
<context position="8925" citStr="Strapparava and Mihalcea, 2007" startWordPosition="1470" endWordPosition="1474"> 1. These formulations allow us to easily replicate their modelling approach, which we evaluate as competitive baselines in our experiments. 3 Experimental Setup To address the feasibility of our approach, we propose a set of experiments with three goals in mind: • To find our whether the ICM is able to learn sensible emotion correlations; • To check if these correlations are able to improve predictions for unseen texts; • To investigate the behaviour of the ICM model as we increase the training set size. Dataset We use the dataset provided by the “Affective Text” shared task in SemEval-2007 (Strapparava and Mihalcea, 2007), which is composed of 1000 news headlines annotated in terms of six emotions: Anger, Disgust, Fear, Joy, Sadness and Surprise. For each emotion, a score between 0 and 100 is given, 0 meaning total lack of emotion and 100 maximum emotional load. We use 100 sentences for training and the remaining 900 for testing. Model For all experiments, we use a Radial Basis Function (RBF) data kernel over a bag-ofwords feature representation. Words were downcased and lemmatized using the WordNet lemmatizer in the NLTK2 toolkit (Bird et al., 2009). We then use the GPy toolkit3 to combine this kernel with a </context>
</contexts>
<marker>Strapparava, Mihalcea, 2007</marker>
<rawString>Carlo Strapparava and Rada Mihalcea. 2007. SemEval-2007 Task 14 : Affective Text. In Proceedings of SEMEVAL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carlo Strapparava</author>
<author>Rada Mihalcea</author>
</authors>
<title>Learning to identify emotions in text.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 ACM Symposium on Applied Computing.</booktitle>
<contexts>
<context position="2036" citStr="Strapparava and Mihalcea, 2008" startWordPosition="298" endWordPosition="302">t knowledge exists in the data. For annotator noise modelling, we assume that a “ground truth” exists and that annotations are some noisy deviations from this truth. However, for some settings these assumptions do not necessarily hold and often tasks can be anti-correlated. For these cases, we need to employ multi-task methods that are able to learn these relations from data and correctly employ them when making predictions, avoiding negative knowledge transfer. An example of a problem that shows this behaviour is Emotion Analysis, where the goal is to automatically detect emotions in a text (Strapparava and Mihalcea, 2008; Mihalcea and Strapparava, 2012). This problem is closely related to Opinion Mining (Pang and Lee, 2008), with similar applications, but it is usually done at a more fine-grained level and involves the prediction of a set of labels (one for each emotion) instead of a single label. While we expect some emotions to have some degree of correlation, this is usually not the case for all possible emotions. For instance, we expect sadness and joy to be anti-correlated. We propose a multi-task setting for Emotion Analysis based on a vector-valued Gaussian Process (GP) approach known as coregionalisat</context>
</contexts>
<marker>Strapparava, Mihalcea, 2008</marker>
<rawString>Carlo Strapparava and Rada Mihalcea. 2008. Learning to identify emotions in text. In Proceedings of the 2008 ACM Symposium on Applied Computing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher K I Williams</author>
<author>David Barber</author>
</authors>
<title>Bayesian Classification with Gaussian Processes.</title>
<date>1998</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>20</volume>
<issue>12</issue>
<contexts>
<context position="15263" citStr="Williams and Barber, 1998" startWordPosition="2497" endWordPosition="2500">non-trivial issue to address, although it is much easier for GPs, where we can use a different likelihood distribution, e.g., a Beta distribution to reflect that the outputs are only valid over a bounded range. Note that non-Gaussian likelihoods mean that exact inference is no longer tractable, due to the lack of conjugacy between the prior and likelihood. However a number of approximate inference methods are appropriate which are already widely used in the GP literature for use with nonGaussian likelihoods, including expectation propagation (Jyl¨anki et al., 2011), the Laplace approximation (Williams and Barber, 1998) and Markov Chain Monte Carlo sampling (Adams et al., 2009). 4.4 Training Set Influence We expect multi-task models to perform better for smaller datasets, when compared to single-task models. This stems from the fact that with small datasets often there is more uncertainty associated with each task, a problem which can be alleviated using statistics from the other tasks. To measure this behaviour, we performed an additional experiment varying the size of the training sets, while using 100 sentences for testing. Figure 3 shows the scores obtained. As expected, for smaller datasets the single-t</context>
</contexts>
<marker>Williams, Barber, 1998</marker>
<rawString>Christopher K. I. Williams and David Barber. 1998. Bayesian Classification with Gaussian Processes. IEEE Transactions on Pattern Analysis and Machine Intelligence, 20(12):1342–1351.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
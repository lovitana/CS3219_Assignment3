<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9994935">
A Regularized Competition Model for Question Difficulty Estimation in
Community Question Answering Services
</title>
<author confidence="0.994927">
Quan Wangs Jing Liu* Bin Wangs Li Guos
</author>
<affiliation confidence="0.994587">
sInstitute of Information Engineering, Chinese Academy of Sciences, Beijing, P. R. China
</affiliation>
<email confidence="0.947165">
{wangquan,wangbin,guoli}@iie.ac.cn
</email>
<affiliation confidence="0.804205">
*Harbin Institute of Technology, Harbin, P. R. China
</affiliation>
<email confidence="0.991725">
jliu@ir.hit.edu.cn
</email>
<sectionHeader confidence="0.997316" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999866875">
Estimating questions’ difficulty levels is
an important task in community question
answering (CQA) services. Previous stud-
ies propose to solve this problem based
on the question-user comparisons extract-
ed from the question answering threads.
However, they suffer from data sparseness
problem as each question only gets a lim-
ited number of comparisons. Moreover,
they cannot handle newly posted question-
s which get no comparisons. In this pa-
per, we propose a novel question difficul-
ty estimation approach called Regularized
Competition Model (RCM), which natu-
rally combines question-user comparisons
and questions’ textual descriptions into a
unified framework. By incorporating tex-
tual information, RCM can effectively deal
with data sparseness problem. We further
employ a K-Nearest Neighbor approach to
estimate difficulty levels of newly post-
ed questions, again by leveraging textu-
al similarities. Experiments on two pub-
licly available data sets show that for both
well-resolved and newly-posted question-
s, RCM performs the estimation task sig-
nificantly better than existing methods,
demonstrating the advantage of incorpo-
rating textual information. More interest-
ingly, we observe that RCM might provide
an automatic way to quantitatively mea-
sure the knowledge levels of words.
</bodyText>
<sectionHeader confidence="0.999626" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.97523875">
Recent years have seen rapid growth in communi-
ty question answering (CQA) services. They have
been widely used in various scenarios, including
general information seeking on the web1, knowl-
</bodyText>
<footnote confidence="0.924966">
1http://answers.yahoo.com/
</footnote>
<bodyText confidence="0.999897948717949">
edge exchange in professional communities2, and
question answering in massive open online cours-
es (MOOCs)3, to name a few.
An important research problem in CQA is
how to automatically estimate the difficulty lev-
els of questions, i.e., question di�culty estima-
tion (QDE). QDE can benefit many applications.
Examples include 1) Question routing. Routing
questions to appropriate answerers can help ob-
tain quick and high-quality answers (Li and K-
ing, 2010; Zhou et al., 2009). Ackerman and
McDonald (1996) have demonstrated that rout-
ing questions by matching question difficulty lev-
el with answerer expertise level will make better
use of answerers’ time and expertise. This is even
more important for enterprise question answering
and MOOCs question answering, where human
resources are expensive. 2) Incentive mechanism
design. Nam et al. (2009) have found that win-
ning point awards offered by reputation system-
s is a driving factor for user participation in C-
QA services. Assigning higher point awards to
more difficult questions will significantly improve
user participation and satisfaction. 3) Linguistics
analysis. Researchers in computational linguistics
are always interested in investigating the correla-
tion between language and knowledge, to see how
the language reflects one’s knowledge (Church,
2011). As we will show in Section 5.4, QDE pro-
vides an automatic way to quantitatively measure
the knowledge levels of words.
Liu et al. (2013) have done the pioneer work
on QDE, by leveraging question-user comparison-
s extracted from the question answering threads.
Specifically, they assumed that the difficulty lev-
el of a question is higher than the expertise level
of the asker (i.e. the user who asked the question),
but lower than that of the best answerer (i.e. the us-
er who provided the best answer). A TrueSkill al-
</bodyText>
<footnote confidence="0.999874">
2http://stackoverflow.com/
3http://coursera.org/
</footnote>
<page confidence="0.895038">
1115
</page>
<note confidence="0.9090815">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1115–1126,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999810866666667">
gorithm (Herbrich et al., 2006) was further adopt-
ed to estimate question difficulty levels as well as
user expertise levels from the pairwise compar-
isons among them. To our knowledge, it is the on-
ly existing work on QDE. Yang et al. (2008) have
proposed a similar idea, but their work focuses on
a different task, i.e., estimating difficulty levels of
tasks in crowdsourcing contest services.
There are two major drawbacks of previous
methods: 1) data sparseness problem and 2) cold-
start problem. By the former, we mean that un-
der the framework of previous work, each question
is compared only twice with the users (once with
the asker and the other with the best answerer),
which might not provide enough information and
contaminate the estimation accuracy. By the latter,
we mean that previous work only deals with well-
resolved questions which have received the best
answers, but cannot handle newly posted question-
s with no answers received. In many real-world
applications such as question routing and incentive
mechanism design, however, it is usually required
that the difficulty level of a question is known in-
stantly after it is posted.
To address the drawbacks, we propose further
exploiting questions’ textual descriptions (e.g., ti-
tle, body, and tags) to perform QDE. Preliminary
observations have shown that a question’s difficul-
ty level can be indicated by its textual descrip-
tion (Liu et al., 2013). We take advantage of the
observations, and assume that if two questions are
close in their textual descriptions, they will also
be close in their difficulty levels, i.e., the smooth-
ness assumption. We employ manifold regular-
ization (Belkin et al., 2006) to characterize the
assumption. Manifold regularization is a well-
known technique to preserve local invariance in
manifold learning algorithms, i.e., nearby points
are likely to have similar embeddings (Belkin and
Niyogi, 2001). Then, we propose a novel Reg-
ularized Competition Model (RCM), which for-
malizes QDE as minimizing a loss on question-
user comparisons with manifold regularization on
questions’ textual descriptions. As the smoothness
assumption offers extra information for inferring
question difficulty levels, incorporating it will ef-
fectively deal with data sparsity. Finally, we adopt
a K-Nearest Neighbor approach (Cover and Hart,
1967) to perform cold-start estimation, again by
leveraging the smoothness assumption.
Experiments on two publicly available data sets
collected from Stack Overflow show that 1) RCM
performs significantly better than existing meth-
ods in the QDE task for both well-resolved and
cold-start questions. 2) The performance of RCM
is insensitive to the particular choice of the term
weighting schema (determines how a question’s
textual description is represented) and the similar-
ity measure (determines how the textual similarity
between two questions is measured). The results
demonstrate the advantage of incorporating textu-
al information for QDE. Qualitative analysis fur-
ther reveals that RCM might provide an automatic
way to quantitatively measure the knowledge lev-
els of words.
The main contributions of this paper include: 1)
We take fully advantage of questions’ textual de-
scriptions to address data sparseness problem and
cold-start problem which previous QDE methods
suffer from. To our knowledge, it is the first time
that textual information is introduced in QDE. 2)
We propose a novel QDE method that natural-
ly combines question-user comparisons and ques-
tions’ textual descriptions into a unified frame-
work. The proposed method performs QDE sig-
nificantly better than existing methods. 3) We
demonstrate the practicability of estimating diffi-
culty levels of cold-start questions purely based on
their textual descriptions, making various applica-
tions feasible in practice. As far as we know, it is
the first work that considers cold-start estimation.
4) We explore how a word’s knowledge level can
be automatically measured by RCM.
The rest of the paper is structured as follows.
Section 2 describes the problem formulation and
the motivation of RCM. Section 3 presents the de-
tails of RCM. Section 4 discusses cold-start esti-
mation. Section 5 reports experiments and results.
Section 6 reviews related work. Section 7 con-
cludes the paper and discusses future work.
</bodyText>
<sectionHeader confidence="0.998648" genericHeader="introduction">
2 Preliminaries
</sectionHeader>
<subsectionHeader confidence="0.999643">
2.1 Problem Formulation
</subsectionHeader>
<bodyText confidence="0.999957555555556">
A CQA service provides a platform where people
can ask questions and seek answers from others.
Given a CQA portal, consider a specific catego-
ry where questions on the same topic are asked
and answered, e.g., the “C++ programming” cat-
egory of Stack Overflow. When an asker ua posts
a question q in the category, there will be sever-
al answerers to answer the question. Among all
the received answers, a best one will be chosen
</bodyText>
<page confidence="0.988287">
1116
</page>
<bodyText confidence="0.999810225806452">
by the asker or voted by the community. The an-
swerer who provides the best answer is called the
best answerer ub. The other answerers are denoted
by O = {uo1, uo2, · · · , uoM}. A question answering
thread (QA thread) is represented as a quadruplet
(q, ua, ub, O). Collecting all such QA threads in the
category, we get M users and N questions, denoted
by U = {u1, u2, ··· , uM} and Q = {q1, q2, ··· , qN}
respectively. Each user um is associated with an
expertise score θm, representing his/her expertise
level. A larger θm indicates a higher expertise lev-
el of the user. Each question qn is associated with
a difficulty score βn, representing its difficulty lev-
el. A larger βn indicates a higher difficulty level
of the question. Difficulty scores (as well as ex-
pertise scores) are assumed to be comparable with
each other in the specified category. Besides, each
question qn has a textual description, and is repre-
sented as a V-dimensional term vector dn, where
V is the vocabulary size.
The question di�culty estimation (QDE) task
aims to automatically learn the question difficul-
ty scores (βn’s) by utilizing the QA threads T =
{(q, ua, ub, O) : q ∈ Q} as well as the question de-
scriptions D = {d1, d2, · · · , dN} in the specified
category. Note that in Section 2 and Section 3, we
consider estimating difficulty scores of resolved
questions, i.e., questions with the best answers se-
lected or voted. Estimating difficulty scores of un-
resolved questions, e.g., newly posted ones, will
be discussed in Section 4.
</bodyText>
<subsectionHeader confidence="0.697211">
2.2 Competition-based Methods
</subsectionHeader>
<bodyText confidence="0.993007933333333">
Liu et al. (2013) have proposed a competition-
based method for QDE. The key idea is to 1) ex-
tract pairwise competitions from the QA threads
and 2) estimate question difficulty scores based on
extracted competitions.
To extract pairwise competitions, it is assumed
that question difficulty scores and user expertise
scores are expressed on the same scale. Given a
QA thread (q, ua, ub, O), it is further assumed that:
Assumption 1(pairwise comparison assumption)
The di�culty score of question q is higher than the
expertise score of the asker ua, but lower than that
of the best answerer ub. Moreover, the expertise
score of the best answerer ub is higher than that
of the asker ua, as well as any answerer in O.4
</bodyText>
<footnote confidence="0.92032975">
4The difficulty score of question q is not assumed to be
lower than the expertise score of any answerer in O, since
such a user may just happen to see the question and respond
to it, rather than knowing the answer well.
</footnote>
<bodyText confidence="0.995604916666667">
Given the assumption, there are (|O |+ 3) pairwise
competitions extracted from the QA thread, in-
cluding 1) one competition between the question
q and the asker ua, 2) one competition between
the question q and the best answerer ub, 3) one
competition between the best answerer ub and the
asker ua, and 4) |O |competitions between the best
answerer ub and each of the answerers in O. The
question q is the winner of the first competition,
and the best answerer ub is the winner of the re-
maining (|O |+ 2) competitions. These pairwise
competitions are denoted by
</bodyText>
<equation confidence="0.9728236">
Cq = {ua ≺q, q≺ub, ua ≺ub, uo1 ≺ub, · · · , uoM ≺ub } ,
where i ≺ j means that competitor j beats com-
petitor i in a competition. Let
UC = Cq (1)
q∈Q
</equation>
<bodyText confidence="0.9998862">
be the set containing all the pairwise competitions
extracted from T.
Given the competition set C, Liu et al. (2013)
further adopted a TrueSkill algorithm (Herbrich
et al., 2006) to learn the competitors’ skill level-
s (i.e. the question difficulty scores and the us-
er expertise scores). TrueSkill assumes that the
practical skill level of each competitor follows a
(normal distribution N µ, σ2), where µ is the aver-
age skill level and σ is the estimation uncertain-
ty. Then it updates the estimations in an online
mode: for a newly observed competition with its
win-loss result, 1) increase the average skill level
of the winner, 2) decrease the average skill level
of the loser, and 3) shrink the uncertainties of both
competitors as more data has been observed. Yang
et al. (2008) have proposed a similar competition-
based method to estimate tasks’ difficulty levels
in crowdsourcing contest services, by leveraging
PageRank (Page et al., 1999) algorithm.
</bodyText>
<subsectionHeader confidence="0.999734">
2.3 Motivating Discussions
</subsectionHeader>
<bodyText confidence="0.999987545454545">
The methods introduced above estimate competi-
tors’ skill levels based solely on the pairwise com-
petitions among them. The more competitions a
competitor participates in, the more accurate the
estimation will be. However, according to the
pairwise comparison assumption (Assumption 1),
each question participates in only two competi-
tions, one with the asker and the other with the
best answerer. Hence, there might be no enough
information to accurately infer its difficulty score.
We call this the data sparseness problem.
</bodyText>
<page confidence="0.992568">
1117
</page>
<figure confidence="0.987519">
(a) Low difficulty. (b) Medium difficulty. (c) High difficulty.
</figure>
<figureCaption confidence="0.999968">
Figure 1: Tag clouds of SO/Math questions with different difficulty levels.
</figureCaption>
<bodyText confidence="0.999490875">
Taking advantage of additional metadata has
been demonstrated to be an effective way of deal-
ing with data sparsity in various applications such
as collaborative filtering (Claypool et al., 1999;
Schein et al., 2002) and personalized search (Dou
et al., 2007; Sugiyama et al., 2004). The ratio-
nale behind is to bridge the gap among users/items
by leveraging their similarities based on the meta-
data. As for QDE, preliminary observations have
shown that a question’s difficulty level can be in-
dicated by its textual description (Liu et al., 2013).
As an example, consider the QA threads in the
“mathematics” category of Stack Overflow. Di-
vide the questions into three groups: 1) low dif-
ficulty, 2) medium difficulty, and 3) high difficul-
ty, according to their difficulty scores estimated by
TrueSkill. Figure 1 visualizes the frequency dis-
tribution of tags in each group, where the size of
each tag is in proportion to its frequency in the
group. The results indicate that the tags associ-
ated with the questions do have the ability to re-
flect the questions’ difficulty levels, e.g., low dif-
ficulty questions usually have tags such as “home-
work” and “calculus”, while high difficulty ones
usually have tags such as “general topology” and
“number theory”. We further calculate the Pearson
correlation coefficient (Rodgers and Nicewander,
1988) between 1) the gap between the averaged
difficulty scores in each two groups and 2) the
Euclidean distance between the aggregated textu-
al descriptions in each two groups . The result is
r = 0.6424, implying that the difficulty gap is posi-
tively correlated with the textual distance. In other
words, the more similar two questions’ textual de-
scriptions are, the more close their difficulty levels
are. Therefore, we take the textual information to
bridge the difficulty gap among questions, by as-
suming that
Assumption 2 (smoothness assumption) If two
questions qi and qj are close in their textual de-
scriptions di and dj, they will also be close in their
di�culty scores βi and βj.
The smoothness assumption brings us additional
information about question difficulty scores by in-
ferring textual similarities. It serves as a supple-
ment to the pairwise competitions, and might help
address the data sparseness problem which previ-
ous methods suffer from.
</bodyText>
<sectionHeader confidence="0.890258" genericHeader="method">
3 Modeling Text Similarities for QDE
</sectionHeader>
<bodyText confidence="0.999962833333333">
This section presents a novel Regularized Compe-
tition Model (RCM) for QDE, which combines the
pairwise competitions and the textual descriptions
into a unified framework. RCM can alleviate the
data sparseness problem and perform more accu-
rate estimation.
</bodyText>
<subsectionHeader confidence="0.981509">
3.1 Regularized Competition Model
</subsectionHeader>
<bodyText confidence="0.978055">
We start with several notations. As question dif-
ficulty scores can be directly compared with user
expertise scores, we take questions as pseudo user-
s. Let θ¯ E RM+N denote the skill levels (i.e. the
expertise scores and the difficulty scores) of all the
(pseudo) users:
</bodyText>
<equation confidence="0.9673515">
� θi, 1&lt;i&lt;M,
¯θi =βi−M, M&lt;i&lt;M+N,
</equation>
<bodyText confidence="0.914578888888889">
where ¯θi is the i-th entry of ¯θ. The first M entries
are the user expertise scores, denoted by ¯θu E RM.
The last N entries are the question difficulty s-
cores, denoted by ¯θq E RN. Let ¯θ(u)
i and ¯θ(q)
i denote
¯θu and ¯θq respectively.
Exploiting Pairwise Competitions. We define
a loss on each pairwise competition i &lt; j:
</bodyText>
<equation confidence="0.95925">
ℓ (¯θi, θj) = max (0, δ − (¯θj − ¯θi))p , (2)
</equation>
<bodyText confidence="0.981485333333333">
where p is either 1 or 2. The loss is defined on the
skill gap between the two competitors, i.e., ¯θj − ¯θi,
the i-th entries of
</bodyText>
<page confidence="0.743619">
1118
</page>
<bodyText confidence="0.999920476190476">
measuring the inconsistency between the expect-
ed outcome and the actual outcome. If the gap is
larger than a predefined threshold δ, competitor j
would probably beat competitor i in the compe-
tition, which coincides with the actual outcome.
Then the loss will be zero. Otherwise, there is a
higher chance that competitor j loses the competi-
tion, which goes against the actual outcome. Then
the loss will be greater than zero. The smaller the
gap is, the higher the chance of inconsistency be-
comes, and the greater the loss will be. Note that
the threshold δ can take any positive value since
we do not pose a norm constraint on ¯θ.5 Without
loss of generality we take δ = 1 throughout this
paper. As we will show in Section 3.2, the loss de-
fined in Eq. (2) has some similarity with the SVM
loss (Chapelle, 2007). We name it hinge loss when
p = 1, and quadratic loss when p = 2.
Given the competition set C, estimating skil-
l levels of (pseudo) users then amounts to solving
the following optimization problem:
</bodyText>
<equation confidence="0.93041">
λ1¯T- 3
ℓ (¯θi, ¯θj) + 2 θ e, ( )
</equation>
<bodyText confidence="0.9998728">
where the first term is the empirical loss measur-
ing the total inconsistency; the second term is a
regularizer to prevent overfitting; and λ1 ≥ 0 is a
trade-off coefficient. It is also a competition-based
QDE method, called Competition Model (CM).
</bodyText>
<subsectionHeader confidence="0.516337">
Exploiting Question Descriptions. Manifold
</subsectionHeader>
<bodyText confidence="0.93284496">
regularization is a well-known technique used in
manifold learning algorithms to preserve local in-
variance, i.e., nearby points are likely to have sim-
ilar embeddings (Belkin and Niyogi, 2001). In
QDE, the smoothness assumption expresses sim-
ilar “invariance”, i.e., nearby questions (in terms
of textual similarities) are likely to have similar
difficulty scores. Hence, we characterize the as-
sumption with the following manifold regularizer:
where wij is the textual similarity between ques-
tion i and question j; W ∈ RN×N is the similarity
matrix with the (i, j)-th entry being wij; D ∈ RN×N
is a diagonal matrix with the i-th entry on the diag-
onal being dii = ∑Nj=1 wij; and L = D − W ∈ RN×N
5Given any ¯θi, ¯θj, and δ, there always exists a linear trans-
( ))
formation which keeps the sign of δ − (¯θj − ¯θi unchanged.
is the graph Laplacian (Chung, 1997). Minimizing
R results in the smoothness assumption: for any
questions i and j, if their textual similarity wij is
high, the difficulty gap (¯θiq) − ¯θ(q))2 will be small.
j
A Hybrid Method. Combining Eq. (3) and
Eq. (4), we obtain RCM, which amounts to the
following optimization problem:
</bodyText>
<equation confidence="0.708214666666667">
∑min λ1¯T ¯ λ2 ¯T
θ¯ (i≺j)∈C ℓ(¯θi, ¯θj)+ 2 θ θ+ 2 θq
L¯θq. (5)
</equation>
<bodyText confidence="0.997975111111111">
Here λ2 ≥ 0 is also a trade-off coefficient. The
advantages of RCM include 1) It naturally formal-
izes QDE as minimizing a manifold regularized
loss function, which seamlessly integrates both the
pairwise competitions and the textual description-
s. 2) By incorporating textual information, it can
address the data sparseness problem which previ-
ous methods suffer from, and perform significantly
better in the QDE task.
</bodyText>
<subsectionHeader confidence="0.999855">
3.2 Learning Algorithm
</subsectionHeader>
<bodyText confidence="0.997348">
Redefine the k-th pairwise competition (assumed
to be carried out between competitors i and j) as
(xk, yk). xk ∈ RM+N indicates the competitors:
</bodyText>
<equation confidence="0.990988">
x(k)
i = 1, x(k) j= −1, and x(k)
l = 0 for any l # i, j,
</equation>
<bodyText confidence="0.9307185">
where x(k)
l is the l-th entry of xk. yk ∈ {1, −1} is
the outcome: if competitor i beats competitor j,
yk = 1; otherwise, yk = −1. The objective in Eq.
(5) can then be rewritten as
max (0, 1 − yk (¯θT xk))p + 2θT Z¯θ,
where Z =(λ1IM λ1IN0 λ2L) is a block matrix; IM ∈
RM×M and IN ∈ RN×N are identity matrices; p =
1 corresponds to the hinge loss, and p = 2 the
quadratic loss. It is clear that the loss defined in
Eq. (2) has the same format as the SVM loss.
The objective L is differentiable for the quadrat-
ic loss but non-differentiable for the hinge loss.
We employ a subgradient method (Boyd et al.,
2003) to solve the optimization problem. The al-
gorithm starts at a point ¯θ0 and, as many iterations
as needed, moves from ¯θt to ¯θt+1 in the direction
of the negative subgradient:
</bodyText>
<equation confidence="0.99524415">
)¯θt+1 = ¯θt − γt∇L (¯θt ,
= ¯θTq D T T ¯θq, (4)
θq − θq Wθq = θq L
2
∑N
i=1
N
∑
j=1
(¯θ(q) _ B(q))2 wij
i j
1
R=
∑min
θ¯ (i≺j)∈C
|C|
∑
k=1
)
L (¯θ =
</equation>
<page confidence="0.978248">
1119
</page>
<figure confidence="0.900673538461538">
Algorithm 1 Regularized Competition Model
Require: competition set C and description set D
1: ¯θ0 ← 1
2: for t = 0 : T − 1 do
{ (¯θT ) }
3: Kt ← k : 1 − yk t xk &gt; 0
)
4: ∇L (¯θt ← calculated by Eq. (6)
)5: ¯θt+1 ← ¯θt − γt∇L (¯θt
}6: Ot+1 ← {¯θ0, ¯θ1, · · · , ¯θt+1
)7: ¯θt+1 ← arg min¯θ∈Ot+1 L (¯θ
8: end for
9: return θT
</figure>
<bodyText confidence="0.937839">
where γt &gt; 0 is the learning rate. The subgradient
is calculated as
</bodyText>
<equation confidence="0.995481">
Z¯θt − ∑ ykxk, p=1,
k∈Kt ykxk, p=2, (6)
</equation>
<bodyText confidence="0.603919">
{ )
where Kt = k : 1 − yk (¯θT }
</bodyText>
<equation confidence="0.9432">
t xk &gt; 0 . As it is not
</equation>
<bodyText confidence="0.9769015">
always a descent method, we keep track of the best
point found so far (Boyd et al., 2003):
</bodyText>
<equation confidence="0.837198666666667">
¯θt+1 = arg min
¯θ∈Ot+1 L (¯θ ,
)
</equation>
<bodyText confidence="0.9988712">
}where Ot+1 = {¯θ0, ¯θ1, · · · , ¯θt+1 . The whole proce-
dure is summarized in Algorithm 1.
Convergence. For constant learning rate (i.e.,
γt = γ), Algorithm 1 is guaranteed to converge to
within some range of the optimal value, i.e.,
</bodyText>
<equation confidence="0.917664">
t→∞
lim )L (¯θt − L∗ &lt; ϵ,
</equation>
<bodyText confidence="0.9572977">
where L∗ denotes the minimum of L(·), and ϵ is a
constant defined by the learning rate γ. For more
details, please refer to (Boyd et al., 2003). During
our experiments, we set the iteration number as
T = 1000 and the learning rate as γt = 0.001, and
convergence was observed.
Complexity. For both the hinge loss and the
quadratic loss, the time complexity (per itera-
tion) and the space complexity of RCM are both
(
</bodyText>
<equation confidence="0.96879">
O |C |+ ηN2). Here, |C |is the total number of
</equation>
<bodyText confidence="0.995094">
competitions, M and N are the numbers of user-
s and questions respectively, and η is the ratio of
non-zero entries in the graph Laplacian L.6 In the
analysis, we have assumed that M ≪ ηN2 and
N ≪ ηN2.
</bodyText>
<footnote confidence="0.721227333333333">
6Owing to the sparse nature of questions’ textual descrip-
tions, the graph Laplacian L is usually sparse, with about
70% entries being zero according to our experiments.
</footnote>
<sectionHeader confidence="0.994793" genericHeader="method">
4 Cold-Start Estimation
</sectionHeader>
<bodyText confidence="0.99982796">
Previous sections discussed estimating difficulty s-
cores of resolved questions, from which pairwise
competitions could be extracted. However, for
newly posted questions without any answers re-
ceived, no competitions could be extracted and
none of the above methods work. We call it the
cold-start problem.
We heuristically apply a K-Nearest Neighbor
(KNN) approach (Cover and Hart, 1967) to cold-
start estimation, again by leveraging the smooth-
ness assumption. The key idea is to propagate
difficulty scores from well-resolved questions to
cold-start ones according to their textual simi-
larities. Specifically, suppose that there exists
a set of well-resolved questions whose difficul-
ty scores have already been estimated by a QDE
method. Given a cold-start question q∗, we first
pick K well-resolved questions that are closest to
q∗ in textual descriptions, referred to as the near-
est neighbors. The difficulty score of question q∗
is then predicted as the averaged difficulty scores
of its nearest neighbors. The KNN method bridges
the gap between cold-start and well-resolved ques-
tions by inferring their textual similarities, and
might effectively deal with the cold-start problem.
</bodyText>
<sectionHeader confidence="0.999788" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.9999176">
We have conducted experiments to test the effec-
tiveness of RCM in estimating difficulty scores of
both well-resolved and cold-start questions. More-
over, we have explored how a word’s difficulty lev-
el can be quantitatively measured by RCM.
</bodyText>
<subsectionHeader confidence="0.970585">
5.1 Experimental Settings
</subsectionHeader>
<bodyText confidence="0.999879833333334">
Data Sets. We obtained a publicly available da-
ta set of Stack Overflow between July 31, 2008
and August 1, 20127, containing QA threads in
various categories. We considered the categories
of “C++ programming” and “mathematics”, and
randomly sampled about 10,000 QA threads from
each category, denoted by SO/CPP and SO/Math
respectively. For each question, we took the title
and body fields as its textual description. For both
data sets, stop words in a standard list8 and words
whose total frequencies are less than 10 were re-
moved. Table 1 gives the statistics of the data sets.
</bodyText>
<footnote confidence="0.804844">
7http://blog.stackoverflow.com/category/cc-wiki-dump/
8http://jmlr.org/papers/volume5/lewis04a/a11-smart-
stop-list/english.stop
</footnote>
<equation confidence="0.5269935">
 

)
∇L (¯θt =
Z¯θt + 2 ∑ xkxTk ¯θt − 2 ∑
k∈Kt k∈Kt
</equation>
<page confidence="0.931903">
1120
</page>
<table confidence="0.966314666666667">
# users # questions # competitions # words
SO/CPP 14,884 10,164 50,043 2,208
SO/Math 6,564 10,528 40,396 2,009
</table>
<tableCaption confidence="0.999582">
Table 1: Statistics of the data sets.
</tableCaption>
<bodyText confidence="0.998057944444444">
For evaluation, we randomly sampled 600 ques-
tion pairs from each data set, and asked annotators
to compare the difficulty levels of the questions
in each pair. We had two graduate students ma-
joring in computer science annotate the SO/CPP
questions, and two majoring in mathematics an-
notate the SO/Math questions. For each question,
only the title, body, and tags were exposed to the
annotators. Given a question pair (q1, q2), the an-
notators were asked to give one of the three labels:
q1 &gt; q2, q2 &gt; q1, or q1 = q2, which respective-
ly means that question q1 has a higher, lower, or
equal difficulty level compared with question q2.
We used Cohen’s kappa coefficient (Cohen, 1960)
to measure the inter-annotator agreement. The re-
sult is K = 0.7533 on SO/CPP and K = 0.8017
on SO/Math, indicating that the inter-annotator a-
greement is quite substantial on both data sets. Af-
ter removing the question pairs with inconsisten-
t labels, we got 521 annotated SO/CPP question
pairs and 539 annotated SO/Math question pairs.
We further randomly split the annotated ques-
tion pairs into development/test/cold-start sets,
with the ratio of 2:2:1. The first two sets were used
to evaluate the methods in estimating difficulty s-
cores of resolved questions. Specifically, the de-
velopment set was used for parameter tuning and
the test set was used for evaluation. The last set
was used to evaluate the methods in cold-start esti-
mation, and the questions in this set were excluded
from the learning process of RCM as well as any
baseline method.
Baseline Methods. We considered three base-
line methods: PageRank (PR), TrueSkill (TS), and
CM, which are based solely on the pairwise com-
petitions.
</bodyText>
<listItem confidence="0.921033230769231">
• PR first constructs a competitor graph, by
creating an edge from competitor i to com-
petitor j if j beats i in a competition. A
PageRank algorithm (Page et al., 1999) is
then utilized to estimate the relative impor-
tance of the nodes, i.e., question difficulty s-
cores and user expertise scores. The damping
factor was set from 0.1 to 0.9 in steps of 0.1.
• TS has been applied to QDE by Liu et al.
(2013). We set the model parameters in the
same way as they suggested.
• CM performs QDE by solving Eq. (3). We
set A1 in {0, 0.01, 0.02,0.05,0.1,0.2,0.5, 1}.
</listItem>
<bodyText confidence="0.9803332">
We compared RCM with the above baseline meth-
ods. In RCM, both parameters A1 and A2 were set
in {0, 0.01, 0.02,0.05,0.1,0.2,0.5, 1}.
Evaluation Metric. We employed accuracy
(ACC) as the evaluation metric:
</bodyText>
<equation confidence="0.4159945">
# correctly judged question pairs
ACC =
</equation>
<bodyText confidence="0.9992384">
A question pair is regarded as correctly judged if
the relative difficulty ranking given by an estima-
tion method is consistent with that given by the
annotators. The higher the accuracy is, the better
a method performs.
</bodyText>
<subsectionHeader confidence="0.89717">
5.2 Estimation for Resolved Questions
</subsectionHeader>
<bodyText confidence="0.998349580645161">
The first experiment tested the methods in estimat-
ing difficulty scores of resolved questions.
Estimation Accuracies. We first compared the
estimation accuracies of PR, TS, CM, and RCM
on the test sets of SO/CPP and SO/Math, obtained
with the best parameter settings determined by the
development sets. Table 2 gives the results, where
“H” denotes the hinge loss and “Q” the quadratic
loss. In RCM, to calculate the graph Laplacian L,
we adopted Boolean term weighting schema and
took Jaccard coefficient as the similarity measure.
From the results, we can see that 1) RCM perform-
s significantly better than the baseline methods on
both data sets (t-test, p-value &lt; 0.05), demonstrat-
ing the advantage of exploiting questions’ textu-
al descriptions for QDE. 2) The improvements of
RCM over the baseline methods on SO/Math are
greater than those on SO/CPP, indicating that the
textual descriptions of the SO/Math questions are
more powerful in reflecting their difficulty level-
s. The reason is that the SO/Math questions are
much more heterogeneous, belonging to various
subfields of mathematics. The difficulty gaps a-
mong different subfields are sometimes obvious
(e.g., a question in topology in general has a high-
er difficulty level than a question in linear algebra),
making the textual descriptions more powerful in
distinguishing the difficulty levels.
Graph Laplacian Variants. We further inves-
tigated the performances of different term weight-
ing schemas and similarity measures in the graph
</bodyText>
<figure confidence="0.261603">
.
# all question pairs
</figure>
<page confidence="0.641725">
1121
</page>
<table confidence="0.7277698">
CM RCM
PR TS
H Q H Q
SO/CPP 0.5876 0.6134 0.6340 0.6753 0.7371 0.7268
SO/Math 0.6067 0.6109 0.6527 0.6820 0.7699 0.7699
</table>
<tableCaption confidence="0.7566916">
Table 2: ACC of different methods for well-
resolved questions.
Notation Definition
i 1, if word w occurs in question q
0, otherwise
</tableCaption>
<equation confidence="0.997158882352941">
TF-1 v(w, q) = f(w, q), the number of occurrences
TF-2 v(w, q) = log (f(w, q) + 1)
0.5 × f(w, q)
TF-3 v(w, q) = 0.5 +
max { f (w, q) : w ∈ q}
TFIDF-1 v(w, q) = TF-1 × log |Q|
|{q∈Q:w∈q}|
TFIDF-2 v(w, q) = TF-2 × log |Q|
|{q∈Q:w∈q}|
TFIDF-3 v(w, q) = TF-3 × log |Q|
|{q∈Q:w∈q}|
dT
Cosine Sim (d1, d2) = ∥d1∥×∥d2∥ ∈ [0, 1]
1 d2
dT
Jaccard Sim (d1, d2) = ∥d1∥2+∥d2∥2−∥d1∥×∥d2∥ ∈ [0, 1]
1 d2
</equation>
<tableCaption confidence="0.892329">
Table 3: Different term weighting schemas and
similarity measures.
</tableCaption>
<bodyText confidence="0.99930792">
Laplacian. The term weighting schema deter-
mines how a question’s textual description is rep-
resented. We explored a Boolean schema, three
TF schemas, and three TFIDF schemas (Salton
and Buckley, 1988). The similarity measure de-
termines how the textual similarity between two
questions is calculated. We explored the Co-
sine similarity and the Jaccard coefficient (Huang,
2008). Detailed descriptions are given in Table 3.
Figure 2 and Figure 3 show the estimation ac-
curacies of the RCM variants on the test sets of
SO/CPP and SO/Math respectively, again obtained
with the best parameter settings determined by
the development sets. The performance of CM
is also given (the straight lines in the figures).9
From the results, we can see that 1) All the RCM
variants can improve over CM on both data sets,
and most of the improvements are significant (t-
test, p-value &lt; 0.05). This further demonstrates
that the effectiveness of incorporating textual de-
scriptions is not affected by the particular choice
of the term weighting schema or similarity mea-
sure. 2) Boolean term weighting schema performs
the best, considering different similarity measures,
loss types, and data sets collectively. 3) Jaccard
</bodyText>
<footnote confidence="0.797219">
9CM performs better than PR and TS on both data sets.
</footnote>
<figureCaption confidence="0.99716625">
Figure 2: ACC of RCM variants for well-resolved
questions on SO/CPP.
Figure 3: ACC of RCM variants for well-resolved
questions on SO/Math.
</figureCaption>
<bodyText confidence="0.9997744">
coefficient performs as well as Cosine similari-
ty on SO/Math, but almost consistently better on
SO/CPP. Throughout the experiments, we adopted
Boolean term weighting schema and Jaccard coef-
ficient to calculate the graph Laplacian.
</bodyText>
<subsectionHeader confidence="0.951039">
5.3 Estimation for Cold-Start Questions
</subsectionHeader>
<bodyText confidence="0.999943318181818">
The second experiment tested the methods in es-
timating difficulty scores of cold-start questions.
We employed Boolean term weighting schema to
represent a cold-start question, and utilized Jac-
card Coefficient to select its nearest neighbors.
Figure 4 and Figure 5 list the cold-start estima-
tion accuracies of different methods on SO/CPP
and SO/Math respectively, with different K val-
ues (the number of nearest neighbors). As the
accuracy oscillates drastically with a K value s-
maller than 11 on SO/CPP and smaller than 6 on
SO/Math, we report the results with K ∈ [11, 20]
on SO/CPP and K ∈ [6,15] on SO/Math. The av-
eraged (over different K values) cold-start estima-
tion accuracies are further given in Table 4. All the
results are reported on the cold-start sets, with the
optimal parameter settings adopted in Section 5.2.
From the results, we can see that 1) Cold-start es-
timation is possible, and can achieve a consider-
ably high accuracy by choosing a proper method
(e.g. RCM), making applications such as better
question routing and better incentive mechanism
</bodyText>
<figure confidence="0.999193111111111">
(a) Hinge loss. (b) Quadratic loss.
0.75
0.65
0.55
0.7
0.6
Cosine Jaccard CM(Q)
Cosine Jaccare CM(H)
0.75
0.7
0.65
0.6
0.55
(a) Hinge loss. (b) Quadratic loss.
0.75
0.65
0.8
0.7
0.6
Cosine Jaccard CM(Q)
Cosine Jaccard CM(R)
0.8
0.75
0.7
0.65
0.6
Boolean v(w, q) =
</figure>
<page confidence="0.837776">
1122
</page>
<figureCaption confidence="0.99686375">
Figure 4: ACC of different methods for cold-start
questions on SO/CPP.
Figure 5: ACC of different methods for cold-start
questions on SO/Math.
</figureCaption>
<bodyText confidence="0.999766666666667">
design feasible in practice. 2) As the value of K
varies, RCM (the red/blue solid line) performs al-
most consistently better than CM with the same
loss type (the red/blue dotted line), as well as PR
and TS (the gray dotted lines), showing the advan-
tages of RCM in the cold-start estimation. 3) The
cold-start estimation accuracies on SO/Math are
higher than those on SO/CPP, again demonstrating
that the textual descriptions of the SO/Math ques-
tions are more powerful in reflecting their difficul-
ty levels. This is consistent with the phenomenon
observed in Section 5.2.
</bodyText>
<subsectionHeader confidence="0.994855">
5.4 Difficulty Levels of Words
</subsectionHeader>
<bodyText confidence="0.9999545">
The third experiment explored how a word’s diffi-
culty level can be measured by RCM automatical-
ly and quantitatively.
On both SO/CPP and SO/Math, we evenly split
the range of question difficulty scores (estimated
by RCM) into 10 buckets, and assigned questions
to the buckets according to their difficulty scores.
A larger bucket ID indicates a higher difficulty lev-
el. Then, given a word w, we calculated its fre-
quency in each bucket as follows:
</bodyText>
<equation confidence="0.8396755">
# questions in bucket i where w occurs
fi(w) _
</equation>
<bodyText confidence="0.9934265">
To make the frequency meaningful, buckets with
less than 50 questions were discarded. We picked
</bodyText>
<table confidence="0.936589">
CM RCM
PR TS
H Q H Q
SO/CPP 0.5870 0.5413 0.6120 0.6304 0.6380 0.6609
SO/Math 0.6411 0.6305 0.6653 0.7263 0.6958 0.7442
</table>
<tableCaption confidence="0.9749475">
Table 4: Averaged ACC of different methods for
cold-start questions.
</tableCaption>
<figureCaption confidence="0.9845815">
Figure 6: Frequencies of different words in the
buckets on SO/CPP.
</figureCaption>
<bodyText confidence="0.999952785714286">
four words from each data set as examples. Their
normalized frequencies in different buckets are
shown in Figure 6 and Figure 7. On SO/CPP,
we can observe that “array” and “string” occur
most frequently in questions with lower difficul-
ty levels, “virtual” higher, and “multithread” the
highest. It coincides with the intuition: “array”
and “string” are usually related to some basic con-
cepts in programming language, while “virtual”
and “multithread” usually discuss more advanced
topics. Similar phenomena can be observed on
SO/Math. The results indicate that RCM might
provide an automatic way to measure the difficul-
ty levels of words.
</bodyText>
<sectionHeader confidence="0.99994" genericHeader="method">
6 Related Work
</sectionHeader>
<bodyText confidence="0.999883125">
QDE is relevant to the problem of estimating task
difficulty levels and user expertise levels in crowd-
sourcing services (Yang et al., 2008; Whitehill et
al., 2009). Studies on this problem fall into two
categories: 1) binary response based and 2) par-
tially ordered response based. In the first cate-
gory, binary responses (i.e. whether the solution
provided by a user is correct or not) are observed,
and techniques based on item response theory are
further employed (Whitehill et al., 2009; Welin-
der et al., 2010; Zhou et al., 2012). In the second
category, partially ordered responses (i.e. which
of the two given solutions is better) are observed,
and pairwise comparison based methods are fur-
ther adopted (Yang et al., 2008; Liu et al., 2013).
QDE belongs to the latter.
</bodyText>
<figure confidence="0.967794965517241">
Accuracy
0.75
0.65
0.55
0.45
11 12 13 14 15 16 17 18 19 20
PR TS CM(H)
CM(Q) RCM(H) RCM(Q)
K
Accuracy
0.8
0.7
0.6
0.5
6 7 8 9 10 11 12 13 14 15
PR TS CM(H)
CM(Q) RCM(H) RCM(Q)
K
Occurrence frequency
0.8
0.4
1.2
0
3 3.5 4 4.5 5 5.5 6 6.5 7
array string virtual multithread
Question buckets
.
# all questions in bucket i
1123
</figure>
<figureCaption confidence="0.993294">
Figure 7: Frequencies of different words in the
buckets on SO/Math.
</figureCaption>
<bodyText confidence="0.999977479166667">
The most relevant work to ours is a pairwise
comparison based approach proposed by Liu et al.
(2013) to estimate question difficulty levels in C-
QA services. They have also demonstrated that
a similar approach can be utilized to estimate us-
er expertise levels (Liu et al., 2011). Yang et al.
(2008) and Chen et al. (2013) have also proposed
pairwise comparison based methods, for task dif-
ficulty estimation and rank aggregation in crowd-
sourcing settings. Our work differs from previous
pairwise comparison based methods in that it fur-
ther utilizes textual information, formalized as a
manifold regularizer.
Manifold regularization is a geometrically mo-
tivated framework for machine learning, enforcing
the learning model to be smooth w.r.t. the geomet-
rical structure of data (Belkin et al., 2006). Within
the framework, dimensionality reduction (Belkin
and Niyogi, 2001; Cai et al., 2008) and semi-
supervised learning (Zhou et al., 2004; Zhu and
Lafferty, 2005) algorithms have been constructed.
In dimensionality reduction, manifold regulariza-
tion is utilized to guarantee that nearby points will
have similar low-dimensional representations (Cai
et al., 2008), while in semi-supervised learning it
is utilized to ensure that nearby points will have
similar labels (Zhou et al., 2004). In our work, we
assume that nearby questions (in terms of textual
similarities) will have similar difficulty levels.
Predicting reading difficulty levels of text is
also a relevant problem (Collins-Thompson and
Callan, 2004; Schwarm and Ostendorf, 2005). It
is a key to automatically finding materials at ap-
preciate reading levels for students, and also helps
in personalized web search (Collins-Thompson et
al., 2011). In the task of predicting reading dif-
ficulty levels, documents targeting different grade
levels are taken as ground truth, which can be eas-
ily obtained from the web. However, there is no
naturally annotated data for our QDE task on the
web. Other related problems include query dif-
ficulty estimation for search engines (Carmel et
al., 2006; Yom-Tov et al., 2005) and question dif-
ficulty estimation for automatic question answer-
ing systems (Lange et al., 2004). In these tasks,
query/question difficulty is system-oriented and ir-
relevant with human knowledge, which is a differ-
ent setting from ours.
</bodyText>
<sectionHeader confidence="0.995961" genericHeader="conclusions">
7 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.9999861">
In this paper, we have proposed a novel method for
estimating question difficulty levels in CQA ser-
vices, called Regularized Competition Model (R-
CM). It takes fully advantage of questions’ textu-
al descriptions besides question-user comparisons,
and thus can effectively deal with data sparsity and
perform more accurate estimation. A K-Nearest
Neighbor approach is further adopted to estimate
difficulty levels of cold-start questions. Experi-
ments on two publicly available data sets show
that RCM performs significantly better than exist-
ing methods in the estimation task, for both well-
resolved and cold-start questions, demonstrating
the advantage of incorporating textual informa-
tion. It is also observed that RCM might automat-
ically measure the knowledge levels of words.
As future work, we plan to 1) Enhance the ef-
ficiency and scalability of RCM. The complexity
analysis in Section 3.2 indicates that storing and
processing the graph Laplacian is a bottleneck of
RCM. We would like to investigate how to deal
with the bottleneck, e.g., via parallel or distribut-
ed computing. 2) Apply RCM to non-technical
domains. For non-technical domains such as the
“news and events” category of Yahoo! Answer-
s, there might be no strongly distinct notions of
“experts” and “non-experts”, and it might be more
difficult to distinguish between “hard questions”
and “easy questions”. It is worthy investigating
whether RCM still works on such domains.
</bodyText>
<sectionHeader confidence="0.998812" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9995605">
We would like to thank the anonymous review-
ers for their helpful comments. This work is
supported by the Strategic Priority Research Pro-
gram of the Chinese Academy of Sciences (grant
No. XDA06030200), the National Key Technolo-
gy R&amp;D Program (grant No. 2012BAH46B03),
and the National Natural Science Foundation of
China (grant No. 61272427).
</bodyText>
<figure confidence="0.999068625">
Occurrence frequency
0.8
0.4
1.2
0
4 5 6 7 8 9
homework calculus ring topology
Question buckets
</figure>
<page confidence="0.991724">
1124
</page>
<sectionHeader confidence="0.995262" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999585785046729">
Mark S. Ackerman and David W. McDonald. 1996.
Answer garden 2: merging organizational memory
with collaborative help. In Proceedings of the 1996
ACM Conference on Computer Supported Coopera-
tive Work, pages 97–105.
Mikhail Belkin and Partha Niyogi. 2001. Laplacian
eigenmaps and spectral techniques for embedding
and clustering. In Advances in Neural Information
Processing Systems, pages 585–591.
Mikhail Belkin, Partha Niyogi, and Vikas Sindhwani.
2006. Manifold regularization: a geometric frame-
work for learning from labeled and unlabeled ex-
amples. Journal of Machine Learning Research,
7:2399–2434.
Stephen Boyd, Lin Xiao, and Almir Mutapcic. 2003.
Subgradient methods. Lecture Notes of EE392o, S-
tanford University.
Deng Cai, Xiaofei He, Xiaoyun Wu, and Jiawei Han.
2008. Non-negative matrix factorization on mani-
fold. In Proceedings of the 8th IEEE International
Conference on Data Mining, pages 63–72.
David Carmel, Elad Yom-Tov, Adam Darlow, and Dan
Pelleg. 2006. What makes a query difficult? In
Proceedings of the 29th International ACM SIGIR
Conference on Research and Development in Infor-
mation Retrieval, pages 390–397.
Olivier Chapelle. 2007. Training a support vec-
tor machine in the primal. Neural Computation,
19(5):1155–1178.
Xi Chen, Paul N. Bennett, Kevyn Collins-Thompson,
and Eric Horvitz. 2013. Pairwise ranking aggrega-
tion in a crowdsourced setting. In Proceedings of the
6th ACM International Conference on Web Search
and Data Mining, pages 193–202.
Fan RK. Chung. 1997. Spectral Graph Theory, vol-
ume 92.
Kenneth Church. 2011. How many multiword expres-
sions do people know. In Proceedings of the ACL-
HLT Workshop on Multiword Expressions: from
Parsing and Generation to the Real World, pages
137–144.
Mark Claypool, Anuja Gokhale, Tim Miranda, Pavel
Murnikov, Dmitry Netes, and Matthew Sartin. 1999.
Combining content-based and collaborative filters in
an online newspaper. In Proceedings of the ACM
SIGIR workshop on Recommender Systems.
Jacob Cohen. 1960. A coefficient of agreemen-
t for nominal scales. Educational and Psychological
Measurement, 20(1):37–46.
Kevyn Collins-Thompson and James P. Callan. 2004.
A language modeling approach to predicting reading
difficulty. In Proceedings of the 2004 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 193–200.
Kevyn Collins-Thompson, Paul N Bennett, Ryen W
White, Sebastian de la Chica, and David Sontag.
2011. Personalizing web search results by reading
level. In Proceedings of the 20th ACM Internation-
al Conference on Information and Knowledge Man-
agement, pages 403–412.
Thomas Cover and Peter Hart. 1967. Nearest neighbor
pattern classification. IEEE Transactions on Infor-
mation Theory, 13(1):21–27.
Zhicheng Dou, Ruihua Song, and Ji Rong Wen. 2007.
A large-scale evaluation and analysis of personal-
ized search strategies. In Proceedings of the 16th
International Conference on World Wide Web, pages
581–590.
Ralf Herbrich, Tom Minka, and Thore Graepel. 2006.
Trueskill: a bayesian skill rating system. In Ad-
vances in Neural Information Processing Systems,
pages 569–576.
Anna Huang. 2008. Similarity measures for text doc-
ument clustering. In Proceedings of the 6th New
Zealand Computer Science Research Student Con-
ference, pages 49–56.
Rense Lange, Juan Moran, Warren R. Greiff, and Lisa
Ferro. 2004. A probabilistic rasch analysis of ques-
tion answering evaluations. In Proceedings of the
2004 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 65–72.
Baichuan Li and Irwin King. 2010. Routing ques-
tions to appropriate answerers in community ques-
tion answering services. In Proceedings of the 19th
ACM International Conference on Information and
Knowledge Management, pages 1585–1588.
Jing Liu, Young-In Song, and Chin-Yew Lin. 2011.
Competition-based user expertise score estimation.
In Proceedings of the 34th International ACM SI-
GIR Conference on Research and Development in
Information Retrieval, pages 425–434.
Jing Liu, Quan Wang, Chin-Yew Lin, and Hsiao-Wuen
Hon. 2013. Question difficulty estimation in com-
munity question answering services. In Proceed-
ings of the 2013 Conference on Empirical Methods
in Natural Language Processing, pages 85–90.
Kevin Kyung Nam, Mark S. Ackerman, and Lada A.
Adamic. 2009. Questions in, knowledge in?: a s-
tudy of naver’s question answering community. In
Proceedings of the SIGCHI Conference on Human
Factors in Computing Systems, pages 779–788.
Larry Page, Sergey Brin, Rajeev Motwani, and Terry
Winograd. 1999. The pagerank citation ranking:
bringing order to the web. Technical Report, Stan-
ford University.
</reference>
<page confidence="0.857505">
1125
</page>
<reference confidence="0.99960816923077">
Joseph Lee Rodgers and W. Alan Nicewander. 1988.
Thirteen ways to look at the correlation coefficient.
The American Statistician, 42(1):59–66.
Gerard Salton and Christopher Buckley. 1988. Term-
weighting approaches in automatic text retrieval. In-
formation Processing fr Management, 24(5):513–
523.
Andrew I. Schein, Alexandrin Popescul, Lyle H. Ungar,
and David M. Pennock. 2002. Methods and met-
rics for cold-start recommendations. In Proceed-
ings of the 25th International ACM SIGIR Confer-
ence on Research and Development in Information
Retrieval, pages 253–260.
Sarah E. Schwarm and Mari Ostendorf. 2005. Reading
level assessment using support vector machines and
statistical language models. In Proceedings of the
43rd Annual Meeting on Association for Computa-
tional Linguistics, pages 523–530.
Kazunari Sugiyama, Kenji Hatano, and Masatoshi
Yoshikawa. 2004. Adaptive web search based on
user profile constructed without any effort from user-
s. In Proceedings of the 13th International Confer-
ence on World Wide Web, pages 675–684.
Peter Welinder, Steve Branson, Serge Belongie, and
Pietro Perona. 2010. The multidimensional wis-
dom of crowds. In Advances in Neural Information
Processing Systems, pages 2424–2432.
Jacob Whitehill, Paul Ruvolo, Tingfan Wu, Jacob
Bergsma, and Javier R Movellan. 2009. Whose
vote should count more: optimal integration of la-
bels from labelers of unknown expertise. In Ad-
vances in Neural Information Processing Systems,
pages 2035–2043.
Jiang Yang, Lada Adamic, and Mark Ackerman. 2008.
Competing to share expertise: the taskcn knowledge
sharing community. In Proceedings of the 2nd In-
ternational AAAI Conference on Weblogs and Social
Media.
Elad Yom-Tov, Shai Fine, David Carmel, and Adam
Darlow. 2005. Learning to estimate query difficulty:
including applications to missing content detection
and distributed information retrieval. In Proceed-
ings of the 28th International ACM SIGIR Confer-
ence on Research and Development in Information
Retrieval, pages 512–519.
Dengyong Zhou, Olivier Bousquet, Thomas Navin
Lal, Jason Weston, and Bernhard Sch¨olkopf. 2004.
Learning with local and global consistency. In Ad-
vances in Neural Information Processing Systems,
pages 321–328.
Yanhong Zhou, Gao Cong, Bin Cui, Christian S.
Jensen, and Junjie Yao. 2009. Routing questions to
the right users in online communities. In Proceed-
ings of the 25th IEEE International Conference on
Data Engineering, pages 700–711.
Dengyong Zhou, John C Platt, Sumit Basu, and Y-
i Mao. 2012. Learning from the wisdom of crowds
by minimax entropy. In Advances in Neural Infor-
mation Processing Systems, pages 2204–2212.
Xiaojin Zhu and John Lafferty. 2005. Harmonic mix-
tures: combining mixture models and graph-based
methods for inductive and scalable semi-supervised
learning. In Proceedings of the 22nd Internation-
al Conference on Machine Learning, pages 1052–
1059.
</reference>
<page confidence="0.994504">
1126
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.823556">
<title confidence="0.9988785">Regularized Competition Model for Question Estimation Community Question Answering Services</title>
<author confidence="0.999021">Jing Bin Li</author>
<affiliation confidence="0.926177">of Information Engineering, Chinese Academy of Sciences, Beijing, P. R. Institute of Technology, Harbin, P. R.</affiliation>
<email confidence="0.898826">jliu@ir.hit.edu.cn</email>
<abstract confidence="0.999602606060606">questions’ levels is an important task in community question answering (CQA) services. Previous studies propose to solve this problem based on the question-user comparisons extracted from the question answering threads. they from data sparseness problem as each question only gets a limited number of comparisons. Moreover, they cannot handle newly posted questions which get no comparisons. In this pawe propose a novel question ty estimation approach called Regularized Competition Model (RCM), which naturally combines question-user comparisons and questions’ textual descriptions into a unified framework. By incorporating texinformation, RCM can deal with data sparseness problem. We further employ a K-Nearest Neighbor approach to levels of newly posted questions, again by leveraging textual similarities. Experiments on two publicly available data sets show that for both well-resolved and newly-posted questions, RCM performs the estimation task significantly better than existing methods, demonstrating the advantage of incorporating textual information. More interestingly, we observe that RCM might provide an automatic way to quantitatively measure the knowledge levels of words.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Mark S Ackerman</author>
<author>David W McDonald</author>
</authors>
<title>Answer garden 2: merging organizational memory with collaborative help.</title>
<date>1996</date>
<booktitle>In Proceedings of the 1996 ACM Conference on Computer Supported Cooperative Work,</booktitle>
<pages>97--105</pages>
<contexts>
<context position="2366" citStr="Ackerman and McDonald (1996)" startWordPosition="338" endWordPosition="341">ave been widely used in various scenarios, including general information seeking on the web1, knowl1http://answers.yahoo.com/ edge exchange in professional communities2, and question answering in massive open online courses (MOOCs)3, to name a few. An important research problem in CQA is how to automatically estimate the difficulty levels of questions, i.e., question di�culty estimation (QDE). QDE can benefit many applications. Examples include 1) Question routing. Routing questions to appropriate answerers can help obtain quick and high-quality answers (Li and King, 2010; Zhou et al., 2009). Ackerman and McDonald (1996) have demonstrated that routing questions by matching question difficulty level with answerer expertise level will make better use of answerers’ time and expertise. This is even more important for enterprise question answering and MOOCs question answering, where human resources are expensive. 2) Incentive mechanism design. Nam et al. (2009) have found that winning point awards offered by reputation systems is a driving factor for user participation in CQA services. Assigning higher point awards to more difficult questions will significantly improve user participation and satisfaction. 3) Lingu</context>
</contexts>
<marker>Ackerman, McDonald, 1996</marker>
<rawString>Mark S. Ackerman and David W. McDonald. 1996. Answer garden 2: merging organizational memory with collaborative help. In Proceedings of the 1996 ACM Conference on Computer Supported Cooperative Work, pages 97–105.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mikhail Belkin</author>
<author>Partha Niyogi</author>
</authors>
<title>Laplacian eigenmaps and spectral techniques for embedding and clustering.</title>
<date>2001</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>585--591</pages>
<contexts>
<context position="5835" citStr="Belkin and Niyogi, 2001" startWordPosition="879" endWordPosition="882">orm QDE. Preliminary observations have shown that a question’s difficulty level can be indicated by its textual description (Liu et al., 2013). We take advantage of the observations, and assume that if two questions are close in their textual descriptions, they will also be close in their difficulty levels, i.e., the smoothness assumption. We employ manifold regularization (Belkin et al., 2006) to characterize the assumption. Manifold regularization is a wellknown technique to preserve local invariance in manifold learning algorithms, i.e., nearby points are likely to have similar embeddings (Belkin and Niyogi, 2001). Then, we propose a novel Regularized Competition Model (RCM), which formalizes QDE as minimizing a loss on questionuser comparisons with manifold regularization on questions’ textual descriptions. As the smoothness assumption offers extra information for inferring question difficulty levels, incorporating it will effectively deal with data sparsity. Finally, we adopt a K-Nearest Neighbor approach (Cover and Hart, 1967) to perform cold-start estimation, again by leveraging the smoothness assumption. Experiments on two publicly available data sets collected from Stack Overflow show that 1) RCM</context>
<context position="18489" citStr="Belkin and Niyogi, 2001" startWordPosition="3012" endWordPosition="3015">et C, estimating skill levels of (pseudo) users then amounts to solving the following optimization problem: λ1¯T- 3 ℓ (¯θi, ¯θj) + 2 θ e, ( ) where the first term is the empirical loss measuring the total inconsistency; the second term is a regularizer to prevent overfitting; and λ1 ≥ 0 is a trade-off coefficient. It is also a competition-based QDE method, called Competition Model (CM). Exploiting Question Descriptions. Manifold regularization is a well-known technique used in manifold learning algorithms to preserve local invariance, i.e., nearby points are likely to have similar embeddings (Belkin and Niyogi, 2001). In QDE, the smoothness assumption expresses similar “invariance”, i.e., nearby questions (in terms of textual similarities) are likely to have similar difficulty scores. Hence, we characterize the assumption with the following manifold regularizer: where wij is the textual similarity between question i and question j; W ∈ RN×N is the similarity matrix with the (i, j)-th entry being wij; D ∈ RN×N is a diagonal matrix with the i-th entry on the diagonal being dii = ∑Nj=1 wij; and L = D − W ∈ RN×N 5Given any ¯θi, ¯θj, and δ, there always exists a linear trans( )) formation which keeps the sign </context>
<context position="37443" citStr="Belkin and Niyogi, 2001" startWordPosition="6302" endWordPosition="6305">ertise levels (Liu et al., 2011). Yang et al. (2008) and Chen et al. (2013) have also proposed pairwise comparison based methods, for task difficulty estimation and rank aggregation in crowdsourcing settings. Our work differs from previous pairwise comparison based methods in that it further utilizes textual information, formalized as a manifold regularizer. Manifold regularization is a geometrically motivated framework for machine learning, enforcing the learning model to be smooth w.r.t. the geometrical structure of data (Belkin et al., 2006). Within the framework, dimensionality reduction (Belkin and Niyogi, 2001; Cai et al., 2008) and semisupervised learning (Zhou et al., 2004; Zhu and Lafferty, 2005) algorithms have been constructed. In dimensionality reduction, manifold regularization is utilized to guarantee that nearby points will have similar low-dimensional representations (Cai et al., 2008), while in semi-supervised learning it is utilized to ensure that nearby points will have similar labels (Zhou et al., 2004). In our work, we assume that nearby questions (in terms of textual similarities) will have similar difficulty levels. Predicting reading difficulty levels of text is also a relevant pr</context>
</contexts>
<marker>Belkin, Niyogi, 2001</marker>
<rawString>Mikhail Belkin and Partha Niyogi. 2001. Laplacian eigenmaps and spectral techniques for embedding and clustering. In Advances in Neural Information Processing Systems, pages 585–591.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mikhail Belkin</author>
<author>Partha Niyogi</author>
<author>Vikas Sindhwani</author>
</authors>
<title>Manifold regularization: a geometric framework for learning from labeled and unlabeled examples.</title>
<date>2006</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>7--2399</pages>
<contexts>
<context position="5608" citStr="Belkin et al., 2006" startWordPosition="847" endWordPosition="850">s usually required that the difficulty level of a question is known instantly after it is posted. To address the drawbacks, we propose further exploiting questions’ textual descriptions (e.g., title, body, and tags) to perform QDE. Preliminary observations have shown that a question’s difficulty level can be indicated by its textual description (Liu et al., 2013). We take advantage of the observations, and assume that if two questions are close in their textual descriptions, they will also be close in their difficulty levels, i.e., the smoothness assumption. We employ manifold regularization (Belkin et al., 2006) to characterize the assumption. Manifold regularization is a wellknown technique to preserve local invariance in manifold learning algorithms, i.e., nearby points are likely to have similar embeddings (Belkin and Niyogi, 2001). Then, we propose a novel Regularized Competition Model (RCM), which formalizes QDE as minimizing a loss on questionuser comparisons with manifold regularization on questions’ textual descriptions. As the smoothness assumption offers extra information for inferring question difficulty levels, incorporating it will effectively deal with data sparsity. Finally, we adopt a</context>
<context position="37370" citStr="Belkin et al., 2006" startWordPosition="6293" endWordPosition="6296">onstrated that a similar approach can be utilized to estimate user expertise levels (Liu et al., 2011). Yang et al. (2008) and Chen et al. (2013) have also proposed pairwise comparison based methods, for task difficulty estimation and rank aggregation in crowdsourcing settings. Our work differs from previous pairwise comparison based methods in that it further utilizes textual information, formalized as a manifold regularizer. Manifold regularization is a geometrically motivated framework for machine learning, enforcing the learning model to be smooth w.r.t. the geometrical structure of data (Belkin et al., 2006). Within the framework, dimensionality reduction (Belkin and Niyogi, 2001; Cai et al., 2008) and semisupervised learning (Zhou et al., 2004; Zhu and Lafferty, 2005) algorithms have been constructed. In dimensionality reduction, manifold regularization is utilized to guarantee that nearby points will have similar low-dimensional representations (Cai et al., 2008), while in semi-supervised learning it is utilized to ensure that nearby points will have similar labels (Zhou et al., 2004). In our work, we assume that nearby questions (in terms of textual similarities) will have similar difficulty l</context>
</contexts>
<marker>Belkin, Niyogi, Sindhwani, 2006</marker>
<rawString>Mikhail Belkin, Partha Niyogi, and Vikas Sindhwani. 2006. Manifold regularization: a geometric framework for learning from labeled and unlabeled examples. Journal of Machine Learning Research, 7:2399–2434.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Boyd</author>
<author>Lin Xiao</author>
<author>Almir Mutapcic</author>
</authors>
<date>2003</date>
<booktitle>Subgradient methods. Lecture Notes of EE392o,</booktitle>
<institution>Stanford University.</institution>
<contexts>
<context position="20759" citStr="Boyd et al., 2003" startWordPosition="3435" endWordPosition="3438"> # i, j, where x(k) l is the l-th entry of xk. yk ∈ {1, −1} is the outcome: if competitor i beats competitor j, yk = 1; otherwise, yk = −1. The objective in Eq. (5) can then be rewritten as max (0, 1 − yk (¯θT xk))p + 2θT Z¯θ, where Z =(λ1IM λ1IN0 λ2L) is a block matrix; IM ∈ RM×M and IN ∈ RN×N are identity matrices; p = 1 corresponds to the hinge loss, and p = 2 the quadratic loss. It is clear that the loss defined in Eq. (2) has the same format as the SVM loss. The objective L is differentiable for the quadratic loss but non-differentiable for the hinge loss. We employ a subgradient method (Boyd et al., 2003) to solve the optimization problem. The algorithm starts at a point ¯θ0 and, as many iterations as needed, moves from ¯θt to ¯θt+1 in the direction of the negative subgradient: )¯θt+1 = ¯θt − γt∇L (¯θt , = ¯θTq D T T ¯θq, (4) θq − θq Wθq = θq L 2 ∑N i=1 N ∑ j=1 (¯θ(q) _ B(q))2 wij i j 1 R= ∑min θ¯ (i≺j)∈C |C| ∑ k=1 ) L (¯θ = 1119 Algorithm 1 Regularized Competition Model Require: competition set C and description set D 1: ¯θ0 ← 1 2: for t = 0 : T − 1 do { (¯θT ) } 3: Kt ← k : 1 − yk t xk &gt; 0 ) 4: ∇L (¯θt ← calculated by Eq. (6) )5: ¯θt+1 ← ¯θt − γt∇L (¯θt }6: Ot+1 ← {¯θ0, ¯θ1, · · · , ¯θt+1 )7</context>
<context position="22107" citStr="Boyd et al., 2003" startWordPosition="3744" endWordPosition="3747">∑ ykxk, p=1, k∈Kt ykxk, p=2, (6) { ) where Kt = k : 1 − yk (¯θT } t xk &gt; 0 . As it is not always a descent method, we keep track of the best point found so far (Boyd et al., 2003): ¯θt+1 = arg min ¯θ∈Ot+1 L (¯θ , ) }where Ot+1 = {¯θ0, ¯θ1, · · · , ¯θt+1 . The whole procedure is summarized in Algorithm 1. Convergence. For constant learning rate (i.e., γt = γ), Algorithm 1 is guaranteed to converge to within some range of the optimal value, i.e., t→∞ lim )L (¯θt − L∗ &lt; ϵ, where L∗ denotes the minimum of L(·), and ϵ is a constant defined by the learning rate γ. For more details, please refer to (Boyd et al., 2003). During our experiments, we set the iteration number as T = 1000 and the learning rate as γt = 0.001, and convergence was observed. Complexity. For both the hinge loss and the quadratic loss, the time complexity (per iteration) and the space complexity of RCM are both ( O |C |+ ηN2). Here, |C |is the total number of competitions, M and N are the numbers of users and questions respectively, and η is the ratio of non-zero entries in the graph Laplacian L.6 In the analysis, we have assumed that M ≪ ηN2 and N ≪ ηN2. 6Owing to the sparse nature of questions’ textual descriptions, the graph Laplaci</context>
</contexts>
<marker>Boyd, Xiao, Mutapcic, 2003</marker>
<rawString>Stephen Boyd, Lin Xiao, and Almir Mutapcic. 2003. Subgradient methods. Lecture Notes of EE392o, Stanford University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deng Cai</author>
<author>Xiaofei He</author>
<author>Xiaoyun Wu</author>
<author>Jiawei Han</author>
</authors>
<title>Non-negative matrix factorization on manifold.</title>
<date>2008</date>
<booktitle>In Proceedings of the 8th IEEE International Conference on Data Mining,</booktitle>
<pages>63--72</pages>
<contexts>
<context position="37462" citStr="Cai et al., 2008" startWordPosition="6306" endWordPosition="6309">, 2011). Yang et al. (2008) and Chen et al. (2013) have also proposed pairwise comparison based methods, for task difficulty estimation and rank aggregation in crowdsourcing settings. Our work differs from previous pairwise comparison based methods in that it further utilizes textual information, formalized as a manifold regularizer. Manifold regularization is a geometrically motivated framework for machine learning, enforcing the learning model to be smooth w.r.t. the geometrical structure of data (Belkin et al., 2006). Within the framework, dimensionality reduction (Belkin and Niyogi, 2001; Cai et al., 2008) and semisupervised learning (Zhou et al., 2004; Zhu and Lafferty, 2005) algorithms have been constructed. In dimensionality reduction, manifold regularization is utilized to guarantee that nearby points will have similar low-dimensional representations (Cai et al., 2008), while in semi-supervised learning it is utilized to ensure that nearby points will have similar labels (Zhou et al., 2004). In our work, we assume that nearby questions (in terms of textual similarities) will have similar difficulty levels. Predicting reading difficulty levels of text is also a relevant problem (Collins-Thom</context>
</contexts>
<marker>Cai, He, Wu, Han, 2008</marker>
<rawString>Deng Cai, Xiaofei He, Xiaoyun Wu, and Jiawei Han. 2008. Non-negative matrix factorization on manifold. In Proceedings of the 8th IEEE International Conference on Data Mining, pages 63–72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Carmel</author>
<author>Elad Yom-Tov</author>
<author>Adam Darlow</author>
<author>Dan Pelleg</author>
</authors>
<title>What makes a query difficult?</title>
<date>2006</date>
<booktitle>In Proceedings of the 29th International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>390--397</pages>
<contexts>
<context position="38619" citStr="Carmel et al., 2006" startWordPosition="6484" endWordPosition="6487">iculty levels of text is also a relevant problem (Collins-Thompson and Callan, 2004; Schwarm and Ostendorf, 2005). It is a key to automatically finding materials at appreciate reading levels for students, and also helps in personalized web search (Collins-Thompson et al., 2011). In the task of predicting reading difficulty levels, documents targeting different grade levels are taken as ground truth, which can be easily obtained from the web. However, there is no naturally annotated data for our QDE task on the web. Other related problems include query difficulty estimation for search engines (Carmel et al., 2006; Yom-Tov et al., 2005) and question difficulty estimation for automatic question answering systems (Lange et al., 2004). In these tasks, query/question difficulty is system-oriented and irrelevant with human knowledge, which is a different setting from ours. 7 Conclusion and Future Work In this paper, we have proposed a novel method for estimating question difficulty levels in CQA services, called Regularized Competition Model (RCM). It takes fully advantage of questions’ textual descriptions besides question-user comparisons, and thus can effectively deal with data sparsity and perform more </context>
</contexts>
<marker>Carmel, Yom-Tov, Darlow, Pelleg, 2006</marker>
<rawString>David Carmel, Elad Yom-Tov, Adam Darlow, and Dan Pelleg. 2006. What makes a query difficult? In Proceedings of the 29th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 390–397.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olivier Chapelle</author>
</authors>
<title>Training a support vector machine in the primal.</title>
<date>2007</date>
<journal>Neural Computation,</journal>
<volume>19</volume>
<issue>5</issue>
<contexts>
<context position="17775" citStr="Chapelle, 2007" startWordPosition="2893" endWordPosition="2894">on, which coincides with the actual outcome. Then the loss will be zero. Otherwise, there is a higher chance that competitor j loses the competition, which goes against the actual outcome. Then the loss will be greater than zero. The smaller the gap is, the higher the chance of inconsistency becomes, and the greater the loss will be. Note that the threshold δ can take any positive value since we do not pose a norm constraint on ¯θ.5 Without loss of generality we take δ = 1 throughout this paper. As we will show in Section 3.2, the loss defined in Eq. (2) has some similarity with the SVM loss (Chapelle, 2007). We name it hinge loss when p = 1, and quadratic loss when p = 2. Given the competition set C, estimating skill levels of (pseudo) users then amounts to solving the following optimization problem: λ1¯T- 3 ℓ (¯θi, ¯θj) + 2 θ e, ( ) where the first term is the empirical loss measuring the total inconsistency; the second term is a regularizer to prevent overfitting; and λ1 ≥ 0 is a trade-off coefficient. It is also a competition-based QDE method, called Competition Model (CM). Exploiting Question Descriptions. Manifold regularization is a well-known technique used in manifold learning algorithms</context>
</contexts>
<marker>Chapelle, 2007</marker>
<rawString>Olivier Chapelle. 2007. Training a support vector machine in the primal. Neural Computation, 19(5):1155–1178.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xi Chen</author>
<author>Paul N Bennett</author>
<author>Kevyn Collins-Thompson</author>
<author>Eric Horvitz</author>
</authors>
<title>Pairwise ranking aggregation in a crowdsourced setting.</title>
<date>2013</date>
<booktitle>In Proceedings of the 6th ACM International Conference on Web Search and Data Mining,</booktitle>
<pages>193--202</pages>
<contexts>
<context position="36895" citStr="Chen et al. (2013)" startWordPosition="6223" endWordPosition="6226">ccuracy 0.8 0.7 0.6 0.5 6 7 8 9 10 11 12 13 14 15 PR TS CM(H) CM(Q) RCM(H) RCM(Q) K Occurrence frequency 0.8 0.4 1.2 0 3 3.5 4 4.5 5 5.5 6 6.5 7 array string virtual multithread Question buckets . # all questions in bucket i 1123 Figure 7: Frequencies of different words in the buckets on SO/Math. The most relevant work to ours is a pairwise comparison based approach proposed by Liu et al. (2013) to estimate question difficulty levels in CQA services. They have also demonstrated that a similar approach can be utilized to estimate user expertise levels (Liu et al., 2011). Yang et al. (2008) and Chen et al. (2013) have also proposed pairwise comparison based methods, for task difficulty estimation and rank aggregation in crowdsourcing settings. Our work differs from previous pairwise comparison based methods in that it further utilizes textual information, formalized as a manifold regularizer. Manifold regularization is a geometrically motivated framework for machine learning, enforcing the learning model to be smooth w.r.t. the geometrical structure of data (Belkin et al., 2006). Within the framework, dimensionality reduction (Belkin and Niyogi, 2001; Cai et al., 2008) and semisupervised learning (Zho</context>
</contexts>
<marker>Chen, Bennett, Collins-Thompson, Horvitz, 2013</marker>
<rawString>Xi Chen, Paul N. Bennett, Kevyn Collins-Thompson, and Eric Horvitz. 2013. Pairwise ranking aggregation in a crowdsourced setting. In Proceedings of the 6th ACM International Conference on Web Search and Data Mining, pages 193–202.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chung</author>
</authors>
<date>1997</date>
<journal>Spectral Graph Theory,</journal>
<volume>92</volume>
<contexts>
<context position="19154" citStr="Chung, 1997" startWordPosition="3136" endWordPosition="3137"> “invariance”, i.e., nearby questions (in terms of textual similarities) are likely to have similar difficulty scores. Hence, we characterize the assumption with the following manifold regularizer: where wij is the textual similarity between question i and question j; W ∈ RN×N is the similarity matrix with the (i, j)-th entry being wij; D ∈ RN×N is a diagonal matrix with the i-th entry on the diagonal being dii = ∑Nj=1 wij; and L = D − W ∈ RN×N 5Given any ¯θi, ¯θj, and δ, there always exists a linear trans( )) formation which keeps the sign of δ − (¯θj − ¯θi unchanged. is the graph Laplacian (Chung, 1997). Minimizing R results in the smoothness assumption: for any questions i and j, if their textual similarity wij is high, the difficulty gap (¯θiq) − ¯θ(q))2 will be small. j A Hybrid Method. Combining Eq. (3) and Eq. (4), we obtain RCM, which amounts to the following optimization problem: ∑min λ1¯T ¯ λ2 ¯T θ¯ (i≺j)∈C ℓ(¯θi, ¯θj)+ 2 θ θ+ 2 θq L¯θq. (5) Here λ2 ≥ 0 is also a trade-off coefficient. The advantages of RCM include 1) It naturally formalizes QDE as minimizing a manifold regularized loss function, which seamlessly integrates both the pairwise competitions and the textual descriptions.</context>
</contexts>
<marker>Chung, 1997</marker>
<rawString>Fan RK. Chung. 1997. Spectral Graph Theory, volume 92.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Church</author>
</authors>
<title>How many multiword expressions do people know.</title>
<date>2011</date>
<booktitle>In Proceedings of the ACLHLT Workshop on Multiword Expressions: from Parsing and Generation to the Real World,</booktitle>
<pages>137--144</pages>
<contexts>
<context position="3174" citStr="Church, 2011" startWordPosition="458" endWordPosition="459">r enterprise question answering and MOOCs question answering, where human resources are expensive. 2) Incentive mechanism design. Nam et al. (2009) have found that winning point awards offered by reputation systems is a driving factor for user participation in CQA services. Assigning higher point awards to more difficult questions will significantly improve user participation and satisfaction. 3) Linguistics analysis. Researchers in computational linguistics are always interested in investigating the correlation between language and knowledge, to see how the language reflects one’s knowledge (Church, 2011). As we will show in Section 5.4, QDE provides an automatic way to quantitatively measure the knowledge levels of words. Liu et al. (2013) have done the pioneer work on QDE, by leveraging question-user comparisons extracted from the question answering threads. Specifically, they assumed that the difficulty level of a question is higher than the expertise level of the asker (i.e. the user who asked the question), but lower than that of the best answerer (i.e. the user who provided the best answer). A TrueSkill al2http://stackoverflow.com/ 3http://coursera.org/ 1115 Proceedings of the 2014 Confe</context>
</contexts>
<marker>Church, 2011</marker>
<rawString>Kenneth Church. 2011. How many multiword expressions do people know. In Proceedings of the ACLHLT Workshop on Multiword Expressions: from Parsing and Generation to the Real World, pages 137–144.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Claypool</author>
<author>Anuja Gokhale</author>
<author>Tim Miranda</author>
<author>Pavel Murnikov</author>
<author>Dmitry Netes</author>
<author>Matthew Sartin</author>
</authors>
<title>Combining content-based and collaborative filters in an online newspaper.</title>
<date>1999</date>
<booktitle>In Proceedings of the ACM SIGIR workshop on Recommender Systems.</booktitle>
<contexts>
<context position="13730" citStr="Claypool et al., 1999" startWordPosition="2197" endWordPosition="2200">irwise comparison assumption (Assumption 1), each question participates in only two competitions, one with the asker and the other with the best answerer. Hence, there might be no enough information to accurately infer its difficulty score. We call this the data sparseness problem. 1117 (a) Low difficulty. (b) Medium difficulty. (c) High difficulty. Figure 1: Tag clouds of SO/Math questions with different difficulty levels. Taking advantage of additional metadata has been demonstrated to be an effective way of dealing with data sparsity in various applications such as collaborative filtering (Claypool et al., 1999; Schein et al., 2002) and personalized search (Dou et al., 2007; Sugiyama et al., 2004). The rationale behind is to bridge the gap among users/items by leveraging their similarities based on the metadata. As for QDE, preliminary observations have shown that a question’s difficulty level can be indicated by its textual description (Liu et al., 2013). As an example, consider the QA threads in the “mathematics” category of Stack Overflow. Divide the questions into three groups: 1) low difficulty, 2) medium difficulty, and 3) high difficulty, according to their difficulty scores estimated by True</context>
</contexts>
<marker>Claypool, Gokhale, Miranda, Murnikov, Netes, Sartin, 1999</marker>
<rawString>Mark Claypool, Anuja Gokhale, Tim Miranda, Pavel Murnikov, Dmitry Netes, and Matthew Sartin. 1999. Combining content-based and collaborative filters in an online newspaper. In Proceedings of the ACM SIGIR workshop on Recommender Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Cohen</author>
</authors>
<title>A coefficient of agreement for nominal scales.</title>
<date>1960</date>
<booktitle>Educational and Psychological Measurement,</booktitle>
<pages>20--1</pages>
<contexts>
<context position="25884" citStr="Cohen, 1960" startWordPosition="4361" endWordPosition="4362">ta set, and asked annotators to compare the difficulty levels of the questions in each pair. We had two graduate students majoring in computer science annotate the SO/CPP questions, and two majoring in mathematics annotate the SO/Math questions. For each question, only the title, body, and tags were exposed to the annotators. Given a question pair (q1, q2), the annotators were asked to give one of the three labels: q1 &gt; q2, q2 &gt; q1, or q1 = q2, which respectively means that question q1 has a higher, lower, or equal difficulty level compared with question q2. We used Cohen’s kappa coefficient (Cohen, 1960) to measure the inter-annotator agreement. The result is K = 0.7533 on SO/CPP and K = 0.8017 on SO/Math, indicating that the inter-annotator agreement is quite substantial on both data sets. After removing the question pairs with inconsistent labels, we got 521 annotated SO/CPP question pairs and 539 annotated SO/Math question pairs. We further randomly split the annotated question pairs into development/test/cold-start sets, with the ratio of 2:2:1. The first two sets were used to evaluate the methods in estimating difficulty scores of resolved questions. Specifically, the development set was</context>
</contexts>
<marker>Cohen, 1960</marker>
<rawString>Jacob Cohen. 1960. A coefficient of agreement for nominal scales. Educational and Psychological Measurement, 20(1):37–46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevyn Collins-Thompson</author>
<author>James P Callan</author>
</authors>
<title>A language modeling approach to predicting reading difficulty.</title>
<date>2004</date>
<booktitle>In Proceedings of the 2004 Conference of</booktitle>
<contexts>
<context position="38083" citStr="Collins-Thompson and Callan, 2004" startWordPosition="6397" endWordPosition="6400">t al., 2008) and semisupervised learning (Zhou et al., 2004; Zhu and Lafferty, 2005) algorithms have been constructed. In dimensionality reduction, manifold regularization is utilized to guarantee that nearby points will have similar low-dimensional representations (Cai et al., 2008), while in semi-supervised learning it is utilized to ensure that nearby points will have similar labels (Zhou et al., 2004). In our work, we assume that nearby questions (in terms of textual similarities) will have similar difficulty levels. Predicting reading difficulty levels of text is also a relevant problem (Collins-Thompson and Callan, 2004; Schwarm and Ostendorf, 2005). It is a key to automatically finding materials at appreciate reading levels for students, and also helps in personalized web search (Collins-Thompson et al., 2011). In the task of predicting reading difficulty levels, documents targeting different grade levels are taken as ground truth, which can be easily obtained from the web. However, there is no naturally annotated data for our QDE task on the web. Other related problems include query difficulty estimation for search engines (Carmel et al., 2006; Yom-Tov et al., 2005) and question difficulty estimation for a</context>
</contexts>
<marker>Collins-Thompson, Callan, 2004</marker>
<rawString>Kevyn Collins-Thompson and James P. Callan. 2004. A language modeling approach to predicting reading difficulty. In Proceedings of the 2004 Conference of</rawString>
</citation>
<citation valid="false">
<authors>
<author>the North</author>
</authors>
<title>American Chapter of the Association for Computational Linguistics: Human Language Technologies,</title>
<pages>193--200</pages>
<marker>North, </marker>
<rawString>the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 193–200.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevyn Collins-Thompson</author>
<author>Paul N Bennett</author>
<author>Ryen W White</author>
<author>Sebastian de la Chica</author>
<author>David Sontag</author>
</authors>
<title>Personalizing web search results by reading level.</title>
<date>2011</date>
<booktitle>In Proceedings of the 20th ACM International Conference on Information and Knowledge Management,</booktitle>
<pages>403--412</pages>
<contexts>
<context position="38278" citStr="Collins-Thompson et al., 2011" startWordPosition="6427" endWordPosition="6430">hat nearby points will have similar low-dimensional representations (Cai et al., 2008), while in semi-supervised learning it is utilized to ensure that nearby points will have similar labels (Zhou et al., 2004). In our work, we assume that nearby questions (in terms of textual similarities) will have similar difficulty levels. Predicting reading difficulty levels of text is also a relevant problem (Collins-Thompson and Callan, 2004; Schwarm and Ostendorf, 2005). It is a key to automatically finding materials at appreciate reading levels for students, and also helps in personalized web search (Collins-Thompson et al., 2011). In the task of predicting reading difficulty levels, documents targeting different grade levels are taken as ground truth, which can be easily obtained from the web. However, there is no naturally annotated data for our QDE task on the web. Other related problems include query difficulty estimation for search engines (Carmel et al., 2006; Yom-Tov et al., 2005) and question difficulty estimation for automatic question answering systems (Lange et al., 2004). In these tasks, query/question difficulty is system-oriented and irrelevant with human knowledge, which is a different setting from ours.</context>
</contexts>
<marker>Collins-Thompson, Bennett, White, Chica, Sontag, 2011</marker>
<rawString>Kevyn Collins-Thompson, Paul N Bennett, Ryen W White, Sebastian de la Chica, and David Sontag. 2011. Personalizing web search results by reading level. In Proceedings of the 20th ACM International Conference on Information and Knowledge Management, pages 403–412.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Cover</author>
<author>Peter Hart</author>
</authors>
<title>Nearest neighbor pattern classification.</title>
<date>1967</date>
<journal>IEEE Transactions on Information Theory,</journal>
<volume>13</volume>
<issue>1</issue>
<contexts>
<context position="6259" citStr="Cover and Hart, 1967" startWordPosition="940" endWordPosition="943">n. Manifold regularization is a wellknown technique to preserve local invariance in manifold learning algorithms, i.e., nearby points are likely to have similar embeddings (Belkin and Niyogi, 2001). Then, we propose a novel Regularized Competition Model (RCM), which formalizes QDE as minimizing a loss on questionuser comparisons with manifold regularization on questions’ textual descriptions. As the smoothness assumption offers extra information for inferring question difficulty levels, incorporating it will effectively deal with data sparsity. Finally, we adopt a K-Nearest Neighbor approach (Cover and Hart, 1967) to perform cold-start estimation, again by leveraging the smoothness assumption. Experiments on two publicly available data sets collected from Stack Overflow show that 1) RCM performs significantly better than existing methods in the QDE task for both well-resolved and cold-start questions. 2) The performance of RCM is insensitive to the particular choice of the term weighting schema (determines how a question’s textual description is represented) and the similarity measure (determines how the textual similarity between two questions is measured). The results demonstrate the advantage of inc</context>
<context position="23205" citStr="Cover and Hart, 1967" startWordPosition="3931" endWordPosition="3934">we have assumed that M ≪ ηN2 and N ≪ ηN2. 6Owing to the sparse nature of questions’ textual descriptions, the graph Laplacian L is usually sparse, with about 70% entries being zero according to our experiments. 4 Cold-Start Estimation Previous sections discussed estimating difficulty scores of resolved questions, from which pairwise competitions could be extracted. However, for newly posted questions without any answers received, no competitions could be extracted and none of the above methods work. We call it the cold-start problem. We heuristically apply a K-Nearest Neighbor (KNN) approach (Cover and Hart, 1967) to coldstart estimation, again by leveraging the smoothness assumption. The key idea is to propagate difficulty scores from well-resolved questions to cold-start ones according to their textual similarities. Specifically, suppose that there exists a set of well-resolved questions whose difficulty scores have already been estimated by a QDE method. Given a cold-start question q∗, we first pick K well-resolved questions that are closest to q∗ in textual descriptions, referred to as the nearest neighbors. The difficulty score of question q∗ is then predicted as the averaged difficulty scores of </context>
</contexts>
<marker>Cover, Hart, 1967</marker>
<rawString>Thomas Cover and Peter Hart. 1967. Nearest neighbor pattern classification. IEEE Transactions on Information Theory, 13(1):21–27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhicheng Dou</author>
<author>Ruihua Song</author>
<author>Ji Rong Wen</author>
</authors>
<title>A large-scale evaluation and analysis of personalized search strategies.</title>
<date>2007</date>
<booktitle>In Proceedings of the 16th International Conference on World Wide Web,</booktitle>
<pages>581--590</pages>
<contexts>
<context position="13794" citStr="Dou et al., 2007" startWordPosition="2208" endWordPosition="2211">es in only two competitions, one with the asker and the other with the best answerer. Hence, there might be no enough information to accurately infer its difficulty score. We call this the data sparseness problem. 1117 (a) Low difficulty. (b) Medium difficulty. (c) High difficulty. Figure 1: Tag clouds of SO/Math questions with different difficulty levels. Taking advantage of additional metadata has been demonstrated to be an effective way of dealing with data sparsity in various applications such as collaborative filtering (Claypool et al., 1999; Schein et al., 2002) and personalized search (Dou et al., 2007; Sugiyama et al., 2004). The rationale behind is to bridge the gap among users/items by leveraging their similarities based on the metadata. As for QDE, preliminary observations have shown that a question’s difficulty level can be indicated by its textual description (Liu et al., 2013). As an example, consider the QA threads in the “mathematics” category of Stack Overflow. Divide the questions into three groups: 1) low difficulty, 2) medium difficulty, and 3) high difficulty, according to their difficulty scores estimated by TrueSkill. Figure 1 visualizes the frequency distribution of tags in</context>
</contexts>
<marker>Dou, Song, Wen, 2007</marker>
<rawString>Zhicheng Dou, Ruihua Song, and Ji Rong Wen. 2007. A large-scale evaluation and analysis of personalized search strategies. In Proceedings of the 16th International Conference on World Wide Web, pages 581–590.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralf Herbrich</author>
<author>Tom Minka</author>
<author>Thore Graepel</author>
</authors>
<title>Trueskill: a bayesian skill rating system.</title>
<date>2006</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>569--576</pages>
<contexts>
<context position="3972" citStr="Herbrich et al., 2006" startWordPosition="579" endWordPosition="582"> leveraging question-user comparisons extracted from the question answering threads. Specifically, they assumed that the difficulty level of a question is higher than the expertise level of the asker (i.e. the user who asked the question), but lower than that of the best answerer (i.e. the user who provided the best answer). A TrueSkill al2http://stackoverflow.com/ 3http://coursera.org/ 1115 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1115–1126, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics gorithm (Herbrich et al., 2006) was further adopted to estimate question difficulty levels as well as user expertise levels from the pairwise comparisons among them. To our knowledge, it is the only existing work on QDE. Yang et al. (2008) have proposed a similar idea, but their work focuses on a different task, i.e., estimating difficulty levels of tasks in crowdsourcing contest services. There are two major drawbacks of previous methods: 1) data sparseness problem and 2) coldstart problem. By the former, we mean that under the framework of previous work, each question is compared only twice with the users (once with the a</context>
<context position="12064" citStr="Herbrich et al., 2006" startWordPosition="1935" endWordPosition="1938">t answerer ub and the asker ua, and 4) |O |competitions between the best answerer ub and each of the answerers in O. The question q is the winner of the first competition, and the best answerer ub is the winner of the remaining (|O |+ 2) competitions. These pairwise competitions are denoted by Cq = {ua ≺q, q≺ub, ua ≺ub, uo1 ≺ub, · · · , uoM ≺ub } , where i ≺ j means that competitor j beats competitor i in a competition. Let UC = Cq (1) q∈Q be the set containing all the pairwise competitions extracted from T. Given the competition set C, Liu et al. (2013) further adopted a TrueSkill algorithm (Herbrich et al., 2006) to learn the competitors’ skill levels (i.e. the question difficulty scores and the user expertise scores). TrueSkill assumes that the practical skill level of each competitor follows a (normal distribution N µ, σ2), where µ is the average skill level and σ is the estimation uncertainty. Then it updates the estimations in an online mode: for a newly observed competition with its win-loss result, 1) increase the average skill level of the winner, 2) decrease the average skill level of the loser, and 3) shrink the uncertainties of both competitors as more data has been observed. Yang et al. (20</context>
</contexts>
<marker>Herbrich, Minka, Graepel, 2006</marker>
<rawString>Ralf Herbrich, Tom Minka, and Thore Graepel. 2006. Trueskill: a bayesian skill rating system. In Advances in Neural Information Processing Systems, pages 569–576.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anna Huang</author>
</authors>
<title>Similarity measures for text document clustering.</title>
<date>2008</date>
<booktitle>In Proceedings of the 6th New Zealand Computer Science Research Student Conference,</booktitle>
<pages>49--56</pages>
<contexts>
<context position="30545" citStr="Huang, 2008" startWordPosition="5155" endWordPosition="5156">Q| |{q∈Q:w∈q}| TFIDF-3 v(w, q) = TF-3 × log |Q| |{q∈Q:w∈q}| dT Cosine Sim (d1, d2) = ∥d1∥×∥d2∥ ∈ [0, 1] 1 d2 dT Jaccard Sim (d1, d2) = ∥d1∥2+∥d2∥2−∥d1∥×∥d2∥ ∈ [0, 1] 1 d2 Table 3: Different term weighting schemas and similarity measures. Laplacian. The term weighting schema determines how a question’s textual description is represented. We explored a Boolean schema, three TF schemas, and three TFIDF schemas (Salton and Buckley, 1988). The similarity measure determines how the textual similarity between two questions is calculated. We explored the Cosine similarity and the Jaccard coefficient (Huang, 2008). Detailed descriptions are given in Table 3. Figure 2 and Figure 3 show the estimation accuracies of the RCM variants on the test sets of SO/CPP and SO/Math respectively, again obtained with the best parameter settings determined by the development sets. The performance of CM is also given (the straight lines in the figures).9 From the results, we can see that 1) All the RCM variants can improve over CM on both data sets, and most of the improvements are significant (ttest, p-value &lt; 0.05). This further demonstrates that the effectiveness of incorporating textual descriptions is not affected </context>
</contexts>
<marker>Huang, 2008</marker>
<rawString>Anna Huang. 2008. Similarity measures for text document clustering. In Proceedings of the 6th New Zealand Computer Science Research Student Conference, pages 49–56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rense Lange</author>
<author>Juan Moran</author>
<author>Warren R Greiff</author>
<author>Lisa Ferro</author>
</authors>
<title>A probabilistic rasch analysis of question answering evaluations.</title>
<date>2004</date>
<booktitle>In Proceedings of the 2004 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>65--72</pages>
<contexts>
<context position="38739" citStr="Lange et al., 2004" startWordPosition="6503" endWordPosition="6506"> a key to automatically finding materials at appreciate reading levels for students, and also helps in personalized web search (Collins-Thompson et al., 2011). In the task of predicting reading difficulty levels, documents targeting different grade levels are taken as ground truth, which can be easily obtained from the web. However, there is no naturally annotated data for our QDE task on the web. Other related problems include query difficulty estimation for search engines (Carmel et al., 2006; Yom-Tov et al., 2005) and question difficulty estimation for automatic question answering systems (Lange et al., 2004). In these tasks, query/question difficulty is system-oriented and irrelevant with human knowledge, which is a different setting from ours. 7 Conclusion and Future Work In this paper, we have proposed a novel method for estimating question difficulty levels in CQA services, called Regularized Competition Model (RCM). It takes fully advantage of questions’ textual descriptions besides question-user comparisons, and thus can effectively deal with data sparsity and perform more accurate estimation. A K-Nearest Neighbor approach is further adopted to estimate difficulty levels of cold-start questi</context>
</contexts>
<marker>Lange, Moran, Greiff, Ferro, 2004</marker>
<rawString>Rense Lange, Juan Moran, Warren R. Greiff, and Lisa Ferro. 2004. A probabilistic rasch analysis of question answering evaluations. In Proceedings of the 2004 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 65–72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Baichuan Li</author>
<author>Irwin King</author>
</authors>
<title>Routing questions to appropriate answerers in community question answering services.</title>
<date>2010</date>
<booktitle>In Proceedings of the 19th ACM International Conference on Information and Knowledge Management,</booktitle>
<pages>1585--1588</pages>
<contexts>
<context position="2316" citStr="Li and King, 2010" startWordPosition="329" endWordPosition="333">estion answering (CQA) services. They have been widely used in various scenarios, including general information seeking on the web1, knowl1http://answers.yahoo.com/ edge exchange in professional communities2, and question answering in massive open online courses (MOOCs)3, to name a few. An important research problem in CQA is how to automatically estimate the difficulty levels of questions, i.e., question di�culty estimation (QDE). QDE can benefit many applications. Examples include 1) Question routing. Routing questions to appropriate answerers can help obtain quick and high-quality answers (Li and King, 2010; Zhou et al., 2009). Ackerman and McDonald (1996) have demonstrated that routing questions by matching question difficulty level with answerer expertise level will make better use of answerers’ time and expertise. This is even more important for enterprise question answering and MOOCs question answering, where human resources are expensive. 2) Incentive mechanism design. Nam et al. (2009) have found that winning point awards offered by reputation systems is a driving factor for user participation in CQA services. Assigning higher point awards to more difficult questions will significantly imp</context>
</contexts>
<marker>Li, King, 2010</marker>
<rawString>Baichuan Li and Irwin King. 2010. Routing questions to appropriate answerers in community question answering services. In Proceedings of the 19th ACM International Conference on Information and Knowledge Management, pages 1585–1588.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jing Liu</author>
<author>Young-In Song</author>
<author>Chin-Yew Lin</author>
</authors>
<title>Competition-based user expertise score estimation.</title>
<date>2011</date>
<booktitle>In Proceedings of the 34th International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>425--434</pages>
<contexts>
<context position="36852" citStr="Liu et al., 2011" startWordPosition="6214" endWordPosition="6217"> 19 20 PR TS CM(H) CM(Q) RCM(H) RCM(Q) K Accuracy 0.8 0.7 0.6 0.5 6 7 8 9 10 11 12 13 14 15 PR TS CM(H) CM(Q) RCM(H) RCM(Q) K Occurrence frequency 0.8 0.4 1.2 0 3 3.5 4 4.5 5 5.5 6 6.5 7 array string virtual multithread Question buckets . # all questions in bucket i 1123 Figure 7: Frequencies of different words in the buckets on SO/Math. The most relevant work to ours is a pairwise comparison based approach proposed by Liu et al. (2013) to estimate question difficulty levels in CQA services. They have also demonstrated that a similar approach can be utilized to estimate user expertise levels (Liu et al., 2011). Yang et al. (2008) and Chen et al. (2013) have also proposed pairwise comparison based methods, for task difficulty estimation and rank aggregation in crowdsourcing settings. Our work differs from previous pairwise comparison based methods in that it further utilizes textual information, formalized as a manifold regularizer. Manifold regularization is a geometrically motivated framework for machine learning, enforcing the learning model to be smooth w.r.t. the geometrical structure of data (Belkin et al., 2006). Within the framework, dimensionality reduction (Belkin and Niyogi, 2001; Cai et </context>
</contexts>
<marker>Liu, Song, Lin, 2011</marker>
<rawString>Jing Liu, Young-In Song, and Chin-Yew Lin. 2011. Competition-based user expertise score estimation. In Proceedings of the 34th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 425–434.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jing Liu</author>
<author>Quan Wang</author>
<author>Chin-Yew Lin</author>
<author>Hsiao-Wuen Hon</author>
</authors>
<title>Question difficulty estimation in community question answering services.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>85--90</pages>
<contexts>
<context position="3312" citStr="Liu et al. (2013)" startWordPosition="481" endWordPosition="484">et al. (2009) have found that winning point awards offered by reputation systems is a driving factor for user participation in CQA services. Assigning higher point awards to more difficult questions will significantly improve user participation and satisfaction. 3) Linguistics analysis. Researchers in computational linguistics are always interested in investigating the correlation between language and knowledge, to see how the language reflects one’s knowledge (Church, 2011). As we will show in Section 5.4, QDE provides an automatic way to quantitatively measure the knowledge levels of words. Liu et al. (2013) have done the pioneer work on QDE, by leveraging question-user comparisons extracted from the question answering threads. Specifically, they assumed that the difficulty level of a question is higher than the expertise level of the asker (i.e. the user who asked the question), but lower than that of the best answerer (i.e. the user who provided the best answer). A TrueSkill al2http://stackoverflow.com/ 3http://coursera.org/ 1115 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1115–1126, October 25-29, 2014, Doha, Qatar. c�2014 Association f</context>
<context position="5353" citStr="Liu et al., 2013" startWordPosition="806" endWordPosition="809"> work only deals with wellresolved questions which have received the best answers, but cannot handle newly posted questions with no answers received. In many real-world applications such as question routing and incentive mechanism design, however, it is usually required that the difficulty level of a question is known instantly after it is posted. To address the drawbacks, we propose further exploiting questions’ textual descriptions (e.g., title, body, and tags) to perform QDE. Preliminary observations have shown that a question’s difficulty level can be indicated by its textual description (Liu et al., 2013). We take advantage of the observations, and assume that if two questions are close in their textual descriptions, they will also be close in their difficulty levels, i.e., the smoothness assumption. We employ manifold regularization (Belkin et al., 2006) to characterize the assumption. Manifold regularization is a wellknown technique to preserve local invariance in manifold learning algorithms, i.e., nearby points are likely to have similar embeddings (Belkin and Niyogi, 2001). Then, we propose a novel Regularized Competition Model (RCM), which formalizes QDE as minimizing a loss on questionu</context>
<context position="10260" citStr="Liu et al. (2013)" startWordPosition="1607" endWordPosition="1610"> vector dn, where V is the vocabulary size. The question di�culty estimation (QDE) task aims to automatically learn the question difficulty scores (βn’s) by utilizing the QA threads T = {(q, ua, ub, O) : q ∈ Q} as well as the question descriptions D = {d1, d2, · · · , dN} in the specified category. Note that in Section 2 and Section 3, we consider estimating difficulty scores of resolved questions, i.e., questions with the best answers selected or voted. Estimating difficulty scores of unresolved questions, e.g., newly posted ones, will be discussed in Section 4. 2.2 Competition-based Methods Liu et al. (2013) have proposed a competitionbased method for QDE. The key idea is to 1) extract pairwise competitions from the QA threads and 2) estimate question difficulty scores based on extracted competitions. To extract pairwise competitions, it is assumed that question difficulty scores and user expertise scores are expressed on the same scale. Given a QA thread (q, ua, ub, O), it is further assumed that: Assumption 1(pairwise comparison assumption) The di�culty score of question q is higher than the expertise score of the asker ua, but lower than that of the best answerer ub. Moreover, the expertise sc</context>
<context position="12002" citStr="Liu et al. (2013)" startWordPosition="1926" endWordPosition="1929"> the best answerer ub, 3) one competition between the best answerer ub and the asker ua, and 4) |O |competitions between the best answerer ub and each of the answerers in O. The question q is the winner of the first competition, and the best answerer ub is the winner of the remaining (|O |+ 2) competitions. These pairwise competitions are denoted by Cq = {ua ≺q, q≺ub, ua ≺ub, uo1 ≺ub, · · · , uoM ≺ub } , where i ≺ j means that competitor j beats competitor i in a competition. Let UC = Cq (1) q∈Q be the set containing all the pairwise competitions extracted from T. Given the competition set C, Liu et al. (2013) further adopted a TrueSkill algorithm (Herbrich et al., 2006) to learn the competitors’ skill levels (i.e. the question difficulty scores and the user expertise scores). TrueSkill assumes that the practical skill level of each competitor follows a (normal distribution N µ, σ2), where µ is the average skill level and σ is the estimation uncertainty. Then it updates the estimations in an online mode: for a newly observed competition with its win-loss result, 1) increase the average skill level of the winner, 2) decrease the average skill level of the loser, and 3) shrink the uncertainties of bo</context>
<context position="14081" citStr="Liu et al., 2013" startWordPosition="2256" endWordPosition="2259">ure 1: Tag clouds of SO/Math questions with different difficulty levels. Taking advantage of additional metadata has been demonstrated to be an effective way of dealing with data sparsity in various applications such as collaborative filtering (Claypool et al., 1999; Schein et al., 2002) and personalized search (Dou et al., 2007; Sugiyama et al., 2004). The rationale behind is to bridge the gap among users/items by leveraging their similarities based on the metadata. As for QDE, preliminary observations have shown that a question’s difficulty level can be indicated by its textual description (Liu et al., 2013). As an example, consider the QA threads in the “mathematics” category of Stack Overflow. Divide the questions into three groups: 1) low difficulty, 2) medium difficulty, and 3) high difficulty, according to their difficulty scores estimated by TrueSkill. Figure 1 visualizes the frequency distribution of tags in each group, where the size of each tag is in proportion to its frequency in the group. The results indicate that the tags associated with the questions do have the ability to reflect the questions’ difficulty levels, e.g., low difficulty questions usually have tags such as “homework” a</context>
<context position="27286" citStr="Liu et al. (2013)" startWordPosition="4602" endWordPosition="4605">from the learning process of RCM as well as any baseline method. Baseline Methods. We considered three baseline methods: PageRank (PR), TrueSkill (TS), and CM, which are based solely on the pairwise competitions. • PR first constructs a competitor graph, by creating an edge from competitor i to competitor j if j beats i in a competition. A PageRank algorithm (Page et al., 1999) is then utilized to estimate the relative importance of the nodes, i.e., question difficulty scores and user expertise scores. The damping factor was set from 0.1 to 0.9 in steps of 0.1. • TS has been applied to QDE by Liu et al. (2013). We set the model parameters in the same way as they suggested. • CM performs QDE by solving Eq. (3). We set A1 in {0, 0.01, 0.02,0.05,0.1,0.2,0.5, 1}. We compared RCM with the above baseline methods. In RCM, both parameters A1 and A2 were set in {0, 0.01, 0.02,0.05,0.1,0.2,0.5, 1}. Evaluation Metric. We employed accuracy (ACC) as the evaluation metric: # correctly judged question pairs ACC = A question pair is regarded as correctly judged if the relative difficulty ranking given by an estimation method is consistent with that given by the annotators. The higher the accuracy is, the better a </context>
<context position="36154" citStr="Liu et al., 2013" startWordPosition="6077" endWordPosition="6080"> al., 2008; Whitehill et al., 2009). Studies on this problem fall into two categories: 1) binary response based and 2) partially ordered response based. In the first category, binary responses (i.e. whether the solution provided by a user is correct or not) are observed, and techniques based on item response theory are further employed (Whitehill et al., 2009; Welinder et al., 2010; Zhou et al., 2012). In the second category, partially ordered responses (i.e. which of the two given solutions is better) are observed, and pairwise comparison based methods are further adopted (Yang et al., 2008; Liu et al., 2013). QDE belongs to the latter. Accuracy 0.75 0.65 0.55 0.45 11 12 13 14 15 16 17 18 19 20 PR TS CM(H) CM(Q) RCM(H) RCM(Q) K Accuracy 0.8 0.7 0.6 0.5 6 7 8 9 10 11 12 13 14 15 PR TS CM(H) CM(Q) RCM(H) RCM(Q) K Occurrence frequency 0.8 0.4 1.2 0 3 3.5 4 4.5 5 5.5 6 6.5 7 array string virtual multithread Question buckets . # all questions in bucket i 1123 Figure 7: Frequencies of different words in the buckets on SO/Math. The most relevant work to ours is a pairwise comparison based approach proposed by Liu et al. (2013) to estimate question difficulty levels in CQA services. They have also demonst</context>
</contexts>
<marker>Liu, Wang, Lin, Hon, 2013</marker>
<rawString>Jing Liu, Quan Wang, Chin-Yew Lin, and Hsiao-Wuen Hon. 2013. Question difficulty estimation in community question answering services. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 85–90.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Kyung Nam</author>
<author>Mark S Ackerman</author>
<author>Lada A Adamic</author>
</authors>
<title>Questions in, knowledge in?: a study of naver’s question answering community.</title>
<date>2009</date>
<booktitle>In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems,</booktitle>
<pages>779--788</pages>
<contexts>
<context position="2708" citStr="Nam et al. (2009)" startWordPosition="389" endWordPosition="392">, i.e., question di�culty estimation (QDE). QDE can benefit many applications. Examples include 1) Question routing. Routing questions to appropriate answerers can help obtain quick and high-quality answers (Li and King, 2010; Zhou et al., 2009). Ackerman and McDonald (1996) have demonstrated that routing questions by matching question difficulty level with answerer expertise level will make better use of answerers’ time and expertise. This is even more important for enterprise question answering and MOOCs question answering, where human resources are expensive. 2) Incentive mechanism design. Nam et al. (2009) have found that winning point awards offered by reputation systems is a driving factor for user participation in CQA services. Assigning higher point awards to more difficult questions will significantly improve user participation and satisfaction. 3) Linguistics analysis. Researchers in computational linguistics are always interested in investigating the correlation between language and knowledge, to see how the language reflects one’s knowledge (Church, 2011). As we will show in Section 5.4, QDE provides an automatic way to quantitatively measure the knowledge levels of words. Liu et al. (2</context>
</contexts>
<marker>Nam, Ackerman, Adamic, 2009</marker>
<rawString>Kevin Kyung Nam, Mark S. Ackerman, and Lada A. Adamic. 2009. Questions in, knowledge in?: a study of naver’s question answering community. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, pages 779–788.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Larry Page</author>
<author>Sergey Brin</author>
<author>Rajeev Motwani</author>
<author>Terry Winograd</author>
</authors>
<title>The pagerank citation ranking: bringing order to the web.</title>
<date>1999</date>
<tech>Technical Report,</tech>
<institution>Stanford University.</institution>
<contexts>
<context position="12830" citStr="Page et al., 1999" startWordPosition="2062" endWordPosition="2065">ill level of each competitor follows a (normal distribution N µ, σ2), where µ is the average skill level and σ is the estimation uncertainty. Then it updates the estimations in an online mode: for a newly observed competition with its win-loss result, 1) increase the average skill level of the winner, 2) decrease the average skill level of the loser, and 3) shrink the uncertainties of both competitors as more data has been observed. Yang et al. (2008) have proposed a similar competitionbased method to estimate tasks’ difficulty levels in crowdsourcing contest services, by leveraging PageRank (Page et al., 1999) algorithm. 2.3 Motivating Discussions The methods introduced above estimate competitors’ skill levels based solely on the pairwise competitions among them. The more competitions a competitor participates in, the more accurate the estimation will be. However, according to the pairwise comparison assumption (Assumption 1), each question participates in only two competitions, one with the asker and the other with the best answerer. Hence, there might be no enough information to accurately infer its difficulty score. We call this the data sparseness problem. 1117 (a) Low difficulty. (b) Medium di</context>
<context position="27049" citStr="Page et al., 1999" startWordPosition="4556" endWordPosition="4559">lved questions. Specifically, the development set was used for parameter tuning and the test set was used for evaluation. The last set was used to evaluate the methods in cold-start estimation, and the questions in this set were excluded from the learning process of RCM as well as any baseline method. Baseline Methods. We considered three baseline methods: PageRank (PR), TrueSkill (TS), and CM, which are based solely on the pairwise competitions. • PR first constructs a competitor graph, by creating an edge from competitor i to competitor j if j beats i in a competition. A PageRank algorithm (Page et al., 1999) is then utilized to estimate the relative importance of the nodes, i.e., question difficulty scores and user expertise scores. The damping factor was set from 0.1 to 0.9 in steps of 0.1. • TS has been applied to QDE by Liu et al. (2013). We set the model parameters in the same way as they suggested. • CM performs QDE by solving Eq. (3). We set A1 in {0, 0.01, 0.02,0.05,0.1,0.2,0.5, 1}. We compared RCM with the above baseline methods. In RCM, both parameters A1 and A2 were set in {0, 0.01, 0.02,0.05,0.1,0.2,0.5, 1}. Evaluation Metric. We employed accuracy (ACC) as the evaluation metric: # corr</context>
</contexts>
<marker>Page, Brin, Motwani, Winograd, 1999</marker>
<rawString>Larry Page, Sergey Brin, Rajeev Motwani, and Terry Winograd. 1999. The pagerank citation ranking: bringing order to the web. Technical Report, Stanford University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Lee Rodgers</author>
<author>W Alan Nicewander</author>
</authors>
<title>Thirteen ways to look at the correlation coefficient.</title>
<date>1988</date>
<journal>The American Statistician,</journal>
<volume>42</volume>
<issue>1</issue>
<contexts>
<context position="14876" citStr="Rodgers and Nicewander, 1988" startWordPosition="2385" endWordPosition="2388">lty, and 3) high difficulty, according to their difficulty scores estimated by TrueSkill. Figure 1 visualizes the frequency distribution of tags in each group, where the size of each tag is in proportion to its frequency in the group. The results indicate that the tags associated with the questions do have the ability to reflect the questions’ difficulty levels, e.g., low difficulty questions usually have tags such as “homework” and “calculus”, while high difficulty ones usually have tags such as “general topology” and “number theory”. We further calculate the Pearson correlation coefficient (Rodgers and Nicewander, 1988) between 1) the gap between the averaged difficulty scores in each two groups and 2) the Euclidean distance between the aggregated textual descriptions in each two groups . The result is r = 0.6424, implying that the difficulty gap is positively correlated with the textual distance. In other words, the more similar two questions’ textual descriptions are, the more close their difficulty levels are. Therefore, we take the textual information to bridge the difficulty gap among questions, by assuming that Assumption 2 (smoothness assumption) If two questions qi and qj are close in their textual d</context>
</contexts>
<marker>Rodgers, Nicewander, 1988</marker>
<rawString>Joseph Lee Rodgers and W. Alan Nicewander. 1988. Thirteen ways to look at the correlation coefficient. The American Statistician, 42(1):59–66.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerard Salton</author>
<author>Christopher Buckley</author>
</authors>
<title>Termweighting approaches in automatic text retrieval.</title>
<date>1988</date>
<booktitle>Information Processing fr Management,</booktitle>
<volume>24</volume>
<issue>5</issue>
<pages>523</pages>
<contexts>
<context position="30370" citStr="Salton and Buckley, 1988" startWordPosition="5127" endWordPosition="5130">umber of occurrences TF-2 v(w, q) = log (f(w, q) + 1) 0.5 × f(w, q) TF-3 v(w, q) = 0.5 + max { f (w, q) : w ∈ q} TFIDF-1 v(w, q) = TF-1 × log |Q| |{q∈Q:w∈q}| TFIDF-2 v(w, q) = TF-2 × log |Q| |{q∈Q:w∈q}| TFIDF-3 v(w, q) = TF-3 × log |Q| |{q∈Q:w∈q}| dT Cosine Sim (d1, d2) = ∥d1∥×∥d2∥ ∈ [0, 1] 1 d2 dT Jaccard Sim (d1, d2) = ∥d1∥2+∥d2∥2−∥d1∥×∥d2∥ ∈ [0, 1] 1 d2 Table 3: Different term weighting schemas and similarity measures. Laplacian. The term weighting schema determines how a question’s textual description is represented. We explored a Boolean schema, three TF schemas, and three TFIDF schemas (Salton and Buckley, 1988). The similarity measure determines how the textual similarity between two questions is calculated. We explored the Cosine similarity and the Jaccard coefficient (Huang, 2008). Detailed descriptions are given in Table 3. Figure 2 and Figure 3 show the estimation accuracies of the RCM variants on the test sets of SO/CPP and SO/Math respectively, again obtained with the best parameter settings determined by the development sets. The performance of CM is also given (the straight lines in the figures).9 From the results, we can see that 1) All the RCM variants can improve over CM on both data sets</context>
</contexts>
<marker>Salton, Buckley, 1988</marker>
<rawString>Gerard Salton and Christopher Buckley. 1988. Termweighting approaches in automatic text retrieval. Information Processing fr Management, 24(5):513– 523.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew I Schein</author>
<author>Alexandrin Popescul</author>
<author>Lyle H Ungar</author>
<author>David M Pennock</author>
</authors>
<title>Methods and metrics for cold-start recommendations.</title>
<date>2002</date>
<booktitle>In Proceedings of the 25th International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>253--260</pages>
<contexts>
<context position="13752" citStr="Schein et al., 2002" startWordPosition="2201" endWordPosition="2204">ption (Assumption 1), each question participates in only two competitions, one with the asker and the other with the best answerer. Hence, there might be no enough information to accurately infer its difficulty score. We call this the data sparseness problem. 1117 (a) Low difficulty. (b) Medium difficulty. (c) High difficulty. Figure 1: Tag clouds of SO/Math questions with different difficulty levels. Taking advantage of additional metadata has been demonstrated to be an effective way of dealing with data sparsity in various applications such as collaborative filtering (Claypool et al., 1999; Schein et al., 2002) and personalized search (Dou et al., 2007; Sugiyama et al., 2004). The rationale behind is to bridge the gap among users/items by leveraging their similarities based on the metadata. As for QDE, preliminary observations have shown that a question’s difficulty level can be indicated by its textual description (Liu et al., 2013). As an example, consider the QA threads in the “mathematics” category of Stack Overflow. Divide the questions into three groups: 1) low difficulty, 2) medium difficulty, and 3) high difficulty, according to their difficulty scores estimated by TrueSkill. Figure 1 visual</context>
</contexts>
<marker>Schein, Popescul, Ungar, Pennock, 2002</marker>
<rawString>Andrew I. Schein, Alexandrin Popescul, Lyle H. Ungar, and David M. Pennock. 2002. Methods and metrics for cold-start recommendations. In Proceedings of the 25th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 253–260.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sarah E Schwarm</author>
<author>Mari Ostendorf</author>
</authors>
<title>Reading level assessment using support vector machines and statistical language models.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>523--530</pages>
<contexts>
<context position="38113" citStr="Schwarm and Ostendorf, 2005" startWordPosition="6401" endWordPosition="6404">rning (Zhou et al., 2004; Zhu and Lafferty, 2005) algorithms have been constructed. In dimensionality reduction, manifold regularization is utilized to guarantee that nearby points will have similar low-dimensional representations (Cai et al., 2008), while in semi-supervised learning it is utilized to ensure that nearby points will have similar labels (Zhou et al., 2004). In our work, we assume that nearby questions (in terms of textual similarities) will have similar difficulty levels. Predicting reading difficulty levels of text is also a relevant problem (Collins-Thompson and Callan, 2004; Schwarm and Ostendorf, 2005). It is a key to automatically finding materials at appreciate reading levels for students, and also helps in personalized web search (Collins-Thompson et al., 2011). In the task of predicting reading difficulty levels, documents targeting different grade levels are taken as ground truth, which can be easily obtained from the web. However, there is no naturally annotated data for our QDE task on the web. Other related problems include query difficulty estimation for search engines (Carmel et al., 2006; Yom-Tov et al., 2005) and question difficulty estimation for automatic question answering sy</context>
</contexts>
<marker>Schwarm, Ostendorf, 2005</marker>
<rawString>Sarah E. Schwarm and Mari Ostendorf. 2005. Reading level assessment using support vector machines and statistical language models. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 523–530.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kazunari Sugiyama</author>
<author>Kenji Hatano</author>
<author>Masatoshi Yoshikawa</author>
</authors>
<title>Adaptive web search based on user profile constructed without any effort from users.</title>
<date>2004</date>
<booktitle>In Proceedings of the 13th International Conference on World Wide Web,</booktitle>
<pages>675--684</pages>
<contexts>
<context position="13818" citStr="Sugiyama et al., 2004" startWordPosition="2212" endWordPosition="2215">petitions, one with the asker and the other with the best answerer. Hence, there might be no enough information to accurately infer its difficulty score. We call this the data sparseness problem. 1117 (a) Low difficulty. (b) Medium difficulty. (c) High difficulty. Figure 1: Tag clouds of SO/Math questions with different difficulty levels. Taking advantage of additional metadata has been demonstrated to be an effective way of dealing with data sparsity in various applications such as collaborative filtering (Claypool et al., 1999; Schein et al., 2002) and personalized search (Dou et al., 2007; Sugiyama et al., 2004). The rationale behind is to bridge the gap among users/items by leveraging their similarities based on the metadata. As for QDE, preliminary observations have shown that a question’s difficulty level can be indicated by its textual description (Liu et al., 2013). As an example, consider the QA threads in the “mathematics” category of Stack Overflow. Divide the questions into three groups: 1) low difficulty, 2) medium difficulty, and 3) high difficulty, according to their difficulty scores estimated by TrueSkill. Figure 1 visualizes the frequency distribution of tags in each group, where the s</context>
</contexts>
<marker>Sugiyama, Hatano, Yoshikawa, 2004</marker>
<rawString>Kazunari Sugiyama, Kenji Hatano, and Masatoshi Yoshikawa. 2004. Adaptive web search based on user profile constructed without any effort from users. In Proceedings of the 13th International Conference on World Wide Web, pages 675–684.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Welinder</author>
<author>Steve Branson</author>
<author>Serge Belongie</author>
<author>Pietro Perona</author>
</authors>
<title>The multidimensional wisdom of crowds.</title>
<date>2010</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>2424--2432</pages>
<contexts>
<context position="35921" citStr="Welinder et al., 2010" startWordPosition="6037" endWordPosition="6041">s indicate that RCM might provide an automatic way to measure the difficulty levels of words. 6 Related Work QDE is relevant to the problem of estimating task difficulty levels and user expertise levels in crowdsourcing services (Yang et al., 2008; Whitehill et al., 2009). Studies on this problem fall into two categories: 1) binary response based and 2) partially ordered response based. In the first category, binary responses (i.e. whether the solution provided by a user is correct or not) are observed, and techniques based on item response theory are further employed (Whitehill et al., 2009; Welinder et al., 2010; Zhou et al., 2012). In the second category, partially ordered responses (i.e. which of the two given solutions is better) are observed, and pairwise comparison based methods are further adopted (Yang et al., 2008; Liu et al., 2013). QDE belongs to the latter. Accuracy 0.75 0.65 0.55 0.45 11 12 13 14 15 16 17 18 19 20 PR TS CM(H) CM(Q) RCM(H) RCM(Q) K Accuracy 0.8 0.7 0.6 0.5 6 7 8 9 10 11 12 13 14 15 PR TS CM(H) CM(Q) RCM(H) RCM(Q) K Occurrence frequency 0.8 0.4 1.2 0 3 3.5 4 4.5 5 5.5 6 6.5 7 array string virtual multithread Question buckets . # all questions in bucket i 1123 Figure 7: Freq</context>
</contexts>
<marker>Welinder, Branson, Belongie, Perona, 2010</marker>
<rawString>Peter Welinder, Steve Branson, Serge Belongie, and Pietro Perona. 2010. The multidimensional wisdom of crowds. In Advances in Neural Information Processing Systems, pages 2424–2432.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Whitehill</author>
<author>Paul Ruvolo</author>
<author>Tingfan Wu</author>
<author>Jacob Bergsma</author>
<author>Javier R Movellan</author>
</authors>
<title>Whose vote should count more: optimal integration of labels from labelers of unknown expertise.</title>
<date>2009</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>2035--2043</pages>
<contexts>
<context position="35572" citStr="Whitehill et al., 2009" startWordPosition="5979" endWordPosition="5982">equently in questions with lower difficulty levels, “virtual” higher, and “multithread” the highest. It coincides with the intuition: “array” and “string” are usually related to some basic concepts in programming language, while “virtual” and “multithread” usually discuss more advanced topics. Similar phenomena can be observed on SO/Math. The results indicate that RCM might provide an automatic way to measure the difficulty levels of words. 6 Related Work QDE is relevant to the problem of estimating task difficulty levels and user expertise levels in crowdsourcing services (Yang et al., 2008; Whitehill et al., 2009). Studies on this problem fall into two categories: 1) binary response based and 2) partially ordered response based. In the first category, binary responses (i.e. whether the solution provided by a user is correct or not) are observed, and techniques based on item response theory are further employed (Whitehill et al., 2009; Welinder et al., 2010; Zhou et al., 2012). In the second category, partially ordered responses (i.e. which of the two given solutions is better) are observed, and pairwise comparison based methods are further adopted (Yang et al., 2008; Liu et al., 2013). QDE belongs to t</context>
</contexts>
<marker>Whitehill, Ruvolo, Wu, Bergsma, Movellan, 2009</marker>
<rawString>Jacob Whitehill, Paul Ruvolo, Tingfan Wu, Jacob Bergsma, and Javier R Movellan. 2009. Whose vote should count more: optimal integration of labels from labelers of unknown expertise. In Advances in Neural Information Processing Systems, pages 2035–2043.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiang Yang</author>
<author>Lada Adamic</author>
<author>Mark Ackerman</author>
</authors>
<title>Competing to share expertise: the taskcn knowledge sharing community.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2nd International AAAI Conference on Weblogs and Social Media.</booktitle>
<contexts>
<context position="4180" citStr="Yang et al. (2008)" startWordPosition="617" endWordPosition="620">er who asked the question), but lower than that of the best answerer (i.e. the user who provided the best answer). A TrueSkill al2http://stackoverflow.com/ 3http://coursera.org/ 1115 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1115–1126, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics gorithm (Herbrich et al., 2006) was further adopted to estimate question difficulty levels as well as user expertise levels from the pairwise comparisons among them. To our knowledge, it is the only existing work on QDE. Yang et al. (2008) have proposed a similar idea, but their work focuses on a different task, i.e., estimating difficulty levels of tasks in crowdsourcing contest services. There are two major drawbacks of previous methods: 1) data sparseness problem and 2) coldstart problem. By the former, we mean that under the framework of previous work, each question is compared only twice with the users (once with the asker and the other with the best answerer), which might not provide enough information and contaminate the estimation accuracy. By the latter, we mean that previous work only deals with wellresolved questions</context>
<context position="12667" citStr="Yang et al. (2008)" startWordPosition="2039" endWordPosition="2042">h et al., 2006) to learn the competitors’ skill levels (i.e. the question difficulty scores and the user expertise scores). TrueSkill assumes that the practical skill level of each competitor follows a (normal distribution N µ, σ2), where µ is the average skill level and σ is the estimation uncertainty. Then it updates the estimations in an online mode: for a newly observed competition with its win-loss result, 1) increase the average skill level of the winner, 2) decrease the average skill level of the loser, and 3) shrink the uncertainties of both competitors as more data has been observed. Yang et al. (2008) have proposed a similar competitionbased method to estimate tasks’ difficulty levels in crowdsourcing contest services, by leveraging PageRank (Page et al., 1999) algorithm. 2.3 Motivating Discussions The methods introduced above estimate competitors’ skill levels based solely on the pairwise competitions among them. The more competitions a competitor participates in, the more accurate the estimation will be. However, according to the pairwise comparison assumption (Assumption 1), each question participates in only two competitions, one with the asker and the other with the best answerer. Hen</context>
<context position="35547" citStr="Yang et al., 2008" startWordPosition="5975" endWordPosition="5978">ring” occur most frequently in questions with lower difficulty levels, “virtual” higher, and “multithread” the highest. It coincides with the intuition: “array” and “string” are usually related to some basic concepts in programming language, while “virtual” and “multithread” usually discuss more advanced topics. Similar phenomena can be observed on SO/Math. The results indicate that RCM might provide an automatic way to measure the difficulty levels of words. 6 Related Work QDE is relevant to the problem of estimating task difficulty levels and user expertise levels in crowdsourcing services (Yang et al., 2008; Whitehill et al., 2009). Studies on this problem fall into two categories: 1) binary response based and 2) partially ordered response based. In the first category, binary responses (i.e. whether the solution provided by a user is correct or not) are observed, and techniques based on item response theory are further employed (Whitehill et al., 2009; Welinder et al., 2010; Zhou et al., 2012). In the second category, partially ordered responses (i.e. which of the two given solutions is better) are observed, and pairwise comparison based methods are further adopted (Yang et al., 2008; Liu et al.</context>
<context position="36872" citStr="Yang et al. (2008)" startWordPosition="6218" endWordPosition="6221">CM(Q) RCM(H) RCM(Q) K Accuracy 0.8 0.7 0.6 0.5 6 7 8 9 10 11 12 13 14 15 PR TS CM(H) CM(Q) RCM(H) RCM(Q) K Occurrence frequency 0.8 0.4 1.2 0 3 3.5 4 4.5 5 5.5 6 6.5 7 array string virtual multithread Question buckets . # all questions in bucket i 1123 Figure 7: Frequencies of different words in the buckets on SO/Math. The most relevant work to ours is a pairwise comparison based approach proposed by Liu et al. (2013) to estimate question difficulty levels in CQA services. They have also demonstrated that a similar approach can be utilized to estimate user expertise levels (Liu et al., 2011). Yang et al. (2008) and Chen et al. (2013) have also proposed pairwise comparison based methods, for task difficulty estimation and rank aggregation in crowdsourcing settings. Our work differs from previous pairwise comparison based methods in that it further utilizes textual information, formalized as a manifold regularizer. Manifold regularization is a geometrically motivated framework for machine learning, enforcing the learning model to be smooth w.r.t. the geometrical structure of data (Belkin et al., 2006). Within the framework, dimensionality reduction (Belkin and Niyogi, 2001; Cai et al., 2008) and semis</context>
</contexts>
<marker>Yang, Adamic, Ackerman, 2008</marker>
<rawString>Jiang Yang, Lada Adamic, and Mark Ackerman. 2008. Competing to share expertise: the taskcn knowledge sharing community. In Proceedings of the 2nd International AAAI Conference on Weblogs and Social Media.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elad Yom-Tov</author>
<author>Shai Fine</author>
<author>David Carmel</author>
<author>Adam Darlow</author>
</authors>
<title>Learning to estimate query difficulty: including applications to missing content detection and distributed information retrieval.</title>
<date>2005</date>
<booktitle>In Proceedings of the 28th International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>512--519</pages>
<contexts>
<context position="38642" citStr="Yom-Tov et al., 2005" startWordPosition="6488" endWordPosition="6491"> is also a relevant problem (Collins-Thompson and Callan, 2004; Schwarm and Ostendorf, 2005). It is a key to automatically finding materials at appreciate reading levels for students, and also helps in personalized web search (Collins-Thompson et al., 2011). In the task of predicting reading difficulty levels, documents targeting different grade levels are taken as ground truth, which can be easily obtained from the web. However, there is no naturally annotated data for our QDE task on the web. Other related problems include query difficulty estimation for search engines (Carmel et al., 2006; Yom-Tov et al., 2005) and question difficulty estimation for automatic question answering systems (Lange et al., 2004). In these tasks, query/question difficulty is system-oriented and irrelevant with human knowledge, which is a different setting from ours. 7 Conclusion and Future Work In this paper, we have proposed a novel method for estimating question difficulty levels in CQA services, called Regularized Competition Model (RCM). It takes fully advantage of questions’ textual descriptions besides question-user comparisons, and thus can effectively deal with data sparsity and perform more accurate estimation. A </context>
</contexts>
<marker>Yom-Tov, Fine, Carmel, Darlow, 2005</marker>
<rawString>Elad Yom-Tov, Shai Fine, David Carmel, and Adam Darlow. 2005. Learning to estimate query difficulty: including applications to missing content detection and distributed information retrieval. In Proceedings of the 28th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 512–519.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dengyong Zhou</author>
<author>Olivier Bousquet</author>
<author>Thomas Navin Lal</author>
<author>Jason Weston</author>
<author>Bernhard Sch¨olkopf</author>
</authors>
<title>Learning with local and global consistency.</title>
<date>2004</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>321--328</pages>
<marker>Zhou, Bousquet, Lal, Weston, Sch¨olkopf, 2004</marker>
<rawString>Dengyong Zhou, Olivier Bousquet, Thomas Navin Lal, Jason Weston, and Bernhard Sch¨olkopf. 2004. Learning with local and global consistency. In Advances in Neural Information Processing Systems, pages 321–328.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yanhong Zhou</author>
<author>Gao Cong</author>
<author>Bin Cui</author>
<author>Christian S Jensen</author>
<author>Junjie Yao</author>
</authors>
<title>Routing questions to the right users in online communities.</title>
<date>2009</date>
<booktitle>In Proceedings of the 25th IEEE International Conference on Data Engineering,</booktitle>
<pages>700--711</pages>
<contexts>
<context position="2336" citStr="Zhou et al., 2009" startWordPosition="334" endWordPosition="337">QA) services. They have been widely used in various scenarios, including general information seeking on the web1, knowl1http://answers.yahoo.com/ edge exchange in professional communities2, and question answering in massive open online courses (MOOCs)3, to name a few. An important research problem in CQA is how to automatically estimate the difficulty levels of questions, i.e., question di�culty estimation (QDE). QDE can benefit many applications. Examples include 1) Question routing. Routing questions to appropriate answerers can help obtain quick and high-quality answers (Li and King, 2010; Zhou et al., 2009). Ackerman and McDonald (1996) have demonstrated that routing questions by matching question difficulty level with answerer expertise level will make better use of answerers’ time and expertise. This is even more important for enterprise question answering and MOOCs question answering, where human resources are expensive. 2) Incentive mechanism design. Nam et al. (2009) have found that winning point awards offered by reputation systems is a driving factor for user participation in CQA services. Assigning higher point awards to more difficult questions will significantly improve user participat</context>
</contexts>
<marker>Zhou, Cong, Cui, Jensen, Yao, 2009</marker>
<rawString>Yanhong Zhou, Gao Cong, Bin Cui, Christian S. Jensen, and Junjie Yao. 2009. Routing questions to the right users in online communities. In Proceedings of the 25th IEEE International Conference on Data Engineering, pages 700–711.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dengyong Zhou</author>
<author>John C Platt</author>
<author>Sumit Basu</author>
<author>Yi Mao</author>
</authors>
<title>Learning from the wisdom of crowds by minimax entropy.</title>
<date>2012</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>2204--2212</pages>
<contexts>
<context position="35941" citStr="Zhou et al., 2012" startWordPosition="6042" endWordPosition="6045">ht provide an automatic way to measure the difficulty levels of words. 6 Related Work QDE is relevant to the problem of estimating task difficulty levels and user expertise levels in crowdsourcing services (Yang et al., 2008; Whitehill et al., 2009). Studies on this problem fall into two categories: 1) binary response based and 2) partially ordered response based. In the first category, binary responses (i.e. whether the solution provided by a user is correct or not) are observed, and techniques based on item response theory are further employed (Whitehill et al., 2009; Welinder et al., 2010; Zhou et al., 2012). In the second category, partially ordered responses (i.e. which of the two given solutions is better) are observed, and pairwise comparison based methods are further adopted (Yang et al., 2008; Liu et al., 2013). QDE belongs to the latter. Accuracy 0.75 0.65 0.55 0.45 11 12 13 14 15 16 17 18 19 20 PR TS CM(H) CM(Q) RCM(H) RCM(Q) K Accuracy 0.8 0.7 0.6 0.5 6 7 8 9 10 11 12 13 14 15 PR TS CM(H) CM(Q) RCM(H) RCM(Q) K Occurrence frequency 0.8 0.4 1.2 0 3 3.5 4 4.5 5 5.5 6 6.5 7 array string virtual multithread Question buckets . # all questions in bucket i 1123 Figure 7: Frequencies of different</context>
</contexts>
<marker>Zhou, Platt, Basu, Mao, 2012</marker>
<rawString>Dengyong Zhou, John C Platt, Sumit Basu, and Yi Mao. 2012. Learning from the wisdom of crowds by minimax entropy. In Advances in Neural Information Processing Systems, pages 2204–2212.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaojin Zhu</author>
<author>John Lafferty</author>
</authors>
<title>Harmonic mixtures: combining mixture models and graph-based methods for inductive and scalable semi-supervised learning.</title>
<date>2005</date>
<booktitle>In Proceedings of the 22nd International Conference on Machine Learning,</booktitle>
<pages>1052--1059</pages>
<contexts>
<context position="37534" citStr="Zhu and Lafferty, 2005" startWordPosition="6318" endWordPosition="6321">sed pairwise comparison based methods, for task difficulty estimation and rank aggregation in crowdsourcing settings. Our work differs from previous pairwise comparison based methods in that it further utilizes textual information, formalized as a manifold regularizer. Manifold regularization is a geometrically motivated framework for machine learning, enforcing the learning model to be smooth w.r.t. the geometrical structure of data (Belkin et al., 2006). Within the framework, dimensionality reduction (Belkin and Niyogi, 2001; Cai et al., 2008) and semisupervised learning (Zhou et al., 2004; Zhu and Lafferty, 2005) algorithms have been constructed. In dimensionality reduction, manifold regularization is utilized to guarantee that nearby points will have similar low-dimensional representations (Cai et al., 2008), while in semi-supervised learning it is utilized to ensure that nearby points will have similar labels (Zhou et al., 2004). In our work, we assume that nearby questions (in terms of textual similarities) will have similar difficulty levels. Predicting reading difficulty levels of text is also a relevant problem (Collins-Thompson and Callan, 2004; Schwarm and Ostendorf, 2005). It is a key to auto</context>
</contexts>
<marker>Zhu, Lafferty, 2005</marker>
<rawString>Xiaojin Zhu and John Lafferty. 2005. Harmonic mixtures: combining mixture models and graph-based methods for inductive and scalable semi-supervised learning. In Proceedings of the 22nd International Conference on Machine Learning, pages 1052– 1059.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
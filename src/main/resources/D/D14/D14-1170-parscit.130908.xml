<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.998858">
Automatic Generation of Related Work Sections in Scientific Papers:
An Optimization Approach
</title>
<author confidence="0.998772">
Yue Hu and Xiaojun Wan
</author>
<affiliation confidence="0.865004333333333">
Institute of Computer Science and Technology
The MOE Key Laboratory of Computational Linguistics
Peking University, Beijing, China
</affiliation>
<email confidence="0.997585">
{ayue.hu,wanxiaojun}@pku.edu.cn
</email>
<sectionHeader confidence="0.998589" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99975848">
In this paper, we investigate a challeng-
ing task of automatic related work gener-
ation. Given multiple reference papers as
input, the task aims to generate a related
work section for a target paper. The gen-
erated related work section can be used
as a draft for the author to complete his
or her final related work section. We
propose our Automatic Related Work
Generation system called ARWG to ad-
dress this task. It first exploits a PLSA
model to split the sentence set of the giv-
en papers into different topic-biased parts,
and then applies regression models to
learn the importance of the sentences. At
last it employs an optimization frame-
work to generate the related work section.
Our evaluation results on a test set of 150
target papers along with their reference
papers show that our proposed ARWG
system can generate related work sec-
tions with better quality. A user study is
also performed to show ARWG can
achieve an improvement over generic
multi-document summarization baselines.
</bodyText>
<sectionHeader confidence="0.99963" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.996528836734694">
The related work section is an important part of a
paper. An author often needs to help readers to
understand the context of his or her research
problem and compare his or her current work
with previous works. A related work section is
often used for this purpose to show the differ-
ences and advantages of his or her work, com-
pared with related research works. In this study,
we attempt to automatically generate a related
work section for a target academic paper with its
reference papers. This kind of related work sec-
tions can be used as a basis to reduce the authorâ€™s
time and effort when he or she wants to complete
his or her final related work section.
Automatic related work section generation is a
very challenging task. It can be considered a top-
ic-biased, multiple-document summarization
problem. The input is a target academic paper,
which has no related work section, along with its
reference papers. The goal is to create a related
work section that describes the related works and
addresses the relationship between the target pa-
per and the reference papers. Here we assume
that the set of reference papers has been given as
part of the input. Existing works in the NLP and
recommendation systems communities have al-
ready focused on the task of finding reference
papers. For example, citation prediction (Nal-
lapati et al., 2008) aims at finding individual pa-
per citation patterns.
Generally speaking, automatic related work
section generation is a strikingly different prob-
lem and it is much more difficult in comparison
with general multi-document summarization
tasks. For example, multi-document summariza-
tion of news articles aims at synthesizing con-
tents of similar news and removing the redundant
information contained by the different news arti-
cles. However, each scientific paper has much
specific content to state its own work and contri-
bution. Even for the papers that investigate the
same research topic, their contributions and con-
tents can be totally different. The related work
section generation task needs to find the specific
contributions of individual papers and arrange
them into one or several paragraphs.
In this study, we focus on the problem of au-
tomatic related work section generation and pro-
pose a novel system called ARWG to address the
</bodyText>
<page confidence="0.944901">
1624
</page>
<note confidence="0.9004735">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1624â€“1633,
October 25-29, 2014, Doha, Qatar. cï¿½2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999456638888889">
problem. For the target paper, we assume that the
abstract and introduction sections have already
been written by the author and they can be used
to help generate the related work section. For the
reference papers, we only consider and extract
the abstract, introduction, related work and con-
clusion sections, because other sections like the
method and evaluation sections always describe
the extreme details of the specific work and they
are not suitable for this task. Then we generate
the related work section using both sentence sets
which are extracted from the target paper and
reference papers, respectively.
Firstly, we use a PLSA model to group both
sentence sets of the target paper and its reference
papers into different topic-biased clusters. Sec-
ondly, the importance of each sentence in the
target paper and the reference papers is learned
by using two different Support Vector Regres-
sion (SVR) models. At last, a global optimization
framework is proposed to generate the related
work section by selecting sentences from both
the target paper and the reference papers. Mean-
while, the framework selects sentences from dif-
ferent topic-biased clusters globally.
Experimental results on a test set of 150 target
papers show our method can generate related
work sections with better quality than those of
several baseline methods. With the ROUGE
toolkit, the results indicate the related work sec-
tions generated by our system can get higher
ROUGE scores. Moreover, our related work sec-
tions can get higher rating scores based on a user
study. Therefore, our related work sections can
be much more suitable for the authors to prepare
their final related work sections.
</bodyText>
<sectionHeader confidence="0.999947" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999960090909092">
There are few studies to directly address auto-
matic related work generation. Hoang and Kan
(2010) proposed a related work summarization
system given the set of keywords arranged in a
hierarchical fashion that describes the paperâ€™s
topic. They used two different rule-based strate-
gies to extract sentences for general topics as
well as detailed ones.
A few studies focus on multi-document scien-
tific article summarization. Agarwal et al., (2011)
introduced an unsupervised approach to the prob-
lem of multi-document summarization. The input
is a list of papers cited together within the same
source article. The key point of this approach is a
topic based clustering of fragments extracted
from each co-cited article. They rank all the clus-
ters using a query generated from the context
surrounding the co-cited list of papers. Yeloglu
et al., (2011) compared four different approaches
for multi-document scientific articles summariza-
tion: MEAD, MEAD with corpus specific vo-
cabulary, LexRank and W3SS.
Other studies investigate mainly on the single-
document scientific article summarization. Early
works including (Luhn 1958; Baxendale 1958;
Edumundson 1969) tried to use various features
specific to scientific text (e.g., sentence position,
or rhetorical clues features). They have proved
that these features are effective for the scientific
article summarization. Citation information has
been already shown effective in summarize the
scientific articles. Works including (Mei and
Zhai 2008; Qazvinian and Radev 2008; Schwartz
and Hearst 2006; Mohammad et al., 2009) em-
ployed citation information for the single scien-
tific article summarization. Earlier work (Nakov
et al., 2004) indicated that citation sentences may
contain important concepts that can give useful
descriptions of a paper.
Various methods have been proposed for news
document summarization, including rule-based
methods (Barzilay and Elhadad 1997; Marcu and
Daniel 1997), graph-based methods (Mani and
Bloedorn 2000; Erkan and Radev 2004; Michal-
cea and Tarau 2005), learning-based methods
(Conroy et al., 2001; Shen et al., 2007; Ouyang
et al., 2007; Galanis et al., 2008), optimization-
based methods (McDonald 2007; Gillick et al.,
2009; Xie et al., 2009; Berg-Kirkpatrick et al.,
2011; Lei Huang et al., 2011; Woodsend et al.,
2012; Galanis 2012), etc.
The most relevant work is (Hoang and Kan,
2010) as mentioned above. They also assumed
the set of reference papers was given as part of
the input. They also adopt the hierarchical topic
tree that describes the topic structure in the target
paper as an essential input for their system.
However, it is non-trivial to build the hierar-
chical topic tree. Moreover, they do not consider
the content of the target paper to construct the
related work section, which is actually crucial in
the related work section. To the best of our
knowledge, no previous works have used super-
vised learning and optimization framework to
deal with the multiple scientific article summari-
zation tasks.
</bodyText>
<sectionHeader confidence="0.673476" genericHeader="method">
3 Problem Analysis and Corpus
</sectionHeader>
<subsectionHeader confidence="0.991372">
3.1 Problem Analysis
</subsectionHeader>
<page confidence="0.898982">
1625
</page>
<bodyText confidence="0.872654857142857">
We firstly analyze the structure of related work
sections briefly. By using examples for illustra-
tion, we can gain insight on how to generate re-
There has been a substantial amount of research on automatic
taxonomy induction. As we mentioned earlier, two main
approaches are pattern-based and clustering-based.
Pattern-based approaches are the main trend for automatic
</bodyText>
<subsectionHeader confidence="0.3940775">
Pattern-based approaches started from and still pay a great deal
Clustering-based approaches usually represent word contexts as
vectors and cluster words based on similarities of the vectors
Many clustering-based approaches face the challenge of
</subsectionHeader>
<bodyText confidence="0.746313714285714">
an incremental clustering approach,... The advantage of the
incremental approach is that it eliminates the trouble of
inventing cluster labels and concentrates on placing terms in the
correct positions in a taxonomy hierarchy.
The work by Snow et al. (2006) is the most similar to ours ...
Moreover, our approach employs heterogeneous features from a
wide range; while their approach only used syntactic dependency.
</bodyText>
<figureCaption confidence="0.996572">
Figure 1: A sample related work section (Yang and
</figureCaption>
<equation confidence="0.273707">
Callan 2009)
</equation>
<bodyText confidence="0.973290266666667">
appropriately labeling non-leaf clusters. ... In this paper, we take
taxonomy induction. ...
of attention to the most common is-a relations. ...
(Brown et al., 1992; Lin, 1998). ...
lated work sections. A specific related work ex-
ample is shown in Figure 1.
This related work section introduces previous
related works for a paper on Automatic Taxono-
my Induction. From Figure 1, we can have a
glance at the structure of related work sections.
Related work sections usually discuss several
different topics, such as â€œpattern-basedâ€ and
â€œcluster-basedâ€ approaches shown in the Figure
1. Besides the knowledge of previous works, the
author often compares his own work with the
previous works. The differences and advantages
are generally mentioned. The example in Figure
1 also indicates this phenomenon.
Therefore, we design our system to generate
related work sections according to the related
work section structure mentioned above. Our
system takes the target paper for which a related
work section needs to be drafted besides its ref-
erence papers as input. The goal of our system is
to generate a related work section with the above
structure. The generated related work section
should have several topic-biased parts. The au-
thor&apos;s own work is also needed to be described
and its difference with other works is needed to
be emphasized on.
</bodyText>
<subsectionHeader confidence="0.999701">
3.2 Corpus and Preprocessing
</subsectionHeader>
<bodyText confidence="0.984063">
We build a corpus that contains academic papers
and their corresponding reference papers. The
academic papers are selected from the ACL An-
thology1. The ACL Anthology currently hosts
</bodyText>
<footnote confidence="0.849564">
1 http://aclweb.org/anthology/
</footnote>
<bodyText confidence="0.999831820512821">
over 24,500 papers from major conferences such
as ACL, EMNLP, COLING in the fields of com-
putational linguistics and natural language pro-
cessing. We remove the papers that contain relat-
ed work sections with very short length, and ran-
domly select 1050 target papers to construct our
whole corpus.
The papers are all in PDF format. We extract
their texts by using PDFlib 2 and detect their
physical structures of paragraphs, subsections
and sections by using ParsCit3. For the target
papers, the related work sections are directly ex-
tracted as the gold summaries. The references are
also extracted. For the references that can be
found in the ACL Anthology, we download them
from the ACL Anthology. The other reference
papers are searched and downloaded by using
Google Scholar. References to books and PhD
theses are discarded, for their verbosity may
change the problem drastically (Mihalcea and
Ceylan, 2007).
The input of our system includes the abstract
and introduction sections of the target paper, and
the abstract, introduction, related work and con-
clusion sections of the reference papers. As men-
tioned above, the method and evaluation sections
in the reference papers are not used as input be-
cause these sections usually describe extreme
details of the methods and evaluation results and
they are not suitable for related work generation.
Note that it is reasonable to make use of the ab-
stract and introduction sections of the target pa-
per to help generate the related work section,
because an author usually has already written the
abstract and introduction sections before he or
she wants to write the related work section for
the target paper. Otherwise, we cannot get any
information about the authorâ€™s own work. All
other sections in the target paper are not used.
</bodyText>
<sectionHeader confidence="0.964521" genericHeader="method">
4 Our Proposed System
</sectionHeader>
<subsectionHeader confidence="0.964598">
4.1 Overview
</subsectionHeader>
<bodyText confidence="0.999987777777778">
In this paper, we propose a system called ARWG
to automatically generate a related work section
for a given target paper. The architecture of our
system is shown in Figure 2. We take both the
target paper and its reference papers as input and
they are represented by several sections men-
tioned in Section 3.2. After preprocessing, we
extract the feature vectors for sentences in the
target paper and the reference papers, respective-
</bodyText>
<footnote confidence="0.9683265">
2 http://www.pdflib.com/
3 http://aye.comp.nus.edu.sg/parsCit/
</footnote>
<figure confidence="0.9959178">
Comparison
with the
authorâ€™s work
Two different
topics
</figure>
<page confidence="0.981022">
1626
</page>
<bodyText confidence="0.9999329375">
ly. The importance scores for sentences in the
target paper and the reference papers are as-
signed by using two SVR based sentence scoring
models. The two SVR models are trained for
sentences in the target paper and the reference
papers, respectively. Meanwhile, a topic model is
applied to the whole set of sentences in both the
target paper and reference papers. The sentences
are grouped into several different topic-biased
clusters. The sentences with importance scores
and topic cluster information are taken as the
input for the global optimization framework. The
optimization framework extracts sentences to
describe both the authorâ€™s own work and back-
ground knowledge. More details of each part will
be discussed in the following sections.
</bodyText>
<sectionHeader confidence="0.501689" genericHeader="method">
4.2 Topic Model Learning
</sectionHeader>
<figure confidence="0.547711666666667">
Target paper Reference papers
Related Work
Section
</figure>
<figureCaption confidence="0.994977">
Figure 2: System Architecture
</figureCaption>
<bodyText confidence="0.9999775">
As mentioned in the previous section, the related
work section usually addresses several different
topics. The topics may be different research
themes or different aspects of a broad research
theme. The related work section should describe
the specific details for each topic, respectively.
Therefore, we aim to discover the hidden top-
ics of the input papers, and we use the Probabil-
istic latent semantic analysis (PLSA) (Hofmann,
1999) to solve this problem.
The PLSA approach models each word in a
document as a sample from a mixture model.
The mixture components are multinomial ran-
dom variables that can be viewed as representa-
tions of â€œtopicsâ€. Different words in a document
may be generated from different topics. Each
document is represented a list of mixing propor-
tions for these mixture components and can be
reduced to a probability distribution on a fixed
set of topics.
Considering that the sentences in one paper
may relate to different topics, we treat each sen-
tence as a â€œdocumentâ€ d. We treat the noun
phases in the sentences as the â€œwordsâ€ w. In or-
der to extract the noun phrases, chunking imple-
mented by the OpenNLP toolkit 4 is applied to
the sentences. Noun phrases that contain words
such as â€œpaperâ€ and â€œdataâ€ are discarded.
Then the sentences with their corresponding
noun phrases are taken as input into the PLSA
model. Here both the sentences in the target pa-
per and the sentences in the reference papers are
treated the same in the model. Finally, we can
get the sentence set with topic information and
use it in the subsequent steps. Each sentence has
a topic weight t in each topic.
</bodyText>
<subsectionHeader confidence="0.995863">
4.3 Sentence Important Assessment
</subsectionHeader>
<bodyText confidence="0.999987052631579">
In our proposed system, sentence importance
assessment aims to assign an importance score to
each sentence in the target paper and reference
papers. The score of each sentence will be used
in the subsequent optimization framework. We
propose to use the support vector regression
model to achieve this goal. In the above topic
model learning process, we do not distinguish the
sentences in the target paper and reference pa-
pers. In contrast, we train two different support
vector regression models separately for the sen-
tences in the target paper and the sentences in the
reference papers. In the related work section, the
sentences that describe the authorâ€™s own work
usually address the differences from the related
works, while the sentences that describe the re-
lated works often focus on the specific details.
We think the two kinds of sentences should be
treated differently.
</bodyText>
<subsectionHeader confidence="0.989829">
Scoring Method
</subsectionHeader>
<bodyText confidence="0.999777166666667">
To construct training data based on the papers
collected, we apply a similarity scoring method
to assign the importance scores to the sentences
in the papers. The main hypothesis is that the
sentences in the gold related work sections
should summarize the target paper and reference
papers as well. Thus the sentences in the papers
which are more similar to the sentences in the
gold related work sections should be considered
more important and suitable to be selected. Our
scoring method should assign higher scores to
them.
</bodyText>
<figure confidence="0.9560175">
4 http://opennlp.apache.org/
Preprocessing
Sentence Score
Assessment(target)
Sentence Score
Assessment(reference)
Optimization
Framework
Topic Model
Postprocessing
</figure>
<page confidence="0.979644">
1627
</page>
<bodyText confidence="0.999967">
We define the importance score of a sentence
in the papers as below:
</bodyText>
<equation confidence="0.99772">
score(s) = max(sim(s, sï¿½ *)) (1)
ï¿½ï¿½ *ï¿½ï¿½*
</equation>
<bodyText confidence="0.992866918367347">
where s is a sentence in the papers, S* is the set
of the sentences in the corresponding gold relat-
ed work section. The standard cosine measure is
employed as the similarity function.
Considering the difference between the sen-
tences that describe the authorâ€™s work and the
sentences that describe the related works, we
split the set of sentences in the gold related work
section into two parts: one discusses the authorâ€™s
own work and the other introduces the related
works. We observe that sentences related to the
authorâ€™s own work often feature specific words
or phrases (such as â€œweâ€, â€œour workâ€, â€œin this
paperâ€ etc.) in the related work section. So we
check the sentences about whether they contain
clue words or phrases (i.e., â€œin this paperâ€, â€œour
workâ€ and 18 other phrases). If the clue phrase
check fails, the sentence belongs to the related
work part. If not, it belongs the own work part.
Thus for the sentences in the target paper, S*
is the set of sentences in the own work part of the
gold related work section, while for the sentences
in the reference papers, S* is the set of sentences
in the related work part of the gold related work
section. Then we can use the scoring method to
compute the target scores of the sentences in the
training set. It is noteworthy that two SVR mod-
els can be trained on the two parts of the training
data, respectively.
Feature
Each sentence is represented by a set of features.
The common features used for the sentences of
the target paper and reference papers are shown
in Table 1. The additional features applied to the
sentences of the target paper are introduced in
Table 2.
Here, s is a sentence that needs to extract fea-
tures. th is paper title, section headings and sub-
section headings set of the reference papers or
target paper for the two SVR models, respective-
ly. Each feature with â€œ*â€ represent a feature set
that contains similar features.
All the features are scaled into [0, 1]. Thus we
can learn SVR models based on the features and
importance scores of the sentences, and then use
the models to predict an importance score for
each sentence in the test set. The SVR models
are trained and applied for the target paper and
reference papers, respectively.
</bodyText>
<tableCaption confidence="0.998788">
Table 1: Common features employed in the SVR
</tableCaption>
<table confidence="0.995224214285715">
models
Feature Description
Sim(s, th)* The similarity between s and each
title in th; Stop words are removed
and stemming is employed.
WS(s,th) Number of words shared by s and
th.
SP(s)* The position of s in its section or
subsection
PTI(s)* The parse tree information of s,
including the number of noun
phrase and verb phrases, the depth
of the parse tree, etc.
IsHead(s)* Indicates whether s is the first sen-
tence of the section or subsection
IsEnd(s)* Indicates whether s is the last sen-
tence of the section or subsection
SWP(s) The percentage of the stop words
Length(s) The length of sentence s
Length_rw(s) The length of s after removing stop
words
SI(s) The section index of s that indi-
cates which section s is from.
CluePhrase(s)* Indicates whether a clue phrase
appears in s. the clue phrases in-
clude â€œour workâ€, â€œproposeâ€ and
other 20 words. Each clue phrase
corresponds to one feature.
</table>
<tableCaption confidence="0.900993">
Table 2: Additional features for sentences in the
target paper
</tableCaption>
<table confidence="0.786912333333333">
Feature Description
HasCitation(s) Indicates whether s contains a
citation
PhraseForCmp(s)* Indicates whether s contains
words or phrases used for com-
parison such as â€œin contrastâ€,
â€œinsteadâ€ and other 26 words.
Each word or phrase corre-
sponds to one feature.
</table>
<subsectionHeader confidence="0.994553">
4.4 A Global Optimization Framework
</subsectionHeader>
<bodyText confidence="0.9999748">
In the above steps, we can get the predicted im-
portance score and topic information for each
sentence in the target paper and reference papers.
Here, we introduce a global optimization frame-
work to generate the related work section.
According to the structure of the related work
section mentioned above, the related work sec-
tion usually discusses several topics. In each top-
ic, the related works and their details are intro-
duced. Besides, the author often compares his
own work with these previous works.
Therefore, we propose to formulate the genera-
tion as an optimization problem. Basically, we
will be searching for a set of sentences to opti-
mize the objective function.
</bodyText>
<page confidence="0.995152">
1628
</page>
<tableCaption confidence="0.999833">
Table 3: Notations used in this section
</tableCaption>
<table confidence="0.863317133333333">
Symbol Description
ğ‘ ğ‘Ÿğ‘–/ğ‘ ğ‘¡ğ‘– the sentence in the reference/target paper
ğ‘™ğ‘Ÿğ‘–/ğ‘™ğ‘¡ğ‘– the length of sentence ğ‘ ğ‘Ÿğ‘–/ ğ‘ ğ‘¡ğ‘–
ğ‘¤ğ‘Ÿğ‘–/ğ‘¤ğ‘¡ğ‘– the importance score of ğ‘ ğ‘Ÿğ‘–/ğ‘ ğ‘¡ğ‘–
ğ‘¥ğ‘Ÿğ‘–ğ‘—/ğ‘¥ğ‘¡ğ‘–ğ‘— indicates whether ğ‘ ğ‘Ÿğ‘–/ğ‘ ğ‘¡ğ‘– is selected into
the part of topic j in the generated related
work section
nr/nt the number of sentences in the refer-
ence/target papers
m the topic count
ğ‘¡ğ‘–ğ‘— the topic weight of ğ‘ ğ‘Ÿğ‘–/ğ‘ ğ‘¡ğ‘– in topic j from
the PLSA model
B the set of unique bigrams
ğ‘¦ğ‘– indicates whether bigram ğ‘ğ‘– is included
in the result
ğ‘ğ‘ğ‘– the count of the occurrences of bigram ğ‘ğ‘–
in the both target paper and reference
papers
ğ¿ğ‘šğ‘ğ‘¥ the maximum word count of the related
work section
ğ¿ğ‘— the maximum word count of the part of
topic j which depends on the percentage
of sentences belong to topic j
ğµâˆ— the total set of bigrams in the whole pa-
per set
ğµğ‘– the set of bigrams that sentence ğ‘ ğ‘Ÿğ‘–/ğ‘ ğ‘¡ğ‘–
contains
ğ‘†ğ‘Ÿğ‘š/ğ‘†ğ‘¡ğ‘š the set of sentences that include bigram
ğ‘ğ‘š in the reference/target papers
ğœ†1, ğœ†2, ğœ†3 parameters for tuning
</table>
<bodyText confidence="0.999861777777778">
To design the objective function, three aspects
should be considered:
1) First, the related work section we generate
should introduce the previous works well. In
our assumption, sentences with higher im-
portance scores are better to be selected. In
addition, very short sentences should be pe-
nalized. So we introduce the first part of our
objective function below:
</bodyText>
<equation confidence="0.990707">
âˆ‘ (ğ‘™ğ‘Ÿğ‘–ğ‘¤ğ‘Ÿğ‘– âˆ‘ ğ‘¡ğ‘–ğ‘—ğ‘¥ğ‘Ÿğ‘–ğ‘—)
ğ‘›ğ‘Ÿ ğ‘š (2)
ğ‘–=1 ğ‘—=1
</equation>
<bodyText confidence="0.982603238095238">
We add the sentence length as a multipli-
cation factor in order to penalize the very
short sentences, or the objective function
tends to select more and shorter sentences.
At the same time, the objective function does
not tend to select the very long sentences.
The total length of the sentences selected is
fixed. So if the objective function tends to
select the longer sentences, the fewer sen-
tences can be selected. A tradeoff needs to be
made between the number and the average
length of the sentences selected.
The constraints introduced below ensure
that the sentence can only be selected into
one topic and the topic weight is used to
measure the degree that the sentence is rele-
vant to the specific topic.
2) Second, similar to the first part, we should
consider the own work part of the related
work section. Thus the second part of our ob-
jective function is shown as follows:
</bodyText>
<equation confidence="0.989503333333333">
âˆ‘ (ğ‘™ğ‘¡ğ‘–ğ‘¤ğ‘¡ğ‘– âˆ‘ ğ‘¡ğ‘–ğ‘—ğ‘¥ğ‘¡ğ‘–ğ‘—)
ğ‘›ğ‘¡ ğ‘š (3)
ğ‘–=1 ğ‘—=1
</equation>
<bodyText confidence="0.800705">
3) At last, redundancy reduction should be con-
sidered in the objective function. The last
part of the objective function is shown below:
</bodyText>
<equation confidence="0.947462333333333">
âˆ‘ ğ‘ğ‘ğ‘–ğ‘¦ğ‘–
|ğµ|
ğ‘–=1 (4)
</equation>
<bodyText confidence="0.982519142857143">
The intuition is that the more unique bi-
grams the related work section contains, the
less redundancy the related work section has.
We add ğ‘ğ‘ğ‘– as the weight of the bigram in or-
der to include more important bigrams.
By combing all the parts defined above, we
have the following full objective function:
</bodyText>
<equation confidence="0.998557916666667">
max ğœ†1 âˆ‘ ( ğ‘™ğ‘Ÿğ‘–
ğ‘›ğ‘Ÿ
ğ‘–=1 ğ›¼ğ¿ğ‘šğ‘ğ‘¥ ğ‘¤ğ‘Ÿğ‘– âˆ‘ ğ‘¡ğ‘–ğ‘—ğ‘¥ğ‘Ÿğ‘–ğ‘—) +
ğ‘š
ğ‘—=1
ğ‘¥ğ‘Ÿ,ğ‘¥ğ‘¡
ğœ†2 âˆ‘ ( ğ‘™ğ‘¡ğ‘–
ğ‘›ğ‘¡ (1âˆ’ğ›¼)ğ¿ğ‘šğ‘ğ‘¥ ğ‘¤ğ‘¡ğ‘– âˆ‘ ğ‘¡ğ‘–ğ‘—ğ‘¥ğ‘¡ğ‘–ğ‘—)
ğ‘š
ğ‘–=1 +
ğ‘—=1
ğœ†3 âˆ‘ ğ‘ğ‘ğ‘–ğ‘¦ğ‘–
|ğµ |(5)
ğ‘–=1 |ğµâˆ—|
Subject to:
âˆ‘ğ‘›ğ‘Ÿ1 ğ‘™ğ‘Ÿğ‘–ğ‘¥ğ‘Ÿğ‘–ğ‘— + âˆ‘ğ‘›ğ‘– ğ‘¡1 ğ‘™ğ‘¡ğ‘–ğ‘¥ğ‘¡ğ‘–ğ‘— &lt; ğ¿ğ‘—, ğ‘“ğ‘œğ‘Ÿ ğ‘—
ğ‘š (6)
âˆ‘ âˆ‘ ğ‘™ğ‘Ÿğ‘–ğ‘¥ğ‘Ÿğ‘–ğ‘—
ğ‘›ğ‘Ÿ ğ‘š &lt; ğ›¼ğ¿ğ‘šğ‘ğ‘¥ (7)
ğ‘–=1 ğ‘—=1
(8)
âˆ‘ ğ‘¥ğ‘Ÿğ‘–ğ‘—
ğ‘š ğ‘—=1 â‰¤ 1, ğ‘“ ğ‘œğ‘Ÿ ğ‘– = 1, ... , ğ‘›ğ‘Ÿ (9)
âˆ‘ ğ‘¥ğ‘¡ğ‘–ğ‘—
ğ‘š ğ‘—=1 â‰¤ 1, ğ‘“ ğ‘œğ‘Ÿ ğ‘– = 1, ... , ğ‘›ğ‘¡ (10)
âˆ‘ğ‘ğ‘˜âˆˆğµğ‘–ğ‘¦ğ‘˜ â‰¥ |ğµğ‘– |âˆ‘ ğ‘¥ğ‘Ÿğ‘–ğ‘—
ğ‘š ğ‘—=1 ,ğ‘“ğ‘œğ‘Ÿ ğ‘– = 1,... ,ğ‘›ğ‘Ÿ (11)
âˆ‘ğ‘ğ‘˜âˆˆğµğ‘–ğ‘¦ğ‘˜ â‰¥ |ğµğ‘– |âˆ‘ğ‘¥ğ‘¡ğ‘–ğ‘—
ğ‘š ğ‘—=1 , ğ‘“ğ‘œğ‘Ÿ ğ‘– = 1,...,ğ‘›ğ‘¡ (12)
âˆ‘ âˆ‘ ğ‘¥ğ‘Ÿğ‘–ğ‘—
ğ‘—=1 + âˆ‘ âˆ‘ ğ‘¥ğ‘¡ğ‘–ğ‘—
ğ‘š
ğ‘š â‰¥ ğ‘¦ğ‘˜,
ğ‘ ğ‘Ÿğ‘–âˆˆğ‘†ğ‘Ÿğ‘˜ ğ‘ ğ‘¡ğ‘–âˆˆğ‘†ğ‘¡ğ‘˜ ğ‘—=1
ğ‘˜ = 1,... |ğµ |(13)
ğ‘¥ğ‘Ÿğ‘–ğ‘—,ğ‘¥ğ‘¡ğ‘–ğ‘—, ğ‘¦ğ‘– âˆˆ {0,1} (14)
</equation>
<bodyText confidence="0.9672334375">
All the three parts in the objective function are
normalized to [0, 1] by using the maximum
length ğ¿ğ‘šğ‘ğ‘¥ and the total number of bigrams |ğµâˆ—|.
ğœ†1, ğœ†2 and ğœ†3 are parameters for tuning the three
parts and we set ğœ†1+ğœ†2+ğœ†3 = 1.
We explain the constraints as follows:
Constraint (6): It ensures that the total word
count of the part of topic j does not exceed ğ¿ğ‘—.
Constraints (7), (8): The two constraints try to
balance the lengths of the previous works part
and the own work part, respectively. ğ›¼ is set to
2/3.
Constraints (9), (10): These two constraints
guarantee that the sentence can only be included
into one topic.
âˆ‘ğ‘›ğ‘¡1 âˆ‘ğ‘š 1 ğ‘™ğ‘¡ğ‘–ğ‘¥ğ‘¡ğ‘–ğ‘— &lt; (1 âˆ’ ğ›¼)ğ¿ğ‘šğ‘ğ‘¥
</bodyText>
<page confidence="0.973008">
1629
</page>
<bodyText confidence="0.965031789473684">
Constraints (11), (12): When these two con-
straints hold, all bigrams that si has are selected
if si is selected.
Constraint (13): This constraint makes sure
that at least one sentence in Srâ€ or Stâ€ is select-
ed if bigram bâ€ is selected.
Therefore, we transform our optimization
problem into a linear programing problem. We
solve this linear programming problem by using
the IBM CPLEX optimizer5. It generally takes
tens of seconds to solve the problem and it is
very efficient.
Finally, ARWG post-processes sentences to
improve readability, including replacing agentive
forms with a citation to the specific article (e.g.,
â€œour workâ€ â†’ â€œ(Hoang and Kan, 2010)â€) for the
sentences extracted from reference papers. The
sentences belonging to different topics are placed
separately.
</bodyText>
<sectionHeader confidence="0.999697" genericHeader="evaluation">
5 Evaluation
</sectionHeader>
<subsectionHeader confidence="0.998524">
5.1 Evaluation Setup
</subsectionHeader>
<bodyText confidence="0.9995948">
To set up our experiments, we divide our dataset
which contains 1050 target papers and their ref-
erence papers into two parts: 700 target papers
for training, 150 papers for test and the other 200
papers for validation. The PLSA topic model is
applied to the whole dataset. We train two SVR
regression models based on the own work part
and the previous work part of the training data
and apply the models to the test data. The global
optimization framework is used to generate the
related work sections. We set the maximum word
count of the generated related work section to be
equal to that of the gold related work section.
The parameter values of A1, A2 and A3 are set to
0.3, 0.1 and 0.6, respectively. The parameter val-
ues are tuned on the validation data.
We compare our system with five baseline sys-
tems: MEAD-WT, LexRank-WT, ARWG-WT,
MEAD and LexRank. MEAD 6 (Radev et al.,
2004) is an open-source extractive multi-
document summarizer. LexRank 7 (Eran and
Radev, 2004) is a multi-document summarization
system which is based on a random walk on the
similarity graph of sentences. We also implement
the MEAD, LexRank baselines and our method
</bodyText>
<footnote confidence="0.84397525">
5 www-01.ibm.com/software/integration/optimization/cplex-
optimizer/
6 http://www.summarization.com/mead/
7 In our experiments, LexRank performs much better than
the more complex variant - C-LexRank (Qazvinian and
Radev, 2008), and thus we choose LexRank, rather than C-
LexRank, to represent graph-based summarization methods
for comparison in this paper.
</footnote>
<bodyText confidence="0.999969384615385">
with only the reference papers (i.e. the target pa-
perâ€™s content is not considered). Those methods
are signed by â€œ-WTâ€.
To evaluate the effectiveness of the SVR mod-
els we employ, we implement a baseline system
RWGOF that uses the random walk scores as the
important scores of the sentences and take the
scores as inputs for the same global optimization
framework as our system to generate the related
work section. The random walk scores are com-
puted for the sentences in the reference papers
and the target paper, respectively.
We use the ROUGE toolkit to evaluate the
content quality of the generated related work sec-
tions. ROUGE (Lin, 2004) is a widely used au-
tomatic summarization evaluation method based
on n-gram comparison. Here, we use the F-
Measure scores of ROUGE-1, ROUGE-2 and
ROUGE-SU4. The model texts are set as the
gold related work sections extracted from the
target papers, and word stemming is utilized.
ROUGE-N is an n-gram based measure between
a candidate text and a reference text. The recall
oriented score, the precision oriented score and
the F-measure score for ROUGE-N are comput-
ed as follows:
</bodyText>
<figure confidence="0.741284615384615">
ROUGE âˆ’ NRecaii
G
= CS SE{Reference Text}CC GSï¿½ gra.n COuntmatch (gramn )
GSE{Reference Text}GgramnCount(gramn) (15)
ROUGE
OUGE âˆ’ Nprecision
= GSE{Reference Text}G
gramï¿½COuntmatch(gramn) /
GSE{Candidate Text} Ggramn Count (gramn)
(16)
ROUGE âˆ’ Np_meaï¿½ï¿½re
= 2 * ROUGE âˆ’ NRecaii * ROUGE âˆ’ Nprecision /
ROUGE âˆ’ NRecaii + ROUGE âˆ’ Nprecision (17)
</figure>
<bodyText confidence="0.999945944444444">
where n stands for the length of the n-gram
gramn , and Countmatch(gramn) is the maxi-
mum number of n-grams co-occurring in a can-
didate text and a reference text.
In addition, we conducted a user study to sub-
jectively evaluate the related work sections to get
more evidences. We selected the related work
sections generated by different methods for 15
random target papers in the test set. We asked
three human judges to follow an evaluation
guideline we design and evaluate these related
work sections. The human judges are graduate
students in the computer science field and they
did not know the identities of the evaluated relat-
ed work sections. They were asked to give a rat-
ing on a scale of 1 (very poor) to 5 (very good)
for the correctness, readability and usefulness of
the related work sections, respectively:
</bodyText>
<page confidence="0.955715">
1630
</page>
<table confidence="0.8138262">
ROUGE-1 0.6 0 0.3 ROUGE-1 0 0.6 0.4- 0.15 0 ROUGE-2 0 0.6 0.1- 0.2 0 ROUGE-SU4 0 0.6 0.15
0.4 0.6 0.9 ğœ†1 0.6 OG2REU- 0.1 0.3 0.6 0.9 ğœ†1 0.15 0.15 0.3 0.6 0.9 ğœ†1 -0.2
0.2 ğœ†2 0.2- 0.05 ğœ†2 ROUGE-SU4 0.05- 0.1 ğœ†2 0.1-
0 0.4 0 0.1 0.05 0.15
0
</table>
<figureCaption confidence="0.963681">
Figure 3: Parameter influences (horizontal, vertical axis are A1, A2 , respectively, A3 = 1 âˆ’ A1 âˆ’ A2 )
</figureCaption>
<listItem confidence="0.9468745">
1) Correctness: Is the related work section ac-
tually related to the target paper?
2) Readability: Is the related work section
easy for the readers to read and grasp the
key content?
3) Usefulness: Is the related work section
useful for the author to prepare their final
related work section?
</listItem>
<bodyText confidence="0.784617">
Paired T-Tests are applied to both the ROUGE
scores and rating scores for comparing ARWG
and baselines and comparing the systems with
WT and without WT.
</bodyText>
<subsectionHeader confidence="0.805535">
5.2 Results and Discussion
</subsectionHeader>
<tableCaption confidence="0.984689">
Table 4: ROUGE F-measure comparison results
</tableCaption>
<table confidence="0.899240733333333">
Method ROUGE-1 ROUGE-2 ROUGE-
SU4
Mead- 0.39720 0.08785 0.14694
WT
LexRank- 0.43267 0.09228 0.16312
WT
ARWG- 0.45077&apos;{1,2} 0.09987&apos;{1,2} 0.16731&apos;{1}&amp;quot;{2}
WT
Mead 0.41012&apos;{1} 0.09642&apos;{1} 0.15441&apos;{1}
LexRank 0.44235&apos;{2} 0.10090&apos;{2} 0.17067&apos;{2}
ARWG ğŸ.ğŸ’ğŸ•ğŸ—ğŸ’ğŸ&apos;{ğŸâˆ’ğŸ“} ğŸ.ğŸğŸğŸğŸ•ğŸ”&apos;{ğŸâˆ’ğŸ“} ğŸ.ğŸğŸ–ğŸ”ğŸğŸ–&apos;{ğŸâˆ’ğŸ“}
(* represents pairwise t-test value p &lt; 0.01; # rep-
resents p &lt; 0.05; the numbers in the brackets rep-
resent the indices of the methods compared, e.g.
1 for MEAD-WT, 2 for LexRank-WT, etc.)
</table>
<tableCaption confidence="0.985161">
Table 5: Average rating scores of judges
</tableCaption>
<table confidence="0.9997795">
Method Correctness Readability Usefulness
Mead 2.971 2.664 2.716
LexRank 2.958 2.847 2.784
ARWG 3.433&apos;&amp;quot; 3.420&apos;&amp;quot; 3.382&apos;&amp;quot;
</table>
<tableCaption confidence="0.6862915">
(*# represents pairwise t-test value p &lt; 0.01,
compared with Mead and LexRank, respectively.)
Table 6: ROUGE F-measure comparison of dif-
ferent sentence importance scores
</tableCaption>
<table confidence="0.975372666666667">
Method ROUGE-1 ROUGE-2 ROUGE-SU4
RWGOF 0.46932 0.11791 0.18426
ARWG 0.47940 0.12176 0.18618
</table>
<bodyText confidence="0.999891829268293">
The evaluation results over ROUGE metrics are
presented in Table 4. It shows that our proposed
system can get higher ROUGE scores, i.e., better
content quality. In our system, we split the sen-
tence set into different topic-biased parts, and the
importance scores of sentences in the target pa-
per and reference papers are learned differently.
So the obtained importance scores of the sen-
tences are more reliable.
The global optimization framework considers
the extraction of both the previous work part and
the own work part. We can see the importance of
the own work part by comparing the results of
the methods with or without considering the own
work part. MEAD, LexRank and our method all
get a significant improvement after considering
the own work part by extracting sentences from
the target paper. The results also prove our as-
sumption about the related work section structure.
Figure 3 presents the fluctuation of ROUGE
scores when tuning the parameters X1, X2 and X3.
We can see our method generally performs better
than the baselines. All the three parts in the ob-
jective function are useful to generate related
work sections with good quality.
The average scores rated by human judges for
each method are showed in Table 5. We can see
that the related work sections generated by our
system are more related to the target papers.
Moreover, because of the good structure of our
generated related work sections, our generated
related work sections are considered more reada-
ble and more useful for the author to prepare the
final related work sections.
T-test results show that the performance im-
provements of our method over baselines are
statistically significant on both automatic and
manual evaluations. Most of p-values for t-test
are far smaller than 0.01.
Overall, the results indicate that our method
can generate much better related work sections
</bodyText>
<page confidence="0.969913">
1631
</page>
<bodyText confidence="0.999741909090909">
than the baselines on both automatic and human
evaluations.
Table 6 shows the comparison results between
ARWG and RWGOF. We can see ARWG per-
forms better than RWGOF. It proves that the
SVR models can better estimate the importance
scores of the sentences. For the SVR models are
trained from the large dataset, the sentence
scores predicted by the SVR models can be more
reliable to be used in the global optimization
framework.
</bodyText>
<sectionHeader confidence="0.998064" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999975684210526">
This paper proposes a novel system called
ARWG to generate related work sections for ac-
ademic papers. It first exploits a PLSA model to
split the sentence set of the given papers into dif-
ferent topic-biased parts, and then applies regres-
sion models to learn the importance scores of the
sentences. At last an optimization framework is
proposed to generate the related work section.
Evaluation results show that our system can gen-
erate much better related work sections than the
baseline methods.
In future work, we will make use of citation
sentences to improve our system. Citation sen-
tences are the sentences that contains an explicit
reference to another paper and they usually high-
light the most important aspects of the cited pa-
pers. So citation sentences are likely to contain
important and rich information for generating
related work sections.
</bodyText>
<sectionHeader confidence="0.998631" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9997652">
The work was supported by National Natural
Science Foundation of China (61170166,
61331011), Beijing Nova Program (2008B03)
and National Hi-Tech Research and Develop-
ment Program (863 Program) of China
(2012AA011101). We also thank the anonymous
reviewers for very helpful comments. The corre-
sponding author of this paper, according to the
meaning given to this role by Peking University,
is Xiaojun Wan.
</bodyText>
<sectionHeader confidence="0.996523" genericHeader="references">
Reference
</sectionHeader>
<reference confidence="0.999334426229508">
Nitin Agarwal, Kiran Gvr, Ravi Shankar Reddy, and
Carolyn Penstein RosÃ©. 2011. Towards multi-
document summarization of scientific articles:
making interesting comparisons with SciSumm.
In Proceedings of the Workshop on Automatic
Summarization for Different Genres, Media, and
Languages, pp. 8-15. Association for Computa-
tional Linguistics.
Phyllis B. Baxendale. 1958. Machine-made index for
technical literature: an experiment. IBM Journal of
Research and Development 2, no. 4: 354-361.
Taylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein.
2011. Jointly learning to extract and compress.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies-Volume 1, pp. 481-490.
Association for Computational Linguistics.
Chih-Chung Chang, and Chih-Jen Lin. 2011.
LIBSVM: a library for support vector ma-
chines. ACM Transactions on Intelligent Systems
and Technology (TIST) 2, no. 3: 27.
John M. Conroy, and Dianne P. O&apos;leary. 2001. Text
summarization via hidden markov models.
In Proceedings of the 24th annual international
ACM SIGIR conference on Research and develop-
ment in information retrieval, pp. 406-407. ACM.
Harold P. Edmundson. 1969. New methods in auto-
matic extracting. Journal of the ACM (JACM) 16,
no. 2: 264-285.
GÃ¼nes Erkan, and Dragomir R. Radev. 2004. LexPag-
eRank: Prestige in Multi-Document Text Summa-
rization. In EMNLP, vol. 4, pp. 365-371.
GÃ¼nes Erkan, and Dragomir R. Radev. 2004.
LexRank: Graph-based lexical centrality as sali-
ence in text summarization. J. Artif. Intell.
Res.(JAIR) 22, no. 1: 457-479.
Dimitrios Galanis, Gerasimos Lampouras, and Ion
Androutsopoulos. 2012. Extractive Multi-
Document Summarization with Integer Linear Pro-
gramming and Support Vector Regression.
In COLING, pp. 911-926.
Dimitrios Galanis, and Prodromos Malakasiotis. 2008.
Aueb at tac 2008. InProceedings of the TAC 2008
Workshop.
Dan Gillick, and Benoit Favre. 2009. A scalable glob-
al model for summarization. InProceedings of the
Workshop on Integer Linear Programming for
Natural Langauge Processing, pp. 10-18. Associa-
tion for Computational Linguistics.
Cong Duy Vu Hoang, and Min-Yen Kan. 2010. To-
wards automated related work summarization.
In Proceedings of the 23rd International Confer-
ence on Computational Linguistics: Posters, pp.
427-435. Association for Computational Linguis-
tics.
Lei Huang, Yanxiang He, Furu Wei, and Wenjie Li.
2010. Modeling document summarization as multi-
objective optimization. In Intelligent Information
Technology and Security Informatics (IITSI), 2010
Third International Symposium on, pp. 382-386.
IEEE.
</reference>
<page confidence="0.850688">
1632
</page>
<reference confidence="0.999871594339623">
Thomas Hofmann. 1999. Probabilistic latent semantic
indexing. In Proceedings of the 22nd annual inter-
national ACM SIGIR conference on Research and
development in information retrieval, pp. 50-57.
ACM.
Chin-Yew Lin. 2004. Rouge: A package for automatic
evaluation of summaries. InText Summarization
Branches Out: Proceedings of the ACL-04 Work-
shop, pp. 74-81.
Hans Peter Luhn. 1958. The automatic creation of
literature abstracts. IBM Journal of research and
development 2, no. 2: 159-165.
Inderjeet Mani, and Eric Bloedorn. 1999. Summariz-
ing similarities and differences among related doc-
uments. Information Retrieval 1, no. 1-2: 35-67.
Ryan McDonald. 2007. A study of global inference
algorithms in multi-document summarization.
Springer Berlin Heidelberg.
Qiaozhu Mei, and ChengXiang Zhai. 2008. Generat-
ing Impact-Based Summaries for Scientific Litera-
ture. In ACL, vol. 8, pp. 816-824.
Rada Mihalcea, and Paul Tarau. 2005. A language
independent algorithm for single and multiple doc-
ument summarization.
Rada Mihalcea, and Hakan Ceylan. 2007. Explora-
tions in Automatic Book Summarization.
In EMNLP-CoNLL, pp. 380-389.
Saif Mohammad, Bonnie Dorr, Melissa Egan, Ahmed
Hassan, Pradeep Muthukrishan, Vahed Qazvinian,
Dragomir Radev, and David Zajic. 2009. Using ci-
tations to generate surveys of scientific paradigms.
In Proceedings of Human Language Technologies:
The 2009 Annual Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics, pp. 584-592. Association for Computa-
tional Linguistics.
Preslav Nakov, Ariel Schwartz, and M. Hearst. 2004.
Citation sentences for semantic analysis of biosci-
ence text. In Proceedings of the SIGIR&apos;04 work-
shop on Search and Discovery in Bioinformatics.
Ramesh M. Nallapati, Amr Ahmed, Eric P. Xing, and
William W. Cohen. 2008. Joint latent topic models
for text and citations. In Proceedings of the 14th
ACM SIGKDD international conference on
Knowledge discovery and data mining, pp. 542-
550. ACM.
Vahed Qazvinian, and Dragomir R. Radev. 2008. Sci-
entific paper summarization using citation sum-
mary networks. In Proceedings of the 22nd Inter-
national Conference on Computational Linguistics-
Volume 1, pp. 689-696. Association for Computa-
tional Linguistics.
You Ouyang, Sujian Li, and Wenjie Li. 2007. Devel-
oping learning strategies for topic-based summari-
zation. In Proceedings of the sixteenth ACM con-
ference on Conference on information and
knowledge management, pp. 79-86. ACM.
Dragomir Radev, Timothy Allison, Sasha Blair-
Goldensohn, John Blitzer, Arda Celebi, Stanko
Dimitrov, Elliott Drabek et al. 2004. MEAD-a plat-
form for multidocument multilingual text summa-
rization. Proceedings of the 4th International Con-
ference on Language Resources and Evaluation
(LREC 2004).
Ariel S. Schwartz, and Marti Hearst. 2006. Summariz-
ing key concepts using citation sentences.
In Proceedings of the Workshop on Linking Natu-
ral Language Processing and Biology: Towards
Deeper Biological Literature Analysis, pp. 134-135.
Association for Computational Linguistics.
Dou Shen, Jian-Tao Sun, Hua Li, Qiang Yang, and
Zheng Chen. 2007. Document Summarization Us-
ing Conditional Random Fields. In IJCAI, vol. 7,
pp. 2862-2867.
Andreas Stolcke, Klaus Ries, Noah Coccaro, Eliza-
beth Shriberg, Rebecca Bates, Daniel Jurafsky,
Paul Taylor, Rachel Martin, Carol Van Ess-
Dykema, and Marie Meteer. 2000. Dialogue act
modeling for automatic tagging and recognition of
conversational speech. Computational linguis-
tics 26, no. 3: 339-373.
Kristian Woodsend, and Mirella Lapata. 2012. Multi-
ple aspect summarization using integer linear pro-
gramming. In Proceedings of the 2012 Joint Con-
ference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural
Language Learning, pp. 233-243. Association for
Computational Linguistics.
Shasha Xie, Benoit Favre, Dilek Hakkani-TÃ¼r, and
Yang Liu. 2009. Leveraging sentence weights in a
concept-based optimization framework for extrac-
tive meeting summarization. In INTERSPEECH,
pp. 1503-1506.
Hui Yang, and Jamie Callan. 2009. A metric-based
framework for automatic taxonomy induction.
In Proceedings of the Joint Conference of the 47th
Annual Meeting of the ACL and the 4th Interna-
tional Joint Conference on Natural Language Pro-
cessing of the AFNLP: Volume 1-Volume 1, pp.
271-279. Association for Computational Linguis-
tics.
Ozge Yeloglu, Evangelos Milios, and Nur Zincir-
Heywood. 2011. Multi-document summarization of
scientific corpora. In Proceedings of the 2011 ACM
Symposium on Applied Computing, pp. 252-258.
ACM.
</reference>
<page confidence="0.977599">
1633
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.798423">
<title confidence="0.999324">Automatic Generation of Related Work Sections in Scientific Papers: An Optimization Approach</title>
<author confidence="0.98818">Yue Hu</author>
<author confidence="0.98818">Xiaojun</author>
<affiliation confidence="0.93986">Institute of Computer Science and The MOE Key Laboratory of Computational Peking University, Beijing, China</affiliation>
<email confidence="0.967724">ayue.hu@pku.edu.cn</email>
<email confidence="0.967724">wanxiaojun@pku.edu.cn</email>
<abstract confidence="0.999745307692308">In this paper, we investigate a challenging task of automatic related work generation. Given multiple reference papers as input, the task aims to generate a related work section for a target paper. The generated related work section can be used as a draft for the author to complete his or her final related work section. We propose our Automatic Related Work Generation system called ARWG to address this task. It first exploits a PLSA model to split the sentence set of the given papers into different topic-biased parts, and then applies regression models to learn the importance of the sentences. At last it employs an optimization framework to generate the related work section. Our evaluation results on a test set of 150 target papers along with their reference papers show that our proposed ARWG system can generate related work sections with better quality. A user study is also performed to show ARWG can achieve an improvement over generic multi-document summarization baselines.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Nitin Agarwal</author>
<author>Kiran Gvr</author>
<author>Ravi Shankar Reddy</author>
<author>Carolyn Penstein RosÃ©</author>
</authors>
<title>Towards multidocument summarization of scientific articles: making interesting comparisons with SciSumm.</title>
<date>2011</date>
<booktitle>In Proceedings of the Workshop on Automatic Summarization for Different Genres, Media, and Languages,</booktitle>
<pages>8--15</pages>
<institution>Association for Computational Linguistics.</institution>
<contexts>
<context position="5910" citStr="Agarwal et al., (2011)" startWordPosition="948" endWordPosition="951"> rating scores based on a user study. Therefore, our related work sections can be much more suitable for the authors to prepare their final related work sections. 2 Related Work There are few studies to directly address automatic related work generation. Hoang and Kan (2010) proposed a related work summarization system given the set of keywords arranged in a hierarchical fashion that describes the paperâ€™s topic. They used two different rule-based strategies to extract sentences for general topics as well as detailed ones. A few studies focus on multi-document scientific article summarization. Agarwal et al., (2011) introduced an unsupervised approach to the problem of multi-document summarization. The input is a list of papers cited together within the same source article. The key point of this approach is a topic based clustering of fragments extracted from each co-cited article. They rank all the clusters using a query generated from the context surrounding the co-cited list of papers. Yeloglu et al., (2011) compared four different approaches for multi-document scientific articles summarization: MEAD, MEAD with corpus specific vocabulary, LexRank and W3SS. Other studies investigate mainly on the singl</context>
</contexts>
<marker>Agarwal, Gvr, Reddy, RosÃ©, 2011</marker>
<rawString>Nitin Agarwal, Kiran Gvr, Ravi Shankar Reddy, and Carolyn Penstein RosÃ©. 2011. Towards multidocument summarization of scientific articles: making interesting comparisons with SciSumm. In Proceedings of the Workshop on Automatic Summarization for Different Genres, Media, and Languages, pp. 8-15. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Phyllis B Baxendale</author>
</authors>
<title>Machine-made index for technical literature: an experiment.</title>
<date>1958</date>
<journal>IBM Journal of Research and Development</journal>
<volume>2</volume>
<pages>354--361</pages>
<contexts>
<context position="6602" citStr="Baxendale 1958" startWordPosition="1054" endWordPosition="1055">ation. The input is a list of papers cited together within the same source article. The key point of this approach is a topic based clustering of fragments extracted from each co-cited article. They rank all the clusters using a query generated from the context surrounding the co-cited list of papers. Yeloglu et al., (2011) compared four different approaches for multi-document scientific articles summarization: MEAD, MEAD with corpus specific vocabulary, LexRank and W3SS. Other studies investigate mainly on the singledocument scientific article summarization. Early works including (Luhn 1958; Baxendale 1958; Edumundson 1969) tried to use various features specific to scientific text (e.g., sentence position, or rhetorical clues features). They have proved that these features are effective for the scientific article summarization. Citation information has been already shown effective in summarize the scientific articles. Works including (Mei and Zhai 2008; Qazvinian and Radev 2008; Schwartz and Hearst 2006; Mohammad et al., 2009) employed citation information for the single scientific article summarization. Earlier work (Nakov et al., 2004) indicated that citation sentences may contain important c</context>
</contexts>
<marker>Baxendale, 1958</marker>
<rawString>Phyllis B. Baxendale. 1958. Machine-made index for technical literature: an experiment. IBM Journal of Research and Development 2, no. 4: 354-361.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taylor Berg-Kirkpatrick</author>
<author>Dan Gillick</author>
<author>Dan Klein</author>
</authors>
<title>Jointly learning to extract and compress.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language</booktitle>
<volume>1</volume>
<pages>481--490</pages>
<institution>Association for Computational Linguistics.</institution>
<contexts>
<context position="7718" citStr="Berg-Kirkpatrick et al., 2011" startWordPosition="1216" endWordPosition="1219">rticle summarization. Earlier work (Nakov et al., 2004) indicated that citation sentences may contain important concepts that can give useful descriptions of a paper. Various methods have been proposed for news document summarization, including rule-based methods (Barzilay and Elhadad 1997; Marcu and Daniel 1997), graph-based methods (Mani and Bloedorn 2000; Erkan and Radev 2004; Michalcea and Tarau 2005), learning-based methods (Conroy et al., 2001; Shen et al., 2007; Ouyang et al., 2007; Galanis et al., 2008), optimizationbased methods (McDonald 2007; Gillick et al., 2009; Xie et al., 2009; Berg-Kirkpatrick et al., 2011; Lei Huang et al., 2011; Woodsend et al., 2012; Galanis 2012), etc. The most relevant work is (Hoang and Kan, 2010) as mentioned above. They also assumed the set of reference papers was given as part of the input. They also adopt the hierarchical topic tree that describes the topic structure in the target paper as an essential input for their system. However, it is non-trivial to build the hierarchical topic tree. Moreover, they do not consider the content of the target paper to construct the related work section, which is actually crucial in the related work section. To the best of our knowl</context>
</contexts>
<marker>Berg-Kirkpatrick, Gillick, Klein, 2011</marker>
<rawString>Taylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein. 2011. Jointly learning to extract and compress. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1, pp. 481-490. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chih-Chung Chang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBSVM: a library for support vector machines.</title>
<date>2011</date>
<journal>ACM Transactions on Intelligent Systems and Technology (TIST)</journal>
<volume>2</volume>
<pages>27</pages>
<marker>Chang, Lin, 2011</marker>
<rawString>Chih-Chung Chang, and Chih-Jen Lin. 2011. LIBSVM: a library for support vector machines. ACM Transactions on Intelligent Systems and Technology (TIST) 2, no. 3: 27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John M Conroy</author>
<author>Dianne P O&apos;leary</author>
</authors>
<title>Text summarization via hidden markov models.</title>
<date>2001</date>
<booktitle>In Proceedings of the 24th annual international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>406--407</pages>
<publisher>ACM.</publisher>
<marker>Conroy, O&apos;leary, 2001</marker>
<rawString>John M. Conroy, and Dianne P. O&apos;leary. 2001. Text summarization via hidden markov models. In Proceedings of the 24th annual international ACM SIGIR conference on Research and development in information retrieval, pp. 406-407. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harold P Edmundson</author>
</authors>
<title>New methods in automatic extracting.</title>
<date>1969</date>
<journal>Journal of the ACM (JACM)</journal>
<volume>16</volume>
<pages>264--285</pages>
<marker>Edmundson, 1969</marker>
<rawString>Harold P. Edmundson. 1969. New methods in automatic extracting. Journal of the ACM (JACM) 16, no. 2: 264-285.</rawString>
</citation>
<citation valid="true">
<authors>
<author>GÃ¼nes Erkan</author>
<author>Dragomir R Radev</author>
</authors>
<title>LexPageRank: Prestige in Multi-Document Text Summarization.</title>
<date>2004</date>
<booktitle>In EMNLP,</booktitle>
<volume>4</volume>
<pages>365--371</pages>
<contexts>
<context position="7470" citStr="Erkan and Radev 2004" startWordPosition="1176" endWordPosition="1179">has been already shown effective in summarize the scientific articles. Works including (Mei and Zhai 2008; Qazvinian and Radev 2008; Schwartz and Hearst 2006; Mohammad et al., 2009) employed citation information for the single scientific article summarization. Earlier work (Nakov et al., 2004) indicated that citation sentences may contain important concepts that can give useful descriptions of a paper. Various methods have been proposed for news document summarization, including rule-based methods (Barzilay and Elhadad 1997; Marcu and Daniel 1997), graph-based methods (Mani and Bloedorn 2000; Erkan and Radev 2004; Michalcea and Tarau 2005), learning-based methods (Conroy et al., 2001; Shen et al., 2007; Ouyang et al., 2007; Galanis et al., 2008), optimizationbased methods (McDonald 2007; Gillick et al., 2009; Xie et al., 2009; Berg-Kirkpatrick et al., 2011; Lei Huang et al., 2011; Woodsend et al., 2012; Galanis 2012), etc. The most relevant work is (Hoang and Kan, 2010) as mentioned above. They also assumed the set of reference papers was given as part of the input. They also adopt the hierarchical topic tree that describes the topic structure in the target paper as an essential input for their system</context>
</contexts>
<marker>Erkan, Radev, 2004</marker>
<rawString>GÃ¼nes Erkan, and Dragomir R. Radev. 2004. LexPageRank: Prestige in Multi-Document Text Summarization. In EMNLP, vol. 4, pp. 365-371.</rawString>
</citation>
<citation valid="true">
<authors>
<author>GÃ¼nes Erkan</author>
<author>Dragomir R Radev</author>
</authors>
<title>LexRank: Graph-based lexical centrality as salience in text summarization.</title>
<date>2004</date>
<journal>J. Artif. Intell. Res.(JAIR)</journal>
<volume>22</volume>
<pages>457--479</pages>
<contexts>
<context position="7470" citStr="Erkan and Radev 2004" startWordPosition="1176" endWordPosition="1179">has been already shown effective in summarize the scientific articles. Works including (Mei and Zhai 2008; Qazvinian and Radev 2008; Schwartz and Hearst 2006; Mohammad et al., 2009) employed citation information for the single scientific article summarization. Earlier work (Nakov et al., 2004) indicated that citation sentences may contain important concepts that can give useful descriptions of a paper. Various methods have been proposed for news document summarization, including rule-based methods (Barzilay and Elhadad 1997; Marcu and Daniel 1997), graph-based methods (Mani and Bloedorn 2000; Erkan and Radev 2004; Michalcea and Tarau 2005), learning-based methods (Conroy et al., 2001; Shen et al., 2007; Ouyang et al., 2007; Galanis et al., 2008), optimizationbased methods (McDonald 2007; Gillick et al., 2009; Xie et al., 2009; Berg-Kirkpatrick et al., 2011; Lei Huang et al., 2011; Woodsend et al., 2012; Galanis 2012), etc. The most relevant work is (Hoang and Kan, 2010) as mentioned above. They also assumed the set of reference papers was given as part of the input. They also adopt the hierarchical topic tree that describes the topic structure in the target paper as an essential input for their system</context>
</contexts>
<marker>Erkan, Radev, 2004</marker>
<rawString>GÃ¼nes Erkan, and Dragomir R. Radev. 2004. LexRank: Graph-based lexical centrality as salience in text summarization. J. Artif. Intell. Res.(JAIR) 22, no. 1: 457-479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dimitrios Galanis</author>
<author>Gerasimos Lampouras</author>
<author>Ion Androutsopoulos</author>
</authors>
<title>Extractive MultiDocument Summarization with Integer Linear Programming and Support Vector Regression.</title>
<date>2012</date>
<booktitle>In COLING,</booktitle>
<pages>911--926</pages>
<marker>Galanis, Lampouras, Androutsopoulos, 2012</marker>
<rawString>Dimitrios Galanis, Gerasimos Lampouras, and Ion Androutsopoulos. 2012. Extractive MultiDocument Summarization with Integer Linear Programming and Support Vector Regression. In COLING, pp. 911-926.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dimitrios Galanis</author>
<author>Prodromos Malakasiotis</author>
</authors>
<title>Aueb at tac</title>
<date>2008</date>
<journal>InProceedings of the TAC</journal>
<note>Workshop.</note>
<marker>Galanis, Malakasiotis, 2008</marker>
<rawString>Dimitrios Galanis, and Prodromos Malakasiotis. 2008. Aueb at tac 2008. InProceedings of the TAC 2008 Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Gillick</author>
<author>Benoit Favre</author>
</authors>
<title>A scalable global model for summarization.</title>
<date>2009</date>
<booktitle>InProceedings of the Workshop on Integer Linear Programming for Natural Langauge Processing,</booktitle>
<pages>10--18</pages>
<institution>Association for Computational Linguistics.</institution>
<marker>Gillick, Favre, 2009</marker>
<rawString>Dan Gillick, and Benoit Favre. 2009. A scalable global model for summarization. InProceedings of the Workshop on Integer Linear Programming for Natural Langauge Processing, pp. 10-18. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cong Duy Vu Hoang</author>
<author>Min-Yen Kan</author>
</authors>
<title>Towards automated related work summarization.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics: Posters,</booktitle>
<pages>427--435</pages>
<institution>Association for Computational Linguistics.</institution>
<contexts>
<context position="5563" citStr="Hoang and Kan (2010)" startWordPosition="895" endWordPosition="898">ly. Experimental results on a test set of 150 target papers show our method can generate related work sections with better quality than those of several baseline methods. With the ROUGE toolkit, the results indicate the related work sections generated by our system can get higher ROUGE scores. Moreover, our related work sections can get higher rating scores based on a user study. Therefore, our related work sections can be much more suitable for the authors to prepare their final related work sections. 2 Related Work There are few studies to directly address automatic related work generation. Hoang and Kan (2010) proposed a related work summarization system given the set of keywords arranged in a hierarchical fashion that describes the paperâ€™s topic. They used two different rule-based strategies to extract sentences for general topics as well as detailed ones. A few studies focus on multi-document scientific article summarization. Agarwal et al., (2011) introduced an unsupervised approach to the problem of multi-document summarization. The input is a list of papers cited together within the same source article. The key point of this approach is a topic based clustering of fragments extracted from each</context>
<context position="7834" citStr="Hoang and Kan, 2010" startWordPosition="1237" endWordPosition="1240"> can give useful descriptions of a paper. Various methods have been proposed for news document summarization, including rule-based methods (Barzilay and Elhadad 1997; Marcu and Daniel 1997), graph-based methods (Mani and Bloedorn 2000; Erkan and Radev 2004; Michalcea and Tarau 2005), learning-based methods (Conroy et al., 2001; Shen et al., 2007; Ouyang et al., 2007; Galanis et al., 2008), optimizationbased methods (McDonald 2007; Gillick et al., 2009; Xie et al., 2009; Berg-Kirkpatrick et al., 2011; Lei Huang et al., 2011; Woodsend et al., 2012; Galanis 2012), etc. The most relevant work is (Hoang and Kan, 2010) as mentioned above. They also assumed the set of reference papers was given as part of the input. They also adopt the hierarchical topic tree that describes the topic structure in the target paper as an essential input for their system. However, it is non-trivial to build the hierarchical topic tree. Moreover, they do not consider the content of the target paper to construct the related work section, which is actually crucial in the related work section. To the best of our knowledge, no previous works have used supervised learning and optimization framework to deal with the multiple scientifi</context>
<context position="26597" citStr="Hoang and Kan, 2010" startWordPosition="4418" endWordPosition="4421">o constraints hold, all bigrams that si has are selected if si is selected. Constraint (13): This constraint makes sure that at least one sentence in Srâ€ or Stâ€ is selected if bigram bâ€ is selected. Therefore, we transform our optimization problem into a linear programing problem. We solve this linear programming problem by using the IBM CPLEX optimizer5. It generally takes tens of seconds to solve the problem and it is very efficient. Finally, ARWG post-processes sentences to improve readability, including replacing agentive forms with a citation to the specific article (e.g., â€œour workâ€ â†’ â€œ(Hoang and Kan, 2010)â€) for the sentences extracted from reference papers. The sentences belonging to different topics are placed separately. 5 Evaluation 5.1 Evaluation Setup To set up our experiments, we divide our dataset which contains 1050 target papers and their reference papers into two parts: 700 target papers for training, 150 papers for test and the other 200 papers for validation. The PLSA topic model is applied to the whole dataset. We train two SVR regression models based on the own work part and the previous work part of the training data and apply the models to the test data. The global optimization</context>
</contexts>
<marker>Hoang, Kan, 2010</marker>
<rawString>Cong Duy Vu Hoang, and Min-Yen Kan. 2010. Towards automated related work summarization. In Proceedings of the 23rd International Conference on Computational Linguistics: Posters, pp. 427-435. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lei Huang</author>
<author>Yanxiang He</author>
<author>Furu Wei</author>
<author>Wenjie Li</author>
</authors>
<title>Modeling document summarization as multiobjective optimization.</title>
<date>2010</date>
<booktitle>In Intelligent Information Technology and Security Informatics (IITSI), 2010 Third International Symposium on,</booktitle>
<pages>382--386</pages>
<publisher>IEEE.</publisher>
<marker>Huang, He, Wei, Li, 2010</marker>
<rawString>Lei Huang, Yanxiang He, Furu Wei, and Wenjie Li. 2010. Modeling document summarization as multiobjective optimization. In Intelligent Information Technology and Security Informatics (IITSI), 2010 Third International Symposium on, pp. 382-386. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Hofmann</author>
</authors>
<title>Probabilistic latent semantic indexing.</title>
<date>1999</date>
<booktitle>In Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>50--57</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="14852" citStr="Hofmann, 1999" startWordPosition="2355" endWordPosition="2356"> More details of each part will be discussed in the following sections. 4.2 Topic Model Learning Target paper Reference papers Related Work Section Figure 2: System Architecture As mentioned in the previous section, the related work section usually addresses several different topics. The topics may be different research themes or different aspects of a broad research theme. The related work section should describe the specific details for each topic, respectively. Therefore, we aim to discover the hidden topics of the input papers, and we use the Probabilistic latent semantic analysis (PLSA) (Hofmann, 1999) to solve this problem. The PLSA approach models each word in a document as a sample from a mixture model. The mixture components are multinomial random variables that can be viewed as representations of â€œtopicsâ€. Different words in a document may be generated from different topics. Each document is represented a list of mixing proportions for these mixture components and can be reduced to a probability distribution on a fixed set of topics. Considering that the sentences in one paper may relate to different topics, we treat each sentence as a â€œdocumentâ€ d. We treat the noun phases in the sent</context>
</contexts>
<marker>Hofmann, 1999</marker>
<rawString>Thomas Hofmann. 1999. Probabilistic latent semantic indexing. In Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval, pp. 50-57. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
</authors>
<title>Rouge: A package for automatic evaluation of summaries. InText Summarization Branches Out:</title>
<date>2004</date>
<booktitle>Proceedings of the ACL-04 Workshop,</booktitle>
<pages>74--81</pages>
<contexts>
<context position="28894" citStr="Lin, 2004" startWordPosition="4794" endWordPosition="4795">i.e. the target paperâ€™s content is not considered). Those methods are signed by â€œ-WTâ€. To evaluate the effectiveness of the SVR models we employ, we implement a baseline system RWGOF that uses the random walk scores as the important scores of the sentences and take the scores as inputs for the same global optimization framework as our system to generate the related work section. The random walk scores are computed for the sentences in the reference papers and the target paper, respectively. We use the ROUGE toolkit to evaluate the content quality of the generated related work sections. ROUGE (Lin, 2004) is a widely used automatic summarization evaluation method based on n-gram comparison. Here, we use the FMeasure scores of ROUGE-1, ROUGE-2 and ROUGE-SU4. The model texts are set as the gold related work sections extracted from the target papers, and word stemming is utilized. ROUGE-N is an n-gram based measure between a candidate text and a reference text. The recall oriented score, the precision oriented score and the F-measure score for ROUGE-N are computed as follows: ROUGE âˆ’ NRecaii G = CS SE{Reference Text}CC GSï¿½ gra.n COuntmatch (gramn ) GSE{Reference Text}GgramnCount(gramn) (15) ROUGE</context>
</contexts>
<marker>Lin, 2004</marker>
<rawString>Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. InText Summarization Branches Out: Proceedings of the ACL-04 Workshop, pp. 74-81.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hans Peter Luhn</author>
</authors>
<title>The automatic creation of literature abstracts.</title>
<date>1958</date>
<journal>IBM Journal of research and</journal>
<volume>2</volume>
<pages>159--165</pages>
<contexts>
<context position="6586" citStr="Luhn 1958" startWordPosition="1052" endWordPosition="1053">nt summarization. The input is a list of papers cited together within the same source article. The key point of this approach is a topic based clustering of fragments extracted from each co-cited article. They rank all the clusters using a query generated from the context surrounding the co-cited list of papers. Yeloglu et al., (2011) compared four different approaches for multi-document scientific articles summarization: MEAD, MEAD with corpus specific vocabulary, LexRank and W3SS. Other studies investigate mainly on the singledocument scientific article summarization. Early works including (Luhn 1958; Baxendale 1958; Edumundson 1969) tried to use various features specific to scientific text (e.g., sentence position, or rhetorical clues features). They have proved that these features are effective for the scientific article summarization. Citation information has been already shown effective in summarize the scientific articles. Works including (Mei and Zhai 2008; Qazvinian and Radev 2008; Schwartz and Hearst 2006; Mohammad et al., 2009) employed citation information for the single scientific article summarization. Earlier work (Nakov et al., 2004) indicated that citation sentences may con</context>
</contexts>
<marker>Luhn, 1958</marker>
<rawString>Hans Peter Luhn. 1958. The automatic creation of literature abstracts. IBM Journal of research and development 2, no. 2: 159-165.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Inderjeet Mani</author>
<author>Eric Bloedorn</author>
</authors>
<title>Summarizing similarities and differences among related documents.</title>
<date>1999</date>
<journal>Information Retrieval</journal>
<volume>1</volume>
<pages>1--2</pages>
<marker>Mani, Bloedorn, 1999</marker>
<rawString>Inderjeet Mani, and Eric Bloedorn. 1999. Summarizing similarities and differences among related documents. Information Retrieval 1, no. 1-2: 35-67.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
</authors>
<title>A study of global inference algorithms in multi-document summarization.</title>
<date>2007</date>
<publisher>Springer</publisher>
<location>Berlin Heidelberg.</location>
<contexts>
<context position="7647" citStr="McDonald 2007" startWordPosition="1206" endWordPosition="1207">ployed citation information for the single scientific article summarization. Earlier work (Nakov et al., 2004) indicated that citation sentences may contain important concepts that can give useful descriptions of a paper. Various methods have been proposed for news document summarization, including rule-based methods (Barzilay and Elhadad 1997; Marcu and Daniel 1997), graph-based methods (Mani and Bloedorn 2000; Erkan and Radev 2004; Michalcea and Tarau 2005), learning-based methods (Conroy et al., 2001; Shen et al., 2007; Ouyang et al., 2007; Galanis et al., 2008), optimizationbased methods (McDonald 2007; Gillick et al., 2009; Xie et al., 2009; Berg-Kirkpatrick et al., 2011; Lei Huang et al., 2011; Woodsend et al., 2012; Galanis 2012), etc. The most relevant work is (Hoang and Kan, 2010) as mentioned above. They also assumed the set of reference papers was given as part of the input. They also adopt the hierarchical topic tree that describes the topic structure in the target paper as an essential input for their system. However, it is non-trivial to build the hierarchical topic tree. Moreover, they do not consider the content of the target paper to construct the related work section, which is</context>
</contexts>
<marker>McDonald, 2007</marker>
<rawString>Ryan McDonald. 2007. A study of global inference algorithms in multi-document summarization. Springer Berlin Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qiaozhu Mei</author>
<author>ChengXiang Zhai</author>
</authors>
<title>Generating Impact-Based Summaries for Scientific Literature.</title>
<date>2008</date>
<booktitle>In ACL,</booktitle>
<volume>8</volume>
<pages>816--824</pages>
<contexts>
<context position="6955" citStr="Mei and Zhai 2008" startWordPosition="1101" endWordPosition="1104">approaches for multi-document scientific articles summarization: MEAD, MEAD with corpus specific vocabulary, LexRank and W3SS. Other studies investigate mainly on the singledocument scientific article summarization. Early works including (Luhn 1958; Baxendale 1958; Edumundson 1969) tried to use various features specific to scientific text (e.g., sentence position, or rhetorical clues features). They have proved that these features are effective for the scientific article summarization. Citation information has been already shown effective in summarize the scientific articles. Works including (Mei and Zhai 2008; Qazvinian and Radev 2008; Schwartz and Hearst 2006; Mohammad et al., 2009) employed citation information for the single scientific article summarization. Earlier work (Nakov et al., 2004) indicated that citation sentences may contain important concepts that can give useful descriptions of a paper. Various methods have been proposed for news document summarization, including rule-based methods (Barzilay and Elhadad 1997; Marcu and Daniel 1997), graph-based methods (Mani and Bloedorn 2000; Erkan and Radev 2004; Michalcea and Tarau 2005), learning-based methods (Conroy et al., 2001; Shen et al.</context>
</contexts>
<marker>Mei, Zhai, 2008</marker>
<rawString>Qiaozhu Mei, and ChengXiang Zhai. 2008. Generating Impact-Based Summaries for Scientific Literature. In ACL, vol. 8, pp. 816-824.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Paul Tarau</author>
</authors>
<title>A language independent algorithm for single and multiple document summarization.</title>
<date>2005</date>
<marker>Mihalcea, Tarau, 2005</marker>
<rawString>Rada Mihalcea, and Paul Tarau. 2005. A language independent algorithm for single and multiple document summarization.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Hakan Ceylan</author>
</authors>
<date>2007</date>
<booktitle>Explorations in Automatic Book Summarization. In EMNLP-CoNLL,</booktitle>
<pages>380--389</pages>
<contexts>
<context position="12103" citStr="Mihalcea and Ceylan, 2007" startWordPosition="1915" endWordPosition="1918">hole corpus. The papers are all in PDF format. We extract their texts by using PDFlib 2 and detect their physical structures of paragraphs, subsections and sections by using ParsCit3. For the target papers, the related work sections are directly extracted as the gold summaries. The references are also extracted. For the references that can be found in the ACL Anthology, we download them from the ACL Anthology. The other reference papers are searched and downloaded by using Google Scholar. References to books and PhD theses are discarded, for their verbosity may change the problem drastically (Mihalcea and Ceylan, 2007). The input of our system includes the abstract and introduction sections of the target paper, and the abstract, introduction, related work and conclusion sections of the reference papers. As mentioned above, the method and evaluation sections in the reference papers are not used as input because these sections usually describe extreme details of the methods and evaluation results and they are not suitable for related work generation. Note that it is reasonable to make use of the abstract and introduction sections of the target paper to help generate the related work section, because an author</context>
</contexts>
<marker>Mihalcea, Ceylan, 2007</marker>
<rawString>Rada Mihalcea, and Hakan Ceylan. 2007. Explorations in Automatic Book Summarization. In EMNLP-CoNLL, pp. 380-389.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Saif Mohammad</author>
<author>Bonnie Dorr</author>
<author>Melissa Egan</author>
<author>Ahmed Hassan</author>
<author>Pradeep Muthukrishan</author>
<author>Vahed Qazvinian</author>
<author>Dragomir Radev</author>
<author>David Zajic</author>
</authors>
<title>Using citations to generate surveys of scientific paradigms.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>584--592</pages>
<institution>Association for Computational Linguistics.</institution>
<contexts>
<context position="7031" citStr="Mohammad et al., 2009" startWordPosition="1113" endWordPosition="1116">MEAD with corpus specific vocabulary, LexRank and W3SS. Other studies investigate mainly on the singledocument scientific article summarization. Early works including (Luhn 1958; Baxendale 1958; Edumundson 1969) tried to use various features specific to scientific text (e.g., sentence position, or rhetorical clues features). They have proved that these features are effective for the scientific article summarization. Citation information has been already shown effective in summarize the scientific articles. Works including (Mei and Zhai 2008; Qazvinian and Radev 2008; Schwartz and Hearst 2006; Mohammad et al., 2009) employed citation information for the single scientific article summarization. Earlier work (Nakov et al., 2004) indicated that citation sentences may contain important concepts that can give useful descriptions of a paper. Various methods have been proposed for news document summarization, including rule-based methods (Barzilay and Elhadad 1997; Marcu and Daniel 1997), graph-based methods (Mani and Bloedorn 2000; Erkan and Radev 2004; Michalcea and Tarau 2005), learning-based methods (Conroy et al., 2001; Shen et al., 2007; Ouyang et al., 2007; Galanis et al., 2008), optimizationbased method</context>
</contexts>
<marker>Mohammad, Dorr, Egan, Hassan, Muthukrishan, Qazvinian, Radev, Zajic, 2009</marker>
<rawString>Saif Mohammad, Bonnie Dorr, Melissa Egan, Ahmed Hassan, Pradeep Muthukrishan, Vahed Qazvinian, Dragomir Radev, and David Zajic. 2009. Using citations to generate surveys of scientific paradigms. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pp. 584-592. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Preslav Nakov</author>
<author>Ariel Schwartz</author>
<author>M Hearst</author>
</authors>
<title>Citation sentences for semantic analysis of bioscience text.</title>
<date>2004</date>
<booktitle>In Proceedings of the SIGIR&apos;04 workshop on Search and Discovery in Bioinformatics.</booktitle>
<contexts>
<context position="7144" citStr="Nakov et al., 2004" startWordPosition="1130" endWordPosition="1133">tific article summarization. Early works including (Luhn 1958; Baxendale 1958; Edumundson 1969) tried to use various features specific to scientific text (e.g., sentence position, or rhetorical clues features). They have proved that these features are effective for the scientific article summarization. Citation information has been already shown effective in summarize the scientific articles. Works including (Mei and Zhai 2008; Qazvinian and Radev 2008; Schwartz and Hearst 2006; Mohammad et al., 2009) employed citation information for the single scientific article summarization. Earlier work (Nakov et al., 2004) indicated that citation sentences may contain important concepts that can give useful descriptions of a paper. Various methods have been proposed for news document summarization, including rule-based methods (Barzilay and Elhadad 1997; Marcu and Daniel 1997), graph-based methods (Mani and Bloedorn 2000; Erkan and Radev 2004; Michalcea and Tarau 2005), learning-based methods (Conroy et al., 2001; Shen et al., 2007; Ouyang et al., 2007; Galanis et al., 2008), optimizationbased methods (McDonald 2007; Gillick et al., 2009; Xie et al., 2009; Berg-Kirkpatrick et al., 2011; Lei Huang et al., 2011; </context>
</contexts>
<marker>Nakov, Schwartz, Hearst, 2004</marker>
<rawString>Preslav Nakov, Ariel Schwartz, and M. Hearst. 2004. Citation sentences for semantic analysis of bioscience text. In Proceedings of the SIGIR&apos;04 workshop on Search and Discovery in Bioinformatics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ramesh M Nallapati</author>
<author>Amr Ahmed</author>
<author>Eric P Xing</author>
<author>William W Cohen</author>
</authors>
<title>Joint latent topic models for text and citations.</title>
<date>2008</date>
<booktitle>In Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining,</booktitle>
<pages>542--550</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="2637" citStr="Nallapati et al., 2008" startWordPosition="433" endWordPosition="437">ing task. It can be considered a topic-biased, multiple-document summarization problem. The input is a target academic paper, which has no related work section, along with its reference papers. The goal is to create a related work section that describes the related works and addresses the relationship between the target paper and the reference papers. Here we assume that the set of reference papers has been given as part of the input. Existing works in the NLP and recommendation systems communities have already focused on the task of finding reference papers. For example, citation prediction (Nallapati et al., 2008) aims at finding individual paper citation patterns. Generally speaking, automatic related work section generation is a strikingly different problem and it is much more difficult in comparison with general multi-document summarization tasks. For example, multi-document summarization of news articles aims at synthesizing contents of similar news and removing the redundant information contained by the different news articles. However, each scientific paper has much specific content to state its own work and contribution. Even for the papers that investigate the same research topic, their contrib</context>
</contexts>
<marker>Nallapati, Ahmed, Xing, Cohen, 2008</marker>
<rawString>Ramesh M. Nallapati, Amr Ahmed, Eric P. Xing, and William W. Cohen. 2008. Joint latent topic models for text and citations. In Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 542-550. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vahed Qazvinian</author>
<author>Dragomir R Radev</author>
</authors>
<title>Scientific paper summarization using citation summary networks.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational LinguisticsVolume 1,</booktitle>
<pages>689--696</pages>
<institution>Association for Computational Linguistics.</institution>
<contexts>
<context position="6981" citStr="Qazvinian and Radev 2008" startWordPosition="1105" endWordPosition="1108">i-document scientific articles summarization: MEAD, MEAD with corpus specific vocabulary, LexRank and W3SS. Other studies investigate mainly on the singledocument scientific article summarization. Early works including (Luhn 1958; Baxendale 1958; Edumundson 1969) tried to use various features specific to scientific text (e.g., sentence position, or rhetorical clues features). They have proved that these features are effective for the scientific article summarization. Citation information has been already shown effective in summarize the scientific articles. Works including (Mei and Zhai 2008; Qazvinian and Radev 2008; Schwartz and Hearst 2006; Mohammad et al., 2009) employed citation information for the single scientific article summarization. Earlier work (Nakov et al., 2004) indicated that citation sentences may contain important concepts that can give useful descriptions of a paper. Various methods have been proposed for news document summarization, including rule-based methods (Barzilay and Elhadad 1997; Marcu and Daniel 1997), graph-based methods (Mani and Bloedorn 2000; Erkan and Radev 2004; Michalcea and Tarau 2005), learning-based methods (Conroy et al., 2001; Shen et al., 2007; Ouyang et al., 200</context>
<context position="28123" citStr="Qazvinian and Radev, 2008" startWordPosition="4663" endWordPosition="4666">alidation data. We compare our system with five baseline systems: MEAD-WT, LexRank-WT, ARWG-WT, MEAD and LexRank. MEAD 6 (Radev et al., 2004) is an open-source extractive multidocument summarizer. LexRank 7 (Eran and Radev, 2004) is a multi-document summarization system which is based on a random walk on the similarity graph of sentences. We also implement the MEAD, LexRank baselines and our method 5 www-01.ibm.com/software/integration/optimization/cplexoptimizer/ 6 http://www.summarization.com/mead/ 7 In our experiments, LexRank performs much better than the more complex variant - C-LexRank (Qazvinian and Radev, 2008), and thus we choose LexRank, rather than CLexRank, to represent graph-based summarization methods for comparison in this paper. with only the reference papers (i.e. the target paperâ€™s content is not considered). Those methods are signed by â€œ-WTâ€. To evaluate the effectiveness of the SVR models we employ, we implement a baseline system RWGOF that uses the random walk scores as the important scores of the sentences and take the scores as inputs for the same global optimization framework as our system to generate the related work section. The random walk scores are computed for the sentences in </context>
</contexts>
<marker>Qazvinian, Radev, 2008</marker>
<rawString>Vahed Qazvinian, and Dragomir R. Radev. 2008. Scientific paper summarization using citation summary networks. In Proceedings of the 22nd International Conference on Computational LinguisticsVolume 1, pp. 689-696. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>You Ouyang</author>
<author>Sujian Li</author>
<author>Wenjie Li</author>
</authors>
<title>Developing learning strategies for topic-based summarization.</title>
<date>2007</date>
<booktitle>In Proceedings of the sixteenth ACM conference on Conference on information and knowledge management,</booktitle>
<pages>79--86</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="7582" citStr="Ouyang et al., 2007" startWordPosition="1195" endWordPosition="1198">ian and Radev 2008; Schwartz and Hearst 2006; Mohammad et al., 2009) employed citation information for the single scientific article summarization. Earlier work (Nakov et al., 2004) indicated that citation sentences may contain important concepts that can give useful descriptions of a paper. Various methods have been proposed for news document summarization, including rule-based methods (Barzilay and Elhadad 1997; Marcu and Daniel 1997), graph-based methods (Mani and Bloedorn 2000; Erkan and Radev 2004; Michalcea and Tarau 2005), learning-based methods (Conroy et al., 2001; Shen et al., 2007; Ouyang et al., 2007; Galanis et al., 2008), optimizationbased methods (McDonald 2007; Gillick et al., 2009; Xie et al., 2009; Berg-Kirkpatrick et al., 2011; Lei Huang et al., 2011; Woodsend et al., 2012; Galanis 2012), etc. The most relevant work is (Hoang and Kan, 2010) as mentioned above. They also assumed the set of reference papers was given as part of the input. They also adopt the hierarchical topic tree that describes the topic structure in the target paper as an essential input for their system. However, it is non-trivial to build the hierarchical topic tree. Moreover, they do not consider the content of</context>
</contexts>
<marker>Ouyang, Li, Li, 2007</marker>
<rawString>You Ouyang, Sujian Li, and Wenjie Li. 2007. Developing learning strategies for topic-based summarization. In Proceedings of the sixteenth ACM conference on Conference on information and knowledge management, pp. 79-86. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dragomir Radev</author>
<author>Timothy Allison</author>
<author>Sasha BlairGoldensohn</author>
<author>John Blitzer</author>
<author>Arda Celebi</author>
<author>Stanko Dimitrov</author>
<author>Elliott Drabek</author>
</authors>
<title>MEAD-a platform for multidocument multilingual text summarization.</title>
<date>2004</date>
<booktitle>Proceedings of the 4th International Conference on Language Resources and Evaluation (LREC</booktitle>
<contexts>
<context position="27638" citStr="Radev et al., 2004" startWordPosition="4598" endWordPosition="4601">aset. We train two SVR regression models based on the own work part and the previous work part of the training data and apply the models to the test data. The global optimization framework is used to generate the related work sections. We set the maximum word count of the generated related work section to be equal to that of the gold related work section. The parameter values of A1, A2 and A3 are set to 0.3, 0.1 and 0.6, respectively. The parameter values are tuned on the validation data. We compare our system with five baseline systems: MEAD-WT, LexRank-WT, ARWG-WT, MEAD and LexRank. MEAD 6 (Radev et al., 2004) is an open-source extractive multidocument summarizer. LexRank 7 (Eran and Radev, 2004) is a multi-document summarization system which is based on a random walk on the similarity graph of sentences. We also implement the MEAD, LexRank baselines and our method 5 www-01.ibm.com/software/integration/optimization/cplexoptimizer/ 6 http://www.summarization.com/mead/ 7 In our experiments, LexRank performs much better than the more complex variant - C-LexRank (Qazvinian and Radev, 2008), and thus we choose LexRank, rather than CLexRank, to represent graph-based summarization methods for comparison i</context>
</contexts>
<marker>Radev, Allison, BlairGoldensohn, Blitzer, Celebi, Dimitrov, Drabek, 2004</marker>
<rawString>Dragomir Radev, Timothy Allison, Sasha BlairGoldensohn, John Blitzer, Arda Celebi, Stanko Dimitrov, Elliott Drabek et al. 2004. MEAD-a platform for multidocument multilingual text summarization. Proceedings of the 4th International Conference on Language Resources and Evaluation (LREC 2004).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ariel S Schwartz</author>
<author>Marti Hearst</author>
</authors>
<title>Summarizing key concepts using citation sentences.</title>
<date>2006</date>
<booktitle>In Proceedings of the Workshop on Linking Natural Language Processing and Biology: Towards Deeper Biological Literature Analysis,</booktitle>
<pages>134--135</pages>
<institution>Association for Computational Linguistics.</institution>
<contexts>
<context position="7007" citStr="Schwartz and Hearst 2006" startWordPosition="1109" endWordPosition="1112">cles summarization: MEAD, MEAD with corpus specific vocabulary, LexRank and W3SS. Other studies investigate mainly on the singledocument scientific article summarization. Early works including (Luhn 1958; Baxendale 1958; Edumundson 1969) tried to use various features specific to scientific text (e.g., sentence position, or rhetorical clues features). They have proved that these features are effective for the scientific article summarization. Citation information has been already shown effective in summarize the scientific articles. Works including (Mei and Zhai 2008; Qazvinian and Radev 2008; Schwartz and Hearst 2006; Mohammad et al., 2009) employed citation information for the single scientific article summarization. Earlier work (Nakov et al., 2004) indicated that citation sentences may contain important concepts that can give useful descriptions of a paper. Various methods have been proposed for news document summarization, including rule-based methods (Barzilay and Elhadad 1997; Marcu and Daniel 1997), graph-based methods (Mani and Bloedorn 2000; Erkan and Radev 2004; Michalcea and Tarau 2005), learning-based methods (Conroy et al., 2001; Shen et al., 2007; Ouyang et al., 2007; Galanis et al., 2008), </context>
</contexts>
<marker>Schwartz, Hearst, 2006</marker>
<rawString>Ariel S. Schwartz, and Marti Hearst. 2006. Summarizing key concepts using citation sentences. In Proceedings of the Workshop on Linking Natural Language Processing and Biology: Towards Deeper Biological Literature Analysis, pp. 134-135. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dou Shen</author>
<author>Jian-Tao Sun</author>
<author>Hua Li</author>
<author>Qiang Yang</author>
<author>Zheng Chen</author>
</authors>
<title>Document Summarization Using Conditional Random Fields.</title>
<date>2007</date>
<booktitle>In IJCAI,</booktitle>
<volume>7</volume>
<pages>2862--2867</pages>
<contexts>
<context position="7561" citStr="Shen et al., 2007" startWordPosition="1191" endWordPosition="1194">d Zhai 2008; Qazvinian and Radev 2008; Schwartz and Hearst 2006; Mohammad et al., 2009) employed citation information for the single scientific article summarization. Earlier work (Nakov et al., 2004) indicated that citation sentences may contain important concepts that can give useful descriptions of a paper. Various methods have been proposed for news document summarization, including rule-based methods (Barzilay and Elhadad 1997; Marcu and Daniel 1997), graph-based methods (Mani and Bloedorn 2000; Erkan and Radev 2004; Michalcea and Tarau 2005), learning-based methods (Conroy et al., 2001; Shen et al., 2007; Ouyang et al., 2007; Galanis et al., 2008), optimizationbased methods (McDonald 2007; Gillick et al., 2009; Xie et al., 2009; Berg-Kirkpatrick et al., 2011; Lei Huang et al., 2011; Woodsend et al., 2012; Galanis 2012), etc. The most relevant work is (Hoang and Kan, 2010) as mentioned above. They also assumed the set of reference papers was given as part of the input. They also adopt the hierarchical topic tree that describes the topic structure in the target paper as an essential input for their system. However, it is non-trivial to build the hierarchical topic tree. Moreover, they do not co</context>
</contexts>
<marker>Shen, Sun, Li, Yang, Chen, 2007</marker>
<rawString>Dou Shen, Jian-Tao Sun, Hua Li, Qiang Yang, and Zheng Chen. 2007. Document Summarization Using Conditional Random Fields. In IJCAI, vol. 7, pp. 2862-2867.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
<author>Klaus Ries</author>
<author>Noah Coccaro</author>
<author>Elizabeth Shriberg</author>
<author>Rebecca Bates</author>
<author>Daniel Jurafsky</author>
<author>Paul Taylor</author>
<author>Rachel Martin</author>
<author>Carol Van EssDykema</author>
<author>Marie Meteer</author>
</authors>
<title>Dialogue act modeling for automatic tagging and recognition of conversational speech.</title>
<date>2000</date>
<journal>Computational linguistics</journal>
<volume>26</volume>
<pages>339--373</pages>
<marker>Stolcke, Ries, Coccaro, Shriberg, Bates, Jurafsky, Taylor, Martin, Van EssDykema, Meteer, 2000</marker>
<rawString>Andreas Stolcke, Klaus Ries, Noah Coccaro, Elizabeth Shriberg, Rebecca Bates, Daniel Jurafsky, Paul Taylor, Rachel Martin, Carol Van EssDykema, and Marie Meteer. 2000. Dialogue act modeling for automatic tagging and recognition of conversational speech. Computational linguistics 26, no. 3: 339-373.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristian Woodsend</author>
<author>Mirella Lapata</author>
</authors>
<title>Multiple aspect summarization using integer linear programming.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>233--243</pages>
<institution>Association for Computational Linguistics.</institution>
<marker>Woodsend, Lapata, 2012</marker>
<rawString>Kristian Woodsend, and Mirella Lapata. 2012. Multiple aspect summarization using integer linear programming. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pp. 233-243. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shasha Xie</author>
<author>Benoit Favre</author>
<author>Dilek Hakkani-TÃ¼r</author>
<author>Yang Liu</author>
</authors>
<title>Leveraging sentence weights in a concept-based optimization framework for extractive meeting summarization.</title>
<date>2009</date>
<booktitle>In INTERSPEECH,</booktitle>
<pages>1503--1506</pages>
<contexts>
<context position="7687" citStr="Xie et al., 2009" startWordPosition="1212" endWordPosition="1215">ingle scientific article summarization. Earlier work (Nakov et al., 2004) indicated that citation sentences may contain important concepts that can give useful descriptions of a paper. Various methods have been proposed for news document summarization, including rule-based methods (Barzilay and Elhadad 1997; Marcu and Daniel 1997), graph-based methods (Mani and Bloedorn 2000; Erkan and Radev 2004; Michalcea and Tarau 2005), learning-based methods (Conroy et al., 2001; Shen et al., 2007; Ouyang et al., 2007; Galanis et al., 2008), optimizationbased methods (McDonald 2007; Gillick et al., 2009; Xie et al., 2009; Berg-Kirkpatrick et al., 2011; Lei Huang et al., 2011; Woodsend et al., 2012; Galanis 2012), etc. The most relevant work is (Hoang and Kan, 2010) as mentioned above. They also assumed the set of reference papers was given as part of the input. They also adopt the hierarchical topic tree that describes the topic structure in the target paper as an essential input for their system. However, it is non-trivial to build the hierarchical topic tree. Moreover, they do not consider the content of the target paper to construct the related work section, which is actually crucial in the related work se</context>
</contexts>
<marker>Xie, Favre, Hakkani-TÃ¼r, Liu, 2009</marker>
<rawString>Shasha Xie, Benoit Favre, Dilek Hakkani-TÃ¼r, and Yang Liu. 2009. Leveraging sentence weights in a concept-based optimization framework for extractive meeting summarization. In INTERSPEECH, pp. 1503-1506.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hui Yang</author>
<author>Jamie Callan</author>
</authors>
<title>A metric-based framework for automatic taxonomy induction.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume</booktitle>
<volume>1</volume>
<pages>271--279</pages>
<institution>Association for Computational Linguistics.</institution>
<contexts>
<context position="9613" citStr="Yang and Callan 2009" startWordPosition="1516" endWordPosition="1519">present word contexts as vectors and cluster words based on similarities of the vectors Many clustering-based approaches face the challenge of an incremental clustering approach,... The advantage of the incremental approach is that it eliminates the trouble of inventing cluster labels and concentrates on placing terms in the correct positions in a taxonomy hierarchy. The work by Snow et al. (2006) is the most similar to ours ... Moreover, our approach employs heterogeneous features from a wide range; while their approach only used syntactic dependency. Figure 1: A sample related work section (Yang and Callan 2009) appropriately labeling non-leaf clusters. ... In this paper, we take taxonomy induction. ... of attention to the most common is-a relations. ... (Brown et al., 1992; Lin, 1998). ... lated work sections. A specific related work example is shown in Figure 1. This related work section introduces previous related works for a paper on Automatic Taxonomy Induction. From Figure 1, we can have a glance at the structure of related work sections. Related work sections usually discuss several different topics, such as â€œpattern-basedâ€ and â€œcluster-basedâ€ approaches shown in the Figure 1. Besides the know</context>
</contexts>
<marker>Yang, Callan, 2009</marker>
<rawString>Hui Yang, and Jamie Callan. 2009. A metric-based framework for automatic taxonomy induction. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 1-Volume 1, pp. 271-279. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ozge Yeloglu</author>
<author>Evangelos Milios</author>
<author>Nur ZincirHeywood</author>
</authors>
<title>Multi-document summarization of scientific corpora.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 ACM Symposium on Applied Computing,</booktitle>
<pages>252--258</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="6313" citStr="Yeloglu et al., (2011)" startWordPosition="1014" endWordPosition="1017">perâ€™s topic. They used two different rule-based strategies to extract sentences for general topics as well as detailed ones. A few studies focus on multi-document scientific article summarization. Agarwal et al., (2011) introduced an unsupervised approach to the problem of multi-document summarization. The input is a list of papers cited together within the same source article. The key point of this approach is a topic based clustering of fragments extracted from each co-cited article. They rank all the clusters using a query generated from the context surrounding the co-cited list of papers. Yeloglu et al., (2011) compared four different approaches for multi-document scientific articles summarization: MEAD, MEAD with corpus specific vocabulary, LexRank and W3SS. Other studies investigate mainly on the singledocument scientific article summarization. Early works including (Luhn 1958; Baxendale 1958; Edumundson 1969) tried to use various features specific to scientific text (e.g., sentence position, or rhetorical clues features). They have proved that these features are effective for the scientific article summarization. Citation information has been already shown effective in summarize the scientific ar</context>
</contexts>
<marker>Yeloglu, Milios, ZincirHeywood, 2011</marker>
<rawString>Ozge Yeloglu, Evangelos Milios, and Nur ZincirHeywood. 2011. Multi-document summarization of scientific corpora. In Proceedings of the 2011 ACM Symposium on Applied Computing, pp. 252-258. ACM.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.987722">
Large-scale Expected BLEU Training of Phrase-based Reordering Models
</title>
<author confidence="0.996232">
Michael Auli, Michel Galley, Jianfeng Gao
</author>
<affiliation confidence="0.969125">
Microsoft Research
</affiliation>
<address confidence="0.981444">
Redmond, WA, USA
</address>
<email confidence="0.996158">
{michael.auli,mgalley,jfgao}@microsoft.com
</email>
<sectionHeader confidence="0.997353" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999739894736842">
Recent work by Cherry (2013) has shown
that directly optimizing phrase-based re-
ordering models towards BLEU can lead
to significant gains. Their approach is lim-
ited to small training sets of a few thou-
sand sentences and a similar number of
sparse features. We show how the ex-
pected BLEU objective allows us to train
a simple linear discriminative reordering
model with millions of sparse features on
hundreds of thousands of sentences re-
sulting in significant improvements. A
comparison to likelihood training demon-
strates that expected BLEU is vastly more
effective. Our best results improve a hi-
erarchical lexicalized reordering baseline
by up to 2.0 BLEU in a single-reference
setting on a French-English WMT 2012
setup.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999870050847458">
Modeling reordering for phrase-based machine
translation has been a long standing problem.
Contrary to synchronous context free grammar-
based translation models (Wu, 1997; Galley et al.,
2004; Galley et al., 2006; Chiang, 2007), phrase-
based models (Koehn et al., 2003; Och and Ney,
2004) have no in-built notion of reordering beyond
what is captured in a single phrase pair, and the
first phrase-based decoders simply scored inter-
phrase reorderings using a restricted linear dis-
tortion feature, which scores a phrase reordering
proportionally to the length of its displacement.
While phrase-based models allow in theory com-
pletely unrestricted reordering patterns, move-
ments are generally limited to a finite distance for
complexity reasons. To address this limitation,
extensive prior work focused on richer feature
sets, in particular on lexicalized reordering mod-
els trained with maximum likelihood-based ap-
proaches (Tillmann, 2003; Xiong et al., 2006; Gal-
ley and Manning, 2008; Nguyen et al.,2009;§2).
More recently, Cherry (2013) proposed a very
effective sparse ordering model relying on a set
of only a few thousand indicator features which
are trained towards a task-specific metric such as
BLEU (Papineni et al., 2002). These features
are simply added to the log-linear framework of
translation that is trained with the Margin Infused
Relaxed Algorithm (MIRA; Chiang et al., 2009)
on a small development set of a few thousand
sentences. While simple, the approach outper-
forms the state-of-the-art hierarchical reordering
model of Galley and Manning (2008), a maximum
likelihood-based model trained on millions of sen-
tences to fit millions of parameters.
Ideally, we would like to scale sparse reorder-
ing models to similar dimensions but recent at-
tempts to increase the amount of training data for
MIRA was met with little success (Eidelman et
al., 2013). In this paper we propose much larger
sparse ordering models that combine the scalabil-
ity of likelihood-based approaches with the higher
accuracy of maximum BLEU training (§3). We
train on the output of a hierarchical reordering
model-based system and scale to millions of fea-
tures learned on hundreds of thousands of sen-
tences (§4). Specifically, we use the expected
BLEU objective function (Rosti et al., 2010; Rosti
et al., 2011; He and Deng, 2012; Gao and He,
2013; Gao et al., 2014; Green et al., 2014) which
allows us to train models that use training data and
feature sets that are two to three orders of magni-
tudes larger than in previous work (§5).
Our models significantly outperform the
state-of-the-art hierarchical lexicalized reordering
model on two language pairs and we demonstrate
that richer feature sets result in significantly
higher accuracy than with a feature set similar
to Cherry (2013). We also demonstrate that our
</bodyText>
<page confidence="0.899522">
1250
</page>
<note confidence="0.89662">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1250–1260,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999767846153846">
approach greatly benefits from more training
data than is typically used for maximum BLEU
training. Previous work concluded that sparse
reordering models perform better than maximum
entropy models, however, the two approaches
do not only differ in the objective function but
also the type of training data (Cherry, 2013). Our
analysis isolates the objective function and shows
that expected BLEU optimization is the most
important factor to train accurate ordering models.
Finally, we compare expected BLEU training to
pair-wise ranked optimization (PRO) on a feature
set similar to Cherry (2013; §7).
</bodyText>
<sectionHeader confidence="0.998074" genericHeader="introduction">
2 Reordering Models
</sectionHeader>
<bodyText confidence="0.999898625">
Reordering models for phrase-based translation
are typically part of the log-linear framework
which forms the basis of many statistical machine
translation systems (Och and Ney, 2004).
Formally, we are given K training pairs D =
(f(1), e(1))...(f(K), e(K)), where each f(z) E F
is drawn from a set of possible foreign sentences,
and each English sentence e(z) E E(f(z)) is drawn
from a set of possible English translations of f(z).
The log-linear model is parameterized by m pa-
rameters B where each Bk E B is the weight of
an associated feature hk(f, e) such as a language
model or a reordering model. Function h(f, e)
maps foreign and English sentences to the vector
h1(f, e)...h.(f, e), and we usually choose trans-
lations eˆ according to the following decision rule:
</bodyText>
<equation confidence="0.9845905">
eˆ = arg max BTh(f,e) (1)
e E £(f)
</equation>
<bodyText confidence="0.993446214285714">
In practice, computing eˆ exactly is intractable and
we resort to an approximate but more efficient
beam search (Och and Ney, 2004).
Early phrase-based models simply relied on a
linear distortion feature, which measures the dis-
tance between the first word of the current source
phrase and the last word of the previous source
phrase (Koehn et al., 2003; Och and Ney, 2004).
Unfortunately, this approach is agnostic to the ac-
tual phrases being reordered, and does not take
into account that certain phrases are more likely
to be reordered than others. This shortcoming led
to a range of lexicalized reordering models that
capture exactly those preferences for individual
phrases (Tillmann, 2003; Koehn et al., 2007).
Reordering models generally assume a se-
quence of English phrases e = {¯e1, ... , ¯e,,,} cur-
rently hypothesized by the decoder, a phrase align-
ment a = {a1, ... , a,,,} that defines a foreign
phrase ¯fa,, for each English phrase ¯ez, and an ori-
entation oz which describes how a phrase pair
should be reordered with respect to the previous
phrases. There are typically three orientation types
and the exact definition depends on the specific
models which we describe below. Orientations can
be determined during decoding and from word-
aligned training corpora. Most models estimate
a probability distribution p(oz|ppz, a1, ... , az) for
the i-th phrase pair ppz = (¯ez, ¯fa,,) and the align-
ments a1, ... , az of the previous target phrases.
Lexicalized Reordering. This model defines the
three orientation types based only on the posi-
tion of the current and previously translated source
phrase az and az−1, respectively (Tillmann, 2003;
Koehn et al., 2007). The orientation types gen-
erally are: monotone (M), indicating that az−1 is
directly followed by az. swap (S) assumes that az
precedes az−1, i.e., the two phrases swap places.
Finally, discontinuous (D) indicates that az is not
adjacent to az−1. The probability distribution over
these reordering events is based on a maximum
likelihood estimate:
</bodyText>
<equation confidence="0.998688">
p(o|pp,az−1,az) = cnt (o, pp)cnt(pp) (2)
</equation>
<bodyText confidence="0.999855217391304">
where o E {M, S, D} and cnt returns smoothed
frequency counts over a word-aligned corpus.
Hierarchical Reordering. An extension of the
lexicalized reordering model better handles long-
distance reordering by conditioning the orientation
of the current phrase on a context larger than just
the previous phrase (Galley and Manning, 2008).
In particular, the hierarchical reordering model
does so by building a compact representations
of the preceding context using an efficient shift-
reduce parser. During translation new phrases get
moved on a stack and are then combined with any
previous phrase if they are adjacent. Figure 1
shows an illustrative example: when the decoder
shifts phrase pp8 onto the stack, this phrase is then
merged with pp7 (reduce operation), which then
can be merged with previous phrases to finally
form a hierarchical block h1. These merge opera-
tions stop once we reach a phrase (here, pp3) that
is not contiguous with the current block. Then, as
another phrase (pp9) is hypothesized, the decoder
uses the hierarchical block at the top of the stack
(h1) to determine the orientation of the current
</bodyText>
<page confidence="0.847779">
1251
</page>
<figure confidence="0.2829845">
input sentence
7i tM ft4 - Arim a );K fix 5 PW *IT fA
</figure>
<bodyText confidence="0.999785777777778">
were generated by unsupervised methods. Instead,
he proposes to learn a discriminative reordering
model based on the outputs of the actual machine
translation system, adjusting the feature weights
to maximize a task-specific objective, which is
BLEU in their case. Their model is based on a
set of sparse features derived from the hierarchi-
cal reordering model which we scale to millions
of features (§6).
</bodyText>
<sectionHeader confidence="0.671553" genericHeader="method">
3 A Simple Linear Reordering Model
</sectionHeader>
<bodyText confidence="0.998941">
Our reordering model is defined as a simple linear
model over the basic orientation types, similar to
Cherry (2013). In particular, our model defines
score sφ(o, e, f) over orientations o = {M, S, D},
and a sentence pair {e, f, a} with alignment a as a
linear combination of weighted indicator features:
</bodyText>
<equation confidence="0.995489">
sφ(o, e, f, a) = φTu(o, e, f, a)
=
=
I
i=1
I
i=1
φTu(o, ppi, ci)
sφ(o,ppi,ci) (3)
</equation>
<bodyText confidence="0.999850857142857">
where φ is a vector of weights, {ppi}Ii=1 is a
set of phrases that decompose the sentence pair
{e, f, a}, and u(o, ppi, ci) is a function that maps
orientation o, phrase pair ppi and local context ci
to a sparse vector of indicator features. The lo-
cal context ci represents information used by the
model that is in addition to the phrase pair. For
example, the features of Cherry (2013) condition
on the top-stack of the hierarchical shift reduce
parser, information that is non-local with respect
to the phrase pair. In our experiments, we use fea-
tures that go beyond the top-stack, in order to con-
dition on various parts of the source and target side
contexts (§7).
</bodyText>
<sectionHeader confidence="0.924902" genericHeader="method">
4 Model Training
</sectionHeader>
<bodyText confidence="0.999765">
Optimization of our model is based on standard
stochastic gradient descent (SGD; Bottou, 2004)
with an expected BLEU loss l(φ) which we detail
next (§5). The update is:
</bodyText>
<equation confidence="0.992768666666667">
∂l(φt−1)
φt = φt−1 − µ(4)
∂φt−1
</equation>
<bodyText confidence="0.99903625">
where φt and φt−1 are model weights at time t and
t − 1 respectively, and µ is a learning rate.
We add the model as a small number of dense
features to the log-linear framework of translation
</bodyText>
<figure confidence="0.990005170212768">
the
russian
side
hopes
to
hold
consultations
with
iran
on
this
issue
in
the
near
future
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
pp3.
.
.
.
.
.
.
.
.
.
.
h1.
.
.
.
.
.
.
.
.
.
.
decoded output
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
pp7
.
.
.
.
.
.
.
.
.
.
. .ppe
.
.
.
.
.
.
.
.
.
.
pp9
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
</figure>
<figureCaption confidence="0.99998">
Figure 1: The hierarchical reordering model
</figureCaption>
<bodyText confidence="0.961789666666667">
(HRM) analyzes a non-local context to determine
the orientation of the current phrase. For exam-
ple, the phrase pair pp9 has a swap orientation
(o9 = S) with respect to a hierarchical block (h1)
that comprises the five preceding phrase pairs.
phrase pp9, which in this case is a swap (S) orien-
tation.1 The model has the advantage that the ori-
entations computed are more robust to derivational
ambiguity of the underlying translation model. A
given surface translation may be derived through
different phrases but the shift-reduce parser com-
bines them into a single representation which is
more consistent with the orientations observed in
the word-aligned training data.
Maximum Entropy-based models. The statis-
tics used to estimate the lexicalized and the hierar-
chical reordering models are based on very sparse
estimates, simply because certain phrases are not
very frequent. Maximum entropy models address
this problem by estimating Eq. 2 through sparse
indicator features over phrase pairs instead, but
prior work with such models still relies on word
aligned corpora for estimation (Xiong et al., 2006;
Nguyen et al., 2009). However, recent evalua-
tions of the approach show little gain over the sim-
pler frequency-based estimation method (Cherry,
2013).
Sparse Hierarchical Reordering model. All of
the models so far are trained to maximize the like-
lihood of reordering decisions observed in word
aligned corpora. Cherry (2013) argues that it
is probably too difficult to learn human reorder-
ing patterns through noisy word alignments that
</bodyText>
<footnote confidence="0.5672545">
1Galley and Manning (2008) provide a more formal ex-
planation.
</footnote>
<page confidence="0.982347">
1252
</page>
<bodyText confidence="0.99922925">
(Eq. 1). Specifically, we extend the m baseline
features by a set of new features hm+1, ... , hm+j,
where each represents a linear combination of
sparse indicator features corresponding to one of
the orientation types. Exposing each orientation
as a separate dense feature within the log-linear
model is common practice for lexicalized reorder-
ing models (Koehn et al., 2005):
</bodyText>
<equation confidence="0.581372">
hm+j = sφ(oj, e, f, a)
</equation>
<bodyText confidence="0.9959035">
where oj E {M, S, D}.
The translation model is then parameterized by
both θ, the log-linear weights of the baseline fea-
tures, as well as φ, the weights of the reordering
model. The reordering model is learned as follows
(Gao and He, 2013; Gao et al., 2014):
</bodyText>
<listItem confidence="0.994536666666667">
1. We first train a baseline translation system to
learn θ, without the discriminative reordering
model, i.e., we set θm+1 = 0, ... , θm+j = 0.
2. Using these weights, we generate n-best lists
for the foreign sentences in the training data
using the setup described in the experimental
section (§7). The n-best lists serve as an ap-
proximation to £(f), the set of possible trans-
lations of f, used in the next step for expected
BLEU training of the reordering model (§5).
3. Next, we fix θ, set θm+1 = 1,... θm+j = 1
and optimize φ with respect to the loss func-
tion on the training data using stochastic gra-
dient descent.2
4. Finally, we fix φ and re-optimize θ in the
presence of the discriminative reordering
model using Minimum Error Rate Training
(MERT; Och 2003; §7).
</listItem>
<bodyText confidence="0.598740333333333">
We found that re-optimizing θ after a few iter-
ations of stochastic gradient descent in step 3 did
not improve accuracy.
</bodyText>
<sectionHeader confidence="0.999108" genericHeader="method">
5 Expected BLEU Objective Function
</sectionHeader>
<bodyText confidence="0.99735575">
The expected BLEU objective (Gao and He, 2013;
Gao et al., 2014) allows us to efficiently optimize
a large scale discriminative reordering model to-
wards the desired task-specific metric, which in
our setting is BLEU.
2We tuned 0.+1,... 0.+j on the development set but
found that setting them uniformly to one resulted in faster
training and equal accuracy.
Formally, we define our loss function l(φ) as
the negative expected BLEU score, denoted as
xBLEU(φ), for a given foreign sentence f and a
log-linear parameter set θ:
</bodyText>
<equation confidence="0.996781">
l(φ) = − xBLEU(φ)
�= − pθ,φ(e|f) sBLEU(e, e(i)) (5)
eE£(f)
</equation>
<bodyText confidence="0.999813538461539">
where sBLEU(e, e(i)) is a smoothed sentence-
level BLEU score with respect to the reference
translation e(i), and £(f) is the generation set ap-
proximated by an n-best list. In our experiments
we use n-best lists with unique entries and there-
fore our definitions do not take into account mul-
tiple derivations of the same translation. Specif-
ically, our n-best lists are generated by choosing
the highest scoring derivation eˆ amongst string
identical translations e for f. We use a sentence-
level BLEU approximation similar to Gao et al.
(2014).3 Finally, pθ,φ(e|f) is the normalized prob-
ability of translation e given f, defined as:
</bodyText>
<equation confidence="0.990952">
exp{γθTh(f, e)}
pθ,φ(e|f) = (6)
Ee,E£(f) exp{γθTh(f, e&apos;)}
</equation>
<bodyText confidence="0.999953769230769">
where θTh(f, e) includes the discriminative re-
ordering model hm+1(e, f), ... , hm+j(e, f) pa-
rameterized by φ, and γ E [0, inf) is a tuned scal-
ing factor that flattens the distribution for γ &lt; 1
and sharpens it for γ &gt; 1 (Tromble et al., 2008).4
Next, we define the gradient of the expected
BLEU loss function l(φ). To simplify our notation
we omit the local context c in sφ(o, pp, c) (Eq. 3)
from now on and assume it to be part of pp. Us-
ing the observation that the loss does not explicitly
depend on φ, we get:
where δo,pp is the error term for orientation o of
phrase pair pp:
</bodyText>
<equation confidence="0.9751095">
∂l(φ)
δo,pp = − ∂sφ(o, pp)
</equation>
<footnote confidence="0.99520425">
3We found in early experiments that the BLEU+1 approx-
imation used by Liang et al. (2006) and Nakov et. al (2012)
worked equally well in our setting.
4γ is only used during expected BLEU training.
</footnote>
<equation confidence="0.997728222222222">
∂l(φ) _ E
∂φ o,pp
�=
o,pp
∂l(φ)
∂sφ(o, pp)
∂sφ(o, pp)
∂φ
−δo,ppu(o, pp)
</equation>
<page confidence="0.794983">
1253
</page>
<bodyText confidence="0.9899026">
The error term indicates how the expected BLEU
loss changes with the reordering score which we
derive in the next section.
Finally, the gradient of the reordering score
sφ(o, pp) with respect to φ is simply given by this:
</bodyText>
<equation confidence="0.867681333333333">
∂sφ(o, pp)
∂φ ∂φ = u(o,pp)
∂φTu(o,pp)
</equation>
<subsectionHeader confidence="0.987766">
5.1 Derivation of the Error Term δo,pp
</subsectionHeader>
<bodyText confidence="0.9999675">
We rewrite the loss function (Eq. 5) using Eq. 6
and separate it into two terms G(φ) and Z(φ):
</bodyText>
<equation confidence="0.99805485">
G(φ)
l(φ) = −xBLEU(φ) = −(7)
Z(φ)
= Pe∈E(f) exp{γθTh(f, e)} sBLEU(e, e(i))
Pe,∈E(f) exp{γθTh(f, e0)}
Next, we apply the quotient rule of differentiation:
∂xBLEU(φ) = ∂(G(φ)/Z(φ))
δo,pp = ∂sφ(o, pp) ∂sφ(o, pp)
� ∂G(φ) �
1 ∂sφ(o, pp) − ∂Z(φ)
∂sφ(o, pp)xBLEU(φ)
Z(φ)
The gradients for G(φ) and Z(φ) with respect to
sφ(o, pp) are:
∂G(φ)
∂sφ(o, pp)
∂ exp{γθTh(f, e)}
∂sφ(o, pp)
∂ exp{γθTh(f, e)}
∂sφ(o, pp)
</equation>
<bodyText confidence="0.940097">
By using the following definition:
</bodyText>
<equation confidence="0.872863">
U(φ, e) = sBLEU(e, e(i)) − xBLEU(φ)
</equation>
<bodyText confidence="0.956173">
together with the chain rule, Eq. 6 and Eq. 7, we
can rewrite δo,pp as follows:
</bodyText>
<equation confidence="0.9605346">
�
1 X �∂ exp{γθTh(f, e)}
δo,pp = ∂sφ(o, pp) U(φ, e)
Z(φ)
e∈E(f)
� �
Xpθ,φ(e|f)∂γθTh(f, e) =U(φ, e)
∂sφ(o, pp)
e∈E(f)
Because φ is only relevant to the reordering
model, represented by hm+1, ... , hm+j, we have:
∂γθTh(f, e)
∂sφ(o, pp) = γλk ∂sφ(o, pp)
∂hk(e, f)
= γλkN(o, pp, e, f)
</equation>
<listItem confidence="0.775209263157895">
1: function TRAINSGD(D, µ)
2: t ← 0
3: for all (f(i), e(i)) in D do
4: xBLEU = 0 . Compute xBLEU
5: for all e in E(f(i)) do
6: wBLEU ← pθ,φt(e|f) sBLEU(e, e(i))
7: xBLEU ← xBLEU + wBLEU
8: end for
9: for all e in E(f(i)) do
10: D = sBLEU(e, e(i)) − xBLEU
11: for all o, pp in he, f(i)i do
12: N = N (o, pp, e, f)
13: δo,pp = pθ,φt(e|f(i))γλkND
14: φt+1 = φt − µδo,ppu(o,pp))
15: end for
16: end for
17: t ← t + 1
18: end for
19: end function
</listItem>
<figureCaption confidence="0.990428666666667">
Figure 2: Algorithm for computing the expected
BLEU loss with SGD updates (Eq. 4) based on
training data D and learning rate µ.
</figureCaption>
<bodyText confidence="0.99918975">
where m + 1 ≤ k ≤ m + j and N(o, pp, e, f) is
the number of times pp with orientation o occurs
in the current sentence pair.
This simplifies the error term to:
</bodyText>
<equation confidence="0.994707">
Xδo,pp = pθ,φ(e|f)γλkN(o,pp, e, f)U(φ, e)
e∈E(f)
(8)
</equation>
<bodyText confidence="0.999918428571429">
where λk is the weight of the dense feature sum-
marizing orientation o in the log-linear model. We
use Eq. 8 in a simple algorithm to train our model
(Figure 2). Our SGD trainer uses a mini-batch size
of a single sentence (§7) which entails all hypoth-
esis in the n-best list for this sentence and the pa-
rameters are updated after each mini-batch.
</bodyText>
<sectionHeader confidence="0.996912" genericHeader="method">
6 Feature Sets
</sectionHeader>
<bodyText confidence="0.999205833333333">
Our features are inspired by Cherry (2013)
who bases his features on the local phrase-pair
pp = h¯e, fi as well as the top stack of the shift re-
duce parser of the baseline hierarchical ordering
model. We experiment with these variants and ex-
tensions:
</bodyText>
<listItem confidence="0.992561">
• SparseHRMLocal: This feature set is exclu-
sively based on the local phrase-pair and
</listItem>
<equation confidence="0.976388">
X sBLEU(e, e(i))
e∈E(f)
∂Z(φ) X
∂sφ(o, pp) e∈E(f)
</equation>
<page confidence="0.957902">
1254
</page>
<bodyText confidence="0.999513555555556">
consists of features over the first and last
word of both the source and target phrase.5
We use four different word representations:
The word identity itself, but only for the
80 most common source and target language
words. The three other word representations
are based on Brown clustering with either 20,
50 or 80 classes (Brown et al., 1992). There
is one feature for every orientation type.
</bodyText>
<listItem confidence="0.8183195">
• SparseHRM: The main feature set of Cherry
(2013). This is an extension of SparseHRM-
</listItem>
<bodyText confidence="0.781903833333333">
Local adding features based on the first and
last word of both the source and the target of
the hierarchical block at the top of the stack.
There are also features based on the source
words in-between the current phrase and the
hierarchical block at the top of the stack.
</bodyText>
<listItem confidence="0.990889166666667">
• SparseHRM+UncommonWords: This set is
identical to SparseHRM, except that word-
identity features are not restricted to the 80
most frequent words, but can be instantiated
for all words, regardless of frequency.
•
</listItem>
<bodyText confidence="0.9982121875">
SparseHRM+BiPhrases: This augments
SparseHRM by phrase-identity features re-
sulting in millions of instances compared to
only a few thousand for SparseHRM. We add
three features for each possible phrase pair:
the source phrase, the target phrase, and the
whole phrase pair.
The baseline hierarchical lexicalized reorder-
ing model is most similar to SparseHRM+BiPhrases
feature set since both have parameters for phrase,
orientation pairs.6 The feature set closest to
Cherry (2013) is SparseHRM. However, while
Cherry had to severely restrict his features for
batch lattice MIRA-based training, our maximum
expected BLEU approach can handle millions of
features.
</bodyText>
<sectionHeader confidence="0.999612" genericHeader="method">
7 Experiments
</sectionHeader>
<bodyText confidence="0.991012180327869">
Baseline. We experiment with a phrase-based
system similar to Moses (Koehn et al., 2007),
5Phrase-local features allow pre-computation which re-
sults in significant speed-ups at run-time. Cherry (2013)
shows that local features are responsible for most of his gains.
6Although, our model is likely to learn significantly fewer
parameters since many phrase, orientation pairs will only be
seen in the word-aligned data but not in actual machine trans-
lation output.
scoring translations by a set of common fea-
tures including maximum likelihood estimates
of source given target phrases pMLE(e|f) and
vice versa, pMLE(f|e), lexically weighted esti-
mates pLW (e|f) and pLW (f|e), word and phrase-
penalties, as well as a linear distortion feature.
The baseline uses a hierarchical reordering model
with five orientation types, including monotone
and swap, described in §2, as well as two discon-
tinuous orientations, distinguishing if the previous
phrase is to the left or right of the current phrase.
Finally, monotone global indicates that all previ-
ous phrases can be combined into a single hier-
archical block. The baseline includes a modified
Kneser-Ney word-based language model trained
on the target-side of the parallel data, which is de-
scribed below. Log-linear weights are estimated
with MERT (Och, 2003). We regard the 1-best
output of the phrase-based decoder with the hierar-
chical reordering model as the baseline accuracy.
Evaluation. We use training and test data from
the WMT 2012 campaign and report results on
French-English and German-English translation
(Callison-Burch et al., 2012). Translation mod-
els are estimated on 102M words of parallel data
for French-English and 91M words for German-
English; between 7.5-8.2M words are newswire,
depending on the language pair, and the remainder
are parliamentary proceedings. All discrimina-
tive reordering models are trained on the newswire
subset since we found this portion of the data to be
most useful in initial experiments. We evaluate on
six newswire domain test sets from 2008, 2010 to
2013 as well as the 2010 system combination test
set containing between 2034 to 3003 sentences.
Log-linear weights are estimated on the 2009 data
set comprising 2525 sentences. We evaluate using
BLEU with a single reference.
Discriminative Reordering Model. We use 100-
best lists generated by the phrase-based decoder
to train the discriminative reordering model. The
n-best lists are generated by ten systems, each
trained on 90% of the available data in order to de-
code the remaining 10%. The purpose of this pro-
cedure is to avoid a bias introduced by generating
n-best lists for sentences on which the translation
model was previously trained.7 Unless otherwise
7Later, we found that the bias has only a negligible effect
on end-to-end accuracy since we obtained very similar results
when decoding with a system trained on all data. This setting
increased the training data BLEU score from 27.5 to 37.8. We
used a maximum source and target phrase length of 7 words.
</bodyText>
<page confidence="0.960667">
1255
</page>
<table confidence="0.985130857142857">
dev 2008 2010 sc2010 2011 2012 2013 AllTest FeatTypes
noRM 23.37 20.18 24.24 24.18 24.83 24.23 24.85 23.93 -
HRM (baseline) 24.11 20.85 24.92 24.83 25.68 25.11 25.76 24.72 -
SparseHRMLocal 25.24 21.26 25.99 25.93 26.98 26.34 26.77 25.77 4,407
SparseHRM 25.29 21.43 26.17 26.14 26.99 26.63 27.01 25.95 9,463
+UncommonWords 25.32 21.76 26.30 26.29 27.15 26.77 27.18 26.12 897,537
+BiPhrases 25.46 21.67 26.19 26.19 27.55 27.07 27.41 26.26 3,043,053
</table>
<tableCaption confidence="0.9699884">
Table 1: French-English results of expected BLEU trained sparse reordering models compared to no
reordering model at all (noRM) and the likelihood trained baseline hierarchical reordering model (HRM)
on WMT test sets; sc2010 is the 2010 system combination test set. FeatTypes is the number of different
types and AllTest is the average BLEU score over all the test sets, weighted by corpus size. All results
for our sparse reordering models include a likelihood-trained hierarchical reordering model.
</tableCaption>
<table confidence="0.999583142857143">
dev 2008 2010 sc2010 2011 2012 2013 AllTest FeatTypes
noRM 18.54 19.28 20.14 20.01 18.90 18.87 21.60 19.81 -
HRM (baseline) 19.35 19.96 20.87 20.66 19.60 19.80 22.48 20.58 -
SparseHRMLocal 19.89 19.86 21.11 20.84 20.04 20.21 22.93 20.88 4,410
SparseHRM 19.83 20.27 21.26 21.05 20.22 20.44 23.17 21.11 9,477
+UncommonWords 20.06 20.35 21.45 21.31 20.28 20.55 23.30 21.24 1,136,248
+BiPhrases 20.09 20.33 21.62 21.47 20.66 20.75 23.27 21.40 3,640,693
</table>
<tableCaption confidence="0.999391">
Table 2: German- English results of expected BLEU trained sparse reordering models (cf. Table 1).
</tableCaption>
<bodyText confidence="0.999989947368421">
mentioned, we train our reordering model on the
news portion of the parallel data, corresponding to
136K-150K sentences, depending on the language
pair. We tuned the various hyper-parameters on a
held-out set, including the learning rate, for which
we found a simple setting of 0.1 to be useful. To
prevent overfitting, we experimented with E2 regu-
larization, but found that it did not improve test ac-
curacy. We also tuned the probability scaling pa-
rameter -y (Eq. 6) but found -y = 1 to be very good
among other settings. We evaluate the perfor-
mance on a held-out validation set during training
and stop whenever the objective changes less than
a factor of 0.0003. For our PRO experiments, we
tuned three hyper-parameters controlling E2 reg-
ularization, sentence-level BLEU smoothing, and
length. The latter is important to eliminate PRO’s
tendency to produce too short translations (Nakov
et al., 2012).
</bodyText>
<subsectionHeader confidence="0.999006">
7.1 Scaling the Feature Set
</subsectionHeader>
<bodyText confidence="0.999835518518518">
We first compare our baseline, a likelihood trained
hierarchical reordering model (HRM; Galley &amp;
Manning, 2008), to various expected BLEU
trained models, starting with SparseHRMLocal,
inspired by Cherry (2013) and compare it to
SparseHRM+BiPhrases, a set that is three orders of
magnitudes larger.
Our results on French-English translation (Ta-
ble 1) and German-English translation (Table 2)
show that the expected BLEU trained models scale
to millions of features and that we outperform the
baseline by up to 2.0 BLEU on newstest2012 for
French-English and by up to 1.1 BLEU on new-
stest2011 for German-English.8 Increasing the
size of the feature set improves accuracy across
the board: The average accuracy over all test sets
improves from 1.0 BLEU for the most basic fea-
ture set to 1.5 BLEU for the largest feature set
on French-English and from 0.3 BLEU to 0.8
BLEU on German-English.9 The most compa-
rable setting to Cherry (2013) is the feature set
SparseHRM, which we outperform by up to 0.5
BLEU on French-English and by 0.3 BLEU on av-
erage on both language pairs, demonstrating the
benefit of being able to effectively train large fea-
ture sets. Furthermore, the increase in the num-
ber of features does not affect runtime, since most
</bodyText>
<footnote confidence="0.648856">
8Different to the setups of Galley &amp; Manning (2008) and
Cherry (2013) our WMT evaluation framework uses only one
instead of four references, which makes our BLEU score im-
provements not directly comparable.
9We attribute smaller improvements on German-English
to the low distortion limit of only six words of our system and
the more difficult reordering patterns when translating from
German which may require more elaborate features.
</footnote>
<page confidence="0.968061">
1256
</page>
<sectionHeader confidence="0.374434" genericHeader="evaluation">
BLEU
</sectionHeader>
<bodyText confidence="0.999439615384615">
features can be pre-computed and stored in the
phrase-table, only requiring a constant time table-
lookup, similar to traditional reordering models.
Another appeal of our approach is that train-
ing is very fast given a set of n-best lists for the
training data. The SparseHRM model with 4,407
features is trained in only 26 minutes, while the
SparseHRM+BiPhrases model with over three mil-
lion parameters can be trained in just over two
hours (136K sentences and 100 epochs in both
cases). We attribute this to the training regime
(§4), which does not iteratively re-decode the
training data for expected BLEU training.10
</bodyText>
<subsectionHeader confidence="0.999577">
7.2 Varying Training Set Size
</subsectionHeader>
<bodyText confidence="0.96411545945946">
Previous work on sparse reordering models was
restricted to small data sets (Cherry, 2013) due
to the limited ability of standard machine trans-
lation optimizers to handle more than a few thou-
sand sentences. In particular, recent attempts to
scale the margin-infused relaxation algorithm, a
variation which was also used by Cherry (2013),
to larger data sets showed that more data does not
necessarily help to improve test set accuracy for
large feature sets (Eidelman et al., 2013).
In the next set of experiments, we shed light on
the advantage of training discriminative reordering
models with expected BLEU on large training sets.
Specifically, we start off by estimating a reorder-
ing model on only 2,000 sentences, similar to the
size of the development set used by Cherry (2013),
and incrementally increase the amount of training
data to nearly three hundred thousand sentences.
To avoid overfitting to small data sets we experi-
ment with our most basic feature set SparseHRM-
Local, comprising of just over 4,400 types.
For this experiment only, we measure accuracy
in a re-ranking framework for faster experimen-
tation where we use the 100-best output of the
baseline system relying on a likelihood-based hi-
erarchical reordering model. We re-estimate the
log-linear weights by running a further iteration of
MERT on the n-best list of the development set
which is augmented by scores corresponding to
the discriminative reordering model. The weights
of those features are initially set to one and we
use 20 random restarts for MERT. At test time we
rescore the 100-best list of the test set using the
new set of log-linear weights learned previously.
10We would expect better accuracy when iteratively decod-
ing the training data but did not do so in this study for effi-
ciency reasons.
</bodyText>
<figure confidence="0.92495">
2K 4K 8K 16K 32K 64K 136K 272K
Training set size
26.6
news2011
25.8
25.6
2K 4K 8K 16K 32K 64K 136K 272K
Training set size
</figure>
<figureCaption confidence="0.725195571428571">
Figure 3: Effect of increasing the training set size
from 2,000 to 272,000 sentences measured on the
dev set (top) and news2011 (bottom) in an n-best
list rescoring setting.
Figure 3 confirms that more training data in-
creases accuracy and that the best model requires
a substantially larger amount of training data than
</figureCaption>
<bodyText confidence="0.9601884">
what is typically used for maximum BLEU train-
ing. We expect an even steeper curve for larger
feature sets where more parameters need to be es-
timated and where the amount of training data is
likely to have an even larger effect.
</bodyText>
<subsectionHeader confidence="0.987216">
7.3 Likelihood versus BLEU Optimization
</subsectionHeader>
<bodyText confidence="0.9997988">
Previous research has shown that directly training
a reordering model for BLEU can vastly outper-
form a likelihood trained maximum entropy re-
ordering model (Cherry, 2013). However, the two
approaches do not only differ in the objectives
used, but also in the type of training data. The
maximum entropy reordering model is trained on
a word-aligned corpus, trying to learn human re-
ordering patterns, whereas the sparse reordering
model is trained on machine translation output,
trying to learn from the mistakes made by the ac-
tual system. It is therefore not clear how much
either one contributes to good accuracy.
Our next experiment teases those two aspects
apart and clearly shows the effect of the objec-
tive function. Specifically, we compare the tra-
ditionally used conditional log-likelihood (CLL)
objective to expected BLEU on the French-
English translation task in a small feature con-
dition (SparseHRM) of about 9K features and
</bodyText>
<figure confidence="0.9832138">
25.2
25.0
dev
BLEU
24.8
24.6
24.4
26.4
26.2
26.0
</figure>
<page confidence="0.963839">
1257
</page>
<table confidence="0.989055285714286">
dev 2008 2010 sc2010 2011 2012 2013 AllTest
noRM 23.37 20.18 24.24 24.18 24.83 24.23 24.85 23.93
HRM (baseline) 24.11 20.85 24.92 24.83 25.68 25.11 25.76 24.72
SparseHRM (CLL) 24.28 21.02 25.11 25.10 25.92 25.24 25.76 24.88
SparseHRM (xBLEU) 25.29 21.43 26.17 26.14 26.99 26.63 27.01 25.95
SparseHRM+BiPhrases (CLL) 24.42 21.17 25.12 25.00 25.86 25.36 26.18 24.98
SparseHRM+BiPhrases (xBLEU) 25.46 21.67 26.19 26.19 27.55 27.07 27.41 26.26
</table>
<tableCaption confidence="0.9294095">
Table 3: French-English results comparing the baseline hierarchical reordering model (HRM) to sparse
reordering model trained towards conditional log-likelihood (CLL) and expected BLEU (xBLEU).
</tableCaption>
<table confidence="0.998502">
dev 2008 2010 sc2010 2011 2012 2013 AllTest
PRO 24.05 20.90 25.42 25.28 25.79 25.09 26.07 24.94
xBLEU 25.24 21.26 25.99 25.93 26.98 26.34 26.77 25.77
</table>
<tableCaption confidence="0.9455285">
Table 4: French-English results on the SparseHRMLocal feature set when when trained with pair-wise
ranked optimization (PRO) and expected BLEU (xBLEU).
</tableCaption>
<bodyText confidence="0.9815953">
a large feature setting of over 3M features
(SparseHRM+BiPhrases). In the CLL setting, we
maximize the likelihood of the hypothesis with the
highest BLEU score in the n-best list of each train-
ing sentence.
Our results (Table 3) show that CLL training
achieves only a fraction of the gains yielded by
the expected BLEU objective. For SparseHRM,
CLL improves the baseline by less than 0.2 BLEU
on average across all test sets, whereas expected
BLEU achieves 1.2 BLEU. Increasing the number
of features to 3M (SparseHRM+BiPhrases) results
in a slightly better average gain of 0.3 BLEU for
CLL but but expected BLEU still achieves a much
higher improvement of 1.5 BLEU. Because our
gains with likelihood training are similar to what
Cherry (2013) reported for his maximum entropy
model, we conclude that the objective function is
the most important factor to achieving good accu-
racy.
</bodyText>
<subsectionHeader confidence="0.995596">
7.4 Comparison to PRO
</subsectionHeader>
<bodyText confidence="0.991449956521739">
In our final experiment we compare expected
BLEU training to pair-wise ranked optimization
(PRO), a popular off the shelf trainer for ma-
chine translation models with large feature sets
(Hopkins and May, 2011).11 Previous work has
shown that PRO does not scale to truly large fea-
ture sets with millions of types (Yu et al., 2013)
and we therefore restrict ourselves to our smallest
11MIRA is another popular optimizer but as previously
mentioned, even the best publicly available implementation
does not scale to large training sets (Eidelman et al., 2013).
set (SparseHRMLocal) of just over 4.4K features.
We train PRO on the development set compris-
ing of 2,525 sentences, a setup that is commonly
used by standard machine translation optimizers.
In this setting, PRO directly learns weights for the
baseline features (§7) as well as the 4.4K indica-
tor features corresponding to the sparse reordering
model. For expected BLEU training we use the
full 136K sentences from the training data. The
results (Table 4) demonstrate that expected BLEU
outperforms a typical setup commonly used to
train large feature sets.
</bodyText>
<sectionHeader confidence="0.994046" genericHeader="conclusions">
8 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999985947368421">
The expected BLEU objective is a simple and ef-
fective approach to train large-scale discriminative
reordering models. We have demonstrated that
it scales to millions of features, which is orders
of magnitudes larger than other modern machine
translation optimizers can currently handle.
Empirically, our sparse reordering model im-
proves machine translation accuracy across the
board, outperforming a strong hierarchical lexi-
calized reordering model by up to 2.0 BLEU on
a French to English WMT2012 setup, where the
baseline was trained on over two million sentence
pairs. We have shown that scaling to large train-
ing sets is crucial to good performance and that
the best performance is reached when hundreds
of thousands of training sentences are used. Fur-
thermore, we demonstrate that task-specific train-
ing towards expected BLEU is much more effec-
tive than optimizing conditional log-likelihood as
</bodyText>
<page confidence="0.972023">
1258
</page>
<bodyText confidence="0.999856866666667">
is usually done. We attribute this to the fact that
likelihood is a strict zero-one loss that does not as-
sign credit to partially correct solutions, whereas
expected BLEU does.
In future work we plan to extend expected
BLEU training to lattices and to evaluate the ef-
fect of estimating weights for the dense baseline
features as well. Our current training procedure
(Gao and He, 2013; Gao et al., 2014) decodes
the training data only once. In future work, we
would like to compare this to repeated decoding
as done by conventional optimization methods as
well as other large-scale discriminative training
approaches (Yu et al., 2013). We expect this to
yield additional accuracy gains.
</bodyText>
<sectionHeader confidence="0.999024" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9998765">
We would like to thank Arul Menezes and Xi-
aodong He for helpful discussion related to this
work and the three anonymous reviewers for their
comments.
</bodyText>
<sectionHeader confidence="0.999427" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999933882352942">
L´eon Bottou. 2004. Stochastic learning. In
Olivier Bousquet and Ulrike von Luxburg, edi-
tors, Advanced Lectures in Machine Learning, Lec-
ture Notes in Artificial Intelligence, pages 146–168.
Springer Verlag, Berlin.
Peter F. Brown, Peter V. deSouza, Robert L. Mer-
cer, Vincent J. Della Pietra, and Jenifer C. Lai.
1992. Class-based n-gram models of natural lan-
guage. Computational Linguistics, 18(4):467–479,
Dec.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 Workshop on Statistical Ma-
chine Translation. In Proc. of WMT, pages 10–51.
Association for Computational Linguistics, June.
Colin Cherry. 2013. Improved Reordering for Phrase-
Based Translation using Sparse Features. In Proc. of
NAACL, pages 9–14. Association for Computational
Linguistics, June.
David Chiang, Kevin Knight, and Wei Wang. 2009.
11,001 New Features for Statistical Machine Trans-
lation. In Proc. of NAACL, pages 218–226. Associ-
ation for Computational Linguistics, June.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201–228.
Vladimir Eidelman, Ke Wu, Ferhan Ture1, Philip
Resnik, and Jimmy Lin. 2013. Mr. MIRA:
Open-Source Large-Margin Structured Learning on
MapReduce. In Proc. of ACL, pages 199–204. As-
sociation for Computational Linguistics, August.
Michel Galley and Christopher D. Manning. 2008. A
Simple and Effective Hierarchical Phrase Reorder-
ing Model. In Proc. of EMNLP, pages 848–856.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What’s in a translation rule?
In Proc. of HLT-NAACL, pages 273–280, Boston,
MA, USA, May.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable Inference and Training
of Context-Rich Syntactic Translation Models. In
Proc. of ACL, pages 961–968, Sydney, Australia,
June.
Jianfeng Gao and Xiaodong He. 2013. Training MRF-
Based Phrase Translation Models using Gradient
Ascent. In Proc. of NAACL-HLT, pages 450–459.
Association for Computational Linguistics, June.
Jianfeng Gao, Xiaodong He, Scott Wen tau Yih, and
Li Deng. 2014. Learning Continuous Phrase Rep-
resentations for Translation Modeling. In Proc.
of ACL. Association for Computational Linguistics,
June.
Spence Green, Daniel Cer, and Christopher Manning.
2014. An Empirical Comparison of Features and
Tuning for Phrase-based Machine Translation. In
Proc. of WMT. Association for Computational Lin-
guistics, June.
Xiaodong He and Li Deng. 2012. Maximum Expected
BLEU Training of Phrase and Lexicon Translation
Models. In Proc. of ACL, pages 8–14. Association
for Computational Linguistics, July.
Mark Hopkins and Jonathan May. 2011. Tuning as
ranking. In Proc. of EMNLP. Association for Com-
putational Linguistics, July.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical Phrase-Based Translation. In
Proc. of HLT-NAACL, pages 127–133, Edmonton,
Canada, May.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne, and
David Talbot. 2005. Edinburgh System Description
for the 2005 IWSLT Speech Translation Evaluation.
In Proc. of IWSLT.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
In Proc. of ACL Demo and Poster Sessions, pages
177–180, Prague, Czech Republic, Jun.
Percy Liang, Alexandre Bouchard-Cˆot´e, Ben Taskar,
and Dan Klein. 2006. An end-to-end discriminative
approach to machine translation. In Proc. of ACL-
COLING, pages 761–768, Jul.
</reference>
<page confidence="0.863661">
1259
</page>
<reference confidence="0.999774882352941">
Preslav Nakov, Francisco Guzman, and Stephan Vo-
gel. 2012. Optimizing for Sentence-Level BLEU+1
Yields Short Translations. In Proc. of COLING. As-
sociation for Computational Linguistics.
Vinh Van Nguyen, Akira Shimazu, Minh Le Nguyen,
and Thai Phuong Nguyen. 2009. Improving A Lex-
icalized Hierarchical Reordering Model Using Max-
imum Entropy. In MT Summit XII. Association for
Computational Linguistics, August.
Franz Josef Och and Hermann Ney. 2004. The
alignment template approach to machine translation.
Computational Linguistics, 30(4):417–449, June.
Franz Josef Och. 2003. Minimum Error Rate Training
in Statistical Machine Translation. In Proc. of ACL,
pages 160–167, Sapporo, Japan, July.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proc. of ACL,
pages 311–318, Philadelphia, PA, USA, Jul.
Antti-Veikko I Rosti, Bing Zhang, Spyros Matsoukas,
and Richard Schwartz. 2010. BBN System De-
scription for WMT10 System Combination Task.
In Proc. of WMT, pages 321–326. Association for
Computational Linguistics, July.
Antti-Veikko I Rosti, Bing Zhang, Spyros Matsoukas,
and Richard Schwartz. 2011. Expected BLEU
Training for Graphs: BBN System Description for
WMT11 System Combination Task. In Proc. of
WMT, pages 159–165. Association for Computa-
tional Linguistics, July.
Christoph Tillmann. 2003. A Unigram Orientation
Model for Statistical Machine Translation. In Proc.
of NAACL, pages 106–108. Association for Compu-
tational Linguistics, June.
Roy W. Tromble, Shankar Kumar, Franz Och, and
Wolfgang Macherey. 2008. Lattice Minimum
Bayes-Risk Decoding for Statistical Machine Trans-
lation. In Proc. of EMNLP, pages 620–629. Associ-
ation for Computational Linguistics, October.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377–403.
Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Max-
imum entropy based phrase reordering model for
statistical machine translation. In Proc. of ACL-
COLING, pages 521–528, Sydney, Jul.
Heng Yu, Liang Huang, Haitao Mi, and Kai Zhao.
2013. Max-Violation Perceptron and Forced Decod-
ing for Scalable MT Training. In Proc. of EMNLP,
pages 1112–1123. Association for Computational
Linguistics, October.
</reference>
<page confidence="0.986942">
1260
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.770391">
<title confidence="0.999923">Large-scale Expected BLEU Training of Phrase-based Reordering Models</title>
<author confidence="0.998903">Michael Auli</author>
<author confidence="0.998903">Michel Galley</author>
<author confidence="0.998903">Jianfeng</author>
<affiliation confidence="0.962644">Microsoft</affiliation>
<address confidence="0.999025">Redmond, WA, USA</address>
<abstract confidence="0.9890382">Recent work by Cherry (2013) has shown that directly optimizing phrase-based reordering models towards BLEU can lead to significant gains. Their approach is limited to small training sets of a few thousand sentences and a similar number of sparse features. We show how the expected BLEU objective allows us to train a simple linear discriminative reordering model with millions of sparse features on hundreds of thousands of sentences resulting in significant improvements. A comparison to likelihood training demonstrates that expected BLEU is vastly more effective. Our best results improve a hierarchical lexicalized reordering baseline by up to 2.0 BLEU in a single-reference setting on a French-English WMT 2012 setup.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>L´eon Bottou</author>
</authors>
<title>Stochastic learning.</title>
<date>2004</date>
<booktitle>Advanced Lectures in Machine Learning, Lecture Notes in Artificial Intelligence,</booktitle>
<pages>146--168</pages>
<editor>In Olivier Bousquet and Ulrike von Luxburg, editors,</editor>
<publisher>Springer Verlag,</publisher>
<location>Berlin.</location>
<contexts>
<context position="10211" citStr="Bottou, 2004" startWordPosition="1649" endWordPosition="1650">ase pair ppi and local context ci to a sparse vector of indicator features. The local context ci represents information used by the model that is in addition to the phrase pair. For example, the features of Cherry (2013) condition on the top-stack of the hierarchical shift reduce parser, information that is non-local with respect to the phrase pair. In our experiments, we use features that go beyond the top-stack, in order to condition on various parts of the source and target side contexts (§7). 4 Model Training Optimization of our model is based on standard stochastic gradient descent (SGD; Bottou, 2004) with an expected BLEU loss l(φ) which we detail next (§5). The update is: ∂l(φt−1) φt = φt−1 − µ(4) ∂φt−1 where φt and φt−1 are model weights at time t and t − 1 respectively, and µ is a learning rate. We add the model as a small number of dense features to the log-linear framework of translation the russian side hopes to hold consultations with iran on this issue in the near future . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . pp3. . . . . . . . . . . h1. . . . . . . . . . . decoded output . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .</context>
</contexts>
<marker>Bottou, 2004</marker>
<rawString>L´eon Bottou. 2004. Stochastic learning. In Olivier Bousquet and Ulrike von Luxburg, editors, Advanced Lectures in Machine Learning, Lecture Notes in Artificial Intelligence, pages 146–168. Springer Verlag, Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Peter V deSouza</author>
<author>Robert L Mercer</author>
<author>Vincent J Della Pietra</author>
<author>Jenifer C Lai</author>
</authors>
<title>Class-based n-gram models of natural language.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<volume>18</volume>
<issue>4</issue>
<contexts>
<context position="19462" citStr="Brown et al., 1992" startWordPosition="3408" endWordPosition="3411">e top stack of the shift reduce parser of the baseline hierarchical ordering model. We experiment with these variants and extensions: • SparseHRMLocal: This feature set is exclusively based on the local phrase-pair and X sBLEU(e, e(i)) e∈E(f) ∂Z(φ) X ∂sφ(o, pp) e∈E(f) 1254 consists of features over the first and last word of both the source and target phrase.5 We use four different word representations: The word identity itself, but only for the 80 most common source and target language words. The three other word representations are based on Brown clustering with either 20, 50 or 80 classes (Brown et al., 1992). There is one feature for every orientation type. • SparseHRM: The main feature set of Cherry (2013). This is an extension of SparseHRMLocal adding features based on the first and last word of both the source and the target of the hierarchical block at the top of the stack. There are also features based on the source words in-between the current phrase and the hierarchical block at the top of the stack. • SparseHRM+UncommonWords: This set is identical to SparseHRM, except that wordidentity features are not restricted to the 80 most frequent words, but can be instantiated for all words, regard</context>
</contexts>
<marker>Brown, deSouza, Mercer, Pietra, Lai, 1992</marker>
<rawString>Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vincent J. Della Pietra, and Jenifer C. Lai. 1992. Class-based n-gram models of natural language. Computational Linguistics, 18(4):467–479, Dec.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Matt Post</author>
<author>Radu Soricut</author>
<author>Lucia Specia</author>
</authors>
<date>2012</date>
<booktitle>Findings of the 2012 Workshop on Statistical Machine Translation. In Proc. of WMT,</booktitle>
<pages>10--51</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics,</institution>
<contexts>
<context position="22348" citStr="Callison-Burch et al., 2012" startWordPosition="3856" endWordPosition="3859">right of the current phrase. Finally, monotone global indicates that all previous phrases can be combined into a single hierarchical block. The baseline includes a modified Kneser-Ney word-based language model trained on the target-side of the parallel data, which is described below. Log-linear weights are estimated with MERT (Och, 2003). We regard the 1-best output of the phrase-based decoder with the hierarchical reordering model as the baseline accuracy. Evaluation. We use training and test data from the WMT 2012 campaign and report results on French-English and German-English translation (Callison-Burch et al., 2012). Translation models are estimated on 102M words of parallel data for French-English and 91M words for GermanEnglish; between 7.5-8.2M words are newswire, depending on the language pair, and the remainder are parliamentary proceedings. All discriminative reordering models are trained on the newswire subset since we found this portion of the data to be most useful in initial experiments. We evaluate on six newswire domain test sets from 2008, 2010 to 2013 as well as the 2010 system combination test set containing between 2034 to 3003 sentences. Log-linear weights are estimated on the 2009 data </context>
</contexts>
<marker>Callison-Burch, Koehn, Monz, Post, Soricut, Specia, 2012</marker>
<rawString>Chris Callison-Burch, Philipp Koehn, Christof Monz, Matt Post, Radu Soricut, and Lucia Specia. 2012. Findings of the 2012 Workshop on Statistical Machine Translation. In Proc. of WMT, pages 10–51. Association for Computational Linguistics, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Cherry</author>
</authors>
<title>Improved Reordering for PhraseBased Translation using Sparse Features.</title>
<date>2013</date>
<booktitle>In Proc. of NAACL,</booktitle>
<pages>9--14</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics,</institution>
<contexts>
<context position="1971" citStr="Cherry (2013)" startWordPosition="291" endWordPosition="292">red interphrase reorderings using a restricted linear distortion feature, which scores a phrase reordering proportionally to the length of its displacement. While phrase-based models allow in theory completely unrestricted reordering patterns, movements are generally limited to a finite distance for complexity reasons. To address this limitation, extensive prior work focused on richer feature sets, in particular on lexicalized reordering models trained with maximum likelihood-based approaches (Tillmann, 2003; Xiong et al., 2006; Galley and Manning, 2008; Nguyen et al.,2009;§2). More recently, Cherry (2013) proposed a very effective sparse ordering model relying on a set of only a few thousand indicator features which are trained towards a task-specific metric such as BLEU (Papineni et al., 2002). These features are simply added to the log-linear framework of translation that is trained with the Margin Infused Relaxed Algorithm (MIRA; Chiang et al., 2009) on a small development set of a few thousand sentences. While simple, the approach outperforms the state-of-the-art hierarchical reordering model of Galley and Manning (2008), a maximum likelihood-based model trained on millions of sentences to</context>
<context position="3711" citStr="Cherry (2013)" startWordPosition="573" endWordPosition="574">eds of thousands of sentences (§4). Specifically, we use the expected BLEU objective function (Rosti et al., 2010; Rosti et al., 2011; He and Deng, 2012; Gao and He, 2013; Gao et al., 2014; Green et al., 2014) which allows us to train models that use training data and feature sets that are two to three orders of magnitudes larger than in previous work (§5). Our models significantly outperform the state-of-the-art hierarchical lexicalized reordering model on two language pairs and we demonstrate that richer feature sets result in significantly higher accuracy than with a feature set similar to Cherry (2013). We also demonstrate that our 1250 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1250–1260, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics approach greatly benefits from more training data than is typically used for maximum BLEU training. Previous work concluded that sparse reordering models perform better than maximum entropy models, however, the two approaches do not only differ in the objective function but also the type of training data (Cherry, 2013). Our analysis isolates the objective function a</context>
<context position="9161" citStr="Cherry (2013)" startWordPosition="1458" endWordPosition="1459">51 input sentence 7i tM ft4 - Arim a );K fix 5 PW *IT fA were generated by unsupervised methods. Instead, he proposes to learn a discriminative reordering model based on the outputs of the actual machine translation system, adjusting the feature weights to maximize a task-specific objective, which is BLEU in their case. Their model is based on a set of sparse features derived from the hierarchical reordering model which we scale to millions of features (§6). 3 A Simple Linear Reordering Model Our reordering model is defined as a simple linear model over the basic orientation types, similar to Cherry (2013). In particular, our model defines score sφ(o, e, f) over orientations o = {M, S, D}, and a sentence pair {e, f, a} with alignment a as a linear combination of weighted indicator features: sφ(o, e, f, a) = φTu(o, e, f, a) = = I i=1 I i=1 φTu(o, ppi, ci) sφ(o,ppi,ci) (3) where φ is a vector of weights, {ppi}Ii=1 is a set of phrases that decompose the sentence pair {e, f, a}, and u(o, ppi, ci) is a function that maps orientation o, phrase pair ppi and local context ci to a sparse vector of indicator features. The local context ci represents information used by the model that is in addition to th</context>
<context position="12268" citStr="Cherry, 2013" startWordPosition="2102" endWordPosition="2103">word-aligned training data. Maximum Entropy-based models. The statistics used to estimate the lexicalized and the hierarchical reordering models are based on very sparse estimates, simply because certain phrases are not very frequent. Maximum entropy models address this problem by estimating Eq. 2 through sparse indicator features over phrase pairs instead, but prior work with such models still relies on word aligned corpora for estimation (Xiong et al., 2006; Nguyen et al., 2009). However, recent evaluations of the approach show little gain over the simpler frequency-based estimation method (Cherry, 2013). Sparse Hierarchical Reordering model. All of the models so far are trained to maximize the likelihood of reordering decisions observed in word aligned corpora. Cherry (2013) argues that it is probably too difficult to learn human reordering patterns through noisy word alignments that 1Galley and Manning (2008) provide a more formal explanation. 1252 (Eq. 1). Specifically, we extend the m baseline features by a set of new features hm+1, ... , hm+j, where each represents a linear combination of sparse indicator features corresponding to one of the orientation types. Exposing each orientation a</context>
<context position="18768" citStr="Cherry (2013)" startWordPosition="3289" endWordPosition="3290">re m + 1 ≤ k ≤ m + j and N(o, pp, e, f) is the number of times pp with orientation o occurs in the current sentence pair. This simplifies the error term to: Xδo,pp = pθ,φ(e|f)γλkN(o,pp, e, f)U(φ, e) e∈E(f) (8) where λk is the weight of the dense feature summarizing orientation o in the log-linear model. We use Eq. 8 in a simple algorithm to train our model (Figure 2). Our SGD trainer uses a mini-batch size of a single sentence (§7) which entails all hypothesis in the n-best list for this sentence and the parameters are updated after each mini-batch. 6 Feature Sets Our features are inspired by Cherry (2013) who bases his features on the local phrase-pair pp = h¯e, fi as well as the top stack of the shift reduce parser of the baseline hierarchical ordering model. We experiment with these variants and extensions: • SparseHRMLocal: This feature set is exclusively based on the local phrase-pair and X sBLEU(e, e(i)) e∈E(f) ∂Z(φ) X ∂sφ(o, pp) e∈E(f) 1254 consists of features over the first and last word of both the source and target phrase.5 We use four different word representations: The word identity itself, but only for the 80 most common source and target language words. The three other word repre</context>
<context position="20561" citStr="Cherry (2013)" startWordPosition="3586" endWordPosition="3587">didentity features are not restricted to the 80 most frequent words, but can be instantiated for all words, regardless of frequency. • SparseHRM+BiPhrases: This augments SparseHRM by phrase-identity features resulting in millions of instances compared to only a few thousand for SparseHRM. We add three features for each possible phrase pair: the source phrase, the target phrase, and the whole phrase pair. The baseline hierarchical lexicalized reordering model is most similar to SparseHRM+BiPhrases feature set since both have parameters for phrase, orientation pairs.6 The feature set closest to Cherry (2013) is SparseHRM. However, while Cherry had to severely restrict his features for batch lattice MIRA-based training, our maximum expected BLEU approach can handle millions of features. 7 Experiments Baseline. We experiment with a phrase-based system similar to Moses (Koehn et al., 2007), 5Phrase-local features allow pre-computation which results in significant speed-ups at run-time. Cherry (2013) shows that local features are responsible for most of his gains. 6Although, our model is likely to learn significantly fewer parameters since many phrase, orientation pairs will only be seen in the word-</context>
<context position="26414" citStr="Cherry (2013)" startWordPosition="4505" endWordPosition="4506">luate the performance on a held-out validation set during training and stop whenever the objective changes less than a factor of 0.0003. For our PRO experiments, we tuned three hyper-parameters controlling E2 regularization, sentence-level BLEU smoothing, and length. The latter is important to eliminate PRO’s tendency to produce too short translations (Nakov et al., 2012). 7.1 Scaling the Feature Set We first compare our baseline, a likelihood trained hierarchical reordering model (HRM; Galley &amp; Manning, 2008), to various expected BLEU trained models, starting with SparseHRMLocal, inspired by Cherry (2013) and compare it to SparseHRM+BiPhrases, a set that is three orders of magnitudes larger. Our results on French-English translation (Table 1) and German-English translation (Table 2) show that the expected BLEU trained models scale to millions of features and that we outperform the baseline by up to 2.0 BLEU on newstest2012 for French-English and by up to 1.1 BLEU on newstest2011 for German-English.8 Increasing the size of the feature set improves accuracy across the board: The average accuracy over all test sets improves from 1.0 BLEU for the most basic feature set to 1.5 BLEU for the largest </context>
<context position="28627" citStr="Cherry, 2013" startWordPosition="4870" endWordPosition="4871">al reordering models. Another appeal of our approach is that training is very fast given a set of n-best lists for the training data. The SparseHRM model with 4,407 features is trained in only 26 minutes, while the SparseHRM+BiPhrases model with over three million parameters can be trained in just over two hours (136K sentences and 100 epochs in both cases). We attribute this to the training regime (§4), which does not iteratively re-decode the training data for expected BLEU training.10 7.2 Varying Training Set Size Previous work on sparse reordering models was restricted to small data sets (Cherry, 2013) due to the limited ability of standard machine translation optimizers to handle more than a few thousand sentences. In particular, recent attempts to scale the margin-infused relaxation algorithm, a variation which was also used by Cherry (2013), to larger data sets showed that more data does not necessarily help to improve test set accuracy for large feature sets (Eidelman et al., 2013). In the next set of experiments, we shed light on the advantage of training discriminative reordering models with expected BLEU on large training sets. Specifically, we start off by estimating a reordering mo</context>
<context position="31204" citStr="Cherry, 2013" startWordPosition="5301" endWordPosition="5302">-best list rescoring setting. Figure 3 confirms that more training data increases accuracy and that the best model requires a substantially larger amount of training data than what is typically used for maximum BLEU training. We expect an even steeper curve for larger feature sets where more parameters need to be estimated and where the amount of training data is likely to have an even larger effect. 7.3 Likelihood versus BLEU Optimization Previous research has shown that directly training a reordering model for BLEU can vastly outperform a likelihood trained maximum entropy reordering model (Cherry, 2013). However, the two approaches do not only differ in the objectives used, but also in the type of training data. The maximum entropy reordering model is trained on a word-aligned corpus, trying to learn human reordering patterns, whereas the sparse reordering model is trained on machine translation output, trying to learn from the mistakes made by the actual system. It is therefore not clear how much either one contributes to good accuracy. Our next experiment teases those two aspects apart and clearly shows the effect of the objective function. Specifically, we compare the traditionally used c</context>
<context position="33699" citStr="Cherry (2013)" startWordPosition="5697" endWordPosition="5698">is with the highest BLEU score in the n-best list of each training sentence. Our results (Table 3) show that CLL training achieves only a fraction of the gains yielded by the expected BLEU objective. For SparseHRM, CLL improves the baseline by less than 0.2 BLEU on average across all test sets, whereas expected BLEU achieves 1.2 BLEU. Increasing the number of features to 3M (SparseHRM+BiPhrases) results in a slightly better average gain of 0.3 BLEU for CLL but but expected BLEU still achieves a much higher improvement of 1.5 BLEU. Because our gains with likelihood training are similar to what Cherry (2013) reported for his maximum entropy model, we conclude that the objective function is the most important factor to achieving good accuracy. 7.4 Comparison to PRO In our final experiment we compare expected BLEU training to pair-wise ranked optimization (PRO), a popular off the shelf trainer for machine translation models with large feature sets (Hopkins and May, 2011).11 Previous work has shown that PRO does not scale to truly large feature sets with millions of types (Yu et al., 2013) and we therefore restrict ourselves to our smallest 11MIRA is another popular optimizer but as previously menti</context>
</contexts>
<marker>Cherry, 2013</marker>
<rawString>Colin Cherry. 2013. Improved Reordering for PhraseBased Translation using Sparse Features. In Proc. of NAACL, pages 9–14. Association for Computational Linguistics, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
<author>Kevin Knight</author>
<author>Wei Wang</author>
</authors>
<title>11,001 New Features for Statistical Machine Translation.</title>
<date>2009</date>
<booktitle>In Proc. of NAACL,</booktitle>
<pages>218--226</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics,</institution>
<contexts>
<context position="2326" citStr="Chiang et al., 2009" startWordPosition="346" endWordPosition="349"> extensive prior work focused on richer feature sets, in particular on lexicalized reordering models trained with maximum likelihood-based approaches (Tillmann, 2003; Xiong et al., 2006; Galley and Manning, 2008; Nguyen et al.,2009;§2). More recently, Cherry (2013) proposed a very effective sparse ordering model relying on a set of only a few thousand indicator features which are trained towards a task-specific metric such as BLEU (Papineni et al., 2002). These features are simply added to the log-linear framework of translation that is trained with the Margin Infused Relaxed Algorithm (MIRA; Chiang et al., 2009) on a small development set of a few thousand sentences. While simple, the approach outperforms the state-of-the-art hierarchical reordering model of Galley and Manning (2008), a maximum likelihood-based model trained on millions of sentences to fit millions of parameters. Ideally, we would like to scale sparse reordering models to similar dimensions but recent attempts to increase the amount of training data for MIRA was met with little success (Eidelman et al., 2013). In this paper we propose much larger sparse ordering models that combine the scalability of likelihood-based approaches with </context>
</contexts>
<marker>Chiang, Knight, Wang, 2009</marker>
<rawString>David Chiang, Kevin Knight, and Wei Wang. 2009. 11,001 New Features for Statistical Machine Translation. In Proc. of NAACL, pages 218–226. Association for Computational Linguistics, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="1164" citStr="Chiang, 2007" startWordPosition="170" endWordPosition="171">ering model with millions of sparse features on hundreds of thousands of sentences resulting in significant improvements. A comparison to likelihood training demonstrates that expected BLEU is vastly more effective. Our best results improve a hierarchical lexicalized reordering baseline by up to 2.0 BLEU in a single-reference setting on a French-English WMT 2012 setup. 1 Introduction Modeling reordering for phrase-based machine translation has been a long standing problem. Contrary to synchronous context free grammarbased translation models (Wu, 1997; Galley et al., 2004; Galley et al., 2006; Chiang, 2007), phrasebased models (Koehn et al., 2003; Och and Ney, 2004) have no in-built notion of reordering beyond what is captured in a single phrase pair, and the first phrase-based decoders simply scored interphrase reorderings using a restricted linear distortion feature, which scores a phrase reordering proportionally to the length of its displacement. While phrase-based models allow in theory completely unrestricted reordering patterns, movements are generally limited to a finite distance for complexity reasons. To address this limitation, extensive prior work focused on richer feature sets, in p</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2):201–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir Eidelman</author>
<author>Ke Wu</author>
<author>Ferhan Ture1</author>
<author>Philip Resnik</author>
<author>Jimmy Lin</author>
</authors>
<title>Open-Source Large-Margin Structured Learning on MapReduce.</title>
<date>2013</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>199--204</pages>
<location>Mr. MIRA:</location>
<marker>Eidelman, Wu, Ture1, Resnik, Lin, 2013</marker>
<rawString>Vladimir Eidelman, Ke Wu, Ferhan Ture1, Philip Resnik, and Jimmy Lin. 2013. Mr. MIRA: Open-Source Large-Margin Structured Learning on MapReduce. In Proc. of ACL, pages 199–204. Association for Computational Linguistics, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Christopher D Manning</author>
</authors>
<title>A Simple and Effective Hierarchical Phrase Reordering Model.</title>
<date>2008</date>
<booktitle>In Proc. of EMNLP,</booktitle>
<pages>848--856</pages>
<contexts>
<context position="1917" citStr="Galley and Manning, 2008" startWordPosition="281" endWordPosition="285">ingle phrase pair, and the first phrase-based decoders simply scored interphrase reorderings using a restricted linear distortion feature, which scores a phrase reordering proportionally to the length of its displacement. While phrase-based models allow in theory completely unrestricted reordering patterns, movements are generally limited to a finite distance for complexity reasons. To address this limitation, extensive prior work focused on richer feature sets, in particular on lexicalized reordering models trained with maximum likelihood-based approaches (Tillmann, 2003; Xiong et al., 2006; Galley and Manning, 2008; Nguyen et al.,2009;§2). More recently, Cherry (2013) proposed a very effective sparse ordering model relying on a set of only a few thousand indicator features which are trained towards a task-specific metric such as BLEU (Papineni et al., 2002). These features are simply added to the log-linear framework of translation that is trained with the Margin Infused Relaxed Algorithm (MIRA; Chiang et al., 2009) on a small development set of a few thousand sentences. While simple, the approach outperforms the state-of-the-art hierarchical reordering model of Galley and Manning (2008), a maximum like</context>
<context position="7759" citStr="Galley and Manning, 2008" startWordPosition="1222" endWordPosition="1225"> az. swap (S) assumes that az precedes az−1, i.e., the two phrases swap places. Finally, discontinuous (D) indicates that az is not adjacent to az−1. The probability distribution over these reordering events is based on a maximum likelihood estimate: p(o|pp,az−1,az) = cnt (o, pp)cnt(pp) (2) where o E {M, S, D} and cnt returns smoothed frequency counts over a word-aligned corpus. Hierarchical Reordering. An extension of the lexicalized reordering model better handles longdistance reordering by conditioning the orientation of the current phrase on a context larger than just the previous phrase (Galley and Manning, 2008). In particular, the hierarchical reordering model does so by building a compact representations of the preceding context using an efficient shiftreduce parser. During translation new phrases get moved on a stack and are then combined with any previous phrase if they are adjacent. Figure 1 shows an illustrative example: when the decoder shifts phrase pp8 onto the stack, this phrase is then merged with pp7 (reduce operation), which then can be merged with previous phrases to finally form a hierarchical block h1. These merge operations stop once we reach a phrase (here, pp3) that is not contiguo</context>
<context position="12581" citStr="Galley and Manning (2008)" startWordPosition="2149" endWordPosition="2152">rough sparse indicator features over phrase pairs instead, but prior work with such models still relies on word aligned corpora for estimation (Xiong et al., 2006; Nguyen et al., 2009). However, recent evaluations of the approach show little gain over the simpler frequency-based estimation method (Cherry, 2013). Sparse Hierarchical Reordering model. All of the models so far are trained to maximize the likelihood of reordering decisions observed in word aligned corpora. Cherry (2013) argues that it is probably too difficult to learn human reordering patterns through noisy word alignments that 1Galley and Manning (2008) provide a more formal explanation. 1252 (Eq. 1). Specifically, we extend the m baseline features by a set of new features hm+1, ... , hm+j, where each represents a linear combination of sparse indicator features corresponding to one of the orientation types. Exposing each orientation as a separate dense feature within the log-linear model is common practice for lexicalized reordering models (Koehn et al., 2005): hm+j = sφ(oj, e, f, a) where oj E {M, S, D}. The translation model is then parameterized by both θ, the log-linear weights of the baseline features, as well as φ, the weights of the r</context>
<context position="26316" citStr="Galley &amp; Manning, 2008" startWordPosition="4490" endWordPosition="4493">d the probability scaling parameter -y (Eq. 6) but found -y = 1 to be very good among other settings. We evaluate the performance on a held-out validation set during training and stop whenever the objective changes less than a factor of 0.0003. For our PRO experiments, we tuned three hyper-parameters controlling E2 regularization, sentence-level BLEU smoothing, and length. The latter is important to eliminate PRO’s tendency to produce too short translations (Nakov et al., 2012). 7.1 Scaling the Feature Set We first compare our baseline, a likelihood trained hierarchical reordering model (HRM; Galley &amp; Manning, 2008), to various expected BLEU trained models, starting with SparseHRMLocal, inspired by Cherry (2013) and compare it to SparseHRM+BiPhrases, a set that is three orders of magnitudes larger. Our results on French-English translation (Table 1) and German-English translation (Table 2) show that the expected BLEU trained models scale to millions of features and that we outperform the baseline by up to 2.0 BLEU on newstest2012 for French-English and by up to 1.1 BLEU on newstest2011 for German-English.8 Increasing the size of the feature set improves accuracy across the board: The average accuracy ove</context>
</contexts>
<marker>Galley, Manning, 2008</marker>
<rawString>Michel Galley and Christopher D. Manning. 2008. A Simple and Effective Hierarchical Phrase Reordering Model. In Proc. of EMNLP, pages 848–856.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Mark Hopkins</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>What’s in a translation rule?</title>
<date>2004</date>
<booktitle>In Proc. of HLT-NAACL,</booktitle>
<pages>273--280</pages>
<location>Boston, MA, USA,</location>
<contexts>
<context position="1128" citStr="Galley et al., 2004" startWordPosition="162" endWordPosition="165">train a simple linear discriminative reordering model with millions of sparse features on hundreds of thousands of sentences resulting in significant improvements. A comparison to likelihood training demonstrates that expected BLEU is vastly more effective. Our best results improve a hierarchical lexicalized reordering baseline by up to 2.0 BLEU in a single-reference setting on a French-English WMT 2012 setup. 1 Introduction Modeling reordering for phrase-based machine translation has been a long standing problem. Contrary to synchronous context free grammarbased translation models (Wu, 1997; Galley et al., 2004; Galley et al., 2006; Chiang, 2007), phrasebased models (Koehn et al., 2003; Och and Ney, 2004) have no in-built notion of reordering beyond what is captured in a single phrase pair, and the first phrase-based decoders simply scored interphrase reorderings using a restricted linear distortion feature, which scores a phrase reordering proportionally to the length of its displacement. While phrase-based models allow in theory completely unrestricted reordering patterns, movements are generally limited to a finite distance for complexity reasons. To address this limitation, extensive prior work </context>
</contexts>
<marker>Galley, Hopkins, Knight, Marcu, 2004</marker>
<rawString>Michel Galley, Mark Hopkins, Kevin Knight, and Daniel Marcu. 2004. What’s in a translation rule? In Proc. of HLT-NAACL, pages 273–280, Boston, MA, USA, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Jonathan Graehl</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
<author>Steve DeNeefe</author>
<author>Wei Wang</author>
<author>Ignacio Thayer</author>
</authors>
<title>Scalable Inference and Training of Context-Rich Syntactic Translation Models.</title>
<date>2006</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>961--968</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="1149" citStr="Galley et al., 2006" startWordPosition="166" endWordPosition="169"> discriminative reordering model with millions of sparse features on hundreds of thousands of sentences resulting in significant improvements. A comparison to likelihood training demonstrates that expected BLEU is vastly more effective. Our best results improve a hierarchical lexicalized reordering baseline by up to 2.0 BLEU in a single-reference setting on a French-English WMT 2012 setup. 1 Introduction Modeling reordering for phrase-based machine translation has been a long standing problem. Contrary to synchronous context free grammarbased translation models (Wu, 1997; Galley et al., 2004; Galley et al., 2006; Chiang, 2007), phrasebased models (Koehn et al., 2003; Och and Ney, 2004) have no in-built notion of reordering beyond what is captured in a single phrase pair, and the first phrase-based decoders simply scored interphrase reorderings using a restricted linear distortion feature, which scores a phrase reordering proportionally to the length of its displacement. While phrase-based models allow in theory completely unrestricted reordering patterns, movements are generally limited to a finite distance for complexity reasons. To address this limitation, extensive prior work focused on richer fea</context>
</contexts>
<marker>Galley, Graehl, Knight, Marcu, DeNeefe, Wang, Thayer, 2006</marker>
<rawString>Michel Galley, Jonathan Graehl, Kevin Knight, Daniel Marcu, Steve DeNeefe, Wei Wang, and Ignacio Thayer. 2006. Scalable Inference and Training of Context-Rich Syntactic Translation Models. In Proc. of ACL, pages 961–968, Sydney, Australia, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianfeng Gao</author>
<author>Xiaodong He</author>
</authors>
<title>Training MRFBased Phrase Translation Models using Gradient Ascent.</title>
<date>2013</date>
<booktitle>In Proc. of NAACL-HLT,</booktitle>
<pages>450--459</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics,</institution>
<contexts>
<context position="3268" citStr="Gao and He, 2013" startWordPosition="500" endWordPosition="503">dels to similar dimensions but recent attempts to increase the amount of training data for MIRA was met with little success (Eidelman et al., 2013). In this paper we propose much larger sparse ordering models that combine the scalability of likelihood-based approaches with the higher accuracy of maximum BLEU training (§3). We train on the output of a hierarchical reordering model-based system and scale to millions of features learned on hundreds of thousands of sentences (§4). Specifically, we use the expected BLEU objective function (Rosti et al., 2010; Rosti et al., 2011; He and Deng, 2012; Gao and He, 2013; Gao et al., 2014; Green et al., 2014) which allows us to train models that use training data and feature sets that are two to three orders of magnitudes larger than in previous work (§5). Our models significantly outperform the state-of-the-art hierarchical lexicalized reordering model on two language pairs and we demonstrate that richer feature sets result in significantly higher accuracy than with a feature set similar to Cherry (2013). We also demonstrate that our 1250 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1250–1260, October </context>
<context position="13258" citStr="Gao and He, 2013" startWordPosition="2266" endWordPosition="2269">y, we extend the m baseline features by a set of new features hm+1, ... , hm+j, where each represents a linear combination of sparse indicator features corresponding to one of the orientation types. Exposing each orientation as a separate dense feature within the log-linear model is common practice for lexicalized reordering models (Koehn et al., 2005): hm+j = sφ(oj, e, f, a) where oj E {M, S, D}. The translation model is then parameterized by both θ, the log-linear weights of the baseline features, as well as φ, the weights of the reordering model. The reordering model is learned as follows (Gao and He, 2013; Gao et al., 2014): 1. We first train a baseline translation system to learn θ, without the discriminative reordering model, i.e., we set θm+1 = 0, ... , θm+j = 0. 2. Using these weights, we generate n-best lists for the foreign sentences in the training data using the setup described in the experimental section (§7). The n-best lists serve as an approximation to £(f), the set of possible translations of f, used in the next step for expected BLEU training of the reordering model (§5). 3. Next, we fix θ, set θm+1 = 1,... θm+j = 1 and optimize φ with respect to the loss function on the training</context>
</contexts>
<marker>Gao, He, 2013</marker>
<rawString>Jianfeng Gao and Xiaodong He. 2013. Training MRFBased Phrase Translation Models using Gradient Ascent. In Proc. of NAACL-HLT, pages 450–459. Association for Computational Linguistics, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianfeng Gao</author>
<author>Xiaodong He</author>
<author>Scott Wen tau Yih</author>
<author>Li Deng</author>
</authors>
<title>Learning Continuous Phrase Representations for Translation Modeling.</title>
<date>2014</date>
<booktitle>In Proc. of ACL. Association for Computational Linguistics,</booktitle>
<contexts>
<context position="3286" citStr="Gao et al., 2014" startWordPosition="504" endWordPosition="507">mensions but recent attempts to increase the amount of training data for MIRA was met with little success (Eidelman et al., 2013). In this paper we propose much larger sparse ordering models that combine the scalability of likelihood-based approaches with the higher accuracy of maximum BLEU training (§3). We train on the output of a hierarchical reordering model-based system and scale to millions of features learned on hundreds of thousands of sentences (§4). Specifically, we use the expected BLEU objective function (Rosti et al., 2010; Rosti et al., 2011; He and Deng, 2012; Gao and He, 2013; Gao et al., 2014; Green et al., 2014) which allows us to train models that use training data and feature sets that are two to three orders of magnitudes larger than in previous work (§5). Our models significantly outperform the state-of-the-art hierarchical lexicalized reordering model on two language pairs and we demonstrate that richer feature sets result in significantly higher accuracy than with a feature set similar to Cherry (2013). We also demonstrate that our 1250 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1250–1260, October 25-29, 2014, Doha,</context>
<context position="13277" citStr="Gao et al., 2014" startWordPosition="2270" endWordPosition="2273"> baseline features by a set of new features hm+1, ... , hm+j, where each represents a linear combination of sparse indicator features corresponding to one of the orientation types. Exposing each orientation as a separate dense feature within the log-linear model is common practice for lexicalized reordering models (Koehn et al., 2005): hm+j = sφ(oj, e, f, a) where oj E {M, S, D}. The translation model is then parameterized by both θ, the log-linear weights of the baseline features, as well as φ, the weights of the reordering model. The reordering model is learned as follows (Gao and He, 2013; Gao et al., 2014): 1. We first train a baseline translation system to learn θ, without the discriminative reordering model, i.e., we set θm+1 = 0, ... , θm+j = 0. 2. Using these weights, we generate n-best lists for the foreign sentences in the training data using the setup described in the experimental section (§7). The n-best lists serve as an approximation to £(f), the set of possible translations of f, used in the next step for expected BLEU training of the reordering model (§5). 3. Next, we fix θ, set θm+1 = 1,... θm+j = 1 and optimize φ with respect to the loss function on the training data using stochas</context>
<context position="15326" citStr="Gao et al. (2014)" startWordPosition="2623" endWordPosition="2626">r parameter set θ: l(φ) = − xBLEU(φ) �= − pθ,φ(e|f) sBLEU(e, e(i)) (5) eE£(f) where sBLEU(e, e(i)) is a smoothed sentencelevel BLEU score with respect to the reference translation e(i), and £(f) is the generation set approximated by an n-best list. In our experiments we use n-best lists with unique entries and therefore our definitions do not take into account multiple derivations of the same translation. Specifically, our n-best lists are generated by choosing the highest scoring derivation eˆ amongst string identical translations e for f. We use a sentencelevel BLEU approximation similar to Gao et al. (2014).3 Finally, pθ,φ(e|f) is the normalized probability of translation e given f, defined as: exp{γθTh(f, e)} pθ,φ(e|f) = (6) Ee,E£(f) exp{γθTh(f, e&apos;)} where θTh(f, e) includes the discriminative reordering model hm+1(e, f), ... , hm+j(e, f) parameterized by φ, and γ E [0, inf) is a tuned scaling factor that flattens the distribution for γ &lt; 1 and sharpens it for γ &gt; 1 (Tromble et al., 2008).4 Next, we define the gradient of the expected BLEU loss function l(φ). To simplify our notation we omit the local context c in sφ(o, pp, c) (Eq. 3) from now on and assume it to be part of pp. Using the observ</context>
</contexts>
<marker>Gao, He, Yih, Deng, 2014</marker>
<rawString>Jianfeng Gao, Xiaodong He, Scott Wen tau Yih, and Li Deng. 2014. Learning Continuous Phrase Representations for Translation Modeling. In Proc. of ACL. Association for Computational Linguistics, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Spence Green</author>
<author>Daniel Cer</author>
<author>Christopher Manning</author>
</authors>
<title>An Empirical Comparison of Features and Tuning for Phrase-based Machine Translation.</title>
<date>2014</date>
<booktitle>In Proc. of WMT. Association for Computational Linguistics,</booktitle>
<contexts>
<context position="3307" citStr="Green et al., 2014" startWordPosition="508" endWordPosition="511">t attempts to increase the amount of training data for MIRA was met with little success (Eidelman et al., 2013). In this paper we propose much larger sparse ordering models that combine the scalability of likelihood-based approaches with the higher accuracy of maximum BLEU training (§3). We train on the output of a hierarchical reordering model-based system and scale to millions of features learned on hundreds of thousands of sentences (§4). Specifically, we use the expected BLEU objective function (Rosti et al., 2010; Rosti et al., 2011; He and Deng, 2012; Gao and He, 2013; Gao et al., 2014; Green et al., 2014) which allows us to train models that use training data and feature sets that are two to three orders of magnitudes larger than in previous work (§5). Our models significantly outperform the state-of-the-art hierarchical lexicalized reordering model on two language pairs and we demonstrate that richer feature sets result in significantly higher accuracy than with a feature set similar to Cherry (2013). We also demonstrate that our 1250 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1250–1260, October 25-29, 2014, Doha, Qatar. c�2014 Associ</context>
</contexts>
<marker>Green, Cer, Manning, 2014</marker>
<rawString>Spence Green, Daniel Cer, and Christopher Manning. 2014. An Empirical Comparison of Features and Tuning for Phrase-based Machine Translation. In Proc. of WMT. Association for Computational Linguistics, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaodong He</author>
<author>Li Deng</author>
</authors>
<title>Maximum Expected BLEU Training of Phrase and Lexicon Translation Models.</title>
<date>2012</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>8--14</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics,</institution>
<contexts>
<context position="3250" citStr="He and Deng, 2012" startWordPosition="496" endWordPosition="499">parse reordering models to similar dimensions but recent attempts to increase the amount of training data for MIRA was met with little success (Eidelman et al., 2013). In this paper we propose much larger sparse ordering models that combine the scalability of likelihood-based approaches with the higher accuracy of maximum BLEU training (§3). We train on the output of a hierarchical reordering model-based system and scale to millions of features learned on hundreds of thousands of sentences (§4). Specifically, we use the expected BLEU objective function (Rosti et al., 2010; Rosti et al., 2011; He and Deng, 2012; Gao and He, 2013; Gao et al., 2014; Green et al., 2014) which allows us to train models that use training data and feature sets that are two to three orders of magnitudes larger than in previous work (§5). Our models significantly outperform the state-of-the-art hierarchical lexicalized reordering model on two language pairs and we demonstrate that richer feature sets result in significantly higher accuracy than with a feature set similar to Cherry (2013). We also demonstrate that our 1250 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1</context>
</contexts>
<marker>He, Deng, 2012</marker>
<rawString>Xiaodong He and Li Deng. 2012. Maximum Expected BLEU Training of Phrase and Lexicon Translation Models. In Proc. of ACL, pages 8–14. Association for Computational Linguistics, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hopkins</author>
<author>Jonathan May</author>
</authors>
<title>Tuning as ranking.</title>
<date>2011</date>
<booktitle>In Proc. of EMNLP. Association for Computational Linguistics,</booktitle>
<contexts>
<context position="34067" citStr="Hopkins and May, 2011" startWordPosition="5754" endWordPosition="5757">of features to 3M (SparseHRM+BiPhrases) results in a slightly better average gain of 0.3 BLEU for CLL but but expected BLEU still achieves a much higher improvement of 1.5 BLEU. Because our gains with likelihood training are similar to what Cherry (2013) reported for his maximum entropy model, we conclude that the objective function is the most important factor to achieving good accuracy. 7.4 Comparison to PRO In our final experiment we compare expected BLEU training to pair-wise ranked optimization (PRO), a popular off the shelf trainer for machine translation models with large feature sets (Hopkins and May, 2011).11 Previous work has shown that PRO does not scale to truly large feature sets with millions of types (Yu et al., 2013) and we therefore restrict ourselves to our smallest 11MIRA is another popular optimizer but as previously mentioned, even the best publicly available implementation does not scale to large training sets (Eidelman et al., 2013). set (SparseHRMLocal) of just over 4.4K features. We train PRO on the development set comprising of 2,525 sentences, a setup that is commonly used by standard machine translation optimizers. In this setting, PRO directly learns weights for the baseline</context>
</contexts>
<marker>Hopkins, May, 2011</marker>
<rawString>Mark Hopkins and Jonathan May. 2011. Tuning as ranking. In Proc. of EMNLP. Association for Computational Linguistics, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical Phrase-Based Translation.</title>
<date>2003</date>
<booktitle>In Proc. of HLT-NAACL,</booktitle>
<pages>127--133</pages>
<location>Edmonton, Canada,</location>
<contexts>
<context position="1204" citStr="Koehn et al., 2003" startWordPosition="175" endWordPosition="178"> features on hundreds of thousands of sentences resulting in significant improvements. A comparison to likelihood training demonstrates that expected BLEU is vastly more effective. Our best results improve a hierarchical lexicalized reordering baseline by up to 2.0 BLEU in a single-reference setting on a French-English WMT 2012 setup. 1 Introduction Modeling reordering for phrase-based machine translation has been a long standing problem. Contrary to synchronous context free grammarbased translation models (Wu, 1997; Galley et al., 2004; Galley et al., 2006; Chiang, 2007), phrasebased models (Koehn et al., 2003; Och and Ney, 2004) have no in-built notion of reordering beyond what is captured in a single phrase pair, and the first phrase-based decoders simply scored interphrase reorderings using a restricted linear distortion feature, which scores a phrase reordering proportionally to the length of its displacement. While phrase-based models allow in theory completely unrestricted reordering patterns, movements are generally limited to a finite distance for complexity reasons. To address this limitation, extensive prior work focused on richer feature sets, in particular on lexicalized reordering mode</context>
<context position="5721" citStr="Koehn et al., 2003" startWordPosition="894" endWordPosition="897">eature hk(f, e) such as a language model or a reordering model. Function h(f, e) maps foreign and English sentences to the vector h1(f, e)...h.(f, e), and we usually choose translations eˆ according to the following decision rule: eˆ = arg max BTh(f,e) (1) e E £(f) In practice, computing eˆ exactly is intractable and we resort to an approximate but more efficient beam search (Och and Ney, 2004). Early phrase-based models simply relied on a linear distortion feature, which measures the distance between the first word of the current source phrase and the last word of the previous source phrase (Koehn et al., 2003; Och and Ney, 2004). Unfortunately, this approach is agnostic to the actual phrases being reordered, and does not take into account that certain phrases are more likely to be reordered than others. This shortcoming led to a range of lexicalized reordering models that capture exactly those preferences for individual phrases (Tillmann, 2003; Koehn et al., 2007). Reordering models generally assume a sequence of English phrases e = {¯e1, ... , ¯e,,,} currently hypothesized by the decoder, a phrase alignment a = {a1, ... , a,,,} that defines a foreign phrase ¯fa,, for each English phrase ¯ez, and </context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical Phrase-Based Translation. In Proc. of HLT-NAACL, pages 127–133, Edmonton, Canada, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Amittai Axelrod</author>
<author>Alexandra Birch Mayne</author>
<author>Chris Callison-Burch</author>
<author>Miles Osborne</author>
<author>David Talbot</author>
</authors>
<title>IWSLT Speech Translation Evaluation.</title>
<date>2005</date>
<booktitle>Edinburgh System Description for the</booktitle>
<contexts>
<context position="12996" citStr="Koehn et al., 2005" startWordPosition="2216" endWordPosition="2219">ordering decisions observed in word aligned corpora. Cherry (2013) argues that it is probably too difficult to learn human reordering patterns through noisy word alignments that 1Galley and Manning (2008) provide a more formal explanation. 1252 (Eq. 1). Specifically, we extend the m baseline features by a set of new features hm+1, ... , hm+j, where each represents a linear combination of sparse indicator features corresponding to one of the orientation types. Exposing each orientation as a separate dense feature within the log-linear model is common practice for lexicalized reordering models (Koehn et al., 2005): hm+j = sφ(oj, e, f, a) where oj E {M, S, D}. The translation model is then parameterized by both θ, the log-linear weights of the baseline features, as well as φ, the weights of the reordering model. The reordering model is learned as follows (Gao and He, 2013; Gao et al., 2014): 1. We first train a baseline translation system to learn θ, without the discriminative reordering model, i.e., we set θm+1 = 0, ... , θm+j = 0. 2. Using these weights, we generate n-best lists for the foreign sentences in the training data using the setup described in the experimental section (§7). The n-best lists </context>
</contexts>
<marker>Koehn, Axelrod, Mayne, Callison-Burch, Osborne, Talbot, 2005</marker>
<rawString>Philipp Koehn, Amittai Axelrod, Alexandra Birch Mayne, Chris Callison-Burch, Miles Osborne, and David Talbot. 2005. Edinburgh System Description for the 2005 IWSLT Speech Translation Evaluation. In Proc. of IWSLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
</authors>
<title>Moses: Open Source Toolkit for Statistical Machine Translation.</title>
<date>2007</date>
<booktitle>In Proc. of ACL Demo and Poster Sessions,</booktitle>
<pages>177--180</pages>
<location>Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra</location>
<contexts>
<context position="6083" citStr="Koehn et al., 2007" startWordPosition="951" endWordPosition="954">ient beam search (Och and Ney, 2004). Early phrase-based models simply relied on a linear distortion feature, which measures the distance between the first word of the current source phrase and the last word of the previous source phrase (Koehn et al., 2003; Och and Ney, 2004). Unfortunately, this approach is agnostic to the actual phrases being reordered, and does not take into account that certain phrases are more likely to be reordered than others. This shortcoming led to a range of lexicalized reordering models that capture exactly those preferences for individual phrases (Tillmann, 2003; Koehn et al., 2007). Reordering models generally assume a sequence of English phrases e = {¯e1, ... , ¯e,,,} currently hypothesized by the decoder, a phrase alignment a = {a1, ... , a,,,} that defines a foreign phrase ¯fa,, for each English phrase ¯ez, and an orientation oz which describes how a phrase pair should be reordered with respect to the previous phrases. There are typically three orientation types and the exact definition depends on the specific models which we describe below. Orientations can be determined during decoding and from wordaligned training corpora. Most models estimate a probability distri</context>
<context position="20845" citStr="Koehn et al., 2007" startWordPosition="3626" endWordPosition="3629"> SparseHRM. We add three features for each possible phrase pair: the source phrase, the target phrase, and the whole phrase pair. The baseline hierarchical lexicalized reordering model is most similar to SparseHRM+BiPhrases feature set since both have parameters for phrase, orientation pairs.6 The feature set closest to Cherry (2013) is SparseHRM. However, while Cherry had to severely restrict his features for batch lattice MIRA-based training, our maximum expected BLEU approach can handle millions of features. 7 Experiments Baseline. We experiment with a phrase-based system similar to Moses (Koehn et al., 2007), 5Phrase-local features allow pre-computation which results in significant speed-ups at run-time. Cherry (2013) shows that local features are responsible for most of his gains. 6Although, our model is likely to learn significantly fewer parameters since many phrase, orientation pairs will only be seen in the word-aligned data but not in actual machine translation output. scoring translations by a set of common features including maximum likelihood estimates of source given target phrases pMLE(e|f) and vice versa, pMLE(f|e), lexically weighted estimates pLW (e|f) and pLW (f|e), word and phrase</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open Source Toolkit for Statistical Machine Translation. In Proc. of ACL Demo and Poster Sessions, pages 177–180, Prague, Czech Republic, Jun.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Alexandre Bouchard-Cˆot´e</author>
<author>Ben Taskar</author>
<author>Dan Klein</author>
</authors>
<title>An end-to-end discriminative approach to machine translation.</title>
<date>2006</date>
<booktitle>In Proc. of ACLCOLING,</booktitle>
<pages>761--768</pages>
<marker>Liang, Bouchard-Cˆot´e, Taskar, Klein, 2006</marker>
<rawString>Percy Liang, Alexandre Bouchard-Cˆot´e, Ben Taskar, and Dan Klein. 2006. An end-to-end discriminative approach to machine translation. In Proc. of ACLCOLING, pages 761–768, Jul.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Preslav Nakov</author>
<author>Francisco Guzman</author>
<author>Stephan Vogel</author>
</authors>
<title>Optimizing for Sentence-Level BLEU+1 Yields Short Translations.</title>
<date>2012</date>
<booktitle>In Proc. of COLING. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="26175" citStr="Nakov et al., 2012" startWordPosition="4469" endWordPosition="4472"> be useful. To prevent overfitting, we experimented with E2 regularization, but found that it did not improve test accuracy. We also tuned the probability scaling parameter -y (Eq. 6) but found -y = 1 to be very good among other settings. We evaluate the performance on a held-out validation set during training and stop whenever the objective changes less than a factor of 0.0003. For our PRO experiments, we tuned three hyper-parameters controlling E2 regularization, sentence-level BLEU smoothing, and length. The latter is important to eliminate PRO’s tendency to produce too short translations (Nakov et al., 2012). 7.1 Scaling the Feature Set We first compare our baseline, a likelihood trained hierarchical reordering model (HRM; Galley &amp; Manning, 2008), to various expected BLEU trained models, starting with SparseHRMLocal, inspired by Cherry (2013) and compare it to SparseHRM+BiPhrases, a set that is three orders of magnitudes larger. Our results on French-English translation (Table 1) and German-English translation (Table 2) show that the expected BLEU trained models scale to millions of features and that we outperform the baseline by up to 2.0 BLEU on newstest2012 for French-English and by up to 1.1 </context>
</contexts>
<marker>Nakov, Guzman, Vogel, 2012</marker>
<rawString>Preslav Nakov, Francisco Guzman, and Stephan Vogel. 2012. Optimizing for Sentence-Level BLEU+1 Yields Short Translations. In Proc. of COLING. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vinh Van Nguyen</author>
<author>Akira Shimazu</author>
<author>Minh Le Nguyen</author>
<author>Thai Phuong Nguyen</author>
</authors>
<title>Improving A Lexicalized Hierarchical Reordering Model Using Maximum Entropy.</title>
<date>2009</date>
<booktitle>In MT Summit XII. Association for Computational Linguistics,</booktitle>
<marker>Van Nguyen, Shimazu, Le Nguyen, Nguyen, 2009</marker>
<rawString>Vinh Van Nguyen, Akira Shimazu, Minh Le Nguyen, and Thai Phuong Nguyen. 2009. Improving A Lexicalized Hierarchical Reordering Model Using Maximum Entropy. In MT Summit XII. Association for Computational Linguistics, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>The alignment template approach to machine translation.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>4</issue>
<contexts>
<context position="1224" citStr="Och and Ney, 2004" startWordPosition="179" endWordPosition="182">s of thousands of sentences resulting in significant improvements. A comparison to likelihood training demonstrates that expected BLEU is vastly more effective. Our best results improve a hierarchical lexicalized reordering baseline by up to 2.0 BLEU in a single-reference setting on a French-English WMT 2012 setup. 1 Introduction Modeling reordering for phrase-based machine translation has been a long standing problem. Contrary to synchronous context free grammarbased translation models (Wu, 1997; Galley et al., 2004; Galley et al., 2006; Chiang, 2007), phrasebased models (Koehn et al., 2003; Och and Ney, 2004) have no in-built notion of reordering beyond what is captured in a single phrase pair, and the first phrase-based decoders simply scored interphrase reorderings using a restricted linear distortion feature, which scores a phrase reordering proportionally to the length of its displacement. While phrase-based models allow in theory completely unrestricted reordering patterns, movements are generally limited to a finite distance for complexity reasons. To address this limitation, extensive prior work focused on richer feature sets, in particular on lexicalized reordering models trained with maxi</context>
<context position="4748" citStr="Och and Ney, 2004" startWordPosition="722" endWordPosition="725">py models, however, the two approaches do not only differ in the objective function but also the type of training data (Cherry, 2013). Our analysis isolates the objective function and shows that expected BLEU optimization is the most important factor to train accurate ordering models. Finally, we compare expected BLEU training to pair-wise ranked optimization (PRO) on a feature set similar to Cherry (2013; §7). 2 Reordering Models Reordering models for phrase-based translation are typically part of the log-linear framework which forms the basis of many statistical machine translation systems (Och and Ney, 2004). Formally, we are given K training pairs D = (f(1), e(1))...(f(K), e(K)), where each f(z) E F is drawn from a set of possible foreign sentences, and each English sentence e(z) E E(f(z)) is drawn from a set of possible English translations of f(z). The log-linear model is parameterized by m parameters B where each Bk E B is the weight of an associated feature hk(f, e) such as a language model or a reordering model. Function h(f, e) maps foreign and English sentences to the vector h1(f, e)...h.(f, e), and we usually choose translations eˆ according to the following decision rule: eˆ = arg max B</context>
</contexts>
<marker>Och, Ney, 2004</marker>
<rawString>Franz Josef Och and Hermann Ney. 2004. The alignment template approach to machine translation. Computational Linguistics, 30(4):417–449, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum Error Rate Training in Statistical Machine Translation.</title>
<date>2003</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>160--167</pages>
<location>Sapporo, Japan,</location>
<contexts>
<context position="14043" citStr="Och 2003" startWordPosition="2411" endWordPosition="2412">se weights, we generate n-best lists for the foreign sentences in the training data using the setup described in the experimental section (§7). The n-best lists serve as an approximation to £(f), the set of possible translations of f, used in the next step for expected BLEU training of the reordering model (§5). 3. Next, we fix θ, set θm+1 = 1,... θm+j = 1 and optimize φ with respect to the loss function on the training data using stochastic gradient descent.2 4. Finally, we fix φ and re-optimize θ in the presence of the discriminative reordering model using Minimum Error Rate Training (MERT; Och 2003; §7). We found that re-optimizing θ after a few iterations of stochastic gradient descent in step 3 did not improve accuracy. 5 Expected BLEU Objective Function The expected BLEU objective (Gao and He, 2013; Gao et al., 2014) allows us to efficiently optimize a large scale discriminative reordering model towards the desired task-specific metric, which in our setting is BLEU. 2We tuned 0.+1,... 0.+j on the development set but found that setting them uniformly to one resulted in faster training and equal accuracy. Formally, we define our loss function l(φ) as the negative expected BLEU score, d</context>
<context position="22059" citStr="Och, 2003" startWordPosition="3815" endWordPosition="3816">lties, as well as a linear distortion feature. The baseline uses a hierarchical reordering model with five orientation types, including monotone and swap, described in §2, as well as two discontinuous orientations, distinguishing if the previous phrase is to the left or right of the current phrase. Finally, monotone global indicates that all previous phrases can be combined into a single hierarchical block. The baseline includes a modified Kneser-Ney word-based language model trained on the target-side of the parallel data, which is described below. Log-linear weights are estimated with MERT (Och, 2003). We regard the 1-best output of the phrase-based decoder with the hierarchical reordering model as the baseline accuracy. Evaluation. We use training and test data from the WMT 2012 campaign and report results on French-English and German-English translation (Callison-Burch et al., 2012). Translation models are estimated on 102M words of parallel data for French-English and 91M words for GermanEnglish; between 7.5-8.2M words are newswire, depending on the language pair, and the remainder are parliamentary proceedings. All discriminative reordering models are trained on the newswire subset sin</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum Error Rate Training in Statistical Machine Translation. In Proc. of ACL, pages 160–167, Sapporo, Japan, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>311--318</pages>
<location>Philadelphia, PA, USA,</location>
<contexts>
<context position="2164" citStr="Papineni et al., 2002" startWordPosition="321" endWordPosition="324">llow in theory completely unrestricted reordering patterns, movements are generally limited to a finite distance for complexity reasons. To address this limitation, extensive prior work focused on richer feature sets, in particular on lexicalized reordering models trained with maximum likelihood-based approaches (Tillmann, 2003; Xiong et al., 2006; Galley and Manning, 2008; Nguyen et al.,2009;§2). More recently, Cherry (2013) proposed a very effective sparse ordering model relying on a set of only a few thousand indicator features which are trained towards a task-specific metric such as BLEU (Papineni et al., 2002). These features are simply added to the log-linear framework of translation that is trained with the Margin Infused Relaxed Algorithm (MIRA; Chiang et al., 2009) on a small development set of a few thousand sentences. While simple, the approach outperforms the state-of-the-art hierarchical reordering model of Galley and Manning (2008), a maximum likelihood-based model trained on millions of sentences to fit millions of parameters. Ideally, we would like to scale sparse reordering models to similar dimensions but recent attempts to increase the amount of training data for MIRA was met with lit</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proc. of ACL, pages 311–318, Philadelphia, PA, USA, Jul.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antti-Veikko I Rosti</author>
<author>Bing Zhang</author>
<author>Spyros Matsoukas</author>
<author>Richard Schwartz</author>
</authors>
<title>BBN System Description for WMT10 System Combination Task.</title>
<date>2010</date>
<booktitle>In Proc. of WMT,</booktitle>
<pages>321--326</pages>
<contexts>
<context position="3211" citStr="Rosti et al., 2010" startWordPosition="488" endWordPosition="491">eters. Ideally, we would like to scale sparse reordering models to similar dimensions but recent attempts to increase the amount of training data for MIRA was met with little success (Eidelman et al., 2013). In this paper we propose much larger sparse ordering models that combine the scalability of likelihood-based approaches with the higher accuracy of maximum BLEU training (§3). We train on the output of a hierarchical reordering model-based system and scale to millions of features learned on hundreds of thousands of sentences (§4). Specifically, we use the expected BLEU objective function (Rosti et al., 2010; Rosti et al., 2011; He and Deng, 2012; Gao and He, 2013; Gao et al., 2014; Green et al., 2014) which allows us to train models that use training data and feature sets that are two to three orders of magnitudes larger than in previous work (§5). Our models significantly outperform the state-of-the-art hierarchical lexicalized reordering model on two language pairs and we demonstrate that richer feature sets result in significantly higher accuracy than with a feature set similar to Cherry (2013). We also demonstrate that our 1250 Proceedings of the 2014 Conference on Empirical Methods in Natur</context>
</contexts>
<marker>Rosti, Zhang, Matsoukas, Schwartz, 2010</marker>
<rawString>Antti-Veikko I Rosti, Bing Zhang, Spyros Matsoukas, and Richard Schwartz. 2010. BBN System Description for WMT10 System Combination Task. In Proc. of WMT, pages 321–326. Association for Computational Linguistics, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antti-Veikko I Rosti</author>
<author>Bing Zhang</author>
<author>Spyros Matsoukas</author>
<author>Richard Schwartz</author>
</authors>
<title>Expected BLEU Training for Graphs: BBN System Description for WMT11 System Combination Task.</title>
<date>2011</date>
<booktitle>In Proc. of WMT,</booktitle>
<pages>159--165</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics,</institution>
<contexts>
<context position="3231" citStr="Rosti et al., 2011" startWordPosition="492" endWordPosition="495">ould like to scale sparse reordering models to similar dimensions but recent attempts to increase the amount of training data for MIRA was met with little success (Eidelman et al., 2013). In this paper we propose much larger sparse ordering models that combine the scalability of likelihood-based approaches with the higher accuracy of maximum BLEU training (§3). We train on the output of a hierarchical reordering model-based system and scale to millions of features learned on hundreds of thousands of sentences (§4). Specifically, we use the expected BLEU objective function (Rosti et al., 2010; Rosti et al., 2011; He and Deng, 2012; Gao and He, 2013; Gao et al., 2014; Green et al., 2014) which allows us to train models that use training data and feature sets that are two to three orders of magnitudes larger than in previous work (§5). Our models significantly outperform the state-of-the-art hierarchical lexicalized reordering model on two language pairs and we demonstrate that richer feature sets result in significantly higher accuracy than with a feature set similar to Cherry (2013). We also demonstrate that our 1250 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processi</context>
</contexts>
<marker>Rosti, Zhang, Matsoukas, Schwartz, 2011</marker>
<rawString>Antti-Veikko I Rosti, Bing Zhang, Spyros Matsoukas, and Richard Schwartz. 2011. Expected BLEU Training for Graphs: BBN System Description for WMT11 System Combination Task. In Proc. of WMT, pages 159–165. Association for Computational Linguistics, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph Tillmann</author>
</authors>
<title>A Unigram Orientation Model for Statistical Machine Translation.</title>
<date>2003</date>
<booktitle>In Proc. of NAACL,</booktitle>
<pages>106--108</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics,</institution>
<contexts>
<context position="1871" citStr="Tillmann, 2003" startWordPosition="275" endWordPosition="276">ering beyond what is captured in a single phrase pair, and the first phrase-based decoders simply scored interphrase reorderings using a restricted linear distortion feature, which scores a phrase reordering proportionally to the length of its displacement. While phrase-based models allow in theory completely unrestricted reordering patterns, movements are generally limited to a finite distance for complexity reasons. To address this limitation, extensive prior work focused on richer feature sets, in particular on lexicalized reordering models trained with maximum likelihood-based approaches (Tillmann, 2003; Xiong et al., 2006; Galley and Manning, 2008; Nguyen et al.,2009;§2). More recently, Cherry (2013) proposed a very effective sparse ordering model relying on a set of only a few thousand indicator features which are trained towards a task-specific metric such as BLEU (Papineni et al., 2002). These features are simply added to the log-linear framework of translation that is trained with the Margin Infused Relaxed Algorithm (MIRA; Chiang et al., 2009) on a small development set of a few thousand sentences. While simple, the approach outperforms the state-of-the-art hierarchical reordering mode</context>
<context position="6062" citStr="Tillmann, 2003" startWordPosition="949" endWordPosition="950">e but more efficient beam search (Och and Ney, 2004). Early phrase-based models simply relied on a linear distortion feature, which measures the distance between the first word of the current source phrase and the last word of the previous source phrase (Koehn et al., 2003; Och and Ney, 2004). Unfortunately, this approach is agnostic to the actual phrases being reordered, and does not take into account that certain phrases are more likely to be reordered than others. This shortcoming led to a range of lexicalized reordering models that capture exactly those preferences for individual phrases (Tillmann, 2003; Koehn et al., 2007). Reordering models generally assume a sequence of English phrases e = {¯e1, ... , ¯e,,,} currently hypothesized by the decoder, a phrase alignment a = {a1, ... , a,,,} that defines a foreign phrase ¯fa,, for each English phrase ¯ez, and an orientation oz which describes how a phrase pair should be reordered with respect to the previous phrases. There are typically three orientation types and the exact definition depends on the specific models which we describe below. Orientations can be determined during decoding and from wordaligned training corpora. Most models estimate</context>
</contexts>
<marker>Tillmann, 2003</marker>
<rawString>Christoph Tillmann. 2003. A Unigram Orientation Model for Statistical Machine Translation. In Proc. of NAACL, pages 106–108. Association for Computational Linguistics, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roy W Tromble</author>
<author>Shankar Kumar</author>
<author>Franz Och</author>
<author>Wolfgang Macherey</author>
</authors>
<title>Lattice Minimum Bayes-Risk Decoding for Statistical Machine Translation.</title>
<date>2008</date>
<booktitle>In Proc. of EMNLP,</booktitle>
<pages>620--629</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics,</institution>
<contexts>
<context position="15716" citStr="Tromble et al., 2008" startWordPosition="2694" endWordPosition="2697"> the same translation. Specifically, our n-best lists are generated by choosing the highest scoring derivation eˆ amongst string identical translations e for f. We use a sentencelevel BLEU approximation similar to Gao et al. (2014).3 Finally, pθ,φ(e|f) is the normalized probability of translation e given f, defined as: exp{γθTh(f, e)} pθ,φ(e|f) = (6) Ee,E£(f) exp{γθTh(f, e&apos;)} where θTh(f, e) includes the discriminative reordering model hm+1(e, f), ... , hm+j(e, f) parameterized by φ, and γ E [0, inf) is a tuned scaling factor that flattens the distribution for γ &lt; 1 and sharpens it for γ &gt; 1 (Tromble et al., 2008).4 Next, we define the gradient of the expected BLEU loss function l(φ). To simplify our notation we omit the local context c in sφ(o, pp, c) (Eq. 3) from now on and assume it to be part of pp. Using the observation that the loss does not explicitly depend on φ, we get: where δo,pp is the error term for orientation o of phrase pair pp: ∂l(φ) δo,pp = − ∂sφ(o, pp) 3We found in early experiments that the BLEU+1 approximation used by Liang et al. (2006) and Nakov et. al (2012) worked equally well in our setting. 4γ is only used during expected BLEU training. ∂l(φ) _ E ∂φ o,pp �= o,pp ∂l(φ) ∂sφ(o, </context>
</contexts>
<marker>Tromble, Kumar, Och, Macherey, 2008</marker>
<rawString>Roy W. Tromble, Shankar Kumar, Franz Och, and Wolfgang Macherey. 2008. Lattice Minimum Bayes-Risk Decoding for Statistical Machine Translation. In Proc. of EMNLP, pages 620–629. Association for Computational Linguistics, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>3</issue>
<contexts>
<context position="1107" citStr="Wu, 1997" startWordPosition="160" endWordPosition="161">ows us to train a simple linear discriminative reordering model with millions of sparse features on hundreds of thousands of sentences resulting in significant improvements. A comparison to likelihood training demonstrates that expected BLEU is vastly more effective. Our best results improve a hierarchical lexicalized reordering baseline by up to 2.0 BLEU in a single-reference setting on a French-English WMT 2012 setup. 1 Introduction Modeling reordering for phrase-based machine translation has been a long standing problem. Contrary to synchronous context free grammarbased translation models (Wu, 1997; Galley et al., 2004; Galley et al., 2006; Chiang, 2007), phrasebased models (Koehn et al., 2003; Och and Ney, 2004) have no in-built notion of reordering beyond what is captured in a single phrase pair, and the first phrase-based decoders simply scored interphrase reorderings using a restricted linear distortion feature, which scores a phrase reordering proportionally to the length of its displacement. While phrase-based models allow in theory completely unrestricted reordering patterns, movements are generally limited to a finite distance for complexity reasons. To address this limitation, </context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>Dekai Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational Linguistics, 23(3):377–403.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deyi Xiong</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Maximum entropy based phrase reordering model for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proc. of ACLCOLING,</booktitle>
<pages>521--528</pages>
<location>Sydney,</location>
<contexts>
<context position="1891" citStr="Xiong et al., 2006" startWordPosition="277" endWordPosition="280">t is captured in a single phrase pair, and the first phrase-based decoders simply scored interphrase reorderings using a restricted linear distortion feature, which scores a phrase reordering proportionally to the length of its displacement. While phrase-based models allow in theory completely unrestricted reordering patterns, movements are generally limited to a finite distance for complexity reasons. To address this limitation, extensive prior work focused on richer feature sets, in particular on lexicalized reordering models trained with maximum likelihood-based approaches (Tillmann, 2003; Xiong et al., 2006; Galley and Manning, 2008; Nguyen et al.,2009;§2). More recently, Cherry (2013) proposed a very effective sparse ordering model relying on a set of only a few thousand indicator features which are trained towards a task-specific metric such as BLEU (Papineni et al., 2002). These features are simply added to the log-linear framework of translation that is trained with the Margin Infused Relaxed Algorithm (MIRA; Chiang et al., 2009) on a small development set of a few thousand sentences. While simple, the approach outperforms the state-of-the-art hierarchical reordering model of Galley and Mann</context>
<context position="12118" citStr="Xiong et al., 2006" startWordPosition="2077" endWordPosition="2080">gh different phrases but the shift-reduce parser combines them into a single representation which is more consistent with the orientations observed in the word-aligned training data. Maximum Entropy-based models. The statistics used to estimate the lexicalized and the hierarchical reordering models are based on very sparse estimates, simply because certain phrases are not very frequent. Maximum entropy models address this problem by estimating Eq. 2 through sparse indicator features over phrase pairs instead, but prior work with such models still relies on word aligned corpora for estimation (Xiong et al., 2006; Nguyen et al., 2009). However, recent evaluations of the approach show little gain over the simpler frequency-based estimation method (Cherry, 2013). Sparse Hierarchical Reordering model. All of the models so far are trained to maximize the likelihood of reordering decisions observed in word aligned corpora. Cherry (2013) argues that it is probably too difficult to learn human reordering patterns through noisy word alignments that 1Galley and Manning (2008) provide a more formal explanation. 1252 (Eq. 1). Specifically, we extend the m baseline features by a set of new features hm+1, ... , hm</context>
</contexts>
<marker>Xiong, Liu, Lin, 2006</marker>
<rawString>Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Maximum entropy based phrase reordering model for statistical machine translation. In Proc. of ACLCOLING, pages 521–528, Sydney, Jul.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heng Yu</author>
<author>Liang Huang</author>
<author>Haitao Mi</author>
<author>Kai Zhao</author>
</authors>
<title>Max-Violation Perceptron and Forced Decoding for Scalable MT Training.</title>
<date>2013</date>
<booktitle>In Proc. of EMNLP,</booktitle>
<pages>1112--1123</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics,</institution>
<contexts>
<context position="34187" citStr="Yu et al., 2013" startWordPosition="5777" endWordPosition="5780">ill achieves a much higher improvement of 1.5 BLEU. Because our gains with likelihood training are similar to what Cherry (2013) reported for his maximum entropy model, we conclude that the objective function is the most important factor to achieving good accuracy. 7.4 Comparison to PRO In our final experiment we compare expected BLEU training to pair-wise ranked optimization (PRO), a popular off the shelf trainer for machine translation models with large feature sets (Hopkins and May, 2011).11 Previous work has shown that PRO does not scale to truly large feature sets with millions of types (Yu et al., 2013) and we therefore restrict ourselves to our smallest 11MIRA is another popular optimizer but as previously mentioned, even the best publicly available implementation does not scale to large training sets (Eidelman et al., 2013). set (SparseHRMLocal) of just over 4.4K features. We train PRO on the development set comprising of 2,525 sentences, a setup that is commonly used by standard machine translation optimizers. In this setting, PRO directly learns weights for the baseline features (§7) as well as the 4.4K indicator features corresponding to the sparse reordering model. For expected BLEU tr</context>
</contexts>
<marker>Yu, Huang, Mi, Zhao, 2013</marker>
<rawString>Heng Yu, Liang Huang, Haitao Mi, and Kai Zhao. 2013. Max-Violation Perceptron and Forced Decoding for Scalable MT Training. In Proc. of EMNLP, pages 1112–1123. Association for Computational Linguistics, October.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
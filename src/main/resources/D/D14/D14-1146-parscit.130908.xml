<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.015650">
<title confidence="0.995006">
Device-Dependent Readability for Improved Text Understanding
</title>
<author confidence="0.997613">
A-Yeong Kim Hyun-Je Song Seong-Bae Park Sang-Jo Lee
</author>
<affiliation confidence="0.9895535">
School of Computer Science and Engineering
Kyungpook National University
</affiliation>
<address confidence="0.559172">
Daegu, 702-701, Korea
</address>
<email confidence="0.997715">
{aykim,hjsong,sbpark}@sejong.knu.ac.kr, sjlee@knu.ac.kr
</email>
<sectionHeader confidence="0.99389" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999926578947368">
Readability is used to provide users with high-
quality service in text recommendation or text
visualization. With the increasing use of hand-
held devices, reading device is regarded as
an important factor for readability. There-
fore, this paper investigates the relationship
between readability and reading devices such
as a smart phone, a tablet, and paper. We sug-
gest readability factors that are strongly related
with the readability of a specific device by
showing the correlations between various fac-
tors in each device and human-rated readabil-
ity. Our experimental results show that each
device has its own readability characteristics,
and thus different weights should be imposed
on readability factors according to the device
type. In order to prove the usefulness of the
results, we apply the device-dependent read-
ability to news article recommendation.
</bodyText>
<sectionHeader confidence="0.99899" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999971362318841">
Readability is a function that maps a given text into a
readability score by considering “how easily the text is
read and understood” (Richards et al., 1992; Zamanian
and Heydari, 2012). Normally, the readability score is
formulated as a combination of various factors. These
factors reflect the easiness and understanding of the
text and include text presentation format, font size, av-
erage ratio of annotated images, and sentence length
(Hasegawa et al., 2008; Kitson, 1927; Ma et al., 2012;
¨Oquist, 2006). Therefore, readability can be used to
provide satisfiable services in text recommendation or
text visualization.
The study on readability has begun in the education
field to measure the level of a text. With the success
of using readability in education (Franc¸ois and Fairon,
2012; Heilman et al., 2008; Ma et al., 2012), read-
ability has been used in a range of domains recently.
For example, in document retrieval, readability is used
to provide documents to non-expert users so that they
can read the retrieved documents easily (Jameel et al.,
2012; Yan et al., 2006). In text mining, readability has
been employed to analyze the characteristics of text.
Especially, Hillbom showed the differences in readabil-
ity between broadsheet newspapers and tabloids that
share a similar political stance (Hillbom, 2009).
There is one important issue of readability that has
not been studied in natural language processing. It is a
reading device. That is, previous studies focused only
on text printed on paper. However, with the increasing
use of hand-held devices, people in these days use var-
ious reading devices such as a tablet and a smart phone
as well as a paper. Readability score can be different
according to the device type, because each device has
its own idiosyncrasy. For example, assume that a sys-
tem recommends the same news article to both user A
who reads it in her smart phone and user B who reads
it on paper. Although both users read the same article,
user A might believe that her article is more difficult to
read than user B because of the screen size of her smart
phone.
This paper explores the relationship between reading
devices and readability. For this purpose, we first inves-
tigate whether readability changes according to device
type or not. Then, we analyze which readability fac-
tors are affected by reading devices. To see the rela-
tionship between readability factors and devices, var-
ious well-known readability factors are computed for
news articles collected from an Internet portal. At the
same time, the readability of each article is also man-
ually rated. When the readability is rated manually, it
is done three times for different reading devices of a
smart phone, a tablet, and paper. The factors that af-
fect the readability actually in each device are found
out through the correlations between the factors and the
manually-labeled readability. Some factors are impor-
tant to the readability of smart phone, but insignificant
to that of paper. Therefore, we discover the importance
of each readability factor for each device by analyzing
the correlations.
The usefulness of the device-dependent readability
is proven by applying it to news article recommenda-
tion. That is, different importance weights for read-
ability factors are considered according to device type
when recommending news articles. Our experimental
results show that the performance of news article rec-
ommendation gets best when the device used for read-
ing news articles is identical to the device used for mea-
suring readability. Therefore, it is essential to consider
different importance weights according to device type
</bodyText>
<page confidence="0.979416">
1396
</page>
<note confidence="0.9011455">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1396–1404,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.9999165625">
in news article recommendation. It also proves that
the proposed device-dependent readability reflects the
characteristics of reading devices well.
The rest of this paper is organized as follows. We
first review related studies on readability. Next, we
introduce various readability factors and propose the
device-dependent readability. Then, the news article
recommendation using the device-dependent readabil-
ity is explained. This recommendation is prepared to
prove the usefulness of the device-dependent readabil-
ity. In the experiments, we present the experimental
results on the relationship between reading devices and
readability. We also describe the experiments on news
recommendation using the device-dependent readabil-
ity and present their results. Finally, we summarize our
research.
</bodyText>
<sectionHeader confidence="0.999593" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.999992607142857">
The history of readability studies began in the 1800s.
Early studies focused on the frequency of easy words,
sentence length, and word length (Huld´en, 2004).
Flesch designed a formula to calculate “reading ease”
using only the average word length and sentence length
(Flesch, 1948). He adjusted the relative importance
between word length and sentence length using 100
words selected randomly from a corpus. This formula
is called the Flesch-Kincaid formula, and is generally
used in measuring the readability of a textbook (Kin-
caid et al., 1975). Dale and Chall (1949) defined a list
of 3,000 easy words. Then, they used the average sen-
tence length and the percentage of words not included
in the list. These studies simply used superficial fac-
tors, and thus do not reflect syntactic factors.
Recent studies on readability use various factors in-
cluding syntactic ones, and combine them to produce
a highly predictive model of readability. Franc¸ois and
Faircon (2012) proposed a readability formula with 46
textual factors for French as a foreign language. The
factors represent lexical, syntactic, and semantic char-
acteristics of sentences, and the specificities of French.
They are extracted from 28 French Foreign Language
(FFL) textbooks written for adults learning FFL. On the
other hand, Pitler and Nenkova (2008) showed the rela-
tion between readability factors and readability. They
used human ratings from the Wall Street Journal cor-
pus, and computed the correlations between the read-
ability factors and the average human ratings. Accord-
ing to their results, the average number of verb phrases
in a sentence, the number of words in an article, the
likelihood of the vocabulary, and the likelihood of the
discourse relations are highly correlated with human
ratings. However, these studies did not consider the
reading devices, but focused on how well a text is writ-
ten. Since the readability can be differentiated accord-
ing to reading device, a reading device should be con-
sidered when computing the readability of a given text.
To the best of our knowledge, there are few studies
on the readability on mobile devices that do not con-
sider language-related aspects. Most studies on mobile
devices focused on the development of new text format
and layout to help users read documents easily. ¨Oquist
(2006) proposed a new text presentation format called
the dynamic Rapid Serial Visual Presentation. Accord-
ing to his experimental results, this format helps to re-
duce eye movements. On the other hand, Hasegawa
et al. (2008) evaluated the readability of documents
on mobile devices with regard to screen and font size.
They reported that the readability is improved when the
characters are vertically enlarged. Readability on mo-
bile devices is not reflected only by the visualization
factors, but also by textual factors. Therefore, this pa-
per explores the readability factors that reflect the lexi-
cal and grammatical complexity of text and are affected
by reading devices.
</bodyText>
<sectionHeader confidence="0.992062" genericHeader="method">
3 Readability Factors
</sectionHeader>
<bodyText confidence="0.999344666666667">
Table 1 lists the readability factors used in this paper.
Basically, they are based on the factors proposed by
Pitler and Nenkova (2008). However, some factors are
excluded and some new factors are added. This is be-
cause some of their factors are computationally infeasi-
ble and language-dependent. As a result, we have thir-
teen readability factors. These readability factors are
divided into four types: superficial, lexical, syntactic
factors, and lexical cohesion.
</bodyText>
<subsectionHeader confidence="0.99911">
3.1 Superficial Factors
</subsectionHeader>
<bodyText confidence="0.999875965517241">
Superficial factors were used in most early readability
studies (Dale and Chall, 1949; Flesch, 1948; Kincaid et
al., 1975), and reflect the construction of a text. We in-
vestigate four factors: text length (TL), sentence length
(SL), average number of words per sentence (WS), and
average number of characters per word (CW). Since
longer text is perceived as “harder-to-read” than short
one, these factors are all reciprocally related with read-
ability.
The first two factors are related to length. TL counts
the number of characters in a text, whereas SL com-
putes the number of sentences. When a writer attempts
to write many topics in a text, she tends to use many
kinds of words simultaneously. As a result, the text be-
comes longer and more complex. Such long length of
text disturbs a reader’s comprehension of the text, and
then it is more difficult for the reader to read the text
(Heilman et al., 2008).
WS counts the average number of words per sen-
tence, and CW reflects the average number of characters
per word. When they are large, the sentence is diffi-
cult to read, which leads to difficulties in understanding
the text. Especially, CW reflects compound nouns and
technical words. For instance, compound nouns in Ko-
rean are usually long, because there is no spacing be-
tween words in a compound noun. For example, let us
consider a compound noun, “Daehanmingukjungboo,”
which means the Korean government. Actually this
compound noun consists of two independent nouns.
</bodyText>
<page confidence="0.989275">
1397
</page>
<table confidence="0.999703928571429">
Type of Factors Abbr. Description
Superficial factors TL The number of characters in a text
SL The number of sentences in a text
WS Average number of words per sentence
CW Average number of characters per word
Lexical factor LL Article likelihood estimated by language model
Syntactic factors PTD Average parse tree depths per sentence
NP Average number of noun phrases per sentence
VP Average number of verb phrases per sentence
SBAR Average number of subordinate clauses per sentence
Lexical cohesion COS Average cosine similarity between pairs of adjacent sentences
WO Average word overlap between pairs of adjacent sentences
NPO Average word overlap over noun and pronoun only
PRP Average number of pronouns per sentence
</table>
<tableCaption confidence="0.999846">
Table 1: Description of readability factors
</tableCaption>
<bodyText confidence="0.999907666666667">
One is “Daehanminguk” meaning Korea and the other
is “Jungboo” meaning a government. The two are con-
catenated to form a compound noun and become a long
single word. In addition, many difficult words such as
domain-specific terms tend to be long. Such lengthy
words make it difficult to read a text.
</bodyText>
<subsectionHeader confidence="0.999812">
3.2 Lexical Factor
</subsectionHeader>
<bodyText confidence="0.999846727272727">
Lexical factor determines whether a given text con-
sists of frequent words. Texts that express a new trend
in various fields often use many newly coined words.
Such neologisms make it difficult to read and under-
stand a text. Therefore, an easily-understandable text
is composed of widely-used words rather than unusual
words.
In order to compute the use of frequent words in a
text, a unigram language model is used as in the work
of Pitler and Nenkova (2008). In this model, the log
likelihood of text t is computed by
</bodyText>
<equation confidence="0.9593995">
E C(w) · log P(w|B). (1)
w∈t
</equation>
<bodyText confidence="0.9999513">
where P(w|B) is the probability of a word w according
to a background corpus B, and C(w) is the number of
times that w appears in t.
This factor examines the familiarity of the words
used in the text. The more frequently a word appears
in the background corpus, the more familiar it is re-
garded. The frequency of a word w is then reflected
into P(w|B) computed from the independent back-
ground corpus B. Therefore, the factor LL is positively
related with readability.
</bodyText>
<subsectionHeader confidence="0.999311">
3.3 Syntactic Factors
</subsectionHeader>
<bodyText confidence="0.999932708333333">
Syntactic factors reflect sentence complexity directly
that affects human processing of a sentence. We con-
sider the average parse tree depth per sentence (PTD),
the average number of noun phrases per sentence (NP),
the average number of verb phrases per sentence (VP),
and the average number of subordinate clauses per sen-
tence (SBAR) as syntactic factors. These four factors
were defined by Schwarm and Ostendorf (2005).
A reader regards a text as difficult when the sen-
tences in the text have large parse tree depths or many
subordinate clauses. Thus, PTD and SBAR are related
negatively with readability. On the other hand, the re-
lationship of NP and VP to readability are not one way.
The large number of noun phrases in a text requires
a reader to remember more items (Barzilay and Lap-
ata, 2008; Pitler and Nenkova, 2008). However, it also
makes the text more interesting. The texts written for
adults actually contain more entities than those writ-
ten for children (Barzilay and Lapata, 2008). The same
is true for VP. The large number of verb phrases in a
sentence makes the sentence more complex. However,
people feel that a text is more easier to comprehend
when related clauses are grouped together (Bailin and
Grafstein, 2001).
</bodyText>
<subsectionHeader confidence="0.981674">
3.4 Lexical Cohesion
</subsectionHeader>
<bodyText confidence="0.99997719047619">
Lexical cohesion denotes how the sentences in a text
are semantically connected. People usually bring con-
tinuous sentences into their mind at the same time, and
interpret them as a single unit (Okazaki et al., 2005). In
other words, a reader prefers text whose sentences are
smoothly connected to text whose sentences are inde-
pendent of one another. Therefore, sentence continuity
plays a primary role in understanding an entire text.
In the classic study of cohesion, various uses of
cohesive elements such as pronouns, definite articles,
and topic continuity have been discussed (Halliday and
Hasan, 1976). This paper uses the average cosine sim-
ilarity (COS), word overlap (WO), word overlap over
just nouns and pronouns (NPO) between pairs of adja-
cent sentences, and the average number of pronouns per
sentence (PRP). COS, WO, and NPO are superficial mea-
sures of topic continuity, whereas PRP is an indicative
feature of sentence continuity. High values for these
factors imply that the sentences in the text are related
somehow. Therefore, these factors are believed to be
related positively with readability.
</bodyText>
<page confidence="0.95985">
1398
</page>
<subsectionHeader confidence="0.992446">
3.5 Measurement of Readability
</subsectionHeader>
<bodyText confidence="0.9999414">
When a reading device d is given, the readability of
text t, represented as R(t|d), is formulated as a com-
bination of readability factors with their corresponding
weight in the device. We assume that wi|d, the weight
of a readability factor fi, is dependent on the reading
device d. Following the previous work of Pitler and
Nenkova (2008), we also assume that each readabil-
ity factor affects readability independently. Therefore,
readability is calculated as a weighted linear sum of all
readability factors. That is, R(t|d) is computed by
</bodyText>
<equation confidence="0.98536">
R(t|d) = � wi|d - fi(t) (2)
i∈{1,2,...,M}
</equation>
<bodyText confidence="0.9995553">
where M is the number of readability factors.
Each weight wi|d is determined from a set of news
articles T. We collected a large number of news arti-
cles from an Internet news portal. The readability of
each article was manually labeled. This is done three
times, since we have three different devices of a smart
phone, a tablet, and paper. Since human rating of each
article t E T is available for each device, wi|d’s can
be estimated by linear regression. These weights are
different according to the devices.
</bodyText>
<sectionHeader confidence="0.9682355" genericHeader="method">
4 News Article Recommendation by
Device-Dependent Readability
</sectionHeader>
<bodyText confidence="0.9726638">
The fact that the weights wi|d in Equation (2) are differ-
ent for each device d implies that the readability mea-
surement should be different depending on the device
type. In order to see the usefulness of this device-
dependent readability, we apply it to news article rec-
ommendation. News article recommendation aims to
provide a user with news articles that interest the user.
Thus, it selects a few articles that meet user preference
from a gigantic amount of news events. Various meth-
ods have reported notable results in news article rec-
ommendation (Das et al., 2007; Li et al., 2010; Liu et
al., 2010). In addition, with the recent interest in hand-
held devices, the demand for news recommendation on
hand-held devices is increasing. However, there has
been, at least as far as we know, no study on the read-
ability of hand-held devices.
Device-dependent readability is reflected into news
article recommendation through a re-ranking frame-
work. Figure 1 depicts the overall process of suggest-
ing news articles for a specific device with the device-
dependent readability. The point of this figure is to
measure how appropriate a news article is for a spe-
cific reading device. For this, a news recommendation
system first chooses a set of news articles from a news
repository based on its own criterion. Then, we re-rank
them by the device-dependent readability to obtain the
final set of ranked news articles for the device.
Formally, a news article recommendation ranks a set
of articles, A = {a1, a2, ..., am1, where ai represents
the i-th article. The order between ranks a1 &gt;- a2 &gt;-
</bodyText>
<table confidence="0.9990906">
Min Max Average
Article length 68 610 346.5
# of sentences 1 14 6.24
# of words per sentence 8 33 16.93
# of words per article 17 178 99.34
</table>
<tableCaption confidence="0.99909">
Table 2: Statistics of the news article data
</tableCaption>
<bodyText confidence="0.999599538461539">
... &gt;- am should be satisfied by the criterion of the
recommendation system. That is, assuming that the
system has a score function score(ai), score(ai) &gt;
score(aj) has to be met if ai &gt;- aj. Then, the top
k(k &lt; m) articles of A by the score function are sug-
gested as appropriate news articles. After that, the se-
lected articles are re-ranked by another criterion, the
device-dependent readability. That is, the final rank of
an article within the selected set is determined by an-
other function, rerank. Since this function has to re-
flect the device-dependent readability, it takes two pa-
rameters. One is an article, and the other is a device
type. The re-rank function is modeled as
</bodyText>
<equation confidence="0.993112666666667">
rerank(a, d) = R(a|d)
�= wi|d - fi(a). (3)
i∈{1,2,...,M}
</equation>
<bodyText confidence="0.999981375">
As a result, the readability-based re-ranking module
suggests the news articles based on how easily the ar-
ticles are read on a specific reading device. Note that
even the same article would be ranked differently ac-
cording to the device type because the article is re-
ranked by the device-dependent readability. At last, the
top k*(k* &lt; k) re-ranked articles among them are sug-
gested as final news articles.
</bodyText>
<sectionHeader confidence="0.999796" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.891532">
5.1 Experiments on Readability Factors
5.1.1 Experiment Settings
</subsectionHeader>
<bodyText confidence="0.999971941176471">
For the experiments of analyzing relationship between
readability factors and readability, we collected a Ko-
rean news corpus from Naver News1. This corpus con-
tains news articles from June 10, 2013 to June 25,
2013. We selected 74 articles randomly from the cor-
pus which were used for readability formula and show-
ing the relationships between readability factors. All
selected articles belong to one of three categories: ‘Pol-
itics’, ‘Entertainment’, and ‘Sports’. A set of these 74
news articles becomes T, and is used to compute the
weights in Equation (2). Table 2 describes a simple
statistics of the selected news articles. The shortest ar-
ticle consists of 68 characters, whereas the longest one
has 610 characters. The average length of article is
346.5. The shortest article is written in one sentence,
and the longest has 14 sentences. One article has ap-
proximately 6.24 sentences on average. In addition, the
</bodyText>
<footnote confidence="0.990534">
1A Korean news portal of which web address is
http://news.naver.com.
</footnote>
<page confidence="0.992196">
1399
</page>
<figure confidence="0.998122">
News
Recommendation
System
News
Articles
News
Repository
Device
dependent
re-ranking
Readability
</figure>
<figureCaption confidence="0.999988">
Figure 1: Overall process of re-ranking news articles based on device-dependent readability
</figureCaption>
<bodyText confidence="0.999504666666666">
number of words per sentence ranges from 8 to 33, and
the average is 16.93. The minimum number of words
in an article is 17, and the maximum number of words
is 178. An article is composed of 99.34 words on aver-
age.
In order to compute the lexical factor LL by Equa-
tion (1), a background corpus B is required. Since this
corpus should be independent from the news articles
explained above, the Naver News is adopted again to
generate B. For the background corpus B, we col-
lected news articles from January 1, 2013 to September
6, 2013, but excluded the articles from June 10 to June
25, because they are already used. This corpus consists
of 298,729 articles with 3,264,104 distinct words.
The readability score for each article was manually
labeled by three undergraduate students. To investigate
the relationship between reading devices and readabil-
ity, each article was read using three different reading
devices. The Galaxy Note 1 with a 5-inch screen is
used as the smart phone, Galaxy Tab 10.1 with a 10.1-
inch screen is used as the tablet, and A4-size paper
is used for the paper. That is, the human annotators
read and rated 74 articles per device. The order of the
devices where the annotators evaluated readability is
smart phone, tablet, and paper. This order was main-
tained for all the experiments. All aspects but content
texts were under control. For instance, font = “Gothic,
12 pt” (this is most commonly used font and size that
most Korean web pages and textbooks use), font color
= “black”, alignment = “both” were used for all three
devices. In addition, the non-content aspects were ex-
actly same for devices because the annotators of read-
ability and the recommended articles shared the read-
ing devices. Although these aspects affect readability
and many previous studies already proved it, it is not
our concern. We only attempt to capture how read-
</bodyText>
<table confidence="0.987911">
Reading device Min Max Average
Smart phone 1.67 5 3.423 ± 0.741
Tablet 1.33 5 3.531 ± 0.837
Paper 2 5 3.360 ± 0.594
</table>
<tableCaption confidence="0.999741">
Table 3: Readability scores given by human annotators
</tableCaption>
<bodyText confidence="0.999756793103448">
ability is affected by the content in different types of
devices.
Human annotators can remember the content of
news articles when they read articles with three de-
vices. The human annotators were asked to read and
evaluate many articles within a relatively short period.
Therefore, before the main experiments, we performed
a pilot experiment on the memory effects of previously
read articles and verified it empirically. We hired three
undergraduate students who were not involved in our
main experiments. The students read the same 250 ar-
ticles four times, and these also come from Naver News
corpus which are not included the previous 74 articles.
After their first reading, they read the articles again in
3, 7, and 14 days later. After 3 days, two students re-
membered the articles somewhat, but one student re-
membered them vaguely. Since they almost forgot the
articles after 7 days, we placed 7 days interval between
devices.
The readability score of an article was rated by the
annotators using the questions in the work of Pitler and
Nenkova (2008). We use only two of the questions,
while they used four questions for the annotators. Their
questions are intended to measure the extent of how
well a text is written, how it fits together, how easy
it is to understand, and how interesting it is. We can
consider “well-written” and “fit-together” as a syntac-
tic perspective, whereas “easy to understand” and “in-
teresting” belong to a content perspective. For such a
</bodyText>
<page confidence="0.967435">
1400
</page>
<table confidence="0.999905333333333">
Smart phone Tablet Paper
Factor Value Factor Value Factor Value
SL -0.394 SL -0.370 NP 0.298
TL -0.293 WS 0.321 WS 0.278
WS 0.288 LL 0.253 LL 0.268
LL 0.249 NP 0.240 VP 0.244
</table>
<tableCaption confidence="0.9678615">
Table 4: Pearson correlation coefficients of important
readability factors
</tableCaption>
<bodyText confidence="0.9059225">
reason, four questions can be summarized in two ques-
tions. The two questions used are
</bodyText>
<listItem confidence="0.999791">
• How well-written is this article?
• How interesting is this article?
</listItem>
<bodyText confidence="0.999980277777778">
For these two questions, each annotator assigns a score
between 1 and 5 to each article. Here, 1 point means
that the article is worst and 5 point implies that it is
best. A readability score of one human annotator is
composed with the average of two questions (well-
written, interesting). We used the average of three hu-
man annotators’ readability scores in our experiments.
Table 3 shows the readability scores of the articles for
each device. According to this table, the readability
score ranges from 1.67 to 5 for the smart phone, 1.33
to 5 for the tablet, and ranges from 2 to 5 for the paper.
The average readability is 3.423 for the smart phone,
3.531 for the tablet, and 3.360 for the paper. To see
the inter-judge agreement among annotators, the Kappa
coefficient (Fleiss, 1971) is used. The Kappa values
for the ‘smart phone’, ‘tablet’, and ‘paper’ are 0.342,
0.333, and 0.361, respectively. All these values corre-
spond to fair agreement.
</bodyText>
<subsubsectionHeader confidence="0.654513">
5.1.2 Experimental Results
</subsubsectionHeader>
<bodyText confidence="0.999994303571429">
In order to see the importance of each factor in a spe-
cific device, we adopt the Pearson correlation coeffi-
cients between readability factors and reading devices.
Table 4 lists the four most important factors in each
device and their Pearson correlation coefficients. Espe-
cially, p-value is smaller than 0.05 for all factors in this
table.
For the smart phone, SL, the number of sentences in
a text, is the most important readability factor. Its cor-
relation with the smart phone is -0.394. TL, the number
of characters, is the second important factor and has a
negative correlation of -0.293. These results imply that
readers are negatively sensitive to the length of an arti-
cle because of the small display size of a smart phone.
That is, in the smart phone, longer articles are recog-
nized as difficult to read compared to shorter ones. The
number of words per sentence, WS, is the third impor-
tant factor with correlation of 0.288. The log-likelihood
of an article, LL, is also positively related with the read-
ability, which proves that widely-used words make it
easy to understand an article. The top three factors are
superficial with regard to text length. Therefore, the su-
perficial factors are more important than other types of
factors for the smart phone.
SL is the most critical readability factor even for the
tablet. It affects readability with high correlation of -
0.370. The second important factor is WS with correla-
tion of 0.321. Both of these factors are superfical. The
third important factor, LL, is positively related with
readability as expected. The fourth factor that affects
readability is the number of noun phrases, NP. It is nat-
ural for NP to be positively related with the readability.
Finally, for the paper, NP is most strongly related to
readability with correlation of 0.298. The second im-
portant factor is WS, whose correlation is 0.278. LL is
the third important factor and shows a positive relation-
ship. Note that WS and LL are important readability
factors for all devices. The next important readabil-
ity factor for the paper is the average number of verb
phrases (VP). The articles with many noun phrases and
verb phrases are perceived as easier-to-read for the pa-
per. Note that the importance of superficial factors is
limited for the paper. We expected that WS is negatively
related, but, it is positively related with readability for
all three devices. The reason for this could be that the
annotators thought the articles with higher WS are more
interesting.
The important factors for the smart phone are differ-
ent from those for the paper. On the other hand, the
tablet shares many factors with both the smart phone
and the paper. Because the screen size of a tablet is
similar to the size of an A4 paper, the tablet and the pa-
per share readability factors. However, length-related
factors play a more important role than syntactic fac-
tors in the smart phone because a smart phone has a
smaller screen.
</bodyText>
<subsectionHeader confidence="0.9970305">
5.2 Experiments on News Recommendation
5.2.1 Experiment Settings
</subsectionHeader>
<bodyText confidence="0.999388">
Experiments for news article recommendation were
performed to see the effectiveness of device-dependent
readability. The process of news recommendation with
device-dependent readability is as follows. For a spe-
cific device,
</bodyText>
<listItem confidence="0.9694999">
1. Select top-k news articles from a news repository
by the criterion of the recommendation system.
2. Re-rank the k articles by the readability of the de-
vice using Equation (3).
3. Select top-k* news articles by the new rank.
4. Human annotators read and rate the k* articles
with the device.
5. Compare the ranks of k* articles by device-
dependent readability with those by human rat-
ings.
</listItem>
<bodyText confidence="0.995743">
Since we have three types of devices, this process is
performed three times with a different device.
The news articles from September 10, 2013 to
September 12, 2013 collected from Naver News were
</bodyText>
<page confidence="0.959122">
1401
</page>
<table confidence="0.9984738">
Min Max Average
Article length 277 6,077 990.68
# of sentences 4 199 22.85
# of words per sentence 4 100 15.73
# of words per article 71 2,034 301.61
</table>
<tableCaption confidence="0.981155">
Table 5: Statistics of news data for recommendation
</tableCaption>
<table confidence="0.99900325">
Reading device Min Max Average
Smart phone 1 5 3.513 f 0.962
Tablet 1 5 3.344 f 0.852
Paper 1 5 3.250 f 0.907
</table>
<tableCaption confidence="0.9428655">
Table 6: Scores of news articles by human annotators
in news recommendation
</tableCaption>
<bodyText confidence="0.998785926829268">
used as the news repository. The number of times that
a news article was actually read by its anonymous read-
ers at the portal site is used as the criterion for the rec-
ommendation system. Since this criterion is provided
on a daily basis and news articles were collected for
three days, the process explained above is performed
three times. The top twenty articles were selected by
the criterion every day. That is, k = 20. Table 5 shows
the statistics of the total 60 articles. The shortest arti-
cle consists of 277 characters, and the longest article
has 6,077 characters. On average, an article is writ-
ten with 990.68 characters. The minimum number of
sentences in an article is 4, and the maximum number
of sentences is 199. An article is composed of 22.85
sentences on average. The average number of words in
a sentence is 15.73, whereas a sentence length ranges
from 4 to 100 words. The shortest article has 71 words,
and the longest article has 2,034 words. One article has
approximately 301.61 words on average.
Three human annotators labeled the scores of the
news articles manually. The annotators were the same
persons who labeled the readability scores. Similar to
the previous experiments, 7 days intervals was placed
among devices to reduce the memory effect. The same
two questions used in the previous section were used
again for this experiment. The annotators assigned a
score between 1 and 5 to every article for each ques-
tion. The final score of an article was obtained by aver-
aging six scores (two questions from three annotators).
Table 6 summarizes the scores of the articles by the
human annotators. As shown in this table, the article
scores vary for all reading devices. The average scores
for smart phone, tablet, and paper are 3.513, 3.344,
and 3.250 respectively. The Kappa value for the ‘smart
phone’ is 0.402, and that for both the ‘tablet’ and the
‘paper’ is 0.393. Thus, the value of ‘smart phone’ falls
into moderate agreement, whereas those of the ‘tablet’
and ‘paper’ correspond to fair agreement. The perfor-
mance of the news article recommendation is evaluated
with the Normalized Discounted Cumulative Gain at
top P (NDCG@P) (J¨arvelin and Kek¨al¨ainen, 2002).
</bodyText>
<figureCaption confidence="0.99410625">
Figure 2: NDCG@k* scores with various k* for the
smart phone.
Figure 3: NDCG@k* scores with various k* for the
tablet.
</figureCaption>
<subsectionHeader confidence="0.502557">
5.2.2 Experimental Results
</subsectionHeader>
<bodyText confidence="0.999156263157895">
For the a baseline criterion, we use the news article
recommendation system in Naver, which recommends
news article by the number of article hits. Figures 2 to 4
show the NDCG@k* scores with 1 &lt; k* &lt; 10 for the
three devices. Each graph in these figures compares the
performance of various devices when the readability
for a specific device is used. That is, Figure 2 depicts
the NDCG@k* scores for the recommended news arti-
cles when the articles are shown in the smart phone, the
tablet, and the paper respectively. In computing their
NDCG@k* scores, the news articles are re-ranked by
readability for the smart phone. Therefore, in this fig-
ure we expect that the NDCG@k* score for using the
smart phone is higher than those for using the tablet and
paper. In the same way, Figure 3 and Figure 4 compare
the NDCG@k* scores when the readabilities for the
tablet and paper are used.
In all three graphs, the best news recommendation
performance is achieved when the device used to read
</bodyText>
<page confidence="0.997167">
1402
</page>
<figureCaption confidence="0.987895">
Figure 4: NDCG@k∗ scores with various k∗ for the
paper.
</figureCaption>
<bodyText confidence="0.999832303030303">
news articles is the same as the device used for read-
ability. In Figure 2, the use of the smart phone outper-
forms those of other devices when k∗ ≥ 6. This proves
that the quality of highly ranked news articles is much
better for the smart phone than for other devices, when
the readability for smart phone is used.
Figure 3 shows the NDCG@k∗ scores for using var-
ious devices when the news articles are re-ranked by
readability for the tablet. In this figure, the use of
the tablet as a reading device is better than using the
smart phone or the paper. The performance difference
is largest at k∗ = 3. The difference becomes smaller
as k∗ increases up to 10, but the performance of tablet
is still higher than those of others. In Figure 2 and 3,
when k∗ = 1, the baseline outperforms other devices.
We believe this happens because the baseline chooses
news articles by user-hit. Therefore, many articles rec-
ommended by the baseline are interesting because peo-
ple tend to click more often when an article is inter-
esting. As noted, readability reflects users’ interests,
which leads to high performance of the baseline. The
performance of paper is best in Figure 4, since the ar-
ticles are re-ranked by the readability for paper. Paper
outperforms all other devices for all k∗s. Note that the
performances of the baseline are always lowest regard-
less of reading device.
From all results above, we can infer that the use of
device-dependent readability is helpful to news article
recommendation. This is because the readability fac-
tors that affect the readers of news articles are different
according to the reading device. Therefore, it is im-
portant to reflect the characteristics of a reading device
when recommending news articles.
</bodyText>
<sectionHeader confidence="0.999486" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999982173913044">
In this paper, we have proposed a device-dependent
readability. Since a reading device is one of the most
important features of readability, different weights have
been assigned to the readability factors according to de-
vice type. We have shown that the important readabil-
ity factors are distinct according to the reading device
by investigating the correlation between the readability
factors and the reading device. Through the correlation,
we found that tablet shares many important factors with
both smart phone and paper.
The experiments on the news articles collected from
an Internet portal proved that readability is actually af-
fected by the reading device. In addition, the validity of
the device-dependent readability was shown by apply-
ing it to the news article recommendation. The news
articles were first ranked by the criterion of the recom-
mendation system. Then, they were re-ranked by the
device-dependent readability. Our experiments showed
that the recommendation performance of the re-ranked
articles gets best when the device used for readability is
the same as the reading device. These two types of ex-
periments proved the importance and effectiveness of
the device-dependent readability.
</bodyText>
<sectionHeader confidence="0.99856" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999839714285714">
This work was supported by the IT R&amp;D program of
MSIP/KEIT (10044494, WiseKB: Big data based self-
evolving knowledge base and reasoning platform) and
the Industrial Strategic Technology Development Pro-
gram (10035348, Development of a Cognitive Planning
and Learning Model for Mobile Platforms) funded by
the Ministry of Knowledge Economy(MKE, Korea).
</bodyText>
<sectionHeader confidence="0.998037" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999025666666667">
Alan Bailin and Ann Grafstein. 2001. The linguistic
assumptions underlying readability formulae: A cri-
tique. Language &amp; Communication, 21(3):285–301.
Regina Barzilay and Mirella Lapata. 2008. Modeling
local coherence: An entity-based approach. Compu-
tational Linguistics, 34(1):1–34.
Edgar Dale and Jeanne Chall. 1949. The concept of
readability. Elementary English, 26(1):19–26.
Abhinandan Das, Mayur Datar, Ashutosh Garg, and
Shyam Rajaram. 2007. Google news personaliza-
tion: scalable online collaborative filtering. In Pro-
ceedings of the 16th International Conference on
World Wide Web, pages 271–280.
Joseph Fleiss. 1971. Measuring nominal scale agree-
ment among many raters. Psychological bulletin,
76(5):378–382.
Rudolph Flesch. 1948. A new readability yardstick.
Journal of Applied Psychology, 32(3):221–233.
Thomas Franc¸ois and C´edrick Fairon. 2012. An
AI readability formula for French as a foreign lan-
guage. In Proceedings of the 2012 Joint Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
pages 466–477.
</reference>
<page confidence="0.566292">
1403
</page>
<reference confidence="0.998168144578313">
Michael Halliday and Ruqaiya Hasan. 1976. Cohesion
in English. Longman Group Ltd.
Satoshi Hasegawa, Kazuhiro Fujikake, Masako Omori,
and Masaru Miyao. 2008. Readability of charac-
ters on mobile phone liquid crystal displays. In-
ternational Journal of Occupational Safety and Er-
gonomics (JOSE), 14(3):293–304.
Michael Heilman, Kevyn Collins-Thompson, and
Maxine Eskenazi. 2008. An analysis of statistical
models and features for reading difficulty prediction.
In Proceedings of the Third Workshop on Innovative
Use of NLP for Building Educational Applications,
pages 71–79.
Kristina Hillbom. 2009. Newspaper Readability: a
Broadsheet vs. a Tabloid. Ph.D. thesis, University of
G¨avle.
M˚ans Huld´en. 2004. Linguistic complexity in
two major american newspapers and the associated
press newswire, 1900–2000. Master’s thesis, ˚Abo
Akademi University.
Shoaib Jameel, Wai Lam, and Xiaojun Qian. 2012.
Ranking text documents based on conceptual dif-
ficulty using term embedding and sequential dis-
course cohesion. In Proceedings of the The 2012
IEEE/WIC/ACM International Joint Conferences on
Web Intelligence and Intelligent Agent Technology-
Volume 01, pages 145–152.
Kalervo J¨arvelin and Jaana Kek¨al¨ainen. 2002. Cu-
mulated gain-based evaluation of IR techniques.
ACM Transactions on Information Systems (TOIS),
20(4):422–446.
J. Peter Kincaid, Robert Fishburne Jr., Richard Rogers,
and Brad Chissom. 1975. Derivation of new read-
ability formulas (automated readability index, fog
count and flesch reading ease formula) for navy en-
listed personnel. Technical report, DTIC Document.
Harry Kitson. 1927. The mind of the buyer. MacMil-
lan Company.
Lihong Li, Wei Chu, John Langford, and Robert E.
Schapire. 2010. A contextual-bandit approach to
personalized news article recommendation. In Pro-
ceedings of the 19th International Conference on
World Wide Web, pages 661–670.
Jiahui Liu, Peter Dolan, and Elin R. Pedersen. 2010.
Personalized news recommendation based on click
behavior. In Proceedings of the 15th International
Conference on Intelligent User Interfaces, pages 31–
40.
Yi Ma, Eric Fosler-Lussier, and Robert Lofthus. 2012.
Ranking-based readability assessment for early pri-
mary children’s literature. In Proceedings of the
2012 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 548–552.
Naoaki Okazaki, Yutaka Matsuo, and Mitsuru
Ishizuka. 2005. Improving chronological ordering
of sentences extracted from multiple newspaper ar-
ticles. ACM Transactions on Asian Language Infor-
mation Processing (TALIP), 4(3):321–339.
Gustav ¨Oquist. 2006. Evaluating readability on mo-
bile devices. Ph.D. thesis, Uppsala University.
Emily Pitler and Ani Nenkova. 2008. Revisiting read-
ability: A unified framework for predicting text qual-
ity. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing, pages
186–195.
Jack Richards, John Platt, Heidi Platt, and Christophe
Candlin. 1992. Longman Dictionary of Language
Teaching and Applied Linguistics, volume 78. Long-
man London.
Sarah Schwarm and Mari Ostendorf. 2005. Reading
level assessment using support vector machines and
statistical language models. In Proceedings of the
43rd Annual Meeting on Association for Computa-
tional Linguistics, pages 523–530.
Xin Yan, Dawei Song, and Xue Li. 2006. Concept-
based document readability in domain specific infor-
mation retrieval. In Proceedings of the 15th ACM In-
ternational Conference on Information and Knowl-
edge Management, pages 540–549.
Mostafa Zamanian and Pooneh Heydari. 2012. Read-
ability of texts: State of the art. Theory and Practice
in Language Studies, 2(1):43–53.
</reference>
<page confidence="0.996494">
1404
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.872665">
<title confidence="0.998824">Device-Dependent Readability for Improved Text Understanding</title>
<author confidence="0.998583">A-Yeong Kim Hyun-Je Song Seong-Bae Park Sang-Jo Lee</author>
<affiliation confidence="0.962468">School of Computer Science and Kyungpook National</affiliation>
<address confidence="0.946945">Daegu, 702-701,</address>
<email confidence="0.989605">sjlee@knu.ac.kr</email>
<abstract confidence="0.9983133">Readability is used to provide users with highquality service in text recommendation or text visualization. With the increasing use of handheld devices, reading device is regarded as an important factor for readability. Therefore, this paper investigates the relationship between readability and reading devices such as a smart phone, a tablet, and paper. We suggest readability factors that are strongly related with the readability of a specific device by showing the correlations between various factors in each device and human-rated readability. Our experimental results show that each device has its own readability characteristics, and thus different weights should be imposed on readability factors according to the device type. In order to prove the usefulness of the results, we apply the device-dependent readability to news article recommendation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Alan Bailin</author>
<author>Ann Grafstein</author>
</authors>
<title>The linguistic assumptions underlying readability formulae: A critique.</title>
<date>2001</date>
<journal>Language &amp; Communication,</journal>
<volume>21</volume>
<issue>3</issue>
<contexts>
<context position="14131" citStr="Bailin and Grafstein, 2001" startWordPosition="2258" endWordPosition="2261">n the other hand, the relationship of NP and VP to readability are not one way. The large number of noun phrases in a text requires a reader to remember more items (Barzilay and Lapata, 2008; Pitler and Nenkova, 2008). However, it also makes the text more interesting. The texts written for adults actually contain more entities than those written for children (Barzilay and Lapata, 2008). The same is true for VP. The large number of verb phrases in a sentence makes the sentence more complex. However, people feel that a text is more easier to comprehend when related clauses are grouped together (Bailin and Grafstein, 2001). 3.4 Lexical Cohesion Lexical cohesion denotes how the sentences in a text are semantically connected. People usually bring continuous sentences into their mind at the same time, and interpret them as a single unit (Okazaki et al., 2005). In other words, a reader prefers text whose sentences are smoothly connected to text whose sentences are independent of one another. Therefore, sentence continuity plays a primary role in understanding an entire text. In the classic study of cohesion, various uses of cohesive elements such as pronouns, definite articles, and topic continuity have been discus</context>
</contexts>
<marker>Bailin, Grafstein, 2001</marker>
<rawString>Alan Bailin and Ann Grafstein. 2001. The linguistic assumptions underlying readability formulae: A critique. Language &amp; Communication, 21(3):285–301.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Mirella Lapata</author>
</authors>
<title>Modeling local coherence: An entity-based approach.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>1</issue>
<contexts>
<context position="13694" citStr="Barzilay and Lapata, 2008" startWordPosition="2185" endWordPosition="2189">umber of noun phrases per sentence (NP), the average number of verb phrases per sentence (VP), and the average number of subordinate clauses per sentence (SBAR) as syntactic factors. These four factors were defined by Schwarm and Ostendorf (2005). A reader regards a text as difficult when the sentences in the text have large parse tree depths or many subordinate clauses. Thus, PTD and SBAR are related negatively with readability. On the other hand, the relationship of NP and VP to readability are not one way. The large number of noun phrases in a text requires a reader to remember more items (Barzilay and Lapata, 2008; Pitler and Nenkova, 2008). However, it also makes the text more interesting. The texts written for adults actually contain more entities than those written for children (Barzilay and Lapata, 2008). The same is true for VP. The large number of verb phrases in a sentence makes the sentence more complex. However, people feel that a text is more easier to comprehend when related clauses are grouped together (Bailin and Grafstein, 2001). 3.4 Lexical Cohesion Lexical cohesion denotes how the sentences in a text are semantically connected. People usually bring continuous sentences into their mind a</context>
</contexts>
<marker>Barzilay, Lapata, 2008</marker>
<rawString>Regina Barzilay and Mirella Lapata. 2008. Modeling local coherence: An entity-based approach. Computational Linguistics, 34(1):1–34.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edgar Dale</author>
<author>Jeanne Chall</author>
</authors>
<title>The concept of readability.</title>
<date>1949</date>
<journal>Elementary English,</journal>
<volume>26</volume>
<issue>1</issue>
<contexts>
<context position="6364" citStr="Dale and Chall (1949)" startWordPosition="975" endWordPosition="978">Finally, we summarize our research. 2 Related work The history of readability studies began in the 1800s. Early studies focused on the frequency of easy words, sentence length, and word length (Huld´en, 2004). Flesch designed a formula to calculate “reading ease” using only the average word length and sentence length (Flesch, 1948). He adjusted the relative importance between word length and sentence length using 100 words selected randomly from a corpus. This formula is called the Flesch-Kincaid formula, and is generally used in measuring the readability of a textbook (Kincaid et al., 1975). Dale and Chall (1949) defined a list of 3,000 easy words. Then, they used the average sentence length and the percentage of words not included in the list. These studies simply used superficial factors, and thus do not reflect syntactic factors. Recent studies on readability use various factors including syntactic ones, and combine them to produce a highly predictive model of readability. Franc¸ois and Faircon (2012) proposed a readability formula with 46 textual factors for French as a foreign language. The factors represent lexical, syntactic, and semantic characteristics of sentences, and the specificities of F</context>
<context position="9373" citStr="Dale and Chall, 1949" startWordPosition="1453" endWordPosition="1456">d by reading devices. 3 Readability Factors Table 1 lists the readability factors used in this paper. Basically, they are based on the factors proposed by Pitler and Nenkova (2008). However, some factors are excluded and some new factors are added. This is because some of their factors are computationally infeasible and language-dependent. As a result, we have thirteen readability factors. These readability factors are divided into four types: superficial, lexical, syntactic factors, and lexical cohesion. 3.1 Superficial Factors Superficial factors were used in most early readability studies (Dale and Chall, 1949; Flesch, 1948; Kincaid et al., 1975), and reflect the construction of a text. We investigate four factors: text length (TL), sentence length (SL), average number of words per sentence (WS), and average number of characters per word (CW). Since longer text is perceived as “harder-to-read” than short one, these factors are all reciprocally related with readability. The first two factors are related to length. TL counts the number of characters in a text, whereas SL computes the number of sentences. When a writer attempts to write many topics in a text, she tends to use many kinds of words simul</context>
</contexts>
<marker>Dale, Chall, 1949</marker>
<rawString>Edgar Dale and Jeanne Chall. 1949. The concept of readability. Elementary English, 26(1):19–26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Abhinandan Das</author>
<author>Mayur Datar</author>
<author>Ashutosh Garg</author>
<author>Shyam Rajaram</author>
</authors>
<title>Google news personalization: scalable online collaborative filtering.</title>
<date>2007</date>
<booktitle>In Proceedings of the 16th International Conference on World Wide Web,</booktitle>
<pages>271--280</pages>
<contexts>
<context position="17027" citStr="Das et al., 2007" startWordPosition="2733" endWordPosition="2736">e Recommendation by Device-Dependent Readability The fact that the weights wi|d in Equation (2) are different for each device d implies that the readability measurement should be different depending on the device type. In order to see the usefulness of this devicedependent readability, we apply it to news article recommendation. News article recommendation aims to provide a user with news articles that interest the user. Thus, it selects a few articles that meet user preference from a gigantic amount of news events. Various methods have reported notable results in news article recommendation (Das et al., 2007; Li et al., 2010; Liu et al., 2010). In addition, with the recent interest in handheld devices, the demand for news recommendation on hand-held devices is increasing. However, there has been, at least as far as we know, no study on the readability of hand-held devices. Device-dependent readability is reflected into news article recommendation through a re-ranking framework. Figure 1 depicts the overall process of suggesting news articles for a specific device with the devicedependent readability. The point of this figure is to measure how appropriate a news article is for a specific reading d</context>
</contexts>
<marker>Das, Datar, Garg, Rajaram, 2007</marker>
<rawString>Abhinandan Das, Mayur Datar, Ashutosh Garg, and Shyam Rajaram. 2007. Google news personalization: scalable online collaborative filtering. In Proceedings of the 16th International Conference on World Wide Web, pages 271–280.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Fleiss</author>
</authors>
<title>Measuring nominal scale agreement among many raters.</title>
<date>1971</date>
<journal>Psychological bulletin,</journal>
<pages>76--5</pages>
<contexts>
<context position="25344" citStr="Fleiss, 1971" startWordPosition="4150" endWordPosition="4151">readability score of one human annotator is composed with the average of two questions (wellwritten, interesting). We used the average of three human annotators’ readability scores in our experiments. Table 3 shows the readability scores of the articles for each device. According to this table, the readability score ranges from 1.67 to 5 for the smart phone, 1.33 to 5 for the tablet, and ranges from 2 to 5 for the paper. The average readability is 3.423 for the smart phone, 3.531 for the tablet, and 3.360 for the paper. To see the inter-judge agreement among annotators, the Kappa coefficient (Fleiss, 1971) is used. The Kappa values for the ‘smart phone’, ‘tablet’, and ‘paper’ are 0.342, 0.333, and 0.361, respectively. All these values correspond to fair agreement. 5.1.2 Experimental Results In order to see the importance of each factor in a specific device, we adopt the Pearson correlation coefficients between readability factors and reading devices. Table 4 lists the four most important factors in each device and their Pearson correlation coefficients. Especially, p-value is smaller than 0.05 for all factors in this table. For the smart phone, SL, the number of sentences in a text, is the most</context>
</contexts>
<marker>Fleiss, 1971</marker>
<rawString>Joseph Fleiss. 1971. Measuring nominal scale agreement among many raters. Psychological bulletin, 76(5):378–382.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rudolph Flesch</author>
</authors>
<title>A new readability yardstick.</title>
<date>1948</date>
<journal>Journal of Applied Psychology,</journal>
<volume>32</volume>
<issue>3</issue>
<contexts>
<context position="6076" citStr="Flesch, 1948" startWordPosition="931" endWordPosition="932">ess of the device-dependent readability. In the experiments, we present the experimental results on the relationship between reading devices and readability. We also describe the experiments on news recommendation using the device-dependent readability and present their results. Finally, we summarize our research. 2 Related work The history of readability studies began in the 1800s. Early studies focused on the frequency of easy words, sentence length, and word length (Huld´en, 2004). Flesch designed a formula to calculate “reading ease” using only the average word length and sentence length (Flesch, 1948). He adjusted the relative importance between word length and sentence length using 100 words selected randomly from a corpus. This formula is called the Flesch-Kincaid formula, and is generally used in measuring the readability of a textbook (Kincaid et al., 1975). Dale and Chall (1949) defined a list of 3,000 easy words. Then, they used the average sentence length and the percentage of words not included in the list. These studies simply used superficial factors, and thus do not reflect syntactic factors. Recent studies on readability use various factors including syntactic ones, and combine</context>
<context position="9387" citStr="Flesch, 1948" startWordPosition="1457" endWordPosition="1458">3 Readability Factors Table 1 lists the readability factors used in this paper. Basically, they are based on the factors proposed by Pitler and Nenkova (2008). However, some factors are excluded and some new factors are added. This is because some of their factors are computationally infeasible and language-dependent. As a result, we have thirteen readability factors. These readability factors are divided into four types: superficial, lexical, syntactic factors, and lexical cohesion. 3.1 Superficial Factors Superficial factors were used in most early readability studies (Dale and Chall, 1949; Flesch, 1948; Kincaid et al., 1975), and reflect the construction of a text. We investigate four factors: text length (TL), sentence length (SL), average number of words per sentence (WS), and average number of characters per word (CW). Since longer text is perceived as “harder-to-read” than short one, these factors are all reciprocally related with readability. The first two factors are related to length. TL counts the number of characters in a text, whereas SL computes the number of sentences. When a writer attempts to write many topics in a text, she tends to use many kinds of words simultaneously. As </context>
</contexts>
<marker>Flesch, 1948</marker>
<rawString>Rudolph Flesch. 1948. A new readability yardstick. Journal of Applied Psychology, 32(3):221–233.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Franc¸ois</author>
<author>C´edrick Fairon</author>
</authors>
<title>An AI readability formula for French as a foreign language.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>466--477</pages>
<marker>Franc¸ois, Fairon, 2012</marker>
<rawString>Thomas Franc¸ois and C´edrick Fairon. 2012. An AI readability formula for French as a foreign language. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 466–477.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Halliday</author>
<author>Ruqaiya Hasan</author>
</authors>
<title>Cohesion in English.</title>
<date>1976</date>
<publisher>Longman Group Ltd.</publisher>
<contexts>
<context position="14761" citStr="Halliday and Hasan, 1976" startWordPosition="2357" endWordPosition="2360"> Lexical Cohesion Lexical cohesion denotes how the sentences in a text are semantically connected. People usually bring continuous sentences into their mind at the same time, and interpret them as a single unit (Okazaki et al., 2005). In other words, a reader prefers text whose sentences are smoothly connected to text whose sentences are independent of one another. Therefore, sentence continuity plays a primary role in understanding an entire text. In the classic study of cohesion, various uses of cohesive elements such as pronouns, definite articles, and topic continuity have been discussed (Halliday and Hasan, 1976). This paper uses the average cosine similarity (COS), word overlap (WO), word overlap over just nouns and pronouns (NPO) between pairs of adjacent sentences, and the average number of pronouns per sentence (PRP). COS, WO, and NPO are superficial measures of topic continuity, whereas PRP is an indicative feature of sentence continuity. High values for these factors imply that the sentences in the text are related somehow. Therefore, these factors are believed to be related positively with readability. 1398 3.5 Measurement of Readability When a reading device d is given, the readability of text</context>
</contexts>
<marker>Halliday, Hasan, 1976</marker>
<rawString>Michael Halliday and Ruqaiya Hasan. 1976. Cohesion in English. Longman Group Ltd.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satoshi Hasegawa</author>
<author>Kazuhiro Fujikake</author>
<author>Masako Omori</author>
<author>Masaru Miyao</author>
</authors>
<title>Readability of characters on mobile phone liquid crystal displays.</title>
<date>2008</date>
<booktitle>International Journal of Occupational Safety and Ergonomics (JOSE),</booktitle>
<pages>14--3</pages>
<contexts>
<context position="1609" citStr="Hasegawa et al., 2008" startWordPosition="232" endWordPosition="235">g to the device type. In order to prove the usefulness of the results, we apply the device-dependent readability to news article recommendation. 1 Introduction Readability is a function that maps a given text into a readability score by considering “how easily the text is read and understood” (Richards et al., 1992; Zamanian and Heydari, 2012). Normally, the readability score is formulated as a combination of various factors. These factors reflect the easiness and understanding of the text and include text presentation format, font size, average ratio of annotated images, and sentence length (Hasegawa et al., 2008; Kitson, 1927; Ma et al., 2012; ¨Oquist, 2006). Therefore, readability can be used to provide satisfiable services in text recommendation or text visualization. The study on readability has begun in the education field to measure the level of a text. With the success of using readability in education (Franc¸ois and Fairon, 2012; Heilman et al., 2008; Ma et al., 2012), readability has been used in a range of domains recently. For example, in document retrieval, readability is used to provide documents to non-expert users so that they can read the retrieved documents easily (Jameel et al., 2012</context>
<context position="8325" citStr="Hasegawa et al. (2008)" startWordPosition="1292" endWordPosition="1295">differentiated according to reading device, a reading device should be considered when computing the readability of a given text. To the best of our knowledge, there are few studies on the readability on mobile devices that do not consider language-related aspects. Most studies on mobile devices focused on the development of new text format and layout to help users read documents easily. ¨Oquist (2006) proposed a new text presentation format called the dynamic Rapid Serial Visual Presentation. According to his experimental results, this format helps to reduce eye movements. On the other hand, Hasegawa et al. (2008) evaluated the readability of documents on mobile devices with regard to screen and font size. They reported that the readability is improved when the characters are vertically enlarged. Readability on mobile devices is not reflected only by the visualization factors, but also by textual factors. Therefore, this paper explores the readability factors that reflect the lexical and grammatical complexity of text and are affected by reading devices. 3 Readability Factors Table 1 lists the readability factors used in this paper. Basically, they are based on the factors proposed by Pitler and Nenkov</context>
</contexts>
<marker>Hasegawa, Fujikake, Omori, Miyao, 2008</marker>
<rawString>Satoshi Hasegawa, Kazuhiro Fujikake, Masako Omori, and Masaru Miyao. 2008. Readability of characters on mobile phone liquid crystal displays. International Journal of Occupational Safety and Ergonomics (JOSE), 14(3):293–304.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Heilman</author>
<author>Kevyn Collins-Thompson</author>
<author>Maxine Eskenazi</author>
</authors>
<title>An analysis of statistical models and features for reading difficulty prediction.</title>
<date>2008</date>
<booktitle>In Proceedings of the Third Workshop on Innovative Use of NLP for Building Educational Applications,</booktitle>
<pages>71--79</pages>
<contexts>
<context position="1961" citStr="Heilman et al., 2008" startWordPosition="288" endWordPosition="291">ly, the readability score is formulated as a combination of various factors. These factors reflect the easiness and understanding of the text and include text presentation format, font size, average ratio of annotated images, and sentence length (Hasegawa et al., 2008; Kitson, 1927; Ma et al., 2012; ¨Oquist, 2006). Therefore, readability can be used to provide satisfiable services in text recommendation or text visualization. The study on readability has begun in the education field to measure the level of a text. With the success of using readability in education (Franc¸ois and Fairon, 2012; Heilman et al., 2008; Ma et al., 2012), readability has been used in a range of domains recently. For example, in document retrieval, readability is used to provide documents to non-expert users so that they can read the retrieved documents easily (Jameel et al., 2012; Yan et al., 2006). In text mining, readability has been employed to analyze the characteristics of text. Especially, Hillbom showed the differences in readability between broadsheet newspapers and tabloids that share a similar political stance (Hillbom, 2009). There is one important issue of readability that has not been studied in natural language</context>
<context position="10195" citStr="Heilman et al., 2008" startWordPosition="1596" endWordPosition="1599">e number of characters per word (CW). Since longer text is perceived as “harder-to-read” than short one, these factors are all reciprocally related with readability. The first two factors are related to length. TL counts the number of characters in a text, whereas SL computes the number of sentences. When a writer attempts to write many topics in a text, she tends to use many kinds of words simultaneously. As a result, the text becomes longer and more complex. Such long length of text disturbs a reader’s comprehension of the text, and then it is more difficult for the reader to read the text (Heilman et al., 2008). WS counts the average number of words per sentence, and CW reflects the average number of characters per word. When they are large, the sentence is difficult to read, which leads to difficulties in understanding the text. Especially, CW reflects compound nouns and technical words. For instance, compound nouns in Korean are usually long, because there is no spacing between words in a compound noun. For example, let us consider a compound noun, “Daehanmingukjungboo,” which means the Korean government. Actually this compound noun consists of two independent nouns. 1397 Type of Factors Abbr. Des</context>
</contexts>
<marker>Heilman, Collins-Thompson, Eskenazi, 2008</marker>
<rawString>Michael Heilman, Kevyn Collins-Thompson, and Maxine Eskenazi. 2008. An analysis of statistical models and features for reading difficulty prediction. In Proceedings of the Third Workshop on Innovative Use of NLP for Building Educational Applications, pages 71–79.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Hillbom</author>
</authors>
<title>Newspaper Readability: a Broadsheet vs. a Tabloid.</title>
<date>2009</date>
<tech>Ph.D. thesis,</tech>
<institution>University of G¨avle.</institution>
<contexts>
<context position="2470" citStr="Hillbom, 2009" startWordPosition="370" endWordPosition="371">ext. With the success of using readability in education (Franc¸ois and Fairon, 2012; Heilman et al., 2008; Ma et al., 2012), readability has been used in a range of domains recently. For example, in document retrieval, readability is used to provide documents to non-expert users so that they can read the retrieved documents easily (Jameel et al., 2012; Yan et al., 2006). In text mining, readability has been employed to analyze the characteristics of text. Especially, Hillbom showed the differences in readability between broadsheet newspapers and tabloids that share a similar political stance (Hillbom, 2009). There is one important issue of readability that has not been studied in natural language processing. It is a reading device. That is, previous studies focused only on text printed on paper. However, with the increasing use of hand-held devices, people in these days use various reading devices such as a tablet and a smart phone as well as a paper. Readability score can be different according to the device type, because each device has its own idiosyncrasy. For example, assume that a system recommends the same news article to both user A who reads it in her smart phone and user B who reads it</context>
</contexts>
<marker>Hillbom, 2009</marker>
<rawString>Kristina Hillbom. 2009. Newspaper Readability: a Broadsheet vs. a Tabloid. Ph.D. thesis, University of G¨avle.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M˚ans Huld´en</author>
</authors>
<title>Linguistic complexity in two major american newspapers and the associated press newswire, 1900–2000. Master’s thesis,</title>
<date>2004</date>
<institution>Abo Akademi University.</institution>
<marker>Huld´en, 2004</marker>
<rawString>M˚ans Huld´en. 2004. Linguistic complexity in two major american newspapers and the associated press newswire, 1900–2000. Master’s thesis, ˚Abo Akademi University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shoaib Jameel</author>
<author>Wai Lam</author>
<author>Xiaojun Qian</author>
</authors>
<title>Ranking text documents based on conceptual difficulty using term embedding and sequential discourse cohesion.</title>
<date>2012</date>
<booktitle>In Proceedings of the The 2012 IEEE/WIC/ACM International Joint Conferences on Web Intelligence and Intelligent Agent TechnologyVolume 01,</booktitle>
<pages>145--152</pages>
<contexts>
<context position="2209" citStr="Jameel et al., 2012" startWordPosition="330" endWordPosition="333">segawa et al., 2008; Kitson, 1927; Ma et al., 2012; ¨Oquist, 2006). Therefore, readability can be used to provide satisfiable services in text recommendation or text visualization. The study on readability has begun in the education field to measure the level of a text. With the success of using readability in education (Franc¸ois and Fairon, 2012; Heilman et al., 2008; Ma et al., 2012), readability has been used in a range of domains recently. For example, in document retrieval, readability is used to provide documents to non-expert users so that they can read the retrieved documents easily (Jameel et al., 2012; Yan et al., 2006). In text mining, readability has been employed to analyze the characteristics of text. Especially, Hillbom showed the differences in readability between broadsheet newspapers and tabloids that share a similar political stance (Hillbom, 2009). There is one important issue of readability that has not been studied in natural language processing. It is a reading device. That is, previous studies focused only on text printed on paper. However, with the increasing use of hand-held devices, people in these days use various reading devices such as a tablet and a smart phone as well</context>
</contexts>
<marker>Jameel, Lam, Qian, 2012</marker>
<rawString>Shoaib Jameel, Wai Lam, and Xiaojun Qian. 2012. Ranking text documents based on conceptual difficulty using term embedding and sequential discourse cohesion. In Proceedings of the The 2012 IEEE/WIC/ACM International Joint Conferences on Web Intelligence and Intelligent Agent TechnologyVolume 01, pages 145–152.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kalervo J¨arvelin</author>
<author>Jaana Kek¨al¨ainen</author>
</authors>
<title>Cumulated gain-based evaluation of IR techniques.</title>
<date>2002</date>
<journal>ACM Transactions on Information Systems (TOIS),</journal>
<volume>20</volume>
<issue>4</issue>
<marker>J¨arvelin, Kek¨al¨ainen, 2002</marker>
<rawString>Kalervo J¨arvelin and Jaana Kek¨al¨ainen. 2002. Cumulated gain-based evaluation of IR techniques. ACM Transactions on Information Systems (TOIS), 20(4):422–446.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Peter Kincaid</author>
<author>Robert Fishburne Jr</author>
<author>Richard Rogers</author>
<author>Brad Chissom</author>
</authors>
<title>Derivation of new readability formulas (automated readability index, fog count and flesch reading ease formula) for navy enlisted personnel.</title>
<date>1975</date>
<tech>Technical report, DTIC Document.</tech>
<contexts>
<context position="6341" citStr="Kincaid et al., 1975" startWordPosition="970" endWordPosition="974">present their results. Finally, we summarize our research. 2 Related work The history of readability studies began in the 1800s. Early studies focused on the frequency of easy words, sentence length, and word length (Huld´en, 2004). Flesch designed a formula to calculate “reading ease” using only the average word length and sentence length (Flesch, 1948). He adjusted the relative importance between word length and sentence length using 100 words selected randomly from a corpus. This formula is called the Flesch-Kincaid formula, and is generally used in measuring the readability of a textbook (Kincaid et al., 1975). Dale and Chall (1949) defined a list of 3,000 easy words. Then, they used the average sentence length and the percentage of words not included in the list. These studies simply used superficial factors, and thus do not reflect syntactic factors. Recent studies on readability use various factors including syntactic ones, and combine them to produce a highly predictive model of readability. Franc¸ois and Faircon (2012) proposed a readability formula with 46 textual factors for French as a foreign language. The factors represent lexical, syntactic, and semantic characteristics of sentences, and</context>
<context position="9410" citStr="Kincaid et al., 1975" startWordPosition="1459" endWordPosition="1462">Factors Table 1 lists the readability factors used in this paper. Basically, they are based on the factors proposed by Pitler and Nenkova (2008). However, some factors are excluded and some new factors are added. This is because some of their factors are computationally infeasible and language-dependent. As a result, we have thirteen readability factors. These readability factors are divided into four types: superficial, lexical, syntactic factors, and lexical cohesion. 3.1 Superficial Factors Superficial factors were used in most early readability studies (Dale and Chall, 1949; Flesch, 1948; Kincaid et al., 1975), and reflect the construction of a text. We investigate four factors: text length (TL), sentence length (SL), average number of words per sentence (WS), and average number of characters per word (CW). Since longer text is perceived as “harder-to-read” than short one, these factors are all reciprocally related with readability. The first two factors are related to length. TL counts the number of characters in a text, whereas SL computes the number of sentences. When a writer attempts to write many topics in a text, she tends to use many kinds of words simultaneously. As a result, the text beco</context>
</contexts>
<marker>Kincaid, Jr, Rogers, Chissom, 1975</marker>
<rawString>J. Peter Kincaid, Robert Fishburne Jr., Richard Rogers, and Brad Chissom. 1975. Derivation of new readability formulas (automated readability index, fog count and flesch reading ease formula) for navy enlisted personnel. Technical report, DTIC Document.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harry Kitson</author>
</authors>
<title>The mind of the buyer.</title>
<date>1927</date>
<publisher>MacMillan Company.</publisher>
<contexts>
<context position="1623" citStr="Kitson, 1927" startWordPosition="236" endWordPosition="237">n order to prove the usefulness of the results, we apply the device-dependent readability to news article recommendation. 1 Introduction Readability is a function that maps a given text into a readability score by considering “how easily the text is read and understood” (Richards et al., 1992; Zamanian and Heydari, 2012). Normally, the readability score is formulated as a combination of various factors. These factors reflect the easiness and understanding of the text and include text presentation format, font size, average ratio of annotated images, and sentence length (Hasegawa et al., 2008; Kitson, 1927; Ma et al., 2012; ¨Oquist, 2006). Therefore, readability can be used to provide satisfiable services in text recommendation or text visualization. The study on readability has begun in the education field to measure the level of a text. With the success of using readability in education (Franc¸ois and Fairon, 2012; Heilman et al., 2008; Ma et al., 2012), readability has been used in a range of domains recently. For example, in document retrieval, readability is used to provide documents to non-expert users so that they can read the retrieved documents easily (Jameel et al., 2012; Yan et al., </context>
</contexts>
<marker>Kitson, 1927</marker>
<rawString>Harry Kitson. 1927. The mind of the buyer. MacMillan Company.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lihong Li</author>
<author>Wei Chu</author>
<author>John Langford</author>
<author>Robert E Schapire</author>
</authors>
<title>A contextual-bandit approach to personalized news article recommendation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 19th International Conference on World Wide Web,</booktitle>
<pages>661--670</pages>
<contexts>
<context position="17044" citStr="Li et al., 2010" startWordPosition="2737" endWordPosition="2740">y Device-Dependent Readability The fact that the weights wi|d in Equation (2) are different for each device d implies that the readability measurement should be different depending on the device type. In order to see the usefulness of this devicedependent readability, we apply it to news article recommendation. News article recommendation aims to provide a user with news articles that interest the user. Thus, it selects a few articles that meet user preference from a gigantic amount of news events. Various methods have reported notable results in news article recommendation (Das et al., 2007; Li et al., 2010; Liu et al., 2010). In addition, with the recent interest in handheld devices, the demand for news recommendation on hand-held devices is increasing. However, there has been, at least as far as we know, no study on the readability of hand-held devices. Device-dependent readability is reflected into news article recommendation through a re-ranking framework. Figure 1 depicts the overall process of suggesting news articles for a specific device with the devicedependent readability. The point of this figure is to measure how appropriate a news article is for a specific reading device. For this, </context>
</contexts>
<marker>Li, Chu, Langford, Schapire, 2010</marker>
<rawString>Lihong Li, Wei Chu, John Langford, and Robert E. Schapire. 2010. A contextual-bandit approach to personalized news article recommendation. In Proceedings of the 19th International Conference on World Wide Web, pages 661–670.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiahui Liu</author>
<author>Peter Dolan</author>
<author>Elin R Pedersen</author>
</authors>
<title>Personalized news recommendation based on click behavior.</title>
<date>2010</date>
<booktitle>In Proceedings of the 15th International Conference on Intelligent User Interfaces,</booktitle>
<pages>31--40</pages>
<contexts>
<context position="17063" citStr="Liu et al., 2010" startWordPosition="2741" endWordPosition="2744">t Readability The fact that the weights wi|d in Equation (2) are different for each device d implies that the readability measurement should be different depending on the device type. In order to see the usefulness of this devicedependent readability, we apply it to news article recommendation. News article recommendation aims to provide a user with news articles that interest the user. Thus, it selects a few articles that meet user preference from a gigantic amount of news events. Various methods have reported notable results in news article recommendation (Das et al., 2007; Li et al., 2010; Liu et al., 2010). In addition, with the recent interest in handheld devices, the demand for news recommendation on hand-held devices is increasing. However, there has been, at least as far as we know, no study on the readability of hand-held devices. Device-dependent readability is reflected into news article recommendation through a re-ranking framework. Figure 1 depicts the overall process of suggesting news articles for a specific device with the devicedependent readability. The point of this figure is to measure how appropriate a news article is for a specific reading device. For this, a news recommendati</context>
</contexts>
<marker>Liu, Dolan, Pedersen, 2010</marker>
<rawString>Jiahui Liu, Peter Dolan, and Elin R. Pedersen. 2010. Personalized news recommendation based on click behavior. In Proceedings of the 15th International Conference on Intelligent User Interfaces, pages 31– 40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yi Ma</author>
<author>Eric Fosler-Lussier</author>
<author>Robert Lofthus</author>
</authors>
<title>Ranking-based readability assessment for early primary children’s literature.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>548--552</pages>
<contexts>
<context position="1640" citStr="Ma et al., 2012" startWordPosition="238" endWordPosition="241">ve the usefulness of the results, we apply the device-dependent readability to news article recommendation. 1 Introduction Readability is a function that maps a given text into a readability score by considering “how easily the text is read and understood” (Richards et al., 1992; Zamanian and Heydari, 2012). Normally, the readability score is formulated as a combination of various factors. These factors reflect the easiness and understanding of the text and include text presentation format, font size, average ratio of annotated images, and sentence length (Hasegawa et al., 2008; Kitson, 1927; Ma et al., 2012; ¨Oquist, 2006). Therefore, readability can be used to provide satisfiable services in text recommendation or text visualization. The study on readability has begun in the education field to measure the level of a text. With the success of using readability in education (Franc¸ois and Fairon, 2012; Heilman et al., 2008; Ma et al., 2012), readability has been used in a range of domains recently. For example, in document retrieval, readability is used to provide documents to non-expert users so that they can read the retrieved documents easily (Jameel et al., 2012; Yan et al., 2006). In text mi</context>
</contexts>
<marker>Ma, Fosler-Lussier, Lofthus, 2012</marker>
<rawString>Yi Ma, Eric Fosler-Lussier, and Robert Lofthus. 2012. Ranking-based readability assessment for early primary children’s literature. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 548–552.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Naoaki Okazaki</author>
<author>Yutaka Matsuo</author>
<author>Mitsuru Ishizuka</author>
</authors>
<title>Improving chronological ordering of sentences extracted from multiple newspaper articles.</title>
<date>2005</date>
<journal>ACM Transactions on Asian Language Information Processing (TALIP),</journal>
<volume>4</volume>
<issue>3</issue>
<contexts>
<context position="14369" citStr="Okazaki et al., 2005" startWordPosition="2297" endWordPosition="2300">e text more interesting. The texts written for adults actually contain more entities than those written for children (Barzilay and Lapata, 2008). The same is true for VP. The large number of verb phrases in a sentence makes the sentence more complex. However, people feel that a text is more easier to comprehend when related clauses are grouped together (Bailin and Grafstein, 2001). 3.4 Lexical Cohesion Lexical cohesion denotes how the sentences in a text are semantically connected. People usually bring continuous sentences into their mind at the same time, and interpret them as a single unit (Okazaki et al., 2005). In other words, a reader prefers text whose sentences are smoothly connected to text whose sentences are independent of one another. Therefore, sentence continuity plays a primary role in understanding an entire text. In the classic study of cohesion, various uses of cohesive elements such as pronouns, definite articles, and topic continuity have been discussed (Halliday and Hasan, 1976). This paper uses the average cosine similarity (COS), word overlap (WO), word overlap over just nouns and pronouns (NPO) between pairs of adjacent sentences, and the average number of pronouns per sentence (</context>
</contexts>
<marker>Okazaki, Matsuo, Ishizuka, 2005</marker>
<rawString>Naoaki Okazaki, Yutaka Matsuo, and Mitsuru Ishizuka. 2005. Improving chronological ordering of sentences extracted from multiple newspaper articles. ACM Transactions on Asian Language Information Processing (TALIP), 4(3):321–339.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gustav ¨Oquist</author>
</authors>
<title>Evaluating readability on mobile devices.</title>
<date>2006</date>
<tech>Ph.D. thesis,</tech>
<institution>Uppsala University.</institution>
<marker>¨Oquist, 2006</marker>
<rawString>Gustav ¨Oquist. 2006. Evaluating readability on mobile devices. Ph.D. thesis, Uppsala University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emily Pitler</author>
<author>Ani Nenkova</author>
</authors>
<title>Revisiting readability: A unified framework for predicting text quality.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>186--195</pages>
<contexts>
<context position="7115" citStr="Pitler and Nenkova (2008)" startWordPosition="1093" endWordPosition="1096">he list. These studies simply used superficial factors, and thus do not reflect syntactic factors. Recent studies on readability use various factors including syntactic ones, and combine them to produce a highly predictive model of readability. Franc¸ois and Faircon (2012) proposed a readability formula with 46 textual factors for French as a foreign language. The factors represent lexical, syntactic, and semantic characteristics of sentences, and the specificities of French. They are extracted from 28 French Foreign Language (FFL) textbooks written for adults learning FFL. On the other hand, Pitler and Nenkova (2008) showed the relation between readability factors and readability. They used human ratings from the Wall Street Journal corpus, and computed the correlations between the readability factors and the average human ratings. According to their results, the average number of verb phrases in a sentence, the number of words in an article, the likelihood of the vocabulary, and the likelihood of the discourse relations are highly correlated with human ratings. However, these studies did not consider the reading devices, but focused on how well a text is written. Since the readability can be differentiat</context>
<context position="8933" citStr="Pitler and Nenkova (2008)" startWordPosition="1388" endWordPosition="1391">awa et al. (2008) evaluated the readability of documents on mobile devices with regard to screen and font size. They reported that the readability is improved when the characters are vertically enlarged. Readability on mobile devices is not reflected only by the visualization factors, but also by textual factors. Therefore, this paper explores the readability factors that reflect the lexical and grammatical complexity of text and are affected by reading devices. 3 Readability Factors Table 1 lists the readability factors used in this paper. Basically, they are based on the factors proposed by Pitler and Nenkova (2008). However, some factors are excluded and some new factors are added. This is because some of their factors are computationally infeasible and language-dependent. As a result, we have thirteen readability factors. These readability factors are divided into four types: superficial, lexical, syntactic factors, and lexical cohesion. 3.1 Superficial Factors Superficial factors were used in most early readability studies (Dale and Chall, 1949; Flesch, 1948; Kincaid et al., 1975), and reflect the construction of a text. We investigate four factors: text length (TL), sentence length (SL), average numb</context>
<context position="12315" citStr="Pitler and Nenkova (2008)" startWordPosition="1944" endWordPosition="1947"> single word. In addition, many difficult words such as domain-specific terms tend to be long. Such lengthy words make it difficult to read a text. 3.2 Lexical Factor Lexical factor determines whether a given text consists of frequent words. Texts that express a new trend in various fields often use many newly coined words. Such neologisms make it difficult to read and understand a text. Therefore, an easily-understandable text is composed of widely-used words rather than unusual words. In order to compute the use of frequent words in a text, a unigram language model is used as in the work of Pitler and Nenkova (2008). In this model, the log likelihood of text t is computed by E C(w) · log P(w|B). (1) w∈t where P(w|B) is the probability of a word w according to a background corpus B, and C(w) is the number of times that w appears in t. This factor examines the familiarity of the words used in the text. The more frequently a word appears in the background corpus, the more familiar it is regarded. The frequency of a word w is then reflected into P(w|B) computed from the independent background corpus B. Therefore, the factor LL is positively related with readability. 3.3 Syntactic Factors Syntactic factors re</context>
<context position="13721" citStr="Pitler and Nenkova, 2008" startWordPosition="2190" endWordPosition="2193">entence (NP), the average number of verb phrases per sentence (VP), and the average number of subordinate clauses per sentence (SBAR) as syntactic factors. These four factors were defined by Schwarm and Ostendorf (2005). A reader regards a text as difficult when the sentences in the text have large parse tree depths or many subordinate clauses. Thus, PTD and SBAR are related negatively with readability. On the other hand, the relationship of NP and VP to readability are not one way. The large number of noun phrases in a text requires a reader to remember more items (Barzilay and Lapata, 2008; Pitler and Nenkova, 2008). However, it also makes the text more interesting. The texts written for adults actually contain more entities than those written for children (Barzilay and Lapata, 2008). The same is true for VP. The large number of verb phrases in a sentence makes the sentence more complex. However, people feel that a text is more easier to comprehend when related clauses are grouped together (Bailin and Grafstein, 2001). 3.4 Lexical Cohesion Lexical cohesion denotes how the sentences in a text are semantically connected. People usually bring continuous sentences into their mind at the same time, and interp</context>
<context position="15643" citStr="Pitler and Nenkova (2008)" startWordPosition="2501" endWordPosition="2504">f topic continuity, whereas PRP is an indicative feature of sentence continuity. High values for these factors imply that the sentences in the text are related somehow. Therefore, these factors are believed to be related positively with readability. 1398 3.5 Measurement of Readability When a reading device d is given, the readability of text t, represented as R(t|d), is formulated as a combination of readability factors with their corresponding weight in the device. We assume that wi|d, the weight of a readability factor fi, is dependent on the reading device d. Following the previous work of Pitler and Nenkova (2008), we also assume that each readability factor affects readability independently. Therefore, readability is calculated as a weighted linear sum of all readability factors. That is, R(t|d) is computed by R(t|d) = � wi|d - fi(t) (2) i∈{1,2,...,M} where M is the number of readability factors. Each weight wi|d is determined from a set of news articles T. We collected a large number of news articles from an Internet news portal. The readability of each article was manually labeled. This is done three times, since we have three different devices of a smart phone, a tablet, and paper. Since human rati</context>
<context position="23728" citStr="Pitler and Nenkova (2008)" startWordPosition="3868" endWordPosition="3871">three undergraduate students who were not involved in our main experiments. The students read the same 250 articles four times, and these also come from Naver News corpus which are not included the previous 74 articles. After their first reading, they read the articles again in 3, 7, and 14 days later. After 3 days, two students remembered the articles somewhat, but one student remembered them vaguely. Since they almost forgot the articles after 7 days, we placed 7 days interval between devices. The readability score of an article was rated by the annotators using the questions in the work of Pitler and Nenkova (2008). We use only two of the questions, while they used four questions for the annotators. Their questions are intended to measure the extent of how well a text is written, how it fits together, how easy it is to understand, and how interesting it is. We can consider “well-written” and “fit-together” as a syntactic perspective, whereas “easy to understand” and “interesting” belong to a content perspective. For such a 1400 Smart phone Tablet Paper Factor Value Factor Value Factor Value SL -0.394 SL -0.370 NP 0.298 TL -0.293 WS 0.321 WS 0.278 WS 0.288 LL 0.253 LL 0.268 LL 0.249 NP 0.240 VP 0.244 Tab</context>
</contexts>
<marker>Pitler, Nenkova, 2008</marker>
<rawString>Emily Pitler and Ani Nenkova. 2008. Revisiting readability: A unified framework for predicting text quality. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 186–195.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jack Richards</author>
<author>John Platt</author>
<author>Heidi Platt</author>
<author>Christophe Candlin</author>
</authors>
<title>Longman Dictionary of Language Teaching and Applied Linguistics, volume 78. Longman London.</title>
<date>1992</date>
<contexts>
<context position="1304" citStr="Richards et al., 1992" startWordPosition="186" endWordPosition="189">ed with the readability of a specific device by showing the correlations between various factors in each device and human-rated readability. Our experimental results show that each device has its own readability characteristics, and thus different weights should be imposed on readability factors according to the device type. In order to prove the usefulness of the results, we apply the device-dependent readability to news article recommendation. 1 Introduction Readability is a function that maps a given text into a readability score by considering “how easily the text is read and understood” (Richards et al., 1992; Zamanian and Heydari, 2012). Normally, the readability score is formulated as a combination of various factors. These factors reflect the easiness and understanding of the text and include text presentation format, font size, average ratio of annotated images, and sentence length (Hasegawa et al., 2008; Kitson, 1927; Ma et al., 2012; ¨Oquist, 2006). Therefore, readability can be used to provide satisfiable services in text recommendation or text visualization. The study on readability has begun in the education field to measure the level of a text. With the success of using readability in ed</context>
</contexts>
<marker>Richards, Platt, Platt, Candlin, 1992</marker>
<rawString>Jack Richards, John Platt, Heidi Platt, and Christophe Candlin. 1992. Longman Dictionary of Language Teaching and Applied Linguistics, volume 78. Longman London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sarah Schwarm</author>
<author>Mari Ostendorf</author>
</authors>
<title>Reading level assessment using support vector machines and statistical language models.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>523--530</pages>
<contexts>
<context position="13315" citStr="Schwarm and Ostendorf (2005)" startWordPosition="2116" endWordPosition="2119"> is regarded. The frequency of a word w is then reflected into P(w|B) computed from the independent background corpus B. Therefore, the factor LL is positively related with readability. 3.3 Syntactic Factors Syntactic factors reflect sentence complexity directly that affects human processing of a sentence. We consider the average parse tree depth per sentence (PTD), the average number of noun phrases per sentence (NP), the average number of verb phrases per sentence (VP), and the average number of subordinate clauses per sentence (SBAR) as syntactic factors. These four factors were defined by Schwarm and Ostendorf (2005). A reader regards a text as difficult when the sentences in the text have large parse tree depths or many subordinate clauses. Thus, PTD and SBAR are related negatively with readability. On the other hand, the relationship of NP and VP to readability are not one way. The large number of noun phrases in a text requires a reader to remember more items (Barzilay and Lapata, 2008; Pitler and Nenkova, 2008). However, it also makes the text more interesting. The texts written for adults actually contain more entities than those written for children (Barzilay and Lapata, 2008). The same is true for </context>
</contexts>
<marker>Schwarm, Ostendorf, 2005</marker>
<rawString>Sarah Schwarm and Mari Ostendorf. 2005. Reading level assessment using support vector machines and statistical language models. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 523–530.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xin Yan</author>
<author>Dawei Song</author>
<author>Xue Li</author>
</authors>
<title>Conceptbased document readability in domain specific information retrieval.</title>
<date>2006</date>
<booktitle>In Proceedings of the 15th ACM International Conference on Information and Knowledge Management,</booktitle>
<pages>540--549</pages>
<contexts>
<context position="2228" citStr="Yan et al., 2006" startWordPosition="334" endWordPosition="337">Kitson, 1927; Ma et al., 2012; ¨Oquist, 2006). Therefore, readability can be used to provide satisfiable services in text recommendation or text visualization. The study on readability has begun in the education field to measure the level of a text. With the success of using readability in education (Franc¸ois and Fairon, 2012; Heilman et al., 2008; Ma et al., 2012), readability has been used in a range of domains recently. For example, in document retrieval, readability is used to provide documents to non-expert users so that they can read the retrieved documents easily (Jameel et al., 2012; Yan et al., 2006). In text mining, readability has been employed to analyze the characteristics of text. Especially, Hillbom showed the differences in readability between broadsheet newspapers and tabloids that share a similar political stance (Hillbom, 2009). There is one important issue of readability that has not been studied in natural language processing. It is a reading device. That is, previous studies focused only on text printed on paper. However, with the increasing use of hand-held devices, people in these days use various reading devices such as a tablet and a smart phone as well as a paper. Readab</context>
</contexts>
<marker>Yan, Song, Li, 2006</marker>
<rawString>Xin Yan, Dawei Song, and Xue Li. 2006. Conceptbased document readability in domain specific information retrieval. In Proceedings of the 15th ACM International Conference on Information and Knowledge Management, pages 540–549.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mostafa Zamanian</author>
<author>Pooneh Heydari</author>
</authors>
<title>Readability of texts: State of the art. Theory and Practice</title>
<date>2012</date>
<booktitle>in Language Studies,</booktitle>
<pages>2--1</pages>
<contexts>
<context position="1333" citStr="Zamanian and Heydari, 2012" startWordPosition="190" endWordPosition="193"> of a specific device by showing the correlations between various factors in each device and human-rated readability. Our experimental results show that each device has its own readability characteristics, and thus different weights should be imposed on readability factors according to the device type. In order to prove the usefulness of the results, we apply the device-dependent readability to news article recommendation. 1 Introduction Readability is a function that maps a given text into a readability score by considering “how easily the text is read and understood” (Richards et al., 1992; Zamanian and Heydari, 2012). Normally, the readability score is formulated as a combination of various factors. These factors reflect the easiness and understanding of the text and include text presentation format, font size, average ratio of annotated images, and sentence length (Hasegawa et al., 2008; Kitson, 1927; Ma et al., 2012; ¨Oquist, 2006). Therefore, readability can be used to provide satisfiable services in text recommendation or text visualization. The study on readability has begun in the education field to measure the level of a text. With the success of using readability in education (Franc¸ois and Fairon</context>
</contexts>
<marker>Zamanian, Heydari, 2012</marker>
<rawString>Mostafa Zamanian and Pooneh Heydari. 2012. Readability of texts: State of the art. Theory and Practice in Language Studies, 2(1):43–53.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
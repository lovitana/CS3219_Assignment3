<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000009">
<title confidence="0.981266">
Combining Visual and Textual Features for Information Extraction from
Online Flyers
</title>
<author confidence="0.942136">
Emilia Apostolova
</author>
<affiliation confidence="0.7835525">
BrokerSavant Inc
2506 N. Clark St.
</affiliation>
<address confidence="0.830577">
Chicago, IL 60614
</address>
<email confidence="0.998747">
emilia@brokersavant.com
</email>
<sectionHeader confidence="0.993894" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999790315789474">
Information in visually rich formats such
as PDF and HTML is often conveyed
by a combination of textual and visual
features. In particular, genres such as
marketing flyers and info-graphics often
augment textual information by its color,
size, positioning, etc. As a result, tradi-
tional text-based approaches to informa-
tion extraction (IE) could underperform.
In this study, we present a supervised ma-
chine learning approach to IE from on-
line commercial real estate flyers. We
evaluated the performance of SVM clas-
sifiers on the task of identifying 12 types
of named entities using a combination of
textual and visual features. Results show
that the addition of visual features such
as color, size, and positioning significantly
increased classifier performance.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999520166666667">
Since the Message Understanding Conferences in
the 1990s (Grishman and Sundheim, 1996; Chin-
chor and Robinson, 1997), Information Extraction
(IE) and Named Entity Recognition (NER) ap-
proaches have been applied and evaluated on a va-
riety of domains and textual genres. The majority
of the work, however, focuses on the journalistic,
scientific, and informal genres (newswires, scien-
tific publications, blogs, tweets, and other social
media texts) (Nadeau and Sekine, 2007) and deals
with purely textual corpora. As a result, the fea-
ture space of NER systems involves purely tex-
tual features, typically word attributes and char-
acteristics (orthography, morphology, dictionary
lookup, etc.), their contexts and document features
(surrounding word window, local syntax, docu-
ment/corpus word frequencies, etc.) (Nadeau and
Sekine, 2007).
</bodyText>
<note confidence="0.8593125">
Noriko Tomuro
DePaul University
243 S. Wabash Ave.
Chicago, IL 60604
</note>
<email confidence="0.984766">
tomuro@cs.depaul.edu
</email>
<bodyText confidence="0.999886875">
At the same time, textual information is often
presented in visually rich formats, e.g. HTML and
PDF. In addition to text, these formats use a vari-
ety of visually salient characteristics, (e.g. color,
font size, positioning) to either highlight or aug-
ment textual information. In some genres and do-
mains, a textual representation of the data, exclud-
ing visual features is often not enough to accu-
rately identify named entities of interest or extract
relevant information. Marketing materials, such
as online flyers or HTML emails, often contain
a plethora of visual features and text-based NER
approaches lead to poor results. In this paper, we
present a supervised approach that uses a combi-
nation of textual and visual features to recognize
named entities in online marketing materials.
</bodyText>
<sectionHeader confidence="0.76385" genericHeader="introduction">
2 Motivation and Problem Definition
</sectionHeader>
<bodyText confidence="0.999751956521739">
A number of broker-based industries (e.g. com-
mercial real estate, heavy equipment machinery,
etc.) lack a centralized searchable database with
industry offerings. In particular, the commercial
real estate industry (unlike residential real estate)
does not have a centralized database or an estab-
lished source of information. Commercial real
estate brokers often need to rely on networking,
chance, and waste time with a variety of commer-
cial real estate databases that often present out-
dated information. While brokers do not often up-
date third party inventory databases, they do create
marketing materials (usually PDF flyers) that con-
tain all relevant listing information. Virtually all
commercial real estate offerings come with pub-
licly available marketing material that contains all
relevant listing information. Our goal is to harness
this source of information (the marketing flyer)
and use it to extract structured listing information.
Figure 1 shows an example of a commercial
real estate flyer. The commercial real estate fly-
ers are often distributed as PDF documents, links
to HTML pages, or visually rich HTML-based
</bodyText>
<page confidence="0.947981">
1924
</page>
<note confidence="0.660336">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1924–1929,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</note>
<figureCaption confidence="0.948966">
Figure 1: An example of a commercial real estate
flyer © Kudan Group Real Estate.
</figureCaption>
<bodyText confidence="0.996375894736842">
emails. They typically contain all relevant listing
information such as the address and neighborhood
of the offering, the names and contact information
of the brokers, the type of space offered (build-
ing, land, unit(s) within a building), etc. Similar to
other info-graphics, relevant information could be
easily pinpointed by visual clues. For example, the
listing street address in Figure 1 (1629 N. Halsted
St., upper left corner) can be quickly identified and
distinguished from the brokerage firm street ad-
dress (156 N. Jefferson St., upper right corner) due
to its visual prominence (font color, size, position-
ing).
In this study we explored a supervised machine
learning approach to the task of identifying list-
ing information from commercial real estate fly-
ers. In particular, we focused on the recognition
of 12 types of named entities as described in Table
1 below.
</bodyText>
<sectionHeader confidence="0.999957" genericHeader="related work">
3 Related Work
</sectionHeader>
<bodyText confidence="0.999038833333333">
Nadeau and Satoshi (2007) present a survey of
NER and describe the feature space of NER re-
search. While they mention multi-media NER in
the context of video/text processing, all described
features/approaches focus only on textual repre-
sentation.
</bodyText>
<figure confidence="0.29614375">
Broker Name The contact information of all
Broker Email listing brokers, including full name,
Broker Phone email address, phone number.
Company Phone The brokerage company phone
number.
Street The address information of the
City listing address including street or
Neighborhood intersection, city, neighborhood,
State state, and zip code.
Zip
Space Size Size and attributes of relevant spaces
Space Type (e.g. 27,042 SF building, 4.44 acres
</figure>
<figureCaption confidence="0.886356571428571">
site, etc.); Mentions of space type
descriptors, e.g. building, land/lot,
floor, unit. This excludes space type
and size information of non-essential
listing attributes (e.g. basement size
or parking lot size).
Confidential Any mentions of confidentiality.
</figureCaption>
<tableCaption confidence="0.636685666666667">
Table 1: Types and descriptions of named enti-
ties relevant to extracting listing information from
commercial real estate flyers.
</tableCaption>
<bodyText confidence="0.99978478125">
The literature on Information Extraction from
HTML resources is dominated by various ap-
proaches based on wrapper induction (Kushmer-
ick, 1997; Kushmerick, 2000). Wrapper induc-
tions rely on common HTML structure (based on
the HTML DOM) and formatting features to ex-
tract structured information from similarly format-
ted HTML pages. This approach, however, is not
applicable to the genres of marketing materials
(PDF and HTML) since they typically do not share
any common structure that can be used to iden-
tify relevant named entities. Laender et al. (2002)
present a survey of data extraction techniques and
tools from structured or semi-structured web re-
sources.
Cai et al. (2003) present a vision-based segmen-
tation algorithm of web pages that uses HTML
layout features and attempts to partition the page
at the semantic level. In (Burget and Rudolfova,
2009) authors propose web-page block classifica-
tion based on visual features. Yang and Zhang
(2001) build a content tree of HTML documents
based on visual consistency inferred semantics.
Burget (2007) proposes a layout based informa-
tion extraction from HTML documents and states
that this visual approach is more robust than tradi-
tional DOM-based methods.
Changuel et al.(2009a) describe a system for
automatically extracting author information from
web-pages. They use spatial information based on
the depth of the text node in the HTML DOM tree.
In (Changuel et al., 2009b) and (Hu et al., 2006),
</bodyText>
<page confidence="0.965858">
1925
</page>
<bodyText confidence="0.9999602">
the authors proposed a machine learning method
for title extraction and utilize format information
such as font size, position, and font weight. In
(Zhu et al., 2007) authors use layout information
based on font size and weight for NER for auto-
mated expense reimbursement.
While the idea of utilizing visual features based
on HTML style has been previously suggested,
this study tackles a non-trivial visually rich dataset
that prevents the use of previously suggested sim-
plistic approaches to computing HTML features
(such as relying on the HTML DOM tree or sim-
plistic HTML style rendering). In addition, we in-
troduce the use of RGB color as a feature and nor-
malize it approximating human perception.
</bodyText>
<sectionHeader confidence="0.989072" genericHeader="method">
4 Dataset and Method
</sectionHeader>
<bodyText confidence="0.992351586206897">
The dataset consists of 800 randomly selected
commercial real estate flyers spanning 315 US
locations, 75 companies, and 730 brokers. The
flyers were collected from various online sources
and were originally generated using a variety of
HTML and PDF creator tools. The collection rep-
resents numerous flyer formats and layouts, com-
mercial real estate property types (industrial, re-
tail, office, land, etc.), and transactions (invest-
ment, sale, lease).
All flyers were converted to a common format
(HTML)1. The HTML versions of all documents
were then annotated by 2 annotators. Figure 2
shows an example of an annotated flyer. Annota-
tion guidelines were developed and the 2 annota-
tors were able to achieve an inter-annotator agree-
ment of 91%2. The named entities with lowest
inter-annotator agreement were entities describ-
ing Space Size and Type because of the some-
what complex rules for determining essential list-
ing space information. For example, one of the
space size/type rules reads as follows: If the list-
ing refers to a building and mentions the lot size, include
both the land size, the building size, and corresponding space
types. Do not include individual parts of the building (e.g.
office/basement) as separate spaces. If the listing refers to a
UNIT within the building, not the whole building, then DO
NOT include the land site as a separate space.
A supervised machine learning approach was
</bodyText>
<footnote confidence="0.9905065">
1PDFs were converted to HTML using the PDFTO-
HTML conversion program http://pdftohtml.
sourceforge.net/.
2The inter-annotator agreement was measured as F1-score
using one of the annotator’s named entities as the gold stan-
dard set and the other as a comparison set.
</footnote>
<figureCaption confidence="0.925376333333333">
Figure 2: The HTML versions of the flyers were
annotated by 2 annotators using a custom web-
based annotation tool.
</figureCaption>
<bodyText confidence="0.999826916666667">
then applied to the task of identifying the 12
named entities shown in Table 1. Flyers were con-
verted to text using an HTML parser while pre-
serving some of the white space formatting. The
text was tokenized and the task was then modeled
as a BIO classification task, classifiers identify the
Beginning, the Inside, and Outside of the text seg-
ments. We first used a traditional set of text-based
features for the classification task. Table 2 lists
the various text-based features used. In all cases,
a sliding window including the 5 preceding and 5
following tokens was used as features.
</bodyText>
<figureCaption confidence="0.745119153846154">
Feature Name Description
Token A normalized string representation of
the token. All tokens were converted
to lower case and all digits were
converted to a common format.
Token Orth The token orthography. Possible values
are lowercase (all token characters are
lower case), all capitals (all token
characters are upper case), upper initial
(the first token character is upper case,
the rest are lower case), mixed (any
mixture of upper and lower case letters
not included in the previous categories).
</figureCaption>
<bodyText confidence="0.683381375">
Token Kind Possible values are word, number,
symbol, punctuation.
Regex type Regex-based rules were used to mark
chunks as one of 3 regex types:
email, phone number, zip code.
Gazetteer Text chunks were marked as possible
US cities or states based on US Census
Bureau city and state data.
</bodyText>
<note confidence="0.285841">
www.census.gov/geo/maps-data/data/gazetteer2013.html.
</note>
<tableCaption confidence="0.640584666666667">
Table 2: List of text-based features used for the
NER task. A sliding window of the 5 preceding
and 5 following tokens was used for all features.
</tableCaption>
<page confidence="0.994832">
1926
</page>
<bodyText confidence="0.772003519230769">
As noted previously, human annotators were
able to quickly spot named entities of interest
solely because of their visual characteristics. For
example, a text-only version of the flyer shown in
Figure 1, stripped of all rich formatting, will make
it quite difficult to distinguish the listing address
(shown in prominent size, position, and color)
from the brokerage company address, which is
rarely prominent as it is not considered important
information in the context of the flyer. Similarly,
the essential size information for the listing shown
on Figure 2 appears prominently on the first page
(square footage of the offered restaurant), while
non-essential size information, such as the size of
the adjacent parking lot or basement, tend to ap-
pear in smaller font on subsequent flyer pages.
To account for such visual characteristics we at-
tempted to also include visual features associated
with text chunks. We used the computed HTML
style attributes for each DOM element containing
text. Table 3 lists the computed visual features.
Feature Name Description
Font Size The computed font-size attribute of
the surrounding HTML DOM element,
normalized to 7 basic sizes (xx-small,
x-small, small, medium, large, x-large,
xx-large).
Color The computed color attribute of the
surrounding HTML DOM element.
The RGB values were normalized
to a set of 100 basic colors. We
converted the RGB values to the
YUV color space, and then used
Euclidian distance to find the
most similar basic color
approximating human perception.
Y Coordinate The computed top attribute of the
surrounding HTML DOM element, i.e.
the y-coordinate in pixels. The pixel
locations was normalized to 150 pixel
increments (roughly 1/5th of the
visible screen for the most common
screen resolution.)
Table 3: List of visual features used for the NER
task. A sliding window of 5 preceding and 5 fol-
lowing DOM elements were used for all features.
Computing the HTML style attributes is a com-
plex task since they are typically defined by a
combination of CSS files, in-lined HTML style
attributes, and browser defaults. The complex-
ities of style definition, inheritance, and over-
writing are handled by browsers3. We used the
</bodyText>
<footnote confidence="0.989123666666667">
3We attempted to use an HTML renderer from the Cobra
java toolkit http://lobobrowser.org/cobra.jsp
to compute HTML style attributes. However, this renderer
</footnote>
<bodyText confidence="0.997856772727273">
Chrome browser to compute dynamically the style
of each DOM element and output it as inline
style attributes. To achieve this we program-
matically inserted a javascript snippet that inlines
the computed style and saves the new version of
the HTML on the local file system utilizing the
HTML5 saveAs interface4. Details on how we
normalized the style attribute values for font size,
RGB color, and Y coordinate are shown in Table
3.
We then applied Support Vector Machines
(SVM) (Vapnik, 2000) on the NER task using the
LibSVM library (Chang and Lin, 2011). We chose
SVMs as they have been shown to perform well
on a variety of NER tasks, for example (Isozaki
and Kazawa, 2002; Takeuchi and Collier, 2002;
Mayfield et al., 2003; Ekbal and Bandyopadhyay,
2008). We used a linear kernel model with the
default parameters. The multi-class problem was
converted to binary problems using the one-vs-
others scheme. 80% of the documents were used
for training, and the remaining 20% for testing.
</bodyText>
<sectionHeader confidence="0.999868" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.981153458333333">
Results are shown in Table 4. We compared clas-
sifier performance using only textual features (first
3 columns), versus performance using both textual
and visual features (next 3 columns). Results were
averaged over 2 runs of randomly selected train-
ing/test documents with 80%/20% ratio. We used
an exact measure which considers an answer to be
correct only if both the entity boundaries and en-
tity type are accurately predicted.
The addition of visual features significantly5
increased the overall F1-score from 83 to 87%.
As expected, performance gains are more signif-
icant for named entities that are typically visu-
ally salient and are otherwise difficult (or impossi-
ble) to identify in a text-only version of the fly-
ers. Named Entities referring to listing address
information showed the most significant improve-
ments. In particular, the F1-score for mentions of
Neighborhoods (typically prominently shown on
the first page of the flyers) improved by 19%; F1-
score for mentions of the listing State improved by
9%; and Street, City, Zip by roughly 4% each, all
produced poor results on our dataset and failed to accurately
compute the pixel location of text elements.
</bodyText>
<footnote confidence="0.99683925">
4https://github.com/eligrey/FileSaver.
js
5The difference is statistically significant with p value &lt;
0.0001% using Z-test on two proportions.
</footnote>
<page confidence="0.968433">
1927
</page>
<table confidence="0.9993355">
Named Entity Pt Rt Ft Pv+t Rv+t Fv+t S
Broker Name 82.7 91.7 87.0 95.0 91.6 93.2 Y
Broker Email 92.3 92.8 92.6 97.2 90.2 93.6 N
Broker Phone 90.2 86.1 88.1 94.7 85.2 89.7 N
Company Ph. 95.2 67.4 78.9 89.8 65.4 75.7 N
Street 87.4 70.5 78.1 87.3 77.3 82.0 Y
City 92.5 88.5 90.5 94.9 92.8 93.8 Y
Neighborhood 68.2 52.8 59.5 85.3 72.9 78.6 Y
State 77.4 97.5 86.3 95.8 95.0 95.4 Y
Zip 89.7 94.5 92.1 96.1 97.1 96.6 Y
Space Size 80.2 65.0 71.8 87.0 70.6 77.9 Y
Space Type 76.0 74.7 75.3 78.6 72.2 75.3 N
Confidential 100 60.0 75.0 75.0 85.7 79.9 N
OVERALL 84.8 81.3 83.0 91.2 83.2 87.0 Y
</table>
<tableCaption confidence="0.998213">
Table 4: Results from applying SVM using the
</tableCaption>
<bodyText confidence="0.937788555555556">
textual features described in Table 2, as well as
both the textual and visual features described in
Tables 2 and 3. t=textual features only, v+t=visual
+ textual features, P=Precision, R=Recall, F=F1-
score, S=Significant Difference
statistically significant. Visual clues are also typi-
cally used when identifying relevant size informa-
tion and, as expected, performance improved sig-
nificantly by roughly 6%. The difference in per-
formance for mentions used to describe confiden-
tial information is not statistically significant6 be-
cause such mentions rarely occurred in the dataset.
Similarly, performance differences for Company
Phone, Broker Phone, Broker Email, and Space
Type are not statistically significant. In all of
these cases, visual features did not influence per-
formance and text-based features proved adequate
predictors.
</bodyText>
<sectionHeader confidence="0.999358" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999854055555556">
We have shown that information extraction in cer-
tain genres and domains spans different media -
textual and visual. Ubiquitous online and dig-
ital formats such as PDF and HTML often ex-
ploit the interaction of textual and visual elements.
Information is often augmented or conveyed by
non-textual features such as positioning, font size,
color, and images. However, traditionally, NER
approaches rely exclusively on textual features
and as a result could perform poorly in visually
rich genres such as online marketing flyers or info-
graphics. We have evaluated the performance gain
on the task of NER from commercial real estate
flyers by adding visual features to a set of tradi-
tional text-based features. We used SVM classi-
fiers for the task of identifying 12 types of named
entities. Results show that overall visual features
improved performance significantly.
</bodyText>
<footnote confidence="0.640713">
6p value = 0.7323% using Z-test on two proportions.
</footnote>
<sectionHeader confidence="0.92492" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999625">
Radek Burget and Ivana Rudolfova. 2009. Web
page element classification based on visual features.
In Intelligent Information and Database Systems,
2009. ACIIDS 2009. First Asian Conference on,
pages 67–72. IEEE.
Radek Burget. 2007. Layout based information extrac-
tion from html documents. In Document Analysis
and Recognition, 2007. ICDAR 2007. Ninth Inter-
national Conference on, volume 2, pages 624–628.
IEEE.
Deng Cai, Shipeng Yu, Ji-Rong Wen, and Wei-Ying
Ma. 2003. Extracting content structure for web
pages based on visual representation. In Web Tech-
nologies and Applications, pages 406–417. Springer.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIB-
SVM: A library for support vector machines. ACM
Transactions on Intelligent Systems and Technol-
ogy, 2:27:1–27:27. Software available at http://
www.csie.ntu.edu.tw/˜cjlin/libsvm.
Sahar Changuel, Nicolas Labroche, and Bernadette
Bouchon-Meunier. 2009a. Automatic web pages
author extraction. In Flexible Query Answering Sys-
tems, pages 300–311. Springer.
Sahar Changuel, Nicolas Labroche, and Bernadette
Bouchon-Meunier. 2009b. A general learning
method for automatic title extraction from html
pages. In Machine Learning and Data Mining in
Pattern Recognition, pages 704–718. Springer.
Nancy Chinchor and Patricia Robinson. 1997. Muc-7
named entity task definition. In Proceedings of the
7th Conference on Message Understanding.
Asif Ekbal and Sivaji Bandyopadhyay. 2008. Named
entity recognition using support vector machine: A
language independent approach. International Jour-
nal of Computer Systems Science &amp; Engineering,
4(2).
Ralph Grishman and Beth Sundheim. 1996. Mes-
sage understanding conference-6: A brief history. In
COLING, volume 96, pages 466–471.
Yunhua Hu, Hang Li, Yunbo Cao, Li Teng, Dmitriy
Meyerzon, and Qinghua Zheng. 2006. Automatic
extraction of titles from general documents using
machine learning. Information processing &amp; man-
agement, 42(5):1276–1293.
Hideki Isozaki and Hideto Kazawa. 2002. Efficient
support vector classifiers for named entity recog-
nition. In Proceedings of the 19th international
conference on Computational linguistics-Volume 1,
pages 1–7. Association for Computational Linguis-
tics.
Nicholas Kushmerick. 1997. Wrapper induction for
information extraction. Ph.D. thesis, University of
Washington.
</reference>
<page confidence="0.875879">
1928
</page>
<reference confidence="0.99976303030303">
Nicholas Kushmerick. 2000. Wrapper induction: Ef-
ficiency and expressiveness. Artificial Intelligence,
118(1):15–68.
Alberto HF Laender, Berthier A Ribeiro-Neto, Alti-
gran S da Silva, and Juliana S Teixeira. 2002. A
brief survey of web data extraction tools. ACM Sig-
mod Record, 31(2):84–93.
James Mayfield, Paul McNamee, and Christine Piatko.
2003. Named entity recognition using hundreds of
thousands of features. In Proceedings of the seventh
conference on Natural language learning at HLT-
NAACL 2003-Volume 4, pages 184–187. Association
for Computational Linguistics.
David Nadeau and Satoshi Sekine. 2007. A sur-
vey of named entity recognition and classification.
Lingvisticae Investigationes, 30(1):3–26.
Koichi Takeuchi and Nigel Collier. 2002. Use of
support vector machines in extended named entity
recognition. In proceedings of the 6th conference
on Natural language learning-Volume 20, pages 1–
7. Association for Computational Linguistics.
Vladimir Vapnik. 2000. The nature of statistical learn-
ing theory. springer.
Yudong Yang and HongJiang Zhang. 2001. Html page
analysis based on visual cues. In DocumentAnalysis
and Recognition, 2001. Proceedings. Sixth Interna-
tional Conference on, pages 859–864. IEEE.
Guangyu Zhu, Timothy J Bethea, and Vikas Krishna.
2007. Extracting relevant named entities for auto-
mated expense reimbursement. In Proceedings of
the 13th ACM SIGKDD international conference on
Knowledge discovery and data mining, pages 1004–
1012. ACM.
</reference>
<page confidence="0.997009">
1929
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.865013">
<title confidence="0.99824">Combining Visual and Textual Features for Information Extraction from Online Flyers</title>
<author confidence="0.998975">Emilia Apostolova</author>
<affiliation confidence="0.999389">BrokerSavant Inc</affiliation>
<address confidence="0.9944645">2506 N. Clark St. Chicago, IL 60614</address>
<email confidence="0.999854">emilia@brokersavant.com</email>
<abstract confidence="0.99388805">Information in visually rich formats such as PDF and HTML is often conveyed by a combination of textual and visual features. In particular, genres such as marketing flyers and info-graphics often augment textual information by its color, size, positioning, etc. As a result, traditional text-based approaches to information extraction (IE) could underperform. In this study, we present a supervised machine learning approach to IE from online commercial real estate flyers. We evaluated the performance of SVM classifiers on the task of identifying 12 types of named entities using a combination of textual and visual features. Results show that the addition of visual features such as color, size, and positioning significantly increased classifier performance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Radek Burget</author>
<author>Ivana Rudolfova</author>
</authors>
<title>Web page element classification based on visual features.</title>
<date>2009</date>
<booktitle>In Intelligent Information and Database Systems,</booktitle>
<pages>67--72</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="6946" citStr="Burget and Rudolfova, 2009" startWordPosition="1053" endWordPosition="1056">M) and formatting features to extract structured information from similarly formatted HTML pages. This approach, however, is not applicable to the genres of marketing materials (PDF and HTML) since they typically do not share any common structure that can be used to identify relevant named entities. Laender et al. (2002) present a survey of data extraction techniques and tools from structured or semi-structured web resources. Cai et al. (2003) present a vision-based segmentation algorithm of web pages that uses HTML layout features and attempts to partition the page at the semantic level. In (Burget and Rudolfova, 2009) authors propose web-page block classification based on visual features. Yang and Zhang (2001) build a content tree of HTML documents based on visual consistency inferred semantics. Burget (2007) proposes a layout based information extraction from HTML documents and states that this visual approach is more robust than traditional DOM-based methods. Changuel et al.(2009a) describe a system for automatically extracting author information from web-pages. They use spatial information based on the depth of the text node in the HTML DOM tree. In (Changuel et al., 2009b) and (Hu et al., 2006), 1925 t</context>
</contexts>
<marker>Burget, Rudolfova, 2009</marker>
<rawString>Radek Burget and Ivana Rudolfova. 2009. Web page element classification based on visual features. In Intelligent Information and Database Systems, 2009. ACIIDS 2009. First Asian Conference on, pages 67–72. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radek Burget</author>
</authors>
<title>Layout based information extraction from html documents. In Document Analysis and Recognition,</title>
<date>2007</date>
<booktitle>ICDAR 2007. Ninth International Conference on,</booktitle>
<volume>2</volume>
<pages>624--628</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="7141" citStr="Burget (2007)" startWordPosition="1084" endWordPosition="1085">lly do not share any common structure that can be used to identify relevant named entities. Laender et al. (2002) present a survey of data extraction techniques and tools from structured or semi-structured web resources. Cai et al. (2003) present a vision-based segmentation algorithm of web pages that uses HTML layout features and attempts to partition the page at the semantic level. In (Burget and Rudolfova, 2009) authors propose web-page block classification based on visual features. Yang and Zhang (2001) build a content tree of HTML documents based on visual consistency inferred semantics. Burget (2007) proposes a layout based information extraction from HTML documents and states that this visual approach is more robust than traditional DOM-based methods. Changuel et al.(2009a) describe a system for automatically extracting author information from web-pages. They use spatial information based on the depth of the text node in the HTML DOM tree. In (Changuel et al., 2009b) and (Hu et al., 2006), 1925 the authors proposed a machine learning method for title extraction and utilize format information such as font size, position, and font weight. In (Zhu et al., 2007) authors use layout informatio</context>
</contexts>
<marker>Burget, 2007</marker>
<rawString>Radek Burget. 2007. Layout based information extraction from html documents. In Document Analysis and Recognition, 2007. ICDAR 2007. Ninth International Conference on, volume 2, pages 624–628. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deng Cai</author>
<author>Shipeng Yu</author>
<author>Ji-Rong Wen</author>
<author>Wei-Ying Ma</author>
</authors>
<title>Extracting content structure for web pages based on visual representation.</title>
<date>2003</date>
<booktitle>In Web Technologies and Applications,</booktitle>
<pages>406--417</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="6766" citStr="Cai et al. (2003)" startWordPosition="1024" endWordPosition="1027">is dominated by various approaches based on wrapper induction (Kushmerick, 1997; Kushmerick, 2000). Wrapper inductions rely on common HTML structure (based on the HTML DOM) and formatting features to extract structured information from similarly formatted HTML pages. This approach, however, is not applicable to the genres of marketing materials (PDF and HTML) since they typically do not share any common structure that can be used to identify relevant named entities. Laender et al. (2002) present a survey of data extraction techniques and tools from structured or semi-structured web resources. Cai et al. (2003) present a vision-based segmentation algorithm of web pages that uses HTML layout features and attempts to partition the page at the semantic level. In (Burget and Rudolfova, 2009) authors propose web-page block classification based on visual features. Yang and Zhang (2001) build a content tree of HTML documents based on visual consistency inferred semantics. Burget (2007) proposes a layout based information extraction from HTML documents and states that this visual approach is more robust than traditional DOM-based methods. Changuel et al.(2009a) describe a system for automatically extracting</context>
</contexts>
<marker>Cai, Yu, Wen, Ma, 2003</marker>
<rawString>Deng Cai, Shipeng Yu, Ji-Rong Wen, and Wei-Ying Ma. 2003. Extracting content structure for web pages based on visual representation. In Web Technologies and Applications, pages 406–417. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chih-Chung Chang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBSVM: A library for support vector machines.</title>
<date>2011</date>
<journal>ACM Transactions on Intelligent Systems and Technology,</journal>
<pages>2--27</pages>
<note>Software available at http:// www.csie.ntu.edu.tw/˜cjlin/libsvm.</note>
<contexts>
<context position="14533" citStr="Chang and Lin, 2011" startWordPosition="2276" endWordPosition="2279">obra.jsp to compute HTML style attributes. However, this renderer Chrome browser to compute dynamically the style of each DOM element and output it as inline style attributes. To achieve this we programmatically inserted a javascript snippet that inlines the computed style and saves the new version of the HTML on the local file system utilizing the HTML5 saveAs interface4. Details on how we normalized the style attribute values for font size, RGB color, and Y coordinate are shown in Table 3. We then applied Support Vector Machines (SVM) (Vapnik, 2000) on the NER task using the LibSVM library (Chang and Lin, 2011). We chose SVMs as they have been shown to perform well on a variety of NER tasks, for example (Isozaki and Kazawa, 2002; Takeuchi and Collier, 2002; Mayfield et al., 2003; Ekbal and Bandyopadhyay, 2008). We used a linear kernel model with the default parameters. The multi-class problem was converted to binary problems using the one-vsothers scheme. 80% of the documents were used for training, and the remaining 20% for testing. 5 Results Results are shown in Table 4. We compared classifier performance using only textual features (first 3 columns), versus performance using both textual and visu</context>
</contexts>
<marker>Chang, Lin, 2011</marker>
<rawString>Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM: A library for support vector machines. ACM Transactions on Intelligent Systems and Technology, 2:27:1–27:27. Software available at http:// www.csie.ntu.edu.tw/˜cjlin/libsvm.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sahar Changuel</author>
<author>Nicolas Labroche</author>
<author>Bernadette Bouchon-Meunier</author>
</authors>
<title>Automatic web pages author extraction. In Flexible Query Answering Systems,</title>
<date>2009</date>
<pages>300--311</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="7514" citStr="Changuel et al., 2009" startWordPosition="1141" endWordPosition="1144">t the semantic level. In (Burget and Rudolfova, 2009) authors propose web-page block classification based on visual features. Yang and Zhang (2001) build a content tree of HTML documents based on visual consistency inferred semantics. Burget (2007) proposes a layout based information extraction from HTML documents and states that this visual approach is more robust than traditional DOM-based methods. Changuel et al.(2009a) describe a system for automatically extracting author information from web-pages. They use spatial information based on the depth of the text node in the HTML DOM tree. In (Changuel et al., 2009b) and (Hu et al., 2006), 1925 the authors proposed a machine learning method for title extraction and utilize format information such as font size, position, and font weight. In (Zhu et al., 2007) authors use layout information based on font size and weight for NER for automated expense reimbursement. While the idea of utilizing visual features based on HTML style has been previously suggested, this study tackles a non-trivial visually rich dataset that prevents the use of previously suggested simplistic approaches to computing HTML features (such as relying on the HTML DOM tree or simplistic</context>
</contexts>
<marker>Changuel, Labroche, Bouchon-Meunier, 2009</marker>
<rawString>Sahar Changuel, Nicolas Labroche, and Bernadette Bouchon-Meunier. 2009a. Automatic web pages author extraction. In Flexible Query Answering Systems, pages 300–311. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sahar Changuel</author>
<author>Nicolas Labroche</author>
<author>Bernadette Bouchon-Meunier</author>
</authors>
<title>A general learning method for automatic title extraction from html pages.</title>
<date>2009</date>
<booktitle>In Machine Learning and Data Mining in Pattern Recognition,</booktitle>
<pages>704--718</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="7514" citStr="Changuel et al., 2009" startWordPosition="1141" endWordPosition="1144">t the semantic level. In (Burget and Rudolfova, 2009) authors propose web-page block classification based on visual features. Yang and Zhang (2001) build a content tree of HTML documents based on visual consistency inferred semantics. Burget (2007) proposes a layout based information extraction from HTML documents and states that this visual approach is more robust than traditional DOM-based methods. Changuel et al.(2009a) describe a system for automatically extracting author information from web-pages. They use spatial information based on the depth of the text node in the HTML DOM tree. In (Changuel et al., 2009b) and (Hu et al., 2006), 1925 the authors proposed a machine learning method for title extraction and utilize format information such as font size, position, and font weight. In (Zhu et al., 2007) authors use layout information based on font size and weight for NER for automated expense reimbursement. While the idea of utilizing visual features based on HTML style has been previously suggested, this study tackles a non-trivial visually rich dataset that prevents the use of previously suggested simplistic approaches to computing HTML features (such as relying on the HTML DOM tree or simplistic</context>
</contexts>
<marker>Changuel, Labroche, Bouchon-Meunier, 2009</marker>
<rawString>Sahar Changuel, Nicolas Labroche, and Bernadette Bouchon-Meunier. 2009b. A general learning method for automatic title extraction from html pages. In Machine Learning and Data Mining in Pattern Recognition, pages 704–718. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nancy Chinchor</author>
<author>Patricia Robinson</author>
</authors>
<title>Muc-7 named entity task definition.</title>
<date>1997</date>
<booktitle>In Proceedings of the 7th Conference on Message Understanding.</booktitle>
<contexts>
<context position="1081" citStr="Chinchor and Robinson, 1997" startWordPosition="156" endWordPosition="160">a result, traditional text-based approaches to information extraction (IE) could underperform. In this study, we present a supervised machine learning approach to IE from online commercial real estate flyers. We evaluated the performance of SVM classifiers on the task of identifying 12 types of named entities using a combination of textual and visual features. Results show that the addition of visual features such as color, size, and positioning significantly increased classifier performance. 1 Introduction Since the Message Understanding Conferences in the 1990s (Grishman and Sundheim, 1996; Chinchor and Robinson, 1997), Information Extraction (IE) and Named Entity Recognition (NER) approaches have been applied and evaluated on a variety of domains and textual genres. The majority of the work, however, focuses on the journalistic, scientific, and informal genres (newswires, scientific publications, blogs, tweets, and other social media texts) (Nadeau and Sekine, 2007) and deals with purely textual corpora. As a result, the feature space of NER systems involves purely textual features, typically word attributes and characteristics (orthography, morphology, dictionary lookup, etc.), their contexts and document</context>
</contexts>
<marker>Chinchor, Robinson, 1997</marker>
<rawString>Nancy Chinchor and Patricia Robinson. 1997. Muc-7 named entity task definition. In Proceedings of the 7th Conference on Message Understanding.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Asif Ekbal</author>
<author>Sivaji Bandyopadhyay</author>
</authors>
<title>Named entity recognition using support vector machine: A language independent approach.</title>
<date>2008</date>
<journal>International Journal of Computer Systems Science &amp; Engineering,</journal>
<volume>4</volume>
<issue>2</issue>
<contexts>
<context position="14736" citStr="Ekbal and Bandyopadhyay, 2008" startWordPosition="2311" endWordPosition="2314">e programmatically inserted a javascript snippet that inlines the computed style and saves the new version of the HTML on the local file system utilizing the HTML5 saveAs interface4. Details on how we normalized the style attribute values for font size, RGB color, and Y coordinate are shown in Table 3. We then applied Support Vector Machines (SVM) (Vapnik, 2000) on the NER task using the LibSVM library (Chang and Lin, 2011). We chose SVMs as they have been shown to perform well on a variety of NER tasks, for example (Isozaki and Kazawa, 2002; Takeuchi and Collier, 2002; Mayfield et al., 2003; Ekbal and Bandyopadhyay, 2008). We used a linear kernel model with the default parameters. The multi-class problem was converted to binary problems using the one-vsothers scheme. 80% of the documents were used for training, and the remaining 20% for testing. 5 Results Results are shown in Table 4. We compared classifier performance using only textual features (first 3 columns), versus performance using both textual and visual features (next 3 columns). Results were averaged over 2 runs of randomly selected training/test documents with 80%/20% ratio. We used an exact measure which considers an answer to be correct only if b</context>
</contexts>
<marker>Ekbal, Bandyopadhyay, 2008</marker>
<rawString>Asif Ekbal and Sivaji Bandyopadhyay. 2008. Named entity recognition using support vector machine: A language independent approach. International Journal of Computer Systems Science &amp; Engineering, 4(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralph Grishman</author>
<author>Beth Sundheim</author>
</authors>
<title>Message understanding conference-6: A brief history.</title>
<date>1996</date>
<booktitle>In COLING,</booktitle>
<volume>96</volume>
<pages>466--471</pages>
<contexts>
<context position="1051" citStr="Grishman and Sundheim, 1996" startWordPosition="152" endWordPosition="155">, size, positioning, etc. As a result, traditional text-based approaches to information extraction (IE) could underperform. In this study, we present a supervised machine learning approach to IE from online commercial real estate flyers. We evaluated the performance of SVM classifiers on the task of identifying 12 types of named entities using a combination of textual and visual features. Results show that the addition of visual features such as color, size, and positioning significantly increased classifier performance. 1 Introduction Since the Message Understanding Conferences in the 1990s (Grishman and Sundheim, 1996; Chinchor and Robinson, 1997), Information Extraction (IE) and Named Entity Recognition (NER) approaches have been applied and evaluated on a variety of domains and textual genres. The majority of the work, however, focuses on the journalistic, scientific, and informal genres (newswires, scientific publications, blogs, tweets, and other social media texts) (Nadeau and Sekine, 2007) and deals with purely textual corpora. As a result, the feature space of NER systems involves purely textual features, typically word attributes and characteristics (orthography, morphology, dictionary lookup, etc.</context>
</contexts>
<marker>Grishman, Sundheim, 1996</marker>
<rawString>Ralph Grishman and Beth Sundheim. 1996. Message understanding conference-6: A brief history. In COLING, volume 96, pages 466–471.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yunhua Hu</author>
<author>Hang Li</author>
<author>Yunbo Cao</author>
<author>Li Teng</author>
<author>Dmitriy Meyerzon</author>
<author>Qinghua Zheng</author>
</authors>
<title>Automatic extraction of titles from general documents using machine learning.</title>
<date>2006</date>
<booktitle>Information processing &amp; management,</booktitle>
<pages>42--5</pages>
<contexts>
<context position="7538" citStr="Hu et al., 2006" startWordPosition="1146" endWordPosition="1149">get and Rudolfova, 2009) authors propose web-page block classification based on visual features. Yang and Zhang (2001) build a content tree of HTML documents based on visual consistency inferred semantics. Burget (2007) proposes a layout based information extraction from HTML documents and states that this visual approach is more robust than traditional DOM-based methods. Changuel et al.(2009a) describe a system for automatically extracting author information from web-pages. They use spatial information based on the depth of the text node in the HTML DOM tree. In (Changuel et al., 2009b) and (Hu et al., 2006), 1925 the authors proposed a machine learning method for title extraction and utilize format information such as font size, position, and font weight. In (Zhu et al., 2007) authors use layout information based on font size and weight for NER for automated expense reimbursement. While the idea of utilizing visual features based on HTML style has been previously suggested, this study tackles a non-trivial visually rich dataset that prevents the use of previously suggested simplistic approaches to computing HTML features (such as relying on the HTML DOM tree or simplistic HTML style rendering). </context>
</contexts>
<marker>Hu, Li, Cao, Teng, Meyerzon, Zheng, 2006</marker>
<rawString>Yunhua Hu, Hang Li, Yunbo Cao, Li Teng, Dmitriy Meyerzon, and Qinghua Zheng. 2006. Automatic extraction of titles from general documents using machine learning. Information processing &amp; management, 42(5):1276–1293.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hideki Isozaki</author>
<author>Hideto Kazawa</author>
</authors>
<title>Efficient support vector classifiers for named entity recognition.</title>
<date>2002</date>
<booktitle>In Proceedings of the 19th international conference on Computational linguistics-Volume 1,</booktitle>
<pages>1--7</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="14653" citStr="Isozaki and Kazawa, 2002" startWordPosition="2299" endWordPosition="2302"> each DOM element and output it as inline style attributes. To achieve this we programmatically inserted a javascript snippet that inlines the computed style and saves the new version of the HTML on the local file system utilizing the HTML5 saveAs interface4. Details on how we normalized the style attribute values for font size, RGB color, and Y coordinate are shown in Table 3. We then applied Support Vector Machines (SVM) (Vapnik, 2000) on the NER task using the LibSVM library (Chang and Lin, 2011). We chose SVMs as they have been shown to perform well on a variety of NER tasks, for example (Isozaki and Kazawa, 2002; Takeuchi and Collier, 2002; Mayfield et al., 2003; Ekbal and Bandyopadhyay, 2008). We used a linear kernel model with the default parameters. The multi-class problem was converted to binary problems using the one-vsothers scheme. 80% of the documents were used for training, and the remaining 20% for testing. 5 Results Results are shown in Table 4. We compared classifier performance using only textual features (first 3 columns), versus performance using both textual and visual features (next 3 columns). Results were averaged over 2 runs of randomly selected training/test documents with 80%/20</context>
</contexts>
<marker>Isozaki, Kazawa, 2002</marker>
<rawString>Hideki Isozaki and Hideto Kazawa. 2002. Efficient support vector classifiers for named entity recognition. In Proceedings of the 19th international conference on Computational linguistics-Volume 1, pages 1–7. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicholas Kushmerick</author>
</authors>
<title>Wrapper induction for information extraction.</title>
<date>1997</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Washington.</institution>
<contexts>
<context position="6228" citStr="Kushmerick, 1997" startWordPosition="938" endWordPosition="940">ze Size and attributes of relevant spaces Space Type (e.g. 27,042 SF building, 4.44 acres site, etc.); Mentions of space type descriptors, e.g. building, land/lot, floor, unit. This excludes space type and size information of non-essential listing attributes (e.g. basement size or parking lot size). Confidential Any mentions of confidentiality. Table 1: Types and descriptions of named entities relevant to extracting listing information from commercial real estate flyers. The literature on Information Extraction from HTML resources is dominated by various approaches based on wrapper induction (Kushmerick, 1997; Kushmerick, 2000). Wrapper inductions rely on common HTML structure (based on the HTML DOM) and formatting features to extract structured information from similarly formatted HTML pages. This approach, however, is not applicable to the genres of marketing materials (PDF and HTML) since they typically do not share any common structure that can be used to identify relevant named entities. Laender et al. (2002) present a survey of data extraction techniques and tools from structured or semi-structured web resources. Cai et al. (2003) present a vision-based segmentation algorithm of web pages th</context>
</contexts>
<marker>Kushmerick, 1997</marker>
<rawString>Nicholas Kushmerick. 1997. Wrapper induction for information extraction. Ph.D. thesis, University of Washington.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicholas Kushmerick</author>
</authors>
<title>Wrapper induction: Efficiency and expressiveness.</title>
<date>2000</date>
<journal>Artificial Intelligence,</journal>
<volume>118</volume>
<issue>1</issue>
<contexts>
<context position="6247" citStr="Kushmerick, 2000" startWordPosition="941" endWordPosition="942">utes of relevant spaces Space Type (e.g. 27,042 SF building, 4.44 acres site, etc.); Mentions of space type descriptors, e.g. building, land/lot, floor, unit. This excludes space type and size information of non-essential listing attributes (e.g. basement size or parking lot size). Confidential Any mentions of confidentiality. Table 1: Types and descriptions of named entities relevant to extracting listing information from commercial real estate flyers. The literature on Information Extraction from HTML resources is dominated by various approaches based on wrapper induction (Kushmerick, 1997; Kushmerick, 2000). Wrapper inductions rely on common HTML structure (based on the HTML DOM) and formatting features to extract structured information from similarly formatted HTML pages. This approach, however, is not applicable to the genres of marketing materials (PDF and HTML) since they typically do not share any common structure that can be used to identify relevant named entities. Laender et al. (2002) present a survey of data extraction techniques and tools from structured or semi-structured web resources. Cai et al. (2003) present a vision-based segmentation algorithm of web pages that uses HTML layout</context>
</contexts>
<marker>Kushmerick, 2000</marker>
<rawString>Nicholas Kushmerick. 2000. Wrapper induction: Efficiency and expressiveness. Artificial Intelligence, 118(1):15–68.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alberto HF Laender</author>
</authors>
<title>Berthier A Ribeiro-Neto, Altigran</title>
<date>2002</date>
<journal>ACM Sigmod Record,</journal>
<volume>31</volume>
<issue>2</issue>
<marker>Laender, 2002</marker>
<rawString>Alberto HF Laender, Berthier A Ribeiro-Neto, Altigran S da Silva, and Juliana S Teixeira. 2002. A brief survey of web data extraction tools. ACM Sigmod Record, 31(2):84–93.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Mayfield</author>
<author>Paul McNamee</author>
<author>Christine Piatko</author>
</authors>
<title>Named entity recognition using hundreds of thousands of features.</title>
<date>2003</date>
<booktitle>In Proceedings of the seventh conference on Natural language learning at HLTNAACL 2003-Volume 4,</booktitle>
<pages>184--187</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="14704" citStr="Mayfield et al., 2003" startWordPosition="2307" endWordPosition="2310">utes. To achieve this we programmatically inserted a javascript snippet that inlines the computed style and saves the new version of the HTML on the local file system utilizing the HTML5 saveAs interface4. Details on how we normalized the style attribute values for font size, RGB color, and Y coordinate are shown in Table 3. We then applied Support Vector Machines (SVM) (Vapnik, 2000) on the NER task using the LibSVM library (Chang and Lin, 2011). We chose SVMs as they have been shown to perform well on a variety of NER tasks, for example (Isozaki and Kazawa, 2002; Takeuchi and Collier, 2002; Mayfield et al., 2003; Ekbal and Bandyopadhyay, 2008). We used a linear kernel model with the default parameters. The multi-class problem was converted to binary problems using the one-vsothers scheme. 80% of the documents were used for training, and the remaining 20% for testing. 5 Results Results are shown in Table 4. We compared classifier performance using only textual features (first 3 columns), versus performance using both textual and visual features (next 3 columns). Results were averaged over 2 runs of randomly selected training/test documents with 80%/20% ratio. We used an exact measure which considers a</context>
</contexts>
<marker>Mayfield, McNamee, Piatko, 2003</marker>
<rawString>James Mayfield, Paul McNamee, and Christine Piatko. 2003. Named entity recognition using hundreds of thousands of features. In Proceedings of the seventh conference on Natural language learning at HLTNAACL 2003-Volume 4, pages 184–187. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Nadeau</author>
<author>Satoshi Sekine</author>
</authors>
<title>A survey of named entity recognition and classification.</title>
<date>2007</date>
<journal>Lingvisticae Investigationes,</journal>
<volume>30</volume>
<issue>1</issue>
<contexts>
<context position="1436" citStr="Nadeau and Sekine, 2007" startWordPosition="210" endWordPosition="213">es. Results show that the addition of visual features such as color, size, and positioning significantly increased classifier performance. 1 Introduction Since the Message Understanding Conferences in the 1990s (Grishman and Sundheim, 1996; Chinchor and Robinson, 1997), Information Extraction (IE) and Named Entity Recognition (NER) approaches have been applied and evaluated on a variety of domains and textual genres. The majority of the work, however, focuses on the journalistic, scientific, and informal genres (newswires, scientific publications, blogs, tweets, and other social media texts) (Nadeau and Sekine, 2007) and deals with purely textual corpora. As a result, the feature space of NER systems involves purely textual features, typically word attributes and characteristics (orthography, morphology, dictionary lookup, etc.), their contexts and document features (surrounding word window, local syntax, document/corpus word frequencies, etc.) (Nadeau and Sekine, 2007). Noriko Tomuro DePaul University 243 S. Wabash Ave. Chicago, IL 60604 tomuro@cs.depaul.edu At the same time, textual information is often presented in visually rich formats, e.g. HTML and PDF. In addition to text, these formats use a varie</context>
</contexts>
<marker>Nadeau, Sekine, 2007</marker>
<rawString>David Nadeau and Satoshi Sekine. 2007. A survey of named entity recognition and classification. Lingvisticae Investigationes, 30(1):3–26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koichi Takeuchi</author>
<author>Nigel Collier</author>
</authors>
<title>Use of support vector machines in extended named entity recognition.</title>
<date>2002</date>
<booktitle>In proceedings of the 6th conference on Natural language learning-Volume 20,</booktitle>
<pages>pages</pages>
<contexts>
<context position="14681" citStr="Takeuchi and Collier, 2002" startWordPosition="2303" endWordPosition="2306">ut it as inline style attributes. To achieve this we programmatically inserted a javascript snippet that inlines the computed style and saves the new version of the HTML on the local file system utilizing the HTML5 saveAs interface4. Details on how we normalized the style attribute values for font size, RGB color, and Y coordinate are shown in Table 3. We then applied Support Vector Machines (SVM) (Vapnik, 2000) on the NER task using the LibSVM library (Chang and Lin, 2011). We chose SVMs as they have been shown to perform well on a variety of NER tasks, for example (Isozaki and Kazawa, 2002; Takeuchi and Collier, 2002; Mayfield et al., 2003; Ekbal and Bandyopadhyay, 2008). We used a linear kernel model with the default parameters. The multi-class problem was converted to binary problems using the one-vsothers scheme. 80% of the documents were used for training, and the remaining 20% for testing. 5 Results Results are shown in Table 4. We compared classifier performance using only textual features (first 3 columns), versus performance using both textual and visual features (next 3 columns). Results were averaged over 2 runs of randomly selected training/test documents with 80%/20% ratio. We used an exact me</context>
</contexts>
<marker>Takeuchi, Collier, 2002</marker>
<rawString>Koichi Takeuchi and Nigel Collier. 2002. Use of support vector machines in extended named entity recognition. In proceedings of the 6th conference on Natural language learning-Volume 20, pages 1– 7. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir Vapnik</author>
</authors>
<title>The nature of statistical learning theory.</title>
<date>2000</date>
<publisher>springer.</publisher>
<contexts>
<context position="14470" citStr="Vapnik, 2000" startWordPosition="2266" endWordPosition="2267">rer from the Cobra java toolkit http://lobobrowser.org/cobra.jsp to compute HTML style attributes. However, this renderer Chrome browser to compute dynamically the style of each DOM element and output it as inline style attributes. To achieve this we programmatically inserted a javascript snippet that inlines the computed style and saves the new version of the HTML on the local file system utilizing the HTML5 saveAs interface4. Details on how we normalized the style attribute values for font size, RGB color, and Y coordinate are shown in Table 3. We then applied Support Vector Machines (SVM) (Vapnik, 2000) on the NER task using the LibSVM library (Chang and Lin, 2011). We chose SVMs as they have been shown to perform well on a variety of NER tasks, for example (Isozaki and Kazawa, 2002; Takeuchi and Collier, 2002; Mayfield et al., 2003; Ekbal and Bandyopadhyay, 2008). We used a linear kernel model with the default parameters. The multi-class problem was converted to binary problems using the one-vsothers scheme. 80% of the documents were used for training, and the remaining 20% for testing. 5 Results Results are shown in Table 4. We compared classifier performance using only textual features (f</context>
</contexts>
<marker>Vapnik, 2000</marker>
<rawString>Vladimir Vapnik. 2000. The nature of statistical learning theory. springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yudong Yang</author>
<author>HongJiang Zhang</author>
</authors>
<title>Html page analysis based on visual cues.</title>
<date>2001</date>
<booktitle>In DocumentAnalysis and Recognition,</booktitle>
<pages>859--864</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="7040" citStr="Yang and Zhang (2001)" startWordPosition="1067" endWordPosition="1070">is approach, however, is not applicable to the genres of marketing materials (PDF and HTML) since they typically do not share any common structure that can be used to identify relevant named entities. Laender et al. (2002) present a survey of data extraction techniques and tools from structured or semi-structured web resources. Cai et al. (2003) present a vision-based segmentation algorithm of web pages that uses HTML layout features and attempts to partition the page at the semantic level. In (Burget and Rudolfova, 2009) authors propose web-page block classification based on visual features. Yang and Zhang (2001) build a content tree of HTML documents based on visual consistency inferred semantics. Burget (2007) proposes a layout based information extraction from HTML documents and states that this visual approach is more robust than traditional DOM-based methods. Changuel et al.(2009a) describe a system for automatically extracting author information from web-pages. They use spatial information based on the depth of the text node in the HTML DOM tree. In (Changuel et al., 2009b) and (Hu et al., 2006), 1925 the authors proposed a machine learning method for title extraction and utilize format informat</context>
</contexts>
<marker>Yang, Zhang, 2001</marker>
<rawString>Yudong Yang and HongJiang Zhang. 2001. Html page analysis based on visual cues. In DocumentAnalysis and Recognition, 2001. Proceedings. Sixth International Conference on, pages 859–864. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guangyu Zhu</author>
<author>Timothy J Bethea</author>
<author>Vikas Krishna</author>
</authors>
<title>Extracting relevant named entities for automated expense reimbursement.</title>
<date>2007</date>
<booktitle>In Proceedings of the 13th ACM SIGKDD international conference on Knowledge discovery and data mining,</booktitle>
<pages>1004--1012</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="7711" citStr="Zhu et al., 2007" startWordPosition="1174" endWordPosition="1177">l consistency inferred semantics. Burget (2007) proposes a layout based information extraction from HTML documents and states that this visual approach is more robust than traditional DOM-based methods. Changuel et al.(2009a) describe a system for automatically extracting author information from web-pages. They use spatial information based on the depth of the text node in the HTML DOM tree. In (Changuel et al., 2009b) and (Hu et al., 2006), 1925 the authors proposed a machine learning method for title extraction and utilize format information such as font size, position, and font weight. In (Zhu et al., 2007) authors use layout information based on font size and weight for NER for automated expense reimbursement. While the idea of utilizing visual features based on HTML style has been previously suggested, this study tackles a non-trivial visually rich dataset that prevents the use of previously suggested simplistic approaches to computing HTML features (such as relying on the HTML DOM tree or simplistic HTML style rendering). In addition, we introduce the use of RGB color as a feature and normalize it approximating human perception. 4 Dataset and Method The dataset consists of 800 randomly select</context>
</contexts>
<marker>Zhu, Bethea, Krishna, 2007</marker>
<rawString>Guangyu Zhu, Timothy J Bethea, and Vikas Krishna. 2007. Extracting relevant named entities for automated expense reimbursement. In Proceedings of the 13th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 1004– 1012. ACM.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
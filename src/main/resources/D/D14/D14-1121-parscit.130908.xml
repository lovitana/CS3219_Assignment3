<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000147">
<title confidence="0.91359">
Developing Age and Gender Predictive Lexica over Social Media
</title>
<author confidence="0.819149">
Maarten Sap&apos; Gregory Park&apos; Johannes C. Eichstaedt&apos; Margaret L. Kern&apos;
David Stillwell3 Michal Kosinski3 Lyle H. Ungar2 and H. Andrew Schwartz2
</author>
<affiliation confidence="0.989531">
&apos;Department of Psychology, University of Pennsylvania
2Computer &amp; Information Science, University of Pennsylvania
3Psychometrics Centre, University of Cambridge
</affiliation>
<email confidence="0.998864">
maarten@sas.upenn.edu
</email>
<sectionHeader confidence="0.993898" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999366692307692">
Demographic lexica have potential for
widespread use in social science, economic,
and business applications. We derive predic-
tive lexica (words and weights) for age and
gender using regression and classification
models from word usage in Facebook, blog,
and Twitter data with associated demographic
labels. The lexica, made publicly available,1
achieved state-of-the-art accuracy in language
based age and gender prediction over Face-
book and Twitter, and were evaluated for
generalization across social media genres as
well as in limited message situations.
</bodyText>
<sectionHeader confidence="0.998796" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.965144964285714">
Use of social media has enabled the study of psycho-
logical and social questions at an unprecedented scale
(Lazer et al., 2009). This allows more data-driven dis-
covery alongside the typical hypothesis-testing social
science process (Schwartz et al., 2013b). Social me-
dia may track disease rates (Paul and Dredze, 2011;
Google, 2014), psychological well-being (Dodds et al.,
2011; De Choudhury et al., 2013; Schwartz et al.,
2013a), and a host of other behavioral, psychological
and medical phenomena (Kosinski et al., 2013).
Unlike traditional hypothesis-driven social science,
such large-scale social media studies rarely take into
account—or have access to—age and gender informa-
tion, which can have a major impact on many ques-
tions. For example, females live almost five years
longer than males (cdc, 2014; Marengoni et al., 2011).
Men and women, on average, differ markedly in their
interests and work preferences (Su et al., 2009). With
age, personalities gradually change, typically becom-
ing less open to experiences but more agreeable and
conscientious (McCrae et al., 1999). Additionally, so-
cial media language varies by age (Kern et al., 2014;
Pennebaker and Stone, 2003) and gender (Huffaker and
Calvert, 2005). Twitter may have a male bias (Mislove
et al., 2011), while social media in general skew to-
wards being young and female (pew, 2014).
Accessible tools to predict demographic variables
can substantially enhance social media’s utility for so-
</bodyText>
<footnote confidence="0.995606">
1download at http://www.wwbp.org/data.html
</footnote>
<bodyText confidence="0.99896876">
cial science, economic, and business applications. For
example, one can post-stratify population-level results
to reflect a representative sample, understand variation
across age and gender groups, or produce personalized
marketing, services, and sentiment recommendations;
a movie may be generally disliked, except by people in
a certain age group, whereas a product might be used
primarily by one gender.
This paper describes the creation of age and gen-
der predictive lexica from a dataset of Facebook users
who agreed to share their status updates and reported
their age and gender. The lexica, in the form of words
with associated weights, are derived from a penalized
linear regression (for continuous valued age) and sup-
port vector classification (for binary-valued gender). In
this modality, the lexica are simply a transparent and
portable means for distributing predictive models based
on words. We test generalization and adapt the lex-
ica to blogs and Twitter, plus consider situations when
limited messages are available. In addition to use in
the computational linguistics community, we believe
the lexicon format will make it easier for social sci-
entists to leverage data-driven models where manually
created lexica currently dominate2 (Dodds et al., 2011;
Tausczik and Pennebaker, 2010).
</bodyText>
<sectionHeader confidence="0.999693" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.9998214375">
Online behavior is representative of many aspects of
a user’s demographics (Pennacchiotti and Popescu,
2011; Rao et al., 2010). Many studies have used lin-
guistic cues (such as ngrams) to determine if someone
belongs to a certain age group, be it on Twitter or an-
other social media platform (Al Zamal et al., 2012;
Argamon et al., 2009; Nguyen et al., 2013; Rangel
and Rosso, 2013). Gender prediction has been studied
across blogs (Burger and Henderson, 2006; Goswami
et al., 2009), Yahoo! search queries (Jones et al., 2007),
and Twitter (Burger et al., 2011; Nguyen et al., 2013;
Liu and Ruths, 2013; Rao et al., 2010). Because Twit-
ter does not make gender or age available, such work
infers gender and age by leveraging profile informa-
tion, such as gender-discriminating names or crawling
for links to publicly available data (e.g. Burger et al.,
</bodyText>
<footnote confidence="0.999514333333333">
2The LIWC lexicon, derived manually based on psycho-
logical theory, (Pennebaker et al., 2001) had 1136 citations in
2013 alone.
</footnote>
<page confidence="0.940122">
1146
</page>
<bodyText confidence="0.988471029411765">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1146–1151,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
2011).
While many studies have examined prediction of age
or gender, none (to our knowledge) have released a
model to the public, much less in the form of a lexi-
con. Additionally, most works in age prediction clas-
sify users into bins rather than predicting a continuous
real-valued age as we do (exceptions: Nguyen et al.,
2013; Jones et al., 2007). People have also used online
media to infer other demographic-like attributes such
as native language (Argamon et al., 2009), origin (Rao
et al., 2010), and location (Jones et al., 2007). An ap-
proach similar to the one presented here could be used
to create lexica for any of these outcomes.
While lexica are not often used for demographics,
data-driven lexicon creation over social media has been
well studied for sentiment, in which univariate tech-
niques (e.g. point-wise mutual information) domi-
nate3. For example, Taboada et al. (2011) expanded
an initial lexicon by adding on co-occurring words.
More recently, Mohammad’s sentiment lexicon (Mo-
hammad et al., 2013) was found to be the most in-
formative feature for the top system in the SemEval-
2013 social media sentiment analysis task (Wilson et
al., 2013). Approaches like point-wise mutual infor-
mation take a univariate view on words–i.e. the weight
given to one feature (word) is not affected by other
features. Since language is highly collinear, we take
a multivariate lexicon development approach, which
takes covariance into account (e.g. someone who men-
tions ‘hair’ often is more likely to mention ‘brushing’,
‘style’, and ‘cut’; weighting these words in isolation
might “double-count” some information).
</bodyText>
<sectionHeader confidence="0.985484" genericHeader="method">
3 Method
</sectionHeader>
<bodyText confidence="0.99994034883721">
Primary data. Our primary dataset consists of Face-
book messages from users of the MyPersonality appli-
cation (Kosinski and Stillwell, 2012). Messages were
posted between January 2009 and October 2011. We
restrict our analysis to those Facebook users meeting
certain criteria: they must indicate English as a primary
language, have written at least 1,000 words in their sta-
tus updates, be younger than 65 years old (data beyond
this age becomes very sparse), and indicate their gen-
der and age. This resulted in a dataset of N = 75,394
users, who wrote over 300 million words collectively.
We split our sample into training and test sets. Our
primary test set consists of a 1,000 randomly selected
Facebook users, while the training set that we used for
creating the lexica was a subset (N = 72,874) of the
remaining users.
Additional data To evaluate our predictive lexica in
differing situations, we utilize three additional datasets:
3Note that the point-wise information-derived sentiment
lexica are often used as features in a supervised model, essen-
tially dimensionally reducing a large set of words into posi-
tive and negative sentiment, while our lexica represent the
predictive model itself.
stratified Facebook data, blogs, and tweets. The strat-
ified Facebook data (exclusively used for testing) con-
sists of equal proportions of 1,520 males and females
across 12 4-year age bins starting at 13 and ending at
60.4 This roughly matchs the size of the main test set.
Seeking out-of-domain data, we downloaded age
and gender annotated blogs from 2004 (Schler et al.,
2006) (also used in Goswami et al., 2009) and gender
labeled tweets (Volkova et al., 2013). Limiting the sam-
ple to users who wrote at least 1000 words, the total
number of bloggers is 15,006, of which 50.6% are fe-
male and only 15% are over 27 (reflecting the younger
population standard in social media). From this we use
a randomly selected 1,000 bloggers as a blogger test set
and the remaining 14,006 bloggers for training. Sim-
ilarly for the Twitter dataset, we use 11,000 random
gender-only annotated users, in which 51.9% are fe-
male. We again randomly select 1,000 users as a test
set for gender prediction and use the remaining 10,000
for training.
</bodyText>
<subsectionHeader confidence="0.997754">
3.1 Lexicon Creation
</subsectionHeader>
<bodyText confidence="0.99991">
We present a method of weighted lexicon creation by
using the coefficients from linear multivariate regres-
sion and classification models. Before delving into the
creation process, consider that a weighted lexicon is of-
ten applied as the sum of all weighted word relative
frequencies over a document:
</bodyText>
<equation confidence="0.9708145">
wley(word) * freq(word, doc)
freq(*, doc)
</equation>
<bodyText confidence="0.999979375">
where wlex(word) is the lexicon (lex) weight for the
word, freq(word, doc) is frequency of the word in the
document (or for a given user), and freq(*, doc) is the
total word count for that document (or user).
Further consider how one applies linear multivariate
models in which the goal is to optimize feature coeffi-
cients that best fit the continuous outcome (regression)
or separate two classes (classification):
</bodyText>
<equation confidence="0.9509485">
y = ( � wf * xf) + w0
f∈features
</equation>
<bodyText confidence="0.885573428571429">
where xf is the value for a feature (f), wf is the fea-
ture coefficient, and w0 is the intercept (a constant fit
to shift the data such that it passes through the origin).
In the case of regression, y is the outcome value (e.g.
age) while in classification y is used to separate classes
(e.g. &gt;= 0 is female, &lt; 0 is male). If all features are
)
word relative frequencies (freq(word,doc freq(∗,doc) )then many
multivariate modeling techniques can simply be seen
as learning a weighted lexicon plus an intercept5.
465 females and 65 males in each of the first 11 bins:
[13,16], [17,20], ..., [53, 56]; the last bin ([57, 60]) contained
45 males and 45 females. The [61.64] bin was excluded as it
was much smaller.
</bodyText>
<footnote confidence="0.956442">
5included in the lexicon distribution
</footnote>
<equation confidence="0.884703">
�usageley =
word∈ley
</equation>
<page confidence="0.910728">
1147
</page>
<table confidence="0.998984">
age gender
model\corpus randFB stratFB randBG randFB stratFB randBG randT
r mae r mae r mae acc acc acc acc
baseline 0 6.14 0 11.62 0 6.11 .617 .500 .508 .518
FBlex .835 3.40 .801 6.94 .710 5.76 .917 .913 .774 .856
BGlex .664 4.26 .656 11.39 .768 3.63 .838 .803 .824 .834
FB+BGlex .831 3.42 .795 7.06 .762 3.76 .913 .909 .822 .858
Tlex .816 .820 .763 .889
FB+BG+Tlex .919 .910 .820 .900
</table>
<tableCaption confidence="0.683057333333333">
Table 1: Prediction accuracies for age (Pearson correlation coefficient(r); mean absolute error (mae) in years)
and gender (accuracy %). Baseline for age is mean age of training sample; for gender, it is the most frequent
class (female). Lexica tested include those derived from Facebook (FBlex), blogs (BGlex), and Twitter (Tlex).
</tableCaption>
<bodyText confidence="0.999757259259259">
We evaluate over a random Facebook sample (randFB), a stratified Facebook sample (stratFB), a random blogger
sample (randBG), and a random twitter sample (randT). All results were a significant (p &lt; 0.001) improvement
over the baseline.
In practice, we learn our 1gram coefficients (i.e. lex-
icon weights) from ridge regression (Hoerl and Ken-
nard, 1970) for age (continuous variable) and from sup-
port vector classification (Fan et al., 2008) for gender
(binary variable). Ridge regression uses an L2 (α||β||2)
penalization to avoid overfitting (Hoerl and Kennard,
1970). Although some words no doubt have a non-
linear relationship with age (e.g., ‘fiance’ peaks in the
20s), we still find high accuracy from a linear model
(see Table 1) and it allows for a distribution of the
model in the accessible form of a lexicon. For gender
prediction, we use an SVM with a linear kernel with
L1 penalization (α||β||1) (Tibshirani, 1996). Because
the L1 penalization zeros-out many coefficients, it has
the added advantage of effectively reducing the size of
the lexica. Using the training data, we test a variety al-
gorithms including the lasso, elastic net regression, and
L2 penalized SVMs in order to decide which learning
algorithms to use.
To extract the words (1grams) to use as features
and which make up lexica, we use the Happier Fun
Tokenizer,6 which handles social media content and
markup such as emoticons or hashtags. For our main
user-level models, word usage is aggregated as the rel-
</bodyText>
<equation confidence="0.6165985">
ative frequency (freq(word,user) ) Due to the sparse
freq(∗,user)
</equation>
<bodyText confidence="0.7519495">
and large vocabulary of social media data, we limit the
1grams to those used by at least 1% of users.
</bodyText>
<sectionHeader confidence="0.999029" genericHeader="evaluation">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.9999429">
We evaluate our predictive lexica across held-out user
data. First, we see how well lexica derived from Face-
book users predict a random set of additional users.
Then, we explore generalization of the models in vari-
ous other settings: on a stratified Facebook test sample,
blogs, and Twitter. Finally, we compare lexica fit to a
restricted number of messages per user.
Results of our evaluation over Facebook users are
shown in Table 1 (randFB columns). Accuracies for
age are reported as Pearson correlation coefficients (r)
</bodyText>
<footnote confidence="0.977266">
6downloaded from http://www.wwbp.org/data.html
</footnote>
<bodyText confidence="0.999950944444444">
and mean absolute errors (mae), measured in years.
For gender, we use an accuracy % (number-correct over
test-size). As baselines, we use the mean for age (23.0
years old) and the most frequent class (female) for gen-
der. We see that for both age and gender, accuracies are
substantially higher than the baseline. These accuracies
were just below with no significant difference previous
state-of-the-art results (Schwartz et al., 2013; r = 0.84
for age and 91.9% accuracy for gender). 7
Because of the nature of our datasets (the Face-
book data is private) and task (user-level predictions),
comparable previous studies are nearly nonexistent.
Nonetheless, the Twitter data was a random subset of
users based on the (Burger et al., 2011) dataset exclud-
ing non-English tweets, making it somewhat compa-
rable. In this case, the lexica outperformed previous
results for gender prediction of Twitter users, which
ranged from 75.5% to 87% (Burger et al., 2011; Ciot
et al., 2013; Liu and Ruths, 2013; Al Zamal et al.,
2012). However, the lexica were unable to match the
92.0% accuracy Burger et al. (2011) achieved when
using profile information in addition to text. No other
similar studies — to the best of our knowledge — have
been conducted.
Application in other settings. While Facebook is the
ideal setting to apply our lexica, we hope that they gen-
eralize to other situations. To evaluate their utility in
other settings, we first tested them over a gender and
age stratified Facebook sample. Our random sample,
like all of Facebook, is biased toward the young; this
stratified test sample contains equal numbers of males
and females, ages 13 to 60. Next, we use the lexica to
predict data from other domains: blogs (Schler et al.,
2006) and Twitter (Volkova et al., 2013). In this case,
our goal was to account for the content and stylistic
variation that may be specific to Facebook.
</bodyText>
<footnote confidence="0.8018714">
7Adding 2 and 3-grams increases the performance of our
model (r = 0.85, 92.7%), just above our previous results
(Schwartz et al., 2013b). However, with the accessibility of
single word lexica in mind, this current work focuses on fea-
tures based entirely on 1grams.
</footnote>
<page confidence="0.953116">
1148
</page>
<table confidence="0.999355666666667">
# Msgs: all 100 20 5 1
age .831 .820 .688 .454 .156
gender .919 .901 .796 .635 .554
</table>
<tableCaption confidence="0.959845">
Table 2: Prediction accuracies for age (Pearson correla-
tion) and gender (accuracy %) when reducing the num-
ber of messages from each user.
Results over these additional datasets are shown in
Table 1 (stratFB, randBG, and randT columns). The
</tableCaption>
<bodyText confidence="0.9958085">
performance decreases as expected since these datasets
have differing distributions, but it is still substantially
above mean and most frequent class baselines on the
stratified dataset. Over blogs and Twitter, both age
and gender prediction accuracies drop to a greater de-
gree (when only using the Facebook-trained models),
suggesting stylistic or content differences between the
domains. However, when using lexica created with
data from across multiple domains, the results in Face-
book, blogs, and Twitter remain in line with results
from models created specifically over their respective
domains. In light of this result, we release the FB+BG
age &amp; FB+BG+T gender models as lexica (available at
www.wwbp.org/data.html).
Limiting messages per user. As previously noted,
some applications of demographic estimation require
predictions over more limited messages. We explore
the accuracy of user-level age and gender predictions
as the number of messages per user decreases in Ta-
ble 2. For these tests we used the FB+BG age &amp;
FB+BG+T gender lexica. Confirming findings by Van
Durme (2012), the fewer posts one has for each user,
the less accurate the gender and age predictions. Still,
given the average user posted 205 messages, it seems
that not all messages from a user are necessary to make
a decent inference on their age and gender. Future work
may explore models developed specifically for these
limited situations.
</bodyText>
<sectionHeader confidence="0.997767" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999986263157895">
We created publicly available lexica (words and
weights) using regression and classification models
over language usage in social media. Evaluation of the
lexica over Facebook yielded accuracies in line with
state-of-the-art age (r = 0.831) and gender (91.9% ac-
curacy) prediction. By deriving the lexica from Face-
book, blogs, and Twitter, we found the predictive power
generalized across all three domains with little sacrifice
to any one domain, suggesting the lexica may be used
in additional social media domains. We also found the
lexica maintain reasonable accuracy when writing sam-
ples were somewhat small (e.g. 20 messages) but other
approaches may be best when dealing with more lim-
ited data.
Given that manual lexica are already extensively em-
ployed in social sciences such as psychology, eco-
nomics, and business, using lexical representations of
data-driven models allows the utility of our models to
extend beyond the borders of the field of NLP.
</bodyText>
<sectionHeader confidence="0.721019" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.998781">
Support for this work was provided by the Templeton
Religion Trust and by Martin Seligman of the Univer-
sity of Pennsylvania’s Positive Psychology Center.
</bodyText>
<sectionHeader confidence="0.81154" genericHeader="references">
References
</sectionHeader>
<bodyText confidence="0.916123083333333">
Faiyaz Al Zamal, Wendy Liu, and Derek Ruths. 2012.
Homophily and latent attribute inference: Inferring
latent attributes of twitter users from neighbors. In
ICWSM.
Shlomo Argamon, Moshe Koppel, James W Pen-
nebaker, and Jonathan Schler. 2009. Automatically
profiling the author of an anonymous text. Commu-
nications of the ACM, 52(2):119–123.
John D Burger and John C Henderson. 2006. An ex-
ploration of observable features related to blogger
age. In AAAI Spring Symposium: Computational
Approaches to Analyzing Weblogs, pages 15–20.
</bodyText>
<reference confidence="0.90791615625">
John D Burger, John C Henderson, George Kim, and
Guido Zarrella. 2011. Discriminating gender on
twitter. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing, pages
1301–1309. Association for Computational Linguis-
tics.
2014. Faststats: How healthy are we. http://www.
cdc.gov/nchs/fastats/healthy.htm.
Accessed on March 12, 2014.
Morgane Ciot, Morgan Sonderegger, and Derek Ruths.
2013. Gender inference of twitter users in non-
english contexts. In Proceedings of the 2013 Con-
ference on Empirical Methods in Natural Language
Processing, Seattle, Wash, pages 18–21.
Munmun De Choudhury, Scott Counts, and Eric
Horvitz. 2013. Predicting postpartum changes in
emotion and behavior via social media. In Pro-
ceedings of the 2013 ACM annual conference on
Human factors in computing systems, pages 3267–
3276. ACM.
Peter Sheridan Dodds, Kameron Decker Harris, Is-
abel M Kloumann, Catherine A Bliss, and Christo-
pher M Danforth. 2011. Temporal patterns of hap-
piness and information in a global social network:
Hedonometrics and twitter. PloS one, 6(12):e26752.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A
library for large linear classification. The Journal of
Machine Learning Research, 9:1871–1874.
Inc. Google. 2014. Google flu trends. http://
www.google.org/flutrends. Accessed on
March 12, 2014.
</reference>
<page confidence="0.985608">
1149
</page>
<reference confidence="0.933792078431372">
Sumit Goswami, Sudeshna Sarkar, and Mayur Rustagi.
2009. Stylometric analysis of bloggers age and gen-
der. In Third International AAAI Conference on We-
blogs and Social Media.
Arthur E Hoerl and Robert W Kennard. 1970. Ridge
regression: Biased estimation for nonorthogonal
problems. Technometrics, 12(1):55–67.
David A Huffaker and Sandra L Calvert. 2005.
Gender, identity, and language use in teenage
blogs. Journal of Computer-Mediated Communica-
tion, 10(2):00–00.
Rosie Jones, Ravi Kumar, Bo Pang, and Andrew
Tomkins. 2007. I know what you did last summer:
query logs and user privacy. In Proceedings of the
sixteenth ACM conference on Conference on infor-
mation and knowledge management, pages 909–914.
ACM.
Margaret L Kern, Johannes C Eichstaedt, H Andrew
Schwartz, Gregory Park, Lyle H Ungar, David J
Stillwell, Michal Kosinski, Lukasz Dziurzynski, and
Martin EP Seligman. 2014. From sooo excited!!!
to so proud: Using language to study development.
Developmental psychology, 50(1):178–188.
Michal Kosinski and David J Still-
well. 2012. mypersonality project.
http://www.mypersonality.org/wiki/.
Michal Kosinski, David J Stillwell, and Thore Graepel.
2013. Private traits and attributes are predictable
from digital records of human behavior. volume
110, pages 5802–5805. National Acad Sciences.
David Lazer, Alex Pentland, Lada Adamic, Sinan Aral,
Albert-Laszlo Barabasi, Devon Brewer, Nicholas
Christakis, Noshir Contractor, James Fowler, Myron
Gutmann, Tony Jebara, Gary King, Michael Macy,
Deb Roy, and Marshall Van Alstyne. 2009. Com-
putational social science. Science, 323(5915):721–
723.
Wendy Liu and Derek Ruths. 2013. Whats in a name?
using first names as features for gender inference in
twitter. In Analyzing Microtext: 2013 AAAI Spring
Symposium.
Alessandra Marengoni, Sara Angleman, Ren´e Melis,
Francesca Mangialasche, Anita Karp, Annika Gar-
men, Bettina Meinow, and Laura Fratiglioni. 2011.
Aging with multimorbidity: a systematic review of
the literature. Ageing research reviews, 10(4):430–
439.
Robert R McCrae, Paul T Costa, Margarida Pedroso
de Lima, Ant´onio Sim˜oes, Fritz Ostendorf, Alois
Angleitner, Iris Maruˇsi´c, Denis Bratko, Gian Vitto-
rio Caprara, Claudio Barbaranelli, et al. 1999. Age
</reference>
<bodyText confidence="0.8923395">
differences in personality across the adult life span:
parallels in five cultures. Developmental Psychol-
ogy, 35(2):466–477.
Alan Mislove, Sune Lehmann, Yong-Yeol Ahn, Jukka-
Pekka Onnela, and J Niels Rosenquist. 2011.
Understanding the demographics of twitter users.
ICWSM, 11:5th.
Saif M Mohammad, Svetlana Kiritchenko, and Xiao-
dan Zhu. 2013. Nrc-canada: Building the state-
of-the-art in sentiment analysis of tweets. arXiv
preprint arXiv:1308.6242.
Dong Nguyen, Rilana Gravel, Dolf Trieschnigg, and
Theo Meder. 2013. how old do you think i am?:
A study of language and age in twitter. In Proceed-
ings of the Seventh International AAAI Conference
on Weblogs and Social Media.
</bodyText>
<reference confidence="0.981988863636364">
Michael J Paul and Mark Dredze. 2011. You are what
you tweet: Analyzing twitter for public health. In
ICWSM.
Marco Pennacchiotti and Ana-Maria Popescu. 2011.
A machine learning approach to twitter user classifi-
cation. In ICWSM.
James W Pennebaker and Lori D Stone. 2003. Words
of wisdom: language use over the life span. Journal
of Personality and Social Psychology, 85(2):291–
301.
James W Pennebaker, Martha E Francis, and Roger J
Booth. 2001. Linguistic inquiry and word count:
Liwc 2001. 71:2001.
2014. Social networking fact sheet. http:
//www.pewinternet.org/fact-sheets/
social-networking-fact-sheet/. Ac-
cessed on August 26, 2014.
Francisco Rangel and Paolo Rosso. 2013. Use of lan-
guage and author profiling: Identification of gender
and age. Natural Language Processing and Cogni-
tive Science, page 177.
Delip Rao, David Yarowsky, Abhishek Shreevats, and
Manaswi Gupta. 2010. Classifying latent user at-
tributes in twitter. In Proceedings of the 2nd in-
ternational workshop on Search and mining user-
generated contents, pages 37–44. ACM.
Jonathan Schler, Moshe Koppel, Shlomo Argamon,
and James W Pennebaker. 2006. Effects of age
and gender on blogging. In AAAI Spring Sympo-
sium: Computational Approaches to Analyzing We-
blogs, volume 6, pages 199–205.
H Andrew Schwartz, Johannes C Eichstaedt, Mar-
garet L Kern, Lukasz Dziurzynski, Megha Agrawal,
Gregory J Park, Shrinidhi K Lakshmikanth, Sneha
Jha, Martin EP Seligman, Lyle Ungar, et al. 2013a.
Characterizing geographic variation in well-being
using tweets. In ICWSM.
H Andrew Schwartz, Johannes C Eichstaedt, Mar-
garet L Kern, Lukasz Dziurzynski, Stephanie M Ra-
mones, Megha Agrawal, Achal Shah, Michal Kosin-
ski, David J Stillwell, Martin EP Seligman, et al.
2013b. Personality, gender, and age in the language
of social media: The open-vocabulary approach.
PloS one, 8(9):e73791.
</reference>
<page confidence="0.809001">
1150
</page>
<reference confidence="0.9995919375">
Rong Su, James Rounds, and Patrick Ian Armstrong.
2009. Men and things, women and people: a meta-
analysis of sex differences in interests. Psychologi-
cal Bulletin, 135(6):859–884.
Maite Taboada, Julian Brooke, Milan Tofiloski, Kim-
berly Voll, and Manfred Stede. 2011. Lexicon-
based methods for sentiment analysis. Computa-
tional linguistics, 37(2):267–307.
Yla R Tausczik and James W Pennebaker. 2010. The
psychological meaning of words: Liwc and comput-
erized text analysis methods. Journal of language
and social psychology, 29(1):24–54.
Robert Tibshirani. 1996. Regression shrinkage and se-
lection via the lasso. Journal of the Royal Statistical
Society. Series B (Methodological), pages 267–288.
Benjamin Van Durme. 2012. Streaming analysis of
discourse participants. In Proceedings of the 2012
Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Nat-
ural Language Learning, pages 48–58. Association
for Computational Linguistics.
Svitlana Volkova, Theresa Wilson, and David
Yarowsky. 2013. Exploring demographic lan-
guage variations to improve multilingual sentiment
analysis in social media. In Proceedings of the
2013 Conference on Empirical Methods on Natural
Language Processing.
Theresa Wilson, Zornitsa Kozareva, Preslav Nakov,
Sara Rosenthal, Veselin Stoyanov, and Alan Ritter.
2013. Semeval-2013 task 2: Sentiment analysis in
twitter. In Proceedings of the International Work-
shop on Semantic Evaluation, SemEval, volume 13.
</reference>
<page confidence="0.994543">
1151
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.945003">
<title confidence="0.998891">Developing Age and Gender Predictive Lexica over Social Media</title>
<author confidence="0.9911005">C L Michal Lyle H</author>
<author confidence="0.9911005">Andrew</author>
<affiliation confidence="0.983402333333333">of Psychology, University of &amp; Information Science, University of Centre, University of</affiliation>
<email confidence="0.998097">maarten@sas.upenn.edu</email>
<abstract confidence="0.999402714285714">Demographic lexica have potential for widespread use in social science, economic, and business applications. We derive predictive lexica (words and weights) for age and gender using regression and classification models from word usage in Facebook, blog, and Twitter data with associated demographic The lexica, made publicly achieved state-of-the-art accuracy in language based age and gender prediction over Facebook and Twitter, and were evaluated for generalization across social media genres as well as in limited message situations.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>John D Burger</author>
<author>John C Henderson</author>
<author>George Kim</author>
<author>Guido Zarrella</author>
</authors>
<title>Discriminating gender on twitter.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1301--1309</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4345" citStr="Burger et al., 2011" startWordPosition="650" endWordPosition="653">et al., 2011; Tausczik and Pennebaker, 2010). 2 Related Work Online behavior is representative of many aspects of a user’s demographics (Pennacchiotti and Popescu, 2011; Rao et al., 2010). Many studies have used linguistic cues (such as ngrams) to determine if someone belongs to a certain age group, be it on Twitter or another social media platform (Al Zamal et al., 2012; Argamon et al., 2009; Nguyen et al., 2013; Rangel and Rosso, 2013). Gender prediction has been studied across blogs (Burger and Henderson, 2006; Goswami et al., 2009), Yahoo! search queries (Jones et al., 2007), and Twitter (Burger et al., 2011; Nguyen et al., 2013; Liu and Ruths, 2013; Rao et al., 2010). Because Twitter does not make gender or age available, such work infers gender and age by leveraging profile information, such as gender-discriminating names or crawling for links to publicly available data (e.g. Burger et al., 2The LIWC lexicon, derived manually based on psychological theory, (Pennebaker et al., 2001) had 1136 citations in 2013 alone. 1146 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1146–1151, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computa</context>
<context position="14094" citStr="Burger et al., 2011" startWordPosition="2255" endWordPosition="2258">. As baselines, we use the mean for age (23.0 years old) and the most frequent class (female) for gender. We see that for both age and gender, accuracies are substantially higher than the baseline. These accuracies were just below with no significant difference previous state-of-the-art results (Schwartz et al., 2013; r = 0.84 for age and 91.9% accuracy for gender). 7 Because of the nature of our datasets (the Facebook data is private) and task (user-level predictions), comparable previous studies are nearly nonexistent. Nonetheless, the Twitter data was a random subset of users based on the (Burger et al., 2011) dataset excluding non-English tweets, making it somewhat comparable. In this case, the lexica outperformed previous results for gender prediction of Twitter users, which ranged from 75.5% to 87% (Burger et al., 2011; Ciot et al., 2013; Liu and Ruths, 2013; Al Zamal et al., 2012). However, the lexica were unable to match the 92.0% accuracy Burger et al. (2011) achieved when using profile information in addition to text. No other similar studies — to the best of our knowledge — have been conducted. Application in other settings. While Facebook is the ideal setting to apply our lexica, we hope t</context>
</contexts>
<marker>Burger, Henderson, Kim, Zarrella, 2011</marker>
<rawString>John D Burger, John C Henderson, George Kim, and Guido Zarrella. 2011. Discriminating gender on twitter. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1301–1309. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<title>Faststats: How healthy are we.</title>
<date>2014</date>
<journal>http://www. cdc.gov/nchs/fastats/healthy.htm. Accessed on March</journal>
<volume>12</volume>
<marker>2014</marker>
<rawString>2014. Faststats: How healthy are we. http://www. cdc.gov/nchs/fastats/healthy.htm. Accessed on March 12, 2014.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Morgane Ciot</author>
<author>Morgan Sonderegger</author>
<author>Derek Ruths</author>
</authors>
<title>Gender inference of twitter users in nonenglish contexts.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>18--21</pages>
<location>Seattle, Wash,</location>
<contexts>
<context position="14329" citStr="Ciot et al., 2013" startWordPosition="2293" endWordPosition="2296"> significant difference previous state-of-the-art results (Schwartz et al., 2013; r = 0.84 for age and 91.9% accuracy for gender). 7 Because of the nature of our datasets (the Facebook data is private) and task (user-level predictions), comparable previous studies are nearly nonexistent. Nonetheless, the Twitter data was a random subset of users based on the (Burger et al., 2011) dataset excluding non-English tweets, making it somewhat comparable. In this case, the lexica outperformed previous results for gender prediction of Twitter users, which ranged from 75.5% to 87% (Burger et al., 2011; Ciot et al., 2013; Liu and Ruths, 2013; Al Zamal et al., 2012). However, the lexica were unable to match the 92.0% accuracy Burger et al. (2011) achieved when using profile information in addition to text. No other similar studies — to the best of our knowledge — have been conducted. Application in other settings. While Facebook is the ideal setting to apply our lexica, we hope that they generalize to other situations. To evaluate their utility in other settings, we first tested them over a gender and age stratified Facebook sample. Our random sample, like all of Facebook, is biased toward the young; this stra</context>
</contexts>
<marker>Ciot, Sonderegger, Ruths, 2013</marker>
<rawString>Morgane Ciot, Morgan Sonderegger, and Derek Ruths. 2013. Gender inference of twitter users in nonenglish contexts. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, Seattle, Wash, pages 18–21.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Munmun De Choudhury</author>
<author>Scott Counts</author>
<author>Eric Horvitz</author>
</authors>
<title>Predicting postpartum changes in emotion and behavior via social media.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 ACM annual conference on Human factors in computing systems,</booktitle>
<pages>3267--3276</pages>
<publisher>ACM.</publisher>
<marker>De Choudhury, Counts, Horvitz, 2013</marker>
<rawString>Munmun De Choudhury, Scott Counts, and Eric Horvitz. 2013. Predicting postpartum changes in emotion and behavior via social media. In Proceedings of the 2013 ACM annual conference on Human factors in computing systems, pages 3267– 3276. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Sheridan Dodds</author>
<author>Kameron Decker Harris</author>
<author>Isabel M Kloumann</author>
<author>Catherine A Bliss</author>
<author>Christopher M Danforth</author>
</authors>
<title>Temporal patterns of happiness and information in a global social network: Hedonometrics and twitter.</title>
<date>2011</date>
<journal>PloS one,</journal>
<pages>6--12</pages>
<contexts>
<context position="1346" citStr="Dodds et al., 2011" startWordPosition="186" endWordPosition="189"> available,1 achieved state-of-the-art accuracy in language based age and gender prediction over Facebook and Twitter, and were evaluated for generalization across social media genres as well as in limited message situations. 1 Introduction Use of social media has enabled the study of psychological and social questions at an unprecedented scale (Lazer et al., 2009). This allows more data-driven discovery alongside the typical hypothesis-testing social science process (Schwartz et al., 2013b). Social media may track disease rates (Paul and Dredze, 2011; Google, 2014), psychological well-being (Dodds et al., 2011; De Choudhury et al., 2013; Schwartz et al., 2013a), and a host of other behavioral, psychological and medical phenomena (Kosinski et al., 2013). Unlike traditional hypothesis-driven social science, such large-scale social media studies rarely take into account—or have access to—age and gender information, which can have a major impact on many questions. For example, females live almost five years longer than males (cdc, 2014; Marengoni et al., 2011). Men and women, on average, differ markedly in their interests and work preferences (Su et al., 2009). With age, personalities gradually change,</context>
<context position="3738" citStr="Dodds et al., 2011" startWordPosition="549" endWordPosition="552">rived from a penalized linear regression (for continuous valued age) and support vector classification (for binary-valued gender). In this modality, the lexica are simply a transparent and portable means for distributing predictive models based on words. We test generalization and adapt the lexica to blogs and Twitter, plus consider situations when limited messages are available. In addition to use in the computational linguistics community, we believe the lexicon format will make it easier for social scientists to leverage data-driven models where manually created lexica currently dominate2 (Dodds et al., 2011; Tausczik and Pennebaker, 2010). 2 Related Work Online behavior is representative of many aspects of a user’s demographics (Pennacchiotti and Popescu, 2011; Rao et al., 2010). Many studies have used linguistic cues (such as ngrams) to determine if someone belongs to a certain age group, be it on Twitter or another social media platform (Al Zamal et al., 2012; Argamon et al., 2009; Nguyen et al., 2013; Rangel and Rosso, 2013). Gender prediction has been studied across blogs (Burger and Henderson, 2006; Goswami et al., 2009), Yahoo! search queries (Jones et al., 2007), and Twitter (Burger et al</context>
</contexts>
<marker>Dodds, Harris, Kloumann, Bliss, Danforth, 2011</marker>
<rawString>Peter Sheridan Dodds, Kameron Decker Harris, Isabel M Kloumann, Catherine A Bliss, and Christopher M Danforth. 2011. Temporal patterns of happiness and information in a global social network: Hedonometrics and twitter. PloS one, 6(12):e26752.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rong-En Fan</author>
<author>Kai-Wei Chang</author>
<author>Cho-Jui Hsieh</author>
<author>XiangRui Wang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>Liblinear: A library for large linear classification.</title>
<date>2008</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>9--1871</pages>
<contexts>
<context position="11559" citStr="Fan et al., 2008" startWordPosition="1842" endWordPosition="1845">s mean age of training sample; for gender, it is the most frequent class (female). Lexica tested include those derived from Facebook (FBlex), blogs (BGlex), and Twitter (Tlex). We evaluate over a random Facebook sample (randFB), a stratified Facebook sample (stratFB), a random blogger sample (randBG), and a random twitter sample (randT). All results were a significant (p &lt; 0.001) improvement over the baseline. In practice, we learn our 1gram coefficients (i.e. lexicon weights) from ridge regression (Hoerl and Kennard, 1970) for age (continuous variable) and from support vector classification (Fan et al., 2008) for gender (binary variable). Ridge regression uses an L2 (α||β||2) penalization to avoid overfitting (Hoerl and Kennard, 1970). Although some words no doubt have a nonlinear relationship with age (e.g., ‘fiance’ peaks in the 20s), we still find high accuracy from a linear model (see Table 1) and it allows for a distribution of the model in the accessible form of a lexicon. For gender prediction, we use an SVM with a linear kernel with L1 penalization (α||β||1) (Tibshirani, 1996). Because the L1 penalization zeros-out many coefficients, it has the added advantage of effectively reducing the s</context>
</contexts>
<marker>Fan, Chang, Hsieh, Wang, Lin, 2008</marker>
<rawString>Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, XiangRui Wang, and Chih-Jen Lin. 2008. Liblinear: A library for large linear classification. The Journal of Machine Learning Research, 9:1871–1874.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Google</author>
</authors>
<title>Google flu trends. http:// www.google.org/flutrends.</title>
<date>2014</date>
<journal>Accessed on March</journal>
<volume>12</volume>
<contexts>
<context position="1300" citStr="Google, 2014" startWordPosition="182" endWordPosition="183">graphic labels. The lexica, made publicly available,1 achieved state-of-the-art accuracy in language based age and gender prediction over Facebook and Twitter, and were evaluated for generalization across social media genres as well as in limited message situations. 1 Introduction Use of social media has enabled the study of psychological and social questions at an unprecedented scale (Lazer et al., 2009). This allows more data-driven discovery alongside the typical hypothesis-testing social science process (Schwartz et al., 2013b). Social media may track disease rates (Paul and Dredze, 2011; Google, 2014), psychological well-being (Dodds et al., 2011; De Choudhury et al., 2013; Schwartz et al., 2013a), and a host of other behavioral, psychological and medical phenomena (Kosinski et al., 2013). Unlike traditional hypothesis-driven social science, such large-scale social media studies rarely take into account—or have access to—age and gender information, which can have a major impact on many questions. For example, females live almost five years longer than males (cdc, 2014; Marengoni et al., 2011). Men and women, on average, differ markedly in their interests and work preferences (Su et al., 20</context>
</contexts>
<marker>Google, 2014</marker>
<rawString>Inc. Google. 2014. Google flu trends. http:// www.google.org/flutrends. Accessed on March 12, 2014.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sumit Goswami</author>
<author>Sudeshna Sarkar</author>
<author>Mayur Rustagi</author>
</authors>
<title>Stylometric analysis of bloggers age and gender.</title>
<date>2009</date>
<booktitle>In Third International AAAI Conference on Weblogs and Social Media.</booktitle>
<contexts>
<context position="4267" citStr="Goswami et al., 2009" startWordPosition="637" endWordPosition="640">age data-driven models where manually created lexica currently dominate2 (Dodds et al., 2011; Tausczik and Pennebaker, 2010). 2 Related Work Online behavior is representative of many aspects of a user’s demographics (Pennacchiotti and Popescu, 2011; Rao et al., 2010). Many studies have used linguistic cues (such as ngrams) to determine if someone belongs to a certain age group, be it on Twitter or another social media platform (Al Zamal et al., 2012; Argamon et al., 2009; Nguyen et al., 2013; Rangel and Rosso, 2013). Gender prediction has been studied across blogs (Burger and Henderson, 2006; Goswami et al., 2009), Yahoo! search queries (Jones et al., 2007), and Twitter (Burger et al., 2011; Nguyen et al., 2013; Liu and Ruths, 2013; Rao et al., 2010). Because Twitter does not make gender or age available, such work infers gender and age by leveraging profile information, such as gender-discriminating names or crawling for links to publicly available data (e.g. Burger et al., 2The LIWC lexicon, derived manually based on psychological theory, (Pennebaker et al., 2001) had 1136 citations in 2013 alone. 1146 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pag</context>
<context position="8200" citStr="Goswami et al., 2009" startWordPosition="1270" endWordPosition="1273"> lexica are often used as features in a supervised model, essentially dimensionally reducing a large set of words into positive and negative sentiment, while our lexica represent the predictive model itself. stratified Facebook data, blogs, and tweets. The stratified Facebook data (exclusively used for testing) consists of equal proportions of 1,520 males and females across 12 4-year age bins starting at 13 and ending at 60.4 This roughly matchs the size of the main test set. Seeking out-of-domain data, we downloaded age and gender annotated blogs from 2004 (Schler et al., 2006) (also used in Goswami et al., 2009) and gender labeled tweets (Volkova et al., 2013). Limiting the sample to users who wrote at least 1000 words, the total number of bloggers is 15,006, of which 50.6% are female and only 15% are over 27 (reflecting the younger population standard in social media). From this we use a randomly selected 1,000 bloggers as a blogger test set and the remaining 14,006 bloggers for training. Similarly for the Twitter dataset, we use 11,000 random gender-only annotated users, in which 51.9% are female. We again randomly select 1,000 users as a test set for gender prediction and use the remaining 10,000 </context>
</contexts>
<marker>Goswami, Sarkar, Rustagi, 2009</marker>
<rawString>Sumit Goswami, Sudeshna Sarkar, and Mayur Rustagi. 2009. Stylometric analysis of bloggers age and gender. In Third International AAAI Conference on Weblogs and Social Media.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arthur E Hoerl</author>
<author>Robert W Kennard</author>
</authors>
<title>Ridge regression: Biased estimation for nonorthogonal problems.</title>
<date>1970</date>
<journal>Technometrics,</journal>
<volume>12</volume>
<issue>1</issue>
<contexts>
<context position="11471" citStr="Hoerl and Kennard, 1970" startWordPosition="1827" endWordPosition="1831">coefficient(r); mean absolute error (mae) in years) and gender (accuracy %). Baseline for age is mean age of training sample; for gender, it is the most frequent class (female). Lexica tested include those derived from Facebook (FBlex), blogs (BGlex), and Twitter (Tlex). We evaluate over a random Facebook sample (randFB), a stratified Facebook sample (stratFB), a random blogger sample (randBG), and a random twitter sample (randT). All results were a significant (p &lt; 0.001) improvement over the baseline. In practice, we learn our 1gram coefficients (i.e. lexicon weights) from ridge regression (Hoerl and Kennard, 1970) for age (continuous variable) and from support vector classification (Fan et al., 2008) for gender (binary variable). Ridge regression uses an L2 (α||β||2) penalization to avoid overfitting (Hoerl and Kennard, 1970). Although some words no doubt have a nonlinear relationship with age (e.g., ‘fiance’ peaks in the 20s), we still find high accuracy from a linear model (see Table 1) and it allows for a distribution of the model in the accessible form of a lexicon. For gender prediction, we use an SVM with a linear kernel with L1 penalization (α||β||1) (Tibshirani, 1996). Because the L1 penalizati</context>
</contexts>
<marker>Hoerl, Kennard, 1970</marker>
<rawString>Arthur E Hoerl and Robert W Kennard. 1970. Ridge regression: Biased estimation for nonorthogonal problems. Technometrics, 12(1):55–67.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David A Huffaker</author>
<author>Sandra L Calvert</author>
</authors>
<title>Gender, identity, and language use in teenage blogs.</title>
<date>2005</date>
<journal>Journal of Computer-Mediated Communication,</journal>
<volume>10</volume>
<issue>2</issue>
<contexts>
<context position="2188" citStr="Huffaker and Calvert, 2005" startWordPosition="316" endWordPosition="319">e social media studies rarely take into account—or have access to—age and gender information, which can have a major impact on many questions. For example, females live almost five years longer than males (cdc, 2014; Marengoni et al., 2011). Men and women, on average, differ markedly in their interests and work preferences (Su et al., 2009). With age, personalities gradually change, typically becoming less open to experiences but more agreeable and conscientious (McCrae et al., 1999). Additionally, social media language varies by age (Kern et al., 2014; Pennebaker and Stone, 2003) and gender (Huffaker and Calvert, 2005). Twitter may have a male bias (Mislove et al., 2011), while social media in general skew towards being young and female (pew, 2014). Accessible tools to predict demographic variables can substantially enhance social media’s utility for so1download at http://www.wwbp.org/data.html cial science, economic, and business applications. For example, one can post-stratify population-level results to reflect a representative sample, understand variation across age and gender groups, or produce personalized marketing, services, and sentiment recommendations; a movie may be generally disliked, except by</context>
</contexts>
<marker>Huffaker, Calvert, 2005</marker>
<rawString>David A Huffaker and Sandra L Calvert. 2005. Gender, identity, and language use in teenage blogs. Journal of Computer-Mediated Communication, 10(2):00–00.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rosie Jones</author>
<author>Ravi Kumar</author>
<author>Bo Pang</author>
<author>Andrew Tomkins</author>
</authors>
<title>I know what you did last summer: query logs and user privacy.</title>
<date>2007</date>
<booktitle>In Proceedings of the sixteenth ACM conference on Conference on information and knowledge management,</booktitle>
<pages>909--914</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="4311" citStr="Jones et al., 2007" startWordPosition="644" endWordPosition="647">lexica currently dominate2 (Dodds et al., 2011; Tausczik and Pennebaker, 2010). 2 Related Work Online behavior is representative of many aspects of a user’s demographics (Pennacchiotti and Popescu, 2011; Rao et al., 2010). Many studies have used linguistic cues (such as ngrams) to determine if someone belongs to a certain age group, be it on Twitter or another social media platform (Al Zamal et al., 2012; Argamon et al., 2009; Nguyen et al., 2013; Rangel and Rosso, 2013). Gender prediction has been studied across blogs (Burger and Henderson, 2006; Goswami et al., 2009), Yahoo! search queries (Jones et al., 2007), and Twitter (Burger et al., 2011; Nguyen et al., 2013; Liu and Ruths, 2013; Rao et al., 2010). Because Twitter does not make gender or age available, such work infers gender and age by leveraging profile information, such as gender-discriminating names or crawling for links to publicly available data (e.g. Burger et al., 2The LIWC lexicon, derived manually based on psychological theory, (Pennebaker et al., 2001) had 1136 citations in 2013 alone. 1146 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1146–1151, October 25-29, 2014, Doha, Qat</context>
</contexts>
<marker>Jones, Kumar, Pang, Tomkins, 2007</marker>
<rawString>Rosie Jones, Ravi Kumar, Bo Pang, and Andrew Tomkins. 2007. I know what you did last summer: query logs and user privacy. In Proceedings of the sixteenth ACM conference on Conference on information and knowledge management, pages 909–914. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Margaret L Kern</author>
<author>Johannes C Eichstaedt</author>
<author>H Andrew Schwartz</author>
<author>Gregory Park</author>
<author>Lyle H Ungar</author>
<author>David J Stillwell</author>
<author>Michal Kosinski</author>
<author>Lukasz Dziurzynski</author>
<author>Martin EP Seligman</author>
</authors>
<title>From sooo excited!!! to so proud: Using language to study development.</title>
<date>2014</date>
<booktitle>Developmental psychology,</booktitle>
<pages>50--1</pages>
<contexts>
<context position="2119" citStr="Kern et al., 2014" startWordPosition="306" endWordPosition="309">aditional hypothesis-driven social science, such large-scale social media studies rarely take into account—or have access to—age and gender information, which can have a major impact on many questions. For example, females live almost five years longer than males (cdc, 2014; Marengoni et al., 2011). Men and women, on average, differ markedly in their interests and work preferences (Su et al., 2009). With age, personalities gradually change, typically becoming less open to experiences but more agreeable and conscientious (McCrae et al., 1999). Additionally, social media language varies by age (Kern et al., 2014; Pennebaker and Stone, 2003) and gender (Huffaker and Calvert, 2005). Twitter may have a male bias (Mislove et al., 2011), while social media in general skew towards being young and female (pew, 2014). Accessible tools to predict demographic variables can substantially enhance social media’s utility for so1download at http://www.wwbp.org/data.html cial science, economic, and business applications. For example, one can post-stratify population-level results to reflect a representative sample, understand variation across age and gender groups, or produce personalized marketing, services, and se</context>
</contexts>
<marker>Kern, Eichstaedt, Schwartz, Park, Ungar, Stillwell, Kosinski, Dziurzynski, Seligman, 2014</marker>
<rawString>Margaret L Kern, Johannes C Eichstaedt, H Andrew Schwartz, Gregory Park, Lyle H Ungar, David J Stillwell, Michal Kosinski, Lukasz Dziurzynski, and Martin EP Seligman. 2014. From sooo excited!!! to so proud: Using language to study development. Developmental psychology, 50(1):178–188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michal Kosinski</author>
<author>David J Stillwell</author>
</authors>
<date>2012</date>
<note>mypersonality project. http://www.mypersonality.org/wiki/.</note>
<contexts>
<context position="6728" citStr="Kosinski and Stillwell, 2012" startWordPosition="1028" endWordPosition="1031">ask (Wilson et al., 2013). Approaches like point-wise mutual information take a univariate view on words–i.e. the weight given to one feature (word) is not affected by other features. Since language is highly collinear, we take a multivariate lexicon development approach, which takes covariance into account (e.g. someone who mentions ‘hair’ often is more likely to mention ‘brushing’, ‘style’, and ‘cut’; weighting these words in isolation might “double-count” some information). 3 Method Primary data. Our primary dataset consists of Facebook messages from users of the MyPersonality application (Kosinski and Stillwell, 2012). Messages were posted between January 2009 and October 2011. We restrict our analysis to those Facebook users meeting certain criteria: they must indicate English as a primary language, have written at least 1,000 words in their status updates, be younger than 65 years old (data beyond this age becomes very sparse), and indicate their gender and age. This resulted in a dataset of N = 75,394 users, who wrote over 300 million words collectively. We split our sample into training and test sets. Our primary test set consists of a 1,000 randomly selected Facebook users, while the training set that</context>
</contexts>
<marker>Kosinski, Stillwell, 2012</marker>
<rawString>Michal Kosinski and David J Stillwell. 2012. mypersonality project. http://www.mypersonality.org/wiki/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michal Kosinski</author>
<author>David J Stillwell</author>
<author>Thore Graepel</author>
</authors>
<title>Private traits and attributes are predictable from digital records of human behavior.</title>
<date>2013</date>
<volume>110</volume>
<pages>5802--5805</pages>
<institution>National Acad Sciences.</institution>
<contexts>
<context position="1491" citStr="Kosinski et al., 2013" startWordPosition="209" endWordPosition="212"> generalization across social media genres as well as in limited message situations. 1 Introduction Use of social media has enabled the study of psychological and social questions at an unprecedented scale (Lazer et al., 2009). This allows more data-driven discovery alongside the typical hypothesis-testing social science process (Schwartz et al., 2013b). Social media may track disease rates (Paul and Dredze, 2011; Google, 2014), psychological well-being (Dodds et al., 2011; De Choudhury et al., 2013; Schwartz et al., 2013a), and a host of other behavioral, psychological and medical phenomena (Kosinski et al., 2013). Unlike traditional hypothesis-driven social science, such large-scale social media studies rarely take into account—or have access to—age and gender information, which can have a major impact on many questions. For example, females live almost five years longer than males (cdc, 2014; Marengoni et al., 2011). Men and women, on average, differ markedly in their interests and work preferences (Su et al., 2009). With age, personalities gradually change, typically becoming less open to experiences but more agreeable and conscientious (McCrae et al., 1999). Additionally, social media language vari</context>
</contexts>
<marker>Kosinski, Stillwell, Graepel, 2013</marker>
<rawString>Michal Kosinski, David J Stillwell, and Thore Graepel. 2013. Private traits and attributes are predictable from digital records of human behavior. volume 110, pages 5802–5805. National Acad Sciences.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Lazer</author>
<author>Alex Pentland</author>
</authors>
<title>Lada Adamic, Sinan Aral, Albert-Laszlo Barabasi, Devon Brewer, Nicholas Christakis, Noshir Contractor,</title>
<date>2009</date>
<journal>Computational social science. Science,</journal>
<volume>323</volume>
<issue>5915</issue>
<pages>723</pages>
<location>James Fowler, Myron Gutmann, Tony Jebara, Gary</location>
<marker>Lazer, Pentland, 2009</marker>
<rawString>David Lazer, Alex Pentland, Lada Adamic, Sinan Aral, Albert-Laszlo Barabasi, Devon Brewer, Nicholas Christakis, Noshir Contractor, James Fowler, Myron Gutmann, Tony Jebara, Gary King, Michael Macy, Deb Roy, and Marshall Van Alstyne. 2009. Computational social science. Science, 323(5915):721– 723.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wendy Liu</author>
<author>Derek Ruths</author>
</authors>
<title>Whats in a name? using first names as features for gender inference in twitter. In Analyzing Microtext:</title>
<date>2013</date>
<publisher>AAAI Spring Symposium.</publisher>
<contexts>
<context position="4387" citStr="Liu and Ruths, 2013" startWordPosition="658" endWordPosition="661">0). 2 Related Work Online behavior is representative of many aspects of a user’s demographics (Pennacchiotti and Popescu, 2011; Rao et al., 2010). Many studies have used linguistic cues (such as ngrams) to determine if someone belongs to a certain age group, be it on Twitter or another social media platform (Al Zamal et al., 2012; Argamon et al., 2009; Nguyen et al., 2013; Rangel and Rosso, 2013). Gender prediction has been studied across blogs (Burger and Henderson, 2006; Goswami et al., 2009), Yahoo! search queries (Jones et al., 2007), and Twitter (Burger et al., 2011; Nguyen et al., 2013; Liu and Ruths, 2013; Rao et al., 2010). Because Twitter does not make gender or age available, such work infers gender and age by leveraging profile information, such as gender-discriminating names or crawling for links to publicly available data (e.g. Burger et al., 2The LIWC lexicon, derived manually based on psychological theory, (Pennebaker et al., 2001) had 1136 citations in 2013 alone. 1146 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1146–1151, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics 2011). While many studi</context>
<context position="14350" citStr="Liu and Ruths, 2013" startWordPosition="2297" endWordPosition="2300">ence previous state-of-the-art results (Schwartz et al., 2013; r = 0.84 for age and 91.9% accuracy for gender). 7 Because of the nature of our datasets (the Facebook data is private) and task (user-level predictions), comparable previous studies are nearly nonexistent. Nonetheless, the Twitter data was a random subset of users based on the (Burger et al., 2011) dataset excluding non-English tweets, making it somewhat comparable. In this case, the lexica outperformed previous results for gender prediction of Twitter users, which ranged from 75.5% to 87% (Burger et al., 2011; Ciot et al., 2013; Liu and Ruths, 2013; Al Zamal et al., 2012). However, the lexica were unable to match the 92.0% accuracy Burger et al. (2011) achieved when using profile information in addition to text. No other similar studies — to the best of our knowledge — have been conducted. Application in other settings. While Facebook is the ideal setting to apply our lexica, we hope that they generalize to other situations. To evaluate their utility in other settings, we first tested them over a gender and age stratified Facebook sample. Our random sample, like all of Facebook, is biased toward the young; this stratified test sample co</context>
</contexts>
<marker>Liu, Ruths, 2013</marker>
<rawString>Wendy Liu and Derek Ruths. 2013. Whats in a name? using first names as features for gender inference in twitter. In Analyzing Microtext: 2013 AAAI Spring Symposium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandra Marengoni</author>
<author>Sara Angleman</author>
<author>Ren´e Melis</author>
<author>Francesca Mangialasche</author>
<author>Anita Karp</author>
<author>Annika Garmen</author>
<author>Bettina Meinow</author>
<author>Laura Fratiglioni</author>
</authors>
<title>Aging with multimorbidity: a systematic review of the literature. Ageing research reviews,</title>
<date>2011</date>
<volume>10</volume>
<issue>4</issue>
<pages>439</pages>
<contexts>
<context position="1801" citStr="Marengoni et al., 2011" startWordPosition="256" endWordPosition="259">ocial science process (Schwartz et al., 2013b). Social media may track disease rates (Paul and Dredze, 2011; Google, 2014), psychological well-being (Dodds et al., 2011; De Choudhury et al., 2013; Schwartz et al., 2013a), and a host of other behavioral, psychological and medical phenomena (Kosinski et al., 2013). Unlike traditional hypothesis-driven social science, such large-scale social media studies rarely take into account—or have access to—age and gender information, which can have a major impact on many questions. For example, females live almost five years longer than males (cdc, 2014; Marengoni et al., 2011). Men and women, on average, differ markedly in their interests and work preferences (Su et al., 2009). With age, personalities gradually change, typically becoming less open to experiences but more agreeable and conscientious (McCrae et al., 1999). Additionally, social media language varies by age (Kern et al., 2014; Pennebaker and Stone, 2003) and gender (Huffaker and Calvert, 2005). Twitter may have a male bias (Mislove et al., 2011), while social media in general skew towards being young and female (pew, 2014). Accessible tools to predict demographic variables can substantially enhance soc</context>
</contexts>
<marker>Marengoni, Angleman, Melis, Mangialasche, Karp, Garmen, Meinow, Fratiglioni, 2011</marker>
<rawString>Alessandra Marengoni, Sara Angleman, Ren´e Melis, Francesca Mangialasche, Anita Karp, Annika Garmen, Bettina Meinow, and Laura Fratiglioni. 2011. Aging with multimorbidity: a systematic review of the literature. Ageing research reviews, 10(4):430– 439.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert R McCrae</author>
<author>Paul T Costa</author>
</authors>
<title>Margarida Pedroso de Lima, Ant´onio Sim˜oes, Fritz Ostendorf, Alois Angleitner, Iris Maruˇsi´c, Denis Bratko, Gian Vittorio Caprara, Claudio Barbaranelli, et al.</title>
<date>1999</date>
<publisher>Age</publisher>
<marker>McCrae, Costa, 1999</marker>
<rawString>Robert R McCrae, Paul T Costa, Margarida Pedroso de Lima, Ant´onio Sim˜oes, Fritz Ostendorf, Alois Angleitner, Iris Maruˇsi´c, Denis Bratko, Gian Vittorio Caprara, Claudio Barbaranelli, et al. 1999. Age</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael J Paul</author>
<author>Mark Dredze</author>
</authors>
<title>You are what you tweet: Analyzing twitter for public health.</title>
<date>2011</date>
<booktitle>In ICWSM.</booktitle>
<contexts>
<context position="1285" citStr="Paul and Dredze, 2011" startWordPosition="178" endWordPosition="181">ta with associated demographic labels. The lexica, made publicly available,1 achieved state-of-the-art accuracy in language based age and gender prediction over Facebook and Twitter, and were evaluated for generalization across social media genres as well as in limited message situations. 1 Introduction Use of social media has enabled the study of psychological and social questions at an unprecedented scale (Lazer et al., 2009). This allows more data-driven discovery alongside the typical hypothesis-testing social science process (Schwartz et al., 2013b). Social media may track disease rates (Paul and Dredze, 2011; Google, 2014), psychological well-being (Dodds et al., 2011; De Choudhury et al., 2013; Schwartz et al., 2013a), and a host of other behavioral, psychological and medical phenomena (Kosinski et al., 2013). Unlike traditional hypothesis-driven social science, such large-scale social media studies rarely take into account—or have access to—age and gender information, which can have a major impact on many questions. For example, females live almost five years longer than males (cdc, 2014; Marengoni et al., 2011). Men and women, on average, differ markedly in their interests and work preferences</context>
</contexts>
<marker>Paul, Dredze, 2011</marker>
<rawString>Michael J Paul and Mark Dredze. 2011. You are what you tweet: Analyzing twitter for public health. In ICWSM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Pennacchiotti</author>
<author>Ana-Maria Popescu</author>
</authors>
<title>A machine learning approach to twitter user classification.</title>
<date>2011</date>
<booktitle>In ICWSM.</booktitle>
<contexts>
<context position="3894" citStr="Pennacchiotti and Popescu, 2011" startWordPosition="571" endWordPosition="574">lity, the lexica are simply a transparent and portable means for distributing predictive models based on words. We test generalization and adapt the lexica to blogs and Twitter, plus consider situations when limited messages are available. In addition to use in the computational linguistics community, we believe the lexicon format will make it easier for social scientists to leverage data-driven models where manually created lexica currently dominate2 (Dodds et al., 2011; Tausczik and Pennebaker, 2010). 2 Related Work Online behavior is representative of many aspects of a user’s demographics (Pennacchiotti and Popescu, 2011; Rao et al., 2010). Many studies have used linguistic cues (such as ngrams) to determine if someone belongs to a certain age group, be it on Twitter or another social media platform (Al Zamal et al., 2012; Argamon et al., 2009; Nguyen et al., 2013; Rangel and Rosso, 2013). Gender prediction has been studied across blogs (Burger and Henderson, 2006; Goswami et al., 2009), Yahoo! search queries (Jones et al., 2007), and Twitter (Burger et al., 2011; Nguyen et al., 2013; Liu and Ruths, 2013; Rao et al., 2010). Because Twitter does not make gender or age available, such work infers gender and age</context>
</contexts>
<marker>Pennacchiotti, Popescu, 2011</marker>
<rawString>Marco Pennacchiotti and Ana-Maria Popescu. 2011. A machine learning approach to twitter user classification. In ICWSM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James W Pennebaker</author>
<author>Lori D Stone</author>
</authors>
<title>Words of wisdom: language use over the life span.</title>
<date>2003</date>
<journal>Journal of Personality and Social Psychology,</journal>
<volume>85</volume>
<issue>2</issue>
<pages>301</pages>
<contexts>
<context position="2148" citStr="Pennebaker and Stone, 2003" startWordPosition="310" endWordPosition="313">s-driven social science, such large-scale social media studies rarely take into account—or have access to—age and gender information, which can have a major impact on many questions. For example, females live almost five years longer than males (cdc, 2014; Marengoni et al., 2011). Men and women, on average, differ markedly in their interests and work preferences (Su et al., 2009). With age, personalities gradually change, typically becoming less open to experiences but more agreeable and conscientious (McCrae et al., 1999). Additionally, social media language varies by age (Kern et al., 2014; Pennebaker and Stone, 2003) and gender (Huffaker and Calvert, 2005). Twitter may have a male bias (Mislove et al., 2011), while social media in general skew towards being young and female (pew, 2014). Accessible tools to predict demographic variables can substantially enhance social media’s utility for so1download at http://www.wwbp.org/data.html cial science, economic, and business applications. For example, one can post-stratify population-level results to reflect a representative sample, understand variation across age and gender groups, or produce personalized marketing, services, and sentiment recommendations; a mo</context>
</contexts>
<marker>Pennebaker, Stone, 2003</marker>
<rawString>James W Pennebaker and Lori D Stone. 2003. Words of wisdom: language use over the life span. Journal of Personality and Social Psychology, 85(2):291– 301.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James W Pennebaker</author>
<author>Martha E Francis</author>
<author>Roger J Booth</author>
</authors>
<title>Linguistic inquiry and word count: Liwc</title>
<date>2001</date>
<pages>71--2001</pages>
<contexts>
<context position="4728" citStr="Pennebaker et al., 2001" startWordPosition="713" endWordPosition="716">amon et al., 2009; Nguyen et al., 2013; Rangel and Rosso, 2013). Gender prediction has been studied across blogs (Burger and Henderson, 2006; Goswami et al., 2009), Yahoo! search queries (Jones et al., 2007), and Twitter (Burger et al., 2011; Nguyen et al., 2013; Liu and Ruths, 2013; Rao et al., 2010). Because Twitter does not make gender or age available, such work infers gender and age by leveraging profile information, such as gender-discriminating names or crawling for links to publicly available data (e.g. Burger et al., 2The LIWC lexicon, derived manually based on psychological theory, (Pennebaker et al., 2001) had 1136 citations in 2013 alone. 1146 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1146–1151, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics 2011). While many studies have examined prediction of age or gender, none (to our knowledge) have released a model to the public, much less in the form of a lexicon. Additionally, most works in age prediction classify users into bins rather than predicting a continuous real-valued age as we do (exceptions: Nguyen et al., 2013; Jones et al., 2007). People have al</context>
</contexts>
<marker>Pennebaker, Francis, Booth, 2001</marker>
<rawString>James W Pennebaker, Martha E Francis, and Roger J Booth. 2001. Linguistic inquiry and word count: Liwc 2001. 71:2001.</rawString>
</citation>
<citation valid="true">
<title>Social networking fact sheet. http: //www.pewinternet.org/fact-sheets/ social-networking-fact-sheet/. Accessed on</title>
<date>2014</date>
<marker>2014</marker>
<rawString>2014. Social networking fact sheet. http: //www.pewinternet.org/fact-sheets/ social-networking-fact-sheet/. Accessed on August 26, 2014.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Francisco Rangel</author>
<author>Paolo Rosso</author>
</authors>
<title>Use of language and author profiling: Identification of gender and age.</title>
<date>2013</date>
<booktitle>Natural Language Processing and Cognitive Science,</booktitle>
<pages>177</pages>
<contexts>
<context position="4167" citStr="Rangel and Rosso, 2013" startWordPosition="622" endWordPosition="625">inguistics community, we believe the lexicon format will make it easier for social scientists to leverage data-driven models where manually created lexica currently dominate2 (Dodds et al., 2011; Tausczik and Pennebaker, 2010). 2 Related Work Online behavior is representative of many aspects of a user’s demographics (Pennacchiotti and Popescu, 2011; Rao et al., 2010). Many studies have used linguistic cues (such as ngrams) to determine if someone belongs to a certain age group, be it on Twitter or another social media platform (Al Zamal et al., 2012; Argamon et al., 2009; Nguyen et al., 2013; Rangel and Rosso, 2013). Gender prediction has been studied across blogs (Burger and Henderson, 2006; Goswami et al., 2009), Yahoo! search queries (Jones et al., 2007), and Twitter (Burger et al., 2011; Nguyen et al., 2013; Liu and Ruths, 2013; Rao et al., 2010). Because Twitter does not make gender or age available, such work infers gender and age by leveraging profile information, such as gender-discriminating names or crawling for links to publicly available data (e.g. Burger et al., 2The LIWC lexicon, derived manually based on psychological theory, (Pennebaker et al., 2001) had 1136 citations in 2013 alone. 1146</context>
</contexts>
<marker>Rangel, Rosso, 2013</marker>
<rawString>Francisco Rangel and Paolo Rosso. 2013. Use of language and author profiling: Identification of gender and age. Natural Language Processing and Cognitive Science, page 177.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Delip Rao</author>
<author>David Yarowsky</author>
<author>Abhishek Shreevats</author>
<author>Manaswi Gupta</author>
</authors>
<title>Classifying latent user attributes in twitter.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2nd international workshop on Search and mining usergenerated contents,</booktitle>
<pages>37--44</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="3913" citStr="Rao et al., 2010" startWordPosition="575" endWordPosition="578">nsparent and portable means for distributing predictive models based on words. We test generalization and adapt the lexica to blogs and Twitter, plus consider situations when limited messages are available. In addition to use in the computational linguistics community, we believe the lexicon format will make it easier for social scientists to leverage data-driven models where manually created lexica currently dominate2 (Dodds et al., 2011; Tausczik and Pennebaker, 2010). 2 Related Work Online behavior is representative of many aspects of a user’s demographics (Pennacchiotti and Popescu, 2011; Rao et al., 2010). Many studies have used linguistic cues (such as ngrams) to determine if someone belongs to a certain age group, be it on Twitter or another social media platform (Al Zamal et al., 2012; Argamon et al., 2009; Nguyen et al., 2013; Rangel and Rosso, 2013). Gender prediction has been studied across blogs (Burger and Henderson, 2006; Goswami et al., 2009), Yahoo! search queries (Jones et al., 2007), and Twitter (Burger et al., 2011; Nguyen et al., 2013; Liu and Ruths, 2013; Rao et al., 2010). Because Twitter does not make gender or age available, such work infers gender and age by leveraging prof</context>
<context position="5465" citStr="Rao et al., 2010" startWordPosition="830" endWordPosition="833">essing (EMNLP), pages 1146–1151, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics 2011). While many studies have examined prediction of age or gender, none (to our knowledge) have released a model to the public, much less in the form of a lexicon. Additionally, most works in age prediction classify users into bins rather than predicting a continuous real-valued age as we do (exceptions: Nguyen et al., 2013; Jones et al., 2007). People have also used online media to infer other demographic-like attributes such as native language (Argamon et al., 2009), origin (Rao et al., 2010), and location (Jones et al., 2007). An approach similar to the one presented here could be used to create lexica for any of these outcomes. While lexica are not often used for demographics, data-driven lexicon creation over social media has been well studied for sentiment, in which univariate techniques (e.g. point-wise mutual information) dominate3. For example, Taboada et al. (2011) expanded an initial lexicon by adding on co-occurring words. More recently, Mohammad’s sentiment lexicon (Mohammad et al., 2013) was found to be the most informative feature for the top system in the SemEval2013</context>
</contexts>
<marker>Rao, Yarowsky, Shreevats, Gupta, 2010</marker>
<rawString>Delip Rao, David Yarowsky, Abhishek Shreevats, and Manaswi Gupta. 2010. Classifying latent user attributes in twitter. In Proceedings of the 2nd international workshop on Search and mining usergenerated contents, pages 37–44. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Schler</author>
<author>Moshe Koppel</author>
<author>Shlomo Argamon</author>
<author>James W Pennebaker</author>
</authors>
<title>Effects of age and gender on blogging.</title>
<date>2006</date>
<booktitle>In AAAI Spring Symposium: Computational Approaches to Analyzing Weblogs,</booktitle>
<volume>6</volume>
<pages>199--205</pages>
<contexts>
<context position="8164" citStr="Schler et al., 2006" startWordPosition="1263" endWordPosition="1266">-wise information-derived sentiment lexica are often used as features in a supervised model, essentially dimensionally reducing a large set of words into positive and negative sentiment, while our lexica represent the predictive model itself. stratified Facebook data, blogs, and tweets. The stratified Facebook data (exclusively used for testing) consists of equal proportions of 1,520 males and females across 12 4-year age bins starting at 13 and ending at 60.4 This roughly matchs the size of the main test set. Seeking out-of-domain data, we downloaded age and gender annotated blogs from 2004 (Schler et al., 2006) (also used in Goswami et al., 2009) and gender labeled tweets (Volkova et al., 2013). Limiting the sample to users who wrote at least 1000 words, the total number of bloggers is 15,006, of which 50.6% are female and only 15% are over 27 (reflecting the younger population standard in social media). From this we use a randomly selected 1,000 bloggers as a blogger test set and the remaining 14,006 bloggers for training. Similarly for the Twitter dataset, we use 11,000 random gender-only annotated users, in which 51.9% are female. We again randomly select 1,000 users as a test set for gender pred</context>
<context position="15095" citStr="Schler et al., 2006" startWordPosition="2425" endWordPosition="2428">g profile information in addition to text. No other similar studies — to the best of our knowledge — have been conducted. Application in other settings. While Facebook is the ideal setting to apply our lexica, we hope that they generalize to other situations. To evaluate their utility in other settings, we first tested them over a gender and age stratified Facebook sample. Our random sample, like all of Facebook, is biased toward the young; this stratified test sample contains equal numbers of males and females, ages 13 to 60. Next, we use the lexica to predict data from other domains: blogs (Schler et al., 2006) and Twitter (Volkova et al., 2013). In this case, our goal was to account for the content and stylistic variation that may be specific to Facebook. 7Adding 2 and 3-grams increases the performance of our model (r = 0.85, 92.7%), just above our previous results (Schwartz et al., 2013b). However, with the accessibility of single word lexica in mind, this current work focuses on features based entirely on 1grams. 1148 # Msgs: all 100 20 5 1 age .831 .820 .688 .454 .156 gender .919 .901 .796 .635 .554 Table 2: Prediction accuracies for age (Pearson correlation) and gender (accuracy %) when reducin</context>
</contexts>
<marker>Schler, Koppel, Argamon, Pennebaker, 2006</marker>
<rawString>Jonathan Schler, Moshe Koppel, Shlomo Argamon, and James W Pennebaker. 2006. Effects of age and gender on blogging. In AAAI Spring Symposium: Computational Approaches to Analyzing Weblogs, volume 6, pages 199–205.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Andrew Schwartz</author>
<author>Johannes C Eichstaedt</author>
<author>Margaret L Kern</author>
<author>Lukasz Dziurzynski</author>
<author>Megha Agrawal</author>
<author>Gregory J Park</author>
<author>Shrinidhi K Lakshmikanth</author>
<author>Sneha Jha</author>
<author>Martin EP Seligman</author>
<author>Lyle Ungar</author>
</authors>
<title>Characterizing geographic variation in well-being using tweets.</title>
<date>2013</date>
<booktitle>In ICWSM.</booktitle>
<contexts>
<context position="1222" citStr="Schwartz et al., 2013" startWordPosition="167" endWordPosition="170">cation models from word usage in Facebook, blog, and Twitter data with associated demographic labels. The lexica, made publicly available,1 achieved state-of-the-art accuracy in language based age and gender prediction over Facebook and Twitter, and were evaluated for generalization across social media genres as well as in limited message situations. 1 Introduction Use of social media has enabled the study of psychological and social questions at an unprecedented scale (Lazer et al., 2009). This allows more data-driven discovery alongside the typical hypothesis-testing social science process (Schwartz et al., 2013b). Social media may track disease rates (Paul and Dredze, 2011; Google, 2014), psychological well-being (Dodds et al., 2011; De Choudhury et al., 2013; Schwartz et al., 2013a), and a host of other behavioral, psychological and medical phenomena (Kosinski et al., 2013). Unlike traditional hypothesis-driven social science, such large-scale social media studies rarely take into account—or have access to—age and gender information, which can have a major impact on many questions. For example, females live almost five years longer than males (cdc, 2014; Marengoni et al., 2011). Men and women, on a</context>
<context position="13792" citStr="Schwartz et al., 2013" startWordPosition="2204" endWordPosition="2207">valuation over Facebook users are shown in Table 1 (randFB columns). Accuracies for age are reported as Pearson correlation coefficients (r) 6downloaded from http://www.wwbp.org/data.html and mean absolute errors (mae), measured in years. For gender, we use an accuracy % (number-correct over test-size). As baselines, we use the mean for age (23.0 years old) and the most frequent class (female) for gender. We see that for both age and gender, accuracies are substantially higher than the baseline. These accuracies were just below with no significant difference previous state-of-the-art results (Schwartz et al., 2013; r = 0.84 for age and 91.9% accuracy for gender). 7 Because of the nature of our datasets (the Facebook data is private) and task (user-level predictions), comparable previous studies are nearly nonexistent. Nonetheless, the Twitter data was a random subset of users based on the (Burger et al., 2011) dataset excluding non-English tweets, making it somewhat comparable. In this case, the lexica outperformed previous results for gender prediction of Twitter users, which ranged from 75.5% to 87% (Burger et al., 2011; Ciot et al., 2013; Liu and Ruths, 2013; Al Zamal et al., 2012). However, the lex</context>
<context position="15378" citStr="Schwartz et al., 2013" startWordPosition="2474" endWordPosition="2477">ity in other settings, we first tested them over a gender and age stratified Facebook sample. Our random sample, like all of Facebook, is biased toward the young; this stratified test sample contains equal numbers of males and females, ages 13 to 60. Next, we use the lexica to predict data from other domains: blogs (Schler et al., 2006) and Twitter (Volkova et al., 2013). In this case, our goal was to account for the content and stylistic variation that may be specific to Facebook. 7Adding 2 and 3-grams increases the performance of our model (r = 0.85, 92.7%), just above our previous results (Schwartz et al., 2013b). However, with the accessibility of single word lexica in mind, this current work focuses on features based entirely on 1grams. 1148 # Msgs: all 100 20 5 1 age .831 .820 .688 .454 .156 gender .919 .901 .796 .635 .554 Table 2: Prediction accuracies for age (Pearson correlation) and gender (accuracy %) when reducing the number of messages from each user. Results over these additional datasets are shown in Table 1 (stratFB, randBG, and randT columns). The performance decreases as expected since these datasets have differing distributions, but it is still substantially above mean and most frequ</context>
</contexts>
<marker>Schwartz, Eichstaedt, Kern, Dziurzynski, Agrawal, Park, Lakshmikanth, Jha, Seligman, Ungar, 2013</marker>
<rawString>H Andrew Schwartz, Johannes C Eichstaedt, Margaret L Kern, Lukasz Dziurzynski, Megha Agrawal, Gregory J Park, Shrinidhi K Lakshmikanth, Sneha Jha, Martin EP Seligman, Lyle Ungar, et al. 2013a. Characterizing geographic variation in well-being using tweets. In ICWSM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Andrew Schwartz</author>
<author>Johannes C Eichstaedt</author>
<author>Margaret L Kern</author>
<author>Lukasz Dziurzynski</author>
<author>Stephanie M Ramones</author>
<author>Megha Agrawal</author>
<author>Achal Shah</author>
<author>Michal Kosinski</author>
<author>David J Stillwell</author>
<author>Martin EP Seligman</author>
</authors>
<title>Personality, gender, and age in the language of social media: The open-vocabulary approach.</title>
<date>2013</date>
<tech>PloS one, 8(9):e73791.</tech>
<contexts>
<context position="1222" citStr="Schwartz et al., 2013" startWordPosition="167" endWordPosition="170">cation models from word usage in Facebook, blog, and Twitter data with associated demographic labels. The lexica, made publicly available,1 achieved state-of-the-art accuracy in language based age and gender prediction over Facebook and Twitter, and were evaluated for generalization across social media genres as well as in limited message situations. 1 Introduction Use of social media has enabled the study of psychological and social questions at an unprecedented scale (Lazer et al., 2009). This allows more data-driven discovery alongside the typical hypothesis-testing social science process (Schwartz et al., 2013b). Social media may track disease rates (Paul and Dredze, 2011; Google, 2014), psychological well-being (Dodds et al., 2011; De Choudhury et al., 2013; Schwartz et al., 2013a), and a host of other behavioral, psychological and medical phenomena (Kosinski et al., 2013). Unlike traditional hypothesis-driven social science, such large-scale social media studies rarely take into account—or have access to—age and gender information, which can have a major impact on many questions. For example, females live almost five years longer than males (cdc, 2014; Marengoni et al., 2011). Men and women, on a</context>
<context position="13792" citStr="Schwartz et al., 2013" startWordPosition="2204" endWordPosition="2207">valuation over Facebook users are shown in Table 1 (randFB columns). Accuracies for age are reported as Pearson correlation coefficients (r) 6downloaded from http://www.wwbp.org/data.html and mean absolute errors (mae), measured in years. For gender, we use an accuracy % (number-correct over test-size). As baselines, we use the mean for age (23.0 years old) and the most frequent class (female) for gender. We see that for both age and gender, accuracies are substantially higher than the baseline. These accuracies were just below with no significant difference previous state-of-the-art results (Schwartz et al., 2013; r = 0.84 for age and 91.9% accuracy for gender). 7 Because of the nature of our datasets (the Facebook data is private) and task (user-level predictions), comparable previous studies are nearly nonexistent. Nonetheless, the Twitter data was a random subset of users based on the (Burger et al., 2011) dataset excluding non-English tweets, making it somewhat comparable. In this case, the lexica outperformed previous results for gender prediction of Twitter users, which ranged from 75.5% to 87% (Burger et al., 2011; Ciot et al., 2013; Liu and Ruths, 2013; Al Zamal et al., 2012). However, the lex</context>
<context position="15378" citStr="Schwartz et al., 2013" startWordPosition="2474" endWordPosition="2477">ity in other settings, we first tested them over a gender and age stratified Facebook sample. Our random sample, like all of Facebook, is biased toward the young; this stratified test sample contains equal numbers of males and females, ages 13 to 60. Next, we use the lexica to predict data from other domains: blogs (Schler et al., 2006) and Twitter (Volkova et al., 2013). In this case, our goal was to account for the content and stylistic variation that may be specific to Facebook. 7Adding 2 and 3-grams increases the performance of our model (r = 0.85, 92.7%), just above our previous results (Schwartz et al., 2013b). However, with the accessibility of single word lexica in mind, this current work focuses on features based entirely on 1grams. 1148 # Msgs: all 100 20 5 1 age .831 .820 .688 .454 .156 gender .919 .901 .796 .635 .554 Table 2: Prediction accuracies for age (Pearson correlation) and gender (accuracy %) when reducing the number of messages from each user. Results over these additional datasets are shown in Table 1 (stratFB, randBG, and randT columns). The performance decreases as expected since these datasets have differing distributions, but it is still substantially above mean and most frequ</context>
</contexts>
<marker>Schwartz, Eichstaedt, Kern, Dziurzynski, Ramones, Agrawal, Shah, Kosinski, Stillwell, Seligman, 2013</marker>
<rawString>H Andrew Schwartz, Johannes C Eichstaedt, Margaret L Kern, Lukasz Dziurzynski, Stephanie M Ramones, Megha Agrawal, Achal Shah, Michal Kosinski, David J Stillwell, Martin EP Seligman, et al. 2013b. Personality, gender, and age in the language of social media: The open-vocabulary approach. PloS one, 8(9):e73791.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rong Su</author>
<author>James Rounds</author>
<author>Patrick Ian Armstrong</author>
</authors>
<title>Men and things, women and people: a metaanalysis of sex differences in interests.</title>
<date>2009</date>
<journal>Psychological Bulletin,</journal>
<volume>135</volume>
<issue>6</issue>
<contexts>
<context position="1903" citStr="Su et al., 2009" startWordPosition="273" endWordPosition="276">Google, 2014), psychological well-being (Dodds et al., 2011; De Choudhury et al., 2013; Schwartz et al., 2013a), and a host of other behavioral, psychological and medical phenomena (Kosinski et al., 2013). Unlike traditional hypothesis-driven social science, such large-scale social media studies rarely take into account—or have access to—age and gender information, which can have a major impact on many questions. For example, females live almost five years longer than males (cdc, 2014; Marengoni et al., 2011). Men and women, on average, differ markedly in their interests and work preferences (Su et al., 2009). With age, personalities gradually change, typically becoming less open to experiences but more agreeable and conscientious (McCrae et al., 1999). Additionally, social media language varies by age (Kern et al., 2014; Pennebaker and Stone, 2003) and gender (Huffaker and Calvert, 2005). Twitter may have a male bias (Mislove et al., 2011), while social media in general skew towards being young and female (pew, 2014). Accessible tools to predict demographic variables can substantially enhance social media’s utility for so1download at http://www.wwbp.org/data.html cial science, economic, and busin</context>
</contexts>
<marker>Su, Rounds, Armstrong, 2009</marker>
<rawString>Rong Su, James Rounds, and Patrick Ian Armstrong. 2009. Men and things, women and people: a metaanalysis of sex differences in interests. Psychological Bulletin, 135(6):859–884.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maite Taboada</author>
<author>Julian Brooke</author>
<author>Milan Tofiloski</author>
<author>Kimberly Voll</author>
<author>Manfred Stede</author>
</authors>
<title>Lexiconbased methods for sentiment analysis.</title>
<date>2011</date>
<journal>Computational linguistics,</journal>
<pages>37--2</pages>
<contexts>
<context position="5853" citStr="Taboada et al. (2011)" startWordPosition="893" endWordPosition="896">s real-valued age as we do (exceptions: Nguyen et al., 2013; Jones et al., 2007). People have also used online media to infer other demographic-like attributes such as native language (Argamon et al., 2009), origin (Rao et al., 2010), and location (Jones et al., 2007). An approach similar to the one presented here could be used to create lexica for any of these outcomes. While lexica are not often used for demographics, data-driven lexicon creation over social media has been well studied for sentiment, in which univariate techniques (e.g. point-wise mutual information) dominate3. For example, Taboada et al. (2011) expanded an initial lexicon by adding on co-occurring words. More recently, Mohammad’s sentiment lexicon (Mohammad et al., 2013) was found to be the most informative feature for the top system in the SemEval2013 social media sentiment analysis task (Wilson et al., 2013). Approaches like point-wise mutual information take a univariate view on words–i.e. the weight given to one feature (word) is not affected by other features. Since language is highly collinear, we take a multivariate lexicon development approach, which takes covariance into account (e.g. someone who mentions ‘hair’ often is mo</context>
</contexts>
<marker>Taboada, Brooke, Tofiloski, Voll, Stede, 2011</marker>
<rawString>Maite Taboada, Julian Brooke, Milan Tofiloski, Kimberly Voll, and Manfred Stede. 2011. Lexiconbased methods for sentiment analysis. Computational linguistics, 37(2):267–307.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yla R Tausczik</author>
<author>James W Pennebaker</author>
</authors>
<title>The psychological meaning of words: Liwc and computerized text analysis methods.</title>
<date>2010</date>
<journal>Journal of language and social psychology,</journal>
<pages>29--1</pages>
<contexts>
<context position="3770" citStr="Tausczik and Pennebaker, 2010" startWordPosition="553" endWordPosition="556">ed linear regression (for continuous valued age) and support vector classification (for binary-valued gender). In this modality, the lexica are simply a transparent and portable means for distributing predictive models based on words. We test generalization and adapt the lexica to blogs and Twitter, plus consider situations when limited messages are available. In addition to use in the computational linguistics community, we believe the lexicon format will make it easier for social scientists to leverage data-driven models where manually created lexica currently dominate2 (Dodds et al., 2011; Tausczik and Pennebaker, 2010). 2 Related Work Online behavior is representative of many aspects of a user’s demographics (Pennacchiotti and Popescu, 2011; Rao et al., 2010). Many studies have used linguistic cues (such as ngrams) to determine if someone belongs to a certain age group, be it on Twitter or another social media platform (Al Zamal et al., 2012; Argamon et al., 2009; Nguyen et al., 2013; Rangel and Rosso, 2013). Gender prediction has been studied across blogs (Burger and Henderson, 2006; Goswami et al., 2009), Yahoo! search queries (Jones et al., 2007), and Twitter (Burger et al., 2011; Nguyen et al., 2013; Li</context>
</contexts>
<marker>Tausczik, Pennebaker, 2010</marker>
<rawString>Yla R Tausczik and James W Pennebaker. 2010. The psychological meaning of words: Liwc and computerized text analysis methods. Journal of language and social psychology, 29(1):24–54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Tibshirani</author>
</authors>
<title>Regression shrinkage and selection via the lasso.</title>
<date>1996</date>
<journal>Journal of the Royal Statistical Society. Series B (Methodological),</journal>
<pages>267--288</pages>
<contexts>
<context position="12044" citStr="Tibshirani, 1996" startWordPosition="1925" endWordPosition="1926">m ridge regression (Hoerl and Kennard, 1970) for age (continuous variable) and from support vector classification (Fan et al., 2008) for gender (binary variable). Ridge regression uses an L2 (α||β||2) penalization to avoid overfitting (Hoerl and Kennard, 1970). Although some words no doubt have a nonlinear relationship with age (e.g., ‘fiance’ peaks in the 20s), we still find high accuracy from a linear model (see Table 1) and it allows for a distribution of the model in the accessible form of a lexicon. For gender prediction, we use an SVM with a linear kernel with L1 penalization (α||β||1) (Tibshirani, 1996). Because the L1 penalization zeros-out many coefficients, it has the added advantage of effectively reducing the size of the lexica. Using the training data, we test a variety algorithms including the lasso, elastic net regression, and L2 penalized SVMs in order to decide which learning algorithms to use. To extract the words (1grams) to use as features and which make up lexica, we use the Happier Fun Tokenizer,6 which handles social media content and markup such as emoticons or hashtags. For our main user-level models, word usage is aggregated as the relative frequency (freq(word,user) ) Due</context>
</contexts>
<marker>Tibshirani, 1996</marker>
<rawString>Robert Tibshirani. 1996. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society. Series B (Methodological), pages 267–288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Van Durme</author>
</authors>
<title>Streaming analysis of discourse participants.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>48--58</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Van Durme, 2012</marker>
<rawString>Benjamin Van Durme. 2012. Streaming analysis of discourse participants. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 48–58. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Svitlana Volkova</author>
<author>Theresa Wilson</author>
<author>David Yarowsky</author>
</authors>
<title>Exploring demographic language variations to improve multilingual sentiment analysis in social media.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods on Natural Language Processing.</booktitle>
<contexts>
<context position="8249" citStr="Volkova et al., 2013" startWordPosition="1278" endWordPosition="1281">d model, essentially dimensionally reducing a large set of words into positive and negative sentiment, while our lexica represent the predictive model itself. stratified Facebook data, blogs, and tweets. The stratified Facebook data (exclusively used for testing) consists of equal proportions of 1,520 males and females across 12 4-year age bins starting at 13 and ending at 60.4 This roughly matchs the size of the main test set. Seeking out-of-domain data, we downloaded age and gender annotated blogs from 2004 (Schler et al., 2006) (also used in Goswami et al., 2009) and gender labeled tweets (Volkova et al., 2013). Limiting the sample to users who wrote at least 1000 words, the total number of bloggers is 15,006, of which 50.6% are female and only 15% are over 27 (reflecting the younger population standard in social media). From this we use a randomly selected 1,000 bloggers as a blogger test set and the remaining 14,006 bloggers for training. Similarly for the Twitter dataset, we use 11,000 random gender-only annotated users, in which 51.9% are female. We again randomly select 1,000 users as a test set for gender prediction and use the remaining 10,000 for training. 3.1 Lexicon Creation We present a m</context>
<context position="15130" citStr="Volkova et al., 2013" startWordPosition="2431" endWordPosition="2434">to text. No other similar studies — to the best of our knowledge — have been conducted. Application in other settings. While Facebook is the ideal setting to apply our lexica, we hope that they generalize to other situations. To evaluate their utility in other settings, we first tested them over a gender and age stratified Facebook sample. Our random sample, like all of Facebook, is biased toward the young; this stratified test sample contains equal numbers of males and females, ages 13 to 60. Next, we use the lexica to predict data from other domains: blogs (Schler et al., 2006) and Twitter (Volkova et al., 2013). In this case, our goal was to account for the content and stylistic variation that may be specific to Facebook. 7Adding 2 and 3-grams increases the performance of our model (r = 0.85, 92.7%), just above our previous results (Schwartz et al., 2013b). However, with the accessibility of single word lexica in mind, this current work focuses on features based entirely on 1grams. 1148 # Msgs: all 100 20 5 1 age .831 .820 .688 .454 .156 gender .919 .901 .796 .635 .554 Table 2: Prediction accuracies for age (Pearson correlation) and gender (accuracy %) when reducing the number of messages from each </context>
</contexts>
<marker>Volkova, Wilson, Yarowsky, 2013</marker>
<rawString>Svitlana Volkova, Theresa Wilson, and David Yarowsky. 2013. Exploring demographic language variations to improve multilingual sentiment analysis in social media. In Proceedings of the 2013 Conference on Empirical Methods on Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Zornitsa Kozareva</author>
<author>Preslav Nakov</author>
<author>Sara Rosenthal</author>
<author>Veselin Stoyanov</author>
<author>Alan Ritter</author>
</authors>
<title>Semeval-2013 task 2: Sentiment analysis in twitter.</title>
<date>2013</date>
<booktitle>In Proceedings of the International Workshop on Semantic Evaluation, SemEval,</booktitle>
<volume>13</volume>
<contexts>
<context position="6124" citStr="Wilson et al., 2013" startWordPosition="938" endWordPosition="941"> approach similar to the one presented here could be used to create lexica for any of these outcomes. While lexica are not often used for demographics, data-driven lexicon creation over social media has been well studied for sentiment, in which univariate techniques (e.g. point-wise mutual information) dominate3. For example, Taboada et al. (2011) expanded an initial lexicon by adding on co-occurring words. More recently, Mohammad’s sentiment lexicon (Mohammad et al., 2013) was found to be the most informative feature for the top system in the SemEval2013 social media sentiment analysis task (Wilson et al., 2013). Approaches like point-wise mutual information take a univariate view on words–i.e. the weight given to one feature (word) is not affected by other features. Since language is highly collinear, we take a multivariate lexicon development approach, which takes covariance into account (e.g. someone who mentions ‘hair’ often is more likely to mention ‘brushing’, ‘style’, and ‘cut’; weighting these words in isolation might “double-count” some information). 3 Method Primary data. Our primary dataset consists of Facebook messages from users of the MyPersonality application (Kosinski and Stillwell, 2</context>
</contexts>
<marker>Wilson, Kozareva, Nakov, Rosenthal, Stoyanov, Ritter, 2013</marker>
<rawString>Theresa Wilson, Zornitsa Kozareva, Preslav Nakov, Sara Rosenthal, Veselin Stoyanov, and Alan Ritter. 2013. Semeval-2013 task 2: Sentiment analysis in twitter. In Proceedings of the International Workshop on Semantic Evaluation, SemEval, volume 13.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
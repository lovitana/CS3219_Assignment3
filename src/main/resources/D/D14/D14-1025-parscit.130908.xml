<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001506">
<title confidence="0.9965385">
Fitting Sentence Level Translation Evaluation
with Many Dense Features
</title>
<author confidence="0.993454">
Miloˇs Stanojevi´c and Khalil Sima’an
</author>
<affiliation confidence="0.9965475">
Institute for Logic, Language and Computation
University of Amsterdam
</affiliation>
<address confidence="0.93288">
Science Park 107, 1098 XG Amsterdam, The Netherlands
</address>
<email confidence="0.998641">
{m.stanojevic,k.simaan}@uva.nl
</email>
<sectionHeader confidence="0.993883" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999856117647059">
Sentence level evaluation in MT has turned out
far more difficult than corpus level evaluation.
Existing sentence level metrics employ a lim-
ited set of features, most of which are rather
sparse at the sentence level, and their intricate
models are rarely trained for ranking. This pa-
per presents a simple linear model exploiting
33 relatively dense features, some of which are
novel while others are known but seldom used,
and train it under the learning-to-rank frame-
work. We evaluate our metric on the stan-
dard WMT12 data showing that it outperforms
the strong baseline METEOR. We also ana-
lyze the contribution of individual features and
the choice of training data, language-pair vs.
target-language data, providing new insights
into this task.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999702923076923">
Evaluating machine translation (MT) output at the sen-
tence/ segment level has turned out far more challeng-
ing than corpus/ system level. Yet, sentence level
evaluation can be useful because it allows fast, fine-
grained analysis of system performance on individual
sentences.
It is instructive to contrast two widely used metrics,
METEOR (Michael Denkowski and Alon Lavie, 2014)
and BLEU (Papineni et al., 2002), on sentence level
evaluation. METEOR constantly shows better corre-
lation with human ranking than BLEU (Papineni et
al., 2002). Arguably, this shows that sentence level
evaluation demands finer grained and trainable models
over less sparse features. Ngrams, the core of BLEU,
are sparse at the sentence level, and a mismatch for
longer ngrams implies that BLEU falls back on shorter
ngrams. In contrast, METEOR has a trainable model
and incorporates a small, yet wider set of features that
are less sparse than ngrams. We think that METEOR’s
features and its training approach only suggest that sen-
tence level evaluation should be treated as a modelling
challenge. This calls for questions such as what model,
what features and what training objective are better
suited for modelling sentence level evaluation.
We start out by explicitly formulating sentence level
evaluation as the problem of ranking a set of compet-
ing hypothesis. Given data consisting of human ranked
system outputs, the problem then is to formulate an
easy to train model for ranking. One particular exist-
ing approach (Ye et al., 2007) looks especially attrac-
tive because we think it meshes well with a range of
effective techniques for learning-to-rank (Li, 2011).
We deliberately select a linear modelling approach
inspired by RankSVM (Herbrich et al., 1999), which is
easily trainable for ranking and allows analysis of the
individual contributions of features. Besides presenting
a new metric and a set of known, but also a set of novel
features, we target three questions of interest to the MT
community:
</bodyText>
<listItem confidence="0.984472428571428">
• What kind of features are more helpful for sen-
tence level evaluation?
• How does a simple linear model trained for rank-
ing compare to the well-developed metric ME-
TEOR on sentence level evaluation?
• Should we train the model for each language pair
separately or for a target language?
</listItem>
<bodyText confidence="0.9995825">
Our new metric dubbed BEER1 outperforms ME-
TEOR on WMT12 data showing the effectiveness of
dense features in a learning-to-rank framework. The
metric and the code are available as free software2.
</bodyText>
<sectionHeader confidence="0.977055" genericHeader="introduction">
2 Model
</sectionHeader>
<bodyText confidence="0.99970575">
Our model is a linear combination of features trained
for ranking similar to RankSVM (Herbrich et al., 1999)
or, to readers familiar with SMT system tuning, to PRO
tuning (Hopkins and May, 2011):
</bodyText>
<equation confidence="0.996829">
score(sys) = w~ · ~xsys
</equation>
<bodyText confidence="0.9862595">
where w~ represents a weight vector and ~xsys a vec-
tor of feature values for system output sys. Look-
ing at evaluation as a ranking problem, we con-
trast (at least) two system translations good and
bad for the same source sentence. Assuming that
humanRank(good) &gt; humanRank(bad) as ranked
</bodyText>
<footnote confidence="0.9987738">
1BEER participated on WMT14 evaluation metrics task
where it was the highest scoring sentence level evaluation
metric on average over all language pairs (Stanojevi´c and
Sima’an, 2014)
2https://github.com/stanojevic/beer
</footnote>
<page confidence="0.954054">
202
</page>
<bodyText confidence="0.277552">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 202–206,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</bodyText>
<equation confidence="0.910354">
by human judgement, we expect metric score(.) to ful-
fillscore(good) &gt; score(bad):
w~• ~xgood &gt; w~ &apos; ~xbad &apos;�*
~xbad &gt; 0 &apos;#�
w~ &apos; (~xgood − ~xbad) &gt; 0 �
w~ &apos; (~xbad − ~xgood) &lt; 0
</equation>
<bodyText confidence="0.9989842">
The two feature vectors (~xgood − ~xbad) and (~xbad −
~xgood) can be considered as positive and negative in-
stances for training our linear classifier. For training
this model, we use Logistic Regression from the Weka
toolkit (Hall et al., 2009).
</bodyText>
<sectionHeader confidence="0.999481" genericHeader="method">
3 Features
</sectionHeader>
<bodyText confidence="0.998811823529412">
Generally speaking we identify adequacy and fluency
features. For both types we devise far less sparse fea-
tures than word ngrams.
Adequacy features We use precision P, recall R and
F1-score F as follows:
Pfunc, Rfunc, Ffunc on matched function words
Pcont, Rcont, Fcont on matched content words
Pall, Rall, Fall on matched words of any type
Pchar, Rchar, Fchar matching of the char ngrams
By differentiating between function and non-function
words, our metric weighs each kind of words accord-
ing to importance for evaluation. Matching character
ngrams, originally proposed in (Yang et al., 2013), re-
wards certain translations even if they did not get the
morphology completely right. Existing metrics use
stemmers for this, but using character ngrams is inde-
pendent of the availability of a good quality stemmer.
Higher-order character ngrams have less risk of sparse
counts than word ngrams. In our experiments we used
char ngrams for n up to 6, which makes the total num-
ber of adequacy features 27.
Fluency features To evaluate word order we follow
(Isozaki et al., 2010; Birch and Osborne, 2010) in rep-
resenting reordering as a permutation pr over [1..n] and
then measuring the distance to the ideal monotone per-
mutation (1, 2, • • • , n). We present a novel approach
based on factorization into permutation trees (PETs)
(Zhang and Gildea, 2007), and contrast it with Kendall
T (Birch and Osborne, 2010; Isozaki et al., 2010). PETs
are factorizations of permutations, which allows for an
abstract and less sparse view of word order as exempli-
fied next. Kendall score was regularly shown to have
high correlation with human judgment on distant lan-
guage pairs (Isozaki et al., 2010; Birch and Osborne,
2010).
Features based on PETs We informally review
PETs in order to exploit them for novel ordering fea-
tures. We refer the reader to (Zhang and Gildea, 2007)
and (Maillette de Buy Wenniger and Sima’an, 2011)
for a formal treatment of PETs and efficient factoriza-
tion algorithms.
A PET of permutation pr is a tree organization of pr’s
unique, atomic building blocks, called operators. Ev-
ery operator on a PET node is an atomic permutation
(not factorizing any further),3 and it stands for the per-
mutation of the direct children of that node. Figure 1a
shows an example PET that has one 4-branching node
with operator (2, 4,1, 3), two binary branching nodes
of which one decorated with the inverted operator (2, 1)
and another with the monotone (1, 2).
PETs have two important properties making them at-
tractive for measuring order difference: firstly, order
difference is measured on the operators – the atomic
reordering building blocks of the permutation, and sec-
ondly, the operators on higher level nodes capture hid-
den ordering patterns that cannot be observed without
factorization. Statistics over ordering patterns in PETs
are far less sparse than word or character ngram statis-
tics.
Intuitively, among the atomic permutations, the bi-
nary monotone operator (1, 2) signifies no ordering dif-
ference at all, whereas the binary inverted (2, 1) signi-
fies the shortest unit of order difference. Operators of
length four like (2, 4, 1, 3) (Wu, 1997) are presumably
more complex than (2, 1), whereas operators longer
than four signify even more complex order difference.
Therefore, we devise possible branching feature func-
tions over the operator length for the nodes in PETs:
</bodyText>
<listItem confidence="0.999662">
• factor 2 - with two features: A[ ]and A&lt;&gt; (there
are no nodes with factor 3 (Wu, 1997))
• factor 4 - feature A=4
• factor bigger than 4 - feature A&gt;4
</listItem>
<bodyText confidence="0.998866947368421">
Consider permutations (2, 1, 4, 3) and (4, 3, 2, 1), none
of which has exactly matching ngrams beyond uni-
grams. Their PETs are in Figures 1b and 1c. Intuitively,
(2, 1, 4, 3) is somewhat less scrambled than (4, 3, 2,1)
because it has at least some position in correct order.
These “abstract ngrams” pertaining to correct order-
ing of full phrases could be counted using A[ ] which
would recognize that on top of the PET in 1b there is
a binary monotone node, unlike the PET in Figure 1c
which has no monotone nodes at all.
Even though the set of operators that describe a per-
mutation is unique for the given permutation, the ways
in which operators are combined (the derivation tree)
is not unique. For example, for the fully monotone
3For example (2, 4, 1, 3) is atomic whereas (4, 3, 2, 1) is
not. The former does not contain any contiguous sub-ranges
of integers whereas the latter contains sub-range {2, 3, 41 in
reverse order (4, 3, 2), which factorizes into two binary in-
verting nodes cf. Fig. 1c.
</bodyText>
<figure confidence="0.972586666666667">
w~•
w~•
~xgood −
203
(2,1)
(2,1)
(2,1)
4 3
1
2
(2, 4,1, 3)
2 (2,1)
1 3
(2,1)
(2,1)
4
(1,2)
(1,2)
5 6 2 1 4 3 (c) Canonical fully
(a) Complex PET (b) PET with inversions inverted PET
(2,1)
(2,1)
4 (2,1)
3 2
1
(2,1)
4 (2,1)
(2,1)
3 2
1
(d) Alternative fully
inverted PET
(2,1)
(2,1) (2,1)
4 3 2 1
(e) Alternative fully
inverted PET
(f) Alternative fully
inverted PET
(2,1)
4 (2,1)
3 (2,1)
2 1
(g) Alternative fully
inverted PET
</figure>
<figureCaption confidence="0.999997">
Figure 1: Examples of PETs
</figureCaption>
<bodyText confidence="0.988337725">
permutation (4, 3, 2,1) there are 5 possible derivations
(PETs) presented in Figures 1c, 1d, 1e, 1f and 1g. The
features on PETs that we described so far look at the
operators independently (they treat a derivation as a
set of operators) so differenct derivations do not influ-
ence the score–whichever derviation we use we will
get the same feature score. However, the number of
derivations might say something about the goodness of
the permutation. Similar property of permutations was
found to be helpful earlier in (Mylonakis and Sima’an,
2008) as an ITG prior for learning translation rule prob-
abilities.
Permutations like (3, 2, 1, 4) and (2, 4, 3,1) have the
same set of operators, but the former factorizes into
more PETs than the latter because (4,3) must group
first before grouping it with 2 and then 1 in (2, 4, 3, 1).
The “freedom to bracket” in different ways could be a
signal of better grouping of words (even if they have
inverted word order). Hence we exploit one more fea-
ture:
Δcount the ratio between the number of alternative
PETs for the given permutation, to the number of
PETs that could be built if permutation was per-
fectly grouped (fully monotone or fully inverted).
Finding the number of PETs that could be built does
not require building all PETs or encoding them in the
chart. The number can be computed directly from the
canonical left-branching PET. Since multiple different
PETs appear only in cases when there is a sequence of
more than one node that is either (1, 2) or (2, 1) (Zhang
et al., 2008), we can use these sequences to predict the
number of PETs that could be built. Let X represent a
set of sequences of the canonical derivation. The num-
ber of PETs is computed in the following way:
where Cat(·) is a Catalan number. The proof for this
formula is beyond the scope of this paper. The reader
can consider the example of the PET in Figure 1c. That
derivation has one sequence of monotone operators of
length 3. So the number of PETs that could be built is
Cat(3) = 5.
</bodyText>
<sectionHeader confidence="0.998758" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.9737255">
We use human judgments from the WMT tasks:
WMT13 is used for training whereas WMT12 for test-
ing. The baseline is METEOR’s latest version (Michael
Denkowski and Alon Lavie, 2014), one of the best met-
rics on sentence level. To avoid contaminating the re-
sults with differences with METEOR due to resources,
we use the same alignment, tokenization and lower-
casing (-norm in METEOR) algorithms, and the same
tables of function words, synonyms, paraphrases and
stemmers.
Kendall τ correlation is borrowed from WMT12
(Callison-Burch et al., 2012):
</bodyText>
<equation confidence="0.8658685">
#concordant − #discordant − #ties
#concordant + #discordant + #ties
</equation>
<bodyText confidence="0.999506">
#concordant represents the number of pairs or-
dered in the same way by metric and by human,
#discordant the number of opposite orderings and
#ties the number of tied rankings by metric.
Beside testing our full metric BEER, we perform ex-
periments where we remove one kind of the following
features at a time:
</bodyText>
<listItem confidence="0.992315166666667">
1. char n-gram features (P, R and F-score)
2. all word features (P, R and F-score for all, function
and content words),
3. all function and content words features
4. all F-scores (all words, function words, content
words, char ngrams)
</listItem>
<equation confidence="0.9992638">
#PETs = 11 Cat(JxJ) (1)
X∈X
Cat(n) = 1/2n\ (2)
n + 1 (n)
τ =
</equation>
<page confidence="0.997603">
204
</page>
<table confidence="0.9984804">
metric en-cs en-fr en-de en-es cs-en fr-en de-en es-en avg T
BEER without char features 0.124 0.178 0.168 0.149 0.121 0.17 0.179 0.078 0.146
BEER without all word features 0.184 0.237 0.223 0.217 0.192 0.209 0.243 0.199 0.213
BEER without all F-scores 0.197 0.243 0.219 0.22 0.177 0.227 0.254 0.211 0.219
METEOR 0.156 0.252 0.173 0.202 0.208 0.249 0.273 0.246 0.22
BEER without PET features 0.202 0.248 0.243 0.225 0.198 0.249 0.268 0.234 0.233
BEER without function words 0.2 0.245 0.231 0.227 0.189 0.268 0.267 0.253 0.235
BEER without fluency features 0.201 0.248 0.236 0.223 0.202 0.257 0.283 0.243 0.237
BEER without Kendall T 0.205 0.246 0.244 0.227 0.202 0.257 0.282 0.248 0.239
BEER full 0.206 0.245 0.244 0.23 0.198 0.263 0.283 0.245 0.239
</table>
<tableCaption confidence="0.999543">
Table 1: Kendall T scores on WMT12 data
</tableCaption>
<listItem confidence="0.989267666666667">
5. PET features
6. Kendall T features
7. all fluency features (PET and Kendall T)
</listItem>
<tableCaption confidence="0.560706">
Table 1 shows the results sorted by their average
Kendall T correlation with human judgment.
</tableCaption>
<sectionHeader confidence="0.980769" genericHeader="evaluation">
5 Analysis
</sectionHeader>
<bodyText confidence="0.9999155">
Given these experimental results, we are coming back
to the questions we asked in the introduction.
</bodyText>
<subsectionHeader confidence="0.8597615">
5.1 What kind of features are more helpful for
sentence level evaluation?
</subsectionHeader>
<bodyText confidence="0.99984996969697">
Fluency vs. Adequacy The fluency features play a
smaller role than adequacy features. Apparently, many
SMT systems participating in this task have rather sim-
ilar reordering models, trained on similar data, which
makes the fluency features not that discriminative rel-
ative to adequacy features. Perhaps in a different ap-
plication, for example MT system tuning, the reorder-
ing features would be far more relevant because ignor-
ing them would basically imply disregarding the im-
portance of the reordering model in MT.
Character vs. Word features We observe that, pre-
cision, recall and F-score on character ngrams are cru-
cial. We think that this shows that less sparse features
are important for sentence level evaluation. The sec-
ond best features are word features. Without word
features, BEER scores just below METEOR, which
suggests that word boundaries play a role as well. In
contrast, differentiating between function and content
words does not seem to be important.
PETs vs. Kendall T Despite the smaller role for
reordering features we can make a few observations.
Firstly, while PETs and Kendall seem to have simi-
lar effect on English-Foreign cases, in all four cases of
Foreign-English PETs give better scores. We hypoth-
esize that the quality of the permutations (induced be-
tween system output and reference) is better for English
than for the other target languages. Discarding PET
features has far larger impact than discarding Kendall.
Most interestingly, for de-en it makes the difference
in outperforming METEOR. In many cases discarding
Kendall T improves the BEER score, likely because it
conflicts with the PET features that are found more ef-
fective.
</bodyText>
<subsectionHeader confidence="0.999836">
5.2 Is a linear model sufficient?
</subsectionHeader>
<bodyText confidence="0.998083904761904">
A further insight, from our perspective, is that F-score
features constitute a crucial set of features, even when
the corresponding precision and recall features are in-
cluded. Because our model merely allows for linear in-
terpolation, whereas F-score is a non-linear function of
precision and recall, we think this suggests that a non-
linear interpolation of precision and recall is useful.4
By formulating the evaluation as a ranking problem it is
relatively easy to “upgrade” for using non-linear mod-
els while using the same (or larger) set of features.
5.3 Train for the language pair or only for the
target language?
All our models were trained for each language pair.
This is not the case with many other metrics which
train their models for each target language instead of
language pair. We contrast these two settings in Table
2. Training for each language pair separately does not
give significant improvement over training for the tar-
get language only. A possible reason could be that by
training for the target language we have more training
data (in this case four times more).
</bodyText>
<table confidence="0.687961">
Train for cs-en fr-en de-en es-en avg T
target lang 0.199 0.257 0.273 0.248 0.244
lang pair 0.198 0.263 0.283 0.245 0.247
</table>
<tableCaption confidence="0.974219">
Table 2: Kendall T scores on WMT12 for different
training data
</tableCaption>
<subsectionHeader confidence="0.966958">
5.4 BEER vs. METEOR
</subsectionHeader>
<bodyText confidence="0.999829714285714">
The results across individual language pairs are mostly
consistent with the averages with a few exceptions.
BEER outperforms METEOR in five out of eight lan-
guage pairs, ties at one (the difference is only 0.001 on
es-en) and loses in two (en-fr and cs-en). In some cases
BEER is better than METEOR by a large margin (see,
e.g., en-cs, en-de).
</bodyText>
<footnote confidence="0.453717">
4Interestingly, METEOR tunes β in Fβ.
</footnote>
<page confidence="0.998164">
205
</page>
<sectionHeader confidence="0.998794" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999948785714286">
In this work we show that combining less sparse fea-
tures at the sentence level into a linear model that is
trained on ranking we can obtain state-of-the-art re-
sults. The analysis of the results shows that features on
character ngrams are crucial, besides the standard word
level features. The reordering features, while rather
important, are less effective within this WMT task, al-
beit the more abstract PET features have larger impact
than the often used Kendall. Good performance of F-
score features leads to the conclusion that linear models
might not be sufficient for modeling human sentence
level ranking and to learn the right relation between
precision and recall it could be worthwhile exploring
non-linear models.
</bodyText>
<sectionHeader confidence="0.998561" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.997486333333333">
This work is supported by STW grant nr. 12271 and
NWO VICI grant nr. 277-89-002. We also thank TAUS
and the other DatAptor project User Board members.
</bodyText>
<sectionHeader confidence="0.997417" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.996691673913044">
Alexandra Birch and Miles Osborne. 2010. LRscore
for Evaluating Lexical and Reordering Quality in
MT. In Proceedings of the Joint Fifth Workshop on
Statistical Machine Translation and MetricsMATR,
pages 327–332, Uppsala, Sweden, July. Association
for Computational Linguistics.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 Workshop on Statistical Ma-
chine Translation. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, pages
10–51, Montr´eal, Canada, June. Association for
Computational Linguistics.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA Data Mining Software: An
Update. SIGKDD Explor. Newsl., 11(1):10–18,
November.
Ralf Herbrich, Thore Graepel, and Klaus Obermayer.
1999. Support Vector Learning for Ordinal Regres-
sion. In In International Conference on Artificial
Neural Networks, pages 97–102.
Mark Hopkins and Jonathan May. 2011. Tuning as
Ranking. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Processing,
pages 1352–1362, Edinburgh, Scotland, UK., July.
Association for Computational Linguistics.
Hideki Isozaki, Tsutomu Hirao, Kevin Duh, Katsuhito
Sudoh, and Hajime Tsukada. 2010. Automatic
Evaluation of Translation Quality for Distant Lan-
guage Pairs. In Proceedings of the 2010 Conference
on Empirical Methods in Natural Language Process-
ing, EMNLP ’10, pages 944–952, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Hang Li. 2011. Learning to Rank for Information Re-
trieval and Natural Language Processing. Synthesis
Lectures on Human Language Technologies. Mor-
gan &amp; Claypool Publishers.
Gideon Maillette de Buy Wenniger and Khalil Sima’an.
2011. Hierarchical Translation Equivalence over
Word Alignments. In ILLC Prepublication Series,
PP-2011-38. University of Amsterdam.
Michael Denkowski and Alon Lavie. 2014. Meteor
Universal: Language Specific Translation Evalua-
tion for Any Target Language. In Proceedings of the
ACL 2014 Workshop on Statistical Machine Transla-
tion.
Markos Mylonakis and Khalil Sima’an. 2008.
Phrase Translation Probabilities with ITG Priors and
Smoothing as Learning Objective. In Proceedings of
the 2008 Conference on Empirical Methods in Natu-
ral Language Processing, pages 630–639, Honolulu,
USA, October. Association for Computational Lin-
guistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A Method for Automatic
Evaluation of Machine Translation. In Proceedings
of the 40th Annual Meeting on Association for Com-
putational Linguistics, ACL ’02, pages 311–318,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Miloˇs Stanojevi´c and Khalil Sima’an. 2014. BEER:
BEtter Evaluation as Ranking. In Proceedings of the
Ninth Workshop on Statistical Machine Translation,
pages 414–419, Baltimore, Maryland, USA, June.
Association for Computational Linguistics.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational linguistics, 23(3):377–403.
Muyun Yang, Junguo Zhu, Sheng Li, and Tiejun Zhao.
2013. Fusion of Word and Letter Based Metrics
for Automatic MT Evaluation. In Proceedings of
the Twenty-Third International Joint Conference on
Artificial Intelligence, IJCAI’13, pages 2204–2210.
AAAI Press.
Yang Ye, Ming Zhou, and Chin-Yew Lin. 2007. Sen-
tence Level Machine Translation Evaluation As a
Ranking Problem: One Step Aside from BLEU. In
Proceedings of the Second Workshop on Statistical
Machine Translation, StatMT ’07, pages 240–247,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Hao Zhang and Daniel Gildea. 2007. Factorization of
synchronous context-free grammars in linear time.
In In NAACL Workshop on Syntax and Structure in
Statistical Translation (SSST.
Hao Zhang, Daniel Gildea, and David Chiang.
2008. Extracting Synchronous Grammar Rules
From Word-Level Alignments in Linear Time. In
Proceedings of the 22nd International Conference
on Computational Linguistics (COLING-08), pages
1081–1088, Manchester, UK.
</reference>
<page confidence="0.998894">
206
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.762275">
<title confidence="0.9985435">Fitting Sentence Level Translation with Many Dense Features</title>
<author confidence="0.943686">Stanojevi´c</author>
<affiliation confidence="0.9935985">Institute for Logic, Language and University of</affiliation>
<address confidence="0.821383">Science Park 107, 1098 XG Amsterdam, The</address>
<email confidence="0.979401">m.stanojevic@uva.nl</email>
<email confidence="0.979401">k.simaan@uva.nl</email>
<abstract confidence="0.999520777777778">Sentence level evaluation in MT has turned out far more difficult than corpus level evaluation. Existing sentence level metrics employ a limited set of features, most of which are rather sparse at the sentence level, and their intricate models are rarely trained for ranking. This paper presents a simple linear model exploiting 33 relatively dense features, some of which are novel while others are known but seldom used, train it under the framework. We evaluate our metric on the standard WMT12 data showing that it outperforms the strong baseline METEOR. We also analyze the contribution of individual features and the choice of training data, language-pair vs. target-language data, providing new insights into this task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Alexandra Birch</author>
<author>Miles Osborne</author>
</authors>
<title>LRscore for Evaluating Lexical and Reordering Quality in MT.</title>
<date>2010</date>
<booktitle>In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR,</booktitle>
<pages>327--332</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="5975" citStr="Birch and Osborne, 2010" startWordPosition="956" endWordPosition="959">nd of words according to importance for evaluation. Matching character ngrams, originally proposed in (Yang et al., 2013), rewards certain translations even if they did not get the morphology completely right. Existing metrics use stemmers for this, but using character ngrams is independent of the availability of a good quality stemmer. Higher-order character ngrams have less risk of sparse counts than word ngrams. In our experiments we used char ngrams for n up to 6, which makes the total number of adequacy features 27. Fluency features To evaluate word order we follow (Isozaki et al., 2010; Birch and Osborne, 2010) in representing reordering as a permutation pr over [1..n] and then measuring the distance to the ideal monotone permutation (1, 2, • • • , n). We present a novel approach based on factorization into permutation trees (PETs) (Zhang and Gildea, 2007), and contrast it with Kendall T (Birch and Osborne, 2010; Isozaki et al., 2010). PETs are factorizations of permutations, which allows for an abstract and less sparse view of word order as exemplified next. Kendall score was regularly shown to have high correlation with human judgment on distant language pairs (Isozaki et al., 2010; Birch and Osbo</context>
</contexts>
<marker>Birch, Osborne, 2010</marker>
<rawString>Alexandra Birch and Miles Osborne. 2010. LRscore for Evaluating Lexical and Reordering Quality in MT. In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR, pages 327–332, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Matt Post</author>
<author>Radu Soricut</author>
<author>Lucia Specia</author>
</authors>
<date>2012</date>
<booktitle>Findings of the 2012 Workshop on Statistical Machine Translation. In Proceedings of the Seventh Workshop on Statistical Machine Translation,</booktitle>
<pages>10--51</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montr´eal, Canada,</location>
<contexts>
<context position="12424" citStr="Callison-Burch et al., 2012" startWordPosition="2097" endWordPosition="2100">tors of length 3. So the number of PETs that could be built is Cat(3) = 5. 4 Experiments We use human judgments from the WMT tasks: WMT13 is used for training whereas WMT12 for testing. The baseline is METEOR’s latest version (Michael Denkowski and Alon Lavie, 2014), one of the best metrics on sentence level. To avoid contaminating the results with differences with METEOR due to resources, we use the same alignment, tokenization and lowercasing (-norm in METEOR) algorithms, and the same tables of function words, synonyms, paraphrases and stemmers. Kendall τ correlation is borrowed from WMT12 (Callison-Burch et al., 2012): #concordant − #discordant − #ties #concordant + #discordant + #ties #concordant represents the number of pairs ordered in the same way by metric and by human, #discordant the number of opposite orderings and #ties the number of tied rankings by metric. Beside testing our full metric BEER, we perform experiments where we remove one kind of the following features at a time: 1. char n-gram features (P, R and F-score) 2. all word features (P, R and F-score for all, function and content words), 3. all function and content words features 4. all F-scores (all words, function words, content words, c</context>
</contexts>
<marker>Callison-Burch, Koehn, Monz, Post, Soricut, Specia, 2012</marker>
<rawString>Chris Callison-Burch, Philipp Koehn, Christof Monz, Matt Post, Radu Soricut, and Lucia Specia. 2012. Findings of the 2012 Workshop on Statistical Machine Translation. In Proceedings of the Seventh Workshop on Statistical Machine Translation, pages 10–51, Montr´eal, Canada, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hall</author>
<author>Eibe Frank</author>
<author>Geoffrey Holmes</author>
<author>Bernhard Pfahringer</author>
<author>Peter Reutemann</author>
<author>Ian H Witten</author>
</authors>
<title>The WEKA Data Mining Software: An Update.</title>
<date>2009</date>
<journal>SIGKDD Explor. Newsl.,</journal>
<volume>11</volume>
<issue>1</issue>
<contexts>
<context position="4864" citStr="Hall et al., 2009" startWordPosition="774" endWordPosition="777">2 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 202–206, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics by human judgement, we expect metric score(.) to fulfillscore(good) &gt; score(bad): w~• ~xgood &gt; w~ &apos; ~xbad &apos;�* ~xbad &gt; 0 &apos;#� w~ &apos; (~xgood − ~xbad) &gt; 0 � w~ &apos; (~xbad − ~xgood) &lt; 0 The two feature vectors (~xgood − ~xbad) and (~xbad − ~xgood) can be considered as positive and negative instances for training our linear classifier. For training this model, we use Logistic Regression from the Weka toolkit (Hall et al., 2009). 3 Features Generally speaking we identify adequacy and fluency features. For both types we devise far less sparse features than word ngrams. Adequacy features We use precision P, recall R and F1-score F as follows: Pfunc, Rfunc, Ffunc on matched function words Pcont, Rcont, Fcont on matched content words Pall, Rall, Fall on matched words of any type Pchar, Rchar, Fchar matching of the char ngrams By differentiating between function and non-function words, our metric weighs each kind of words according to importance for evaluation. Matching character ngrams, originally proposed in (Yang et al</context>
</contexts>
<marker>Hall, Frank, Holmes, Pfahringer, Reutemann, Witten, 2009</marker>
<rawString>Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann, and Ian H. Witten. 2009. The WEKA Data Mining Software: An Update. SIGKDD Explor. Newsl., 11(1):10–18, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralf Herbrich</author>
<author>Thore Graepel</author>
<author>Klaus Obermayer</author>
</authors>
<title>Support Vector Learning for Ordinal Regression. In</title>
<date>1999</date>
<booktitle>In International Conference on Artificial Neural Networks,</booktitle>
<pages>97--102</pages>
<contexts>
<context position="2776" citStr="Herbrich et al., 1999" startWordPosition="428" endWordPosition="431"> what features and what training objective are better suited for modelling sentence level evaluation. We start out by explicitly formulating sentence level evaluation as the problem of ranking a set of competing hypothesis. Given data consisting of human ranked system outputs, the problem then is to formulate an easy to train model for ranking. One particular existing approach (Ye et al., 2007) looks especially attractive because we think it meshes well with a range of effective techniques for learning-to-rank (Li, 2011). We deliberately select a linear modelling approach inspired by RankSVM (Herbrich et al., 1999), which is easily trainable for ranking and allows analysis of the individual contributions of features. Besides presenting a new metric and a set of known, but also a set of novel features, we target three questions of interest to the MT community: • What kind of features are more helpful for sentence level evaluation? • How does a simple linear model trained for ranking compare to the well-developed metric METEOR on sentence level evaluation? • Should we train the model for each language pair separately or for a target language? Our new metric dubbed BEER1 outperforms METEOR on WMT12 data sh</context>
</contexts>
<marker>Herbrich, Graepel, Obermayer, 1999</marker>
<rawString>Ralf Herbrich, Thore Graepel, and Klaus Obermayer. 1999. Support Vector Learning for Ordinal Regression. In In International Conference on Artificial Neural Networks, pages 97–102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hopkins</author>
<author>Jonathan May</author>
</authors>
<title>Tuning as Ranking.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1352--1362</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Edinburgh, Scotland, UK.,</location>
<contexts>
<context position="3710" citStr="Hopkins and May, 2011" startWordPosition="587" endWordPosition="590">aluation? • How does a simple linear model trained for ranking compare to the well-developed metric METEOR on sentence level evaluation? • Should we train the model for each language pair separately or for a target language? Our new metric dubbed BEER1 outperforms METEOR on WMT12 data showing the effectiveness of dense features in a learning-to-rank framework. The metric and the code are available as free software2. 2 Model Our model is a linear combination of features trained for ranking similar to RankSVM (Herbrich et al., 1999) or, to readers familiar with SMT system tuning, to PRO tuning (Hopkins and May, 2011): score(sys) = w~ · ~xsys where w~ represents a weight vector and ~xsys a vector of feature values for system output sys. Looking at evaluation as a ranking problem, we contrast (at least) two system translations good and bad for the same source sentence. Assuming that humanRank(good) &gt; humanRank(bad) as ranked 1BEER participated on WMT14 evaluation metrics task where it was the highest scoring sentence level evaluation metric on average over all language pairs (Stanojevi´c and Sima’an, 2014) 2https://github.com/stanojevic/beer 202 Proceedings of the 2014 Conference on Empirical Methods in Nat</context>
</contexts>
<marker>Hopkins, May, 2011</marker>
<rawString>Mark Hopkins and Jonathan May. 2011. Tuning as Ranking. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1352–1362, Edinburgh, Scotland, UK., July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hideki Isozaki</author>
<author>Tsutomu Hirao</author>
<author>Kevin Duh</author>
<author>Katsuhito Sudoh</author>
<author>Hajime Tsukada</author>
</authors>
<title>Automatic Evaluation of Translation Quality for Distant Language Pairs.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP ’10,</booktitle>
<pages>944--952</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="5949" citStr="Isozaki et al., 2010" startWordPosition="952" endWordPosition="955"> metric weighs each kind of words according to importance for evaluation. Matching character ngrams, originally proposed in (Yang et al., 2013), rewards certain translations even if they did not get the morphology completely right. Existing metrics use stemmers for this, but using character ngrams is independent of the availability of a good quality stemmer. Higher-order character ngrams have less risk of sparse counts than word ngrams. In our experiments we used char ngrams for n up to 6, which makes the total number of adequacy features 27. Fluency features To evaluate word order we follow (Isozaki et al., 2010; Birch and Osborne, 2010) in representing reordering as a permutation pr over [1..n] and then measuring the distance to the ideal monotone permutation (1, 2, • • • , n). We present a novel approach based on factorization into permutation trees (PETs) (Zhang and Gildea, 2007), and contrast it with Kendall T (Birch and Osborne, 2010; Isozaki et al., 2010). PETs are factorizations of permutations, which allows for an abstract and less sparse view of word order as exemplified next. Kendall score was regularly shown to have high correlation with human judgment on distant language pairs (Isozaki et</context>
</contexts>
<marker>Isozaki, Hirao, Duh, Sudoh, Tsukada, 2010</marker>
<rawString>Hideki Isozaki, Tsutomu Hirao, Kevin Duh, Katsuhito Sudoh, and Hajime Tsukada. 2010. Automatic Evaluation of Translation Quality for Distant Language Pairs. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP ’10, pages 944–952, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hang Li</author>
</authors>
<title>Learning to Rank for Information Retrieval and Natural Language Processing. Synthesis Lectures on Human Language Technologies.</title>
<date>2011</date>
<publisher>Morgan &amp; Claypool Publishers.</publisher>
<contexts>
<context position="2680" citStr="Li, 2011" startWordPosition="416" endWordPosition="417">d be treated as a modelling challenge. This calls for questions such as what model, what features and what training objective are better suited for modelling sentence level evaluation. We start out by explicitly formulating sentence level evaluation as the problem of ranking a set of competing hypothesis. Given data consisting of human ranked system outputs, the problem then is to formulate an easy to train model for ranking. One particular existing approach (Ye et al., 2007) looks especially attractive because we think it meshes well with a range of effective techniques for learning-to-rank (Li, 2011). We deliberately select a linear modelling approach inspired by RankSVM (Herbrich et al., 1999), which is easily trainable for ranking and allows analysis of the individual contributions of features. Besides presenting a new metric and a set of known, but also a set of novel features, we target three questions of interest to the MT community: • What kind of features are more helpful for sentence level evaluation? • How does a simple linear model trained for ranking compare to the well-developed metric METEOR on sentence level evaluation? • Should we train the model for each language pair sepa</context>
</contexts>
<marker>Li, 2011</marker>
<rawString>Hang Li. 2011. Learning to Rank for Information Retrieval and Natural Language Processing. Synthesis Lectures on Human Language Technologies. Morgan &amp; Claypool Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gideon Maillette de Buy Wenniger</author>
<author>Khalil Sima’an</author>
</authors>
<title>Hierarchical Translation Equivalence over Word Alignments.</title>
<date>2011</date>
<booktitle>In ILLC Prepublication Series,</booktitle>
<pages>2011--38</pages>
<institution>University of Amsterdam.</institution>
<marker>Wenniger, Sima’an, 2011</marker>
<rawString>Gideon Maillette de Buy Wenniger and Khalil Sima’an. 2011. Hierarchical Translation Equivalence over Word Alignments. In ILLC Prepublication Series, PP-2011-38. University of Amsterdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Denkowski</author>
<author>Alon Lavie</author>
</authors>
<title>Meteor Universal: Language Specific Translation Evaluation for Any Target Language.</title>
<date>2014</date>
<booktitle>In Proceedings of the ACL 2014 Workshop on Statistical Machine Translation.</booktitle>
<marker>Denkowski, Lavie, 2014</marker>
<rawString>Michael Denkowski and Alon Lavie. 2014. Meteor Universal: Language Specific Translation Evaluation for Any Target Language. In Proceedings of the ACL 2014 Workshop on Statistical Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markos Mylonakis</author>
<author>Khalil Sima’an</author>
</authors>
<title>Phrase Translation Probabilities with ITG Priors and Smoothing as Learning Objective.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>630--639</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Honolulu, USA,</location>
<marker>Mylonakis, Sima’an, 2008</marker>
<rawString>Markos Mylonakis and Khalil Sima’an. 2008. Phrase Translation Probabilities with ITG Priors and Smoothing as Learning Objective. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 630–639, Honolulu, USA, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: A Method for Automatic Evaluation of Machine Translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL ’02,</booktitle>
<pages>311--318</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1444" citStr="Papineni et al., 2002" startWordPosition="216" endWordPosition="219">s the strong baseline METEOR. We also analyze the contribution of individual features and the choice of training data, language-pair vs. target-language data, providing new insights into this task. 1 Introduction Evaluating machine translation (MT) output at the sentence/ segment level has turned out far more challenging than corpus/ system level. Yet, sentence level evaluation can be useful because it allows fast, finegrained analysis of system performance on individual sentences. It is instructive to contrast two widely used metrics, METEOR (Michael Denkowski and Alon Lavie, 2014) and BLEU (Papineni et al., 2002), on sentence level evaluation. METEOR constantly shows better correlation with human ranking than BLEU (Papineni et al., 2002). Arguably, this shows that sentence level evaluation demands finer grained and trainable models over less sparse features. Ngrams, the core of BLEU, are sparse at the sentence level, and a mismatch for longer ngrams implies that BLEU falls back on shorter ngrams. In contrast, METEOR has a trainable model and incorporates a small, yet wider set of features that are less sparse than ngrams. We think that METEOR’s features and its training approach only suggest that sent</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: A Method for Automatic Evaluation of Machine Translation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL ’02, pages 311–318, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Miloˇs Stanojevi´c</author>
<author>Khalil Sima’an</author>
</authors>
<title>BEER: BEtter Evaluation as Ranking.</title>
<date>2014</date>
<booktitle>In Proceedings of the Ninth Workshop on Statistical Machine Translation,</booktitle>
<pages>414--419</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, Maryland, USA,</location>
<marker>Stanojevi´c, Sima’an, 2014</marker>
<rawString>Miloˇs Stanojevi´c and Khalil Sima’an. 2014. BEER: BEtter Evaluation as Ranking. In Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 414–419, Baltimore, Maryland, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
<date>1997</date>
<journal>Computational linguistics,</journal>
<pages>23--3</pages>
<contexts>
<context position="8027" citStr="Wu, 1997" startWordPosition="1302" endWordPosition="1303">e: firstly, order difference is measured on the operators – the atomic reordering building blocks of the permutation, and secondly, the operators on higher level nodes capture hidden ordering patterns that cannot be observed without factorization. Statistics over ordering patterns in PETs are far less sparse than word or character ngram statistics. Intuitively, among the atomic permutations, the binary monotone operator (1, 2) signifies no ordering difference at all, whereas the binary inverted (2, 1) signifies the shortest unit of order difference. Operators of length four like (2, 4, 1, 3) (Wu, 1997) are presumably more complex than (2, 1), whereas operators longer than four signify even more complex order difference. Therefore, we devise possible branching feature functions over the operator length for the nodes in PETs: • factor 2 - with two features: A[ ]and A&lt;&gt; (there are no nodes with factor 3 (Wu, 1997)) • factor 4 - feature A=4 • factor bigger than 4 - feature A&gt;4 Consider permutations (2, 1, 4, 3) and (4, 3, 2, 1), none of which has exactly matching ngrams beyond unigrams. Their PETs are in Figures 1b and 1c. Intuitively, (2, 1, 4, 3) is somewhat less scrambled than (4, 3, 2,1) be</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>Dekai Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational linguistics, 23(3):377–403.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Muyun Yang</author>
<author>Junguo Zhu</author>
<author>Sheng Li</author>
<author>Tiejun Zhao</author>
</authors>
<title>Fusion of Word and Letter Based Metrics for Automatic MT Evaluation.</title>
<date>2013</date>
<booktitle>In Proceedings of the Twenty-Third International Joint Conference on Artificial Intelligence, IJCAI’13,</booktitle>
<pages>2204--2210</pages>
<publisher>AAAI Press.</publisher>
<contexts>
<context position="5472" citStr="Yang et al., 2013" startWordPosition="871" endWordPosition="874">al., 2009). 3 Features Generally speaking we identify adequacy and fluency features. For both types we devise far less sparse features than word ngrams. Adequacy features We use precision P, recall R and F1-score F as follows: Pfunc, Rfunc, Ffunc on matched function words Pcont, Rcont, Fcont on matched content words Pall, Rall, Fall on matched words of any type Pchar, Rchar, Fchar matching of the char ngrams By differentiating between function and non-function words, our metric weighs each kind of words according to importance for evaluation. Matching character ngrams, originally proposed in (Yang et al., 2013), rewards certain translations even if they did not get the morphology completely right. Existing metrics use stemmers for this, but using character ngrams is independent of the availability of a good quality stemmer. Higher-order character ngrams have less risk of sparse counts than word ngrams. In our experiments we used char ngrams for n up to 6, which makes the total number of adequacy features 27. Fluency features To evaluate word order we follow (Isozaki et al., 2010; Birch and Osborne, 2010) in representing reordering as a permutation pr over [1..n] and then measuring the distance to th</context>
</contexts>
<marker>Yang, Zhu, Li, Zhao, 2013</marker>
<rawString>Muyun Yang, Junguo Zhu, Sheng Li, and Tiejun Zhao. 2013. Fusion of Word and Letter Based Metrics for Automatic MT Evaluation. In Proceedings of the Twenty-Third International Joint Conference on Artificial Intelligence, IJCAI’13, pages 2204–2210. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Ye</author>
<author>Ming Zhou</author>
<author>Chin-Yew Lin</author>
</authors>
<title>Sentence Level Machine Translation Evaluation As a Ranking Problem: One Step Aside from BLEU.</title>
<date>2007</date>
<booktitle>In Proceedings of the Second Workshop on Statistical Machine Translation, StatMT ’07,</booktitle>
<pages>240--247</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="2551" citStr="Ye et al., 2007" startWordPosition="394" endWordPosition="397">are less sparse than ngrams. We think that METEOR’s features and its training approach only suggest that sentence level evaluation should be treated as a modelling challenge. This calls for questions such as what model, what features and what training objective are better suited for modelling sentence level evaluation. We start out by explicitly formulating sentence level evaluation as the problem of ranking a set of competing hypothesis. Given data consisting of human ranked system outputs, the problem then is to formulate an easy to train model for ranking. One particular existing approach (Ye et al., 2007) looks especially attractive because we think it meshes well with a range of effective techniques for learning-to-rank (Li, 2011). We deliberately select a linear modelling approach inspired by RankSVM (Herbrich et al., 1999), which is easily trainable for ranking and allows analysis of the individual contributions of features. Besides presenting a new metric and a set of known, but also a set of novel features, we target three questions of interest to the MT community: • What kind of features are more helpful for sentence level evaluation? • How does a simple linear model trained for ranking </context>
</contexts>
<marker>Ye, Zhou, Lin, 2007</marker>
<rawString>Yang Ye, Ming Zhou, and Chin-Yew Lin. 2007. Sentence Level Machine Translation Evaluation As a Ranking Problem: One Step Aside from BLEU. In Proceedings of the Second Workshop on Statistical Machine Translation, StatMT ’07, pages 240–247, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hao Zhang</author>
<author>Daniel Gildea</author>
</authors>
<title>Factorization of synchronous context-free grammars in linear time.</title>
<date>2007</date>
<booktitle>In In NAACL Workshop on Syntax and Structure in Statistical Translation (SSST.</booktitle>
<contexts>
<context position="6225" citStr="Zhang and Gildea, 2007" startWordPosition="1000" endWordPosition="1003">ut using character ngrams is independent of the availability of a good quality stemmer. Higher-order character ngrams have less risk of sparse counts than word ngrams. In our experiments we used char ngrams for n up to 6, which makes the total number of adequacy features 27. Fluency features To evaluate word order we follow (Isozaki et al., 2010; Birch and Osborne, 2010) in representing reordering as a permutation pr over [1..n] and then measuring the distance to the ideal monotone permutation (1, 2, • • • , n). We present a novel approach based on factorization into permutation trees (PETs) (Zhang and Gildea, 2007), and contrast it with Kendall T (Birch and Osborne, 2010; Isozaki et al., 2010). PETs are factorizations of permutations, which allows for an abstract and less sparse view of word order as exemplified next. Kendall score was regularly shown to have high correlation with human judgment on distant language pairs (Isozaki et al., 2010; Birch and Osborne, 2010). Features based on PETs We informally review PETs in order to exploit them for novel ordering features. We refer the reader to (Zhang and Gildea, 2007) and (Maillette de Buy Wenniger and Sima’an, 2011) for a formal treatment of PETs and ef</context>
</contexts>
<marker>Zhang, Gildea, 2007</marker>
<rawString>Hao Zhang and Daniel Gildea. 2007. Factorization of synchronous context-free grammars in linear time. In In NAACL Workshop on Syntax and Structure in Statistical Translation (SSST.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hao Zhang</author>
<author>Daniel Gildea</author>
<author>David Chiang</author>
</authors>
<title>Extracting Synchronous Grammar Rules From Word-Level Alignments in Linear Time.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics (COLING-08),</booktitle>
<pages>1081--1088</pages>
<location>Manchester, UK.</location>
<contexts>
<context position="11392" citStr="Zhang et al., 2008" startWordPosition="1917" endWordPosition="1920"> words (even if they have inverted word order). Hence we exploit one more feature: Δcount the ratio between the number of alternative PETs for the given permutation, to the number of PETs that could be built if permutation was perfectly grouped (fully monotone or fully inverted). Finding the number of PETs that could be built does not require building all PETs or encoding them in the chart. The number can be computed directly from the canonical left-branching PET. Since multiple different PETs appear only in cases when there is a sequence of more than one node that is either (1, 2) or (2, 1) (Zhang et al., 2008), we can use these sequences to predict the number of PETs that could be built. Let X represent a set of sequences of the canonical derivation. The number of PETs is computed in the following way: where Cat(·) is a Catalan number. The proof for this formula is beyond the scope of this paper. The reader can consider the example of the PET in Figure 1c. That derivation has one sequence of monotone operators of length 3. So the number of PETs that could be built is Cat(3) = 5. 4 Experiments We use human judgments from the WMT tasks: WMT13 is used for training whereas WMT12 for testing. The baseli</context>
</contexts>
<marker>Zhang, Gildea, Chiang, 2008</marker>
<rawString>Hao Zhang, Daniel Gildea, and David Chiang. 2008. Extracting Synchronous Grammar Rules From Word-Level Alignments in Linear Time. In Proceedings of the 22nd International Conference on Computational Linguistics (COLING-08), pages 1081–1088, Manchester, UK.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
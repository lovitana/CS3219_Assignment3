<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000004">
<title confidence="0.92749">
Predicting Chinese Abbreviations with Minimum Semantic Unit and
Global Constraints
</title>
<author confidence="0.952591">
Longkai Zhang Li Li Houfeng Wang Xu Sun
</author>
<affiliation confidence="0.9138905">
Key Laboratory of Computational Linguistics (Peking University)
Ministry of Education, China
</affiliation>
<email confidence="0.995167">
zhlongk@qq.com, {li.l,wanghf,xusun}@pku.edu.cn
</email>
<sectionHeader confidence="0.994732" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99997875">
We propose a new Chinese abbreviation
prediction method which can incorporate
rich local information while generating the
abbreviation globally. Different to previ-
ous character tagging methods, we intro-
duce the minimum semantic unit, which is
more fine-grained than character but more
coarse-grained than word, to capture word
level information in the sequence labeling
framework. To solve the “character dupli-
cation” problem in Chinese abbreviation
prediction, we also use a substring tagging
strategy to generate local substring tagging
candidates. We use an integer linear pro-
gramming (ILP) formulation with various
constraints to globally decode the final ab-
breviation from the generated candidates.
Experiments show that our method outper-
forms the state-of-the-art systems, without
using any extra resource.
</bodyText>
<sectionHeader confidence="0.998783" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999988982758621">
Abbreviation is defined as a shortened description
of the original fully expanded form. For example,
“NLP” is the abbreviation for the corresponding
full form “Natural Language Processing”. The ex-
istence of abbreviations makes it difficult to iden-
tify the terms conveying the same concept in the
information retrieval (IR) systems and machine
translation (MT) systems. Therefore, it is impor-
tant to maintain a dictionary of the prevalent orig-
inal full forms and the corresponding abbrevia-
tions.
Previous works on Chinese abbreviation gen-
eration focus on the sequence labeling method,
which give each character in the full form an extra
label to indicate whether it is kept in the abbre-
viation. One drawback of the character tagging
strategy is that Chinese characters only contain
limited amount of information. Using character-
based method alone is not enough for Chinese ab-
breviation generation. Intuitively we can think of a
word as the basic tagging unit to incorporate more
information. However, if the basic tagging unit
is word, we need to design lots of tags to repre-
sent which characters are kept for each unit. For a
word with n characters, we should design at least
2n labels to cover all possible situations. This re-
duces the generalization ability of the proposed
model. Besides, the Chinese word segmentation
errors may also hurt the performance. Therefore
we propose the idea of “Minimum Semantic Unit”
(MSU) which is the minimum semantic unit in
Chinese language. Some of the MSUs are words,
while others are more fine-grained than words.
The task of selecting representative characters in
the full form can be further broken down into se-
lecting representative characters in the MSUs. We
model this using the MSU-based tagging method,
which can both utilize semantic information while
keeping the tag set small.
Meanwhile, the sequence labeling method per-
forms badly when the “character duplication” phe-
nomenon exists. Many Chinese long phrases con-
tain duplicated characters, which we refer to as
the “character duplication” phenomenon. There is
no sound criterion for the character tagging mod-
els to decide which of the duplicated character
should be kept in the abbreviation and which one
to be skipped. An example is “��Jrn�Jrn��
�”(Beijing University of Aeronautics and Astro-
nautics) whose abbreviation is “ZJrn”. The char-
acter “Jrn” appears twice in the full form and only
one is kept in the abbreviation. In these cases, we
can break the long phase into local substrings. We
can find the representative characters in the sub-
strings instead of the long full form and let the de-
coding phase to integrate useful information glob-
ally. We utilize this sub-string based approach and
obtain this local tagging information by labeling
</bodyText>
<page confidence="0.925652">
1405
</page>
<note confidence="0.897192">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1405–1414,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.99981159375">
on the sub-string of the full character sequence.
Given the MSU-based and substring-based
methods mentioned above, we can get a list of
potential abbreviation candidates. Some of these
candidates may not agree on keeping or skipping
of some specific characters. To integrate their ad-
vantages while considering the consistency, we
further propose a global decoding strategy using
Integer Linear Programming(ILP). The constraints
in ILP can naturally incorporate ‘non-local’ infor-
mation in contrast to probabilistic constraints that
are estimated from training examples. We can also
use linguistic constraints like “adjacent identical
characters is not allowed” to decode the correct
abbreviation in examples like the previous “IM”
example.
Experiments show that our Chinese abbrevia-
tion prediction system outperforms the state-of-
the-art systems. In order to reduce the size of
the search space, we further propose pruning con-
straints that are learnt from the training corpus.
Experiment shows that the average number of con-
straints is reduced by about 30%, while the top-1
accuracy is not affected.
The paper is structured as follows. Section 1
gives the introduction. In section 2 we describe
our method, including the MSUs, the substring-
based tagging strategy and the ILP decoding pro-
cess. Experiments are described in section 3. We
also give a detailed analysis of the results in sec-
tion 3. In section 4 related works are introduced,
and the paper is concluded in the last section.
</bodyText>
<sectionHeader confidence="0.987382" genericHeader="introduction">
2 System Architecture
</sectionHeader>
<subsectionHeader confidence="0.997824">
2.1 Chinese Abbreviation Prediction
</subsectionHeader>
<bodyText confidence="0.858381142857143">
Chinese abbreviations are generated by selecting
representative characters from the full forms. For
example, the abbreviation of “IL)�&apos;*” (Peking
University) is “~&apos;” which is generated by se-
lecting the first and third characters, see TABLE
1. This can be tackled from the sequence labeling
point of view.
</bodyText>
<table confidence="0.976906">
Full form IL Si, &apos;
Status Keep Skip Keep Skip
Result IL &apos;
</table>
<tableCaption confidence="0.8155965">
Table 1: The abbreviation “I&apos;” of the full form
“IL)�&apos;*” (Peking University)
</tableCaption>
<bodyText confidence="0.999607294117647">
From TABLE 1 we can see that Chinese abbre-
viation prediction is a problem of selecting repre-
sentative characters from the original full form1.
Based on this assumption, previous works mainly
focus on this character tagging schema. In these
methods, the basic tagging unit is the Chinese
character. Each character in the full form is la-
beled as ‘K’ or ‘S’, where ‘K’ means the current
character should be kept in abbreviation and ‘S’
means the current character should be skipped.
However, a Chinese character can only contain
limited amount of information. Using character-
based method alone is not enough for Chinese
abbreviation generation. We introduce an MSU-
based method, which models the process of se-
lecting representative characters given local MSU
information.
</bodyText>
<subsectionHeader confidence="0.970372">
2.2 MSU Based Tagging
2.2.1 Minimum Semantic Unit
</subsectionHeader>
<bodyText confidence="0.997979">
Because using the character-based method is not
enough for Chinese abbreviation generation, we
may think of word as the basic tagging unit to in-
corporate more information intuitively. In English,
the abbreviations (similar to acronyms) are usually
formed by concatenating initial letters or parts of a
series of words. In other words, English abbrevia-
tion generation is based on words in the full form.
However, in Chinese, word is not the most suit-
able abbreviating unit. Firstly, there is no natural
boundary between Chinese words. Errors from the
Chinese word segmentation tools will accumulate
to harm the performance of abbreviation predic-
tion. Second, it is hard to design a reasonable tag
set when the length of a possible Chinese word is
very long. The second column of TABLE 2 shows
different ways of selecting representative charac-
ters of Chinese words with length 3. For a Chi-
nese compound word with 3 characters, there are 6
possible ways to select characters. In this case we
should have at least 6 kinds of tags to cover all pos-
sible situations. The case is even worse for words
with more complicated structures. A suitable ab-
breviating unit should be smaller than word.
We propose the “Minimum Semantic Unit
(MSU)” as the basic tagging unit. We define MSU
as follows:
</bodyText>
<listItem confidence="0.593619">
1. A word whose length is less or equal to 2 is
an MSU.
</listItem>
<footnote confidence="0.980372">
1A small portion of Chinese abbreviations are not gener-
ated from the full form. For example, the abbreviation of “LU
71r”(Shan Dong Province) is “0”. However, we can use a
look-up table to get this kind of abbreviations.
</footnote>
<page confidence="0.966595">
1406
</page>
<bodyText confidence="0.595985">
Full form SK Label MSUs
</bodyText>
<equation confidence="0.997649833333333">
MJLí(nursery) M/K JL/S í/S MJL+í
*49(allowance) */S 4/K 9/S *4+9
áfa(Credit card) á/S f/S a/K áf+a
7KqM(Hydropower Station) 7K/K q/K M/S 7K+q+M
Â®rx(Senate) Â/K ®/S rr/K Â+®+rx
�â(Music group) AA-/S 9�;/K â/K &apos; +â
</equation>
<tableCaption confidence="0.9917725">
Table 2: Representing characters of Chinese words with length 3 (K for keep and S for skip) and the
corresponding MSUs
</tableCaption>
<bodyText confidence="0.991332666666667">
2. A word whose length is larger than 2, but
does not contain any MSUs with length equal
to 2. For example, “k-7r-M”(Railway Sta-
tion) is not an MSU because the first two
characters “k-7r-”(Train) can form an MSU.
By this definition, all 6 strings in TABLE 2 are
often thought as a word, but they are not MSUs
in our view. Their corresponding MSU forms are
shown in TABLE 2.
We collect all the MSUs from the benchmark
datasets provided by the second International Chi-
nese Word Segmentation Bakeoff2. We choose the
Peking University (PKU) data because it is more
fine-grained than all other corpora. Suppose we
represent the segmented data as L (In our case L
is the PKU word segmentation data), the MSU se-
lecting algorithm is shown in TABLE 3.
For a given full form, we first segment it us-
ing a standard word segmenter to get a coarse-
grained segmentation result. Here we use the Stan-
ford Chinese Word Segmenter 3. Then we use the
MSU set to segment each word using the strategy
of “Maximum Forward Matching”4 to get the fine-
grained MSU segmentation result.
</bodyText>
<subsectionHeader confidence="0.953171">
2.2.2 Labeling strategy
</subsectionHeader>
<bodyText confidence="0.999992">
For MSU-based tagging, we use a labeling method
which uses four tags, “KSFL”. “K” stands for
“Keep the whole unit”, “S” stands for “Skip the
whole unit”, “F” stands for “keep the First charac-
ter of the unit”, and Label “L” stands for “keep the
Last character of the unit”. An example is shown
in TABLE 4.
The “KSFL” tag set is also applicable for MSUs
whose length is greater than 2 (an example is “11
K›/chocolate”). By examining the corpus we
find that such MSUs are either kept of skipped in
</bodyText>
<footnote confidence="0.995503">
2http://www.sighan.org/bakeoff2005/
3http://nlp.stanford.edu/software/
segmenter.shtml
4In Chinese, “Forward” means from left to right.
</footnote>
<table confidence="0.9854645">
“ý¶í-93ZWZ\T�_R k” (The ab-
breviation is “ý¶í�”)
KSFL ý¶/K í-9/F 3ZW/S Z\/S T�
_R /F k/S
</table>
<tableCaption confidence="0.786510666666667">
Table 4: The abbreviation “ý¶í ” of “ý¶í
-9�WZ\�” (National Linguistics Work Com-
mittee) based on MSU tagging.
</tableCaption>
<bodyText confidence="0.99894975">
the final abbreviations. Therefore, the labels of
these long MSUs are either ‘K’ or ‘S’. Empirically,
this assumption holds for MSUs, but does not hold
for words5.
</bodyText>
<subsectionHeader confidence="0.758886">
2.2.3 Feature templates
</subsectionHeader>
<bodyText confidence="0.991502">
The feature templates we use are as follows. See
TABLE 5.
</bodyText>
<listItem confidence="0.9996544">
1. Word Xi (−2 ≤ i ≤ 2)
2. POS tag of word Xi (−2 ≤ i ≤ 2)
3. Word Bigrams (Xi, Xi+1) (−2 ≤ i ≤ 1)
4. Type of word Xi (−2 ≤ i ≤ 2)
5. Length of word Xi (−2 ≤ i ≤ 2)
</listItem>
<bodyText confidence="0.953787583333333">
Table 5: Feature templates for unit tagging. X
represents the MSU sequence of the full form. Xi
represents the ith MSU in the sequence.
Templates 1, 2 and 3 express word uni-grams
and bi-grams. In MSU-based tagging, we can uti-
lize the POS information, which we get from the
Stanford Chinese POS Tagger6. In template 4, the
type of word refers to whether it is a number, an
English word or a Chinese word. Because the ba-
sic tagging unit is MSU, which carries word infor-
mation, we can use many features that are infeasi-
ble in character-based tagging.
</bodyText>
<footnote confidence="0.992772333333333">
5In table 2, all examples are partly kept.
6http://nlp.stanford.edu/software/
tagger.shtml
</footnote>
<page confidence="0.990358">
1407
</page>
<bodyText confidence="0.522693333333333">
Init:
Let M5U5et = empty set
For each word w in L:
</bodyText>
<figure confidence="0.9876721">
If Length(w) ≤ 2
Add w to M5U5et
End if
End for
For each word w in L:
If Length(w) &gt; 2 and no word x in M5U5et is a substring of w
Add w to M5U5et
End if
End for
Return M5U5et
</figure>
<tableCaption confidence="0.97414">
Table 3: Algorithm for collecting MSUs from the PKU corpus
</tableCaption>
<subsectionHeader confidence="0.872953">
2.2.4 Sequence Labeling Model
</subsectionHeader>
<bodyText confidence="0.999989375">
The MSU-based method gives each MSU an ex-
tra indicative label. Therefore any sequence label-
ing model is appropriate for the method. Previous
works showed that Conditional Random Fields
(CRFs) can outperform other sequence labeling
models like MEMMs in abbreviation generation
tasks (Sun et al., 2009; Tsuruoka et al., 2005). For
this reason we choose CRFs model in our system.
For a given full form’s MSU list, many can-
didate abbreviations are generated by choosing
the k-best results of the CRFs. We can use the
forward-backward algorithm to calculate the prob-
ability of a specified tagging result. To reduce the
searching complexity in the ILP decoding process,
we delete those candidate tagged sequences with
low probability.
</bodyText>
<subsectionHeader confidence="0.999646">
2.3 Substring Based Tagging
</subsectionHeader>
<bodyText confidence="0.999977441860465">
As mentioned in the introduction, the sequence
labeling method, no matter character-based or
MSU-based, perform badly when the “character
duplication” phenomenon exists. When the full
form contains duplicated characters, there is no
sound criterion for the sequence tagging strategy
to decide which of the duplicated character should
be kept in the abbreviation and which one to be
skipped. On the other hand, we can tag the sub-
strings of the full form to find the local represen-
tative characters in the substrings of the long full
form. Therefore, we propose the sub-string based
approach to given labeling results on sub-strings.
These results can be integrated into a more accu-
rate result using ILP constraints, which we will de-
scribe in the next section.
Another reason for using the sub-string based
methods is that long full forms contain more char-
acters and are much easier to make mistakes dur-
ing the sequence labeling phase. Zhang et al.
(2012) shows that if the full form contains less
than 5 characters, a simple tagger can reach an ac-
curacy of 70%. Zhang et al. (2012) also shows that
if the full form is longer than 10 characters, the
average accuracy is less than 30%. The numerous
potential candidates make it hard for the tagger to
choose the correct one. For the long full forms,
although the whole sequence is not correctly la-
beled, we find that if we only consider its short
substrings, we may find the correct representative
characters. This information can be integrated into
the decoding model to adjust the final result.
We use the MSU-based tagging method in the
sub-string tagging. The labeling strategy and fea-
ture templates are the same to the MSU-based tag-
ging method. In practice, enumerating all sub-
sequences of a given full form is infeasible if the
full form is very long. For a given full form,
we use the boundary MSUs to reduce the pos-
sible sub-sequence set. For example, “rPQ�
*rx”(Chinese Academy of Science) has 5 sub-
sequences: “rPQ”, “rPP4�*”, “�*”, “�*
rx” and “rx”.
</bodyText>
<subsectionHeader confidence="0.898463">
2.4 ILP Formulation of Decoding
</subsectionHeader>
<bodyText confidence="0.9998795">
Given the MSU-based and sub-sequence-based
methods mentioned above as well as the preva-
lent character-based methods, we can get a list
of potential abbreviation candidates and abbrevi-
ated substrings. We should integrate their advan-
tages while keeping the consistency between each
</bodyText>
<page confidence="0.969071">
1408
</page>
<bodyText confidence="0.99977128">
candidate. Therefore we further propose a global
decoding strategy using Integer Linear Program-
ming(ILP). The constraints in ILP can naturally
incorporate ’non-local’ information in contrast to
probabilistic constraints that are estimated from
training examples. We can also use linguistic con-
straints like “adjacent identical characters is not
allowed” to decode the correct abbreviation in ex-
amples like the “IM” example in section 1.
Formally, given the character sequence of the
full form c = c1...cl, we keep Q top-ranked
MSU-based tagging results T=(T1, ..., TQ) and M
tagged substrings S=(S1, ..., SM) using the meth-
ods described in previous sections. We also
use N top-ranked character-based tagging results
R=(R1, ..., RN) based on the previous character-
based works. We also define the set U = SURUT
as the union of all candidate sequences. Our goal
is to find an optimal binary variable vector solution
v� = xyz = (x1, ..., xM, y1, ..., yN, z1, ..., zQ) that
maximizes the object function:
are kept adjacently, only one of them will be kept.
Which one will be kept depends on the global de-
coding score. This is the advantage of ILP against
traditional sequence labeling methods.
</bodyText>
<subsectionHeader confidence="0.993933">
2.5 Pruning Constraints
</subsectionHeader>
<bodyText confidence="0.999897333333333">
The efficiency of solving the ILP decoding prob-
lem depends on the number of candidate tagging
sequences N and Q, as well as the number of sub-
sequences M. Usually, N and Q is less than 10 in
our experiment. Therefore, M influences the time
complexity the most. Because we use the bound-
ary of MSUs instead of enumerating all possible
subsequences, the value of M can be largely re-
duced.
Some characters are always labeled as “S” or
“K” once the context is given. We can use this
phenomenon to reduce the search space of decod-
ing. Let ci denote the ith character relative to the
current character c0 and ti denote the tag of ci. The
context templates we use are listed in TABLE 7.
</bodyText>
<equation confidence="0.585476833333333">
Uni-gram Contexts c0, c_1, c1
Bi-gram Contexts c_1c0, c_1c1, c0c1
M N
A1 score(Si) · xi + A2 score(Ri) · yi
i=1 i=1
score(Ti) · zi
</equation>
<bodyText confidence="0.96733625">
subject to constrains in TABLE 6. The parame-
ters A1, A2, A3 controls the preference of the three
parts, and can be decided using cross-validation.
Constraint 1 indicates that xi, yi, zi are all
boolean variables. They are used as indicator vari-
ables to show whether the corresponding tagged
sequence is in accordance with the final result.
Constraint 2 is used to guarantee that at most
one candidate from the character-based tagging is
preserved. We relax the constraint to allow the
sum to be zero in case that none of the top-ranked
candidate is suitable to be the final result. If the
sum equals zero, then the sub-sequence based tag-
ging method will generate a more suitable result.
Constrain 3 has the same utility for the MSU-
based tagging.
Constraint 4, 5, 6 are inter-method constraints.
We use them to guarantee that the labels of the
preserved sequences of different tagging methods
do not conflict with each other. Constraint 7 is
used to guarantee that the labels of the preserved
sub-strings do not conflict with each other.
Constraint 8 is used to solve the “character du-
plicate” problem. When two identical characters
</bodyText>
<tableCaption confidence="0.981849">
Table 7: Context templates used in pruning
</tableCaption>
<bodyText confidence="0.919175428571429">
With respect to a training corpus, if a context
C relative to c0 always assigns a certain tag t to
c0, then we can use this constraint in pruning. We
judge the degree of “always” by checking whether
count(Cnto=t) &gt; threshold. The threshold is a
count(C)
non-negative real number under 1.0.
</bodyText>
<sectionHeader confidence="0.999726" genericHeader="background">
3 Experiments
</sectionHeader>
<subsectionHeader confidence="0.999618">
3.1 Data and Evaluation Metric
</subsectionHeader>
<bodyText confidence="0.999990375">
We use the abbreviation corpus provided by Insti-
tute of Computational Linguistics (ICL) of Peking
University in our experiments. The corpus is sim-
ilar to the corpus used in Sun et al. (2008, 2009);
Zhang et al. (2012). It contains 8, 015 Chinese ab-
breviations, including noun phrases, organization
names and some other types. Some examples are
presented in TABLE 8. We use 80% abbreviations
as training data and the rest as testing data. In
some cases, a long phrase may contain more than
one abbreviation. For these cases, the corpus just
keeps their most commonly used abbreviation for
each full form.
The evaluation metric used in our experiment
is the top-K accuracy, which is also used by
Tsuruoka et al. (2005), Sun et al. (2009) and
</bodyText>
<equation confidence="0.965582666666667">
� Q
i=1
+A3
</equation>
<page confidence="0.962162">
1409
</page>
<listItem confidence="0.997269">
1. xi ∈ {0, 1}, yi ∈ {0, 1}, zi ∈ {0, 1}
2. EN i=1 yi ≤ 1
3. i=1 zi ≤ 1
4. ∀Ri ∈ R, Sj ∈ S, if Ri and Sj have a same position but the position gets different labels,
then yi + xj ≤ 1
5. ∀Ti ∈ T, Sj ∈ S, if Ti and Sj have a same position but the position gets different labels,
then zi + xj ≤ 1
6. ∀Ri ∈ R, Tj ∈ T, if Ri and Tj have a same position but the position gets different labels,
then xi + zj ≤ 1
7. ∀Si, Sj ∈ S if Si and Sj have a same position but the position gets different labels, then
zi + zj ≤ 1
8. ∀Si, Sj ∈ S if the last character Si keeps is the same as the first character Sj keeps, then
zi + zj ≤ 1
</listItem>
<tableCaption confidence="0.889815">
Table 6: Constraints for ILP
</tableCaption>
<table confidence="0.9999">
Type Full form Abbreviation
Noun Phrase ft p #(Excellent articles) VA.)
Organization \*�k�(Writers’ Association) \�
Coordinate phrase xf_$./Et(Injuries and deaths) $t
Proper noun ­’Ë(Media) �
</table>
<tableCaption confidence="0.999515">
Table 8: Examples of the corpus (Noun Phrase, Organization, Coordinate Phrase, Proper Noun)
</tableCaption>
<bodyText confidence="0.999754769230769">
Zhang et al. (2012). The top-K accuracy measures
what percentage of the reference abbreviations are
found if we take the top N candidate abbreviations
from all the results. In our experiment, top-10 can-
didates are considered in re-ranking phrase and the
measurement used is top-1 accuracy (which is the
accuracy we usually refer to) because the final aim
of the algorithm is to detect the exact abbreviation.
CRF++7 , an open source linear chain CRF tool,
is used in the sequence labeling part. For ILP part,
we use lpsolve8, which is also an open source tool.
The parameters of these tools are tuned through
cross-validation on the training data.
</bodyText>
<subsectionHeader confidence="0.933539">
3.2 Results
</subsectionHeader>
<bodyText confidence="0.999024363636364">
TABLE 9 shows the top-K accuracy of the
character-based and MSU-based method. We can
see that the MSU-based tagging method can uti-
lize word information, which can get better perfor-
mance than the character-based method. We can
also figure out that the top-5 candidates include the
reference abbreviation for most full forms. There-
fore reasonable decoding by considering all possi-
ble labeling of sequences may improve the perfor-
mance. Although the MSU-based methods only
outperforms character-based methods by 0.75%
</bodyText>
<footnote confidence="0.999663">
7http://crfpp.sourceforge.net/
8http://lpsolve.sourceforge.net/5.5/
</footnote>
<bodyText confidence="0.9950315">
for top-1 accuracy, it is much better when consid-
ering top-2 to top-5 accuracy (+2.5%). We further
select the top-ranked candidates for ILP decod-
ing. Therefore the MSU-based method can further
improve the performance in the global decoding
phase.
</bodyText>
<table confidence="0.988822666666667">
K char-based MSU-based
1 0.5714 0.5789
2 0.6879 0.7155
3 0.7681 0.7819
4 0.8070 0.8283
5 0.8333 0.8583
</table>
<tableCaption confidence="0.879542">
Table 9: Top-K (K ≤ 5) results of character-based
tagging and MSU-based tagging
</tableCaption>
<bodyText confidence="0.99993875">
We then use the top-5 candidates of character-
based method and MSU-based method, as well
as the top-2 results of sub-sequence labeling in
the ILP decoding phase. Then we select the top-
ranked candidate as the final abbreviation of each
instance. TABLE 10 shows the results. We can see
that the accuracy of our method is 61.0%, which
improved by +3.89% compared to the character-
based method, and +3.14% compared to the MSU-
based method.
We find that the ILP decoding phase do play
an important role in generating the right an-
</bodyText>
<page confidence="0.971854">
1410
</page>
<table confidence="0.99457425">
Method Top-1 Accuracy
Char-based 0.5714
MSU-based 0.5789
ILP Result 0.6103
</table>
<tableCaption confidence="0.997853">
Table 10: Top-1 Accuracy after ILP decoding
</tableCaption>
<bodyText confidence="0.9994264">
swer. Some reference abbreviations which are not
picked out by either tagging method can be found
out after decoding. TABLE 11 shows the exam-
ple of the organization name “Ø *WjJ-&apos;LE-Ût
žlV” (Higher Education Admissions Office).
Neither the character-based method nor the MSU-
based method finds the correct answer “ØÛž”,
while after ILP decoding, “ØÛž” becomes the
final result. TABLE 12 and TABLE 13 give two
more examples.
</bodyText>
<figure confidence="0.43926825">
ØÛž
Øž
Ø��E),�]I
Ø�y N /J`
</figure>
<tableCaption confidence="0.879351">
Table 11: Top-1 result of “Ø����Û�ž
lV” (Higher Education Admissions Office)
</tableCaption>
<table confidence="0.794629">
True Result 0-T,
Char-based VT1
MSU-based MT,
ILP Decoding 0-T,
</table>
<tableCaption confidence="0.957272">
Table 12: Top-1 result of “VO- T,” (Articles
exceed the value)
</tableCaption>
<figure confidence="0.649116">
True Result P°IUriSA
Char-based �ISA
MSU-based ðSAœ
ILP Decoding �I�SA
</figure>
<tableCaption confidence="0.9871375">
Table 13: Top-1 result of “J41-1-�kTIU912SA4”
(Visual effects of sound and lights)
</tableCaption>
<subsectionHeader confidence="0.999154">
3.3 Improvements Considering Length
</subsectionHeader>
<bodyText confidence="0.999765692307692">
Full forms that are longer than five characters are
long terms. Long terms contain more characters,
which is much easier to make mistakes. Figure
1 shows the top-1 accuracy respect to the term
length using different tagging methods and using
ILP decoding. The x-axis represents the length of
the full form. The y-axis represents top-1 accu-
racy. We find that our method works especially
better than pure character-based or MSU-based
approach when the full form is long. By decod-
ing using ILP, both local and global information
are incorporated. Therefore many of these errors
can be eliminated.
</bodyText>
<figureCaption confidence="0.97277">
Figure 1: Top-1 accuracy of different methods
considering length
</figureCaption>
<subsectionHeader confidence="0.999224">
3.4 Effect of pruning
</subsectionHeader>
<bodyText confidence="0.999695888888889">
As discussed in previous sections, if we are able
to pre-determine that some characters in a certain
context should be kept or skipped, then the num-
ber of possible boolean variable x can be reduced.
TABLE 14 shows the differences. To guarantee
a high accuracy, we set the threshold to be 0.99.
When the original full form is partially tagged by
the pruning constraints, the number of boolean
variables per full form is reduced from 34.4 to
25.5. By doing this, we can improve the predic-
tion speed over taking the raw input.
From TABLE 14 we can also see that the top-
1 accuracy is not affected by these pruning con-
straints. This is obvious, because CRF itself has
a strong modeling ability. The pruning constraints
cannot improve the model accuracy. But they can
help eliminate those false candidates to make the
ILP decoding faster.
</bodyText>
<table confidence="0.992815">
Accuracy Average length Time(s)
raw 0.6103 34.4 12.5
pruned 0.6103 25.5 7.1
</table>
<tableCaption confidence="0.976792">
Table 14: Comparison of testing time of raw input
and pruned input
</tableCaption>
<subsectionHeader confidence="0.9975525">
3.5 Compare with the State-of-the-art
Systems
</subsectionHeader>
<bodyText confidence="0.9998895">
We also compare our method with previous meth-
ods, including Sun et al. (2009) and Zhang et al.
(2012). Because we use a different corpus, we
re-implement the system Sun et al. (2009), Zhang
</bodyText>
<figure confidence="0.991281">
True Result
Char-based
MSU-based
ILP Decoding
</figure>
<page confidence="0.990164">
1411
</page>
<bodyText confidence="0.999529181818182">
et al. (2012) and Sun et al. (2013), and experi-
ment on our corpus. The first two are CRF+GI
and DPLVM+GI in Sun et al. (2009), which are
reported to outperform the methods in Tsuruoka
et al. (2005) and Sun et al. (2008). For DPLVM
we use the same model in Sun et al. (2009) and
experiment on our own data. We also compare
our approach with the method in Zhang et al.
(2012). However, Zhang et al. (2012) uses dif-
ferent sources of search engine result information
to re-rank the original candidates. We do not use
any extra web resources. Because Zhang et al.
(2012) uses web information only in its second
stage, we use “BIEP”(the tag set used by Zhang
et al. (2012)) to denote the first stage of Zhang
et al. (2012), which also uses no web information.
TABLE 15 shows the results of the comparisons.
We can see that our method outperforms all other
methods which use no extra resource. Because
Zhang et al. (2012) uses extra web resource, the
top-1 accuracy of Zhang et al. (2012) is slightly
better than ours.
</bodyText>
<table confidence="0.999167166666667">
Method Top-1 Accuracy
CRF+GI 0.5850
DPLVM+GI 0.5990
BIEP 0.5812
Zhang et al. (2012) 0.6205
Our Result 0.6103
</table>
<tableCaption confidence="0.950696">
Table 15: Comparison with the state-of-the-art
systems
</tableCaption>
<sectionHeader confidence="0.999563" genericHeader="related work">
4 Related Work
</sectionHeader>
<bodyText confidence="0.99951355882353">
Previous research mainly focuses on “abbrevia-
tion disambiguation”, and machine learning ap-
proaches are commonly used (Park and Byrd,
2001; HaCohen-Kerner et al., 2008; Yu et al.,
2006; Ao and Takagi, 2005). These ways of link-
ing abbreviation pairs are effective, however, they
cannot solve our problem directly. In many cases
the full form is definite while we don’t know the
corresponding abbreviation.
To solve this problem, some approaches main-
tain a database of abbreviations and their corre-
sponding “full form” pairs. The major problem
of pure database-building approach is obvious. It
is impossible to cover all abbreviations, and the
building process is quit laborious. To find these
pairs automatically, a powerful approach is to find
the reference for a full form given the context,
which is referred to as “abbreviation generation”.
There is research on heuristic rules for gen-
erating abbreviations Barrett and Grems (1960);
Bourne and Ford (1961); Taghva and Gilbreth
(1999); Park and Byrd (2001); Wren et al. (2002);
Hearst (2003). Most of them achieved high per-
formance. However, hand-crafted rules are time
consuming to create, and it is not easy to transfer
the knowledge of rules from one language to an-
other.
Recent studies of abbreviation generation have
focused on the use of machine learning tech-
niques. Sun et al. (2008) proposed a supervised
learning approach by using SVM model. Tsu-
ruoka et al. (2005); Sun et al. (2009) formal-
ized the process of abbreviation generation as a
sequence labeling problem. In Tsuruoka et al.
(2005) each character in the full form is associated
with a binary value label y, which takes the value
S (Skip) if the character is not in the abbreviation,
and value P (Preserve) if the character is in the ab-
breviation. Then a MEMM model is used to model
the generating process. Sun et al. (2009) followed
this schema but used DPLVM model to incor-
porate both local and global information, which
yields better results. Sun et al. (2013) also uses
machine learning based methods, but focuses on
the negative full form problem, which is a little
different from our work.
Besides these pure statistical approaches, there
are also many approaches using Web as a corpus
in machine learning approaches for generating ab-
breviations.Adar (2004) proposed methods to de-
tect such pairs from biomedical documents. Jain
et al. (2007) used web search results as well as
search logs to find and rank abbreviates full pairs,
which show good result. The disadvantage is that
search log data is only available in a search en-
gine backend. The ordinary approaches do not
have access to search engine internals. Zhang et al.
(2012) used web search engine information to re-
rank the candidate abbreviations generated by sta-
tistical approaches. Compared to their approaches,
our method uses no extra resource, but reaches
comparable results.
ILP shows good results in many NLP tasks.
Punyakanok et al. (2004); Roth and Yih (2005)
used it in semantic role labeling (SRL). Martins
et al. (2009) used it in dependency parsing. (Zhao
and Marcus, 2012) used it in Chinese word seg-
mentation. (Riedel and Clarke, 2006) used ILP
</bodyText>
<page confidence="0.984107">
1412
</page>
<bodyText confidence="0.99997">
in dependency parsing. However, previous works
mainly focus on the constraints of avoiding bound-
ary confliction. For example, in SRL, two argu-
ment of cannot overlap. In CWS, two Chinese
words cannot share a same character. Different to
their methods, we investigate on the conflict of la-
bels of character sub-sequences.
</bodyText>
<sectionHeader confidence="0.993801" genericHeader="conclusions">
5 Conclusion and Future work
</sectionHeader>
<bodyText confidence="0.999976375">
We propose a new Chinese abbreviation predic-
tion method which can incorporate rich local in-
formation while generating the abbreviation glob-
ally. We propose the MSU, which is more coarse-
grained than character but more fine-grained than
word, to capture word information in the se-
quence labeling framework. Besides the MSU-
based method, we use a substring tagging strategy
to generate local substring tagging candidates. We
use an ILP formulation with various constraints
to globally decode the final abbreviation from the
generated candidates. Experiments show that our
method outperforms the state-of-the-art systems,
without using any extra resource. This method
is not limited to Chinese abbreviation generation,
it can also be applied to similar languages like
Japanese.
The results are promising and outperform the
baseline methods. The accuracy can still be im-
proved. Potential future works may include using
semi-supervised methods to incorporate unlabeled
data and design reasonable features from large cor-
pora. We are going to study on these issues in the
future.
</bodyText>
<sectionHeader confidence="0.99756" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9996396875">
This research was partly supported by Na-
tional Natural Science Foundation of China
(No.61370117,61333018,61300063),Major
National Social Science Fund of
China(No.12&amp;ZD227), National High Tech-
nology Research and Development Program of
China (863 Program) (No. 2012AA011101), and
Doctoral Fund of Ministry of Education of China
(No. 20130001120004). The contact author of
this paper, according to the meaning given to
this role by Key Laboratory of Computational
Linguistics, Ministry of Education, School of
Electronics Engineering and Computer Science,
Peking University, is Houfeng Wang. We thank
Ke Wu for part of our work is inspired by his
previous work at KLCL.
</bodyText>
<sectionHeader confidence="0.943025" genericHeader="references">
References
</sectionHeader>
<bodyText confidence="0.8441261">
Adar, E. (2004). Sarad: A simple and ro-
bust abbreviation dictionary. Bioinformatics,
20(4):527–533.
Ao, H. and Takagi, T. (2005). Alice: an algorithm
to extract abbreviations from medline. Journal
of the American Medical Informatics Associa-
tion, 12(5):576–586.
Barrett, J. and Grems, M. (1960). Abbreviating
words systematically. Communications of the
ACM, 3(5):323–324.
</bodyText>
<reference confidence="0.970727540540541">
Bourne, C. and Ford, D. (1961). A study of
methods for systematically abbreviating english
words and names. Journal of the ACM (JACM),
8(4):538–552.
HaCohen-Kerner, Y., Kass, A., and Peretz, A.
(2008). Combined one sense disambiguation
of abbreviations. In Proceedings of the 46th
Annual Meeting of the Association for Compu-
tational Linguistics on Human Language Tech-
nologies: Short Papers, pages 61–64. Associa-
tion for Computational Linguistics.
Hearst, M. S. (2003). A simple algorithm for
identifying abbreviation definitions in biomed-
ical text.
Jain, A., Cucerzan, S., and Azzam, S. (2007).
Acronym-expansion recognition and ranking on
the web. In Information Reuse and Integration,
2007. IRI 2007. IEEE International Conference
on, pages 209–214. IEEE.
Martins, A. F., Smith, N. A., and Xing, E. P.
(2009). Concise integer linear programming
formulations for dependency parsing. In Pro-
ceedings of the Joint Conference of the 47th An-
nual Meeting of the ACL and the 4th Interna-
tional Joint Conference on Natural Language
Processing of the AFNLP: Volume 1-Volume 1,
pages 342–350. Association for Computational
Linguistics.
Park, Y. and Byrd, R. (2001). Hybrid text mining
for finding abbreviations and their definitions.
In Proceedings of the 2001 conference on em-
pirical methods in natural language processing,
pages 126–133.
Punyakanok, V., Roth, D., Yih, W.-t., and Zimak,
D. (2004). Semantic role labeling via integer
linear programming inference. In Proceedings
of the 20th international conference on Compu-
</reference>
<page confidence="0.599845">
1413
</page>
<reference confidence="0.940516225352113">
tational Linguistics, page 1346. Association for
Computational Linguistics.
Riedel, S. and Clarke, J. (2006). Incremental in-
teger linear programming for non-projective de-
pendency parsing. In Proceedings of the 2006
Conference on Empirical Methods in Natural
Language Processing, pages 129–137. Associ-
ation for Computational Linguistics.
Roth, D. and Yih, W.-t. (2005). Integer linear
programming inference for conditional random
fields. In Proceedings of the 22nd international
conference on Machine learning, pages 736–
743. ACM.
Sun, X., Li, W., Meng, F., and Wang, H. (2013).
Generalized abbreviation prediction with nega-
tive full forms and its application on improv-
ing chinese web search. In Proceedings of the
Sixth International Joint Conference on Natural
Language Processing, pages 641–647, Nagoya,
Japan. Asian Federation of Natural Language
Processing.
Sun, X., Okazaki, N., and Tsujii, J. (2009). Ro-
bust approach to abbreviating terms: A discrim-
inative latent variable model with global infor-
mation. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on Nat-
ural Language Processing of the AFNLP: Vol-
ume 2-Volume 2, pages 905–913. Association
for Computational Linguistics.
Sun, X., Wang, H., and Wang, B. (2008). Pre-
dicting chinese abbreviations from definitions:
An empirical learning approach using support
vector regression. Journal of Computer Science
and Technology, 23(4):602–611.
Taghva, K. and Gilbreth, J. (1999). Recognizing
acronyms and their definitions. International
Journal on Document Analysis and Recogni-
tion, 1(4):191–198.
Tsuruoka, Y., Ananiadou, S., and Tsujii, J. (2005).
A machine learning approach to acronym gen-
eration. In Proceedings of the ACL-ISMB Work-
shop on Linking Biological Literature, Ontolo-
gies and Databases: Mining Biological Seman-
tics, pages 25–31. Association for Computa-
tional Linguistics.
Wren, J., Garner, H., et al. (2002). Heuristics
for identification of acronym-definition patterns
within text: towards an automated construc-
tion of comprehensive acronym-definition dic-
tionaries. Methods of information in medicine,
41(5):426–434.
Yu, H., Kim, W., Hatzivassiloglou, V., and Wilbur,
J. (2006). A large scale, corpus-based approach
for automatically disambiguating biomedical
abbreviations. ACM Transactions on Informa-
tion Systems (TOIS), 24(3):380–404.
Zhang, L., Li, S., Wang, H., Sun, N., and Meng,
X. (2012). Constructing Chinese abbreviation
dictionary: A stacked approach. In Proceedings
of COLING 2012, pages 3055–3070, Mumbai,
India. The COLING 2012 Organizing Commit-
tee.
Zhao, Q. and Marcus, M. (2012). Exploring deter-
ministic constraints: from a constrained english
pos tagger to an efficient ilp solution to chinese
word segmentation. In Proceedings of the 50th
Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers),
pages 1054–1062, Jeju Island, Korea. Associa-
tion for Computational Linguistics.
</reference>
<page confidence="0.994695">
1414
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.950835">
<title confidence="0.999349">Predicting Chinese Abbreviations with Minimum Semantic Unit Global Constraints</title>
<author confidence="0.999569">Longkai Zhang Li Li Houfeng Wang Xu</author>
<affiliation confidence="0.978">Key Laboratory of Computational Linguistics (Peking Ministry of Education,</affiliation>
<abstract confidence="0.999700476190476">We propose a new Chinese abbreviation prediction method which can incorporate rich local information while generating the abbreviation globally. Different to previous character tagging methods, we introduce the minimum semantic unit, which is more fine-grained than character but more coarse-grained than word, to capture word level information in the sequence labeling framework. To solve the “character duplication” problem in Chinese abbreviation prediction, we also use a substring tagging strategy to generate local substring tagging candidates. We use an integer linear programming (ILP) formulation with various constraints to globally decode the final abbreviation from the generated candidates. Experiments show that our method outperforms the state-of-the-art systems, without using any extra resource.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>C Bourne</author>
<author>D Ford</author>
</authors>
<title>A study of methods for systematically abbreviating english words and names.</title>
<date>1961</date>
<journal>Journal of the ACM (JACM),</journal>
<volume>8</volume>
<issue>4</issue>
<contexts>
<context position="27607" citStr="Bourne and Ford (1961)" startWordPosition="4628" endWordPosition="4631">form is definite while we don’t know the corresponding abbreviation. To solve this problem, some approaches maintain a database of abbreviations and their corresponding “full form” pairs. The major problem of pure database-building approach is obvious. It is impossible to cover all abbreviations, and the building process is quit laborious. To find these pairs automatically, a powerful approach is to find the reference for a full form given the context, which is referred to as “abbreviation generation”. There is research on heuristic rules for generating abbreviations Barrett and Grems (1960); Bourne and Ford (1961); Taghva and Gilbreth (1999); Park and Byrd (2001); Wren et al. (2002); Hearst (2003). Most of them achieved high performance. However, hand-crafted rules are time consuming to create, and it is not easy to transfer the knowledge of rules from one language to another. Recent studies of abbreviation generation have focused on the use of machine learning techniques. Sun et al. (2008) proposed a supervised learning approach by using SVM model. Tsuruoka et al. (2005); Sun et al. (2009) formalized the process of abbreviation generation as a sequence labeling problem. In Tsuruoka et al. (2005) each </context>
</contexts>
<marker>Bourne, Ford, 1961</marker>
<rawString>Bourne, C. and Ford, D. (1961). A study of methods for systematically abbreviating english words and names. Journal of the ACM (JACM), 8(4):538–552.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y HaCohen-Kerner</author>
<author>A Kass</author>
<author>A Peretz</author>
</authors>
<title>Combined one sense disambiguation of abbreviations.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics on Human Language Technologies: Short Papers,</booktitle>
<pages>61--64</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="26816" citStr="HaCohen-Kerner et al., 2008" startWordPosition="4502" endWordPosition="4505">ses no web information. TABLE 15 shows the results of the comparisons. We can see that our method outperforms all other methods which use no extra resource. Because Zhang et al. (2012) uses extra web resource, the top-1 accuracy of Zhang et al. (2012) is slightly better than ours. Method Top-1 Accuracy CRF+GI 0.5850 DPLVM+GI 0.5990 BIEP 0.5812 Zhang et al. (2012) 0.6205 Our Result 0.6103 Table 15: Comparison with the state-of-the-art systems 4 Related Work Previous research mainly focuses on “abbreviation disambiguation”, and machine learning approaches are commonly used (Park and Byrd, 2001; HaCohen-Kerner et al., 2008; Yu et al., 2006; Ao and Takagi, 2005). These ways of linking abbreviation pairs are effective, however, they cannot solve our problem directly. In many cases the full form is definite while we don’t know the corresponding abbreviation. To solve this problem, some approaches maintain a database of abbreviations and their corresponding “full form” pairs. The major problem of pure database-building approach is obvious. It is impossible to cover all abbreviations, and the building process is quit laborious. To find these pairs automatically, a powerful approach is to find the reference for a ful</context>
</contexts>
<marker>HaCohen-Kerner, Kass, Peretz, 2008</marker>
<rawString>HaCohen-Kerner, Y., Kass, A., and Peretz, A. (2008). Combined one sense disambiguation of abbreviations. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics on Human Language Technologies: Short Papers, pages 61–64. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M S Hearst</author>
</authors>
<title>A simple algorithm for identifying abbreviation definitions in biomedical text.</title>
<date>2003</date>
<contexts>
<context position="27692" citStr="Hearst (2003)" startWordPosition="4644" endWordPosition="4645">me approaches maintain a database of abbreviations and their corresponding “full form” pairs. The major problem of pure database-building approach is obvious. It is impossible to cover all abbreviations, and the building process is quit laborious. To find these pairs automatically, a powerful approach is to find the reference for a full form given the context, which is referred to as “abbreviation generation”. There is research on heuristic rules for generating abbreviations Barrett and Grems (1960); Bourne and Ford (1961); Taghva and Gilbreth (1999); Park and Byrd (2001); Wren et al. (2002); Hearst (2003). Most of them achieved high performance. However, hand-crafted rules are time consuming to create, and it is not easy to transfer the knowledge of rules from one language to another. Recent studies of abbreviation generation have focused on the use of machine learning techniques. Sun et al. (2008) proposed a supervised learning approach by using SVM model. Tsuruoka et al. (2005); Sun et al. (2009) formalized the process of abbreviation generation as a sequence labeling problem. In Tsuruoka et al. (2005) each character in the full form is associated with a binary value label y, which takes the</context>
</contexts>
<marker>Hearst, 2003</marker>
<rawString>Hearst, M. S. (2003). A simple algorithm for identifying abbreviation definitions in biomedical text.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Jain</author>
<author>S Cucerzan</author>
<author>S Azzam</author>
</authors>
<title>Acronym-expansion recognition and ranking on the web.</title>
<date>2007</date>
<booktitle>In Information Reuse and Integration,</booktitle>
<pages>209--214</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="29016" citStr="Jain et al. (2007)" startWordPosition="4866" endWordPosition="4869"> the abbreviation. Then a MEMM model is used to model the generating process. Sun et al. (2009) followed this schema but used DPLVM model to incorporate both local and global information, which yields better results. Sun et al. (2013) also uses machine learning based methods, but focuses on the negative full form problem, which is a little different from our work. Besides these pure statistical approaches, there are also many approaches using Web as a corpus in machine learning approaches for generating abbreviations.Adar (2004) proposed methods to detect such pairs from biomedical documents. Jain et al. (2007) used web search results as well as search logs to find and rank abbreviates full pairs, which show good result. The disadvantage is that search log data is only available in a search engine backend. The ordinary approaches do not have access to search engine internals. Zhang et al. (2012) used web search engine information to rerank the candidate abbreviations generated by statistical approaches. Compared to their approaches, our method uses no extra resource, but reaches comparable results. ILP shows good results in many NLP tasks. Punyakanok et al. (2004); Roth and Yih (2005) used it in sem</context>
</contexts>
<marker>Jain, Cucerzan, Azzam, 2007</marker>
<rawString>Jain, A., Cucerzan, S., and Azzam, S. (2007). Acronym-expansion recognition and ranking on the web. In Information Reuse and Integration, 2007. IRI 2007. IEEE International Conference on, pages 209–214. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A F Martins</author>
<author>N A Smith</author>
<author>E P Xing</author>
</authors>
<title>Concise integer linear programming formulations for dependency parsing.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume</booktitle>
<volume>1</volume>
<pages>342--350</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="29664" citStr="Martins et al. (2009)" startWordPosition="4974" endWordPosition="4977"> well as search logs to find and rank abbreviates full pairs, which show good result. The disadvantage is that search log data is only available in a search engine backend. The ordinary approaches do not have access to search engine internals. Zhang et al. (2012) used web search engine information to rerank the candidate abbreviations generated by statistical approaches. Compared to their approaches, our method uses no extra resource, but reaches comparable results. ILP shows good results in many NLP tasks. Punyakanok et al. (2004); Roth and Yih (2005) used it in semantic role labeling (SRL). Martins et al. (2009) used it in dependency parsing. (Zhao and Marcus, 2012) used it in Chinese word segmentation. (Riedel and Clarke, 2006) used ILP 1412 in dependency parsing. However, previous works mainly focus on the constraints of avoiding boundary confliction. For example, in SRL, two argument of cannot overlap. In CWS, two Chinese words cannot share a same character. Different to their methods, we investigate on the conflict of labels of character sub-sequences. 5 Conclusion and Future work We propose a new Chinese abbreviation prediction method which can incorporate rich local information while generating</context>
</contexts>
<marker>Martins, Smith, Xing, 2009</marker>
<rawString>Martins, A. F., Smith, N. A., and Xing, E. P. (2009). Concise integer linear programming formulations for dependency parsing. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 1-Volume 1, pages 342–350. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Park</author>
<author>R Byrd</author>
</authors>
<title>Hybrid text mining for finding abbreviations and their definitions.</title>
<date>2001</date>
<booktitle>In Proceedings of the 2001 conference on empirical methods in natural language processing,</booktitle>
<pages>126--133</pages>
<contexts>
<context position="26787" citStr="Park and Byrd, 2001" startWordPosition="4498" endWordPosition="4501"> (2012), which also uses no web information. TABLE 15 shows the results of the comparisons. We can see that our method outperforms all other methods which use no extra resource. Because Zhang et al. (2012) uses extra web resource, the top-1 accuracy of Zhang et al. (2012) is slightly better than ours. Method Top-1 Accuracy CRF+GI 0.5850 DPLVM+GI 0.5990 BIEP 0.5812 Zhang et al. (2012) 0.6205 Our Result 0.6103 Table 15: Comparison with the state-of-the-art systems 4 Related Work Previous research mainly focuses on “abbreviation disambiguation”, and machine learning approaches are commonly used (Park and Byrd, 2001; HaCohen-Kerner et al., 2008; Yu et al., 2006; Ao and Takagi, 2005). These ways of linking abbreviation pairs are effective, however, they cannot solve our problem directly. In many cases the full form is definite while we don’t know the corresponding abbreviation. To solve this problem, some approaches maintain a database of abbreviations and their corresponding “full form” pairs. The major problem of pure database-building approach is obvious. It is impossible to cover all abbreviations, and the building process is quit laborious. To find these pairs automatically, a powerful approach is to</context>
</contexts>
<marker>Park, Byrd, 2001</marker>
<rawString>Park, Y. and Byrd, R. (2001). Hybrid text mining for finding abbreviations and their definitions. In Proceedings of the 2001 conference on empirical methods in natural language processing, pages 126–133.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Punyakanok</author>
<author>D Roth</author>
<author>W-t Yih</author>
<author>D Zimak</author>
</authors>
<title>Semantic role labeling via integer linear programming inference.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th international conference on Computational Linguistics,</booktitle>
<pages>1346</pages>
<institution>Association for Computational Linguistics.</institution>
<contexts>
<context position="29580" citStr="Punyakanok et al. (2004)" startWordPosition="4959" endWordPosition="4962">ect such pairs from biomedical documents. Jain et al. (2007) used web search results as well as search logs to find and rank abbreviates full pairs, which show good result. The disadvantage is that search log data is only available in a search engine backend. The ordinary approaches do not have access to search engine internals. Zhang et al. (2012) used web search engine information to rerank the candidate abbreviations generated by statistical approaches. Compared to their approaches, our method uses no extra resource, but reaches comparable results. ILP shows good results in many NLP tasks. Punyakanok et al. (2004); Roth and Yih (2005) used it in semantic role labeling (SRL). Martins et al. (2009) used it in dependency parsing. (Zhao and Marcus, 2012) used it in Chinese word segmentation. (Riedel and Clarke, 2006) used ILP 1412 in dependency parsing. However, previous works mainly focus on the constraints of avoiding boundary confliction. For example, in SRL, two argument of cannot overlap. In CWS, two Chinese words cannot share a same character. Different to their methods, we investigate on the conflict of labels of character sub-sequences. 5 Conclusion and Future work We propose a new Chinese abbrevia</context>
</contexts>
<marker>Punyakanok, Roth, Yih, Zimak, 2004</marker>
<rawString>Punyakanok, V., Roth, D., Yih, W.-t., and Zimak, D. (2004). Semantic role labeling via integer linear programming inference. In Proceedings of the 20th international conference on Computational Linguistics, page 1346. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Riedel</author>
<author>J Clarke</author>
</authors>
<title>Incremental integer linear programming for non-projective dependency parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>129--137</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="29783" citStr="Riedel and Clarke, 2006" startWordPosition="4994" endWordPosition="4997">h log data is only available in a search engine backend. The ordinary approaches do not have access to search engine internals. Zhang et al. (2012) used web search engine information to rerank the candidate abbreviations generated by statistical approaches. Compared to their approaches, our method uses no extra resource, but reaches comparable results. ILP shows good results in many NLP tasks. Punyakanok et al. (2004); Roth and Yih (2005) used it in semantic role labeling (SRL). Martins et al. (2009) used it in dependency parsing. (Zhao and Marcus, 2012) used it in Chinese word segmentation. (Riedel and Clarke, 2006) used ILP 1412 in dependency parsing. However, previous works mainly focus on the constraints of avoiding boundary confliction. For example, in SRL, two argument of cannot overlap. In CWS, two Chinese words cannot share a same character. Different to their methods, we investigate on the conflict of labels of character sub-sequences. 5 Conclusion and Future work We propose a new Chinese abbreviation prediction method which can incorporate rich local information while generating the abbreviation globally. We propose the MSU, which is more coarsegrained than character but more fine-grained than w</context>
</contexts>
<marker>Riedel, Clarke, 2006</marker>
<rawString>Riedel, S. and Clarke, J. (2006). Incremental integer linear programming for non-projective dependency parsing. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, pages 129–137. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Roth</author>
<author>W-t Yih</author>
</authors>
<title>Integer linear programming inference for conditional random fields.</title>
<date>2005</date>
<booktitle>In Proceedings of the 22nd international conference on Machine learning,</booktitle>
<pages>736--743</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="29601" citStr="Roth and Yih (2005)" startWordPosition="4963" endWordPosition="4966">ical documents. Jain et al. (2007) used web search results as well as search logs to find and rank abbreviates full pairs, which show good result. The disadvantage is that search log data is only available in a search engine backend. The ordinary approaches do not have access to search engine internals. Zhang et al. (2012) used web search engine information to rerank the candidate abbreviations generated by statistical approaches. Compared to their approaches, our method uses no extra resource, but reaches comparable results. ILP shows good results in many NLP tasks. Punyakanok et al. (2004); Roth and Yih (2005) used it in semantic role labeling (SRL). Martins et al. (2009) used it in dependency parsing. (Zhao and Marcus, 2012) used it in Chinese word segmentation. (Riedel and Clarke, 2006) used ILP 1412 in dependency parsing. However, previous works mainly focus on the constraints of avoiding boundary confliction. For example, in SRL, two argument of cannot overlap. In CWS, two Chinese words cannot share a same character. Different to their methods, we investigate on the conflict of labels of character sub-sequences. 5 Conclusion and Future work We propose a new Chinese abbreviation prediction metho</context>
</contexts>
<marker>Roth, Yih, 2005</marker>
<rawString>Roth, D. and Yih, W.-t. (2005). Integer linear programming inference for conditional random fields. In Proceedings of the 22nd international conference on Machine learning, pages 736– 743. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Sun</author>
<author>W Li</author>
<author>F Meng</author>
<author>H Wang</author>
</authors>
<title>Generalized abbreviation prediction with negative full forms and its application on improving chinese web search.</title>
<date>2013</date>
<journal>Asian Federation of Natural Language Processing.</journal>
<booktitle>In Proceedings of the Sixth International Joint Conference on Natural Language Processing,</booktitle>
<pages>641--647</pages>
<location>Nagoya,</location>
<contexts>
<context position="25494" citStr="Sun et al. (2013)" startWordPosition="4273" endWordPosition="4276">ability. The pruning constraints cannot improve the model accuracy. But they can help eliminate those false candidates to make the ILP decoding faster. Accuracy Average length Time(s) raw 0.6103 34.4 12.5 pruned 0.6103 25.5 7.1 Table 14: Comparison of testing time of raw input and pruned input 3.5 Compare with the State-of-the-art Systems We also compare our method with previous methods, including Sun et al. (2009) and Zhang et al. (2012). Because we use a different corpus, we re-implement the system Sun et al. (2009), Zhang True Result Char-based MSU-based ILP Decoding 1411 et al. (2012) and Sun et al. (2013), and experiment on our corpus. The first two are CRF+GI and DPLVM+GI in Sun et al. (2009), which are reported to outperform the methods in Tsuruoka et al. (2005) and Sun et al. (2008). For DPLVM we use the same model in Sun et al. (2009) and experiment on our own data. We also compare our approach with the method in Zhang et al. (2012). However, Zhang et al. (2012) uses different sources of search engine result information to re-rank the original candidates. We do not use any extra web resources. Because Zhang et al. (2012) uses web information only in its second stage, we use “BIEP”(the tag </context>
<context position="28632" citStr="Sun et al. (2013)" startWordPosition="4806" endWordPosition="4809">earning approach by using SVM model. Tsuruoka et al. (2005); Sun et al. (2009) formalized the process of abbreviation generation as a sequence labeling problem. In Tsuruoka et al. (2005) each character in the full form is associated with a binary value label y, which takes the value S (Skip) if the character is not in the abbreviation, and value P (Preserve) if the character is in the abbreviation. Then a MEMM model is used to model the generating process. Sun et al. (2009) followed this schema but used DPLVM model to incorporate both local and global information, which yields better results. Sun et al. (2013) also uses machine learning based methods, but focuses on the negative full form problem, which is a little different from our work. Besides these pure statistical approaches, there are also many approaches using Web as a corpus in machine learning approaches for generating abbreviations.Adar (2004) proposed methods to detect such pairs from biomedical documents. Jain et al. (2007) used web search results as well as search logs to find and rank abbreviates full pairs, which show good result. The disadvantage is that search log data is only available in a search engine backend. The ordinary app</context>
</contexts>
<marker>Sun, Li, Meng, Wang, 2013</marker>
<rawString>Sun, X., Li, W., Meng, F., and Wang, H. (2013). Generalized abbreviation prediction with negative full forms and its application on improving chinese web search. In Proceedings of the Sixth International Joint Conference on Natural Language Processing, pages 641–647, Nagoya, Japan. Asian Federation of Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Sun</author>
<author>N Okazaki</author>
<author>J Tsujii</author>
</authors>
<title>Robust approach to abbreviating terms: A discriminative latent variable model with global information.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume</booktitle>
<volume>2</volume>
<pages>905--913</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="12351" citStr="Sun et al., 2009" startWordPosition="2032" endWordPosition="2035"> Init: Let M5U5et = empty set For each word w in L: If Length(w) ≤ 2 Add w to M5U5et End if End for For each word w in L: If Length(w) &gt; 2 and no word x in M5U5et is a substring of w Add w to M5U5et End if End for Return M5U5et Table 3: Algorithm for collecting MSUs from the PKU corpus 2.2.4 Sequence Labeling Model The MSU-based method gives each MSU an extra indicative label. Therefore any sequence labeling model is appropriate for the method. Previous works showed that Conditional Random Fields (CRFs) can outperform other sequence labeling models like MEMMs in abbreviation generation tasks (Sun et al., 2009; Tsuruoka et al., 2005). For this reason we choose CRFs model in our system. For a given full form’s MSU list, many candidate abbreviations are generated by choosing the k-best results of the CRFs. We can use the forward-backward algorithm to calculate the probability of a specified tagging result. To reduce the searching complexity in the ILP decoding process, we delete those candidate tagged sequences with low probability. 2.3 Substring Based Tagging As mentioned in the introduction, the sequence labeling method, no matter character-based or MSU-based, perform badly when the “character dupl</context>
<context position="19400" citStr="Sun et al. (2009)" startWordPosition="3229" endWordPosition="3232">periments. The corpus is similar to the corpus used in Sun et al. (2008, 2009); Zhang et al. (2012). It contains 8, 015 Chinese abbreviations, including noun phrases, organization names and some other types. Some examples are presented in TABLE 8. We use 80% abbreviations as training data and the rest as testing data. In some cases, a long phrase may contain more than one abbreviation. For these cases, the corpus just keeps their most commonly used abbreviation for each full form. The evaluation metric used in our experiment is the top-K accuracy, which is also used by Tsuruoka et al. (2005), Sun et al. (2009) and � Q i=1 +A3 1409 1. xi ∈ {0, 1}, yi ∈ {0, 1}, zi ∈ {0, 1} 2. EN i=1 yi ≤ 1 3. i=1 zi ≤ 1 4. ∀Ri ∈ R, Sj ∈ S, if Ri and Sj have a same position but the position gets different labels, then yi + xj ≤ 1 5. ∀Ti ∈ T, Sj ∈ S, if Ti and Sj have a same position but the position gets different labels, then zi + xj ≤ 1 6. ∀Ri ∈ R, Tj ∈ T, if Ri and Tj have a same position but the position gets different labels, then xi + zj ≤ 1 7. ∀Si, Sj ∈ S if Si and Sj have a same position but the position gets different labels, then zi + zj ≤ 1 8. ∀Si, Sj ∈ S if the last character Si keeps is the same as the fi</context>
<context position="25295" citStr="Sun et al. (2009)" startWordPosition="4238" endWordPosition="4241">ediction speed over taking the raw input. From TABLE 14 we can also see that the top1 accuracy is not affected by these pruning constraints. This is obvious, because CRF itself has a strong modeling ability. The pruning constraints cannot improve the model accuracy. But they can help eliminate those false candidates to make the ILP decoding faster. Accuracy Average length Time(s) raw 0.6103 34.4 12.5 pruned 0.6103 25.5 7.1 Table 14: Comparison of testing time of raw input and pruned input 3.5 Compare with the State-of-the-art Systems We also compare our method with previous methods, including Sun et al. (2009) and Zhang et al. (2012). Because we use a different corpus, we re-implement the system Sun et al. (2009), Zhang True Result Char-based MSU-based ILP Decoding 1411 et al. (2012) and Sun et al. (2013), and experiment on our corpus. The first two are CRF+GI and DPLVM+GI in Sun et al. (2009), which are reported to outperform the methods in Tsuruoka et al. (2005) and Sun et al. (2008). For DPLVM we use the same model in Sun et al. (2009) and experiment on our own data. We also compare our approach with the method in Zhang et al. (2012). However, Zhang et al. (2012) uses different sources of search</context>
<context position="28093" citStr="Sun et al. (2009)" startWordPosition="4711" endWordPosition="4714">ion generation”. There is research on heuristic rules for generating abbreviations Barrett and Grems (1960); Bourne and Ford (1961); Taghva and Gilbreth (1999); Park and Byrd (2001); Wren et al. (2002); Hearst (2003). Most of them achieved high performance. However, hand-crafted rules are time consuming to create, and it is not easy to transfer the knowledge of rules from one language to another. Recent studies of abbreviation generation have focused on the use of machine learning techniques. Sun et al. (2008) proposed a supervised learning approach by using SVM model. Tsuruoka et al. (2005); Sun et al. (2009) formalized the process of abbreviation generation as a sequence labeling problem. In Tsuruoka et al. (2005) each character in the full form is associated with a binary value label y, which takes the value S (Skip) if the character is not in the abbreviation, and value P (Preserve) if the character is in the abbreviation. Then a MEMM model is used to model the generating process. Sun et al. (2009) followed this schema but used DPLVM model to incorporate both local and global information, which yields better results. Sun et al. (2013) also uses machine learning based methods, but focuses on the</context>
</contexts>
<marker>Sun, Okazaki, Tsujii, 2009</marker>
<rawString>Sun, X., Okazaki, N., and Tsujii, J. (2009). Robust approach to abbreviating terms: A discriminative latent variable model with global information. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 2-Volume 2, pages 905–913. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Sun</author>
<author>H Wang</author>
<author>B Wang</author>
</authors>
<title>Predicting chinese abbreviations from definitions: An empirical learning approach using support vector regression.</title>
<date>2008</date>
<journal>Journal of Computer Science and Technology,</journal>
<volume>23</volume>
<issue>4</issue>
<contexts>
<context position="18854" citStr="Sun et al. (2008" startWordPosition="3136" endWordPosition="3139">uplicate” problem. When two identical characters Table 7: Context templates used in pruning With respect to a training corpus, if a context C relative to c0 always assigns a certain tag t to c0, then we can use this constraint in pruning. We judge the degree of “always” by checking whether count(Cnto=t) &gt; threshold. The threshold is a count(C) non-negative real number under 1.0. 3 Experiments 3.1 Data and Evaluation Metric We use the abbreviation corpus provided by Institute of Computational Linguistics (ICL) of Peking University in our experiments. The corpus is similar to the corpus used in Sun et al. (2008, 2009); Zhang et al. (2012). It contains 8, 015 Chinese abbreviations, including noun phrases, organization names and some other types. Some examples are presented in TABLE 8. We use 80% abbreviations as training data and the rest as testing data. In some cases, a long phrase may contain more than one abbreviation. For these cases, the corpus just keeps their most commonly used abbreviation for each full form. The evaluation metric used in our experiment is the top-K accuracy, which is also used by Tsuruoka et al. (2005), Sun et al. (2009) and � Q i=1 +A3 1409 1. xi ∈ {0, 1}, yi ∈ {0, 1}, zi </context>
<context position="25678" citStr="Sun et al. (2008)" startWordPosition="4308" endWordPosition="4311">raw 0.6103 34.4 12.5 pruned 0.6103 25.5 7.1 Table 14: Comparison of testing time of raw input and pruned input 3.5 Compare with the State-of-the-art Systems We also compare our method with previous methods, including Sun et al. (2009) and Zhang et al. (2012). Because we use a different corpus, we re-implement the system Sun et al. (2009), Zhang True Result Char-based MSU-based ILP Decoding 1411 et al. (2012) and Sun et al. (2013), and experiment on our corpus. The first two are CRF+GI and DPLVM+GI in Sun et al. (2009), which are reported to outperform the methods in Tsuruoka et al. (2005) and Sun et al. (2008). For DPLVM we use the same model in Sun et al. (2009) and experiment on our own data. We also compare our approach with the method in Zhang et al. (2012). However, Zhang et al. (2012) uses different sources of search engine result information to re-rank the original candidates. We do not use any extra web resources. Because Zhang et al. (2012) uses web information only in its second stage, we use “BIEP”(the tag set used by Zhang et al. (2012)) to denote the first stage of Zhang et al. (2012), which also uses no web information. TABLE 15 shows the results of the comparisons. We can see that ou</context>
<context position="27991" citStr="Sun et al. (2008)" startWordPosition="4693" endWordPosition="4696">pproach is to find the reference for a full form given the context, which is referred to as “abbreviation generation”. There is research on heuristic rules for generating abbreviations Barrett and Grems (1960); Bourne and Ford (1961); Taghva and Gilbreth (1999); Park and Byrd (2001); Wren et al. (2002); Hearst (2003). Most of them achieved high performance. However, hand-crafted rules are time consuming to create, and it is not easy to transfer the knowledge of rules from one language to another. Recent studies of abbreviation generation have focused on the use of machine learning techniques. Sun et al. (2008) proposed a supervised learning approach by using SVM model. Tsuruoka et al. (2005); Sun et al. (2009) formalized the process of abbreviation generation as a sequence labeling problem. In Tsuruoka et al. (2005) each character in the full form is associated with a binary value label y, which takes the value S (Skip) if the character is not in the abbreviation, and value P (Preserve) if the character is in the abbreviation. Then a MEMM model is used to model the generating process. Sun et al. (2009) followed this schema but used DPLVM model to incorporate both local and global information, which</context>
</contexts>
<marker>Sun, Wang, Wang, 2008</marker>
<rawString>Sun, X., Wang, H., and Wang, B. (2008). Predicting chinese abbreviations from definitions: An empirical learning approach using support vector regression. Journal of Computer Science and Technology, 23(4):602–611.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Taghva</author>
<author>J Gilbreth</author>
</authors>
<title>Recognizing acronyms and their definitions.</title>
<date>1999</date>
<journal>International Journal on Document Analysis and Recognition,</journal>
<volume>1</volume>
<issue>4</issue>
<contexts>
<context position="27635" citStr="Taghva and Gilbreth (1999)" startWordPosition="4632" endWordPosition="4635">e don’t know the corresponding abbreviation. To solve this problem, some approaches maintain a database of abbreviations and their corresponding “full form” pairs. The major problem of pure database-building approach is obvious. It is impossible to cover all abbreviations, and the building process is quit laborious. To find these pairs automatically, a powerful approach is to find the reference for a full form given the context, which is referred to as “abbreviation generation”. There is research on heuristic rules for generating abbreviations Barrett and Grems (1960); Bourne and Ford (1961); Taghva and Gilbreth (1999); Park and Byrd (2001); Wren et al. (2002); Hearst (2003). Most of them achieved high performance. However, hand-crafted rules are time consuming to create, and it is not easy to transfer the knowledge of rules from one language to another. Recent studies of abbreviation generation have focused on the use of machine learning techniques. Sun et al. (2008) proposed a supervised learning approach by using SVM model. Tsuruoka et al. (2005); Sun et al. (2009) formalized the process of abbreviation generation as a sequence labeling problem. In Tsuruoka et al. (2005) each character in the full form i</context>
</contexts>
<marker>Taghva, Gilbreth, 1999</marker>
<rawString>Taghva, K. and Gilbreth, J. (1999). Recognizing acronyms and their definitions. International Journal on Document Analysis and Recognition, 1(4):191–198.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Tsuruoka</author>
<author>S Ananiadou</author>
<author>J Tsujii</author>
</authors>
<title>A machine learning approach to acronym generation.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL-ISMB Workshop on Linking Biological Literature, Ontologies and Databases: Mining Biological Semantics,</booktitle>
<pages>25--31</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="12375" citStr="Tsuruoka et al., 2005" startWordPosition="2036" endWordPosition="2039">= empty set For each word w in L: If Length(w) ≤ 2 Add w to M5U5et End if End for For each word w in L: If Length(w) &gt; 2 and no word x in M5U5et is a substring of w Add w to M5U5et End if End for Return M5U5et Table 3: Algorithm for collecting MSUs from the PKU corpus 2.2.4 Sequence Labeling Model The MSU-based method gives each MSU an extra indicative label. Therefore any sequence labeling model is appropriate for the method. Previous works showed that Conditional Random Fields (CRFs) can outperform other sequence labeling models like MEMMs in abbreviation generation tasks (Sun et al., 2009; Tsuruoka et al., 2005). For this reason we choose CRFs model in our system. For a given full form’s MSU list, many candidate abbreviations are generated by choosing the k-best results of the CRFs. We can use the forward-backward algorithm to calculate the probability of a specified tagging result. To reduce the searching complexity in the ILP decoding process, we delete those candidate tagged sequences with low probability. 2.3 Substring Based Tagging As mentioned in the introduction, the sequence labeling method, no matter character-based or MSU-based, perform badly when the “character duplication” phenomenon exis</context>
<context position="19381" citStr="Tsuruoka et al. (2005)" startWordPosition="3225" endWordPosition="3228">ing University in our experiments. The corpus is similar to the corpus used in Sun et al. (2008, 2009); Zhang et al. (2012). It contains 8, 015 Chinese abbreviations, including noun phrases, organization names and some other types. Some examples are presented in TABLE 8. We use 80% abbreviations as training data and the rest as testing data. In some cases, a long phrase may contain more than one abbreviation. For these cases, the corpus just keeps their most commonly used abbreviation for each full form. The evaluation metric used in our experiment is the top-K accuracy, which is also used by Tsuruoka et al. (2005), Sun et al. (2009) and � Q i=1 +A3 1409 1. xi ∈ {0, 1}, yi ∈ {0, 1}, zi ∈ {0, 1} 2. EN i=1 yi ≤ 1 3. i=1 zi ≤ 1 4. ∀Ri ∈ R, Sj ∈ S, if Ri and Sj have a same position but the position gets different labels, then yi + xj ≤ 1 5. ∀Ti ∈ T, Sj ∈ S, if Ti and Sj have a same position but the position gets different labels, then zi + xj ≤ 1 6. ∀Ri ∈ R, Tj ∈ T, if Ri and Tj have a same position but the position gets different labels, then xi + zj ≤ 1 7. ∀Si, Sj ∈ S if Si and Sj have a same position but the position gets different labels, then zi + zj ≤ 1 8. ∀Si, Sj ∈ S if the last character Si keeps is</context>
<context position="25656" citStr="Tsuruoka et al. (2005)" startWordPosition="4303" endWordPosition="4306">acy Average length Time(s) raw 0.6103 34.4 12.5 pruned 0.6103 25.5 7.1 Table 14: Comparison of testing time of raw input and pruned input 3.5 Compare with the State-of-the-art Systems We also compare our method with previous methods, including Sun et al. (2009) and Zhang et al. (2012). Because we use a different corpus, we re-implement the system Sun et al. (2009), Zhang True Result Char-based MSU-based ILP Decoding 1411 et al. (2012) and Sun et al. (2013), and experiment on our corpus. The first two are CRF+GI and DPLVM+GI in Sun et al. (2009), which are reported to outperform the methods in Tsuruoka et al. (2005) and Sun et al. (2008). For DPLVM we use the same model in Sun et al. (2009) and experiment on our own data. We also compare our approach with the method in Zhang et al. (2012). However, Zhang et al. (2012) uses different sources of search engine result information to re-rank the original candidates. We do not use any extra web resources. Because Zhang et al. (2012) uses web information only in its second stage, we use “BIEP”(the tag set used by Zhang et al. (2012)) to denote the first stage of Zhang et al. (2012), which also uses no web information. TABLE 15 shows the results of the compariso</context>
<context position="28074" citStr="Tsuruoka et al. (2005)" startWordPosition="4706" endWordPosition="4710">eferred to as “abbreviation generation”. There is research on heuristic rules for generating abbreviations Barrett and Grems (1960); Bourne and Ford (1961); Taghva and Gilbreth (1999); Park and Byrd (2001); Wren et al. (2002); Hearst (2003). Most of them achieved high performance. However, hand-crafted rules are time consuming to create, and it is not easy to transfer the knowledge of rules from one language to another. Recent studies of abbreviation generation have focused on the use of machine learning techniques. Sun et al. (2008) proposed a supervised learning approach by using SVM model. Tsuruoka et al. (2005); Sun et al. (2009) formalized the process of abbreviation generation as a sequence labeling problem. In Tsuruoka et al. (2005) each character in the full form is associated with a binary value label y, which takes the value S (Skip) if the character is not in the abbreviation, and value P (Preserve) if the character is in the abbreviation. Then a MEMM model is used to model the generating process. Sun et al. (2009) followed this schema but used DPLVM model to incorporate both local and global information, which yields better results. Sun et al. (2013) also uses machine learning based methods,</context>
</contexts>
<marker>Tsuruoka, Ananiadou, Tsujii, 2005</marker>
<rawString>Tsuruoka, Y., Ananiadou, S., and Tsujii, J. (2005). A machine learning approach to acronym generation. In Proceedings of the ACL-ISMB Workshop on Linking Biological Literature, Ontologies and Databases: Mining Biological Semantics, pages 25–31. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Wren</author>
<author>H Garner</author>
</authors>
<title>Heuristics for identification of acronym-definition patterns within text: towards an automated construction of comprehensive acronym-definition dictionaries. Methods of information in medicine,</title>
<date>2002</date>
<pages>41--5</pages>
<marker>Wren, Garner, 2002</marker>
<rawString>Wren, J., Garner, H., et al. (2002). Heuristics for identification of acronym-definition patterns within text: towards an automated construction of comprehensive acronym-definition dictionaries. Methods of information in medicine, 41(5):426–434.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Yu</author>
<author>W Kim</author>
<author>V Hatzivassiloglou</author>
<author>J Wilbur</author>
</authors>
<title>A large scale, corpus-based approach for automatically disambiguating biomedical abbreviations.</title>
<date>2006</date>
<journal>ACM Transactions on Information Systems (TOIS),</journal>
<volume>24</volume>
<issue>3</issue>
<contexts>
<context position="26833" citStr="Yu et al., 2006" startWordPosition="4506" endWordPosition="4509"> 15 shows the results of the comparisons. We can see that our method outperforms all other methods which use no extra resource. Because Zhang et al. (2012) uses extra web resource, the top-1 accuracy of Zhang et al. (2012) is slightly better than ours. Method Top-1 Accuracy CRF+GI 0.5850 DPLVM+GI 0.5990 BIEP 0.5812 Zhang et al. (2012) 0.6205 Our Result 0.6103 Table 15: Comparison with the state-of-the-art systems 4 Related Work Previous research mainly focuses on “abbreviation disambiguation”, and machine learning approaches are commonly used (Park and Byrd, 2001; HaCohen-Kerner et al., 2008; Yu et al., 2006; Ao and Takagi, 2005). These ways of linking abbreviation pairs are effective, however, they cannot solve our problem directly. In many cases the full form is definite while we don’t know the corresponding abbreviation. To solve this problem, some approaches maintain a database of abbreviations and their corresponding “full form” pairs. The major problem of pure database-building approach is obvious. It is impossible to cover all abbreviations, and the building process is quit laborious. To find these pairs automatically, a powerful approach is to find the reference for a full form given the </context>
</contexts>
<marker>Yu, Kim, Hatzivassiloglou, Wilbur, 2006</marker>
<rawString>Yu, H., Kim, W., Hatzivassiloglou, V., and Wilbur, J. (2006). A large scale, corpus-based approach for automatically disambiguating biomedical abbreviations. ACM Transactions on Information Systems (TOIS), 24(3):380–404.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Zhang</author>
<author>S Li</author>
<author>H Wang</author>
<author>N Sun</author>
<author>X Meng</author>
</authors>
<title>Constructing Chinese abbreviation dictionary: A stacked approach.</title>
<date>2012</date>
<booktitle>In Proceedings of COLING 2012,</booktitle>
<pages>3055--3070</pages>
<location>Mumbai,</location>
<contexts>
<context position="13762" citStr="Zhang et al. (2012)" startWordPosition="2263" endWordPosition="2266">be kept in the abbreviation and which one to be skipped. On the other hand, we can tag the substrings of the full form to find the local representative characters in the substrings of the long full form. Therefore, we propose the sub-string based approach to given labeling results on sub-strings. These results can be integrated into a more accurate result using ILP constraints, which we will describe in the next section. Another reason for using the sub-string based methods is that long full forms contain more characters and are much easier to make mistakes during the sequence labeling phase. Zhang et al. (2012) shows that if the full form contains less than 5 characters, a simple tagger can reach an accuracy of 70%. Zhang et al. (2012) also shows that if the full form is longer than 10 characters, the average accuracy is less than 30%. The numerous potential candidates make it hard for the tagger to choose the correct one. For the long full forms, although the whole sequence is not correctly labeled, we find that if we only consider its short substrings, we may find the correct representative characters. This information can be integrated into the decoding model to adjust the final result. We use th</context>
<context position="18882" citStr="Zhang et al. (2012)" startWordPosition="3141" endWordPosition="3144">wo identical characters Table 7: Context templates used in pruning With respect to a training corpus, if a context C relative to c0 always assigns a certain tag t to c0, then we can use this constraint in pruning. We judge the degree of “always” by checking whether count(Cnto=t) &gt; threshold. The threshold is a count(C) non-negative real number under 1.0. 3 Experiments 3.1 Data and Evaluation Metric We use the abbreviation corpus provided by Institute of Computational Linguistics (ICL) of Peking University in our experiments. The corpus is similar to the corpus used in Sun et al. (2008, 2009); Zhang et al. (2012). It contains 8, 015 Chinese abbreviations, including noun phrases, organization names and some other types. Some examples are presented in TABLE 8. We use 80% abbreviations as training data and the rest as testing data. In some cases, a long phrase may contain more than one abbreviation. For these cases, the corpus just keeps their most commonly used abbreviation for each full form. The evaluation metric used in our experiment is the top-K accuracy, which is also used by Tsuruoka et al. (2005), Sun et al. (2009) and � Q i=1 +A3 1409 1. xi ∈ {0, 1}, yi ∈ {0, 1}, zi ∈ {0, 1} 2. EN i=1 yi ≤ 1 3.</context>
<context position="20373" citStr="Zhang et al. (2012)" startWordPosition="3438" endWordPosition="3441">a same position but the position gets different labels, then xi + zj ≤ 1 7. ∀Si, Sj ∈ S if Si and Sj have a same position but the position gets different labels, then zi + zj ≤ 1 8. ∀Si, Sj ∈ S if the last character Si keeps is the same as the first character Sj keeps, then zi + zj ≤ 1 Table 6: Constraints for ILP Type Full form Abbreviation Noun Phrase ft p #(Excellent articles) VA.) Organization \*�k�(Writers’ Association) \� Coordinate phrase xf_$./Et(Injuries and deaths) $t Proper noun ’Ë(Media) � Table 8: Examples of the corpus (Noun Phrase, Organization, Coordinate Phrase, Proper Noun) Zhang et al. (2012). The top-K accuracy measures what percentage of the reference abbreviations are found if we take the top N candidate abbreviations from all the results. In our experiment, top-10 candidates are considered in re-ranking phrase and the measurement used is top-1 accuracy (which is the accuracy we usually refer to) because the final aim of the algorithm is to detect the exact abbreviation. CRF++7 , an open source linear chain CRF tool, is used in the sequence labeling part. For ILP part, we use lpsolve8, which is also an open source tool. The parameters of these tools are tuned through cross-vali</context>
<context position="25319" citStr="Zhang et al. (2012)" startWordPosition="4243" endWordPosition="4246">king the raw input. From TABLE 14 we can also see that the top1 accuracy is not affected by these pruning constraints. This is obvious, because CRF itself has a strong modeling ability. The pruning constraints cannot improve the model accuracy. But they can help eliminate those false candidates to make the ILP decoding faster. Accuracy Average length Time(s) raw 0.6103 34.4 12.5 pruned 0.6103 25.5 7.1 Table 14: Comparison of testing time of raw input and pruned input 3.5 Compare with the State-of-the-art Systems We also compare our method with previous methods, including Sun et al. (2009) and Zhang et al. (2012). Because we use a different corpus, we re-implement the system Sun et al. (2009), Zhang True Result Char-based MSU-based ILP Decoding 1411 et al. (2012) and Sun et al. (2013), and experiment on our corpus. The first two are CRF+GI and DPLVM+GI in Sun et al. (2009), which are reported to outperform the methods in Tsuruoka et al. (2005) and Sun et al. (2008). For DPLVM we use the same model in Sun et al. (2009) and experiment on our own data. We also compare our approach with the method in Zhang et al. (2012). However, Zhang et al. (2012) uses different sources of search engine result informati</context>
<context position="26554" citStr="Zhang et al. (2012)" startWordPosition="4464" endWordPosition="4467">e original candidates. We do not use any extra web resources. Because Zhang et al. (2012) uses web information only in its second stage, we use “BIEP”(the tag set used by Zhang et al. (2012)) to denote the first stage of Zhang et al. (2012), which also uses no web information. TABLE 15 shows the results of the comparisons. We can see that our method outperforms all other methods which use no extra resource. Because Zhang et al. (2012) uses extra web resource, the top-1 accuracy of Zhang et al. (2012) is slightly better than ours. Method Top-1 Accuracy CRF+GI 0.5850 DPLVM+GI 0.5990 BIEP 0.5812 Zhang et al. (2012) 0.6205 Our Result 0.6103 Table 15: Comparison with the state-of-the-art systems 4 Related Work Previous research mainly focuses on “abbreviation disambiguation”, and machine learning approaches are commonly used (Park and Byrd, 2001; HaCohen-Kerner et al., 2008; Yu et al., 2006; Ao and Takagi, 2005). These ways of linking abbreviation pairs are effective, however, they cannot solve our problem directly. In many cases the full form is definite while we don’t know the corresponding abbreviation. To solve this problem, some approaches maintain a database of abbreviations and their corresponding </context>
<context position="29306" citStr="Zhang et al. (2012)" startWordPosition="4917" endWordPosition="4920">on the negative full form problem, which is a little different from our work. Besides these pure statistical approaches, there are also many approaches using Web as a corpus in machine learning approaches for generating abbreviations.Adar (2004) proposed methods to detect such pairs from biomedical documents. Jain et al. (2007) used web search results as well as search logs to find and rank abbreviates full pairs, which show good result. The disadvantage is that search log data is only available in a search engine backend. The ordinary approaches do not have access to search engine internals. Zhang et al. (2012) used web search engine information to rerank the candidate abbreviations generated by statistical approaches. Compared to their approaches, our method uses no extra resource, but reaches comparable results. ILP shows good results in many NLP tasks. Punyakanok et al. (2004); Roth and Yih (2005) used it in semantic role labeling (SRL). Martins et al. (2009) used it in dependency parsing. (Zhao and Marcus, 2012) used it in Chinese word segmentation. (Riedel and Clarke, 2006) used ILP 1412 in dependency parsing. However, previous works mainly focus on the constraints of avoiding boundary conflict</context>
</contexts>
<marker>Zhang, Li, Wang, Sun, Meng, 2012</marker>
<rawString>Zhang, L., Li, S., Wang, H., Sun, N., and Meng, X. (2012). Constructing Chinese abbreviation dictionary: A stacked approach. In Proceedings of COLING 2012, pages 3055–3070, Mumbai, India. The COLING 2012 Organizing Committee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Q Zhao</author>
<author>M Marcus</author>
</authors>
<title>Exploring deterministic constraints: from a constrained english pos tagger to an efficient ilp solution to chinese word segmentation.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>1054--1062</pages>
<institution>Jeju Island, Korea. Association for Computational Linguistics.</institution>
<contexts>
<context position="29719" citStr="Zhao and Marcus, 2012" startWordPosition="4983" endWordPosition="4986"> pairs, which show good result. The disadvantage is that search log data is only available in a search engine backend. The ordinary approaches do not have access to search engine internals. Zhang et al. (2012) used web search engine information to rerank the candidate abbreviations generated by statistical approaches. Compared to their approaches, our method uses no extra resource, but reaches comparable results. ILP shows good results in many NLP tasks. Punyakanok et al. (2004); Roth and Yih (2005) used it in semantic role labeling (SRL). Martins et al. (2009) used it in dependency parsing. (Zhao and Marcus, 2012) used it in Chinese word segmentation. (Riedel and Clarke, 2006) used ILP 1412 in dependency parsing. However, previous works mainly focus on the constraints of avoiding boundary confliction. For example, in SRL, two argument of cannot overlap. In CWS, two Chinese words cannot share a same character. Different to their methods, we investigate on the conflict of labels of character sub-sequences. 5 Conclusion and Future work We propose a new Chinese abbreviation prediction method which can incorporate rich local information while generating the abbreviation globally. We propose the MSU, which i</context>
</contexts>
<marker>Zhao, Marcus, 2012</marker>
<rawString>Zhao, Q. and Marcus, M. (2012). Exploring deterministic constraints: from a constrained english pos tagger to an efficient ilp solution to chinese word segmentation. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1054–1062, Jeju Island, Korea. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
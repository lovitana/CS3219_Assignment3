<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001590">
<title confidence="0.925851">
#TAGSPACE: Semantic Embeddings from Hashtags
</title>
<author confidence="0.956408">
Jason Weston
</author>
<affiliation confidence="0.866502">
Facebook AI Research
</affiliation>
<email confidence="0.968506">
jase@fb.com
</email>
<author confidence="0.773961">
Sumit Chopra
</author>
<affiliation confidence="0.703973">
Facebook AI Research
</affiliation>
<email confidence="0.944535">
spchopra@fb.com
</email>
<author confidence="0.704315">
Keith Adams
</author>
<affiliation confidence="0.649311">
Facebook AI Research
</affiliation>
<email confidence="0.980536">
kma@fb.com
</email>
<sectionHeader confidence="0.993538" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999906769230769">
We describe a convolutional neural net-
work that learns feature representations for
short textual posts using hashtags as a su-
pervised signal. The proposed approach is
trained on up to 5.5 billion words predict-
ing 100,000 possible hashtags. As well as
strong performance on the hashtag predic-
tion task itself, we show that its learned
representation of text (ignoring the hash-
tag labels) is useful for other tasks as well.
To that end, we present results on a docu-
ment recommendation task, where it also
outperforms a number of baselines.
</bodyText>
<sectionHeader confidence="0.998796" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999942830769231">
Hashtags (single tokens often composed of nat-
ural language n-grams or abbreviations, prefixed
with the character ‘#’) are ubiquitous on social
networking services, particularly in short textual
documents (a.k.a. posts). Authors use hashtags to
diverse ends, many of which can be seen as labels
for classical NLP tasks: disambiguation (chips
#futurism vs. chips #junkfood); identi-
fication of named entities (#sf49ers); sentiment
(#dislike); and topic annotation (#yoga).
Hashtag prediction is the task of mapping text to
its accompanying hashtags. In this work we pro-
pose a novel model for hashtag prediction, and
show that this task is also a useful surrogate for
learning good representations of text.
Latent representations, or embeddings, are vec-
torial representations of words or documents, tra-
ditionally learned in an unsupervised manner over
large corpora. For example LSA (Deerwester et
al., 1990) and its variants, and more recent neural-
network inspired methods like those of Bengio et
al. (2006), Collobert et al. (2011) and word2vec
(Mikolov et al., 2013) learn word embeddings. In
the word embedding paradigm, each word is rep-
resented as a vector in R&apos;, where n is a hyper-
parameter that controls capacity. The embeddings
of words comprising a text are combined using a
model-dependent, possibly learned function, pro-
ducing a point in the same embedding space. A
similarity measure (for example, inner product)
gauges the pairwise relevance of points in the em-
bedding space.
Unsupervised word embedding methods train
with a reconstruction objective in which the em-
beddings are used to predict the original text. For
example, word2vec tries to predict all the words
in the document, given the embeddings of sur-
rounding words. We argue that hashtag predic-
tion provides a more direct form of supervision:
the tags are a labeling by the author of the salient
aspects of the text. Hence, predicting them may
provide stronger semantic guidance than unsuper-
vised learning alone. The abundance of hashtags
in real posts provides a huge labeled dataset for
learning potentially sophisticated models.
In this work we develop a convolutional net-
work for large scale ranking tasks, and apply it
to hashtag prediction. Our model represents both
words and the entire textual post as embeddings as
intermediate steps. We show that our method out-
performs existing unsupervised (word2vec) and
supervised (WSABIE (Weston et al., 2011)) em-
bedding methods, and other baselines, at the hash-
tag prediction task.
We then probe our model’s generality, by trans-
fering its learned representations to the task of per-
sonalized document recommendation: for each of
M users, given N previous positive interactions
with documents (likes, clicks, etc.), predict the
N + 1’th document the user will positively inter-
act with. To perform well on this task, the rep-
resentation should capture the user’s interest in
textual content. We find representations trained
on hashtag prediction outperform representations
from unsupervised learning, and that our convolu-
</bodyText>
<page confidence="0.913521">
1822
</page>
<bodyText confidence="0.485365">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1822–1827,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</bodyText>
<figure confidence="0.744125791666666">
tanh
layer
tanh
layer
linear
layer
convolution
layer
max
pooling
t
hashtag
lookup
table
(l + K - 1) ⇥ d l ⇥ H l ⇥ H H H d
f(w, t)
word
lookup
table
N ⇥ d
W1
W2
wi
d
</figure>
<figureCaption confidence="0.999218">
Figure 1: #TAGSPACE convolutional network f(w, t) for scoring a (document, hashtag) pair.
</figureCaption>
<bodyText confidence="0.9995065">
tional architecture performs better than WSABIE
trained on the same hashtag task.
</bodyText>
<sectionHeader confidence="0.99631" genericHeader="introduction">
2 Prior Work
</sectionHeader>
<bodyText confidence="0.999400451612903">
Some previous work (Davidov et al., 2010; Godin
et al., 2013; She and Chen, 2014) has addressed
hashtag prediction. Most such work applies to
much smaller sets of hashtags than the 100,000 we
consider, with the notable exception of Ding et al.
(2012), which uses an unsupervised method.
As mentioned in Section 1, many approaches
learn unsupervised word embeddings. In our
experiments we use word2vec (Mikolov et al.,
2013) as a representative scalable model for un-
supervised embeddings. WSABIE (Weston et al.,
2011) is a supervised embedding approach that has
shown promise in NLP tasks (Weston et al., 2013;
Hermann et al., 2014). WSABIE is shallow, linear,
and ignores word order information, and so may
have less modeling power than our approach.
Convolutional neural networks (CNNs), in
which shared weights are applied across the in-
put, are popular in the vision domain and have re-
cently been applied to semantic role labeling (Col-
lobert et al., 2011) and parsing (Collobert, 2011).
Neural networks in general have also been applied
to part-of-speech tagging, chunking, named en-
tity recognition (Collobert et al., 2011; Turian et
al., 2010), and sentiment detection (Socher et al.,
2013). All these tasks involve predicting a limited
(2-30) number of labels. In this work, we make
use of CNNs, but apply them to the task of rank-
ing a very large set of tags. We thus propose a
model and training scheme that can scale to this
class of problem.
</bodyText>
<sectionHeader confidence="0.996207" genericHeader="method">
3 Convolutional Embedding Model
</sectionHeader>
<bodyText confidence="0.99997925">
Our model #TAGSPACE (see Figure 1), like other
word embedding models, starts by assigning a d-
dimensional vector to each of the l words of an
input document w1, ... , wl, resulting in a matrix
of size l x d. This is achieved using a matrix of
N x d parameters, termed the lookup-table layer
(Collobert et al., 2011), where N is the vocabulary
size. In this work N is 106, and each row of the
matrix represents one of the million most frequent
words in the training corpus.
A convolution layer is then applied to the l x d
input matrix, which considers all successive win-
dows of text of size K, sliding over the docu-
ment from position 1 to l. This requires a fur-
ther Kd x H weights and H biases to be learned.
To account for words at the two boundaries of the
document we also apply a special padding vector
at both ends. In our experiments K was set to 5
and H was set to 1000. After the convolutional
step, a tanh nonlinearity followed by a max op-
eration over the l x H features extracts a fixed-
size (H-dimensional) global feature vector, which
is independent of document size. Finally, another
tanh non-linearity followed by a fully connected
linear layer of size Hxd is applied to represent the
entire document in the original embedding space
of d-dimensions.
Hashtags are also represented using d-
dimensional embeddings using a lookup-table.
We represent the top 100,000 most frequent tags.
For a given document w we then rank any given
hashtag t using the scoring function:
</bodyText>
<equation confidence="0.986098">
f(w,t) = econv(w) - elt(t)
</equation>
<page confidence="0.594292">
1823
</page>
<bodyText confidence="0.99938">
where e,,,,,,,(w) is the embedding of the document
by the CNN just described and elt(t) is the em-
bedding of a candidate tag t. We can thus rank all
candidate hashtags via their scores f(w, t), largest
first.
To train the above scoring function, and hence
the parameters of the model we minimize a rank-
ing loss similar to the one used in WSABIE as
a training objective: for each training example,
we sample a positive tag, compute f(w, t+), then
sample random tags t¯ up to 1000 times until
f(w, ¯t) &gt; m + f(w, t+), where m is the mar-
gin. A gradient step is then made to optimize the
pairwise hinge loss:
</bodyText>
<equation confidence="0.994874">
G = max{0, m − f(w, t+) + f(w, ¯t)}.
</equation>
<bodyText confidence="0.999818">
We use m = 0.1 in our experiments. This loss
function is referred to as the WARP loss in (We-
ston et al., 2011) and is used to approximately
optimizing the top of the ranked list, useful for
metrics like precision and recall@k. In particu-
lar, the search for a negative candidate tag means
that more energy is spent on improving the rank-
ing performance of positive labels already near the
top of the ranked list, compared to only randomly
sampling of negatives, which would optimize the
average rank instead.
Minimizing our loss is achieved with parallel
stochastic gradient descent using the hogwild al-
gorithm (Niu et al., 2011). The lookup-table lay-
ers are initialized with the embeddings learned by
WSABIE to expedite convergence. This kind of
‘pre-training’ is a standard trick in the neural net-
work literature, see e.g. (Socher et al., 2011).
The ranking loss makes our model scalable to
100,000 (or more) hashtags. At each training ex-
ample only a subset of tags have to be computed,
so it is far more efficient than a standard classifi-
cation loss that considers them all.
</bodyText>
<sectionHeader confidence="0.999669" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.894669">
4.1 Data
</subsectionHeader>
<bodyText confidence="0.999840285714286">
Our experiments use two large corpora of posts
containing hashtags from a popular social net-
work.1 The first corpus, which we call people,
consists of 201 million posts from individual user
accounts, comprising 5.5 billion words.
The second corpus, which we call pages, con-
sists of 35.3 million page posts, comprising 1.6
</bodyText>
<footnote confidence="0.477718">
1Both corpora were de-identified during collection.
</footnote>
<table confidence="0.47533975">
Posts Words
Dataset (millions) (billions) Top 4 tags
#fitness,
Pages 35.3 1.6 #beauty,
#luxury,#cars
#FacebookIs10,
People 201 5.5 #love, #tbt,
#happy
</table>
<tableCaption confidence="0.997534">
Table 1: Datasets used in hashtag prediction.
</tableCaption>
<bodyText confidence="0.999706888888889">
billion words. These posts’ authorial voice is a
public entity, such as a business, celebrity, brand,
or product. The posts in the pages dataset are pre-
sumably intended for a wider, more general audi-
ence than the posts in the people dataset. Both are
summarized in Table 1.
Both corpora comprise posts between February
1st and February 17th, 2014. Since we are not at-
tempting a multi-language model, we use a simple
trigram-based language prediction model to con-
sider only posts whose most likely language is En-
glish.
The two datasets use hashtags very differently.
The pages dataset has a fatter head, with popular
tags covering more examples. The people dataset
uses obscure tags more heavily. For example, the
top 100 tags account for 33.9% of page tags, but
only 13.1% of people tags.
</bodyText>
<subsectionHeader confidence="0.991869">
4.2 Hashtag prediction
</subsectionHeader>
<bodyText confidence="0.999599761904762">
The hashtag prediction task attempts to rank a
post’s ground-truth hashtags higher than hash-
tags it does not contain. We trained models on
both the people and page datasets, and collected
precision at 1, recall at 10, and mean rank for
50,000 randomly selected posts withheld from
training. A further 50,000 withheld posts are
used for selecting hyperparameters. We compare
#TAGSPACE with the following models:
Frequency This simple baseline ignores input
text, always ranking hashtags by their frequency
in the training data.
#words This baseline assigns each tag a static
score based on its frequency plus a large bonus if
it corresponds to a word in the input text. For ex-
ample, on input “crazy commute this am”, #words
ranks #crazy, #commute, #this and #am
highest, in frequency order.
Word2vec We trained the unsupervised model
of Mikolov et al. (2013) on both datasets, treat-
ing hashtags the same as all other words. To ap-
</bodyText>
<page confidence="0.914508">
1824
</page>
<bodyText confidence="0.9990846">
Crazy commute this am, #nyc, #snow, #puremichigan, #snowday, #snowstorm,
was lucky to even get in to work. #tubestrike, #blizzard, #commute, #snowpocalypse, #chiberia
This can’t go on anymore, #samelove, #equalrights, #equality, #equalityforall, #loveislove,
we need marriage equality now! #lgbt, #marriageequality, #noh8, #gayrights, #gaymarriage
Kevin spacey what a super hottie :) #houseofcards, #hoc, #houseofcardsseason2, #season2, #kevinspacey,
#frankunderwood, #netflix, #suits, #swoon, #hubbahubba
Went shopping today and found a really #mango, #shopping, #heaven, #100happydays, #yummy,
good place to get fresh mango. #lunch, #retailtherapy, #yum, #cravings, #wholefoods
Went running today -- #running, #ouch, #pain, #nopainnogain, #nike
my feet hurt so much! #marathontraining, #sore, #outofshape, #nikeplus, #runnerproblems
Wow, what a goal that was, #arsenal, #coyg, #ozil, #afc, #arsenalfc
just too fast, Mesut Ozil is the best! #lfc,#ynwa,#mesut,#gunners,#ucl
Working really hard on the paper #thestruggle, #smh, #lol, #collegelife, #homework
all last night. #sad,#wtf,#confused,#stressed,#work
The restaurant was too expensive #ripoff, #firstworldproblems, #smh, #fail, #justsaying
and the service was slow. #restaurant, #badservice, #food, #middleclassproblems, #neveragain
The restaurant had great food #dinner, #restaurant, #yum, #food, #delicious
and was reasonably priced. #stuffed, #goodtimes, #foodporn, #yummy, #winning
He has the longest whiskers, #cat, #kitty, #meow, #cats, #catsofinstagram
omg so sweet! #crazycatlady, #cute, #kitten, #catlady, #adorable
</bodyText>
<tableCaption confidence="0.920342">
Table 2: #TAGSPACE (256 dim) predictions for some example posts.
</tableCaption>
<bodyText confidence="0.9745781">
ply these word embeddings to ranking, we first
sum the embeddings of each word in the text (as
word2vec does), and then rank hashtags by simi-
larity of their embedding to that of the text.2
WSABIE (Weston et al., 2011) is a supervised
bilinear embedding model. Each word and tag has
an embedding. The words in a text are averaged
to produce an embedding of the text, and hash-
tags are ranked by similarity to the text embed-
ding. That is, the model is of the form:
f(w, t) = wTUTV t
where the post w is represented as a bag of words
(a sparse vector in RN), the tag is a one-hot-vector
in RN, and U and V are k × N embedding matri-
ces. The WARP loss, as described in section 3, is
used for training.
Performance of all these models at hashtag pre-
diction is summarized in Tables 3 and 4. We find
similar results for both datasets. The frequency
and #words baselines perform poorly across the
</bodyText>
<footnote confidence="0.585388625">
2Note that the unsupervised Word2vec embeddings could
be used as input to a supervised classifier, which we did not
do. For a supervised embedding baseline we instead use WS-
ABIE. WSABIE trains word embeddings U and hashtag em-
beddings V in a supervised fashion, whereas Word2vec trains
them both unsupervised. Adding supervision to Word2vec
would effectively do something in-between: U would still be
unsupervised, but V would then be supervised.
</footnote>
<bodyText confidence="0.999847892857143">
board, establishing the need to learn from text.
Among the learning models, the unsupervised
word2vec performs the worst. We believe this
is due to it being unsupervised – adding super-
vision better optimizes the metric we evaluate.
#TAGSPACE outperforms WSABIE at all dimen-
sionalities. Due to the relatively large test sets,
the results are statistically significant; for example,
comparing #TAGSPACE (64 dim) beats Wsabie (64
dim) for the page dataset 56% of the time, and
draws 23% of the time in terms of the rank met-
ric, and is statistically significant with a Wilcoxon
signed-rank test.
Some example predictions for #TAGSPACE are
given for some constructed examples in Table 2.
We also show nearest word embeddings to the
posts. Training data was collected at the time of
the pax winter storm, explaining predictions for
the first post, and Kevin Spacey appears in the
show “House of Cards,”. In all cases the hash-
tags reveal labels that capture the semantics of the
posts, not just syntactic similarity of individual
words.
Comparison to Production System We also
compare to a proprietary system in production in
Facebook for hashtag prediction. It trains a lo-
gistic regression model for every hashtag, using
a bag of unigrams, bigrams, and trigrams as the
</bodyText>
<page confidence="0.967671">
1825
</page>
<table confidence="0.99991475">
Method dim P@1 R@10 Rank
Freq. baseline - 1.06% 2.48% 11277
#words baseline - 0.90% 3.01% 11034
Word2Vec 256 1.21% 2.85% 9973
Word2Vec 512 1.14% 2.93% 8727
WSABIE 64 4.55% 8.80% 6921
WSABIE 128 5.25% 9.33% 6208
WSABIE 256 5.66% 10.34% 5519
WSABIE 512 5.92% 10.74% 5452
#TAGSPACE 64 6.69% 12.42% 3569
#TAGSPACE 128 6.91% 12.57% 3858
#TAGSPACE 256 7.37% 12.58% 3820
</table>
<tableCaption confidence="0.999638">
Table 3: Hashtag test results for people dataset.
</tableCaption>
<table confidence="0.99994225">
Method dim P@1 R@10 Rank
Freq. baseline - 4.20% 1.59% 11103
#words baseline - 2.63% 5.05% 10581
Word2Vec 256 4.66% 8.15% 10149
Word2Vec 512 5.26% 9.33% 9800
WSABIE 64 24.45% 29.64% 2619
WSABIE 128 27.47% 32.94% 2325
WSABIE 256 29.76% 35.28% 1992
WSABIE 512 30.90% 36.96% 1184
#TAGSPACE 64 34.08% 38.96% 1184
#TAGSPACE 128 36.27% 41.42% 1165
#TAGSPACE 256 37.42% 43.01% 1155
</table>
<tableCaption confidence="0.999969">
Table 4: Hashtag test results for pages dataset.
</tableCaption>
<bodyText confidence="0.999979">
input features. Unlike the other models we con-
sider here, this baseline has been trained using a
set of approximately 10 million posts. Engineer-
ing constraints prevent measuring mean rank per-
formance. We present it here as a serious effort
at solving the same problem from outside the em-
bedding paradigm. On the people dataset this sys-
tem achieves 3.47% P@1 and 5.33% R@10. On
the pages dataset it obtains 5.97% P@1 and 6.30%
R@10. It is thus outperformed by our method.
However, we note the differences in experimen-
tal setting mean this comparison is perhaps not
completely fair (different training sets). We expect
performance of linear models such as this to be
similar to WSABIE as that has been in the case in
other datasets (Gupta et al., 2014), but at the cost
of more memory usage. Note that models like lo-
gistic regression and SVM do not scale well if you
have millions of hashtags, which we could handle
in our models.
</bodyText>
<subsectionHeader confidence="0.992669">
4.3 Personalized document recommendation
</subsectionHeader>
<bodyText confidence="0.998354666666667">
To investigate the generality of these learned rep-
resentations, we apply them to the task of recom-
mending documents to users based on the user’s
interaction history. The data for this task comprise
anonymized day-long interaction histories for a
tiny subset of people on a popular social network-
</bodyText>
<table confidence="0.999859538461539">
Method dim P@1 R@10 R@50
Word2Vec 256 0.75% 1.96% 3.82%
BoW - 1.36% 4.29% 8.03%
WSABIE 64 0.98% 3.14% 6.65%
WSABIE 128 1.02% 3.30% 6.71%
WSABIE 256 1.01% 2.98% 5.99%
WSABIE 512 1.01% 2.76% 5.19%
#TAGSPACE 64 1.27% 4.56% 9.64%
#TAGSPACE 128 1.48% 4.74% 9.96%
#TAGSPACE 256 1.66% 5.29% 10.69%
WSABIE+ BoW 64 1.61% 4.83% 9.00%
#TAGSPACE+ BoW 64 1.80% 5.90% 11.22%
#TAGSPACE+ BoW 256 1.92% 6.15% 11.53%
</table>
<tableCaption confidence="0.997381">
Table 5: Document recommendation task results.
</tableCaption>
<bodyText confidence="0.999868095238095">
ing service. For each of the 34 thousand people
considered, we collected the text of between 5 and
167 posts that she has expressed previous positive
interactions with (likes, clicks, etc.). Given the
person’s trailing n −1 posts, we use our models to
predict the n’th post by ranking it against 10,000
other unrelated posts, and measuring precison and
recall. The score of the nth post is obtained by
taking the max similarity over the n −1 posts. We
use cosine similarity between post embeddings in-
stead of the inner product that was used for hash-
tag training so that the scores are not unduly influ-
enced by document length. All learned hashtag
models were trained on the people dataset. We
also consider a TF-IDF weighted bag-of-words
baseline (BoW). The results are given in Table 5.
Hashtag-based embeddings outperform BoW
and unsupervised embeddings across the board,
and #TAGSPACE outperforms WSABIE. The best
results come from summing the bag-of-words
scores with those of #TAGSPACE.
</bodyText>
<sectionHeader confidence="0.998952" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999742272727273">
#TAGSPACE is a convolutional neural network
that learns to rank hashtags with a minimum of
task-specific assumptions and engineering. It per-
forms well, beating several baselines and an in-
dustrial system engineered for hashtag prediction.
The semantics of hashtags cause #TAGSPACE to
learn a representation that captures many salient
aspects of text. This representation is general
enough to port to the task of personalized docu-
ment recommendation, where it outperforms other
well-known representations.
</bodyText>
<sectionHeader confidence="0.996514" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.99724">
We thank Ledell Wu and Jeff Pasternak for their
help with datasets and baselines.
</bodyText>
<page confidence="0.991457">
1826
</page>
<sectionHeader confidence="0.989593" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999784395604395">
Yoshua Bengio, Holger Schwenk, Jean-S´ebastien
Sen´ecal, Fr´ederic Morin, and Jean-Luc Gauvain.
2006. Neural probabilistic language models. In
Innovations in Machine Learning, pages 137–186.
Springer.
Ronan Collobert, Jason Weston, L´eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. The Journal of Machine Learning Re-
search, 12:2493–2537.
Ronan Collobert. 2011. Deep learning for efficient dis-
criminative parsing. In International Conference on
Artificial Intelligence and Statistics, number EPFL-
CONF-192374.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.
Enhanced sentiment learning using twitter hashtags
and smileys. In Proceedings of the 23rd Inter-
national Conference on Computational Linguistics:
Posters, COLING ’10, pages 241–249, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Scott C. Deerwester, Susan T Dumais, Thomas K. Lan-
dauer, George W. Furnas, and Richard A. Harshman.
1990. Indexing by latent semantic analysis. JASIS,
41(6):391–407.
Zhuoye Ding, Qi Zhang, and Xuanjing Huang. 2012.
Automatic hashtag recommendation for microblogs
using topic-specific translation model. In COLING
(Posters)’12, pages 265–274.
Fr´ederic Godin, Viktor Slavkovikj, Wesley De Neve,
Benjamin Schrauwen, and Rik Van de Walle. 2013.
Using topic models for twitter hashtag recommen-
dation. In Proceedings of the 22nd international
conference on World Wide Web companion, pages
593–596. International World Wide Web Confer-
ences Steering Committee.
Maya R Gupta, Samy Bengio, and Jason Weston.
2014. Training highly multiclass classifiers. Jour-
nal of Machine Learning Research, 15:1–48.
Karl Moritz Hermann, Dipanjan Das, Jason Weston,
and Kuzman Ganchev. 2014. Semantic frame iden-
tification with distributed word representations. In
Proceedings of ACL.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781.
Feng Niu, Benjamin Recht, Christopher R´e, and
Stephen J Wright. 2011. Hogwild!: A lock-free ap-
proach to parallelizing stochastic gradient descent.
Advances in Neural Information Processing Sys-
tems, 24:693–701.
Jieying She and Lei Chen. 2014. Tomoha: Topic
model-based hashtag recommendation on twitter. In
Proceedings of the companion publication of the
23rd international conference on World wide web
companion, pages 371–372. International World
Wide Web Conferences Steering Committee.
Richard Socher, Jeffrey Pennington, Eric H Huang,
Andrew Y Ng, and Christopher D Manning. 2011.
Semi-supervised recursive autoencoders for predict-
ing sentiment distributions. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 151–161. Association for
Computational Linguistics.
Richard Socher, Alex Perelygin, Jean Y Wu, Jason
Chuang, Christopher D Manning, Andrew Y Ng,
and Christopher Potts. 2013. Recursive deep mod-
els for semantic compositionality over a sentiment
treebank. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 1631–1642.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: A simple and general method
for semi-supervised learning. In Proceedings of the
48th Annual Meeting of the Association for Com-
putational Linguistics, ACL ’10, pages 384–394,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Jason Weston, Samy Bengio, and Nicolas Usunier.
2011. Wsabie: Scaling up to large vocabulary
image annotation. In Proceedings of the Twenty-
Second international joint conference on Artificial
Intelligence-Volume Volume Three, pages 2764–
2770. AAAI Press.
Jason Weston, Antoine Bordes, Oksana Yakhnenko,
and Nicolas Usunier. 2013. Connecting language
and knowledge bases with embedding models for
relation extraction. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language
Processing (EMNLP).
</reference>
<page confidence="0.994223">
1827
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.191883">
<title confidence="0.999819">Semantic Embeddings from Hashtags</title>
<author confidence="0.904462">Jason Facebook AI</author>
<email confidence="0.998178">jase@fb.com</email>
<title confidence="0.425521">Sumit</title>
<author confidence="0.772694">Facebook AI</author>
<email confidence="0.992914">spchopra@fb.com</email>
<author confidence="0.7665995">Keith Facebook AI</author>
<email confidence="0.999437">kma@fb.com</email>
<abstract confidence="0.998412428571428">We describe a convolutional neural network that learns feature representations for short textual posts using hashtags as a supervised signal. The proposed approach is trained on up to 5.5 billion words predicting 100,000 possible hashtags. As well as strong performance on the hashtag prediction task itself, we show that its learned representation of text (ignoring the hashtag labels) is useful for other tasks as well. To that end, we present results on a document recommendation task, where it also outperforms a number of baselines.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>Holger Schwenk</author>
<author>Jean-S´ebastien Sen´ecal</author>
<author>Fr´ederic Morin</author>
<author>Jean-Luc Gauvain</author>
</authors>
<title>Neural probabilistic language models.</title>
<date>2006</date>
<booktitle>In Innovations in Machine Learning,</booktitle>
<pages>137--186</pages>
<publisher>Springer.</publisher>
<marker>Bengio, Schwenk, Sen´ecal, Morin, Gauvain, 2006</marker>
<rawString>Yoshua Bengio, Holger Schwenk, Jean-S´ebastien Sen´ecal, Fr´ederic Morin, and Jean-Luc Gauvain. 2006. Neural probabilistic language models. In Innovations in Machine Learning, pages 137–186. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
<author>L´eon Bottou</author>
<author>Michael Karlen</author>
<author>Koray Kavukcuoglu</author>
<author>Pavel Kuksa</author>
</authors>
<title>Natural language processing (almost) from scratch.</title>
<date>2011</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>12--2493</pages>
<contexts>
<context position="1776" citStr="Collobert et al. (2011)" startWordPosition="268" endWordPosition="271">(#sf49ers); sentiment (#dislike); and topic annotation (#yoga). Hashtag prediction is the task of mapping text to its accompanying hashtags. In this work we propose a novel model for hashtag prediction, and show that this task is also a useful surrogate for learning good representations of text. Latent representations, or embeddings, are vectorial representations of words or documents, traditionally learned in an unsupervised manner over large corpora. For example LSA (Deerwester et al., 1990) and its variants, and more recent neuralnetwork inspired methods like those of Bengio et al. (2006), Collobert et al. (2011) and word2vec (Mikolov et al., 2013) learn word embeddings. In the word embedding paradigm, each word is represented as a vector in R&apos;, where n is a hyperparameter that controls capacity. The embeddings of words comprising a text are combined using a model-dependent, possibly learned function, producing a point in the same embedding space. A similarity measure (for example, inner product) gauges the pairwise relevance of points in the embedding space. Unsupervised word embedding methods train with a reconstruction objective in which the embeddings are used to predict the original text. For exa</context>
<context position="5290" citStr="Collobert et al., 2011" startWordPosition="838" endWordPosition="842">n unsupervised word embeddings. In our experiments we use word2vec (Mikolov et al., 2013) as a representative scalable model for unsupervised embeddings. WSABIE (Weston et al., 2011) is a supervised embedding approach that has shown promise in NLP tasks (Weston et al., 2013; Hermann et al., 2014). WSABIE is shallow, linear, and ignores word order information, and so may have less modeling power than our approach. Convolutional neural networks (CNNs), in which shared weights are applied across the input, are popular in the vision domain and have recently been applied to semantic role labeling (Collobert et al., 2011) and parsing (Collobert, 2011). Neural networks in general have also been applied to part-of-speech tagging, chunking, named entity recognition (Collobert et al., 2011; Turian et al., 2010), and sentiment detection (Socher et al., 2013). All these tasks involve predicting a limited (2-30) number of labels. In this work, we make use of CNNs, but apply them to the task of ranking a very large set of tags. We thus propose a model and training scheme that can scale to this class of problem. 3 Convolutional Embedding Model Our model #TAGSPACE (see Figure 1), like other word embedding models, starts</context>
</contexts>
<marker>Collobert, Weston, Bottou, Karlen, Kavukcuoglu, Kuksa, 2011</marker>
<rawString>Ronan Collobert, Jason Weston, L´eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural language processing (almost) from scratch. The Journal of Machine Learning Research, 12:2493–2537.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
</authors>
<title>Deep learning for efficient discriminative parsing.</title>
<date>2011</date>
<booktitle>In International Conference on Artificial Intelligence and Statistics, number EPFLCONF-192374.</booktitle>
<contexts>
<context position="5320" citStr="Collobert, 2011" startWordPosition="845" endWordPosition="846">r experiments we use word2vec (Mikolov et al., 2013) as a representative scalable model for unsupervised embeddings. WSABIE (Weston et al., 2011) is a supervised embedding approach that has shown promise in NLP tasks (Weston et al., 2013; Hermann et al., 2014). WSABIE is shallow, linear, and ignores word order information, and so may have less modeling power than our approach. Convolutional neural networks (CNNs), in which shared weights are applied across the input, are popular in the vision domain and have recently been applied to semantic role labeling (Collobert et al., 2011) and parsing (Collobert, 2011). Neural networks in general have also been applied to part-of-speech tagging, chunking, named entity recognition (Collobert et al., 2011; Turian et al., 2010), and sentiment detection (Socher et al., 2013). All these tasks involve predicting a limited (2-30) number of labels. In this work, we make use of CNNs, but apply them to the task of ranking a very large set of tags. We thus propose a model and training scheme that can scale to this class of problem. 3 Convolutional Embedding Model Our model #TAGSPACE (see Figure 1), like other word embedding models, starts by assigning a ddimensional v</context>
</contexts>
<marker>Collobert, 2011</marker>
<rawString>Ronan Collobert. 2011. Deep learning for efficient discriminative parsing. In International Conference on Artificial Intelligence and Statistics, number EPFLCONF-192374.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitry Davidov</author>
<author>Oren Tsur</author>
<author>Ari Rappoport</author>
</authors>
<title>Enhanced sentiment learning using twitter hashtags and smileys.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics: Posters, COLING ’10,</booktitle>
<pages>241--249</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="4373" citStr="Davidov et al., 2010" startWordPosition="689" endWordPosition="692">earning, and that our convolu1822 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1822–1827, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics tanh layer tanh layer linear layer convolution layer max pooling t hashtag lookup table (l + K - 1) ⇥ d l ⇥ H l ⇥ H H H d f(w, t) word lookup table N ⇥ d W1 W2 wi d Figure 1: #TAGSPACE convolutional network f(w, t) for scoring a (document, hashtag) pair. tional architecture performs better than WSABIE trained on the same hashtag task. 2 Prior Work Some previous work (Davidov et al., 2010; Godin et al., 2013; She and Chen, 2014) has addressed hashtag prediction. Most such work applies to much smaller sets of hashtags than the 100,000 we consider, with the notable exception of Ding et al. (2012), which uses an unsupervised method. As mentioned in Section 1, many approaches learn unsupervised word embeddings. In our experiments we use word2vec (Mikolov et al., 2013) as a representative scalable model for unsupervised embeddings. WSABIE (Weston et al., 2011) is a supervised embedding approach that has shown promise in NLP tasks (Weston et al., 2013; Hermann et al., 2014). WSABIE </context>
</contexts>
<marker>Davidov, Tsur, Rappoport, 2010</marker>
<rawString>Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010. Enhanced sentiment learning using twitter hashtags and smileys. In Proceedings of the 23rd International Conference on Computational Linguistics: Posters, COLING ’10, pages 241–249, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott C Deerwester</author>
<author>Susan T Dumais</author>
<author>Thomas K Landauer</author>
<author>George W Furnas</author>
<author>Richard A Harshman</author>
</authors>
<title>Indexing by latent semantic analysis.</title>
<date>1990</date>
<journal>JASIS,</journal>
<volume>41</volume>
<issue>6</issue>
<contexts>
<context position="1651" citStr="Deerwester et al., 1990" startWordPosition="247" endWordPosition="250">een as labels for classical NLP tasks: disambiguation (chips #futurism vs. chips #junkfood); identification of named entities (#sf49ers); sentiment (#dislike); and topic annotation (#yoga). Hashtag prediction is the task of mapping text to its accompanying hashtags. In this work we propose a novel model for hashtag prediction, and show that this task is also a useful surrogate for learning good representations of text. Latent representations, or embeddings, are vectorial representations of words or documents, traditionally learned in an unsupervised manner over large corpora. For example LSA (Deerwester et al., 1990) and its variants, and more recent neuralnetwork inspired methods like those of Bengio et al. (2006), Collobert et al. (2011) and word2vec (Mikolov et al., 2013) learn word embeddings. In the word embedding paradigm, each word is represented as a vector in R&apos;, where n is a hyperparameter that controls capacity. The embeddings of words comprising a text are combined using a model-dependent, possibly learned function, producing a point in the same embedding space. A similarity measure (for example, inner product) gauges the pairwise relevance of points in the embedding space. Unsupervised word e</context>
</contexts>
<marker>Deerwester, Dumais, Landauer, Furnas, Harshman, 1990</marker>
<rawString>Scott C. Deerwester, Susan T Dumais, Thomas K. Landauer, George W. Furnas, and Richard A. Harshman. 1990. Indexing by latent semantic analysis. JASIS, 41(6):391–407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhuoye Ding</author>
<author>Qi Zhang</author>
<author>Xuanjing Huang</author>
</authors>
<title>Automatic hashtag recommendation for microblogs using topic-specific translation model.</title>
<date>2012</date>
<booktitle>In COLING (Posters)’12,</booktitle>
<pages>265--274</pages>
<contexts>
<context position="4583" citStr="Ding et al. (2012)" startWordPosition="725" endWordPosition="728">ional Linguistics tanh layer tanh layer linear layer convolution layer max pooling t hashtag lookup table (l + K - 1) ⇥ d l ⇥ H l ⇥ H H H d f(w, t) word lookup table N ⇥ d W1 W2 wi d Figure 1: #TAGSPACE convolutional network f(w, t) for scoring a (document, hashtag) pair. tional architecture performs better than WSABIE trained on the same hashtag task. 2 Prior Work Some previous work (Davidov et al., 2010; Godin et al., 2013; She and Chen, 2014) has addressed hashtag prediction. Most such work applies to much smaller sets of hashtags than the 100,000 we consider, with the notable exception of Ding et al. (2012), which uses an unsupervised method. As mentioned in Section 1, many approaches learn unsupervised word embeddings. In our experiments we use word2vec (Mikolov et al., 2013) as a representative scalable model for unsupervised embeddings. WSABIE (Weston et al., 2011) is a supervised embedding approach that has shown promise in NLP tasks (Weston et al., 2013; Hermann et al., 2014). WSABIE is shallow, linear, and ignores word order information, and so may have less modeling power than our approach. Convolutional neural networks (CNNs), in which shared weights are applied across the input, are pop</context>
</contexts>
<marker>Ding, Zhang, Huang, 2012</marker>
<rawString>Zhuoye Ding, Qi Zhang, and Xuanjing Huang. 2012. Automatic hashtag recommendation for microblogs using topic-specific translation model. In COLING (Posters)’12, pages 265–274.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fr´ederic Godin</author>
<author>Viktor Slavkovikj</author>
<author>Wesley De Neve</author>
<author>Benjamin Schrauwen</author>
<author>Rik Van de Walle</author>
</authors>
<title>Using topic models for twitter hashtag recommendation.</title>
<date>2013</date>
<booktitle>In Proceedings of the 22nd international conference on World Wide Web companion,</booktitle>
<pages>593--596</pages>
<institution>International World Wide Web Conferences Steering Committee.</institution>
<marker>Godin, Slavkovikj, De Neve, Schrauwen, Van de Walle, 2013</marker>
<rawString>Fr´ederic Godin, Viktor Slavkovikj, Wesley De Neve, Benjamin Schrauwen, and Rik Van de Walle. 2013. Using topic models for twitter hashtag recommendation. In Proceedings of the 22nd international conference on World Wide Web companion, pages 593–596. International World Wide Web Conferences Steering Committee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maya R Gupta</author>
<author>Samy Bengio</author>
<author>Jason Weston</author>
</authors>
<title>Training highly multiclass classifiers.</title>
<date>2014</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>15--1</pages>
<contexts>
<context position="17214" citStr="Gupta et al., 2014" startWordPosition="2818" endWordPosition="2821">sts. Engineering constraints prevent measuring mean rank performance. We present it here as a serious effort at solving the same problem from outside the embedding paradigm. On the people dataset this system achieves 3.47% P@1 and 5.33% R@10. On the pages dataset it obtains 5.97% P@1 and 6.30% R@10. It is thus outperformed by our method. However, we note the differences in experimental setting mean this comparison is perhaps not completely fair (different training sets). We expect performance of linear models such as this to be similar to WSABIE as that has been in the case in other datasets (Gupta et al., 2014), but at the cost of more memory usage. Note that models like logistic regression and SVM do not scale well if you have millions of hashtags, which we could handle in our models. 4.3 Personalized document recommendation To investigate the generality of these learned representations, we apply them to the task of recommending documents to users based on the user’s interaction history. The data for this task comprise anonymized day-long interaction histories for a tiny subset of people on a popular social networkMethod dim P@1 R@10 R@50 Word2Vec 256 0.75% 1.96% 3.82% BoW - 1.36% 4.29% 8.03% WSABI</context>
</contexts>
<marker>Gupta, Bengio, Weston, 2014</marker>
<rawString>Maya R Gupta, Samy Bengio, and Jason Weston. 2014. Training highly multiclass classifiers. Journal of Machine Learning Research, 15:1–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karl Moritz Hermann</author>
<author>Dipanjan Das</author>
<author>Jason Weston</author>
<author>Kuzman Ganchev</author>
</authors>
<title>Semantic frame identification with distributed word representations.</title>
<date>2014</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="4964" citStr="Hermann et al., 2014" startWordPosition="785" endWordPosition="788">ous work (Davidov et al., 2010; Godin et al., 2013; She and Chen, 2014) has addressed hashtag prediction. Most such work applies to much smaller sets of hashtags than the 100,000 we consider, with the notable exception of Ding et al. (2012), which uses an unsupervised method. As mentioned in Section 1, many approaches learn unsupervised word embeddings. In our experiments we use word2vec (Mikolov et al., 2013) as a representative scalable model for unsupervised embeddings. WSABIE (Weston et al., 2011) is a supervised embedding approach that has shown promise in NLP tasks (Weston et al., 2013; Hermann et al., 2014). WSABIE is shallow, linear, and ignores word order information, and so may have less modeling power than our approach. Convolutional neural networks (CNNs), in which shared weights are applied across the input, are popular in the vision domain and have recently been applied to semantic role labeling (Collobert et al., 2011) and parsing (Collobert, 2011). Neural networks in general have also been applied to part-of-speech tagging, chunking, named entity recognition (Collobert et al., 2011; Turian et al., 2010), and sentiment detection (Socher et al., 2013). All these tasks involve predicting a</context>
</contexts>
<marker>Hermann, Das, Weston, Ganchev, 2014</marker>
<rawString>Karl Moritz Hermann, Dipanjan Das, Jason Weston, and Kuzman Ganchev. 2014. Semantic frame identification with distributed word representations. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.</title>
<date>2013</date>
<contexts>
<context position="1812" citStr="Mikolov et al., 2013" startWordPosition="274" endWordPosition="277">topic annotation (#yoga). Hashtag prediction is the task of mapping text to its accompanying hashtags. In this work we propose a novel model for hashtag prediction, and show that this task is also a useful surrogate for learning good representations of text. Latent representations, or embeddings, are vectorial representations of words or documents, traditionally learned in an unsupervised manner over large corpora. For example LSA (Deerwester et al., 1990) and its variants, and more recent neuralnetwork inspired methods like those of Bengio et al. (2006), Collobert et al. (2011) and word2vec (Mikolov et al., 2013) learn word embeddings. In the word embedding paradigm, each word is represented as a vector in R&apos;, where n is a hyperparameter that controls capacity. The embeddings of words comprising a text are combined using a model-dependent, possibly learned function, producing a point in the same embedding space. A similarity measure (for example, inner product) gauges the pairwise relevance of points in the embedding space. Unsupervised word embedding methods train with a reconstruction objective in which the embeddings are used to predict the original text. For example, word2vec tries to predict all </context>
<context position="4756" citStr="Mikolov et al., 2013" startWordPosition="751" endWordPosition="754">d W1 W2 wi d Figure 1: #TAGSPACE convolutional network f(w, t) for scoring a (document, hashtag) pair. tional architecture performs better than WSABIE trained on the same hashtag task. 2 Prior Work Some previous work (Davidov et al., 2010; Godin et al., 2013; She and Chen, 2014) has addressed hashtag prediction. Most such work applies to much smaller sets of hashtags than the 100,000 we consider, with the notable exception of Ding et al. (2012), which uses an unsupervised method. As mentioned in Section 1, many approaches learn unsupervised word embeddings. In our experiments we use word2vec (Mikolov et al., 2013) as a representative scalable model for unsupervised embeddings. WSABIE (Weston et al., 2011) is a supervised embedding approach that has shown promise in NLP tasks (Weston et al., 2013; Hermann et al., 2014). WSABIE is shallow, linear, and ignores word order information, and so may have less modeling power than our approach. Convolutional neural networks (CNNs), in which shared weights are applied across the input, are popular in the vision domain and have recently been applied to semantic role labeling (Collobert et al., 2011) and parsing (Collobert, 2011). Neural networks in general have al</context>
<context position="11303" citStr="Mikolov et al. (2013)" startWordPosition="1882" endWordPosition="1885">50,000 randomly selected posts withheld from training. A further 50,000 withheld posts are used for selecting hyperparameters. We compare #TAGSPACE with the following models: Frequency This simple baseline ignores input text, always ranking hashtags by their frequency in the training data. #words This baseline assigns each tag a static score based on its frequency plus a large bonus if it corresponds to a word in the input text. For example, on input “crazy commute this am”, #words ranks #crazy, #commute, #this and #am highest, in frequency order. Word2vec We trained the unsupervised model of Mikolov et al. (2013) on both datasets, treating hashtags the same as all other words. To ap1824 Crazy commute this am, #nyc, #snow, #puremichigan, #snowday, #snowstorm, was lucky to even get in to work. #tubestrike, #blizzard, #commute, #snowpocalypse, #chiberia This can’t go on anymore, #samelove, #equalrights, #equality, #equalityforall, #loveislove, we need marriage equality now! #lgbt, #marriageequality, #noh8, #gayrights, #gaymarriage Kevin spacey what a super hottie :) #houseofcards, #hoc, #houseofcardsseason2, #season2, #kevinspacey, #frankunderwood, #netflix, #suits, #swoon, #hubbahubba Went shopping toda</context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Feng Niu</author>
<author>Benjamin Recht</author>
<author>Christopher R´e</author>
<author>Stephen J Wright</author>
</authors>
<title>Hogwild!: A lock-free approach to parallelizing stochastic gradient descent.</title>
<date>2011</date>
<booktitle>Advances in Neural Information Processing Systems,</booktitle>
<pages>24--693</pages>
<marker>Niu, Recht, R´e, Wright, 2011</marker>
<rawString>Feng Niu, Benjamin Recht, Christopher R´e, and Stephen J Wright. 2011. Hogwild!: A lock-free approach to parallelizing stochastic gradient descent. Advances in Neural Information Processing Systems, 24:693–701.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jieying She</author>
<author>Lei Chen</author>
</authors>
<title>Tomoha: Topic model-based hashtag recommendation on twitter.</title>
<date>2014</date>
<booktitle>In Proceedings of the companion publication of the 23rd international conference on World wide web companion,</booktitle>
<pages>371--372</pages>
<institution>International World Wide Web Conferences Steering Committee.</institution>
<contexts>
<context position="4414" citStr="She and Chen, 2014" startWordPosition="697" endWordPosition="700">ngs of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1822–1827, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics tanh layer tanh layer linear layer convolution layer max pooling t hashtag lookup table (l + K - 1) ⇥ d l ⇥ H l ⇥ H H H d f(w, t) word lookup table N ⇥ d W1 W2 wi d Figure 1: #TAGSPACE convolutional network f(w, t) for scoring a (document, hashtag) pair. tional architecture performs better than WSABIE trained on the same hashtag task. 2 Prior Work Some previous work (Davidov et al., 2010; Godin et al., 2013; She and Chen, 2014) has addressed hashtag prediction. Most such work applies to much smaller sets of hashtags than the 100,000 we consider, with the notable exception of Ding et al. (2012), which uses an unsupervised method. As mentioned in Section 1, many approaches learn unsupervised word embeddings. In our experiments we use word2vec (Mikolov et al., 2013) as a representative scalable model for unsupervised embeddings. WSABIE (Weston et al., 2011) is a supervised embedding approach that has shown promise in NLP tasks (Weston et al., 2013; Hermann et al., 2014). WSABIE is shallow, linear, and ignores word orde</context>
</contexts>
<marker>She, Chen, 2014</marker>
<rawString>Jieying She and Lei Chen. 2014. Tomoha: Topic model-based hashtag recommendation on twitter. In Proceedings of the companion publication of the 23rd international conference on World wide web companion, pages 371–372. International World Wide Web Conferences Steering Committee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Jeffrey Pennington</author>
<author>Eric H Huang</author>
<author>Andrew Y Ng</author>
<author>Christopher D Manning</author>
</authors>
<title>Semi-supervised recursive autoencoders for predicting sentiment distributions.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>151--161</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="8808" citStr="Socher et al., 2011" startWordPosition="1473" endWordPosition="1476">k. In particular, the search for a negative candidate tag means that more energy is spent on improving the ranking performance of positive labels already near the top of the ranked list, compared to only randomly sampling of negatives, which would optimize the average rank instead. Minimizing our loss is achieved with parallel stochastic gradient descent using the hogwild algorithm (Niu et al., 2011). The lookup-table layers are initialized with the embeddings learned by WSABIE to expedite convergence. This kind of ‘pre-training’ is a standard trick in the neural network literature, see e.g. (Socher et al., 2011). The ranking loss makes our model scalable to 100,000 (or more) hashtags. At each training example only a subset of tags have to be computed, so it is far more efficient than a standard classification loss that considers them all. 4 Experiments 4.1 Data Our experiments use two large corpora of posts containing hashtags from a popular social network.1 The first corpus, which we call people, consists of 201 million posts from individual user accounts, comprising 5.5 billion words. The second corpus, which we call pages, consists of 35.3 million page posts, comprising 1.6 1Both corpora were de-i</context>
</contexts>
<marker>Socher, Pennington, Huang, Ng, Manning, 2011</marker>
<rawString>Richard Socher, Jeffrey Pennington, Eric H Huang, Andrew Y Ng, and Christopher D Manning. 2011. Semi-supervised recursive autoencoders for predicting sentiment distributions. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 151–161. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Alex Perelygin</author>
<author>Jean Y Wu</author>
<author>Jason Chuang</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
<author>Christopher Potts</author>
</authors>
<title>Recursive deep models for semantic compositionality over a sentiment treebank.</title>
<date>2013</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>1631--1642</pages>
<contexts>
<context position="5526" citStr="Socher et al., 2013" startWordPosition="874" endWordPosition="877">e in NLP tasks (Weston et al., 2013; Hermann et al., 2014). WSABIE is shallow, linear, and ignores word order information, and so may have less modeling power than our approach. Convolutional neural networks (CNNs), in which shared weights are applied across the input, are popular in the vision domain and have recently been applied to semantic role labeling (Collobert et al., 2011) and parsing (Collobert, 2011). Neural networks in general have also been applied to part-of-speech tagging, chunking, named entity recognition (Collobert et al., 2011; Turian et al., 2010), and sentiment detection (Socher et al., 2013). All these tasks involve predicting a limited (2-30) number of labels. In this work, we make use of CNNs, but apply them to the task of ranking a very large set of tags. We thus propose a model and training scheme that can scale to this class of problem. 3 Convolutional Embedding Model Our model #TAGSPACE (see Figure 1), like other word embedding models, starts by assigning a ddimensional vector to each of the l words of an input document w1, ... , wl, resulting in a matrix of size l x d. This is achieved using a matrix of N x d parameters, termed the lookup-table layer (Collobert et al., 201</context>
</contexts>
<marker>Socher, Perelygin, Wu, Chuang, Manning, Ng, Potts, 2013</marker>
<rawString>Richard Socher, Alex Perelygin, Jean Y Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1631–1642.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>Lev Ratinov</author>
<author>Yoshua Bengio</author>
</authors>
<title>Word representations: A simple and general method for semi-supervised learning.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10,</booktitle>
<pages>384--394</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="5479" citStr="Turian et al., 2010" startWordPosition="867" endWordPosition="870">rvised embedding approach that has shown promise in NLP tasks (Weston et al., 2013; Hermann et al., 2014). WSABIE is shallow, linear, and ignores word order information, and so may have less modeling power than our approach. Convolutional neural networks (CNNs), in which shared weights are applied across the input, are popular in the vision domain and have recently been applied to semantic role labeling (Collobert et al., 2011) and parsing (Collobert, 2011). Neural networks in general have also been applied to part-of-speech tagging, chunking, named entity recognition (Collobert et al., 2011; Turian et al., 2010), and sentiment detection (Socher et al., 2013). All these tasks involve predicting a limited (2-30) number of labels. In this work, we make use of CNNs, but apply them to the task of ranking a very large set of tags. We thus propose a model and training scheme that can scale to this class of problem. 3 Convolutional Embedding Model Our model #TAGSPACE (see Figure 1), like other word embedding models, starts by assigning a ddimensional vector to each of the l words of an input document w1, ... , wl, resulting in a matrix of size l x d. This is achieved using a matrix of N x d parameters, terme</context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: A simple and general method for semi-supervised learning. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10, pages 384–394, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Weston</author>
<author>Samy Bengio</author>
<author>Nicolas Usunier</author>
</authors>
<title>Wsabie: Scaling up to large vocabulary image annotation.</title>
<date>2011</date>
<booktitle>In Proceedings of the TwentySecond international joint conference on Artificial Intelligence-Volume Volume Three,</booktitle>
<pages>2764--2770</pages>
<publisher>AAAI Press.</publisher>
<contexts>
<context position="3173" citStr="Weston et al., 2011" startWordPosition="493" endWordPosition="496">sion: the tags are a labeling by the author of the salient aspects of the text. Hence, predicting them may provide stronger semantic guidance than unsupervised learning alone. The abundance of hashtags in real posts provides a huge labeled dataset for learning potentially sophisticated models. In this work we develop a convolutional network for large scale ranking tasks, and apply it to hashtag prediction. Our model represents both words and the entire textual post as embeddings as intermediate steps. We show that our method outperforms existing unsupervised (word2vec) and supervised (WSABIE (Weston et al., 2011)) embedding methods, and other baselines, at the hashtag prediction task. We then probe our model’s generality, by transfering its learned representations to the task of personalized document recommendation: for each of M users, given N previous positive interactions with documents (likes, clicks, etc.), predict the N + 1’th document the user will positively interact with. To perform well on this task, the representation should capture the user’s interest in textual content. We find representations trained on hashtag prediction outperform representations from unsupervised learning, and that ou</context>
<context position="4849" citStr="Weston et al., 2011" startWordPosition="765" endWordPosition="768">g) pair. tional architecture performs better than WSABIE trained on the same hashtag task. 2 Prior Work Some previous work (Davidov et al., 2010; Godin et al., 2013; She and Chen, 2014) has addressed hashtag prediction. Most such work applies to much smaller sets of hashtags than the 100,000 we consider, with the notable exception of Ding et al. (2012), which uses an unsupervised method. As mentioned in Section 1, many approaches learn unsupervised word embeddings. In our experiments we use word2vec (Mikolov et al., 2013) as a representative scalable model for unsupervised embeddings. WSABIE (Weston et al., 2011) is a supervised embedding approach that has shown promise in NLP tasks (Weston et al., 2013; Hermann et al., 2014). WSABIE is shallow, linear, and ignores word order information, and so may have less modeling power than our approach. Convolutional neural networks (CNNs), in which shared weights are applied across the input, are popular in the vision domain and have recently been applied to semantic role labeling (Collobert et al., 2011) and parsing (Collobert, 2011). Neural networks in general have also been applied to part-of-speech tagging, chunking, named entity recognition (Collobert et a</context>
<context position="8074" citStr="Weston et al., 2011" startWordPosition="1352" endWordPosition="1356">an thus rank all candidate hashtags via their scores f(w, t), largest first. To train the above scoring function, and hence the parameters of the model we minimize a ranking loss similar to the one used in WSABIE as a training objective: for each training example, we sample a positive tag, compute f(w, t+), then sample random tags t¯ up to 1000 times until f(w, ¯t) &gt; m + f(w, t+), where m is the margin. A gradient step is then made to optimize the pairwise hinge loss: G = max{0, m − f(w, t+) + f(w, ¯t)}. We use m = 0.1 in our experiments. This loss function is referred to as the WARP loss in (Weston et al., 2011) and is used to approximately optimizing the top of the ranked list, useful for metrics like precision and recall@k. In particular, the search for a negative candidate tag means that more energy is spent on improving the ranking performance of positive labels already near the top of the ranked list, compared to only randomly sampling of negatives, which would optimize the average rank instead. Minimizing our loss is achieved with parallel stochastic gradient descent using the hogwild algorithm (Niu et al., 2011). The lookup-table layers are initialized with the embeddings learned by WSABIE to </context>
<context position="13243" citStr="Weston et al., 2011" startWordPosition="2140" endWordPosition="2143">ice, #food, #middleclassproblems, #neveragain The restaurant had great food #dinner, #restaurant, #yum, #food, #delicious and was reasonably priced. #stuffed, #goodtimes, #foodporn, #yummy, #winning He has the longest whiskers, #cat, #kitty, #meow, #cats, #catsofinstagram omg so sweet! #crazycatlady, #cute, #kitten, #catlady, #adorable Table 2: #TAGSPACE (256 dim) predictions for some example posts. ply these word embeddings to ranking, we first sum the embeddings of each word in the text (as word2vec does), and then rank hashtags by similarity of their embedding to that of the text.2 WSABIE (Weston et al., 2011) is a supervised bilinear embedding model. Each word and tag has an embedding. The words in a text are averaged to produce an embedding of the text, and hashtags are ranked by similarity to the text embedding. That is, the model is of the form: f(w, t) = wTUTV t where the post w is represented as a bag of words (a sparse vector in RN), the tag is a one-hot-vector in RN, and U and V are k × N embedding matrices. The WARP loss, as described in section 3, is used for training. Performance of all these models at hashtag prediction is summarized in Tables 3 and 4. We find similar results for both d</context>
</contexts>
<marker>Weston, Bengio, Usunier, 2011</marker>
<rawString>Jason Weston, Samy Bengio, and Nicolas Usunier. 2011. Wsabie: Scaling up to large vocabulary image annotation. In Proceedings of the TwentySecond international joint conference on Artificial Intelligence-Volume Volume Three, pages 2764– 2770. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Weston</author>
<author>Antoine Bordes</author>
<author>Oksana Yakhnenko</author>
<author>Nicolas Usunier</author>
</authors>
<title>Connecting language and knowledge bases with embedding models for relation extraction.</title>
<date>2013</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="4941" citStr="Weston et al., 2013" startWordPosition="781" endWordPosition="784">Prior Work Some previous work (Davidov et al., 2010; Godin et al., 2013; She and Chen, 2014) has addressed hashtag prediction. Most such work applies to much smaller sets of hashtags than the 100,000 we consider, with the notable exception of Ding et al. (2012), which uses an unsupervised method. As mentioned in Section 1, many approaches learn unsupervised word embeddings. In our experiments we use word2vec (Mikolov et al., 2013) as a representative scalable model for unsupervised embeddings. WSABIE (Weston et al., 2011) is a supervised embedding approach that has shown promise in NLP tasks (Weston et al., 2013; Hermann et al., 2014). WSABIE is shallow, linear, and ignores word order information, and so may have less modeling power than our approach. Convolutional neural networks (CNNs), in which shared weights are applied across the input, are popular in the vision domain and have recently been applied to semantic role labeling (Collobert et al., 2011) and parsing (Collobert, 2011). Neural networks in general have also been applied to part-of-speech tagging, chunking, named entity recognition (Collobert et al., 2011; Turian et al., 2010), and sentiment detection (Socher et al., 2013). All these tas</context>
</contexts>
<marker>Weston, Bordes, Yakhnenko, Usunier, 2013</marker>
<rawString>Jason Weston, Antoine Bordes, Oksana Yakhnenko, and Nicolas Usunier. 2013. Connecting language and knowledge bases with embedding models for relation extraction. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
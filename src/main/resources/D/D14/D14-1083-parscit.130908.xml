<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002220">
<title confidence="0.9894395">
Why are You Taking this Stance?
Identifying and Classifying Reasons in Ideological Debates
</title>
<author confidence="0.850006">
Kazi Saidul Hasan and Vincent Ng
</author>
<affiliation confidence="0.911321">
Human Language Technology Research Institute
University of Texas at Dallas
</affiliation>
<address confidence="0.863737">
Richardson, TX 75083-0688
</address>
<email confidence="0.999617">
{saidul,vince}@hlt.utdallas.edu
</email>
<sectionHeader confidence="0.99517" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.995207428571429">
Recent years have seen a surge of interest
in stance classification in online debates.
Oftentimes, however, it is important to de-
termine not only the stance expressed by
an author in her debate posts, but also the
reasons behind her supporting or oppos-
ing the issue under debate. We therefore
examine the new task of reason classifi-
cation in this paper. Given the close in-
terplay between stance classification and
reason classification, we design computa-
tional models for examining how automat-
ically computed stance information can be
profitably exploited for reason classifica-
tion. Experiments on our reason-annotated
corpus of ideological debate posts from
four domains demonstrate that sophisti-
cated models of stances and reasons can
indeed yield more accurate reason and
stance classification results than their sim-
pler counterparts.
</bodyText>
<sectionHeader confidence="0.998784" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998625043478261">
In recent years, researchers have begun exploring
new opinion mining tasks. One such task is debate
stance classi�cation (SC): given a post written for
a two-sided topic discussed in an online debate fo-
rum, determine which of the two sides (i.e., for or
against) its author is taking (Agrawal et al., 2003;
Thomas et al., 2006; Bansal et al., 2008; Soma-
sundaran and Wiebe, 2009; Burfoot et al., 2011;
Hasan and Ng, 2013b). For example, the author of
the post shown in Figure 1 is pro-abortion.
Oftentimes, however, it is important to deter-
mine not only the author’s stance expressed in her
debate posts, but also the reasons why she supports
or opposes the issue under debate. Intuitively,
given a debate topic such as “Should abortion be
banned?” or “Do you support Obamacare?”, it
[I feel that abortion should remain legal, or rather, parents
should have the power to make the decision themselves and
not face any legal hindrance of any form.]1 Let us take a
look from the social perspective. [If parents cannot afford
to provide for the child, or if the family is facing financial
constraints, it is understandable that abortion can remain as
one of the options.]2
</bodyText>
<figureCaption confidence="0.6917946">
Reason 1: Woman’s right to abort
Reason 2: Unwanted babies are threat to their parents’ fu-
ture
Figure 1: A sample post on abortion annotated
with reasons.
</figureCaption>
<bodyText confidence="0.999956866666667">
should not be difficult for us to come up with a set
of reasons people typically use to back up their
stances. Given a set of reasons associated with
each stance in an online debate, the goal of post-
level reason classi�cation is to identify those rea-
son(s) an author uses to back up her stance in her
debate post. A more challenging version of this
task is sentence-level reason classi�cation, where
the goal is to identify not only the reason(s) an au-
thor uses in her post, but also the sentence(s) in
the post that the author uses to describe each of
her reasons. For example, the author of the post
shown in Figure 1 mentions two reasons why she
supports abortion, namely it’s a woman’s right to
abort and unwanted babies are threat to their par-
ents’ future, which are mentioned in the first and
third sentences in the post respectively.
Our goal in this paper is to examine post- and
sentence-level reason classification (RC) in ideo-
logical debates. Many online debaters use emo-
tional languages, which may involve sarcasm and
insults, to express their points, thereby making
RC and SC in ideological debates potentially more
challenging than that in other debate settings such
as congressional debates and company-internal
discussions (Walker et al., 2012).
Besides examining the new task of RC in ide-
ological debates, we believe that our work makes
three contributions. First, we propose to address
post-level RC by means of sentence-level RC by
</bodyText>
<page confidence="0.972673">
751
</page>
<note confidence="0.910899">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 751–762,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999471047619048">
(1) determining the reason(s) associated with each
of its sentences (if any), and then (2) taking the
union of the set of reasons associated with all of its
sentences to be the set of reasons associated with
the post. We hypothesize that this sentence-based
approach, which exploits a training set in which
each sentence in a post is labeled with its reason,
would achieve better performance than a multi-
label text classification approach to post-level RC,
which learns to determine the subset of reasons a
post contains directly from a training set in which
each post is labeled with the corresponding set of
reasons. In other words, we hypothesize that we
could achieve better results for post-level RC by
learning from sentence-level than from post-level
reason annotations, as sentence-level reason anno-
tations can enable a learning algorithm to accu-
rately attribute an annotated reason to a particular
portion of a post.
Second, we propose stance-supported RC sys-
tems, hypothesizing that automatically computed
stance information can be profitably exploited for
RC. Since we are exploiting automatically com-
puted (and thus potentially noisy) stance informa-
tion, we hypothesize that the effectiveness of such
information would depend in part on the way it is
exploited in RC systems. As a result, we introduce
a set of stance-supported models for RC, start-
ing with simple pipeline models and then mov-
ing on to joint models with increasing sophisti-
cation. Note that exploiting stance information
by no means guarantees that RC performance will
improve, as an incorrect determination of stance
could lead to an incorrect identification of rea-
sons. Hence, one of our goals is to examine how to
model stances and reasons so that RC can benefit
from stance information.
Finally, since progress on RC is hindered in part
by the lack of an annotated corpus, we make our
reason-annotated dataset publicly available.1 To
our knowledge, this will be the first publicly avail-
able corpus for sentence- and post-level RC.
</bodyText>
<sectionHeader confidence="0.93774" genericHeader="introduction">
2 Corpus and Annotation
</sectionHeader>
<bodyText confidence="0.9998235">
We collected debate posts from four popular
domains, Abortion (ABO), Gay Rights (GAY),
Obama (OBA), and Marijuana (MAR), from an
online debate forum2. All debates are two-sided,
</bodyText>
<footnote confidence="0.998785333333333">
1http://www.hlt.utdallas.edu/˜saidul/
stance/
2http://www.createdebate.com/
</footnote>
<bodyText confidence="0.998873862745098">
so each post receives one of two stance labels, for
or against, depending on whether the author of
the post supports or opposes abortion, gay rights,
Obama, or the legalization of marijuana respec-
tively. A post’s stance label is given by its author.
Note that each post belongs to a thread, which
is a tree with one or more nodes such that (1) each
node corresponds to a debate post, and (2) a post
yz is the parent of another post yj if yj is a reply
to yz. Given a thread, we generate post sequences,
each of which is a path from the root of the thread
to one of its leaves. Hence, a post sequence is an
ordered set of posts such that each post is a reply
to its immediately preceding post in the sequence.
Table 2a shows the statistics of the four stance-
labeled datasets.
While the debate posts contain the stance labels
given by their authors, they are not annotated with
reasons. As part of our study of RC, we annotate
each post with the reasons it gives for its stance.
Our annotation procedure is composed of three
steps. First, two human annotators independently
examined each post and identified the reasons au-
thors present to support their stances (i.e., for and
against) in each domain. Second, they discussed
and agreed on the reasons identified for each do-
main. Third, they independently annotated the text
of each post with reason labels from the post’s do-
main. To do this, they labeled each sentence of a
post with the set of reasons the author expressed in
that sentence. Any sentence that does not belong
to any reason class was assigned the NONE class.
After the annotators completed the aforemen-
tioned steps, they were asked to collapse all the
reason classes that occur in less than 2% of the
sentences annotated with non-NONE classes into
the OTHER class. In other words, all the sentences
that were originally annotated with one of these
infrequent reason classes will now be labeled as
OTHER. Our decision to merge infrequent classes
is motivated by two observations. First, from a
practical point of view, infrequent reasons do not
carry much weight. Second, from a modeling per-
spective, it is often not worth increasing model
complexity by handling infrequent classes. The
resulting set of reason classes for each domain is
shown in Table 1.
A closer examination of the resulting annota-
tions reveals that approximately 3% of the sen-
tences received multiple reason labels. Again, to
avoid the complexity of modeling multi-labeled
</bodyText>
<page confidence="0.996822">
752
</page>
<table confidence="0.999929724137931">
Domain Stance Reason classes
ABO for [F1] Abortion is a woman’s right (26%); [F2] Rape victims need it to be legal (7%); [F3] A fetus
is not human (38%); [F4] Mother’s life in danger (5%); [F5] Unwanted babies are ill-treated by
parents (8%); [F6] Birth control fails at times (3%); [F7] Abortion is not murder (3%); [F8] Mother
is not healthy/financially solvent (4%); [F9] Others (6%)
against [A1] Put baby up for adoption (9%); [A2] Abortion kills a life (29%); [A3] An unborn baby is a
human and has the right to live (40%); [A4] Be willing to have the baby if you have sex (14%);
[A5] Abortion is harmful for women (5%); [A6] Others (3%)
GAY for [F1] Gay marriage is like any other marriage (14%); [F2] Gay people should have the same rights
as straight people (36%); [F3] Gay parents can adopt and ensure a happy life for a baby (10%); [F4]
People are born gay (18%); [F5] Religion should not be used against gay rights (11%); [F6] Others
(11%)
against [A1] Religion does not permit gay marriages (18%); [A2] Gay marriages are not normal/against
nature (39%); [A3] Gay parents can not raise kids properly (11%); [A4] Gay people have problems
and create social issues (16%); [A5] Others (16%)
OBA for [F1] Fixed the economy (21%); [F2] Ending the wars (7%); [F3] Better than the republican candi-
dates (25%); [F4] Makes good decisions/policies (8%); [F5] Has qualities of a good leader (14%);
[F6] Ensured better healthcare (8%); [F7] Executed effective foreign policies (6%); [F8] Created
more jobs (4%); [F9] Others (7%)
against [A1] Destroyed our economy (26%); [A2] Wars are still on (11%); [A3] Unemployment rate is high
(5%); [A4] Healthcare bill is a failure(9%); [A5] Poor decision-maker (7%); [A6] We have better
republicans than Obama (5%); [A7] Not eligible as a leader (20%); [A8] Ineffective foreign policies
(4%); [A9] Others (13%)
MAR for [F1] Not addictive (23%); [F2] Used as a medicine (11%); [F3] Legalized marijuana can be con-
trolled and regulated by the government (33%); [F4] Prohibition violates human rights (15%); [F5]
Does not cause any damage to our bodies (6%); [F6] Others (12%)
against [A1] Damages our bodies (23%); [A2] Responsible for brain damage (22%); [A3] If legalized,
people will use marijuana and other drugs more (12%); [A4] Causes crime (9%); [A5] Highly
addictive (17%); [A6] Others (17%)
</table>
<tableCaption confidence="0.999905">
Table 1: Reason classes and their percentages in the corresponding stance for each domain.
</tableCaption>
<bodyText confidence="0.998464">
sentences given their rarity, we asked each annota-
tor to pick the reason that was highlighted the most
in each multi-labeled sentence.
Inter-annotator agreement scores at the sentence
level and the post level, expressed in terms of Co-
hen’s Kappa (Carletta, 1996), are shown in Ta-
ble 2b. Given that the majority of sentences were
labeled as NONE, we avoid inflating agreement by
not considering the sentences labeled with NONE
by both annotators when computing Kappa. As we
can see, we achieved substantial post-level agree-
ment and high sentence-level agreement.
The major source of inter-annotator disagree-
ment for all four datasets stems from the fact that
in many cases, the annotators, while agreeing on
the reason class, differ on how long the text span
for a reason should be. This hurts sentence-level
agreement but not post-level agreement, since the
latter only concerns whether a reason was men-
tioned in a post, and explains why the sentence-
level agreement scores are lower than the corre-
sponding post-level scores. Minor sources of dis-
agreement arise from the facts that (1) the anno-
tators selected different reason labels for some of
the multi-labeled sentences, and (2) they tend to
disagree in some cases where authors use sarcasm
</bodyText>
<table confidence="0.717189818181818">
ABO GAY OBA MAR
Stance-labeled posts 1741 1376 985 626
for posts (%) 54.9 63.4 53.9 69.5
Average post sequence length 4.1 4.0 2.6 2.5
(a) Statistics of stance-labeled posts
ABO GAY OBA MAR
Reason-labeled posts 463 561 447 432
% of sentences w/ reason tags 20.4 29.8 34.4 43.7
Kappa (sentence) 0.66 0.63 0.61 0.67
Kappa (post) 0.82 0.80 0.78 0.83
(b) Statistics of reason-labeled posts
</table>
<tableCaption confidence="0.993255">
Table 2: Stance and reason annotation statistics.
</tableCaption>
<bodyText confidence="0.992819">
to present a reason. Each case of disagreement is
resolved through discussion among the annotators.
</bodyText>
<sectionHeader confidence="0.972971" genericHeader="method">
3 Baseline RC System
</sectionHeader>
<bodyText confidence="0.991406">
Our baseline system uses a maximum entropy
(MaxEnt) classifier to determine whether a reason
is expressed in a post and/or its sentence(s). We
create one training instance for each sentence in
each post in the training set, using the reason label
as its class label. We represent each instance using
five types of features, as described below.
N-gram features. We encode each unigram and
bigram collected from the training sentences as a
</bodyText>
<page confidence="0.998088">
753
</page>
<bodyText confidence="0.999485054794521">
binary feature indicating the n-gram’s presence or
absence in a given sentence.
Dependency-based features. To capture the
inter-word relationships that n-grams may not,
we employ the dependency-based features previ-
ously used for stance classification in Anand et
al. (2011). These features have three variants. In
the first variant, the pair of arguments involved in
each dependency relation extracted by a depen-
dency parser is used as a feature. The second vari-
ant is the same as the first except that the head (i.e.,
the first argument in a relation) is replaced by its
part-of-speech tag. The features in the third vari-
ant, the topic-opinion features, are created by re-
placing each sentiment-bearing word in features of
the first two types with its corresponding polarity
label (i.e., + or −).
Frame-semantic features. While dependency-
based features capture the syntactic dependencies,
frame-semantic features encode the semantic rep-
resentation of the concepts in a sentence. Fol-
lowing our previous work on stance classification
(Hasan and Ng, 2013c), we employ three types
of features computed based on the frame-semantic
parse of each sentence in a post obtained from SE-
MAFOR (Das et al., 2010). Frame-word interac-
tion features encode whether two words appear in
different elements of the same frame. Hence, each
frame-word interaction feature consists of (1) the
name of the frame f from which it is created, and
(2) an unordered word pair in which the words are
taken from two frame elements of f. A frame-pair
feature is represented as a word pair corresponding
to the names of two frames and encodes whether
the target word of the first frame appears within
an element of the second frame. Finally, frame n-
gram features are a variant of word n-grams. For
each word n-gram in the sentence, a frame n-gram
feature is created by replacing one or more words
in the word n-gram with the name of the frame or
the frame element in which the word appears. A
detailed description of these features can be found
in Hasan and Ng (2013c).
Quotation features. We employ two quotation
features. IsQuote is a binary feature that indicates
whether a sentence is a quote or not (i.e., whether
it appeared in its parent post in the post sequence).
Note that if an instance is a quote from a previ-
ous post, it is unlikely that it represents a reason
the author is presenting to support her argument.
Instead, the author may have quoted this before
stating her counter-argument. FollowsQuote is a
binary feature that indicates whether a sentence
follows a sentence for which the IsQuote feature
value is true. Intuitively, a sentence following a
quote is likely to present a counter-argument.
Positional feature. We split each post into four
parts (such that each part contains roughly the
same number of sentences) and create one posi-
tional feature that encodes which part of the post
contains a given sentence. This feature is moti-
vated by our observations on the training data that
(1) reasons are more likely to appear in the second
half of a post and (2) on average more than one-
third of the reasons appear in the last quarter of a
post.
After training, we can apply the resulting RC
system to classify the test instances, which are
generated in the same way as the training in-
stances. Once the sentences of a test post are clas-
sified, we simply assume its post-level reason la-
bels to be the set of reason labels assigned by the
classifier to its sentences.
</bodyText>
<sectionHeader confidence="0.970243" genericHeader="method">
4 Stance-Supported RC Systems
</sectionHeader>
<bodyText confidence="0.999984142857143">
In this section, we propose a set of systems for
RC. Unlike the baseline RC system, these RC
systems are stance-supported, enabling us to ex-
plore how different ways of modeling automati-
cally computed stances and reasons can improve
RC classification. Below we present our systems
in increasing order of modeling sophistication.
</bodyText>
<subsectionHeader confidence="0.995525">
4.1 Pipeline Systems
</subsectionHeader>
<bodyText confidence="0.999890777777778">
We examine two pipeline systems, P1 and P2.
Given a set of test posts, both systems first de-
termine the stance of each post and then apply a
stance-specific reason classifier to each of them.
More specifically, both P1 and P2 employ two
stance-specific reason classifiers: one is trained on
all the posts labeled as for and the other is trained
on all the posts labeled as against. Each stance-
specific reason classifier is trained using MaxEnt
on the same feature set as that of the Baseline RC
system. It computes for a particular stance s the
probability P(rjs, t), where r is a reason label and
t is a sentence in a test post p.
P1 and P2 differ only with respect to the SC
model used to stance-label each post. In P1, the
stance s of a post p is determined by applying to p
a stance classifier that computes P(sjp). To train
the classifier, we employ MaxEnt. Each train-
</bodyText>
<page confidence="0.992656">
754
</page>
<bodyText confidence="0.996496043478261">
ing instance corresponds to a training post and
is represented by all but the quotation and posi-
tional features used to train the Baseline RC sys-
tem, since these two feature types are sentence-
based rather than post-based. After training, the
resulting classifier can be used to stance-label a
post independently of the other posts.
In P2, on the other hand, we recast SC as a se-
quence labeling task. In other words, we train a
SC model that assumes as input a post sequence
and outputs a stance sequence, with one stance la-
bel for each post in the input post sequence. This
choice is motivated by an observation we made
previously (Hasan and Ng, 2013a): since each post
in a sequence is a reply to the preceding post, we
could exploit their dependencies by determining
their stance labels together.3
As our sequence learner, we employ a maxi-
mum entropy Markov model (MEMM) (McCal-
lum et al., 2000). Given an input post sequence
PS = (p1, p2, . . . , pn), the MEMM finds the most
probable stance sequence S = (s1, s2, ... , sn) by
computing P(S|PS), where:
</bodyText>
<equation confidence="0.994213">
n
P(S|PS) = Y P(sk|sk−1,pk) (1)
k=1
</equation>
<bodyText confidence="0.99996152">
This probability can be computed efficiently via
dynamic programming (DP), using a modified ver-
sion of the Viterbi algorithm (Viterbi, 1967).
There is a caveat, however. Recall that the post
sequences are generated from a thread. Since a
test post may appear in more than one sequence,
different occurrences of it may be assigned differ-
ent stance labels by the MEMM. To determine the
final stance label for the post, we average the prob-
abilities assigned to the for stance over all its oc-
currences; if the average is &gt; 0.5, then its final
label is for; otherwise, its label is against.
a RC system independently of each other. We em-
ploy the Baseline as our RC system, since this is
the only RC system that is not stance-specific. For
the SC system, we employ P2.
Since the SC system and the RC system are
trained independently of each other, their outputs
may not be consistent. For instance, an inconsis-
tency arises if a post is labeled as for but one or
more of its reasons are associated with the oppos-
ing stance. In fact, an inconsistency can arise in
the output of the RC system alone: reasons associ-
ated with both stances may be assigned by the RC
systems to different sentences of a given post.
To enforce consistency, we apply integer lin-
ear programming (ILP) (Roth and Yih, 2004). We
formulate one ILP program for each debate post.
Each ILP program contains two post-stance vari-
ables (xfor and xagainst) and |T |* |LR |reason
variables (i.e., one indicator variable zt,r for each
reason class r and each sentence t), where |T |is
the number of sentences in the post and |LR |is the
number of reason labels. Our objective is to maxi-
mize the linear combination of these variables and
their corresponding probabilities assigned by their
respective classifiers (see (2) below) subject to two
types of constraints, the integrity constraints and
the post-reason constraints. The integrity con-
straints ensure that each post is assigned exactly
one stance and each sentence in a post is assigned
exactly one reason class (see the two equality con-
straints in (3)). The post-reason constraints ensure
consistency between the predictions made by the
SC and the RC systems. Specifically, (1) if there
is at least one reason supporting the for stance, the
post must be assigned a for label; and (2) a for
post must have at least one for reason. These con-
straints are defined for the against label as well
(see the constraints in (4)).
</bodyText>
<equation confidence="0.980021055555556">
Maximize:
bt,rzt,r (2)
(3)
X
sELS
X
rELR
subject to:
X
sELS
Xxs = 1, V t
rELR
zt,r = 1,
1
|T|
asxs +
X |T|
t=1
</equation>
<subsectionHeader confidence="0.977018">
4.2 System based on Joint Inference
</subsectionHeader>
<bodyText confidence="0.999888833333333">
One weakness of the pipeline systems is that errors
may propagate from the SC system to the RC sys-
tem. If the stance of a post is incorrectly labeled,
its reasons will also be incorrectly labeled.
To avoid this problem, we employ joint infer-
ence. Specifically, we first train a SC system and
</bodyText>
<footnote confidence="0.990036833333333">
3While we could similarly recast the problem of assigning
reasons to the sentences in a post as a sequence learning task,
we did not pursue this idea further because preliminary ex-
periments indicated that sequence learning for RC was inef-
fective: there is little, if any, dependency between the reason
labels in consecutive sentences.
</footnote>
<equation confidence="0.8708155">
xs E 10, 11, zt,r E 10, 11
|T|
V t xs &gt; zt,r, X zt,r &gt; xs (4)
t=1
</equation>
<page confidence="0.988705">
755
</page>
<bodyText confidence="0.999221428571429">
Note that (1) as and bt,r are two sets of probabil-
ities assigned by the SC and RC systems respec-
tively; (2) LS and LR denote the set of stance
labels and reason labels respectively; and (3) the
fraction 1
|T  |ensures that both classifiers are con-
tributing equally to the objective function.
</bodyText>
<subsectionHeader confidence="0.997882">
4.3 Systems based on Joint Learning
</subsectionHeader>
<bodyText confidence="0.999944166666667">
Another way to avoid the error propagation prob-
lem in pipeline systems is to perform joint learn-
ing. In joint learning, the two tasks, SC and RC,
are learned jointly. Below we propose three joint
models in increasing level of sophistication.
J1 is a joint model that, given a test post p, finds
the stance label s and the reason label for each of
the sentences that together maximize the probabil-
ity P(Rp, s|p), where Rp = (r1, r2,... , rn) is the
sequence of reason labels with ri (1 ≤ i ≤ n)
being the reason label assigned to ti, the i-th sen-
tence in p. Using Chain Rule,
</bodyText>
<equation confidence="0.999476666666667">
P(Rp,s|p) = P(s|p)P(Rp|s,p)
= P(s|p) n P(ri|s, ti) (5)
i=1
</equation>
<bodyText confidence="0.996264892857143">
Hence, P(Rp, s|p) can be computed by using
the stance-specific RC classifier and the SC classi-
fier employed in P1.
The second joint model, J2, is the same as
J1, except that we recast SC as a sequence la-
beling task. As before, we employ MEMM to
learn how to predict stance sequences. Given a
post sequence PS = (p1, p2, . . . , pn), J2 finds the
stance sequence S = (s1, s2, ... , sn) and rea-
sons R = (R1, R2,. . . , Rn) that jointly maximize
P(R, S|PS). Note that Ri is the sequence of rea-
son labels assigned to the sentences in post i.
The R and S that jointly maximize P(R, S|PS)
can be found efficiently via DP, using a modified
version of the Viterbi algorithm. Unlike in P2, in
J2 the decoding process is slightly more compli-
cated because we have to take into account Ri. Be-
low we show the recursive definitions used to com-
pute the entries in the DP table, where vk(h) is the
(k,h)-th entry of the table; P(h|p) is provided by
the MaxEnt stance classifier used in P1; P(h|j, p)
is provided by the MEMM stance classifier used
in P2; P(rmax
i |h, ti) is provided by the stance-
specific reason classifier used in the pipeline sys-
tems; and rmax iis the reason label for sentence ti
that has the highest probability according to the
reason classifier.
</bodyText>
<equation confidence="0.950349166666667">
Base case:
P(rmax
i |h,ti) (6)
Recursive definition:
vk(h) = max vk−1(j)P(h|j, p) n P(rmax i|h, ti)
j i=1 (7)
</equation>
<bodyText confidence="0.999993153846154">
To motivate our third joint model, J3, we make
the following observation. Recall that a post in
a post sequence is a reply to its preceding post.
An inspection of the training data reveals that in
many cases, a reply is a rebuttal to the preced-
ing post, where an author attempts to argue why
the points or reasons raised in the preceding post
are wrong and then provides her reasons for the
opposing stance. Motivated by this observation,
we hypothesize that the reasons mentioned in the
preceding post could be useful for predicting the
reasons in the current post. However, none of the
models we have presented so far makes use of the
reasons predicted for the preceding post.
This motivates the design of J3, which we build
on top of J2. Specifically, to incorporate the reason
labels predicted for the preceding post in a post se-
quence, we augment the feature set of the stance-
specific reason classifiers with a set of reason fea-
tures, with one binary feature for each reason. The
value of a reason feature is 1 if and only if the cor-
responding reason is predicted to be present in the
preceding post. Hence, in J3, we can apply the
same DP equations we used in J2 except that the
set of features used by the reason classifier is aug-
mented with the reason features.
</bodyText>
<sectionHeader confidence="0.998548" genericHeader="evaluation">
5 Evaluation
</sectionHeader>
<bodyText confidence="0.9999628">
While our primary goal is to evaluate the RC sys-
tems introduced in the previous section, we are
also interested in whether SC performance can im-
prove when SC is jointly modeled with RC. More
specifically, our evaluation is driven by the follow-
ing question: will RC performance and SC perfor-
mance improve as we employ more sophisticated
methods for modeling reasons and stances? Be-
fore showing the results, we describe the metrics
for evaluating RC and SC systems.
</bodyText>
<equation confidence="0.982185">
n
v1(h) = P(h|p)
i=1
</equation>
<page confidence="0.988207">
756
</page>
<table confidence="0.9795361">
System ABO GAY OBA MAR
Stance Reason Stance Reason Stance Reason Stance Reason
Sentence Post Sentence Post Sentence Post Sentence Post
Baseline – 32.7 45.0 – 23.3 40.5 – 19.5 31.5 – 28.7 44.2
P1 62.8 34.5 46.3 63.4 24.5 43.2 61.0 20.3 33.5 67.2 30.5 47.3
P2 65.1 36.1 47.7 64.2 26.6 45.5 63.8 21.1 34.4 68.5 32.9 48.8
ILP 65.2 36.5 48.4 64.6 28.0 46.7 63.6 22.8 35.0 68.8 33.1 48.9
J1 62.5 36.0 47.6 64.0 26.7 45.6 61.2 23.1 35.7 67.8 33.3 49.2
J2 65.9 37.9 50.6 65.3 29.6 48.5 63.5 24.5 37.1 68.7 34.5 50.5
J3 66.3 39.5 52.3 65.7 31.4 49.8 64.0 25.1 38.0 69.0 35.1 51.1
</table>
<tableCaption confidence="0.999577">
Table 3: SC accuracies and RC F-scores for our five-fold cross-validation experiments.
</tableCaption>
<subsectionHeader confidence="0.984209">
5.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.9999932">
We express SC results in terms of accuracy (i.e.,
the percentage of test posts labeled with the cor-
rect stance) and RC results in terms of F-score
micro-averaged over all reason classes except the
NONE class. For each RC system, we report its
sentence-level RC score and post-level RC score,
which are computed over sentences and posts re-
spectively. As mentioned at the end of Section 3,
the set of post-level reason labels of a given post
is automatically obtained by taking the union of
the set of reason labels assigned to each of its sen-
tences. Hence, a reason classifier will be rewarded
as long as it can predict, for any sentence in a test
post, a reason label that the annotators assigned to
some sentence in the same post.
We obtain these scores via five-fold cross-
validation experiments. During fold partition, all
posts that are in the same post sequence are as-
signed to the same fold. All reason and stance
classifiers are domain-specific, meaning that each
of them is trained on sentences/posts from ex-
actly one domain and is applied to classify sen-
tences/posts from the same domain. We use the
Stanford maximum entropy classifier4 for classifi-
cation and solve ILP programs using lpsolve5.
</bodyText>
<subsectionHeader confidence="0.969813">
5.2 Results and Discussion
</subsectionHeader>
<bodyText confidence="0.998107666666667">
Results are shown in Table 3. Each row corre-
sponds to one of our seven RC systems, showing
its SC accuracy as well as its sentence- and post-
level RC F-scores for each domain.
Let us begin by discussing the RC results. First,
P1 and P2 significantly beat the Baseline on all
</bodyText>
<footnote confidence="0.99269875">
4http://nlp.stanford.edu/software/
classifier.shtml
5http://sourceforge.net/projects/
lpsolve/
</footnote>
<bodyText confidence="0.999635810810811">
four domains by an average of 1.4 and 3.1 points
at the sentence level and by an average of 2.3 and
3.8 points at the post level respectively.6 These re-
sults show that stance information can indeed be
profitably used for RC even if it is incorporated
into RC systems in a simple manner. Second,
improving SC through sequence learning can im-
prove RC: the systems in which SC is recast as se-
quence labeling (P2 and J2) perform significantly
better than the corresponding systems that do not
(P1 and J1). Third, ILP significantly beats P2 on
two domains (ABO and GAY) and achieves the
same level of performance as P2 on the remain-
ing domains. These results suggest that joint infer-
ence is no worse (and sometimes even better) than
pipeline learning as far as exploiting stance infor-
mation for RC is concerned. Fourth, the systems
trained via joint learning (J1 and J2) beat their cor-
responding pipeline counterparts (P1 and P2) on
all four domains, significantly so by an average of
2.3 and 2.5 points at the sentence level and by an
average of 2.0 and 2.6 points at the post level re-
spectively, suggesting that joint learning is indeed
a better way to incorporate stance information than
pipeline learning. Finally, J3, the joint system that
exploits reasons predicted for the previous post,
significantly beats J2, the system on which it is
built, by 1.6 and 1.8 points at the sentence level
and by 1.7 and 1.3 points at the post level for ABO
and GAY respectively. It also yields small, statis-
tically insignificant, improvements (0.6 points at
the sentence level and 0.6–0.9 points at the post
level) for the remaining two domains. These re-
sults suggest that the reasons predicted for the pre-
vious post indeed provide useful information for
predicting the current post’s reasons.
Overall, these results are consistent with our hy-
</bodyText>
<footnote confidence="0.945176">
6All significance tests are paired t-tests (p &lt; 0.05).
</footnote>
<page confidence="0.996926">
757
</page>
<bodyText confidence="0.99998845">
pothesis that the usefulness of stance information
depends in part on the way it is exploited, and
that RC performance increases as we employ more
sophisticated methods for modeling reasons and
stances. Our best system, J3, significantly beats
the Baseline by an average of 6.7 and 7.5 points at
the sentence and post levels respectively.
As mentioned earlier, a secondary goal of this
work is to determine whether joint modeling can
improve SC as well. For that reason, we com-
pare the performances of the best pipeline model
(P2) and the best joint model (J3) on each domain.
We find that in terms of SC accuracy, J3 is signifi-
cantly better than P2 on ABO and GAY, and yields
slightly, though insignificantly, better performance
on the remaining two domains. In other words, our
results suggest that joint modeling of SC and RC
has a positive impact on SC performance on all
domains, and the impact can sometimes be large
enough to yield significantly better results.
</bodyText>
<subsectionHeader confidence="0.986316">
5.3 Further Comparison
</subsectionHeader>
<bodyText confidence="0.99997327631579">
We hypothesized in the introduction that the
sentence-based approach to post-level RC would
yield better performance than the multi-label text
classification approach. In Section 5.2, we pre-
sented results of the sentence-based approach to
RC. So, to test this hypothesis, we next evaluate
the multi-label text classification approach to RC.
Recall that the multi-label text classification ap-
proach assumes the following setup. Given a set
of training posts where each post is multi-labeled
with the set of reasons it contains, the goal is to
train a system to determine the set of reasons a
test post contains. Hence, unlike in the sentence-
based approach, in this approach no sentence-level
reason annotations are exploited during training.
We implement this approach by recasting multi-
label text classification as n binary text classifica-
tion tasks, where n is the number of reason classes
for a domain. In the binary classification task for
predicting reason i, we train a binary classifier cz
using the one-versus-all training scheme. Specif-
ically, to train cz, we create one training instance
for each post p in the training set, labeling it as
positive if and only if p contains reason i. Note
that if i is a minority reason, the class distribution
of the resulting training set will be highly skewed
towards the negatives, which will in turn cause the
resulting MaxEnt classifier to be biased towards
predicting a test instance as negative.
To address this problem, we adjust the classifi-
cation thresholds associated with the binary classi-
fiers. Recall that a test instance is classified as pos-
itive by a binary classifier if and only if its prob-
ability of belonging to the positive class is above
the classification threshold used. Hence, adjusting
the threshold amounts to adjusting the number of
test instances classified as positive, thus address-
ing the bias problem mentioned above. Specifi-
cally, we adjust the thresholds of the classifiers as
follows. We train the binary classifiers to optimize
the overall F-score by jointly tuning their classifi-
cation thresholds on 25% of the training data re-
served for development purposes. Since comput-
ing the exact solution to this optimization prob-
lem is computationally expensive, we employ a lo-
cal search algorithm that changes the value of one
threshold at a time to optimize F-score while keep-
ing the remaining thresholds fixed. During testing,
classifier cz will classify a test instance as positive
if its probability of belonging to the positive class
is above the corresponding threshold.
We apply this multi-label text classification ap-
proach to obtain post-level RC scores for the Base-
line, P1 and P2. Note that since P1 and P2 are
pipeline systems, the binary classifiers they use to
predict a test post’s reasons depend on the post’s
predicted stance. Specifically, if a test post is pre-
dicted to have a positive (negative) stance, then
only the reason classifiers associated with the pos-
itive (negative) stance will be used to predict the
reasons it contains. On the other hand, this ap-
proach cannot be used in combination with ILP or
the joint models to produce post-level RC scores:
they all require a reason classifier trained on
reason-annotated sentences, which are not avail-
able in the multi-label text classification approach.
Post-level RC results of the Baseline and the
two pipeline systems, P1 and P2, obtained via this
multi-label text classification approach are shown
in Table 4. These scores are significantly lower
than the corresponding scores in Table 3 by 3.2,
2.9, and 3.1 points for the Baseline, P1, and P2 re-
spectively, when averaged over the four domains.
They confirm our hypothesis that the sentence-
based approach to post-level RC is indeed better
than its multi-class text classification counterpart.
</bodyText>
<subsectionHeader confidence="0.909355">
5.4 Error Analysis
</subsectionHeader>
<bodyText confidence="0.999697">
To get a better understanding of our best-
performing RC system (J3), we examine its major
</bodyText>
<page confidence="0.994337">
758
</page>
<table confidence="0.96198775">
System ABO GAY OBA MAR
Baseline 39.8 37.9 30.1 40.8
P1 41.5 41.0 31.7 44.7
P2 43.3 42.6 32.0 46.3
</table>
<tableCaption confidence="0.97837">
Table 4: Post-level RC F-scores obtained via the
multi-class text classification approach.
</tableCaption>
<bodyText confidence="0.991035107142857">
sources of error in this subsection.
For the four domains, 75–83% of the errors
can be attributed to the system’s inability to de-
cide whether a sentence describes a reason or not.
Specifically, in 51–54% of the erroneous cases, a
reason sentence is misclassified as NONE. On the
other hand, 23–30% of the cases are concerned
with assigning a reason label to a NONE sentence.
The remaining 17–25% of the errors concern mis-
labeling a reason sentence with the wrong reason.
A closer examination of the errors reveals that
they resulted primarily from (1) the lack of access
to background knowledge, (2) the failure to pro-
cess complex discourse structures, and (3) the fail-
ure to process sarcastic statements and rhetorical
questions. We present two examples for each of
these three major sources of error from the ABO
and OBA domains in Table 5. In each example,
we show their predicted (P) and gold (G) labels.
Lack of access to background knowledge.
Consider the first example for ABO in Table 5.
Our system misclassifies this sentence in part be-
cause it lacks the background knowledge that “ge-
netic code” is one of the characteristics of life and
a fetus having it means a fetus has life (A3). Sim-
ilarly, the system cannot determine the reason for
the first OBA example without the knowledge that
“deficit spending” is a term related to the econ-
omy and that increasing it is bad (A1). We believe
some of these relations can be extracted from lex-
ical knowledge bases such as YAGO2 (Suchanek
et al., 2007), Freebase (Bollacker et al., 2008), and
BabelNet (Navigli and Ponzetto, 2012).
Failure to process complex discourse struc-
tures. Our system misclassifies the second ex-
ample for ABO in part because the first part of
the sentence (i.e., Sure, the fetus has the potential
to one day be a person) expresses a meaning that
is completely inverted by the second part. Such
complex discourse structures often lead to classi-
fication errors even for sentences whose interpre-
tation requires no background knowledge. We be-
lieve that this problem can be addressed in part
by a better understanding of the structure of a dis-
course, particularly the relation between two dis-
course segments, using a discourse parser.
Failure to process sarcastic statements and
rhetorical questions. Owing to the nature of
our dataset (i.e., debate posts), many errors arise
from sentences containing sarcasm and/or rhetori-
cal questions. This is especially a problem in long
post sequences, where authors frequently restate
their opponents’ positions, sometimes ironically.
A first step towards handling these errors would
therefore be to identify sentences containing sar-
casm and/or rhetorical questions.
</bodyText>
<sectionHeader confidence="0.999878" genericHeader="related work">
6 Related Work
</sectionHeader>
<bodyText confidence="0.999865222222222">
In this section, we discuss related work in the areas
of document-level RC, argument recognition, tex-
tual entailment in online debates, argumentation
mining, and sentiment analysis.
Document-level reason classification. Persing
and Ng (2009) apply a multi-label text classifi-
cation approach to document-level RC of aviation
safety incident reports. Given a set of pre-defined
reasons, their RC system seeks to identify the rea-
sons that can explain why the incident described
in a given report occurred. Their work is dif-
ferent from ours in at least two respects. First,
while our posts occur in post sequences (which
can be profitably exploited in RC, for example, as
in J3), their incident reports were written indepen-
dently of each other. Second, they do not perform
sentence-level RC, as the lack of sentence-level
reason annotations in their dataset prevented them
from training a sentence-level reason classifier.
Argument recognition. Boltuˇzi´c and ˇSnajder
(2014) propose a multi-class classification task
called argument recognition in online discussions.
Given a post and a reason for a particular domain,
the task is to predict the extent to which the au-
thor of the post supports or opposes the reason
as measured on a five-point ordinal scale rang-
ing from “explicitly supports” to “explicitly op-
poses”. Hence, unlike RC, argument recognition
focuses on the magnitude rather than the exis-
tence of a post-reason relationship. In addition,
Boltuˇzi´c and ˇSnajder focus on post-level (rather
than sentence-level) classification and employ per-
fect (rather than predicted) stance information.
Textual entailment in online debates. Given
the title of a debate and a post written in re-
sponse to it, this task seeks to detect arguments in
</bodyText>
<page confidence="0.995923">
759
</page>
<table confidence="0.999777909090909">
Domain Background knowledge Complex discourse structure Sarcasm/rhetorical questions
Example P G Example P G Example P G
ABO Science does agree that NONE A3 Sure, the fetus has the NONE F3 So are there enough F9 F5
the fetus has an individ- potential to one day be homes for 50,000,000
ual genetic code and fits a person, but right now babies?
into the biological defi- it is not.
nition of life.
OBA Democrats have NONE A1 Bush raised the debt A2 A1 I agree, Bush put us in NONE F3
increased deficit spend- by two billion for the debt for the next 100
ing by 2 trillion dollars wars, Obama has out- years, so we can blame
over 2 years. spent that in a week. Obama forever.
</table>
<tableCaption confidence="0.999766">
Table 5: Examples of the major sources of error. P and G stand for predicted tag and gold tag respectively.
</tableCaption>
<bodyText confidence="0.999973169230769">
the post that entail or contradict the title (Cabrio
and Villata, 2012). Hence, this task is concerned
with identifying text segments that correspond to
rationales without a predefined set of rationales,
whereas RC is concerned with both identifying
text segments and classifying them based on a
given set of reasons.
Argumentation mining. The goal of this task is
to extract the argumentative structure of a docu-
ment. Researchers have proposed approaches to
mine the structure of scientific papers (Teufel and
Moens, 2000; Teufel, 2001), product reviews (Vil-
lalba and Saint-Dizier, 2012; Wyner et al., 2012),
newspaper articles (Feng and Hirst, 2011), and le-
gal documents (Br¨uninghaus and Ashley, 2005;
Wyner et al., 2010; Palau and Moens, 2011; Ash-
ley and Walker, 2013). A major difference be-
tween this task and RC is that the argument types
refer to generic structural cues, textual patterns
etc., whereas our reason classes refer to the spe-
cific reasons an author may mention to support her
stance in a domain. For instance, in the case of
a scientific article, the argument types correspond
to general background, description of the paper’s
or some other papers’ approach, objective, con-
trastive and/or comparative comments, etc. (Teufel
and Moens, 2000). The argument types for legal
documents refer to legal factors which are either
pro-plaintiff or pro-defendant (Br¨uninghaus and
Ashley, 2005). For instance, for trade secret law
cases, factors such as Waiver-of-Confidentiality
and Disclosure-in-Public-Forum refer to certain
facts strengthening the claim of one of the sides
participating in a case.
Sentiment analysis. RC resembles certain tasks
in sentiment analysis. One such task is pro and con
reason classification in reviews (Kim and Hovy,
2006), where sentences containing opinions as
well as reasons justifying the opinions are to be
extracted and classified as PRO, CON, or NONE.
Hence, this task focuses on categorizing sentences
into coarse-grained, high-level groups (e.g., PRO
vs. CON, POSITIVE vs. NEGATIVE), but does not
attempt to subcategorize the PRO and CON classes
into fine-grained reason classes, unlike RC. Some-
what similar to the PRO and CON sentence classifi-
cation task is the task of determining the relevance
of a sentence in a review for polarity classifica-
tion. Zaidan et al. (2007) coined the term ratio-
nale to refer to any subjective textual content that
contains evidence supporting the author’s opinion
or stance. These rationales, however, may not al-
ways contain reasons. For instance, a sentence that
mentions that the author likes a product is a ra-
tionale, but it does not contain any reason for her
liking it. Methods have been proposed for auto-
matically identifying rationales (e.g., Yessenalina
et al. (2010), Trivedi and Eisenstein (2013)) and
distinguishing subjective from objective materials
in a review (e.g., Pang and Lee (2004), Wiebe and
Riloff (2005), McDonald et al. (2007), Zhao et al.
(2008)). Note that in all these attempts, the end
goal is not to classify sentences, but to employ
the results of sentence classification to improve a
higher-level task, such as sentiment classification.
</bodyText>
<sectionHeader confidence="0.998191" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999958615384616">
We examined the new task of reason classification.
We exploited stance information for reason classi-
fication, proposing systems of varying complexity
for modeling stances and reasons. Experiments on
our reason-annotated corpus of ideological debate
posts from four domains demonstrate that sophis-
ticated models of stances and reasons can indeed
yield more accurate reason and stance classifica-
tion results than their simpler counterparts. Nev-
ertheless, reason classification remains a challeng-
ing task: the best post-level F-scores are in the low
50s. By making our corpus publicly available, we
hope to stimulate further research on this task.
</bodyText>
<page confidence="0.991949">
760
</page>
<sectionHeader confidence="0.99804" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9999605">
We thank the three anonymous reviewers for their
detailed and insightful comments on an earlier
draft of this paper. This work was supported in
part by NSF Grants IIS-1147644 and IIS-1219142.
Any opinions, findings, conclusions or recommen-
dations expressed in this paper are those of the au-
thors and do not necessarily reflect the views or of-
ficial policies, either expressed or implied, of NSF.
</bodyText>
<sectionHeader confidence="0.997778" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999328284210527">
Rakesh Agrawal, Sridhar Rajagopalan, Ramakrishnan
Srikant, and Yirong Xu. 2003. Mining newsgroups
using networks arising from social behavior. In Pro-
ceedings of the 12th International Conference on
World Wide Web, pages 529–535.
Pranav Anand, Marilyn Walker, Rob Abbott, Jean E.
Fox Tree, Robeson Bowmani, and Michael Minor.
2011. Cats rule and dogs drool!: Classifying stance
in online debate. In Proceedings of the 2nd Work-
shop on Computational Approaches to Subjectivity
and Sentiment Analysis, pages 1–9.
Kevin D. Ashley and Vern R. Walker. 2013. From in-
formation retrieval (IR) to argument retrieval (AR)
for legal cases: Report on a baseline study. In Pro-
ceedings of the 26th International Conference on Le-
gal Knowledge and Information System, pages 29–
38.
Mohit Bansal, Claire Cardie, and Lillian Lee. 2008.
The power of negative thinking: Exploiting label
disagreement in the min-cut classification frame-
work. In COLING 2008: Companion volume:
Posters, pages 15–18.
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: A col-
laboratively created graph database for structuring
human knowledge. In Proceedings of the 2008 ACM
SIGMOD International Conference on Management
of Data, pages 1247–1250.
Filip Boltuˇzi´c and Jan ˇSnajder. 2014. Back up your
stance: Recognizing arguments in online discus-
sions. In Proceedings of the First Workshop on Ar-
gumentation Mining, pages 49–58.
Stefanie Br¨uninghaus and Kevin D. Ashley. 2005.
Reasoning with textual cases. In Proceedings of the
6th International Conference on Case-Based Rea-
soning, pages 137–151.
Clinton Burfoot, Steven Bird, and Timothy Baldwin.
2011. Collective classification of congressional
floor-debate transcripts. In Proceedings of the 49th
Annual Meeting of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 1506–1515.
Elena Cabrio and Serena Villata. 2012. Natural lan-
guage arguments: A combined approach. In Pro-
ceedings of the 20th European Conference on Artifi-
cial Intelligence, pages 205–210.
Jean Carletta. 1996. Assessing agreement on classi-
fication tasks: The Kappa statistic. Computational
Linguistics, 22(2):249–254.
Dipanjan Das, Nathan Schneider, Desai Chen, and
Noah A. Smith. 2010. SEMAFOR 1.0: A
probabilistic frame-semantic parser. Technical re-
port, Carnegie Mellon University Technical Report
CMU-LTI-10-001.
Vanessa Wei Feng and Graeme Hirst. 2011. Classi-
fying arguments by scheme. In Proceedings of the
49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, pages 987–996.
Kazi Saidul Hasan and Vincent Ng. 2013a. Extra-
linguistic constraints on stance recognition in ide-
ological debates. In Proceedings of the 51st Annual
Meeting of the Association for Computational Lin-
guistics (Volume 2: Short Papers), pages 816–821.
Kazi Saidul Hasan and Vincent Ng. 2013b. Frame se-
mantics for stance classification. In Proceedings of
the Seventeenth Conference on Computational Nat-
ural Language Learning, pages 124–132.
Kazi Saidul Hasan and Vincent Ng. 2013c. Stance
classification of ideological debates: Data, mod-
els, features, and constraints. In Proceedings of
the Sixth International Joint Conference on Natural
Language Processing, pages 1348–1356.
Soo-Min Kim and Eduard Hovy. 2006. Automatic
identification of pro and con reasons in online re-
views. In Proceedings of the COLING/ACL Main
Conference Poster Sessions, pages 483–490.
Andrew McCallum, Dayne Freitag, and Fernando
Pereira. 2000. Maximum entropy Markov mod-
els for information extraction and segmentation. In
Proceedings of the 17th International Conference on
Machine Learning, pages 591–598.
Ryan McDonald, Kerry Hannan, Tyler Neylon, Mike
Wells, and Jeff Reynar. 2007. Structured models for
fine-to-coarse sentiment analysis. In Proceedings of
the 45th Annual Meeting of the Association of Com-
putational Linguistics, pages 432–439.
Roberto Navigli and Simone Paolo Ponzetto. 2012.
BabelNet: The automatic construction, evaluation
and application of a wide-coverage multilingual se-
mantic network. Artificial Intelligence, 193:217–
250.
Raquel Mochales Palau and Marie-Francine Moens.
2011. Argumentation mining. Artificial Intelligence
and Law, 19(1):1–22.
</reference>
<page confidence="0.964771">
761
</page>
<reference confidence="0.999910567010309">
Bo Pang and Lillian Lee. 2004. A sentimental educa-
tion: Sentiment analysis using subjectivity summa-
rization based on minimum cuts. In Proceedings of
the 42nd Meeting of the Association for Computa-
tional Linguistics, pages 271–278.
Isaac Persing and Vincent Ng. 2009. Semi-supervised
cause identification from aviation safety reports. In
Proceedings of the Joint Conference of the 47th An-
nual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing
of the AFNLP, pages 843–851.
Dan Roth and Wen-tau Yih. 2004. A linear program-
ming formulation for global inference in natural lan-
guage tasks. In Proceedings of the Eighth Confer-
ence on Computational Natural Language Learning,
pages 1–8.
Swapna Somasundaran and Janyce Wiebe. 2009. Rec-
ognizing stances in online debates. In Proceed-
ings of the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International Joint
Conference on Natural Language Processing of the
AFNLP, pages 226–234.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. YAGO: A core of semantic knowl-
edge. In Proceedings of the 16th International
World Wide Web Conference, pages 697–706.
Simone Teufel and Marc Moens. 2000. What’s yours
and what’s mine: Determining intellectual attribu-
tion in scientific text. In Proceedings of the 2000
Joint SIGDAT Conference on Empirical Methods in
Natural Language Processing and Very Large Cor-
pora, pages 9–17.
Simone Teufel. 2001. Task-based evaluation of sum-
mary quality: Describing relationships between sci-
entific papers. In Proceedings of the NAACL Work-
shop on Automatic Summarization, pages 12–21.
Matt Thomas, Bo Pang, and Lillian Lee. 2006. Get out
the vote: Determining support or opposition from
congressional floor-debate transcripts. In Proceed-
ings of the 2006 Conference on Empirical Methods
in Natural Language Processing, pages 327–335.
Rakshit Trivedi and Jacob Eisenstein. 2013. Dis-
course connectors for latent subjectivity in sentiment
analysis. In Proceedings of the 2013 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 808–813.
Maria Paz Garcia Villalba and Patrick Saint-Dizier.
2012. Some facets of argument mining for opinion
analysis. In Proceedings of the Fourth International
Conference on Computational Models of Argument,
pages 23–34.
Andrew J. Viterbi. 1967. Error bounds for convolu-
tional codes and an asymptotically optimum decod-
ing algorithm. IEEE Transactions on Information
Theory, 13(2):260–269.
Marilyn Walker, Pranav Anand, Rob Abbott, and Ricky
Grant. 2012. Stance classification using dialogic
properties of persuasion. In Proceedings of the 2012
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, pages 592–596.
Janyce Wiebe and Ellen Riloff. 2005. Creating subjec-
tive and objective sentence classifiers from unanno-
tated texts. In Proceedings of the 6th International
Conference on Computational Linguistics and Intel-
ligent Text Processing, pages 486–497.
Adam Wyner, Raquel Mochales-Palau, Marie-Francine
Moens, and David Milward. 2010. Approaches to
text mining arguments from legal cases. In Enrico
Francesconi, Simonetta Montemagni, Wim Peters,
and Daniela Tiscornia, editors, Semantic Processing
of Legal Texts: Where the Language of Law Meets
the Law of Language, pages 60–79. Springer-Verlag.
Adam Wyner, Jodi Schneider, Katie Atkinson, and
Trevor J. M. Bench-Capon. 2012. Semi-automated
argumentative analysis of online product reviews. In
Proceedings of the Fourth International Conference
on Computational Models of Argument, pages 43–
50.
Ainur Yessenalina, Yejin Choi, and Claire Cardie.
2010. Automatically generating annotator rationales
to improve sentiment classification. In Proceedings
of the ACL 2010 Conference Short Papers, pages
336–341.
Omar Zaidan, Jason Eisner, and Christine Piatko.
2007. Using ”annotator rationales” to improve ma-
chine learning for text categorization. In Human
Language Technologies 2007: The Conference of
the North American Chapter of the Association for
Computational Linguistics; Proceedings of the Main
Conference, pages 260–267.
Jun Zhao, Kang Liu, and Gen Wang. 2008. Adding re-
dundant features for crfs-based sentence sentiment
classification. In Proceedings of the 2008 Confer-
ence on Empirical Methods in Natural Language
Processing, pages 117–126.
</reference>
<page confidence="0.996657">
762
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.455210">
<title confidence="0.999016">Why are You Taking this Stance? Identifying and Classifying Reasons in Ideological Debates</title>
<author confidence="0.997373">Saidul Hasan</author>
<affiliation confidence="0.990093">Human Language Technology Research University of Texas at</affiliation>
<address confidence="0.549029">Richardson, TX</address>
<abstract confidence="0.992944363636364">Recent years have seen a surge of interest in stance classification in online debates. Oftentimes, however, it is important to determine not only the stance expressed by an author in her debate posts, but also the reasons behind her supporting or opposing the issue under debate. We therefore examine the new task of reason classification in this paper. Given the close interplay between stance classification and reason classification, we design computational models for examining how automatically computed stance information can be profitably exploited for reason classification. Experiments on our reason-annotated corpus of ideological debate posts from four domains demonstrate that sophisticated models of stances and reasons can indeed yield more accurate reason and stance classification results than their simpler counterparts.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Rakesh Agrawal</author>
<author>Sridhar Rajagopalan</author>
<author>Ramakrishnan Srikant</author>
<author>Yirong Xu</author>
</authors>
<title>Mining newsgroups using networks arising from social behavior.</title>
<date>2003</date>
<booktitle>In Proceedings of the 12th International Conference on World Wide Web,</booktitle>
<pages>529--535</pages>
<contexts>
<context position="1424" citStr="Agrawal et al., 2003" startWordPosition="213" endWordPosition="216">fitably exploited for reason classification. Experiments on our reason-annotated corpus of ideological debate posts from four domains demonstrate that sophisticated models of stances and reasons can indeed yield more accurate reason and stance classification results than their simpler counterparts. 1 Introduction In recent years, researchers have begun exploring new opinion mining tasks. One such task is debate stance classi�cation (SC): given a post written for a two-sided topic discussed in an online debate forum, determine which of the two sides (i.e., for or against) its author is taking (Agrawal et al., 2003; Thomas et al., 2006; Bansal et al., 2008; Somasundaran and Wiebe, 2009; Burfoot et al., 2011; Hasan and Ng, 2013b). For example, the author of the post shown in Figure 1 is pro-abortion. Oftentimes, however, it is important to determine not only the author’s stance expressed in her debate posts, but also the reasons why she supports or opposes the issue under debate. Intuitively, given a debate topic such as “Should abortion be banned?” or “Do you support Obamacare?”, it [I feel that abortion should remain legal, or rather, parents should have the power to make the decision themselves and no</context>
</contexts>
<marker>Agrawal, Rajagopalan, Srikant, Xu, 2003</marker>
<rawString>Rakesh Agrawal, Sridhar Rajagopalan, Ramakrishnan Srikant, and Yirong Xu. 2003. Mining newsgroups using networks arising from social behavior. In Proceedings of the 12th International Conference on World Wide Web, pages 529–535.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pranav Anand</author>
<author>Marilyn Walker</author>
<author>Rob Abbott</author>
<author>Jean E Fox Tree</author>
<author>Robeson Bowmani</author>
<author>Michael Minor</author>
</authors>
<title>Cats rule and dogs drool!: Classifying stance in online debate.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis,</booktitle>
<pages>1--9</pages>
<contexts>
<context position="13783" citStr="Anand et al. (2011)" startWordPosition="2267" endWordPosition="2270">expressed in a post and/or its sentence(s). We create one training instance for each sentence in each post in the training set, using the reason label as its class label. We represent each instance using five types of features, as described below. N-gram features. We encode each unigram and bigram collected from the training sentences as a 753 binary feature indicating the n-gram’s presence or absence in a given sentence. Dependency-based features. To capture the inter-word relationships that n-grams may not, we employ the dependency-based features previously used for stance classification in Anand et al. (2011). These features have three variants. In the first variant, the pair of arguments involved in each dependency relation extracted by a dependency parser is used as a feature. The second variant is the same as the first except that the head (i.e., the first argument in a relation) is replaced by its part-of-speech tag. The features in the third variant, the topic-opinion features, are created by replacing each sentiment-bearing word in features of the first two types with its corresponding polarity label (i.e., + or −). Frame-semantic features. While dependencybased features capture the syntacti</context>
</contexts>
<marker>Anand, Walker, Abbott, Tree, Bowmani, Minor, 2011</marker>
<rawString>Pranav Anand, Marilyn Walker, Rob Abbott, Jean E. Fox Tree, Robeson Bowmani, and Michael Minor. 2011. Cats rule and dogs drool!: Classifying stance in online debate. In Proceedings of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, pages 1–9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin D Ashley</author>
<author>Vern R Walker</author>
</authors>
<title>From information retrieval (IR) to argument retrieval (AR) for legal cases: Report on a baseline study.</title>
<date>2013</date>
<booktitle>In Proceedings of the 26th International Conference on Legal Knowledge and Information System,</booktitle>
<pages>29--38</pages>
<contexts>
<context position="41927" citStr="Ashley and Walker, 2013" startWordPosition="7122" endWordPosition="7126">d to rationales without a predefined set of rationales, whereas RC is concerned with both identifying text segments and classifying them based on a given set of reasons. Argumentation mining. The goal of this task is to extract the argumentative structure of a document. Researchers have proposed approaches to mine the structure of scientific papers (Teufel and Moens, 2000; Teufel, 2001), product reviews (Villalba and Saint-Dizier, 2012; Wyner et al., 2012), newspaper articles (Feng and Hirst, 2011), and legal documents (Br¨uninghaus and Ashley, 2005; Wyner et al., 2010; Palau and Moens, 2011; Ashley and Walker, 2013). A major difference between this task and RC is that the argument types refer to generic structural cues, textual patterns etc., whereas our reason classes refer to the specific reasons an author may mention to support her stance in a domain. For instance, in the case of a scientific article, the argument types correspond to general background, description of the paper’s or some other papers’ approach, objective, contrastive and/or comparative comments, etc. (Teufel and Moens, 2000). The argument types for legal documents refer to legal factors which are either pro-plaintiff or pro-defendant </context>
</contexts>
<marker>Ashley, Walker, 2013</marker>
<rawString>Kevin D. Ashley and Vern R. Walker. 2013. From information retrieval (IR) to argument retrieval (AR) for legal cases: Report on a baseline study. In Proceedings of the 26th International Conference on Legal Knowledge and Information System, pages 29– 38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohit Bansal</author>
<author>Claire Cardie</author>
<author>Lillian Lee</author>
</authors>
<title>The power of negative thinking: Exploiting label disagreement in the min-cut classification framework.</title>
<date>2008</date>
<booktitle>In COLING 2008: Companion volume: Posters,</booktitle>
<pages>15--18</pages>
<contexts>
<context position="1466" citStr="Bansal et al., 2008" startWordPosition="221" endWordPosition="224">. Experiments on our reason-annotated corpus of ideological debate posts from four domains demonstrate that sophisticated models of stances and reasons can indeed yield more accurate reason and stance classification results than their simpler counterparts. 1 Introduction In recent years, researchers have begun exploring new opinion mining tasks. One such task is debate stance classi�cation (SC): given a post written for a two-sided topic discussed in an online debate forum, determine which of the two sides (i.e., for or against) its author is taking (Agrawal et al., 2003; Thomas et al., 2006; Bansal et al., 2008; Somasundaran and Wiebe, 2009; Burfoot et al., 2011; Hasan and Ng, 2013b). For example, the author of the post shown in Figure 1 is pro-abortion. Oftentimes, however, it is important to determine not only the author’s stance expressed in her debate posts, but also the reasons why she supports or opposes the issue under debate. Intuitively, given a debate topic such as “Should abortion be banned?” or “Do you support Obamacare?”, it [I feel that abortion should remain legal, or rather, parents should have the power to make the decision themselves and not face any legal hindrance of any form.]1 </context>
</contexts>
<marker>Bansal, Cardie, Lee, 2008</marker>
<rawString>Mohit Bansal, Claire Cardie, and Lillian Lee. 2008. The power of negative thinking: Exploiting label disagreement in the min-cut classification framework. In COLING 2008: Companion volume: Posters, pages 15–18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kurt Bollacker</author>
<author>Colin Evans</author>
<author>Praveen Paritosh</author>
<author>Tim Sturge</author>
<author>Jamie Taylor</author>
</authors>
<title>Freebase: A collaboratively created graph database for structuring human knowledge.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data,</booktitle>
<pages>1247--1250</pages>
<contexts>
<context position="37472" citStr="Bollacker et al., 2008" startWordPosition="6404" endWordPosition="6407">f access to background knowledge. Consider the first example for ABO in Table 5. Our system misclassifies this sentence in part because it lacks the background knowledge that “genetic code” is one of the characteristics of life and a fetus having it means a fetus has life (A3). Similarly, the system cannot determine the reason for the first OBA example without the knowledge that “deficit spending” is a term related to the economy and that increasing it is bad (A1). We believe some of these relations can be extracted from lexical knowledge bases such as YAGO2 (Suchanek et al., 2007), Freebase (Bollacker et al., 2008), and BabelNet (Navigli and Ponzetto, 2012). Failure to process complex discourse structures. Our system misclassifies the second example for ABO in part because the first part of the sentence (i.e., Sure, the fetus has the potential to one day be a person) expresses a meaning that is completely inverted by the second part. Such complex discourse structures often lead to classification errors even for sentences whose interpretation requires no background knowledge. We believe that this problem can be addressed in part by a better understanding of the structure of a discourse, particularly the </context>
</contexts>
<marker>Bollacker, Evans, Paritosh, Sturge, Taylor, 2008</marker>
<rawString>Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. 2008. Freebase: A collaboratively created graph database for structuring human knowledge. In Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data, pages 1247–1250.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Filip Boltuˇzi´c</author>
<author>Jan ˇSnajder</author>
</authors>
<title>Back up your stance: Recognizing arguments in online discussions.</title>
<date>2014</date>
<booktitle>In Proceedings of the First Workshop on Argumentation Mining,</booktitle>
<pages>49--58</pages>
<marker>Boltuˇzi´c, ˇSnajder, 2014</marker>
<rawString>Filip Boltuˇzi´c and Jan ˇSnajder. 2014. Back up your stance: Recognizing arguments in online discussions. In Proceedings of the First Workshop on Argumentation Mining, pages 49–58.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefanie Br¨uninghaus</author>
<author>Kevin D Ashley</author>
</authors>
<title>Reasoning with textual cases.</title>
<date>2005</date>
<booktitle>In Proceedings of the 6th International Conference on Case-Based Reasoning,</booktitle>
<pages>137--151</pages>
<marker>Br¨uninghaus, Ashley, 2005</marker>
<rawString>Stefanie Br¨uninghaus and Kevin D. Ashley. 2005. Reasoning with textual cases. In Proceedings of the 6th International Conference on Case-Based Reasoning, pages 137–151.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Clinton Burfoot</author>
<author>Steven Bird</author>
<author>Timothy Baldwin</author>
</authors>
<title>Collective classification of congressional floor-debate transcripts.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>1506--1515</pages>
<contexts>
<context position="1518" citStr="Burfoot et al., 2011" startWordPosition="230" endWordPosition="233">ological debate posts from four domains demonstrate that sophisticated models of stances and reasons can indeed yield more accurate reason and stance classification results than their simpler counterparts. 1 Introduction In recent years, researchers have begun exploring new opinion mining tasks. One such task is debate stance classi�cation (SC): given a post written for a two-sided topic discussed in an online debate forum, determine which of the two sides (i.e., for or against) its author is taking (Agrawal et al., 2003; Thomas et al., 2006; Bansal et al., 2008; Somasundaran and Wiebe, 2009; Burfoot et al., 2011; Hasan and Ng, 2013b). For example, the author of the post shown in Figure 1 is pro-abortion. Oftentimes, however, it is important to determine not only the author’s stance expressed in her debate posts, but also the reasons why she supports or opposes the issue under debate. Intuitively, given a debate topic such as “Should abortion be banned?” or “Do you support Obamacare?”, it [I feel that abortion should remain legal, or rather, parents should have the power to make the decision themselves and not face any legal hindrance of any form.]1 Let us take a look from the social perspective. [If </context>
</contexts>
<marker>Burfoot, Bird, Baldwin, 2011</marker>
<rawString>Clinton Burfoot, Steven Bird, and Timothy Baldwin. 2011. Collective classification of congressional floor-debate transcripts. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 1506–1515.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elena Cabrio</author>
<author>Serena Villata</author>
</authors>
<title>Natural language arguments: A combined approach.</title>
<date>2012</date>
<booktitle>In Proceedings of the 20th European Conference on Artificial Intelligence,</booktitle>
<pages>205--210</pages>
<contexts>
<context position="41226" citStr="Cabrio and Villata, 2012" startWordPosition="7013" endWordPosition="7016"> an individ- potential to one day be homes for 50,000,000 ual genetic code and fits a person, but right now babies? into the biological defi- it is not. nition of life. OBA Democrats have NONE A1 Bush raised the debt A2 A1 I agree, Bush put us in NONE F3 increased deficit spend- by two billion for the debt for the next 100 ing by 2 trillion dollars wars, Obama has out- years, so we can blame over 2 years. spent that in a week. Obama forever. Table 5: Examples of the major sources of error. P and G stand for predicted tag and gold tag respectively. the post that entail or contradict the title (Cabrio and Villata, 2012). Hence, this task is concerned with identifying text segments that correspond to rationales without a predefined set of rationales, whereas RC is concerned with both identifying text segments and classifying them based on a given set of reasons. Argumentation mining. The goal of this task is to extract the argumentative structure of a document. Researchers have proposed approaches to mine the structure of scientific papers (Teufel and Moens, 2000; Teufel, 2001), product reviews (Villalba and Saint-Dizier, 2012; Wyner et al., 2012), newspaper articles (Feng and Hirst, 2011), and legal document</context>
</contexts>
<marker>Cabrio, Villata, 2012</marker>
<rawString>Elena Cabrio and Serena Villata. 2012. Natural language arguments: A combined approach. In Proceedings of the 20th European Conference on Artificial Intelligence, pages 205–210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean Carletta</author>
</authors>
<title>Assessing agreement on classification tasks: The Kappa statistic.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>2</issue>
<contexts>
<context position="11530" citStr="Carletta, 1996" startWordPosition="1902" endWordPosition="1903">to our bodies (6%); [F6] Others (12%) against [A1] Damages our bodies (23%); [A2] Responsible for brain damage (22%); [A3] If legalized, people will use marijuana and other drugs more (12%); [A4] Causes crime (9%); [A5] Highly addictive (17%); [A6] Others (17%) Table 1: Reason classes and their percentages in the corresponding stance for each domain. sentences given their rarity, we asked each annotator to pick the reason that was highlighted the most in each multi-labeled sentence. Inter-annotator agreement scores at the sentence level and the post level, expressed in terms of Cohen’s Kappa (Carletta, 1996), are shown in Table 2b. Given that the majority of sentences were labeled as NONE, we avoid inflating agreement by not considering the sentences labeled with NONE by both annotators when computing Kappa. As we can see, we achieved substantial post-level agreement and high sentence-level agreement. The major source of inter-annotator disagreement for all four datasets stems from the fact that in many cases, the annotators, while agreeing on the reason class, differ on how long the text span for a reason should be. This hurts sentence-level agreement but not post-level agreement, since the latt</context>
</contexts>
<marker>Carletta, 1996</marker>
<rawString>Jean Carletta. 1996. Assessing agreement on classification tasks: The Kappa statistic. Computational Linguistics, 22(2):249–254.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dipanjan Das</author>
<author>Nathan Schneider</author>
<author>Desai Chen</author>
<author>Noah A Smith</author>
</authors>
<title>SEMAFOR 1.0: A probabilistic frame-semantic parser.</title>
<date>2010</date>
<tech>Technical report,</tech>
<institution>Carnegie Mellon University</institution>
<contexts>
<context position="14709" citStr="Das et al., 2010" startWordPosition="2418" endWordPosition="2421">eech tag. The features in the third variant, the topic-opinion features, are created by replacing each sentiment-bearing word in features of the first two types with its corresponding polarity label (i.e., + or −). Frame-semantic features. While dependencybased features capture the syntactic dependencies, frame-semantic features encode the semantic representation of the concepts in a sentence. Following our previous work on stance classification (Hasan and Ng, 2013c), we employ three types of features computed based on the frame-semantic parse of each sentence in a post obtained from SEMAFOR (Das et al., 2010). Frame-word interaction features encode whether two words appear in different elements of the same frame. Hence, each frame-word interaction feature consists of (1) the name of the frame f from which it is created, and (2) an unordered word pair in which the words are taken from two frame elements of f. A frame-pair feature is represented as a word pair corresponding to the names of two frames and encodes whether the target word of the first frame appears within an element of the second frame. Finally, frame ngram features are a variant of word n-grams. For each word n-gram in the sentence, a</context>
</contexts>
<marker>Das, Schneider, Chen, Smith, 2010</marker>
<rawString>Dipanjan Das, Nathan Schneider, Desai Chen, and Noah A. Smith. 2010. SEMAFOR 1.0: A probabilistic frame-semantic parser. Technical report, Carnegie Mellon University Technical Report CMU-LTI-10-001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vanessa Wei Feng</author>
<author>Graeme Hirst</author>
</authors>
<title>Classifying arguments by scheme.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>987--996</pages>
<contexts>
<context position="41806" citStr="Feng and Hirst, 2011" startWordPosition="7102" endWordPosition="7105">dict the title (Cabrio and Villata, 2012). Hence, this task is concerned with identifying text segments that correspond to rationales without a predefined set of rationales, whereas RC is concerned with both identifying text segments and classifying them based on a given set of reasons. Argumentation mining. The goal of this task is to extract the argumentative structure of a document. Researchers have proposed approaches to mine the structure of scientific papers (Teufel and Moens, 2000; Teufel, 2001), product reviews (Villalba and Saint-Dizier, 2012; Wyner et al., 2012), newspaper articles (Feng and Hirst, 2011), and legal documents (Br¨uninghaus and Ashley, 2005; Wyner et al., 2010; Palau and Moens, 2011; Ashley and Walker, 2013). A major difference between this task and RC is that the argument types refer to generic structural cues, textual patterns etc., whereas our reason classes refer to the specific reasons an author may mention to support her stance in a domain. For instance, in the case of a scientific article, the argument types correspond to general background, description of the paper’s or some other papers’ approach, objective, contrastive and/or comparative comments, etc. (Teufel and Moe</context>
</contexts>
<marker>Feng, Hirst, 2011</marker>
<rawString>Vanessa Wei Feng and Graeme Hirst. 2011. Classifying arguments by scheme. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 987–996.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kazi Saidul Hasan</author>
<author>Vincent Ng</author>
</authors>
<title>Extralinguistic constraints on stance recognition in ideological debates.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),</booktitle>
<pages>816--821</pages>
<contexts>
<context position="1538" citStr="Hasan and Ng, 2013" startWordPosition="234" endWordPosition="237">from four domains demonstrate that sophisticated models of stances and reasons can indeed yield more accurate reason and stance classification results than their simpler counterparts. 1 Introduction In recent years, researchers have begun exploring new opinion mining tasks. One such task is debate stance classi�cation (SC): given a post written for a two-sided topic discussed in an online debate forum, determine which of the two sides (i.e., for or against) its author is taking (Agrawal et al., 2003; Thomas et al., 2006; Bansal et al., 2008; Somasundaran and Wiebe, 2009; Burfoot et al., 2011; Hasan and Ng, 2013b). For example, the author of the post shown in Figure 1 is pro-abortion. Oftentimes, however, it is important to determine not only the author’s stance expressed in her debate posts, but also the reasons why she supports or opposes the issue under debate. Intuitively, given a debate topic such as “Should abortion be banned?” or “Do you support Obamacare?”, it [I feel that abortion should remain legal, or rather, parents should have the power to make the decision themselves and not face any legal hindrance of any form.]1 Let us take a look from the social perspective. [If parents cannot affor</context>
<context position="14561" citStr="Hasan and Ng, 2013" startWordPosition="2392" endWordPosition="2395">as a feature. The second variant is the same as the first except that the head (i.e., the first argument in a relation) is replaced by its part-of-speech tag. The features in the third variant, the topic-opinion features, are created by replacing each sentiment-bearing word in features of the first two types with its corresponding polarity label (i.e., + or −). Frame-semantic features. While dependencybased features capture the syntactic dependencies, frame-semantic features encode the semantic representation of the concepts in a sentence. Following our previous work on stance classification (Hasan and Ng, 2013c), we employ three types of features computed based on the frame-semantic parse of each sentence in a post obtained from SEMAFOR (Das et al., 2010). Frame-word interaction features encode whether two words appear in different elements of the same frame. Hence, each frame-word interaction feature consists of (1) the name of the frame f from which it is created, and (2) an unordered word pair in which the words are taken from two frame elements of f. A frame-pair feature is represented as a word pair corresponding to the names of two frames and encodes whether the target word of the first frame</context>
<context position="18868" citStr="Hasan and Ng, 2013" startWordPosition="3155" endWordPosition="3158">aining post and is represented by all but the quotation and positional features used to train the Baseline RC system, since these two feature types are sentencebased rather than post-based. After training, the resulting classifier can be used to stance-label a post independently of the other posts. In P2, on the other hand, we recast SC as a sequence labeling task. In other words, we train a SC model that assumes as input a post sequence and outputs a stance sequence, with one stance label for each post in the input post sequence. This choice is motivated by an observation we made previously (Hasan and Ng, 2013a): since each post in a sequence is a reply to the preceding post, we could exploit their dependencies by determining their stance labels together.3 As our sequence learner, we employ a maximum entropy Markov model (MEMM) (McCallum et al., 2000). Given an input post sequence PS = (p1, p2, . . . , pn), the MEMM finds the most probable stance sequence S = (s1, s2, ... , sn) by computing P(S|PS), where: n P(S|PS) = Y P(sk|sk−1,pk) (1) k=1 This probability can be computed efficiently via dynamic programming (DP), using a modified version of the Viterbi algorithm (Viterbi, 1967). There is a caveat</context>
</contexts>
<marker>Hasan, Ng, 2013</marker>
<rawString>Kazi Saidul Hasan and Vincent Ng. 2013a. Extralinguistic constraints on stance recognition in ideological debates. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 816–821.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kazi Saidul Hasan</author>
<author>Vincent Ng</author>
</authors>
<title>Frame semantics for stance classification.</title>
<date>2013</date>
<booktitle>In Proceedings of the Seventeenth Conference on Computational Natural Language Learning,</booktitle>
<pages>124--132</pages>
<contexts>
<context position="1538" citStr="Hasan and Ng, 2013" startWordPosition="234" endWordPosition="237">from four domains demonstrate that sophisticated models of stances and reasons can indeed yield more accurate reason and stance classification results than their simpler counterparts. 1 Introduction In recent years, researchers have begun exploring new opinion mining tasks. One such task is debate stance classi�cation (SC): given a post written for a two-sided topic discussed in an online debate forum, determine which of the two sides (i.e., for or against) its author is taking (Agrawal et al., 2003; Thomas et al., 2006; Bansal et al., 2008; Somasundaran and Wiebe, 2009; Burfoot et al., 2011; Hasan and Ng, 2013b). For example, the author of the post shown in Figure 1 is pro-abortion. Oftentimes, however, it is important to determine not only the author’s stance expressed in her debate posts, but also the reasons why she supports or opposes the issue under debate. Intuitively, given a debate topic such as “Should abortion be banned?” or “Do you support Obamacare?”, it [I feel that abortion should remain legal, or rather, parents should have the power to make the decision themselves and not face any legal hindrance of any form.]1 Let us take a look from the social perspective. [If parents cannot affor</context>
<context position="14561" citStr="Hasan and Ng, 2013" startWordPosition="2392" endWordPosition="2395">as a feature. The second variant is the same as the first except that the head (i.e., the first argument in a relation) is replaced by its part-of-speech tag. The features in the third variant, the topic-opinion features, are created by replacing each sentiment-bearing word in features of the first two types with its corresponding polarity label (i.e., + or −). Frame-semantic features. While dependencybased features capture the syntactic dependencies, frame-semantic features encode the semantic representation of the concepts in a sentence. Following our previous work on stance classification (Hasan and Ng, 2013c), we employ three types of features computed based on the frame-semantic parse of each sentence in a post obtained from SEMAFOR (Das et al., 2010). Frame-word interaction features encode whether two words appear in different elements of the same frame. Hence, each frame-word interaction feature consists of (1) the name of the frame f from which it is created, and (2) an unordered word pair in which the words are taken from two frame elements of f. A frame-pair feature is represented as a word pair corresponding to the names of two frames and encodes whether the target word of the first frame</context>
<context position="18868" citStr="Hasan and Ng, 2013" startWordPosition="3155" endWordPosition="3158">aining post and is represented by all but the quotation and positional features used to train the Baseline RC system, since these two feature types are sentencebased rather than post-based. After training, the resulting classifier can be used to stance-label a post independently of the other posts. In P2, on the other hand, we recast SC as a sequence labeling task. In other words, we train a SC model that assumes as input a post sequence and outputs a stance sequence, with one stance label for each post in the input post sequence. This choice is motivated by an observation we made previously (Hasan and Ng, 2013a): since each post in a sequence is a reply to the preceding post, we could exploit their dependencies by determining their stance labels together.3 As our sequence learner, we employ a maximum entropy Markov model (MEMM) (McCallum et al., 2000). Given an input post sequence PS = (p1, p2, . . . , pn), the MEMM finds the most probable stance sequence S = (s1, s2, ... , sn) by computing P(S|PS), where: n P(S|PS) = Y P(sk|sk−1,pk) (1) k=1 This probability can be computed efficiently via dynamic programming (DP), using a modified version of the Viterbi algorithm (Viterbi, 1967). There is a caveat</context>
</contexts>
<marker>Hasan, Ng, 2013</marker>
<rawString>Kazi Saidul Hasan and Vincent Ng. 2013b. Frame semantics for stance classification. In Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 124–132.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kazi Saidul Hasan</author>
<author>Vincent Ng</author>
</authors>
<title>Stance classification of ideological debates: Data, models, features, and constraints.</title>
<date>2013</date>
<booktitle>In Proceedings of the Sixth International Joint Conference on Natural Language Processing,</booktitle>
<pages>1348--1356</pages>
<contexts>
<context position="1538" citStr="Hasan and Ng, 2013" startWordPosition="234" endWordPosition="237">from four domains demonstrate that sophisticated models of stances and reasons can indeed yield more accurate reason and stance classification results than their simpler counterparts. 1 Introduction In recent years, researchers have begun exploring new opinion mining tasks. One such task is debate stance classi�cation (SC): given a post written for a two-sided topic discussed in an online debate forum, determine which of the two sides (i.e., for or against) its author is taking (Agrawal et al., 2003; Thomas et al., 2006; Bansal et al., 2008; Somasundaran and Wiebe, 2009; Burfoot et al., 2011; Hasan and Ng, 2013b). For example, the author of the post shown in Figure 1 is pro-abortion. Oftentimes, however, it is important to determine not only the author’s stance expressed in her debate posts, but also the reasons why she supports or opposes the issue under debate. Intuitively, given a debate topic such as “Should abortion be banned?” or “Do you support Obamacare?”, it [I feel that abortion should remain legal, or rather, parents should have the power to make the decision themselves and not face any legal hindrance of any form.]1 Let us take a look from the social perspective. [If parents cannot affor</context>
<context position="14561" citStr="Hasan and Ng, 2013" startWordPosition="2392" endWordPosition="2395">as a feature. The second variant is the same as the first except that the head (i.e., the first argument in a relation) is replaced by its part-of-speech tag. The features in the third variant, the topic-opinion features, are created by replacing each sentiment-bearing word in features of the first two types with its corresponding polarity label (i.e., + or −). Frame-semantic features. While dependencybased features capture the syntactic dependencies, frame-semantic features encode the semantic representation of the concepts in a sentence. Following our previous work on stance classification (Hasan and Ng, 2013c), we employ three types of features computed based on the frame-semantic parse of each sentence in a post obtained from SEMAFOR (Das et al., 2010). Frame-word interaction features encode whether two words appear in different elements of the same frame. Hence, each frame-word interaction feature consists of (1) the name of the frame f from which it is created, and (2) an unordered word pair in which the words are taken from two frame elements of f. A frame-pair feature is represented as a word pair corresponding to the names of two frames and encodes whether the target word of the first frame</context>
<context position="18868" citStr="Hasan and Ng, 2013" startWordPosition="3155" endWordPosition="3158">aining post and is represented by all but the quotation and positional features used to train the Baseline RC system, since these two feature types are sentencebased rather than post-based. After training, the resulting classifier can be used to stance-label a post independently of the other posts. In P2, on the other hand, we recast SC as a sequence labeling task. In other words, we train a SC model that assumes as input a post sequence and outputs a stance sequence, with one stance label for each post in the input post sequence. This choice is motivated by an observation we made previously (Hasan and Ng, 2013a): since each post in a sequence is a reply to the preceding post, we could exploit their dependencies by determining their stance labels together.3 As our sequence learner, we employ a maximum entropy Markov model (MEMM) (McCallum et al., 2000). Given an input post sequence PS = (p1, p2, . . . , pn), the MEMM finds the most probable stance sequence S = (s1, s2, ... , sn) by computing P(S|PS), where: n P(S|PS) = Y P(sk|sk−1,pk) (1) k=1 This probability can be computed efficiently via dynamic programming (DP), using a modified version of the Viterbi algorithm (Viterbi, 1967). There is a caveat</context>
</contexts>
<marker>Hasan, Ng, 2013</marker>
<rawString>Kazi Saidul Hasan and Vincent Ng. 2013c. Stance classification of ideological debates: Data, models, features, and constraints. In Proceedings of the Sixth International Joint Conference on Natural Language Processing, pages 1348–1356.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Soo-Min Kim</author>
<author>Eduard Hovy</author>
</authors>
<title>Automatic identification of pro and con reasons in online reviews.</title>
<date>2006</date>
<booktitle>In Proceedings of the COLING/ACL Main Conference Poster Sessions,</booktitle>
<pages>483--490</pages>
<contexts>
<context position="42919" citStr="Kim and Hovy, 2006" startWordPosition="7275" endWordPosition="7278">s or some other papers’ approach, objective, contrastive and/or comparative comments, etc. (Teufel and Moens, 2000). The argument types for legal documents refer to legal factors which are either pro-plaintiff or pro-defendant (Br¨uninghaus and Ashley, 2005). For instance, for trade secret law cases, factors such as Waiver-of-Confidentiality and Disclosure-in-Public-Forum refer to certain facts strengthening the claim of one of the sides participating in a case. Sentiment analysis. RC resembles certain tasks in sentiment analysis. One such task is pro and con reason classification in reviews (Kim and Hovy, 2006), where sentences containing opinions as well as reasons justifying the opinions are to be extracted and classified as PRO, CON, or NONE. Hence, this task focuses on categorizing sentences into coarse-grained, high-level groups (e.g., PRO vs. CON, POSITIVE vs. NEGATIVE), but does not attempt to subcategorize the PRO and CON classes into fine-grained reason classes, unlike RC. Somewhat similar to the PRO and CON sentence classification task is the task of determining the relevance of a sentence in a review for polarity classification. Zaidan et al. (2007) coined the term rationale to refer to a</context>
</contexts>
<marker>Kim, Hovy, 2006</marker>
<rawString>Soo-Min Kim and Eduard Hovy. 2006. Automatic identification of pro and con reasons in online reviews. In Proceedings of the COLING/ACL Main Conference Poster Sessions, pages 483–490.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew McCallum</author>
<author>Dayne Freitag</author>
<author>Fernando Pereira</author>
</authors>
<title>Maximum entropy Markov models for information extraction and segmentation.</title>
<date>2000</date>
<booktitle>In Proceedings of the 17th International Conference on Machine Learning,</booktitle>
<pages>591--598</pages>
<contexts>
<context position="19114" citStr="McCallum et al., 2000" startWordPosition="3196" endWordPosition="3200"> to stance-label a post independently of the other posts. In P2, on the other hand, we recast SC as a sequence labeling task. In other words, we train a SC model that assumes as input a post sequence and outputs a stance sequence, with one stance label for each post in the input post sequence. This choice is motivated by an observation we made previously (Hasan and Ng, 2013a): since each post in a sequence is a reply to the preceding post, we could exploit their dependencies by determining their stance labels together.3 As our sequence learner, we employ a maximum entropy Markov model (MEMM) (McCallum et al., 2000). Given an input post sequence PS = (p1, p2, . . . , pn), the MEMM finds the most probable stance sequence S = (s1, s2, ... , sn) by computing P(S|PS), where: n P(S|PS) = Y P(sk|sk−1,pk) (1) k=1 This probability can be computed efficiently via dynamic programming (DP), using a modified version of the Viterbi algorithm (Viterbi, 1967). There is a caveat, however. Recall that the post sequences are generated from a thread. Since a test post may appear in more than one sequence, different occurrences of it may be assigned different stance labels by the MEMM. To determine the final stance label fo</context>
</contexts>
<marker>McCallum, Freitag, Pereira, 2000</marker>
<rawString>Andrew McCallum, Dayne Freitag, and Fernando Pereira. 2000. Maximum entropy Markov models for information extraction and segmentation. In Proceedings of the 17th International Conference on Machine Learning, pages 591–598.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Kerry Hannan</author>
<author>Tyler Neylon</author>
<author>Mike Wells</author>
<author>Jeff Reynar</author>
</authors>
<title>Structured models for fine-to-coarse sentiment analysis.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>432--439</pages>
<contexts>
<context position="44090" citStr="McDonald et al. (2007)" startWordPosition="7462" endWordPosition="7465"> al. (2007) coined the term rationale to refer to any subjective textual content that contains evidence supporting the author’s opinion or stance. These rationales, however, may not always contain reasons. For instance, a sentence that mentions that the author likes a product is a rationale, but it does not contain any reason for her liking it. Methods have been proposed for automatically identifying rationales (e.g., Yessenalina et al. (2010), Trivedi and Eisenstein (2013)) and distinguishing subjective from objective materials in a review (e.g., Pang and Lee (2004), Wiebe and Riloff (2005), McDonald et al. (2007), Zhao et al. (2008)). Note that in all these attempts, the end goal is not to classify sentences, but to employ the results of sentence classification to improve a higher-level task, such as sentiment classification. 7 Conclusion We examined the new task of reason classification. We exploited stance information for reason classification, proposing systems of varying complexity for modeling stances and reasons. Experiments on our reason-annotated corpus of ideological debate posts from four domains demonstrate that sophisticated models of stances and reasons can indeed yield more accurate reas</context>
</contexts>
<marker>McDonald, Hannan, Neylon, Wells, Reynar, 2007</marker>
<rawString>Ryan McDonald, Kerry Hannan, Tyler Neylon, Mike Wells, and Jeff Reynar. 2007. Structured models for fine-to-coarse sentiment analysis. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 432–439.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
<author>Simone Paolo Ponzetto</author>
</authors>
<title>BabelNet: The automatic construction, evaluation and application of a wide-coverage multilingual semantic network.</title>
<date>2012</date>
<journal>Artificial Intelligence,</journal>
<volume>193</volume>
<pages>250</pages>
<contexts>
<context position="37515" citStr="Navigli and Ponzetto, 2012" startWordPosition="6410" endWordPosition="6413">der the first example for ABO in Table 5. Our system misclassifies this sentence in part because it lacks the background knowledge that “genetic code” is one of the characteristics of life and a fetus having it means a fetus has life (A3). Similarly, the system cannot determine the reason for the first OBA example without the knowledge that “deficit spending” is a term related to the economy and that increasing it is bad (A1). We believe some of these relations can be extracted from lexical knowledge bases such as YAGO2 (Suchanek et al., 2007), Freebase (Bollacker et al., 2008), and BabelNet (Navigli and Ponzetto, 2012). Failure to process complex discourse structures. Our system misclassifies the second example for ABO in part because the first part of the sentence (i.e., Sure, the fetus has the potential to one day be a person) expresses a meaning that is completely inverted by the second part. Such complex discourse structures often lead to classification errors even for sentences whose interpretation requires no background knowledge. We believe that this problem can be addressed in part by a better understanding of the structure of a discourse, particularly the relation between two discourse segments, us</context>
</contexts>
<marker>Navigli, Ponzetto, 2012</marker>
<rawString>Roberto Navigli and Simone Paolo Ponzetto. 2012. BabelNet: The automatic construction, evaluation and application of a wide-coverage multilingual semantic network. Artificial Intelligence, 193:217– 250.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raquel Mochales Palau</author>
<author>Marie-Francine Moens</author>
</authors>
<date>2011</date>
<booktitle>Argumentation mining. Artificial Intelligence and Law,</booktitle>
<contexts>
<context position="41901" citStr="Palau and Moens, 2011" startWordPosition="7118" endWordPosition="7121">segments that correspond to rationales without a predefined set of rationales, whereas RC is concerned with both identifying text segments and classifying them based on a given set of reasons. Argumentation mining. The goal of this task is to extract the argumentative structure of a document. Researchers have proposed approaches to mine the structure of scientific papers (Teufel and Moens, 2000; Teufel, 2001), product reviews (Villalba and Saint-Dizier, 2012; Wyner et al., 2012), newspaper articles (Feng and Hirst, 2011), and legal documents (Br¨uninghaus and Ashley, 2005; Wyner et al., 2010; Palau and Moens, 2011; Ashley and Walker, 2013). A major difference between this task and RC is that the argument types refer to generic structural cues, textual patterns etc., whereas our reason classes refer to the specific reasons an author may mention to support her stance in a domain. For instance, in the case of a scientific article, the argument types correspond to general background, description of the paper’s or some other papers’ approach, objective, contrastive and/or comparative comments, etc. (Teufel and Moens, 2000). The argument types for legal documents refer to legal factors which are either pro-p</context>
</contexts>
<marker>Palau, Moens, 2011</marker>
<rawString>Raquel Mochales Palau and Marie-Francine Moens. 2011. Argumentation mining. Artificial Intelligence and Law, 19(1):1–22.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Meeting of the Association for Computational Linguistics,</booktitle>
<pages>271--278</pages>
<contexts>
<context position="44041" citStr="Pang and Lee (2004)" startWordPosition="7454" endWordPosition="7457"> review for polarity classification. Zaidan et al. (2007) coined the term rationale to refer to any subjective textual content that contains evidence supporting the author’s opinion or stance. These rationales, however, may not always contain reasons. For instance, a sentence that mentions that the author likes a product is a rationale, but it does not contain any reason for her liking it. Methods have been proposed for automatically identifying rationales (e.g., Yessenalina et al. (2010), Trivedi and Eisenstein (2013)) and distinguishing subjective from objective materials in a review (e.g., Pang and Lee (2004), Wiebe and Riloff (2005), McDonald et al. (2007), Zhao et al. (2008)). Note that in all these attempts, the end goal is not to classify sentences, but to employ the results of sentence classification to improve a higher-level task, such as sentiment classification. 7 Conclusion We examined the new task of reason classification. We exploited stance information for reason classification, proposing systems of varying complexity for modeling stances and reasons. Experiments on our reason-annotated corpus of ideological debate posts from four domains demonstrate that sophisticated models of stance</context>
</contexts>
<marker>Pang, Lee, 2004</marker>
<rawString>Bo Pang and Lillian Lee. 2004. A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts. In Proceedings of the 42nd Meeting of the Association for Computational Linguistics, pages 271–278.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Isaac Persing</author>
<author>Vincent Ng</author>
</authors>
<title>Semi-supervised cause identification from aviation safety reports.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>843--851</pages>
<contexts>
<context position="38867" citStr="Persing and Ng (2009)" startWordPosition="6615" endWordPosition="6618"> debate posts), many errors arise from sentences containing sarcasm and/or rhetorical questions. This is especially a problem in long post sequences, where authors frequently restate their opponents’ positions, sometimes ironically. A first step towards handling these errors would therefore be to identify sentences containing sarcasm and/or rhetorical questions. 6 Related Work In this section, we discuss related work in the areas of document-level RC, argument recognition, textual entailment in online debates, argumentation mining, and sentiment analysis. Document-level reason classification. Persing and Ng (2009) apply a multi-label text classification approach to document-level RC of aviation safety incident reports. Given a set of pre-defined reasons, their RC system seeks to identify the reasons that can explain why the incident described in a given report occurred. Their work is different from ours in at least two respects. First, while our posts occur in post sequences (which can be profitably exploited in RC, for example, as in J3), their incident reports were written independently of each other. Second, they do not perform sentence-level RC, as the lack of sentence-level reason annotations in t</context>
</contexts>
<marker>Persing, Ng, 2009</marker>
<rawString>Isaac Persing and Vincent Ng. 2009. Semi-supervised cause identification from aviation safety reports. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 843–851.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Roth</author>
<author>Wen-tau Yih</author>
</authors>
<title>A linear programming formulation for global inference in natural language tasks.</title>
<date>2004</date>
<booktitle>In Proceedings of the Eighth Conference on Computational Natural Language Learning,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="20597" citStr="Roth and Yih, 2004" startWordPosition="3468" endWordPosition="3471">ince this is the only RC system that is not stance-specific. For the SC system, we employ P2. Since the SC system and the RC system are trained independently of each other, their outputs may not be consistent. For instance, an inconsistency arises if a post is labeled as for but one or more of its reasons are associated with the opposing stance. In fact, an inconsistency can arise in the output of the RC system alone: reasons associated with both stances may be assigned by the RC systems to different sentences of a given post. To enforce consistency, we apply integer linear programming (ILP) (Roth and Yih, 2004). We formulate one ILP program for each debate post. Each ILP program contains two post-stance variables (xfor and xagainst) and |T |* |LR |reason variables (i.e., one indicator variable zt,r for each reason class r and each sentence t), where |T |is the number of sentences in the post and |LR |is the number of reason labels. Our objective is to maximize the linear combination of these variables and their corresponding probabilities assigned by their respective classifiers (see (2) below) subject to two types of constraints, the integrity constraints and the post-reason constraints. The integr</context>
</contexts>
<marker>Roth, Yih, 2004</marker>
<rawString>Dan Roth and Wen-tau Yih. 2004. A linear programming formulation for global inference in natural language tasks. In Proceedings of the Eighth Conference on Computational Natural Language Learning, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Swapna Somasundaran</author>
<author>Janyce Wiebe</author>
</authors>
<title>Recognizing stances in online debates.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>226--234</pages>
<contexts>
<context position="1496" citStr="Somasundaran and Wiebe, 2009" startWordPosition="225" endWordPosition="229">reason-annotated corpus of ideological debate posts from four domains demonstrate that sophisticated models of stances and reasons can indeed yield more accurate reason and stance classification results than their simpler counterparts. 1 Introduction In recent years, researchers have begun exploring new opinion mining tasks. One such task is debate stance classi�cation (SC): given a post written for a two-sided topic discussed in an online debate forum, determine which of the two sides (i.e., for or against) its author is taking (Agrawal et al., 2003; Thomas et al., 2006; Bansal et al., 2008; Somasundaran and Wiebe, 2009; Burfoot et al., 2011; Hasan and Ng, 2013b). For example, the author of the post shown in Figure 1 is pro-abortion. Oftentimes, however, it is important to determine not only the author’s stance expressed in her debate posts, but also the reasons why she supports or opposes the issue under debate. Intuitively, given a debate topic such as “Should abortion be banned?” or “Do you support Obamacare?”, it [I feel that abortion should remain legal, or rather, parents should have the power to make the decision themselves and not face any legal hindrance of any form.]1 Let us take a look from the so</context>
</contexts>
<marker>Somasundaran, Wiebe, 2009</marker>
<rawString>Swapna Somasundaran and Janyce Wiebe. 2009. Recognizing stances in online debates. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 226–234.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabian M Suchanek</author>
<author>Gjergji Kasneci</author>
<author>Gerhard Weikum</author>
</authors>
<title>YAGO: A core of semantic knowledge.</title>
<date>2007</date>
<booktitle>In Proceedings of the 16th International World Wide Web Conference,</booktitle>
<pages>697--706</pages>
<contexts>
<context position="37437" citStr="Suchanek et al., 2007" startWordPosition="6399" endWordPosition="6402">ed (P) and gold (G) labels. Lack of access to background knowledge. Consider the first example for ABO in Table 5. Our system misclassifies this sentence in part because it lacks the background knowledge that “genetic code” is one of the characteristics of life and a fetus having it means a fetus has life (A3). Similarly, the system cannot determine the reason for the first OBA example without the knowledge that “deficit spending” is a term related to the economy and that increasing it is bad (A1). We believe some of these relations can be extracted from lexical knowledge bases such as YAGO2 (Suchanek et al., 2007), Freebase (Bollacker et al., 2008), and BabelNet (Navigli and Ponzetto, 2012). Failure to process complex discourse structures. Our system misclassifies the second example for ABO in part because the first part of the sentence (i.e., Sure, the fetus has the potential to one day be a person) expresses a meaning that is completely inverted by the second part. Such complex discourse structures often lead to classification errors even for sentences whose interpretation requires no background knowledge. We believe that this problem can be addressed in part by a better understanding of the structur</context>
</contexts>
<marker>Suchanek, Kasneci, Weikum, 2007</marker>
<rawString>Fabian M. Suchanek, Gjergji Kasneci, and Gerhard Weikum. 2007. YAGO: A core of semantic knowledge. In Proceedings of the 16th International World Wide Web Conference, pages 697–706.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simone Teufel</author>
<author>Marc Moens</author>
</authors>
<title>What’s yours and what’s mine: Determining intellectual attribution in scientific text.</title>
<date>2000</date>
<booktitle>In Proceedings of the 2000 Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora,</booktitle>
<pages>9--17</pages>
<contexts>
<context position="41677" citStr="Teufel and Moens, 2000" startWordPosition="7083" endWordPosition="7086"> Examples of the major sources of error. P and G stand for predicted tag and gold tag respectively. the post that entail or contradict the title (Cabrio and Villata, 2012). Hence, this task is concerned with identifying text segments that correspond to rationales without a predefined set of rationales, whereas RC is concerned with both identifying text segments and classifying them based on a given set of reasons. Argumentation mining. The goal of this task is to extract the argumentative structure of a document. Researchers have proposed approaches to mine the structure of scientific papers (Teufel and Moens, 2000; Teufel, 2001), product reviews (Villalba and Saint-Dizier, 2012; Wyner et al., 2012), newspaper articles (Feng and Hirst, 2011), and legal documents (Br¨uninghaus and Ashley, 2005; Wyner et al., 2010; Palau and Moens, 2011; Ashley and Walker, 2013). A major difference between this task and RC is that the argument types refer to generic structural cues, textual patterns etc., whereas our reason classes refer to the specific reasons an author may mention to support her stance in a domain. For instance, in the case of a scientific article, the argument types correspond to general background, de</context>
</contexts>
<marker>Teufel, Moens, 2000</marker>
<rawString>Simone Teufel and Marc Moens. 2000. What’s yours and what’s mine: Determining intellectual attribution in scientific text. In Proceedings of the 2000 Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora, pages 9–17.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simone Teufel</author>
</authors>
<title>Task-based evaluation of summary quality: Describing relationships between scientific papers.</title>
<date>2001</date>
<booktitle>In Proceedings of the NAACL Workshop on Automatic Summarization,</booktitle>
<pages>12--21</pages>
<contexts>
<context position="41692" citStr="Teufel, 2001" startWordPosition="7087" endWordPosition="7088">ources of error. P and G stand for predicted tag and gold tag respectively. the post that entail or contradict the title (Cabrio and Villata, 2012). Hence, this task is concerned with identifying text segments that correspond to rationales without a predefined set of rationales, whereas RC is concerned with both identifying text segments and classifying them based on a given set of reasons. Argumentation mining. The goal of this task is to extract the argumentative structure of a document. Researchers have proposed approaches to mine the structure of scientific papers (Teufel and Moens, 2000; Teufel, 2001), product reviews (Villalba and Saint-Dizier, 2012; Wyner et al., 2012), newspaper articles (Feng and Hirst, 2011), and legal documents (Br¨uninghaus and Ashley, 2005; Wyner et al., 2010; Palau and Moens, 2011; Ashley and Walker, 2013). A major difference between this task and RC is that the argument types refer to generic structural cues, textual patterns etc., whereas our reason classes refer to the specific reasons an author may mention to support her stance in a domain. For instance, in the case of a scientific article, the argument types correspond to general background, description of th</context>
</contexts>
<marker>Teufel, 2001</marker>
<rawString>Simone Teufel. 2001. Task-based evaluation of summary quality: Describing relationships between scientific papers. In Proceedings of the NAACL Workshop on Automatic Summarization, pages 12–21.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matt Thomas</author>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Get out the vote: Determining support or opposition from congressional floor-debate transcripts.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>327--335</pages>
<contexts>
<context position="1445" citStr="Thomas et al., 2006" startWordPosition="217" endWordPosition="220">reason classification. Experiments on our reason-annotated corpus of ideological debate posts from four domains demonstrate that sophisticated models of stances and reasons can indeed yield more accurate reason and stance classification results than their simpler counterparts. 1 Introduction In recent years, researchers have begun exploring new opinion mining tasks. One such task is debate stance classi�cation (SC): given a post written for a two-sided topic discussed in an online debate forum, determine which of the two sides (i.e., for or against) its author is taking (Agrawal et al., 2003; Thomas et al., 2006; Bansal et al., 2008; Somasundaran and Wiebe, 2009; Burfoot et al., 2011; Hasan and Ng, 2013b). For example, the author of the post shown in Figure 1 is pro-abortion. Oftentimes, however, it is important to determine not only the author’s stance expressed in her debate posts, but also the reasons why she supports or opposes the issue under debate. Intuitively, given a debate topic such as “Should abortion be banned?” or “Do you support Obamacare?”, it [I feel that abortion should remain legal, or rather, parents should have the power to make the decision themselves and not face any legal hind</context>
</contexts>
<marker>Thomas, Pang, Lee, 2006</marker>
<rawString>Matt Thomas, Bo Pang, and Lillian Lee. 2006. Get out the vote: Determining support or opposition from congressional floor-debate transcripts. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, pages 327–335.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rakshit Trivedi</author>
<author>Jacob Eisenstein</author>
</authors>
<title>Discourse connectors for latent subjectivity in sentiment analysis.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>808--813</pages>
<contexts>
<context position="43946" citStr="Trivedi and Eisenstein (2013)" startWordPosition="7440" endWordPosition="7443"> the PRO and CON sentence classification task is the task of determining the relevance of a sentence in a review for polarity classification. Zaidan et al. (2007) coined the term rationale to refer to any subjective textual content that contains evidence supporting the author’s opinion or stance. These rationales, however, may not always contain reasons. For instance, a sentence that mentions that the author likes a product is a rationale, but it does not contain any reason for her liking it. Methods have been proposed for automatically identifying rationales (e.g., Yessenalina et al. (2010), Trivedi and Eisenstein (2013)) and distinguishing subjective from objective materials in a review (e.g., Pang and Lee (2004), Wiebe and Riloff (2005), McDonald et al. (2007), Zhao et al. (2008)). Note that in all these attempts, the end goal is not to classify sentences, but to employ the results of sentence classification to improve a higher-level task, such as sentiment classification. 7 Conclusion We examined the new task of reason classification. We exploited stance information for reason classification, proposing systems of varying complexity for modeling stances and reasons. Experiments on our reason-annotated corpu</context>
</contexts>
<marker>Trivedi, Eisenstein, 2013</marker>
<rawString>Rakshit Trivedi and Jacob Eisenstein. 2013. Discourse connectors for latent subjectivity in sentiment analysis. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 808–813.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maria Paz Garcia Villalba</author>
<author>Patrick Saint-Dizier</author>
</authors>
<title>Some facets of argument mining for opinion analysis.</title>
<date>2012</date>
<booktitle>In Proceedings of the Fourth International Conference on Computational Models of Argument,</booktitle>
<pages>23--34</pages>
<contexts>
<context position="41742" citStr="Villalba and Saint-Dizier, 2012" startWordPosition="7091" endWordPosition="7095">or predicted tag and gold tag respectively. the post that entail or contradict the title (Cabrio and Villata, 2012). Hence, this task is concerned with identifying text segments that correspond to rationales without a predefined set of rationales, whereas RC is concerned with both identifying text segments and classifying them based on a given set of reasons. Argumentation mining. The goal of this task is to extract the argumentative structure of a document. Researchers have proposed approaches to mine the structure of scientific papers (Teufel and Moens, 2000; Teufel, 2001), product reviews (Villalba and Saint-Dizier, 2012; Wyner et al., 2012), newspaper articles (Feng and Hirst, 2011), and legal documents (Br¨uninghaus and Ashley, 2005; Wyner et al., 2010; Palau and Moens, 2011; Ashley and Walker, 2013). A major difference between this task and RC is that the argument types refer to generic structural cues, textual patterns etc., whereas our reason classes refer to the specific reasons an author may mention to support her stance in a domain. For instance, in the case of a scientific article, the argument types correspond to general background, description of the paper’s or some other papers’ approach, objectiv</context>
</contexts>
<marker>Villalba, Saint-Dizier, 2012</marker>
<rawString>Maria Paz Garcia Villalba and Patrick Saint-Dizier. 2012. Some facets of argument mining for opinion analysis. In Proceedings of the Fourth International Conference on Computational Models of Argument, pages 23–34.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew J Viterbi</author>
</authors>
<title>Error bounds for convolutional codes and an asymptotically optimum decoding algorithm.</title>
<date>1967</date>
<journal>IEEE Transactions on Information Theory,</journal>
<volume>13</volume>
<issue>2</issue>
<contexts>
<context position="19449" citStr="Viterbi, 1967" startWordPosition="3260" endWordPosition="3261">de previously (Hasan and Ng, 2013a): since each post in a sequence is a reply to the preceding post, we could exploit their dependencies by determining their stance labels together.3 As our sequence learner, we employ a maximum entropy Markov model (MEMM) (McCallum et al., 2000). Given an input post sequence PS = (p1, p2, . . . , pn), the MEMM finds the most probable stance sequence S = (s1, s2, ... , sn) by computing P(S|PS), where: n P(S|PS) = Y P(sk|sk−1,pk) (1) k=1 This probability can be computed efficiently via dynamic programming (DP), using a modified version of the Viterbi algorithm (Viterbi, 1967). There is a caveat, however. Recall that the post sequences are generated from a thread. Since a test post may appear in more than one sequence, different occurrences of it may be assigned different stance labels by the MEMM. To determine the final stance label for the post, we average the probabilities assigned to the for stance over all its occurrences; if the average is &gt; 0.5, then its final label is for; otherwise, its label is against. a RC system independently of each other. We employ the Baseline as our RC system, since this is the only RC system that is not stance-specific. For the SC</context>
</contexts>
<marker>Viterbi, 1967</marker>
<rawString>Andrew J. Viterbi. 1967. Error bounds for convolutional codes and an asymptotically optimum decoding algorithm. IEEE Transactions on Information Theory, 13(2):260–269.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marilyn Walker</author>
<author>Pranav Anand</author>
<author>Rob Abbott</author>
<author>Ricky Grant</author>
</authors>
<title>Stance classification using dialogic properties of persuasion.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>592--596</pages>
<contexts>
<context position="3702" citStr="Walker et al., 2012" startWordPosition="604" endWordPosition="607"> why she supports abortion, namely it’s a woman’s right to abort and unwanted babies are threat to their parents’ future, which are mentioned in the first and third sentences in the post respectively. Our goal in this paper is to examine post- and sentence-level reason classification (RC) in ideological debates. Many online debaters use emotional languages, which may involve sarcasm and insults, to express their points, thereby making RC and SC in ideological debates potentially more challenging than that in other debate settings such as congressional debates and company-internal discussions (Walker et al., 2012). Besides examining the new task of RC in ideological debates, we believe that our work makes three contributions. First, we propose to address post-level RC by means of sentence-level RC by 751 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 751–762, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics (1) determining the reason(s) associated with each of its sentences (if any), and then (2) taking the union of the set of reasons associated with all of its sentences to be the set of reasons associated with the</context>
</contexts>
<marker>Walker, Anand, Abbott, Grant, 2012</marker>
<rawString>Marilyn Walker, Pranav Anand, Rob Abbott, and Ricky Grant. 2012. Stance classification using dialogic properties of persuasion. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 592–596.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce Wiebe</author>
<author>Ellen Riloff</author>
</authors>
<title>Creating subjective and objective sentence classifiers from unannotated texts.</title>
<date>2005</date>
<booktitle>In Proceedings of the 6th International Conference on Computational Linguistics and Intelligent Text Processing,</booktitle>
<pages>486--497</pages>
<contexts>
<context position="44066" citStr="Wiebe and Riloff (2005)" startWordPosition="7458" endWordPosition="7461">classification. Zaidan et al. (2007) coined the term rationale to refer to any subjective textual content that contains evidence supporting the author’s opinion or stance. These rationales, however, may not always contain reasons. For instance, a sentence that mentions that the author likes a product is a rationale, but it does not contain any reason for her liking it. Methods have been proposed for automatically identifying rationales (e.g., Yessenalina et al. (2010), Trivedi and Eisenstein (2013)) and distinguishing subjective from objective materials in a review (e.g., Pang and Lee (2004), Wiebe and Riloff (2005), McDonald et al. (2007), Zhao et al. (2008)). Note that in all these attempts, the end goal is not to classify sentences, but to employ the results of sentence classification to improve a higher-level task, such as sentiment classification. 7 Conclusion We examined the new task of reason classification. We exploited stance information for reason classification, proposing systems of varying complexity for modeling stances and reasons. Experiments on our reason-annotated corpus of ideological debate posts from four domains demonstrate that sophisticated models of stances and reasons can indeed </context>
</contexts>
<marker>Wiebe, Riloff, 2005</marker>
<rawString>Janyce Wiebe and Ellen Riloff. 2005. Creating subjective and objective sentence classifiers from unannotated texts. In Proceedings of the 6th International Conference on Computational Linguistics and Intelligent Text Processing, pages 486–497.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Wyner</author>
<author>Raquel Mochales-Palau</author>
<author>Marie-Francine Moens</author>
<author>David Milward</author>
</authors>
<title>Approaches to text mining arguments from legal cases.</title>
<date>2010</date>
<booktitle>Semantic Processing of Legal Texts: Where the Language of Law Meets the Law of Language,</booktitle>
<pages>60--79</pages>
<editor>In Enrico Francesconi, Simonetta Montemagni, Wim Peters, and Daniela Tiscornia, editors,</editor>
<publisher>Springer-Verlag.</publisher>
<contexts>
<context position="41878" citStr="Wyner et al., 2010" startWordPosition="7114" endWordPosition="7117">th identifying text segments that correspond to rationales without a predefined set of rationales, whereas RC is concerned with both identifying text segments and classifying them based on a given set of reasons. Argumentation mining. The goal of this task is to extract the argumentative structure of a document. Researchers have proposed approaches to mine the structure of scientific papers (Teufel and Moens, 2000; Teufel, 2001), product reviews (Villalba and Saint-Dizier, 2012; Wyner et al., 2012), newspaper articles (Feng and Hirst, 2011), and legal documents (Br¨uninghaus and Ashley, 2005; Wyner et al., 2010; Palau and Moens, 2011; Ashley and Walker, 2013). A major difference between this task and RC is that the argument types refer to generic structural cues, textual patterns etc., whereas our reason classes refer to the specific reasons an author may mention to support her stance in a domain. For instance, in the case of a scientific article, the argument types correspond to general background, description of the paper’s or some other papers’ approach, objective, contrastive and/or comparative comments, etc. (Teufel and Moens, 2000). The argument types for legal documents refer to legal factors</context>
</contexts>
<marker>Wyner, Mochales-Palau, Moens, Milward, 2010</marker>
<rawString>Adam Wyner, Raquel Mochales-Palau, Marie-Francine Moens, and David Milward. 2010. Approaches to text mining arguments from legal cases. In Enrico Francesconi, Simonetta Montemagni, Wim Peters, and Daniela Tiscornia, editors, Semantic Processing of Legal Texts: Where the Language of Law Meets the Law of Language, pages 60–79. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Wyner</author>
<author>Jodi Schneider</author>
<author>Katie Atkinson</author>
<author>Trevor J M Bench-Capon</author>
</authors>
<title>Semi-automated argumentative analysis of online product reviews.</title>
<date>2012</date>
<booktitle>In Proceedings of the Fourth International Conference on Computational Models of Argument,</booktitle>
<pages>43--50</pages>
<contexts>
<context position="41763" citStr="Wyner et al., 2012" startWordPosition="7096" endWordPosition="7099">pectively. the post that entail or contradict the title (Cabrio and Villata, 2012). Hence, this task is concerned with identifying text segments that correspond to rationales without a predefined set of rationales, whereas RC is concerned with both identifying text segments and classifying them based on a given set of reasons. Argumentation mining. The goal of this task is to extract the argumentative structure of a document. Researchers have proposed approaches to mine the structure of scientific papers (Teufel and Moens, 2000; Teufel, 2001), product reviews (Villalba and Saint-Dizier, 2012; Wyner et al., 2012), newspaper articles (Feng and Hirst, 2011), and legal documents (Br¨uninghaus and Ashley, 2005; Wyner et al., 2010; Palau and Moens, 2011; Ashley and Walker, 2013). A major difference between this task and RC is that the argument types refer to generic structural cues, textual patterns etc., whereas our reason classes refer to the specific reasons an author may mention to support her stance in a domain. For instance, in the case of a scientific article, the argument types correspond to general background, description of the paper’s or some other papers’ approach, objective, contrastive and/or</context>
</contexts>
<marker>Wyner, Schneider, Atkinson, Bench-Capon, 2012</marker>
<rawString>Adam Wyner, Jodi Schneider, Katie Atkinson, and Trevor J. M. Bench-Capon. 2012. Semi-automated argumentative analysis of online product reviews. In Proceedings of the Fourth International Conference on Computational Models of Argument, pages 43– 50.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ainur Yessenalina</author>
<author>Yejin Choi</author>
<author>Claire Cardie</author>
</authors>
<title>Automatically generating annotator rationales to improve sentiment classification.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACL 2010 Conference Short Papers,</booktitle>
<pages>336--341</pages>
<contexts>
<context position="43915" citStr="Yessenalina et al. (2010)" startWordPosition="7436" endWordPosition="7439">ike RC. Somewhat similar to the PRO and CON sentence classification task is the task of determining the relevance of a sentence in a review for polarity classification. Zaidan et al. (2007) coined the term rationale to refer to any subjective textual content that contains evidence supporting the author’s opinion or stance. These rationales, however, may not always contain reasons. For instance, a sentence that mentions that the author likes a product is a rationale, but it does not contain any reason for her liking it. Methods have been proposed for automatically identifying rationales (e.g., Yessenalina et al. (2010), Trivedi and Eisenstein (2013)) and distinguishing subjective from objective materials in a review (e.g., Pang and Lee (2004), Wiebe and Riloff (2005), McDonald et al. (2007), Zhao et al. (2008)). Note that in all these attempts, the end goal is not to classify sentences, but to employ the results of sentence classification to improve a higher-level task, such as sentiment classification. 7 Conclusion We examined the new task of reason classification. We exploited stance information for reason classification, proposing systems of varying complexity for modeling stances and reasons. Experiment</context>
</contexts>
<marker>Yessenalina, Choi, Cardie, 2010</marker>
<rawString>Ainur Yessenalina, Yejin Choi, and Claire Cardie. 2010. Automatically generating annotator rationales to improve sentiment classification. In Proceedings of the ACL 2010 Conference Short Papers, pages 336–341.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omar Zaidan</author>
<author>Jason Eisner</author>
<author>Christine Piatko</author>
</authors>
<title>Using ”annotator rationales” to improve machine learning for text categorization.</title>
<date>2007</date>
<booktitle>In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference,</booktitle>
<pages>260--267</pages>
<contexts>
<context position="43479" citStr="Zaidan et al. (2007)" startWordPosition="7365" endWordPosition="7368">nd con reason classification in reviews (Kim and Hovy, 2006), where sentences containing opinions as well as reasons justifying the opinions are to be extracted and classified as PRO, CON, or NONE. Hence, this task focuses on categorizing sentences into coarse-grained, high-level groups (e.g., PRO vs. CON, POSITIVE vs. NEGATIVE), but does not attempt to subcategorize the PRO and CON classes into fine-grained reason classes, unlike RC. Somewhat similar to the PRO and CON sentence classification task is the task of determining the relevance of a sentence in a review for polarity classification. Zaidan et al. (2007) coined the term rationale to refer to any subjective textual content that contains evidence supporting the author’s opinion or stance. These rationales, however, may not always contain reasons. For instance, a sentence that mentions that the author likes a product is a rationale, but it does not contain any reason for her liking it. Methods have been proposed for automatically identifying rationales (e.g., Yessenalina et al. (2010), Trivedi and Eisenstein (2013)) and distinguishing subjective from objective materials in a review (e.g., Pang and Lee (2004), Wiebe and Riloff (2005), McDonald et</context>
</contexts>
<marker>Zaidan, Eisner, Piatko, 2007</marker>
<rawString>Omar Zaidan, Jason Eisner, and Christine Piatko. 2007. Using ”annotator rationales” to improve machine learning for text categorization. In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference, pages 260–267.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun Zhao</author>
<author>Kang Liu</author>
<author>Gen Wang</author>
</authors>
<title>Adding redundant features for crfs-based sentence sentiment classification.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>117--126</pages>
<contexts>
<context position="44110" citStr="Zhao et al. (2008)" startWordPosition="7466" endWordPosition="7469">erm rationale to refer to any subjective textual content that contains evidence supporting the author’s opinion or stance. These rationales, however, may not always contain reasons. For instance, a sentence that mentions that the author likes a product is a rationale, but it does not contain any reason for her liking it. Methods have been proposed for automatically identifying rationales (e.g., Yessenalina et al. (2010), Trivedi and Eisenstein (2013)) and distinguishing subjective from objective materials in a review (e.g., Pang and Lee (2004), Wiebe and Riloff (2005), McDonald et al. (2007), Zhao et al. (2008)). Note that in all these attempts, the end goal is not to classify sentences, but to employ the results of sentence classification to improve a higher-level task, such as sentiment classification. 7 Conclusion We examined the new task of reason classification. We exploited stance information for reason classification, proposing systems of varying complexity for modeling stances and reasons. Experiments on our reason-annotated corpus of ideological debate posts from four domains demonstrate that sophisticated models of stances and reasons can indeed yield more accurate reason and stance classi</context>
</contexts>
<marker>Zhao, Liu, Wang, 2008</marker>
<rawString>Jun Zhao, Kang Liu, and Gen Wang. 2008. Adding redundant features for crfs-based sentence sentiment classification. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 117–126.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
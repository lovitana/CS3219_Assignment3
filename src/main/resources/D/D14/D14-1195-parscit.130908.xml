<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001253">
<title confidence="0.987608">
Joint Decoding of Tree Transduction Models for Sentence Compression
</title>
<author confidence="0.991626">
Jin-ge Yao Xiaojun Wan Jianguo Xiao
</author>
<affiliation confidence="0.9364305">
Institute of Computer Science and Technology, Peking University, Beijing 100871, China
Key Laboratory of Computational Linguistic (Peking University), MOE, China
</affiliation>
<email confidence="0.979154">
{yaojinge, wanxiaojun, xiaojianguo}@pku.edu.cn
</email>
<sectionHeader confidence="0.993686" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999512">
In this paper, we provide a new method for
decoding tree transduction based sentence
compression models augmented with lan-
guage model scores, by jointly decoding
two components. In our proposed so-
lution, rich local discriminative features
can be easily integrated without increasing
computational complexity. Utilizing an
unobvious fact that the resulted two com-
ponents can be independently decoded, we
conduct efficient joint decoding based on
dual decomposition. Experimental results
show that our method outperforms tradi-
tional beam search decoding and achieves
the state-of-the-art performance.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999955680851064">
Sentence compression is the task of generating a
grammatical and shorter summary for a long sen-
tence while preserving its most important informa-
tion. One specific instantiation is deletion-based
compression, namely generating a compression by
dropping words. Various approaches have been
proposed to challenge the task of deletion-based
compression. Earlier pioneering works (Knight
and Marcu, 2000) considered several insightful
approaches, including noisy-channel based gen-
erative models and discriminative decision tree
models. Structured discriminative compression
models (McDonald, 2006) are capable of inte-
grating rich features and have been proved effec-
tive for this task. Another powerful paradigm for
sentence compression should be mentioned here
is constraints-based compression,including inte-
ger linear programming solutions (Clarke and La-
pata, 2008) and first-order Markov logic networks
(Huang et al., 2012; Yoshikawa et al., 2012).
A notable class of methods that explicitly deal
with syntactic structures are tree transduction
models (Cohn and Lapata, 2007; Cohn and Lap-
ata, 2009). In such models a synchronous gram-
mar is extracted from a corpus of parallel syn-
tax trees with leaves aligned. Compressions are
generated from the grammar with learned weights.
Previous works have noticed that local coherence
is usually needed by introducing ngram language
model scores, which will make accurate decoding
intractable. Traditional approaches conduct beam
search to find approximate solutions (Cohn and
Lapata, 2009).
In this paper we propose a joint decoding strat-
egy to challenge this decoding task. We ad-
dress the problem as jointly decoding a simple
tree transduction model that only considers rule
weights and an ngram compression model. Al-
though either part can be independently solved by
dynamic programming, the naive way to integrate
two groups of partial scores into a huge dynamic
programming chart table is computationally im-
practical. We provide an effective dual decompo-
sition solution that utilizes the efficient decoding
of both parts. By integrating rich structured fea-
tures that cannot be efficiently involved in normal
formulation, results get significantly improved.
</bodyText>
<sectionHeader confidence="0.991211" genericHeader="introduction">
2 Motivation
</sectionHeader>
<bodyText confidence="0.999921428571428">
Under the tree transduction models, the sentence
compression task is formulated as learning a map-
ping from an input source syntax tree to a target
tree with reduced number of leaves. This map-
ping is known as a synchronous grammar. The
synchronous grammar discussed through out this
paper will be synchronous tree substitution gram-
mar (STSG), as in previous studies.
In such formulations, sentence compression is
finding the best derivation from a syntax tree that
produces a simpler target tree, under the current
definition of grammar and learned parameters.
Each derivation is attached with a score. For the
sake of efficient decoding, the score often decom-
</bodyText>
<page confidence="0.912427">
1828
</page>
<bodyText confidence="0.945524666666667">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1828–1833,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
poses with rules involved in the derivation. A typ-
ical score definition for a derivation y of source
tree x is in such form (Cohn and Lapata, 2008;
Cohn and Lapata, 2009):
</bodyText>
<equation confidence="0.9718525">
S(x, y)_ � wTφr(x)+log P(ngram(y)) (1)
rEy
</equation>
<bodyText confidence="0.999883153846154">
The first term is a weighted sum of features φr(x)
defined on each rule r. It is plausible to introduce
local scores from ngram models. The second term
in the above score definition is added with such
purpose.
Cohn and Lapata (2009) explained that ex-
act decoding of Equation 1 is intractable. They
proposed a beam search decoding strategy cou-
pled with cube-pruning heuristic (Chiang, 2007),
which can further improve decoding efficiency at
the cost of largely losing exactness in log probabil-
ity calculations. For efficiency reasons, rich local
ngram features have not been introduced as well.
</bodyText>
<sectionHeader confidence="0.899066" genericHeader="method">
3 Components of Joint Decoding
</sectionHeader>
<bodyText confidence="0.9998970625">
The score in Equation 1 consists of two parts: sum
of weighted rule features and local ngram scores
retrieved from a language model. There is an im-
plicit fact that either part can be used alone with
slight modifications to generate a coarse candidate
compression. Therefore, we can build a joint de-
coding system that consists of these two indepen-
dently decodable components.
In this section we will refer to these two in-
dependent models as the pure tree transduction
model and the pure ngram compression model,
described in Section 3.1 and Section 3.2 respec-
tively. There is a direct generalization of the
ngram model by introducing rich local features,
which results in the structured discriminative mod-
els (Section 3.3).
</bodyText>
<subsectionHeader confidence="0.99955">
3.1 Pure Tree Transduction model
</subsectionHeader>
<bodyText confidence="0.999400444444444">
By merely considering scores from tree transduc-
tion rules, i.e. the first part of Equation 1, we can
have our scores factorized with rules. Then finding
the best derivation from a STSG grammar can be
easily solved by a dynamic programming process
described by Cohn and Lapata (2007).
This simplified pure tree transduction model
can still produce decent compressions if the rule
weights are properly learned during training.
</bodyText>
<subsectionHeader confidence="0.999375">
3.2 Pure Ngram based Compression
</subsectionHeader>
<bodyText confidence="0.999974363636363">
The pure ngram based model will try to find
the most locally smooth compression, reflected
by having the maximum log probability score of
ngrams.
To avoid the trivial solution of deleting all
words, we find the target compression with speci-
fied length by dynamic programming.
Furthermore, we can integrate features other
than log probabilities. This is equivalent to using a
structured discriminative model with rich features
on ngrams of candidate compressions.
</bodyText>
<subsectionHeader confidence="0.996827">
3.3 Structured Discriminative Model
</subsectionHeader>
<bodyText confidence="0.999478">
The structured discriminative model proposed by
McDonald (2006) defines rich features on bigrams
of possible compressions. The score is defined as
weighted linear combination of those features:
</bodyText>
<equation confidence="0.996765">
f(x,z) _ � |z |w · f(x, L(zj−1), L(zj)) (2)
j=2
</equation>
<bodyText confidence="0.999985375">
where the function L(zk) maps a token zk in com-
pression z back to the index of the original sen-
tence x. Decoding can still be efficiently done by
dynamic programming.
With rich local structural information, the struc-
tured discriminative model can play a complemen-
tary role to the tree transduction model that focus
more on global syntactic structures.
</bodyText>
<sectionHeader confidence="0.984239" genericHeader="method">
4 Joint Decoding
</sectionHeader>
<bodyText confidence="0.999773866666667">
From now on the remaining issue is jointly de-
coding the components. Either part factorizes
over local structures: rules for the tree transduc-
tion model and ngrams for the language model or
structured discriminative model. We may build a
large dynamic programming table to utilize this
kind of locality. Unfortunately this is computa-
tionally impractical. It is mathematically equiva-
lent to perform exact dynamic programming de-
coding of Equation 1, which would consume
asymptotically O(SRL2(n−1)V ) 1 time for build-
ing the chart (Cohn and Lapata, 2009). Cohn and
Lapata (2009) proposed a beam search approxima-
tion along with cube-pruning heuristics to reduce
the time complexity down to O(SRBV ) 2.
</bodyText>
<footnote confidence="0.985655">
1S, R, L and V denote respectively for the number of
source tree nodes, the number of rules, size of target lexicon
and number of variables involved in each rule.
2B denotes the beam width.
</footnote>
<page confidence="0.995991">
1829
</page>
<bodyText confidence="0.999990571428572">
In this work we utilize the efficiency of indepen-
dent decoding from the two components respec-
tively and then combine their solutions according
to certain standards. This naturally results in a
dual decomposition (Rush et al., 2010) solution.
Dual decomposition has been applied in sev-
eral natural language processing tasks, including
dependency parsing (Koo et al., 2010), machine
translation (Chang and Collins, 2011; Rush and
Collins, 2011) and information extraction (Re-
ichart and Barzilay, 2012). However, the strength
of this inference strategy has seldom been noticed
in researches on language generation tasks.
We briefly describe the formulation here.
</bodyText>
<subsectionHeader confidence="0.971235">
4.1 Description
</subsectionHeader>
<bodyText confidence="0.999987">
We denote the pure tree transduction part and the
pure ngram part as g(y) and f(z) respectively.
Then joint decoding is equivalent to solving:
</bodyText>
<equation confidence="0.994611">
max g(y) + f(z) (3)
y∈Y,z∈Z
</equation>
<bodyText confidence="0.993519266666667">
s.t. zkt = ykt, ∀k ∈ {1, ..., n}, ∀t ∈ {0, 1},
where y denotes a derivation which yields a final
compression {y1, ..., ym}. This derivation comes
from a pure tree transduction model. z denotes the
compression composed of {z1, ..., zm} from an
ngram compression model. Without loss of gener-
ality, we consider yk and zk as indicators that take
value 1 if the k’s token of original sentence has
been preserved in the compression and 0 if it has
been deleted. In the constraints of problem 3, ykt
or zkt denote indicator variables that take value 1
if yk or zk = t and 0 otherwise.
Let L(u, y, z) be the Lagrangian of (3). Then
the dual objective naturally factorizes into two
parts that can be evaluated independently:
</bodyText>
<equation confidence="0.972664416666667">
L(u) = max
y∈Y,z∈Z
�
=max g(y) + f(z) +
y∈Y,z∈Z k,t
�
=max(g(y) −
y∈Y k,t
�
max (f(z) +
z∈Z
k,t
</equation>
<bodyText confidence="0.946509142857143">
With this factorization, Algorithm 1 tries to
solve the dual problem minu L(u) by alternatively
decoding each component.
This framework is feasible and plausible in that
the two subproblems (line 3 and line 4 in Algo-
rithm 1) can be easily solved with slight modifica-
Algorithm 1 Dual Decomposition Joint Decoding
</bodyText>
<equation confidence="0.9255422">
1: Initialization: u(0)
k = 0, ∀k ∈ {1, ..., n}
2: for i = 1 to MAX ITER do
3: y(i) ← argmaxyEY(g(y) − Ek,t u(i−1)
kt ykt)
4: z(i) ← argmax,EZ(f(z) + Ek,t u(i−1)
kt zkt)
5: if y(i)
kt = z(i)
kt ∀k ∀t then
</equation>
<listItem confidence="0.312116">
6: return (y(i), z(i))
7: else
8: u(i) , u(i−1) — 6(zit − y(t )
kt kt i
9: end if
10: end for
</listItem>
<bodyText confidence="0.99937585">
tions on the values of the original dynamic pro-
gramming chart. Joint decoding of a pure tree
transduction model and a structured discriminative
model is almost the same.
The asymptotic time complexity of Algorithm 1
is O(k(5RV + L2(n−1))), where k denotes the
number of iterations. This is a significant re-
duction of O(5RL2(n−1)V ) by directly solving
the original problem and is also comparable to
O(5RBV) of conducting beam search decoding.
We apply a similar heuristic with Rush and
Collins (2012) to set the step size Ji = 1
t+1, where
t &lt; i is the number of past iterations that increase
the dual value. This setting decreases the step
size only when the dual value moves towards the
wrong direction. We limit the maximum iteration
number to 50 and return the best primal solution
y(i) among all previous iterations for cases that do
not converge in reasonable time.
</bodyText>
<sectionHeader confidence="0.999583" genericHeader="method">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.943895">
5.1 Baselines
</subsectionHeader>
<bodyText confidence="0.9999529">
The pure tree transduction model and the discrim-
inative model naturally become part of our base-
lines for comparison 3. Besides comparing our
methods against the tree-transduction model with
ngram scores by beam search decoding, we also
compare them against the available previous work
from Galanis and Androutsopoulos (2010). This
state-of-the-art work adopts a two-stage method to
rerank results generated by a discriminative maxi-
mum entropy model.
</bodyText>
<subsectionHeader confidence="0.992391">
5.2 Data Preparation
</subsectionHeader>
<bodyText confidence="0.999243">
We evaluated our methods on two standard cor-
pora 4, refer to as Written and Spoken respectively.
</bodyText>
<footnote confidence="0.9965148">
3The pure ngram language model should not be consid-
ered here as it requires additional length constraints and in
general does not produce competitive results at all merely by
itself.
4Available at http://jamesclarke.net/research/resources
</footnote>
<equation confidence="0.9963675">
L(u, y, z)
ukt(zkt − ykt)
uktykt) +
uktzkt)
</equation>
<page confidence="0.86908">
1830
</page>
<bodyText confidence="0.812138">
We split the datasets according to Table 1.
</bodyText>
<tableCaption confidence="0.997872">
Table 1: Dataset partition (number of sentences)
</tableCaption>
<table confidence="0.999029666666667">
Corpus Training Development Testing
Written 1,014 324 294
Spoken 931 83 254
</table>
<bodyText confidence="0.7631836">
All tree transduction models require parallel
parse trees with aligned leaves. We parsed all sen-
tences with the Stanford Parser 5 and aligned sen-
tence pairs with minimum edit distance heuristic
6. Syntactic features of the discriminative model
were also taken from these parse trees.
For systems involving ngram scores, we trained
a trigram language model on the Reuters Corpus
(Volume 1) 7 with modified Kneser-Ney smooth-
ing, using the widely used tool SRILM 8.
</bodyText>
<subsectionHeader confidence="0.998102">
5.3 Model Training
</subsectionHeader>
<bodyText confidence="0.9999482">
The training process of a tree transduction model
followed similarly to Cohn and Lapata (2007) us-
ing structured SVMs (Tsochantaridis et al., 2005).
The structured discriminative models were trained
according to McDonald (2006).
</bodyText>
<subsectionHeader confidence="0.965882">
5.4 Evaluation Metrics
</subsectionHeader>
<bodyText confidence="0.999967083333333">
We assessed the compression results by the F1-
score of grammatical relations (provided by a
dependency parser) of generated compressions
against the gold-standard compression (Clarke and
Lapata, 2006). All systems were controlled to pro-
duce similar compression ratios (CR) for fair com-
parison. We also reported manual evaluation on a
sampled subset of 30 sentences from each dataset.
Three unpaid volunteers with self-reported fluency
in English were asked to rate every candidate. Rat-
ings are in the form of 1-5 scores for each com-
pression.
</bodyText>
<sectionHeader confidence="0.999946" genericHeader="evaluation">
6 Results
</sectionHeader>
<bodyText confidence="0.9999185">
We report test set performance of the struc-
tured discriminative model, the pure tree transduc-
tion (T3), Galanis and Androutsopoulos (2010)’s
method (G&amp;A2010), tree transduction with lan-
guage model scores by beam search and the pro-
posed joint decoding solutions.
</bodyText>
<footnote confidence="0.9993265">
5http://nlp.stanford.edu/software/lex-parser.shtml
6Ties were broken by always aligning a token in compres-
sion to its last appearance in the original sentence. This may
better preserve the alignments of full constituents.
7http://trec.nist.gov/data/reuters/reuters.html
8http://www-speech.sri.com/projects/srilm/
</footnote>
<bodyText confidence="0.998745086956522">
Table 2 shows the compression ratios and F-
measure of grammatical relations in average for
each dataset. Table 3 presents averaged human rat-
ing results for each dataset. We carried out pair-
wise t-test to examine the statistical significance
of the differences 9. In both datasets joint decod-
ing with dual decomposition solution outperforms
other systems, especially when structured models
involved. We can also find certain improvements
of joint modeling with dual decomposition on the
original beam search decoding of Equation 1, un-
der very close compression ratios.
Joint decoding of pure tree transduction and dis-
criminative model gives better performance than
the joint model of tree transduction and language
model. From Table 3 we can see that integrat-
ing discriminative model will mostly improve the
preservation of important information rather than
grammaticality. This is reasonable under the fact
that the language model is trained on large scale
data and will often preserve local grammatical co-
herence, while the discriminative model is trained
on small but more compression specific corpora.
</bodyText>
<tableCaption confidence="0.924548666666667">
Table 2: Results of automatic evaluation. (†:
sig. diff. from T3+LM(DD); *: sig. diff. from
T3+Discr.(DD) for p &lt; 0.01)
</tableCaption>
<table confidence="0.9998351875">
Written CR(%) GR-F1(%)
Discriminative 70.3 52.4†*
G&amp;A2010 71.6 60.2*
Pure Tree-Transduction 72.6 52.3†*
T3+LM (Beam Search) 70.4 58.8*
T3+LM (Dual Decomp.) 70.7 60.5
T3+Discr. (Dual Decomp.) 71.0 62.3
Gold-Standard 71.4 100.0
Spoken CR(%) GR-F1(%)
Discriminative 69.5 50.6†*
G&amp;A2010 71.7 59.2*
Pure Tree-Transduction 73.6 53.8†*
T3+LM (Beam Search) 75.5 59.5*
T3+LM (Dual Decomp.) 75.3 61.5
T3+Discr. (Dual Decomp.) 74.9 63.3
Gold-Standard 72.4 100.0
</table>
<bodyText confidence="0.937589222222222">
Table 4 shows some examples of compressed
sentences produced by all the systems in compar-
ison. The two groups of outputs are compressions
of one sentence from the Written corpora and
the Spoken corpora respectively. Ungrammatical
compressions can be found very often by several
baselines for different reasons, such as the outputs
from pure tree transduction and the discriminative
model in the first group. The reason behind the
</bodyText>
<footnote confidence="0.7470415">
9For all multiple comparisons in this paper, significance
level was adjusted by the Holm-Bonferroni method.
</footnote>
<page confidence="0.996173">
1831
</page>
<tableCaption confidence="0.999819666666667">
Table 3: Results of human rating. (†: sig.
diff. from T3+LM(DD); *: sig. diff. from
T3+Discr.(DD), for p &lt; 0.01)
</tableCaption>
<table confidence="0.99987025">
Written GR. Imp. CR(%)
Discriminative 3.92†* 3.46†* 70.6
G&amp;A2010 4.11†* 3.50†* 72.4
Pure Tree-Transduction 3.85†* 3.42†* 70.1
T3+LM (Beam Search) 4.22†* 3.69* 73.0
T3+LM (Dual Decomp.) 4.63 3.98 73.2
T3+Discr. (Dual Decomp.) 4.62 4.25 73.5
Gold-Standard 4.89 4.76 72.9
Spoken GR. Imp. CR(%)
Discriminative 3.95†* 3.62†* 71.2
G&amp;A2010 4.09†* 3.96* 72.5
Pure Tree-Transduction 3.92†* 3.55†* 71.4
T3+LM (Beam Search) 4.20* 3.78* 75.0
T3+LM (Dual Decomp.) 4.35 4.18 74.5
T3+Discr. (Dual Decomp.) 4.47 4.26 74.7
Gold-Standard 4.83 4.80 73.1
</table>
<bodyText confidence="0.99945747368421">
under generation of pure tree transduction is that it
mainly deals with global syntactic integrity merely
in terms of the application of synchronous rules.
Introducing language model scores will smooth
the candidate compressions and avoid many ag-
gressive decisions of tree transduction. Discrim-
inative models are good at local decisions with
poor consideration of grammaticality. We can see
that the joint models have collected their predic-
tive power together. Unfortunately we can still
observe some redundancy from our outputs in the
examples. The size of training corpus is not large
enough to provide enough lexicalized information.
On the other hand, the time consumption of
the joint model with dual decomposition decoding
in our experiments matched the aforementioned
asymptotic analysis. The training process based
on new decoding method consumes similar time
as beam search with cube-pruning heuristic.
</bodyText>
<sectionHeader confidence="0.962689" genericHeader="conclusions">
7 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.996242727272727">
In this paper we propose a joint decoding scheme
for tree transduction based sentence compression.
Experimental results suggest that the proposed
framework works well. The overall performance
gets further improved under our framework by in-
troducing the structured discriminative model.
As several recent efforts have focused on ex-
tracting large-scale parallel corpus for sentence
compression (Filippova and Altun, 2013), we
would like to study how larger corpora can af-
fect tree transduction and our joint decoding so-
</bodyText>
<tableCaption confidence="0.951604">
Table 4: Example outputs
</tableCaption>
<bodyText confidence="0.981091804347826">
Original: It was very high for people who took their
full-time education beyond the age of 18 , and higher
among women than men for all art forms except jazz
and art galleries.
Discr.: It was high for people took education higher
among women.
(Galanis and Androutsopoulos, 2010): It was high for
people who took their education beyond the age of 18 ,
and higher among women.
Pure T3: It was very high for people who took.
T3+LM-BeamSearch: It was very high for people who
took their education beyond the age of 18 , and higher
among women than men.
T3+LM-DualDecomp: It was very high for people who
took their education beyond the age of 18 , and higher
among women than men.
T3+Discr.: It was high for people who took education
beyond the age of 18 , and higher among women than
men.
Gold-Standard: It was very high for people who took
full-time education beyond 18 , and higher among
women for all except jazz and galleries.
Original: But they are still continuing to search the
area to try and see if there were, in fact, any further
shooting incidents.
Discr.: they are continuing to search the area to try and
see if there were, further shooting incidents.
(Galanis and Androutsopoulos, 2010): But they are still
continuing to search the area to try and see if there
were , in fact, any further shooting incidents .
Pure T3: they are continuing to search the area to try
and see if there were any further shooting incidents.
T3+LM-BeamSearch: But they are continuing to
search the area to try and see if there were, in fact,
any further shooting incidents.
T3+LM-DualDecomp: But they are continuing to
search the area to try and see if there were any further
shooting incidents.
T3+Discr.: they are continuing to search the area to try
and see if there were further shooting incidents.
Gold-Standard: they are continuing to search the area
to see if there were any further incidents.
lution. Meanwhile, We would like to explore on
how other text-rewriting problems can be formu-
lated as a joint model and be applicable to similar
strategies described in this work.
</bodyText>
<sectionHeader confidence="0.99495" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999886666666667">
This work was supported by National Hi-Tech Re-
search and Development Program (863 Program)
of China (2014AA015102, 2012AA011101) and
National Natural Science Foundation of China
(61170166, 61331011). We also thank the anony-
mous reviewers for very helpful comments.
The contact author of this paper, according to
the meaning given to this role by Peking Univer-
sity, is Xiaojun Wan.
</bodyText>
<page confidence="0.994443">
1832
</page>
<sectionHeader confidence="0.989913" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999853901960784">
Yin-Wen Chang and Michael Collins. 2011. Exact de-
coding of phrase-based translation models through
lagrangian relaxation. In Proceedings of the 2011
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 26–37, Edinburgh, Scot-
land, UK., July. Association for Computational Lin-
guistics.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. computational linguistics, 33(2):201–228.
James Clarke and Mirella Lapata. 2006. Models
for sentence compression: A comparison across do-
mains, training requirements and evaluation mea-
sures. In Proceedings of the 21st International Con-
ference on Computational Linguistics and the 44th
annual meeting of the Association for Computa-
tional Linguistics, pages 377–384. Association for
Computational Linguistics.
James Clarke and Mirella Lapata. 2008. Global in-
ference for sentence compression: An integer linear
programming approach. Journal ofArtificial Intelli-
gence Research, 31:273–381.
Trevor Cohn and Mirella Lapata. 2007. Large mar-
gin synchronous generation and its application to
sentence compression. In Proceedings of the 2007
Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Nat-
ural Language Learning (EMNLP-CoNLL), pages
73–82, Prague, Czech Republic, June. Association
for Computational Linguistics.
Trevor Cohn and Mirella Lapata. 2008. Sentence
compression beyond word deletion. In Proceedings
of the 22nd International Conference on Computa-
tional Linguistics-Volume 1, pages 137–144. Asso-
ciation for Computational Linguistics.
Trevor Cohn and Mirella Lapata. 2009. Sentence com-
pression as tree transduction. Journal of Artificial
Intelligence Research, 34:637–674.
Katja Filippova and Yasemin Altun. 2013. Overcom-
ing the lack of parallel data in sentence compres-
sion. In Proceedings of the 2013 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1481–1491, Seattle, Washington, USA,
October. Association for Computational Linguistics.
Dimitrios Galanis and Ion Androutsopoulos. 2010. An
extractive supervised two-stage method for sentence
compression. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 885–893, Los Angeles, California,
June. Association for Computational Linguistics.
Minlie Huang, Xing Shi, Feng Jin, and Xiaoyan Zhu.
2012. Using first-order logic to compress sentences.
In AAAI.
Kevin Knight and Daniel Marcu. 2000. Statistics-
based summarization-step one: Sentence compres-
sion. In AAAI/IAAI, pages 703–710.
Terry Koo, Alexander M. Rush, Michael Collins,
Tommi Jaakkola, and David Sontag. 2010. Dual
decomposition for parsing with non-projective head
automata. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1288–1298, Cambridge, MA, October.
Association for Computational Linguistics.
Ryan T McDonald. 2006. Discriminative sentence
compression with soft syntactic evidence. In EACL.
Roi Reichart and Regina Barzilay. 2012. Multi-event
extraction guided by global constraints. In Proceed-
ings of the 2012 Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, pages 70–
79, Montr´eal, Canada, June. Association for Com-
putational Linguistics.
Alexander M. Rush and Michael Collins. 2011. Exact
decoding of syntactic translation models through la-
grangian relaxation. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
72–82, Portland, Oregon, USA, June. Association
for Computational Linguistics.
Alexander M Rush and Michael Collins. 2012. A tuto-
rial on dual decomposition and lagrangian relaxation
for inference in natural language processing. Jour-
nal of Artificial Intelligence Research, 45:305–362.
Alexander M Rush, David Sontag, Michael Collins,
and Tommi Jaakkola. 2010. On dual decomposition
and linear programming relaxations for natural lan-
guage processing. In Proceedings of the 2010 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1–11, Cambridge, MA, October.
Association for Computational Linguistics.
Ioannis Tsochantaridis, Thorsten Joachims, Thomas
Hofmann, and Yasemin Altun. 2005. Large mar-
gin methods for structured and interdependent out-
put variables. In Journal of Machine Learning Re-
search, pages 1453–1484.
Katsumasa Yoshikawa, Tsutomu Hirao, Ryu Iida, and
Manabu Okumura. 2012. Sentence compression
with semantic role constraints. In Proceedings of the
50th Annual Meeting of the Association for Compu-
tational Linguistics: Short Papers-Volume 2, pages
349–353. Association for Computational Linguis-
tics.
</reference>
<page confidence="0.980239">
1833
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.809817">
<title confidence="0.973817">Joint Decoding of Tree Transduction Models for Sentence Compression</title>
<author confidence="0.954505">Jin-ge Yao Xiaojun Wan Jianguo Xiao</author>
<affiliation confidence="0.967381">Institute of Computer Science and Technology, Peking University, Beijing 100871, China</affiliation>
<address confidence="0.883224">Key Laboratory of Computational Linguistic (Peking University), MOE, China</address>
<email confidence="0.994111">wanxiaojun,</email>
<abstract confidence="0.9991036875">In this paper, we provide a new method for decoding tree transduction based sentence compression models augmented with language model scores, by jointly decoding two components. In our proposed solution, rich local discriminative features can be easily integrated without increasing computational complexity. Utilizing an unobvious fact that the resulted two components can be independently decoded, we conduct efficient joint decoding based on dual decomposition. Experimental results show that our method outperforms traditional beam search decoding and achieves the state-of-the-art performance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yin-Wen Chang</author>
<author>Michael Collins</author>
</authors>
<title>Exact decoding of phrase-based translation models through lagrangian relaxation.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>26--37</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Edinburgh, Scotland, UK.,</location>
<contexts>
<context position="8480" citStr="Chang and Collins, 2011" startWordPosition="1300" endWordPosition="1303">(SRBV ) 2. 1S, R, L and V denote respectively for the number of source tree nodes, the number of rules, size of target lexicon and number of variables involved in each rule. 2B denotes the beam width. 1829 In this work we utilize the efficiency of independent decoding from the two components respectively and then combine their solutions according to certain standards. This naturally results in a dual decomposition (Rush et al., 2010) solution. Dual decomposition has been applied in several natural language processing tasks, including dependency parsing (Koo et al., 2010), machine translation (Chang and Collins, 2011; Rush and Collins, 2011) and information extraction (Reichart and Barzilay, 2012). However, the strength of this inference strategy has seldom been noticed in researches on language generation tasks. We briefly describe the formulation here. 4.1 Description We denote the pure tree transduction part and the pure ngram part as g(y) and f(z) respectively. Then joint decoding is equivalent to solving: max g(y) + f(z) (3) y∈Y,z∈Z s.t. zkt = ykt, ∀k ∈ {1, ..., n}, ∀t ∈ {0, 1}, where y denotes a derivation which yields a final compression {y1, ..., ym}. This derivation comes from a pure tree transdu</context>
</contexts>
<marker>Chang, Collins, 2011</marker>
<rawString>Yin-Wen Chang and Michael Collins. 2011. Exact decoding of phrase-based translation models through lagrangian relaxation. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 26–37, Edinburgh, Scotland, UK., July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation. computational linguistics,</title>
<date>2007</date>
<pages>33--2</pages>
<contexts>
<context position="4611" citStr="Chiang, 2007" startWordPosition="683" endWordPosition="684">guistics poses with rules involved in the derivation. A typical score definition for a derivation y of source tree x is in such form (Cohn and Lapata, 2008; Cohn and Lapata, 2009): S(x, y)_ � wTφr(x)+log P(ngram(y)) (1) rEy The first term is a weighted sum of features φr(x) defined on each rule r. It is plausible to introduce local scores from ngram models. The second term in the above score definition is added with such purpose. Cohn and Lapata (2009) explained that exact decoding of Equation 1 is intractable. They proposed a beam search decoding strategy coupled with cube-pruning heuristic (Chiang, 2007), which can further improve decoding efficiency at the cost of largely losing exactness in log probability calculations. For efficiency reasons, rich local ngram features have not been introduced as well. 3 Components of Joint Decoding The score in Equation 1 consists of two parts: sum of weighted rule features and local ngram scores retrieved from a language model. There is an implicit fact that either part can be used alone with slight modifications to generate a coarse candidate compression. Therefore, we can build a joint decoding system that consists of these two independently decodable c</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. computational linguistics, 33(2):201–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Clarke</author>
<author>Mirella Lapata</author>
</authors>
<title>Models for sentence compression: A comparison across domains, training requirements and evaluation measures.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics,</booktitle>
<pages>377--384</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="13192" citStr="Clarke and Lapata, 2006" startWordPosition="2099" endWordPosition="2102">m scores, we trained a trigram language model on the Reuters Corpus (Volume 1) 7 with modified Kneser-Ney smoothing, using the widely used tool SRILM 8. 5.3 Model Training The training process of a tree transduction model followed similarly to Cohn and Lapata (2007) using structured SVMs (Tsochantaridis et al., 2005). The structured discriminative models were trained according to McDonald (2006). 5.4 Evaluation Metrics We assessed the compression results by the F1- score of grammatical relations (provided by a dependency parser) of generated compressions against the gold-standard compression (Clarke and Lapata, 2006). All systems were controlled to produce similar compression ratios (CR) for fair comparison. We also reported manual evaluation on a sampled subset of 30 sentences from each dataset. Three unpaid volunteers with self-reported fluency in English were asked to rate every candidate. Ratings are in the form of 1-5 scores for each compression. 6 Results We report test set performance of the structured discriminative model, the pure tree transduction (T3), Galanis and Androutsopoulos (2010)’s method (G&amp;A2010), tree transduction with language model scores by beam search and the proposed joint decodi</context>
</contexts>
<marker>Clarke, Lapata, 2006</marker>
<rawString>James Clarke and Mirella Lapata. 2006. Models for sentence compression: A comparison across domains, training requirements and evaluation measures. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, pages 377–384. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Clarke</author>
<author>Mirella Lapata</author>
</authors>
<title>Global inference for sentence compression: An integer linear programming approach.</title>
<date>2008</date>
<journal>Journal ofArtificial Intelligence Research,</journal>
<pages>31--273</pages>
<contexts>
<context position="1797" citStr="Clarke and Lapata, 2008" startWordPosition="236" endWordPosition="240">on by dropping words. Various approaches have been proposed to challenge the task of deletion-based compression. Earlier pioneering works (Knight and Marcu, 2000) considered several insightful approaches, including noisy-channel based generative models and discriminative decision tree models. Structured discriminative compression models (McDonald, 2006) are capable of integrating rich features and have been proved effective for this task. Another powerful paradigm for sentence compression should be mentioned here is constraints-based compression,including integer linear programming solutions (Clarke and Lapata, 2008) and first-order Markov logic networks (Huang et al., 2012; Yoshikawa et al., 2012). A notable class of methods that explicitly deal with syntactic structures are tree transduction models (Cohn and Lapata, 2007; Cohn and Lapata, 2009). In such models a synchronous grammar is extracted from a corpus of parallel syntax trees with leaves aligned. Compressions are generated from the grammar with learned weights. Previous works have noticed that local coherence is usually needed by introducing ngram language model scores, which will make accurate decoding intractable. Traditional approaches conduct</context>
</contexts>
<marker>Clarke, Lapata, 2008</marker>
<rawString>James Clarke and Mirella Lapata. 2008. Global inference for sentence compression: An integer linear programming approach. Journal ofArtificial Intelligence Research, 31:273–381.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Cohn</author>
<author>Mirella Lapata</author>
</authors>
<title>Large margin synchronous generation and its application to sentence compression.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</booktitle>
<pages>73--82</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="2007" citStr="Cohn and Lapata, 2007" startWordPosition="269" endWordPosition="272">g noisy-channel based generative models and discriminative decision tree models. Structured discriminative compression models (McDonald, 2006) are capable of integrating rich features and have been proved effective for this task. Another powerful paradigm for sentence compression should be mentioned here is constraints-based compression,including integer linear programming solutions (Clarke and Lapata, 2008) and first-order Markov logic networks (Huang et al., 2012; Yoshikawa et al., 2012). A notable class of methods that explicitly deal with syntactic structures are tree transduction models (Cohn and Lapata, 2007; Cohn and Lapata, 2009). In such models a synchronous grammar is extracted from a corpus of parallel syntax trees with leaves aligned. Compressions are generated from the grammar with learned weights. Previous works have noticed that local coherence is usually needed by introducing ngram language model scores, which will make accurate decoding intractable. Traditional approaches conduct beam search to find approximate solutions (Cohn and Lapata, 2009). In this paper we propose a joint decoding strategy to challenge this decoding task. We address the problem as jointly decoding a simple tree t</context>
<context position="5885" citStr="Cohn and Lapata (2007)" startWordPosition="890" endWordPosition="893">o independent models as the pure tree transduction model and the pure ngram compression model, described in Section 3.1 and Section 3.2 respectively. There is a direct generalization of the ngram model by introducing rich local features, which results in the structured discriminative models (Section 3.3). 3.1 Pure Tree Transduction model By merely considering scores from tree transduction rules, i.e. the first part of Equation 1, we can have our scores factorized with rules. Then finding the best derivation from a STSG grammar can be easily solved by a dynamic programming process described by Cohn and Lapata (2007). This simplified pure tree transduction model can still produce decent compressions if the rule weights are properly learned during training. 3.2 Pure Ngram based Compression The pure ngram based model will try to find the most locally smooth compression, reflected by having the maximum log probability score of ngrams. To avoid the trivial solution of deleting all words, we find the target compression with specified length by dynamic programming. Furthermore, we can integrate features other than log probabilities. This is equivalent to using a structured discriminative model with rich feature</context>
<context position="12834" citStr="Cohn and Lapata (2007)" startWordPosition="2050" endWordPosition="2053">sting Written 1,014 324 294 Spoken 931 83 254 All tree transduction models require parallel parse trees with aligned leaves. We parsed all sentences with the Stanford Parser 5 and aligned sentence pairs with minimum edit distance heuristic 6. Syntactic features of the discriminative model were also taken from these parse trees. For systems involving ngram scores, we trained a trigram language model on the Reuters Corpus (Volume 1) 7 with modified Kneser-Ney smoothing, using the widely used tool SRILM 8. 5.3 Model Training The training process of a tree transduction model followed similarly to Cohn and Lapata (2007) using structured SVMs (Tsochantaridis et al., 2005). The structured discriminative models were trained according to McDonald (2006). 5.4 Evaluation Metrics We assessed the compression results by the F1- score of grammatical relations (provided by a dependency parser) of generated compressions against the gold-standard compression (Clarke and Lapata, 2006). All systems were controlled to produce similar compression ratios (CR) for fair comparison. We also reported manual evaluation on a sampled subset of 30 sentences from each dataset. Three unpaid volunteers with self-reported fluency in Engl</context>
</contexts>
<marker>Cohn, Lapata, 2007</marker>
<rawString>Trevor Cohn and Mirella Lapata. 2007. Large margin synchronous generation and its application to sentence compression. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 73–82, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Cohn</author>
<author>Mirella Lapata</author>
</authors>
<title>Sentence compression beyond word deletion.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics-Volume 1,</booktitle>
<pages>137--144</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4153" citStr="Cohn and Lapata, 2008" startWordPosition="604" endWordPosition="607">s, sentence compression is finding the best derivation from a syntax tree that produces a simpler target tree, under the current definition of grammar and learned parameters. Each derivation is attached with a score. For the sake of efficient decoding, the score often decom1828 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1828–1833, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics poses with rules involved in the derivation. A typical score definition for a derivation y of source tree x is in such form (Cohn and Lapata, 2008; Cohn and Lapata, 2009): S(x, y)_ � wTφr(x)+log P(ngram(y)) (1) rEy The first term is a weighted sum of features φr(x) defined on each rule r. It is plausible to introduce local scores from ngram models. The second term in the above score definition is added with such purpose. Cohn and Lapata (2009) explained that exact decoding of Equation 1 is intractable. They proposed a beam search decoding strategy coupled with cube-pruning heuristic (Chiang, 2007), which can further improve decoding efficiency at the cost of largely losing exactness in log probability calculations. For efficiency reason</context>
</contexts>
<marker>Cohn, Lapata, 2008</marker>
<rawString>Trevor Cohn and Mirella Lapata. 2008. Sentence compression beyond word deletion. In Proceedings of the 22nd International Conference on Computational Linguistics-Volume 1, pages 137–144. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Cohn</author>
<author>Mirella Lapata</author>
</authors>
<title>Sentence compression as tree transduction.</title>
<date>2009</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>34--637</pages>
<contexts>
<context position="2031" citStr="Cohn and Lapata, 2009" startWordPosition="273" endWordPosition="277">enerative models and discriminative decision tree models. Structured discriminative compression models (McDonald, 2006) are capable of integrating rich features and have been proved effective for this task. Another powerful paradigm for sentence compression should be mentioned here is constraints-based compression,including integer linear programming solutions (Clarke and Lapata, 2008) and first-order Markov logic networks (Huang et al., 2012; Yoshikawa et al., 2012). A notable class of methods that explicitly deal with syntactic structures are tree transduction models (Cohn and Lapata, 2007; Cohn and Lapata, 2009). In such models a synchronous grammar is extracted from a corpus of parallel syntax trees with leaves aligned. Compressions are generated from the grammar with learned weights. Previous works have noticed that local coherence is usually needed by introducing ngram language model scores, which will make accurate decoding intractable. Traditional approaches conduct beam search to find approximate solutions (Cohn and Lapata, 2009). In this paper we propose a joint decoding strategy to challenge this decoding task. We address the problem as jointly decoding a simple tree transduction model that o</context>
<context position="4177" citStr="Cohn and Lapata, 2009" startWordPosition="608" endWordPosition="611"> is finding the best derivation from a syntax tree that produces a simpler target tree, under the current definition of grammar and learned parameters. Each derivation is attached with a score. For the sake of efficient decoding, the score often decom1828 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1828–1833, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics poses with rules involved in the derivation. A typical score definition for a derivation y of source tree x is in such form (Cohn and Lapata, 2008; Cohn and Lapata, 2009): S(x, y)_ � wTφr(x)+log P(ngram(y)) (1) rEy The first term is a weighted sum of features φr(x) defined on each rule r. It is plausible to introduce local scores from ngram models. The second term in the above score definition is added with such purpose. Cohn and Lapata (2009) explained that exact decoding of Equation 1 is intractable. They proposed a beam search decoding strategy coupled with cube-pruning heuristic (Chiang, 2007), which can further improve decoding efficiency at the cost of largely losing exactness in log probability calculations. For efficiency reasons, rich local ngram feat</context>
<context position="7721" citStr="Cohn and Lapata, 2009" startWordPosition="1177" endWordPosition="1180"> model that focus more on global syntactic structures. 4 Joint Decoding From now on the remaining issue is jointly decoding the components. Either part factorizes over local structures: rules for the tree transduction model and ngrams for the language model or structured discriminative model. We may build a large dynamic programming table to utilize this kind of locality. Unfortunately this is computationally impractical. It is mathematically equivalent to perform exact dynamic programming decoding of Equation 1, which would consume asymptotically O(SRL2(n−1)V ) 1 time for building the chart (Cohn and Lapata, 2009). Cohn and Lapata (2009) proposed a beam search approximation along with cube-pruning heuristics to reduce the time complexity down to O(SRBV ) 2. 1S, R, L and V denote respectively for the number of source tree nodes, the number of rules, size of target lexicon and number of variables involved in each rule. 2B denotes the beam width. 1829 In this work we utilize the efficiency of independent decoding from the two components respectively and then combine their solutions according to certain standards. This naturally results in a dual decomposition (Rush et al., 2010) solution. Dual decompositi</context>
</contexts>
<marker>Cohn, Lapata, 2009</marker>
<rawString>Trevor Cohn and Mirella Lapata. 2009. Sentence compression as tree transduction. Journal of Artificial Intelligence Research, 34:637–674.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katja Filippova</author>
<author>Yasemin Altun</author>
</authors>
<title>Overcoming the lack of parallel data in sentence compression.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1481--1491</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Seattle, Washington, USA,</location>
<contexts>
<context position="18345" citStr="Filippova and Altun, 2013" startWordPosition="2863" endWordPosition="2866">r experiments matched the aforementioned asymptotic analysis. The training process based on new decoding method consumes similar time as beam search with cube-pruning heuristic. 7 Conclusion and Future Work In this paper we propose a joint decoding scheme for tree transduction based sentence compression. Experimental results suggest that the proposed framework works well. The overall performance gets further improved under our framework by introducing the structured discriminative model. As several recent efforts have focused on extracting large-scale parallel corpus for sentence compression (Filippova and Altun, 2013), we would like to study how larger corpora can affect tree transduction and our joint decoding soTable 4: Example outputs Original: It was very high for people who took their full-time education beyond the age of 18 , and higher among women than men for all art forms except jazz and art galleries. Discr.: It was high for people took education higher among women. (Galanis and Androutsopoulos, 2010): It was high for people who took their education beyond the age of 18 , and higher among women. Pure T3: It was very high for people who took. T3+LM-BeamSearch: It was very high for people who took </context>
</contexts>
<marker>Filippova, Altun, 2013</marker>
<rawString>Katja Filippova and Yasemin Altun. 2013. Overcoming the lack of parallel data in sentence compression. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1481–1491, Seattle, Washington, USA, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dimitrios Galanis</author>
<author>Ion Androutsopoulos</author>
</authors>
<title>An extractive supervised two-stage method for sentence compression. In Human Language Technologies: The</title>
<date>2010</date>
<booktitle>Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>885--893</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Los Angeles, California,</location>
<contexts>
<context position="11557" citStr="Galanis and Androutsopoulos (2010)" startWordPosition="1849" endWordPosition="1852">se the dual value. This setting decreases the step size only when the dual value moves towards the wrong direction. We limit the maximum iteration number to 50 and return the best primal solution y(i) among all previous iterations for cases that do not converge in reasonable time. 5 Experiments 5.1 Baselines The pure tree transduction model and the discriminative model naturally become part of our baselines for comparison 3. Besides comparing our methods against the tree-transduction model with ngram scores by beam search decoding, we also compare them against the available previous work from Galanis and Androutsopoulos (2010). This state-of-the-art work adopts a two-stage method to rerank results generated by a discriminative maximum entropy model. 5.2 Data Preparation We evaluated our methods on two standard corpora 4, refer to as Written and Spoken respectively. 3The pure ngram language model should not be considered here as it requires additional length constraints and in general does not produce competitive results at all merely by itself. 4Available at http://jamesclarke.net/research/resources L(u, y, z) ukt(zkt − ykt) uktykt) + uktzkt) 1830 We split the datasets according to Table 1. Table 1: Dataset partiti</context>
<context position="13682" citStr="Galanis and Androutsopoulos (2010)" startWordPosition="2179" endWordPosition="2182">rammatical relations (provided by a dependency parser) of generated compressions against the gold-standard compression (Clarke and Lapata, 2006). All systems were controlled to produce similar compression ratios (CR) for fair comparison. We also reported manual evaluation on a sampled subset of 30 sentences from each dataset. Three unpaid volunteers with self-reported fluency in English were asked to rate every candidate. Ratings are in the form of 1-5 scores for each compression. 6 Results We report test set performance of the structured discriminative model, the pure tree transduction (T3), Galanis and Androutsopoulos (2010)’s method (G&amp;A2010), tree transduction with language model scores by beam search and the proposed joint decoding solutions. 5http://nlp.stanford.edu/software/lex-parser.shtml 6Ties were broken by always aligning a token in compression to its last appearance in the original sentence. This may better preserve the alignments of full constituents. 7http://trec.nist.gov/data/reuters/reuters.html 8http://www-speech.sri.com/projects/srilm/ Table 2 shows the compression ratios and Fmeasure of grammatical relations in average for each dataset. Table 3 presents averaged human rating results for each dat</context>
<context position="18746" citStr="Galanis and Androutsopoulos, 2010" startWordPosition="2933" endWordPosition="2936">mance gets further improved under our framework by introducing the structured discriminative model. As several recent efforts have focused on extracting large-scale parallel corpus for sentence compression (Filippova and Altun, 2013), we would like to study how larger corpora can affect tree transduction and our joint decoding soTable 4: Example outputs Original: It was very high for people who took their full-time education beyond the age of 18 , and higher among women than men for all art forms except jazz and art galleries. Discr.: It was high for people took education higher among women. (Galanis and Androutsopoulos, 2010): It was high for people who took their education beyond the age of 18 , and higher among women. Pure T3: It was very high for people who took. T3+LM-BeamSearch: It was very high for people who took their education beyond the age of 18 , and higher among women than men. T3+LM-DualDecomp: It was very high for people who took their education beyond the age of 18 , and higher among women than men. T3+Discr.: It was high for people who took education beyond the age of 18 , and higher among women than men. Gold-Standard: It was very high for people who took full-time education beyond 18 , and highe</context>
</contexts>
<marker>Galanis, Androutsopoulos, 2010</marker>
<rawString>Dimitrios Galanis and Ion Androutsopoulos. 2010. An extractive supervised two-stage method for sentence compression. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 885–893, Los Angeles, California, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minlie Huang</author>
<author>Xing Shi</author>
<author>Feng Jin</author>
<author>Xiaoyan Zhu</author>
</authors>
<title>Using first-order logic to compress sentences.</title>
<date>2012</date>
<booktitle>In AAAI.</booktitle>
<contexts>
<context position="1855" citStr="Huang et al., 2012" startWordPosition="246" endWordPosition="249">hallenge the task of deletion-based compression. Earlier pioneering works (Knight and Marcu, 2000) considered several insightful approaches, including noisy-channel based generative models and discriminative decision tree models. Structured discriminative compression models (McDonald, 2006) are capable of integrating rich features and have been proved effective for this task. Another powerful paradigm for sentence compression should be mentioned here is constraints-based compression,including integer linear programming solutions (Clarke and Lapata, 2008) and first-order Markov logic networks (Huang et al., 2012; Yoshikawa et al., 2012). A notable class of methods that explicitly deal with syntactic structures are tree transduction models (Cohn and Lapata, 2007; Cohn and Lapata, 2009). In such models a synchronous grammar is extracted from a corpus of parallel syntax trees with leaves aligned. Compressions are generated from the grammar with learned weights. Previous works have noticed that local coherence is usually needed by introducing ngram language model scores, which will make accurate decoding intractable. Traditional approaches conduct beam search to find approximate solutions (Cohn and Lapat</context>
</contexts>
<marker>Huang, Shi, Jin, Zhu, 2012</marker>
<rawString>Minlie Huang, Xing Shi, Feng Jin, and Xiaoyan Zhu. 2012. Using first-order logic to compress sentences. In AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>Statisticsbased summarization-step one: Sentence compression.</title>
<date>2000</date>
<booktitle>In AAAI/IAAI,</booktitle>
<pages>703--710</pages>
<contexts>
<context position="1335" citStr="Knight and Marcu, 2000" startWordPosition="177" endWordPosition="180"> decoded, we conduct efficient joint decoding based on dual decomposition. Experimental results show that our method outperforms traditional beam search decoding and achieves the state-of-the-art performance. 1 Introduction Sentence compression is the task of generating a grammatical and shorter summary for a long sentence while preserving its most important information. One specific instantiation is deletion-based compression, namely generating a compression by dropping words. Various approaches have been proposed to challenge the task of deletion-based compression. Earlier pioneering works (Knight and Marcu, 2000) considered several insightful approaches, including noisy-channel based generative models and discriminative decision tree models. Structured discriminative compression models (McDonald, 2006) are capable of integrating rich features and have been proved effective for this task. Another powerful paradigm for sentence compression should be mentioned here is constraints-based compression,including integer linear programming solutions (Clarke and Lapata, 2008) and first-order Markov logic networks (Huang et al., 2012; Yoshikawa et al., 2012). A notable class of methods that explicitly deal with </context>
</contexts>
<marker>Knight, Marcu, 2000</marker>
<rawString>Kevin Knight and Daniel Marcu. 2000. Statisticsbased summarization-step one: Sentence compression. In AAAI/IAAI, pages 703–710.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Koo</author>
<author>Alexander M Rush</author>
<author>Michael Collins</author>
<author>Tommi Jaakkola</author>
<author>David Sontag</author>
</authors>
<title>Dual decomposition for parsing with non-projective head automata.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1288--1298</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Cambridge, MA,</location>
<contexts>
<context position="8434" citStr="Koo et al., 2010" startWordPosition="1294" endWordPosition="1297"> to reduce the time complexity down to O(SRBV ) 2. 1S, R, L and V denote respectively for the number of source tree nodes, the number of rules, size of target lexicon and number of variables involved in each rule. 2B denotes the beam width. 1829 In this work we utilize the efficiency of independent decoding from the two components respectively and then combine their solutions according to certain standards. This naturally results in a dual decomposition (Rush et al., 2010) solution. Dual decomposition has been applied in several natural language processing tasks, including dependency parsing (Koo et al., 2010), machine translation (Chang and Collins, 2011; Rush and Collins, 2011) and information extraction (Reichart and Barzilay, 2012). However, the strength of this inference strategy has seldom been noticed in researches on language generation tasks. We briefly describe the formulation here. 4.1 Description We denote the pure tree transduction part and the pure ngram part as g(y) and f(z) respectively. Then joint decoding is equivalent to solving: max g(y) + f(z) (3) y∈Y,z∈Z s.t. zkt = ykt, ∀k ∈ {1, ..., n}, ∀t ∈ {0, 1}, where y denotes a derivation which yields a final compression {y1, ..., ym}. </context>
</contexts>
<marker>Koo, Rush, Collins, Jaakkola, Sontag, 2010</marker>
<rawString>Terry Koo, Alexander M. Rush, Michael Collins, Tommi Jaakkola, and David Sontag. 2010. Dual decomposition for parsing with non-projective head automata. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1288–1298, Cambridge, MA, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan T McDonald</author>
</authors>
<title>Discriminative sentence compression with soft syntactic evidence.</title>
<date>2006</date>
<booktitle>In EACL.</booktitle>
<contexts>
<context position="1528" citStr="McDonald, 2006" startWordPosition="200" endWordPosition="201">rmance. 1 Introduction Sentence compression is the task of generating a grammatical and shorter summary for a long sentence while preserving its most important information. One specific instantiation is deletion-based compression, namely generating a compression by dropping words. Various approaches have been proposed to challenge the task of deletion-based compression. Earlier pioneering works (Knight and Marcu, 2000) considered several insightful approaches, including noisy-channel based generative models and discriminative decision tree models. Structured discriminative compression models (McDonald, 2006) are capable of integrating rich features and have been proved effective for this task. Another powerful paradigm for sentence compression should be mentioned here is constraints-based compression,including integer linear programming solutions (Clarke and Lapata, 2008) and first-order Markov logic networks (Huang et al., 2012; Yoshikawa et al., 2012). A notable class of methods that explicitly deal with syntactic structures are tree transduction models (Cohn and Lapata, 2007; Cohn and Lapata, 2009). In such models a synchronous grammar is extracted from a corpus of parallel syntax trees with l</context>
<context position="6623" citStr="McDonald (2006)" startWordPosition="1000" endWordPosition="1001">ed during training. 3.2 Pure Ngram based Compression The pure ngram based model will try to find the most locally smooth compression, reflected by having the maximum log probability score of ngrams. To avoid the trivial solution of deleting all words, we find the target compression with specified length by dynamic programming. Furthermore, we can integrate features other than log probabilities. This is equivalent to using a structured discriminative model with rich features on ngrams of candidate compressions. 3.3 Structured Discriminative Model The structured discriminative model proposed by McDonald (2006) defines rich features on bigrams of possible compressions. The score is defined as weighted linear combination of those features: f(x,z) _ � |z |w · f(x, L(zj−1), L(zj)) (2) j=2 where the function L(zk) maps a token zk in compression z back to the index of the original sentence x. Decoding can still be efficiently done by dynamic programming. With rich local structural information, the structured discriminative model can play a complementary role to the tree transduction model that focus more on global syntactic structures. 4 Joint Decoding From now on the remaining issue is jointly decoding </context>
<context position="12966" citStr="McDonald (2006)" startWordPosition="2070" endWordPosition="2071">sentences with the Stanford Parser 5 and aligned sentence pairs with minimum edit distance heuristic 6. Syntactic features of the discriminative model were also taken from these parse trees. For systems involving ngram scores, we trained a trigram language model on the Reuters Corpus (Volume 1) 7 with modified Kneser-Ney smoothing, using the widely used tool SRILM 8. 5.3 Model Training The training process of a tree transduction model followed similarly to Cohn and Lapata (2007) using structured SVMs (Tsochantaridis et al., 2005). The structured discriminative models were trained according to McDonald (2006). 5.4 Evaluation Metrics We assessed the compression results by the F1- score of grammatical relations (provided by a dependency parser) of generated compressions against the gold-standard compression (Clarke and Lapata, 2006). All systems were controlled to produce similar compression ratios (CR) for fair comparison. We also reported manual evaluation on a sampled subset of 30 sentences from each dataset. Three unpaid volunteers with self-reported fluency in English were asked to rate every candidate. Ratings are in the form of 1-5 scores for each compression. 6 Results We report test set per</context>
</contexts>
<marker>McDonald, 2006</marker>
<rawString>Ryan T McDonald. 2006. Discriminative sentence compression with soft syntactic evidence. In EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roi Reichart</author>
<author>Regina Barzilay</author>
</authors>
<title>Multi-event extraction guided by global constraints.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>70--79</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montr´eal, Canada,</location>
<contexts>
<context position="8562" citStr="Reichart and Barzilay, 2012" startWordPosition="1311" endWordPosition="1315">odes, the number of rules, size of target lexicon and number of variables involved in each rule. 2B denotes the beam width. 1829 In this work we utilize the efficiency of independent decoding from the two components respectively and then combine their solutions according to certain standards. This naturally results in a dual decomposition (Rush et al., 2010) solution. Dual decomposition has been applied in several natural language processing tasks, including dependency parsing (Koo et al., 2010), machine translation (Chang and Collins, 2011; Rush and Collins, 2011) and information extraction (Reichart and Barzilay, 2012). However, the strength of this inference strategy has seldom been noticed in researches on language generation tasks. We briefly describe the formulation here. 4.1 Description We denote the pure tree transduction part and the pure ngram part as g(y) and f(z) respectively. Then joint decoding is equivalent to solving: max g(y) + f(z) (3) y∈Y,z∈Z s.t. zkt = ykt, ∀k ∈ {1, ..., n}, ∀t ∈ {0, 1}, where y denotes a derivation which yields a final compression {y1, ..., ym}. This derivation comes from a pure tree transduction model. z denotes the compression composed of {z1, ..., zm} from an ngram com</context>
</contexts>
<marker>Reichart, Barzilay, 2012</marker>
<rawString>Roi Reichart and Regina Barzilay. 2012. Multi-event extraction guided by global constraints. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 70– 79, Montr´eal, Canada, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander M Rush</author>
<author>Michael Collins</author>
</authors>
<title>Exact decoding of syntactic translation models through lagrangian relaxation.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>72--82</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="8505" citStr="Rush and Collins, 2011" startWordPosition="1304" endWordPosition="1307"> denote respectively for the number of source tree nodes, the number of rules, size of target lexicon and number of variables involved in each rule. 2B denotes the beam width. 1829 In this work we utilize the efficiency of independent decoding from the two components respectively and then combine their solutions according to certain standards. This naturally results in a dual decomposition (Rush et al., 2010) solution. Dual decomposition has been applied in several natural language processing tasks, including dependency parsing (Koo et al., 2010), machine translation (Chang and Collins, 2011; Rush and Collins, 2011) and information extraction (Reichart and Barzilay, 2012). However, the strength of this inference strategy has seldom been noticed in researches on language generation tasks. We briefly describe the formulation here. 4.1 Description We denote the pure tree transduction part and the pure ngram part as g(y) and f(z) respectively. Then joint decoding is equivalent to solving: max g(y) + f(z) (3) y∈Y,z∈Z s.t. zkt = ykt, ∀k ∈ {1, ..., n}, ∀t ∈ {0, 1}, where y denotes a derivation which yields a final compression {y1, ..., ym}. This derivation comes from a pure tree transduction model. z denotes th</context>
</contexts>
<marker>Rush, Collins, 2011</marker>
<rawString>Alexander M. Rush and Michael Collins. 2011. Exact decoding of syntactic translation models through lagrangian relaxation. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 72–82, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander M Rush</author>
<author>Michael Collins</author>
</authors>
<title>A tutorial on dual decomposition and lagrangian relaxation for inference in natural language processing.</title>
<date>2012</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>45--305</pages>
<contexts>
<context position="10833" citStr="Rush and Collins (2012)" startWordPosition="1728" endWordPosition="1731">y(i) kt = z(i) kt ∀k ∀t then 6: return (y(i), z(i)) 7: else 8: u(i) , u(i−1) — 6(zit − y(t ) kt kt i 9: end if 10: end for tions on the values of the original dynamic programming chart. Joint decoding of a pure tree transduction model and a structured discriminative model is almost the same. The asymptotic time complexity of Algorithm 1 is O(k(5RV + L2(n−1))), where k denotes the number of iterations. This is a significant reduction of O(5RL2(n−1)V ) by directly solving the original problem and is also comparable to O(5RBV) of conducting beam search decoding. We apply a similar heuristic with Rush and Collins (2012) to set the step size Ji = 1 t+1, where t &lt; i is the number of past iterations that increase the dual value. This setting decreases the step size only when the dual value moves towards the wrong direction. We limit the maximum iteration number to 50 and return the best primal solution y(i) among all previous iterations for cases that do not converge in reasonable time. 5 Experiments 5.1 Baselines The pure tree transduction model and the discriminative model naturally become part of our baselines for comparison 3. Besides comparing our methods against the tree-transduction model with ngram scor</context>
</contexts>
<marker>Rush, Collins, 2012</marker>
<rawString>Alexander M Rush and Michael Collins. 2012. A tutorial on dual decomposition and lagrangian relaxation for inference in natural language processing. Journal of Artificial Intelligence Research, 45:305–362.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander M Rush</author>
<author>David Sontag</author>
<author>Michael Collins</author>
<author>Tommi Jaakkola</author>
</authors>
<title>On dual decomposition and linear programming relaxations for natural language processing.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1--11</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Cambridge, MA,</location>
<contexts>
<context position="8294" citStr="Rush et al., 2010" startWordPosition="1274" endWordPosition="1277">or building the chart (Cohn and Lapata, 2009). Cohn and Lapata (2009) proposed a beam search approximation along with cube-pruning heuristics to reduce the time complexity down to O(SRBV ) 2. 1S, R, L and V denote respectively for the number of source tree nodes, the number of rules, size of target lexicon and number of variables involved in each rule. 2B denotes the beam width. 1829 In this work we utilize the efficiency of independent decoding from the two components respectively and then combine their solutions according to certain standards. This naturally results in a dual decomposition (Rush et al., 2010) solution. Dual decomposition has been applied in several natural language processing tasks, including dependency parsing (Koo et al., 2010), machine translation (Chang and Collins, 2011; Rush and Collins, 2011) and information extraction (Reichart and Barzilay, 2012). However, the strength of this inference strategy has seldom been noticed in researches on language generation tasks. We briefly describe the formulation here. 4.1 Description We denote the pure tree transduction part and the pure ngram part as g(y) and f(z) respectively. Then joint decoding is equivalent to solving: max g(y) + f</context>
</contexts>
<marker>Rush, Sontag, Collins, Jaakkola, 2010</marker>
<rawString>Alexander M Rush, David Sontag, Michael Collins, and Tommi Jaakkola. 2010. On dual decomposition and linear programming relaxations for natural language processing. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1–11, Cambridge, MA, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ioannis Tsochantaridis</author>
<author>Thorsten Joachims</author>
<author>Thomas Hofmann</author>
<author>Yasemin Altun</author>
</authors>
<title>Large margin methods for structured and interdependent output variables.</title>
<date>2005</date>
<journal>In Journal of Machine Learning Research,</journal>
<pages>1453--1484</pages>
<contexts>
<context position="12886" citStr="Tsochantaridis et al., 2005" startWordPosition="2058" endWordPosition="2061">All tree transduction models require parallel parse trees with aligned leaves. We parsed all sentences with the Stanford Parser 5 and aligned sentence pairs with minimum edit distance heuristic 6. Syntactic features of the discriminative model were also taken from these parse trees. For systems involving ngram scores, we trained a trigram language model on the Reuters Corpus (Volume 1) 7 with modified Kneser-Ney smoothing, using the widely used tool SRILM 8. 5.3 Model Training The training process of a tree transduction model followed similarly to Cohn and Lapata (2007) using structured SVMs (Tsochantaridis et al., 2005). The structured discriminative models were trained according to McDonald (2006). 5.4 Evaluation Metrics We assessed the compression results by the F1- score of grammatical relations (provided by a dependency parser) of generated compressions against the gold-standard compression (Clarke and Lapata, 2006). All systems were controlled to produce similar compression ratios (CR) for fair comparison. We also reported manual evaluation on a sampled subset of 30 sentences from each dataset. Three unpaid volunteers with self-reported fluency in English were asked to rate every candidate. Ratings are </context>
</contexts>
<marker>Tsochantaridis, Joachims, Hofmann, Altun, 2005</marker>
<rawString>Ioannis Tsochantaridis, Thorsten Joachims, Thomas Hofmann, and Yasemin Altun. 2005. Large margin methods for structured and interdependent output variables. In Journal of Machine Learning Research, pages 1453–1484.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katsumasa Yoshikawa</author>
<author>Tsutomu Hirao</author>
<author>Ryu Iida</author>
<author>Manabu Okumura</author>
</authors>
<title>Sentence compression with semantic role constraints.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers-Volume 2,</booktitle>
<pages>349--353</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1880" citStr="Yoshikawa et al., 2012" startWordPosition="250" endWordPosition="253"> deletion-based compression. Earlier pioneering works (Knight and Marcu, 2000) considered several insightful approaches, including noisy-channel based generative models and discriminative decision tree models. Structured discriminative compression models (McDonald, 2006) are capable of integrating rich features and have been proved effective for this task. Another powerful paradigm for sentence compression should be mentioned here is constraints-based compression,including integer linear programming solutions (Clarke and Lapata, 2008) and first-order Markov logic networks (Huang et al., 2012; Yoshikawa et al., 2012). A notable class of methods that explicitly deal with syntactic structures are tree transduction models (Cohn and Lapata, 2007; Cohn and Lapata, 2009). In such models a synchronous grammar is extracted from a corpus of parallel syntax trees with leaves aligned. Compressions are generated from the grammar with learned weights. Previous works have noticed that local coherence is usually needed by introducing ngram language model scores, which will make accurate decoding intractable. Traditional approaches conduct beam search to find approximate solutions (Cohn and Lapata, 2009). In this paper w</context>
</contexts>
<marker>Yoshikawa, Hirao, Iida, Okumura, 2012</marker>
<rawString>Katsumasa Yoshikawa, Tsutomu Hirao, Ryu Iida, and Manabu Okumura. 2012. Sentence compression with semantic role constraints. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers-Volume 2, pages 349–353. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
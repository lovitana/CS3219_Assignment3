<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000009">
<title confidence="0.999026">
Low-dimensional Embeddings for Interpretable
Anchor-based Topic Inference
</title>
<author confidence="0.991322">
Moontae Lee
</author>
<affiliation confidence="0.996123">
Dept. of Computer Science
Cornell University
</affiliation>
<address confidence="0.817774">
Ithaca, NY, 14853
</address>
<email confidence="0.999038">
moontae@cs.cornell.edu
</email>
<author confidence="0.997677">
David Mimno
</author>
<affiliation confidence="0.9961745">
Dept. of Information Science
Cornell University
</affiliation>
<address confidence="0.817725">
Ithaca, NY, 14853
</address>
<email confidence="0.999055">
mimno@cornell.edu
</email>
<sectionHeader confidence="0.9939" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999971">
The anchor words algorithm performs
provably efficient topic model inference
by finding an approximate convex hull
in a high-dimensional word co-occurrence
space. However, the existing greedy al-
gorithm often selects poor anchor words,
reducing topic quality and interpretability.
Rather than finding an approximate con-
vex hull in a high-dimensional space, we
propose to find an exact convex hull in
a visualizable 2- or 3-dimensional space.
Such low-dimensional embeddings both
improve topics and clearly show users why
the algorithm selects certain words.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.98013512195122">
Statistical topic modeling is useful in exploratory
data analysis (Blei et al., 2003), but model infer-
ence is known to be NP-hard even for the sim-
plest models with only two topics (Sontag and
Roy, 2011), and training often remains a black
box to users. Likelihood-based training requires
expensive approximate inference such as varia-
tional methods (Blei et al., 2003), which are deter-
ministic but sensitive to initialization, or Markov
chain Monte Carlo (MCMC) methods (Griffiths
and Steyvers, 2004), which have no finite conver-
gence guarantees. Recently Arora et al. proposed
the Anchor Words algorithm (Arora et al., 2013),
which casts topic inference as statistical recovery
using a separability assumption: each topic has
a specific anchor word that appears only in the
context of that single topic. Each anchor word
can be used as a unique pivot to disambiguate the
corresponding topic distribution. We then recon-
struct the word co-occurrence pattern of each non-
anchor words as a convex combination of the co-
occurrence patterns of the anchor words.
Figure 1: 2D t-SNE projection of a Yelp review
corpus and its convex hull. The words corre-
sponding to vertices are anchor words for topics,
whereas non-anchor words correspond to the inte-
rior points.
This algorithm is fast, requiring only one pass
through the training documents, and provides
provable guarantees, but results depend entirely on
selecting good anchor words. (Arora et al., 2013)
propose a greedy method that finds an approxi-
mate convex hull around a set of vectors corre-
sponding to the word co-occurrence patterns for
each vocabulary word. Although this method is
an improvement over previous work that used im-
practical linear programming methods (Arora et
al., 2012), serious problems remain. The method
greedily chooses the farthest point from the cur-
rent subspace until the given number of anchors
have been found. Particularly at the early stages
</bodyText>
<figure confidence="0.997736380952381">
good
chicken
salad
pizza
burger
popcorn
stadium
views
tire
screen
movies
told
car
called
yoga
bagels
sashimi
hotel
dog
shopping
movie
</figure>
<page confidence="0.968498">
1319
</page>
<note confidence="0.9025335">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1319–1328,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999976409090909">
of the algorithm, the words associated with the
farthest points are likely to be infrequent and id-
iosyncratic, and thus form poor bases for human
interpretation and topic recovery. This poor choice
of anchors noticeably affects topic quality: the an-
chor words algorithm tends to produce large num-
bers of nearly identical topics.
Besides providing a separability criterion, an-
chor words also have the potential to improve topic
interpretability. After learning topics for given text
collections, users often request a label that sum-
marizes each topic. Manually labeling topics is ar-
duous, and labels often do not carry over between
random initializations and models with differing
numbers of topics. Moreover, it is hard to con-
trol the subjectivity in labelings between annota-
tors, which is open to interpretive errors. There
has been considerable interest in automating the
labeling process (Mei et al., 2007; Lau et al., 2011;
Chuang et al., 2012). (Chuang et al., 2012) pro-
pose a measure of saliency: a good summary term
should be both distinctive specifically to one topic
and probable in that topic. Anchor words are by
definition optimally distinct, and therefore may
seem to be good candidates for topic labels, but
greedily selecting extreme words often results in
anchor words that have low probability.
In this work we explore the opposite of Arora et
al.’s method: rather than finding an approximate
convex hull for an exact set of vectors, we find an
exact convex hull for an approximate set of vec-
tors. We project the V x V word co-occurrence
matrix to visualizable 2- and 3-dimensional spaces
using methods such as t-SNE (van der Maaten and
Hinton, 2008), resulting in an input matrix up to
3600 times narrower than the original input for
our training corpora. Despite this radically low-
dimensional projection, the method not only finds
topics that are as good or better than the greedy
anchor method, it also finds highly salient, in-
terpretable anchor words and provides users with
a clear visual explanation for why the algorithm
chooses particular words, all while maintaining
the original algorithm’s computational benefits.
</bodyText>
<sectionHeader confidence="0.999783" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.996340607142857">
Latent Dirichlet allocation (LDA) (Blei et al.,
2003) models D documents with a vocabulary V
using a predefined number of topics by K. LDA
views both {Ak}Kk=1, a set of K topic-word distri-
butions for each topic k, and {Wd}Dd=1, a set of D
document-topic distributions for each document d,
and {zd}Dd=1, a set of topic-assignment vectors for
word tokens in the document d, as randomly gen-
erated from known stochastic processes. Merging
{Ak} as k-th column vector of V x K matrix A,
{Wd} as d-th column vector of K x D matrix W,
the learning task is to estimate the posterior dis-
tribution of latent variables A, W, and {zd} given
V x D word-document matrix ˆM, which is the
only observed variable where d-th column corre-
sponds to the empirical word frequencies in the
training documents d.
(Arora et al., 2013) recover word-topic matrix
A and topic-topic matrix R = E[W WT ] instead
of W in the spirit of nonnegative matrix factoriza-
tion. Though the true underlying word distribu-
tion for each document is unknown and could be
far from the sample observation ˆM, the empirical
word-word matrix Qˆ converges to its expectation
AE[WWT ]AT = ARAT as the number of docu-
ments increases. Thus the learning task is to ap-
proximately recover A and R pretending that the
empirical Qˆ is close to the true second-order mo-
ment matrix Q.
The critical assumption for this method is to
suppose that every topic k has a specific anchor
word sk that occurs with non-negligible probabil-
ity (&gt; 0) only in that topic. The anchor word sk
need not always appear in every document about
the topic k, but we can be confident that the doc-
ument is at least to some degree about the topic k
if it contains sk. This assumption drastically im-
proves inference by guaranteeing the presence of
a diagonal sub-matrix inside the word-topic ma-
trix A. After constructing an estimate ˆQ, the al-
gorithm in (Arora et al., 2013) first finds a set
S = {s1, ..., sK} of K anchor words (K is user-
specified), and recovers A and R subsequently
based on S. Due to this structure, overall perfor-
mance depends heavily on the quality of anchor
words.
In the matrix algebra literature this greedy
anchor finding method is called QR with row-
pivoting. Previous work classifies a matrix into
two sets of row (or column) vectors where the vec-
tors in one set can effectively reconstruct the vec-
tors in another set, called subset-selection algo-
rithms. (Gu and Eisenstat, 1996) suggest one im-
portant deterministic algorithm. A randomized al-
gorithm provided by (Boutsidis et al., 2009) is the
state-of-the art using a pre-stage that selects the
</bodyText>
<page confidence="0.981296">
1320
</page>
<bodyText confidence="0.999931578947368">
candidates in addition to (Gu and Eisenstat, 1996).
We found no change in anchor selection using
these algorithms, verifying the difficulty of the an-
chor finding process. This difficulty is mostly be-
cause anchors must be nonnegative convex bases,
whereas the classified vectors from the subset se-
lection algorithms yield unconstrained bases.
The t-SNE model has previously been used to
display high-dimensional embeddings of words in
2D space by Turian.1 Low-dimensional embed-
dings of topic spaces have also been used to sup-
port user interaction with models: (Eisenstein et
al., 2011) use a visual display of a topic embed-
ding to create a navigator interface. Although
t-SNE has been used to visualize the results of
topic models, for example by (Lacoste-Julien et
al., 2008) and (Zhu et al., 2009), we are not aware
of any use of the method as a fundamental compo-
nent of topic inference.
</bodyText>
<sectionHeader confidence="0.998515" genericHeader="method">
3 Low-dimensional Embeddings
</sectionHeader>
<bodyText confidence="0.999073827586207">
Real text corpora typically involve vocabularies in
the tens of thousands of distinct words. As the
input matrix Qˆ scales quadratically with V , the
Anchor Words algorithm must depend on a low-
dimensional projection of Qˆ in order to be practi-
cal. Previous work (Arora et al., 2013) uses ran-
dom projections via either Gaussian random ma-
trices (Johnson and Lindenstrauss, 1984) or sparse
random matrices (Achlioptas, 2001), reducing the
representation of each word to around 1,000 di-
mensions. Since the dimensionality of the com-
pressed word co-occurrence space is an order of
magnitude larger than K, we must still approxi-
mate the convex hull by choosing extreme points
as before.
In this work we explore two projection meth-
ods: PCA and t-SNE (van der Maaten and Hinton,
2008). Principle Component Analysis (PCA) is a
commonly-used dimensionality reduction scheme
that linearly transforms the data to new coordi-
nates where the largest variances are orthogonally
captured for each dimension. By choosing only a
few such principle axes, we can represent the data
in a lower dimensional space. In contrast, t-SNE
embedding performs a non-linear dimensionality
reduction preserving the local structures. Given a
set of points {xi} in a high-dimensional space X,
t-SNE allocates probability mass for each pair of
points so that pairs of similar (closer) points be-
</bodyText>
<footnote confidence="0.98346">
1http://metaoptimize.com/projects/wordreprs/
</footnote>
<figureCaption confidence="0.990111">
Figure 2: 2D PCA projections of a Yelp review
corpus and its convex hulls.
</figureCaption>
<bodyText confidence="0.6852535">
come more likely to co-occur than dissimilar (dis-
tant) points.
</bodyText>
<equation confidence="0.9942846">
exp(−d(xi, xj)2/2σ2i )
1
Ek6=i exp(−d(xi, xk)2/2σ2i )
pj|i + pi|j (symmetrized) (2)
2N
</equation>
<bodyText confidence="0.9997716">
Then it generates a set of new points {yi} in
low-dimensional space Y so that probability dis-
tribution over points in Y behaves similarly to the
distribution over points in X by minimizing KL-
divergence between two distributions:
</bodyText>
<equation confidence="0.996339857142857">
(1 + kyi − yjk2)−1
+
qij =�k6=l(1
kyk−ylk2)−1
�
min KL(P||Q) =
i6=j
</equation>
<bodyText confidence="0.999477923076923">
Instead of approximating a convex hull in such
a high-dimensional space, we select the exact
vertices of the convex hull formed in a low-
dimensional projected space, which can be calcu-
lated efficiently. Figures 1 and 2 show the con-
vex hulls for 2D projections of Qˆ using t-SNE and
PCA for a corpus of Yelp reviews. Figure 3 il-
lustrates the convex hulls for 3D t-SNE projection
for the same corpus. Anchor words correspond to
the vertices of these convex hulls. Note that we
present the 2D projections as illustrative examples
only; we find that three dimensional projections
perform better in practice.
</bodyText>
<figure confidence="0.991495238095238">
good
chicken
rice
lettuce
teriyaki
to-go
broccoli
response
shower
yoga
salon
place
great
store
love
pj|i =
pij =
pij log pij
qij
(4)
(3)
</figure>
<page confidence="0.632915">
1321
</page>
<figureCaption confidence="0.786794666666667">
Figure 3: 3D t-SNE projection of a Yelp review
corpus and its convex hull. Vertices on the convex
hull correspond to anchor words.
</figureCaption>
<bodyText confidence="0.9999686">
In addition to the computational advantages,
this approach benefits anchor-based topic model-
ing in two aspects. First, as we now compute the
exact convex hull, the number of topics depends
on the dimensionality of the embedding, v. For
example in the figures, 2D projection has 21 ver-
tices, whereas 3D projection supports 69 vertices.
This implies users can easily tune the granularity
of topic clusters by varying v = 2,3,4,... with-
out increasing the number of topics by one each
time. Second, we can effectively visualize the the-
matic relationships between topic anchors and the
rest of words in the vocabulary, enhancing both
interpretability and options for further vocabulary
curation.
</bodyText>
<sectionHeader confidence="0.994168" genericHeader="method">
4 Experimental Results
</sectionHeader>
<bodyText confidence="0.99980603030303">
We find that radically low-dimensional t-SNE pro-
jections are effective at finding anchor words that
are much more salient than the greedy method, and
topics that are more distinctive, while maintain-
ing comparable held-out likelihood and semantic
coherence. As noted in Section 1, the previous
greedy anchor words algorithm tends to produce
many nearly identical topics. For example, 37 out
of 100 topics trained on a 2008 political blog cor-
pus have obama, mccain, bush, iraq or palin as
their most probable word, including 17 just for
obama. Only 66% of topics have a unique top
word. In contrast, the t-SNE model on the same
dataset has only one topic whose most probable
word is obama, and 86% of topics have a unique
top word (mccain is the most frequent top word,
with five topics).
We use three real datasets: business reviews
from the Yelp Academic Dataset,2 political blogs
from the 2008 US presidential election (Eisen-
stein and Xing, 2010), and New York Times ar-
ticles from 2007.3 Details are shown in Table
1. Documents with fewer than 10 word tokens
are discarded due to possible instability in con-
structing ˆQ. We perform minimal vocabulary cu-
ration, eliminating a standard list of English stop-
words4 and terms that occur below frequency cut-
offs: 100 times (Yelp, Blog) and 150 times (NYT).
We further restrict possible anchor words to words
that occur in more than three documents. As our
datasets are not artificially synthesized, we reserve
5% from each set of documents for held-out like-
lihood computation.
</bodyText>
<figure confidence="0.55286425">
Name Documents Vocab. Avg. length
Yelp 20,000 1,606 40.6
Blog 13,000 4,447 161.3
NYT 41,000 10,713 277.8
</figure>
<tableCaption confidence="0.969306">
Table 1: Statistics for datasets used in experiments
</tableCaption>
<bodyText confidence="0.9998319">
Unlike (Arora et al., 2013), which presents
results on synthetic datasets to compare perfor-
mance across different recovery methods given in-
creasing numbers of documents, we are are inter-
ested in comparing anchor finding methods, and
are mainly concerned with semantic quality. As
a result, although we have conducted experiments
on synthetic document collections,5 we focus on
real datasets for this work. We also choose to com-
pare only anchor finding algorithms, so we do not
report comparisons to likelihood-based methods,
which can be found in (Arora et al., 2013).
For both PCA and t-SNE, we use three-
dimensional embeddings across all experiments.
This projection results in matrices that are 0.03%
as wide as the original V × V matrix for the
NYT dataset. Without low-dimensional embed-
ding, each word is represented by a V-dimensional
vector where only several terms are non-zero due
to the sparse co-occurrence patterns. Thus a ver-
</bodyText>
<footnote confidence="0.982730375">
2https://www.yelp.com/academic dataset
3http://catalog.ldc.upenn.edu/LDC2008T19
4We used the list of 524 stop words included in the Mallet
library.
5None of the algorithms are particularly effective at find-
ing synthetically introduced anchor words possibly because
there are other candidates around anchor vertices that approx-
imate the convex hull to almost the same degree.
</footnote>
<figure confidence="0.998722952380952">
cheesecake
ground
sashimi
bbq
rice
hotel
rooms
dog
shopping
yoga
hummus
cookie
chorizo
bagels
enchilada
dim
cupcakesdonuts
chicken
bottle
years manager specials prompt screen
music dive
shop beers
called
tires
mexi
can
hour
highly
ba staf love atmosphere
waiter
food
group
wnebeer
kids
location
glass
starbucks
peaks
shower
play
movie
hair
</figure>
<page confidence="0.985159">
1322
</page>
<bodyText confidence="0.999985444444444">
tex captured by the greedy anchor-finding method
is likely to be one of many eccentric vertices in
very high-dimensional space. In contrast, t-SNE
creates an effective dense representation where a
small number of pivotal vertices are more clearly
visible, improving both performance and inter-
pretability.
Note that since we can find an exact convex hull
in these spaces,6 there is an upper bound to the
number of topics that can be found given a partic-
ular projection. If more topics are desired, one can
simply increase the dimensionality of the projec-
tion. For the greedy algorithm we use sparse ran-
dom projections with 1,000 dimensions with 5%
negative entries and 5% positive entries. PCA and
t-SNE choose (49, 32, 47) and (69, 77, 107) an-
chors, respectively for each of three Yelp, Blog,
and NYTimes datasets.
</bodyText>
<subsectionHeader confidence="0.972744">
4.1 Anchor-word Selection
</subsectionHeader>
<bodyText confidence="0.999246083333333">
We begin by comparing the behavior of low-
dimensional embeddings to the behavior of the
standard greedy algorithm. Table 2 shows ordered
lists of the first 12 anchor words selected by three
algorithms: t-SNE embedding, PCA embedding,
and the original greedy algorithm. Anchor words
selected by t-SNE (police, business, court) are
more general than anchor words selected by the
greedy algorithm (cavalry, al-sadr, yiddish). Ad-
ditional examples of anchor words and their asso-
ciated topics are shown in Table 3 and discussed
in Section 4.2.
</bodyText>
<table confidence="0.999894846153846">
# t-SNE PCA Greedy
1 police beloved cavalry
2 bonds york biodiesel
3 business family h/w
4 day loving kingsley
5 initial late mourners
6 million president pcl
7 article people carlisle
8 wife article al-sadr
9 site funeral kaye
10 mother million abc’s
11 court board yiddish
12 percent percent great-grandmother
</table>
<tableCaption confidence="0.890735">
Table 2: The first 12 anchor words selected by
three algorithms for the NYT corpus.
</tableCaption>
<bodyText confidence="0.772925">
The Gram-Schimdt process used by Arora et
al. greedily selects anchors in high-dimensional
space. As each word is represented within V -
</bodyText>
<footnote confidence="0.910811">
6In order to efficiently find an exact convex hull, we use
the Quickhull algorithm.
</footnote>
<table confidence="0.999208871794872">
Type # HR Top Words (Yelp)
t-SNE 16 0 mexican good service great eat restaurant authentic delicious
PCA 15 0 mexican authentic eat chinese don’t restaurant fast salsa
Greedy 34 35 good great food place service restaurant it’s mexican
t-SNE 6 0 beer selection good pizza great wings tap nice
PCA 39 6 wine beer selection nice list glass wines bar
Greedy 99 11 beer selection great happy place wine good bar
t-SNE 3 0 prices great good service selection price nice quality
PCA 12 0 atmosphere prices drinks friendly selection nice beer ambiance
Greedy 34 35 good great food place service restaurant it’s mexican
t-SNE 10 0 chicken salad good lunch sauce ordered fried soup
PCA 10 0 chicken salad lunch fried pita time back sauce
Greedy 69 12 chicken rice sauce fried ordered i’m spicy soup
Type # HR Top Words (Blog)
t-SNE 10 0 hillary clinton campaign democratic bill party win race
PCA 4 0 hillary clinton campaign democratic party bill democrats vote
Greedy 45 19 obama hillary campaign clinton obama’s barack it’s democratic
t-SNE 3 0 iraq war troops iraqi mccain surge security american
PCA 9 1 iraq iraqi war troops military forces security american
Greedy 91 8 iraq mccain war bush troops withdrawal obama iraqi
t-SNE 9 0 allah muhammad qur verses unbelievers ibn muslims world
PCA 18 14 allah muhammad qur verses unbelievers story time update
Greedy 4 5 allah muhammad people qur verses unbelievers ibn sura
t-SNE 19 0 catholic abortion catholics life hagee time biden human
PCA 2 0 people it’s time don’t good make years palin
Greedy 40 1 abortion parenthood planned people time state life government
Type # HR Top Words (NYT)
t-SNE 0 0 police man yesterday officers shot officer year-old charged
PCA 6 0 people it’s police way those three back don’t
Greedy 50 198 police man yesterday officers officer people street city
t-SNE 19 0 senator republican senate democratic democrat state bill
PCA 33 2 state republican republicans senate senator house bill party
Greedy 85 33 senator republican president state campaign presidential people
t-SNE 2 0 business chief companies executive group yesterday billion
PCA 21 0 billion companies business deal group chief states united
Greedy 55 10 radio business companies percent day music article satellite
t-SNE 14 0 market sales stock companies prices billion investors price
PCA 11 0 percent market rate week state those increase high
Greedy 77 44 companies percent billion million group business chrysler people
</table>
<tableCaption confidence="0.67440975">
Table 3: Example t-SNE topics and their most
similar topics across algorithms. The Greedy algo-
rithm can find similar topics, but the anchor words
are much less salient.
</tableCaption>
<bodyText confidence="0.999951333333334">
dimensions, finding the word that has the next
most distinctive co-occurrence pattern tends to
prefer overly eccentric words with only short, in-
tense bursts of co-occurring words. While the
bases corresponding to these anchor words could
be theoretically relevant for the original space in
high-dimension, they are less likely to be equally
important in low-dimensional space. Thus project-
ing down to low-dimensional space can rearrange
the points emphasizing not only uniqueness, but
also longevity, achieving the ability to form mea-
surably more specific topics.
Concretely, neither cavalry, al-sadr, yiddish nor
police, business, court are full representations of
New York Times articles, but the latter is a much
better basis than the former due to its greater gen-
erality. We see the effect of this difference in the
specificity of the resulting topics (for example in
17 obama topics). Most words in the vocabulary
have little connection to the word cavalry, so the
probability p(zlw) does not change much across
different w. When we convert these distributions
into P(wlz) using the Bayes’ rule, the resulting
topics are very close to the corpus distribution, a
</bodyText>
<equation confidence="0.7354178">
1323
NormalizedEntropy
unigram distribution p(w).
p(w|z = kcavalry) ∝ p(z = kcavalry|w)p(w)
≈ p(w)
</equation>
<bodyText confidence="0.9903765">
This lack of specificity results in the observed sim-
ilarity of topics.
</bodyText>
<subsectionHeader confidence="0.995719">
4.2 Quantitative Results
</subsectionHeader>
<bodyText confidence="0.999869727272727">
In this section we compare PCA and t-SNE pro-
jections to the greedy algorithm along several
quantitative metrics. To show the effect of dif-
ferent values of K, we report results for varying
numbers of topics. As the anchor finding algo-
rithms are deterministic, the anchor words in a K-
dimensional model are identical to the first K an-
chor words in a (K + 1)-dimensional model. For
the greedy algorithm we select anchor words in
the order they are chosen. For the PCA and t-
SNE methods, which find anchors jointly, we sort
words in descending order by their distance from
their centroid.
Recovery Error. Each non-anchor word is ap-
proximated by a convex combination of the K
anchor words. The projected gradient algorithm
(Arora et al., 2013) determines these convex coef-
ficients so that the gap between the original word
vector and the approximation becomes minimized.
As choosing a good basis of K anchor words de-
creases this gap, Recovery Error (RE) is defined
by the average `2-residuals across all words.
</bodyText>
<equation confidence="0.626973">
p(z1 = k|w1 = i) QSkk2
</equation>
<figure confidence="0.947360727272727">
(5)
Recovery error decreases with the number of top-
Yelp Blog NYTimes
0.05
Algorithm
Greedy
PCA
tSNE
0.00
0 30 60 90 0 30 60 90 0 30 60 90
Topics
</figure>
<figureCaption confidence="0.9699545">
Figure 4: Recovery error is similar across algo-
rithms.
</figureCaption>
<bodyText confidence="0.999025666666667">
ics, and improves substantially after the first 10–15
anchor words for all methods. The t-SNE method
has slightly better performance than the greedy al-
gorithm, but they are similar. Results for recovery
with the original, unprojected matrix (not shown)
are much worse than the other algorithms, sug-
gesting that the initial anchor words chosen are es-
pecially likely to be uninformative.
Normalized Entropy. As shown previously, if
the probability of topics given a word is close to
uniform, the probability of that word in topics will
be close to the corpus distribution. Normalized
Entropy (NE) measures the entropy of this distri-
bution relative to the entropy of a K-dimensional
uniform distribution:
</bodyText>
<equation confidence="0.964518">
H(z|w = i) . (6)
log K
</equation>
<bodyText confidence="0.919856">
The normalized entropy of topics given word dis-
</bodyText>
<figure confidence="0.993389555555555">
Yelp Blog NYTimes
1.00
Algorithm
Greedy
PCA
tSNE
0.25
0 30 60 90 0 30 60 90 0 30 60 90
Topics
</figure>
<figureCaption confidence="0.990940666666667">
Figure 5: Words have higher topic entropy in the
greedy model, especially in NYT, resulting in less
specific topics.
</figureCaption>
<bodyText confidence="0.997897058823529">
tributions usually decreases as we add more top-
ics, although both t-SNE and PCA show a dip in
entropy for low numbers of topics. This result in-
dicates that words become more closely associated
with particular topics as we increase the number of
topics. The low-dimensional embedding methods
(t-SNE and PCA) have consistently lower entropy.
Topic Specificity and Topic Dissimilarity. We
want topics to be both specific (that is, not overly
general) and different from each other. When there
are insufficient number of topics, p(w|z) often re-
sembles the corpus distribution p(w), where high
frequency terms become the top words contribut-
ing to most topics. Topic Specificity (TS) is de-
fined by the average KL divergence from each
topic’s conditional distribution to the corpus dis-
tribution.7
</bodyText>
<equation confidence="0.972139">
KL(p(w|z = k)  ||p(w)) (7)
</equation>
<footnote confidence="0.972764">
7We prefer specificity to (AlSumait et al., 2009)’s term
vacuousness because the metric increases as we move away
from the corpus distribution.
</footnote>
<figure confidence="0.982360888888889">
1 V k QZ − K
RE = V Z=1 k=1
0.04
Recovery
0.03
0.02
0.01
1
NE =
V
�V
Z=1
0.75
0.50
1
TS = K
K
k=1
</figure>
<page confidence="0.991572">
1324
</page>
<bodyText confidence="0.934268538461538">
One way to define the distance between multiple
points is the minimum radius of a ball that cov-
ers every point. Whereas this is simply the dis-
tance form the centroid to the farthest point in
the Euclidean space, it is an itself difficult opti-
mization problem to find such centroid of distri-
butions under metrics such as KL-divergence and
Jensen-Shannon divergence. To avoid this prob-
lem, we measure Topic Dissimilarity (TD) view-
ing each conditional distribution p(w|z) as a sim-
ple V-dimensional vector in RV . Recall aik =
p(w = i|z = k),
probable words (i.e., top words) for the topic k.
</bodyText>
<equation confidence="0.98002">
� D(w1, w2) + c
TC = log (9)
w1�=w2EW(T) D(w1)
k
</equation>
<bodyText confidence="0.998674142857143">
Here D(w1, w2) is the co-document frequency,
which is the number of documents in D consisting
of two words w1 and w2 simultaneously. D(w)
is the simple document frequency with the word
w. The numerator contains smoothing count c
in order to avoid taking the logarithm of zero.
Coherence scores for t-SNE and PCA are worse
</bodyText>
<equation confidence="0.663959">
a*k, − a*kk2. (8)
1 x
TD = max k
1&lt;k&lt;x K
k&apos;=1
</equation>
<figure confidence="0.996302657894737">
Yelp
Blog
NYTimes
Specificity and dissimilarity increase with the
Coherence
−400
−450
−500
−550
−600
Algorithm
Greedy
PCA
tSNE
0 30 60 90 0 30 60 90 0 30 60 90
Topics
Yelp Blog NYTimes
2.0
Specificity
1.5
1.0
0.5
0.0
Algorithm
Greedy
PCA
tSNE
0 30 60 90 0 30 60 90 0 30 60 90
Topics
Yelp Blog NYTimes
0.6
Algorithm
Greedy
PCA
tSNE
0.0
0 30 60 90 0 30 60 90 0 30 60 90
Topics
</figure>
<figureCaption confidence="0.8742605">
Figure 6: Greedy topics look more like the corpus
distribution and more like each other.
</figureCaption>
<bodyText confidence="0.966694261904762">
number of topics, suggesting that with few anchor
words, the topic distributions are close to the over-
all corpus distribution and very similar to one an-
other. The t-SNE and PCA algorithms produce
consistently better specificity and dissimilarity, in-
dicating that they produce more useful topics early
with small numbers of topics. The greedy algo-
rithm produces topics that are closer to the corpus
distribution and less distinct from each other (17
obama topics).
Topic Coherence is known to correlate with the
semantic quality of topic judged by human anno-
tators (Mimno et al., 2011). Let W(kT) be T most
Figure 7: The greedy algorithm creates more co-
herent topics (higher is better), but at the cost of
many overly general or repetitive topics.
than those for the greedy method, but this result
must be understood in combination with the Speci-
ficity and Dissimilarity metrics. The most frequent
terms in the overall corpus distribution p(w) often
appear together in documents. Thus a model creat-
ing many topics similar to the corpus distribution
is likely to achieve high Coherence, but low Speci-
ficity by definition.
Saliency. (Chuang et al., 2012) define saliency
for topic words as a combination of distinctive-
ness and probability within a topic. Anchor words
are distinctive by construction, so we can increase
saliency by selecting more probable anchor words.
We measure the probability of anchor words in
two ways. First, we report the zero-based rank of
anchor words within their topics. Examples of this
metric, which we call “hard” rank are shown in Ta-
ble 3. The hard rank of the anchors in the PCA and
t-SNE models are close to zero, while the anchor
words for the greedy algorithm are much lower
ranked, well below the range usually displayed to
users. Second, while hard rank measures the per-
ceived difference in rank of contributing words,
position may not fully capture the relative impor-
tance of the anchor word. “Soft” rank quantifies
the average log ratio between probabilities of the
</bodyText>
<figure confidence="0.989762428571429">
Dissimilarity
0.4
0.2
1325
SoftAnchorRank
prominent word w∗k and the anchor word sk.
4
Algorithm
Greedy
PCA
tSNE
0
0 30 60 90 0 30 60 90 0 30 60 90
Topics
</figure>
<figureCaption confidence="0.993694">
Figure 8: Anchor words have higher probability,
and therefore greater salience, in t-SNE and PCA
models (1 ≈ one third the probability of the top
ranked word).
</figureCaption>
<bodyText confidence="0.9996259375">
Lower values of soft rank (Fig. 8 indicate that
the anchor word has greater relative probability to
occur within a topic. As we increase the num-
ber of topics, anchor words become more promi-
nent in topics learned by the greedy method, but
t-SNE anchor words remain relatively more prob-
able within their topics as measured by soft rank.
Held-out Probability. Given an estimate of
the topic-word matrix A, we can compute the
marginal probability of held-out documents under
that model. We use the left-to-right estimator in-
troduced by (Wallach et al., 2009), which uses a
sequential algorithm similar to a Gibbs sampler.
This method requires a smoothing parameter for
document-topic Dirichlet distributions, which we
set to αk = 0.1. We note that the greedy algo-
</bodyText>
<figure confidence="0.9750509">
Yelp Blog NYTimes
−8.1
Algorithm
Greedy
PCA
tSNE
−8.4
−6.65
0 25 50 75 100 0 25 50 75 100 0 30 60 90
Topics
</figure>
<figureCaption confidence="0.9914565">
Figure 9: t-SNE topics have better held-out prob-
ability than greedy topics.
</figureCaption>
<bodyText confidence="0.999882527777778">
rithm run on the original, unprojected matrix has
better held-out probability values than t-SNE for
the Yelp corpus, but as this method does not scale
to realistic vocabularies we compare here to the
sparse random projection method used in (Arora
et al., 2013). The t-SNE method appears to do
best, particularly in the NYT corpus, which has a
larger vocabulary and longer training documents.
The length of individual held-out documents has
no correlation with held-out probability.
We emphasize that Held-out Probability is sen-
sitive to smoothing parameters and should only be
used in combination with a range of other topic-
quality metrics. In initial experiments, we ob-
served significantly worse held-out performance
for the t-SNE algorithm. This phenomenon was
because setting the probability of anchor words to
zero for all but their own topics led to large neg-
ative values in held-out log probability for those
words. As t-SNE tends to choose more frequent
terms as anchor words, these “spikes” significantly
affected overall probability estimates. To make the
calculation more fair, we added 10−5 to any zero
entries for anchor words in the topic-word matrix
A across all models and renormalized.
Because t-SNE is a stochastic model, different
initializations can result in different embeddings.
To evaluate how steady anchor word selection is,
we ran five random initializations for each dataset.
For the Yelp dataset, the number of anchor words
varies from 59 to 69, and 43 out of those are shared
across at least four trials. For the Blog dataset, the
number of anchor words varies from 80 to 95, with
56 shared across at least four trials. For the NYT
dataset, this number varies between 83 and 107,
with 51 shared across at least four models.
</bodyText>
<subsectionHeader confidence="0.997979">
4.3 Qualitative Results
</subsectionHeader>
<bodyText confidence="0.999980882352941">
Table 3 shows topics trained by three methods (t-
SNE, PCA, and greedy) for all three datasets. For
each model, we select five topics at random from
the t-SNE model, and then find the closest topic
from each of the other models. If anchor words
present in the top eight words, they are shown in
boldface.
A fundamental difference between anchor-
based inference and traditional likelihood-based
inference is that we can give an order to top-
ics according to their contribution to word co-
occurrence convex hull. This order is intrinsic to
the original algorithm, and we heuristically give
orders to t-SNE and PCA based on their contri-
butions. This order is listed as # in the previous
table. For all but one topic, the closest topic from
the greedy model has a higher order number than
</bodyText>
<figure confidence="0.997080526315789">
1 K lo P(w = w∗k |z = k) (10)
SR = K k=1 gAw = sk|z = k)
Yelp
Blog
NYTimes
3
2
1
HeldOutLL
−6.55
−6.60
−7.45
−7.50
−7.55
−7.60
−7.65
−7.70
−8.2
−8.3
</figure>
<page confidence="0.987355">
1326
</page>
<bodyText confidence="0.999905264705882">
the associated t-SNE topic. As shown above, the
standard algorithm tends to pick less useful anchor
words at the initial stage; only the later, higher or-
dered topics are specific.
The most clear distinction between models is
the rank of anchor words represented by Hard
Rank for each topic. Only one topic correspond-
ing to (initial) has the anchor word which does
not coincide with the top-ranked word. For the
greedy algorithm, anchor words are often tens of
words down the list in rank, indicating that they
are unlikely to find a connection to the topic’s se-
mantic core. In cases where the anchor word is
highly ranked (unbelievers, parenthood) the word
is a good indicator of the topic, but still less deci-
sive.
t-SNE and PCA are often consistent in their se-
lection of anchor words, which provides useful
validation that low-dimensional embeddings dis-
cern more relevant anchor words regardless of lin-
ear vs non-linear projections. Note that we are
only varying the anchor selection part of the An-
chor Words algorithm in these experiments, recov-
ering topic-word distributions in the same manner
given anchor words. As a result, any differences
between topics with the same anchor word (for ex-
ample chicken) are due to the difference in either
the number of topics or the rest of anchor words.
Since PCA suffers from a crowding problem in
lower-dimensional projection (see Figure 2) and
the problem could be severe in a dataset with a
large vocabulary, t-SNE is more likely to find the
proper number of anchors given a specified granu-
larity.
</bodyText>
<sectionHeader confidence="0.998953" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999985045454546">
One of the main advantages of the anchor words
algorithm is that the running time is largely inde-
pendent of corpus size. Adding more documents
would not affect the size of the co-occurrence ma-
trix, requiring more times to construct the co-
occurrence matrix at the beginning. While the
inference is scalable depending only on the size
of the vocabulary, finding quality anchor words is
crucial for the performance of the inference.
(Arora et al., 2013) presents a greedy anchor
finding algorithm that improves over previous lin-
ear programming methods, but finding quality an-
chor words remains an open problem in spec-
tral topic inference. We have shown that previ-
ous approaches have several limitations. Exhaus-
tively finding anchor words by eliminating words
that are reproducible by other words (Arora et
al., 2012) is impractical. The anchor words se-
lected by the greedy algorithm are overly eccen-
tric, particularly at the early stages of the algo-
rithm, causing topics to be poorly differentiated.
We find that using low-dimensional embeddings
of word co-occurrence statistics allows us to ap-
proximate a better convex hull. The resulting
anchor words are highly salient, being both dis-
tinctive and probable. The models trained with
these words have better quantitative and qualita-
tive properties along various metrics. Most im-
portantly, using radically low-dimensional projec-
tions allows us to provide users with clear visual
explanations for the model’s anchor word selec-
tions.
An interesting property of using low-
dimensional embeddings is that the number
of topics depends only on the projecting dimen-
sion. Since we can efficiently find an exact convex
hull in low-dimensional space, users can achieve
topics with their preferred level of granularities
by changing the projection dimension. We do
not insist this is the “correct” number of topics
for a corpus, but this method, along with the
range of metrics described in this paper, provides
users with additional perspective when choosing a
dimensionality that is appropriate for their needs.
We find that the t-SNE method, besides its
well-known ability to produce high quality lay-
outs, provides the best overall anchor selection
performance. This method consistently selects
higher-frequency terms as anchor words, resulting
in greater clarity and interpretability. Embeddings
with PCA are also effective, but they result in less
well-formed spaces, being less effective in held-
out probability for sufficiently large corpora.
Anchor word finding methods based on low-
dimensional projections offer several important
advantages for topic model users. In addition to
producing more salient anchor words that can be
used effectively as topic labels, the relationship of
anchor words to a visualizable word co-occurrence
space offers significant potential. Users who can
see why the algorithm chose a particular model
will have greater confidence in the model and in
any findings that result from topic-based analy-
sis. Finally, visualizable spaces offer the poten-
tial to produce interactive environments for semi-
supervised topic reconstruction.
</bodyText>
<page confidence="0.992271">
1327
</page>
<sectionHeader confidence="0.998804" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.996897">
We thank David Bindel and the anonymous re-
viewers for their valuable comments and sugges-
tions, and Laurens van der Maaten for providing
his t-SNE implementation.
</bodyText>
<sectionHeader confidence="0.998952" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999759453125">
Dimitris Achlioptas. 2001. Database-friendly random
projections. In SIGMOD, pages 274–281.
Loulwah AlSumait, Daniel Barbar, James Gentle, and
Carlotta Domeniconi. 2009. Topic significance
ranking of lda generative models. In ECML.
S. Arora, R. Ge, and A. Moitra. 2012. Learning topic
models – going beyond svd. In FOCS.
Sanjeev Arora, Rong Ge, Yonatan Halpern, David
Mimno, Ankur Moitra, David Sontag, Yichen Wu,
and Michael Zhu. 2013. A practical algorithm for
topic modeling with provable guarantees. In ICML.
D. Blei, A. Ng, and M. Jordan. 2003. Latent dirichlet
allocation. Journal of Machine Learning Research,
pages 993–1022. Preliminary version in NIPS 2001.
Christos Boutsidis, Michael W. Mahoney, and Petros
Drineas. 2009. An improved approximation algo-
rithm for the column subset selection problem. In
SODA, pages 968–977.
Jason Chuang, Christopher D. Manning, and Jeffrey
Heer. 2012. Termite: Visualization techniques
for assessing textual topic models. In International
Working Conference on Advanced Visual Interfaces
(AVI), pages 74–77.
Jacob Eisenstein and Eric Xing. 2010. The CMU
2008 political blog corpus. Technical report, CMU,
March.
Jacob Eisenstein, Duen Horng Chau, Aniket Kittur, and
Eric P. Xing. 2011. Topicviz: Semantic navigation
of document collections. CoRR, abs/1110.6200.
T. L. Griffiths and M. Steyvers. 2004. Finding scien-
tific topics. Proceedings of the National Academy of
Sciences, 101:5228–5235.
Ming Gu and Stanley C. Eisenstat. 1996. Efficient
algorithms for computing a strong rank-revealing qr
factorization. In SIAM J. Sci Comput, pages 848–
869.
William B. Johnson and Joram Lindenstrauss. 1984.
Extensions of lipschitz mappings into a hilbert
space. Contemporary Mathematics, 26:189–206.
Simon Lacoste-Julien, Fei Sha, and Michael I. Jordan.
2008. DiscLDA: Discriminative learning for dimen-
sionality reduction and classification. In NIPS.
Jey Han Lau, Karl Grieser, David Newman, and Tim-
othy Baldwin. 2011. Automatic labelling of topic
models. In HLT, pages 1536–1545.
Qiaozhu Mei, Xuehua Shen, and ChengXiang Zhai.
2007. Automatic labeling of multinomial topic
models. In KDD, pages 490–499.
David Mimno, Hanna Wallach, Edmund Talley,
Miriam Leenders, and Andrew McCallum. 2011.
Optimizing semantic coherence in topic models. In
EMNLP.
D. Sontag and D. Roy. 2011. Complexity of inference
in latent dirichlet allocation. In NIPS, pages 1008–
1016.
L.J.P. van der Maaten and G.E. Hinton. 2008. Visu-
alizing high-dimensional data using t-SNE. JMLR,
9:2579–2605, Nov.
Hanna Wallach, Iain Murray, Ruslan Salakhutdinov,
and David Mimno. 2009. Evaluation methods for
topic models. In ICML.
Jun Zhu, Amr Ahmed, and Eric P. Xing. 2009.
MedLDA: Maximum margin supervised topic mod-
els for regression and classication. In ICML.
</reference>
<page confidence="0.993814">
1328
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.270825">
<title confidence="0.979463">Low-dimensional Embeddings for Anchor-based Topic Inference</title>
<author confidence="0.794685">Moontae</author>
<affiliation confidence="0.882337333333333">Dept. of Computer Cornell Ithaca, NY,</affiliation>
<email confidence="0.999225">moontae@cs.cornell.edu</email>
<author confidence="0.815943">David</author>
<affiliation confidence="0.870766666666667">Dept. of Information Cornell Ithaca, NY,</affiliation>
<email confidence="0.999696">mimno@cornell.edu</email>
<abstract confidence="0.998581466666667">The anchor words algorithm performs provably efficient topic model inference by finding an approximate convex hull in a high-dimensional word co-occurrence space. However, the existing greedy algorithm often selects poor anchor words, reducing topic quality and interpretability. Rather than finding an approximate convex hull in a high-dimensional space, we propose to find an exact convex hull in a visualizable 2or 3-dimensional space. Such low-dimensional embeddings both improve topics and clearly show users why the algorithm selects certain words.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Dimitris Achlioptas</author>
</authors>
<title>Database-friendly random projections.</title>
<date>2001</date>
<booktitle>In SIGMOD,</booktitle>
<pages>274--281</pages>
<contexts>
<context position="9190" citStr="Achlioptas, 2001" startWordPosition="1495" endWordPosition="1496"> models, for example by (Lacoste-Julien et al., 2008) and (Zhu et al., 2009), we are not aware of any use of the method as a fundamental component of topic inference. 3 Low-dimensional Embeddings Real text corpora typically involve vocabularies in the tens of thousands of distinct words. As the input matrix Qˆ scales quadratically with V , the Anchor Words algorithm must depend on a lowdimensional projection of Qˆ in order to be practical. Previous work (Arora et al., 2013) uses random projections via either Gaussian random matrices (Johnson and Lindenstrauss, 1984) or sparse random matrices (Achlioptas, 2001), reducing the representation of each word to around 1,000 dimensions. Since the dimensionality of the compressed word co-occurrence space is an order of magnitude larger than K, we must still approximate the convex hull by choosing extreme points as before. In this work we explore two projection methods: PCA and t-SNE (van der Maaten and Hinton, 2008). Principle Component Analysis (PCA) is a commonly-used dimensionality reduction scheme that linearly transforms the data to new coordinates where the largest variances are orthogonally captured for each dimension. By choosing only a few such pri</context>
</contexts>
<marker>Achlioptas, 2001</marker>
<rawString>Dimitris Achlioptas. 2001. Database-friendly random projections. In SIGMOD, pages 274–281.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Loulwah AlSumait</author>
<author>Daniel Barbar</author>
<author>James Gentle</author>
<author>Carlotta Domeniconi</author>
</authors>
<title>Topic significance ranking of lda generative models.</title>
<date>2009</date>
<booktitle>In ECML.</booktitle>
<contexts>
<context position="24684" citStr="AlSumait et al., 2009" startWordPosition="4035" endWordPosition="4038">f topics. The low-dimensional embedding methods (t-SNE and PCA) have consistently lower entropy. Topic Specificity and Topic Dissimilarity. We want topics to be both specific (that is, not overly general) and different from each other. When there are insufficient number of topics, p(w|z) often resembles the corpus distribution p(w), where high frequency terms become the top words contributing to most topics. Topic Specificity (TS) is defined by the average KL divergence from each topic’s conditional distribution to the corpus distribution.7 KL(p(w|z = k) ||p(w)) (7) 7We prefer specificity to (AlSumait et al., 2009)’s term vacuousness because the metric increases as we move away from the corpus distribution. 1 V k QZ − K RE = V Z=1 k=1 0.04 Recovery 0.03 0.02 0.01 1 NE = V �V Z=1 0.75 0.50 1 TS = K K k=1 1324 One way to define the distance between multiple points is the minimum radius of a ball that covers every point. Whereas this is simply the distance form the centroid to the farthest point in the Euclidean space, it is an itself difficult optimization problem to find such centroid of distributions under metrics such as KL-divergence and Jensen-Shannon divergence. To avoid this problem, we measure Top</context>
</contexts>
<marker>AlSumait, Barbar, Gentle, Domeniconi, 2009</marker>
<rawString>Loulwah AlSumait, Daniel Barbar, James Gentle, and Carlotta Domeniconi. 2009. Topic significance ranking of lda generative models. In ECML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Arora</author>
<author>R Ge</author>
<author>A Moitra</author>
</authors>
<title>Learning topic models – going beyond svd.</title>
<date>2012</date>
<booktitle>In FOCS.</booktitle>
<contexts>
<context position="2586" citStr="Arora et al., 2012" startWordPosition="390" endWordPosition="393">nvex hull. The words corresponding to vertices are anchor words for topics, whereas non-anchor words correspond to the interior points. This algorithm is fast, requiring only one pass through the training documents, and provides provable guarantees, but results depend entirely on selecting good anchor words. (Arora et al., 2013) propose a greedy method that finds an approximate convex hull around a set of vectors corresponding to the word co-occurrence patterns for each vocabulary word. Although this method is an improvement over previous work that used impractical linear programming methods (Arora et al., 2012), serious problems remain. The method greedily chooses the farthest point from the current subspace until the given number of anchors have been found. Particularly at the early stages good chicken salad pizza burger popcorn stadium views tire screen movies told car called yoga bagels sashimi hotel dog shopping movie 1319 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1319–1328, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics of the algorithm, the words associated with the farthest points are likely to be </context>
<context position="34695" citStr="Arora et al., 2012" startWordPosition="5776" endWordPosition="5779">ix, requiring more times to construct the cooccurrence matrix at the beginning. While the inference is scalable depending only on the size of the vocabulary, finding quality anchor words is crucial for the performance of the inference. (Arora et al., 2013) presents a greedy anchor finding algorithm that improves over previous linear programming methods, but finding quality anchor words remains an open problem in spectral topic inference. We have shown that previous approaches have several limitations. Exhaustively finding anchor words by eliminating words that are reproducible by other words (Arora et al., 2012) is impractical. The anchor words selected by the greedy algorithm are overly eccentric, particularly at the early stages of the algorithm, causing topics to be poorly differentiated. We find that using low-dimensional embeddings of word co-occurrence statistics allows us to approximate a better convex hull. The resulting anchor words are highly salient, being both distinctive and probable. The models trained with these words have better quantitative and qualitative properties along various metrics. Most importantly, using radically low-dimensional projections allows us to provide users with c</context>
</contexts>
<marker>Arora, Ge, Moitra, 2012</marker>
<rawString>S. Arora, R. Ge, and A. Moitra. 2012. Learning topic models – going beyond svd. In FOCS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanjeev Arora</author>
<author>Rong Ge</author>
<author>Yonatan Halpern</author>
<author>David Mimno</author>
<author>Ankur Moitra</author>
<author>David Sontag</author>
<author>Yichen Wu</author>
<author>Michael Zhu</author>
</authors>
<title>A practical algorithm for topic modeling with provable guarantees.</title>
<date>2013</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="1473" citStr="Arora et al., 2013" startWordPosition="210" endWordPosition="213">stical topic modeling is useful in exploratory data analysis (Blei et al., 2003), but model inference is known to be NP-hard even for the simplest models with only two topics (Sontag and Roy, 2011), and training often remains a black box to users. Likelihood-based training requires expensive approximate inference such as variational methods (Blei et al., 2003), which are deterministic but sensitive to initialization, or Markov chain Monte Carlo (MCMC) methods (Griffiths and Steyvers, 2004), which have no finite convergence guarantees. Recently Arora et al. proposed the Anchor Words algorithm (Arora et al., 2013), which casts topic inference as statistical recovery using a separability assumption: each topic has a specific anchor word that appears only in the context of that single topic. Each anchor word can be used as a unique pivot to disambiguate the corresponding topic distribution. We then reconstruct the word co-occurrence pattern of each nonanchor words as a convex combination of the cooccurrence patterns of the anchor words. Figure 1: 2D t-SNE projection of a Yelp review corpus and its convex hull. The words corresponding to vertices are anchor words for topics, whereas non-anchor words corre</context>
<context position="6070" citStr="Arora et al., 2013" startWordPosition="960" endWordPosition="963">utions for each topic k, and {Wd}Dd=1, a set of D document-topic distributions for each document d, and {zd}Dd=1, a set of topic-assignment vectors for word tokens in the document d, as randomly generated from known stochastic processes. Merging {Ak} as k-th column vector of V x K matrix A, {Wd} as d-th column vector of K x D matrix W, the learning task is to estimate the posterior distribution of latent variables A, W, and {zd} given V x D word-document matrix ˆM, which is the only observed variable where d-th column corresponds to the empirical word frequencies in the training documents d. (Arora et al., 2013) recover word-topic matrix A and topic-topic matrix R = E[W WT ] instead of W in the spirit of nonnegative matrix factorization. Though the true underlying word distribution for each document is unknown and could be far from the sample observation ˆM, the empirical word-word matrix Qˆ converges to its expectation AE[WWT ]AT = ARAT as the number of documents increases. Thus the learning task is to approximately recover A and R pretending that the empirical Qˆ is close to the true second-order moment matrix Q. The critical assumption for this method is to suppose that every topic k has a specifi</context>
<context position="9051" citStr="Arora et al., 2013" startWordPosition="1473" endWordPosition="1476">11) use a visual display of a topic embedding to create a navigator interface. Although t-SNE has been used to visualize the results of topic models, for example by (Lacoste-Julien et al., 2008) and (Zhu et al., 2009), we are not aware of any use of the method as a fundamental component of topic inference. 3 Low-dimensional Embeddings Real text corpora typically involve vocabularies in the tens of thousands of distinct words. As the input matrix Qˆ scales quadratically with V , the Anchor Words algorithm must depend on a lowdimensional projection of Qˆ in order to be practical. Previous work (Arora et al., 2013) uses random projections via either Gaussian random matrices (Johnson and Lindenstrauss, 1984) or sparse random matrices (Achlioptas, 2001), reducing the representation of each word to around 1,000 dimensions. Since the dimensionality of the compressed word co-occurrence space is an order of magnitude larger than K, we must still approximate the convex hull by choosing extreme points as before. In this work we explore two projection methods: PCA and t-SNE (van der Maaten and Hinton, 2008). Principle Component Analysis (PCA) is a commonly-used dimensionality reduction scheme that linearly trans</context>
<context position="13990" citStr="Arora et al., 2013" startWordPosition="2284" endWordPosition="2287">e instability in constructing ˆQ. We perform minimal vocabulary curation, eliminating a standard list of English stopwords4 and terms that occur below frequency cutoffs: 100 times (Yelp, Blog) and 150 times (NYT). We further restrict possible anchor words to words that occur in more than three documents. As our datasets are not artificially synthesized, we reserve 5% from each set of documents for held-out likelihood computation. Name Documents Vocab. Avg. length Yelp 20,000 1,606 40.6 Blog 13,000 4,447 161.3 NYT 41,000 10,713 277.8 Table 1: Statistics for datasets used in experiments Unlike (Arora et al., 2013), which presents results on synthetic datasets to compare performance across different recovery methods given increasing numbers of documents, we are are interested in comparing anchor finding methods, and are mainly concerned with semantic quality. As a result, although we have conducted experiments on synthetic document collections,5 we focus on real datasets for this work. We also choose to compare only anchor finding algorithms, so we do not report comparisons to likelihood-based methods, which can be found in (Arora et al., 2013). For both PCA and t-SNE, we use threedimensional embeddings</context>
<context position="22345" citStr="Arora et al., 2013" startWordPosition="3639" endWordPosition="3642">how the effect of different values of K, we report results for varying numbers of topics. As the anchor finding algorithms are deterministic, the anchor words in a Kdimensional model are identical to the first K anchor words in a (K + 1)-dimensional model. For the greedy algorithm we select anchor words in the order they are chosen. For the PCA and tSNE methods, which find anchors jointly, we sort words in descending order by their distance from their centroid. Recovery Error. Each non-anchor word is approximated by a convex combination of the K anchor words. The projected gradient algorithm (Arora et al., 2013) determines these convex coefficients so that the gap between the original word vector and the approximation becomes minimized. As choosing a good basis of K anchor words decreases this gap, Recovery Error (RE) is defined by the average `2-residuals across all words. p(z1 = k|w1 = i) QSkk2 (5) Recovery error decreases with the number of topYelp Blog NYTimes 0.05 Algorithm Greedy PCA tSNE 0.00 0 30 60 90 0 30 60 90 0 30 60 90 Topics Figure 4: Recovery error is similar across algorithms. ics, and improves substantially after the first 10–15 anchor words for all methods. The t-SNE method has slig</context>
<context position="29874" citStr="Arora et al., 2013" startWordPosition="4958" endWordPosition="4961">rithm similar to a Gibbs sampler. This method requires a smoothing parameter for document-topic Dirichlet distributions, which we set to αk = 0.1. We note that the greedy algoYelp Blog NYTimes −8.1 Algorithm Greedy PCA tSNE −8.4 −6.65 0 25 50 75 100 0 25 50 75 100 0 30 60 90 Topics Figure 9: t-SNE topics have better held-out probability than greedy topics. rithm run on the original, unprojected matrix has better held-out probability values than t-SNE for the Yelp corpus, but as this method does not scale to realistic vocabularies we compare here to the sparse random projection method used in (Arora et al., 2013). The t-SNE method appears to do best, particularly in the NYT corpus, which has a larger vocabulary and longer training documents. The length of individual held-out documents has no correlation with held-out probability. We emphasize that Held-out Probability is sensitive to smoothing parameters and should only be used in combination with a range of other topicquality metrics. In initial experiments, we observed significantly worse held-out performance for the t-SNE algorithm. This phenomenon was because setting the probability of anchor words to zero for all but their own topics led to large</context>
<context position="34332" citStr="Arora et al., 2013" startWordPosition="5719" endWordPosition="5722"> the problem could be severe in a dataset with a large vocabulary, t-SNE is more likely to find the proper number of anchors given a specified granularity. 5 Conclusion One of the main advantages of the anchor words algorithm is that the running time is largely independent of corpus size. Adding more documents would not affect the size of the co-occurrence matrix, requiring more times to construct the cooccurrence matrix at the beginning. While the inference is scalable depending only on the size of the vocabulary, finding quality anchor words is crucial for the performance of the inference. (Arora et al., 2013) presents a greedy anchor finding algorithm that improves over previous linear programming methods, but finding quality anchor words remains an open problem in spectral topic inference. We have shown that previous approaches have several limitations. Exhaustively finding anchor words by eliminating words that are reproducible by other words (Arora et al., 2012) is impractical. The anchor words selected by the greedy algorithm are overly eccentric, particularly at the early stages of the algorithm, causing topics to be poorly differentiated. We find that using low-dimensional embeddings of word</context>
</contexts>
<marker>Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu, Zhu, 2013</marker>
<rawString>Sanjeev Arora, Rong Ge, Yonatan Halpern, David Mimno, Ankur Moitra, David Sontag, Yichen Wu, and Michael Zhu. 2013. A practical algorithm for topic modeling with provable guarantees. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Blei</author>
<author>A Ng</author>
<author>M Jordan</author>
</authors>
<title>Latent dirichlet allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>993--1022</pages>
<note>Preliminary version in NIPS</note>
<contexts>
<context position="934" citStr="Blei et al., 2003" startWordPosition="124" endWordPosition="127">vably efficient topic model inference by finding an approximate convex hull in a high-dimensional word co-occurrence space. However, the existing greedy algorithm often selects poor anchor words, reducing topic quality and interpretability. Rather than finding an approximate convex hull in a high-dimensional space, we propose to find an exact convex hull in a visualizable 2- or 3-dimensional space. Such low-dimensional embeddings both improve topics and clearly show users why the algorithm selects certain words. 1 Introduction Statistical topic modeling is useful in exploratory data analysis (Blei et al., 2003), but model inference is known to be NP-hard even for the simplest models with only two topics (Sontag and Roy, 2011), and training often remains a black box to users. Likelihood-based training requires expensive approximate inference such as variational methods (Blei et al., 2003), which are deterministic but sensitive to initialization, or Markov chain Monte Carlo (MCMC) methods (Griffiths and Steyvers, 2004), which have no finite convergence guarantees. Recently Arora et al. proposed the Anchor Words algorithm (Arora et al., 2013), which casts topic inference as statistical recovery using a</context>
<context position="5315" citStr="Blei et al., 2003" startWordPosition="824" endWordPosition="827">sional spaces using methods such as t-SNE (van der Maaten and Hinton, 2008), resulting in an input matrix up to 3600 times narrower than the original input for our training corpora. Despite this radically lowdimensional projection, the method not only finds topics that are as good or better than the greedy anchor method, it also finds highly salient, interpretable anchor words and provides users with a clear visual explanation for why the algorithm chooses particular words, all while maintaining the original algorithm’s computational benefits. 2 Related Work Latent Dirichlet allocation (LDA) (Blei et al., 2003) models D documents with a vocabulary V using a predefined number of topics by K. LDA views both {Ak}Kk=1, a set of K topic-word distributions for each topic k, and {Wd}Dd=1, a set of D document-topic distributions for each document d, and {zd}Dd=1, a set of topic-assignment vectors for word tokens in the document d, as randomly generated from known stochastic processes. Merging {Ak} as k-th column vector of V x K matrix A, {Wd} as d-th column vector of K x D matrix W, the learning task is to estimate the posterior distribution of latent variables A, W, and {zd} given V x D word-document matri</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>D. Blei, A. Ng, and M. Jordan. 2003. Latent dirichlet allocation. Journal of Machine Learning Research, pages 993–1022. Preliminary version in NIPS 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christos Boutsidis</author>
<author>Michael W Mahoney</author>
<author>Petros Drineas</author>
</authors>
<title>An improved approximation algorithm for the column subset selection problem.</title>
<date>2009</date>
<booktitle>In SODA,</booktitle>
<pages>968--977</pages>
<contexts>
<context position="7786" citStr="Boutsidis et al., 2009" startWordPosition="1263" endWordPosition="1266">et S = {s1, ..., sK} of K anchor words (K is userspecified), and recovers A and R subsequently based on S. Due to this structure, overall performance depends heavily on the quality of anchor words. In the matrix algebra literature this greedy anchor finding method is called QR with rowpivoting. Previous work classifies a matrix into two sets of row (or column) vectors where the vectors in one set can effectively reconstruct the vectors in another set, called subset-selection algorithms. (Gu and Eisenstat, 1996) suggest one important deterministic algorithm. A randomized algorithm provided by (Boutsidis et al., 2009) is the state-of-the art using a pre-stage that selects the 1320 candidates in addition to (Gu and Eisenstat, 1996). We found no change in anchor selection using these algorithms, verifying the difficulty of the anchor finding process. This difficulty is mostly because anchors must be nonnegative convex bases, whereas the classified vectors from the subset selection algorithms yield unconstrained bases. The t-SNE model has previously been used to display high-dimensional embeddings of words in 2D space by Turian.1 Low-dimensional embeddings of topic spaces have also been used to support user i</context>
</contexts>
<marker>Boutsidis, Mahoney, Drineas, 2009</marker>
<rawString>Christos Boutsidis, Michael W. Mahoney, and Petros Drineas. 2009. An improved approximation algorithm for the column subset selection problem. In SODA, pages 968–977.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Chuang</author>
<author>Christopher D Manning</author>
<author>Jeffrey Heer</author>
</authors>
<title>Termite: Visualization techniques for assessing textual topic models.</title>
<date>2012</date>
<booktitle>In International Working Conference on Advanced Visual Interfaces (AVI),</booktitle>
<pages>74--77</pages>
<contexts>
<context position="4053" citStr="Chuang et al., 2012" startWordPosition="617" endWordPosition="620"> Besides providing a separability criterion, anchor words also have the potential to improve topic interpretability. After learning topics for given text collections, users often request a label that summarizes each topic. Manually labeling topics is arduous, and labels often do not carry over between random initializations and models with differing numbers of topics. Moreover, it is hard to control the subjectivity in labelings between annotators, which is open to interpretive errors. There has been considerable interest in automating the labeling process (Mei et al., 2007; Lau et al., 2011; Chuang et al., 2012). (Chuang et al., 2012) propose a measure of saliency: a good summary term should be both distinctive specifically to one topic and probable in that topic. Anchor words are by definition optimally distinct, and therefore may seem to be good candidates for topic labels, but greedily selecting extreme words often results in anchor words that have low probability. In this work we explore the opposite of Arora et al.’s method: rather than finding an approximate convex hull for an exact set of vectors, we find an exact convex hull for an approximate set of vectors. We project the V x V word co-occu</context>
<context position="27503" citStr="Chuang et al., 2012" startWordPosition="4549" endWordPosition="4552">topic judged by human annotators (Mimno et al., 2011). Let W(kT) be T most Figure 7: The greedy algorithm creates more coherent topics (higher is better), but at the cost of many overly general or repetitive topics. than those for the greedy method, but this result must be understood in combination with the Specificity and Dissimilarity metrics. The most frequent terms in the overall corpus distribution p(w) often appear together in documents. Thus a model creating many topics similar to the corpus distribution is likely to achieve high Coherence, but low Specificity by definition. Saliency. (Chuang et al., 2012) define saliency for topic words as a combination of distinctiveness and probability within a topic. Anchor words are distinctive by construction, so we can increase saliency by selecting more probable anchor words. We measure the probability of anchor words in two ways. First, we report the zero-based rank of anchor words within their topics. Examples of this metric, which we call “hard” rank are shown in Table 3. The hard rank of the anchors in the PCA and t-SNE models are close to zero, while the anchor words for the greedy algorithm are much lower ranked, well below the range usually displ</context>
</contexts>
<marker>Chuang, Manning, Heer, 2012</marker>
<rawString>Jason Chuang, Christopher D. Manning, and Jeffrey Heer. 2012. Termite: Visualization techniques for assessing textual topic models. In International Working Conference on Advanced Visual Interfaces (AVI), pages 74–77.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Eisenstein</author>
<author>Eric Xing</author>
</authors>
<title>political blog corpus.</title>
<date>2010</date>
<booktitle>The CMU</booktitle>
<tech>Technical report, CMU,</tech>
<contexts>
<context position="13230" citStr="Eisenstein and Xing, 2010" startWordPosition="2157" endWordPosition="2161">m tends to produce many nearly identical topics. For example, 37 out of 100 topics trained on a 2008 political blog corpus have obama, mccain, bush, iraq or palin as their most probable word, including 17 just for obama. Only 66% of topics have a unique top word. In contrast, the t-SNE model on the same dataset has only one topic whose most probable word is obama, and 86% of topics have a unique top word (mccain is the most frequent top word, with five topics). We use three real datasets: business reviews from the Yelp Academic Dataset,2 political blogs from the 2008 US presidential election (Eisenstein and Xing, 2010), and New York Times articles from 2007.3 Details are shown in Table 1. Documents with fewer than 10 word tokens are discarded due to possible instability in constructing ˆQ. We perform minimal vocabulary curation, eliminating a standard list of English stopwords4 and terms that occur below frequency cutoffs: 100 times (Yelp, Blog) and 150 times (NYT). We further restrict possible anchor words to words that occur in more than three documents. As our datasets are not artificially synthesized, we reserve 5% from each set of documents for held-out likelihood computation. Name Documents Vocab. Avg</context>
</contexts>
<marker>Eisenstein, Xing, 2010</marker>
<rawString>Jacob Eisenstein and Eric Xing. 2010. The CMU 2008 political blog corpus. Technical report, CMU, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Eisenstein</author>
</authors>
<title>Duen Horng Chau, Aniket Kittur, and Eric</title>
<date>2011</date>
<location>CoRR, abs/1110.6200.</location>
<marker>Eisenstein, 2011</marker>
<rawString>Jacob Eisenstein, Duen Horng Chau, Aniket Kittur, and Eric P. Xing. 2011. Topicviz: Semantic navigation of document collections. CoRR, abs/1110.6200.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T L Griffiths</author>
<author>M Steyvers</author>
</authors>
<title>Finding scientific topics.</title>
<date>2004</date>
<booktitle>Proceedings of the National Academy of Sciences,</booktitle>
<pages>101--5228</pages>
<contexts>
<context position="1348" citStr="Griffiths and Steyvers, 2004" startWordPosition="190" endWordPosition="193">uch low-dimensional embeddings both improve topics and clearly show users why the algorithm selects certain words. 1 Introduction Statistical topic modeling is useful in exploratory data analysis (Blei et al., 2003), but model inference is known to be NP-hard even for the simplest models with only two topics (Sontag and Roy, 2011), and training often remains a black box to users. Likelihood-based training requires expensive approximate inference such as variational methods (Blei et al., 2003), which are deterministic but sensitive to initialization, or Markov chain Monte Carlo (MCMC) methods (Griffiths and Steyvers, 2004), which have no finite convergence guarantees. Recently Arora et al. proposed the Anchor Words algorithm (Arora et al., 2013), which casts topic inference as statistical recovery using a separability assumption: each topic has a specific anchor word that appears only in the context of that single topic. Each anchor word can be used as a unique pivot to disambiguate the corresponding topic distribution. We then reconstruct the word co-occurrence pattern of each nonanchor words as a convex combination of the cooccurrence patterns of the anchor words. Figure 1: 2D t-SNE projection of a Yelp revie</context>
</contexts>
<marker>Griffiths, Steyvers, 2004</marker>
<rawString>T. L. Griffiths and M. Steyvers. 2004. Finding scientific topics. Proceedings of the National Academy of Sciences, 101:5228–5235.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ming Gu</author>
<author>Stanley C Eisenstat</author>
</authors>
<title>Efficient algorithms for computing a strong rank-revealing qr factorization.</title>
<date>1996</date>
<journal>In SIAM J. Sci Comput,</journal>
<pages>848--869</pages>
<contexts>
<context position="7679" citStr="Gu and Eisenstat, 1996" startWordPosition="1247" endWordPosition="1250">rd-topic matrix A. After constructing an estimate ˆQ, the algorithm in (Arora et al., 2013) first finds a set S = {s1, ..., sK} of K anchor words (K is userspecified), and recovers A and R subsequently based on S. Due to this structure, overall performance depends heavily on the quality of anchor words. In the matrix algebra literature this greedy anchor finding method is called QR with rowpivoting. Previous work classifies a matrix into two sets of row (or column) vectors where the vectors in one set can effectively reconstruct the vectors in another set, called subset-selection algorithms. (Gu and Eisenstat, 1996) suggest one important deterministic algorithm. A randomized algorithm provided by (Boutsidis et al., 2009) is the state-of-the art using a pre-stage that selects the 1320 candidates in addition to (Gu and Eisenstat, 1996). We found no change in anchor selection using these algorithms, verifying the difficulty of the anchor finding process. This difficulty is mostly because anchors must be nonnegative convex bases, whereas the classified vectors from the subset selection algorithms yield unconstrained bases. The t-SNE model has previously been used to display high-dimensional embeddings of wor</context>
</contexts>
<marker>Gu, Eisenstat, 1996</marker>
<rawString>Ming Gu and Stanley C. Eisenstat. 1996. Efficient algorithms for computing a strong rank-revealing qr factorization. In SIAM J. Sci Comput, pages 848– 869.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William B Johnson</author>
<author>Joram Lindenstrauss</author>
</authors>
<title>Extensions of lipschitz mappings into a hilbert space.</title>
<date>1984</date>
<journal>Contemporary Mathematics,</journal>
<pages>26--189</pages>
<contexts>
<context position="9145" citStr="Johnson and Lindenstrauss, 1984" startWordPosition="1487" endWordPosition="1490">though t-SNE has been used to visualize the results of topic models, for example by (Lacoste-Julien et al., 2008) and (Zhu et al., 2009), we are not aware of any use of the method as a fundamental component of topic inference. 3 Low-dimensional Embeddings Real text corpora typically involve vocabularies in the tens of thousands of distinct words. As the input matrix Qˆ scales quadratically with V , the Anchor Words algorithm must depend on a lowdimensional projection of Qˆ in order to be practical. Previous work (Arora et al., 2013) uses random projections via either Gaussian random matrices (Johnson and Lindenstrauss, 1984) or sparse random matrices (Achlioptas, 2001), reducing the representation of each word to around 1,000 dimensions. Since the dimensionality of the compressed word co-occurrence space is an order of magnitude larger than K, we must still approximate the convex hull by choosing extreme points as before. In this work we explore two projection methods: PCA and t-SNE (van der Maaten and Hinton, 2008). Principle Component Analysis (PCA) is a commonly-used dimensionality reduction scheme that linearly transforms the data to new coordinates where the largest variances are orthogonally captured for ea</context>
</contexts>
<marker>Johnson, Lindenstrauss, 1984</marker>
<rawString>William B. Johnson and Joram Lindenstrauss. 1984. Extensions of lipschitz mappings into a hilbert space. Contemporary Mathematics, 26:189–206.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simon Lacoste-Julien</author>
<author>Fei Sha</author>
<author>Michael I Jordan</author>
</authors>
<title>DiscLDA: Discriminative learning for dimensionality reduction and classification.</title>
<date>2008</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="8626" citStr="Lacoste-Julien et al., 2008" startWordPosition="1398" endWordPosition="1401">anchor finding process. This difficulty is mostly because anchors must be nonnegative convex bases, whereas the classified vectors from the subset selection algorithms yield unconstrained bases. The t-SNE model has previously been used to display high-dimensional embeddings of words in 2D space by Turian.1 Low-dimensional embeddings of topic spaces have also been used to support user interaction with models: (Eisenstein et al., 2011) use a visual display of a topic embedding to create a navigator interface. Although t-SNE has been used to visualize the results of topic models, for example by (Lacoste-Julien et al., 2008) and (Zhu et al., 2009), we are not aware of any use of the method as a fundamental component of topic inference. 3 Low-dimensional Embeddings Real text corpora typically involve vocabularies in the tens of thousands of distinct words. As the input matrix Qˆ scales quadratically with V , the Anchor Words algorithm must depend on a lowdimensional projection of Qˆ in order to be practical. Previous work (Arora et al., 2013) uses random projections via either Gaussian random matrices (Johnson and Lindenstrauss, 1984) or sparse random matrices (Achlioptas, 2001), reducing the representation of eac</context>
</contexts>
<marker>Lacoste-Julien, Sha, Jordan, 2008</marker>
<rawString>Simon Lacoste-Julien, Fei Sha, and Michael I. Jordan. 2008. DiscLDA: Discriminative learning for dimensionality reduction and classification. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jey Han Lau</author>
<author>Karl Grieser</author>
<author>David Newman</author>
<author>Timothy Baldwin</author>
</authors>
<title>Automatic labelling of topic models.</title>
<date>2011</date>
<booktitle>In HLT,</booktitle>
<pages>1536--1545</pages>
<contexts>
<context position="4031" citStr="Lau et al., 2011" startWordPosition="613" endWordPosition="616"> identical topics. Besides providing a separability criterion, anchor words also have the potential to improve topic interpretability. After learning topics for given text collections, users often request a label that summarizes each topic. Manually labeling topics is arduous, and labels often do not carry over between random initializations and models with differing numbers of topics. Moreover, it is hard to control the subjectivity in labelings between annotators, which is open to interpretive errors. There has been considerable interest in automating the labeling process (Mei et al., 2007; Lau et al., 2011; Chuang et al., 2012). (Chuang et al., 2012) propose a measure of saliency: a good summary term should be both distinctive specifically to one topic and probable in that topic. Anchor words are by definition optimally distinct, and therefore may seem to be good candidates for topic labels, but greedily selecting extreme words often results in anchor words that have low probability. In this work we explore the opposite of Arora et al.’s method: rather than finding an approximate convex hull for an exact set of vectors, we find an exact convex hull for an approximate set of vectors. We project </context>
</contexts>
<marker>Lau, Grieser, Newman, Baldwin, 2011</marker>
<rawString>Jey Han Lau, Karl Grieser, David Newman, and Timothy Baldwin. 2011. Automatic labelling of topic models. In HLT, pages 1536–1545.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qiaozhu Mei</author>
<author>Xuehua Shen</author>
<author>ChengXiang Zhai</author>
</authors>
<title>Automatic labeling of multinomial topic models.</title>
<date>2007</date>
<booktitle>In KDD,</booktitle>
<pages>490--499</pages>
<contexts>
<context position="4013" citStr="Mei et al., 2007" startWordPosition="609" endWordPosition="612"> numbers of nearly identical topics. Besides providing a separability criterion, anchor words also have the potential to improve topic interpretability. After learning topics for given text collections, users often request a label that summarizes each topic. Manually labeling topics is arduous, and labels often do not carry over between random initializations and models with differing numbers of topics. Moreover, it is hard to control the subjectivity in labelings between annotators, which is open to interpretive errors. There has been considerable interest in automating the labeling process (Mei et al., 2007; Lau et al., 2011; Chuang et al., 2012). (Chuang et al., 2012) propose a measure of saliency: a good summary term should be both distinctive specifically to one topic and probable in that topic. Anchor words are by definition optimally distinct, and therefore may seem to be good candidates for topic labels, but greedily selecting extreme words often results in anchor words that have low probability. In this work we explore the opposite of Arora et al.’s method: rather than finding an approximate convex hull for an exact set of vectors, we find an exact convex hull for an approximate set of ve</context>
</contexts>
<marker>Mei, Shen, Zhai, 2007</marker>
<rawString>Qiaozhu Mei, Xuehua Shen, and ChengXiang Zhai. 2007. Automatic labeling of multinomial topic models. In KDD, pages 490–499.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Mimno</author>
<author>Hanna Wallach</author>
<author>Edmund Talley</author>
<author>Miriam Leenders</author>
<author>Andrew McCallum</author>
</authors>
<title>Optimizing semantic coherence in topic models.</title>
<date>2011</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="26936" citStr="Mimno et al., 2011" startWordPosition="4455" endWordPosition="4458">ribution and more like each other. number of topics, suggesting that with few anchor words, the topic distributions are close to the overall corpus distribution and very similar to one another. The t-SNE and PCA algorithms produce consistently better specificity and dissimilarity, indicating that they produce more useful topics early with small numbers of topics. The greedy algorithm produces topics that are closer to the corpus distribution and less distinct from each other (17 obama topics). Topic Coherence is known to correlate with the semantic quality of topic judged by human annotators (Mimno et al., 2011). Let W(kT) be T most Figure 7: The greedy algorithm creates more coherent topics (higher is better), but at the cost of many overly general or repetitive topics. than those for the greedy method, but this result must be understood in combination with the Specificity and Dissimilarity metrics. The most frequent terms in the overall corpus distribution p(w) often appear together in documents. Thus a model creating many topics similar to the corpus distribution is likely to achieve high Coherence, but low Specificity by definition. Saliency. (Chuang et al., 2012) define saliency for topic words </context>
</contexts>
<marker>Mimno, Wallach, Talley, Leenders, McCallum, 2011</marker>
<rawString>David Mimno, Hanna Wallach, Edmund Talley, Miriam Leenders, and Andrew McCallum. 2011. Optimizing semantic coherence in topic models. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Sontag</author>
<author>D Roy</author>
</authors>
<title>Complexity of inference in latent dirichlet allocation.</title>
<date>2011</date>
<booktitle>In NIPS,</booktitle>
<pages>1008--1016</pages>
<contexts>
<context position="1051" citStr="Sontag and Roy, 2011" startWordPosition="147" endWordPosition="150">ce space. However, the existing greedy algorithm often selects poor anchor words, reducing topic quality and interpretability. Rather than finding an approximate convex hull in a high-dimensional space, we propose to find an exact convex hull in a visualizable 2- or 3-dimensional space. Such low-dimensional embeddings both improve topics and clearly show users why the algorithm selects certain words. 1 Introduction Statistical topic modeling is useful in exploratory data analysis (Blei et al., 2003), but model inference is known to be NP-hard even for the simplest models with only two topics (Sontag and Roy, 2011), and training often remains a black box to users. Likelihood-based training requires expensive approximate inference such as variational methods (Blei et al., 2003), which are deterministic but sensitive to initialization, or Markov chain Monte Carlo (MCMC) methods (Griffiths and Steyvers, 2004), which have no finite convergence guarantees. Recently Arora et al. proposed the Anchor Words algorithm (Arora et al., 2013), which casts topic inference as statistical recovery using a separability assumption: each topic has a specific anchor word that appears only in the context of that single topic</context>
</contexts>
<marker>Sontag, Roy, 2011</marker>
<rawString>D. Sontag and D. Roy. 2011. Complexity of inference in latent dirichlet allocation. In NIPS, pages 1008– 1016.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L J P van der Maaten</author>
<author>G E Hinton</author>
</authors>
<title>Visualizing high-dimensional data using t-SNE.</title>
<date>2008</date>
<journal>JMLR,</journal>
<pages>9--2579</pages>
<marker>van der Maaten, Hinton, 2008</marker>
<rawString>L.J.P. van der Maaten and G.E. Hinton. 2008. Visualizing high-dimensional data using t-SNE. JMLR, 9:2579–2605, Nov.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hanna Wallach</author>
<author>Iain Murray</author>
<author>Ruslan Salakhutdinov</author>
<author>David Mimno</author>
</authors>
<title>Evaluation methods for topic models.</title>
<date>2009</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="29225" citStr="Wallach et al., 2009" startWordPosition="4845" endWordPosition="4848">PCA models (1 ≈ one third the probability of the top ranked word). Lower values of soft rank (Fig. 8 indicate that the anchor word has greater relative probability to occur within a topic. As we increase the number of topics, anchor words become more prominent in topics learned by the greedy method, but t-SNE anchor words remain relatively more probable within their topics as measured by soft rank. Held-out Probability. Given an estimate of the topic-word matrix A, we can compute the marginal probability of held-out documents under that model. We use the left-to-right estimator introduced by (Wallach et al., 2009), which uses a sequential algorithm similar to a Gibbs sampler. This method requires a smoothing parameter for document-topic Dirichlet distributions, which we set to αk = 0.1. We note that the greedy algoYelp Blog NYTimes −8.1 Algorithm Greedy PCA tSNE −8.4 −6.65 0 25 50 75 100 0 25 50 75 100 0 30 60 90 Topics Figure 9: t-SNE topics have better held-out probability than greedy topics. rithm run on the original, unprojected matrix has better held-out probability values than t-SNE for the Yelp corpus, but as this method does not scale to realistic vocabularies we compare here to the sparse rand</context>
</contexts>
<marker>Wallach, Murray, Salakhutdinov, Mimno, 2009</marker>
<rawString>Hanna Wallach, Iain Murray, Ruslan Salakhutdinov, and David Mimno. 2009. Evaluation methods for topic models. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun Zhu</author>
<author>Amr Ahmed</author>
<author>Eric P Xing</author>
</authors>
<title>MedLDA: Maximum margin supervised topic models for regression and classication.</title>
<date>2009</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="8649" citStr="Zhu et al., 2009" startWordPosition="1403" endWordPosition="1406">culty is mostly because anchors must be nonnegative convex bases, whereas the classified vectors from the subset selection algorithms yield unconstrained bases. The t-SNE model has previously been used to display high-dimensional embeddings of words in 2D space by Turian.1 Low-dimensional embeddings of topic spaces have also been used to support user interaction with models: (Eisenstein et al., 2011) use a visual display of a topic embedding to create a navigator interface. Although t-SNE has been used to visualize the results of topic models, for example by (Lacoste-Julien et al., 2008) and (Zhu et al., 2009), we are not aware of any use of the method as a fundamental component of topic inference. 3 Low-dimensional Embeddings Real text corpora typically involve vocabularies in the tens of thousands of distinct words. As the input matrix Qˆ scales quadratically with V , the Anchor Words algorithm must depend on a lowdimensional projection of Qˆ in order to be practical. Previous work (Arora et al., 2013) uses random projections via either Gaussian random matrices (Johnson and Lindenstrauss, 1984) or sparse random matrices (Achlioptas, 2001), reducing the representation of each word to around 1,000 </context>
</contexts>
<marker>Zhu, Ahmed, Xing, 2009</marker>
<rawString>Jun Zhu, Amr Ahmed, and Eric P. Xing. 2009. MedLDA: Maximum margin supervised topic models for regression and classication. In ICML.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000211">
<title confidence="0.999505">
Dependency Parsing for Weibo:
An Efficient Probabilistic Logic Programming Approach
</title>
<author confidence="0.999309">
William Yang Wang, Lingpeng Kong, Kathryn Mazaitis, William W. Cohen
</author>
<affiliation confidence="0.911515333333333">
Language Technologies Institute &amp; Machine Learning Department
Carnegie Mellon University
Pittsburgh, PA 15213, USA.
</affiliation>
<email confidence="0.999491">
{yww,lingpenk,krivard,wcohen}@cs.cmu.edu
</email>
<sectionHeader confidence="0.997397" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999941681818182">
Dependency parsing is a core task in NLP,
and it is widely used by many applica-
tions such as information extraction, ques-
tion answering, and machine translation.
In the era of social media, a big chal-
lenge is that parsers trained on traditional
newswire corpora typically suffer from the
domain mismatch issue, and thus perform
poorly on social media data. We present a
new GFL/FUDG-annotated Chinese tree-
bank with more than 18K tokens from Sina
Weibo (the Chinese equivalent of Twit-
ter). We formulate the dependency pars-
ing problem as many small and paralleliz-
able arc prediction tasks: for each task,
we use a programmable probabilistic first-
order logic to infer the dependency arc of a
token in the sentence. In experiments, we
show that the proposed model outperforms
an off-the-shelf Stanford Chinese parser,
as well as a strong MaltParser baseline that
is trained on the same in-domain data.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999911416666667">
Weibo, in particular Sina Weibo1, has attracted
more than 30% of Internet users (Yang et al.,
2012), making it one of the most popular social
media services in the world. While Weibo posts
are abundantly available, NLP techniques for ana-
lyzing Weibo posts have not been well-studied in
the past.
Syntactic analysis of Weibo is made difficult
for three reasons: first, in the last few decades,
Computational Linguistics researchers have pri-
marily focused on building resources and tools us-
ing standard English newswire corpora2, and thus,
</bodyText>
<footnote confidence="0.983617">
1http://en.wikipedia.org/wiki/Sina Weibo
2For example, Wall Street Journal articles are used for
building the Penn Treebank (Marcus et al., 1993).
</footnote>
<bodyText confidence="0.961338285714286">
there are fewer resources in other languages in
general. Second, microblog posts are typically
short, noisy (Gimpel et al., 2011), and can be
considered as a “dialect”, which is very differ-
ent from news data. Due to the differences in
genre, part-of-speech taggers and parsers trained
on newswire corpora typically fail on social media
texts. Third, most existing parsers use language-
independent standard features (McDonald et al.,
2005), and these features may not be optimal for
Chinese (Martins, 2012). To most of the applica-
tion developers, the parser is more like a blackbox,
which is not directly programmable. Therefore,
it is non-trivial to adapt these generic parsers to
language-specific social media text.
In this paper, we present a new probabilistic de-
pendency parsing approach for Weibo, with the
following contributions:
• We present a freely available Chinese Weibo
dependency treebank3, manually annotated
with more than 18,000 tokens;
</bodyText>
<listItem confidence="0.62353525">
• We introduce a novel probabilistic logic
programming approach for dependency arc
prediction, making the parser directly pro-
grammable for theory engineering;
• We show that the proposed approach outper-
forms an off-the-shelf dependency parser, as
well as a strong baseline trained on the same
in-domain data.
</listItem>
<bodyText confidence="0.999185625">
In the next section, we describe existing work
on dependency parsing for Chinese. In Section 3,
we present the new Chinese Weibo Treebank to
the research community. In Section 4, we intro-
duce the proposed efficient probabilistic program-
ming approach for parsing Weibo. We show the
experimental results in Section 5, and conclude in
Section 6.
</bodyText>
<footnote confidence="0.965975">
3http://www.cs.cmu.edu/˜yww/data/WeiboTreebank.zip
</footnote>
<page confidence="0.873102">
1152
</page>
<bodyText confidence="0.521685">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1152–1158,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</bodyText>
<sectionHeader confidence="0.999629" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999940921052632">
Chinese dependency parsing has attracted many
interests in the last fifteen years. Bikel and Chi-
ang (2000; 2002) are among the first to use Penn
Chinese Tree Bank for dependency parsing, where
they adapted Xia’s head rules (Xia, 1999). An im-
portant milestone for Chinese dependency pars-
ing is that, a few years later, the CoNLL shared
task launched a track for multilingual dependency
parsing, which also included Chinese (Buchholz
and Marsi, 2006; Nilsson et al., 2007). These
shared tasks soon popularized Chinese depen-
dency parsing by making datasets available, and
there has been growing amount of literature since
then (Zhang and Clark, 2008; Nivre et al., 2007;
Sagae and Tsujii, 2007; Che et al., 2010; Carreras,
2007; Duan et al., 2007).
Besides the CoNLL shared tasks, there are also
many interesting studies on Chinese dependency
parsing. For example, researchers have studied
case (Yu et al., 2008) and morphological (Li and
Zhou, 2012) structures for learning a Chinese de-
pendency parser. Another direction is to perform
joint learning and inference for POS tagging and
dependency parsing (Li et al., 2011; Hatori et al.,
2011; Li et al., 2011; Ma et al., 2012). In recent
years, there has been growing interests in depen-
dency arc prediction in Chinese (Che et al., 2014),
and researchers have also investigated character-
level Chinese dependency parsing (Zhang et al.,
2014). However, even though the above methods
all have merits, the results are reported only on
standard newswire based Chinese Treebank (e.g.
from People’s Daily (Liu et al., 2006)), and it is
unclear how they would perform on Weibo data.
To the best of our knowledge, together with the
recent study on parsing tweets (Kong et al., 2014),
we are among the first to study the problem of de-
pendency parsing for social media text.
</bodyText>
<sectionHeader confidence="0.98586" genericHeader="method">
3 The Chinese Weibo Treebank
</sectionHeader>
<bodyText confidence="0.992856142857143">
We use the publicly available µtopia dataset (Ling
et al., 2013) for dependency annotation. An in-
teresting aspect of this Weibo dataset is that, be-
sides the Chinese posts, it also includes a copy of
the English translations. This allows us to observe
some interesting phenomena that mark the differ-
ences of the two languages. For example:
</bodyText>
<listItem confidence="0.982044">
• Function words are more frequently used in
English than in Chinese. When examin-
</listItem>
<figureCaption confidence="0.971053">
Figure 1: An example of pro-drop phenomenon
from the Weibo data.
</figureCaption>
<bodyText confidence="0.977571833333333">
ing this English version of the Weibo cor-
pus for the total counts of the word “the”,
there are 2,084 occurrences in 2,003 sen-
tences. Whereas in Chinese, there are only
52 occurrences of the word “the” out of the
2,003 sentences.
</bodyText>
<listItem confidence="0.843641916666667">
• The other interesting thing is the position of
the head. In English, the head of the tree
occurs more frequent on the left-to-middle
of the sentence, while the distribution of the
head is more complicated in Chinese. This is
also verified from the parallel Weibo data.
• Another well-known issue in Chinese is that
Chinese is a pro-drop topical language. This
is extremely prominent in the short text,
which clearly creates a problem for parsing.
For example, in the Chinese Weibo data, we
have observed the sentence in Figure 1.
</listItem>
<bodyText confidence="0.999893695652174">
To facilitate the annotation process, we first
preprocess the Weibo posts using the Stanford
NLP pipeline, including a Chinese Word Seg-
menter (Tseng et al., 2005) and a Chinese Part-
of-Speech tagger (Toutanova and Manning, 2000).
Two native speakers of Chinese with strong lin-
guistic backgrounds have annotated the depen-
dency relations from 1,000 posts of the µtopia
dataset, using the FUDG (Schneider et al., 2013)
and GFL annotation tool (Mordowanec et al.,
2014). The annotators communicate regularly dur-
ing the annotation process, and a coding man-
ual that relies majorly on the Stanford Dependen-
cies (Chang et al., 2009) is designed. The anno-
tation process has two stages: in the first stage,
we rely on the word segmentation produced by
the segmenter, and produce a draft version of the
treebank; in the second stage, the annotators ac-
tively discuss the difficult cases to reach agree-
ments, manually correct the mis-segmented word
tokens, and revise the annotations of the tricky
cases. The final inter-annotator agreement rate on
a randomly-selected subset of 373 tokens in this
</bodyText>
<page confidence="0.933743">
1153
</page>
<bodyText confidence="0.999073315789474">
treebank is 82.31%.
Fragmentary Unlabeled Dependency Grammar
(FUDG) is a newly proposed flexible framework
that offers a relative easy way to annotate the syn-
tactic structure of text. Beyond the traditional tree
view of dependency syntax in which the tokens
of a sentence form nodes in a tree, FUDG also
allows the annotation of additional lexical items
such as multiword expressions. It provides special
devices for coordination and coreference; and fa-
cilitates underspecified (partial) annotations where
producing a complete parse would be difficult.
Graph Fragment Language (GFL) is an implemen-
tation of unlabeled dependency annotations in the
FUDG framework, which fully supports Chinese,
English and other languages. The training set of
our Chinese Weibo Treebank4 includes 14,774 to-
kens, while the development and test sets include
1,846 and 1,857 tokens respectively.
</bodyText>
<sectionHeader confidence="0.849771" genericHeader="method">
4 A Programmable Parser with
Personalized PageRank Inference
</sectionHeader>
<bodyText confidence="0.999936074074074">
A key problem in multilingual dependency parsing
is that generic feature templates may not work well
for every language. For example, Martins (2012)
shows that for Chinese dependency parsing, when
adding the generic grandparents and siblings fea-
tures, the performance was worse than using the
standard bilexical, unilexical, and part-of-speech
features. Unfortunately, for many parsers such
as Stanford Chinese Parser (Levy and Manning,
2003) and MaltParser (Nivre et al., 2007), it is
very difficult for programmers to specify the fea-
ture templates and inference rules for dependency
arc prediction.
In this work, we present a Chinese dependency
parsing method for Weibo, based on efficient prob-
abilistic first-order logic programming (Wang et
al., 2013). The advantage of probabilistic pro-
gramming for parsing is that, software engineers
can simply conduct theory engineering, and op-
timize the performance of the parser for a spe-
cific genre of the target language. Recently, proba-
bilistic programming approaches (Goodman et al.,
2012; Wang et al., 2013; Lloyd et al., 2014) have
demonstrated its efficiency and effectiveness in
many areas such as information extraction (Wang
et al., 2014), entity linking, and text classifica-
tion (Wang et al., 2013).
</bodyText>
<footnote confidence="0.8492255">
4The corpus is freely available for download at the URL
specified in Section 1.
</footnote>
<listItem confidence="0.8712415">
Algorithm 1 A Dependency Arc Inference Algo-
rithm for Parsing Weibo
Given:
(1) a sentence with tokens Ti, where i is the in-
dex, and L is the length;
(2) a database D of token relations from the cor-
pus;
(3) first-order logic inference rule set R.
</listItem>
<equation confidence="0.55960425">
for i = 1 ! L tokens do
S Construct5earch5pace(Ti, R, D);
Pi InferParentUsingProPPR(Ti, S);
end for
Greedy Global Inference
for i = 1 ! L tokens do
Yi = arg max A;
end for
</equation>
<subsectionHeader confidence="0.956657">
4.1 Problem Formulation
</subsectionHeader>
<bodyText confidence="0.999985954545454">
We formulate the dependency parsing prob-
lem as many small dependency arc prediction
problems. For each token, we form the par-
ent inference problem of a token Ti as solving a
query edge(Ti, ?) using stochastic theorem prov-
ing on a search graph. Our approach relies on a
database D of inter-token relations. To construct
the database, we automatically extract the token
relations from the text data. For example, to de-
note the adjacency of two tokens T1 and T2, we
store the entry adjacent(T1,T2) in D. One can
also store the part-of-speech tag of a token in the
form haspos(T1, DT). There is no limitations
on the arity and the types of the predicates in the
database.
Given the database of token relations, one then
needs to construct the first-order logic inference
theory R for predicting dependency arcs. For ex-
ample, to construct simple bilexical and bi-POS
inference rules to model the dependency of an ad-
jacent head and a modifier, one can write first-
order clauses such as:
</bodyText>
<equation confidence="0.999729125">
edge(V1,V2) :-
adjacent(V1,V2),hasword(V1,W1),
hasword(V2,W2),keyword(W1,W2) #adjWord.
edge(V1,V2) :-
adjacent(V1,V2),haspos(V1,W1),
haspos(V2,W2),keypos(W1,W2) #adjPos.
keyword(W1,W2) :- # kw(W1,W2).
keypos(W1,W2) :- # kp(W1,W2).
</equation>
<page confidence="0.978475">
1154
</page>
<figureCaption confidence="0.968246666666667">
Figure 2: After mapping the database D to theory R, here is an example of search space for dependency
arc inference. The query is edge(S1T5, X), and there exists one correct and multiple incorrect solutions
(highlighted in bold).
</figureCaption>
<bodyText confidence="0.992152794117647">
Here, we associate a feature vector 0, with each
clause, which is annotated using the # symbol af-
ter each clause in the theory set. Note that the last
two (keyword and keypos) clauses are feature tem-
plates that allow us to learn the specific bi-POS
tags and bilexical words from the data. In order
for one to solve the query edge(TZ, ?), we first
need to map the entities from D to R to construct
the search space. The details for constructing and
searching in the graph can be found in previous
studies on probabilistic first-order logic (Wang et
al., 2013) and stochastic logic programs (Cussens,
2001). An example search space is illustrated in
Figure 2. Note that now the edges in the search
graph correspond to the feature vector 0, in R.
The overall dependency arc inference algorithm
can be found in Algorithm 1. For each of the par-
ent inference subtask, we use ProPPR (Wang et al.,
2013) to perform efficient personalized PageRank
inference. Note that to ensure the validity of the
dependency tree, we break the loops in the final
parse graph into a parse tree using the maximum
personalized PageRank score criteria. When mul-
tiple roots are predicted, we also select the most
likely root by comparing the personalized PageR-
ank solution scores.
To learn the more plausible theories, one needs
to upweight weights for relevant features, so
that they have higher transition probabilities on
the corresponding edges. To do this, we use
stochastic gradient descent to learn from training
queries, where the correct and incorrect solutions
are known. The details of the learning algorithm
are described in the last part of this section.
</bodyText>
<subsectionHeader confidence="0.965545">
4.2 Personalized PageRank Inference
</subsectionHeader>
<bodyText confidence="0.999978416666667">
For the inference of the parent of each token, we
utilize ProPPR (Wang et al., 2013). ProPPR al-
lows a fast approximate proof procedure, in which
only a small subset of the full proof graph is
generated. In particular, if α upper-bounds the
reset probability, and d upperbounds the degree
of nodes in the graph, then one can efficiently
find a subgraph with O(1α�) nodes which approx-
imates the weight for every node within an er-
ror of de (Wang et al., 2013), using a variant of
the PageRank-Nibble algorithm of Andersen et al
(2008).
</bodyText>
<subsectionHeader confidence="0.994594">
4.3 Parameter Estimation
</subsectionHeader>
<bodyText confidence="0.95805125">
Our parameter learning algorithm is implemented
using a parallel stochastic gradient descent vari-
ant to optimize the log loss using the supervised
personalized PageRank algorithm (Backstrom and
</bodyText>
<page confidence="0.980901">
1155
</page>
<table confidence="0.999198363636364">
Method Dev. Test
Stanford Parser (Xinhua) 0.507 0.489
Stanford Parser (Chinese) 0.597 0.581
MaltParser (Full) 0.669 0.654
Our methods — ProPPR
ReLU (Bi-POS) 0.506 0.517
ReLU (Bilexical) 0.635 0.616
ReLU (Full) 0.668 0.666
Truncated tanh (Bi-POS) 0.601 0.594
Truncated tanh (Bilexical) 0.650 0.634
Truncated tanh (Full) 0.667 0.675*
</table>
<tableCaption confidence="0.996728">
Table 1: Comparing our Weibo parser to other
</tableCaption>
<bodyText confidence="0.827443714285714">
baselines (UAS). The off-the-shelf Stanford parser
uses its attached Xinhua and Chinese factored
models, which are trained on external Chinese
treebank of newswire data. MaltParser was trained
on the same in-domain data as our proposed ap-
proach. * indicates p &lt; .001 comparing to the
MaltParser.
Leskovec, 2011). The idea is that, given the
training queries, we perform a random walk with
restart process, and upweight the edges that are
more likely to end up with a known correct parent.
We learn the transition probability from two nodes
(u, v) in the search graph using: Prw(v|u) =
Z f(w, Φc
</bodyText>
<listItem confidence="0.925694571428571">
1 restart), where we use two popular non-
linear parameter learning functions from the deep
learning community:
• Rectified Linear Unit (ReLU) (Nair and Hin-
ton, 2010): max(0, x);
• The Hyperbolic Function (Glorot and Ben-
gio, 2010): tanh(x).
</listItem>
<bodyText confidence="0.9990985">
as the f in this study. ReLU is a desirable
non-linear function, because it does not have the
vanishing gradient problem, and produces sparse
weights. For the weights learned from tanh(x),
we truncate the negative weights on the edges,
since the default weight on the feature edges is
w = 1.0 (existence), and w = 0.0 means that the
edge does not exist in the inference stage.
</bodyText>
<sectionHeader confidence="0.99976" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.99983075862069">
In this experiment, we compare the proposed
parser with two well-known baselines. First,
we compare with an off-the-shelf Stanford Chi-
nese Parser (Levy and Manning, 2003). Second,
we compare with the MaltParser (Nivre et al.,
2007) that is trained on the same in-domain Weibo
dataset. The train, development, and test splits are
described in Section 3. We tune the regulariza-
tion hyperparameters of the models on the dev. set,
and report Unlabeled Attachment Score (UAS) re-
sults for both the dev. set and the hold-out test set.
We experiment with the bilexical and bi-POS first-
order logic theory separately, as well as a com-
bined full model with directional and distance fea-
tures.
The results are shown in Table 1. We see that
both of the two attached pre-trained models from
the Stanford parser do not perform very well on
this Weibo dataset, probably because of the mis-
matched training and test data. MaltParser is
widely considered as one of the most popular de-
pendency parsers, not only because of its speed,
but also the acclaimed accuracy. We see that when
using the full model, the UAS results between our
methods and MaltParser are very similar on the de-
velopment set, but both of our approaches outper-
form the Maltparser in the holdout test set. The
truncated tanh variant of ProPPR obtains the best
UAS score of 0.675.
</bodyText>
<sectionHeader confidence="0.996173" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999957333333333">
In this paper, we present a novel Chinese de-
pendency treebank, annotated using Weibo data.
We introduce a probabilistic programming depen-
dency arc prediction approach, where theory en-
gineering is made easy. In experiments, we show
that our methods outperform an off-the-shelf Stan-
ford Chinese Parser, as well a strong MaltParser
that is trained on the same in-domain data. The
Chinese Weibo Treebank is made freely available
to the research community. In the future, we plan
to apply the proposed approaches to dependency
and semantic parsing of other languages.
</bodyText>
<sectionHeader confidence="0.975801" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999853125">
We are grateful to anonymous reviewers for useful
comments. This research was supported in part
by DARPA grant FA8750-12-2-0342 funded un-
der the DEFT program, and a Google Research
Award. The authors are solely responsible for the
contents of the paper, and the opinions expressed
in this publication do not reflect those of the fund-
ing agencies.
</bodyText>
<page confidence="0.994588">
1156
</page>
<sectionHeader confidence="0.984925" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.996085738317757">
Reid Andersen, Fan R. K. Chung, and Kevin J. Lang.
2008. Local partitioning for directed graphs using
pagerank. Internet Mathematics, 5(1):3–22.
Lars Backstrom and Jure Leskovec. 2011. Supervised
random walks: predicting and recommending links
in social networks. In Proceedings of the fourth
ACM international conference on Web search and
data mining, pages 635–644. ACM.
Daniel M Bikel and David Chiang. 2000. Two statis-
tical parsing models applied to the chinese treebank.
In Proceedings of the second workshop on Chinese
language processing: held in conjunction with the
38th Annual Meeting of the Association for Compu-
tational Linguistics-Volume 12, pages 1–6. Associa-
tion for Computational Linguistics.
Sabine Buchholz and Erwin Marsi. 2006. Conll-x
shared task on multilingual dependency parsing. In
Proceedings of the Tenth Conference on Computa-
tional Natural Language Learning, pages 149–164.
Association for Computational Linguistics.
Xavier Carreras. 2007. Experiments with a higher-
order projective dependency parser. In EMNLP-
CoNLL, pages 957–961.
Pi-Chuan Chang, Huihsin Tseng, Dan Jurafsky, and
Christopher D Manning. 2009. Discriminative
reordering with chinese grammatical relations fea-
tures. In Proceedings of the Third Workshop on Syn-
tax and Structure in Statistical Translation, pages
51–59. Association for Computational Linguistics.
Wanxiang Che, Zhenghua Li, and Ting Liu. 2010. Ltp:
A chinese language technology platform. In Pro-
ceedings of the 23rd International Conference on
Computational Linguistics: Demonstrations, pages
13–16. Association for Computational Linguistics.
Wanxiang Che, Jiang Guo, and Ting Liu. 2014. Re-
liable dependency arc recognition. Expert Systems
with Applications, 41(4):1716–1722.
David Chiang and Daniel M. Bikel. 2002. Recover-
ing latent information in treebanks. In Proceedings
of the 19th International Conference on Computa-
tional Linguistics - Volume 1, COLING ’02, pages
1–7, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
James Cussens. 2001. Parameter estimation in
stochastic logic programs. Machine Learning,
44(3):245–271.
Xiangyu Duan, Jun Zhao, and Bo Xu. 2007. Proba-
bilistic parsing action models for multi-lingual de-
pendency parsing. In EMNLP-CoNLL, pages 940–
946.
Kevin Gimpel, Nathan Schneider, Brendan O’Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein,
Michael Heilman, Dani Yogatama, Jeffrey Flani-
gan, and Noah A Smith. 2011. Part-of-speech tag-
ging for twitter: Annotation, features, and experi-
ments. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies: short papers-
Volume 2, pages 42–47. Association for Computa-
tional Linguistics.
Xavier Glorot and Yoshua Bengio. 2010. Understand-
ing the difficulty of training deep feedforward neural
networks. In International Conference on Artificial
Intelligence and Statistics, pages 249–256.
Noah Goodman, Vikash Mansinghka, Daniel Roy,
Keith Bonawitz, and Daniel Tarlow. 2012. Church:
a language for generative models. arXiv preprint
arXiv:1206.3255.
Jun Hatori, Takuya Matsuzaki, Yusuke Miyao, and
Jun’ichi Tsujii. 2011. Incremental joint pos tag-
ging and dependency parsing in chinese. In IJC-
NLP, pages 1216–1224.
Lingpeng Kong, Nathan Schneider, Swabha
Swayamdipta, Archna Bhatia, Chris Dyer, and
Noah A. Smith. 2014. A dependency parser for
tweets. In Proceedings of the 2014 Conference on
Empirical Methods in Natural Language Processing
(EMNLP 2014), Doha, Qatar, October. ACL.
Roger Levy and Christopher Manning. 2003. Is it
harder to parse chinese, or the chinese treebank?
In Proceedings of the 41st Annual Meeting on As-
sociation for Computational Linguistics-Volume 1,
pages 439–446. Association for Computational Lin-
guistics.
Zhongguo Li and Guodong Zhou. 2012. Unified de-
pendency parsing of chinese morphological and syn-
tactic structures. In Proceedings of the 2012 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, pages 1445–1454. Association for
Computational Linguistics.
Zhenghua Li, Min Zhang, Wanxiang Che, Ting Liu,
Wenliang Chen, and Haizhou Li. 2011. Joint mod-
els for chinese pos tagging and dependency pars-
ing. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing, pages
1180–1191. Association for Computational Linguis-
tics.
Wang Ling, Guang Xiang, Chris Dyer, Alan Black, and
Isabel Trancoso. 2013. Microblogs as parallel cor-
pora. In Proceedings of the 51st Annual Meeting
on Association for Computational Linguistics, ACL
’13. Association for Computational Linguistics.
Ting Liu, Jinshan Ma, and Sheng Li. 2006. Build-
ing a dependency treebank for improving chinese
parser. Journal of Chinese Language and Comput-
ing, 16(4):207–224.
</reference>
<page confidence="0.94679">
1157
</page>
<reference confidence="0.999772150943396">
James Robert Lloyd, David Duvenaud, Roger Grosse,
Joshua B Tenenbaum, and Zoubin Ghahramani.
2014. Automatic construction and natural-language
description of nonparametric regression models.
arXiv preprint arXiv:1402.4304.
Ji Ma, Tong Xiao, Jingbo Zhu, and Feiliang Ren. 2012.
Easy-first chinese pos tagging and dependency pars-
ing. In COLING, pages 1731–1746.
Mitchell P Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of english: The penn treebank. Compu-
tational linguistics, 19(2):313–330.
Andr´e Filipe Torres Martins. 2012. The Geometry of
Constrained Structured Prediction: Applications to
Inference and Learning of Natural Language Syn-
tax. Ph.D. thesis, Columbia University.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of
dependency parsers. In Proceedings of the 43rd
Annual Meeting on Association for Computational
Linguistics, pages 91–98. Association for Computa-
tional Linguistics.
Michael T. Mordowanec, Nathan Schneider, Chris
Dyer, and Noah A. Smith. 2014. Simplified de-
pendency annotations with gfl-web. In Proceedings
of the 52nd Annual Meeting of the Association for
Computational Linguistics: System Demonstrations.
ACL.
Vinod Nair and Geoffrey E Hinton. 2010. Rectified
linear units improve restricted boltzmann machines.
In Proceedings of the 27th International Conference
on Machine Learning (ICML-10), pages 807–814.
Jens Nilsson, Sebastian Riedel, and Deniz Yuret. 2007.
The conll 2007 shared task on dependency parsing.
In Proceedings of the CoNLL Shared Task Session of
EMNLP-CoNLL, pages 915–932. sn.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas
Chanev, G¨ulsen Eryigit, Sandra K¨ubler, Svetoslav
Marinov, and Erwin Marsi. 2007. Maltparser:
A language-independent system for data-driven de-
pendency parsing. Natural Language Engineering,
13(2):95–135.
Kenji Sagae and Jun’ichi Tsujii. 2007. Depen-
dency parsing and domain adaptation with lr models
and parser ensembles. In EMNLP-CoNLL, volume
2007, pages 1044–1050.
Nathan Schneider, Brendan O’Connor, Naomi Saphra,
David Bamman, Manaal Faruqui, Noah A Smith,
Chris Dyer, and Jason Baldridge. 2013. A
framework for (under) specifying dependency syn-
tax without overloading annotators. arXiv preprint
arXiv:1306.2091.
Kristina Toutanova and Christopher D Manning. 2000.
Enriching the knowledge sources used in a maxi-
mum entropy part-of-speech tagger. In Proceedings
of the 2000 Joint SIGDAT conference on Empirical
methods in natural language processing and very
large corpora: held in conjunction with the 38th An-
nual Meeting of the Association for Computational
Linguistics-Volume 13, pages 63–70. Association for
Computational Linguistics.
Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel
Jurafsky, and Christopher Manning. 2005. A condi-
tional random field word segmenter for sighan bake-
off 2005. In Proceedings of the Fourth SIGHAN
Workshop on Chinese Language Processing, volume
171.
William Yang Wang, Kathryn Mazaitis, and William W
Cohen. 2013. Programming with personalized
pagerank: a locally groundable first-order proba-
bilistic logic. In Proceedings of the 22nd ACM in-
ternational conference on Conference on informa-
tion &amp; knowledge management, pages 2129–2138.
ACM.
William Yang Wang, Kathryn Mazaitis, Ni Lao, Tom
Mitchell, and William W Cohen. 2014. Effi-
cient inference and learning in a large knowledge
base: Reasoning with extracted information using
a locally groundable first-order probabilistic logic.
arXiv preprint arXiv:1404.3301.
Fei Xia. 1999. Extracting tree adjoining grammars
from bracketed corpora. In Proceedings of the 5th
Natural Language Processing Pacific Rim Sympo-
sium (NLPRS-99), pages 398–403.
Fan Yang, Yang Liu, Xiaohui Yu, and Min Yang. 2012.
Automatic detection of rumor on sina weibo. In Pro-
ceedings of the ACM SIGKDD Workshop on Mining
Data Semantics, page 13. ACM.
Kun Yu, Daisuke Kawahara, and Sadao Kurohashi.
2008. Chinese dependency parsing with large scale
automatically constructed case structures. In Pro-
ceedings of the 22nd International Conference on
Computational Linguistics-Volume 1, pages 1049–
1056. Association for Computational Linguistics.
Yue Zhang and Stephen Clark. 2008. A tale of
two parsers: investigating and combining graph-
based and transition-based dependency parsing us-
ing beam-search. In Proceedings of the Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 562–571. Association for Computa-
tional Linguistics.
Meishan Zhang, Yue Zhang, Wanxiang Che, and Ting
Liu. 2014. Character-level chinese dependency
parsing. In Proceedings of the 52th Annual Meet-
ing of the Association for Computational Linguistics
(ACL 2014), Baltimore, MD, USA, June. ACL.
</reference>
<page confidence="0.995272">
1158
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.910114">
<title confidence="0.999662">Dependency Parsing for Weibo: An Efficient Probabilistic Logic Programming Approach</title>
<author confidence="0.998181">William Yang Wang</author>
<author confidence="0.998181">Lingpeng Kong</author>
<author confidence="0.998181">Kathryn Mazaitis</author>
<author confidence="0.998181">W William</author>
<affiliation confidence="0.961089">Language Technologies Institute &amp; Machine Learning Carnegie Mellon</affiliation>
<address confidence="0.993006">Pittsburgh, PA 15213,</address>
<abstract confidence="0.999795391304348">Dependency parsing is a core task in NLP, and it is widely used by many applications such as information extraction, question answering, and machine translation. In the era of social media, a big challenge is that parsers trained on traditional newswire corpora typically suffer from the domain mismatch issue, and thus perform poorly on social media data. We present a new GFL/FUDG-annotated Chinese treebank with more than 18K tokens from Sina Weibo (the Chinese equivalent of Twitter). We formulate the dependency parsing problem as many small and parallelizable arc prediction tasks: for each task, we use a programmable probabilistic firstorder logic to infer the dependency arc of a token in the sentence. In experiments, we show that the proposed model outperforms an off-the-shelf Stanford Chinese parser, as well as a strong MaltParser baseline that is trained on the same in-domain data.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Reid Andersen</author>
<author>Fan R K Chung</author>
<author>Kevin J Lang</author>
</authors>
<title>Local partitioning for directed graphs using pagerank.</title>
<date>2008</date>
<journal>Internet Mathematics,</journal>
<volume>5</volume>
<issue>1</issue>
<contexts>
<context position="14326" citStr="Andersen et al (2008)" startWordPosition="2297" endWordPosition="2300">rithm are described in the last part of this section. 4.2 Personalized PageRank Inference For the inference of the parent of each token, we utilize ProPPR (Wang et al., 2013). ProPPR allows a fast approximate proof procedure, in which only a small subset of the full proof graph is generated. In particular, if α upper-bounds the reset probability, and d upperbounds the degree of nodes in the graph, then one can efficiently find a subgraph with O(1α�) nodes which approximates the weight for every node within an error of de (Wang et al., 2013), using a variant of the PageRank-Nibble algorithm of Andersen et al (2008). 4.3 Parameter Estimation Our parameter learning algorithm is implemented using a parallel stochastic gradient descent variant to optimize the log loss using the supervised personalized PageRank algorithm (Backstrom and 1155 Method Dev. Test Stanford Parser (Xinhua) 0.507 0.489 Stanford Parser (Chinese) 0.597 0.581 MaltParser (Full) 0.669 0.654 Our methods — ProPPR ReLU (Bi-POS) 0.506 0.517 ReLU (Bilexical) 0.635 0.616 ReLU (Full) 0.668 0.666 Truncated tanh (Bi-POS) 0.601 0.594 Truncated tanh (Bilexical) 0.650 0.634 Truncated tanh (Full) 0.667 0.675* Table 1: Comparing our Weibo parser to oth</context>
</contexts>
<marker>Andersen, Chung, Lang, 2008</marker>
<rawString>Reid Andersen, Fan R. K. Chung, and Kevin J. Lang. 2008. Local partitioning for directed graphs using pagerank. Internet Mathematics, 5(1):3–22.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lars Backstrom</author>
<author>Jure Leskovec</author>
</authors>
<title>Supervised random walks: predicting and recommending links in social networks.</title>
<date>2011</date>
<booktitle>In Proceedings of the fourth ACM international conference on Web search and data mining,</booktitle>
<pages>635--644</pages>
<publisher>ACM.</publisher>
<marker>Backstrom, Leskovec, 2011</marker>
<rawString>Lars Backstrom and Jure Leskovec. 2011. Supervised random walks: predicting and recommending links in social networks. In Proceedings of the fourth ACM international conference on Web search and data mining, pages 635–644. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel M Bikel</author>
<author>David Chiang</author>
</authors>
<title>Two statistical parsing models applied to the chinese treebank.</title>
<date>2000</date>
<booktitle>In Proceedings of the second workshop on Chinese language processing: held in conjunction with the 38th Annual Meeting of the Association for Computational Linguistics-Volume 12,</booktitle>
<pages>1--6</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="3894" citStr="Bikel and Chiang (2000" startWordPosition="584" endWordPosition="588">3, we present the new Chinese Weibo Treebank to the research community. In Section 4, we introduce the proposed efficient probabilistic programming approach for parsing Weibo. We show the experimental results in Section 5, and conclude in Section 6. 3http://www.cs.cmu.edu/˜yww/data/WeiboTreebank.zip 1152 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1152–1158, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics 2 Related Work Chinese dependency parsing has attracted many interests in the last fifteen years. Bikel and Chiang (2000; 2002) are among the first to use Penn Chinese Tree Bank for dependency parsing, where they adapted Xia’s head rules (Xia, 1999). An important milestone for Chinese dependency parsing is that, a few years later, the CoNLL shared task launched a track for multilingual dependency parsing, which also included Chinese (Buchholz and Marsi, 2006; Nilsson et al., 2007). These shared tasks soon popularized Chinese dependency parsing by making datasets available, and there has been growing amount of literature since then (Zhang and Clark, 2008; Nivre et al., 2007; Sagae and Tsujii, 2007; Che et al., 2</context>
</contexts>
<marker>Bikel, Chiang, 2000</marker>
<rawString>Daniel M Bikel and David Chiang. 2000. Two statistical parsing models applied to the chinese treebank. In Proceedings of the second workshop on Chinese language processing: held in conjunction with the 38th Annual Meeting of the Association for Computational Linguistics-Volume 12, pages 1–6. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Buchholz</author>
<author>Erwin Marsi</author>
</authors>
<title>Conll-x shared task on multilingual dependency parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of the Tenth Conference on Computational Natural Language Learning,</booktitle>
<pages>149--164</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4236" citStr="Buchholz and Marsi, 2006" startWordPosition="641" endWordPosition="644"> on Empirical Methods in Natural Language Processing (EMNLP), pages 1152–1158, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics 2 Related Work Chinese dependency parsing has attracted many interests in the last fifteen years. Bikel and Chiang (2000; 2002) are among the first to use Penn Chinese Tree Bank for dependency parsing, where they adapted Xia’s head rules (Xia, 1999). An important milestone for Chinese dependency parsing is that, a few years later, the CoNLL shared task launched a track for multilingual dependency parsing, which also included Chinese (Buchholz and Marsi, 2006; Nilsson et al., 2007). These shared tasks soon popularized Chinese dependency parsing by making datasets available, and there has been growing amount of literature since then (Zhang and Clark, 2008; Nivre et al., 2007; Sagae and Tsujii, 2007; Che et al., 2010; Carreras, 2007; Duan et al., 2007). Besides the CoNLL shared tasks, there are also many interesting studies on Chinese dependency parsing. For example, researchers have studied case (Yu et al., 2008) and morphological (Li and Zhou, 2012) structures for learning a Chinese dependency parser. Another direction is to perform joint learning</context>
</contexts>
<marker>Buchholz, Marsi, 2006</marker>
<rawString>Sabine Buchholz and Erwin Marsi. 2006. Conll-x shared task on multilingual dependency parsing. In Proceedings of the Tenth Conference on Computational Natural Language Learning, pages 149–164. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Carreras</author>
</authors>
<title>Experiments with a higherorder projective dependency parser.</title>
<date>2007</date>
<booktitle>In EMNLPCoNLL,</booktitle>
<pages>957--961</pages>
<contexts>
<context position="4513" citStr="Carreras, 2007" startWordPosition="688" endWordPosition="689">2) are among the first to use Penn Chinese Tree Bank for dependency parsing, where they adapted Xia’s head rules (Xia, 1999). An important milestone for Chinese dependency parsing is that, a few years later, the CoNLL shared task launched a track for multilingual dependency parsing, which also included Chinese (Buchholz and Marsi, 2006; Nilsson et al., 2007). These shared tasks soon popularized Chinese dependency parsing by making datasets available, and there has been growing amount of literature since then (Zhang and Clark, 2008; Nivre et al., 2007; Sagae and Tsujii, 2007; Che et al., 2010; Carreras, 2007; Duan et al., 2007). Besides the CoNLL shared tasks, there are also many interesting studies on Chinese dependency parsing. For example, researchers have studied case (Yu et al., 2008) and morphological (Li and Zhou, 2012) structures for learning a Chinese dependency parser. Another direction is to perform joint learning and inference for POS tagging and dependency parsing (Li et al., 2011; Hatori et al., 2011; Li et al., 2011; Ma et al., 2012). In recent years, there has been growing interests in dependency arc prediction in Chinese (Che et al., 2014), and researchers have also investigated </context>
</contexts>
<marker>Carreras, 2007</marker>
<rawString>Xavier Carreras. 2007. Experiments with a higherorder projective dependency parser. In EMNLPCoNLL, pages 957–961.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pi-Chuan Chang</author>
<author>Huihsin Tseng</author>
<author>Dan Jurafsky</author>
<author>Christopher D Manning</author>
</authors>
<title>Discriminative reordering with chinese grammatical relations features.</title>
<date>2009</date>
<booktitle>In Proceedings of the Third Workshop on Syntax and Structure in Statistical Translation,</booktitle>
<pages>51--59</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="7497" citStr="Chang et al., 2009" startWordPosition="1187" endWordPosition="1190">acilitate the annotation process, we first preprocess the Weibo posts using the Stanford NLP pipeline, including a Chinese Word Segmenter (Tseng et al., 2005) and a Chinese Partof-Speech tagger (Toutanova and Manning, 2000). Two native speakers of Chinese with strong linguistic backgrounds have annotated the dependency relations from 1,000 posts of the µtopia dataset, using the FUDG (Schneider et al., 2013) and GFL annotation tool (Mordowanec et al., 2014). The annotators communicate regularly during the annotation process, and a coding manual that relies majorly on the Stanford Dependencies (Chang et al., 2009) is designed. The annotation process has two stages: in the first stage, we rely on the word segmentation produced by the segmenter, and produce a draft version of the treebank; in the second stage, the annotators actively discuss the difficult cases to reach agreements, manually correct the mis-segmented word tokens, and revise the annotations of the tricky cases. The final inter-annotator agreement rate on a randomly-selected subset of 373 tokens in this 1153 treebank is 82.31%. Fragmentary Unlabeled Dependency Grammar (FUDG) is a newly proposed flexible framework that offers a relative easy</context>
</contexts>
<marker>Chang, Tseng, Jurafsky, Manning, 2009</marker>
<rawString>Pi-Chuan Chang, Huihsin Tseng, Dan Jurafsky, and Christopher D Manning. 2009. Discriminative reordering with chinese grammatical relations features. In Proceedings of the Third Workshop on Syntax and Structure in Statistical Translation, pages 51–59. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wanxiang Che</author>
<author>Zhenghua Li</author>
<author>Ting Liu</author>
</authors>
<title>Ltp: A chinese language technology platform.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics: Demonstrations,</booktitle>
<pages>13--16</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4497" citStr="Che et al., 2010" startWordPosition="684" endWordPosition="687"> Chiang (2000; 2002) are among the first to use Penn Chinese Tree Bank for dependency parsing, where they adapted Xia’s head rules (Xia, 1999). An important milestone for Chinese dependency parsing is that, a few years later, the CoNLL shared task launched a track for multilingual dependency parsing, which also included Chinese (Buchholz and Marsi, 2006; Nilsson et al., 2007). These shared tasks soon popularized Chinese dependency parsing by making datasets available, and there has been growing amount of literature since then (Zhang and Clark, 2008; Nivre et al., 2007; Sagae and Tsujii, 2007; Che et al., 2010; Carreras, 2007; Duan et al., 2007). Besides the CoNLL shared tasks, there are also many interesting studies on Chinese dependency parsing. For example, researchers have studied case (Yu et al., 2008) and morphological (Li and Zhou, 2012) structures for learning a Chinese dependency parser. Another direction is to perform joint learning and inference for POS tagging and dependency parsing (Li et al., 2011; Hatori et al., 2011; Li et al., 2011; Ma et al., 2012). In recent years, there has been growing interests in dependency arc prediction in Chinese (Che et al., 2014), and researchers have al</context>
</contexts>
<marker>Che, Li, Liu, 2010</marker>
<rawString>Wanxiang Che, Zhenghua Li, and Ting Liu. 2010. Ltp: A chinese language technology platform. In Proceedings of the 23rd International Conference on Computational Linguistics: Demonstrations, pages 13–16. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wanxiang Che</author>
<author>Jiang Guo</author>
<author>Ting Liu</author>
</authors>
<title>Reliable dependency arc recognition.</title>
<date>2014</date>
<booktitle>Expert Systems with Applications,</booktitle>
<pages>41--4</pages>
<contexts>
<context position="5072" citStr="Che et al., 2014" startWordPosition="779" endWordPosition="782">; Sagae and Tsujii, 2007; Che et al., 2010; Carreras, 2007; Duan et al., 2007). Besides the CoNLL shared tasks, there are also many interesting studies on Chinese dependency parsing. For example, researchers have studied case (Yu et al., 2008) and morphological (Li and Zhou, 2012) structures for learning a Chinese dependency parser. Another direction is to perform joint learning and inference for POS tagging and dependency parsing (Li et al., 2011; Hatori et al., 2011; Li et al., 2011; Ma et al., 2012). In recent years, there has been growing interests in dependency arc prediction in Chinese (Che et al., 2014), and researchers have also investigated characterlevel Chinese dependency parsing (Zhang et al., 2014). However, even though the above methods all have merits, the results are reported only on standard newswire based Chinese Treebank (e.g. from People’s Daily (Liu et al., 2006)), and it is unclear how they would perform on Weibo data. To the best of our knowledge, together with the recent study on parsing tweets (Kong et al., 2014), we are among the first to study the problem of dependency parsing for social media text. 3 The Chinese Weibo Treebank We use the publicly available µtopia dataset</context>
</contexts>
<marker>Che, Guo, Liu, 2014</marker>
<rawString>Wanxiang Che, Jiang Guo, and Ting Liu. 2014. Reliable dependency arc recognition. Expert Systems with Applications, 41(4):1716–1722.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
<author>Daniel M Bikel</author>
</authors>
<title>Recovering latent information in treebanks.</title>
<date>2002</date>
<booktitle>In Proceedings of the 19th International Conference on Computational Linguistics - Volume 1, COLING ’02,</booktitle>
<pages>1--7</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker>Chiang, Bikel, 2002</marker>
<rawString>David Chiang and Daniel M. Bikel. 2002. Recovering latent information in treebanks. In Proceedings of the 19th International Conference on Computational Linguistics - Volume 1, COLING ’02, pages 1–7, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Cussens</author>
</authors>
<title>Parameter estimation in stochastic logic programs.</title>
<date>2001</date>
<booktitle>Machine Learning,</booktitle>
<volume>44</volume>
<issue>3</issue>
<contexts>
<context position="12724" citStr="Cussens, 2001" startWordPosition="2029" endWordPosition="2030">in bold). Here, we associate a feature vector 0, with each clause, which is annotated using the # symbol after each clause in the theory set. Note that the last two (keyword and keypos) clauses are feature templates that allow us to learn the specific bi-POS tags and bilexical words from the data. In order for one to solve the query edge(TZ, ?), we first need to map the entities from D to R to construct the search space. The details for constructing and searching in the graph can be found in previous studies on probabilistic first-order logic (Wang et al., 2013) and stochastic logic programs (Cussens, 2001). An example search space is illustrated in Figure 2. Note that now the edges in the search graph correspond to the feature vector 0, in R. The overall dependency arc inference algorithm can be found in Algorithm 1. For each of the parent inference subtask, we use ProPPR (Wang et al., 2013) to perform efficient personalized PageRank inference. Note that to ensure the validity of the dependency tree, we break the loops in the final parse graph into a parse tree using the maximum personalized PageRank score criteria. When multiple roots are predicted, we also select the most likely root by compa</context>
</contexts>
<marker>Cussens, 2001</marker>
<rawString>James Cussens. 2001. Parameter estimation in stochastic logic programs. Machine Learning, 44(3):245–271.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiangyu Duan</author>
<author>Jun Zhao</author>
<author>Bo Xu</author>
</authors>
<title>Probabilistic parsing action models for multi-lingual dependency parsing.</title>
<date>2007</date>
<booktitle>In EMNLP-CoNLL,</booktitle>
<pages>940--946</pages>
<contexts>
<context position="4533" citStr="Duan et al., 2007" startWordPosition="690" endWordPosition="693"> first to use Penn Chinese Tree Bank for dependency parsing, where they adapted Xia’s head rules (Xia, 1999). An important milestone for Chinese dependency parsing is that, a few years later, the CoNLL shared task launched a track for multilingual dependency parsing, which also included Chinese (Buchholz and Marsi, 2006; Nilsson et al., 2007). These shared tasks soon popularized Chinese dependency parsing by making datasets available, and there has been growing amount of literature since then (Zhang and Clark, 2008; Nivre et al., 2007; Sagae and Tsujii, 2007; Che et al., 2010; Carreras, 2007; Duan et al., 2007). Besides the CoNLL shared tasks, there are also many interesting studies on Chinese dependency parsing. For example, researchers have studied case (Yu et al., 2008) and morphological (Li and Zhou, 2012) structures for learning a Chinese dependency parser. Another direction is to perform joint learning and inference for POS tagging and dependency parsing (Li et al., 2011; Hatori et al., 2011; Li et al., 2011; Ma et al., 2012). In recent years, there has been growing interests in dependency arc prediction in Chinese (Che et al., 2014), and researchers have also investigated characterlevel Chine</context>
</contexts>
<marker>Duan, Zhao, Xu, 2007</marker>
<rawString>Xiangyu Duan, Jun Zhao, and Bo Xu. 2007. Probabilistic parsing action models for multi-lingual dependency parsing. In EMNLP-CoNLL, pages 940– 946.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Kevin Gimpel</author>
<author>Nathan Schneider</author>
<author>Brendan O’Connor</author>
<author>Dipanjan Das</author>
<author>Daniel Mills</author>
<author>Jacob Eisenstein</author>
<author>Michael Heilman</author>
<author>Dani Yogatama</author>
<author>Jeffrey Flanigan</author>
<author>Noah A Smith</author>
</authors>
<title>Part-of-speech tagging for twitter: Annotation, features, and experiments.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association</booktitle>
<pages>42--47</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Gimpel, Schneider, O’Connor, Das, Mills, Eisenstein, Heilman, Yogatama, Flanigan, Smith, 2011</marker>
<rawString>Kevin Gimpel, Nathan Schneider, Brendan O’Connor, Dipanjan Das, Daniel Mills, Jacob Eisenstein, Michael Heilman, Dani Yogatama, Jeffrey Flanigan, and Noah A Smith. 2011. Part-of-speech tagging for twitter: Annotation, features, and experiments. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papersVolume 2, pages 42–47. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Glorot</author>
<author>Yoshua Bengio</author>
</authors>
<title>Understanding the difficulty of training deep feedforward neural networks.</title>
<date>2010</date>
<booktitle>In International Conference on Artificial Intelligence and Statistics,</booktitle>
<pages>249--256</pages>
<contexts>
<context position="15752" citStr="Glorot and Bengio, 2010" startWordPosition="2521" endWordPosition="2525"> same in-domain data as our proposed approach. * indicates p &lt; .001 comparing to the MaltParser. Leskovec, 2011). The idea is that, given the training queries, we perform a random walk with restart process, and upweight the edges that are more likely to end up with a known correct parent. We learn the transition probability from two nodes (u, v) in the search graph using: Prw(v|u) = Z f(w, Φc 1 restart), where we use two popular nonlinear parameter learning functions from the deep learning community: • Rectified Linear Unit (ReLU) (Nair and Hinton, 2010): max(0, x); • The Hyperbolic Function (Glorot and Bengio, 2010): tanh(x). as the f in this study. ReLU is a desirable non-linear function, because it does not have the vanishing gradient problem, and produces sparse weights. For the weights learned from tanh(x), we truncate the negative weights on the edges, since the default weight on the feature edges is w = 1.0 (existence), and w = 0.0 means that the edge does not exist in the inference stage. 5 Experiments In this experiment, we compare the proposed parser with two well-known baselines. First, we compare with an off-the-shelf Stanford Chinese Parser (Levy and Manning, 2003). Second, we compare with th</context>
</contexts>
<marker>Glorot, Bengio, 2010</marker>
<rawString>Xavier Glorot and Yoshua Bengio. 2010. Understanding the difficulty of training deep feedforward neural networks. In International Conference on Artificial Intelligence and Statistics, pages 249–256.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noah Goodman</author>
<author>Vikash Mansinghka</author>
<author>Daniel Roy</author>
<author>Keith Bonawitz</author>
<author>Daniel Tarlow</author>
</authors>
<title>Church: a language for generative models. arXiv preprint arXiv:1206.3255.</title>
<date>2012</date>
<contexts>
<context position="9934" citStr="Goodman et al., 2012" startWordPosition="1558" endWordPosition="1561">er (Levy and Manning, 2003) and MaltParser (Nivre et al., 2007), it is very difficult for programmers to specify the feature templates and inference rules for dependency arc prediction. In this work, we present a Chinese dependency parsing method for Weibo, based on efficient probabilistic first-order logic programming (Wang et al., 2013). The advantage of probabilistic programming for parsing is that, software engineers can simply conduct theory engineering, and optimize the performance of the parser for a specific genre of the target language. Recently, probabilistic programming approaches (Goodman et al., 2012; Wang et al., 2013; Lloyd et al., 2014) have demonstrated its efficiency and effectiveness in many areas such as information extraction (Wang et al., 2014), entity linking, and text classification (Wang et al., 2013). 4The corpus is freely available for download at the URL specified in Section 1. Algorithm 1 A Dependency Arc Inference Algorithm for Parsing Weibo Given: (1) a sentence with tokens Ti, where i is the index, and L is the length; (2) a database D of token relations from the corpus; (3) first-order logic inference rule set R. for i = 1 ! L tokens do S Construct5earch5pace(Ti, R, D)</context>
</contexts>
<marker>Goodman, Mansinghka, Roy, Bonawitz, Tarlow, 2012</marker>
<rawString>Noah Goodman, Vikash Mansinghka, Daniel Roy, Keith Bonawitz, and Daniel Tarlow. 2012. Church: a language for generative models. arXiv preprint arXiv:1206.3255.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun Hatori</author>
<author>Takuya Matsuzaki</author>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Incremental joint pos tagging and dependency parsing in chinese.</title>
<date>2011</date>
<booktitle>In IJCNLP,</booktitle>
<pages>1216--1224</pages>
<contexts>
<context position="4927" citStr="Hatori et al., 2011" startWordPosition="752" endWordPosition="755">endency parsing by making datasets available, and there has been growing amount of literature since then (Zhang and Clark, 2008; Nivre et al., 2007; Sagae and Tsujii, 2007; Che et al., 2010; Carreras, 2007; Duan et al., 2007). Besides the CoNLL shared tasks, there are also many interesting studies on Chinese dependency parsing. For example, researchers have studied case (Yu et al., 2008) and morphological (Li and Zhou, 2012) structures for learning a Chinese dependency parser. Another direction is to perform joint learning and inference for POS tagging and dependency parsing (Li et al., 2011; Hatori et al., 2011; Li et al., 2011; Ma et al., 2012). In recent years, there has been growing interests in dependency arc prediction in Chinese (Che et al., 2014), and researchers have also investigated characterlevel Chinese dependency parsing (Zhang et al., 2014). However, even though the above methods all have merits, the results are reported only on standard newswire based Chinese Treebank (e.g. from People’s Daily (Liu et al., 2006)), and it is unclear how they would perform on Weibo data. To the best of our knowledge, together with the recent study on parsing tweets (Kong et al., 2014), we are among the </context>
</contexts>
<marker>Hatori, Matsuzaki, Miyao, Tsujii, 2011</marker>
<rawString>Jun Hatori, Takuya Matsuzaki, Yusuke Miyao, and Jun’ichi Tsujii. 2011. Incremental joint pos tagging and dependency parsing in chinese. In IJCNLP, pages 1216–1224.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lingpeng Kong</author>
<author>Nathan Schneider</author>
<author>Swabha Swayamdipta</author>
<author>Archna Bhatia</author>
<author>Chris Dyer</author>
<author>Noah A Smith</author>
</authors>
<title>A dependency parser for tweets.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP 2014),</booktitle>
<publisher>ACL.</publisher>
<location>Doha, Qatar,</location>
<contexts>
<context position="5508" citStr="Kong et al., 2014" startWordPosition="850" endWordPosition="853">(Li et al., 2011; Hatori et al., 2011; Li et al., 2011; Ma et al., 2012). In recent years, there has been growing interests in dependency arc prediction in Chinese (Che et al., 2014), and researchers have also investigated characterlevel Chinese dependency parsing (Zhang et al., 2014). However, even though the above methods all have merits, the results are reported only on standard newswire based Chinese Treebank (e.g. from People’s Daily (Liu et al., 2006)), and it is unclear how they would perform on Weibo data. To the best of our knowledge, together with the recent study on parsing tweets (Kong et al., 2014), we are among the first to study the problem of dependency parsing for social media text. 3 The Chinese Weibo Treebank We use the publicly available µtopia dataset (Ling et al., 2013) for dependency annotation. An interesting aspect of this Weibo dataset is that, besides the Chinese posts, it also includes a copy of the English translations. This allows us to observe some interesting phenomena that mark the differences of the two languages. For example: • Function words are more frequently used in English than in Chinese. When examinFigure 1: An example of pro-drop phenomenon from the Weibo d</context>
</contexts>
<marker>Kong, Schneider, Swayamdipta, Bhatia, Dyer, Smith, 2014</marker>
<rawString>Lingpeng Kong, Nathan Schneider, Swabha Swayamdipta, Archna Bhatia, Chris Dyer, and Noah A. Smith. 2014. A dependency parser for tweets. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP 2014), Doha, Qatar, October. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger Levy</author>
<author>Christopher Manning</author>
</authors>
<title>Is it harder to parse chinese, or the chinese treebank?</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1,</booktitle>
<pages>439--446</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="9341" citStr="Levy and Manning, 2003" startWordPosition="1466" endWordPosition="1469">eibo Treebank4 includes 14,774 tokens, while the development and test sets include 1,846 and 1,857 tokens respectively. 4 A Programmable Parser with Personalized PageRank Inference A key problem in multilingual dependency parsing is that generic feature templates may not work well for every language. For example, Martins (2012) shows that for Chinese dependency parsing, when adding the generic grandparents and siblings features, the performance was worse than using the standard bilexical, unilexical, and part-of-speech features. Unfortunately, for many parsers such as Stanford Chinese Parser (Levy and Manning, 2003) and MaltParser (Nivre et al., 2007), it is very difficult for programmers to specify the feature templates and inference rules for dependency arc prediction. In this work, we present a Chinese dependency parsing method for Weibo, based on efficient probabilistic first-order logic programming (Wang et al., 2013). The advantage of probabilistic programming for parsing is that, software engineers can simply conduct theory engineering, and optimize the performance of the parser for a specific genre of the target language. Recently, probabilistic programming approaches (Goodman et al., 2012; Wang </context>
<context position="16324" citStr="Levy and Manning, 2003" startWordPosition="2618" endWordPosition="2621">• The Hyperbolic Function (Glorot and Bengio, 2010): tanh(x). as the f in this study. ReLU is a desirable non-linear function, because it does not have the vanishing gradient problem, and produces sparse weights. For the weights learned from tanh(x), we truncate the negative weights on the edges, since the default weight on the feature edges is w = 1.0 (existence), and w = 0.0 means that the edge does not exist in the inference stage. 5 Experiments In this experiment, we compare the proposed parser with two well-known baselines. First, we compare with an off-the-shelf Stanford Chinese Parser (Levy and Manning, 2003). Second, we compare with the MaltParser (Nivre et al., 2007) that is trained on the same in-domain Weibo dataset. The train, development, and test splits are described in Section 3. We tune the regularization hyperparameters of the models on the dev. set, and report Unlabeled Attachment Score (UAS) results for both the dev. set and the hold-out test set. We experiment with the bilexical and bi-POS firstorder logic theory separately, as well as a combined full model with directional and distance features. The results are shown in Table 1. We see that both of the two attached pre-trained models</context>
</contexts>
<marker>Levy, Manning, 2003</marker>
<rawString>Roger Levy and Christopher Manning. 2003. Is it harder to parse chinese, or the chinese treebank? In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1, pages 439–446. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhongguo Li</author>
<author>Guodong Zhou</author>
</authors>
<title>Unified dependency parsing of chinese morphological and syntactic structures.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>1445--1454</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4736" citStr="Li and Zhou, 2012" startWordPosition="721" endWordPosition="724">hared task launched a track for multilingual dependency parsing, which also included Chinese (Buchholz and Marsi, 2006; Nilsson et al., 2007). These shared tasks soon popularized Chinese dependency parsing by making datasets available, and there has been growing amount of literature since then (Zhang and Clark, 2008; Nivre et al., 2007; Sagae and Tsujii, 2007; Che et al., 2010; Carreras, 2007; Duan et al., 2007). Besides the CoNLL shared tasks, there are also many interesting studies on Chinese dependency parsing. For example, researchers have studied case (Yu et al., 2008) and morphological (Li and Zhou, 2012) structures for learning a Chinese dependency parser. Another direction is to perform joint learning and inference for POS tagging and dependency parsing (Li et al., 2011; Hatori et al., 2011; Li et al., 2011; Ma et al., 2012). In recent years, there has been growing interests in dependency arc prediction in Chinese (Che et al., 2014), and researchers have also investigated characterlevel Chinese dependency parsing (Zhang et al., 2014). However, even though the above methods all have merits, the results are reported only on standard newswire based Chinese Treebank (e.g. from People’s Daily (Li</context>
</contexts>
<marker>Li, Zhou, 2012</marker>
<rawString>Zhongguo Li and Guodong Zhou. 2012. Unified dependency parsing of chinese morphological and syntactic structures. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1445–1454. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhenghua Li</author>
<author>Min Zhang</author>
<author>Wanxiang Che</author>
<author>Ting Liu</author>
<author>Wenliang Chen</author>
<author>Haizhou Li</author>
</authors>
<title>Joint models for chinese pos tagging and dependency parsing.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1180--1191</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4906" citStr="Li et al., 2011" startWordPosition="748" endWordPosition="751">rized Chinese dependency parsing by making datasets available, and there has been growing amount of literature since then (Zhang and Clark, 2008; Nivre et al., 2007; Sagae and Tsujii, 2007; Che et al., 2010; Carreras, 2007; Duan et al., 2007). Besides the CoNLL shared tasks, there are also many interesting studies on Chinese dependency parsing. For example, researchers have studied case (Yu et al., 2008) and morphological (Li and Zhou, 2012) structures for learning a Chinese dependency parser. Another direction is to perform joint learning and inference for POS tagging and dependency parsing (Li et al., 2011; Hatori et al., 2011; Li et al., 2011; Ma et al., 2012). In recent years, there has been growing interests in dependency arc prediction in Chinese (Che et al., 2014), and researchers have also investigated characterlevel Chinese dependency parsing (Zhang et al., 2014). However, even though the above methods all have merits, the results are reported only on standard newswire based Chinese Treebank (e.g. from People’s Daily (Liu et al., 2006)), and it is unclear how they would perform on Weibo data. To the best of our knowledge, together with the recent study on parsing tweets (Kong et al., 201</context>
</contexts>
<marker>Li, Zhang, Che, Liu, Chen, Li, 2011</marker>
<rawString>Zhenghua Li, Min Zhang, Wanxiang Che, Ting Liu, Wenliang Chen, and Haizhou Li. 2011. Joint models for chinese pos tagging and dependency parsing. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1180–1191. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wang Ling</author>
<author>Guang Xiang</author>
<author>Chris Dyer</author>
<author>Alan Black</author>
<author>Isabel Trancoso</author>
</authors>
<title>Microblogs as parallel corpora.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting on Association for Computational Linguistics, ACL ’13. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="5692" citStr="Ling et al., 2013" startWordPosition="883" endWordPosition="886">and researchers have also investigated characterlevel Chinese dependency parsing (Zhang et al., 2014). However, even though the above methods all have merits, the results are reported only on standard newswire based Chinese Treebank (e.g. from People’s Daily (Liu et al., 2006)), and it is unclear how they would perform on Weibo data. To the best of our knowledge, together with the recent study on parsing tweets (Kong et al., 2014), we are among the first to study the problem of dependency parsing for social media text. 3 The Chinese Weibo Treebank We use the publicly available µtopia dataset (Ling et al., 2013) for dependency annotation. An interesting aspect of this Weibo dataset is that, besides the Chinese posts, it also includes a copy of the English translations. This allows us to observe some interesting phenomena that mark the differences of the two languages. For example: • Function words are more frequently used in English than in Chinese. When examinFigure 1: An example of pro-drop phenomenon from the Weibo data. ing this English version of the Weibo corpus for the total counts of the word “the”, there are 2,084 occurrences in 2,003 sentences. Whereas in Chinese, there are only 52 occurren</context>
</contexts>
<marker>Ling, Xiang, Dyer, Black, Trancoso, 2013</marker>
<rawString>Wang Ling, Guang Xiang, Chris Dyer, Alan Black, and Isabel Trancoso. 2013. Microblogs as parallel corpora. In Proceedings of the 51st Annual Meeting on Association for Computational Linguistics, ACL ’13. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ting Liu</author>
<author>Jinshan Ma</author>
<author>Sheng Li</author>
</authors>
<title>Building a dependency treebank for improving chinese parser.</title>
<date>2006</date>
<journal>Journal of Chinese Language and Computing,</journal>
<volume>16</volume>
<issue>4</issue>
<contexts>
<context position="5351" citStr="Liu et al., 2006" startWordPosition="821" endWordPosition="824">2) structures for learning a Chinese dependency parser. Another direction is to perform joint learning and inference for POS tagging and dependency parsing (Li et al., 2011; Hatori et al., 2011; Li et al., 2011; Ma et al., 2012). In recent years, there has been growing interests in dependency arc prediction in Chinese (Che et al., 2014), and researchers have also investigated characterlevel Chinese dependency parsing (Zhang et al., 2014). However, even though the above methods all have merits, the results are reported only on standard newswire based Chinese Treebank (e.g. from People’s Daily (Liu et al., 2006)), and it is unclear how they would perform on Weibo data. To the best of our knowledge, together with the recent study on parsing tweets (Kong et al., 2014), we are among the first to study the problem of dependency parsing for social media text. 3 The Chinese Weibo Treebank We use the publicly available µtopia dataset (Ling et al., 2013) for dependency annotation. An interesting aspect of this Weibo dataset is that, besides the Chinese posts, it also includes a copy of the English translations. This allows us to observe some interesting phenomena that mark the differences of the two language</context>
</contexts>
<marker>Liu, Ma, Li, 2006</marker>
<rawString>Ting Liu, Jinshan Ma, and Sheng Li. 2006. Building a dependency treebank for improving chinese parser. Journal of Chinese Language and Computing, 16(4):207–224.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Robert Lloyd</author>
<author>David Duvenaud</author>
<author>Roger Grosse</author>
<author>Joshua B Tenenbaum</author>
<author>Zoubin Ghahramani</author>
</authors>
<title>Automatic construction and natural-language description of nonparametric regression models. arXiv preprint arXiv:1402.4304.</title>
<date>2014</date>
<contexts>
<context position="9974" citStr="Lloyd et al., 2014" startWordPosition="1566" endWordPosition="1569">r (Nivre et al., 2007), it is very difficult for programmers to specify the feature templates and inference rules for dependency arc prediction. In this work, we present a Chinese dependency parsing method for Weibo, based on efficient probabilistic first-order logic programming (Wang et al., 2013). The advantage of probabilistic programming for parsing is that, software engineers can simply conduct theory engineering, and optimize the performance of the parser for a specific genre of the target language. Recently, probabilistic programming approaches (Goodman et al., 2012; Wang et al., 2013; Lloyd et al., 2014) have demonstrated its efficiency and effectiveness in many areas such as information extraction (Wang et al., 2014), entity linking, and text classification (Wang et al., 2013). 4The corpus is freely available for download at the URL specified in Section 1. Algorithm 1 A Dependency Arc Inference Algorithm for Parsing Weibo Given: (1) a sentence with tokens Ti, where i is the index, and L is the length; (2) a database D of token relations from the corpus; (3) first-order logic inference rule set R. for i = 1 ! L tokens do S Construct5earch5pace(Ti, R, D); Pi InferParentUsingProPPR(Ti, S); end </context>
</contexts>
<marker>Lloyd, Duvenaud, Grosse, Tenenbaum, Ghahramani, 2014</marker>
<rawString>James Robert Lloyd, David Duvenaud, Roger Grosse, Joshua B Tenenbaum, and Zoubin Ghahramani. 2014. Automatic construction and natural-language description of nonparametric regression models. arXiv preprint arXiv:1402.4304.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ji Ma</author>
<author>Tong Xiao</author>
<author>Jingbo Zhu</author>
<author>Feiliang Ren</author>
</authors>
<title>Easy-first chinese pos tagging and dependency parsing.</title>
<date>2012</date>
<booktitle>In COLING,</booktitle>
<pages>1731--1746</pages>
<contexts>
<context position="4962" citStr="Ma et al., 2012" startWordPosition="760" endWordPosition="763">ilable, and there has been growing amount of literature since then (Zhang and Clark, 2008; Nivre et al., 2007; Sagae and Tsujii, 2007; Che et al., 2010; Carreras, 2007; Duan et al., 2007). Besides the CoNLL shared tasks, there are also many interesting studies on Chinese dependency parsing. For example, researchers have studied case (Yu et al., 2008) and morphological (Li and Zhou, 2012) structures for learning a Chinese dependency parser. Another direction is to perform joint learning and inference for POS tagging and dependency parsing (Li et al., 2011; Hatori et al., 2011; Li et al., 2011; Ma et al., 2012). In recent years, there has been growing interests in dependency arc prediction in Chinese (Che et al., 2014), and researchers have also investigated characterlevel Chinese dependency parsing (Zhang et al., 2014). However, even though the above methods all have merits, the results are reported only on standard newswire based Chinese Treebank (e.g. from People’s Daily (Liu et al., 2006)), and it is unclear how they would perform on Weibo data. To the best of our knowledge, together with the recent study on parsing tweets (Kong et al., 2014), we are among the first to study the problem of depen</context>
</contexts>
<marker>Ma, Xiao, Zhu, Ren, 2012</marker>
<rawString>Ji Ma, Tong Xiao, Jingbo Zhu, and Feiliang Ren. 2012. Easy-first chinese pos tagging and dependency parsing. In COLING, pages 1731–1746.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Mary Ann Marcinkiewicz</author>
<author>Beatrice Santorini</author>
</authors>
<title>Building a large annotated corpus of english: The penn treebank.</title>
<date>1993</date>
<booktitle>Computational linguistics,</booktitle>
<pages>19--2</pages>
<contexts>
<context position="1915" citStr="Marcus et al., 1993" startWordPosition="288" endWordPosition="291">f Internet users (Yang et al., 2012), making it one of the most popular social media services in the world. While Weibo posts are abundantly available, NLP techniques for analyzing Weibo posts have not been well-studied in the past. Syntactic analysis of Weibo is made difficult for three reasons: first, in the last few decades, Computational Linguistics researchers have primarily focused on building resources and tools using standard English newswire corpora2, and thus, 1http://en.wikipedia.org/wiki/Sina Weibo 2For example, Wall Street Journal articles are used for building the Penn Treebank (Marcus et al., 1993). there are fewer resources in other languages in general. Second, microblog posts are typically short, noisy (Gimpel et al., 2011), and can be considered as a “dialect”, which is very different from news data. Due to the differences in genre, part-of-speech taggers and parsers trained on newswire corpora typically fail on social media texts. Third, most existing parsers use languageindependent standard features (McDonald et al., 2005), and these features may not be optimal for Chinese (Martins, 2012). To most of the application developers, the parser is more like a blackbox, which is not dire</context>
</contexts>
<marker>Marcus, Marcinkiewicz, Santorini, 1993</marker>
<rawString>Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. 1993. Building a large annotated corpus of english: The penn treebank. Computational linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andr´e Filipe Torres Martins</author>
</authors>
<title>The Geometry of Constrained Structured Prediction: Applications to Inference and Learning of Natural Language Syntax.</title>
<date>2012</date>
<tech>Ph.D. thesis,</tech>
<institution>Columbia University.</institution>
<contexts>
<context position="2421" citStr="Martins, 2012" startWordPosition="369" endWordPosition="370">ibo 2For example, Wall Street Journal articles are used for building the Penn Treebank (Marcus et al., 1993). there are fewer resources in other languages in general. Second, microblog posts are typically short, noisy (Gimpel et al., 2011), and can be considered as a “dialect”, which is very different from news data. Due to the differences in genre, part-of-speech taggers and parsers trained on newswire corpora typically fail on social media texts. Third, most existing parsers use languageindependent standard features (McDonald et al., 2005), and these features may not be optimal for Chinese (Martins, 2012). To most of the application developers, the parser is more like a blackbox, which is not directly programmable. Therefore, it is non-trivial to adapt these generic parsers to language-specific social media text. In this paper, we present a new probabilistic dependency parsing approach for Weibo, with the following contributions: • We present a freely available Chinese Weibo dependency treebank3, manually annotated with more than 18,000 tokens; • We introduce a novel probabilistic logic programming approach for dependency arc prediction, making the parser directly programmable for theory engin</context>
<context position="9047" citStr="Martins (2012)" startWordPosition="1427" endWordPosition="1428">ecified (partial) annotations where producing a complete parse would be difficult. Graph Fragment Language (GFL) is an implementation of unlabeled dependency annotations in the FUDG framework, which fully supports Chinese, English and other languages. The training set of our Chinese Weibo Treebank4 includes 14,774 tokens, while the development and test sets include 1,846 and 1,857 tokens respectively. 4 A Programmable Parser with Personalized PageRank Inference A key problem in multilingual dependency parsing is that generic feature templates may not work well for every language. For example, Martins (2012) shows that for Chinese dependency parsing, when adding the generic grandparents and siblings features, the performance was worse than using the standard bilexical, unilexical, and part-of-speech features. Unfortunately, for many parsers such as Stanford Chinese Parser (Levy and Manning, 2003) and MaltParser (Nivre et al., 2007), it is very difficult for programmers to specify the feature templates and inference rules for dependency arc prediction. In this work, we present a Chinese dependency parsing method for Weibo, based on efficient probabilistic first-order logic programming (Wang et al.</context>
</contexts>
<marker>Martins, 2012</marker>
<rawString>Andr´e Filipe Torres Martins. 2012. The Geometry of Constrained Structured Prediction: Applications to Inference and Learning of Natural Language Syntax. Ph.D. thesis, Columbia University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Koby Crammer</author>
<author>Fernando Pereira</author>
</authors>
<title>Online large-margin training of dependency parsers.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>91--98</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2354" citStr="McDonald et al., 2005" startWordPosition="356" endWordPosition="359"> English newswire corpora2, and thus, 1http://en.wikipedia.org/wiki/Sina Weibo 2For example, Wall Street Journal articles are used for building the Penn Treebank (Marcus et al., 1993). there are fewer resources in other languages in general. Second, microblog posts are typically short, noisy (Gimpel et al., 2011), and can be considered as a “dialect”, which is very different from news data. Due to the differences in genre, part-of-speech taggers and parsers trained on newswire corpora typically fail on social media texts. Third, most existing parsers use languageindependent standard features (McDonald et al., 2005), and these features may not be optimal for Chinese (Martins, 2012). To most of the application developers, the parser is more like a blackbox, which is not directly programmable. Therefore, it is non-trivial to adapt these generic parsers to language-specific social media text. In this paper, we present a new probabilistic dependency parsing approach for Weibo, with the following contributions: • We present a freely available Chinese Weibo dependency treebank3, manually annotated with more than 18,000 tokens; • We introduce a novel probabilistic logic programming approach for dependency arc p</context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005. Online large-margin training of dependency parsers. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 91–98. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael T Mordowanec</author>
<author>Nathan Schneider</author>
<author>Chris Dyer</author>
<author>Noah A Smith</author>
</authors>
<title>Simplified dependency annotations with gfl-web.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations. ACL.</booktitle>
<contexts>
<context position="7338" citStr="Mordowanec et al., 2014" startWordPosition="1161" endWordPosition="1164">ly prominent in the short text, which clearly creates a problem for parsing. For example, in the Chinese Weibo data, we have observed the sentence in Figure 1. To facilitate the annotation process, we first preprocess the Weibo posts using the Stanford NLP pipeline, including a Chinese Word Segmenter (Tseng et al., 2005) and a Chinese Partof-Speech tagger (Toutanova and Manning, 2000). Two native speakers of Chinese with strong linguistic backgrounds have annotated the dependency relations from 1,000 posts of the µtopia dataset, using the FUDG (Schneider et al., 2013) and GFL annotation tool (Mordowanec et al., 2014). The annotators communicate regularly during the annotation process, and a coding manual that relies majorly on the Stanford Dependencies (Chang et al., 2009) is designed. The annotation process has two stages: in the first stage, we rely on the word segmentation produced by the segmenter, and produce a draft version of the treebank; in the second stage, the annotators actively discuss the difficult cases to reach agreements, manually correct the mis-segmented word tokens, and revise the annotations of the tricky cases. The final inter-annotator agreement rate on a randomly-selected subset of</context>
</contexts>
<marker>Mordowanec, Schneider, Dyer, Smith, 2014</marker>
<rawString>Michael T. Mordowanec, Nathan Schneider, Chris Dyer, and Noah A. Smith. 2014. Simplified dependency annotations with gfl-web. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vinod Nair</author>
<author>Geoffrey E Hinton</author>
</authors>
<title>Rectified linear units improve restricted boltzmann machines.</title>
<date>2010</date>
<booktitle>In Proceedings of the 27th International Conference on Machine Learning (ICML-10),</booktitle>
<pages>807--814</pages>
<contexts>
<context position="15688" citStr="Nair and Hinton, 2010" startWordPosition="2510" endWordPosition="2514">inese treebank of newswire data. MaltParser was trained on the same in-domain data as our proposed approach. * indicates p &lt; .001 comparing to the MaltParser. Leskovec, 2011). The idea is that, given the training queries, we perform a random walk with restart process, and upweight the edges that are more likely to end up with a known correct parent. We learn the transition probability from two nodes (u, v) in the search graph using: Prw(v|u) = Z f(w, Φc 1 restart), where we use two popular nonlinear parameter learning functions from the deep learning community: • Rectified Linear Unit (ReLU) (Nair and Hinton, 2010): max(0, x); • The Hyperbolic Function (Glorot and Bengio, 2010): tanh(x). as the f in this study. ReLU is a desirable non-linear function, because it does not have the vanishing gradient problem, and produces sparse weights. For the weights learned from tanh(x), we truncate the negative weights on the edges, since the default weight on the feature edges is w = 1.0 (existence), and w = 0.0 means that the edge does not exist in the inference stage. 5 Experiments In this experiment, we compare the proposed parser with two well-known baselines. First, we compare with an off-the-shelf Stanford Chi</context>
</contexts>
<marker>Nair, Hinton, 2010</marker>
<rawString>Vinod Nair and Geoffrey E Hinton. 2010. Rectified linear units improve restricted boltzmann machines. In Proceedings of the 27th International Conference on Machine Learning (ICML-10), pages 807–814.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jens Nilsson</author>
<author>Sebastian Riedel</author>
<author>Deniz Yuret</author>
</authors>
<title>The conll</title>
<date>2007</date>
<booktitle>In Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL,</booktitle>
<pages>915--932</pages>
<contexts>
<context position="4259" citStr="Nilsson et al., 2007" startWordPosition="645" endWordPosition="648">atural Language Processing (EMNLP), pages 1152–1158, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics 2 Related Work Chinese dependency parsing has attracted many interests in the last fifteen years. Bikel and Chiang (2000; 2002) are among the first to use Penn Chinese Tree Bank for dependency parsing, where they adapted Xia’s head rules (Xia, 1999). An important milestone for Chinese dependency parsing is that, a few years later, the CoNLL shared task launched a track for multilingual dependency parsing, which also included Chinese (Buchholz and Marsi, 2006; Nilsson et al., 2007). These shared tasks soon popularized Chinese dependency parsing by making datasets available, and there has been growing amount of literature since then (Zhang and Clark, 2008; Nivre et al., 2007; Sagae and Tsujii, 2007; Che et al., 2010; Carreras, 2007; Duan et al., 2007). Besides the CoNLL shared tasks, there are also many interesting studies on Chinese dependency parsing. For example, researchers have studied case (Yu et al., 2008) and morphological (Li and Zhou, 2012) structures for learning a Chinese dependency parser. Another direction is to perform joint learning and inference for POS </context>
</contexts>
<marker>Nilsson, Riedel, Yuret, 2007</marker>
<rawString>Jens Nilsson, Sebastian Riedel, and Deniz Yuret. 2007. The conll 2007 shared task on dependency parsing. In Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL, pages 915–932. sn.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Jens Nilsson</author>
</authors>
<title>Atanas Chanev, G¨ulsen Eryigit, Sandra K¨ubler, Svetoslav Marinov, and Erwin Marsi.</title>
<date>2007</date>
<journal>Natural Language Engineering,</journal>
<volume>13</volume>
<issue>2</issue>
<contexts>
<context position="4455" citStr="Nivre et al., 2007" startWordPosition="676" endWordPosition="679">terests in the last fifteen years. Bikel and Chiang (2000; 2002) are among the first to use Penn Chinese Tree Bank for dependency parsing, where they adapted Xia’s head rules (Xia, 1999). An important milestone for Chinese dependency parsing is that, a few years later, the CoNLL shared task launched a track for multilingual dependency parsing, which also included Chinese (Buchholz and Marsi, 2006; Nilsson et al., 2007). These shared tasks soon popularized Chinese dependency parsing by making datasets available, and there has been growing amount of literature since then (Zhang and Clark, 2008; Nivre et al., 2007; Sagae and Tsujii, 2007; Che et al., 2010; Carreras, 2007; Duan et al., 2007). Besides the CoNLL shared tasks, there are also many interesting studies on Chinese dependency parsing. For example, researchers have studied case (Yu et al., 2008) and morphological (Li and Zhou, 2012) structures for learning a Chinese dependency parser. Another direction is to perform joint learning and inference for POS tagging and dependency parsing (Li et al., 2011; Hatori et al., 2011; Li et al., 2011; Ma et al., 2012). In recent years, there has been growing interests in dependency arc prediction in Chinese (</context>
<context position="9377" citStr="Nivre et al., 2007" startWordPosition="1472" endWordPosition="1475">hile the development and test sets include 1,846 and 1,857 tokens respectively. 4 A Programmable Parser with Personalized PageRank Inference A key problem in multilingual dependency parsing is that generic feature templates may not work well for every language. For example, Martins (2012) shows that for Chinese dependency parsing, when adding the generic grandparents and siblings features, the performance was worse than using the standard bilexical, unilexical, and part-of-speech features. Unfortunately, for many parsers such as Stanford Chinese Parser (Levy and Manning, 2003) and MaltParser (Nivre et al., 2007), it is very difficult for programmers to specify the feature templates and inference rules for dependency arc prediction. In this work, we present a Chinese dependency parsing method for Weibo, based on efficient probabilistic first-order logic programming (Wang et al., 2013). The advantage of probabilistic programming for parsing is that, software engineers can simply conduct theory engineering, and optimize the performance of the parser for a specific genre of the target language. Recently, probabilistic programming approaches (Goodman et al., 2012; Wang et al., 2013; Lloyd et al., 2014) ha</context>
<context position="16385" citStr="Nivre et al., 2007" startWordPosition="2628" endWordPosition="2631">the f in this study. ReLU is a desirable non-linear function, because it does not have the vanishing gradient problem, and produces sparse weights. For the weights learned from tanh(x), we truncate the negative weights on the edges, since the default weight on the feature edges is w = 1.0 (existence), and w = 0.0 means that the edge does not exist in the inference stage. 5 Experiments In this experiment, we compare the proposed parser with two well-known baselines. First, we compare with an off-the-shelf Stanford Chinese Parser (Levy and Manning, 2003). Second, we compare with the MaltParser (Nivre et al., 2007) that is trained on the same in-domain Weibo dataset. The train, development, and test splits are described in Section 3. We tune the regularization hyperparameters of the models on the dev. set, and report Unlabeled Attachment Score (UAS) results for both the dev. set and the hold-out test set. We experiment with the bilexical and bi-POS firstorder logic theory separately, as well as a combined full model with directional and distance features. The results are shown in Table 1. We see that both of the two attached pre-trained models from the Stanford parser do not perform very well on this We</context>
</contexts>
<marker>Nivre, Hall, Nilsson, 2007</marker>
<rawString>Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev, G¨ulsen Eryigit, Sandra K¨ubler, Svetoslav Marinov, and Erwin Marsi. 2007. Maltparser: A language-independent system for data-driven dependency parsing. Natural Language Engineering, 13(2):95–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Sagae</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Dependency parsing and domain adaptation with lr models and parser ensembles. In EMNLP-CoNLL, volume</title>
<date>2007</date>
<pages>1044--1050</pages>
<contexts>
<context position="4479" citStr="Sagae and Tsujii, 2007" startWordPosition="680" endWordPosition="683">fifteen years. Bikel and Chiang (2000; 2002) are among the first to use Penn Chinese Tree Bank for dependency parsing, where they adapted Xia’s head rules (Xia, 1999). An important milestone for Chinese dependency parsing is that, a few years later, the CoNLL shared task launched a track for multilingual dependency parsing, which also included Chinese (Buchholz and Marsi, 2006; Nilsson et al., 2007). These shared tasks soon popularized Chinese dependency parsing by making datasets available, and there has been growing amount of literature since then (Zhang and Clark, 2008; Nivre et al., 2007; Sagae and Tsujii, 2007; Che et al., 2010; Carreras, 2007; Duan et al., 2007). Besides the CoNLL shared tasks, there are also many interesting studies on Chinese dependency parsing. For example, researchers have studied case (Yu et al., 2008) and morphological (Li and Zhou, 2012) structures for learning a Chinese dependency parser. Another direction is to perform joint learning and inference for POS tagging and dependency parsing (Li et al., 2011; Hatori et al., 2011; Li et al., 2011; Ma et al., 2012). In recent years, there has been growing interests in dependency arc prediction in Chinese (Che et al., 2014), and r</context>
</contexts>
<marker>Sagae, Tsujii, 2007</marker>
<rawString>Kenji Sagae and Jun’ichi Tsujii. 2007. Dependency parsing and domain adaptation with lr models and parser ensembles. In EMNLP-CoNLL, volume 2007, pages 1044–1050.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nathan Schneider</author>
<author>Brendan O’Connor</author>
<author>Naomi Saphra</author>
<author>David Bamman</author>
<author>Manaal Faruqui</author>
<author>Noah A Smith</author>
<author>Chris Dyer</author>
<author>Jason Baldridge</author>
</authors>
<title>A framework for (under) specifying dependency syntax without overloading annotators. arXiv preprint arXiv:1306.2091.</title>
<date>2013</date>
<marker>Schneider, O’Connor, Saphra, Bamman, Faruqui, Smith, Dyer, Baldridge, 2013</marker>
<rawString>Nathan Schneider, Brendan O’Connor, Naomi Saphra, David Bamman, Manaal Faruqui, Noah A Smith, Chris Dyer, and Jason Baldridge. 2013. A framework for (under) specifying dependency syntax without overloading annotators. arXiv preprint arXiv:1306.2091.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Kristina Toutanova</author>
<author>Christopher D Manning</author>
</authors>
<title>Enriching the knowledge sources used in a maximum entropy part-of-speech tagger.</title>
<date>2000</date>
<booktitle>In Proceedings of the 2000 Joint SIGDAT conference on Empirical methods in natural language processing and very large corpora: held in conjunction with the 38th Annual Meeting of the Association for Computational Linguistics-Volume 13,</booktitle>
<pages>63--70</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="7101" citStr="Toutanova and Manning, 2000" startWordPosition="1123" endWordPosition="1126">dle of the sentence, while the distribution of the head is more complicated in Chinese. This is also verified from the parallel Weibo data. • Another well-known issue in Chinese is that Chinese is a pro-drop topical language. This is extremely prominent in the short text, which clearly creates a problem for parsing. For example, in the Chinese Weibo data, we have observed the sentence in Figure 1. To facilitate the annotation process, we first preprocess the Weibo posts using the Stanford NLP pipeline, including a Chinese Word Segmenter (Tseng et al., 2005) and a Chinese Partof-Speech tagger (Toutanova and Manning, 2000). Two native speakers of Chinese with strong linguistic backgrounds have annotated the dependency relations from 1,000 posts of the µtopia dataset, using the FUDG (Schneider et al., 2013) and GFL annotation tool (Mordowanec et al., 2014). The annotators communicate regularly during the annotation process, and a coding manual that relies majorly on the Stanford Dependencies (Chang et al., 2009) is designed. The annotation process has two stages: in the first stage, we rely on the word segmentation produced by the segmenter, and produce a draft version of the treebank; in the second stage, the a</context>
</contexts>
<marker>Toutanova, Manning, 2000</marker>
<rawString>Kristina Toutanova and Christopher D Manning. 2000. Enriching the knowledge sources used in a maximum entropy part-of-speech tagger. In Proceedings of the 2000 Joint SIGDAT conference on Empirical methods in natural language processing and very large corpora: held in conjunction with the 38th Annual Meeting of the Association for Computational Linguistics-Volume 13, pages 63–70. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Huihsin Tseng</author>
<author>Pichuan Chang</author>
<author>Galen Andrew</author>
<author>Daniel Jurafsky</author>
<author>Christopher Manning</author>
</authors>
<title>A conditional random field word segmenter for sighan bakeoff</title>
<date>2005</date>
<booktitle>In Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing,</booktitle>
<volume>171</volume>
<contexts>
<context position="7036" citStr="Tseng et al., 2005" startWordPosition="1113" endWordPosition="1116">head of the tree occurs more frequent on the left-to-middle of the sentence, while the distribution of the head is more complicated in Chinese. This is also verified from the parallel Weibo data. • Another well-known issue in Chinese is that Chinese is a pro-drop topical language. This is extremely prominent in the short text, which clearly creates a problem for parsing. For example, in the Chinese Weibo data, we have observed the sentence in Figure 1. To facilitate the annotation process, we first preprocess the Weibo posts using the Stanford NLP pipeline, including a Chinese Word Segmenter (Tseng et al., 2005) and a Chinese Partof-Speech tagger (Toutanova and Manning, 2000). Two native speakers of Chinese with strong linguistic backgrounds have annotated the dependency relations from 1,000 posts of the µtopia dataset, using the FUDG (Schneider et al., 2013) and GFL annotation tool (Mordowanec et al., 2014). The annotators communicate regularly during the annotation process, and a coding manual that relies majorly on the Stanford Dependencies (Chang et al., 2009) is designed. The annotation process has two stages: in the first stage, we rely on the word segmentation produced by the segmenter, and pr</context>
</contexts>
<marker>Tseng, Chang, Andrew, Jurafsky, Manning, 2005</marker>
<rawString>Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel Jurafsky, and Christopher Manning. 2005. A conditional random field word segmenter for sighan bakeoff 2005. In Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing, volume 171.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Yang Wang</author>
<author>Kathryn Mazaitis</author>
<author>William W Cohen</author>
</authors>
<title>Programming with personalized pagerank: a locally groundable first-order probabilistic logic.</title>
<date>2013</date>
<booktitle>In Proceedings of the 22nd ACM international conference on Conference on information &amp; knowledge management,</booktitle>
<pages>2129--2138</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="9654" citStr="Wang et al., 2013" startWordPosition="1515" endWordPosition="1518">tins (2012) shows that for Chinese dependency parsing, when adding the generic grandparents and siblings features, the performance was worse than using the standard bilexical, unilexical, and part-of-speech features. Unfortunately, for many parsers such as Stanford Chinese Parser (Levy and Manning, 2003) and MaltParser (Nivre et al., 2007), it is very difficult for programmers to specify the feature templates and inference rules for dependency arc prediction. In this work, we present a Chinese dependency parsing method for Weibo, based on efficient probabilistic first-order logic programming (Wang et al., 2013). The advantage of probabilistic programming for parsing is that, software engineers can simply conduct theory engineering, and optimize the performance of the parser for a specific genre of the target language. Recently, probabilistic programming approaches (Goodman et al., 2012; Wang et al., 2013; Lloyd et al., 2014) have demonstrated its efficiency and effectiveness in many areas such as information extraction (Wang et al., 2014), entity linking, and text classification (Wang et al., 2013). 4The corpus is freely available for download at the URL specified in Section 1. Algorithm 1 A Depende</context>
<context position="12678" citStr="Wang et al., 2013" startWordPosition="2021" endWordPosition="2024">ect and multiple incorrect solutions (highlighted in bold). Here, we associate a feature vector 0, with each clause, which is annotated using the # symbol after each clause in the theory set. Note that the last two (keyword and keypos) clauses are feature templates that allow us to learn the specific bi-POS tags and bilexical words from the data. In order for one to solve the query edge(TZ, ?), we first need to map the entities from D to R to construct the search space. The details for constructing and searching in the graph can be found in previous studies on probabilistic first-order logic (Wang et al., 2013) and stochastic logic programs (Cussens, 2001). An example search space is illustrated in Figure 2. Note that now the edges in the search graph correspond to the feature vector 0, in R. The overall dependency arc inference algorithm can be found in Algorithm 1. For each of the parent inference subtask, we use ProPPR (Wang et al., 2013) to perform efficient personalized PageRank inference. Note that to ensure the validity of the dependency tree, we break the loops in the final parse graph into a parse tree using the maximum personalized PageRank score criteria. When multiple roots are predicted</context>
<context position="14251" citStr="Wang et al., 2013" startWordPosition="2285" endWordPosition="2288">rect and incorrect solutions are known. The details of the learning algorithm are described in the last part of this section. 4.2 Personalized PageRank Inference For the inference of the parent of each token, we utilize ProPPR (Wang et al., 2013). ProPPR allows a fast approximate proof procedure, in which only a small subset of the full proof graph is generated. In particular, if α upper-bounds the reset probability, and d upperbounds the degree of nodes in the graph, then one can efficiently find a subgraph with O(1α�) nodes which approximates the weight for every node within an error of de (Wang et al., 2013), using a variant of the PageRank-Nibble algorithm of Andersen et al (2008). 4.3 Parameter Estimation Our parameter learning algorithm is implemented using a parallel stochastic gradient descent variant to optimize the log loss using the supervised personalized PageRank algorithm (Backstrom and 1155 Method Dev. Test Stanford Parser (Xinhua) 0.507 0.489 Stanford Parser (Chinese) 0.597 0.581 MaltParser (Full) 0.669 0.654 Our methods — ProPPR ReLU (Bi-POS) 0.506 0.517 ReLU (Bilexical) 0.635 0.616 ReLU (Full) 0.668 0.666 Truncated tanh (Bi-POS) 0.601 0.594 Truncated tanh (Bilexical) 0.650 0.634 Tr</context>
</contexts>
<marker>Wang, Mazaitis, Cohen, 2013</marker>
<rawString>William Yang Wang, Kathryn Mazaitis, and William W Cohen. 2013. Programming with personalized pagerank: a locally groundable first-order probabilistic logic. In Proceedings of the 22nd ACM international conference on Conference on information &amp; knowledge management, pages 2129–2138. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Yang Wang</author>
<author>Kathryn Mazaitis</author>
<author>Ni Lao</author>
<author>Tom Mitchell</author>
<author>William W Cohen</author>
</authors>
<title>Efficient inference and learning in a large knowledge base: Reasoning with extracted information using a locally groundable first-order probabilistic logic. arXiv preprint arXiv:1404.3301.</title>
<date>2014</date>
<contexts>
<context position="10090" citStr="Wang et al., 2014" startWordPosition="1583" endWordPosition="1586"> dependency arc prediction. In this work, we present a Chinese dependency parsing method for Weibo, based on efficient probabilistic first-order logic programming (Wang et al., 2013). The advantage of probabilistic programming for parsing is that, software engineers can simply conduct theory engineering, and optimize the performance of the parser for a specific genre of the target language. Recently, probabilistic programming approaches (Goodman et al., 2012; Wang et al., 2013; Lloyd et al., 2014) have demonstrated its efficiency and effectiveness in many areas such as information extraction (Wang et al., 2014), entity linking, and text classification (Wang et al., 2013). 4The corpus is freely available for download at the URL specified in Section 1. Algorithm 1 A Dependency Arc Inference Algorithm for Parsing Weibo Given: (1) a sentence with tokens Ti, where i is the index, and L is the length; (2) a database D of token relations from the corpus; (3) first-order logic inference rule set R. for i = 1 ! L tokens do S Construct5earch5pace(Ti, R, D); Pi InferParentUsingProPPR(Ti, S); end for Greedy Global Inference for i = 1 ! L tokens do Yi = arg max A; end for 4.1 Problem Formulation We formulate the</context>
</contexts>
<marker>Wang, Mazaitis, Lao, Mitchell, Cohen, 2014</marker>
<rawString>William Yang Wang, Kathryn Mazaitis, Ni Lao, Tom Mitchell, and William W Cohen. 2014. Efficient inference and learning in a large knowledge base: Reasoning with extracted information using a locally groundable first-order probabilistic logic. arXiv preprint arXiv:1404.3301.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Xia</author>
</authors>
<title>Extracting tree adjoining grammars from bracketed corpora.</title>
<date>1999</date>
<booktitle>In Proceedings of the 5th Natural Language Processing Pacific Rim Symposium (NLPRS-99),</booktitle>
<pages>398--403</pages>
<contexts>
<context position="4023" citStr="Xia, 1999" startWordPosition="609" endWordPosition="610">amming approach for parsing Weibo. We show the experimental results in Section 5, and conclude in Section 6. 3http://www.cs.cmu.edu/˜yww/data/WeiboTreebank.zip 1152 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1152–1158, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics 2 Related Work Chinese dependency parsing has attracted many interests in the last fifteen years. Bikel and Chiang (2000; 2002) are among the first to use Penn Chinese Tree Bank for dependency parsing, where they adapted Xia’s head rules (Xia, 1999). An important milestone for Chinese dependency parsing is that, a few years later, the CoNLL shared task launched a track for multilingual dependency parsing, which also included Chinese (Buchholz and Marsi, 2006; Nilsson et al., 2007). These shared tasks soon popularized Chinese dependency parsing by making datasets available, and there has been growing amount of literature since then (Zhang and Clark, 2008; Nivre et al., 2007; Sagae and Tsujii, 2007; Che et al., 2010; Carreras, 2007; Duan et al., 2007). Besides the CoNLL shared tasks, there are also many interesting studies on Chinese depen</context>
</contexts>
<marker>Xia, 1999</marker>
<rawString>Fei Xia. 1999. Extracting tree adjoining grammars from bracketed corpora. In Proceedings of the 5th Natural Language Processing Pacific Rim Symposium (NLPRS-99), pages 398–403.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fan Yang</author>
<author>Yang Liu</author>
<author>Xiaohui Yu</author>
<author>Min Yang</author>
</authors>
<title>Automatic detection of rumor on sina weibo.</title>
<date>2012</date>
<booktitle>In Proceedings of the ACM SIGKDD Workshop on Mining Data Semantics,</booktitle>
<pages>13</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="1331" citStr="Yang et al., 2012" startWordPosition="200" endWordPosition="203">hinese treebank with more than 18K tokens from Sina Weibo (the Chinese equivalent of Twitter). We formulate the dependency parsing problem as many small and parallelizable arc prediction tasks: for each task, we use a programmable probabilistic firstorder logic to infer the dependency arc of a token in the sentence. In experiments, we show that the proposed model outperforms an off-the-shelf Stanford Chinese parser, as well as a strong MaltParser baseline that is trained on the same in-domain data. 1 Introduction Weibo, in particular Sina Weibo1, has attracted more than 30% of Internet users (Yang et al., 2012), making it one of the most popular social media services in the world. While Weibo posts are abundantly available, NLP techniques for analyzing Weibo posts have not been well-studied in the past. Syntactic analysis of Weibo is made difficult for three reasons: first, in the last few decades, Computational Linguistics researchers have primarily focused on building resources and tools using standard English newswire corpora2, and thus, 1http://en.wikipedia.org/wiki/Sina Weibo 2For example, Wall Street Journal articles are used for building the Penn Treebank (Marcus et al., 1993). there are fewe</context>
</contexts>
<marker>Yang, Liu, Yu, Yang, 2012</marker>
<rawString>Fan Yang, Yang Liu, Xiaohui Yu, and Min Yang. 2012. Automatic detection of rumor on sina weibo. In Proceedings of the ACM SIGKDD Workshop on Mining Data Semantics, page 13. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kun Yu</author>
<author>Daisuke Kawahara</author>
<author>Sadao Kurohashi</author>
</authors>
<title>Chinese dependency parsing with large scale automatically constructed case structures.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics-Volume 1,</booktitle>
<pages>1049--1056</pages>
<institution>Association for Computational Linguistics.</institution>
<contexts>
<context position="4698" citStr="Yu et al., 2008" startWordPosition="715" endWordPosition="718">that, a few years later, the CoNLL shared task launched a track for multilingual dependency parsing, which also included Chinese (Buchholz and Marsi, 2006; Nilsson et al., 2007). These shared tasks soon popularized Chinese dependency parsing by making datasets available, and there has been growing amount of literature since then (Zhang and Clark, 2008; Nivre et al., 2007; Sagae and Tsujii, 2007; Che et al., 2010; Carreras, 2007; Duan et al., 2007). Besides the CoNLL shared tasks, there are also many interesting studies on Chinese dependency parsing. For example, researchers have studied case (Yu et al., 2008) and morphological (Li and Zhou, 2012) structures for learning a Chinese dependency parser. Another direction is to perform joint learning and inference for POS tagging and dependency parsing (Li et al., 2011; Hatori et al., 2011; Li et al., 2011; Ma et al., 2012). In recent years, there has been growing interests in dependency arc prediction in Chinese (Che et al., 2014), and researchers have also investigated characterlevel Chinese dependency parsing (Zhang et al., 2014). However, even though the above methods all have merits, the results are reported only on standard newswire based Chinese </context>
</contexts>
<marker>Yu, Kawahara, Kurohashi, 2008</marker>
<rawString>Kun Yu, Daisuke Kawahara, and Sadao Kurohashi. 2008. Chinese dependency parsing with large scale automatically constructed case structures. In Proceedings of the 22nd International Conference on Computational Linguistics-Volume 1, pages 1049– 1056. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>A tale of two parsers: investigating and combining graphbased and transition-based dependency parsing using beam-search.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>562--571</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4435" citStr="Zhang and Clark, 2008" startWordPosition="672" endWordPosition="675">g has attracted many interests in the last fifteen years. Bikel and Chiang (2000; 2002) are among the first to use Penn Chinese Tree Bank for dependency parsing, where they adapted Xia’s head rules (Xia, 1999). An important milestone for Chinese dependency parsing is that, a few years later, the CoNLL shared task launched a track for multilingual dependency parsing, which also included Chinese (Buchholz and Marsi, 2006; Nilsson et al., 2007). These shared tasks soon popularized Chinese dependency parsing by making datasets available, and there has been growing amount of literature since then (Zhang and Clark, 2008; Nivre et al., 2007; Sagae and Tsujii, 2007; Che et al., 2010; Carreras, 2007; Duan et al., 2007). Besides the CoNLL shared tasks, there are also many interesting studies on Chinese dependency parsing. For example, researchers have studied case (Yu et al., 2008) and morphological (Li and Zhou, 2012) structures for learning a Chinese dependency parser. Another direction is to perform joint learning and inference for POS tagging and dependency parsing (Li et al., 2011; Hatori et al., 2011; Li et al., 2011; Ma et al., 2012). In recent years, there has been growing interests in dependency arc pre</context>
</contexts>
<marker>Zhang, Clark, 2008</marker>
<rawString>Yue Zhang and Stephen Clark. 2008. A tale of two parsers: investigating and combining graphbased and transition-based dependency parsing using beam-search. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 562–571. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Meishan Zhang</author>
<author>Yue Zhang</author>
<author>Wanxiang Che</author>
<author>Ting Liu</author>
</authors>
<title>Character-level chinese dependency parsing.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52th Annual Meeting of the Association for Computational Linguistics (ACL 2014),</booktitle>
<publisher>ACL.</publisher>
<location>Baltimore, MD, USA,</location>
<contexts>
<context position="5175" citStr="Zhang et al., 2014" startWordPosition="793" endWordPosition="796">red tasks, there are also many interesting studies on Chinese dependency parsing. For example, researchers have studied case (Yu et al., 2008) and morphological (Li and Zhou, 2012) structures for learning a Chinese dependency parser. Another direction is to perform joint learning and inference for POS tagging and dependency parsing (Li et al., 2011; Hatori et al., 2011; Li et al., 2011; Ma et al., 2012). In recent years, there has been growing interests in dependency arc prediction in Chinese (Che et al., 2014), and researchers have also investigated characterlevel Chinese dependency parsing (Zhang et al., 2014). However, even though the above methods all have merits, the results are reported only on standard newswire based Chinese Treebank (e.g. from People’s Daily (Liu et al., 2006)), and it is unclear how they would perform on Weibo data. To the best of our knowledge, together with the recent study on parsing tweets (Kong et al., 2014), we are among the first to study the problem of dependency parsing for social media text. 3 The Chinese Weibo Treebank We use the publicly available µtopia dataset (Ling et al., 2013) for dependency annotation. An interesting aspect of this Weibo dataset is that, be</context>
</contexts>
<marker>Zhang, Zhang, Che, Liu, 2014</marker>
<rawString>Meishan Zhang, Yue Zhang, Wanxiang Che, and Ting Liu. 2014. Character-level chinese dependency parsing. In Proceedings of the 52th Annual Meeting of the Association for Computational Linguistics (ACL 2014), Baltimore, MD, USA, June. ACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
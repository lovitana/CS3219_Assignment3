<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000046">
<title confidence="0.993563">
Ambiguity Resolution for Vt-N Structures in Chinese
</title>
<author confidence="0.999649">
Yu-Ming Hsieh1,2 Jason S. Chang2 Keh-Jiann Chen1
</author>
<affiliation confidence="0.988192">
1 Institute of Information Science, Academia Sinica, Taiwan
2 Department of Computer Science, National Tsing-Hua University, Taiwan
</affiliation>
<email confidence="0.9882295">
morris@iis.sinica.edu.tw, jason.jschang@gmail.com
kchen@iis.sinica.edu.tw
</email>
<sectionHeader confidence="0.998539" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999900103448276">
The syntactic ambiguity of a transitive
verb (Vt) followed by a noun (N) has
long been a problem in Chinese parsing.
In this paper, we propose a classifier to
resolve the ambiguity of Vt-N structures.
The design of the classifier is based on
three important guidelines, namely,
adopting linguistically motivated features,
using all available resources, and easy in-
tegration into a parsing model. The lin-
guistically motivated features include
semantic relations, context, and morpho-
logical structures; and the available re-
sources are treebank, thesaurus, affix da-
tabase, and large corpora. We also pro-
pose two learning approaches that resolve
the problem of data sparseness by auto-
parsing and extracting relative
knowledge from large-scale unlabeled
data. Our experiment results show that
the Vt-N classifier outperforms the cur-
rent PCFG parser. Furthermore, it can be
easily and effectively integrated into the
PCFG parser and general statistical pars-
ing models. Evaluation of the learning
approaches indicates that world
knowledge facilitates Vt-N disambigua-
tion through data selection and error cor-
rection.
</bodyText>
<sectionHeader confidence="0.99963" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998823846153846">
In Chinese, the structure of a transitive verb (Vt)
followed by a noun (N) may be a verb phrase
(VP), a noun phrase (NP), or there may not be a
dependent relation, as shown in (1) below. In
general, parsers may prefer VP reading because a
transitive verb followed by a noun object is nor-
mally a VP structure. However, Chinese verbs
can also modify nouns without morphological
inflection, e.g., 養殖/farming 池/pond. Conse-
quently, parsing Vt-N structures is difficult be-
cause it is hard to resolve such ambiguities with-
out prior knowledge. The following are some
typical examples of various Vt-N structures:
</bodyText>
<figure confidence="0.84473025">
1)
解決/solve 問題/problem 4 VP
解決/solving 方案/method 4 NP
解決/solve 人類/mankind (問題/problem)4None
</figure>
<bodyText confidence="0.999375631578947">
To find the most effective disambiguation fea-
tures, we need more information about the Vt-N
4 NP construction and the semantic relations
between Vt and N. Statistical data from the Sini-
ca Treebank (Chen et al., 2003) indicates that
58% of Vt-N structures are verb phrases, 16%
are noun phrases, and 26% do not have any de-
pendent relations. It is obvious that the semantic
relations between a Vt-N structure and its con-
text information are very important for differen-
tiating between dependent relations. Although
the verb-argument relation of VP structures is
well understood, it is not clear what kind of se-
mantic relations result in NP structures. In the
next sub-section, we consider three questions:
What sets of nouns accept verbs as their modifi-
ers? Is it possible to identify the semantic types
of such pairs of verbs and nouns? What are their
semantic relations?
</bodyText>
<subsectionHeader confidence="0.997421">
1.1 Problem Analysis
</subsectionHeader>
<bodyText confidence="0.989279666666667">
Analysis of the instances of NP(Vt-N) structures
in the Sinica Treebank reveals the following four
types of semantic structures, which are used in
the design of our classifier.
Type 1. Telic(Vt) + Host(N): Vt denotes the
telic function (purpose) of the head noun N, e.g.,
</bodyText>
<page confidence="0.955778">
928
</page>
<note confidence="0.923002">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 928–937,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.994439418918919">
Ur)ITI 5t /research T_ A. /tool; 9 igq /explore fA
/machine; N/gamble M/house; a34/search �
J/program. The telic function must be a salient
property of head nouns, such as tools, buildings,
artifacts, organizations and people. To identify
such cases, we need to know the types of nouns
which take telic function as their salient property.
Furthermore, many of the nouns are monosyl-
labic words, such as _R_/people, a/instruments,
M/machines.
Type 2. Host-Event(Vt) + Attribute(N):
Head nouns are attribute nouns that denote the
attributes of the verb, e.g., �ff5t/research )j A
/method (method of research); JA�/attack %ftllj�
/strategy (attacking strategy); �A/write Ng
/context (context of writing); N/gamble ffl/rule
(gambling rules). An attribute noun is a special
type of noun. Semantically, attribute nouns de-
note the attribute types of objects or events, such
as weight, color, method, and rule. Syntactically,
attribute nouns do not play adjectival roles (Liu,
2008). By contrast, object nouns may modify
nouns. The number of attributes for events is
limited. If we could discover all event-attribute
relations, then we can solve this type of construc-
tion.
Type 3. Agentive + Host: There is only a lim-
ited number of such constructions and the results
of the constructions are usually ambiguous, e.g.,
0�/fried rice (NP), Fq /shouting sound. The
first example also has the VP reading.
Type 4. Apposition + Affair: Head nouns are
event nouns and modifiers are verbs of apposi-
tion events, e.g. L9�/collide �&amp;/accident, Q&amp;
IX /destruct A S1 /movement, &apos; 10&apos;[R /hate T7 �
/behavior. There is finite number of event nouns.
Furthermore, when we consider verbal modi-
fiers, we find that verbs can play adjectival roles
in Chinese without inflection, but not all verbs
play adjectival roles. According to Chang et al.
(2000) and our observations, adjectival verbs are
verbs that denote event types rather than event
instances; that is, they denote a class of events
which that are concepts in an upper-level ontolo-
gy. One important characteristic of adjectival
verbs is that they have conjunctive morphologi-
cal structures, i.e., the words are conjunct with
two nearly synonymous verbs, e.g., �ff/study 5t
/search (research), 9 /explore igq /detect (ex-
plore), and a3/search 4/find (search). Therefore,
we need a morphological classifier that can de-
tect the conjunctive morphological structure of a
verb by checking the semantic parity of two
morphemes of the verb.
Based on our analysis, we designed a Vt-N
classifier that incorporates the above features to
solve the problem. However, there is a data
sparseness problem because of the limited size of
the current Treebank. In other words, Treebank
cannot provide enough training data to train a
classifier properly. To resolve the problem, we
should mine useful information from all availa-
ble resources.
The remainder of this paper is organized as
follows. Section 2 provides a review of related
works. In Section 3, we describe the disambigua-
tion model with our selected features, and intro-
duce a strategy for handling unknown words. We
also propose a learning approach for a large-
scale unlabeled corpus. In Section 4, we report
the results of experiments conducted to evaluate
the proposed Vt-N classifier on different feature
combinations and learning approaches. Section 5
contains our concluding remarks.
</bodyText>
<sectionHeader confidence="0.999879" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999150233333333">
Most works on V-N structure identification focus
on two types of relation classification: modifier-
head relations and predicate-object relations (Wu,
2003; Qiu, 2005; Chen, 2008; Chen et al., 2008;
Yu et al., 2008). They exclude the independent
structure and conjunctive head-head relation, but
the cross-bracket relation does exist between two
adjacent words in real language. For example, if
“遍佈/all over 世界/world ” was included in the
short sentence “遍佈/all over 世界/world 各國
/countries”, it would be an independent structure.
A conjunctive head-head relation between a verb
and a noun is rare. However, in the sentence “服
務 設備 都 甚 周到” (Both service and equip-
ment are very thoughtful.), there is a conjunctive
head-head relation between the verb 服 務
/service and the noun 設備/equipment. Therefore,
we use four types of relations to describe the V-
N structures in our experiments. The symbol
‘H/X’ denotes a predicate-object relation; ‘X/H’
denotes a modifier-head relation; ‘H/H’ denotes
a conjunctive head-head relation; and ‘X/X’ de-
notes an independent relation.
Feature selection is an important task in V-N
disambiguation. Hence, a number of studies have
suggested features that may help resolve the am-
biguity of V-N structures (Zhao and Huang, 1999;
Sun and Jurafsky, 2003; Chiu et al., 2004; Qiu,
2005; Chen, 2008). Zhao and Huang used lexi-
cons, semantic knowledge, and word length in-
</bodyText>
<page confidence="0.997611">
929
</page>
<bodyText confidence="0.999992732394366">
formation to increase the accuracy of identifica-
tion. Although they used the Chinese thesaurus
CiLin (Mei et al., 1983) to derive lexical seman-
tic knowledge, the word coverage of CiLin is
insufficient. Moreover, none of the above papers
tackle the problem of unknown words. Sun and
Jurafsky exploit the probabilistic rhythm feature
(i.e., the number of syllables in a word or the
number of words in a phrase) in their shallow
parser. Their results show that the feature im-
proves the parsing performance, which coincides
with our analysis in Section 1.1. Chiu et al.’s
study shows that the morphological structure of
verbs influences their syntactic behavior. We
follow this finding and utilize the morphological
structure of verbs as a feature in the proposed Vt-
N classifier. Qiu’s approach uses an electronic
syntactic dictionary and a semantic dictionary to
analyze the relations of V-N phrases. However,
the approach suffers from two problems: (1) low
word coverage of the semantic dictionary and (2)
the semantic type classifier is inadequate. Finally,
Chen proposed an automatic VN combination
method with features of verbs, nouns, context,
and the syllables of words. The experiment re-
sults show that the method performs reasonably
well without using any other resources.
Based on the above feature selection methods,
we extract relevant knowledge from Treebank to
design a Vt-N classifier. However we have to
resolve the common problem of data sparseness.
Learning knowledge by analyzing large-scale
unlabeled data is necessary and proved useful in
previous works (Wu, 2003; Chen et al., 2008; Yu
et al., 2008). Wu developed a machine learning
method that acquires verb-object and modifier-
head relations automatically. The mutual infor-
mation scores are then used to prune verb-noun
whose scores are below a certain threshold. The
author found that accurate identification of the
verb-noun relation improved the parsing perfor-
mance by 4%. Yu et al. learned head-modifier
pairs from parsed data and proposed a head-
modifier classifier to filter the data. The filtering
model uses the following features: a PoS-tag pair
of the head and the modifier; the distance be-
tween the head and the modifier; and the pres-
ence or absence of punctuation marks (e.g.,
commas, colons, and semi-colons) between the
head and the modifier. Although the method im-
proves the parsing performance by 2%, the filter-
ing model obtains limited data; the recall rate is
only 46.35%. The authors also fail to solve the
problem of Vt-N ambiguity.
Our review of previous works and the obser-
vations in Section 1.1 show that lexical words,
semantic information, the syllabic length of
words, neighboring PoSs and the knowledge
learned from large-scale data are important for
Vt-N disambiguation. We consider more features
for disambiguating Vt-N structures than previous
studies. For example, we utilize (1) four relation
classification in a real environment, including
‘X/H’, ‘H/X’, ‘X/X’ and ‘H/H’ relations; (2) un-
known word processing of Vt-N words (includ-
ing semantic type predication and morph-
structure predication); (3) unsupervised data se-
lection (a simple and effective way to extend
knowledge); and (4) supervised knowledge cor-
rection, which makes the extracted knowledge
more useful.
</bodyText>
<sectionHeader confidence="0.965732" genericHeader="method">
3 Design of the Disambiguation Model
</sectionHeader>
<bodyText confidence="0.9999725">
The disambiguation model is a Vt-N relation
classifier that classifies Vt-N relations into ‘H/X’
(predicate-object relations), ‘X/H’ (modifier-
head relations), ‘H/H’ (conjunctive head-head
relations), or ‘X/X’ (independent relations). We
use the Maximum Entropy toolkit (Zhang, 2004)
to construct the classifier. The advantage of us-
ing the Maximum Entropy model is twofold: (1)
it has the flexibility to adjust features; and (2) it
provides the probability values of the classifica-
tion, which can be easily integrated into our
PCFG parsing model.
In the following sections, we discuss the de-
sign of our model for feature selection and ex-
traction, unknown word processing, and world
knowledge learning.
</bodyText>
<subsectionHeader confidence="0.999815">
3.1 Feature Selection and Extraction
</subsectionHeader>
<bodyText confidence="0.999948352941176">
We divide the selected features into five groups:
PoS tags of Vt and N, PoS tags of the context,
words, semantics, and additional information.
Table 1 shows the feature types and symbol nota-
tions. We use symbols of t1 and t2 to denote the
PoS of Vt and N respectively. The context fea-
ture is neighboring PoSs of Vt and N: the sym-
bols of t-2 and t-1 represent its left PoSs, and the
symbol t3 and t4 represent its right PoSs. The se-
mantic feature is the lexicon’s semantic type ex-
tracted from E-HowNet sense expressions
(Huang et al., 2008). For example, the E-
HowNet expression of “ * 輛 /vehicles” is
{LandVehicle |* :quantity={mass |眾 }}, so its
semantic type is {LandVehicle|*}. We discuss
the model’s performance with different feature
combinations in Section 4.
</bodyText>
<page confidence="0.98929">
930
</page>
<table confidence="0.999102538461539">
Feature Feature Description
PoS PoS of Vt and N
t1; t2
Context Neighboring PoSs
t-2; t-1; t3; t4
Word Lexical word
w1; w2
Semantic Semantic type of word
st1; st2
Additional Morphological structure of verb
Information Vmorph
Syllabic length of noun
Nlen
</table>
<tableCaption confidence="0.99963">
Table 1. The features used in the Vt-N classifier
</tableCaption>
<bodyText confidence="0.960854076923077">
The example in Figure 1 illustrates feature la-
beling of a Vt-N structure. First, an instance of a
Vt-N structure is identified from Treebank. Then,
we assign the semantic type of each word with-
out considering the problem of sense ambiguity
for the moment. This is because sense ambigui-
ties are partially resolved by PoS tagging, and
the general problem of sense disambiguation is
beyond the scope of this paper. Furthermore,
Zhao and Huang (1999) demonstrated that the
retained ambiguity does not have an adverse im-
pact on identification. Therefore, we keep the
ambiguous semantic type for future processing.
zhe zaochen xuexi zhongwen DE fongchao
this cause learn Chinese trend
“This causes the trend of learning Chinese.”
Figure 1. An example of a tree with a Vt-N struc-
ture
Table 2 shows the labeled features for “學習
/learn 中文/Chinese” in Figure 1. The column x
and y describe relevant features in “MW/learn”
and “rpm/Chinese” respectively. Some features
are not explicitly annotated in the Treebank, e.g.,
the semantic types of words and the morphologi-
cal structure of verbs. We propose labeling
methods for them in the next sub-section.
</bodyText>
<table confidence="0.999615375">
Feature Type x y
Word w1=學習 w2=中文
PoS t1=VC t2=Na
Semantic st1=study|學習 st2=language|語言
Context t-2=Nep; t-1=VK; t3=DE; t4=Na
Additional Vmorph=VV Nlen=2
Information
Relation Type rt = H/X
</table>
<tableCaption confidence="0.945293">
Table 2. The feature labels of Vt-N pair in Figure
1
</tableCaption>
<subsectionHeader confidence="0.998586">
3.2 Unknown Word Processing
</subsectionHeader>
<bodyText confidence="0.996273607142857">
In Chinese documents, 3% to 7% of the words
are usually unknown (Sproat and Emerson,
2003). By ‘unknown words’, we mean words not
listed in the dictionary. More specifically, in this
paper, unknown words means words without se-
mantic type information (i.e., E-HowNet expres-
sions) and verbs without morphological structure
information. Therefore, we propose a method for
predicting the semantic types of unknown words,
and use an affix database to train a morph-
structure classifier to derive the morphological
structure of verbs.
Morph-Structure Predication of Verbs: We
use data analyzed by Chiu et al. (2004) to devel-
op a classifier for predicating the morphological
structure of verbs. There are four types of mor-
phological structures for verbs: the coordinating
structure (VV), the modifier-head structure (AV),
the verb-complement structure (VR), and the
verb-object structure (VO). To classify verbs
automatically, we incorporate three features in
the proposed classifier, namely, the lexeme itself,
the prefix and the suffix, and the semantic types
of the prefix and the suffix. Then, we use train-
ing data from the affix database to train the clas-
sifier. Table 3 shows an example of the unknown
verb “ 傳 播 到 /disseminate” and the morph-
structure classifier shows that it is a ‘VR’ type.
</bodyText>
<table confidence="0.99933725">
Feature Feature Description
Word=傳播到 Lexicon
PW=傳播 Prefix word
PWST={disseminate|傳播} Semantic Type of
Prefix Word 傳播
SW=到 Suffix Word
SWST={Vachieve|達成} Semantic Type of
Suffix Word 到
</table>
<tableCaption confidence="0.9923895">
Table 3. An example of an unknown verb and
feature templates for morph-structure predication
</tableCaption>
<page confidence="0.995136">
931
</page>
<bodyText confidence="0.999580565217391">
Semantic Type Provider: The system ex-
ploits WORD, PoS, affix and E-HowNet infor-
mation to obtain the semantic types of words (see
Figure 2). If a word is known and its PoS is giv-
en, we can usually find its semantic type by
searching the E-HowNet database. For an un-
known word, the semantic type of its head mor-
pheme is its semantic type; and the semantic type
of the head morpheme is obtained from E-
HowNet1. For example, the unknown word “傳
播 到 /disseminate”, its prefix word is “ 傳 播
/disseminate” and we learn that its semantic type
is {disseminate|傳播} from E-HowNet. There-
fore, we assign {disseminate|傳播} as the se-
mantic type of “ 傳 播 到 /disseminate”. If the
word or head morpheme does not exist in the
affix database, we assign a general semantic type
based on its PoS, e.g., nouns are {thing|萬物}
and verbs are {act|行動}. In this matching pro-
cedure, we may encounter multiple matching
data of words and affixes. Our strategy is to keep
the ambiguous semantic type for future pro-
cessing.
</bodyText>
<table confidence="0.917152347826087">
Input: WORD, PoS
Output: Semantic Type (ST)
procedure STP(WORD, PoS)
(* Initial Step *)
ST := null;
(* Step 1: Known word *)
if WORD already in E-HowNet then
ST := EHowNet(WORD, PoS);
else if WORD in Affix database then
ST := EHowNet(affix of WORD, PoS);
(* Step 2 : Unknown word *)
if ST is null and PoS is ‘Vt’ then
ST := EHowNet(prefix of WORD, PoS);
else if ST is null and PoS is ‘N’ then
ST := EHowNet(suffix of WORD, PoS);
(* Step 3 : default *)
if ST is null and PoS is ‘Vt’ then
ST := ‘act|行動’;
else if ST is null and PoS is ‘N’ then
ST := ‘thing|萬物’
(* Finally *)
STP := ST;
end;
</table>
<figureCaption confidence="0.9914975">
Figure 2. The Pseudo-code of the Semantic Type
Predication Algorithm.
</figureCaption>
<bodyText confidence="0.661822">
1 The E-HowNet function in Figure 2 will return a null ST
value where words do not exist in E-HowNet or Affix data-
base.
</bodyText>
<subsectionHeader confidence="0.997808">
3.3 Learning World Knowledge
</subsectionHeader>
<bodyText confidence="0.999986384615384">
Based on the features discussed in the previous
sub-section, we extract prior knowledge from
Treebank to design the Vt-N classifier. However,
the training suffers from the data sparseness
problem. Furthermore most ambiguous Vt-N
relations are resolved by common sense
knowledge that makes it even harder to construct
a well-trained system. An alternative way to ex-
tend world knowledge is to learn from large-
scale unlabeled data (Wu, 2003; Chen et al.,
2008; Yu et al., 2008). However, the unsuper-
vised approach accumulates errors caused by
automatic annotation processes, such as word
segmentation, PoS tagging, syntactic parsing,
and semantic role assignment. Therefore, how to
extract useful knowledge accurately is an im-
portant issue.
To resolve the error accumulation problem, we
propose two methods: unsupervised NP selection
and supervised error correction. The NP selec-
tion method exploits the fact that an intransitive
verb followed by a noun can only be interpreted
as an NP structure, not a VP structure. It is easy
to find such instances with high precision by
parsing a large corpus. Based on the selection
method, we can extend contextual knowledge
about NP(V+N) and extract nouns that take ad-
jectival verbs as modifiers. The error correction
method involves a small amount of manual edit-
ing in order to make the data more useful and
reduce the number of errors in auto-extracted
knowledge. The rationale is that, in general, high
frequency Vt-N word-bigram is either VP or NP
without ambiguity. Therefore, to obtain more
accurate training data, we simply classify each
high frequency Vt-N word bigram into a unique
correct type without checking all of its instances.
We provide more detailed information about the
method in Section 4.3.
</bodyText>
<sectionHeader confidence="0.998024" genericHeader="method">
4 Experiments and Results
</sectionHeader>
<subsectionHeader confidence="0.975696">
4.1 Experimental Setting
</subsectionHeader>
<bodyText confidence="0.9998926">
We classify Vt-N structures into four types of
syntactic structures by using the bracketed in-
formation (tree structure) and dependency rela-
tion (head-modifier) to extract the Vt-N relations
from treebank automatically. The resources used
in the experiments as follows.
Treebank: The Sinica Treebank contains
61,087 syntactic tree structures with 361,834
words. We extracted 9,017 instances of Vt-N
structures from the corpus. Then, we randomly
</bodyText>
<page confidence="0.994436">
932
</page>
<bodyText confidence="0.999632681818182">
selected 1,000 of the instances as test data and
used the remainder (8,017 instances) as training
data. Labeled information of word segmentation
and PoS-tagging were retained and utilized in the
experiments.
E-HowNet: E-HowNet contains 99,525 lexi-
cal semantic definitions that provide information
about the semantic type of words. We also im-
plement the semantic type predication algorithm
in Figure 2 to generate the semantic types of all
Vt and N words, including unknown words.
Affix Data: The database includes 13,287 ex-
amples of verbs and 27,267 examples of nouns,
each example relates to an affix. The detailed
statistics of the verb morph-structure categoriza-
tion are shown in Table 4. The data is used to
train a classifier to predicate the morph-structure
of verbs. We found that verbs with a conjunctive
structure (VV) are more likely to play adjectival
roles than the other three types of verbs. The
classifier achieved 87.88% accuracy on 10-fold
cross validation of the above 13,287 verbs.
</bodyText>
<table confidence="0.996776333333333">
VV VR AV VO
Prefix 920 2,892 904 662
Suffix 439 7,388 51 31
</table>
<tableCaption confidence="0.9018775">
Table 4. The statistics of verb morph-structure
categorization
</tableCaption>
<figureCaption confidence="0.958651428571429">
Large Corpus: We used a Chinese parser to
analyze sentence structures automatically. The
auto-parsed tree structures are used in Experi-
ment 2 (described in the Sub-section 4.3). We
obtained 1,262,420 parsed sentences and derived
237,843 instances of Vt-N structure as our da-
taset (called as ASBC).
</figureCaption>
<subsectionHeader confidence="0.936688">
4.2 Experiment 1: Evaluation of the Vt-N
Classifier
</subsectionHeader>
<bodyText confidence="0.999188">
In this experiment, we used the Maximum En-
tropy Toolkit (Zhang, 2004) to develop the Vt-N
classifier. Based on the features discussed in Sec-
tion 3.1, we designed five models to evaluate the
classifier’s performance on different feature
combinations.
The features and used in each model are de-
scribed below. The feature values shown in
brackets refer to the example in Figure 1.
</bodyText>
<listItem confidence="0.99055975">
• M1 is the baseline model. It uses PoS-tag
pairs as features, such as (t1=VC, t2=Na).
• M2 extends the M1 model by adding con-
text features of (t-1=VK, t1=VC), (t2=Na,
</listItem>
<equation confidence="0.7884715">
t3=DE), (t-2=Nep, t-1=VK, t1=VC), (t2=Na,
t3=DE, t4=Na) and (t-1=VK, t3=DE).
</equation>
<listItem confidence="0.993616">
• M3 extends the M2 model by adding lexi-
con features of (w1=學習, t1=VK, w2=中
文, t2=Na), (w1=學習, w2=中文), (w1=學
習) and (w2=中文).
• M4 extends the M3 model by adding se-
mantic features of (st1=study|學習, t1=VK ,
st2=language|語言, t2=Na), (st1=study|學
習 , t1=VK) and (st2=language |語 言 ,
t2=Na).
• M5 extends the M4 model by adding two
features: the morph-structure of verbs; and
the syllabic length of nouns
(Vmorph=‘VV’) and (Nlen=2).
</listItem>
<bodyText confidence="0.999960590909091">
Table 5 shows the results of using different
feature combinations in the models. The symbol
P1(%) is the 10-fold cross validation accuracy of
the training data, and the symbol P2(%) is the
accuracy of the test data. By adding contextual
features, the accuracy rate of M2 increases from
59.10% to 72.30%. The result shows that contex-
tual information is the most important feature
used to disambiguate VP, NP and independent
structures. The accuracy of M2 is approximately
the same as the result of our PCFG parser be-
cause both systems use contextual information.
By adding lexical features (M3), the accuracy
rate increases from 72.30% to 80.20%. For se-
mantic type features (M4), the accuracy rate in-
creases from 80.20% to 81.90%. The 1.7% in-
crease in the accuracy rate indicates that seman-
tic generalization is useful. Finally, in M5, the
accuracy rate increases from 81.90% to 83.00%.
The improvement demonstrates the benefits of
using the verb morph-structure and noun length
features.
</bodyText>
<sectionHeader confidence="0.726805" genericHeader="method">
Models Feature for Vt-N P1(%) P2(%)
</sectionHeader>
<equation confidence="0.892958625">
M1 (t1,t2) 61.94 59.10
M2 + (t-1,t1) (t2,t3) (t-2,t- 76.59 72.30
1,t1) (t2,t3,t4) (t-1,t3)
M3 + (w1,t1,w2,t2) (w1,w2) 83.55 80.20
(w2) (w1)
M4 + (st1,t1,st2,t2) (st1,t1) 84.63 81.90
(st2, t2)
M5 + (Vmorph) (Nlen) 85.01 83.00
</equation>
<tableCaption confidence="0.9148275">
Table 5. The results of using different feature
combinations
</tableCaption>
<page confidence="0.998425">
933
</page>
<bodyText confidence="0.9999930625">
Next, we consider the influence of unknown
words on the Vt-N classifier. The statistics shows
that 17% of the words in Treebank lack semantic
type information, e.g., er±/StayIn, flAf7/fill, g�
Lb/posted, and iS§f/tied. The accuracy of the
Vt-N classifier declines by 0.7% without seman-
tic type information for unknown words. In other
words, lexical semantic information improves the
accuracy of the Vt-N classifier. Regarding the
problem of unknown morph-structure of words,
we observe that over 85% of verbs with more
than 2 characters are not found in the affix data-
base. If we exclude unknown words, the accura-
cy of the Vt-N prediction decreases by 1%.
Therefore, morph-structure information has a
positive effect on the classifier.
</bodyText>
<subsectionHeader confidence="0.695987">
4.3 Experiment 2: Using Knowledge Ob-
</subsectionHeader>
<bodyText confidence="0.964331666666667">
tained from Large-scale Unlabeled Data
by the Selection and Correction Meth-
ods.
In this experiment, we evaluated the two
methods discussed in Section 3, i.e., unsuper-
vised NP selection and supervised error correc-
tion. We applied the data selection method (i.e.,
distance=1, with an intransitive verb (Vi) fol-
lowed by an object noun (Na)) to select 46,258
instances from the ASBC corpus and compile a
dataset called Treebank+ASBC-Vi-N. Table 6
shows the performance of model 5 (M5) on the
training data derived from Treebank and Tree-
bank+ASBC-Vi-N. The results demonstrate that
learning more nouns that accept verbal modifiers
improves the accuracy.
Treebank+ Treebank
ASBC-Vi-N
size of training 46,258 8,017
instances
M5 - P2(%) 83.90 83.00
Table 6. Experiment results on the test data for
various knowledge sources
We had also try to use the auto-parsed results
of the Vt-N structures from the ASBC corpus as
supplementary training data for train M5. It de-
grades the model’s performance by too much
error when using the supplementary training data.
To resolve the problem, we utilize the supervised
error correction method, which manually correct
errors rapidly because high frequency instances
(w1, w2) rarely have ambiguous classifications in
different contexts. So we designed an editing tool
to correct errors made by the parser in the classi-
fication of high frequency Vt-N word pairs. After
the manual correction operation, which takes 40
man-hours, we assign the correct classifications
(w1, t1, w2, t2, rt) for 2,674 Vt-N structure types
which contains 10,263 instances to creates the
ASBC+Correction dataset. Adding the corrected
data to the original training data increases the
precision rate to 88.40% and reduces the number
of errors by approximately 31.76%, as shown in
the Treebank+ASBC+Correction column of Ta-
ble 7.
</bodyText>
<table confidence="0.9958082">
Treebank+ Treebank+ Treebank
ASBC+Correction ASBC-Vi-N
size of train- 56,521 46,258 8,017
ing instances
M5 - P2(%) 88.40 83.90 83.00
</table>
<tableCaption confidence="0.8858985">
Table 7. Experiment results of classifiers with
different training data
</tableCaption>
<bodyText confidence="0.999721923076923">
We also used the precision and recall rates to
evaluate the performance of the models on each
type of relation. The results are shown in Table 8.
Overall, the Treebank+ASBC+Correction meth-
od achieves the best performance in terms of the
precision rate. The results for Treebank+ASBC-
Vi-N show that the unsupervised data selection
method can find some knowledge to help identi-
fy NP structures. In addition, the proposed mod-
els achieve better precision rates than the PCFG
parser. The results demonstrate that using our
guidelines to design a disambiguation model to
resolve the Vt-N problem is successful.
</bodyText>
<table confidence="0.999504444444444">
H/X X/H X/X
Treebank R(%) 91.11 67.90 74.62
P(%) 84.43 78.57 81.86
Treebank+ R(%) 91.00 72.22 71.54
ASBC-Vi-N P(%) 84.57 72.67 85.71
Treebank+ R(%) 98.62 60.49 83.08
ASBC+Correction P(%) 86.63 88.29 93.51
PCFG R(%) 90.54 23.63 80.21
P(%) 78.24 73.58 75.00
</table>
<tableCaption confidence="0.992468">
Table 8. Performance comparison of different
classification models.
</tableCaption>
<subsectionHeader confidence="0.6978675">
4.4 Experiment 3: Integrating the Vt-N
classifier with the PCFG Parser
</subsectionHeader>
<bodyText confidence="0.6101905">
Identifying Vt-N structures correctly facilitates
statistical parsing, machine translation, infor-
</bodyText>
<page confidence="0.996422">
934
</page>
<bodyText confidence="0.999924272727273">
mation retrieval, and text classification. In this
experiment, we develop a baseline PCFG parser
based on feature-based grammar representation
by Hsieh et al. (2012) to find the best tree struc-
tures (T) of a given sentence (S). The parser then
selects the best tree according to the evaluation
score Score(T,S) of all possible trees. If there are
n PCFG rules in the tree T, the Score(T,S) is the
accumulation of the logarithmic probabilities of
the i-th grammar rule (RPi). Formula 1 shows the
baseline PCFG parser.
</bodyText>
<equation confidence="0.968824666666667">
n
Score(T,S)= ∑(RPi) (1)
i 1
</equation>
<bodyText confidence="0.999937909090909">
The Vt-N models can be easily integrated into
the PCFG parser. Formula 2 represents the inte-
grated structural evaluation model. We combine
RPi and VtNPi with the weights w1 and w2 re-
spectively, and set the value of w2 higher than
that of w1. VtNPi is the probability produced by
the Vt-N classifier for the type of the relation
between Vt-N bigram determined by the PCFG
parsing. The classifier is triggered when a [Vt, N]
structure is encountered; otherwise, the Vt-N
model is not processed.
</bodyText>
<equation confidence="0.9799766">
n
Score T S
( , ) ( 1
= ∑= w × RPi + w2 × VtNPi) (2)
i 1
</equation>
<bodyText confidence="0.995734090909091">
The results of evaluating the parsing model in-
corporated with the Vt-N classifier (see Formula
2) are shown in Table 9 and Table 10. The P2 is
the accuracy of Vt-N classification on the test
data. The bracketed f-score (BF2) is the parsing
performance metric. Based on these results, the
integrated model outperforms the PCFG parser in
terms of Vt-N classification. Because the Vt-N
classifier only considers sentences that contain
Vt-N structures, it does not affect the parsing
accuracies of other sentences.
</bodyText>
<table confidence="0.3098125">
P2(%) 80.68 77.09
BF(%) 83.64 82.80
</table>
<tableCaption confidence="0.9707765">
Table 9. The performance of the PCFG parser
with and without model M5 from Treebank.
</tableCaption>
<footnote confidence="0.983258">
2 The evaluation formula is (BP*BR*2) / (BP+BR), where
BP is the precision and BR is the recall.
</footnote>
<note confidence="0.5933805">
P2(%) 87.88 77.09
BF(%) 84.68 82.80
</note>
<tableCaption confidence="0.878261">
Table 10. The performance of the PCFG parser
with and without model M5 from Tree-
bank+ASBC+Correction data set.
</tableCaption>
<subsectionHeader confidence="0.942302">
4.5 Experiment 4: Comparison of Various
Chinese Parsers
</subsectionHeader>
<bodyText confidence="0.999800923076923">
In this experiment, we give some comparison
results in various parser: ‘PCFG Parser’ (base-
line), ‘CDM Parser’ (Hsieh et al., 2012), and
‘Berkeley Parser’ (Petrov et al., 2006). The CDM
parser achieves the best score in Traditional Chi-
nese Parsing task of SIGHAN Bake-offs 2012
(Tseng et al., 2012). Petrov’s parser (as Berkeley,
version is 2009 1.1) is the best PCFG parser for
non-English language and it is an open source. In
our comparison, we use the same training data
for training models and parse the same test da-
taset based on the gold standard word segmenta-
tion and PoS tags. We have already discussed the
PCFG parser in Section 4.4. As for CDM parser,
we retrain relevant model in our experiments.
And since Berkeley parser take different tree
structure (Penn Treebank format), we transform
the experimental data to Berkeley CoNLL format
and re-train a new model with parameters “-
treebank CHINESE -SMcycles 4” 3 from training
data. Moreover we use “-useGoldPOS” parame-
ters to parse test data and further transform them
to Sinica Treebank style from the Berkeley par-
ser’s results. The different tree structure formats
of Sinica Treebank and Penn Treebank are as
follow:
</bodyText>
<table confidence="0.698531833333333">
Sinica Treebank:
S(NP(Head:Nh:他們)|Head:VC:散播
|NP(Head:Na:熱情))
Penn Treebank:
( (S (NP (Head:Nh (Nh 他們))) (Head:VC
(VC 散播)) (NP (Head:Na (Na 熱情)))))
</table>
<bodyText confidence="0.9996172">
The evaluation results on the testing data, i.e.
in P2 metric, are as follows. The accuracy of
PCFG parser is 77.09%; CDM parser reaches
78.45% of accuracy; and Berkeley parser is
70.68%. The results show that the problem of Vt-
</bodyText>
<footnote confidence="0.962539">
3 The “-treebank CHINESE -SMcycles 4” is the best train-
ing parameter in Traditional Chinese Parsing task of
SIGHAN Bake-offs 2012.
</footnote>
<equation confidence="0.819712166666667">
PCFG +
M5 (Treebank)
PCFG
PCFG +
M5 (Treebank+ASBC+Correction)
PCFG
</equation>
<page confidence="0.994523">
935
</page>
<bodyText confidence="0.999943375">
N cannot be well solved by any general parser
including CDM parser and Berkeley’s parser. It
is necessary to have a different approach aside
from the general model. So we set the target for a
better model for Vt-N classification which can be
easily integrated into the existing parsing model.
So far our best model achieved the P2 accuracy
of 87.88%.
</bodyText>
<sectionHeader confidence="0.985215" genericHeader="method">
5 Concluding Remarks
</sectionHeader>
<bodyText confidence="0.999988545454545">
We have proposed a classifier to resolve the am-
biguity of Vt-N structures. The design of the
classifier is based on three important guidelines,
namely, adopting linguistically motivated fea-
tures, using all available resources, and easy in-
tegration into parsing model. After analyzing the
Vt-N structures, we identify linguistically moti-
vated features, such as lexical words, semantic
knowledge, the morphological structure of verbs,
neighboring parts-of-speech, and the syllabic
length of words. Then, we design a classifier to
verify the usefulness of each feature. We also
resolve the technical problems that affect the
prediction of the semantic types and morph-
structures of unknown words. In addition, we
propose a framework for unsupervised data se-
lection and supervised error correction for learn-
ing more useful knowledge. Our experiment re-
sults show that the proposed Vt-N classifier sig-
nificantly outperforms the PCFG Chinese parser
in terms of Vt-N structure identification. Moreo-
ver, integrating the Vt-N classifier with a parsing
model improves the overall parsing performance
without side effects.
In our future research, we will exploit the pro-
posed framework to resolve other parsing diffi-
culties in Chinese, e.g., N-N combination. We
will also extend the Semantic Type Predication
Algorithm (Figure 2) to deal with all Chinese
words. Finally, for real world knowledge learn-
ing, we will continue to learn more useful
knowledge by auto-parsing to improve the pars-
ing performance.
</bodyText>
<sectionHeader confidence="0.999332" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9996905">
We thank the anonymous reviewers for their val-
uable comments. This work was supported by
National Science Council under Grant NSC99-
2221-E-001-014-MY3.
</bodyText>
<sectionHeader confidence="0.997717" genericHeader="references">
Reference
</sectionHeader>
<reference confidence="0.99956601754386">
Li-li Chang, Keh-Jiann Chen, and Chu-Ren Huang.
2000. Alternation Across Semantic Fields: A Study
on Mandarin Verbs of Emotion. Internal Journal of
Computational Linguistics and Chinese Language
Processing (IJCLCLP), 5(1):61-80.
Keh-Jiann Chen, Chu-Ren Huang, Chi-Ching Luo,
Feng-Yi Chen, Ming-Chung Chang, Chao-Jan
Chen, , and Zhao-Ming Gao. 2003. Sinica Tree-
bank: Design Criteria, Representational Issues and
Implementation. In (Abeille 2003) Treebanks:
Building and Using Parsed Corpora, pages 231-
248. Dordrecht, the Netherlands: Kluwer.
Li-jiang Chen. 2008. Autolabeling of VN Combina-
tion Based on Multi-classifier. Journal of Comput-
er Engineering, 34(5):79-81.
Wenliang Chen, Daisuke Kawahara, Kiyotaka
Uchimoto, Yujjie Zhang, and Hitoshi Isahara. 2008.
Dependency Parsig with Short Dependency Rela-
tions in Unlabeled Data. In Proceedings of the
third International Joint Conference on Natural
Language Processing (IJCNLP). pages 88-94..
Chih-ming Chiu, Ji-Chin Lo, and Keh-Jiann Chen.
2004. Compositional Semantics of Mandarin Affix
Verbs. In Proceedings of the Research on Compu-
tational Linguistics Conference (ROCLING), pages
131-139.
Yu-Ming Hsieh, Ming-Hong Bai, Jason S. Chang, and
Keh-Jiann Chen. 2012. Improving PCFG Chinese
Parsing with Context-Dependent Probability Re-
estimation, In Proceedings of the Second CIPS-
SIGHAN Joint Conference on Chinese Language
Processing, pages 216–221.
Shu-Ling Huang, You-Shan Chung, Keh-Jiann Chen.
2008. E-HowNet: the Expansion of HowNet. In
Proceedings of the First National HowNet work-
shop, pages 10-22, Beijing, China.
Chunhi Liu, Xiandai Hanyu Shuxing Fanchou Yianjiu
(現代漢語屬性範疇研究). Chengdu: Bashu Books,
2008.
Jiaju Mei, Yiming Lan, Yunqi Gao, and Yongxian
Ying. 1983. A Dictionary of Synonyms. Shanghai
Cishu Chubanshe.
Slav Petrov, Leon Barrett, Romain Thibaux and Dan
Klein. 2006. Learning Accurate, Compact, and In-
terpretable Tree Annotation. In Proceesings of
COLING/ACL, pages 433-400.
Likun Qiu. 2005. Constitutive Relation Analysis for
V-N Phrases. Journal of Chinese Language and
Computing, 15(3):173-183.
Richard Sproat and Thomas Emerson, 2003. The first
International Chinese Word Segmentation Bakeoff.
In Proceedings of the Second SIGHAN Workshop
on Chinese Language Processing, pages 133-143.
Honglin Sun and Dan Jurafsky. 2003. The Effect of
Rhythm on Structural Disambiguation in Chinese.
In Proceedings of the Second SIGHAN Workshop
on Chinese Language Processing, pages 39-46.
</reference>
<page confidence="0.985732">
936
</page>
<reference confidence="0.99934319047619">
Yuen-Hsieh Tseng, Lung-Hao Lee, and Liang-Chih
Yu. 2012. Tranditional Chinese Parsing Evaluation
at SIGHAN Bake-offs 2012. In Proceedings of the
Second CIPS-SIGHAN Joint Conference on Chi-
nese Language Processing, pages 199-205.
Andi Wu. 2003. Learning Verb-Noun Relations to
Improve Parsing. In Proceedings of the Second
SIGHAN workshop on Chinese Language Pro-
cessing, pages 119-124.
Kun Yu, Daisuke Kawahara, and Sadao Kurohashi.
2008. Chinese Dependency Parsing with Large
Scale Automatically Constructed Case Structures,
In Proceedings of the 22nd International Confer-
ence on Computational Linguistics (COLING2008),
pages 1049-1056.
Jun Zhao and Chang-ning Huang. 1999. The Com-
plex-feature-based Model for Acquisition of VN-
construction Structure Templates. Journal of Soft-
ware, 10(1):92-99.
Le Zhang. 2004. Maximum Entropy Modeling
Toolkit for Python and C++. Reference Manual.
</reference>
<page confidence="0.997573">
937
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.538728">
<title confidence="0.999769">Ambiguity Resolution for Vt-N Structures in Chinese</title>
<author confidence="0.999927">Jason S Keh-Jiann</author>
<affiliation confidence="0.8734885">of Information Science, Academia Sinica, Taiwan of Computer Science, National Tsing-Hua University, Taiwan</affiliation>
<email confidence="0.9287355">morris@iis.sinica.edu.tw,kchen@iis.sinica.edu.tw</email>
<abstract confidence="0.993287766666667">The syntactic ambiguity of a transitive verb (Vt) followed by a noun (N) has long been a problem in Chinese parsing. In this paper, we propose a classifier to resolve the ambiguity of Vt-N structures. The design of the classifier is based on three important guidelines, namely, adopting linguistically motivated features, using all available resources, and easy integration into a parsing model. The linguistically motivated features include semantic relations, context, and morphological structures; and the available resources are treebank, thesaurus, affix database, and large corpora. We also propose two learning approaches that resolve the problem of data sparseness by autoparsing and extracting relative knowledge from large-scale unlabeled data. Our experiment results show that the Vt-N classifier outperforms the current PCFG parser. Furthermore, it can be easily and effectively integrated into the PCFG parser and general statistical parsing models. Evaluation of the learning approaches indicates that world knowledge facilitates Vt-N disambiguation through data selection and error correction.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Li-li Chang</author>
<author>Keh-Jiann Chen</author>
<author>Chu-Ren Huang</author>
</authors>
<title>Alternation Across Semantic Fields: A Study on</title>
<date>2000</date>
<booktitle>Mandarin Verbs of Emotion. Internal Journal of Computational Linguistics and Chinese Language Processing (IJCLCLP),</booktitle>
<pages>5--1</pages>
<contexts>
<context position="5318" citStr="Chang et al. (2000)" startWordPosition="823" endWordPosition="826"> a limited number of such constructions and the results of the constructions are usually ambiguous, e.g., 0�/fried rice (NP), Fq /shouting sound. The first example also has the VP reading. Type 4. Apposition + Affair: Head nouns are event nouns and modifiers are verbs of apposition events, e.g. L9�/collide �&amp;/accident, Q&amp; IX /destruct A S1 /movement, &apos; 10&apos;[R /hate T7 � /behavior. There is finite number of event nouns. Furthermore, when we consider verbal modifiers, we find that verbs can play adjectival roles in Chinese without inflection, but not all verbs play adjectival roles. According to Chang et al. (2000) and our observations, adjectival verbs are verbs that denote event types rather than event instances; that is, they denote a class of events which that are concepts in an upper-level ontology. One important characteristic of adjectival verbs is that they have conjunctive morphological structures, i.e., the words are conjunct with two nearly synonymous verbs, e.g., �ff/study 5t /search (research), 9 /explore igq /detect (explore), and a3/search 4/find (search). Therefore, we need a morphological classifier that can detect the conjunctive morphological structure of a verb by checking the semant</context>
</contexts>
<marker>Chang, Chen, Huang, 2000</marker>
<rawString>Li-li Chang, Keh-Jiann Chen, and Chu-Ren Huang. 2000. Alternation Across Semantic Fields: A Study on Mandarin Verbs of Emotion. Internal Journal of Computational Linguistics and Chinese Language Processing (IJCLCLP), 5(1):61-80.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Keh-Jiann Chen</author>
<author>Chu-Ren Huang</author>
<author>Chi-Ching Luo</author>
<author>Feng-Yi Chen</author>
<author>Ming-Chung Chang</author>
<author>Chao-Jan Chen</author>
</authors>
<title>Sinica Treebank: Design Criteria, Representational Issues and Implementation.</title>
<date>2003</date>
<booktitle>In (Abeille</booktitle>
<pages>231--248</pages>
<publisher>Kluwer.</publisher>
<location>Dordrecht, the Netherlands:</location>
<contexts>
<context position="2352" citStr="Chen et al., 2003" startWordPosition="351" endWordPosition="354"> VP structure. However, Chinese verbs can also modify nouns without morphological inflection, e.g., 養殖/farming 池/pond. Consequently, parsing Vt-N structures is difficult because it is hard to resolve such ambiguities without prior knowledge. The following are some typical examples of various Vt-N structures: 1) 解決/solve 問題/problem 4 VP 解決/solving 方案/method 4 NP 解決/solve 人類/mankind (問題/problem)4None To find the most effective disambiguation features, we need more information about the Vt-N 4 NP construction and the semantic relations between Vt and N. Statistical data from the Sinica Treebank (Chen et al., 2003) indicates that 58% of Vt-N structures are verb phrases, 16% are noun phrases, and 26% do not have any dependent relations. It is obvious that the semantic relations between a Vt-N structure and its context information are very important for differentiating between dependent relations. Although the verb-argument relation of VP structures is well understood, it is not clear what kind of semantic relations result in NP structures. In the next sub-section, we consider three questions: What sets of nouns accept verbs as their modifiers? Is it possible to identify the semantic types of such pairs o</context>
</contexts>
<marker>Chen, Huang, Luo, Chen, Chang, Chen, 2003</marker>
<rawString>Keh-Jiann Chen, Chu-Ren Huang, Chi-Ching Luo, Feng-Yi Chen, Ming-Chung Chang, Chao-Jan Chen, , and Zhao-Ming Gao. 2003. Sinica Treebank: Design Criteria, Representational Issues and Implementation. In (Abeille 2003) Treebanks: Building and Using Parsed Corpora, pages 231-248. Dordrecht, the Netherlands: Kluwer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Li-jiang Chen</author>
</authors>
<date>2008</date>
<journal>Autolabeling of VN Combination Based on Multi-classifier. Journal of Computer Engineering,</journal>
<pages>34--5</pages>
<contexts>
<context position="7045" citStr="Chen, 2008" startWordPosition="1094" endWordPosition="1095">ated works. In Section 3, we describe the disambiguation model with our selected features, and introduce a strategy for handling unknown words. We also propose a learning approach for a largescale unlabeled corpus. In Section 4, we report the results of experiments conducted to evaluate the proposed Vt-N classifier on different feature combinations and learning approaches. Section 5 contains our concluding remarks. 2 Related Work Most works on V-N structure identification focus on two types of relation classification: modifierhead relations and predicate-object relations (Wu, 2003; Qiu, 2005; Chen, 2008; Chen et al., 2008; Yu et al., 2008). They exclude the independent structure and conjunctive head-head relation, but the cross-bracket relation does exist between two adjacent words in real language. For example, if “遍佈/all over 世界/world ” was included in the short sentence “遍佈/all over 世界/world 各國 /countries”, it would be an independent structure. A conjunctive head-head relation between a verb and a noun is rare. However, in the sentence “服 務 設備 都 甚 周到” (Both service and equipment are very thoughtful.), there is a conjunctive head-head relation between the verb 服 務 /service and the noun 設備/</context>
</contexts>
<marker>Chen, 2008</marker>
<rawString>Li-jiang Chen. 2008. Autolabeling of VN Combination Based on Multi-classifier. Journal of Computer Engineering, 34(5):79-81.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenliang Chen</author>
<author>Daisuke Kawahara</author>
<author>Kiyotaka Uchimoto</author>
<author>Yujjie Zhang</author>
<author>Hitoshi Isahara</author>
</authors>
<title>Dependency Parsig with Short Dependency Relations in Unlabeled Data.</title>
<date>2008</date>
<booktitle>In Proceedings of the third International Joint Conference on Natural Language Processing (IJCNLP).</booktitle>
<pages>88--94</pages>
<contexts>
<context position="7064" citStr="Chen et al., 2008" startWordPosition="1096" endWordPosition="1099">In Section 3, we describe the disambiguation model with our selected features, and introduce a strategy for handling unknown words. We also propose a learning approach for a largescale unlabeled corpus. In Section 4, we report the results of experiments conducted to evaluate the proposed Vt-N classifier on different feature combinations and learning approaches. Section 5 contains our concluding remarks. 2 Related Work Most works on V-N structure identification focus on two types of relation classification: modifierhead relations and predicate-object relations (Wu, 2003; Qiu, 2005; Chen, 2008; Chen et al., 2008; Yu et al., 2008). They exclude the independent structure and conjunctive head-head relation, but the cross-bracket relation does exist between two adjacent words in real language. For example, if “遍佈/all over 世界/world ” was included in the short sentence “遍佈/all over 世界/world 各國 /countries”, it would be an independent structure. A conjunctive head-head relation between a verb and a noun is rare. However, in the sentence “服 務 設備 都 甚 周到” (Both service and equipment are very thoughtful.), there is a conjunctive head-head relation between the verb 服 務 /service and the noun 設備/equipment. Therefor</context>
<context position="9861" citStr="Chen et al., 2008" startWordPosition="1541" endWordPosition="1544">ic dictionary and (2) the semantic type classifier is inadequate. Finally, Chen proposed an automatic VN combination method with features of verbs, nouns, context, and the syllables of words. The experiment results show that the method performs reasonably well without using any other resources. Based on the above feature selection methods, we extract relevant knowledge from Treebank to design a Vt-N classifier. However we have to resolve the common problem of data sparseness. Learning knowledge by analyzing large-scale unlabeled data is necessary and proved useful in previous works (Wu, 2003; Chen et al., 2008; Yu et al., 2008). Wu developed a machine learning method that acquires verb-object and modifierhead relations automatically. The mutual information scores are then used to prune verb-noun whose scores are below a certain threshold. The author found that accurate identification of the verb-noun relation improved the parsing performance by 4%. Yu et al. learned head-modifier pairs from parsed data and proposed a headmodifier classifier to filter the data. The filtering model uses the following features: a PoS-tag pair of the head and the modifier; the distance between the head and the modifier</context>
<context position="18612" citStr="Chen et al., 2008" startWordPosition="2993" endWordPosition="2996"> Algorithm. 1 The E-HowNet function in Figure 2 will return a null ST value where words do not exist in E-HowNet or Affix database. 3.3 Learning World Knowledge Based on the features discussed in the previous sub-section, we extract prior knowledge from Treebank to design the Vt-N classifier. However, the training suffers from the data sparseness problem. Furthermore most ambiguous Vt-N relations are resolved by common sense knowledge that makes it even harder to construct a well-trained system. An alternative way to extend world knowledge is to learn from largescale unlabeled data (Wu, 2003; Chen et al., 2008; Yu et al., 2008). However, the unsupervised approach accumulates errors caused by automatic annotation processes, such as word segmentation, PoS tagging, syntactic parsing, and semantic role assignment. Therefore, how to extract useful knowledge accurately is an important issue. To resolve the error accumulation problem, we propose two methods: unsupervised NP selection and supervised error correction. The NP selection method exploits the fact that an intransitive verb followed by a noun can only be interpreted as an NP structure, not a VP structure. It is easy to find such instances with hi</context>
</contexts>
<marker>Chen, Kawahara, Uchimoto, Zhang, Isahara, 2008</marker>
<rawString>Wenliang Chen, Daisuke Kawahara, Kiyotaka Uchimoto, Yujjie Zhang, and Hitoshi Isahara. 2008. Dependency Parsig with Short Dependency Relations in Unlabeled Data. In Proceedings of the third International Joint Conference on Natural Language Processing (IJCNLP). pages 88-94..</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chih-ming Chiu</author>
<author>Ji-Chin Lo</author>
<author>Keh-Jiann Chen</author>
</authors>
<title>Compositional Semantics of Mandarin Affix Verbs.</title>
<date>2004</date>
<booktitle>In Proceedings of the Research on Computational Linguistics Conference (ROCLING),</booktitle>
<pages>131--139</pages>
<contexts>
<context position="8164" citStr="Chiu et al., 2004" startWordPosition="1272" endWordPosition="1275">ghtful.), there is a conjunctive head-head relation between the verb 服 務 /service and the noun 設備/equipment. Therefore, we use four types of relations to describe the VN structures in our experiments. The symbol ‘H/X’ denotes a predicate-object relation; ‘X/H’ denotes a modifier-head relation; ‘H/H’ denotes a conjunctive head-head relation; and ‘X/X’ denotes an independent relation. Feature selection is an important task in V-N disambiguation. Hence, a number of studies have suggested features that may help resolve the ambiguity of V-N structures (Zhao and Huang, 1999; Sun and Jurafsky, 2003; Chiu et al., 2004; Qiu, 2005; Chen, 2008). Zhao and Huang used lexicons, semantic knowledge, and word length in929 formation to increase the accuracy of identification. Although they used the Chinese thesaurus CiLin (Mei et al., 1983) to derive lexical semantic knowledge, the word coverage of CiLin is insufficient. Moreover, none of the above papers tackle the problem of unknown words. Sun and Jurafsky exploit the probabilistic rhythm feature (i.e., the number of syllables in a word or the number of words in a phrase) in their shallow parser. Their results show that the feature improves the parsing performance</context>
<context position="15395" citStr="Chiu et al. (2004)" startWordPosition="2430" endWordPosition="2433">rocessing In Chinese documents, 3% to 7% of the words are usually unknown (Sproat and Emerson, 2003). By ‘unknown words’, we mean words not listed in the dictionary. More specifically, in this paper, unknown words means words without semantic type information (i.e., E-HowNet expressions) and verbs without morphological structure information. Therefore, we propose a method for predicting the semantic types of unknown words, and use an affix database to train a morphstructure classifier to derive the morphological structure of verbs. Morph-Structure Predication of Verbs: We use data analyzed by Chiu et al. (2004) to develop a classifier for predicating the morphological structure of verbs. There are four types of morphological structures for verbs: the coordinating structure (VV), the modifier-head structure (AV), the verb-complement structure (VR), and the verb-object structure (VO). To classify verbs automatically, we incorporate three features in the proposed classifier, namely, the lexeme itself, the prefix and the suffix, and the semantic types of the prefix and the suffix. Then, we use training data from the affix database to train the classifier. Table 3 shows an example of the unknown verb “ 傳</context>
</contexts>
<marker>Chiu, Lo, Chen, 2004</marker>
<rawString>Chih-ming Chiu, Ji-Chin Lo, and Keh-Jiann Chen. 2004. Compositional Semantics of Mandarin Affix Verbs. In Proceedings of the Research on Computational Linguistics Conference (ROCLING), pages 131-139.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yu-Ming Hsieh</author>
<author>Ming-Hong Bai</author>
<author>Jason S Chang</author>
<author>Keh-Jiann Chen</author>
</authors>
<title>Improving PCFG Chinese Parsing with Context-Dependent Probability Reestimation,</title>
<date>2012</date>
<booktitle>In Proceedings of the Second CIPSSIGHAN Joint Conference on Chinese Language Processing,</booktitle>
<pages>216--221</pages>
<contexts>
<context position="28318" citStr="Hsieh et al. (2012)" startWordPosition="4526" endWordPosition="4529">2 P(%) 84.43 78.57 81.86 Treebank+ R(%) 91.00 72.22 71.54 ASBC-Vi-N P(%) 84.57 72.67 85.71 Treebank+ R(%) 98.62 60.49 83.08 ASBC+Correction P(%) 86.63 88.29 93.51 PCFG R(%) 90.54 23.63 80.21 P(%) 78.24 73.58 75.00 Table 8. Performance comparison of different classification models. 4.4 Experiment 3: Integrating the Vt-N classifier with the PCFG Parser Identifying Vt-N structures correctly facilitates statistical parsing, machine translation, infor934 mation retrieval, and text classification. In this experiment, we develop a baseline PCFG parser based on feature-based grammar representation by Hsieh et al. (2012) to find the best tree structures (T) of a given sentence (S). The parser then selects the best tree according to the evaluation score Score(T,S) of all possible trees. If there are n PCFG rules in the tree T, the Score(T,S) is the accumulation of the logarithmic probabilities of the i-th grammar rule (RPi). Formula 1 shows the baseline PCFG parser. n Score(T,S)= ∑(RPi) (1) i 1 The Vt-N models can be easily integrated into the PCFG parser. Formula 2 represents the integrated structural evaluation model. We combine RPi and VtNPi with the weights w1 and w2 respectively, and set the value of w2 h</context>
<context position="30311" citStr="Hsieh et al., 2012" startWordPosition="4874" endWordPosition="4877">ures, it does not affect the parsing accuracies of other sentences. P2(%) 80.68 77.09 BF(%) 83.64 82.80 Table 9. The performance of the PCFG parser with and without model M5 from Treebank. 2 The evaluation formula is (BP*BR*2) / (BP+BR), where BP is the precision and BR is the recall. P2(%) 87.88 77.09 BF(%) 84.68 82.80 Table 10. The performance of the PCFG parser with and without model M5 from Treebank+ASBC+Correction data set. 4.5 Experiment 4: Comparison of Various Chinese Parsers In this experiment, we give some comparison results in various parser: ‘PCFG Parser’ (baseline), ‘CDM Parser’ (Hsieh et al., 2012), and ‘Berkeley Parser’ (Petrov et al., 2006). The CDM parser achieves the best score in Traditional Chinese Parsing task of SIGHAN Bake-offs 2012 (Tseng et al., 2012). Petrov’s parser (as Berkeley, version is 2009 1.1) is the best PCFG parser for non-English language and it is an open source. In our comparison, we use the same training data for training models and parse the same test dataset based on the gold standard word segmentation and PoS tags. We have already discussed the PCFG parser in Section 4.4. As for CDM parser, we retrain relevant model in our experiments. And since Berkeley par</context>
</contexts>
<marker>Hsieh, Bai, Chang, Chen, 2012</marker>
<rawString>Yu-Ming Hsieh, Ming-Hong Bai, Jason S. Chang, and Keh-Jiann Chen. 2012. Improving PCFG Chinese Parsing with Context-Dependent Probability Reestimation, In Proceedings of the Second CIPSSIGHAN Joint Conference on Chinese Language Processing, pages 216–221.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shu-Ling Huang</author>
<author>You-Shan Chung</author>
<author>Keh-Jiann Chen</author>
</authors>
<title>E-HowNet: the Expansion of HowNet.</title>
<date>2008</date>
<booktitle>In Proceedings of the First National HowNet workshop,</booktitle>
<pages>10--22</pages>
<location>Beijing, China.</location>
<contexts>
<context position="12842" citStr="Huang et al., 2008" startWordPosition="2019" endWordPosition="2022"> processing, and world knowledge learning. 3.1 Feature Selection and Extraction We divide the selected features into five groups: PoS tags of Vt and N, PoS tags of the context, words, semantics, and additional information. Table 1 shows the feature types and symbol notations. We use symbols of t1 and t2 to denote the PoS of Vt and N respectively. The context feature is neighboring PoSs of Vt and N: the symbols of t-2 and t-1 represent its left PoSs, and the symbol t3 and t4 represent its right PoSs. The semantic feature is the lexicon’s semantic type extracted from E-HowNet sense expressions (Huang et al., 2008). For example, the EHowNet expression of “ * 輛 /vehicles” is {LandVehicle |* :quantity={mass |眾 }}, so its semantic type is {LandVehicle|*}. We discuss the model’s performance with different feature combinations in Section 4. 930 Feature Feature Description PoS PoS of Vt and N t1; t2 Context Neighboring PoSs t-2; t-1; t3; t4 Word Lexical word w1; w2 Semantic Semantic type of word st1; st2 Additional Morphological structure of verb Information Vmorph Syllabic length of noun Nlen Table 1. The features used in the Vt-N classifier The example in Figure 1 illustrates feature labeling of a Vt-N stru</context>
</contexts>
<marker>Huang, Chung, Chen, 2008</marker>
<rawString>Shu-Ling Huang, You-Shan Chung, Keh-Jiann Chen. 2008. E-HowNet: the Expansion of HowNet. In Proceedings of the First National HowNet workshop, pages 10-22, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chunhi Liu</author>
</authors>
<title>Xiandai Hanyu Shuxing Fanchou Yianjiu (現代漢語屬性範疇研究).</title>
<date>2008</date>
<publisher>Bashu Books,</publisher>
<location>Chengdu:</location>
<contexts>
<context position="4470" citStr="Liu, 2008" startWordPosition="683" endWordPosition="684"> nouns are monosyllabic words, such as _R_/people, a/instruments, M/machines. Type 2. Host-Event(Vt) + Attribute(N): Head nouns are attribute nouns that denote the attributes of the verb, e.g., �ff5t/research )j A /method (method of research); JA�/attack %ftllj� /strategy (attacking strategy); �A/write Ng /context (context of writing); N/gamble ffl/rule (gambling rules). An attribute noun is a special type of noun. Semantically, attribute nouns denote the attribute types of objects or events, such as weight, color, method, and rule. Syntactically, attribute nouns do not play adjectival roles (Liu, 2008). By contrast, object nouns may modify nouns. The number of attributes for events is limited. If we could discover all event-attribute relations, then we can solve this type of construction. Type 3. Agentive + Host: There is only a limited number of such constructions and the results of the constructions are usually ambiguous, e.g., 0�/fried rice (NP), Fq /shouting sound. The first example also has the VP reading. Type 4. Apposition + Affair: Head nouns are event nouns and modifiers are verbs of apposition events, e.g. L9�/collide �&amp;/accident, Q&amp; IX /destruct A S1 /movement, &apos; 10&apos;[R /hate T7 �</context>
</contexts>
<marker>Liu, 2008</marker>
<rawString>Chunhi Liu, Xiandai Hanyu Shuxing Fanchou Yianjiu (現代漢語屬性範疇研究). Chengdu: Bashu Books, 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiaju Mei</author>
<author>Yiming Lan</author>
<author>Yunqi Gao</author>
<author>Yongxian Ying</author>
</authors>
<title>A Dictionary of Synonyms. Shanghai Cishu Chubanshe.</title>
<date>1983</date>
<contexts>
<context position="8381" citStr="Mei et al., 1983" startWordPosition="1308" endWordPosition="1311"> denotes a predicate-object relation; ‘X/H’ denotes a modifier-head relation; ‘H/H’ denotes a conjunctive head-head relation; and ‘X/X’ denotes an independent relation. Feature selection is an important task in V-N disambiguation. Hence, a number of studies have suggested features that may help resolve the ambiguity of V-N structures (Zhao and Huang, 1999; Sun and Jurafsky, 2003; Chiu et al., 2004; Qiu, 2005; Chen, 2008). Zhao and Huang used lexicons, semantic knowledge, and word length in929 formation to increase the accuracy of identification. Although they used the Chinese thesaurus CiLin (Mei et al., 1983) to derive lexical semantic knowledge, the word coverage of CiLin is insufficient. Moreover, none of the above papers tackle the problem of unknown words. Sun and Jurafsky exploit the probabilistic rhythm feature (i.e., the number of syllables in a word or the number of words in a phrase) in their shallow parser. Their results show that the feature improves the parsing performance, which coincides with our analysis in Section 1.1. Chiu et al.’s study shows that the morphological structure of verbs influences their syntactic behavior. We follow this finding and utilize the morphological structu</context>
</contexts>
<marker>Mei, Lan, Gao, Ying, 1983</marker>
<rawString>Jiaju Mei, Yiming Lan, Yunqi Gao, and Yongxian Ying. 1983. A Dictionary of Synonyms. Shanghai Cishu Chubanshe.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Leon Barrett</author>
<author>Romain Thibaux</author>
<author>Dan Klein</author>
</authors>
<title>Learning Accurate, Compact, and Interpretable Tree Annotation.</title>
<date>2006</date>
<booktitle>In Proceesings of COLING/ACL,</booktitle>
<pages>433--400</pages>
<contexts>
<context position="30356" citStr="Petrov et al., 2006" startWordPosition="4881" endWordPosition="4884">ies of other sentences. P2(%) 80.68 77.09 BF(%) 83.64 82.80 Table 9. The performance of the PCFG parser with and without model M5 from Treebank. 2 The evaluation formula is (BP*BR*2) / (BP+BR), where BP is the precision and BR is the recall. P2(%) 87.88 77.09 BF(%) 84.68 82.80 Table 10. The performance of the PCFG parser with and without model M5 from Treebank+ASBC+Correction data set. 4.5 Experiment 4: Comparison of Various Chinese Parsers In this experiment, we give some comparison results in various parser: ‘PCFG Parser’ (baseline), ‘CDM Parser’ (Hsieh et al., 2012), and ‘Berkeley Parser’ (Petrov et al., 2006). The CDM parser achieves the best score in Traditional Chinese Parsing task of SIGHAN Bake-offs 2012 (Tseng et al., 2012). Petrov’s parser (as Berkeley, version is 2009 1.1) is the best PCFG parser for non-English language and it is an open source. In our comparison, we use the same training data for training models and parse the same test dataset based on the gold standard word segmentation and PoS tags. We have already discussed the PCFG parser in Section 4.4. As for CDM parser, we retrain relevant model in our experiments. And since Berkeley parser take different tree structure (Penn Treeb</context>
</contexts>
<marker>Petrov, Barrett, Thibaux, Klein, 2006</marker>
<rawString>Slav Petrov, Leon Barrett, Romain Thibaux and Dan Klein. 2006. Learning Accurate, Compact, and Interpretable Tree Annotation. In Proceesings of COLING/ACL, pages 433-400.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Likun Qiu</author>
</authors>
<title>Constitutive Relation Analysis for V-N Phrases.</title>
<date>2005</date>
<journal>Journal of Chinese Language and Computing,</journal>
<pages>15--3</pages>
<contexts>
<context position="7033" citStr="Qiu, 2005" startWordPosition="1092" endWordPosition="1093">view of related works. In Section 3, we describe the disambiguation model with our selected features, and introduce a strategy for handling unknown words. We also propose a learning approach for a largescale unlabeled corpus. In Section 4, we report the results of experiments conducted to evaluate the proposed Vt-N classifier on different feature combinations and learning approaches. Section 5 contains our concluding remarks. 2 Related Work Most works on V-N structure identification focus on two types of relation classification: modifierhead relations and predicate-object relations (Wu, 2003; Qiu, 2005; Chen, 2008; Chen et al., 2008; Yu et al., 2008). They exclude the independent structure and conjunctive head-head relation, but the cross-bracket relation does exist between two adjacent words in real language. For example, if “遍佈/all over 世界/world ” was included in the short sentence “遍佈/all over 世界/world 各國 /countries”, it would be an independent structure. A conjunctive head-head relation between a verb and a noun is rare. However, in the sentence “服 務 設備 都 甚 周到” (Both service and equipment are very thoughtful.), there is a conjunctive head-head relation between the verb 服 務 /service and </context>
</contexts>
<marker>Qiu, 2005</marker>
<rawString>Likun Qiu. 2005. Constitutive Relation Analysis for V-N Phrases. Journal of Chinese Language and Computing, 15(3):173-183.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Sproat</author>
<author>Thomas Emerson</author>
</authors>
<title>The first International Chinese Word Segmentation Bakeoff.</title>
<date>2003</date>
<booktitle>In Proceedings of the Second SIGHAN Workshop on Chinese Language Processing,</booktitle>
<pages>133--143</pages>
<contexts>
<context position="14877" citStr="Sproat and Emerson, 2003" startWordPosition="2351" endWordPosition="2354">be relevant features in “MW/learn” and “rpm/Chinese” respectively. Some features are not explicitly annotated in the Treebank, e.g., the semantic types of words and the morphological structure of verbs. We propose labeling methods for them in the next sub-section. Feature Type x y Word w1=學習 w2=中文 PoS t1=VC t2=Na Semantic st1=study|學習 st2=language|語言 Context t-2=Nep; t-1=VK; t3=DE; t4=Na Additional Vmorph=VV Nlen=2 Information Relation Type rt = H/X Table 2. The feature labels of Vt-N pair in Figure 1 3.2 Unknown Word Processing In Chinese documents, 3% to 7% of the words are usually unknown (Sproat and Emerson, 2003). By ‘unknown words’, we mean words not listed in the dictionary. More specifically, in this paper, unknown words means words without semantic type information (i.e., E-HowNet expressions) and verbs without morphological structure information. Therefore, we propose a method for predicting the semantic types of unknown words, and use an affix database to train a morphstructure classifier to derive the morphological structure of verbs. Morph-Structure Predication of Verbs: We use data analyzed by Chiu et al. (2004) to develop a classifier for predicating the morphological structure of verbs. The</context>
</contexts>
<marker>Sproat, Emerson, 2003</marker>
<rawString>Richard Sproat and Thomas Emerson, 2003. The first International Chinese Word Segmentation Bakeoff. In Proceedings of the Second SIGHAN Workshop on Chinese Language Processing, pages 133-143.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Honglin Sun</author>
<author>Dan Jurafsky</author>
</authors>
<title>The Effect of Rhythm on Structural Disambiguation in Chinese.</title>
<date>2003</date>
<booktitle>In Proceedings of the Second SIGHAN Workshop on Chinese Language Processing,</booktitle>
<pages>39--46</pages>
<contexts>
<context position="8145" citStr="Sun and Jurafsky, 2003" startWordPosition="1268" endWordPosition="1271"> equipment are very thoughtful.), there is a conjunctive head-head relation between the verb 服 務 /service and the noun 設備/equipment. Therefore, we use four types of relations to describe the VN structures in our experiments. The symbol ‘H/X’ denotes a predicate-object relation; ‘X/H’ denotes a modifier-head relation; ‘H/H’ denotes a conjunctive head-head relation; and ‘X/X’ denotes an independent relation. Feature selection is an important task in V-N disambiguation. Hence, a number of studies have suggested features that may help resolve the ambiguity of V-N structures (Zhao and Huang, 1999; Sun and Jurafsky, 2003; Chiu et al., 2004; Qiu, 2005; Chen, 2008). Zhao and Huang used lexicons, semantic knowledge, and word length in929 formation to increase the accuracy of identification. Although they used the Chinese thesaurus CiLin (Mei et al., 1983) to derive lexical semantic knowledge, the word coverage of CiLin is insufficient. Moreover, none of the above papers tackle the problem of unknown words. Sun and Jurafsky exploit the probabilistic rhythm feature (i.e., the number of syllables in a word or the number of words in a phrase) in their shallow parser. Their results show that the feature improves the </context>
</contexts>
<marker>Sun, Jurafsky, 2003</marker>
<rawString>Honglin Sun and Dan Jurafsky. 2003. The Effect of Rhythm on Structural Disambiguation in Chinese. In Proceedings of the Second SIGHAN Workshop on Chinese Language Processing, pages 39-46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuen-Hsieh Tseng</author>
<author>Lung-Hao Lee</author>
<author>Liang-Chih Yu</author>
</authors>
<title>Tranditional Chinese Parsing Evaluation at SIGHAN Bake-offs</title>
<date>2012</date>
<booktitle>In Proceedings of the Second CIPS-SIGHAN Joint Conference on Chinese Language Processing,</booktitle>
<pages>199--205</pages>
<contexts>
<context position="30478" citStr="Tseng et al., 2012" startWordPosition="4902" endWordPosition="4905">del M5 from Treebank. 2 The evaluation formula is (BP*BR*2) / (BP+BR), where BP is the precision and BR is the recall. P2(%) 87.88 77.09 BF(%) 84.68 82.80 Table 10. The performance of the PCFG parser with and without model M5 from Treebank+ASBC+Correction data set. 4.5 Experiment 4: Comparison of Various Chinese Parsers In this experiment, we give some comparison results in various parser: ‘PCFG Parser’ (baseline), ‘CDM Parser’ (Hsieh et al., 2012), and ‘Berkeley Parser’ (Petrov et al., 2006). The CDM parser achieves the best score in Traditional Chinese Parsing task of SIGHAN Bake-offs 2012 (Tseng et al., 2012). Petrov’s parser (as Berkeley, version is 2009 1.1) is the best PCFG parser for non-English language and it is an open source. In our comparison, we use the same training data for training models and parse the same test dataset based on the gold standard word segmentation and PoS tags. We have already discussed the PCFG parser in Section 4.4. As for CDM parser, we retrain relevant model in our experiments. And since Berkeley parser take different tree structure (Penn Treebank format), we transform the experimental data to Berkeley CoNLL format and re-train a new model with parameters “- treeb</context>
</contexts>
<marker>Tseng, Lee, Yu, 2012</marker>
<rawString>Yuen-Hsieh Tseng, Lung-Hao Lee, and Liang-Chih Yu. 2012. Tranditional Chinese Parsing Evaluation at SIGHAN Bake-offs 2012. In Proceedings of the Second CIPS-SIGHAN Joint Conference on Chinese Language Processing, pages 199-205.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andi Wu</author>
</authors>
<title>Learning Verb-Noun Relations to Improve Parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the Second SIGHAN workshop on Chinese Language Processing,</booktitle>
<pages>119--124</pages>
<contexts>
<context position="7022" citStr="Wu, 2003" startWordPosition="1090" endWordPosition="1091">vides a review of related works. In Section 3, we describe the disambiguation model with our selected features, and introduce a strategy for handling unknown words. We also propose a learning approach for a largescale unlabeled corpus. In Section 4, we report the results of experiments conducted to evaluate the proposed Vt-N classifier on different feature combinations and learning approaches. Section 5 contains our concluding remarks. 2 Related Work Most works on V-N structure identification focus on two types of relation classification: modifierhead relations and predicate-object relations (Wu, 2003; Qiu, 2005; Chen, 2008; Chen et al., 2008; Yu et al., 2008). They exclude the independent structure and conjunctive head-head relation, but the cross-bracket relation does exist between two adjacent words in real language. For example, if “遍佈/all over 世界/world ” was included in the short sentence “遍佈/all over 世界/world 各國 /countries”, it would be an independent structure. A conjunctive head-head relation between a verb and a noun is rare. However, in the sentence “服 務 設備 都 甚 周到” (Both service and equipment are very thoughtful.), there is a conjunctive head-head relation between the verb 服 務 /s</context>
<context position="9842" citStr="Wu, 2003" startWordPosition="1539" endWordPosition="1540">the semantic dictionary and (2) the semantic type classifier is inadequate. Finally, Chen proposed an automatic VN combination method with features of verbs, nouns, context, and the syllables of words. The experiment results show that the method performs reasonably well without using any other resources. Based on the above feature selection methods, we extract relevant knowledge from Treebank to design a Vt-N classifier. However we have to resolve the common problem of data sparseness. Learning knowledge by analyzing large-scale unlabeled data is necessary and proved useful in previous works (Wu, 2003; Chen et al., 2008; Yu et al., 2008). Wu developed a machine learning method that acquires verb-object and modifierhead relations automatically. The mutual information scores are then used to prune verb-noun whose scores are below a certain threshold. The author found that accurate identification of the verb-noun relation improved the parsing performance by 4%. Yu et al. learned head-modifier pairs from parsed data and proposed a headmodifier classifier to filter the data. The filtering model uses the following features: a PoS-tag pair of the head and the modifier; the distance between the he</context>
<context position="18593" citStr="Wu, 2003" startWordPosition="2991" endWordPosition="2992">redication Algorithm. 1 The E-HowNet function in Figure 2 will return a null ST value where words do not exist in E-HowNet or Affix database. 3.3 Learning World Knowledge Based on the features discussed in the previous sub-section, we extract prior knowledge from Treebank to design the Vt-N classifier. However, the training suffers from the data sparseness problem. Furthermore most ambiguous Vt-N relations are resolved by common sense knowledge that makes it even harder to construct a well-trained system. An alternative way to extend world knowledge is to learn from largescale unlabeled data (Wu, 2003; Chen et al., 2008; Yu et al., 2008). However, the unsupervised approach accumulates errors caused by automatic annotation processes, such as word segmentation, PoS tagging, syntactic parsing, and semantic role assignment. Therefore, how to extract useful knowledge accurately is an important issue. To resolve the error accumulation problem, we propose two methods: unsupervised NP selection and supervised error correction. The NP selection method exploits the fact that an intransitive verb followed by a noun can only be interpreted as an NP structure, not a VP structure. It is easy to find suc</context>
</contexts>
<marker>Wu, 2003</marker>
<rawString>Andi Wu. 2003. Learning Verb-Noun Relations to Improve Parsing. In Proceedings of the Second SIGHAN workshop on Chinese Language Processing, pages 119-124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kun Yu</author>
<author>Daisuke Kawahara</author>
<author>Sadao Kurohashi</author>
</authors>
<title>Chinese Dependency Parsing with Large Scale Automatically Constructed Case Structures,</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics (COLING2008),</booktitle>
<pages>1049--1056</pages>
<contexts>
<context position="7082" citStr="Yu et al., 2008" startWordPosition="1100" endWordPosition="1103">scribe the disambiguation model with our selected features, and introduce a strategy for handling unknown words. We also propose a learning approach for a largescale unlabeled corpus. In Section 4, we report the results of experiments conducted to evaluate the proposed Vt-N classifier on different feature combinations and learning approaches. Section 5 contains our concluding remarks. 2 Related Work Most works on V-N structure identification focus on two types of relation classification: modifierhead relations and predicate-object relations (Wu, 2003; Qiu, 2005; Chen, 2008; Chen et al., 2008; Yu et al., 2008). They exclude the independent structure and conjunctive head-head relation, but the cross-bracket relation does exist between two adjacent words in real language. For example, if “遍佈/all over 世界/world ” was included in the short sentence “遍佈/all over 世界/world 各國 /countries”, it would be an independent structure. A conjunctive head-head relation between a verb and a noun is rare. However, in the sentence “服 務 設備 都 甚 周到” (Both service and equipment are very thoughtful.), there is a conjunctive head-head relation between the verb 服 務 /service and the noun 設備/equipment. Therefore, we use four typ</context>
<context position="9879" citStr="Yu et al., 2008" startWordPosition="1545" endWordPosition="1548">2) the semantic type classifier is inadequate. Finally, Chen proposed an automatic VN combination method with features of verbs, nouns, context, and the syllables of words. The experiment results show that the method performs reasonably well without using any other resources. Based on the above feature selection methods, we extract relevant knowledge from Treebank to design a Vt-N classifier. However we have to resolve the common problem of data sparseness. Learning knowledge by analyzing large-scale unlabeled data is necessary and proved useful in previous works (Wu, 2003; Chen et al., 2008; Yu et al., 2008). Wu developed a machine learning method that acquires verb-object and modifierhead relations automatically. The mutual information scores are then used to prune verb-noun whose scores are below a certain threshold. The author found that accurate identification of the verb-noun relation improved the parsing performance by 4%. Yu et al. learned head-modifier pairs from parsed data and proposed a headmodifier classifier to filter the data. The filtering model uses the following features: a PoS-tag pair of the head and the modifier; the distance between the head and the modifier; and the presence</context>
<context position="18630" citStr="Yu et al., 2008" startWordPosition="2997" endWordPosition="3000">-HowNet function in Figure 2 will return a null ST value where words do not exist in E-HowNet or Affix database. 3.3 Learning World Knowledge Based on the features discussed in the previous sub-section, we extract prior knowledge from Treebank to design the Vt-N classifier. However, the training suffers from the data sparseness problem. Furthermore most ambiguous Vt-N relations are resolved by common sense knowledge that makes it even harder to construct a well-trained system. An alternative way to extend world knowledge is to learn from largescale unlabeled data (Wu, 2003; Chen et al., 2008; Yu et al., 2008). However, the unsupervised approach accumulates errors caused by automatic annotation processes, such as word segmentation, PoS tagging, syntactic parsing, and semantic role assignment. Therefore, how to extract useful knowledge accurately is an important issue. To resolve the error accumulation problem, we propose two methods: unsupervised NP selection and supervised error correction. The NP selection method exploits the fact that an intransitive verb followed by a noun can only be interpreted as an NP structure, not a VP structure. It is easy to find such instances with high precision by pa</context>
</contexts>
<marker>Yu, Kawahara, Kurohashi, 2008</marker>
<rawString>Kun Yu, Daisuke Kawahara, and Sadao Kurohashi. 2008. Chinese Dependency Parsing with Large Scale Automatically Constructed Case Structures, In Proceedings of the 22nd International Conference on Computational Linguistics (COLING2008), pages 1049-1056.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun Zhao</author>
<author>Chang-ning Huang</author>
</authors>
<title>The Complex-feature-based Model for Acquisition of VNconstruction Structure Templates.</title>
<date>1999</date>
<journal>Journal of Software,</journal>
<pages>10--1</pages>
<contexts>
<context position="8121" citStr="Zhao and Huang, 1999" startWordPosition="1264" endWordPosition="1267"> 周到” (Both service and equipment are very thoughtful.), there is a conjunctive head-head relation between the verb 服 務 /service and the noun 設備/equipment. Therefore, we use four types of relations to describe the VN structures in our experiments. The symbol ‘H/X’ denotes a predicate-object relation; ‘X/H’ denotes a modifier-head relation; ‘H/H’ denotes a conjunctive head-head relation; and ‘X/X’ denotes an independent relation. Feature selection is an important task in V-N disambiguation. Hence, a number of studies have suggested features that may help resolve the ambiguity of V-N structures (Zhao and Huang, 1999; Sun and Jurafsky, 2003; Chiu et al., 2004; Qiu, 2005; Chen, 2008). Zhao and Huang used lexicons, semantic knowledge, and word length in929 formation to increase the accuracy of identification. Although they used the Chinese thesaurus CiLin (Mei et al., 1983) to derive lexical semantic knowledge, the word coverage of CiLin is insufficient. Moreover, none of the above papers tackle the problem of unknown words. Sun and Jurafsky exploit the probabilistic rhythm feature (i.e., the number of syllables in a word or the number of words in a phrase) in their shallow parser. Their results show that t</context>
<context position="13821" citStr="Zhao and Huang (1999)" startWordPosition="2181" endWordPosition="2184">ic Semantic type of word st1; st2 Additional Morphological structure of verb Information Vmorph Syllabic length of noun Nlen Table 1. The features used in the Vt-N classifier The example in Figure 1 illustrates feature labeling of a Vt-N structure. First, an instance of a Vt-N structure is identified from Treebank. Then, we assign the semantic type of each word without considering the problem of sense ambiguity for the moment. This is because sense ambiguities are partially resolved by PoS tagging, and the general problem of sense disambiguation is beyond the scope of this paper. Furthermore, Zhao and Huang (1999) demonstrated that the retained ambiguity does not have an adverse impact on identification. Therefore, we keep the ambiguous semantic type for future processing. zhe zaochen xuexi zhongwen DE fongchao this cause learn Chinese trend “This causes the trend of learning Chinese.” Figure 1. An example of a tree with a Vt-N structure Table 2 shows the labeled features for “學習 /learn 中文/Chinese” in Figure 1. The column x and y describe relevant features in “MW/learn” and “rpm/Chinese” respectively. Some features are not explicitly annotated in the Treebank, e.g., the semantic types of words and the </context>
</contexts>
<marker>Zhao, Huang, 1999</marker>
<rawString>Jun Zhao and Chang-ning Huang. 1999. The Complex-feature-based Model for Acquisition of VNconstruction Structure Templates. Journal of Software, 10(1):92-99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Le Zhang</author>
</authors>
<title>Maximum Entropy Modeling Toolkit for Python and C++. Reference Manual.</title>
<date>2004</date>
<marker>Le Zhang, 2004</marker>
<rawString>Le Zhang. 2004. Maximum Entropy Modeling Toolkit for Python and C++. Reference Manual.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
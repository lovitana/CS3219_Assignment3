<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000283">
<title confidence="0.911341">
Syntax-Augmented Machine Translation using Syntax-Label Clustering
</title>
<author confidence="0.993776">
Hideya Mino, Taro Watanabe and Eiichiro Sumita
</author>
<affiliation confidence="0.995113">
National Institute of Information and Communications Technology
</affiliation>
<address confidence="0.924447">
3-5 Hikaridai, Seika-cho, Soraku-gun, Kyoto, JAPAN
</address>
<email confidence="0.988847">
{hideya.mino, taro.watanabe, eiichiro.sumita}@nict.go.jp
</email>
<sectionHeader confidence="0.997323" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999882956521739">
Recently, syntactic information has helped
significantly to improve statistical ma-
chine translation. However, the use of syn-
tactic information may have a negative im-
pact on the speed of translation because of
the large number of rules, especially when
syntax labels are projected from a parser in
syntax-augmented machine translation. In
this paper, we propose a syntax-label clus-
tering method that uses an exchange algo-
rithm in which syntax labels are clustered
together to reduce the number of rules.
The proposed method achieves clustering
by directly maximizing the likelihood of
synchronous rules, whereas previous work
considered only the similarity of proba-
bilistic distributions of labels. We tested
the proposed method on Japanese-English
and Chinese-English translation tasks and
found order-of-magnitude higher cluster-
ing speeds for reducing labels and gains
in translation quality compared with pre-
vious clustering method.
</bodyText>
<sectionHeader confidence="0.999472" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99997830952381">
In recent years, statistical machine translation
(SMT) models that use syntactic information have
received significant research attention. These
models use syntactic information on the source
side (Liu et al., 2006; Mylonakis and Sima’an,
2011), the target side (Galley et al., 2006; Huang
and Knight, 2006) or both sides (Chiang, 2010;
Hanneman and Lavie, 2013) produce syntactically
correct translations. Zollmann and Venugopal
(2006) proposed syntax-augmented MT (SAMT),
which is a MT system that uses syntax labels of a
parser. The SAMT grammar directly encodes syn-
tactic information into the synchronous context-
free grammar (SCFG) of Hiero (Chiang, 2007),
which relies on two nonterminal labels. One prob-
lem in adding syntax labels to Hiero-style rules
is that only partial phrases are assigned labels.
It is common practice to extend labels by us-
ing the idea of combinatory categorial grammar
(CCG) (Steedman, 2000) on the problem. Al-
though this extended syntactical information may
improve the coverage of rules and syntactic cor-
rectness in translation, the increased grammar size
causes serious speed and data-sparseness prob-
lems. To address these problems, Hanneman and
Lavie (2013) coarsen syntactic labels using the
similarity of the probabilistic distributions of la-
bels in synchronous rules and showed that perfor-
mance improved.
In the present work, we follow the idea of label-
set coarsening and propose a new method to group
syntax labels. First, as an optimization criterion,
we use the logarithm of the likelihood of syn-
chronous rules instead of the similarity of prob-
abilistic distributions of syntax labels. Second,
we use exchange clustering (Uszkoreit and Brants,
2008), which is faster than the agglomerative-
clustering algorithm used in the previous work.
We tested our proposed method on Japanese-
English and Chinese-English translation tasks and
observed gains comparable to those of previous
work with similar reductions in grammar size.
</bodyText>
<sectionHeader confidence="0.995388" genericHeader="method">
2 Syntax-Augmented Machine
Translation
</sectionHeader>
<bodyText confidence="0.8345645">
SAMT is an instance of SCFG g, which can be
formally defined as
</bodyText>
<equation confidence="0.202245">
9 = (N, S, TQ, TT, R)
</equation>
<bodyText confidence="0.99988825">
where N is a set of nonterminals, S E N is a
start label, TQ and TT are the source- and target-
side terminals, and R is a set of synchronous rules.
Each synchronous rule in R takes the form
</bodyText>
<equation confidence="0.966633">
X — ⟨α, Q, -⟩
</equation>
<page confidence="0.940559">
165
</page>
<bodyText confidence="0.969714029411765">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 165–171,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
where X ∈ N is a nonterminal, a ∈ (N ∪ Tσ)∗
is a sequence of nonterminals or source-side ter-
minals, and Q ∈ (N ∪ Tτ)∗ is a sequence of
nonterminals or target-side terminals. The num-
ber #NT (a) of nonterminals in a is equal to
the number #NT (Q) of nonterminals in Q, and
∼: {1, ..., #NT (a)} → {1, ..., #NT (Q)} is a
one-to-one mapping from nonterminals in a to
nonterminals in Q. For each synchronous rule, a
nonnegative real-value weight w(X → ⟨a, Q, ∼⟩)
is assigned and the sum of the weights of all rules
sharing the same left-hand side in a grammar is
unity.
Hierarchical phrase-based SMT (Hiero) (Chi-
ang, 2007) translates by using synchronous rules
that only have two nonterminal labels X and 5 but
have no linguistic information. SAMT augments
the Hiero-style rules with syntax labels from a
parser and extends these labels based on CCG.
Although the use of extended syntax labels may
increase the coverage of rules and improve the
potential for syntactically correct translations, the
growth of the nonterminal symbols significantly
affects the speed of decoding and causes a serious
data-sparseness problem.
To address these problems, Hanneman and
Lavie (2013) proposed a label-collapsing algo-
rithm, in which syntax labels are clustered by us-
ing the similarity of the probabilistic distributions
of clustered labels in synchronous rules. First,
Hanneman and Lavie defined the label-alignment
distribution as
</bodyText>
<equation confidence="0.9987125">
P(s|t) _ #(s, t) (1)
#(t)
</equation>
<bodyText confidence="0.999890555555556">
where Nσ and Nτ are the source- and target-side
nonterminals in synchronous rules, s ∈ Nσ and
t ∈ Nτ are syntax labels from the source and tar-
get sides, #(s, t) denotes the number of left-hand-
side label pairs, and #(t) denotes the number of
target-side labels. Second, for each target-side la-
bel pair (ti, tj), we calculate the total distance d of
the absolute differences in the likelihood of labels
that are aligned to a source-side label s:
</bodyText>
<equation confidence="0.9069325">
d(ti, tj) _ � |P(s|ti) − P(s|tj) |(2)
s∈N,
</equation>
<bodyText confidence="0.9998266">
Next, the closest syntax-label pair of t� and P is
combined into a new single label. The agglomera-
tive clustering is applied iteratively until the num-
ber of the syntax labels reaches a given value.
The clustering of Hanneman and Lavie proved
successful in decreasing the grammar size and pro-
viding a statistically significant improvement in
translation quality. However, their method relies
on an agglomerative clustering with a worst-case
time complexity of O(|N|2 log |N|). Also, clus-
tering based on label distributions does not al-
ways imply higher-quality rules, because it does
not consider the interactions of the nonterminals
on the left-hand side and the right-hand side in
each synchronous rule.
</bodyText>
<sectionHeader confidence="0.995429" genericHeader="method">
3 Syntax-Label Clustering
</sectionHeader>
<bodyText confidence="0.999632111111111">
As an alternative to using the similarity of proba-
bilistic distributions as a criterion for syntax-label
clustering, we propose a clustering method based
on the maximum likelihood of the synchronous
rules in a training data D. We uses the idea
of maximizing the Bayesian posterior probability
P(M|D) of the overall model structure M given
data D (Stolcke and Omohundro, 1994). While
their goal is to maximize the posterior
</bodyText>
<equation confidence="0.995633">
P(M|D) ∝ P(M)P(D|M) (3)
</equation>
<bodyText confidence="0.9998744">
we omit the prior term P(M) and directly max-
imize the P(D|M). A model M is a clustering
structure&apos; . The synchronous rule in the data D
for SAMT with target-side syntax labels is repre-
sented as
</bodyText>
<equation confidence="0.988847">
X → ⟨a1Y (1)a2Z(2)a3, b1Y (1)b2Z(2)b3⟩ (4)
</equation>
<bodyText confidence="0.999995">
where a1, a2, a3 and b1, b2, b3 are the source- and
target-side terminals, respectively X, Y , Z are
nonterminal syntax labels, and the superscript
number indicates alignment between the source-
and target-side nonterminals. Using Equation (4)
we maximize the posterior probability P(D|M)
which we define as the probability of right-hand
side given the syntax label X of the left-hand side
rule in the training data as follows:
</bodyText>
<equation confidence="0.9803675">
� log Pr(⟨a,Q, ∼⟩|X) (5)
X→⟨α,β,∼⟩∈D
</equation>
<bodyText confidence="0.9998616">
For the sake of simplicity, we assume that the
generative probability for each rule does not de-
pend on the existence of terminal symbols and that
the reordering in the target side may be ignored.
Therefore, Equation (5) simplifies to
</bodyText>
<equation confidence="0.92709">
E log p(Y,Z|X) (6)
X→⟨a1Y (1)a2Z(2)a3,b1Y (1)b2Z(2)b3⟩
&apos;P(M) is reflected by the number of clusters.
</equation>
<page confidence="0.988464">
166
</page>
<subsectionHeader confidence="0.997896">
3.1 Optimization Criterion
</subsectionHeader>
<bodyText confidence="0.999879666666667">
The generative probability in each rule of the form
of Equation (6) can be approximated by clustering
nonterminal symbols as follows:
</bodyText>
<equation confidence="0.9822025">
p(Y, Z|X) ^ p(Y |c(Y )) · p(Z|c(Z))
·p(c(Y ), c(Z)|c(X)) (7)
</equation>
<bodyText confidence="0.999986285714286">
where we map a syntax label X to its equivalence
cluster c(X). This can be regarded as the cluster-
ing criterion usually used in a class-based n-gram
language model (Brown et al., 1992). If each label
on the right-hand side of a synchronous rule (4) is
independent of each other, we can factor the joint
model as follows:
</bodyText>
<equation confidence="0.9873735">
p(Y, Z|X) ^ p(Y |c(Y )) · p(Z|c(Z))
·p(c(Y )|c(X))·p(c(Z)|c(X)) (8)
</equation>
<bodyText confidence="0.999829">
We introduce the predictive idea of Uszkoreit and
Brants (2008) to Equation (8), which doesn’t con-
dition on the clustered label c(X), but directly on
the syntax label X:
</bodyText>
<equation confidence="0.940005166666667">
p(Y, Z|X) ^ p(Y |c(Y )) · p(Z|c(Z))
·p(c(Y )|X) · p(c(Z)|X) (9)
The objective in Equation (9) is represented using
the frequency in the training data as
N(X, c(Z))(10)
N(X)
</equation>
<bodyText confidence="0.999918714285714">
where N(X) and N(c(X)) denote the frequency2
of X and c(X), and N(X, K) denotes the fre-
quency of cluster K in the right-hand side of a
synchronous rule whose left-hand side syntax la-
bel is X. By replacing the rule probabilities in
Equation (9) with Equation (10) and plugging the
result into Equation (6), our objective becomes
</bodyText>
<equation confidence="0.966113333333333">
N(Y )
N(Y ) · log N(c(Y ))
+XEE EC N(X, K)
N(X, K) · log N(X)
∑_ N(Y ) · log N(Y )
YEN
∑− N(Y ) · logN(c(Y))
YEN
+ ∑ N(X, K) · log N(X, K)
XEN,KEC
∑− N(X,K) · log N(X)(11)
XEN,KEC
</equation>
<bodyText confidence="0.77252925">
2We use a fractional count (Chiang, 2007) which adds up
to one as a frequency.
start with the initial mapping (label X --+ c(X))
compute objective function F(C)
</bodyText>
<tableCaption confidence="0.983643">
Table 1: Outline of syntax-label clustering method
</tableCaption>
<bodyText confidence="0.977866">
where C denotes all clusters and N denotes all
syntax labels. For Equation (11), the last summa-
tion is equivalent to the sum of the occurrences
of all syntax labels, and canceled out by the first
summation. K in the third summation consid-
ers clusters in a synchronous rule whose left-hand
side label is X, and we let ch(X) denote a set
of those clusters. The second summation equals
∑KEC N(K) · log N(K). As a result, Equation
(11) simplifies to
</bodyText>
<equation confidence="0.9734825">
F(C) _ ∑ N(X, K) · log N(X, K)
XEN,KEch(X)
∑− N(K) · log N(K) (12)
KEC
</equation>
<subsectionHeader confidence="0.997968">
3.2 Exchange Clustering
</subsectionHeader>
<bodyText confidence="0.999920625">
We used an exchange clustering algorithm
(Uszkoreit and Brants, 2008) which was proven
to be very efficient in word clustering with a vo-
cabulary of over 1 million words. The exchange
clustering for words begins with the initial cluster-
ing of words and greedily exchanges words from
one cluster to another such that an optimization
criterion is maximized after the move. While ag-
glomerative clustering requires recalculation for
all pair-wise distances between words, exchange
clustering only demands computing the difference
of the objective for the word pair involved in a par-
ticular movement. We applied this exchange clus-
tering to syntax-label clustering. Table 1 shows
the outline. For initial clustering, we partitioned
all the syntax labels into clusters according to the
frequency of syntax labels in synchronous rules. If
remove and move are as computationally inten-
sive as computing the change in F(C) in Equation
(12), then the time complexity of remove and
move is O(K) (Martin et al., 1998), where K is
the number of clusters. Since the remove proce-
dure is called once for each label and, for a given
label, the move procedure is called K − 1 times
</bodyText>
<equation confidence="0.464931">
for each label X do
remove label X from c(X)
for each cluster K do
</equation>
<bodyText confidence="0.7444322">
move label X tentatively to cluster K
compute F(C) for this exchange
move label X to cluster with maximum F(C)
do until the cluster mapping does not change
N(Y ) N(X, c(Y )) N(Z)
</bodyText>
<equation confidence="0.83299425">
N(c(Y )) N(X) · N(c(Z))
∑
F(C) _
YEN
</equation>
<page confidence="0.930991">
167
</page>
<table confidence="0.9988174">
Data Lang sent Training tgt-tokens Development sent Test
src-tokens sent tgt-tokens tgt-tokens
IWSLT07 JtoE 40 K 483 K 369 K 500 7.4 K 489 3.7 K
FBIS C to E 302 K 2.7 M 3.4 M 1,664 47 K 919 30 K
NIST08 1 M 15 M 17 M
</table>
<tableCaption confidence="0.8195">
Table 2: Data sets: The “sent” column indicates the number of sentences. The “src-tokens” and “tgt-
tokens” columns indicate the number of words in the source- and the target-side sentences.
</tableCaption>
<bodyText confidence="0.999535">
to find the maximum F(C), the worst-time com-
plexity for one iteration of the syntax-label clus-
tering is O(|N|K2). The exchange procedure is
continued until the cluster mapping is stable or the
number of iterations reaches a threshold value of
100.
</bodyText>
<sectionHeader confidence="0.999829" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.922041">
4.1 Data
</subsectionHeader>
<bodyText confidence="0.999962571428571">
We conducted experiments on Japanese-English
(ja-en) and Chinese-English (zh-en) translation
tasks. The ja-en data comes from IWSLT07
(Fordyce, 2007) in a spoken travel domain. The
tuning set has seven English references and the test
set has six English references. For zh-en data we
prepared two kind of data. The one is extracted
from FBIS3, which is a collection of news arti-
cles. The other is 1 M sentences extracted ron-
domly from NIST Open MT 2008 task (NIST08).
We use the NIST Open MT 2006 for tuning and
the MT 2003 for testing. The tuning and test sets
have four English references. Table 2 shows the
details for each corpus. Each corpus is tokenized,
put in lower-case, and sentences with over 40 to-
kens on either side are removed from the training
data. We use KyTea (Neubig et al., 2011) to to-
kenize the Japanese data and Stanford Word Seg-
menter (Tseng et al., 2005) to tokenize the Chinese
data. We parse the English data with the Berkeley
parser (Petrov and Klein, 2007).
</bodyText>
<subsectionHeader confidence="0.99523">
4.2 Experiment design
</subsectionHeader>
<bodyText confidence="0.999722375">
We did experiments with the SAMT (Zollmann
and Venugopal, 2006) model with the Moses
(Koehn et al., 2007). For the SAMT model, we
conducted experiments with two label sets. One
is extracted from the phrase structure parses and
the other is extended with CCG4. We applied the
proposed method (+clustering) and the baseline
method (+coarsening), which uses the Hanneman
</bodyText>
<footnote confidence="0.906267">
3LDC2003E14
4Using the relax-parse with option SAMT 4 for IWSLT07
and FBIS and SAMT 2 for NIST08 in the Moses
</footnote>
<table confidence="0.999463">
Label set Label Rule F(C) SD
parse 63 0.3 K - -
CCG 3,147 4.2 M - -
+ coarsening 80 2.4 M -3.8 e+08 249
+ clustering 80 3.8 M -7.2 e+07 73
</table>
<tableCaption confidence="0.989246">
Table 3: SAMT grammars on ja-en experiments
</tableCaption>
<table confidence="0.9997948">
Label set Label Rule F(C) SD
FBIS
parse 70 2.1 M - -
CCG 5,460 60 M - -
+ coarsening 80 32 M -1.5 e+10 526
+ clustering 80 38 M -7.9 e+09 154
NIST08 70 12 M - -
parse
CCG 7,328 120 M - -
+ clustering 80 100 M -2.6 e+10 218
</table>
<tableCaption confidence="0.999558">
Table 4: SAMT grammars on zh-en experiments
</tableCaption>
<bodyText confidence="0.999918461538461">
label-collapsing algorithm described in Section 2,
for syntax-label clustering to the SAMT models
with CCG. The number of clusters for each clus-
tering was set to 80. The language models were
built using SRILM Toolkits (Stolcke, 2002). The
language model with the IWSLT07 is a 5-gram
model trained on the training data, and the lan-
guage model with the FBIS and NIST08 is a 5-
gram model trained on the Xinhua portion of En-
glish GigaWord. For word alignments, we used
MGIZA++ (Gao and Vogel, 2008). To tune the
weights for BLEU (Papineni et al., 2002), we used
the n-best batch MIRA (Cherry and Foster, 2012).
</bodyText>
<sectionHeader confidence="0.999277" genericHeader="evaluation">
5 Results and analysis
</sectionHeader>
<bodyText confidence="0.999923818181818">
Tables 3 and 4 present the details of SAMT gram-
mars with each label set learned by the exper-
iments using the IWSLT07 (ja-en), FBIS and
NIST08 (zh-en), which include the number of syn-
tax labels and synchronous rules, the values of the
objective (F(C)), and the standard deviation (SD)
of the number of labels assigned to each cluster.
For NIST08 we applied only the + clustering be-
cause the + coarsening needs a huge amount of
computation time. Table 5 shows the differences
between the BLEU score and the rule number for
</bodyText>
<page confidence="0.996796">
168
</page>
<bodyText confidence="0.999947142857143">
each cluster number when using the IWSLT07
dataset.
Since the +clustering maximizes the likelihood
of synchronous rules, it can introduce appropriate
rules adapted to training data given a fixed number
of clusters. For each experiment, SAMT gram-
mars with the +clustering have a greater number
of rules than with the +coarsening and, as shown
in Table 5, the number of synchronous rules with
+clustering increase with the number of clusters.
For +clustering with eight clusters and +coars-
ening with 80 clusters, which have almost 2.�M
rules, the BLEU score of +clustering with eight
clusters is higher. Also, the SD of the number
of labels, which indicates the balance of the num-
ber of labels among clusters, with +clustering is
smaller than with +coarsening. These results sug-
gest that +clustering maintain a large-scale varia-
tion of synchronous rules for high performance by
balancing the number of labels in each cluster.
The number of synchronous rules grows as you
progress from +coarsening to +clustering and fi-
nally to raw label with CCG. To confirm the ef-
fect of the number of rules, we measured the de-
coding time per sentence for translating the test
set by taking the average of ten runs with FBIS
corpus. +coarsening takes 0.14 s and +clustering
takes 0.16 s while raw label with CCG takes 0.37s.
Thus the increase in the number of synchronous
rules adversely affects the decoding speed.
Table 6 presents the results for the experiments5
using ja-en and zh-en with the BLEU metric.
SAMT with parse have the lowest BLEU scores.
It appears that the linguistic information of the
raw syntax labels of the phrase structure parses
is not enough to improve the translation perfor-
mance. Hiero has the higher BLEU score than
SAMT with CCG on zh-en. This is likely due to
the low accuracy of the parses, on which SAMT
relies while Hiero doesn’t. SAMT with + clus-
tering have the higher BLEU score than raw label
with CCG. For SAMT with CCG using IWSLT07
and FBIS, though the statistical significance tests
were not significant when p &lt; 0.05, +clustering
have the higher BLEU scores than +coarsening.
For these results, the performance of +clustering
is comparable to that of +coarsening. For the
complexity of both clustering algorithm, though it
is difficult to evaluate directly because the speed
</bodyText>
<footnote confidence="0.9727835">
5As another baseline, we also used Phrase-based SMT
(Koehn et al., 2003) and Hiero (Chiang, 2007).
</footnote>
<table confidence="0.99732975">
+clustering +coarsening
Cluster 80 40 8 4 80
BLEU 50.21 49.49 49.96 50.25 49.54
Rule 3.8 M 3.5 M 2.4 M 2.2 M 2.4 M
</table>
<tableCaption confidence="0.9292455">
Table 5: BLEU score and rule number for each
cluster number using IWSLT07
</tableCaption>
<table confidence="0.89677425">
ja-en zh-en
Model parse CCG parse CCG parse CCG
Hiero 48.91 28.31 27.62
PB-SMT 49.14 26.88 26.71
</table>
<tableCaption confidence="0.998109">
Table 6: BLEU scores on each experiments
</tableCaption>
<bodyText confidence="0.999770285714286">
depends on how each algorithm is implemented,
+clustering is an order of magnitude faster than
+coarsening. For the clustering experiment that
groups 5460 raw labels with CCG into 80 clus-
ters using FBIS corpus, +coarsening takes about
1 week whereas +clustering takes about 10 min-
utes.
</bodyText>
<sectionHeader confidence="0.996167" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.99995925">
In this paper, we propose syntax-label clustering
for SAMT, which uses syntax-label information to
generate syntactically correct translations. One of
the problems of SAMT is the large grammar size
when a CCG-style extended label set is used in the
grammar, which make decoding slower. We clus-
ter syntax labels with a very fast exchange algo-
rithm in which the generative probabilities of syn-
chronous rules are maximized. We demonstrate
the effectiveness of the proposed method by us-
ing it to translate Japanese-English and Chinese-
English tasks and measuring the decoding speed,
the accuracy and the clustering speed. Future work
involves improving the optimization criterion. We
expect to make a new objective that includes the
terminal symbols and the reordering of nontermi-
nal symbols that were ignored in this work. An-
other interesting direction is to determine the ap-
propriate number of clusters for each corpus and
the initialization method for clustering.
</bodyText>
<sectionHeader confidence="0.998812" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999788">
We thank the anonymous reviewers for their sug-
gestions and helpful comments on the early ver-
sion of this paper.
</bodyText>
<figure confidence="0.99662525">
42.58 48.77
- 49.54
- 50.21
23.66 26.97
- 27.12
- 27.47
SAMT
+coarsening
+clustering
24.67 27.28
- -
- 27.29
</figure>
<page confidence="0.992686">
169
</page>
<sectionHeader confidence="0.995854" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999207810810811">
Peter F. Brown, Vincent J. Della Pietra, Peter V. deS-
ouza, Jenifer C. Lai, and Robert L. Mercer. 1992.
Class-based n-gram models of natural language.
Computational Linguistics, 18(4):467–479.
Colin Cherry and George Foster. 2012. Batch tuning
strategies for statistical machine translation. In Pro-
ceedings of the 2012 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
427–436, Montr´eal, Canada, June. Association for
Computational Linguistics.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, pages 201–228,
June.
David Chiang. 2010. Learning to translate with source
and target syntax. In Proceedings of the 48th Annual
Meeting of the Association for Computational Lin-
guistics, pages 1443–1452, Uppsala, Sweden, July.
Association for Computational Linguistics.
Cameron Shaw Fordyce. 2007. Overview of the 4th
international workshop on spoken language transla-
tion iwslt 2007 evaluation campaign. In In Proceed-
ings of IWSLT 2007, pages 1–12, Trento, Italy, Oc-
tober.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Pro-
ceedings of the 21st International Conference on
Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 961–968, Sydney, Australia, July. Asso-
ciation for Computational Linguistics.
Qin Gao and Stephan Vogel. 2008. Parallel implemen-
tations of word alignment tool. In Software Engi-
neering, Testing, and Quality Assurance for Natu-
ral Language Processing, pages 49–57, Columbus,
Ohio, June. Association for Computational Linguis-
tics.
Greg Hanneman and Alon Lavie. 2013. Improving
syntax-augmented machine translation by coarsen-
ing the label set. In Proceedings of the 2013 Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 288–297, Atlanta, Geor-
gia, June. Association for Computational Linguis-
tics.
Bryant Huang and Kevin Knight. 2006. Relabeling
syntax trees to improve syntax-based machine trans-
lation quality. In Proceedings of the Human Lan-
guage Technology Conference of the NAACL, Main
Conference, pages 240–247, New York City, USA,
June. Association for Computational Linguistics.
Phillip Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In In
Proceedings of HLT-NAACL, pages 48–54, Edmon-
ton, Canada, May/July.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics Com-
panion Volume Proceedings of the Demo and Poster
Sessions, pages 177–180, Prague, Czech Republic,
June. Association for Computational Linguistics.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment template for statistical machine
translation. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
44th Annual Meeting of the Association for Compu-
tational Linguistics, pages 609–616, Sydney, Aus-
tralia, July. Association for Computational Linguis-
tics.
Sven Martin, Jorg Liermann, and Hermann Ney. 1998.
Algorithms for bigram and trigram word clustering.
In Speech Communication, pages 19–37.
Markos Mylonakis and Khalil Sima’an. 2011. Learn-
ing hierarchical translation structure with linguis-
tic annotations. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
642–652, Portland, Oregon, USA, June. Association
for Computational Linguistics.
Graham Neubig, Yosuke Nakata, and Shinsuke Mori.
2011. Pointwise prediction for robust, adaptable
japanese morphological analysis. In Proceedings of
the 49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, pages 529–533, Portland, Oregon, USA, June.
Association for Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In Proceedings
of 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311–318, Philadelphia,
Pennsylvania, USA, July. Association for Computa-
tional Linguistics.
Slav Petrov and Dan Klein. 2007. Improved infer-
ence for unlexicalized parsing. In Human Language
Technologies 2007: The Conference of the North
American Chapter of the Association for Computa-
tional Linguistics; Proceedings of the Main Confer-
ence, pages 404–411, Rochester, New York, April.
Association for Computational Linguistics.
Mark Steedman. 2000. The syntactic process, vol-
ume 27. MIT Press.
Andreas Stolcke and Stephen Omohundro. 1994. In-
ducing probabilistic grammars by bayesian model
</reference>
<page confidence="0.970062">
170
</page>
<reference confidence="0.999741782608696">
merging. In R. C. Carrasco and J. Oncina, editors,
Grammatical Inference and Applications (ICGI-94),
pages 106–118. Berlin, Heidelberg.
Andreas Stolcke. 2002. Srilm an extensible language
modeling toolkit. In In Proceedings of the Seventh
International Conference on Spoken Language Pro-
cessing, pages 901–904.
Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel
Jurafsky, and Christopher Manning. 2005. A con-
ditional random field word segmenter. In Fourth
SIGHAN Workshop on Chinese Language Process-
ing, pages 168–171. Jeju Island, Korea.
Jakob Uszkoreit and Thorsten Brants. 2008. Dis-
tributed word clustering for large scale class-based
language modeling in machine translation. In Pro-
ceedings of ACL-08: HLT, pages 755–762, Colum-
bus, Ohio, June. Association for Computational Lin-
guistics.
Andreas Zollmann and Ashish Venugopal. 2006. Syn-
tax augmented machine translation via chart parsing.
In Proceedings on the Workshop on Statistical Ma-
chine Translation, pages 138–141, New York City,
June. Association for Computational Linguistics.
</reference>
<page confidence="0.998206">
171
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.899962">
<title confidence="0.993318">Syntax-Augmented Machine Translation using Syntax-Label Clustering</title>
<author confidence="0.969418">Taro Watanabe Mino</author>
<affiliation confidence="0.994436">National Institute of Information and Communications</affiliation>
<address confidence="0.976974">3-5 Hikaridai, Seika-cho, Soraku-gun, Kyoto,</address>
<email confidence="0.990035">taro.watanabe,</email>
<abstract confidence="0.998320541666667">Recently, syntactic information has helped significantly to improve statistical machine translation. However, the use of syntactic information may have a negative impact on the speed of translation because of the large number of rules, especially when syntax labels are projected from a parser in syntax-augmented machine translation. In this paper, we propose a syntax-label clustering method that uses an exchange algorithm in which syntax labels are clustered together to reduce the number of rules. The proposed method achieves clustering by directly maximizing the likelihood of synchronous rules, whereas previous work considered only the similarity of probabilistic distributions of labels. We tested the proposed method on Japanese-English and Chinese-English translation tasks and found order-of-magnitude higher clustering speeds for reducing labels and gains in translation quality compared with previous clustering method.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Vincent J Della Pietra</author>
<author>Peter V deSouza</author>
<author>Jenifer C Lai</author>
<author>Robert L Mercer</author>
</authors>
<title>Class-based n-gram models of natural language.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<volume>18</volume>
<issue>4</issue>
<contexts>
<context position="8284" citStr="Brown et al., 1992" startWordPosition="1322" endWordPosition="1325">al symbols and that the reordering in the target side may be ignored. Therefore, Equation (5) simplifies to E log p(Y,Z|X) (6) X→⟨a1Y (1)a2Z(2)a3,b1Y (1)b2Z(2)b3⟩ &apos;P(M) is reflected by the number of clusters. 166 3.1 Optimization Criterion The generative probability in each rule of the form of Equation (6) can be approximated by clustering nonterminal symbols as follows: p(Y, Z|X) ^ p(Y |c(Y )) · p(Z|c(Z)) ·p(c(Y ), c(Z)|c(X)) (7) where we map a syntax label X to its equivalence cluster c(X). This can be regarded as the clustering criterion usually used in a class-based n-gram language model (Brown et al., 1992). If each label on the right-hand side of a synchronous rule (4) is independent of each other, we can factor the joint model as follows: p(Y, Z|X) ^ p(Y |c(Y )) · p(Z|c(Z)) ·p(c(Y )|c(X))·p(c(Z)|c(X)) (8) We introduce the predictive idea of Uszkoreit and Brants (2008) to Equation (8), which doesn’t condition on the clustered label c(X), but directly on the syntax label X: p(Y, Z|X) ^ p(Y |c(Y )) · p(Z|c(Z)) ·p(c(Y )|X) · p(c(Z)|X) (9) The objective in Equation (9) is represented using the frequency in the training data as N(X, c(Z))(10) N(X) where N(X) and N(c(X)) denote the frequency2 of X an</context>
</contexts>
<marker>Brown, Pietra, deSouza, Lai, Mercer, 1992</marker>
<rawString>Peter F. Brown, Vincent J. Della Pietra, Peter V. deSouza, Jenifer C. Lai, and Robert L. Mercer. 1992. Class-based n-gram models of natural language. Computational Linguistics, 18(4):467–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Cherry</author>
<author>George Foster</author>
</authors>
<title>Batch tuning strategies for statistical machine translation.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>427--436</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montr´eal, Canada,</location>
<contexts>
<context position="14759" citStr="Cherry and Foster, 2012" startWordPosition="2498" endWordPosition="2501">zh-en experiments label-collapsing algorithm described in Section 2, for syntax-label clustering to the SAMT models with CCG. The number of clusters for each clustering was set to 80. The language models were built using SRILM Toolkits (Stolcke, 2002). The language model with the IWSLT07 is a 5-gram model trained on the training data, and the language model with the FBIS and NIST08 is a 5- gram model trained on the Xinhua portion of English GigaWord. For word alignments, we used MGIZA++ (Gao and Vogel, 2008). To tune the weights for BLEU (Papineni et al., 2002), we used the n-best batch MIRA (Cherry and Foster, 2012). 5 Results and analysis Tables 3 and 4 present the details of SAMT grammars with each label set learned by the experiments using the IWSLT07 (ja-en), FBIS and NIST08 (zh-en), which include the number of syntax labels and synchronous rules, the values of the objective (F(C)), and the standard deviation (SD) of the number of labels assigned to each cluster. For NIST08 we applied only the + clustering because the + coarsening needs a huge amount of computation time. Table 5 shows the differences between the BLEU score and the rule number for 168 each cluster number when using the IWSLT07 dataset</context>
</contexts>
<marker>Cherry, Foster, 2012</marker>
<rawString>Colin Cherry and George Foster. 2012. Batch tuning strategies for statistical machine translation. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 427–436, Montr´eal, Canada, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<pages>201--228</pages>
<contexts>
<context position="1904" citStr="Chiang, 2007" startWordPosition="264" endWordPosition="265">on (SMT) models that use syntactic information have received significant research attention. These models use syntactic information on the source side (Liu et al., 2006; Mylonakis and Sima’an, 2011), the target side (Galley et al., 2006; Huang and Knight, 2006) or both sides (Chiang, 2010; Hanneman and Lavie, 2013) produce syntactically correct translations. Zollmann and Venugopal (2006) proposed syntax-augmented MT (SAMT), which is a MT system that uses syntax labels of a parser. The SAMT grammar directly encodes syntactic information into the synchronous contextfree grammar (SCFG) of Hiero (Chiang, 2007), which relies on two nonterminal labels. One problem in adding syntax labels to Hiero-style rules is that only partial phrases are assigned labels. It is common practice to extend labels by using the idea of combinatory categorial grammar (CCG) (Steedman, 2000) on the problem. Although this extended syntactical information may improve the coverage of rules and syntactic correctness in translation, the increased grammar size causes serious speed and data-sparseness problems. To address these problems, Hanneman and Lavie (2013) coarsen syntactic labels using the similarity of the probabilistic </context>
<context position="4348" citStr="Chiang, 2007" startWordPosition="679" endWordPosition="681">re X ∈ N is a nonterminal, a ∈ (N ∪ Tσ)∗ is a sequence of nonterminals or source-side terminals, and Q ∈ (N ∪ Tτ)∗ is a sequence of nonterminals or target-side terminals. The number #NT (a) of nonterminals in a is equal to the number #NT (Q) of nonterminals in Q, and ∼: {1, ..., #NT (a)} → {1, ..., #NT (Q)} is a one-to-one mapping from nonterminals in a to nonterminals in Q. For each synchronous rule, a nonnegative real-value weight w(X → ⟨a, Q, ∼⟩) is assigned and the sum of the weights of all rules sharing the same left-hand side in a grammar is unity. Hierarchical phrase-based SMT (Hiero) (Chiang, 2007) translates by using synchronous rules that only have two nonterminal labels X and 5 but have no linguistic information. SAMT augments the Hiero-style rules with syntax labels from a parser and extends these labels based on CCG. Although the use of extended syntax labels may increase the coverage of rules and improve the potential for syntactically correct translations, the growth of the nonterminal symbols significantly affects the speed of decoding and causes a serious data-sparseness problem. To address these problems, Hanneman and Lavie (2013) proposed a label-collapsing algorithm, in whic</context>
<context position="9381" citStr="Chiang, 2007" startWordPosition="1531" endWordPosition="1532">using the frequency in the training data as N(X, c(Z))(10) N(X) where N(X) and N(c(X)) denote the frequency2 of X and c(X), and N(X, K) denotes the frequency of cluster K in the right-hand side of a synchronous rule whose left-hand side syntax label is X. By replacing the rule probabilities in Equation (9) with Equation (10) and plugging the result into Equation (6), our objective becomes N(Y ) N(Y ) · log N(c(Y )) +XEE EC N(X, K) N(X, K) · log N(X) ∑_ N(Y ) · log N(Y ) YEN ∑− N(Y ) · logN(c(Y)) YEN + ∑ N(X, K) · log N(X, K) XEN,KEC ∑− N(X,K) · log N(X)(11) XEN,KEC 2We use a fractional count (Chiang, 2007) which adds up to one as a frequency. start with the initial mapping (label X --+ c(X)) compute objective function F(C) Table 1: Outline of syntax-label clustering method where C denotes all clusters and N denotes all syntax labels. For Equation (11), the last summation is equivalent to the sum of the occurrences of all syntax labels, and canceled out by the first summation. K in the third summation considers clusters in a synchronous rule whose left-hand side label is X, and we let ch(X) denote a set of those clusters. The second summation equals ∑KEC N(K) · log N(K). As a result, Equation (1</context>
<context position="17705" citStr="Chiang, 2007" startWordPosition="2999" endWordPosition="3000">curacy of the parses, on which SAMT relies while Hiero doesn’t. SAMT with + clustering have the higher BLEU score than raw label with CCG. For SAMT with CCG using IWSLT07 and FBIS, though the statistical significance tests were not significant when p &lt; 0.05, +clustering have the higher BLEU scores than +coarsening. For these results, the performance of +clustering is comparable to that of +coarsening. For the complexity of both clustering algorithm, though it is difficult to evaluate directly because the speed 5As another baseline, we also used Phrase-based SMT (Koehn et al., 2003) and Hiero (Chiang, 2007). +clustering +coarsening Cluster 80 40 8 4 80 BLEU 50.21 49.49 49.96 50.25 49.54 Rule 3.8 M 3.5 M 2.4 M 2.2 M 2.4 M Table 5: BLEU score and rule number for each cluster number using IWSLT07 ja-en zh-en Model parse CCG parse CCG parse CCG Hiero 48.91 28.31 27.62 PB-SMT 49.14 26.88 26.71 Table 6: BLEU scores on each experiments depends on how each algorithm is implemented, +clustering is an order of magnitude faster than +coarsening. For the clustering experiment that groups 5460 raw labels with CCG into 80 clusters using FBIS corpus, +coarsening takes about 1 week whereas +clustering takes abo</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, pages 201–228, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Learning to translate with source and target syntax.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1443--1452</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="1580" citStr="Chiang, 2010" startWordPosition="217" endWordPosition="218">distributions of labels. We tested the proposed method on Japanese-English and Chinese-English translation tasks and found order-of-magnitude higher clustering speeds for reducing labels and gains in translation quality compared with previous clustering method. 1 Introduction In recent years, statistical machine translation (SMT) models that use syntactic information have received significant research attention. These models use syntactic information on the source side (Liu et al., 2006; Mylonakis and Sima’an, 2011), the target side (Galley et al., 2006; Huang and Knight, 2006) or both sides (Chiang, 2010; Hanneman and Lavie, 2013) produce syntactically correct translations. Zollmann and Venugopal (2006) proposed syntax-augmented MT (SAMT), which is a MT system that uses syntax labels of a parser. The SAMT grammar directly encodes syntactic information into the synchronous contextfree grammar (SCFG) of Hiero (Chiang, 2007), which relies on two nonterminal labels. One problem in adding syntax labels to Hiero-style rules is that only partial phrases are assigned labels. It is common practice to extend labels by using the idea of combinatory categorial grammar (CCG) (Steedman, 2000) on the proble</context>
</contexts>
<marker>Chiang, 2010</marker>
<rawString>David Chiang. 2010. Learning to translate with source and target syntax. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1443–1452, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cameron Shaw Fordyce</author>
</authors>
<title>Overview of the 4th international workshop on spoken language translation iwslt</title>
<date>2007</date>
<booktitle>In Proceedings of IWSLT</booktitle>
<pages>1--12</pages>
<location>Trento, Italy,</location>
<contexts>
<context position="12366" citStr="Fordyce, 2007" startWordPosition="2054" endWordPosition="2055">17 M Table 2: Data sets: The “sent” column indicates the number of sentences. The “src-tokens” and “tgttokens” columns indicate the number of words in the source- and the target-side sentences. to find the maximum F(C), the worst-time complexity for one iteration of the syntax-label clustering is O(|N|K2). The exchange procedure is continued until the cluster mapping is stable or the number of iterations reaches a threshold value of 100. 4 Experiments 4.1 Data We conducted experiments on Japanese-English (ja-en) and Chinese-English (zh-en) translation tasks. The ja-en data comes from IWSLT07 (Fordyce, 2007) in a spoken travel domain. The tuning set has seven English references and the test set has six English references. For zh-en data we prepared two kind of data. The one is extracted from FBIS3, which is a collection of news articles. The other is 1 M sentences extracted rondomly from NIST Open MT 2008 task (NIST08). We use the NIST Open MT 2006 for tuning and the MT 2003 for testing. The tuning and test sets have four English references. Table 2 shows the details for each corpus. Each corpus is tokenized, put in lower-case, and sentences with over 40 tokens on either side are removed from the</context>
</contexts>
<marker>Fordyce, 2007</marker>
<rawString>Cameron Shaw Fordyce. 2007. Overview of the 4th international workshop on spoken language translation iwslt 2007 evaluation campaign. In In Proceedings of IWSLT 2007, pages 1–12, Trento, Italy, October.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Michel Galley</author>
<author>Jonathan Graehl</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
<author>Steve DeNeefe</author>
<author>Wei Wang</author>
<author>Ignacio Thayer</author>
</authors>
<title>Scalable inference and training of context-rich syntactic translation models.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>961--968</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sydney, Australia,</location>
<contexts>
<context position="1527" citStr="Galley et al., 2006" startWordPosition="206" endWordPosition="209">evious work considered only the similarity of probabilistic distributions of labels. We tested the proposed method on Japanese-English and Chinese-English translation tasks and found order-of-magnitude higher clustering speeds for reducing labels and gains in translation quality compared with previous clustering method. 1 Introduction In recent years, statistical machine translation (SMT) models that use syntactic information have received significant research attention. These models use syntactic information on the source side (Liu et al., 2006; Mylonakis and Sima’an, 2011), the target side (Galley et al., 2006; Huang and Knight, 2006) or both sides (Chiang, 2010; Hanneman and Lavie, 2013) produce syntactically correct translations. Zollmann and Venugopal (2006) proposed syntax-augmented MT (SAMT), which is a MT system that uses syntax labels of a parser. The SAMT grammar directly encodes syntactic information into the synchronous contextfree grammar (SCFG) of Hiero (Chiang, 2007), which relies on two nonterminal labels. One problem in adding syntax labels to Hiero-style rules is that only partial phrases are assigned labels. It is common practice to extend labels by using the idea of combinatory ca</context>
</contexts>
<marker>Galley, Graehl, Knight, Marcu, DeNeefe, Wang, Thayer, 2006</marker>
<rawString>Michel Galley, Jonathan Graehl, Kevin Knight, Daniel Marcu, Steve DeNeefe, Wei Wang, and Ignacio Thayer. 2006. Scalable inference and training of context-rich syntactic translation models. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 961–968, Sydney, Australia, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qin Gao</author>
<author>Stephan Vogel</author>
</authors>
<title>Parallel implementations of word alignment tool.</title>
<date>2008</date>
<booktitle>In Software Engineering, Testing, and Quality Assurance for Natural Language Processing,</booktitle>
<pages>49--57</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<contexts>
<context position="14648" citStr="Gao and Vogel, 2008" startWordPosition="2478" endWordPosition="2481">NIST08 70 12 M - - parse CCG 7,328 120 M - - + clustering 80 100 M -2.6 e+10 218 Table 4: SAMT grammars on zh-en experiments label-collapsing algorithm described in Section 2, for syntax-label clustering to the SAMT models with CCG. The number of clusters for each clustering was set to 80. The language models were built using SRILM Toolkits (Stolcke, 2002). The language model with the IWSLT07 is a 5-gram model trained on the training data, and the language model with the FBIS and NIST08 is a 5- gram model trained on the Xinhua portion of English GigaWord. For word alignments, we used MGIZA++ (Gao and Vogel, 2008). To tune the weights for BLEU (Papineni et al., 2002), we used the n-best batch MIRA (Cherry and Foster, 2012). 5 Results and analysis Tables 3 and 4 present the details of SAMT grammars with each label set learned by the experiments using the IWSLT07 (ja-en), FBIS and NIST08 (zh-en), which include the number of syntax labels and synchronous rules, the values of the objective (F(C)), and the standard deviation (SD) of the number of labels assigned to each cluster. For NIST08 we applied only the + clustering because the + coarsening needs a huge amount of computation time. Table 5 shows the di</context>
</contexts>
<marker>Gao, Vogel, 2008</marker>
<rawString>Qin Gao and Stephan Vogel. 2008. Parallel implementations of word alignment tool. In Software Engineering, Testing, and Quality Assurance for Natural Language Processing, pages 49–57, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Greg Hanneman</author>
<author>Alon Lavie</author>
</authors>
<title>Improving syntax-augmented machine translation by coarsening the label set.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>288--297</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Atlanta, Georgia,</location>
<contexts>
<context position="1607" citStr="Hanneman and Lavie, 2013" startWordPosition="219" endWordPosition="222">of labels. We tested the proposed method on Japanese-English and Chinese-English translation tasks and found order-of-magnitude higher clustering speeds for reducing labels and gains in translation quality compared with previous clustering method. 1 Introduction In recent years, statistical machine translation (SMT) models that use syntactic information have received significant research attention. These models use syntactic information on the source side (Liu et al., 2006; Mylonakis and Sima’an, 2011), the target side (Galley et al., 2006; Huang and Knight, 2006) or both sides (Chiang, 2010; Hanneman and Lavie, 2013) produce syntactically correct translations. Zollmann and Venugopal (2006) proposed syntax-augmented MT (SAMT), which is a MT system that uses syntax labels of a parser. The SAMT grammar directly encodes syntactic information into the synchronous contextfree grammar (SCFG) of Hiero (Chiang, 2007), which relies on two nonterminal labels. One problem in adding syntax labels to Hiero-style rules is that only partial phrases are assigned labels. It is common practice to extend labels by using the idea of combinatory categorial grammar (CCG) (Steedman, 2000) on the problem. Although this extended s</context>
<context position="4901" citStr="Hanneman and Lavie (2013)" startWordPosition="762" endWordPosition="765">a grammar is unity. Hierarchical phrase-based SMT (Hiero) (Chiang, 2007) translates by using synchronous rules that only have two nonterminal labels X and 5 but have no linguistic information. SAMT augments the Hiero-style rules with syntax labels from a parser and extends these labels based on CCG. Although the use of extended syntax labels may increase the coverage of rules and improve the potential for syntactically correct translations, the growth of the nonterminal symbols significantly affects the speed of decoding and causes a serious data-sparseness problem. To address these problems, Hanneman and Lavie (2013) proposed a label-collapsing algorithm, in which syntax labels are clustered by using the similarity of the probabilistic distributions of clustered labels in synchronous rules. First, Hanneman and Lavie defined the label-alignment distribution as P(s|t) _ #(s, t) (1) #(t) where Nσ and Nτ are the source- and target-side nonterminals in synchronous rules, s ∈ Nσ and t ∈ Nτ are syntax labels from the source and target sides, #(s, t) denotes the number of left-handside label pairs, and #(t) denotes the number of target-side labels. Second, for each target-side label pair (ti, tj), we calculate th</context>
</contexts>
<marker>Hanneman, Lavie, 2013</marker>
<rawString>Greg Hanneman and Alon Lavie. 2013. Improving syntax-augmented machine translation by coarsening the label set. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 288–297, Atlanta, Georgia, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bryant Huang</author>
<author>Kevin Knight</author>
</authors>
<title>Relabeling syntax trees to improve syntax-based machine translation quality.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference,</booktitle>
<pages>240--247</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>New York City, USA,</location>
<contexts>
<context position="1552" citStr="Huang and Knight, 2006" startWordPosition="210" endWordPosition="213">d only the similarity of probabilistic distributions of labels. We tested the proposed method on Japanese-English and Chinese-English translation tasks and found order-of-magnitude higher clustering speeds for reducing labels and gains in translation quality compared with previous clustering method. 1 Introduction In recent years, statistical machine translation (SMT) models that use syntactic information have received significant research attention. These models use syntactic information on the source side (Liu et al., 2006; Mylonakis and Sima’an, 2011), the target side (Galley et al., 2006; Huang and Knight, 2006) or both sides (Chiang, 2010; Hanneman and Lavie, 2013) produce syntactically correct translations. Zollmann and Venugopal (2006) proposed syntax-augmented MT (SAMT), which is a MT system that uses syntax labels of a parser. The SAMT grammar directly encodes syntactic information into the synchronous contextfree grammar (SCFG) of Hiero (Chiang, 2007), which relies on two nonterminal labels. One problem in adding syntax labels to Hiero-style rules is that only partial phrases are assigned labels. It is common practice to extend labels by using the idea of combinatory categorial grammar (CCG) (S</context>
</contexts>
<marker>Huang, Knight, 2006</marker>
<rawString>Bryant Huang and Kevin Knight. 2006. Relabeling syntax trees to improve syntax-based machine translation quality. In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference, pages 240–247, New York City, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Phillip Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation. In</title>
<date>2003</date>
<booktitle>In Proceedings of HLT-NAACL,</booktitle>
<pages>48--54</pages>
<location>Edmonton, Canada, May/July.</location>
<contexts>
<context position="17680" citStr="Koehn et al., 2003" startWordPosition="2993" endWordPosition="2996">his is likely due to the low accuracy of the parses, on which SAMT relies while Hiero doesn’t. SAMT with + clustering have the higher BLEU score than raw label with CCG. For SAMT with CCG using IWSLT07 and FBIS, though the statistical significance tests were not significant when p &lt; 0.05, +clustering have the higher BLEU scores than +coarsening. For these results, the performance of +clustering is comparable to that of +coarsening. For the complexity of both clustering algorithm, though it is difficult to evaluate directly because the speed 5As another baseline, we also used Phrase-based SMT (Koehn et al., 2003) and Hiero (Chiang, 2007). +clustering +coarsening Cluster 80 40 8 4 80 BLEU 50.21 49.49 49.96 50.25 49.54 Rule 3.8 M 3.5 M 2.4 M 2.2 M 2.4 M Table 5: BLEU score and rule number for each cluster number using IWSLT07 ja-en zh-en Model parse CCG parse CCG parse CCG Hiero 48.91 28.31 27.62 PB-SMT 49.14 26.88 26.71 Table 6: BLEU scores on each experiments depends on how each algorithm is implemented, +clustering is an order of magnitude faster than +coarsening. For the clustering experiment that groups 5460 raw labels with CCG into 80 clusters using FBIS corpus, +coarsening takes about 1 week wher</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Phillip Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In In Proceedings of HLT-NAACL, pages 48–54, Edmonton, Canada, May/July.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
<author>Chris Dyer</author>
<author>Ondrej Bojar</author>
<author>Alexandra Constantin</author>
<author>Evan Herbst</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions,</booktitle>
<pages>177--180</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="13330" citStr="Koehn et al., 2007" startWordPosition="2227" endWordPosition="2230">06 for tuning and the MT 2003 for testing. The tuning and test sets have four English references. Table 2 shows the details for each corpus. Each corpus is tokenized, put in lower-case, and sentences with over 40 tokens on either side are removed from the training data. We use KyTea (Neubig et al., 2011) to tokenize the Japanese data and Stanford Word Segmenter (Tseng et al., 2005) to tokenize the Chinese data. We parse the English data with the Berkeley parser (Petrov and Klein, 2007). 4.2 Experiment design We did experiments with the SAMT (Zollmann and Venugopal, 2006) model with the Moses (Koehn et al., 2007). For the SAMT model, we conducted experiments with two label sets. One is extracted from the phrase structure parses and the other is extended with CCG4. We applied the proposed method (+clustering) and the baseline method (+coarsening), which uses the Hanneman 3LDC2003E14 4Using the relax-parse with option SAMT 4 for IWSLT07 and FBIS and SAMT 2 for NIST08 in the Moses Label set Label Rule F(C) SD parse 63 0.3 K - - CCG 3,147 4.2 M - - + coarsening 80 2.4 M -3.8 e+08 249 + clustering 80 3.8 M -7.2 e+07 73 Table 3: SAMT grammars on ja-en experiments Label set Label Rule F(C) SD FBIS parse 70 2</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, Herbst, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions, pages 177–180, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Treeto-string alignment template for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>609--616</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sydney, Australia,</location>
<contexts>
<context position="1459" citStr="Liu et al., 2006" startWordPosition="195" endWordPosition="198">rectly maximizing the likelihood of synchronous rules, whereas previous work considered only the similarity of probabilistic distributions of labels. We tested the proposed method on Japanese-English and Chinese-English translation tasks and found order-of-magnitude higher clustering speeds for reducing labels and gains in translation quality compared with previous clustering method. 1 Introduction In recent years, statistical machine translation (SMT) models that use syntactic information have received significant research attention. These models use syntactic information on the source side (Liu et al., 2006; Mylonakis and Sima’an, 2011), the target side (Galley et al., 2006; Huang and Knight, 2006) or both sides (Chiang, 2010; Hanneman and Lavie, 2013) produce syntactically correct translations. Zollmann and Venugopal (2006) proposed syntax-augmented MT (SAMT), which is a MT system that uses syntax labels of a parser. The SAMT grammar directly encodes syntactic information into the synchronous contextfree grammar (SCFG) of Hiero (Chiang, 2007), which relies on two nonterminal labels. One problem in adding syntax labels to Hiero-style rules is that only partial phrases are assigned labels. It is </context>
</contexts>
<marker>Liu, Liu, Lin, 2006</marker>
<rawString>Yang Liu, Qun Liu, and Shouxun Lin. 2006. Treeto-string alignment template for statistical machine translation. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 609–616, Sydney, Australia, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sven Martin</author>
<author>Jorg Liermann</author>
<author>Hermann Ney</author>
</authors>
<title>Algorithms for bigram and trigram word clustering. In Speech Communication,</title>
<date>1998</date>
<pages>pages</pages>
<contexts>
<context position="11095" citStr="Martin et al., 1998" startWordPosition="1818" endWordPosition="1821">erative clustering requires recalculation for all pair-wise distances between words, exchange clustering only demands computing the difference of the objective for the word pair involved in a particular movement. We applied this exchange clustering to syntax-label clustering. Table 1 shows the outline. For initial clustering, we partitioned all the syntax labels into clusters according to the frequency of syntax labels in synchronous rules. If remove and move are as computationally intensive as computing the change in F(C) in Equation (12), then the time complexity of remove and move is O(K) (Martin et al., 1998), where K is the number of clusters. Since the remove procedure is called once for each label and, for a given label, the move procedure is called K − 1 times for each label X do remove label X from c(X) for each cluster K do move label X tentatively to cluster K compute F(C) for this exchange move label X to cluster with maximum F(C) do until the cluster mapping does not change N(Y ) N(X, c(Y )) N(Z) N(c(Y )) N(X) · N(c(Z)) ∑ F(C) _ YEN 167 Data Lang sent Training tgt-tokens Development sent Test src-tokens sent tgt-tokens tgt-tokens IWSLT07 JtoE 40 K 483 K 369 K 500 7.4 K 489 3.7 K FBIS C to</context>
</contexts>
<marker>Martin, Liermann, Ney, 1998</marker>
<rawString>Sven Martin, Jorg Liermann, and Hermann Ney. 1998. Algorithms for bigram and trigram word clustering. In Speech Communication, pages 19–37.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markos Mylonakis</author>
<author>Khalil Sima’an</author>
</authors>
<title>Learning hierarchical translation structure with linguistic annotations.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>642--652</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<marker>Mylonakis, Sima’an, 2011</marker>
<rawString>Markos Mylonakis and Khalil Sima’an. 2011. Learning hierarchical translation structure with linguistic annotations. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 642–652, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graham Neubig</author>
<author>Yosuke Nakata</author>
<author>Shinsuke Mori</author>
</authors>
<title>Pointwise prediction for robust, adaptable japanese morphological analysis.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>529--533</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="13016" citStr="Neubig et al., 2011" startWordPosition="2173" endWordPosition="2176">e tuning set has seven English references and the test set has six English references. For zh-en data we prepared two kind of data. The one is extracted from FBIS3, which is a collection of news articles. The other is 1 M sentences extracted rondomly from NIST Open MT 2008 task (NIST08). We use the NIST Open MT 2006 for tuning and the MT 2003 for testing. The tuning and test sets have four English references. Table 2 shows the details for each corpus. Each corpus is tokenized, put in lower-case, and sentences with over 40 tokens on either side are removed from the training data. We use KyTea (Neubig et al., 2011) to tokenize the Japanese data and Stanford Word Segmenter (Tseng et al., 2005) to tokenize the Chinese data. We parse the English data with the Berkeley parser (Petrov and Klein, 2007). 4.2 Experiment design We did experiments with the SAMT (Zollmann and Venugopal, 2006) model with the Moses (Koehn et al., 2007). For the SAMT model, we conducted experiments with two label sets. One is extracted from the phrase structure parses and the other is extended with CCG4. We applied the proposed method (+clustering) and the baseline method (+coarsening), which uses the Hanneman 3LDC2003E14 4Using the </context>
</contexts>
<marker>Neubig, Nakata, Mori, 2011</marker>
<rawString>Graham Neubig, Yosuke Nakata, and Shinsuke Mori. 2011. Pointwise prediction for robust, adaptable japanese morphological analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 529–533, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>311--318</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Philadelphia, Pennsylvania, USA,</location>
<contexts>
<context position="14702" citStr="Papineni et al., 2002" startWordPosition="2488" endWordPosition="2491">ering 80 100 M -2.6 e+10 218 Table 4: SAMT grammars on zh-en experiments label-collapsing algorithm described in Section 2, for syntax-label clustering to the SAMT models with CCG. The number of clusters for each clustering was set to 80. The language models were built using SRILM Toolkits (Stolcke, 2002). The language model with the IWSLT07 is a 5-gram model trained on the training data, and the language model with the FBIS and NIST08 is a 5- gram model trained on the Xinhua portion of English GigaWord. For word alignments, we used MGIZA++ (Gao and Vogel, 2008). To tune the weights for BLEU (Papineni et al., 2002), we used the n-best batch MIRA (Cherry and Foster, 2012). 5 Results and analysis Tables 3 and 4 present the details of SAMT grammars with each label set learned by the experiments using the IWSLT07 (ja-en), FBIS and NIST08 (zh-en), which include the number of syntax labels and synchronous rules, the values of the objective (F(C)), and the standard deviation (SD) of the number of labels assigned to each cluster. For NIST08 we applied only the + clustering because the + coarsening needs a huge amount of computation time. Table 5 shows the differences between the BLEU score and the rule number f</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics, pages 311–318, Philadelphia, Pennsylvania, USA, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dan Klein</author>
</authors>
<title>Improved inference for unlexicalized parsing. In Human Language Technologies</title>
<date>2007</date>
<booktitle>The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference,</booktitle>
<pages>404--411</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Rochester, New York,</location>
<contexts>
<context position="13201" citStr="Petrov and Klein, 2007" startWordPosition="2206" endWordPosition="2209">tion of news articles. The other is 1 M sentences extracted rondomly from NIST Open MT 2008 task (NIST08). We use the NIST Open MT 2006 for tuning and the MT 2003 for testing. The tuning and test sets have four English references. Table 2 shows the details for each corpus. Each corpus is tokenized, put in lower-case, and sentences with over 40 tokens on either side are removed from the training data. We use KyTea (Neubig et al., 2011) to tokenize the Japanese data and Stanford Word Segmenter (Tseng et al., 2005) to tokenize the Chinese data. We parse the English data with the Berkeley parser (Petrov and Klein, 2007). 4.2 Experiment design We did experiments with the SAMT (Zollmann and Venugopal, 2006) model with the Moses (Koehn et al., 2007). For the SAMT model, we conducted experiments with two label sets. One is extracted from the phrase structure parses and the other is extended with CCG4. We applied the proposed method (+clustering) and the baseline method (+coarsening), which uses the Hanneman 3LDC2003E14 4Using the relax-parse with option SAMT 4 for IWSLT07 and FBIS and SAMT 2 for NIST08 in the Moses Label set Label Rule F(C) SD parse 63 0.3 K - - CCG 3,147 4.2 M - - + coarsening 80 2.4 M -3.8 e+0</context>
</contexts>
<marker>Petrov, Klein, 2007</marker>
<rawString>Slav Petrov and Dan Klein. 2007. Improved inference for unlexicalized parsing. In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference, pages 404–411, Rochester, New York, April. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steedman</author>
</authors>
<title>The syntactic process, volume 27.</title>
<date>2000</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="2166" citStr="Steedman, 2000" startWordPosition="307" endWordPosition="308">) or both sides (Chiang, 2010; Hanneman and Lavie, 2013) produce syntactically correct translations. Zollmann and Venugopal (2006) proposed syntax-augmented MT (SAMT), which is a MT system that uses syntax labels of a parser. The SAMT grammar directly encodes syntactic information into the synchronous contextfree grammar (SCFG) of Hiero (Chiang, 2007), which relies on two nonterminal labels. One problem in adding syntax labels to Hiero-style rules is that only partial phrases are assigned labels. It is common practice to extend labels by using the idea of combinatory categorial grammar (CCG) (Steedman, 2000) on the problem. Although this extended syntactical information may improve the coverage of rules and syntactic correctness in translation, the increased grammar size causes serious speed and data-sparseness problems. To address these problems, Hanneman and Lavie (2013) coarsen syntactic labels using the similarity of the probabilistic distributions of labels in synchronous rules and showed that performance improved. In the present work, we follow the idea of labelset coarsening and propose a new method to group syntax labels. First, as an optimization criterion, we use the logarithm of the li</context>
</contexts>
<marker>Steedman, 2000</marker>
<rawString>Mark Steedman. 2000. The syntactic process, volume 27. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
<author>Stephen Omohundro</author>
</authors>
<title>Inducing probabilistic grammars by bayesian model merging. In</title>
<date>1994</date>
<booktitle>Grammatical Inference and Applications (ICGI-94),</booktitle>
<pages>106--118</pages>
<editor>R. C. Carrasco and J. Oncina, editors,</editor>
<location>Berlin, Heidelberg.</location>
<contexts>
<context position="6767" citStr="Stolcke and Omohundro, 1994" startWordPosition="1066" endWordPosition="1069">lso, clustering based on label distributions does not always imply higher-quality rules, because it does not consider the interactions of the nonterminals on the left-hand side and the right-hand side in each synchronous rule. 3 Syntax-Label Clustering As an alternative to using the similarity of probabilistic distributions as a criterion for syntax-label clustering, we propose a clustering method based on the maximum likelihood of the synchronous rules in a training data D. We uses the idea of maximizing the Bayesian posterior probability P(M|D) of the overall model structure M given data D (Stolcke and Omohundro, 1994). While their goal is to maximize the posterior P(M|D) ∝ P(M)P(D|M) (3) we omit the prior term P(M) and directly maximize the P(D|M). A model M is a clustering structure&apos; . The synchronous rule in the data D for SAMT with target-side syntax labels is represented as X → ⟨a1Y (1)a2Z(2)a3, b1Y (1)b2Z(2)b3⟩ (4) where a1, a2, a3 and b1, b2, b3 are the source- and target-side terminals, respectively X, Y , Z are nonterminal syntax labels, and the superscript number indicates alignment between the sourceand target-side nonterminals. Using Equation (4) we maximize the posterior probability P(D|M) whic</context>
</contexts>
<marker>Stolcke, Omohundro, 1994</marker>
<rawString>Andreas Stolcke and Stephen Omohundro. 1994. Inducing probabilistic grammars by bayesian model merging. In R. C. Carrasco and J. Oncina, editors, Grammatical Inference and Applications (ICGI-94), pages 106–118. Berlin, Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>Srilm an extensible language modeling toolkit. In</title>
<date>2002</date>
<booktitle>In Proceedings of the Seventh International Conference on Spoken Language Processing,</booktitle>
<pages>901--904</pages>
<contexts>
<context position="14386" citStr="Stolcke, 2002" startWordPosition="2431" endWordPosition="2432"> coarsening 80 2.4 M -3.8 e+08 249 + clustering 80 3.8 M -7.2 e+07 73 Table 3: SAMT grammars on ja-en experiments Label set Label Rule F(C) SD FBIS parse 70 2.1 M - - CCG 5,460 60 M - - + coarsening 80 32 M -1.5 e+10 526 + clustering 80 38 M -7.9 e+09 154 NIST08 70 12 M - - parse CCG 7,328 120 M - - + clustering 80 100 M -2.6 e+10 218 Table 4: SAMT grammars on zh-en experiments label-collapsing algorithm described in Section 2, for syntax-label clustering to the SAMT models with CCG. The number of clusters for each clustering was set to 80. The language models were built using SRILM Toolkits (Stolcke, 2002). The language model with the IWSLT07 is a 5-gram model trained on the training data, and the language model with the FBIS and NIST08 is a 5- gram model trained on the Xinhua portion of English GigaWord. For word alignments, we used MGIZA++ (Gao and Vogel, 2008). To tune the weights for BLEU (Papineni et al., 2002), we used the n-best batch MIRA (Cherry and Foster, 2012). 5 Results and analysis Tables 3 and 4 present the details of SAMT grammars with each label set learned by the experiments using the IWSLT07 (ja-en), FBIS and NIST08 (zh-en), which include the number of syntax labels and synch</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. Srilm an extensible language modeling toolkit. In In Proceedings of the Seventh International Conference on Spoken Language Processing, pages 901–904.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Huihsin Tseng</author>
<author>Pichuan Chang</author>
<author>Galen Andrew</author>
<author>Daniel Jurafsky</author>
<author>Christopher Manning</author>
</authors>
<title>A conditional random field word segmenter.</title>
<date>2005</date>
<booktitle>In Fourth SIGHAN Workshop on Chinese Language Processing,</booktitle>
<pages>168--171</pages>
<location>Jeju Island,</location>
<contexts>
<context position="13095" citStr="Tseng et al., 2005" startWordPosition="2188" endWordPosition="2191">ences. For zh-en data we prepared two kind of data. The one is extracted from FBIS3, which is a collection of news articles. The other is 1 M sentences extracted rondomly from NIST Open MT 2008 task (NIST08). We use the NIST Open MT 2006 for tuning and the MT 2003 for testing. The tuning and test sets have four English references. Table 2 shows the details for each corpus. Each corpus is tokenized, put in lower-case, and sentences with over 40 tokens on either side are removed from the training data. We use KyTea (Neubig et al., 2011) to tokenize the Japanese data and Stanford Word Segmenter (Tseng et al., 2005) to tokenize the Chinese data. We parse the English data with the Berkeley parser (Petrov and Klein, 2007). 4.2 Experiment design We did experiments with the SAMT (Zollmann and Venugopal, 2006) model with the Moses (Koehn et al., 2007). For the SAMT model, we conducted experiments with two label sets. One is extracted from the phrase structure parses and the other is extended with CCG4. We applied the proposed method (+clustering) and the baseline method (+coarsening), which uses the Hanneman 3LDC2003E14 4Using the relax-parse with option SAMT 4 for IWSLT07 and FBIS and SAMT 2 for NIST08 in th</context>
</contexts>
<marker>Tseng, Chang, Andrew, Jurafsky, Manning, 2005</marker>
<rawString>Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel Jurafsky, and Christopher Manning. 2005. A conditional random field word segmenter. In Fourth SIGHAN Workshop on Chinese Language Processing, pages 168–171. Jeju Island, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jakob Uszkoreit</author>
<author>Thorsten Brants</author>
</authors>
<title>Distributed word clustering for large scale class-based language modeling in machine translation.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>755--762</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<contexts>
<context position="2934" citStr="Uszkoreit and Brants, 2008" startWordPosition="424" endWordPosition="427">he increased grammar size causes serious speed and data-sparseness problems. To address these problems, Hanneman and Lavie (2013) coarsen syntactic labels using the similarity of the probabilistic distributions of labels in synchronous rules and showed that performance improved. In the present work, we follow the idea of labelset coarsening and propose a new method to group syntax labels. First, as an optimization criterion, we use the logarithm of the likelihood of synchronous rules instead of the similarity of probabilistic distributions of syntax labels. Second, we use exchange clustering (Uszkoreit and Brants, 2008), which is faster than the agglomerativeclustering algorithm used in the previous work. We tested our proposed method on JapaneseEnglish and Chinese-English translation tasks and observed gains comparable to those of previous work with similar reductions in grammar size. 2 Syntax-Augmented Machine Translation SAMT is an instance of SCFG g, which can be formally defined as 9 = (N, S, TQ, TT, R) where N is a set of nonterminals, S E N is a start label, TQ and TT are the source- and targetside terminals, and R is a set of synchronous rules. Each synchronous rule in R takes the form X — ⟨α, Q, -⟩ </context>
<context position="8552" citStr="Uszkoreit and Brants (2008)" startWordPosition="1368" endWordPosition="1371">bility in each rule of the form of Equation (6) can be approximated by clustering nonterminal symbols as follows: p(Y, Z|X) ^ p(Y |c(Y )) · p(Z|c(Z)) ·p(c(Y ), c(Z)|c(X)) (7) where we map a syntax label X to its equivalence cluster c(X). This can be regarded as the clustering criterion usually used in a class-based n-gram language model (Brown et al., 1992). If each label on the right-hand side of a synchronous rule (4) is independent of each other, we can factor the joint model as follows: p(Y, Z|X) ^ p(Y |c(Y )) · p(Z|c(Z)) ·p(c(Y )|c(X))·p(c(Z)|c(X)) (8) We introduce the predictive idea of Uszkoreit and Brants (2008) to Equation (8), which doesn’t condition on the clustered label c(X), but directly on the syntax label X: p(Y, Z|X) ^ p(Y |c(Y )) · p(Z|c(Z)) ·p(c(Y )|X) · p(c(Z)|X) (9) The objective in Equation (9) is represented using the frequency in the training data as N(X, c(Z))(10) N(X) where N(X) and N(c(X)) denote the frequency2 of X and c(X), and N(X, K) denotes the frequency of cluster K in the right-hand side of a synchronous rule whose left-hand side syntax label is X. By replacing the rule probabilities in Equation (9) with Equation (10) and plugging the result into Equation (6), our objective </context>
<context position="10162" citStr="Uszkoreit and Brants, 2008" startWordPosition="1668" endWordPosition="1671">lustering method where C denotes all clusters and N denotes all syntax labels. For Equation (11), the last summation is equivalent to the sum of the occurrences of all syntax labels, and canceled out by the first summation. K in the third summation considers clusters in a synchronous rule whose left-hand side label is X, and we let ch(X) denote a set of those clusters. The second summation equals ∑KEC N(K) · log N(K). As a result, Equation (11) simplifies to F(C) _ ∑ N(X, K) · log N(X, K) XEN,KEch(X) ∑− N(K) · log N(K) (12) KEC 3.2 Exchange Clustering We used an exchange clustering algorithm (Uszkoreit and Brants, 2008) which was proven to be very efficient in word clustering with a vocabulary of over 1 million words. The exchange clustering for words begins with the initial clustering of words and greedily exchanges words from one cluster to another such that an optimization criterion is maximized after the move. While agglomerative clustering requires recalculation for all pair-wise distances between words, exchange clustering only demands computing the difference of the objective for the word pair involved in a particular movement. We applied this exchange clustering to syntax-label clustering. Table 1 sh</context>
</contexts>
<marker>Uszkoreit, Brants, 2008</marker>
<rawString>Jakob Uszkoreit and Thorsten Brants. 2008. Distributed word clustering for large scale class-based language modeling in machine translation. In Proceedings of ACL-08: HLT, pages 755–762, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Zollmann</author>
<author>Ashish Venugopal</author>
</authors>
<title>Syntax augmented machine translation via chart parsing.</title>
<date>2006</date>
<booktitle>In Proceedings on the Workshop on Statistical Machine Translation,</booktitle>
<pages>138--141</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>New York City,</location>
<contexts>
<context position="1681" citStr="Zollmann and Venugopal (2006)" startWordPosition="227" endWordPosition="230">se-English translation tasks and found order-of-magnitude higher clustering speeds for reducing labels and gains in translation quality compared with previous clustering method. 1 Introduction In recent years, statistical machine translation (SMT) models that use syntactic information have received significant research attention. These models use syntactic information on the source side (Liu et al., 2006; Mylonakis and Sima’an, 2011), the target side (Galley et al., 2006; Huang and Knight, 2006) or both sides (Chiang, 2010; Hanneman and Lavie, 2013) produce syntactically correct translations. Zollmann and Venugopal (2006) proposed syntax-augmented MT (SAMT), which is a MT system that uses syntax labels of a parser. The SAMT grammar directly encodes syntactic information into the synchronous contextfree grammar (SCFG) of Hiero (Chiang, 2007), which relies on two nonterminal labels. One problem in adding syntax labels to Hiero-style rules is that only partial phrases are assigned labels. It is common practice to extend labels by using the idea of combinatory categorial grammar (CCG) (Steedman, 2000) on the problem. Although this extended syntactical information may improve the coverage of rules and syntactic cor</context>
<context position="13288" citStr="Zollmann and Venugopal, 2006" startWordPosition="2219" endWordPosition="2222">en MT 2008 task (NIST08). We use the NIST Open MT 2006 for tuning and the MT 2003 for testing. The tuning and test sets have four English references. Table 2 shows the details for each corpus. Each corpus is tokenized, put in lower-case, and sentences with over 40 tokens on either side are removed from the training data. We use KyTea (Neubig et al., 2011) to tokenize the Japanese data and Stanford Word Segmenter (Tseng et al., 2005) to tokenize the Chinese data. We parse the English data with the Berkeley parser (Petrov and Klein, 2007). 4.2 Experiment design We did experiments with the SAMT (Zollmann and Venugopal, 2006) model with the Moses (Koehn et al., 2007). For the SAMT model, we conducted experiments with two label sets. One is extracted from the phrase structure parses and the other is extended with CCG4. We applied the proposed method (+clustering) and the baseline method (+coarsening), which uses the Hanneman 3LDC2003E14 4Using the relax-parse with option SAMT 4 for IWSLT07 and FBIS and SAMT 2 for NIST08 in the Moses Label set Label Rule F(C) SD parse 63 0.3 K - - CCG 3,147 4.2 M - - + coarsening 80 2.4 M -3.8 e+08 249 + clustering 80 3.8 M -7.2 e+07 73 Table 3: SAMT grammars on ja-en experiments La</context>
</contexts>
<marker>Zollmann, Venugopal, 2006</marker>
<rawString>Andreas Zollmann and Ashish Venugopal. 2006. Syntax augmented machine translation via chart parsing. In Proceedings on the Workshop on Statistical Machine Translation, pages 138–141, New York City, June. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.808795">
Aligning context-based statistical models of language
with brain activity during reading
</title>
<author confidence="0.855631">
Leila Wehbe1,2, Ashish Vaswani3, Kevin Knight3 and Tom Mitchell1,2
</author>
<affiliation confidence="0.720013666666667">
1 Machine Learning Department, Carnegie Mellon University, Pittsburgh, PA
2 Center for the Neural Basis of Computation, Carnegie Mellon University, Pittsburgh, PA
3 Information Sciences Institute, University of Southern California, Los Angeles, CA
</affiliation>
<email confidence="0.98577">
lwehbe@cs.cmu.edu, vaswani@usc.edu, knight@isi.edu, tom.mitchell@cs.cmu.edu
</email>
<sectionHeader confidence="0.994782" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999706446808511">
Many statistical models for natural language pro-
cessing exist, including context-based neural net-
works that (1) model the previously seen context
as a latent feature vector, (2) integrate successive
words into the context using some learned represen-
tation (embedding), and (3) compute output proba-
bilities for incoming words given the context. On
the other hand, brain imaging studies have sug-
gested that during reading, the brain (a) continu-
ously builds a context from the successive words
and every time it encounters a word it (b) fetches its
properties from memory and (c) integrates it with
the previous context with a degree of effort that is
inversely proportional to how probable the word is.
This hints to a parallelism between the neural net-
works and the brain in modeling context (1 and a),
representing the incoming words (2 and b) and in-
tegrating it (3 and c). We explore this parallelism to
better understand the brain processes and the neu-
ral networks representations. We study the align-
ment between the latent vectors used by neural net-
works and brain activity observed via Magnetoen-
cephalography (MEG) when subjects read a story.
For that purpose we apply the neural network to the
same text the subjects are reading, and explore the
ability of these three vector representations to pre-
dict the observed word-by-word brain activity.
Our novel results show that: before a new word i
is read, brain activity is well predicted by the neural
network latent representation of context and the pre-
dictability decreases as the brain integrates the word
and changes its own representation of context. Sec-
ondly, the neural network embedding of word i can
predict the MEG activity when word i is presented
to the subject, revealing that it is correlated with the
brain’s own representation of word i. Moreover, we
obtain that the activity is predicted in different re-
gions of the brain with varying delay. The delay is
consistent with the placement of each region on the
processing pathway that starts in the visual cortex
and moves to higher level regions. Finally, we show
that the output probability computed by the neural
networks agrees with the brain’s own assessment of
the probability of word i, as it can be used to predict
the brain activity after the word i’s properties have
been fetched from memory and the brain is in the
process of integrating it into the context.
</bodyText>
<sectionHeader confidence="0.999163" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999986953488372">
Natural language processing has recently seen a
surge in increasingly complex models that achieve
impressive goals. Models like deep neural net-
works and vector space models have become pop-
ular to solve diverse tasks like sentiment analy-
sis and machine translation. Because of the com-
plexity of these models, it is not always clear how
to assess and compare their performances as they
might be useful for one task and not the other.
It is also not easy to interpret their very high-
dimensional and mostly unsupervised representa-
tions. The brain is another computational system
that processes language. Since we can record brain
activity using neuroimaging, we propose a new di-
rection that promises to improve our understand-
ing of both how the brain is processing language
and of what the neural networks are modeling by
aligning the brain data with the neural networks
representations.
In this paper we study the representations of two
kinds of neural networks that are built to predict
the incoming word: recurrent and finite context
models. The first model is the Recurrent Neural
Network Language Model (Mikolov et al., 2011)
which uses the entire history of words to model
context. The second is the Neural Probabilistic
Language Model (NPLM) which uses limited con-
text constrained to the recent words (3 grams or 5
grams). We trained these models on a large Harry
Potter fan fiction corpus and we then used them to
predict the words of chapter 9 of Harry Potter and
the Sorcerer’s Stone (Rowling, 2012). In paral-
lel, we ran an MEG experiment in which 3 subject
read the words of chapter 9 one by one while their
brain activity was recorded. We then looked for
the alignment between the word-by-word vectors
produced by the neural networks and the word-by-
word neural activity recorded by MEG.
Our neural networks have 3 key constituents:
a hidden layer that summarizes the history of the
previous words ; an embeddings vector that sum-
marizes the (constant) properties of a given word
and finally the output probability of a word given
</bodyText>
<page confidence="0.98265">
233
</page>
<note confidence="0.9510135">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 233–243,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</note>
<figure confidence="0.959687666666667">
Visual feature Letter-string Lexical-semantic
analysis analysis analysis
0 400 800 0 400 800 0 400 800
Non-specific Words =
Nonwords Nonwords
Time (ms)
</figure>
<bodyText confidence="0.644294111111111">
guage. MEG records the change in the magnetic
field on the surface of the head that is caused by
a largeset of aligned neurons that are changing
their firing patterns in synchrony in response to
a stimulus. Because of the nature of the signal,
MEG recordings are directly related to neural ac-
tivity and have no latency. They are sampled at
a high frequency (typically 1kHz) that is ideal for
tracking the fast dynamics of language processing.
</bodyText>
<figure confidence="0.941984741935484">
ilkin ippiott Willis &amp; � 200
I
vatio piFigureh1: Corticalndynamics ofs silentereading. Thismfigure In this
or t t 20–6 efles ial-antic analy and, py poga ayis
is adapted from (Salmelin, 2007). Dots represent projected
sources of activity in the visual cortex (left brain sketch) and
the temporal cortex (right brain sketch). Th display
theHmean time coursePofkactivation
ar-
easkforn differentnconditions.
anal-
ysis innthe visual cortex at ∼100 non-specifict lan-
guage.
guage. Comparing responses toaletter stringssand other vi-
sualmstimulinrevealspthatloletter s
150 ms. Finally comparing the re
words (made-up words)nreveals lexical-semantic analysisain
theotemporal cortex at —200-500 ms.
The curves of inco
in the depicted source integrate
Thehinitial visualufeature analogou
ms is o an- that are
tring analysisfoccursolaround
sponseseto words an
ms
d non- analogou
y &apos;
which su
theaucontext. Wewset out tonfind theObrain analogs
ofathese model constituent
ing task.sWeocompare t
</figure>
<bodyText confidence="0.882237230769231">
their representations int f h well they
can be used to decode th
MEG data. We obtain co
theeemodelstanduthedbrain
with a modelg of langua
brain activity encodes st
each new word generates additional brain activity,
visual processing areas to
,dculminatingoinagan updated
storytcontext, andereflecting anuoverallEmagnitude
of neural effort influenced by the probability of
that new word given the previous context.
ot
</bodyText>
<subsectionHeader confidence="0.998624">
1.1 Neural processes involved in reading
</subsectionHeader>
<bodyText confidence="0.914407117647059">
Humans read with an average speed of 3 words
per second. Reading requires us to perceive in-
coming words and gradually integrate them into
a representation of the meaning. As words are
read, it takes 100ms for the visual input to reach
the visual cortex. 50ms later, the visual input is
processed as letter strings in a specialized region
of the left visual cortex (Salmelin, 2007). Be-
tween 200-500ms, the word’s semantic properties
are processed (see Fig. 1). Less is understood
about the cortical dynamics of word integration, as
multiple theories exist (Friederici, 2002; Hagoort,
2003).
Magnetoencephalography (MEG) is a brain-
imaging tool that is well suited for studying lan-
rd is analogous to the embedding that the
ing the w
</bodyText>
<subsectionHeader confidence="0.701724">
of a wo
</subsectionHeader>
<bodyText confidence="0.841516142857143">
work, we are interested in the mecha-
nism of human text understanding as the meaning
ming words is fetched from memory and
d with the context. Interestingly, this is
s to neural network models of language
used to predict the incoming word. The
mental representation of the previous context is
s to the latent layer of the neural network
mmarizes the relevant context before see-
ord. The representation of the meaning
s the word with inversely proportional ef-
cortex that has been recently shown to be
analogous to the output probability of the incom-
ing word from the neural network.
</bodyText>
<equation confidence="0.833134666666667">
see-
Fig. 2 shows a hypothetical activity in an MEG
hiol
</equation>
<bodyText confidence="0.993005725">
sensor as a subject reads a story in our experi-
ment, in which words are presented one at a time
for 500ms each. We conjecture that the activity in
time window a, i.e. before word i is understood, is
mostly related to the previous context before see-
ing word i. We also conjecture that the activity in
time window b is related to understanding word i
and integrating it into the context, leading to a new
representation of context in window c.
Using three types of features from neural net-
works (hidden layer context representation, output
probabilities and word embeddings) from three
different models of language (one recurrent model
and two finite context models), we therefore set to
predict the activity in the brain in different time
windows. We want to align the brain data with the
various model constituents to understand where
and when different types of processes are com-
puted in the brain, and simultaneously, we want to
e apart earl pre-lexical prcesses in
flowing generally from
d lleagu (Tkiin l
syllables, and sinle letter, imbed
more high level areas
etwork learns in training and then uses.
one common hypotheses is that the brain
neural n
sifiusing anTMEG decod- Finally,
hebdifferentemodels andsintegrate
fort to how predictable the word is (Frank et al.,
. There is a well studied response known as
0 that is an increase of the activity in the
y the amount of surprisal of the incoming
word given the context (Frank et al., 2013). This is
terms o ow we
e word beingtread from 2013)
rrespondences between theuN40
data that are-consistent temporal
gefi processing ind which graded b
ry context,eandewhere
</bodyText>
<page confidence="0.99278">
234
</page>
<figureCaption confidence="0.996683">
Figure 2: [Top] Sketch of the updates of a neural network
</figureCaption>
<bodyText confidence="0.972234222222222">
ila Whb
reading chapter 9 after it has been trained. Every word cor-
responds to a fixed embedding vector (magenta). A context
vector (blue) is computed before the word is seen given the
previous words. Given the context vector, the probability of
every word can be computed (symbolized by the histogram
in green). We only use the output probability of the actual
word (red circle). [Bottom] Hypothetical activity in an MEG
sensor when the subject reads the corresponding words. The
time periods approximated as a, b and c can be tested for in-
formation content relating to: the context of the story before
seeing word i (modeled by the context vector at i), the repre-
sentation of the properties of word i (the embedding of word
i) and the integration of word i into the context (the output
probability of word i). The periods drawn here are only a
conjecture on the timings of such cognitive events.
use the brain data to shed light on what the neural
network vectors are representing.
</bodyText>
<subsectionHeader confidence="0.579887">
Related work
</subsectionHeader>
<bodyText confidence="0.999972714285714">
Decoding cognitive states from brain data is a
recent field that has been growing in popularity.
Most decoding studies that study language use
functional Magnetic Resonance Imaging (fMRI),
while some studies use MEG. MEG’s high tempo-
ral resolution makes it invaluable for looking at the
dynamics of language understanding. (Sudre et
al., 2012) decode from MEG the word a subject is
reading. The authors estimate from the MEG data
the semantic features of the word and use these as
an intermediate step to decode what the word is.
This is in principle similar to the classification ap-
proach we follow, as we will also use the feature
vectors as an intermediate step for word classifica-
tion. However the experimental paradigm in (Su-
dre et al., 2012) is to present to the subjects sin-
gle isolated words and to find how the brain rep-
resents their semantic features; whereas we have a
much more complex and “naturalistic” experiment
in which the subjects read a non-artificial passage
of text, and we look at processes that exceed in-
dividual word processing: the construction of the
meanings of the successive words and the predic-
tion/integration of incoming words.
In (Frank et al., 2013), the amount of surprisal
that a word has given its context is used to pre-
dict the intensity of the N400 response described
previously. This is the closest study we could find
to our approach. This study was concerned with
analyzing the brain processes related only to sur-
prisal while we propose a more integral account
of the processes in the brain. The study also didn’t
address the major contribution we propose here,
which is to shed light on the inner constituents of
language models using brain imaging.
</bodyText>
<subsectionHeader confidence="0.782647">
1.2 Recurrent and finite context neural
networks
</subsectionHeader>
<bodyText confidence="0.99998264">
Similar to standard language models, neural lan-
guage models also learn probability distributions
over words given their previous context. However,
unlike standard language models, words are rep-
resented as real-valued vectors in a high dimen-
sional space. These word vectors, referred to as
word embeddings, can be different for input and
output words, and are learned from training data.
Thus, although at training and test time, the in-
put and output to the neural language models are
one-hot representation of words, it is their em-
beddings that are used to compute word proba-
bility distributions. After training the embedding
vectors are fixed and it is these vectors that we
will use later on to predict MEG data. To predict
MEG data, we will also use the latent vector rep-
resentations of context that these neural networks
produce, as well as the probability of the current
word given the context. In this section, we will
describe how recurrent neural network language
models and feedforward neural probabilistic lan-
guage models compute word probabilities. In the
interest of space, we keep this description brief,
and for details, the reader is requested to refer to
the original papers describing these models.
</bodyText>
<figure confidence="0.997628909090909">
embedding(iC1)
context(iC2)
ons
Harry
out. prob.(iC1) out. prob.(i) out. prob.(i+1)
Harry
embedding(i)
context(iC1)
on
had
had
embedding(i+1)
context(i)
never
never
... Harry had never ...
word i-1 word i
0.5 s 0.5 s 0.5 s
a
b
c
word i+1
</figure>
<page confidence="0.781828">
235
</page>
<figureCaption confidence="0.989158">
Figure 3: Recurrent neural network language model.
</figureCaption>
<figure confidence="0.991376909090909">
w(t) s(t − 1)
output
P(wt+1 I s(t))
hidden
s(t)
y(t) c(t)
D0 X
D W
output
P(w I u) D0
hidden
h2
hidden
h1
M
C1 C2
input
embeddings
input
words
D
u1 u2
</figure>
<figureCaption confidence="0.999737">
Figure 4: Neural probabilistic language model
</figureCaption>
<bodyText confidence="0.976942551724138">
Recurrent Neural Network Language Model
Unlike standard feedforward neural language
models that only look at a fixed number of past
words, recurrent neural network language models
use all the previous history from position 1 to t−1
to predict the next word. This is typically achieved
by feedback connections, where the hidden layer
activations used for predicting the word in posi-
tion t − 1 are fed back into the network to com-
pute the hidden layer activations for predicting the
next word. The hidden layer thus stores the history
of all previous words. We use the RNNLM archi-
tecture as described in Mikolov (2012), shown in
Figure 3. The input to the RNNLM at position t
are the one-hot representation of the current word,
w(t), and the activations from the hidden layer at
position t − 1, s(t − 1). The output of the hidden
layer at position t − 1 is
1+exp(−x),
that is applied elementwise. We need to compute
the probability of the next word w(t + 1) given
the hidden state s(t). For fast estimation of out-
put word probabilities, Mikolov (2012) divides the
computation into two stages: First, the probability
distribution over word classes is computed, after
which the probability distribution over the subset
of words belonging to the class are computed. The
class probability of a particular class with index m
at position t is computed as:
</bodyText>
<equation confidence="0.942810333333333">
exp P(cm(t)  |s(t)) = EC,
c=1 ((lit( ()m) ))
ex s t Xv�
</equation>
<bodyText confidence="0.997946375">
where X is a matrix of class embeddings and vm
is a one-hot vector representing the class with in-
dex m. The normalization constant is computed
over all classes C. Each class specifies a subset
V 0 of words, potentially smaller than the entire vo-
cabulary V . The probability of an output word l at
position t + 1 given that its class is m is defined
as:
</bodyText>
<equation confidence="0.99942425">
P(yl(t + 1)  |cm(t), s(t)) =
exp (s(t)D0vl)
�V 0 ,
k=1 (exp (s(t)D0vk))
</equation>
<bodyText confidence="0.99954625">
where D0 is a matrix of output word embeddings
and vl is a one hot vector representing the word
with index l. The probability of the word w(t+ 1)
given its class ci can now be computed as:
</bodyText>
<equation confidence="0.994751">
P(w(t + 1)  |s(t)) =P(w(t + 1)  |ci, s(t))
P(ci  |s(t)).
</equation>
<subsectionHeader confidence="0.518977">
Neural Probabilistic Language Model
</subsectionHeader>
<bodyText confidence="0.999469333333333">
We use the feedforward neural probabilistic lan-
guage model architecture of Vaswani et al. (2013),
as shown in Figure 4. Each context u comprises
a sequence of words uj (1 G j G n − 1) repre-
sented as one-hot vectors, which are fed as input
to the neural network. At the output layer, the neu-
ral network computes the probability P(w  |u) for
each word w, as follows.
The output of the first hidden layer h1 is
</bodyText>
<equation confidence="0.98828175">
⎛ ⎞
n−1�
h1 = φ ⎝CjDuj + b1 ⎠,
j=1
</equation>
<bodyText confidence="0.9999085">
where D is a matrix of input word embeddings
which is shared across all positions, the Cj are the
context matrices for each word in u, b1 is a vec-
tor of biases with the same dimension as h1, and φ
is applied elementwise. Vaswani et al. (2013) use
rectified linear units (Nair and Hinton, 2010) for
</bodyText>
<equation confidence="0.994141">
s(t) = φ (Dw(t) + Ws(t − 1)) ,
</equation>
<bodyText confidence="0.991652666666667">
where D is the matrix of input word embeddings,
W is a matrix that transforms the activations from
the hidden layer in position t − 1, and φ is a
</bodyText>
<table confidence="0.633383">
1
sigmoid function, defined as φ(x) =
</table>
<page confidence="0.990663">
236
</page>
<bodyText confidence="0.984888666666667">
the hidden layers h1 and h2, which use the activa-
tion function φ(x) = max(0, x).
The output of the second layer h2 is
</bodyText>
<equation confidence="0.981795">
h2 = φ (Mh1 + b2) ,
</equation>
<bodyText confidence="0.9999245">
where M is a weight matrix between h1 and h2
and b2 is a vector of biases for h2. The probabil-
ity of the output word is computed at the output
softmax layer as:
where D&apos; is the matrix of output word embed-
dings, b is a vector of biases for every output word
and vw its the one hot representation of the word
w in the vocabulary.
</bodyText>
<sectionHeader confidence="0.997588" genericHeader="introduction">
2 Methods
</sectionHeader>
<bodyText confidence="0.999993136363637">
We describe in this section our approach. In sum-
mary, we trained the neural network models on
a Harry Potter fan fiction database. We then ran
these models on chapter 9 of Harry Potter and the
Sorcerer’s Stone (Rowling, 2012) and computed
the context and embedding vectors and the output
probability for each word. In parallel, 3 subjects
read the same chapter in an MEG scanner. We
build models that predict the MEG data for each
word as a function of the different neural network
constituents. We then test these models with a
classification task that we explain below. We de-
tect correspondences between the neural network
components and the brain processes that under-
lie reading in the following fashion. If using a
neural network vector (e.g. the RNNLM embed-
ding vector) allows us to classify significantly bet-
ter than chance in a given region of the brain at
a given time (e.g. the visual cortex at time 100-
200ms), then we can hypothesize a relationship
between that neural network constituent and the
time/location of the analogous brain process.
</bodyText>
<subsectionHeader confidence="0.977703">
2.1 Training the Neural Networks
</subsectionHeader>
<bodyText confidence="0.9998918">
We used the freely available training tools pro-
vided by Mikolov (2012)1 and Vaswani et al.
(2013)2 to train our RNNLM and NPLM models
used in our brain data classification experiments.
Our training data comprised around 67.5 million
</bodyText>
<footnote confidence="0.9989335">
1http://rnnlm.org/
2http://nlg.isi.edu/software/nplm
</footnote>
<bodyText confidence="0.999596666666667">
words for training and 100 thousand words for val-
idation from the Harry Potter fan fiction database
(http://harrypotterfanfiction.com). We restricted
the vocabulary to the top 100 thousand words
which covered all but 4 words from Chapter 9 of
Harry Potter and the Sorcerer’s Stone.
For the RNNLM, we trained models with differ-
ent hidden layers and learning rates and found the
RNNLM with 250 hidden units to perform best on
the validation set. We extracted our word embed-
dings from the input matrix D (Figure 3). We used
the default settings for all other hyper parameters.
We trained 3-gram and 5-gram NPLMs with
150 dimensional word embeddings and experi-
mented with different number of units for the first
hidden layer (h1 in Figure 4), and different learn-
ing rates. For both the 3-gram and 5-gram mod-
els, we found 750 hidden units to perform the best
on the validation set and chose those models for
our final experiments. We used the output word
embeddings D&apos; in our experiments. We visually
inspected the nearest neighbors in the 150 dimen-
sional word embedding space for some words and
didn’t find the neighbors from D&apos; or D to be dis-
tinctly better than each other. We leave the com-
parison of input and output embeddings on brain
activity prediction for future work.
</bodyText>
<subsectionHeader confidence="0.997486">
2.2 MEG paradigm
</subsectionHeader>
<bodyText confidence="0.999905590909091">
We recorded MEG data for three subjects (2 fe-
males and one male) while they read chapter 9
of Harry Potter and the Sorcerer’s Stone (Rowl-
ing, 2012). The participants were native English
speakers and right handed. They were chosen to
be familiar with the material: we made sure they
had read the Harry Potter books or seen the movies
series and were familiar with the characters and
the story. All the participants signed the consent
form, which was approved by the University of
Pittsburgh Institutional Review Board, and were
compensated for their participation.
The words of the story were presented in rapid
serial visual format (Buchweitz et al., 2009):
words were presented one by one at the center
of the screen for 0.5 seconds each. The text was
shown in 4 experimental blocks of ∼11 minutes.
In total, 5176 words were presented. Chapter 9
was presented in its entirety without modifications
and each subject read the chapter only once.
One can think of an MEG machine as a large
helmet, with sensors located on the helmet that
</bodyText>
<equation confidence="0.998780333333333">
P(w  |u) =
Ew,=1 exp (vw,D&apos;h2 + bTvw,),
exp (vwD&apos;h2 + bTvw)
</equation>
<page confidence="0.984215">
237
</page>
<bodyText confidence="0.999979666666667">
record the magnetic activity. Our MEG recordings
were acquired on an Elekta Neuromag device at
the University of Pittsburgh Medical Center Pres-
byterian Hospital. This machine has 306 sensors
distributed into 102 locations on the surface of the
subject’s head. Each location groups 3 sensors or
two types: one magnometer that records the in-
tensity of the magnetic field and two planar gra-
diometers that record the change in the magnetic
field along two orthogonal planes3.
Our sampling frequency was 1kHz. For prepro-
cessing, we used Signal Space Separation method
(SSS, (Taulu et al., 2004)), followed by its tempo-
ral extension (tSSS, (Taulu and Simola, 2006)).
For each subject, the experiment data consists
therefore of a 306 dimensional time series of
length ∼45 minutes. We averaged the signal in ev-
ery sensor into 100ms non-overlapping time bins.
Since words were presented for 500ms each, we
therefore obtain for every word p = 306 × 5 val-
ues corresponding to 306 vectors of 5 points.
</bodyText>
<subsectionHeader confidence="0.999497">
2.3 Decoding experiment
</subsectionHeader>
<bodyText confidence="0.999954423076923">
To find which parts of brain activity are related to
the neural network constituents (e.g. the RNNLM
context vector), we run a prediction and classifica-
tion experiment in a 10-fold cross validated fash-
ion. At every fold, we train a linear model to pre-
dict MEG data as a function of one of the feature
sets, using 90% of the data. On the remaining 10%
of the data, we run a classification experiment.
MEG data is very noisy. Therefore, classify-
ing single word waveforms yields a low accuracy,
peaking at 60%, which might lead to false nega-
tives when looking for correspondences between
neural network features and brain data. To reveal
informative features, one can boost signal by ei-
ther having several repetitions of the stimuli in the
experiment and then averaging (Sudre et al., 2012)
or by combining the words into larger chunks (We-
hbe et al., 2014). We chose the latter because the
former sacrifices word and feature diversity.
At testing, we therefore repeat the following
300 times. Two sets of words are chosen ran-
domly from the test fold. To form the first set, 20
words are sampled without replacement from the
test sample (unseen by the classifier). To form the
second set, the kth word is chosen randomly from
all words in the test fold having the same length as
</bodyText>
<footnote confidence="0.933192">
3In this paper, we treat these three different sensors as
three different dimensions without further exploiting their
physical properties.
</footnote>
<bodyText confidence="0.999775533333333">
the kth word of the first set. Since every fold of
the data was used 9 times in the training phase and
once in the testing phase, and since we use a high
number of randomized comparisons, this averages
out biases in the accuracy estimation. Classifying
sets of 20 words improves the classification accu-
racy greatly while lowering its variance and makes
it dissociable from chance performance. We com-
pare only between words of equal length, to mini-
mize the effect of the low level visual features on
the classification accuracy.
After averaging out the results of multiple folds,
we end up with average accuracies that reveal how
related one of the models’ constituents (e.g. the
RNNLM context vector) is to brain data.
</bodyText>
<subsectionHeader confidence="0.839538">
2.3.1 Annotation of the stimulus text
</subsectionHeader>
<bodyText confidence="0.999834">
We have 9 sets of annotations for the words of the
experiment. Each set j can be described as a ma-
trix Fj in which each row i corresponds to the vec-
tor of annotations of word i. Our annotations cor-
respond to the 3 model constituents for each of the
3 models: the hidden layer representation before
word i, the output probability of word i and the
learned embeddings for word i.
</bodyText>
<subsectionHeader confidence="0.83652">
2.3.2 Classification
</subsectionHeader>
<bodyText confidence="0.99225728">
In order to align the brain processes and the differ-
ent constituents of the different models, we use a
classification task. The task is to classify the word
a subject is reading out of two possible choices
from its MEG recording. The classifier uses one
type of feature in an intermediate classification
step. For example, the classifier learns to predict
the MEG activity for any setting of the RNNLM
hidden layer. Given an unseen MEG recording for
an unknown word i and two possible story words
i&apos; and i&apos;&apos; (one of which being the true word i), the
classifier predicts the MEG activity when reading
i&apos; and i&apos;&apos; from their hidden layer vectors. It then
assigns the label i&apos; or i&apos;&apos; to the word recording i
depending on which prediction is the closest to the
recording. The following are the detailed steps of
this complex classification task. However, for the
rest of the paper the most useful point to keep in
mind is that the main purpose of the classification
is to find a correspondence between the brain data
and a given feature set j.
1. Normalize the columns of M (zero mean,
standard deviation = 1). Pick feature set Fj
and normalize its columns to a minimum of 0
and a maximum of 1.
</bodyText>
<page confidence="0.988458">
238
</page>
<listItem confidence="0.9810402">
2. Divide the data into 10 folds, for each fold b:
(a) Isolate Mb and Fbj as test data. The re-
mainder M−b and F−b jwill be used for
training4.
(b) Subtract the mean of the columns of
M−b from Mb and M−b and the mean
of the columns of F−b
j from Fbj and F−b
j
(c) Use ridge regression to solve
</listItem>
<equation confidence="0.9527685">
M−b = F−b
j x βt j
</equation>
<bodyText confidence="0.856179277777778">
by tuning the A parameter to every one
of the p output dimensions indepen-
dently. A is chosen via generalized cross
validation (Golub et al., 1979).
(d) Perform a binary classification. Sample
from the set of words in b a set c of 20
words. Then sample from b another set
of 20 words such that the kth word in c
and d have the same number of letters.
For every sample (c,d):
i. predict the MEG data for c and d as:
Pc = Fcj x Γbj and Pd = Fdj x Γb j
ii. assign to Mc the label c or d depend-
ing on which of Pc or Pd is closest
(Euclidean distance).
iii. assign to Md the label c or d de-
pending on which of Pc or Pd is
closest (Euclidean distance).
</bodyText>
<sectionHeader confidence="0.442445" genericHeader="method">
3. Compute the average accuracy.
</sectionHeader>
<subsectionHeader confidence="0.61829">
2.3.3 Restricting the analysis spatially: a
searchlight equivalent
</subsectionHeader>
<bodyText confidence="0.9988091875">
We adapt the searchlight method (Kriegeskorte et
al., 2006) to MEG. The searchlight is a discovery
procedure used in fMRI in which a cube is slid
over the brain and an analysis is performed in each
location separately. It allows to find regions in the
brain where a specific phenomenon is occurring.
In the MEG sensor space, for every one of the 102
sensor locations E, we assign a group of sensors g`.
For every location E, we identify the locations that
immediately surround it in any direction (Anterior,
Right Anterior, Right etc...) when looking at the
2D flat representation of the location of the sensors
in the MEG helmet (see Fig. 9 for an illustration of
the 2D helmet). g` therefore contains the 3 sensors
at location E and at the neighboring locations. The
maximum number of sensors in a group is 3 x 9.
</bodyText>
<footnote confidence="0.912201">
4The rows from M−b and F−b
j that correspond to the five
</footnote>
<bodyText confidence="0.926689566666667">
words before or after the test set are ignored in order to make
the test set independent.
The locations at the edge of the helmet have fewer
sensors because of the missing neighbor locations.
2.3.4 Restricting the analysis temporally
Instead of using the entire time course of the word,
we can use only one of the corresponding 100ms
time windows. Obtaining a high classification ac-
curacy using one of the time windows and feature
set j means that the analogous type of information
is encoded at that time.
2.3.5 Classification accuracy by time and
region
The above steps compute whole brain accuracy us-
ing all the time series. In order to perform a more
precise spatio-temporal analysis, one can use only
one time window m and one location E for the clas-
sification. This can answer the question of when
and where different information is represented by
brain activity. For every location, we will use only
the columns corresponding to the time point m for
the sensors belonging to the group g`. Step (d) of
the classification procedure is changed as such:
(d) Perform a binary classification. Sample from
the set of words in b a set c of 20 words. Then
sample from b another set of 20 words such
that the kth word in c and d have the same
number of letters. For every sample (c,d), and
for every setting of {m, E}:
Pi. predict the MEG data for c and d as:
</bodyText>
<equation confidence="0.9797195">
C = c b
P F� x Pjm,P} { and
d _ d b
P{m,`} — Fd x Γj {m,`}
</equation>
<bodyText confidence="0.998456833333333">
ii. assign to Mc{m,`} the label c or d depend-
ing on which of Pc{m,`} or Pd{m,`} is clos-
est (Euclidean distance).
iii. assign to Md{m,`} the label c or d depend-
ing on which of Pc{m,`} or Pd{m,`} is clos-
est (Euclidean distance).
</bodyText>
<subsectionHeader confidence="0.945759">
2.3.6 Statistical significance testing
</subsectionHeader>
<bodyText confidence="0.999998363636364">
We determine the distribution for chance perfor-
mance empirically. Because the successive word
samples in our MEG and feature matrices are not
independent and identically distributed, we break
the relationship between the MEG and feature ma-
trices by shifting the feature matrices by large de-
lays (e.g. 2000 to 2500 words) and we repeat
the classification using the delayed matrices. This
simulates chance performance more fairly than a
permutation test because it keeps the time struc-
ture of the matrices. It was used in (Wehbe et al.,
</bodyText>
<page confidence="0.995431">
239
</page>
<bodyText confidence="0.999723470588235">
2014) and inspired by (Chwialkowski and Gret-
ton, 2014). For every {m, i} setting we can there-
fore compute a standardized z-value by subtract-
ing the mean of the shifted classifications and di-
viding by the standard deviation. We then com-
pute the p-value for the true classification accu-
racy being due to chance. Since the three p-values
for the three subjects for a given {m, i} are inde-
pendent, we combine them using Fisher’s method
for independent test statistics (Fisher, 1925). The
statistics we obtain for every {m, i} are depen-
dent because they comprise nearby time and space
windows. We control the false discovery rate us-
ing (Benjamini and Yekutieli, 2001) to adjust for
the testing at multiple locations and time windows.
This method doesn’t assume any kind of indepen-
dence or positive dependence.
</bodyText>
<sectionHeader confidence="0.999889" genericHeader="evaluation">
3 Results
</sectionHeader>
<bodyText confidence="0.999818727272727">
We present in Fig. 5 the accuracy using all the time
windows and sensors. In Fig. 6 we present the
classification accuracy when running the classifi-
cation at every time window exclusively. In Fig. 9
we present the accuracy when running the classifi-
cation using different time windows and groups of
sensors centered at every one of the 102 locations.
It is important to lay down some conventions
to understand the complex results in these plots.
To recap, we are trying to find parallels between
model constituents and brain processes. We use:
</bodyText>
<listItem confidence="0.97550275">
• a subset of the data (for example the time
window 0-100ms and all the sensors)
• one type of feature (for example the hidden
context layer from the NPLM 3g model)
</listItem>
<bodyText confidence="0.997352875">
and we obtain a classification accuracy A. If A
is low, there is probably no relationship between
the feature set and the subset of data. If A is high,
it hints to an association between the subset of data
and the mental process that is analogous to the fea-
ture set. For example, when using all the sensors
and time window 0-100ms, along with the NPLM
3g hidden layer, we obtain an accuracy of 0.70
(higher than chance with p &lt; 10−14, see Fig. 6).
Since the NPLM 3g hidden layer summarizes the
context of the story before seeing word i, this sug-
gests that the brain is still processing the context
of the story before word i between 0-100ms.
Fig. 6 shows the accuracy for different types
of features when using all of the time points and
all the sensors to classify a word. We can see
</bodyText>
<figureCaption confidence="0.983012666666667">
Figure 5: Average accuracy using all time windows and
sensors, grouped by model (top) and type of feature (bot-
tom). All accuracies are significantly higher than chance
</figureCaption>
<figure confidence="0.949255166666667">
(p &lt; 10−8).
200 400 200 400
RNNLM
0.7
0.6
0.5
</figure>
<figureCaption confidence="0.999406">
Figure 6: Average accuracy in different time windows
</figureCaption>
<bodyText confidence="0.96960648">
when using different types of features as input to the clas-
sifier, for different models. Accuracy is plotted in the center
of the respective time window. Points marked with a circle
are significantly higher than chance accuracy for the given
feature set and time window after correction.
similar classification accuracies for the three types
of models, with RNNLM ahead for the hidden
layer and embeddings and behind for the output
probability features. The hidden layer features
are the most powerful for classification. Between
the three types of features, the hidden layer fea-
tures are the best at capturing the information con-
tained in the brain data, suggesting that most of
the brain activity is encoding the previous context.
The embedding features are the second best. Fi-
nally the output probability have the smallest ac-
curacies. This makes sense considering that they
capture much less information than the other two
high dimensional descriptive vectors, as they do
not represent the complex properties of the words,
only a numerical assessment of their likelihood.
Fig. 6 shows the accuracy when using different
windows of time exclusively, for the 100ms time
hidden layer output probability embeddings
hidden layer output probability embeddings
</bodyText>
<figure confidence="0.990674238095238">
NPLM 3g NPLM 5g RNNLM
NPLM 3g NPLM 5g RNNLM
1
classification accuracy
0.8
0.6
1
classification accuracy
0.8
0.6
NPLM 3g NPLM 5g
0.7
0.6
0.5
0.7
0.6
0.5
hidden layer
output probability
embeddings
200 400
</figure>
<page confidence="0.983461">
240
</page>
<bodyText confidence="0.999975761904762">
windows starting at 0,100 ... 400ms after word
presentation. We can see that using the embed-
ding vector becomes increasingly more useful for
classification until 300-400ms, and then its perfor-
mance starts decreasing. This results aligns with
the following hypothesis: the word is being per-
ceived and understood by the brain gradually after
its presentation, and therefore the brain represen-
tation of the word becomes gradually similar to the
neural network representation of the word (i.e. the
embedding vector). The output probability feature
accuracy peaks at a later time than the embeddings
accuracy. Obtaining a higher than chance accu-
racy at time window m using the output probabil-
ity as input to the classifier suggests strongly that
the brain is integrating the word at time window
m, because it is responding differently for pre-
dictable and unpredictable words5. The integra-
tion step happens after the perception step, which
is probably why the output probability curves peak
later than the embeddings curves.
</bodyText>
<figure confidence="0.762845">
Hidden Layer
</figure>
<figureCaption confidence="0.978781333333333">
Figure 7: Average accuracy in time for the different hidden
layers. The analysis is extended to the time windows before
and after the word is presented, the input feature is restricted
to be the hidden layer before the central word is seen. The
first vertical bar indicates the onset of the word, the second
one indicates the end of its presentation.
</figureCaption>
<figure confidence="0.9823216">
Subject 3
0.8
0.6
−1000 0 1000
hidden layer output probability embeddings
</figure>
<figureCaption confidence="0.9828975">
Figure 8: Accuracy in time when using the RNNLM fea-
tures for each of the three subjects.
</figureCaption>
<bodyText confidence="0.998632">
To understand the time dynamics of the hidden
layer accuracy we need to see a larger time scale
than the word itself. The hidden layer captures the
</bodyText>
<footnote confidence="0.9956">
5the fact that we can classify accurately during windows
300-400ms indicates that the classifier is taking advantage of
the N400 response discussed in the introduction
</footnote>
<bodyText confidence="0.999975588235294">
context before word i is seen. Therefore it seems
reasonable that the hidden layer is not only related
to the activity when the word is on the screen, but
also related to the activity before the word is pre-
sented, which is the time when the brain is inte-
grating the previous words to build that context.
On the other hand, as the word i and subsequent
words are integrated, the context starts diverging
from the context of word i (computed before see-
ing word i). We therefore ran the same analysis
as before, but this time we also included the time
windows before and after word i in the analysis,
while maintaining the hidden layer vector to be the
context before word i is seen. We see the behav-
ior we predicted in the results: the context before
seeing word i becomes gradually more useful for
classification until word i is seen, and then it grad-
ually decreases until it is no longer useful since
the context has changed. We observe the RNNLM
hidden layer has a higher classification accuracy
than the finite context NPLMs. This might be due
to the fact that the RNNLM has a more complete
representation of context that captures more of the
properties of the previous words.
To show the consistency of the results, we plot
as illustration the three curves we obtain for each
subject for the RNNLM (Fig. 8). The patterns
seem very consistent indicating the phenomena we
described can be detected at the subject level.
We now move on to the spatial decomposition
of the analysis. When the visual input enters the
brain, it first reaches the visual cortex at the back
of the head, and then moves anteriorly towards the
left and right temporal cortices and eventually the
frontal cortex. As it flows through these areas, it
is processed to higher levels of interpretations. In
Fig. 9, we plot the accuracy for different regions
of the brain and different time windows for the
RNNLM features. To make the plots simpler we
multiplied by zero the accuracies which were not
significantly higher than chance. We expand a few
characteristic plots. We see that in the back of the
head the embedding features have an accuracy that
seems to peak very early on. As we move forward
in the brain towards the left and right temporal cor-
tices, we see the embeddings accuracy peaking at
a later time, reflecting the delay it takes for the in-
formation to reach this part of the brain. The out-
put probability start being useful for classification
after the embeddings, and specifically in the left
temporal cortex which is the cite where the N400
</bodyText>
<figure confidence="0.99152132">
−500 0 500 1000
0.8
0.7
0.6
0.5
NPLM 3G
NPLM 5G
RNNLM
Subject 1 Subject 2
0.8 0.8
0.6 0.6
−1000 0 1000 −1000 0 1000
241
Left
Back
hidden layer
output probability
embeddings
accuracy
Right
0.7
0.6
0.5
time (s)
0 500
</figure>
<figureCaption confidence="0.9964424">
Figure 9: Average accuracy in time and space on the MEG helmet when using the RNNLM features. For each of the 102
locations the average accuracy for the group of sensors centered at that location is plotted versus time. The axes are defined
in the rightmost, empty plot. Three plots have been magnified to show the increasing delay in high accuracy when using the
embeddings feature, reflecting the delay in processing the incoming word as information travels through the brain. A sensor
map is provided in the lower right corner: visual cortex = cyan, temporal = red, frontal = dark green.
</figureCaption>
<bodyText confidence="0.999234666666667">
is reported in the literature. Finally, as we reach
the frontal cortex, we see that the embeddings fea-
tures have an even later accuracy peak.
</bodyText>
<sectionHeader confidence="0.988558" genericHeader="conclusions">
4 Conclusion and contributions
</sectionHeader>
<bodyText confidence="0.999955340909091">
Novel brain data exploration We present here
a novel and revealing approach to shed light on
the brain processes involved in reading. This is a
departure from the classical approach of control-
ling for a few variables in the text (e.g. showing
a sentence with an expected target word versus an
unexpected one). While we cannot make clear cut
causal claims because we did not control for our
variables, we are able to explore the data much
more and offer a much richer interpretation than
is possible with artificially constrained stimuli.
Comparing two models of language Adding
brain data into the equation allowed us to com-
pare the performance of the models and to identify
a slight advantage for the RNNLM in capturing
the text contents. Numerical comparison is how-
ever a secondary contribution of our approach. We
showed that it might be possible to use brain data
to understand, interpret and illustrate what exactly
is being encoded by the obscure vectors that neural
networks compute, by drawing parallels between
the models constituents and brain processes.
Anecdotally, in the process of running the ex-
periments, we noticed that the accuracy for the
hidden layer of the RNNLM was peaking in the
time window corresponding to word i−2, and that
it was decreasing during word i − 1. Since this
was against our expectations, we went back and
looked at the code and found that it was indeed
returning a delayed value and corrected the fea-
tures. We therefore used the brain data in order to
correct a mis-specification in our neural network
model. This hints if not proves the potential of our
approach for assessing language models.
Future Work The work described here is our
first attempt along the promising endeavor of
matching complex computational models of lan-
guage with brain processes using brain recordings.
We plan to extend our efforts by (1) collecting data
from more subjects and using various types of text
and (2) make the brain data help us with training
better statistical language models by using it to de-
termine whether the models are expressive enough
or have reached a sufficient degree of convergence.
</bodyText>
<sectionHeader confidence="0.997642" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.998097666666667">
This research was supported in part by NICHD
grant 5R01HD07328-02. We thank Nicole Rafidi
for help with data acquisition.
</bodyText>
<page confidence="0.995667">
242
</page>
<sectionHeader confidence="0.993725" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997898138888889">
Yoav Benjamini and Daniel Yekutieli. 2001. The con-
trol of the false discovery rate in multiple testing un-
der dependency. Annals of statistics, pages 1165–
1188.
Augusto Buchweitz, Robert A Mason, Lˆeda Tomitch,
and Marcel Adam Just. 2009. Brain activation
for reading and listening comprehension: An fMRI
study of modality effects and individual differences
in language comprehension. Psychology &amp; neuro-
science, 2(2):111–123.
Kacper Chwialkowski and Arthur Gretton. 2014.
A kernel independence test for random processes.
arXiv preprint arXiv:1402.4501.
Ronald Aylmer Fisher. 1925. Statistical methods for
research workers. Genesis Publishing Pvt Ltd.
Stefan L Frank, Leun J Otten, Giulia Galli, and
Gabriella Vigliocco. 2013. Word surprisal predicts
N400 amplitude during reading. In Proceedings of
the 51st annual meeting of the Association for Com-
putational Linguistics, pages 878–883.
Angela D Friederici. 2002. Towards a neural basis
of auditory sentence processing. Trends in cognitive
sciences, 6(2):78–84.
Gene H Golub, Michael Heath, and Grace Wahba.
1979. Generalized cross-validation as a method for
choosing a good ridge parameter. Technometrics,
21(2):215–223.
Peter Hagoort. 2003. How the brain solves the binding
problem for language: a neurocomputational model
of syntactic processing. Neuroimage, 20:S18–S29.
Nikolaus Kriegeskorte, Rainer Goebel, and Peter Ban-
dettini. 2006. Information-based functional brain
mapping. Proceedings of the National Academy
of Sciences of the United States of America,
103(10):3863–3868.
Tomas Mikolov, Stefan Kombrink, Anoop Deoras,
Lukar Burget, and J Cernocky. 2011. RNNLM-
recurrent neural network language modeling toolkit.
In Proc. of the 2011 ASRU Workshop, pages 196–
201.
Tomas Mikolov. 2012. Statistical Language Models
Based on Neural Networks. Ph.D. thesis, Brno Uni-
versity of Technology.
Vinod Nair and Geoffrey E. Hinton. 2010. Rectified
linear units improve restricted Boltzmann machines.
In Proceedings of ICML, pages 807–814.
Joanne K. Rowling. 2012. Harry Potter and the Sor-
cerer’s Stone. Harry Potter US. Pottermore Limited.
Riitta Salmelin. 2007. Clinical neurophysiology of
language: the MEG approach. Clinical Neurophysi-
ology, 118(2):237–254.
Gustavo Sudre, Dean Pomerleau, Mark Palatucci, Leila
Wehbe, Alona Fyshe, Riitta Salmelin, and Tom
Mitchell. 2012. Tracking neural coding of percep-
tual and semantic features of concrete nouns. Neu-
roImage, 62(1):451–463.
Samu Taulu and Juha Simola. 2006. Spatiotem-
poral signal space separation method for rejecting
nearby interference in MEG measurements. Physics
in medicine and biology, 51(7):1759.
Samu Taulu, Matti Kajola, and Juha Simola. 2004.
Suppression of interference and artifacts by the sig-
nal space separation method. Brain topography,
16(4):269–275.
Ashish Vaswani, Yinggong Zhao, Victoria Fossum, and
David Chiang. 2013. Decoding with large-scale
neural language models improves translation.
Leila Wehbe, Brian Murphy, Partha Talukdar, Alona
Fyshe, Aaditya Ramdas, and Tom Mitchell. 2014.
Simultaneously uncovering the patterns of brain re-
gions involved in different story reading subpro-
cesses. in press.
</reference>
<page confidence="0.999051">
243
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.813479">
<title confidence="0.9925645">Aligning context-based statistical models of with brain activity during reading</title>
<author confidence="0.995235">Ashish Kevin</author>
<author confidence="0.995235">Tom</author>
<affiliation confidence="0.938663">1Machine Learning Department, Carnegie Mellon University, Pittsburgh, 2Center for the Neural Basis of Computation, Carnegie Mellon University, Pittsburgh,</affiliation>
<address confidence="0.985426">3Information Sciences Institute, University of Southern California, Los Angeles, CA</address>
<email confidence="0.999571">lwehbe@cs.cmu.edu,vaswani@usc.edu,knight@isi.edu,tom.mitchell@cs.cmu.edu</email>
<abstract confidence="0.998930270833333">Many statistical models for natural language processing exist, including context-based neural networks that (1) model the previously seen context as a latent feature vector, (2) integrate successive words into the context using some learned representation (embedding), and (3) compute output probabilities for incoming words given the context. On the other hand, brain imaging studies have suggested that during reading, the brain (a) continuously builds a context from the successive words and every time it encounters a word it (b) fetches its properties from memory and (c) integrates it with the previous context with a degree of effort that is inversely proportional to how probable the word is. This hints to a parallelism between the neural networks and the brain in modeling context (1 and a), representing the incoming words (2 and b) and integrating it (3 and c). We explore this parallelism to better understand the brain processes and the neural networks representations. We study the alignment between the latent vectors used by neural networks and brain activity observed via Magnetoencephalography (MEG) when subjects read a story. For that purpose we apply the neural network to the same text the subjects are reading, and explore the ability of these three vector representations to predict the observed word-by-word brain activity. novel results show that: before a new word is read, brain activity is well predicted by the neural network latent representation of context and the predictability decreases as the brain integrates the word and changes its own representation of context. Secthe neural network embedding of word the MEG activity when word presented to the subject, revealing that it is correlated with the own representation of word Moreover, we obtain that the activity is predicted in different regions of the brain with varying delay. The delay is consistent with the placement of each region on the processing pathway that starts in the visual cortex and moves to higher level regions. Finally, we show that the output probability computed by the neural networks agrees with the brain’s own assessment of probability of word as it can be used to predict brain activity after the word properties have been fetched from memory and the brain is in the process of integrating it into the context.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yoav Benjamini</author>
<author>Daniel Yekutieli</author>
</authors>
<title>The control of the false discovery rate in multiple testing under dependency. Annals of statistics,</title>
<date>2001</date>
<pages>1165--1188</pages>
<contexts>
<context position="31554" citStr="Benjamini and Yekutieli, 2001" startWordPosition="5441" endWordPosition="5444">ski and Gretton, 2014). For every {m, i} setting we can therefore compute a standardized z-value by subtracting the mean of the shifted classifications and dividing by the standard deviation. We then compute the p-value for the true classification accuracy being due to chance. Since the three p-values for the three subjects for a given {m, i} are independent, we combine them using Fisher’s method for independent test statistics (Fisher, 1925). The statistics we obtain for every {m, i} are dependent because they comprise nearby time and space windows. We control the false discovery rate using (Benjamini and Yekutieli, 2001) to adjust for the testing at multiple locations and time windows. This method doesn’t assume any kind of independence or positive dependence. 3 Results We present in Fig. 5 the accuracy using all the time windows and sensors. In Fig. 6 we present the classification accuracy when running the classification at every time window exclusively. In Fig. 9 we present the accuracy when running the classification using different time windows and groups of sensors centered at every one of the 102 locations. It is important to lay down some conventions to understand the complex results in these plots. To</context>
</contexts>
<marker>Benjamini, Yekutieli, 2001</marker>
<rawString>Yoav Benjamini and Daniel Yekutieli. 2001. The control of the false discovery rate in multiple testing under dependency. Annals of statistics, pages 1165– 1188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Augusto Buchweitz</author>
<author>Robert A Mason</author>
<author>Lˆeda Tomitch</author>
<author>Marcel Adam Just</author>
</authors>
<title>Brain activation for reading and listening comprehension: An fMRI study of modality effects and individual differences in language comprehension.</title>
<date>2009</date>
<booktitle>Psychology &amp; neuroscience,</booktitle>
<pages>2--2</pages>
<contexts>
<context position="21520" citStr="Buchweitz et al., 2009" startWordPosition="3637" endWordPosition="3640">bjects (2 females and one male) while they read chapter 9 of Harry Potter and the Sorcerer’s Stone (Rowling, 2012). The participants were native English speakers and right handed. They were chosen to be familiar with the material: we made sure they had read the Harry Potter books or seen the movies series and were familiar with the characters and the story. All the participants signed the consent form, which was approved by the University of Pittsburgh Institutional Review Board, and were compensated for their participation. The words of the story were presented in rapid serial visual format (Buchweitz et al., 2009): words were presented one by one at the center of the screen for 0.5 seconds each. The text was shown in 4 experimental blocks of ∼11 minutes. In total, 5176 words were presented. Chapter 9 was presented in its entirety without modifications and each subject read the chapter only once. One can think of an MEG machine as a large helmet, with sensors located on the helmet that P(w |u) = Ew,=1 exp (vw,D&apos;h2 + bTvw,), exp (vwD&apos;h2 + bTvw) 237 record the magnetic activity. Our MEG recordings were acquired on an Elekta Neuromag device at the University of Pittsburgh Medical Center Presbyterian Hospit</context>
</contexts>
<marker>Buchweitz, Mason, Tomitch, Just, 2009</marker>
<rawString>Augusto Buchweitz, Robert A Mason, Lˆeda Tomitch, and Marcel Adam Just. 2009. Brain activation for reading and listening comprehension: An fMRI study of modality effects and individual differences in language comprehension. Psychology &amp; neuroscience, 2(2):111–123.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kacper Chwialkowski</author>
<author>Arthur Gretton</author>
</authors>
<title>A kernel independence test for random processes. arXiv preprint arXiv:1402.4501.</title>
<date>2014</date>
<contexts>
<context position="30946" citStr="Chwialkowski and Gretton, 2014" startWordPosition="5336" endWordPosition="5340">.3.6 Statistical significance testing We determine the distribution for chance performance empirically. Because the successive word samples in our MEG and feature matrices are not independent and identically distributed, we break the relationship between the MEG and feature matrices by shifting the feature matrices by large delays (e.g. 2000 to 2500 words) and we repeat the classification using the delayed matrices. This simulates chance performance more fairly than a permutation test because it keeps the time structure of the matrices. It was used in (Wehbe et al., 239 2014) and inspired by (Chwialkowski and Gretton, 2014). For every {m, i} setting we can therefore compute a standardized z-value by subtracting the mean of the shifted classifications and dividing by the standard deviation. We then compute the p-value for the true classification accuracy being due to chance. Since the three p-values for the three subjects for a given {m, i} are independent, we combine them using Fisher’s method for independent test statistics (Fisher, 1925). The statistics we obtain for every {m, i} are dependent because they comprise nearby time and space windows. We control the false discovery rate using (Benjamini and Yekutiel</context>
</contexts>
<marker>Chwialkowski, Gretton, 2014</marker>
<rawString>Kacper Chwialkowski and Arthur Gretton. 2014. A kernel independence test for random processes. arXiv preprint arXiv:1402.4501.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald Aylmer Fisher</author>
</authors>
<title>Statistical methods for research workers.</title>
<date>1925</date>
<publisher>Genesis Publishing Pvt Ltd.</publisher>
<contexts>
<context position="31370" citStr="Fisher, 1925" startWordPosition="5412" endWordPosition="5413">performance more fairly than a permutation test because it keeps the time structure of the matrices. It was used in (Wehbe et al., 239 2014) and inspired by (Chwialkowski and Gretton, 2014). For every {m, i} setting we can therefore compute a standardized z-value by subtracting the mean of the shifted classifications and dividing by the standard deviation. We then compute the p-value for the true classification accuracy being due to chance. Since the three p-values for the three subjects for a given {m, i} are independent, we combine them using Fisher’s method for independent test statistics (Fisher, 1925). The statistics we obtain for every {m, i} are dependent because they comprise nearby time and space windows. We control the false discovery rate using (Benjamini and Yekutieli, 2001) to adjust for the testing at multiple locations and time windows. This method doesn’t assume any kind of independence or positive dependence. 3 Results We present in Fig. 5 the accuracy using all the time windows and sensors. In Fig. 6 we present the classification accuracy when running the classification at every time window exclusively. In Fig. 9 we present the accuracy when running the classification using di</context>
</contexts>
<marker>Fisher, 1925</marker>
<rawString>Ronald Aylmer Fisher. 1925. Statistical methods for research workers. Genesis Publishing Pvt Ltd.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan L Frank</author>
<author>Leun J Otten</author>
<author>Giulia Galli</author>
<author>Gabriella Vigliocco</author>
</authors>
<title>Word surprisal predicts N400 amplitude during reading.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st annual meeting of the Association for Computational Linguistics,</booktitle>
<pages>878--883</pages>
<contexts>
<context position="9978" citStr="Frank et al., 2013" startWordPosition="1614" endWordPosition="1617">where and when different types of processes are computed in the brain, and simultaneously, we want to e apart earl pre-lexical prcesses in flowing generally from d lleagu (Tkiin l syllables, and sinle letter, imbed more high level areas etwork learns in training and then uses. one common hypotheses is that the brain neural n sifiusing anTMEG decod- Finally, hebdifferentemodels andsintegrate fort to how predictable the word is (Frank et al., . There is a well studied response known as 0 that is an increase of the activity in the y the amount of surprisal of the incoming word given the context (Frank et al., 2013). This is terms o ow we e word beingtread from 2013) rrespondences between theuN40 data that are-consistent temporal gefi processing ind which graded b ry context,eandewhere 234 Figure 2: [Top] Sketch of the updates of a neural network ila Whb reading chapter 9 after it has been trained. Every word corresponds to a fixed embedding vector (magenta). A context vector (blue) is computed before the word is seen given the previous words. Given the context vector, the probability of every word can be computed (symbolized by the histogram in green). We only use the output probability of the actual wo</context>
<context position="12395" citStr="Frank et al., 2013" startWordPosition="2025" endWordPosition="2028"> classification approach we follow, as we will also use the feature vectors as an intermediate step for word classification. However the experimental paradigm in (Sudre et al., 2012) is to present to the subjects single isolated words and to find how the brain represents their semantic features; whereas we have a much more complex and “naturalistic” experiment in which the subjects read a non-artificial passage of text, and we look at processes that exceed individual word processing: the construction of the meanings of the successive words and the prediction/integration of incoming words. In (Frank et al., 2013), the amount of surprisal that a word has given its context is used to predict the intensity of the N400 response described previously. This is the closest study we could find to our approach. This study was concerned with analyzing the brain processes related only to surprisal while we propose a more integral account of the processes in the brain. The study also didn’t address the major contribution we propose here, which is to shed light on the inner constituents of language models using brain imaging. 1.2 Recurrent and finite context neural networks Similar to standard language models, neur</context>
</contexts>
<marker>Frank, Otten, Galli, Vigliocco, 2013</marker>
<rawString>Stefan L Frank, Leun J Otten, Giulia Galli, and Gabriella Vigliocco. 2013. Word surprisal predicts N400 amplitude during reading. In Proceedings of the 51st annual meeting of the Association for Computational Linguistics, pages 878–883.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Angela D Friederici</author>
</authors>
<title>Towards a neural basis of auditory sentence processing. Trends in cognitive sciences,</title>
<date>2002</date>
<pages>6--2</pages>
<contexts>
<context position="7729" citStr="Friederici, 2002" startWordPosition="1226" endWordPosition="1227">ot 1.1 Neural processes involved in reading Humans read with an average speed of 3 words per second. Reading requires us to perceive incoming words and gradually integrate them into a representation of the meaning. As words are read, it takes 100ms for the visual input to reach the visual cortex. 50ms later, the visual input is processed as letter strings in a specialized region of the left visual cortex (Salmelin, 2007). Between 200-500ms, the word’s semantic properties are processed (see Fig. 1). Less is understood about the cortical dynamics of word integration, as multiple theories exist (Friederici, 2002; Hagoort, 2003). Magnetoencephalography (MEG) is a brainimaging tool that is well suited for studying lanrd is analogous to the embedding that the ing the w of a wo work, we are interested in the mechanism of human text understanding as the meaning ming words is fetched from memory and d with the context. Interestingly, this is s to neural network models of language used to predict the incoming word. The mental representation of the previous context is s to the latent layer of the neural network mmarizes the relevant context before seeord. The representation of the meaning s the word with inv</context>
</contexts>
<marker>Friederici, 2002</marker>
<rawString>Angela D Friederici. 2002. Towards a neural basis of auditory sentence processing. Trends in cognitive sciences, 6(2):78–84.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gene H Golub</author>
<author>Michael Heath</author>
<author>Grace Wahba</author>
</authors>
<title>Generalized cross-validation as a method for choosing a good ridge parameter.</title>
<date>1979</date>
<journal>Technometrics,</journal>
<volume>21</volume>
<issue>2</issue>
<contexts>
<context position="27198" citStr="Golub et al., 1979" startWordPosition="4644" endWordPosition="4647">et j. 1. Normalize the columns of M (zero mean, standard deviation = 1). Pick feature set Fj and normalize its columns to a minimum of 0 and a maximum of 1. 238 2. Divide the data into 10 folds, for each fold b: (a) Isolate Mb and Fbj as test data. The remainder M−b and F−b jwill be used for training4. (b) Subtract the mean of the columns of M−b from Mb and M−b and the mean of the columns of F−b j from Fbj and F−b j (c) Use ridge regression to solve M−b = F−b j x βt j by tuning the A parameter to every one of the p output dimensions independently. A is chosen via generalized cross validation (Golub et al., 1979). (d) Perform a binary classification. Sample from the set of words in b a set c of 20 words. Then sample from b another set of 20 words such that the kth word in c and d have the same number of letters. For every sample (c,d): i. predict the MEG data for c and d as: Pc = Fcj x Γbj and Pd = Fdj x Γb j ii. assign to Mc the label c or d depending on which of Pc or Pd is closest (Euclidean distance). iii. assign to Md the label c or d depending on which of Pc or Pd is closest (Euclidean distance). 3. Compute the average accuracy. 2.3.3 Restricting the analysis spatially: a searchlight equivalent </context>
</contexts>
<marker>Golub, Heath, Wahba, 1979</marker>
<rawString>Gene H Golub, Michael Heath, and Grace Wahba. 1979. Generalized cross-validation as a method for choosing a good ridge parameter. Technometrics, 21(2):215–223.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Hagoort</author>
</authors>
<title>How the brain solves the binding problem for language: a neurocomputational model of syntactic processing.</title>
<date>2003</date>
<tech>Neuroimage,</tech>
<contexts>
<context position="7745" citStr="Hagoort, 2003" startWordPosition="1228" endWordPosition="1229">esses involved in reading Humans read with an average speed of 3 words per second. Reading requires us to perceive incoming words and gradually integrate them into a representation of the meaning. As words are read, it takes 100ms for the visual input to reach the visual cortex. 50ms later, the visual input is processed as letter strings in a specialized region of the left visual cortex (Salmelin, 2007). Between 200-500ms, the word’s semantic properties are processed (see Fig. 1). Less is understood about the cortical dynamics of word integration, as multiple theories exist (Friederici, 2002; Hagoort, 2003). Magnetoencephalography (MEG) is a brainimaging tool that is well suited for studying lanrd is analogous to the embedding that the ing the w of a wo work, we are interested in the mechanism of human text understanding as the meaning ming words is fetched from memory and d with the context. Interestingly, this is s to neural network models of language used to predict the incoming word. The mental representation of the previous context is s to the latent layer of the neural network mmarizes the relevant context before seeord. The representation of the meaning s the word with inversely proportio</context>
</contexts>
<marker>Hagoort, 2003</marker>
<rawString>Peter Hagoort. 2003. How the brain solves the binding problem for language: a neurocomputational model of syntactic processing. Neuroimage, 20:S18–S29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nikolaus Kriegeskorte</author>
<author>Rainer Goebel</author>
<author>Peter Bandettini</author>
</authors>
<title>Information-based functional brain mapping.</title>
<date>2006</date>
<booktitle>Proceedings of the National Academy of Sciences of the United States of America,</booktitle>
<volume>103</volume>
<issue>10</issue>
<contexts>
<context position="27857" citStr="Kriegeskorte et al., 2006" startWordPosition="4777" endWordPosition="4780">tion. Sample from the set of words in b a set c of 20 words. Then sample from b another set of 20 words such that the kth word in c and d have the same number of letters. For every sample (c,d): i. predict the MEG data for c and d as: Pc = Fcj x Γbj and Pd = Fdj x Γb j ii. assign to Mc the label c or d depending on which of Pc or Pd is closest (Euclidean distance). iii. assign to Md the label c or d depending on which of Pc or Pd is closest (Euclidean distance). 3. Compute the average accuracy. 2.3.3 Restricting the analysis spatially: a searchlight equivalent We adapt the searchlight method (Kriegeskorte et al., 2006) to MEG. The searchlight is a discovery procedure used in fMRI in which a cube is slid over the brain and an analysis is performed in each location separately. It allows to find regions in the brain where a specific phenomenon is occurring. In the MEG sensor space, for every one of the 102 sensor locations E, we assign a group of sensors g`. For every location E, we identify the locations that immediately surround it in any direction (Anterior, Right Anterior, Right etc...) when looking at the 2D flat representation of the location of the sensors in the MEG helmet (see Fig. 9 for an illustrati</context>
</contexts>
<marker>Kriegeskorte, Goebel, Bandettini, 2006</marker>
<rawString>Nikolaus Kriegeskorte, Rainer Goebel, and Peter Bandettini. 2006. Information-based functional brain mapping. Proceedings of the National Academy of Sciences of the United States of America, 103(10):3863–3868.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Stefan Kombrink</author>
<author>Anoop Deoras</author>
<author>Lukar Burget</author>
<author>J Cernocky</author>
</authors>
<title>RNNLMrecurrent neural network language modeling toolkit.</title>
<date>2011</date>
<booktitle>In Proc. of the 2011 ASRU Workshop,</booktitle>
<pages>196--201</pages>
<contexts>
<context position="4016" citStr="Mikolov et al., 2011" startWordPosition="642" endWordPosition="645">supervised representations. The brain is another computational system that processes language. Since we can record brain activity using neuroimaging, we propose a new direction that promises to improve our understanding of both how the brain is processing language and of what the neural networks are modeling by aligning the brain data with the neural networks representations. In this paper we study the representations of two kinds of neural networks that are built to predict the incoming word: recurrent and finite context models. The first model is the Recurrent Neural Network Language Model (Mikolov et al., 2011) which uses the entire history of words to model context. The second is the Neural Probabilistic Language Model (NPLM) which uses limited context constrained to the recent words (3 grams or 5 grams). We trained these models on a large Harry Potter fan fiction corpus and we then used them to predict the words of chapter 9 of Harry Potter and the Sorcerer’s Stone (Rowling, 2012). In parallel, we ran an MEG experiment in which 3 subject read the words of chapter 9 one by one while their brain activity was recorded. We then looked for the alignment between the word-by-word vectors produced by the </context>
</contexts>
<marker>Mikolov, Kombrink, Deoras, Burget, Cernocky, 2011</marker>
<rawString>Tomas Mikolov, Stefan Kombrink, Anoop Deoras, Lukar Burget, and J Cernocky. 2011. RNNLMrecurrent neural network language modeling toolkit. In Proc. of the 2011 ASRU Workshop, pages 196– 201.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
</authors>
<title>Statistical Language Models Based on Neural Networks.</title>
<date>2012</date>
<tech>Ph.D. thesis,</tech>
<institution>Brno University of Technology.</institution>
<contexts>
<context position="15276" citStr="Mikolov (2012)" startWordPosition="2512" endWordPosition="2513">current Neural Network Language Model Unlike standard feedforward neural language models that only look at a fixed number of past words, recurrent neural network language models use all the previous history from position 1 to t−1 to predict the next word. This is typically achieved by feedback connections, where the hidden layer activations used for predicting the word in position t − 1 are fed back into the network to compute the hidden layer activations for predicting the next word. The hidden layer thus stores the history of all previous words. We use the RNNLM architecture as described in Mikolov (2012), shown in Figure 3. The input to the RNNLM at position t are the one-hot representation of the current word, w(t), and the activations from the hidden layer at position t − 1, s(t − 1). The output of the hidden layer at position t − 1 is 1+exp(−x), that is applied elementwise. We need to compute the probability of the next word w(t + 1) given the hidden state s(t). For fast estimation of output word probabilities, Mikolov (2012) divides the computation into two stages: First, the probability distribution over word classes is computed, after which the probability distribution over the subset o</context>
<context position="19359" citStr="Mikolov (2012)" startWordPosition="3279" endWordPosition="3280">task that we explain below. We detect correspondences between the neural network components and the brain processes that underlie reading in the following fashion. If using a neural network vector (e.g. the RNNLM embedding vector) allows us to classify significantly better than chance in a given region of the brain at a given time (e.g. the visual cortex at time 100- 200ms), then we can hypothesize a relationship between that neural network constituent and the time/location of the analogous brain process. 2.1 Training the Neural Networks We used the freely available training tools provided by Mikolov (2012)1 and Vaswani et al. (2013)2 to train our RNNLM and NPLM models used in our brain data classification experiments. Our training data comprised around 67.5 million 1http://rnnlm.org/ 2http://nlg.isi.edu/software/nplm words for training and 100 thousand words for validation from the Harry Potter fan fiction database (http://harrypotterfanfiction.com). We restricted the vocabulary to the top 100 thousand words which covered all but 4 words from Chapter 9 of Harry Potter and the Sorcerer’s Stone. For the RNNLM, we trained models with different hidden layers and learning rates and found the RNNLM w</context>
</contexts>
<marker>Mikolov, 2012</marker>
<rawString>Tomas Mikolov. 2012. Statistical Language Models Based on Neural Networks. Ph.D. thesis, Brno University of Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vinod Nair</author>
<author>Geoffrey E Hinton</author>
</authors>
<title>Rectified linear units improve restricted Boltzmann machines.</title>
<date>2010</date>
<booktitle>In Proceedings of ICML,</booktitle>
<pages>807--814</pages>
<contexts>
<context position="17501" citStr="Nair and Hinton, 2010" startWordPosition="2932" endWordPosition="2935">ure 4. Each context u comprises a sequence of words uj (1 G j G n − 1) represented as one-hot vectors, which are fed as input to the neural network. At the output layer, the neural network computes the probability P(w |u) for each word w, as follows. The output of the first hidden layer h1 is ⎛ ⎞ n−1� h1 = φ ⎝CjDuj + b1 ⎠, j=1 where D is a matrix of input word embeddings which is shared across all positions, the Cj are the context matrices for each word in u, b1 is a vector of biases with the same dimension as h1, and φ is applied elementwise. Vaswani et al. (2013) use rectified linear units (Nair and Hinton, 2010) for s(t) = φ (Dw(t) + Ws(t − 1)) , where D is the matrix of input word embeddings, W is a matrix that transforms the activations from the hidden layer in position t − 1, and φ is a 1 sigmoid function, defined as φ(x) = 236 the hidden layers h1 and h2, which use the activation function φ(x) = max(0, x). The output of the second layer h2 is h2 = φ (Mh1 + b2) , where M is a weight matrix between h1 and h2 and b2 is a vector of biases for h2. The probability of the output word is computed at the output softmax layer as: where D&apos; is the matrix of output word embeddings, b is a vector of biases for</context>
</contexts>
<marker>Nair, Hinton, 2010</marker>
<rawString>Vinod Nair and Geoffrey E. Hinton. 2010. Rectified linear units improve restricted Boltzmann machines. In Proceedings of ICML, pages 807–814.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joanne K Rowling</author>
</authors>
<title>Harry Potter and the Sorcerer’s Stone. Harry Potter US.</title>
<date>2012</date>
<publisher>Pottermore Limited.</publisher>
<contexts>
<context position="4395" citStr="Rowling, 2012" startWordPosition="711" endWordPosition="712"> paper we study the representations of two kinds of neural networks that are built to predict the incoming word: recurrent and finite context models. The first model is the Recurrent Neural Network Language Model (Mikolov et al., 2011) which uses the entire history of words to model context. The second is the Neural Probabilistic Language Model (NPLM) which uses limited context constrained to the recent words (3 grams or 5 grams). We trained these models on a large Harry Potter fan fiction corpus and we then used them to predict the words of chapter 9 of Harry Potter and the Sorcerer’s Stone (Rowling, 2012). In parallel, we ran an MEG experiment in which 3 subject read the words of chapter 9 one by one while their brain activity was recorded. We then looked for the alignment between the word-by-word vectors produced by the neural networks and the word-byword neural activity recorded by MEG. Our neural networks have 3 key constituents: a hidden layer that summarizes the history of the previous words ; an embeddings vector that summarizes the (constant) properties of a given word and finally the output probability of a word given 233 Proceedings of the 2014 Conference on Empirical Methods in Natur</context>
<context position="18426" citStr="Rowling, 2012" startWordPosition="3123" endWordPosition="3124">tput of the second layer h2 is h2 = φ (Mh1 + b2) , where M is a weight matrix between h1 and h2 and b2 is a vector of biases for h2. The probability of the output word is computed at the output softmax layer as: where D&apos; is the matrix of output word embeddings, b is a vector of biases for every output word and vw its the one hot representation of the word w in the vocabulary. 2 Methods We describe in this section our approach. In summary, we trained the neural network models on a Harry Potter fan fiction database. We then ran these models on chapter 9 of Harry Potter and the Sorcerer’s Stone (Rowling, 2012) and computed the context and embedding vectors and the output probability for each word. In parallel, 3 subjects read the same chapter in an MEG scanner. We build models that predict the MEG data for each word as a function of the different neural network constituents. We then test these models with a classification task that we explain below. We detect correspondences between the neural network components and the brain processes that underlie reading in the following fashion. If using a neural network vector (e.g. the RNNLM embedding vector) allows us to classify significantly better than ch</context>
<context position="21011" citStr="Rowling, 2012" startWordPosition="3556" endWordPosition="3558">idden units to perform the best on the validation set and chose those models for our final experiments. We used the output word embeddings D&apos; in our experiments. We visually inspected the nearest neighbors in the 150 dimensional word embedding space for some words and didn’t find the neighbors from D&apos; or D to be distinctly better than each other. We leave the comparison of input and output embeddings on brain activity prediction for future work. 2.2 MEG paradigm We recorded MEG data for three subjects (2 females and one male) while they read chapter 9 of Harry Potter and the Sorcerer’s Stone (Rowling, 2012). The participants were native English speakers and right handed. They were chosen to be familiar with the material: we made sure they had read the Harry Potter books or seen the movies series and were familiar with the characters and the story. All the participants signed the consent form, which was approved by the University of Pittsburgh Institutional Review Board, and were compensated for their participation. The words of the story were presented in rapid serial visual format (Buchweitz et al., 2009): words were presented one by one at the center of the screen for 0.5 seconds each. The tex</context>
</contexts>
<marker>Rowling, 2012</marker>
<rawString>Joanne K. Rowling. 2012. Harry Potter and the Sorcerer’s Stone. Harry Potter US. Pottermore Limited.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Riitta Salmelin</author>
</authors>
<title>Clinical neurophysiology of language:</title>
<date>2007</date>
<journal>the MEG approach. Clinical Neurophysiology,</journal>
<volume>118</volume>
<issue>2</issue>
<contexts>
<context position="5911" citStr="Salmelin, 2007" startWordPosition="960" endWordPosition="961">the change in the magnetic field on the surface of the head that is caused by a largeset of aligned neurons that are changing their firing patterns in synchrony in response to a stimulus. Because of the nature of the signal, MEG recordings are directly related to neural activity and have no latency. They are sampled at a high frequency (typically 1kHz) that is ideal for tracking the fast dynamics of language processing. ilkin ippiott Willis &amp; � 200 I vatio piFigureh1: Corticalndynamics ofs silentereading. Thismfigure In this or t t 20–6 efles ial-antic analy and, py poga ayis is adapted from (Salmelin, 2007). Dots represent projected sources of activity in the visual cortex (left brain sketch) and the temporal cortex (right brain sketch). Th display theHmean time coursePofkactivation areaskforn differentnconditions. analysis innthe visual cortex at ∼100 non-specifict language. guage. Comparing responses toaletter stringssand other visualmstimulinrevealspthatloletter s 150 ms. Finally comparing the re words (made-up words)nreveals lexical-semantic analysisain theotemporal cortex at —200-500 ms. The curves of inco in the depicted source integrate Thehinitial visualufeature analogou ms is o an- that</context>
<context position="7537" citStr="Salmelin, 2007" startWordPosition="1198" endWordPosition="1199">rocessing areas to ,dculminatingoinagan updated storytcontext, andereflecting anuoverallEmagnitude of neural effort influenced by the probability of that new word given the previous context. ot 1.1 Neural processes involved in reading Humans read with an average speed of 3 words per second. Reading requires us to perceive incoming words and gradually integrate them into a representation of the meaning. As words are read, it takes 100ms for the visual input to reach the visual cortex. 50ms later, the visual input is processed as letter strings in a specialized region of the left visual cortex (Salmelin, 2007). Between 200-500ms, the word’s semantic properties are processed (see Fig. 1). Less is understood about the cortical dynamics of word integration, as multiple theories exist (Friederici, 2002; Hagoort, 2003). Magnetoencephalography (MEG) is a brainimaging tool that is well suited for studying lanrd is analogous to the embedding that the ing the w of a wo work, we are interested in the mechanism of human text understanding as the meaning ming words is fetched from memory and d with the context. Interestingly, this is s to neural network models of language used to predict the incoming word. The</context>
</contexts>
<marker>Salmelin, 2007</marker>
<rawString>Riitta Salmelin. 2007. Clinical neurophysiology of language: the MEG approach. Clinical Neurophysiology, 118(2):237–254.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gustavo Sudre</author>
<author>Dean Pomerleau</author>
<author>Mark Palatucci</author>
<author>Leila Wehbe</author>
<author>Alona Fyshe</author>
<author>Riitta Salmelin</author>
<author>Tom Mitchell</author>
</authors>
<title>Tracking neural coding of perceptual and semantic features of concrete nouns.</title>
<date>2012</date>
<journal>NeuroImage,</journal>
<volume>62</volume>
<issue>1</issue>
<contexts>
<context position="11554" citStr="Sudre et al., 2012" startWordPosition="1879" endWordPosition="1882">word i) and the integration of word i into the context (the output probability of word i). The periods drawn here are only a conjecture on the timings of such cognitive events. use the brain data to shed light on what the neural network vectors are representing. Related work Decoding cognitive states from brain data is a recent field that has been growing in popularity. Most decoding studies that study language use functional Magnetic Resonance Imaging (fMRI), while some studies use MEG. MEG’s high temporal resolution makes it invaluable for looking at the dynamics of language understanding. (Sudre et al., 2012) decode from MEG the word a subject is reading. The authors estimate from the MEG data the semantic features of the word and use these as an intermediate step to decode what the word is. This is in principle similar to the classification approach we follow, as we will also use the feature vectors as an intermediate step for word classification. However the experimental paradigm in (Sudre et al., 2012) is to present to the subjects single isolated words and to find how the brain represents their semantic features; whereas we have a much more complex and “naturalistic” experiment in which the su</context>
<context position="23763" citStr="Sudre et al., 2012" startWordPosition="4021" endWordPosition="4024">in a 10-fold cross validated fashion. At every fold, we train a linear model to predict MEG data as a function of one of the feature sets, using 90% of the data. On the remaining 10% of the data, we run a classification experiment. MEG data is very noisy. Therefore, classifying single word waveforms yields a low accuracy, peaking at 60%, which might lead to false negatives when looking for correspondences between neural network features and brain data. To reveal informative features, one can boost signal by either having several repetitions of the stimuli in the experiment and then averaging (Sudre et al., 2012) or by combining the words into larger chunks (Wehbe et al., 2014). We chose the latter because the former sacrifices word and feature diversity. At testing, we therefore repeat the following 300 times. Two sets of words are chosen randomly from the test fold. To form the first set, 20 words are sampled without replacement from the test sample (unseen by the classifier). To form the second set, the kth word is chosen randomly from all words in the test fold having the same length as 3In this paper, we treat these three different sensors as three different dimensions without further exploiting </context>
</contexts>
<marker>Sudre, Pomerleau, Palatucci, Wehbe, Fyshe, Salmelin, Mitchell, 2012</marker>
<rawString>Gustavo Sudre, Dean Pomerleau, Mark Palatucci, Leila Wehbe, Alona Fyshe, Riitta Salmelin, and Tom Mitchell. 2012. Tracking neural coding of perceptual and semantic features of concrete nouns. NeuroImage, 62(1):451–463.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Samu Taulu</author>
<author>Juha Simola</author>
</authors>
<title>Spatiotemporal signal space separation method for rejecting nearby interference</title>
<date>2006</date>
<booktitle>in MEG measurements. Physics in medicine and biology,</booktitle>
<pages>51--7</pages>
<contexts>
<context position="22620" citStr="Taulu and Simola, 2006" startWordPosition="3824" endWordPosition="3827">EG recordings were acquired on an Elekta Neuromag device at the University of Pittsburgh Medical Center Presbyterian Hospital. This machine has 306 sensors distributed into 102 locations on the surface of the subject’s head. Each location groups 3 sensors or two types: one magnometer that records the intensity of the magnetic field and two planar gradiometers that record the change in the magnetic field along two orthogonal planes3. Our sampling frequency was 1kHz. For preprocessing, we used Signal Space Separation method (SSS, (Taulu et al., 2004)), followed by its temporal extension (tSSS, (Taulu and Simola, 2006)). For each subject, the experiment data consists therefore of a 306 dimensional time series of length ∼45 minutes. We averaged the signal in every sensor into 100ms non-overlapping time bins. Since words were presented for 500ms each, we therefore obtain for every word p = 306 × 5 values corresponding to 306 vectors of 5 points. 2.3 Decoding experiment To find which parts of brain activity are related to the neural network constituents (e.g. the RNNLM context vector), we run a prediction and classification experiment in a 10-fold cross validated fashion. At every fold, we train a linear model</context>
</contexts>
<marker>Taulu, Simola, 2006</marker>
<rawString>Samu Taulu and Juha Simola. 2006. Spatiotemporal signal space separation method for rejecting nearby interference in MEG measurements. Physics in medicine and biology, 51(7):1759.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Samu Taulu</author>
<author>Matti Kajola</author>
<author>Juha Simola</author>
</authors>
<title>Suppression of interference and artifacts by the signal space separation method. Brain topography,</title>
<date>2004</date>
<pages>16--4</pages>
<contexts>
<context position="22551" citStr="Taulu et al., 2004" startWordPosition="3813" endWordPosition="3816">vw,), exp (vwD&apos;h2 + bTvw) 237 record the magnetic activity. Our MEG recordings were acquired on an Elekta Neuromag device at the University of Pittsburgh Medical Center Presbyterian Hospital. This machine has 306 sensors distributed into 102 locations on the surface of the subject’s head. Each location groups 3 sensors or two types: one magnometer that records the intensity of the magnetic field and two planar gradiometers that record the change in the magnetic field along two orthogonal planes3. Our sampling frequency was 1kHz. For preprocessing, we used Signal Space Separation method (SSS, (Taulu et al., 2004)), followed by its temporal extension (tSSS, (Taulu and Simola, 2006)). For each subject, the experiment data consists therefore of a 306 dimensional time series of length ∼45 minutes. We averaged the signal in every sensor into 100ms non-overlapping time bins. Since words were presented for 500ms each, we therefore obtain for every word p = 306 × 5 values corresponding to 306 vectors of 5 points. 2.3 Decoding experiment To find which parts of brain activity are related to the neural network constituents (e.g. the RNNLM context vector), we run a prediction and classification experiment in a 10</context>
</contexts>
<marker>Taulu, Kajola, Simola, 2004</marker>
<rawString>Samu Taulu, Matti Kajola, and Juha Simola. 2004. Suppression of interference and artifacts by the signal space separation method. Brain topography, 16(4):269–275.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ashish Vaswani</author>
<author>Yinggong Zhao</author>
<author>Victoria Fossum</author>
<author>David Chiang</author>
</authors>
<title>Decoding with large-scale neural language models improves translation.</title>
<date>2013</date>
<contexts>
<context position="16862" citStr="Vaswani et al. (2013)" startWordPosition="2802" endWordPosition="2805">ifies a subset V 0 of words, potentially smaller than the entire vocabulary V . The probability of an output word l at position t + 1 given that its class is m is defined as: P(yl(t + 1) |cm(t), s(t)) = exp (s(t)D0vl) �V 0 , k=1 (exp (s(t)D0vk)) where D0 is a matrix of output word embeddings and vl is a one hot vector representing the word with index l. The probability of the word w(t+ 1) given its class ci can now be computed as: P(w(t + 1) |s(t)) =P(w(t + 1) |ci, s(t)) P(ci |s(t)). Neural Probabilistic Language Model We use the feedforward neural probabilistic language model architecture of Vaswani et al. (2013), as shown in Figure 4. Each context u comprises a sequence of words uj (1 G j G n − 1) represented as one-hot vectors, which are fed as input to the neural network. At the output layer, the neural network computes the probability P(w |u) for each word w, as follows. The output of the first hidden layer h1 is ⎛ ⎞ n−1� h1 = φ ⎝CjDuj + b1 ⎠, j=1 where D is a matrix of input word embeddings which is shared across all positions, the Cj are the context matrices for each word in u, b1 is a vector of biases with the same dimension as h1, and φ is applied elementwise. Vaswani et al. (2013) use rectifi</context>
<context position="19386" citStr="Vaswani et al. (2013)" startWordPosition="3282" endWordPosition="3285"> below. We detect correspondences between the neural network components and the brain processes that underlie reading in the following fashion. If using a neural network vector (e.g. the RNNLM embedding vector) allows us to classify significantly better than chance in a given region of the brain at a given time (e.g. the visual cortex at time 100- 200ms), then we can hypothesize a relationship between that neural network constituent and the time/location of the analogous brain process. 2.1 Training the Neural Networks We used the freely available training tools provided by Mikolov (2012)1 and Vaswani et al. (2013)2 to train our RNNLM and NPLM models used in our brain data classification experiments. Our training data comprised around 67.5 million 1http://rnnlm.org/ 2http://nlg.isi.edu/software/nplm words for training and 100 thousand words for validation from the Harry Potter fan fiction database (http://harrypotterfanfiction.com). We restricted the vocabulary to the top 100 thousand words which covered all but 4 words from Chapter 9 of Harry Potter and the Sorcerer’s Stone. For the RNNLM, we trained models with different hidden layers and learning rates and found the RNNLM with 250 hidden units to per</context>
</contexts>
<marker>Vaswani, Zhao, Fossum, Chiang, 2013</marker>
<rawString>Ashish Vaswani, Yinggong Zhao, Victoria Fossum, and David Chiang. 2013. Decoding with large-scale neural language models improves translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leila Wehbe</author>
<author>Brian Murphy</author>
</authors>
<title>Partha Talukdar, Alona Fyshe, Aaditya Ramdas, and Tom Mitchell.</title>
<date>2014</date>
<note>in press.</note>
<marker>Wehbe, Murphy, 2014</marker>
<rawString>Leila Wehbe, Brian Murphy, Partha Talukdar, Alona Fyshe, Aaditya Ramdas, and Tom Mitchell. 2014. Simultaneously uncovering the patterns of brain regions involved in different story reading subprocesses. in press.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
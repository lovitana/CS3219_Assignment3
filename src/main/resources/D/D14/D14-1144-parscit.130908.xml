<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.007187">
<title confidence="0.994686">
Language Transfer Hypotheses with Linear SVM Weights
</title>
<author confidence="0.99737">
Shervin Malmasi
</author>
<affiliation confidence="0.995213">
Centre for Language Technology
Macquarie University
</affiliation>
<address confidence="0.672717">
Sydney, NSW, Australia
</address>
<email confidence="0.998001">
shervin.malmasi@mq.edu.au
</email>
<sectionHeader confidence="0.993875" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99989635">
Language transfer, the characteristic sec-
ond language usage patterns caused by na-
tive language interference, is investigated
by Second Language Acquisition (SLA)
researchers seeking to find overused and
underused linguistic features. In this pa-
per we develop and present a methodology
for deriving ranked lists of such features.
Using very large learner data, we show
our method’s ability to find relevant can-
didates using sophisticated linguistic fea-
tures. To illustrate its applicability to SLA
research, we formulate plausible language
transfer hypotheses supported by current
evidence. This is the first work to ex-
tend Native Language Identification to a
broader linguistic interpretation of learner
data and address the automatic extraction
of underused features on a per-native lan-
guage basis.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999330470588235">
It has been noted in the linguistics literature since
the 1950s that speakers of particular languages
have characteristic production patterns when writ-
ing in a second language. This language transfer
phenomenon has been investigated independently
in a number of fields from different perspectives,
including qualitative research in Second Language
Acquisition (SLA) and more recently though pre-
dictive computational models in NLP.
Motivated by the aim of improving foreign lan-
guage teaching and learning, such analyses are of-
ten done manually in SLA, and are difficult to
perform for large corpora. Smaller studies yield
poor results due to the sample size, leading to
extreme variability (Ellis, 2008). Recently, re-
searchers have noted that NLP has the tools to use
large amounts of data to automate this analysis,
</bodyText>
<author confidence="0.723882">
Mark Dras
</author>
<affiliation confidence="0.731424333333333">
Centre for Language Technology
Macquarie University
Sydney, NSW, Australia
</affiliation>
<email confidence="0.93619">
mark.dras@mq.edu.au
</email>
<bodyText confidence="0.999828736842105">
using complex feature types. This has motivated
studies in Native Language Identification (NLI), a
subtype of text classification where the goal is to
determine the native language (L1) of an author
using texts they have written in a second language
(L2) (Tetreault et al., 2013).
Despite the good results in predicting L1s, few
attempts have been made to interpret the features
that distinguish L1s. This is partly because no
methods for an SLA-oriented feature analysis have
been proposed; most work focuses on testing fea-
ture types using standard machine learning tools.
The overarching contribution of this work is to
develop a methodology that enables the transfor-
mation of the NLI paradigm into SLA applications
that can be used to link these features to their un-
derlying linguistic causes and explanations. These
candidates can then be applied in other areas such
as remedial SLA strategies or error detection.
</bodyText>
<sectionHeader confidence="0.999745" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999650444444445">
SLA research aims to find distributional differ-
ences in language use between L1s, often referred
to as overuse, the extensive use of some linguis-
tic structures, and underuse, the underutilization
of particular structures, also known as avoidance
(Gass and Selinker, 2008). While there have been
some attempts in SLA to use computational ap-
proaches on small-scale data,1 these still use fairly
elementary techniques and have several shortcom-
ings, including in the manual approaches to an-
notation and the computational artefacts derived
from these.
Conversely, NLI work has focused on automatic
learner L1 classification using Machine Learning
with large-scale data and sophisticated linguistic
features (Tetreault et al., 2012). Here, feature
ranking could be performed with relevancy meth-
ods such as the F-score:
</bodyText>
<footnote confidence="0.86532">
1E.g. Chen (2013), Lozan´o and Mendikoetxea (2010) and
Di´ez-Bedmar and Papp (2008).
</footnote>
<page confidence="0.863331">
1385
</page>
<bodyText confidence="0.694048">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1385–1390,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</bodyText>
<equation confidence="0.999773">
� �2 � �2
¯x(+)
j − ¯xj + ¯x(−)
j − ¯xj
n+� � �2 (1)
��2n−�
x(+)
i,j−¯x(+)+ 1 x(−)
i,j − ¯x(−)
jn−−1 j
i=1 i=1
</equation>
<bodyText confidence="0.997691888888889">
The F-score (Fisher score) measures the ratio
between the intraclass and interclass variance in
the values of feature j, where x represents the fea-
ture values in the negative and positive examples.2
More discriminative features have higher scores.
Another alternative method is Information Gain
(Yang and Pedersen, 1997). As defined in equation
(2), it measures the entropy gain associated with
feature t in assigning the class label c.
</bodyText>
<equation confidence="0.991364333333333">
G(t) = − Em i=1 Pr (ci) log Pr (ci)
+ Pr (t) Em i=1 Pr (cijt) log Pr (cijt) (2)
+ Pr (¯t)�m i=1 Pr (cij¯t) log Pr (cij¯t)
</equation>
<bodyText confidence="0.998699">
However, these methods are limited: they do not
provide ranked lists per-L1 class, and more impor-
tantly, they do not explicitly capture underuse.
Among the efflorescence of NLI work, a new
trend explored by Swanson and Charniak (2014)
aims to extract lists of candidate language transfer
features by comparing L2 data against the writer’s
L1 to find features where the L1 use is mirrored in
L2 use. This allows the detection of obvious ef-
fects, but Jarvis and Crossley (2012) note (p. 183)
that many transfer effects are “too complex” to ob-
serve in this manner. Moreover, this method is un-
able to detect underuse, is only suitable for syn-
tactic features, and has only been applied to very
small data (4,000 sentences) over three L1s. Ad-
dressing these issues is the focus of the present
work.
</bodyText>
<sectionHeader confidence="0.994341" genericHeader="method">
3 Experimental Setup
</sectionHeader>
<subsectionHeader confidence="0.996292">
3.1 Corpus
</subsectionHeader>
<bodyText confidence="0.999976333333333">
We use TOEFL11, the largest publicly available
corpus of English L2 texts (Blanchard et al.,
2013), containing 11 L1s with 1,100 texts each.3
</bodyText>
<subsectionHeader confidence="0.980365">
3.2 Features
</subsectionHeader>
<bodyText confidence="0.997629">
Adaptor grammar collocations Per Wong et al.
(2012), we utilize an adaptor grammar to discover
arbitrary length n-gram collocations. We explore
both the pure part-of-speech (POS) n-grams as
</bodyText>
<footnote confidence="0.999547">
2See Chang and Lin (2008) for more details.
3Over 4 million tokens in 12,100 texts.
</footnote>
<bodyText confidence="0.996391428571429">
well as the more promising mixtures of POS and
function words. We derive two adaptor grammars
where each is associated with a different set of vo-
cabulary: either pure POS or the mixture of POS
and function words. We use the grammar pro-
posed by Johnson (2010) for capturing topical col-
locations:
</bodyText>
<equation confidence="0.825846111111111">
Sentence → Docj j C 1,...,m
Docj → j j C 1,...,m
Docj → Docj Topici i C 1, ... , t;
jC 1,..., m
Topici → Words i C 1, ... ,t
Words → Word
Words → Words Word
Word → w w C Vpos;
w C Vpos+fw
</equation>
<bodyText confidence="0.997152">
Vpos contains 119 distinct POS tags based on the
Brown tagset and Vpos+fw is extended with 398
function words. The number of topics t is set to
50. The inference algorithm for the adaptor gram-
mars are based on the Markov Chain Monte Carlo
technique made available by Johnson (2010).4
Stanford dependencies We use Stanford de-
pendencies as a syntactic feature: for each
text we extract all the basic dependencies re-
turned by the Stanford Parser (de Marneffe et
al., 2006). We then generate all the variations
for each of the dependencies (grammatical rela-
tions) by substituting each lemma with its cor-
responding POS tag. For instance, a gram-
matical relation of det(knowledge, the)
yields the following variations: det(NN, the),
det(knowledge, DT), and det(NN, DT).
Lexical features Content and function words
are also considered as two feature types related to
learner’s vocabulary and spelling.
</bodyText>
<subsectionHeader confidence="0.999832">
3.3 Extracting Linear SVM Feature Weights
</subsectionHeader>
<bodyText confidence="0.9998631">
Using the extracted features, we train linear Sup-
port Vector Machine (SVM) models for each
L1. We use a one-vs-rest approach to find fea-
tures most relevant to each native language. L2-
regularization is applied to remove noisy features
and reduce the size of the candidate feature list.
More specifically, we employ the LIBLINEAR
SVM package (Fan et al., 2008)5 as it is well-
suited to text classification tasks with large num-
bers of features and texts as is the case here.
</bodyText>
<footnote confidence="0.997944">
4http://web.science.mq.edu.au/%7Emjohnson/Software.htm
5http://www.csie.ntu.edu.tw/%7Ecjlin/liblinear/
</footnote>
<equation confidence="0.740373">
F(j) ≡
1
n+−1
</equation>
<page confidence="0.854553">
1386
</page>
<bodyText confidence="0.97087">
In training the models for each feature, the SVM
weight vector6 is calculated according to (3):
</bodyText>
<equation confidence="0.9921255">
�w = αiyixi (3)
i
</equation>
<bodyText confidence="0.999973571428571">
After training, the positive and negative weights
are split into two lists and ranked by weight.
The positive weights represent overused features,
while features whose absence (i.e. underuse) is
indicative of an L1 class will have large negative
weights. This yields two candidate language trans-
fer feature lists per L1.
</bodyText>
<sectionHeader confidence="0.999927" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.999947111111111">
We now turn to an analysis of the output from our
system to illustrate its applicability for SLA re-
search. Table 1 lists some elements from the un-
deruse and overuse lists for various L1s. The lists
are of different feature types. They have been cho-
sen to demonstrate all feature types and also a va-
riety of different languages. For reasons of space,
only several of the top features are analysed here.
Hindi L1 writers are distinguished by certain
function words including hence, thus, and etc, and
a much higher usage rate of male pronouns. It has
been observed in the literature (Sanyal, 2007, for
example) that the English spoken in India still re-
tains characteristics of the English that was spoken
during the time of the Raj and the East India Com-
pany that have disappeared from other English va-
rieties, so it sounds more formal to other speakers,
or retains traces of an archaic business correspon-
dence style; the features noted fit that pattern.
The second list includes content words overused
by Arabic L1 learners. Analysis of content words
here, and for other L1s in our data, reveals very
frequent misspellings which are believed to be due
to orthographic or phonetic influences (Tsur and
Rappoport, 2007; Odlin, 1989). Since Arabic does
not share orthography with English, we believe
most of these are due to phonetics. Looking at
items 1, 3 and 5 we can see a common pattern:
the English letter u which has various phonetic re-
alizations is being replaced by a vowel that more
often represents that sound. Items 2 and 5 are also
phonetically similar to the intended words.
For Spanish L1 authors we provide both under-
use and overuse lists of syntactic dependencies.
The top 3 overuse rules show the word that is very
often used as the subject of verbs. This is almost
</bodyText>
<footnote confidence="0.558021">
6See Burges (1998) for a detailed explanation.
</footnote>
<bodyText confidence="0.99920306122449">
certainly a consequence of the prominent syntac-
tic role played by the Spanish word que which, de-
pending on the context, is equivalent to the English
words whom, who, which, and most commonly,
that. The fourth rule shows they often use this as a
determiner for plural nouns. A survey of the cor-
pus reveals many such errors in texts of Spanish
learners, e.g. this actions or this emissions. The
fifth rule shows that the adjectival modifier of a
plural noun is often being incorrectly pluralised to
match the noun in number as would be required in
Spanish, for example, differents subjects.
Turning to the underused features in Spanish L1
texts, we see that four related features rank highly,
showing that these is not commonly used as a de-
terminer for plural nouns and which is rarely used
as a subject. The final feature shows that no is
avoided as a determiner. This may be because
while no mostly has the same role in Spanish as it
does in English, it cannot be used as a determiner;
ning´un must be used instead. We hypothesize that
this construction is being avoided as placing no be-
fore a noun in Spanish is ungrammatical. This ex-
ample demonstrates that our two list methodology
can not only help identify overused structures, but
also uncovers the related constructs that are being
underutilized at their expense.
The final list in Table 1 is of underused Adap-
tor Grammar patterns by Chinese learners. The
first three features show that these writers signif-
icantly underuse determiners, here an, other and
these before nouns. This is not unexpected since
Chinese learners’ difficulties with English articles
are well known (Robertson, 2000). More inter-
estingly, we find underuse of features like even if
and might, along with others not listed here such
as could VB7 plus many other variants related to
the subjunctive mood. One explanation is that lin-
guistic differences between Chinese and English
in expressing counterfactuals could cause them to
avoid such constructions in L2 English. Previous
research in this area has linked the absence of sub-
junctive linguistic structures in Chinese to differ-
ent cognitive representations of the world and con-
sequences for thinking counterfactually (Bloom,
2014), although this has been disputed (Au, 1983;
Garbern Liu, 1985).
Adaptor Grammars also reveal frequent use of
the “existential there”8 in German L1 data while
</bodyText>
<footnote confidence="0.9085355">
7e.g. could be, could have, could go and other variants
8e.g. There is/are ..., as opposed to the locative there.
</footnote>
<page confidence="0.880012">
1387
</page>
<table confidence="0.999608714285714">
Overuse Underuse
Hindi Arabic Spanish Spanish Chinese
#2: thus #2: anderstand #1: nsubj(VBP,that) #2: det(NNS,these) #12: an NN
#4: hence #4: mony #2: nsubj(VBZ,that) #3: nsubj(VBZ,which) #16: other NN
#22: his #6: besy #3: nsubj(VB,that) #6: nsubj(VB,which) #18: these NNS
#30: etc #15: diffrent #4: det(NNS,this) #7: nsubj(VBP,which) #19: even if
#33:rather #38: seccessful #25: amod(NNS,differents) #10: det(NN,no) #68: might
</table>
<tableCaption confidence="0.968454">
Table 1: Example transfer candidates and rankings from the overuse/underuse lists for various L1s and
features types, in order: Hindi function words, Arabic content words, Spanish dependencies (2) and
Chinese Adaptor Grammars.
</tableCaption>
<table confidence="0.9993818">
English Spanish English Spanish
diferent diferente conclution conclusi´on
consecuence consecuencia desagree Neg. affix des-
responsability responsabilidad especific espec´ıfico
oportunity oportunidad necesary necesario
</table>
<tableCaption confidence="0.7625025">
Table 2: Highly ranked English misspellings of
Spanish learners and their Spanish cognates.
</tableCaption>
<bodyText confidence="0.998741413793103">
they are highly underused in French L1 data. The
literature supports our data: The German equiv-
alent es gibt is common while French use is far
more constrained (Cappelle and Loock, 2013).
Lexical analysis also revealed Spanish–English
orthographic transfer, listed in Table 2. This list
includes many cognates, in contrast with the Ara-
bic L1 data where most misspellings were pho-
netic in nature.
We also observe other patterns which remain
unexplained. For instance, Chinese, Japanese and
Korean speakers make excessive use of phrases
such as however, first and second. One possibil-
ity is that this relates to argumentation styles that
are possibly influenced by cultural norms. More
broadly, this effect could also be teaching rather
than transfer related. For example, it may be case
that a widely-used text book for learning English
in Korea happens to overuse this construction.
Some recent findings from the 2013 NLI Shared
Task found that L1 Hindi and Telugu learners of
English had similar transfer effects and their writ-
ings were difficult to distinguish. It has been
posited that this is likely due to shared culture and
teaching environments (Malmasi et al., 2013).
Despite some clearcut instances of overuse,9
more research is required to determine the causal
factors. We hope to expand on this in future work
using more data.
</bodyText>
<footnote confidence="0.8657635">
9More than half of the Korean scripts contained a
sentence-initial however.
</footnote>
<sectionHeader confidence="0.988629" genericHeader="conclusions">
5 Discussion and Conclusion
</sectionHeader>
<bodyText confidence="0.999986512820513">
Using the proposed methodology, we generated
lists of linguistic features overused and underused
by English learners of various L1 backgrounds.
Through an analysis of the top items in these
ranked lists, we demonstrated the high applicabil-
ity of the output by formulating plausible language
transfer hypotheses supported by current evidence.
We also showcased the method’s generalizability
to numerous linguistic feature types.
Our method’s output consists of two ranked lists
of linguistic features: one for overuse and the
other for underuse, something which had not been
addressed by research to date. We also found
Adaptor Grammar collocations to be highly infor-
mative for this task.
This work, an intersection of NLP, Machine
Learning and SLA, illustrates how the various dis-
ciplines can complement each other by bringing
together theoretical, experimental and computa-
tional issues. NLP provides accurate and auto-
mated tagging of large corpora with sophisticated
features not available in corpus linguistics, e.g.
with state-of-the-art dependency parsing. Sophis-
ticated machine learning techniques then enable
the processing of large quantities of data (thou-
sands of times the size of manual studies) in a way
that will let SLA researchers explore a variety of
assumptions and theoretical analyses. And con-
versely, NLP can benefit from the long-term study
and language acquisition insights from SLA.
In terms of NLI, this work is the first attempt to
expand NLI to a broad linguistic interpretation of
the data, including feature underuse. NLI systems
achieve classification accuracies of over 80% on
this 11-class task, leading to theoretical questions
about the features that make them so effective.
This work also has a backwards link in this regard
by providing qualitative evidence about the under-
pinning linguistic theories that make NLI work.
</bodyText>
<page confidence="0.98456">
1388
</page>
<bodyText confidence="0.999987814814815">
The work presented here has a number of ap-
plications; chief among them is the development
of tools for SLA researchers. This would enable
them to not just provide new evidence for previ-
ous findings, but to also perform semi-automated
data-driven generation of new and viable hypothe-
ses. This, in turn, can help reduce expert effort and
involvement in the process, particularly as such
studies expand to more corpora and emerging lan-
guage like Chinese (Malmasi and Dras, 2014b)
and Arabic (Malmasi and Dras, 2014a).
The brief analysis included here represents only
a tiny portion of what can be achieved with this
methodology. We included but a few of the thou-
sands of features revealed by this method; prac-
tical SLA tools based on this would have a great
impact on current research.
In addition to language transfer hypotheses,
such systems could also be applied to aid devel-
opment of pedagogical material within a needs-
based and data-driven approach. Once language
use patterns are uncovered, they can be assessed
for teachability and used to create tailored, L1-
specific exercises and teaching material.
From the examples discussed in Section 4 these
could include highly specific and targeted student
exercises to improve spelling, expand vocabulary
and enrich syntactic knowledge — all relative to
their mother tongue. Such exercises can not only
help beginners improve their fundamental skills
and redress their errors but also assist advanced
learners in moving closer to near-nativeness.
The extracted features and their weights could
also be used to build statistical models for gram-
matical error detection (Leacock et al., 2014).
Contrary to the norm of developing error checkers
for native writers, such models could be specifi-
cally targeted towards learners or even particular
L1–L2 pairs which could be useful in Computer-
Assisted Language Learning (CALL) systems.
One limitation here is that our features may
be corpus-dependent as they are all exam essays.
This can be addressed by augmenting the data with
new learner corpora, as they become available.
While a strength here is that we compared each L1
against others, a paired comparison only against
native texts can be insightful too.
There are several directions for future work.
The first relates to clustering the data within the
lists. Our intuition is that there might be coher-
ent clusters of related features, with these clusters
characterising typical errors or idiosyncrasies, that
are predictive of a particular L1. As shown in our
results, some features are highly related and may
be caused by the same underlying transfer phe-
nomena. For example, our list of overused syntac-
tic constructs by Spanish learners includes three
high ranking features related to the same transfer
effect. The use of unsupervised learning meth-
ods such as Bayesian mixture models may be ap-
propriate here. For parse features, tree kernels
could help measure similarity between the trees
and fragments (Collins and Duffy, 2001).
Another avenue is to implement weight-based
ranking methods to further refine and re-rank the
lists, potentially by incorporating the measures
mentioned in Section 2 to assign weights to fea-
tures. As the corpus we used includes learner
proficiency metadata, it may also be possible to
create proficiency-segregated models to find the
features that characterise errors at each language
proficiency level. Finally, the use of other lin-
guistic features such as Context-free Grammar
phrase structure rules or Tree Substitution Gram-
mars could provide additional insights.
In addition to these further technical investiga-
tions, we see as a particularly useful direction the
development of an SLA research tool to conduct a
large SLA study with a wide range of experts. We
believe that this study makes a contribution to this
area and hope that it will motivate future work.
</bodyText>
<sectionHeader confidence="0.998571" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.988791388888889">
Terry Kit-Fong Au. 1983. Chinese and English coun-
terfactuals: the Sapir-Whorf hypothesis revisited.
Cognition, 15(1):155–187.
Daniel Blanchard, Joel Tetreault, Derrick Higgins,
Aoife Cahill, and Martin Chodorow. 2013.
TOEFL11: A Corpus of Non-Native English. Tech-
nical report, Educational Testing Service.
Alfred H Bloom. 2014. The linguistic shaping of
thought: A study in the impact of language on think-
ing in China and the West. Psychology Press.
Christopher JC Burges. 1998. A tutorial on Support
Vector Machines for Pattern Recognition. Data min-
ing and knowledge discovery, 2(2):121–167.
Bert Cappelle and Rudy Loock. 2013. Is there in-
terference of usage constraints?: A frequency study
of existential there is and its French equivalent il
ya in translated vs. non-translated texts. Target,
25(2):252–275.
</reference>
<page confidence="0.953919">
1389
</page>
<reference confidence="0.999201972222223">
Yin-Wen Chang and Chih-Jen Lin. 2008. Feature
ranking using linear svm. Causation and Prediction
Challenge Challenges in Machine Learning, Volume
2, page 47.
Meilin Chen. 2013. Overuse or underuse: A cor-
pus study of English phrasal verb use by Chinese,
British and American university students. Interna-
tional Journal of Corpus Linguistics, 18(3).
Michael Collins and Nigel Duffy. 2001. Convolution
Kernels for Natural Language. In Advances in Neu-
ral Information Processing Systems, pages 625–632.
Marie-Catherine de Marneffe, Bill Maccartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of the Fifth International Conference
on Language Resources and Evaluation (LREC’06),
pages 449–454, Genoa, Italy.
Mar´ıa Bel´en Di´ez-Bedmar and Szilvia Papp. 2008.
The use of the English article system by Chinese
and Spanish learners. Language and Computers,
66(1):147–176.
Rod Ellis. 2008. The Study of Second Language Ac-
quisition, 2nd edition. Oxford University Press, Ox-
ford, UK.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A library for large linear classification. Journal of
Machine Learning Research, 9:1871–1874.
Lisa Garbern Liu. 1985. Reasoning counterfactually
in Chinese: Are there any obstacles? Cognition,
21(3):239–270.
Susan M. Gass and Larry Selinker. 2008. Second Lan-
guage Acquisition: An Introductory Course. Rout-
ledge, New York.
Scott Jarvis and Scott Crossley, editors. 2012. Ap-
proaching Language Transfer Through Text Classi-
fication: Explorations in the Detection-based Ap-
proach. Multilingual Matters, Bristol, UK.
Mark Johnson. 2010. PCFGs, Topic Models, Adaptor
Grammars and Learning Topical Collocations and
the Structure of Proper Names. In Proceedings of
the 48th Annual Meeting of the Association for Com-
putational Linguistics, pages 1148–1157, Uppsala,
Sweden, July. Association for Computational Lin-
guistics.
Claudia Leacock, Martin Chodorow, Michael Gamon,
and Joel Tetreault. 2014. Automated grammati-
cal error detection for language learners. Synthesis
Lectures on Human Language Technologies, 7(1):1–
170.
Cristobal Lozan´o and Amaya Mendikoetxea. 2010. In-
terface conditions on postverbal subjects: A corpus
study of L2 English. Bilingualism: Language and
Cognition, 13(4):475–497.
Shervin Malmasi and Mark Dras. 2014a. Arabic Na-
tive Language Identification. In Proceedings of the
Arabic Natural Language Processing Workshop (co-
located with EMNLP 2014), Doha, Qatar, October.
Association for Computational Linguistics.
Shervin Malmasi and Mark Dras. 2014b. Chinese
Native Language Identification. Proceedings of the
14th Conference of the European Chapter of the As-
sociation for Computational Linguistics, April.
Shervin Malmasi, Sze-Meng Jojo Wong, and Mark
Dras. 2013. NLI Shared Task 2013: MQ Submis-
sion. In Proceedings of the Eighth Workshop on In-
novative Use of NLP for Building Educational Ap-
plications, pages 124–133, Atlanta, Georgia, June.
Association for Computational Linguistics.
Terence Odlin. 1989. Language Transfer: Cross-
linguistic Influence in Language Learning. Cam-
bridge University Press, Cambridge, UK.
Daniel Robertson. 2000. Variability in the use of the
English article system by Chinese learners of En-
glish. Second Language Research, 16(2):135–172.
Jyoti Sanyal. 2007. Indlish: The Book for Every
English-Speaking Indian. Viva Books Private Lim-
ited.
Ben Swanson and Eugene Charniak. 2014. Data
Driven Language Transfer Hypotheses. EACL 2014,
page 169.
Joel Tetreault, Daniel Blanchard, Aoife Cahill, and
Martin Chodorow. 2012. Native Tongues, Lost
and Found: Resources and Empirical Evaluations in
Native Language Identification. In Proceedings of
COLING 2012, pages 2585–2602, Mumbai, India,
December. The COLING 2012 Organizing Commit-
tee.
Joel Tetreault, Daniel Blanchard, and Aoife Cahill.
2013. A report on the first native language identi-
fication shared task. In Proceedings of the Eighth
Workshop on Innovative Use of NLP for Build-
ing Educational Applications, pages 48–57, Atlanta,
Georgia, June. Association for Computational Lin-
guistics.
Oren Tsur and Ari Rappoport. 2007. Using classifier
features for studying the effect of native language
on the choice of written second language words. In
Proc. Workshop on Cognitive Aspects of Computat.
Language Acquisition, pages 9–16.
Sze-Meng Jojo Wong, Mark Dras, and Mark John-
son. 2012. Exploring Adaptor Grammars for Na-
tive Language Identification. In Proc. Conf. Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 699–709.
Yiming Yang and Jan O Pedersen. 1997. A compara-
tive study on feature selection in text categorization.
In ICML, volume 97, pages 412–420.
</reference>
<page confidence="0.989762">
1390
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.738479">
<title confidence="0.902362">Language Transfer Hypotheses with Linear SVM Weights Shervin Centre for Language Macquarie</title>
<author confidence="0.974301">NSW Sydney</author>
<email confidence="0.996334">shervin.malmasi@mq.edu.au</email>
<abstract confidence="0.997654666666667">Language transfer, the characteristic second language usage patterns caused by native language interference, is investigated by Second Language Acquisition (SLA) researchers seeking to find overused and underused linguistic features. In this paper we develop and present a methodology for deriving ranked lists of such features. Using very large learner data, we show our method’s ability to find relevant candidates using sophisticated linguistic features. To illustrate its applicability to SLA research, we formulate plausible language transfer hypotheses supported by current evidence. This is the first work to extend Native Language Identification to a broader linguistic interpretation of learner data and address the automatic extraction of underused features on a per-native language basis.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Terry Kit-Fong Au</author>
</authors>
<title>Chinese and English counterfactuals: the Sapir-Whorf hypothesis revisited.</title>
<date>1983</date>
<journal>Cognition,</journal>
<volume>15</volume>
<issue>1</issue>
<contexts>
<context position="12409" citStr="Au, 1983" startWordPosition="2029" endWordPosition="2030">). More interestingly, we find underuse of features like even if and might, along with others not listed here such as could VB7 plus many other variants related to the subjunctive mood. One explanation is that linguistic differences between Chinese and English in expressing counterfactuals could cause them to avoid such constructions in L2 English. Previous research in this area has linked the absence of subjunctive linguistic structures in Chinese to different cognitive representations of the world and consequences for thinking counterfactually (Bloom, 2014), although this has been disputed (Au, 1983; Garbern Liu, 1985). Adaptor Grammars also reveal frequent use of the “existential there”8 in German L1 data while 7e.g. could be, could have, could go and other variants 8e.g. There is/are ..., as opposed to the locative there. 1387 Overuse Underuse Hindi Arabic Spanish Spanish Chinese #2: thus #2: anderstand #1: nsubj(VBP,that) #2: det(NNS,these) #12: an NN #4: hence #4: mony #2: nsubj(VBZ,that) #3: nsubj(VBZ,which) #16: other NN #22: his #6: besy #3: nsubj(VB,that) #6: nsubj(VB,which) #18: these NNS #30: etc #15: diffrent #4: det(NNS,this) #7: nsubj(VBP,which) #19: even if #33:rather #38: </context>
</contexts>
<marker>Au, 1983</marker>
<rawString>Terry Kit-Fong Au. 1983. Chinese and English counterfactuals: the Sapir-Whorf hypothesis revisited. Cognition, 15(1):155–187.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Daniel Blanchard</author>
<author>Joel Tetreault</author>
</authors>
<location>Derrick Higgins,</location>
<marker>Blanchard, Tetreault, </marker>
<rawString>Daniel Blanchard, Joel Tetreault, Derrick Higgins,</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aoife Cahill</author>
<author>Martin Chodorow</author>
</authors>
<title>TOEFL11: A Corpus of Non-Native English.</title>
<date>2013</date>
<tech>Technical report, Educational Testing Service.</tech>
<marker>Cahill, Chodorow, 2013</marker>
<rawString>Aoife Cahill, and Martin Chodorow. 2013. TOEFL11: A Corpus of Non-Native English. Technical report, Educational Testing Service.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alfred H Bloom</author>
</authors>
<title>The linguistic shaping of thought: A study in the impact of language on thinking in China and the West.</title>
<date>2014</date>
<publisher>Psychology Press.</publisher>
<contexts>
<context position="12366" citStr="Bloom, 2014" startWordPosition="2022" endWordPosition="2023">nglish articles are well known (Robertson, 2000). More interestingly, we find underuse of features like even if and might, along with others not listed here such as could VB7 plus many other variants related to the subjunctive mood. One explanation is that linguistic differences between Chinese and English in expressing counterfactuals could cause them to avoid such constructions in L2 English. Previous research in this area has linked the absence of subjunctive linguistic structures in Chinese to different cognitive representations of the world and consequences for thinking counterfactually (Bloom, 2014), although this has been disputed (Au, 1983; Garbern Liu, 1985). Adaptor Grammars also reveal frequent use of the “existential there”8 in German L1 data while 7e.g. could be, could have, could go and other variants 8e.g. There is/are ..., as opposed to the locative there. 1387 Overuse Underuse Hindi Arabic Spanish Spanish Chinese #2: thus #2: anderstand #1: nsubj(VBP,that) #2: det(NNS,these) #12: an NN #4: hence #4: mony #2: nsubj(VBZ,that) #3: nsubj(VBZ,which) #16: other NN #22: his #6: besy #3: nsubj(VB,that) #6: nsubj(VB,which) #18: these NNS #30: etc #15: diffrent #4: det(NNS,this) #7: nsu</context>
</contexts>
<marker>Bloom, 2014</marker>
<rawString>Alfred H Bloom. 2014. The linguistic shaping of thought: A study in the impact of language on thinking in China and the West. Psychology Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher JC Burges</author>
</authors>
<title>A tutorial on Support Vector Machines for Pattern Recognition. Data mining and knowledge discovery,</title>
<date>1998</date>
<pages>2--2</pages>
<contexts>
<context position="10127" citStr="Burges (1998)" startWordPosition="1649" endWordPosition="1650">nces (Tsur and Rappoport, 2007; Odlin, 1989). Since Arabic does not share orthography with English, we believe most of these are due to phonetics. Looking at items 1, 3 and 5 we can see a common pattern: the English letter u which has various phonetic realizations is being replaced by a vowel that more often represents that sound. Items 2 and 5 are also phonetically similar to the intended words. For Spanish L1 authors we provide both underuse and overuse lists of syntactic dependencies. The top 3 overuse rules show the word that is very often used as the subject of verbs. This is almost 6See Burges (1998) for a detailed explanation. certainly a consequence of the prominent syntactic role played by the Spanish word que which, depending on the context, is equivalent to the English words whom, who, which, and most commonly, that. The fourth rule shows they often use this as a determiner for plural nouns. A survey of the corpus reveals many such errors in texts of Spanish learners, e.g. this actions or this emissions. The fifth rule shows that the adjectival modifier of a plural noun is often being incorrectly pluralised to match the noun in number as would be required in Spanish, for example, dif</context>
</contexts>
<marker>Burges, 1998</marker>
<rawString>Christopher JC Burges. 1998. A tutorial on Support Vector Machines for Pattern Recognition. Data mining and knowledge discovery, 2(2):121–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bert Cappelle</author>
<author>Rudy Loock</author>
</authors>
<title>Is there interference of usage constraints?: A frequency study of existential there is and its French equivalent il ya in translated vs. non-translated texts.</title>
<date>2013</date>
<journal>Target,</journal>
<volume>25</volume>
<issue>2</issue>
<contexts>
<context position="13795" citStr="Cappelle and Loock, 2013" startWordPosition="2225" endWordPosition="2228"> and features types, in order: Hindi function words, Arabic content words, Spanish dependencies (2) and Chinese Adaptor Grammars. English Spanish English Spanish diferent diferente conclution conclusi´on consecuence consecuencia desagree Neg. affix desresponsability responsabilidad especific espec´ıfico oportunity oportunidad necesary necesario Table 2: Highly ranked English misspellings of Spanish learners and their Spanish cognates. they are highly underused in French L1 data. The literature supports our data: The German equivalent es gibt is common while French use is far more constrained (Cappelle and Loock, 2013). Lexical analysis also revealed Spanish–English orthographic transfer, listed in Table 2. This list includes many cognates, in contrast with the Arabic L1 data where most misspellings were phonetic in nature. We also observe other patterns which remain unexplained. For instance, Chinese, Japanese and Korean speakers make excessive use of phrases such as however, first and second. One possibility is that this relates to argumentation styles that are possibly influenced by cultural norms. More broadly, this effect could also be teaching rather than transfer related. For example, it may be case </context>
</contexts>
<marker>Cappelle, Loock, 2013</marker>
<rawString>Bert Cappelle and Rudy Loock. 2013. Is there interference of usage constraints?: A frequency study of existential there is and its French equivalent il ya in translated vs. non-translated texts. Target, 25(2):252–275.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yin-Wen Chang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>Feature ranking using linear svm. Causation and Prediction Challenge Challenges</title>
<date>2008</date>
<booktitle>in Machine Learning,</booktitle>
<volume>2</volume>
<pages>47</pages>
<contexts>
<context position="5811" citStr="Chang and Lin (2008)" startWordPosition="910" endWordPosition="913">is method is unable to detect underuse, is only suitable for syntactic features, and has only been applied to very small data (4,000 sentences) over three L1s. Addressing these issues is the focus of the present work. 3 Experimental Setup 3.1 Corpus We use TOEFL11, the largest publicly available corpus of English L2 texts (Blanchard et al., 2013), containing 11 L1s with 1,100 texts each.3 3.2 Features Adaptor grammar collocations Per Wong et al. (2012), we utilize an adaptor grammar to discover arbitrary length n-gram collocations. We explore both the pure part-of-speech (POS) n-grams as 2See Chang and Lin (2008) for more details. 3Over 4 million tokens in 12,100 texts. well as the more promising mixtures of POS and function words. We derive two adaptor grammars where each is associated with a different set of vocabulary: either pure POS or the mixture of POS and function words. We use the grammar proposed by Johnson (2010) for capturing topical collocations: Sentence → Docj j C 1,...,m Docj → j j C 1,...,m Docj → Docj Topici i C 1, ... , t; jC 1,..., m Topici → Words i C 1, ... ,t Words → Word Words → Words Word Word → w w C Vpos; w C Vpos+fw Vpos contains 119 distinct POS tags based on the Brown tag</context>
</contexts>
<marker>Chang, Lin, 2008</marker>
<rawString>Yin-Wen Chang and Chih-Jen Lin. 2008. Feature ranking using linear svm. Causation and Prediction Challenge Challenges in Machine Learning, Volume 2, page 47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Meilin Chen</author>
</authors>
<title>Overuse or underuse: A corpus study of English phrasal verb use by Chinese, British and American university students.</title>
<date>2013</date>
<journal>International Journal of Corpus Linguistics,</journal>
<volume>18</volume>
<issue>3</issue>
<contexts>
<context position="3678" citStr="Chen (2013)" startWordPosition="550" endWordPosition="551">s, also known as avoidance (Gass and Selinker, 2008). While there have been some attempts in SLA to use computational approaches on small-scale data,1 these still use fairly elementary techniques and have several shortcomings, including in the manual approaches to annotation and the computational artefacts derived from these. Conversely, NLI work has focused on automatic learner L1 classification using Machine Learning with large-scale data and sophisticated linguistic features (Tetreault et al., 2012). Here, feature ranking could be performed with relevancy methods such as the F-score: 1E.g. Chen (2013), Lozan´o and Mendikoetxea (2010) and Di´ez-Bedmar and Papp (2008). 1385 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1385–1390, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics � �2 � �2 ¯x(+) j − ¯xj + ¯x(−) j − ¯xj n+� � �2 (1) ��2n−� x(+) i,j−¯x(+)+ 1 x(−) i,j − ¯x(−) jn−−1 j i=1 i=1 The F-score (Fisher score) measures the ratio between the intraclass and interclass variance in the values of feature j, where x represents the feature values in the negative and positive examples.2 More discriminative f</context>
</contexts>
<marker>Chen, 2013</marker>
<rawString>Meilin Chen. 2013. Overuse or underuse: A corpus study of English phrasal verb use by Chinese, British and American university students. International Journal of Corpus Linguistics, 18(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Nigel Duffy</author>
</authors>
<title>Convolution Kernels for Natural Language.</title>
<date>2001</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>625--632</pages>
<contexts>
<context position="19890" citStr="Collins and Duffy, 2001" startWordPosition="3185" endWordPosition="3188">lusters of related features, with these clusters characterising typical errors or idiosyncrasies, that are predictive of a particular L1. As shown in our results, some features are highly related and may be caused by the same underlying transfer phenomena. For example, our list of overused syntactic constructs by Spanish learners includes three high ranking features related to the same transfer effect. The use of unsupervised learning methods such as Bayesian mixture models may be appropriate here. For parse features, tree kernels could help measure similarity between the trees and fragments (Collins and Duffy, 2001). Another avenue is to implement weight-based ranking methods to further refine and re-rank the lists, potentially by incorporating the measures mentioned in Section 2 to assign weights to features. As the corpus we used includes learner proficiency metadata, it may also be possible to create proficiency-segregated models to find the features that characterise errors at each language proficiency level. Finally, the use of other linguistic features such as Context-free Grammar phrase structure rules or Tree Substitution Grammars could provide additional insights. In addition to these further te</context>
</contexts>
<marker>Collins, Duffy, 2001</marker>
<rawString>Michael Collins and Nigel Duffy. 2001. Convolution Kernels for Natural Language. In Advances in Neural Information Processing Systems, pages 625–632.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Bill Maccartney</author>
<author>Christopher D Manning</author>
</authors>
<title>Generating typed dependency parses from phrase structure parses.</title>
<date>2006</date>
<booktitle>In Proceedings of the Fifth International Conference on Language Resources and Evaluation (LREC’06),</booktitle>
<pages>449--454</pages>
<location>Genoa, Italy.</location>
<marker>de Marneffe, Maccartney, Manning, 2006</marker>
<rawString>Marie-Catherine de Marneffe, Bill Maccartney, and Christopher D. Manning. 2006. Generating typed dependency parses from phrase structure parses. In Proceedings of the Fifth International Conference on Language Resources and Evaluation (LREC’06), pages 449–454, Genoa, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mar´ıa Bel´en Di´ez-Bedmar</author>
<author>Szilvia Papp</author>
</authors>
<title>The use of the English article system by Chinese and Spanish learners.</title>
<date>2008</date>
<journal>Language and Computers,</journal>
<volume>66</volume>
<issue>1</issue>
<marker>Di´ez-Bedmar, Papp, 2008</marker>
<rawString>Mar´ıa Bel´en Di´ez-Bedmar and Szilvia Papp. 2008. The use of the English article system by Chinese and Spanish learners. Language and Computers, 66(1):147–176.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rod Ellis</author>
</authors>
<title>The Study of Second Language Acquisition, 2nd edition.</title>
<date>2008</date>
<publisher>Oxford University Press,</publisher>
<location>Oxford, UK.</location>
<contexts>
<context position="1696" citStr="Ellis, 2008" startWordPosition="244" endWordPosition="245">r languages have characteristic production patterns when writing in a second language. This language transfer phenomenon has been investigated independently in a number of fields from different perspectives, including qualitative research in Second Language Acquisition (SLA) and more recently though predictive computational models in NLP. Motivated by the aim of improving foreign language teaching and learning, such analyses are often done manually in SLA, and are difficult to perform for large corpora. Smaller studies yield poor results due to the sample size, leading to extreme variability (Ellis, 2008). Recently, researchers have noted that NLP has the tools to use large amounts of data to automate this analysis, Mark Dras Centre for Language Technology Macquarie University Sydney, NSW, Australia mark.dras@mq.edu.au using complex feature types. This has motivated studies in Native Language Identification (NLI), a subtype of text classification where the goal is to determine the native language (L1) of an author using texts they have written in a second language (L2) (Tetreault et al., 2013). Despite the good results in predicting L1s, few attempts have been made to interpret the features th</context>
</contexts>
<marker>Ellis, 2008</marker>
<rawString>Rod Ellis. 2008. The Study of Second Language Acquisition, 2nd edition. Oxford University Press, Oxford, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rong-En Fan</author>
<author>Kai-Wei Chang</author>
<author>Cho-Jui Hsieh</author>
<author>XiangRui Wang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBLINEAR: A library for large linear classification.</title>
<date>2008</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>9--1871</pages>
<contexts>
<context position="7649" citStr="Fan et al., 2008" startWordPosition="1231" endWordPosition="1234">yields the following variations: det(NN, the), det(knowledge, DT), and det(NN, DT). Lexical features Content and function words are also considered as two feature types related to learner’s vocabulary and spelling. 3.3 Extracting Linear SVM Feature Weights Using the extracted features, we train linear Support Vector Machine (SVM) models for each L1. We use a one-vs-rest approach to find features most relevant to each native language. L2- regularization is applied to remove noisy features and reduce the size of the candidate feature list. More specifically, we employ the LIBLINEAR SVM package (Fan et al., 2008)5 as it is wellsuited to text classification tasks with large numbers of features and texts as is the case here. 4http://web.science.mq.edu.au/%7Emjohnson/Software.htm 5http://www.csie.ntu.edu.tw/%7Ecjlin/liblinear/ F(j) ≡ 1 n+−1 1386 In training the models for each feature, the SVM weight vector6 is calculated according to (3): �w = αiyixi (3) i After training, the positive and negative weights are split into two lists and ranked by weight. The positive weights represent overused features, while features whose absence (i.e. underuse) is indicative of an L1 class will have large negative weigh</context>
</contexts>
<marker>Fan, Chang, Hsieh, Wang, Lin, 2008</marker>
<rawString>Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, XiangRui Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A library for large linear classification. Journal of Machine Learning Research, 9:1871–1874.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lisa Garbern Liu</author>
</authors>
<title>Reasoning counterfactually in Chinese: Are there any obstacles?</title>
<date>1985</date>
<journal>Cognition,</journal>
<volume>21</volume>
<issue>3</issue>
<contexts>
<context position="12429" citStr="Liu, 1985" startWordPosition="2032" endWordPosition="2033">gly, we find underuse of features like even if and might, along with others not listed here such as could VB7 plus many other variants related to the subjunctive mood. One explanation is that linguistic differences between Chinese and English in expressing counterfactuals could cause them to avoid such constructions in L2 English. Previous research in this area has linked the absence of subjunctive linguistic structures in Chinese to different cognitive representations of the world and consequences for thinking counterfactually (Bloom, 2014), although this has been disputed (Au, 1983; Garbern Liu, 1985). Adaptor Grammars also reveal frequent use of the “existential there”8 in German L1 data while 7e.g. could be, could have, could go and other variants 8e.g. There is/are ..., as opposed to the locative there. 1387 Overuse Underuse Hindi Arabic Spanish Spanish Chinese #2: thus #2: anderstand #1: nsubj(VBP,that) #2: det(NNS,these) #12: an NN #4: hence #4: mony #2: nsubj(VBZ,that) #3: nsubj(VBZ,which) #16: other NN #22: his #6: besy #3: nsubj(VB,that) #6: nsubj(VB,which) #18: these NNS #30: etc #15: diffrent #4: det(NNS,this) #7: nsubj(VBP,which) #19: even if #33:rather #38: seccessful #25: amod</context>
</contexts>
<marker>Liu, 1985</marker>
<rawString>Lisa Garbern Liu. 1985. Reasoning counterfactually in Chinese: Are there any obstacles? Cognition, 21(3):239–270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Susan M Gass</author>
<author>Larry Selinker</author>
</authors>
<title>Second Language Acquisition: An Introductory Course.</title>
<date>2008</date>
<publisher>Routledge,</publisher>
<location>New York.</location>
<contexts>
<context position="3119" citStr="Gass and Selinker, 2008" startWordPosition="465" endWordPosition="468">erarching contribution of this work is to develop a methodology that enables the transformation of the NLI paradigm into SLA applications that can be used to link these features to their underlying linguistic causes and explanations. These candidates can then be applied in other areas such as remedial SLA strategies or error detection. 2 Related Work SLA research aims to find distributional differences in language use between L1s, often referred to as overuse, the extensive use of some linguistic structures, and underuse, the underutilization of particular structures, also known as avoidance (Gass and Selinker, 2008). While there have been some attempts in SLA to use computational approaches on small-scale data,1 these still use fairly elementary techniques and have several shortcomings, including in the manual approaches to annotation and the computational artefacts derived from these. Conversely, NLI work has focused on automatic learner L1 classification using Machine Learning with large-scale data and sophisticated linguistic features (Tetreault et al., 2012). Here, feature ranking could be performed with relevancy methods such as the F-score: 1E.g. Chen (2013), Lozan´o and Mendikoetxea (2010) and Di´</context>
</contexts>
<marker>Gass, Selinker, 2008</marker>
<rawString>Susan M. Gass and Larry Selinker. 2008. Second Language Acquisition: An Introductory Course. Routledge, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Jarvis</author>
<author>Scott Crossley</author>
<author>editors</author>
</authors>
<title>Approaching Language Transfer Through Text Classification: Explorations in the Detection-based Approach. Multilingual Matters,</title>
<date>2012</date>
<location>Bristol, UK.</location>
<marker>Jarvis, Crossley, editors, 2012</marker>
<rawString>Scott Jarvis and Scott Crossley, editors. 2012. Approaching Language Transfer Through Text Classification: Explorations in the Detection-based Approach. Multilingual Matters, Bristol, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
</authors>
<title>PCFGs, Topic Models, Adaptor Grammars and Learning Topical Collocations and the Structure of Proper Names.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1148--1157</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="6128" citStr="Johnson (2010)" startWordPosition="969" endWordPosition="970"> (Blanchard et al., 2013), containing 11 L1s with 1,100 texts each.3 3.2 Features Adaptor grammar collocations Per Wong et al. (2012), we utilize an adaptor grammar to discover arbitrary length n-gram collocations. We explore both the pure part-of-speech (POS) n-grams as 2See Chang and Lin (2008) for more details. 3Over 4 million tokens in 12,100 texts. well as the more promising mixtures of POS and function words. We derive two adaptor grammars where each is associated with a different set of vocabulary: either pure POS or the mixture of POS and function words. We use the grammar proposed by Johnson (2010) for capturing topical collocations: Sentence → Docj j C 1,...,m Docj → j j C 1,...,m Docj → Docj Topici i C 1, ... , t; jC 1,..., m Topici → Words i C 1, ... ,t Words → Word Words → Words Word Word → w w C Vpos; w C Vpos+fw Vpos contains 119 distinct POS tags based on the Brown tagset and Vpos+fw is extended with 398 function words. The number of topics t is set to 50. The inference algorithm for the adaptor grammars are based on the Markov Chain Monte Carlo technique made available by Johnson (2010).4 Stanford dependencies We use Stanford dependencies as a syntactic feature: for each text we</context>
</contexts>
<marker>Johnson, 2010</marker>
<rawString>Mark Johnson. 2010. PCFGs, Topic Models, Adaptor Grammars and Learning Topical Collocations and the Structure of Proper Names. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1148–1157, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claudia Leacock</author>
<author>Martin Chodorow</author>
<author>Michael Gamon</author>
<author>Joel Tetreault</author>
</authors>
<title>Automated grammatical error detection for language learners. Synthesis Lectures on Human Language Technologies,</title>
<date>2014</date>
<volume>7</volume>
<issue>1</issue>
<pages>170</pages>
<contexts>
<context position="18549" citStr="Leacock et al., 2014" startWordPosition="2970" endWordPosition="2973">r teachability and used to create tailored, L1- specific exercises and teaching material. From the examples discussed in Section 4 these could include highly specific and targeted student exercises to improve spelling, expand vocabulary and enrich syntactic knowledge — all relative to their mother tongue. Such exercises can not only help beginners improve their fundamental skills and redress their errors but also assist advanced learners in moving closer to near-nativeness. The extracted features and their weights could also be used to build statistical models for grammatical error detection (Leacock et al., 2014). Contrary to the norm of developing error checkers for native writers, such models could be specifically targeted towards learners or even particular L1–L2 pairs which could be useful in ComputerAssisted Language Learning (CALL) systems. One limitation here is that our features may be corpus-dependent as they are all exam essays. This can be addressed by augmenting the data with new learner corpora, as they become available. While a strength here is that we compared each L1 against others, a paired comparison only against native texts can be insightful too. There are several directions for fu</context>
</contexts>
<marker>Leacock, Chodorow, Gamon, Tetreault, 2014</marker>
<rawString>Claudia Leacock, Martin Chodorow, Michael Gamon, and Joel Tetreault. 2014. Automated grammatical error detection for language learners. Synthesis Lectures on Human Language Technologies, 7(1):1– 170.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cristobal Lozan´o</author>
<author>Amaya Mendikoetxea</author>
</authors>
<title>Interface conditions on postverbal subjects: A corpus study of L2</title>
<date>2010</date>
<journal>English. Bilingualism: Language and Cognition,</journal>
<volume>13</volume>
<issue>4</issue>
<marker>Lozan´o, Mendikoetxea, 2010</marker>
<rawString>Cristobal Lozan´o and Amaya Mendikoetxea. 2010. Interface conditions on postverbal subjects: A corpus study of L2 English. Bilingualism: Language and Cognition, 13(4):475–497.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shervin Malmasi</author>
<author>Mark Dras</author>
</authors>
<title>Arabic Native Language Identification.</title>
<date>2014</date>
<booktitle>In Proceedings of the Arabic Natural Language Processing Workshop (colocated with EMNLP 2014),</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Doha, Qatar,</location>
<contexts>
<context position="17385" citStr="Malmasi and Dras, 2014" startWordPosition="2787" endWordPosition="2790">work also has a backwards link in this regard by providing qualitative evidence about the underpinning linguistic theories that make NLI work. 1388 The work presented here has a number of applications; chief among them is the development of tools for SLA researchers. This would enable them to not just provide new evidence for previous findings, but to also perform semi-automated data-driven generation of new and viable hypotheses. This, in turn, can help reduce expert effort and involvement in the process, particularly as such studies expand to more corpora and emerging language like Chinese (Malmasi and Dras, 2014b) and Arabic (Malmasi and Dras, 2014a). The brief analysis included here represents only a tiny portion of what can be achieved with this methodology. We included but a few of the thousands of features revealed by this method; practical SLA tools based on this would have a great impact on current research. In addition to language transfer hypotheses, such systems could also be applied to aid development of pedagogical material within a needsbased and data-driven approach. Once language use patterns are uncovered, they can be assessed for teachability and used to create tailored, L1- specific </context>
</contexts>
<marker>Malmasi, Dras, 2014</marker>
<rawString>Shervin Malmasi and Mark Dras. 2014a. Arabic Native Language Identification. In Proceedings of the Arabic Natural Language Processing Workshop (colocated with EMNLP 2014), Doha, Qatar, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shervin Malmasi</author>
<author>Mark Dras</author>
</authors>
<title>Chinese Native Language Identification.</title>
<date>2014</date>
<booktitle>Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<contexts>
<context position="17385" citStr="Malmasi and Dras, 2014" startWordPosition="2787" endWordPosition="2790">work also has a backwards link in this regard by providing qualitative evidence about the underpinning linguistic theories that make NLI work. 1388 The work presented here has a number of applications; chief among them is the development of tools for SLA researchers. This would enable them to not just provide new evidence for previous findings, but to also perform semi-automated data-driven generation of new and viable hypotheses. This, in turn, can help reduce expert effort and involvement in the process, particularly as such studies expand to more corpora and emerging language like Chinese (Malmasi and Dras, 2014b) and Arabic (Malmasi and Dras, 2014a). The brief analysis included here represents only a tiny portion of what can be achieved with this methodology. We included but a few of the thousands of features revealed by this method; practical SLA tools based on this would have a great impact on current research. In addition to language transfer hypotheses, such systems could also be applied to aid development of pedagogical material within a needsbased and data-driven approach. Once language use patterns are uncovered, they can be assessed for teachability and used to create tailored, L1- specific </context>
</contexts>
<marker>Malmasi, Dras, 2014</marker>
<rawString>Shervin Malmasi and Mark Dras. 2014b. Chinese Native Language Identification. Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shervin Malmasi</author>
<author>Sze-Meng Jojo Wong</author>
<author>Mark Dras</author>
</authors>
<title>NLI Shared Task 2013: MQ Submission.</title>
<date>2013</date>
<booktitle>In Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications,</booktitle>
<pages>124--133</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Atlanta, Georgia,</location>
<contexts>
<context position="14783" citStr="Malmasi et al., 2013" startWordPosition="2382" endWordPosition="2385">irst and second. One possibility is that this relates to argumentation styles that are possibly influenced by cultural norms. More broadly, this effect could also be teaching rather than transfer related. For example, it may be case that a widely-used text book for learning English in Korea happens to overuse this construction. Some recent findings from the 2013 NLI Shared Task found that L1 Hindi and Telugu learners of English had similar transfer effects and their writings were difficult to distinguish. It has been posited that this is likely due to shared culture and teaching environments (Malmasi et al., 2013). Despite some clearcut instances of overuse,9 more research is required to determine the causal factors. We hope to expand on this in future work using more data. 9More than half of the Korean scripts contained a sentence-initial however. 5 Discussion and Conclusion Using the proposed methodology, we generated lists of linguistic features overused and underused by English learners of various L1 backgrounds. Through an analysis of the top items in these ranked lists, we demonstrated the high applicability of the output by formulating plausible language transfer hypotheses supported by current </context>
</contexts>
<marker>Malmasi, Wong, Dras, 2013</marker>
<rawString>Shervin Malmasi, Sze-Meng Jojo Wong, and Mark Dras. 2013. NLI Shared Task 2013: MQ Submission. In Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 124–133, Atlanta, Georgia, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terence Odlin</author>
</authors>
<title>Language Transfer: Crosslinguistic Influence in Language Learning.</title>
<date>1989</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, UK.</location>
<contexts>
<context position="9558" citStr="Odlin, 1989" startWordPosition="1546" endWordPosition="1547"> English spoken in India still retains characteristics of the English that was spoken during the time of the Raj and the East India Company that have disappeared from other English varieties, so it sounds more formal to other speakers, or retains traces of an archaic business correspondence style; the features noted fit that pattern. The second list includes content words overused by Arabic L1 learners. Analysis of content words here, and for other L1s in our data, reveals very frequent misspellings which are believed to be due to orthographic or phonetic influences (Tsur and Rappoport, 2007; Odlin, 1989). Since Arabic does not share orthography with English, we believe most of these are due to phonetics. Looking at items 1, 3 and 5 we can see a common pattern: the English letter u which has various phonetic realizations is being replaced by a vowel that more often represents that sound. Items 2 and 5 are also phonetically similar to the intended words. For Spanish L1 authors we provide both underuse and overuse lists of syntactic dependencies. The top 3 overuse rules show the word that is very often used as the subject of verbs. This is almost 6See Burges (1998) for a detailed explanation. ce</context>
</contexts>
<marker>Odlin, 1989</marker>
<rawString>Terence Odlin. 1989. Language Transfer: Crosslinguistic Influence in Language Learning. Cambridge University Press, Cambridge, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Robertson</author>
</authors>
<title>Variability in the use of the English article system by Chinese learners of English.</title>
<date>2000</date>
<journal>Second Language Research,</journal>
<volume>16</volume>
<issue>2</issue>
<contexts>
<context position="11802" citStr="Robertson, 2000" startWordPosition="1934" endWordPosition="1935"> that this construction is being avoided as placing no before a noun in Spanish is ungrammatical. This example demonstrates that our two list methodology can not only help identify overused structures, but also uncovers the related constructs that are being underutilized at their expense. The final list in Table 1 is of underused Adaptor Grammar patterns by Chinese learners. The first three features show that these writers significantly underuse determiners, here an, other and these before nouns. This is not unexpected since Chinese learners’ difficulties with English articles are well known (Robertson, 2000). More interestingly, we find underuse of features like even if and might, along with others not listed here such as could VB7 plus many other variants related to the subjunctive mood. One explanation is that linguistic differences between Chinese and English in expressing counterfactuals could cause them to avoid such constructions in L2 English. Previous research in this area has linked the absence of subjunctive linguistic structures in Chinese to different cognitive representations of the world and consequences for thinking counterfactually (Bloom, 2014), although this has been disputed (A</context>
</contexts>
<marker>Robertson, 2000</marker>
<rawString>Daniel Robertson. 2000. Variability in the use of the English article system by Chinese learners of English. Second Language Research, 16(2):135–172.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jyoti Sanyal</author>
</authors>
<title>Indlish: The Book for Every English-Speaking Indian.</title>
<date>2007</date>
<publisher>Viva Books Private Limited.</publisher>
<contexts>
<context position="8923" citStr="Sanyal, 2007" startWordPosition="1440" endWordPosition="1441">per L1. 4 Results We now turn to an analysis of the output from our system to illustrate its applicability for SLA research. Table 1 lists some elements from the underuse and overuse lists for various L1s. The lists are of different feature types. They have been chosen to demonstrate all feature types and also a variety of different languages. For reasons of space, only several of the top features are analysed here. Hindi L1 writers are distinguished by certain function words including hence, thus, and etc, and a much higher usage rate of male pronouns. It has been observed in the literature (Sanyal, 2007, for example) that the English spoken in India still retains characteristics of the English that was spoken during the time of the Raj and the East India Company that have disappeared from other English varieties, so it sounds more formal to other speakers, or retains traces of an archaic business correspondence style; the features noted fit that pattern. The second list includes content words overused by Arabic L1 learners. Analysis of content words here, and for other L1s in our data, reveals very frequent misspellings which are believed to be due to orthographic or phonetic influences (Tsu</context>
</contexts>
<marker>Sanyal, 2007</marker>
<rawString>Jyoti Sanyal. 2007. Indlish: The Book for Every English-Speaking Indian. Viva Books Private Limited.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ben Swanson</author>
<author>Eugene Charniak</author>
</authors>
<title>Data Driven Language Transfer Hypotheses.</title>
<date>2014</date>
<booktitle>EACL 2014,</booktitle>
<pages>169</pages>
<contexts>
<context position="4851" citStr="Swanson and Charniak (2014)" startWordPosition="748" endWordPosition="751">negative and positive examples.2 More discriminative features have higher scores. Another alternative method is Information Gain (Yang and Pedersen, 1997). As defined in equation (2), it measures the entropy gain associated with feature t in assigning the class label c. G(t) = − Em i=1 Pr (ci) log Pr (ci) + Pr (t) Em i=1 Pr (cijt) log Pr (cijt) (2) + Pr (¯t)�m i=1 Pr (cij¯t) log Pr (cij¯t) However, these methods are limited: they do not provide ranked lists per-L1 class, and more importantly, they do not explicitly capture underuse. Among the efflorescence of NLI work, a new trend explored by Swanson and Charniak (2014) aims to extract lists of candidate language transfer features by comparing L2 data against the writer’s L1 to find features where the L1 use is mirrored in L2 use. This allows the detection of obvious effects, but Jarvis and Crossley (2012) note (p. 183) that many transfer effects are “too complex” to observe in this manner. Moreover, this method is unable to detect underuse, is only suitable for syntactic features, and has only been applied to very small data (4,000 sentences) over three L1s. Addressing these issues is the focus of the present work. 3 Experimental Setup 3.1 Corpus We use TOE</context>
</contexts>
<marker>Swanson, Charniak, 2014</marker>
<rawString>Ben Swanson and Eugene Charniak. 2014. Data Driven Language Transfer Hypotheses. EACL 2014, page 169.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joel Tetreault</author>
<author>Daniel Blanchard</author>
<author>Aoife Cahill</author>
<author>Martin Chodorow</author>
</authors>
<title>Native Tongues, Lost and Found: Resources and Empirical Evaluations in Native Language Identification.</title>
<date>2012</date>
<booktitle>In Proceedings of COLING 2012,</booktitle>
<pages>2585--2602</pages>
<location>Mumbai, India,</location>
<contexts>
<context position="3574" citStr="Tetreault et al., 2012" startWordPosition="531" endWordPosition="534">overuse, the extensive use of some linguistic structures, and underuse, the underutilization of particular structures, also known as avoidance (Gass and Selinker, 2008). While there have been some attempts in SLA to use computational approaches on small-scale data,1 these still use fairly elementary techniques and have several shortcomings, including in the manual approaches to annotation and the computational artefacts derived from these. Conversely, NLI work has focused on automatic learner L1 classification using Machine Learning with large-scale data and sophisticated linguistic features (Tetreault et al., 2012). Here, feature ranking could be performed with relevancy methods such as the F-score: 1E.g. Chen (2013), Lozan´o and Mendikoetxea (2010) and Di´ez-Bedmar and Papp (2008). 1385 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1385–1390, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics � �2 � �2 ¯x(+) j − ¯xj + ¯x(−) j − ¯xj n+� � �2 (1) ��2n−� x(+) i,j−¯x(+)+ 1 x(−) i,j − ¯x(−) jn−−1 j i=1 i=1 The F-score (Fisher score) measures the ratio between the intraclass and interclass variance in the values of featur</context>
</contexts>
<marker>Tetreault, Blanchard, Cahill, Chodorow, 2012</marker>
<rawString>Joel Tetreault, Daniel Blanchard, Aoife Cahill, and Martin Chodorow. 2012. Native Tongues, Lost and Found: Resources and Empirical Evaluations in Native Language Identification. In Proceedings of COLING 2012, pages 2585–2602, Mumbai, India, December. The COLING 2012 Organizing Committee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joel Tetreault</author>
<author>Daniel Blanchard</author>
<author>Aoife Cahill</author>
</authors>
<title>A report on the first native language identification shared task.</title>
<date>2013</date>
<booktitle>In Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications,</booktitle>
<pages>48--57</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Atlanta, Georgia,</location>
<contexts>
<context position="2194" citStr="Tetreault et al., 2013" startWordPosition="319" endWordPosition="322">rm for large corpora. Smaller studies yield poor results due to the sample size, leading to extreme variability (Ellis, 2008). Recently, researchers have noted that NLP has the tools to use large amounts of data to automate this analysis, Mark Dras Centre for Language Technology Macquarie University Sydney, NSW, Australia mark.dras@mq.edu.au using complex feature types. This has motivated studies in Native Language Identification (NLI), a subtype of text classification where the goal is to determine the native language (L1) of an author using texts they have written in a second language (L2) (Tetreault et al., 2013). Despite the good results in predicting L1s, few attempts have been made to interpret the features that distinguish L1s. This is partly because no methods for an SLA-oriented feature analysis have been proposed; most work focuses on testing feature types using standard machine learning tools. The overarching contribution of this work is to develop a methodology that enables the transformation of the NLI paradigm into SLA applications that can be used to link these features to their underlying linguistic causes and explanations. These candidates can then be applied in other areas such as remed</context>
</contexts>
<marker>Tetreault, Blanchard, Cahill, 2013</marker>
<rawString>Joel Tetreault, Daniel Blanchard, and Aoife Cahill. 2013. A report on the first native language identification shared task. In Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 48–57, Atlanta, Georgia, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oren Tsur</author>
<author>Ari Rappoport</author>
</authors>
<title>Using classifier features for studying the effect of native language on the choice of written second language words.</title>
<date>2007</date>
<booktitle>In Proc. Workshop on Cognitive Aspects of Computat. Language Acquisition,</booktitle>
<pages>9--16</pages>
<contexts>
<context position="9544" citStr="Tsur and Rappoport, 2007" startWordPosition="1542" endWordPosition="1545">007, for example) that the English spoken in India still retains characteristics of the English that was spoken during the time of the Raj and the East India Company that have disappeared from other English varieties, so it sounds more formal to other speakers, or retains traces of an archaic business correspondence style; the features noted fit that pattern. The second list includes content words overused by Arabic L1 learners. Analysis of content words here, and for other L1s in our data, reveals very frequent misspellings which are believed to be due to orthographic or phonetic influences (Tsur and Rappoport, 2007; Odlin, 1989). Since Arabic does not share orthography with English, we believe most of these are due to phonetics. Looking at items 1, 3 and 5 we can see a common pattern: the English letter u which has various phonetic realizations is being replaced by a vowel that more often represents that sound. Items 2 and 5 are also phonetically similar to the intended words. For Spanish L1 authors we provide both underuse and overuse lists of syntactic dependencies. The top 3 overuse rules show the word that is very often used as the subject of verbs. This is almost 6See Burges (1998) for a detailed e</context>
</contexts>
<marker>Tsur, Rappoport, 2007</marker>
<rawString>Oren Tsur and Ari Rappoport. 2007. Using classifier features for studying the effect of native language on the choice of written second language words. In Proc. Workshop on Cognitive Aspects of Computat. Language Acquisition, pages 9–16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sze-Meng Jojo Wong</author>
<author>Mark Dras</author>
<author>Mark Johnson</author>
</authors>
<title>Exploring Adaptor Grammars for Native Language Identification.</title>
<date>2012</date>
<booktitle>In Proc. Conf. Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>699--709</pages>
<contexts>
<context position="5647" citStr="Wong et al. (2012)" startWordPosition="885" endWordPosition="888">he detection of obvious effects, but Jarvis and Crossley (2012) note (p. 183) that many transfer effects are “too complex” to observe in this manner. Moreover, this method is unable to detect underuse, is only suitable for syntactic features, and has only been applied to very small data (4,000 sentences) over three L1s. Addressing these issues is the focus of the present work. 3 Experimental Setup 3.1 Corpus We use TOEFL11, the largest publicly available corpus of English L2 texts (Blanchard et al., 2013), containing 11 L1s with 1,100 texts each.3 3.2 Features Adaptor grammar collocations Per Wong et al. (2012), we utilize an adaptor grammar to discover arbitrary length n-gram collocations. We explore both the pure part-of-speech (POS) n-grams as 2See Chang and Lin (2008) for more details. 3Over 4 million tokens in 12,100 texts. well as the more promising mixtures of POS and function words. We derive two adaptor grammars where each is associated with a different set of vocabulary: either pure POS or the mixture of POS and function words. We use the grammar proposed by Johnson (2010) for capturing topical collocations: Sentence → Docj j C 1,...,m Docj → j j C 1,...,m Docj → Docj Topici i C 1, ... , t</context>
</contexts>
<marker>Wong, Dras, Johnson, 2012</marker>
<rawString>Sze-Meng Jojo Wong, Mark Dras, and Mark Johnson. 2012. Exploring Adaptor Grammars for Native Language Identification. In Proc. Conf. Empirical Methods in Natural Language Processing (EMNLP), pages 699–709.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yiming Yang</author>
<author>Jan O Pedersen</author>
</authors>
<title>A comparative study on feature selection in text categorization.</title>
<date>1997</date>
<booktitle>In ICML,</booktitle>
<volume>97</volume>
<pages>412--420</pages>
<contexts>
<context position="4378" citStr="Yang and Pedersen, 1997" startWordPosition="660" endWordPosition="663">ceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1385–1390, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics � �2 � �2 ¯x(+) j − ¯xj + ¯x(−) j − ¯xj n+� � �2 (1) ��2n−� x(+) i,j−¯x(+)+ 1 x(−) i,j − ¯x(−) jn−−1 j i=1 i=1 The F-score (Fisher score) measures the ratio between the intraclass and interclass variance in the values of feature j, where x represents the feature values in the negative and positive examples.2 More discriminative features have higher scores. Another alternative method is Information Gain (Yang and Pedersen, 1997). As defined in equation (2), it measures the entropy gain associated with feature t in assigning the class label c. G(t) = − Em i=1 Pr (ci) log Pr (ci) + Pr (t) Em i=1 Pr (cijt) log Pr (cijt) (2) + Pr (¯t)�m i=1 Pr (cij¯t) log Pr (cij¯t) However, these methods are limited: they do not provide ranked lists per-L1 class, and more importantly, they do not explicitly capture underuse. Among the efflorescence of NLI work, a new trend explored by Swanson and Charniak (2014) aims to extract lists of candidate language transfer features by comparing L2 data against the writer’s L1 to find features wh</context>
</contexts>
<marker>Yang, Pedersen, 1997</marker>
<rawString>Yiming Yang and Jan O Pedersen. 1997. A comparative study on feature selection in text categorization. In ICML, volume 97, pages 412–420.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
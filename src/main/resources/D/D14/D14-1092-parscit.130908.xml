<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.008542">
<title confidence="0.997634">
A Joint Model for Unsupervised Chinese Word Segmentation
</title>
<author confidence="0.991982">
Miaohong Chen Baobao Chang Wenzhe Pei
</author>
<affiliation confidence="0.9905115">
Key Laboratory of Computational Linguistics, Ministry of Education
School of Electronics Engineering and Computer Science, Peking University
</affiliation>
<address confidence="0.839833">
Beijing, P.R.China, 100871
</address>
<email confidence="0.994363">
miaohong-chen@foxmail.com,{chbb,peiwenzhe}@pku.edu.cn
</email>
<sectionHeader confidence="0.997367" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999927703703704">
In this paper, we propose a joint model for
unsupervised Chinese word segmentation
(CWS). Inspired by the “products of ex-
perts” idea, our joint model firstly com-
bines two generative models, which are
word-based hierarchical Dirichlet process
model and character-based hidden Markov
model, by simply multiplying their proba-
bilities together. Gibbs sampling is used
for model inference. In order to further
combine the strength of goodness-based
model, we then integrated nVBE into our
joint model by using it to initializing the
Gibbs sampler. We conduct our experi-
ments on PKU and MSRA datasets pro-
vided by the second SIGHAN bakeoff.
Test results on these two datasets show
that the joint model achieves much bet-
ter results than all of its component mod-
els. Statistical significance tests also show
that it is significantly better than state-
of-the-art systems, achieving the highest
F-scores. Finally, analysis indicates that
compared with nVBE and HDP, the joint
model has a stronger ability to solve both
combinational and overlapping ambigui-
ties in Chinese word segmentation.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999965607843137">
Unlike English and many other western languages,
there are no explicit word boundaries in Chinese
sentences. Therefore, word segmentation is a cru-
cial first step for many Chinese language process-
ing tasks such as syntactic parsing, information re-
trieval and machine translation. A great deal of su-
pervised methods have been proposed for Chinese
word segmentation. While successful, they re-
quire manually labeled resources and often suffer
from issues like poor domain adaptability. Thus,
unsupervised word segmentation methods are still
attractive to researchers due to its independence on
domain and manually labeled corpora.
Previous unsupervised approaches to word seg-
mentation can be roughly classified into two types.
The first type uses carefully designed goodness
measure to identify word candidates. Popular
goodness measures include description length gain
(DLG) (Kit and Wilks, 1999), accessor variety
(AV) (Feng et al., 2004), boundary entropy (BE)
(Jin and Tanaka-Ishii, 2006) and normalized vari-
ation of branching entropy (nVBE) (Magistry and
Sagot, 2012) etc. Goodness measure based model
is not segmentation model in a very strict mean-
ing and is actually strong in generating word list
without supervision. It inherently lacks capabil-
ity to deal with ambiguous string, which is one of
main sources of segmentation errors and has been
extensively explored in supervised Chinese word
segmentation.
The second type focuses on designing sophis-
ticated statistical model, usually nonparametric
Bayesian models, to find the segmentation with
highest posterior probability, given the observed
character sequences. Typical statistical mod-
els includes Hierarchical Dirichlet process (HDP)
model (Goldwater et al., 2009), Nested Pitman-
Yor process (NPY) model (Mochihashi et al.,
2009) etc, which are actually nonparametric lan-
guage models and therefor can be categorized as
word-based model. Word-based model makes de-
cision on wordhood of a candidate character se-
quence mainly based on information outside the
sequence, namely, the wordhood of character se-
quences being adjacent to the concerned sequence.
Inspired by the success of character-based
model in supervised word segmentation, we pro-
pose a Bayesian HMM model for unsupervised
Chinese word segmentation. With the Bayesian
HMM model, we formulate the unsupervised seg-
mentation tasks as procedure of tagging positional
</bodyText>
<page confidence="0.984569">
854
</page>
<note confidence="0.910995">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 854–863,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999746888888889">
tags to characters. Different from word-based
model, character-based model like HMM-based
model as we propose make decisions on word-
hood of a candidate character sequence based on
information inside the sequence, namely, ability of
characters to form words. Although the Bayesian
HMM model alone does not produce competi-
tive results, it contributes substantially to the joint
model as proposed in this paper.
Our joint model takes advantage from three dif-
ferent models: namely, a character-based model
(HMM-based), a word-based model (HDP-based)
and a goodness measure based model (nVBE
model). The combination of HDP-based model
and HMM-based model enables to utilize infor-
mation of both word-level and character-level. We
also show that using nVBE model as initialization
model could further improve the performance to
outperform the state-of-the-art systems and leads
to improvement in both wordhood judgment and
disambiguation ability.
Word segmentation systems are usually eval-
uated with metrics like precision, recall and F-
Score, regardless of supervised or unsupervised.
Following normal practice, we evaluate our model
and compare it with state-of-the-art systems us-
ing F-Score. However, we argue that the ability
to solve segmentation ambiguities is also impor-
tant when evaluating different types of unsuper-
vised word segmentation systems.
This paper is organized as follows. In Section
2, we will introduce several related systems for
unsupervised word segmentation. Then our joint
model is presented in Section 3. Section 4 shows
our experiment results on the benchmark datasets
and Section 5 concludes the paper.
</bodyText>
<sectionHeader confidence="0.99989" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999862152542373">
Unsupervised Chinese word segmentation has
been explored in a number of previous works and
by various methods. Most of these methods can
be divided into two categories: goodness measure
based methods and nonparametric Bayesian meth-
ods.
There have been a plenty of work that is based
on a specific goodness measure. Zhao and Kit
(2008) compared several popular unsupervised
models within a unified framework. They tried
various types of goodness measures, such as De-
scription Length Gain (DLG) proposed by Kit and
Wilks (1999), Accessor Variety (AV) proposed by
Feng et al. (2004) and Boundary Entropy (Jin and
Tanaka-Ishii, 2006). A notable goodness-based
method is ESA: “Evaluation, Selection, Adjust-
ment”, which is proposed by Wang et al. (2011)
for unsupervised Mandarin Chinese word segmen-
tation. ESA is an iterative model based on a new
goodness algorithm that adopts a local maximum
strategy and avoids threshold setting. One disad-
vantage of ESA is that it needs to iterate the pro-
cess several times on the corpus to get good perfor-
mance. Another disadvantage is the requirement
for a manually segmented training corpus to find
best value for parameters (they called it proper ex-
ponent). Another notable work is nVBE: Mag-
istry and Sagot (2012) proposed a model based
on the Variation of Branching Entropy. By adding
normalization and viterbi decoding, they improve
performance over Jin and Tanaka-Ishii (2006)
and remove most of the parameters and thresholds
from the model.
Nonparametric Bayesian models also achieved
state-of-the-art performance in unsupervised word
segmentation. Goldwater et al. (2009) introduced
a unigram and a bigram model for unsupervised
word segmentation, which are based on Dirichlet
process and hierarchical Dirichlet process (Teh et
al., 2006) respectively. The main drawback is that
it needs almost 20,000 iterations before the Gibbs
sampler converges. Mochihashi et al. (2009) ex-
tended this method by introducing a nested charac-
ter model and an efficient blocked Gibbs sampler.
Their method is based on what they called nested
Pitman-Yor language model.
One disadvantage of goodness measure based
methods is that they do not have any disambigua-
tion ability in theory in spite of their competitive
performances. This is because once the goodness
measure is given, the decoding algorithm will seg-
ment any ambiguous strings into the same word
sequences, no matter what their context is. In
contrast, nonparametric Bayesian language mod-
els aim to segment character string into a “reason-
able” sentence according to the posterior probabil-
ity. Thus, theoretically, this method should have
better ability to solve ambiguities over goodness
measure based methods.
</bodyText>
<sectionHeader confidence="0.999229" genericHeader="method">
3 Joint Model
</sectionHeader>
<bodyText confidence="0.9358775">
In this section, we will discuss our joint model in
detail.
</bodyText>
<page confidence="0.997585">
855
</page>
<subsectionHeader confidence="0.999839">
3.1 Combining HDP and HMM
</subsectionHeader>
<bodyText confidence="0.999645518518519">
In supervised Chinese word segmentation lit-
erature, word-based approaches and character-
based approaches often have complementary ad-
vantages (Wang et al., 2010).Since the two types
of model try to solve the problem from different
perspectives and by utilizing different levels of in-
formation (word level and character level). In un-
supervised Chinese word segmentation literature,
the HDP-base model can be viewed as a typi-
cal word-based method. And we can also build
a character-based unsupervised model by using a
hidden Markov model. We believe that the HDP-
based model and the HMM-based model are also
complementary with each other, and a combina-
tion of them will take advantage of both and thus
capture different levels of information.
Now the problem we are facing is how to com-
bine these two models. To keep the joint model
simple and involve as little extra parameters as
possible, we combine the two baseline models by
just multiplying their probabilities together and
then renormalizing it. Let C = c1c2 · · · c|C |be a
string of characters and W = w1w2 · · · w|W |is the
corresponding segmented words sequence. Then
the conditional probability of the segmentation W
given the character string C in our joint model is
defined as:
</bodyText>
<equation confidence="0.998631">
1
PJ(W|C) = Z(C)PD(W|C)PM(W|C) (1)
</equation>
<bodyText confidence="0.999968076923077">
where PD(W |C) is the probability from the HDP
model as given in Equation 6 and PM(W |C)
is the probability given by the Bayesian HMM
model as given in Equation 2. Z(C) is a nor-
malization term to make sure that PJ(W|C) is a
probability distribution. The combining method is
inspired by Hinton (1999), which proved that it is
possible to combine many individual expert mod-
els by multiplying the probabilities and then renor-
malizing it. They called it “product of experts”.
We can see that combining models in this way
does not involve any extra parameters and Gibbs
sampling can be easily used for model inference.
</bodyText>
<subsectionHeader confidence="0.999338">
3.2 Bayesian HMM
</subsectionHeader>
<bodyText confidence="0.999723458333333">
The dominant method for supervised Chinese
word segmentation is character-based model
which was first proposed by Xue (2003). This
method treats word segmentation as a tagging
problem, each tag indicates the position of a char-
acter within a word. The most commonly used
tag set is {Single, Begin, Middle, End}. Specifi-
cally, S means the character forms a single word,
B/E means the character is the begining/ending
character of the word, and M means the charac-
ter is in the middle of the word. Existing models
are trained on manually annotated data in a super-
vised way based on discriminative models such as
Conditional Random Fields (Peng et al., 2004;
Tseng et al., 2005). Supervised character-based
methods make full use of character level informa-
tion and thus have been very successful in the last
decade. However, no unsupervised model has uti-
lized character level information in the way as su-
pervised method does.
We can also build a character-based model for
Chinese word segmentation using hidden Markov
model(HMM) as formulated in the following
equation:
</bodyText>
<equation confidence="0.9997715">
PM(W|C) = � |C |Pt(ti|ti−1)Pe(ci|ti) (2)
i=1
</equation>
<bodyText confidence="0.999982166666667">
where C and W have the same meaning as be-
fore. Pt(ti|ti−1) is the transition probability of
tag ti given its former tag ti−1 and Pe(ci|ti) is the
emission probability of character ci given its tag ti.
This model can be easily trained with Maximum
Likelihood Estimation (MLE) on annotated data
or with Expectation Maximization (EM) on raw
texts. But using any of this methods will make it
difficult to combine it with the HDP-based model.
Instead, we propose a Bayesian HMM for unsu-
pervised word segmentation. The Bayesian HMM
model is defined as follows:
</bodyText>
<equation confidence="0.9998585">
ti|ti−1 = t, pt — Mult(pt)
ci|ti = t, et — Mult(et)
pt|θ — Dirichlet(θ)
et|σ — Dirichlet(σ)
</equation>
<bodyText confidence="0.9795568">
where pt and et are transition and emission dis-
tributions, θ and σ are the symmetric parameters
of Dirichlet distributions. Now suppose we have
observed tagged text h, then the conditional prob-
ability PM(wi|wi−1 = l, h) can be obtained:
</bodyText>
<equation confidence="0.982972333333333">
PM(wi|wi−1 = l, h)
= � |wi |Pt(tj|tj−1, h)Pe(cj|tj, h) (3)
j=1
</equation>
<bodyText confidence="0.999969">
where &lt; wi−1, wi &gt; is a word bigram, l is the in-
dex of word wi−1, cj is the jth character in word
</bodyText>
<page confidence="0.993186">
856
</page>
<bodyText confidence="0.996624666666667">
wi and tj is the corresponding tag.Pt(tj|tj−1, h)
and Pe(cj|tj, h) are the posterior probabilities,
they are given as:
</bodyText>
<equation confidence="0.999302">
n&lt;tj−1,tj&gt; + B
Pt(tj|tj−1, h) = (4)
n&lt;tj−1,*&gt; + TB
n&lt;tj,cj&gt; + Q
Pe(cj|tj, h) = (5)
n&lt;tj,*&gt; + V Q
</equation>
<bodyText confidence="0.999930857142857">
where n&lt;tj−1,tj&gt; is the tag bigram count of &lt;
tj−1, tj &gt; in h, n&lt;tj,cj&gt; denotes the number of oc-
currences of tag tj and character cj, and * means
a sum operation. T and V are the size of character
tag set (we follow the commonly used {SBME}
tag set and thus T = 4 in this case) and character
vocabulary.
</bodyText>
<subsectionHeader confidence="0.939605">
3.3 HDP Model
</subsectionHeader>
<bodyText confidence="0.999367833333333">
Goldwater et al. (2009) proposed a nonparametric
Bayesian model for unsupervised word segmenta-
tion which is based on HDP (Teh et al., 2006). In
this model, the conditional probability of the seg-
mentation W given the character string C is de-
fined as:
</bodyText>
<equation confidence="0.996392333333333">
|W|
PD(W|C) = H PD(wi|wi−1) (6)
i=0
</equation>
<bodyText confidence="0.998639166666667">
where wi is the ith word in W. This is actually
a nonparametric bigram language model. This bi-
gram model assumes that each different word has
a different distribution over words following it, but
all these different distributions are linked through
a HDP model:
</bodyText>
<equation confidence="0.999918">
wi|wi−1 = l — Gl
Gl — DP(α1, G0)
G0 — DP(α, H)
</equation>
<bodyText confidence="0.997011">
where DP denotes a Dirichlet process.
Suppose we have observed segmentation re-
sult h, then we can get the posterior probability
</bodyText>
<equation confidence="0.947484333333333">
PD(wi|wi−1 = l, h) by integrating out Gl:
PD(wi|wi−1 = l, h)
n&lt;wi−1,wi&gt; + α1PD(wi|h) (7)
</equation>
<bodyText confidence="0.9990185">
where n&lt;wi−1,wi&gt; denotes the total number of oc-
currences of the bigram &lt; wi−1, wi &gt; in the ob-
servation h. And PD(wi|h) can be got by integrat-
ing out G0:
</bodyText>
<equation confidence="0.989169333333333">
twi + αH(wi)
PD(wi|h) = (8)
t + α
</equation>
<bodyText confidence="0.9996252">
where twi denotes the number of tables associ-
ated with wi in the Chinese Restaurant Franchise
metaphor (Teh et al., 2006), t is the total number
of tables and H(wi) is the base measure of G0. In
fact, H(wi) is the prior distribution over words, so
prior knowledge can be injected in this distribution
to enhance the performance.
In Goldwater et al. (2009)’s work, the base
measure H(wi) are defined as a character unigram
model:
</bodyText>
<equation confidence="0.9932865">
H(wi) = (1 _ ps)|wi|−1ps TT
1j1
</equation>
<bodyText confidence="0.9962572">
where, ps is the probability of generating a word
boundary. P(cij) is the probability of the jth char-
acter cij in word wi, this probability can be esti-
mated from the training data using maximum like-
lihood estimation.
</bodyText>
<subsectionHeader confidence="0.987196">
3.4 Initializing with nVBE
</subsectionHeader>
<bodyText confidence="0.999893956521739">
Among various goodness measure based models,
we choose nVBE (Magistry and Sagot, 2012) to
initialize our Gibbs sampler with its segmentation
results. nVBE achieved a relatively high perfo-
mance over other goodness measure based meth-
ods. And it’s very simple as well as efficient.
Theoretically, the Gibbs sampler may be initial-
ized at random or using any other methods. Initial-
ization does not make a difference since the Gibbs
sampler will eventually converge to the posterior
distribution if it iterates as much as possible. This
is an essential attribute of Gibbs sampling. How-
ever, we believe that initializing the Gibbs sam-
pler with the result of nVBE will benefit us in
two ways. On one hand, in consideration of its
combination of nonparametric Bayesian method
and goodness-based method, it will improve the
overall performance as well as solve more seg-
mentation ambiguities with the help of HDP-based
model. On the other hand, it makes the conver-
gence of Gibbs sampling faster. In practice, ran-
dom initialization often leads to extremely slow
convergence.
</bodyText>
<subsectionHeader confidence="0.835598">
3.5 Inference with Gibbs Sampling
</subsectionHeader>
<bodyText confidence="0.999974">
In our proposed joint model, Gibbs sam-
pling (Casella and George, 1992) can be easily
used to identify the highest probability segmen-
tation from among all possibilities. Following
Goldwater et al. (2009), we can repeatedly sample
from potential word boundaries. Each boundary
</bodyText>
<equation confidence="0.9779915">
n&lt;wi−1,*&gt; + α1
P(cij)
</equation>
<page confidence="0.962297">
857
</page>
<bodyText confidence="0.996827">
variable can only take on two possible values, cor-
responding to a word boundary or not word bound-
ary.
For instance, suppose we have obtained a seg-
mentation result βJci−2ci−1cici+1ci+2Jry, where β
and ry are the words sequences to the left and
right and ci−2ci−1cici+1ci+2 are characters be-
tween them. Now we are sampling at location i
to decide whether there is a word boundary be-
tween ci and ci+1. Denote h1 as the hypothesis
that it forms a word boundary (the correspond-
ing result is βw1w2ry where w1 = ci−2ci−1ci and
w2 = ci+1ci+2), and h2 as the opposite hypoth-
esis (then the corresponding result is βwry where
w = ci−2ci−1cici+1ci+2). The posterior probabil-
ity for these two hypotheses would be:
</bodyText>
<equation confidence="0.999944">
P(h1Jh−) a PD(h1Jh−)PM(h1Jh−) (9)
P(h2Jh−) a PD(h2Jh−)PM(h2Jh−) (10)
</equation>
<bodyText confidence="0.999945125">
where PD(hJh−) and PM(hJh−) are the pos-
terior probabilities in HDP-based model and in
HMM-based model, and h− denotes the current
segmentation results for all observed data except
ci−2ci−1cici+1ci+2. Note that the normalization
term Z(C) can be ignored during inference. The
posterior probabilities for these two hypotheses in
the HDP-based model is given as:
</bodyText>
<equation confidence="0.99979525">
PD(h1Jh−) = PD(w1Jwl, h−)
x PD(w2Jw1, h−)PD(wrJw2, h−) (11)
PD(h2Jh−) = PD(wJwl, h−)
x PD(wrJw,h−) (12)
</equation>
<bodyText confidence="0.996015">
where wl(wr) is the first word to the left (right) of
w. And the posterior probabilities for the Bayesian
HMM model is given as:
</bodyText>
<equation confidence="0.999374">
PM(h1Jh−)
a i+2ri Pt(tjJtj−1, h−)P,(cjJtj, h−) (13)
j=i−2
PM(h2Jh−)
Pt(tjJtj−1, h−)P,(cjJtj, h−) (14)
</equation>
<bodyText confidence="0.9996618">
where Pt(tjJtj−1, h−) and P,(cjJtj, h−) are given
in Equation 4 and 5. The difference is that un-
der hypothesis h1, ci−2ci−1cici+1ci+2 are tagged
as `BMEBE” and under hypothesis h2 as `BM-
MME”.
Once the Gibbs sampler is converged, a natu-
ral way to is to treat the result of last iteration as
the final segmentation result, since each set of as-
signments to the boundary variables uniquely de-
termines a segmentation.
</bodyText>
<sectionHeader confidence="0.999681" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999973833333333">
In this section, we test our joint model on PKU
and MSRA datesets provided by the Second Seg-
mentation Bake-off (SIGHAN 2005) (Emerson,
2005). Most previous works reported their results
on these two datasets, this will make it convenient
to directly compare our joint model with theirs.
</bodyText>
<subsectionHeader confidence="0.996815">
4.1 Setting
</subsectionHeader>
<bodyText confidence="0.99950888">
The second SIGHAN Bakeoff provides several
large-scale labeled data for evaluating the per-
formance of Chinese word segmentation systems.
Two of the four datasets are used in our exper-
iments. Both of the dataset contains only sim-
plified Chinese. Table 1 shows the statistics of
the two selected corpus. For development set, we
randomly select a small subset (about 10%) of
the training data. Specifically, 2000 sentences are
selected for PKU corpus and 8000 sentences for
MSRA corpus. The rest training data plus the test
set is then combined for segmentation but only test
data is used for evaluation. The development set is
used to tune parameters of the HDP-based model
and HMM-based model separately. Since our joint
model does not involve any additional parameters,
we reuse the parameters of the HDP-based model
and HMM-based model in the joint model. Specif-
ically, we set α1 = 1000.0, α = 10.0, ps = 0.5 for
the HDP-based model and set θ = 1.0, σ = 0.01
for the HMM-based model.
For evaluation, we use standard F-Score on
words for all following experiments. F-Score is
the harmonic mean of the word precision and re-
call. Precision is given as:
</bodyText>
<equation confidence="0.892154454545455">
P =
#correct words in result
#total words in result
and recall is given as:
R = #correct words in result #total words in gold corpus
then F-Score is calculated as:
2 x R x F
R + F
a i+2 1i1
j2
F=
</equation>
<page confidence="0.996931">
858
</page>
<table confidence="0.992897333333333">
Corpus TrainingSize (words) TestSize (words)
PKU 1.1M 104K
MSRA 2.37M 107K
</table>
<tableCaption confidence="0.999887">
Table 1: Statistics of training and testing data
</tableCaption>
<bodyText confidence="0.999014333333333">
Huang and Zhao (2007) provided an empirical
method to estimate the consistency between the
four different segmentation standards involved in
the Bakeoff-3. A lowest consistency rate 84.8%
is found among the four standards. Zhao and Kit
(2008) considered this figure as the upper bound
for any unsupervised Chinese word segmentation
systems. We also use it as the topline in our com-
parison.
</bodyText>
<subsectionHeader confidence="0.967331">
4.2 Prior Knowledge Used
</subsectionHeader>
<bodyText confidence="0.99997690625">
When it comes to the evaluation and compari-
son for unsupervised word segmentation systems,
an important issue is what kind of pre-processing
steps and prior knowledge are needed. To be fully
unsupervised, any prior knowledge such as punc-
tuation information, encoding scheme and word
length could not be used in principle. Neverthe-
less, information like punctuation can be easily in-
jected to most existing systems and significantly
enhance the performance. The problem we are
faced with is that we don’t know for sure what
kind of prior information are used in other sys-
tems. One may use a small punctuation set to
segment a long sentence into shorter ones, while
another may write simple regular expressions to
identify dates and numbers. Lot of work we com-
pare to don’t even mention this subject.
Fortunately, we notice that Wang et al. (2011)
provided four kinds of preprocessings (they call
settings). In their settings 1 and 2, punctuation
and other encoding information are not used. In
setting 3, punctuation is used to segment charac-
ter sequences into sentences, and both punctuation
and other encoding information are used in setting
4. Then the results reported in Magistry and Sagot
(2012) relied on setting 3 and setting 4. In order
to make the comparison as fair as possible, we use
setting 3 in our experiment, i.e., only a punctua-
tion set for simplified Chinese is used in all our
experiments. We will compare our experiment re-
sults to previous work on the same setting if they
are provided .
</bodyText>
<subsectionHeader confidence="0.999082">
4.3 Experiment Results
</subsectionHeader>
<bodyText confidence="0.999771">
Table 2 summarizes the F-Scores obtained by dif-
ferent models on PKU and MSRA corpus, as well
as several state-of-the-art systems. Detailed infor-
mation about the presented models are listed as
follows:
</bodyText>
<listItem confidence="0.997026380952381">
• nVBE: the model based on Variation of
Branching Entropy in Magistry and Sagot
(2012). We re-implement their model on set-
ting 31.
• HDP: the HDP-based model proposed by
Goldwater et al. (2009), initialized randomly.
• HDP+HMM: the model combining HDP-
based model and HMM-based model as pro-
posed in Section 3, initialized randomly.
• HDP+nVBE: the HDP-based model, initial-
ized with the results of nVBE model.
• Joint: the “HDP+HMM” model initialized
with nVBE model.
• ESA: the model proposed in Wang et al.
(2011), as mentioned above, the conducted
experiments on four different settings, we re-
port their results on setting 3.
• NPY(2): the 2-gram language model pre-
sented by Mochihashi et al. (2009).
• NPY(3): the 3-gram language model pre-
sented by Mochihashi et al. (2009).
</listItem>
<bodyText confidence="0.998068333333333">
For all of our Gibbs samplers, we run 5 times to
get the averaged F-Scores. We also give the vari-
ance of the F-Scores in Table 2. For each run, we
find that random initialization takes around 1,000
iterations to converge, while initialing with nVBE
only takes as few as 10 iterations. This makes
</bodyText>
<footnote confidence="0.861578">
1The results we got with our implementation is slightly
lower than what was reported in Magistry and Sagot (2012).
According to Pei et al. (2013), they had contacted the authors
and confirmed that the higher results was due to a bug in code.
So we report the results with our bug free implementation as
Pei et al. (2013) did. Our reported results are identical to
those of Pei et al. (2013)
</footnote>
<page confidence="0.994337">
859
</page>
<table confidence="0.999876545454546">
System PKU MSRA
R P F R P F
nVBE 78.3 77.5 77.9 79.1 77.3 78.2
HDP 69.0 68.4 68.7(0.012) 70.4 69.4 69.9(0.020)
HDP+HMM 77.5 73.2 75.3(0.005) 79.9 73.0 76.3(0.013)
HDP+nVBE 80.7 77.9 79.3(0.012) 81.8 77.3 79.5(0.005)
Joint 83.1 79.2 81.1(0.002) 84.2 79.3 81.7(0.005)
ESA N/A N/A 77.4 N/A N/A 78.4
NPY(2) N/A N/A N/A N/A N/A 80.2
NPY(3) N/A N/A N/A N/A N/A 80.7
Topline N/A N/A 84.8 N/A N/A 84.8
</table>
<tableCaption confidence="0.992963">
Table 2: Experiment results and comparison to state-of-the-art systems. The figures in parentheses denote
</tableCaption>
<bodyText confidence="0.993071847457627">
the variance the of F-Scores.
our joint model very efficient and possible to work
in practical applications as well. At last, a single
sample (the last one) is used for evaluation.
From Table 2, we can see that the joint
model (Joint) outperforms all the presented sys-
tems in F-Score on all testing corpora. Specifi-
cally, comparing “HDP+HMM” with “HDP”, the
former model increases the overall F-Score from
68.7% to 75.3% (+6.6%) in PKU corpora and
from 69.9% to 76.3% (+6.4%) in MSRA corpora,
which proves that the character information in
the HMM-based model can actually enhance the
performance of the HDP-based model. Compar-
ing “HDP+nVBE” with “HDP”, the former model
also increases the overall F-Score by 10.6%/9.6%
in PKU/MSRA corpora, which demonstrates that
initializing the HDP-based model with nVBE will
improve the performance by a large margin. Fi-
nally, the joint model “Joint” take advantage from
both from the character-based HMM model and
the nVBE model, it achieves a F-Score of 81.1%
on PKU and 81.7% on MSRA. This result outper-
forms all its component baselines such as “HDP”,
“HDP+HMM” and “HDP+nVBE”.
Our joint model also shows competitive advan-
tages over several state-of-the-art systems. Com-
pared with nVBE,the F-Score increases by 3.2%
on PKU corpora and by 3.5% on MSRA cor-
pora. Compared with ESA, the F-Score increases
by 3.7%/3.3% in PKU/MSRA corpora. Lastly,
compared to the nonparametric Bayesian models
(NPY(n)), our joint model still increases the F-
Score by 1.5% (NPY(2)) and 1.0% (NPY(3)) on
MSRA corpora. Moreover, compared with the
empirical topline figure 84.8%, our joint model
achieves a pretty close F-Score. The differences
are 3.7% on PKU corpora and 3.1% on MSRA
corpora.
An phenomenon we should pay attention to is
the poor performance of the HMM-based model.
With our implementation of the Bayesian HMM,
we achieves a 34.3% F-Score on PKU corpora and
a 34.9% F-Score on MSRA corpora, just slightly
better than random segmentation. The result show
that the hidden Markov Model alone is not suit-
able for character-based Chinese word segmenta-
tion problem. However, it still substantially con-
tributes to the joint model.
We find that the variance of the results are rather
small, this shows the stability of our Gibbs sam-
plers. From the segmentation results generated
by the joint model, we also found that quite a
large amount of errors it made are related to dates,
numbers (both Chinese and English) and English
words. This problem can be easily addressed dur-
ing preprocessing by considering encoding infor-
mation as previous work, and we believe this will
bring us much better performance.
</bodyText>
<subsectionHeader confidence="0.999277">
4.4 Disambiguation Ability
</subsectionHeader>
<bodyText confidence="0.999959583333334">
Previous unsupervised work usually evaluated
their models using F-score, regardless of goodness
measure based model or nonparametric Bayesian
model. However, segmentation ambiguity is
a very important factor influencing accuracy of
Chinese word segmentation systems (Huang and
Zhao, 2007). We believe that the disambigua-
tion ability of the models should also be consid-
ered when evaluating different types of unsuper-
vised segmentation systems, since different type
of models shows different disambiguation ability.
We will compare the disambiguation ability of dif-
</bodyText>
<page confidence="0.988587">
860
</page>
<bodyText confidence="0.961372333333333">
ferent systems in this section.
In general, there are mainly two kinds of ambi-
guity in Chinese word segmentation problem:
</bodyText>
<listItem confidence="0.999182285714286">
• Combinational Ambiguity: Given charac-
ter strings “A” and “B”, if “A”, “B”, “AB”
are all in the vocabulary, and “AB” or “A-B”
(here “-” denotes a space) occurred in the real
text,then “AB” can be called a combinational
ambiguous string.
• Overlapping Ambiguity: Given character
</listItem>
<bodyText confidence="0.976099744186047">
strings “A”, “J” and “B”, if “A”, “B”, “AJ”
and “JB” are all in the vocabulary, and “A-
JB” or “AJ-B” occurred in the real text, then
“AJB” can be called an overlapping ambigu-
ous string.
We count the total number of mistakes differ-
ent systems made at ambiguous strings (the vo-
cabulary is obtained from the gold standard an-
swer of testing set). As we have mentioned in
Section 2, goodness measure based methods such
as nVBE do not have any disambiguation ability
in theory. Our observation is identical to this ar-
gument. We find that nVBE always segments am-
biguous strings into the same result. Take a combi-
national string “•k” as an example, “• (just)”,
“k (have)” and “•k (only)” are all in the vo-
cabulary. In the PKU test set, this string occurs
14 times as “•-k (just have)” and 18 times as
“•k (only)”, 32 times in total. nVBE segments
all the 32 strings into “•k (only)” (i.e. 18 of
them are correct), while the joint model segments
it 22 times as “•k (only)” and 10 times as “•-
k (just have)” according to its context, and 24 of
them are correct.
Table 3 and 4 show the statistics of combi-
national ambiguity and overlapping ambiguity re-
spectively. The numbers in parentheses denote the
total number of ambiguous strings. From these
tables, we can see that HDP+nVBE makes less
mistakes than nVBE in most circumstances, ex-
cept that it solves less combinational ambigui-
ties on MSRA corpora. But our proposed joint
model solves the most combinational and over-
lapping ambiguities, on both PKU and MSRA
corpora. Specifically, compared to nVBE, the
joint model correctly solves 171/871 more com-
binational ambiguities on PKU/MSRA corpora,
which is a 0.6%/13.8% relative error reduction.
It also solves 28/45 more overlapping ambiguities
on PKU/MSRA corpora, which is a 11.5%/23.4%
relative error reduction. This indicates that the
joint model has a stronger ability of disambigua-
tion over the compared systems.
</bodyText>
<table confidence="0.97768275">
System PKU(35371) MSRA(38506)
nVBE 8087 7236
HDP+nVBE 7970 7500
Joint 7916 6305
</table>
<tableCaption confidence="0.996755">
Table 3: Statistics of combinational ambiguity.
</tableCaption>
<bodyText confidence="0.9957434">
This table shows the total number of mistakes
made by different systems at combinational am-
biguous strings. The numbers in parentheses de-
note the total number of combinational ambiguous
strings.
</bodyText>
<table confidence="0.9936115">
System PKU(603) MSRA(467)
nVBE 244 192
HDP+nVBE 239 164
Joint 216 157
</table>
<tableCaption confidence="0.979666">
Table 4: Statistics of overlapping ambiguity. This
</tableCaption>
<bodyText confidence="0.9445885">
table shows the total number of mistakes made
by different systems at overlapping ambiguous
strings. The numbers in parentheses denote the to-
tal number of overlapping ambiguous strings.
</bodyText>
<subsectionHeader confidence="0.98566">
4.5 Statistical Significance Test
</subsectionHeader>
<bodyText confidence="0.999981">
The main results presented in Table 2 has shown
that our proposed joint model outperforms the
two baselines as well as state-of-the-art systems.
But it is also important to know if the improve-
ment is statistically significant over these sys-
tems. So we conduct statistical significance tests
of F-scores among these various models. Follow-
ing Wang et al. (2010), we use the bootstrapping
method (Zhang et al., 2004).
Here is how it works: suppose we have a testing
set T0 to test several word segmentation systems,
there are N testing examples (sentences or line of
characters) in T0. We create a new testing set T1
with N examples by sampling with replacement
from T0, then repeat these process M − 1 times.
And we will have a total M +1 testing sets. In our
test procedures, M is set to 2000.
Since we just implement our joint model and
its component models, we can not generate paired
samples for other models (i.e. ESA and NPY(n)).
Instead, we follow Wang et al. (2010)’s method
and first calculate the 95% confidence interval for
</bodyText>
<page confidence="0.992978">
861
</page>
<bodyText confidence="0.999878588235294">
our proposed model. Then other systems can be
compared with the joint model in this way: if the
F-score of system B doesn’t fall into the 95% con-
fidence interval of system A, they are considered
as statistically significantly different from each
other.
For all significant tests, we measure the 95%
confidence interval for the difference between
two models. First, the test results show that
“HDP+nVBE” and “HDP+HMM” are both sig-
nificantly better than “HDP”. Second, the
“Joint” model significantly outperforms all its
component models, including “HDP”, “nVBE”,
“HDP+nVBE” and “HDP+HMM”. Finally, the
comparison also shows that the joint model signif-
icantly outperforms state-of-the-art systems like
ESA and NPY(n).
</bodyText>
<sectionHeader confidence="0.999642" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999982173913044">
In this paper, we proposed a joint model for un-
supervised Chinese word segmentation. Our joint
model is a combination of the HDP-based model,
which is a word-based model, and HMM-based
model, which is a character-based model. The
way we combined these two component base-
lines makes it natural and simple to inference with
Gibbs sampling. Then the joint model take ad-
vantage of a goodness-based method (nVBE) by
using it to initialize the sampler. Experiment re-
sults conducted on PKU and MSRA datasets pro-
vided by the second SIGHAN Bakeoff show that
the proposed joint model not only outperforms the
baseline systems but also achieves better perfor-
mance (F-Score) over several state-of-the-art sys-
tems. Significance tests showed that the improve-
ment is statistically significant. Analysis also in-
dicates that the joint model has a stronger abil-
ity to solve ambiguities in Chinese word segmen-
tation. In summary, the joint model we pro-
posed combines the strengths of character-based
model, nonparametric Bayesian language model
and goodness-based model.
</bodyText>
<sectionHeader confidence="0.999394" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.8993135">
The contact author of this paper, according to
the meaning given to this role by Key Labora-
</bodyText>
<reference confidence="0.970098285714286">
tory of Computational Linguistics, Ministry of Ed-
ucation, School of Electronics Engineering and
Computer Science, Peking University, is Baobao
Chang. And this work is supported by National
Natural Science Foundation of China under Grant
No. 61273318 and National Key Basic Research
Program of China 2014CB340504.
</reference>
<sectionHeader confidence="0.888857" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999922367346939">
George Casella, Edward I. George. 1992. Explain-
ing the Gibbs sampler. The American Statistician,
46(3): 167-174.
Thomas Emerson. 2005. The second international chi-
nese word segmentation bakeoff. Proceedings of
the Fourth SIGHAN Workshop on Chinese Language
Processing, 133. MLA.
Haodi Feng, Kang Chen, Xiaotie Deng, et al. 2004.
Accessor variety criteria for Chinese word extraction
Computational Linguistics, 30(1): 75-93.
Sharon Goldwater, Thomas L. Griffiths, Mark Johnson.
2009. A Bayesian framework for word segmenta-
tion: Exploring the effects of context. Cognition
112(1): 21-54.
Geoffrey E. Hinton. 1999. Products of experts. Arti-
ficial Neural Networks. Ninth International Confer-
ence on Vol. 1.
Changning Huang, Hai Zhao. 2007. Chinese word
segmentation: A decade review. Journal of Chinese
Information Processing, 21(3): 8-20.
Zhihui Jin, Kumiko Tanaka-Ishii. 2006. Unsupervised
segmentation of Chinese text by use of branching
entropy. Proceedings of the COLING/ACL on Main
conference poster sessions, page 428-435.
Chunyu Kit, Yorick Wilks. 1999. Unsupervised
learning of word boundary with description length
gain. Proceedings of the CoNLL99 ACL Workshop.
Bergen, Norway: Association for Computational
Linguis-tics, page 1-6.
Pierre Magistry, Benoit Sagot. 2012. Unsuper-
vized word segmentation: the case for mandarin
chinese. Proceedings of the 50th Annual Meeting
of the Association for Computational Linguistics:
Short Papers-Volume 2. Association for Computa-
tional Linguistics, page 383-387.
Daichi Mochihashi, Takeshi Yamada, Naonori Ueda.
2009. Bayesian unsupervised word segmentation
with nested Pitman-Yor language modeling. Pro-
ceedings of the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International Joint
Conference on Natural Language Processing of the
AFNLP: Volume 1-Volume 1. Association for Com-
putational Linguistics, page 100-108.
Wenzhe Pei, Dongxu Han, Baobao Chang. 2013. A
Refined HDP-Based Model for Unsupervised Chi-
nese Word Segmentation. Chinese Computational
Linguistics and Natural Language Processing Based
on Naturally Annotated Big Data. Springer Berlin
Heidelberg, page 44-51.
</reference>
<page confidence="0.977165">
862
</page>
<reference confidence="0.9994451">
Yee Whye Teh, Michael I. Jordan, Matthew J. Beal, et
al. 2006. Sharing Clusters among Related Groups:
Hierarchical Dirichlet Processes. NIPS.
Fuchun Peng, Fangfang Feng, Andrew McCallum.
2004. Chinese segmentation and new word detec-
tion using conditional random fields. Proceedings
of COLING, page 562-568.
Huihsin Tseng, Pichuan Chang, Galen Andrew, et al.
2005. A conditional random field word segmenter
for sighan bakeoff 2005. Proceedings of the Fourth
SIGHAN Workshop on Chinese Language Process-
ing, Vol. 171.
Kun Wang, Chengqing Zong, Keh-Yih Su. 2010. A
character-based joint model for Chinese word seg-
mentation. Proceedings of the 23rd International
Conference on Computational Linguistics. Associa-
tion for Computational Linguistics, page 1173-1181.
Hanshi Wang, Jian Zhu, Shiping Tang, et al. 2011. A
new unsupervised approach to word segmentation.
Computational Linguistics, 37(3): 421-454.
Nianwen Xue. 2003. Chinese word segmentation as
character tagging. Computational Linguistics and
Chinese Language Processing, 8(1): 29-48.
Hai Zhao, Chunyu Kit. 2008. An Empirical Compar-
ison of Goodness Measures for Unsupervised Chi-
nese Word Segmentation with a Unified Framework.
IJCNLP, page 6-16.
Ying Zhang, Stephan Vogel, Alex Waibel. 2004. In-
terpreting BLEU/NIST scores: How much improve-
ment do we need to have a better system? LREC.
</reference>
<page confidence="0.999177">
863
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.227201">
<title confidence="0.999904">A Joint Model for Unsupervised Chinese Word Segmentation</title>
<author confidence="0.5770815">Miaohong Chen Baobao Chang Wenzhe Key Laboratory of Computational Linguistics</author>
<author confidence="0.5770815">Ministry of</author>
<affiliation confidence="0.985262">School of Electronics Engineering and Computer Science, Peking</affiliation>
<address confidence="0.913976">Beijing, P.R.China,</address>
<abstract confidence="0.997850785714286">In this paper, we propose a joint model for unsupervised Chinese word segmentation (CWS). Inspired by the “products of experts” idea, our joint model firstly combines two generative models, which are word-based hierarchical Dirichlet process model and character-based hidden Markov model, by simply multiplying their probabilities together. Gibbs sampling is used for model inference. In order to further combine the strength of goodness-based model, we then integrated nVBE into our joint model by using it to initializing the Gibbs sampler. We conduct our experiments on PKU and MSRA datasets provided by the second SIGHAN bakeoff. Test results on these two datasets show that the joint model achieves much better results than all of its component models. Statistical significance tests also show that it is significantly better than stateof-the-art systems, achieving the highest F-scores. Finally, analysis indicates that compared with nVBE and HDP, the joint model has a stronger ability to solve both combinational and overlapping ambiguities in Chinese word segmentation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<title>Baobao Chang. And this work is supported by</title>
<booktitle>National Natural Science Foundation of China under Grant No. 61273318 and National Key Basic Research Program of China 2014CB340504.</booktitle>
<institution>tory of Computational Linguistics, Ministry of Education, School of Electronics Engineering and Computer Science, Peking University, is</institution>
<marker></marker>
<rawString>tory of Computational Linguistics, Ministry of Education, School of Electronics Engineering and Computer Science, Peking University, is Baobao Chang. And this work is supported by National Natural Science Foundation of China under Grant No. 61273318 and National Key Basic Research Program of China 2014CB340504.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Casella</author>
<author>Edward I George</author>
</authors>
<title>Explaining the Gibbs sampler.</title>
<date>1992</date>
<journal>The American Statistician,</journal>
<volume>46</volume>
<issue>3</issue>
<pages>167--174</pages>
<contexts>
<context position="15902" citStr="Casella and George, 1992" startWordPosition="2562" endWordPosition="2565">tial attribute of Gibbs sampling. However, we believe that initializing the Gibbs sampler with the result of nVBE will benefit us in two ways. On one hand, in consideration of its combination of nonparametric Bayesian method and goodness-based method, it will improve the overall performance as well as solve more segmentation ambiguities with the help of HDP-based model. On the other hand, it makes the convergence of Gibbs sampling faster. In practice, random initialization often leads to extremely slow convergence. 3.5 Inference with Gibbs Sampling In our proposed joint model, Gibbs sampling (Casella and George, 1992) can be easily used to identify the highest probability segmentation from among all possibilities. Following Goldwater et al. (2009), we can repeatedly sample from potential word boundaries. Each boundary n&lt;wi−1,*&gt; + α1 P(cij) 857 variable can only take on two possible values, corresponding to a word boundary or not word boundary. For instance, suppose we have obtained a segmentation result βJci−2ci−1cici+1ci+2Jry, where β and ry are the words sequences to the left and right and ci−2ci−1cici+1ci+2 are characters between them. Now we are sampling at location i to decide whether there is a word </context>
</contexts>
<marker>Casella, George, 1992</marker>
<rawString>George Casella, Edward I. George. 1992. Explaining the Gibbs sampler. The American Statistician, 46(3): 167-174.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Emerson</author>
</authors>
<title>The second international chinese word segmentation bakeoff.</title>
<date>2005</date>
<booktitle>Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing,</booktitle>
<pages>133</pages>
<publisher>MLA.</publisher>
<contexts>
<context position="18165" citStr="Emerson, 2005" startWordPosition="2934" endWordPosition="2935">−2 PM(h2Jh−) Pt(tjJtj−1, h−)P,(cjJtj, h−) (14) where Pt(tjJtj−1, h−) and P,(cjJtj, h−) are given in Equation 4 and 5. The difference is that under hypothesis h1, ci−2ci−1cici+1ci+2 are tagged as `BMEBE” and under hypothesis h2 as `BMMME”. Once the Gibbs sampler is converged, a natural way to is to treat the result of last iteration as the final segmentation result, since each set of assignments to the boundary variables uniquely determines a segmentation. 4 Experiments In this section, we test our joint model on PKU and MSRA datesets provided by the Second Segmentation Bake-off (SIGHAN 2005) (Emerson, 2005). Most previous works reported their results on these two datasets, this will make it convenient to directly compare our joint model with theirs. 4.1 Setting The second SIGHAN Bakeoff provides several large-scale labeled data for evaluating the performance of Chinese word segmentation systems. Two of the four datasets are used in our experiments. Both of the dataset contains only simplified Chinese. Table 1 shows the statistics of the two selected corpus. For development set, we randomly select a small subset (about 10%) of the training data. Specifically, 2000 sentences are selected for PKU c</context>
</contexts>
<marker>Emerson, 2005</marker>
<rawString>Thomas Emerson. 2005. The second international chinese word segmentation bakeoff. Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing, 133. MLA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haodi Feng</author>
<author>Kang Chen</author>
<author>Xiaotie Deng</author>
</authors>
<title>Accessor variety criteria for Chinese word extraction</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>1</issue>
<pages>75--93</pages>
<contexts>
<context position="2356" citStr="Feng et al., 2004" startWordPosition="341" endWordPosition="344"> been proposed for Chinese word segmentation. While successful, they require manually labeled resources and often suffer from issues like poor domain adaptability. Thus, unsupervised word segmentation methods are still attractive to researchers due to its independence on domain and manually labeled corpora. Previous unsupervised approaches to word segmentation can be roughly classified into two types. The first type uses carefully designed goodness measure to identify word candidates. Popular goodness measures include description length gain (DLG) (Kit and Wilks, 1999), accessor variety (AV) (Feng et al., 2004), boundary entropy (BE) (Jin and Tanaka-Ishii, 2006) and normalized variation of branching entropy (nVBE) (Magistry and Sagot, 2012) etc. Goodness measure based model is not segmentation model in a very strict meaning and is actually strong in generating word list without supervision. It inherently lacks capability to deal with ambiguous string, which is one of main sources of segmentation errors and has been extensively explored in supervised Chinese word segmentation. The second type focuses on designing sophisticated statistical model, usually nonparametric Bayesian models, to find the segm</context>
<context position="6218" citStr="Feng et al. (2004)" startWordPosition="918" endWordPosition="921"> 5 concludes the paper. 2 Related Work Unsupervised Chinese word segmentation has been explored in a number of previous works and by various methods. Most of these methods can be divided into two categories: goodness measure based methods and nonparametric Bayesian methods. There have been a plenty of work that is based on a specific goodness measure. Zhao and Kit (2008) compared several popular unsupervised models within a unified framework. They tried various types of goodness measures, such as Description Length Gain (DLG) proposed by Kit and Wilks (1999), Accessor Variety (AV) proposed by Feng et al. (2004) and Boundary Entropy (Jin and Tanaka-Ishii, 2006). A notable goodness-based method is ESA: “Evaluation, Selection, Adjustment”, which is proposed by Wang et al. (2011) for unsupervised Mandarin Chinese word segmentation. ESA is an iterative model based on a new goodness algorithm that adopts a local maximum strategy and avoids threshold setting. One disadvantage of ESA is that it needs to iterate the process several times on the corpus to get good performance. Another disadvantage is the requirement for a manually segmented training corpus to find best value for parameters (they called it pro</context>
</contexts>
<marker>Feng, Chen, Deng, 2004</marker>
<rawString>Haodi Feng, Kang Chen, Xiaotie Deng, et al. 2004. Accessor variety criteria for Chinese word extraction Computational Linguistics, 30(1): 75-93.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>Thomas L Griffiths</author>
<author>Mark Johnson</author>
</authors>
<title>A Bayesian framework for word segmentation: Exploring the effects of context.</title>
<date>2009</date>
<journal>Cognition</journal>
<volume>112</volume>
<issue>1</issue>
<pages>21--54</pages>
<contexts>
<context position="3144" citStr="Goldwater et al., 2009" startWordPosition="455" endWordPosition="458">l is not segmentation model in a very strict meaning and is actually strong in generating word list without supervision. It inherently lacks capability to deal with ambiguous string, which is one of main sources of segmentation errors and has been extensively explored in supervised Chinese word segmentation. The second type focuses on designing sophisticated statistical model, usually nonparametric Bayesian models, to find the segmentation with highest posterior probability, given the observed character sequences. Typical statistical models includes Hierarchical Dirichlet process (HDP) model (Goldwater et al., 2009), Nested PitmanYor process (NPY) model (Mochihashi et al., 2009) etc, which are actually nonparametric language models and therefor can be categorized as word-based model. Word-based model makes decision on wordhood of a candidate character sequence mainly based on information outside the sequence, namely, the wordhood of character sequences being adjacent to the concerned sequence. Inspired by the success of character-based model in supervised word segmentation, we propose a Bayesian HMM model for unsupervised Chinese word segmentation. With the Bayesian HMM model, we formulate the unsupervis</context>
<context position="7251" citStr="Goldwater et al. (2009)" startWordPosition="1078" endWordPosition="1081">ss several times on the corpus to get good performance. Another disadvantage is the requirement for a manually segmented training corpus to find best value for parameters (they called it proper exponent). Another notable work is nVBE: Magistry and Sagot (2012) proposed a model based on the Variation of Branching Entropy. By adding normalization and viterbi decoding, they improve performance over Jin and Tanaka-Ishii (2006) and remove most of the parameters and thresholds from the model. Nonparametric Bayesian models also achieved state-of-the-art performance in unsupervised word segmentation. Goldwater et al. (2009) introduced a unigram and a bigram model for unsupervised word segmentation, which are based on Dirichlet process and hierarchical Dirichlet process (Teh et al., 2006) respectively. The main drawback is that it needs almost 20,000 iterations before the Gibbs sampler converges. Mochihashi et al. (2009) extended this method by introducing a nested character model and an efficient blocked Gibbs sampler. Their method is based on what they called nested Pitman-Yor language model. One disadvantage of goodness measure based methods is that they do not have any disambiguation ability in theory in spit</context>
<context position="13054" citStr="Goldwater et al. (2009)" startWordPosition="2069" endWordPosition="2072"> l is the index of word wi−1, cj is the jth character in word 856 wi and tj is the corresponding tag.Pt(tj|tj−1, h) and Pe(cj|tj, h) are the posterior probabilities, they are given as: n&lt;tj−1,tj&gt; + B Pt(tj|tj−1, h) = (4) n&lt;tj−1,*&gt; + TB n&lt;tj,cj&gt; + Q Pe(cj|tj, h) = (5) n&lt;tj,*&gt; + V Q where n&lt;tj−1,tj&gt; is the tag bigram count of &lt; tj−1, tj &gt; in h, n&lt;tj,cj&gt; denotes the number of occurrences of tag tj and character cj, and * means a sum operation. T and V are the size of character tag set (we follow the commonly used {SBME} tag set and thus T = 4 in this case) and character vocabulary. 3.3 HDP Model Goldwater et al. (2009) proposed a nonparametric Bayesian model for unsupervised word segmentation which is based on HDP (Teh et al., 2006). In this model, the conditional probability of the segmentation W given the character string C is defined as: |W| PD(W|C) = H PD(wi|wi−1) (6) i=0 where wi is the ith word in W. This is actually a nonparametric bigram language model. This bigram model assumes that each different word has a different distribution over words following it, but all these different distributions are linked through a HDP model: wi|wi−1 = l — Gl Gl — DP(α1, G0) G0 — DP(α, H) where DP denotes a Dirichlet</context>
<context position="14385" citStr="Goldwater et al. (2009)" startWordPosition="2312" endWordPosition="2315">−1 = l, h) by integrating out Gl: PD(wi|wi−1 = l, h) n&lt;wi−1,wi&gt; + α1PD(wi|h) (7) where n&lt;wi−1,wi&gt; denotes the total number of occurrences of the bigram &lt; wi−1, wi &gt; in the observation h. And PD(wi|h) can be got by integrating out G0: twi + αH(wi) PD(wi|h) = (8) t + α where twi denotes the number of tables associated with wi in the Chinese Restaurant Franchise metaphor (Teh et al., 2006), t is the total number of tables and H(wi) is the base measure of G0. In fact, H(wi) is the prior distribution over words, so prior knowledge can be injected in this distribution to enhance the performance. In Goldwater et al. (2009)’s work, the base measure H(wi) are defined as a character unigram model: H(wi) = (1 _ ps)|wi|−1ps TT 1j1 where, ps is the probability of generating a word boundary. P(cij) is the probability of the jth character cij in word wi, this probability can be estimated from the training data using maximum likelihood estimation. 3.4 Initializing with nVBE Among various goodness measure based models, we choose nVBE (Magistry and Sagot, 2012) to initialize our Gibbs sampler with its segmentation results. nVBE achieved a relatively high perfomance over other goodness measure based methods. And it’s very </context>
<context position="16034" citStr="Goldwater et al. (2009)" startWordPosition="2582" endWordPosition="2585">wo ways. On one hand, in consideration of its combination of nonparametric Bayesian method and goodness-based method, it will improve the overall performance as well as solve more segmentation ambiguities with the help of HDP-based model. On the other hand, it makes the convergence of Gibbs sampling faster. In practice, random initialization often leads to extremely slow convergence. 3.5 Inference with Gibbs Sampling In our proposed joint model, Gibbs sampling (Casella and George, 1992) can be easily used to identify the highest probability segmentation from among all possibilities. Following Goldwater et al. (2009), we can repeatedly sample from potential word boundaries. Each boundary n&lt;wi−1,*&gt; + α1 P(cij) 857 variable can only take on two possible values, corresponding to a word boundary or not word boundary. For instance, suppose we have obtained a segmentation result βJci−2ci−1cici+1ci+2Jry, where β and ry are the words sequences to the left and right and ci−2ci−1cici+1ci+2 are characters between them. Now we are sampling at location i to decide whether there is a word boundary between ci and ci+1. Denote h1 as the hypothesis that it forms a word boundary (the corresponding result is βw1w2ry where w</context>
<context position="22137" citStr="Goldwater et al. (2009)" startWordPosition="3608" endWordPosition="3611">g 3 in our experiment, i.e., only a punctuation set for simplified Chinese is used in all our experiments. We will compare our experiment results to previous work on the same setting if they are provided . 4.3 Experiment Results Table 2 summarizes the F-Scores obtained by different models on PKU and MSRA corpus, as well as several state-of-the-art systems. Detailed information about the presented models are listed as follows: • nVBE: the model based on Variation of Branching Entropy in Magistry and Sagot (2012). We re-implement their model on setting 31. • HDP: the HDP-based model proposed by Goldwater et al. (2009), initialized randomly. • HDP+HMM: the model combining HDPbased model and HMM-based model as proposed in Section 3, initialized randomly. • HDP+nVBE: the HDP-based model, initialized with the results of nVBE model. • Joint: the “HDP+HMM” model initialized with nVBE model. • ESA: the model proposed in Wang et al. (2011), as mentioned above, the conducted experiments on four different settings, we report their results on setting 3. • NPY(2): the 2-gram language model presented by Mochihashi et al. (2009). • NPY(3): the 3-gram language model presented by Mochihashi et al. (2009). For all of our G</context>
</contexts>
<marker>Goldwater, Griffiths, Johnson, 2009</marker>
<rawString>Sharon Goldwater, Thomas L. Griffiths, Mark Johnson. 2009. A Bayesian framework for word segmentation: Exploring the effects of context. Cognition 112(1): 21-54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey E Hinton</author>
</authors>
<title>Products of experts. Artificial Neural Networks.</title>
<date>1999</date>
<booktitle>Ninth International Conference on</booktitle>
<volume>1</volume>
<contexts>
<context position="10007" citStr="Hinton (1999)" startWordPosition="1535" endWordPosition="1536">ies together and then renormalizing it. Let C = c1c2 · · · c|C |be a string of characters and W = w1w2 · · · w|W |is the corresponding segmented words sequence. Then the conditional probability of the segmentation W given the character string C in our joint model is defined as: 1 PJ(W|C) = Z(C)PD(W|C)PM(W|C) (1) where PD(W |C) is the probability from the HDP model as given in Equation 6 and PM(W |C) is the probability given by the Bayesian HMM model as given in Equation 2. Z(C) is a normalization term to make sure that PJ(W|C) is a probability distribution. The combining method is inspired by Hinton (1999), which proved that it is possible to combine many individual expert models by multiplying the probabilities and then renormalizing it. They called it “product of experts”. We can see that combining models in this way does not involve any extra parameters and Gibbs sampling can be easily used for model inference. 3.2 Bayesian HMM The dominant method for supervised Chinese word segmentation is character-based model which was first proposed by Xue (2003). This method treats word segmentation as a tagging problem, each tag indicates the position of a character within a word. The most commonly use</context>
</contexts>
<marker>Hinton, 1999</marker>
<rawString>Geoffrey E. Hinton. 1999. Products of experts. Artificial Neural Networks. Ninth International Conference on Vol. 1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Changning Huang</author>
<author>Hai Zhao</author>
</authors>
<title>Chinese word segmentation: A decade review.</title>
<date>2007</date>
<journal>Journal of Chinese Information Processing,</journal>
<volume>21</volume>
<issue>3</issue>
<pages>8--20</pages>
<contexts>
<context position="19819" citStr="Huang and Zhao (2007)" startWordPosition="3222" endWordPosition="3225">et α1 = 1000.0, α = 10.0, ps = 0.5 for the HDP-based model and set θ = 1.0, σ = 0.01 for the HMM-based model. For evaluation, we use standard F-Score on words for all following experiments. F-Score is the harmonic mean of the word precision and recall. Precision is given as: P = #correct words in result #total words in result and recall is given as: R = #correct words in result #total words in gold corpus then F-Score is calculated as: 2 x R x F R + F a i+2 1i1 j2 F= 858 Corpus TrainingSize (words) TestSize (words) PKU 1.1M 104K MSRA 2.37M 107K Table 1: Statistics of training and testing data Huang and Zhao (2007) provided an empirical method to estimate the consistency between the four different segmentation standards involved in the Bakeoff-3. A lowest consistency rate 84.8% is found among the four standards. Zhao and Kit (2008) considered this figure as the upper bound for any unsupervised Chinese word segmentation systems. We also use it as the topline in our comparison. 4.2 Prior Knowledge Used When it comes to the evaluation and comparison for unsupervised word segmentation systems, an important issue is what kind of pre-processing steps and prior knowledge are needed. To be fully unsupervised, a</context>
<context position="26856" citStr="Huang and Zhao, 2007" startWordPosition="4388" endWordPosition="4391"> we also found that quite a large amount of errors it made are related to dates, numbers (both Chinese and English) and English words. This problem can be easily addressed during preprocessing by considering encoding information as previous work, and we believe this will bring us much better performance. 4.4 Disambiguation Ability Previous unsupervised work usually evaluated their models using F-score, regardless of goodness measure based model or nonparametric Bayesian model. However, segmentation ambiguity is a very important factor influencing accuracy of Chinese word segmentation systems (Huang and Zhao, 2007). We believe that the disambiguation ability of the models should also be considered when evaluating different types of unsupervised segmentation systems, since different type of models shows different disambiguation ability. We will compare the disambiguation ability of dif860 ferent systems in this section. In general, there are mainly two kinds of ambiguity in Chinese word segmentation problem: • Combinational Ambiguity: Given character strings “A” and “B”, if “A”, “B”, “AB” are all in the vocabulary, and “AB” or “A-B” (here “-” denotes a space) occurred in the real text,then “AB” can be ca</context>
</contexts>
<marker>Huang, Zhao, 2007</marker>
<rawString>Changning Huang, Hai Zhao. 2007. Chinese word segmentation: A decade review. Journal of Chinese Information Processing, 21(3): 8-20.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhihui Jin</author>
<author>Kumiko Tanaka-Ishii</author>
</authors>
<title>Unsupervised segmentation of Chinese text by use of branching entropy.</title>
<date>2006</date>
<booktitle>Proceedings of the COLING/ACL on Main conference poster sessions,</booktitle>
<pages>428--435</pages>
<contexts>
<context position="2408" citStr="Jin and Tanaka-Ishii, 2006" startWordPosition="348" endWordPosition="351">n. While successful, they require manually labeled resources and often suffer from issues like poor domain adaptability. Thus, unsupervised word segmentation methods are still attractive to researchers due to its independence on domain and manually labeled corpora. Previous unsupervised approaches to word segmentation can be roughly classified into two types. The first type uses carefully designed goodness measure to identify word candidates. Popular goodness measures include description length gain (DLG) (Kit and Wilks, 1999), accessor variety (AV) (Feng et al., 2004), boundary entropy (BE) (Jin and Tanaka-Ishii, 2006) and normalized variation of branching entropy (nVBE) (Magistry and Sagot, 2012) etc. Goodness measure based model is not segmentation model in a very strict meaning and is actually strong in generating word list without supervision. It inherently lacks capability to deal with ambiguous string, which is one of main sources of segmentation errors and has been extensively explored in supervised Chinese word segmentation. The second type focuses on designing sophisticated statistical model, usually nonparametric Bayesian models, to find the segmentation with highest posterior probability, given t</context>
<context position="6268" citStr="Jin and Tanaka-Ishii, 2006" startWordPosition="925" endWordPosition="928">supervised Chinese word segmentation has been explored in a number of previous works and by various methods. Most of these methods can be divided into two categories: goodness measure based methods and nonparametric Bayesian methods. There have been a plenty of work that is based on a specific goodness measure. Zhao and Kit (2008) compared several popular unsupervised models within a unified framework. They tried various types of goodness measures, such as Description Length Gain (DLG) proposed by Kit and Wilks (1999), Accessor Variety (AV) proposed by Feng et al. (2004) and Boundary Entropy (Jin and Tanaka-Ishii, 2006). A notable goodness-based method is ESA: “Evaluation, Selection, Adjustment”, which is proposed by Wang et al. (2011) for unsupervised Mandarin Chinese word segmentation. ESA is an iterative model based on a new goodness algorithm that adopts a local maximum strategy and avoids threshold setting. One disadvantage of ESA is that it needs to iterate the process several times on the corpus to get good performance. Another disadvantage is the requirement for a manually segmented training corpus to find best value for parameters (they called it proper exponent). Another notable work is nVBE: Magis</context>
</contexts>
<marker>Jin, Tanaka-Ishii, 2006</marker>
<rawString>Zhihui Jin, Kumiko Tanaka-Ishii. 2006. Unsupervised segmentation of Chinese text by use of branching entropy. Proceedings of the COLING/ACL on Main conference poster sessions, page 428-435.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chunyu Kit</author>
<author>Yorick Wilks</author>
</authors>
<title>Unsupervised learning of word boundary with description length gain.</title>
<date>1999</date>
<booktitle>Proceedings of the CoNLL99 ACL Workshop.</booktitle>
<pages>1--6</pages>
<location>Bergen, Norway:</location>
<contexts>
<context position="2313" citStr="Kit and Wilks, 1999" startWordPosition="334" endWordPosition="337">tion. A great deal of supervised methods have been proposed for Chinese word segmentation. While successful, they require manually labeled resources and often suffer from issues like poor domain adaptability. Thus, unsupervised word segmentation methods are still attractive to researchers due to its independence on domain and manually labeled corpora. Previous unsupervised approaches to word segmentation can be roughly classified into two types. The first type uses carefully designed goodness measure to identify word candidates. Popular goodness measures include description length gain (DLG) (Kit and Wilks, 1999), accessor variety (AV) (Feng et al., 2004), boundary entropy (BE) (Jin and Tanaka-Ishii, 2006) and normalized variation of branching entropy (nVBE) (Magistry and Sagot, 2012) etc. Goodness measure based model is not segmentation model in a very strict meaning and is actually strong in generating word list without supervision. It inherently lacks capability to deal with ambiguous string, which is one of main sources of segmentation errors and has been extensively explored in supervised Chinese word segmentation. The second type focuses on designing sophisticated statistical model, usually nonp</context>
<context position="6164" citStr="Kit and Wilks (1999)" startWordPosition="909" endWordPosition="912">experiment results on the benchmark datasets and Section 5 concludes the paper. 2 Related Work Unsupervised Chinese word segmentation has been explored in a number of previous works and by various methods. Most of these methods can be divided into two categories: goodness measure based methods and nonparametric Bayesian methods. There have been a plenty of work that is based on a specific goodness measure. Zhao and Kit (2008) compared several popular unsupervised models within a unified framework. They tried various types of goodness measures, such as Description Length Gain (DLG) proposed by Kit and Wilks (1999), Accessor Variety (AV) proposed by Feng et al. (2004) and Boundary Entropy (Jin and Tanaka-Ishii, 2006). A notable goodness-based method is ESA: “Evaluation, Selection, Adjustment”, which is proposed by Wang et al. (2011) for unsupervised Mandarin Chinese word segmentation. ESA is an iterative model based on a new goodness algorithm that adopts a local maximum strategy and avoids threshold setting. One disadvantage of ESA is that it needs to iterate the process several times on the corpus to get good performance. Another disadvantage is the requirement for a manually segmented training corpus</context>
</contexts>
<marker>Kit, Wilks, 1999</marker>
<rawString>Chunyu Kit, Yorick Wilks. 1999. Unsupervised learning of word boundary with description length gain. Proceedings of the CoNLL99 ACL Workshop. Bergen, Norway: Association for Computational Linguis-tics, page 1-6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pierre Magistry</author>
<author>Benoit Sagot</author>
</authors>
<title>Unsupervized word segmentation: the case for mandarin chinese.</title>
<date>2012</date>
<booktitle>Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers-Volume 2. Association for Computational Linguistics,</booktitle>
<pages>383--387</pages>
<contexts>
<context position="2488" citStr="Magistry and Sagot, 2012" startWordPosition="360" endWordPosition="363"> issues like poor domain adaptability. Thus, unsupervised word segmentation methods are still attractive to researchers due to its independence on domain and manually labeled corpora. Previous unsupervised approaches to word segmentation can be roughly classified into two types. The first type uses carefully designed goodness measure to identify word candidates. Popular goodness measures include description length gain (DLG) (Kit and Wilks, 1999), accessor variety (AV) (Feng et al., 2004), boundary entropy (BE) (Jin and Tanaka-Ishii, 2006) and normalized variation of branching entropy (nVBE) (Magistry and Sagot, 2012) etc. Goodness measure based model is not segmentation model in a very strict meaning and is actually strong in generating word list without supervision. It inherently lacks capability to deal with ambiguous string, which is one of main sources of segmentation errors and has been extensively explored in supervised Chinese word segmentation. The second type focuses on designing sophisticated statistical model, usually nonparametric Bayesian models, to find the segmentation with highest posterior probability, given the observed character sequences. Typical statistical models includes Hierarchica</context>
<context position="6888" citStr="Magistry and Sagot (2012)" startWordPosition="1027" endWordPosition="1031">2006). A notable goodness-based method is ESA: “Evaluation, Selection, Adjustment”, which is proposed by Wang et al. (2011) for unsupervised Mandarin Chinese word segmentation. ESA is an iterative model based on a new goodness algorithm that adopts a local maximum strategy and avoids threshold setting. One disadvantage of ESA is that it needs to iterate the process several times on the corpus to get good performance. Another disadvantage is the requirement for a manually segmented training corpus to find best value for parameters (they called it proper exponent). Another notable work is nVBE: Magistry and Sagot (2012) proposed a model based on the Variation of Branching Entropy. By adding normalization and viterbi decoding, they improve performance over Jin and Tanaka-Ishii (2006) and remove most of the parameters and thresholds from the model. Nonparametric Bayesian models also achieved state-of-the-art performance in unsupervised word segmentation. Goldwater et al. (2009) introduced a unigram and a bigram model for unsupervised word segmentation, which are based on Dirichlet process and hierarchical Dirichlet process (Teh et al., 2006) respectively. The main drawback is that it needs almost 20,000 iterat</context>
<context position="14821" citStr="Magistry and Sagot, 2012" startWordPosition="2386" endWordPosition="2389">s the base measure of G0. In fact, H(wi) is the prior distribution over words, so prior knowledge can be injected in this distribution to enhance the performance. In Goldwater et al. (2009)’s work, the base measure H(wi) are defined as a character unigram model: H(wi) = (1 _ ps)|wi|−1ps TT 1j1 where, ps is the probability of generating a word boundary. P(cij) is the probability of the jth character cij in word wi, this probability can be estimated from the training data using maximum likelihood estimation. 3.4 Initializing with nVBE Among various goodness measure based models, we choose nVBE (Magistry and Sagot, 2012) to initialize our Gibbs sampler with its segmentation results. nVBE achieved a relatively high perfomance over other goodness measure based methods. And it’s very simple as well as efficient. Theoretically, the Gibbs sampler may be initialized at random or using any other methods. Initialization does not make a difference since the Gibbs sampler will eventually converge to the posterior distribution if it iterates as much as possible. This is an essential attribute of Gibbs sampling. However, we believe that initializing the Gibbs sampler with the result of nVBE will benefit us in two ways. O</context>
<context position="21412" citStr="Magistry and Sagot (2012)" startWordPosition="3481" endWordPosition="3484"> One may use a small punctuation set to segment a long sentence into shorter ones, while another may write simple regular expressions to identify dates and numbers. Lot of work we compare to don’t even mention this subject. Fortunately, we notice that Wang et al. (2011) provided four kinds of preprocessings (they call settings). In their settings 1 and 2, punctuation and other encoding information are not used. In setting 3, punctuation is used to segment character sequences into sentences, and both punctuation and other encoding information are used in setting 4. Then the results reported in Magistry and Sagot (2012) relied on setting 3 and setting 4. In order to make the comparison as fair as possible, we use setting 3 in our experiment, i.e., only a punctuation set for simplified Chinese is used in all our experiments. We will compare our experiment results to previous work on the same setting if they are provided . 4.3 Experiment Results Table 2 summarizes the F-Scores obtained by different models on PKU and MSRA corpus, as well as several state-of-the-art systems. Detailed information about the presented models are listed as follows: • nVBE: the model based on Variation of Branching Entropy in Magistr</context>
<context position="23130" citStr="Magistry and Sagot (2012)" startWordPosition="3778" endWordPosition="3781">on four different settings, we report their results on setting 3. • NPY(2): the 2-gram language model presented by Mochihashi et al. (2009). • NPY(3): the 3-gram language model presented by Mochihashi et al. (2009). For all of our Gibbs samplers, we run 5 times to get the averaged F-Scores. We also give the variance of the F-Scores in Table 2. For each run, we find that random initialization takes around 1,000 iterations to converge, while initialing with nVBE only takes as few as 10 iterations. This makes 1The results we got with our implementation is slightly lower than what was reported in Magistry and Sagot (2012). According to Pei et al. (2013), they had contacted the authors and confirmed that the higher results was due to a bug in code. So we report the results with our bug free implementation as Pei et al. (2013) did. Our reported results are identical to those of Pei et al. (2013) 859 System PKU MSRA R P F R P F nVBE 78.3 77.5 77.9 79.1 77.3 78.2 HDP 69.0 68.4 68.7(0.012) 70.4 69.4 69.9(0.020) HDP+HMM 77.5 73.2 75.3(0.005) 79.9 73.0 76.3(0.013) HDP+nVBE 80.7 77.9 79.3(0.012) 81.8 77.3 79.5(0.005) Joint 83.1 79.2 81.1(0.002) 84.2 79.3 81.7(0.005) ESA N/A N/A 77.4 N/A N/A 78.4 NPY(2) N/A N/A N/A N/A</context>
</contexts>
<marker>Magistry, Sagot, 2012</marker>
<rawString>Pierre Magistry, Benoit Sagot. 2012. Unsupervized word segmentation: the case for mandarin chinese. Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers-Volume 2. Association for Computational Linguistics, page 383-387.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daichi Mochihashi</author>
<author>Takeshi Yamada</author>
<author>Naonori Ueda</author>
</authors>
<title>Bayesian unsupervised word segmentation with nested Pitman-Yor language modeling.</title>
<date>2009</date>
<booktitle>Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 1-Volume 1. Association for Computational Linguistics,</booktitle>
<pages>100--108</pages>
<contexts>
<context position="3208" citStr="Mochihashi et al., 2009" startWordPosition="465" endWordPosition="468">ually strong in generating word list without supervision. It inherently lacks capability to deal with ambiguous string, which is one of main sources of segmentation errors and has been extensively explored in supervised Chinese word segmentation. The second type focuses on designing sophisticated statistical model, usually nonparametric Bayesian models, to find the segmentation with highest posterior probability, given the observed character sequences. Typical statistical models includes Hierarchical Dirichlet process (HDP) model (Goldwater et al., 2009), Nested PitmanYor process (NPY) model (Mochihashi et al., 2009) etc, which are actually nonparametric language models and therefor can be categorized as word-based model. Word-based model makes decision on wordhood of a candidate character sequence mainly based on information outside the sequence, namely, the wordhood of character sequences being adjacent to the concerned sequence. Inspired by the success of character-based model in supervised word segmentation, we propose a Bayesian HMM model for unsupervised Chinese word segmentation. With the Bayesian HMM model, we formulate the unsupervised segmentation tasks as procedure of tagging positional 854 Pro</context>
<context position="7553" citStr="Mochihashi et al. (2009)" startWordPosition="1123" endWordPosition="1126">of Branching Entropy. By adding normalization and viterbi decoding, they improve performance over Jin and Tanaka-Ishii (2006) and remove most of the parameters and thresholds from the model. Nonparametric Bayesian models also achieved state-of-the-art performance in unsupervised word segmentation. Goldwater et al. (2009) introduced a unigram and a bigram model for unsupervised word segmentation, which are based on Dirichlet process and hierarchical Dirichlet process (Teh et al., 2006) respectively. The main drawback is that it needs almost 20,000 iterations before the Gibbs sampler converges. Mochihashi et al. (2009) extended this method by introducing a nested character model and an efficient blocked Gibbs sampler. Their method is based on what they called nested Pitman-Yor language model. One disadvantage of goodness measure based methods is that they do not have any disambiguation ability in theory in spite of their competitive performances. This is because once the goodness measure is given, the decoding algorithm will segment any ambiguous strings into the same word sequences, no matter what their context is. In contrast, nonparametric Bayesian language models aim to segment character string into a “</context>
<context position="22644" citStr="Mochihashi et al. (2009)" startWordPosition="3692" endWordPosition="3695">got (2012). We re-implement their model on setting 31. • HDP: the HDP-based model proposed by Goldwater et al. (2009), initialized randomly. • HDP+HMM: the model combining HDPbased model and HMM-based model as proposed in Section 3, initialized randomly. • HDP+nVBE: the HDP-based model, initialized with the results of nVBE model. • Joint: the “HDP+HMM” model initialized with nVBE model. • ESA: the model proposed in Wang et al. (2011), as mentioned above, the conducted experiments on four different settings, we report their results on setting 3. • NPY(2): the 2-gram language model presented by Mochihashi et al. (2009). • NPY(3): the 3-gram language model presented by Mochihashi et al. (2009). For all of our Gibbs samplers, we run 5 times to get the averaged F-Scores. We also give the variance of the F-Scores in Table 2. For each run, we find that random initialization takes around 1,000 iterations to converge, while initialing with nVBE only takes as few as 10 iterations. This makes 1The results we got with our implementation is slightly lower than what was reported in Magistry and Sagot (2012). According to Pei et al. (2013), they had contacted the authors and confirmed that the higher results was due to </context>
</contexts>
<marker>Mochihashi, Yamada, Ueda, 2009</marker>
<rawString>Daichi Mochihashi, Takeshi Yamada, Naonori Ueda. 2009. Bayesian unsupervised word segmentation with nested Pitman-Yor language modeling. Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 1-Volume 1. Association for Computational Linguistics, page 100-108.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenzhe Pei</author>
<author>Dongxu Han</author>
<author>Baobao Chang</author>
</authors>
<title>A Refined HDP-Based Model for Unsupervised Chinese Word Segmentation. Chinese Computational Linguistics and Natural Language Processing Based on Naturally Annotated Big Data.</title>
<date>2013</date>
<pages>44--51</pages>
<publisher>Springer</publisher>
<location>Berlin Heidelberg,</location>
<contexts>
<context position="23162" citStr="Pei et al. (2013)" startWordPosition="3784" endWordPosition="3787">eir results on setting 3. • NPY(2): the 2-gram language model presented by Mochihashi et al. (2009). • NPY(3): the 3-gram language model presented by Mochihashi et al. (2009). For all of our Gibbs samplers, we run 5 times to get the averaged F-Scores. We also give the variance of the F-Scores in Table 2. For each run, we find that random initialization takes around 1,000 iterations to converge, while initialing with nVBE only takes as few as 10 iterations. This makes 1The results we got with our implementation is slightly lower than what was reported in Magistry and Sagot (2012). According to Pei et al. (2013), they had contacted the authors and confirmed that the higher results was due to a bug in code. So we report the results with our bug free implementation as Pei et al. (2013) did. Our reported results are identical to those of Pei et al. (2013) 859 System PKU MSRA R P F R P F nVBE 78.3 77.5 77.9 79.1 77.3 78.2 HDP 69.0 68.4 68.7(0.012) 70.4 69.4 69.9(0.020) HDP+HMM 77.5 73.2 75.3(0.005) 79.9 73.0 76.3(0.013) HDP+nVBE 80.7 77.9 79.3(0.012) 81.8 77.3 79.5(0.005) Joint 83.1 79.2 81.1(0.002) 84.2 79.3 81.7(0.005) ESA N/A N/A 77.4 N/A N/A 78.4 NPY(2) N/A N/A N/A N/A N/A 80.2 NPY(3) N/A N/A N/A N/A</context>
</contexts>
<marker>Pei, Han, Chang, 2013</marker>
<rawString>Wenzhe Pei, Dongxu Han, Baobao Chang. 2013. A Refined HDP-Based Model for Unsupervised Chinese Word Segmentation. Chinese Computational Linguistics and Natural Language Processing Based on Naturally Annotated Big Data. Springer Berlin Heidelberg, page 44-51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Whye Teh</author>
<author>Michael I Jordan</author>
<author>Matthew J Beal</author>
</authors>
<title>Sharing Clusters among Related Groups: Hierarchical Dirichlet Processes.</title>
<date>2006</date>
<publisher>NIPS.</publisher>
<contexts>
<context position="7418" citStr="Teh et al., 2006" startWordPosition="1103" endWordPosition="1106">hey called it proper exponent). Another notable work is nVBE: Magistry and Sagot (2012) proposed a model based on the Variation of Branching Entropy. By adding normalization and viterbi decoding, they improve performance over Jin and Tanaka-Ishii (2006) and remove most of the parameters and thresholds from the model. Nonparametric Bayesian models also achieved state-of-the-art performance in unsupervised word segmentation. Goldwater et al. (2009) introduced a unigram and a bigram model for unsupervised word segmentation, which are based on Dirichlet process and hierarchical Dirichlet process (Teh et al., 2006) respectively. The main drawback is that it needs almost 20,000 iterations before the Gibbs sampler converges. Mochihashi et al. (2009) extended this method by introducing a nested character model and an efficient blocked Gibbs sampler. Their method is based on what they called nested Pitman-Yor language model. One disadvantage of goodness measure based methods is that they do not have any disambiguation ability in theory in spite of their competitive performances. This is because once the goodness measure is given, the decoding algorithm will segment any ambiguous strings into the same word s</context>
<context position="13170" citStr="Teh et al., 2006" startWordPosition="2088" endWordPosition="2091">(cj|tj, h) are the posterior probabilities, they are given as: n&lt;tj−1,tj&gt; + B Pt(tj|tj−1, h) = (4) n&lt;tj−1,*&gt; + TB n&lt;tj,cj&gt; + Q Pe(cj|tj, h) = (5) n&lt;tj,*&gt; + V Q where n&lt;tj−1,tj&gt; is the tag bigram count of &lt; tj−1, tj &gt; in h, n&lt;tj,cj&gt; denotes the number of occurrences of tag tj and character cj, and * means a sum operation. T and V are the size of character tag set (we follow the commonly used {SBME} tag set and thus T = 4 in this case) and character vocabulary. 3.3 HDP Model Goldwater et al. (2009) proposed a nonparametric Bayesian model for unsupervised word segmentation which is based on HDP (Teh et al., 2006). In this model, the conditional probability of the segmentation W given the character string C is defined as: |W| PD(W|C) = H PD(wi|wi−1) (6) i=0 where wi is the ith word in W. This is actually a nonparametric bigram language model. This bigram model assumes that each different word has a different distribution over words following it, but all these different distributions are linked through a HDP model: wi|wi−1 = l — Gl Gl — DP(α1, G0) G0 — DP(α, H) where DP denotes a Dirichlet process. Suppose we have observed segmentation result h, then we can get the posterior probability PD(wi|wi−1 = l, </context>
</contexts>
<marker>Teh, Jordan, Beal, 2006</marker>
<rawString>Yee Whye Teh, Michael I. Jordan, Matthew J. Beal, et al. 2006. Sharing Clusters among Related Groups: Hierarchical Dirichlet Processes. NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fuchun Peng</author>
<author>Fangfang Feng</author>
<author>Andrew McCallum</author>
</authors>
<title>Chinese segmentation and new word detection using conditional random fields.</title>
<date>2004</date>
<booktitle>Proceedings of COLING,</booktitle>
<pages>562--568</pages>
<contexts>
<context position="10991" citStr="Peng et al., 2004" startWordPosition="1699" endWordPosition="1702">Chinese word segmentation is character-based model which was first proposed by Xue (2003). This method treats word segmentation as a tagging problem, each tag indicates the position of a character within a word. The most commonly used tag set is {Single, Begin, Middle, End}. Specifically, S means the character forms a single word, B/E means the character is the begining/ending character of the word, and M means the character is in the middle of the word. Existing models are trained on manually annotated data in a supervised way based on discriminative models such as Conditional Random Fields (Peng et al., 2004; Tseng et al., 2005). Supervised character-based methods make full use of character level information and thus have been very successful in the last decade. However, no unsupervised model has utilized character level information in the way as supervised method does. We can also build a character-based model for Chinese word segmentation using hidden Markov model(HMM) as formulated in the following equation: PM(W|C) = � |C |Pt(ti|ti−1)Pe(ci|ti) (2) i=1 where C and W have the same meaning as before. Pt(ti|ti−1) is the transition probability of tag ti given its former tag ti−1 and Pe(ci|ti) is t</context>
</contexts>
<marker>Peng, Feng, McCallum, 2004</marker>
<rawString>Fuchun Peng, Fangfang Feng, Andrew McCallum. 2004. Chinese segmentation and new word detection using conditional random fields. Proceedings of COLING, page 562-568.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Huihsin Tseng</author>
<author>Pichuan Chang</author>
<author>Galen Andrew</author>
</authors>
<title>A conditional random field word segmenter for sighan bakeoff</title>
<date>2005</date>
<booktitle>Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing,</booktitle>
<volume>171</volume>
<contexts>
<context position="11012" citStr="Tseng et al., 2005" startWordPosition="1703" endWordPosition="1706">tation is character-based model which was first proposed by Xue (2003). This method treats word segmentation as a tagging problem, each tag indicates the position of a character within a word. The most commonly used tag set is {Single, Begin, Middle, End}. Specifically, S means the character forms a single word, B/E means the character is the begining/ending character of the word, and M means the character is in the middle of the word. Existing models are trained on manually annotated data in a supervised way based on discriminative models such as Conditional Random Fields (Peng et al., 2004; Tseng et al., 2005). Supervised character-based methods make full use of character level information and thus have been very successful in the last decade. However, no unsupervised model has utilized character level information in the way as supervised method does. We can also build a character-based model for Chinese word segmentation using hidden Markov model(HMM) as formulated in the following equation: PM(W|C) = � |C |Pt(ti|ti−1)Pe(ci|ti) (2) i=1 where C and W have the same meaning as before. Pt(ti|ti−1) is the transition probability of tag ti given its former tag ti−1 and Pe(ci|ti) is the emission probabili</context>
</contexts>
<marker>Tseng, Chang, Andrew, 2005</marker>
<rawString>Huihsin Tseng, Pichuan Chang, Galen Andrew, et al. 2005. A conditional random field word segmenter for sighan bakeoff 2005. Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing, Vol. 171.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kun Wang</author>
<author>Chengqing Zong</author>
<author>Keh-Yih Su</author>
</authors>
<title>A character-based joint model for Chinese word segmentation.</title>
<date>2010</date>
<booktitle>Proceedings of the 23rd International Conference on Computational Linguistics. Association for Computational Linguistics,</booktitle>
<pages>1173--1181</pages>
<contexts>
<context position="8595" citStr="Wang et al., 2010" startWordPosition="1287" endWordPosition="1290">ny ambiguous strings into the same word sequences, no matter what their context is. In contrast, nonparametric Bayesian language models aim to segment character string into a “reasonable” sentence according to the posterior probability. Thus, theoretically, this method should have better ability to solve ambiguities over goodness measure based methods. 3 Joint Model In this section, we will discuss our joint model in detail. 855 3.1 Combining HDP and HMM In supervised Chinese word segmentation literature, word-based approaches and characterbased approaches often have complementary advantages (Wang et al., 2010).Since the two types of model try to solve the problem from different perspectives and by utilizing different levels of information (word level and character level). In unsupervised Chinese word segmentation literature, the HDP-base model can be viewed as a typical word-based method. And we can also build a character-based unsupervised model by using a hidden Markov model. We believe that the HDPbased model and the HMM-based model are also complementary with each other, and a combination of them will take advantage of both and thus capture different levels of information. Now the problem we ar</context>
<context position="30467" citStr="Wang et al. (2010)" startWordPosition="4983" endWordPosition="4986">tistics of overlapping ambiguity. This table shows the total number of mistakes made by different systems at overlapping ambiguous strings. The numbers in parentheses denote the total number of overlapping ambiguous strings. 4.5 Statistical Significance Test The main results presented in Table 2 has shown that our proposed joint model outperforms the two baselines as well as state-of-the-art systems. But it is also important to know if the improvement is statistically significant over these systems. So we conduct statistical significance tests of F-scores among these various models. Following Wang et al. (2010), we use the bootstrapping method (Zhang et al., 2004). Here is how it works: suppose we have a testing set T0 to test several word segmentation systems, there are N testing examples (sentences or line of characters) in T0. We create a new testing set T1 with N examples by sampling with replacement from T0, then repeat these process M − 1 times. And we will have a total M +1 testing sets. In our test procedures, M is set to 2000. Since we just implement our joint model and its component models, we can not generate paired samples for other models (i.e. ESA and NPY(n)). Instead, we follow Wang e</context>
</contexts>
<marker>Wang, Zong, Su, 2010</marker>
<rawString>Kun Wang, Chengqing Zong, Keh-Yih Su. 2010. A character-based joint model for Chinese word segmentation. Proceedings of the 23rd International Conference on Computational Linguistics. Association for Computational Linguistics, page 1173-1181.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hanshi Wang</author>
<author>Jian Zhu</author>
<author>Shiping Tang</author>
</authors>
<title>A new unsupervised approach to word segmentation.</title>
<date>2011</date>
<journal>Computational Linguistics,</journal>
<volume>37</volume>
<issue>3</issue>
<pages>421--454</pages>
<contexts>
<context position="6386" citStr="Wang et al. (2011)" startWordPosition="943" endWordPosition="946">ods can be divided into two categories: goodness measure based methods and nonparametric Bayesian methods. There have been a plenty of work that is based on a specific goodness measure. Zhao and Kit (2008) compared several popular unsupervised models within a unified framework. They tried various types of goodness measures, such as Description Length Gain (DLG) proposed by Kit and Wilks (1999), Accessor Variety (AV) proposed by Feng et al. (2004) and Boundary Entropy (Jin and Tanaka-Ishii, 2006). A notable goodness-based method is ESA: “Evaluation, Selection, Adjustment”, which is proposed by Wang et al. (2011) for unsupervised Mandarin Chinese word segmentation. ESA is an iterative model based on a new goodness algorithm that adopts a local maximum strategy and avoids threshold setting. One disadvantage of ESA is that it needs to iterate the process several times on the corpus to get good performance. Another disadvantage is the requirement for a manually segmented training corpus to find best value for parameters (they called it proper exponent). Another notable work is nVBE: Magistry and Sagot (2012) proposed a model based on the Variation of Branching Entropy. By adding normalization and viterbi</context>
<context position="21057" citStr="Wang et al. (2011)" startWordPosition="3425" endWordPosition="3428">uch as punctuation information, encoding scheme and word length could not be used in principle. Nevertheless, information like punctuation can be easily injected to most existing systems and significantly enhance the performance. The problem we are faced with is that we don’t know for sure what kind of prior information are used in other systems. One may use a small punctuation set to segment a long sentence into shorter ones, while another may write simple regular expressions to identify dates and numbers. Lot of work we compare to don’t even mention this subject. Fortunately, we notice that Wang et al. (2011) provided four kinds of preprocessings (they call settings). In their settings 1 and 2, punctuation and other encoding information are not used. In setting 3, punctuation is used to segment character sequences into sentences, and both punctuation and other encoding information are used in setting 4. Then the results reported in Magistry and Sagot (2012) relied on setting 3 and setting 4. In order to make the comparison as fair as possible, we use setting 3 in our experiment, i.e., only a punctuation set for simplified Chinese is used in all our experiments. We will compare our experiment resul</context>
<context position="22457" citStr="Wang et al. (2011)" startWordPosition="3661" endWordPosition="3664">several state-of-the-art systems. Detailed information about the presented models are listed as follows: • nVBE: the model based on Variation of Branching Entropy in Magistry and Sagot (2012). We re-implement their model on setting 31. • HDP: the HDP-based model proposed by Goldwater et al. (2009), initialized randomly. • HDP+HMM: the model combining HDPbased model and HMM-based model as proposed in Section 3, initialized randomly. • HDP+nVBE: the HDP-based model, initialized with the results of nVBE model. • Joint: the “HDP+HMM” model initialized with nVBE model. • ESA: the model proposed in Wang et al. (2011), as mentioned above, the conducted experiments on four different settings, we report their results on setting 3. • NPY(2): the 2-gram language model presented by Mochihashi et al. (2009). • NPY(3): the 3-gram language model presented by Mochihashi et al. (2009). For all of our Gibbs samplers, we run 5 times to get the averaged F-Scores. We also give the variance of the F-Scores in Table 2. For each run, we find that random initialization takes around 1,000 iterations to converge, while initialing with nVBE only takes as few as 10 iterations. This makes 1The results we got with our implementat</context>
</contexts>
<marker>Wang, Zhu, Tang, 2011</marker>
<rawString>Hanshi Wang, Jian Zhu, Shiping Tang, et al. 2011. A new unsupervised approach to word segmentation. Computational Linguistics, 37(3): 421-454.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
</authors>
<title>Chinese word segmentation as character tagging.</title>
<date>2003</date>
<journal>Computational Linguistics and Chinese Language Processing,</journal>
<volume>8</volume>
<issue>1</issue>
<pages>29--48</pages>
<contexts>
<context position="10463" citStr="Xue (2003)" startWordPosition="1609" endWordPosition="1610">ven in Equation 2. Z(C) is a normalization term to make sure that PJ(W|C) is a probability distribution. The combining method is inspired by Hinton (1999), which proved that it is possible to combine many individual expert models by multiplying the probabilities and then renormalizing it. They called it “product of experts”. We can see that combining models in this way does not involve any extra parameters and Gibbs sampling can be easily used for model inference. 3.2 Bayesian HMM The dominant method for supervised Chinese word segmentation is character-based model which was first proposed by Xue (2003). This method treats word segmentation as a tagging problem, each tag indicates the position of a character within a word. The most commonly used tag set is {Single, Begin, Middle, End}. Specifically, S means the character forms a single word, B/E means the character is the begining/ending character of the word, and M means the character is in the middle of the word. Existing models are trained on manually annotated data in a supervised way based on discriminative models such as Conditional Random Fields (Peng et al., 2004; Tseng et al., 2005). Supervised character-based methods make full use </context>
</contexts>
<marker>Xue, 2003</marker>
<rawString>Nianwen Xue. 2003. Chinese word segmentation as character tagging. Computational Linguistics and Chinese Language Processing, 8(1): 29-48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai Zhao</author>
<author>Chunyu Kit</author>
</authors>
<title>An Empirical Comparison of Goodness Measures for Unsupervised Chinese Word Segmentation with a Unified Framework. IJCNLP,</title>
<date>2008</date>
<pages>6--16</pages>
<contexts>
<context position="5973" citStr="Zhao and Kit (2008)" startWordPosition="880" endWordPosition="883">aper is organized as follows. In Section 2, we will introduce several related systems for unsupervised word segmentation. Then our joint model is presented in Section 3. Section 4 shows our experiment results on the benchmark datasets and Section 5 concludes the paper. 2 Related Work Unsupervised Chinese word segmentation has been explored in a number of previous works and by various methods. Most of these methods can be divided into two categories: goodness measure based methods and nonparametric Bayesian methods. There have been a plenty of work that is based on a specific goodness measure. Zhao and Kit (2008) compared several popular unsupervised models within a unified framework. They tried various types of goodness measures, such as Description Length Gain (DLG) proposed by Kit and Wilks (1999), Accessor Variety (AV) proposed by Feng et al. (2004) and Boundary Entropy (Jin and Tanaka-Ishii, 2006). A notable goodness-based method is ESA: “Evaluation, Selection, Adjustment”, which is proposed by Wang et al. (2011) for unsupervised Mandarin Chinese word segmentation. ESA is an iterative model based on a new goodness algorithm that adopts a local maximum strategy and avoids threshold setting. One di</context>
<context position="20040" citStr="Zhao and Kit (2008)" startWordPosition="3255" endWordPosition="3258">he word precision and recall. Precision is given as: P = #correct words in result #total words in result and recall is given as: R = #correct words in result #total words in gold corpus then F-Score is calculated as: 2 x R x F R + F a i+2 1i1 j2 F= 858 Corpus TrainingSize (words) TestSize (words) PKU 1.1M 104K MSRA 2.37M 107K Table 1: Statistics of training and testing data Huang and Zhao (2007) provided an empirical method to estimate the consistency between the four different segmentation standards involved in the Bakeoff-3. A lowest consistency rate 84.8% is found among the four standards. Zhao and Kit (2008) considered this figure as the upper bound for any unsupervised Chinese word segmentation systems. We also use it as the topline in our comparison. 4.2 Prior Knowledge Used When it comes to the evaluation and comparison for unsupervised word segmentation systems, an important issue is what kind of pre-processing steps and prior knowledge are needed. To be fully unsupervised, any prior knowledge such as punctuation information, encoding scheme and word length could not be used in principle. Nevertheless, information like punctuation can be easily injected to most existing systems and significan</context>
</contexts>
<marker>Zhao, Kit, 2008</marker>
<rawString>Hai Zhao, Chunyu Kit. 2008. An Empirical Comparison of Goodness Measures for Unsupervised Chinese Word Segmentation with a Unified Framework. IJCNLP, page 6-16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ying Zhang</author>
<author>Stephan Vogel</author>
<author>Alex Waibel</author>
</authors>
<title>Interpreting BLEU/NIST scores: How much improvement do we need to have a better system?</title>
<date>2004</date>
<publisher>LREC.</publisher>
<contexts>
<context position="30521" citStr="Zhang et al., 2004" startWordPosition="4992" endWordPosition="4995">e total number of mistakes made by different systems at overlapping ambiguous strings. The numbers in parentheses denote the total number of overlapping ambiguous strings. 4.5 Statistical Significance Test The main results presented in Table 2 has shown that our proposed joint model outperforms the two baselines as well as state-of-the-art systems. But it is also important to know if the improvement is statistically significant over these systems. So we conduct statistical significance tests of F-scores among these various models. Following Wang et al. (2010), we use the bootstrapping method (Zhang et al., 2004). Here is how it works: suppose we have a testing set T0 to test several word segmentation systems, there are N testing examples (sentences or line of characters) in T0. We create a new testing set T1 with N examples by sampling with replacement from T0, then repeat these process M − 1 times. And we will have a total M +1 testing sets. In our test procedures, M is set to 2000. Since we just implement our joint model and its component models, we can not generate paired samples for other models (i.e. ESA and NPY(n)). Instead, we follow Wang et al. (2010)’s method and first calculate the 95% conf</context>
</contexts>
<marker>Zhang, Vogel, Waibel, 2004</marker>
<rawString>Ying Zhang, Stephan Vogel, Alex Waibel. 2004. Interpreting BLEU/NIST scores: How much improvement do we need to have a better system? LREC.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
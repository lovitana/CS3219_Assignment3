<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000086">
<title confidence="0.992724">
Nothing like Good Old Frequency:
Studying Context Filters for Distributional Thesauri
</title>
<author confidence="0.999746">
Muntsa Padr´o,4 Marco Idiart°, Carlos Ramisch♦, Aline Villavicencio4
</author>
<affiliation confidence="0.849410666666667">
4Institute of Informatics, Federal University of Rio Grande do Sul (Brazil)
°Institute of Physics, Federal University of Rio Grande do Sul (Brazil)
♦Aix Marseille Universit´e, CNRS, LIF UMR 7279, 13288, Marseille (France)
</affiliation>
<email confidence="0.9850385">
muntsa.padro@inf.ufrgs.br,marco.idiart@gmail.com,
carlos.ramisch@lif.univ-mrs.fr, avillavicencio@inf.ufrgs.br
</email>
<sectionHeader confidence="0.993776" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999878583333333">
Much attention has been given to the
impact of informativeness and similar-
ity measures on distributional thesauri.
We investigate the effects of context fil-
ters on thesaurus quality and propose the
use of cooccurrence frequency as a sim-
ple and inexpensive criterion. For eval-
uation, we measure thesaurus agreement
with WordNet and performance in answer-
ing TOEFL-like questions. Results illus-
trate the sensitivity of distributional the-
sauri to filters.
</bodyText>
<sectionHeader confidence="0.998795" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999921844444444">
Large-scale distributional thesauri created auto-
matically from corpora (Grefenstette, 1994; Lin,
1998; Weeds et al., 2004; Ferret, 2012) are an
inexpensive and fast alternative for representing
semantic relatedness between words, when man-
ually constructed resources like WordNet (Fell-
baum, 1998) are unavailable or lack coverage. To
construct a distributional thesaurus, the (colloca-
tional or syntactic) contexts in which a target word
occurs are used as the basis for calculating its sim-
ilarity with other words. That is, two words are
similar if they share a large proportion of contexts.
Much attention has been devoted to refin-
ing thesaurus quality, improving informativeness
and similarity measures (Lin, 1998; Curran and
Moens, 2002; Ferret, 2010), identifying and de-
moting bad neighbors (Ferret, 2013), or using
more relevant contexts (Broda et al., 2009; Bie-
mann and Riedl, 2013). For the latter in particular,
as words vary in their collocational tendencies, it
is difficult to determine how informative a given
context is. To remove uninformative and noisy
contexts, filters have often been applied like point-
wise mutual information (PMI), lexicographer’s
mutual information (LMI) (Biemann and Riedl,
2013), t-score (Piasecki et al., 2007) and z-score
(Broda et al., 2009). However, the selection of a
measure and of a threshold value for these filters
is generally empirically determined. We argue that
these filtering parameters have a great influence on
the quality of the generated thesauri.
The goal of this paper is to quantify the im-
pact of context filters on distributional thesauri.
We experiment with different filter methods and
measures to assess context significance. We pro-
pose the use of simple cooccurrence frequency as
a filter and show that it leads to better results than
more expensive measures such as LMI or PMI.
Thus we propose a cheap and effective way of fil-
tering contexts while maintaining quality.
This paper is organized as follows: in §2 we
discuss evaluation of distributional thesauri. The
methodology adopted in the work and the results
are discussed in §3 and §4. We finish with some
conclusions and discussion of future work.
</bodyText>
<sectionHeader confidence="0.999745" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999604421052632">
In a nutshell, the standard approach to build a dis-
tributional thesaurus consists of: (i) the extraction
of contexts for the target words from corpora, (ii)
the application of an informativeness measure to
represent these contexts and (iii) the application of
a similarity measure to compare sets of contexts.
The contexts in which a target word appears can
be extracted in terms of a window of cooccurring
(content) words surrounding the target (Freitag et
al., 2005; Ferret, 2012; Erk and Pado, 2010) or in
terms of the syntactic dependencies in which the
target appears (Lin, 1998; McCarthy et al., 2003;
Weeds et al., 2004). The informativeness of each
context is calculated using measures like PMI, and
t-test while the similarity between contexts is cal-
culated using measures like Lin’s (1998), cosine,
Jensen-Shannon divergence, Dice or Jaccard.
Evaluation of the quality of distributional the-
sauri is a well know problem in the area (Lin,
</bodyText>
<page confidence="0.973252">
419
</page>
<bodyText confidence="0.963504551724138">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 419–424,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
1998; Curran and Moens, 2002). For instance, for
intrinsic evaluation, the agreement between the-
sauri has been examined, looking at the average
similarity of a word in the thesauri (Lin, 1998),
and at the overlap and rank agreement between the
thesauri for target words like nouns (Weeds et al.,
2004). Although much attention has been given to
the evaluation of various informativeness and sim-
ilarity measures, a careful assessment of the ef-
fects of filtering on the resulting thesauri is also
needed. For instance, Biemann and Riedl (2013)
found that filtering a subset of contexts based on
LMI increased the similarity of a thesaurus with
WordNet. In this work, we compare the impact of
using different types of filters in terms of thesaurus
agreement with WordNet, focusing on a distribu-
tional thesaurus of English verbs. We also propose
a frequency-based saliency measure to rank and
filter contexts and compare it with PMI and LMI.
Extrinsic evaluation of distributional thesauri
has been carried out for tasks such as En-
glish lexical substitution (McCarthy and Navigli,
2009), phrasal verb compositionality detection
(McCarthy et al., 2003) and the WordNet-based
synonymy test (WBST) (Freitag et al., 2005). For
comparative purposes in this work we adopt the
latter.
</bodyText>
<sectionHeader confidence="0.998487" genericHeader="method">
3 Methodology
</sectionHeader>
<bodyText confidence="0.910797521739131">
We focus on thesauri of English verbs constructed
from the BNC (Burnard, 2007)1. Contexts are ex-
tracted from syntactic dependencies generated by
RASP (Briscoe et al., 2006), using nouns (heads
of NPs) which have subject and direct object rela-
tions with the target verb. Thus, each target verb
is represented by a set of triples containing (i) the
verb itself, (ii) a context noun and (iii) a syntac-
tic relation (object, subject). The thesauri were
constructed using Lin’s (1998) method. Lin’s ver-
sion of the distributional hypothesis states that two
words (verbs v1 and v2 in our case) are similar if
they share a large proportion of contexts weighted
by their information content, assessed with PMI
(Bansal et al., 2012; Turney, 2013).
In the literature, little attention is paid to context
filters. To investigate their impact, we compare
two kinds of filters, and before calculating similar-
ity using Lin’s measure, we apply them to remove
1Even though larger corpora are available, we use a tradi-
tional carefully constructed corpus with representative sam-
ples of written English to control the quality of the thesaurus.
potentially noisy triples:
</bodyText>
<listItem confidence="0.980971555555556">
• Threshold (th): we remove triples that oc-
cur less than a threshold th. Threshold values
vary from 1 to 50 counts per triple.
• Relevance (p): we keep only the top p most
relevant contexts for each verb, were rele-
vance is defined according to the following
measures: (a) frequency, (b) PMI, and (c)
LMI (Biemann and Riedl, 2013). Values of
p vary between 10 and 1000.
</listItem>
<bodyText confidence="0.994122172413793">
In this work, we want to answer two ques-
tions: (a) Do more selective filters improve intrin-
sic evaluation of thesaurus? and (b) Do they also
help in extrinsic evaluation?
For intrinsic evaluation, we determine agree-
ment between a distributional thesaurus and Word-
Net as the path similarities for the first k distri-
butional neighbors of a verb. A single score is
obtained by averaging the similarities of all verbs
with their k first neighbors. The higher this score
is, the closer the neighbors are to the target in
WordNet, and the better the thesaurus. Several
values of k were tested and the results showed ex-
actly the same curve shapes for all values, with
WordNet similarity decreasing linearly with k. For
the remainder of the paper we adopt k = 10, as it
is widely used in the literature.
For extrinsic evaluation, we use the WBST set
for verbs (Freitag et al., 2005) with 7,398 ques-
tions and an average polysemy of 10.4. The task
consists of choosing the most suitable synonym
for a word among a set of four options. The the-
saurus is used to rank the candidate answers by
similarity scores, and select the first one as the
correct synonym. As discussed by Freitag et al.
(2005), the upper bound reached by English na-
tive speakers is 88.4% accuracy, and simple lower
bounds are 25% (random choice) and 34.5% (al-
ways choosing the most frequent option).
</bodyText>
<sectionHeader confidence="0.999911" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.999682125">
Figure 1 shows average WordNet similarities for
thesauri built filtering by frequency threshold th
and by p most frequent contexts. Table 1 sum-
marizes the parametrization leading to the best
WordNet similarity for each kind of filter. In all
cases we show the results obtained for different
frequency ranges2 as well as the results when av-
eraging over all verbs.
</bodyText>
<footnote confidence="0.638569">
2In order to study the influence of verb frequency on the
results, we divide the verbs in three groups: high-frequency
</footnote>
<page confidence="0.991875">
420
</page>
<figure confidence="0.934445">
WordNet path Similarity for different frequency ranges, k=10
Filtering triples with frequency under th
WordNet path Similarity for different frequency ranges, k=10
Keeping p most frequent triples per verb
all verbs
high frequent verbs
mid frequent verbs
low frequent verbs
WN similarity
0.25
0.15
0.05
0.2
0.1
0
10 100 1000
p
all verbs
high frequent verbs
mid frequent verbs
low frequent verbs
1 10
th
WN similarity 0.25
0.2
0.15
0.1
0.05
0
</figure>
<figureCaption confidence="0.957693">
Figure 1: WordNet scores for verb frequency ranges, filtering by frequency threshold th (left) and p most
frequent contexts (right).
</figureCaption>
<table confidence="0.999219714285714">
Filter All verbs Low Frequency range High
Mid
No filter - 0.148 - 0.101 - 0.144 - 0.198
Filter low freq. contexts th = 50 0.164 th = 50 0.202 th = 50 0.154 th = 1 0.200
Keep p contexts (freq.) p = 200 0.158 p = 500 0.138 p = 200 0.149 p = 200 0.206
Keep p contexts (PMI) p = 1000 0.139 p = 1000 0.101 p = 1000 0.136 p = 1000 0.181
Keep p contexts (LMI) p = 200 0.155 p = 100 0.112 p = 200 0.147 p = 200 0.208
</table>
<tableCaption confidence="0.8909755">
Table 1: Best scores obtained for each filter for all verbs and frequency ranges. Scores are given in terms
of WordNet path. Confidence interval is arround ± 0.002 in all cases.
</tableCaption>
<bodyText confidence="0.999607875">
When using a threshold filter (Figure 1 left),
high values lead to better performance for mid-
and low-frequency verbs. This is because, for high
th values, there are few low and mid-frequency
verbs left, since a verb that occurs less has less
chances to be seen often in the same context. The
similarity for verbs with no contexts over the fre-
quency threshold cannot be assessed and as a con-
sequence those verbs are not included in the fi-
nal thesaurus. As Figure 2 shows, the number
of verbs decreases much faster for low and mid
frequency verbs when th increases.3 For exam-
ple, for th = 50, there are only 7 remaining low-
frequency verbs in the thesaurus and these tend
to be idiosyncratic multiword expressions. One
example is wreak, and the only triple contain-
ing this verb that appeared more than 50 times is
wreak havoc (71 occurrences). The neighbors of
this verb are cause and play, which yield a good
similarity score in WordNet. Therefore, although
higher thresholds result in higher similarities for
low and mid-frequency verbs, this comes at a cost,
as the number of verbs included in the thesaurus
decreases considerably.
</bodyText>
<equation confidence="0.376594">
(||v ||&gt; 500), mid-frequency (150 &lt; ||v ||&lt; 500) and low-
frequency (||v ||&lt; 150).
</equation>
<footnote confidence="0.9618085">
3For p most salient contexts, the number of verbs does not
vary and is the same shown in Figure 2 for th = 1 (no filter).
</footnote>
<subsectionHeader confidence="0.52194">
Number of verbs in WordNet
Filtering triples with frequency under th
</subsectionHeader>
<bodyText confidence="0.948579571428571">
Figure 2: Number of verbs per frequency ranges
when filtering by context frequency threshold th
As expected, the best performance is obtained
for high-frequency verbs and no filter, since it re-
sults in more context information per verb. In-
creasing th decreases similarity due to the removal
of some of these contexts. In average, higher th
values lead to better overall similarity among the
frequency ranges (from 0.148 with th = 1 to
0.164 with th = 50). The higher the threshold,
the more high-frequency verbs will prevail in the
thesauri, for which the WordNet path similarities
are higher.
On the other hand, when adopting a relevance
</bodyText>
<figure confidence="0.9880752">
1 10
th
Number of verbs
3500
3000
2500
2000
1500
1000
500
0
all verbs
high frequent verbs
mid frequent verbs
low frequent verbs
</figure>
<page confidence="0.719717">
421
</page>
<figure confidence="0.961117071428571">
WBST task: P, R and F1
Filtering triples with frequency under th
WBST task: P, R and F1
Keeping p most frequent triples per verb
1 10
th
P, R, F1
0.8
0.6
0.4
0.2
0
1
Precision
Recall
F1
1
0.8
0.6
0.4
0.2
0
10 100 1000
p
P, R, F1
Precision
Recall
F1
</figure>
<figureCaption confidence="0.987966">
Figure 3: WBST task scores filtering by frequency threshold th (left) and p most frequent contexts
(right).
</figureCaption>
<bodyText confidence="0.998363285714285">
filter of keeping the p most relevant contexts for
each verb (Figure 1 right), we obtain similar re-
sults, but more stable thesauri. The number of
verbs remains constant, since we keep a fixed
number of contexts for each verb and verbs are not
removed when the threshold is modified. Word-
Net similarity increases as more contexts are taken
into account, for all frequency ranges. There is a
maximum around p = 200, though larger values
do not lead to a drastic drop in quality. This sug-
gests that the noise introduced by low-frequency
contexts is compensated by the increase of infor-
mativeness for other contexts. An ideal balance
is reached by the lowest possible p that maintains
high WordNet similarity, since the lower the p the
faster the thesaurus construction.
In terms of saliency measure, when keeping
only the p most relevant contexts, sorting them
with PMI leads to much worse results than LMI
or frequency, as PMI gives too much weight to
infrequent combinations. This is consistent with
results of Biemann and Riedl (2013). Regarding
LMI versus frequency, the results using the latter
are slightly better (or with no significant differ-
ence, depending on the frequency range). The ad-
vantage of using frequency instead of LMI is that
it makes the process simpler and faster while lead-
ing to equal or better performance in all frequency
ranges. Therefore for the extrinsic evaluation us-
ing WBST task, we use frequency to select the
p most relevant contexts and then compute Lin’s
similarity using only those contexts.
Figure 3 shows the performance of the thesauri
in the WBST task in terms of precision, recall and
F1.4 For precision, the best filter is to remove con-
</bodyText>
<footnote confidence="0.602835">
4Filters based on LMI and PMI were also tested with the
</footnote>
<bodyText confidence="0.999877142857143">
texts occurring less than th times, but, this also
leads to poor recall, since many verbs are left out
of the thesauri and their WSBT questions cannot
be answered. On the other hand, keeping the most
relevant p contexts leads to more stable results and
when p is high (right plot), they are similar to those
shown in the left plot of Figure 3.
</bodyText>
<subsectionHeader confidence="0.978756">
4.1 Discussion
</subsectionHeader>
<bodyText confidence="0.981012481481481">
The answer to our questions in Section 3 is yes,
more selective filters improve intrinsic and extrin-
sic thesaurus quality. The use of both filtering
methods results in thesauri in which the neighbors
of target verbs are closer in WordNet and get better
scores in TOEFL-like tests. However, the fact that
filtering contexts with frequency under th removes
verbs in the final thesaurus is a drawback, as high-
lighted in the extrinsic evaluation on the WBST
task.
Furthermore, we demonstrated that competitive
results can be obtained keeping only the p most
relevant contexts per verb. On the one hand, this
method leads to much more stable thesauri, with
the same verbs for all values of p. On the other
hand, it is important to highlight that the best re-
sults to assess the relevance of the contexts are ob-
tained using frequency while more sophisticated
filters such as LMI do not improve thesaurus qual-
ity. Although an LMI filter is relatively fast com-
pared to dimensionality reduction techniques such
as singular value decomposition (Landauer and
Dumais, 1997), it is still considerably more expen-
sive than a simple frequency filter.
In short, our experiments indicate that a reason-
same results as intrinsic evaluation: sorting contexts by fre-
quency leads to better results.
</bodyText>
<page confidence="0.996377">
422
</page>
<bodyText confidence="0.999889416666667">
able trade-off between noise, coverage and com-
putational efficiency is obtained for p = 200 most
frequent contexts, as confirmed by intrinsic and
extrinsic evaluation. Frequency threshold th is
not recommended: it degrades recall because the
contexts for many verbs are not frequent enough.
This result is useful for extracting distributional
thesauri from very large corpora like the UKWaC
(Ferraresi et al., 2008) by proposing an alterna-
tive that minimizes the required computational re-
sources while efficiently removing a significant
amount of noise.
</bodyText>
<sectionHeader confidence="0.989168" genericHeader="conclusions">
5 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.9995337">
In this paper we addressed the impact of filters
on the quality of distributional thesauri, evaluat-
ing a set of standard thesauri and different filtering
methods. The results suggest that the use of fil-
ters and their parameters greatly affect the thesauri
generated. We show that it is better to use a filter
that selects the most relevant contexts for a verb
than to simply remove rare contexts. Furthermore,
the best performance was obtained with the sim-
plest method: frequency was found to be a simple
and inexpensive measure of context salience. This
is especially important when dealing with large
amounts of data, since computing LMI for all con-
texts would be computationally costly. With our
proposal to keep just the p most frequent contexts
per verb, a great deal of contexts are cheaply re-
moved and thus the computational power required
for assessing similarity is drastically reduced.
As future work, we plan to use these filters to
build thesauri from larger corpora. We would like
to generalize our findings to other syntactic con-
figurations (e.g. noun-adjective) as well as to other
similarity and informativeness measures. For in-
stance, ongoing experiments indicate that the same
parameters apply when Lin’s similarity is replaced
by cosine. Finally, we would like to compare the
proposed heuristics with more sophisticated filter-
ing strategies like singular value decomposition
(Landauer and Dumais, 1997) and non-negative
matrix factorization (Van de Cruys, 2009).
</bodyText>
<sectionHeader confidence="0.998295" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9951074">
We would like to thank the support of projects
CAPES/COFECUB 707/11, PNPD 2484/2009,
FAPERGS-INRIA 1706-2551/13-7, CNPq
312184/2012-3, 551964/2011-1, 482520/2012-4
and 312077/2012-2.
</bodyText>
<sectionHeader confidence="0.988042" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99940487037037">
Mohit Bansal, John DeNero, and Dekang Lin. 2012.
Unsupervised translation sense clustering. In Pro-
ceedings of the 2012 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
773–782, Montr´eal, Canada, June. Association for
Computational Linguistics.
Chris Biemann and Martin Riedl. 2013. Text: Now
in 2D! a framework for lexical expansion with con-
textual similarity. Journal of Language Modelling,
1(1).
Ted Briscoe, John Carroll, and Rebecca Watson. 2006.
The second release of the RASP system. In James
Curran, editor, Proc. of the COLING/ACL 2006 In-
teractive Presentation Sessions, pages 77–80, Sid-
ney, Australia, Jul. ACL.
Bartosz Broda, Maciej Piasecki, and Stan Szpakow-
icz. 2009. Rank-based transformation in mea-
suring semantic relatedness. In Proceedings of
the 22nd Canadian Conference on Artificial Intel-
ligence: Advances in Artificial Intelligence, Cana-
dian AI ’09, pages 187–190, Berlin, Heidelberg.
Springer-Verlag.
Lou Burnard. 2007. User Reference Guide for the
British National Corpus. Technical report, Oxford
University Computing Services, Feb.
James R. Curran and Marc Moens. 2002. Improve-
ments in automatic thesaurus extraction. In Proc.of
the ACL 2002 Workshop on Unsupervised Lexical
Acquisition, pages 59–66, Philadelphia, Pennsylva-
nia, USA. ACL.
Katrin Erk and Sebastian Pado. 2010. Exemplar-based
models for word meaning in context. In Proc. of the
ACL 2010 Conference Short Papers, pages 92–97,
Uppsala, Sweden, Jun. ACL.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database (Language, Speech, and
Communication). MIT Press, May. 423 p.
Adriano Ferraresi, Eros Zanchetta, Marco Baroni, and
Silvia Bernardini. 2008. Introducing and evaluat-
ing UKWaC, a very large web-derived corpus of En-
glish. In In Proceedings of the 4th Web as Corpus
Workshop (WAC-4.
Olivier Ferret. 2010. Testing semantic similarity mea-
sures for extracting synonyms from a corpus. In
Proc. of the Seventh LREC (LREC 2010), pages
3338–3343, Valetta, Malta, May. ELRA.
Olivier Ferret. 2012. Combining bootstrapping and
feature selection for improving a distributional the-
saurus. In ECAI, pages 336–341.
Olivier Ferret. 2013. Identifying bad semantic neigh-
bors for improving distributional thesauri. In Proc.
of the 51st ACL (Volume 1: Long Papers), pages
561–571, Sofia, Bulgaria, Aug. ACL.
</reference>
<page confidence="0.991001">
423
</page>
<reference confidence="0.999784816326531">
Dayne Freitag, Matthias Blume, John Byrnes, Ed-
mond Chow, Sadik Kapadia, Richard Rohwer, and
Zhiqiang Wang. 2005. New experiments in distri-
butional representations of synonymy. In Ido Dagan
and Dan Gildea, editors, Proc. of the Ninth CoNLL
(CoNLL-2005), pages 25–32, University of Michi-
gan, MI, USA, Jun. ACL.
Gregory Grefenstette. 1994. Explorations in Au-
tomatic Thesaurus Discovery. Springer, Norwell,
MA, USA.
Thomas K Landauer and Susan T. Dumais. 1997. A
solution to platos problem: The latent semantic anal-
ysis theory of acquisition, induction, and represen-
tation of knowledge. Psychological review, pages
211–240.
Dekang Lin. 1998. Automatic retrieval and cluster-
ing of similar words. In Proc. of the 36th ACL and
17th COLING, Volume 2, pages 768–774, Montreal,
Quebec, Canada, Aug. ACL.
Diana McCarthy and Roberto Navigli. 2009. The en-
glish lexical substitution task. Language Resources
and Evaluation, 43(2):139–159.
Diana McCarthy, Bill Keller, and John Carroll.
2003. Detecting a continuum of compositionality
in phrasal verbs. In Francis Bond, Anna Korhonen,
Diana McCarthy, and Aline Villavicencio, editors,
Proc. of the ACL Workshop on MWEs: Analysis, Ac-
quisition and Treatment (MWE 2003), pages 73–80,
Sapporo, Japan, Jul. ACL.
Maciej Piasecki, Stanislaw Szpakowicz, and Bartosz
Broda. 2007. Automatic selection of heterogeneous
syntactic features in semantic similarity of polish
nouns. In Proceedings of the 10th international
conference on Text, speech and dialogue, TSD’07,
pages 99–106, Berlin, Heidelberg. Springer-Verlag.
Peter D. Turney. 2013. Distributional semantics be-
yond words: Supervised learning of analogy and
paraphrase. 1:353–366.
Tim Van de Cruys. 2009. A non-negative tensor factor-
ization model for selectional preference induction.
In Proceedings of the Workshop on Geometrical
Models of Natural Language Semantics, pages 83–
90, Athens, Greece, March. Association for Compu-
tational Linguistics.
Julie Weeds, David Weir, and Diana McCarthy. 2004.
Characterising measures of lexical distributional
similarity. In Proc. of the 20th COLING (COL-
ING 2004), pages 1015–1021, Geneva, Switzerland,
Aug. ICCL.
</reference>
<page confidence="0.999083">
424
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.761392">
<title confidence="0.9931965">Nothing like Good Old Studying Context Filters for Distributional Thesauri</title>
<author confidence="0.9573505">Marco Carlos Aline of Informatics</author>
<author confidence="0.9573505">Federal University of Rio Grande do Sul</author>
<affiliation confidence="0.983086">of Physics, Federal University of Rio Grande do Sul (Brazil)</affiliation>
<address confidence="0.943076">Aix Marseille Universit´e, CNRS, LIF UMR 7279, 13288, Marseille</address>
<abstract confidence="0.991107">Much attention has been given to the impact of informativeness and similarity measures on distributional thesauri. We investigate the effects of context filters on thesaurus quality and propose the use of cooccurrence frequency as a simple and inexpensive criterion. For evaluation, we measure thesaurus agreement with WordNet and performance in answering TOEFL-like questions. Results illustrate the sensitivity of distributional thesauri to filters.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Mohit Bansal</author>
<author>John DeNero</author>
<author>Dekang Lin</author>
</authors>
<title>Unsupervised translation sense clustering.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>773--782</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montr´eal, Canada,</location>
<contexts>
<context position="6302" citStr="Bansal et al., 2012" startWordPosition="969" endWordPosition="972">ed from syntactic dependencies generated by RASP (Briscoe et al., 2006), using nouns (heads of NPs) which have subject and direct object relations with the target verb. Thus, each target verb is represented by a set of triples containing (i) the verb itself, (ii) a context noun and (iii) a syntactic relation (object, subject). The thesauri were constructed using Lin’s (1998) method. Lin’s version of the distributional hypothesis states that two words (verbs v1 and v2 in our case) are similar if they share a large proportion of contexts weighted by their information content, assessed with PMI (Bansal et al., 2012; Turney, 2013). In the literature, little attention is paid to context filters. To investigate their impact, we compare two kinds of filters, and before calculating similarity using Lin’s measure, we apply them to remove 1Even though larger corpora are available, we use a traditional carefully constructed corpus with representative samples of written English to control the quality of the thesaurus. potentially noisy triples: • Threshold (th): we remove triples that occur less than a threshold th. Threshold values vary from 1 to 50 counts per triple. • Relevance (p): we keep only the top p mos</context>
</contexts>
<marker>Bansal, DeNero, Lin, 2012</marker>
<rawString>Mohit Bansal, John DeNero, and Dekang Lin. 2012. Unsupervised translation sense clustering. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 773–782, Montr´eal, Canada, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Biemann</author>
<author>Martin Riedl</author>
</authors>
<title>Text: Now in 2D! a framework for lexical expansion with contextual similarity.</title>
<date>2013</date>
<journal>Journal of Language Modelling,</journal>
<volume>1</volume>
<issue>1</issue>
<contexts>
<context position="1850" citStr="Biemann and Riedl, 2013" startWordPosition="255" endWordPosition="259">es like WordNet (Fellbaum, 1998) are unavailable or lack coverage. To construct a distributional thesaurus, the (collocational or syntactic) contexts in which a target word occurs are used as the basis for calculating its similarity with other words. That is, two words are similar if they share a large proportion of contexts. Much attention has been devoted to refining thesaurus quality, improving informativeness and similarity measures (Lin, 1998; Curran and Moens, 2002; Ferret, 2010), identifying and demoting bad neighbors (Ferret, 2013), or using more relevant contexts (Broda et al., 2009; Biemann and Riedl, 2013). For the latter in particular, as words vary in their collocational tendencies, it is difficult to determine how informative a given context is. To remove uninformative and noisy contexts, filters have often been applied like pointwise mutual information (PMI), lexicographer’s mutual information (LMI) (Biemann and Riedl, 2013), t-score (Piasecki et al., 2007) and z-score (Broda et al., 2009). However, the selection of a measure and of a threshold value for these filters is generally empirically determined. We argue that these filtering parameters have a great influence on the quality of the g</context>
<context position="4835" citStr="Biemann and Riedl (2013)" startWordPosition="732" endWordPosition="735">October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics 1998; Curran and Moens, 2002). For instance, for intrinsic evaluation, the agreement between thesauri has been examined, looking at the average similarity of a word in the thesauri (Lin, 1998), and at the overlap and rank agreement between the thesauri for target words like nouns (Weeds et al., 2004). Although much attention has been given to the evaluation of various informativeness and similarity measures, a careful assessment of the effects of filtering on the resulting thesauri is also needed. For instance, Biemann and Riedl (2013) found that filtering a subset of contexts based on LMI increased the similarity of a thesaurus with WordNet. In this work, we compare the impact of using different types of filters in terms of thesaurus agreement with WordNet, focusing on a distributional thesaurus of English verbs. We also propose a frequency-based saliency measure to rank and filter contexts and compare it with PMI and LMI. Extrinsic evaluation of distributional thesauri has been carried out for tasks such as English lexical substitution (McCarthy and Navigli, 2009), phrasal verb compositionality detection (McCarthy et al.,</context>
<context position="7061" citStr="Biemann and Riedl, 2013" startWordPosition="1095" endWordPosition="1098">ilters, and before calculating similarity using Lin’s measure, we apply them to remove 1Even though larger corpora are available, we use a traditional carefully constructed corpus with representative samples of written English to control the quality of the thesaurus. potentially noisy triples: • Threshold (th): we remove triples that occur less than a threshold th. Threshold values vary from 1 to 50 counts per triple. • Relevance (p): we keep only the top p most relevant contexts for each verb, were relevance is defined according to the following measures: (a) frequency, (b) PMI, and (c) LMI (Biemann and Riedl, 2013). Values of p vary between 10 and 1000. In this work, we want to answer two questions: (a) Do more selective filters improve intrinsic evaluation of thesaurus? and (b) Do they also help in extrinsic evaluation? For intrinsic evaluation, we determine agreement between a distributional thesaurus and WordNet as the path similarities for the first k distributional neighbors of a verb. A single score is obtained by averaging the similarities of all verbs with their k first neighbors. The higher this score is, the closer the neighbors are to the target in WordNet, and the better the thesaurus. Sever</context>
<context position="13683" citStr="Biemann and Riedl (2013)" startWordPosition="2279" endWordPosition="2282">00, though larger values do not lead to a drastic drop in quality. This suggests that the noise introduced by low-frequency contexts is compensated by the increase of informativeness for other contexts. An ideal balance is reached by the lowest possible p that maintains high WordNet similarity, since the lower the p the faster the thesaurus construction. In terms of saliency measure, when keeping only the p most relevant contexts, sorting them with PMI leads to much worse results than LMI or frequency, as PMI gives too much weight to infrequent combinations. This is consistent with results of Biemann and Riedl (2013). Regarding LMI versus frequency, the results using the latter are slightly better (or with no significant difference, depending on the frequency range). The advantage of using frequency instead of LMI is that it makes the process simpler and faster while leading to equal or better performance in all frequency ranges. Therefore for the extrinsic evaluation using WBST task, we use frequency to select the p most relevant contexts and then compute Lin’s similarity using only those contexts. Figure 3 shows the performance of the thesauri in the WBST task in terms of precision, recall and F1.4 For </context>
</contexts>
<marker>Biemann, Riedl, 2013</marker>
<rawString>Chris Biemann and Martin Riedl. 2013. Text: Now in 2D! a framework for lexical expansion with contextual similarity. Journal of Language Modelling, 1(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Briscoe</author>
<author>John Carroll</author>
<author>Rebecca Watson</author>
</authors>
<title>The second release of the RASP system.</title>
<date>2006</date>
<booktitle>Proc. of the COLING/ACL 2006 Interactive Presentation Sessions,</booktitle>
<pages>77--80</pages>
<editor>In James Curran, editor,</editor>
<publisher>ACL.</publisher>
<location>Sidney, Australia,</location>
<contexts>
<context position="5754" citStr="Briscoe et al., 2006" startWordPosition="876" endWordPosition="879">se a frequency-based saliency measure to rank and filter contexts and compare it with PMI and LMI. Extrinsic evaluation of distributional thesauri has been carried out for tasks such as English lexical substitution (McCarthy and Navigli, 2009), phrasal verb compositionality detection (McCarthy et al., 2003) and the WordNet-based synonymy test (WBST) (Freitag et al., 2005). For comparative purposes in this work we adopt the latter. 3 Methodology We focus on thesauri of English verbs constructed from the BNC (Burnard, 2007)1. Contexts are extracted from syntactic dependencies generated by RASP (Briscoe et al., 2006), using nouns (heads of NPs) which have subject and direct object relations with the target verb. Thus, each target verb is represented by a set of triples containing (i) the verb itself, (ii) a context noun and (iii) a syntactic relation (object, subject). The thesauri were constructed using Lin’s (1998) method. Lin’s version of the distributional hypothesis states that two words (verbs v1 and v2 in our case) are similar if they share a large proportion of contexts weighted by their information content, assessed with PMI (Bansal et al., 2012; Turney, 2013). In the literature, little attention</context>
</contexts>
<marker>Briscoe, Carroll, Watson, 2006</marker>
<rawString>Ted Briscoe, John Carroll, and Rebecca Watson. 2006. The second release of the RASP system. In James Curran, editor, Proc. of the COLING/ACL 2006 Interactive Presentation Sessions, pages 77–80, Sidney, Australia, Jul. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bartosz Broda</author>
<author>Maciej Piasecki</author>
<author>Stan Szpakowicz</author>
</authors>
<title>Rank-based transformation in measuring semantic relatedness.</title>
<date>2009</date>
<booktitle>In Proceedings of the 22nd Canadian Conference on Artificial Intelligence: Advances in Artificial Intelligence, Canadian AI ’09,</booktitle>
<pages>187--190</pages>
<publisher>Springer-Verlag.</publisher>
<location>Berlin, Heidelberg.</location>
<contexts>
<context position="1824" citStr="Broda et al., 2009" startWordPosition="251" endWordPosition="254"> constructed resources like WordNet (Fellbaum, 1998) are unavailable or lack coverage. To construct a distributional thesaurus, the (collocational or syntactic) contexts in which a target word occurs are used as the basis for calculating its similarity with other words. That is, two words are similar if they share a large proportion of contexts. Much attention has been devoted to refining thesaurus quality, improving informativeness and similarity measures (Lin, 1998; Curran and Moens, 2002; Ferret, 2010), identifying and demoting bad neighbors (Ferret, 2013), or using more relevant contexts (Broda et al., 2009; Biemann and Riedl, 2013). For the latter in particular, as words vary in their collocational tendencies, it is difficult to determine how informative a given context is. To remove uninformative and noisy contexts, filters have often been applied like pointwise mutual information (PMI), lexicographer’s mutual information (LMI) (Biemann and Riedl, 2013), t-score (Piasecki et al., 2007) and z-score (Broda et al., 2009). However, the selection of a measure and of a threshold value for these filters is generally empirically determined. We argue that these filtering parameters have a great influen</context>
</contexts>
<marker>Broda, Piasecki, Szpakowicz, 2009</marker>
<rawString>Bartosz Broda, Maciej Piasecki, and Stan Szpakowicz. 2009. Rank-based transformation in measuring semantic relatedness. In Proceedings of the 22nd Canadian Conference on Artificial Intelligence: Advances in Artificial Intelligence, Canadian AI ’09, pages 187–190, Berlin, Heidelberg. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lou Burnard</author>
</authors>
<title>User Reference Guide for the British National Corpus.</title>
<date>2007</date>
<tech>Technical report,</tech>
<institution>Oxford University Computing Services,</institution>
<contexts>
<context position="5660" citStr="Burnard, 2007" startWordPosition="864" endWordPosition="865">nt with WordNet, focusing on a distributional thesaurus of English verbs. We also propose a frequency-based saliency measure to rank and filter contexts and compare it with PMI and LMI. Extrinsic evaluation of distributional thesauri has been carried out for tasks such as English lexical substitution (McCarthy and Navigli, 2009), phrasal verb compositionality detection (McCarthy et al., 2003) and the WordNet-based synonymy test (WBST) (Freitag et al., 2005). For comparative purposes in this work we adopt the latter. 3 Methodology We focus on thesauri of English verbs constructed from the BNC (Burnard, 2007)1. Contexts are extracted from syntactic dependencies generated by RASP (Briscoe et al., 2006), using nouns (heads of NPs) which have subject and direct object relations with the target verb. Thus, each target verb is represented by a set of triples containing (i) the verb itself, (ii) a context noun and (iii) a syntactic relation (object, subject). The thesauri were constructed using Lin’s (1998) method. Lin’s version of the distributional hypothesis states that two words (verbs v1 and v2 in our case) are similar if they share a large proportion of contexts weighted by their information conte</context>
</contexts>
<marker>Burnard, 2007</marker>
<rawString>Lou Burnard. 2007. User Reference Guide for the British National Corpus. Technical report, Oxford University Computing Services, Feb.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James R Curran</author>
<author>Marc Moens</author>
</authors>
<title>Improvements in automatic thesaurus extraction.</title>
<date>2002</date>
<booktitle>In Proc.of the ACL 2002 Workshop on Unsupervised Lexical Acquisition,</booktitle>
<pages>59--66</pages>
<publisher>ACL.</publisher>
<location>Philadelphia, Pennsylvania, USA.</location>
<contexts>
<context position="1701" citStr="Curran and Moens, 2002" startWordPosition="232" endWordPosition="235"> 2004; Ferret, 2012) are an inexpensive and fast alternative for representing semantic relatedness between words, when manually constructed resources like WordNet (Fellbaum, 1998) are unavailable or lack coverage. To construct a distributional thesaurus, the (collocational or syntactic) contexts in which a target word occurs are used as the basis for calculating its similarity with other words. That is, two words are similar if they share a large proportion of contexts. Much attention has been devoted to refining thesaurus quality, improving informativeness and similarity measures (Lin, 1998; Curran and Moens, 2002; Ferret, 2010), identifying and demoting bad neighbors (Ferret, 2013), or using more relevant contexts (Broda et al., 2009; Biemann and Riedl, 2013). For the latter in particular, as words vary in their collocational tendencies, it is difficult to determine how informative a given context is. To remove uninformative and noisy contexts, filters have often been applied like pointwise mutual information (PMI), lexicographer’s mutual information (LMI) (Biemann and Riedl, 2013), t-score (Piasecki et al., 2007) and z-score (Broda et al., 2009). However, the selection of a measure and of a threshold</context>
<context position="4323" citStr="Curran and Moens, 2002" startWordPosition="649" endWordPosition="652">ch the target appears (Lin, 1998; McCarthy et al., 2003; Weeds et al., 2004). The informativeness of each context is calculated using measures like PMI, and t-test while the similarity between contexts is calculated using measures like Lin’s (1998), cosine, Jensen-Shannon divergence, Dice or Jaccard. Evaluation of the quality of distributional thesauri is a well know problem in the area (Lin, 419 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 419–424, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics 1998; Curran and Moens, 2002). For instance, for intrinsic evaluation, the agreement between thesauri has been examined, looking at the average similarity of a word in the thesauri (Lin, 1998), and at the overlap and rank agreement between the thesauri for target words like nouns (Weeds et al., 2004). Although much attention has been given to the evaluation of various informativeness and similarity measures, a careful assessment of the effects of filtering on the resulting thesauri is also needed. For instance, Biemann and Riedl (2013) found that filtering a subset of contexts based on LMI increased the similarity of a th</context>
</contexts>
<marker>Curran, Moens, 2002</marker>
<rawString>James R. Curran and Marc Moens. 2002. Improvements in automatic thesaurus extraction. In Proc.of the ACL 2002 Workshop on Unsupervised Lexical Acquisition, pages 59–66, Philadelphia, Pennsylvania, USA. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Erk</author>
<author>Sebastian Pado</author>
</authors>
<title>Exemplar-based models for word meaning in context.</title>
<date>2010</date>
<booktitle>In Proc. of the ACL 2010 Conference Short Papers,</booktitle>
<pages>92--97</pages>
<publisher>ACL.</publisher>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="3651" citStr="Erk and Pado, 2010" startWordPosition="547" endWordPosition="550">he results are discussed in §3 and §4. We finish with some conclusions and discussion of future work. 2 Related Work In a nutshell, the standard approach to build a distributional thesaurus consists of: (i) the extraction of contexts for the target words from corpora, (ii) the application of an informativeness measure to represent these contexts and (iii) the application of a similarity measure to compare sets of contexts. The contexts in which a target word appears can be extracted in terms of a window of cooccurring (content) words surrounding the target (Freitag et al., 2005; Ferret, 2012; Erk and Pado, 2010) or in terms of the syntactic dependencies in which the target appears (Lin, 1998; McCarthy et al., 2003; Weeds et al., 2004). The informativeness of each context is calculated using measures like PMI, and t-test while the similarity between contexts is calculated using measures like Lin’s (1998), cosine, Jensen-Shannon divergence, Dice or Jaccard. Evaluation of the quality of distributional thesauri is a well know problem in the area (Lin, 419 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 419–424, October 25-29, 2014, Doha, Qatar. c�2014</context>
</contexts>
<marker>Erk, Pado, 2010</marker>
<rawString>Katrin Erk and Sebastian Pado. 2010. Exemplar-based models for word meaning in context. In Proc. of the ACL 2010 Conference Short Papers, pages 92–97, Uppsala, Sweden, Jun. ACL.</rawString>
</citation>
<citation valid="true">
<title>WordNet: An Electronic Lexical Database (Language, Speech, and Communication).</title>
<date>1998</date>
<volume>423</volume>
<pages>p.</pages>
<editor>Christiane Fellbaum, editor.</editor>
<publisher>MIT Press,</publisher>
<contexts>
<context position="3948" citStr="(1998)" startWordPosition="598" endWordPosition="598">s measure to represent these contexts and (iii) the application of a similarity measure to compare sets of contexts. The contexts in which a target word appears can be extracted in terms of a window of cooccurring (content) words surrounding the target (Freitag et al., 2005; Ferret, 2012; Erk and Pado, 2010) or in terms of the syntactic dependencies in which the target appears (Lin, 1998; McCarthy et al., 2003; Weeds et al., 2004). The informativeness of each context is calculated using measures like PMI, and t-test while the similarity between contexts is calculated using measures like Lin’s (1998), cosine, Jensen-Shannon divergence, Dice or Jaccard. Evaluation of the quality of distributional thesauri is a well know problem in the area (Lin, 419 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 419–424, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics 1998; Curran and Moens, 2002). For instance, for intrinsic evaluation, the agreement between thesauri has been examined, looking at the average similarity of a word in the thesauri (Lin, 1998), and at the overlap and rank agreement between the thesauri f</context>
<context position="6060" citStr="(1998)" startWordPosition="931" endWordPosition="931">Net-based synonymy test (WBST) (Freitag et al., 2005). For comparative purposes in this work we adopt the latter. 3 Methodology We focus on thesauri of English verbs constructed from the BNC (Burnard, 2007)1. Contexts are extracted from syntactic dependencies generated by RASP (Briscoe et al., 2006), using nouns (heads of NPs) which have subject and direct object relations with the target verb. Thus, each target verb is represented by a set of triples containing (i) the verb itself, (ii) a context noun and (iii) a syntactic relation (object, subject). The thesauri were constructed using Lin’s (1998) method. Lin’s version of the distributional hypothesis states that two words (verbs v1 and v2 in our case) are similar if they share a large proportion of contexts weighted by their information content, assessed with PMI (Bansal et al., 2012; Turney, 2013). In the literature, little attention is paid to context filters. To investigate their impact, we compare two kinds of filters, and before calculating similarity using Lin’s measure, we apply them to remove 1Even though larger corpora are available, we use a traditional carefully constructed corpus with representative samples of written Engl</context>
</contexts>
<marker>1998</marker>
<rawString>Christiane Fellbaum, editor. 1998. WordNet: An Electronic Lexical Database (Language, Speech, and Communication). MIT Press, May. 423 p.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adriano Ferraresi</author>
<author>Eros Zanchetta</author>
<author>Marco Baroni</author>
<author>Silvia Bernardini</author>
</authors>
<title>Introducing and evaluating UKWaC, a very large web-derived corpus of English. In</title>
<date>2008</date>
<booktitle>In Proceedings of the 4th Web as Corpus Workshop (WAC-4.</booktitle>
<contexts>
<context position="16435" citStr="Ferraresi et al., 2008" startWordPosition="2740" endWordPosition="2743">ll considerably more expensive than a simple frequency filter. In short, our experiments indicate that a reasonsame results as intrinsic evaluation: sorting contexts by frequency leads to better results. 422 able trade-off between noise, coverage and computational efficiency is obtained for p = 200 most frequent contexts, as confirmed by intrinsic and extrinsic evaluation. Frequency threshold th is not recommended: it degrades recall because the contexts for many verbs are not frequent enough. This result is useful for extracting distributional thesauri from very large corpora like the UKWaC (Ferraresi et al., 2008) by proposing an alternative that minimizes the required computational resources while efficiently removing a significant amount of noise. 5 Conclusions and Future Work In this paper we addressed the impact of filters on the quality of distributional thesauri, evaluating a set of standard thesauri and different filtering methods. The results suggest that the use of filters and their parameters greatly affect the thesauri generated. We show that it is better to use a filter that selects the most relevant contexts for a verb than to simply remove rare contexts. Furthermore, the best performance </context>
</contexts>
<marker>Ferraresi, Zanchetta, Baroni, Bernardini, 2008</marker>
<rawString>Adriano Ferraresi, Eros Zanchetta, Marco Baroni, and Silvia Bernardini. 2008. Introducing and evaluating UKWaC, a very large web-derived corpus of English. In In Proceedings of the 4th Web as Corpus Workshop (WAC-4.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olivier Ferret</author>
</authors>
<title>Testing semantic similarity measures for extracting synonyms from a corpus.</title>
<date>2010</date>
<booktitle>In Proc. of the Seventh LREC (LREC 2010),</booktitle>
<pages>3338--3343</pages>
<publisher>ELRA.</publisher>
<location>Valetta, Malta,</location>
<contexts>
<context position="1716" citStr="Ferret, 2010" startWordPosition="236" endWordPosition="237"> an inexpensive and fast alternative for representing semantic relatedness between words, when manually constructed resources like WordNet (Fellbaum, 1998) are unavailable or lack coverage. To construct a distributional thesaurus, the (collocational or syntactic) contexts in which a target word occurs are used as the basis for calculating its similarity with other words. That is, two words are similar if they share a large proportion of contexts. Much attention has been devoted to refining thesaurus quality, improving informativeness and similarity measures (Lin, 1998; Curran and Moens, 2002; Ferret, 2010), identifying and demoting bad neighbors (Ferret, 2013), or using more relevant contexts (Broda et al., 2009; Biemann and Riedl, 2013). For the latter in particular, as words vary in their collocational tendencies, it is difficult to determine how informative a given context is. To remove uninformative and noisy contexts, filters have often been applied like pointwise mutual information (PMI), lexicographer’s mutual information (LMI) (Biemann and Riedl, 2013), t-score (Piasecki et al., 2007) and z-score (Broda et al., 2009). However, the selection of a measure and of a threshold value for thes</context>
</contexts>
<marker>Ferret, 2010</marker>
<rawString>Olivier Ferret. 2010. Testing semantic similarity measures for extracting synonyms from a corpus. In Proc. of the Seventh LREC (LREC 2010), pages 3338–3343, Valetta, Malta, May. ELRA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olivier Ferret</author>
</authors>
<title>Combining bootstrapping and feature selection for improving a distributional thesaurus. In</title>
<date>2012</date>
<booktitle>ECAI,</booktitle>
<pages>336--341</pages>
<contexts>
<context position="1099" citStr="Ferret, 2012" startWordPosition="142" endWordPosition="143"> Abstract Much attention has been given to the impact of informativeness and similarity measures on distributional thesauri. We investigate the effects of context filters on thesaurus quality and propose the use of cooccurrence frequency as a simple and inexpensive criterion. For evaluation, we measure thesaurus agreement with WordNet and performance in answering TOEFL-like questions. Results illustrate the sensitivity of distributional thesauri to filters. 1 Introduction Large-scale distributional thesauri created automatically from corpora (Grefenstette, 1994; Lin, 1998; Weeds et al., 2004; Ferret, 2012) are an inexpensive and fast alternative for representing semantic relatedness between words, when manually constructed resources like WordNet (Fellbaum, 1998) are unavailable or lack coverage. To construct a distributional thesaurus, the (collocational or syntactic) contexts in which a target word occurs are used as the basis for calculating its similarity with other words. That is, two words are similar if they share a large proportion of contexts. Much attention has been devoted to refining thesaurus quality, improving informativeness and similarity measures (Lin, 1998; Curran and Moens, 20</context>
<context position="3630" citStr="Ferret, 2012" startWordPosition="545" endWordPosition="546">the work and the results are discussed in §3 and §4. We finish with some conclusions and discussion of future work. 2 Related Work In a nutshell, the standard approach to build a distributional thesaurus consists of: (i) the extraction of contexts for the target words from corpora, (ii) the application of an informativeness measure to represent these contexts and (iii) the application of a similarity measure to compare sets of contexts. The contexts in which a target word appears can be extracted in terms of a window of cooccurring (content) words surrounding the target (Freitag et al., 2005; Ferret, 2012; Erk and Pado, 2010) or in terms of the syntactic dependencies in which the target appears (Lin, 1998; McCarthy et al., 2003; Weeds et al., 2004). The informativeness of each context is calculated using measures like PMI, and t-test while the similarity between contexts is calculated using measures like Lin’s (1998), cosine, Jensen-Shannon divergence, Dice or Jaccard. Evaluation of the quality of distributional thesauri is a well know problem in the area (Lin, 419 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 419–424, October 25-29, 2014</context>
</contexts>
<marker>Ferret, 2012</marker>
<rawString>Olivier Ferret. 2012. Combining bootstrapping and feature selection for improving a distributional thesaurus. In ECAI, pages 336–341.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olivier Ferret</author>
</authors>
<title>Identifying bad semantic neighbors for improving distributional thesauri.</title>
<date>2013</date>
<booktitle>In Proc. of the 51st ACL (Volume 1: Long Papers),</booktitle>
<pages>561--571</pages>
<publisher>ACL.</publisher>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="1771" citStr="Ferret, 2013" startWordPosition="244" endWordPosition="245">emantic relatedness between words, when manually constructed resources like WordNet (Fellbaum, 1998) are unavailable or lack coverage. To construct a distributional thesaurus, the (collocational or syntactic) contexts in which a target word occurs are used as the basis for calculating its similarity with other words. That is, two words are similar if they share a large proportion of contexts. Much attention has been devoted to refining thesaurus quality, improving informativeness and similarity measures (Lin, 1998; Curran and Moens, 2002; Ferret, 2010), identifying and demoting bad neighbors (Ferret, 2013), or using more relevant contexts (Broda et al., 2009; Biemann and Riedl, 2013). For the latter in particular, as words vary in their collocational tendencies, it is difficult to determine how informative a given context is. To remove uninformative and noisy contexts, filters have often been applied like pointwise mutual information (PMI), lexicographer’s mutual information (LMI) (Biemann and Riedl, 2013), t-score (Piasecki et al., 2007) and z-score (Broda et al., 2009). However, the selection of a measure and of a threshold value for these filters is generally empirically determined. We argue</context>
</contexts>
<marker>Ferret, 2013</marker>
<rawString>Olivier Ferret. 2013. Identifying bad semantic neighbors for improving distributional thesauri. In Proc. of the 51st ACL (Volume 1: Long Papers), pages 561–571, Sofia, Bulgaria, Aug. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dayne Freitag</author>
<author>Matthias Blume</author>
<author>John Byrnes</author>
<author>Edmond Chow</author>
<author>Sadik Kapadia</author>
<author>Richard Rohwer</author>
<author>Zhiqiang Wang</author>
</authors>
<title>New experiments in distributional representations of synonymy.</title>
<date>2005</date>
<booktitle>In Ido Dagan and Dan Gildea, editors, Proc. of the Ninth CoNLL (CoNLL-2005),</booktitle>
<pages>25--32</pages>
<publisher>ACL.</publisher>
<location>University of Michigan, MI, USA,</location>
<contexts>
<context position="3616" citStr="Freitag et al., 2005" startWordPosition="541" endWordPosition="544">ethodology adopted in the work and the results are discussed in §3 and §4. We finish with some conclusions and discussion of future work. 2 Related Work In a nutshell, the standard approach to build a distributional thesaurus consists of: (i) the extraction of contexts for the target words from corpora, (ii) the application of an informativeness measure to represent these contexts and (iii) the application of a similarity measure to compare sets of contexts. The contexts in which a target word appears can be extracted in terms of a window of cooccurring (content) words surrounding the target (Freitag et al., 2005; Ferret, 2012; Erk and Pado, 2010) or in terms of the syntactic dependencies in which the target appears (Lin, 1998; McCarthy et al., 2003; Weeds et al., 2004). The informativeness of each context is calculated using measures like PMI, and t-test while the similarity between contexts is calculated using measures like Lin’s (1998), cosine, Jensen-Shannon divergence, Dice or Jaccard. Evaluation of the quality of distributional thesauri is a well know problem in the area (Lin, 419 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 419–424, Octob</context>
<context position="5507" citStr="Freitag et al., 2005" startWordPosition="837" endWordPosition="840"> LMI increased the similarity of a thesaurus with WordNet. In this work, we compare the impact of using different types of filters in terms of thesaurus agreement with WordNet, focusing on a distributional thesaurus of English verbs. We also propose a frequency-based saliency measure to rank and filter contexts and compare it with PMI and LMI. Extrinsic evaluation of distributional thesauri has been carried out for tasks such as English lexical substitution (McCarthy and Navigli, 2009), phrasal verb compositionality detection (McCarthy et al., 2003) and the WordNet-based synonymy test (WBST) (Freitag et al., 2005). For comparative purposes in this work we adopt the latter. 3 Methodology We focus on thesauri of English verbs constructed from the BNC (Burnard, 2007)1. Contexts are extracted from syntactic dependencies generated by RASP (Briscoe et al., 2006), using nouns (heads of NPs) which have subject and direct object relations with the target verb. Thus, each target verb is represented by a set of triples containing (i) the verb itself, (ii) a context noun and (iii) a syntactic relation (object, subject). The thesauri were constructed using Lin’s (1998) method. Lin’s version of the distributional hy</context>
<context position="7975" citStr="Freitag et al., 2005" startWordPosition="1257" endWordPosition="1260">s and WordNet as the path similarities for the first k distributional neighbors of a verb. A single score is obtained by averaging the similarities of all verbs with their k first neighbors. The higher this score is, the closer the neighbors are to the target in WordNet, and the better the thesaurus. Several values of k were tested and the results showed exactly the same curve shapes for all values, with WordNet similarity decreasing linearly with k. For the remainder of the paper we adopt k = 10, as it is widely used in the literature. For extrinsic evaluation, we use the WBST set for verbs (Freitag et al., 2005) with 7,398 questions and an average polysemy of 10.4. The task consists of choosing the most suitable synonym for a word among a set of four options. The thesaurus is used to rank the candidate answers by similarity scores, and select the first one as the correct synonym. As discussed by Freitag et al. (2005), the upper bound reached by English native speakers is 88.4% accuracy, and simple lower bounds are 25% (random choice) and 34.5% (always choosing the most frequent option). 4 Results Figure 1 shows average WordNet similarities for thesauri built filtering by frequency threshold th and by</context>
</contexts>
<marker>Freitag, Blume, Byrnes, Chow, Kapadia, Rohwer, Wang, 2005</marker>
<rawString>Dayne Freitag, Matthias Blume, John Byrnes, Edmond Chow, Sadik Kapadia, Richard Rohwer, and Zhiqiang Wang. 2005. New experiments in distributional representations of synonymy. In Ido Dagan and Dan Gildea, editors, Proc. of the Ninth CoNLL (CoNLL-2005), pages 25–32, University of Michigan, MI, USA, Jun. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregory Grefenstette</author>
</authors>
<title>Explorations in Automatic Thesaurus Discovery.</title>
<date>1994</date>
<publisher>Springer,</publisher>
<location>Norwell, MA, USA.</location>
<contexts>
<context position="1053" citStr="Grefenstette, 1994" startWordPosition="134" endWordPosition="135">amisch@lif.univ-mrs.fr, avillavicencio@inf.ufrgs.br Abstract Much attention has been given to the impact of informativeness and similarity measures on distributional thesauri. We investigate the effects of context filters on thesaurus quality and propose the use of cooccurrence frequency as a simple and inexpensive criterion. For evaluation, we measure thesaurus agreement with WordNet and performance in answering TOEFL-like questions. Results illustrate the sensitivity of distributional thesauri to filters. 1 Introduction Large-scale distributional thesauri created automatically from corpora (Grefenstette, 1994; Lin, 1998; Weeds et al., 2004; Ferret, 2012) are an inexpensive and fast alternative for representing semantic relatedness between words, when manually constructed resources like WordNet (Fellbaum, 1998) are unavailable or lack coverage. To construct a distributional thesaurus, the (collocational or syntactic) contexts in which a target word occurs are used as the basis for calculating its similarity with other words. That is, two words are similar if they share a large proportion of contexts. Much attention has been devoted to refining thesaurus quality, improving informativeness and simila</context>
</contexts>
<marker>Grefenstette, 1994</marker>
<rawString>Gregory Grefenstette. 1994. Explorations in Automatic Thesaurus Discovery. Springer, Norwell, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas K Landauer</author>
<author>Susan T Dumais</author>
</authors>
<title>A solution to platos problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge. Psychological review,</title>
<date>1997</date>
<pages>211--240</pages>
<contexts>
<context position="15801" citStr="Landauer and Dumais, 1997" startWordPosition="2642" endWordPosition="2645">luation on the WBST task. Furthermore, we demonstrated that competitive results can be obtained keeping only the p most relevant contexts per verb. On the one hand, this method leads to much more stable thesauri, with the same verbs for all values of p. On the other hand, it is important to highlight that the best results to assess the relevance of the contexts are obtained using frequency while more sophisticated filters such as LMI do not improve thesaurus quality. Although an LMI filter is relatively fast compared to dimensionality reduction techniques such as singular value decomposition (Landauer and Dumais, 1997), it is still considerably more expensive than a simple frequency filter. In short, our experiments indicate that a reasonsame results as intrinsic evaluation: sorting contexts by frequency leads to better results. 422 able trade-off between noise, coverage and computational efficiency is obtained for p = 200 most frequent contexts, as confirmed by intrinsic and extrinsic evaluation. Frequency threshold th is not recommended: it degrades recall because the contexts for many verbs are not frequent enough. This result is useful for extracting distributional thesauri from very large corpora like </context>
</contexts>
<marker>Landauer, Dumais, 1997</marker>
<rawString>Thomas K Landauer and Susan T. Dumais. 1997. A solution to platos problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge. Psychological review, pages 211–240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar words.</title>
<date>1998</date>
<booktitle>In Proc. of the 36th ACL and 17th COLING,</booktitle>
<volume>2</volume>
<pages>768--774</pages>
<publisher>ACL.</publisher>
<location>Montreal, Quebec, Canada,</location>
<contexts>
<context position="1064" citStr="Lin, 1998" startWordPosition="136" endWordPosition="137">fr, avillavicencio@inf.ufrgs.br Abstract Much attention has been given to the impact of informativeness and similarity measures on distributional thesauri. We investigate the effects of context filters on thesaurus quality and propose the use of cooccurrence frequency as a simple and inexpensive criterion. For evaluation, we measure thesaurus agreement with WordNet and performance in answering TOEFL-like questions. Results illustrate the sensitivity of distributional thesauri to filters. 1 Introduction Large-scale distributional thesauri created automatically from corpora (Grefenstette, 1994; Lin, 1998; Weeds et al., 2004; Ferret, 2012) are an inexpensive and fast alternative for representing semantic relatedness between words, when manually constructed resources like WordNet (Fellbaum, 1998) are unavailable or lack coverage. To construct a distributional thesaurus, the (collocational or syntactic) contexts in which a target word occurs are used as the basis for calculating its similarity with other words. That is, two words are similar if they share a large proportion of contexts. Much attention has been devoted to refining thesaurus quality, improving informativeness and similarity measur</context>
<context position="3732" citStr="Lin, 1998" startWordPosition="563" endWordPosition="564">ture work. 2 Related Work In a nutshell, the standard approach to build a distributional thesaurus consists of: (i) the extraction of contexts for the target words from corpora, (ii) the application of an informativeness measure to represent these contexts and (iii) the application of a similarity measure to compare sets of contexts. The contexts in which a target word appears can be extracted in terms of a window of cooccurring (content) words surrounding the target (Freitag et al., 2005; Ferret, 2012; Erk and Pado, 2010) or in terms of the syntactic dependencies in which the target appears (Lin, 1998; McCarthy et al., 2003; Weeds et al., 2004). The informativeness of each context is calculated using measures like PMI, and t-test while the similarity between contexts is calculated using measures like Lin’s (1998), cosine, Jensen-Shannon divergence, Dice or Jaccard. Evaluation of the quality of distributional thesauri is a well know problem in the area (Lin, 419 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 419–424, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics 1998; Curran and Moens, 2002). For ins</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. Automatic retrieval and clustering of similar words. In Proc. of the 36th ACL and 17th COLING, Volume 2, pages 768–774, Montreal, Quebec, Canada, Aug. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diana McCarthy</author>
<author>Roberto Navigli</author>
</authors>
<title>The english lexical substitution task.</title>
<date>2009</date>
<journal>Language Resources and Evaluation,</journal>
<volume>43</volume>
<issue>2</issue>
<contexts>
<context position="5376" citStr="McCarthy and Navigli, 2009" startWordPosition="819" endWordPosition="822">ering on the resulting thesauri is also needed. For instance, Biemann and Riedl (2013) found that filtering a subset of contexts based on LMI increased the similarity of a thesaurus with WordNet. In this work, we compare the impact of using different types of filters in terms of thesaurus agreement with WordNet, focusing on a distributional thesaurus of English verbs. We also propose a frequency-based saliency measure to rank and filter contexts and compare it with PMI and LMI. Extrinsic evaluation of distributional thesauri has been carried out for tasks such as English lexical substitution (McCarthy and Navigli, 2009), phrasal verb compositionality detection (McCarthy et al., 2003) and the WordNet-based synonymy test (WBST) (Freitag et al., 2005). For comparative purposes in this work we adopt the latter. 3 Methodology We focus on thesauri of English verbs constructed from the BNC (Burnard, 2007)1. Contexts are extracted from syntactic dependencies generated by RASP (Briscoe et al., 2006), using nouns (heads of NPs) which have subject and direct object relations with the target verb. Thus, each target verb is represented by a set of triples containing (i) the verb itself, (ii) a context noun and (iii) a sy</context>
</contexts>
<marker>McCarthy, Navigli, 2009</marker>
<rawString>Diana McCarthy and Roberto Navigli. 2009. The english lexical substitution task. Language Resources and Evaluation, 43(2):139–159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diana McCarthy</author>
<author>Bill Keller</author>
<author>John Carroll</author>
</authors>
<title>Detecting a continuum of compositionality in phrasal verbs.</title>
<date>2003</date>
<booktitle>Proc. of the ACL Workshop on MWEs: Analysis, Acquisition and Treatment (MWE 2003),</booktitle>
<pages>73--80</pages>
<editor>In Francis Bond, Anna Korhonen, Diana McCarthy, and Aline Villavicencio, editors,</editor>
<publisher>ACL.</publisher>
<location>Sapporo, Japan,</location>
<contexts>
<context position="3755" citStr="McCarthy et al., 2003" startWordPosition="565" endWordPosition="568">2 Related Work In a nutshell, the standard approach to build a distributional thesaurus consists of: (i) the extraction of contexts for the target words from corpora, (ii) the application of an informativeness measure to represent these contexts and (iii) the application of a similarity measure to compare sets of contexts. The contexts in which a target word appears can be extracted in terms of a window of cooccurring (content) words surrounding the target (Freitag et al., 2005; Ferret, 2012; Erk and Pado, 2010) or in terms of the syntactic dependencies in which the target appears (Lin, 1998; McCarthy et al., 2003; Weeds et al., 2004). The informativeness of each context is calculated using measures like PMI, and t-test while the similarity between contexts is calculated using measures like Lin’s (1998), cosine, Jensen-Shannon divergence, Dice or Jaccard. Evaluation of the quality of distributional thesauri is a well know problem in the area (Lin, 419 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 419–424, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics 1998; Curran and Moens, 2002). For instance, for intrinsic ev</context>
<context position="5441" citStr="McCarthy et al., 2003" startWordPosition="827" endWordPosition="830">and Riedl (2013) found that filtering a subset of contexts based on LMI increased the similarity of a thesaurus with WordNet. In this work, we compare the impact of using different types of filters in terms of thesaurus agreement with WordNet, focusing on a distributional thesaurus of English verbs. We also propose a frequency-based saliency measure to rank and filter contexts and compare it with PMI and LMI. Extrinsic evaluation of distributional thesauri has been carried out for tasks such as English lexical substitution (McCarthy and Navigli, 2009), phrasal verb compositionality detection (McCarthy et al., 2003) and the WordNet-based synonymy test (WBST) (Freitag et al., 2005). For comparative purposes in this work we adopt the latter. 3 Methodology We focus on thesauri of English verbs constructed from the BNC (Burnard, 2007)1. Contexts are extracted from syntactic dependencies generated by RASP (Briscoe et al., 2006), using nouns (heads of NPs) which have subject and direct object relations with the target verb. Thus, each target verb is represented by a set of triples containing (i) the verb itself, (ii) a context noun and (iii) a syntactic relation (object, subject). The thesauri were constructed</context>
</contexts>
<marker>McCarthy, Keller, Carroll, 2003</marker>
<rawString>Diana McCarthy, Bill Keller, and John Carroll. 2003. Detecting a continuum of compositionality in phrasal verbs. In Francis Bond, Anna Korhonen, Diana McCarthy, and Aline Villavicencio, editors, Proc. of the ACL Workshop on MWEs: Analysis, Acquisition and Treatment (MWE 2003), pages 73–80, Sapporo, Japan, Jul. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maciej Piasecki</author>
<author>Stanislaw Szpakowicz</author>
<author>Bartosz Broda</author>
</authors>
<title>Automatic selection of heterogeneous syntactic features in semantic similarity of polish nouns.</title>
<date>2007</date>
<booktitle>In Proceedings of the 10th international conference on Text, speech and dialogue, TSD’07,</booktitle>
<pages>99--106</pages>
<publisher>Springer-Verlag.</publisher>
<location>Berlin, Heidelberg.</location>
<contexts>
<context position="2212" citStr="Piasecki et al., 2007" startWordPosition="309" endWordPosition="312">efining thesaurus quality, improving informativeness and similarity measures (Lin, 1998; Curran and Moens, 2002; Ferret, 2010), identifying and demoting bad neighbors (Ferret, 2013), or using more relevant contexts (Broda et al., 2009; Biemann and Riedl, 2013). For the latter in particular, as words vary in their collocational tendencies, it is difficult to determine how informative a given context is. To remove uninformative and noisy contexts, filters have often been applied like pointwise mutual information (PMI), lexicographer’s mutual information (LMI) (Biemann and Riedl, 2013), t-score (Piasecki et al., 2007) and z-score (Broda et al., 2009). However, the selection of a measure and of a threshold value for these filters is generally empirically determined. We argue that these filtering parameters have a great influence on the quality of the generated thesauri. The goal of this paper is to quantify the impact of context filters on distributional thesauri. We experiment with different filter methods and measures to assess context significance. We propose the use of simple cooccurrence frequency as a filter and show that it leads to better results than more expensive measures such as LMI or PMI. Thus</context>
</contexts>
<marker>Piasecki, Szpakowicz, Broda, 2007</marker>
<rawString>Maciej Piasecki, Stanislaw Szpakowicz, and Bartosz Broda. 2007. Automatic selection of heterogeneous syntactic features in semantic similarity of polish nouns. In Proceedings of the 10th international conference on Text, speech and dialogue, TSD’07, pages 99–106, Berlin, Heidelberg. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>Distributional semantics beyond words: Supervised learning of analogy and paraphrase.</title>
<date>2013</date>
<pages>1--353</pages>
<contexts>
<context position="6317" citStr="Turney, 2013" startWordPosition="973" endWordPosition="974">endencies generated by RASP (Briscoe et al., 2006), using nouns (heads of NPs) which have subject and direct object relations with the target verb. Thus, each target verb is represented by a set of triples containing (i) the verb itself, (ii) a context noun and (iii) a syntactic relation (object, subject). The thesauri were constructed using Lin’s (1998) method. Lin’s version of the distributional hypothesis states that two words (verbs v1 and v2 in our case) are similar if they share a large proportion of contexts weighted by their information content, assessed with PMI (Bansal et al., 2012; Turney, 2013). In the literature, little attention is paid to context filters. To investigate their impact, we compare two kinds of filters, and before calculating similarity using Lin’s measure, we apply them to remove 1Even though larger corpora are available, we use a traditional carefully constructed corpus with representative samples of written English to control the quality of the thesaurus. potentially noisy triples: • Threshold (th): we remove triples that occur less than a threshold th. Threshold values vary from 1 to 50 counts per triple. • Relevance (p): we keep only the top p most relevant cont</context>
</contexts>
<marker>Turney, 2013</marker>
<rawString>Peter D. Turney. 2013. Distributional semantics beyond words: Supervised learning of analogy and paraphrase. 1:353–366.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tim Van de Cruys</author>
</authors>
<title>A non-negative tensor factorization model for selectional preference induction.</title>
<date>2009</date>
<booktitle>In Proceedings of the Workshop on Geometrical Models of Natural Language Semantics,</booktitle>
<pages>83--90</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Athens, Greece,</location>
<marker>Van de Cruys, 2009</marker>
<rawString>Tim Van de Cruys. 2009. A non-negative tensor factorization model for selectional preference induction. In Proceedings of the Workshop on Geometrical Models of Natural Language Semantics, pages 83– 90, Athens, Greece, March. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julie Weeds</author>
<author>David Weir</author>
<author>Diana McCarthy</author>
</authors>
<title>Characterising measures of lexical distributional similarity.</title>
<date>2004</date>
<booktitle>In Proc. of the 20th COLING (COLING 2004),</booktitle>
<pages>1015--1021</pages>
<publisher>ICCL.</publisher>
<location>Geneva, Switzerland,</location>
<contexts>
<context position="1084" citStr="Weeds et al., 2004" startWordPosition="138" endWordPosition="141">icencio@inf.ufrgs.br Abstract Much attention has been given to the impact of informativeness and similarity measures on distributional thesauri. We investigate the effects of context filters on thesaurus quality and propose the use of cooccurrence frequency as a simple and inexpensive criterion. For evaluation, we measure thesaurus agreement with WordNet and performance in answering TOEFL-like questions. Results illustrate the sensitivity of distributional thesauri to filters. 1 Introduction Large-scale distributional thesauri created automatically from corpora (Grefenstette, 1994; Lin, 1998; Weeds et al., 2004; Ferret, 2012) are an inexpensive and fast alternative for representing semantic relatedness between words, when manually constructed resources like WordNet (Fellbaum, 1998) are unavailable or lack coverage. To construct a distributional thesaurus, the (collocational or syntactic) contexts in which a target word occurs are used as the basis for calculating its similarity with other words. That is, two words are similar if they share a large proportion of contexts. Much attention has been devoted to refining thesaurus quality, improving informativeness and similarity measures (Lin, 1998; Curra</context>
<context position="3776" citStr="Weeds et al., 2004" startWordPosition="569" endWordPosition="572">shell, the standard approach to build a distributional thesaurus consists of: (i) the extraction of contexts for the target words from corpora, (ii) the application of an informativeness measure to represent these contexts and (iii) the application of a similarity measure to compare sets of contexts. The contexts in which a target word appears can be extracted in terms of a window of cooccurring (content) words surrounding the target (Freitag et al., 2005; Ferret, 2012; Erk and Pado, 2010) or in terms of the syntactic dependencies in which the target appears (Lin, 1998; McCarthy et al., 2003; Weeds et al., 2004). The informativeness of each context is calculated using measures like PMI, and t-test while the similarity between contexts is calculated using measures like Lin’s (1998), cosine, Jensen-Shannon divergence, Dice or Jaccard. Evaluation of the quality of distributional thesauri is a well know problem in the area (Lin, 419 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 419–424, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics 1998; Curran and Moens, 2002). For instance, for intrinsic evaluation, the agreeme</context>
</contexts>
<marker>Weeds, Weir, McCarthy, 2004</marker>
<rawString>Julie Weeds, David Weir, and Diana McCarthy. 2004. Characterising measures of lexical distributional similarity. In Proc. of the 20th COLING (COLING 2004), pages 1015–1021, Geneva, Switzerland, Aug. ICCL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.992931">
A Fast and Accurate Dependency Parser using Neural Networks
</title>
<author confidence="0.997475">
Danqi Chen Christopher D. Manning
</author>
<affiliation confidence="0.999351">
Computer Science Department Computer Science Department
Stanford University Stanford University
</affiliation>
<email confidence="0.996758">
danqi@cs.stanford.edu manning@stanford.edu
</email>
<sectionHeader confidence="0.993834" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999607277777778">
Almost all current dependency parsers
classify based on millions of sparse indi-
cator features. Not only do these features
generalize poorly, but the cost of feature
computation restricts parsing speed signif-
icantly. In this work, we propose a novel
way of learning a neural network classifier
for use in a greedy, transition-based depen-
dency parser. Because this classifier learns
and uses just a small number of dense fea-
tures, it can work very fast, while achiev-
ing an about 2% improvement in unla-
beled and labeled attachment scores on
both English and Chinese datasets. Con-
cretely, our parser is able to parse more
than 1000 sentences per second at 92.2%
unlabeled attachment score on the English
Penn Treebank.
</bodyText>
<sectionHeader confidence="0.99899" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999975163934427">
In recent years, enormous parsing success has
been achieved by the use of feature-based discrim-
inative dependency parsers (K¨ubler et al., 2009).
In particular, for practical applications, the speed
of the subclass of transition-based dependency
parsers has been very appealing.
However, these parsers are not perfect. First,
from a statistical perspective, these parsers suffer
from the use of millions of mainly poorly esti-
mated feature weights. While in aggregate both
lexicalized features and higher-order interaction
term features are very important in improving the
performance of these systems, nevertheless, there
is insufficient data to correctly weight most such
features. For this reason, techniques for introduc-
ing higher-support features such as word class fea-
tures have also been very successful in improving
parsing performance (Koo et al., 2008). Second,
almost all existing parsers rely on a manually de-
signed set of feature templates, which require a lot
of expertise and are usually incomplete. Third, the
use of many feature templates cause a less stud-
ied problem: in modern dependency parsers, most
of the runtime is consumed not by the core pars-
ing algorithm but in the feature extraction step (He
et al., 2013). For instance, Bohnet (2010) reports
that his baseline parser spends 99% of its time do-
ing feature extraction, despite that being done in
standard efficient ways.
In this work, we address all of these problems
by using dense features in place of the sparse indi-
cator features. This is inspired by the recent suc-
cess of distributed word representations in many
NLP tasks, e.g., POS tagging (Collobert et al.,
2011), machine translation (Devlin et al., 2014),
and constituency parsing (Socher et al., 2013).
Low-dimensional, dense word embeddings can ef-
fectively alleviate sparsity by sharing statistical
strength between similar words, and can provide
us a good starting point to construct features of
words and their interactions.
Nevertheless, there remain challenging prob-
lems of how to encode all the available infor-
mation from the configuration and how to model
higher-order features based on the dense repre-
sentations. In this paper, we train a neural net-
work classifier to make parsing decisions within
a transition-based dependency parser. The neu-
ral network learns compact dense vector represen-
tations of words, part-of-speech (POS) tags, and
dependency labels. This results in a fast, com-
pact classifier, which uses only 200 learned dense
features while yielding good gains in parsing ac-
curacy and speed on two languages (English and
Chinese) and two different dependency represen-
tations (CoNLL and Stanford dependencies). The
main contributions of this work are: (i) showing
the usefulness of dense representations that are
learned within the parsing task, (ii) developing a
neural network architecture that gives good accu-
racy and speed, and (iii) introducing a novel acti-
</bodyText>
<page confidence="0.950362">
740
</page>
<note confidence="0.9099315">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 740–750,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.993057">
vation function for the neural network that better
captures higher-order interaction features.
</bodyText>
<sectionHeader confidence="0.85155" genericHeader="method">
2 Transition-based Dependency Parsing
</sectionHeader>
<bodyText confidence="0.999262148148148">
Transition-based dependency parsing aims to pre-
dict a transition sequence from an initial configu-
ration to some terminal configuration, which de-
rives a target dependency parse tree, as shown in
Figure 1. In this paper, we examine only greedy
parsing, which uses a classifier to predict the cor-
rect transition based on features extracted from the
configuration. This class of parsers is of great in-
terest because of their efficiency, although they
tend to perform slightly worse than the search-
based parsers because of subsequent error prop-
agation. However, our greedy parser can achieve
comparable accuracy with a very good speed.1
As the basis of our parser, we employ the
arc-standard system (Nivre, 2004), one of the
most popular transition systems. In the arc-
standard system, a configuration c = (s, b, A)
consists of a stack s, a buffer b, and a set of
dependency arcs A. The initial configuration
for a sentence w1, ... , wn is s = [ROOT], b =
[w1, ... , wn], A = 0. A configuration c is termi-
nal if the buffer is empty and the stack contains
the single node ROOT, and the parse tree is given
by Ac. Denoting si (i = 1, 2, ...) as the ith top
element on the stack, and bi (i = 1, 2, ...) as the
ith element on the buffer, the arc-standard system
defines three types of transitions:
</bodyText>
<listItem confidence="0.999921125">
• LEFT-ARC(l): adds an arc s1 -* s2 with
label l and removes s2 from the stack. Pre-
condition: |s |&gt; 2.
• RIGHT-ARC(l): adds an arc s2 -* s1 with
label l and removes s1 from the stack. Pre-
condition: |s |&gt; 2.
• SHIFT: moves b1 from the buffer to the
stack. Precondition: |b |&gt; 1.
</listItem>
<bodyText confidence="0.974900777777778">
In the labeled version of parsing, there are in total
|T  |= 2Nl + 1 transitions, where Nl is number
of different arc labels. Figure 1 illustrates an ex-
ample of one transition sequence from the initial
configuration to a terminal one.
The essential goal of a greedy parser is to pre-
dict a correct transition from T, based on one
1Additionally, our parser can be naturally incorporated
with beam search, but we leave this to future work.
</bodyText>
<equation confidence="0.994752">
Single-word features (9)
s1.w; s1.t; s1.wt; s2.w; s2.t;
s2.wt; b1.w; b1.t; b1.wt
Word-pair features (8)
s1.wt o s2.wt; s1.wt o s2.w; s1.wts2.t;
s1.w o s2.wt; s1.t o s2.wt; s1.w o s2.w
s1.t o s2.t; s1.t o b1.t
Three-word feaures (8)
</equation>
<tableCaption confidence="0.729872125">
s2.t o s1.t o b1.t; s2.t o s1.t o lc1(s1).t;
s2.t o s1.t o rc1(s1).t; s2.t o s1.t o lc1(s2).t;
s2.t o s1.t o rc1(s2).t; s2.t o s1.w o rc1(s2).t;
s2.t o s1.w o lc1(s1).t; s2.t o s1.w o b1.t
Table 1: The feature templates used for analysis.
lc1(si) and rc1(si) denote the leftmost and right-
most children of si, w denotes word, t denotes
POS tag.
</tableCaption>
<bodyText confidence="0.992249033333333">
given configuration. Information that can be ob-
tained from one configuration includes: (1) all
the words and their corresponding POS tags (e.g.,
has / VBZ); (2) the head of a word and its label
(e.g., nsubj, dobj) if applicable; (3) the posi-
tion of a word on the stack/buffer or whether it has
already been removed from the stack.
Conventional approaches extract indicator fea-
tures such as the conjunction of 1 - 3 elements
from the stack/buffer using their words, POS tags
or arc labels. Table 1 lists a typical set of feature
templates chosen from the ones of (Huang et al.,
2009; Zhang and Nivre, 2011).2 These features
suffer from the following problems:
• Sparsity. The features, especially lexicalized
features are highly sparse, and this is a com-
mon problem in many NLP tasks. The sit-
uation is severe in dependency parsing, be-
cause it depends critically on word-to-word
interactions and thus the high-order features.
To give a better understanding, we perform a
feature analysis using the features in Table 1
on the English Penn Treebank (CoNLL rep-
resentations). The results given in Table 2
demonstrate that: (1) lexicalized features are
indispensable; (2) Not only are the word-pair
features (especially s1 and s2) vital for pre-
dictions, the three-word conjunctions (e.g.,
{s2, s1, b1}, {s2, lc1(s1), s1}) are also very
important.
</bodyText>
<footnote confidence="0.585134">
2We exclude sophisticated features using labels, distance,
valency and third-order features in this analysis, but we will
include all of them in the final evaluation.
</footnote>
<page confidence="0.993092">
741
</page>
<figure confidence="0.79805">
punct
root
nsubi
dobi
amod
ROOT He has good control .
PRP VBZ JJ NN .
Correct transition: SHIFT
Stack Bu↵er
ROOT has VBZ good JJ
control NN . .
He PRP
</figure>
<table confidence="0.990138818181818">
Transition Stack Buffer A
[ROOT] [He has good control.] ∅
SHIFT [ROOT He] [has good control.]
SHIFT [ROOT He has] [good control.]
LEFT-ARC(nsubj) [ROOT has] [good control.] AU nsubj(has,He)
SHIFT [ROOT has good] [control .]
SHIFT [ROOT has good control] [.]
LEFT-ARC(amod) [ROOT has control] [.] AUamod(control,good)
RIGHT-ARC(dobj) [ROOT has] [.] AU dobj(has,control)
. . . . . . . . . . . .
RIGHT-ARC(root) [ROOT] [] AUroot(ROOT,has)
</table>
<figureCaption confidence="0.9591895">
Figure 1: An example of transition-based dependency parsing. Above left: a desired dependency tree,
above right: an intermediate configuration, bottom: a transition sequence of the arc-standard system.
</figureCaption>
<table confidence="0.9990384">
Features UAS
All features in Table 1 88.0
single-word &amp; word-pair features 82.7
only single-word features 76.9
excluding all lexicalized features 81.5
</table>
<tableCaption confidence="0.981657">
Table 2: Performance of different feature sets.
UAS: unlabeled attachment score.
</tableCaption>
<listItem confidence="0.944889263157895">
• Incompleteness. Incompleteness is an un-
avoidable issue in all existing feature tem-
plates. Because even with expertise and man-
ual handling involved, they still do not in-
clude the conjunction of every useful word
combination. For example, the conjunc-
tion of s1 and b2 is omitted in almost all
commonly used feature templates, however
it could indicate that we cannot perform a
RIGHT-ARC action if there is an arc from s1
to b2.
• Expensive feature computation. The fea-
ture generation of indicator features is gen-
erally expensive — we have to concatenate
some words, POS tags, or arc labels for gen-
erating feature strings, and look them up in a
huge table containing several millions of fea-
tures. In our experiments, more than 95% of
1
</listItem>
<bodyText confidence="0.9893905">
the time is consumed by feature computation
during the parsing process.
So far, we have discussed preliminaries of
transition-based dependency parsing and existing
problems of sparse indicator features. In the fol-
lowing sections, we will elaborate our neural net-
work model for learning dense features along with
experimental evaluations that prove its efficiency.
</bodyText>
<sectionHeader confidence="0.998257" genericHeader="method">
3 Neural Network Based Parser
</sectionHeader>
<bodyText confidence="0.955416">
In this section, we first present our neural network
</bodyText>
<equation confidence="0.638482">
1
</equation>
<bodyText confidence="0.9913205">
model and its main components. Later, we give
details of training and speedup of parsing process.
</bodyText>
<subsectionHeader confidence="0.998106">
3.1 Model
</subsectionHeader>
<bodyText confidence="0.999876722222222">
Figure 2 describes our neural network architec-
ture. First, as usual word embeddings, we repre-
sent each word as a d-dimensional vector ewi E Rd
and the full embedding matrix is Ew E Rd×Nw
where Nw is the dictionary size. Meanwhile,
we also map POS tags and arc labels to a d-
dimensional vector space, where eti, elj E Rd are
the representations of ith POS tag and jth arc la-
bel. Correspondingly, the POS and label embed-
ding matrices are Et E Rd×Nt and El E Rd×Nl
where Nt and Nl are the number of distinct POS
tags and arc labels.
We choose a set of elements based on the
stack / buffer positions for each type of in-
formation (word, POS or label), which might
be useful for our predictions. We denote the
sets as 5w, 5t, 5l respectively. For example,
given the configuration in Figure 2 and 5t =
</bodyText>
<page confidence="0.974601">
742
</page>
<figure confidence="0.966369466666667">
words POS tags arc labels
Stack Buffer
Softmax layer:
p = softmax(W2h)
Hidden layer:
h = (W1w xw + Wt1xt + Wl1xl + b1)3
· · ·
· · ·
Input layer: [xw, xt, xl]
· · · · · ·
Configuration
nsubj
He PRP
ROOT has VBZ good JJ
control NN . .
</figure>
<figureCaption confidence="0.999214">
Figure 2: Our neural network architecture.
</figureCaption>
<bodyText confidence="0.965804575757576">
{lc1(s2).t, s2.t, rc1(s2).t, s1.t}, we will extract
PRP, VBZ, NULL, JJ in order. Here we use a spe-
cial token NULL to represent a non-existent ele-
ment.
We build a standard neural network with one
hidden layer, where the corresponding embed-
dings of our chosen elements from Sw, St, Sl will
be added to the input layer. Denoting nw, nt, nl as
the number of chosen elements of each type, we
add xw = [eww1; eww2; ... ewwnw ] to the input layer,
where Sw = {w1, ... , wnw}. Similarly, we add
the POS tag features xt and arc label features xl to
the input layer.
We map the input layer to a hidden layer with
dh nodes through a cube activation function:
h = (W1w xw + Wt1xt + Wl1xl + b1)3
where W1w E Rdhx(d·nw), W1t E Rdhx(d·nt),
W1l E Rdhx(d·nl), and b1 E Rdh is the bias.
A softmax layer is finally added on the top of
the hidden layer for modeling multi-class prob-
abilities p = softmax(W2h), where W2 E
R|T |xdh.
POS and label embeddings
To our best knowledge, this is the first attempt to
introduce POS tag and arc label embeddings in-
stead of discrete representations.
Although the POS tags P = {NN, NNP,
NNS, DT, JJ, ...} (for English) and arc labels
G = {amod, tmod, nsubj, csubj, dobj, ...}
(for Stanford Dependencies on English) are rela-
tively small discrete sets, they still exhibit many
semantical similarities like words. For example,
NN (singular noun) should be closer to NNS (plural
</bodyText>
<figure confidence="0.97973725">
1
0.5
−1 −0.8 −0.6 −0.4 −0.2
cube
sigmoid
tanh
identity
−1
</figure>
<figureCaption confidence="0.98895">
Figure 3: Different activation functions used in
neural networks.
</figureCaption>
<bodyText confidence="0.980225333333333">
noun) than DT (determiner), and amod (adjective
modifier) should be closer to num (numeric mod-
ifier) than nsubj (nominal subject). We expect
these semantic meanings to be effectively captured
by the dense representations.
Cube activation function
As stated above, we introduce a novel activation
function: cube g(x) = x3 in our model instead
of the commonly used tanh or sigmoid functions
(Figure 3).
Intuitively, every hidden unit is computed by a
(non-linear) mapping on a weighted sum of input
units plus a bias. Using g(x) = x3 can model
the product terms of xixjxk for any three different
elements at the input layer directly:
</bodyText>
<equation confidence="0.954668">
g(w1x1 + ... + wmxm + b) =
� �(wiwjwk)xixjxk + b(wiwj)xixj...
i,j,k i,j
</equation>
<bodyText confidence="0.999549666666667">
In our case, xi, xj, xk could come from different
dimensions of three embeddings. We believe that
this better captures the interaction of three ele-
</bodyText>
<figure confidence="0.909577">
−0.5
0.2 0.4 0.6 0.8
1
</figure>
<page confidence="0.993581">
743
</page>
<bodyText confidence="0.998474">
ments, which is a very desired property of depen-
dency parsing.
Experimental results also verify the success of
the cube activation function empirically (see more
comparisons in Section 4). However, the expres-
sive power of this activation function is still open
to investigate theoretically.
The choice of Sw, St, Sl
Following (Zhang and Nivre, 2011), we pick a
rich set of elements for our final parser. In de-
tail, Sw contains nw = 18 elements: (1) The top 3
words on the stack and buffer: s1, s2, s3, b1, b2, b3;
(2) The first and second leftmost / rightmost
children of the top two words on the stack:
lc1(si), rc1(si), lc2(si), rc2(si), i = 1, 2. (3)
The leftmost of leftmost / rightmost of right-
most children of the top two words on the stack:
lc1(lc1(si)), rc1(rc1(si)), i = 1, 2.
We use the corresponding POS tags for St
(nt = 18), and the corresponding arc labels of
words excluding those 6 words on the stack/buffer
for Sl (nl = 12). A good advantage of our parser
is that we can add a rich set of elements cheaply,
instead of hand-crafting many more indicator fea-
tures.
</bodyText>
<subsectionHeader confidence="0.997565">
3.2 Training
</subsectionHeader>
<bodyText confidence="0.999885285714286">
We first generate training examples {(ci, ti)lmi=1
from the training sentences and their gold parse
trees using a “shortest stack” oracle which always
prefers LEFT-ARCl over SHIFT, where ci is a
configuration, ti E T is the oracle transition.
The final training objective is to minimize the
cross-entropy loss, plus a l2-regularization term:
</bodyText>
<equation confidence="0.852788">
λ
logpti + 2 11θ112
</equation>
<bodyText confidence="0.9992556">
where θ is the set of all parameters
{W1w , Wt1, Wl1, b1, W2, Ew, Et, Ell. A slight
variation is that we compute the softmax prob-
abilities only among the feasible transitions in
practice.
For initialization of parameters, we use pre-
trained word embeddings to initialize Ew and use
random initialization within (−0.01, 0.01) for Et
and El. Concretely, we use the pre-trained word
embeddings from (Collobert et al., 2011) for En-
glish (#dictionary = 130,000, coverage = 72.7%),
and our trained 50-dimensional word2vec em-
beddings (Mikolov et al., 2013) on Wikipedia
and Gigaword corpus for Chinese (#dictionary =
285,791, coverage = 79.0%). We will also com-
pare with random initialization of Ew in Section
4. The training error derivatives will be back-
propagated to these embeddings during the train-
ing process.
We use mini-batched AdaGrad (Duchi et al.,
2011) for optimization and also apply a dropout
(Hinton et al., 2012) with 0.5 rate. The parame-
ters which achieve the best unlabeled attachment
score on the development set will be chosen for
final evaluation.
</bodyText>
<subsectionHeader confidence="0.998641">
3.3 Parsing
</subsectionHeader>
<bodyText confidence="0.999954296296296">
We perform greedy decoding in parsing. At each
step, we extract all the corresponding word, POS
and label embeddings from the current configu-
ration c, compute the hidden layer h(c) E Rdh,
and pick the transition with the highest score:
t = arg maxt is feasible W2(t, ·)h(c), and then ex-
ecute c —* t(c).
Comparing with indicator features, our parser
does not need to compute conjunction features and
look them up in a huge feature table, and thus
greatly reduces feature generation time. Instead,
it involves many matrix addition and multiplica-
tion operations. To further speed up the parsing
time, we apply a pre-computation trick, similar
to (Devlin et al., 2014). For each position cho-
sen from Sw, we pre-compute matrix multiplica-
tions for most top frequent 10, 000 words. Thus,
computing the hidden layer only requires looking
up the table for these frequent words, and adding
the dh-dimensional vector. Similarly, we also pre-
compute matrix computations for all positions and
all POS tags and arc labels. We only use this opti-
mization in the neural network parser, but it is only
feasible for a parser like the neural network parser
which uses a small number of features. In prac-
tice, this pre-computation step increases the speed
of our parser 8 — 10 times.
</bodyText>
<sectionHeader confidence="0.999695" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.823355">
4.1 Datasets
</subsectionHeader>
<bodyText confidence="0.99839525">
We conduct our experiments on the English Penn
Treebank (PTB) and the Chinese Penn Treebank
(CTB) datasets.
For English, we follow the standard splits of
PTB3, using sections 2-21 for training, section
22 as development set and 23 as test set. We
adopt two different dependency representations:
CoNLL Syntactic Dependencies (CD) (Johansson
</bodyText>
<equation confidence="0.968311666666667">
�
L(θ) = −
i
</equation>
<page confidence="0.982236">
744
</page>
<table confidence="0.99890175">
Dataset #Train #Dev #Test #words (Nw) #POS (Nt) #labels (Nl) projective (%)
PTB: CD 39,832 1,700 2,416 44,352 45 17 99.4
PTB: SD 39,832 1,700 2,416 44,389 45 45 99.9
CTB 16,091 803 1,910 34,577 35 12 100.0
</table>
<tableCaption confidence="0.998518">
Table 3: Data Statistics. “Projective” is the percentage of projective trees on the training set.
</tableCaption>
<figureCaption confidence="0.748517571428571">
and Nugues, 2007) using the LTH Constituent-to-
Dependency Conversion Tool3 and Stanford Basic
Dependencies (SD) (de Marneffe et al., 2006) us-
ing the Stanford parser v3.3.0.4 The POS tags are
assigned using Stanford POS tagger (Toutanova et
al., 2003) with ten-way jackknifing of the training
data (accuracy ≈ 97.3%).
</figureCaption>
<bodyText confidence="0.9989969">
For Chinese, we adopt the same split of CTB5
as described in (Zhang and Clark, 2008). Depen-
dencies are converted using the Penn2Malt tool5
with the head-finding rules of (Zhang and Clark,
2008). And following (Zhang and Clark, 2008;
Zhang and Nivre, 2011), we use gold segmenta-
tion and POS tags for the input.
Table 3 gives statistics of the three datasets.6 In
particular, over 99% of the trees are projective in
all datasets.
</bodyText>
<sectionHeader confidence="0.670492" genericHeader="evaluation">
4.2 Results
</sectionHeader>
<bodyText confidence="0.9998790625">
The following hyper-parameters are used in all ex-
periments: embedding size d = 50, hidden layer
size h = 200, regularization parameter A = 10−8,
initial learning rate of Adagrad α = 0.01.
To situate the performance of our parser, we first
make a comparison with our own implementa-
tion of greedy arc-eager and arc-standard parsers.
These parsers are trained with structured averaged
perceptron using the “early-update” strategy. The
feature templates of (Zhang and Nivre, 2011) are
used for the arc-eager system, and they are also
adapted to the arc-standard system.7
Furthermore, we also compare our parser
with two popular, off-the-shelf parsers: Malt-
Parser — a greedy transition-based dependency
parser (Nivre et al., 2006),8 and MSTParser —
</bodyText>
<footnote confidence="0.887070818181818">
3http://nlp.cs.lth.se/software/treebank converter/
4http://nlp.stanford.edu/software/lex-parser.shtml
5http://stp.lingfil.uu.se/ nivre/research/Penn2Malt.html
6Pennconverter and Stanford dependencies generate
slightly different tokenization, e.g., Pennconverter splits the
token WCRS\/Boston NNP into three tokens WCRS NNP /
CC Boston NNP.
7Since arc-standard is bottom-up, we remove all features
using the head of stack elements, and also add the right child
features of the first stack element.
8http://www.maltparser.org/
</footnote>
<bodyText confidence="0.996915315789474">
a first-order graph-based parser (McDonald and
Pereira, 2006).9 In this comparison, for Malt-
Parser, we select stackproj (arc-standard) and
nivreeager (arc-eager) as parsing algorithms,
and liblinear (Fan et al., 2008) for optimization.10
For MSTParser, we use default options.
On all datasets, we report unlabeled attach-
ment scores (UAS) and labeled attachment scores
(LAS) and punctuation is excluded in all evalua-
tion metrics.11 Our parser and the baseline arc-
standard and arc-eager parsers are all implemented
in Java. The parsing speeds are measured on an
Intel Core i7 2.7GHz CPU with 16GB RAM and
the runtime does not include pre-computation or
parameter loading time.
Table 4, Table 5 and Table 6 show the com-
parison of accuracy and parsing speed on PTB
(CoNLL dependencies), PTB (Stanford dependen-
cies) and CTB respectively.
</bodyText>
<table confidence="0.99962575">
Parser Dev Test Speed
UAS LAS UAS LAS (sent/s)
standard 89.9 88.7 89.7 88.3 51
eager 90.3 89.2 89.9 88.6 63
Malt:sp 90.0 88.8 89.9 88.5 560
Malt:eager 90.1 88.9 90.1 88.7 535
MSTParser 92.1 90.8 92.0 90.5 12
Our parser 92.2 91.0 92.0 90.7 1013
</table>
<tableCaption confidence="0.975924">
Table 4: Accuracy and parsing speed on PTB +
CoNLL dependencies.
</tableCaption>
<bodyText confidence="0.991467714285714">
Clearly, our parser is superior in terms of both
accuracy and speed. Comparing with the base-
lines of arc-eager and arc-standard parsers, our
parser achieves around 2% improvement in UAS
and LAS on all datasets, while running about 20
times faster.
It is worth noting that the efficiency of our
</bodyText>
<footnote confidence="0.996756571428571">
9http://www.seas.upenn.edu/ strctlrn/MSTParser/
MSTParser.html
10We do not compare with libsvm optimization, which is
known to be sightly more accurate, but orders of magnitude
slower (Kong and Smith, 2014).
11A token is a punctuation if its gold POS tag is {“ ” : , .}
for English and PU for Chinese.
</footnote>
<page confidence="0.992588">
745
</page>
<table confidence="0.999649375">
Parser Dev Test Speed
UAS LAS UAS LAS (sent/s)
standard 90.2 87.8 89.4 87.3 26
eager 89.8 87.4 89.6 87.4 34
Malt:sp 89.8 87.2 89.3 86.9 469
Malt:eager 89.6 86.9 89.4 86.8 448
MSTParser 91.4 88.1 90.7 87.6 10
Our parser 92.0 89.7 91.8 89.6 654
</table>
<tableCaption confidence="0.9678275">
Table 5: Accuracy and parsing speed on PTB +
Stanford dependencies.
</tableCaption>
<table confidence="0.99989275">
Parser Dev Test Speed
UAS LAS UAS LAS (sent/s)
standard 82.4 80.9 82.7 81.2 72
eager 81.1 79.7 80.3 78.7 80
Malt:sp 82.4 80.5 82.4 80.6 420
Malt:eager 81.2 79.3 80.2 78.4 393
MSTParser 84.0 82.1 83.0 81.2 6
Our parser 84.0 82.4 83.9 82.4 936
</table>
<tableCaption confidence="0.999245">
Table 6: Accuracy and parsing speed on CTB.
</tableCaption>
<bodyText confidence="0.9999015">
parser even surpasses MaltParser using liblinear,
which is known to be highly optimized, while our
parser achieves much better accuracy.
Also, despite the fact that the graph-based MST-
Parser achieves a similar result to ours on PTB
(CoNLL dependencies), our parser is nearly 100
times faster. In particular, our transition-based
parser has a great advantage in LAS, especially
for the fine-grained label set of Stanford depen-
dencies.
</bodyText>
<subsectionHeader confidence="0.9999">
4.3 Effects of Parser Components
</subsectionHeader>
<bodyText confidence="0.999966">
Herein, we examine components that account for
the performance of our parser.
</bodyText>
<subsectionHeader confidence="0.826402">
Cube activation function
</subsectionHeader>
<bodyText confidence="0.999965">
We compare our cube activation function (x3)
with two widely used non-linear functions: tanh
</bodyText>
<equation confidence="0.6914065">
ex −e−x
(ex+e−x), sigmoid ( 1
</equation>
<bodyText confidence="0.959785333333333">
1+e−x), and also the
identity function (x), as shown in Figure 4
(left).
In short, cube outperforms all other activation
functions significantly and identity works the
worst. Concretely, cube can achieve 0.8% ∼
1.2% improvement in UAS over tanh and other
functions, thus verifying the effectiveness of the
cube activation function empirically.
</bodyText>
<subsectionHeader confidence="0.943995">
Initialization of pre-trained word embeddings
</subsectionHeader>
<bodyText confidence="0.979699636363636">
We further analyze the influence of using pre-
trained word embeddings for initialization. Fig-
ure 4 (middle) shows that using pre-trained word
embeddings can obtain around 0.7% improve-
ment on PTB and 1.7% improvement on CTB,
compared with using random initialization within
(−0.01, 0.01). On the one hand, the pre-trained
word embeddings of Chinese appear more use-
ful than those of English; on the other hand, our
model is still able to achieve comparable accuracy
without the help of pre-trained word embeddings.
POS tag and arc label embeddings
As shown in Figure 4 (right), POS embeddings
yield around 1.7% improvement on PTB and
nearly 10% improvement on CTB and the label
embeddings yield a much smaller 0.3% and 1.4%
improvement respectively.
However, we can obtain little gain from la-
bel embeddings when the POS embeddings are
present. This may be because the POS tags of two
tokens already capture most of the label informa-
tion between them.
</bodyText>
<subsectionHeader confidence="0.999609">
4.4 Model Analysis
</subsectionHeader>
<bodyText confidence="0.9999732">
Last but not least, we will examine the parame-
ters we have learned, and hope to investigate what
these dense features capture. We use the weights
learned from the English Penn Treebank using
Stanford dependencies for analysis.
</bodyText>
<subsectionHeader confidence="0.442163">
What do Et, El capture?
</subsectionHeader>
<bodyText confidence="0.998874555555555">
We first introduced Et and El as the dense rep-
resentations of all POS tags and arc labels, and
we wonder whether these embeddings could carry
some semantic information.
Figure 5 presents t-SNE visualizations (van der
Maaten and Hinton, 2008) of these embeddings.
It clearly shows that these embeddings effectively
exhibit the similarities between POS tags or arc
labels. For instance, the three adjective POS tags
JJ, JJR, JJS have very close embeddings, and
also the three labels representing clausal comple-
ments acomp, ccomp, xcomp are grouped to-
gether.
Since these embeddings can effectively encode
the semantic regularities, we believe that they can
be also used as alternative features of POS tags (or
arc labels) in other NLP tasks, and help boost the
performance.
</bodyText>
<page confidence="0.995083">
746
</page>
<bodyText confidence="0.970787444444444">
What do W1w, Wt1, W1l capture?
Knowing that Et and El (as well as the word em-
beddings Ew) can capture semantic information
very well, next we hope to investigate what each
feature in the hidden layer has really learned.
Since we currently only have h = 200 learned
dense features, we wonder if it is sufficient to
learn the word conjunctions as sparse indicator
features, or even more. We examine the weights
W1w (k, ·) E Rd·nw, Wt1(k, ·) E Rd·nt, Wl1(k, ·) E
Rd·nl for each hidden unit k, and reshape them to
d x nt, d x nw, d x nl matrices, such that the
weights of each column corresponds to the embed-
dings of one specific element (e.g., s1.t).
We pick the weights with absolute value &gt; 0.2,
and visualize them for each feature. Figure 6 gives
the visualization of three sampled features, and it
exhibits many interesting phenomena:
</bodyText>
<listItem confidence="0.984509357142857">
• Different features have varied distributions of
the weights. However, most of the discrim-
inative weights come from W1t (the middle
zone in Figure 6), and this further justifies the
importance of POS tags in dependency pars-
ing.
• We carefully examine many of the h = 200
features, and find that they actually encode
very different views of information. For the
three sampled features in Figure 6, the largest
weights are dominated by:
– Feature 1: s1.t, s2.t, lc(s1).t.
– Feautre 2: rc(s1).t, s1.t, b1.t.
– Feature 3: s1.t, s1.w, lc(s1).t, lc(s1).l.
</listItem>
<bodyText confidence="0.9993015">
These features all seem very plausible, as ob-
served in the experiments on indicator feature
systems. Thus our model is able to automati-
cally identify the most useful information for
predictions, instead of hand-crafting them as
indicator features.
</bodyText>
<listItem confidence="0.812494">
• More importantly, we can extract features re-
</listItem>
<bodyText confidence="0.818786444444444">
garding the conjunctions of more than 3 ele-
ments easily, and also those not presented in
the indicator feature systems. For example,
the 3rd feature above captures the conjunc-
tion of words and POS tags of s1, the tag of
its leftmost child, and also the label between
them, while this information is not encoded
in the original feature templates of (Zhang
and Nivre, 2011).
</bodyText>
<sectionHeader confidence="0.99956" genericHeader="related work">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999984735294118">
There have been several lines of earlier work in us-
ing neural networks for parsing which have points
of overlap but also major differences from our
work here. One big difference is that much early
work uses localist one-hot word representations
rather than the distributed representations of mod-
ern work. (Mayberry III and Miikkulainen, 1999)
explored a shift reduce constituency parser with
one-hot word representations and did subsequent
parsing work in (Mayberry III and Miikkulainen,
2005).
(Henderson, 2004) was the first to attempt to use
neural networks in a broad-coverage Penn Tree-
bank parser, using a simple synchrony network to
predict parse decisions in a constituency parser.
More recently, (Titov and Henderson, 2007) ap-
plied Incremental Sigmoid Belief Networks to
constituency parsing and then (Garg and Hender-
son, 2011) extended this work to transition-based
dependency parsers using a Temporal Restricted
Boltzman Machine. These are very different neu-
ral network architectures, and are much less scal-
able and in practice a restricted vocabulary was
used to make the architecture practical.
There have been a number of recent uses of
deep learning for constituency parsing (Collobert,
2011; Socher et al., 2013). (Socher et al., 2014)
has also built models over dependency representa-
tions but this work has not attempted to learn neu-
ral networks for dependency parsing.
Most recently, (Stenetorp, 2013) attempted to
build recursive neural networks for transition-
based dependency parsing, however the empirical
performance of his model is still unsatisfactory.
</bodyText>
<sectionHeader confidence="0.995037" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999932571428571">
We have presented a novel dependency parser us-
ing neural networks. Experimental evaluations
show that our parser outperforms other greedy
parsers using sparse indicator features in both ac-
curacy and speed. This is achieved by represent-
ing all words, POS tags and arc labels as dense
vectors, and modeling their interactions through a
novel cube activation function. Our model only
relies on dense features, and is able to automat-
ically learn the most useful feature conjunctions
for making predictions.
An interesting line of future work is to combine
our neural network based classifier with search-
based models to further improve accuracy. Also,
</bodyText>
<page confidence="0.972084">
747
</page>
<figure confidence="0.999830364285715">
−600 −400 −200 0 200 400 600 800
WP$
WP
WDT
PRP$
CC
TO
SYM
VBG
$
PRP
WRB
IN
LS
POS
FW
UH
‘‘
(
−ROOT−
VBN
VB
RP
DT
PDT
JJRJJ
JJS
:
.
)
’’
,
NN
NNS
NNPNNPS
EX
RB
RBSRBR
−400 −200 0 200 400 600
600
400
200
0
−200
−400
−600
−800
−600
800
600
400
200
0
−200
−400
−600
−800
1000
#
CD
misc
noun
punctuation
verb
adverb
adjective
VBZVBD
VBP
MD
advmod
prep
number
parataxis
possessive
quantmod
auxpass
conj
discourse
amod
num
predetdet
nn
poss
nsubj
nsubjpass
csubj
expl
appos
rcmod
prt
cc
mark
dep
csubjpass
punct
iobj
neg
root
partmod
infmod
dobj
acomp
advcl
xcomp
ccomp
misc
clausal complement
noun pre−modifier
verbal auxiliaries
subject
preposition complement
noun post−modifier
aux
cop
tmod
preconj
npadvmod
mwe
pcomp
pobj
PTB:CD PTB:SD CTB
cube tanh sigmoid identity
PTB:CD PTB:SD CTB
pre-trained random
PTB:CD PTB:SD CTB
95
90
UAS score
85
80
75
70
90
90
UAS score
UAS score
85
85
80
80
</figure>
<figureCaption confidence="0.9573775">
Figure 4: Effects of different parser components. Left: comparison of different activation functions.
Middle: comparison of pre-trained word vectors and random initialization. Right: effects of POS and
label embeddings.
word+POS+label word+POS word+label word
Figure 5: t-SNE visualization of POS and label embeddings.
Figure 6: Three sampled features. In each feature, each row denotes a dimension of embeddings and
</figureCaption>
<bodyText confidence="0.971886666666667">
each column denotes a chosen element, e.g., si.t or lc(si).w, and the parameters are divided into 3
zones, corresponding to Wi (k, :) (left), Wt1(k, :) (middle) and Wl1(k, :) (right). White and black dots
denote the most positive weights and most negative weights respectively.
</bodyText>
<page confidence="0.990332">
748
</page>
<bodyText confidence="0.999979333333333">
there is still room for improvement in our architec-
ture, such as better capturing word conjunctions,
or adding richer features (e.g., distance, valency).
</bodyText>
<sectionHeader confidence="0.996563" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999082461538461">
Stanford University gratefully acknowledges the
support of the Defense Advanced Research
Projects Agency (DARPA) Deep Exploration and
Filtering of Text (DEFT) Program under Air
Force Research Laboratory (AFRL) contract no.
FA8750-13-2-0040 and the Defense Threat Re-
duction Agency (DTRA) under Air Force Re-
search Laboratory (AFRL) contract no. FA8650-
10-C-7020. Any opinions, findings, and conclu-
sion or recommendations expressed in this mate-
rial are those of the authors and do not necessarily
reflect the view of the DARPA, AFRL, or the US
government.
</bodyText>
<sectionHeader confidence="0.999417" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997541780487804">
Bernd Bohnet. 2010. Very high accuracy and fast de-
pendency parsing is not a contradiction. In Coling.
Ronan Collobert, Jason Weston, L´eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. Journal of Machine Learning Research.
Ronan Collobert. 2011. Deep learning for efficient
discriminative parsing. In AISTATS.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
LREC.
Jacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas
Lamar, Richard Schwartz, and John Makhoul. 2014.
Fast and robust neural network joint models for sta-
tistical machine translation. In ACL.
John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. The Journal of Ma-
chine Learning Research.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A
library for large linear classification. The Journal of
Machine Learning Research.
Nikhil Garg and James Henderson. 2011. Temporal
restricted boltzmann machines for dependency pars-
ing. In ACL-HLT.
He He, Hal Daum´e III, and Jason Eisner. 2013. Dy-
namic feature selection for dependency parsing. In
EMNLP.
James Henderson. 2004. Discriminative training of a
neural network statistical parser. In ACL.
Geoffrey E. Hinton, Nitish Srivastava, Alex
Krizhevsky, Ilya Sutskever, and Ruslan Salakhut-
dinov. 2012. Improving neural networks by
preventing co-adaptation of feature detectors.
CoRR, abs/1207.0580.
Liang Huang, Wenbin Jiang, and Qun Liu. 2009.
Bilingually-constrained (monolingual) shift-reduce
parsing. In EMNLP.
Richard Johansson and Pierre Nugues. 2007. Ex-
tended constituent-to-dependency conversion for en-
glish. In Proceedings of NODALIDA, Tartu, Estonia.
Lingpeng Kong and Noah A. Smith. 2014. An em-
pirical comparison of parsing methods for Stanford
dependencies. CoRR, abs/1404.4314.
Terry Koo, Xavier Carreras, and Michael Collins.
2008. Simple semi-supervised dependency parsing.
In ACL.
Sandra K¨ubler, Ryan McDonald, and Joakim Nivre.
2009. Dependency Parsing. Synthesis Lectures on
Human Language Technologies. Morgan &amp; Clay-
pool.
Marshall R. Mayberry III and Risto Miikkulainen.
1999. Sardsrn: A neural network shift-reduce
parser. In IJCAI.
Marshall R. Mayberry III and Risto Miikkulainen.
2005. Broad-coverage parsing with neural net-
works. Neural Processing Letters.
Ryan McDonald and Fernando Pereira. 2006. Online
learning of approximate dependency parsing algo-
rithms. In EACL.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In NIPS.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006.
Maltparser: A data-driven parser-generator for de-
pendency parsing. In LREC.
Richard Socher, John Bauer, Christopher D Manning,
and Andrew Y Ng. 2013. Parsing with composi-
tional vector grammars. In ACL.
Richard Socher, Andrej Karpathy, Quoc V. Le, Christo-
pher D. Manning, and Andrew Y. Ng. 2014.
Grounded compositional semantics for finding and
describing images with sentences. TACL.
Pontus Stenetorp. 2013. Transition-based dependency
parsing using recursive neural networks. In NIPS
Workshop on Deep Learning.
Ivan Titov and James Henderson. 2007. Fast and ro-
bust multilingual dependency parsing with a gener-
ative latent variable model. In EMNLP-CoNLL.
</reference>
<page confidence="0.98566">
749
</page>
<reference confidence="0.997854285714286">
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In NAACL.
Laurens van der Maaten and Geoffrey Hinton. 2008.
Visualizing data using t-SNE. The Journal of Ma-
chine Learning Research.
Yue Zhang and Stephen Clark. 2008. A tale of
two parsers: Investigating and combining graph-
based and transition-based dependency parsing us-
ing beam-search. In EMNLP.
Yue Zhang and Joakim Nivre. 2011. Transition-based
dependency parsing with rich non-local features. In
ACL.
</reference>
<page confidence="0.997628">
750
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.466680">
<title confidence="0.999887">A Fast and Accurate Dependency Parser using Neural Networks</title>
<author confidence="0.999879">Danqi Chen Christopher D Manning</author>
<affiliation confidence="0.9998515">Computer Science Department Computer Science Department Stanford University Stanford University</affiliation>
<email confidence="0.999595">danqi@cs.stanford.edumanning@stanford.edu</email>
<abstract confidence="0.990217555555555">Almost all current dependency parsers classify based on millions of sparse indicator features. Not only do these features generalize poorly, but the cost of feature computation restricts parsing speed significantly. In this work, we propose a novel way of learning a neural network classifier for use in a greedy, transition-based dependency parser. Because this classifier learns and uses just a small number of dense features, it can work very fast, while achieving an about 2% improvement in unlabeled and labeled attachment scores on both English and Chinese datasets. Concretely, our parser is able to parse more than 1000 sentences per second at 92.2% unlabeled attachment score on the English</abstract>
<author confidence="0.614323">Penn Treebank</author>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Bernd Bohnet</author>
</authors>
<title>Very high accuracy and fast dependency parsing is not a contradiction. In Coling.</title>
<date>2010</date>
<contexts>
<context position="2234" citStr="Bohnet (2010)" startWordPosition="340" endWordPosition="341">rectly weight most such features. For this reason, techniques for introducing higher-support features such as word class features have also been very successful in improving parsing performance (Koo et al., 2008). Second, almost all existing parsers rely on a manually designed set of feature templates, which require a lot of expertise and are usually incomplete. Third, the use of many feature templates cause a less studied problem: in modern dependency parsers, most of the runtime is consumed not by the core parsing algorithm but in the feature extraction step (He et al., 2013). For instance, Bohnet (2010) reports that his baseline parser spends 99% of its time doing feature extraction, despite that being done in standard efficient ways. In this work, we address all of these problems by using dense features in place of the sparse indicator features. This is inspired by the recent success of distributed word representations in many NLP tasks, e.g., POS tagging (Collobert et al., 2011), machine translation (Devlin et al., 2014), and constituency parsing (Socher et al., 2013). Low-dimensional, dense word embeddings can effectively alleviate sparsity by sharing statistical strength between similar </context>
</contexts>
<marker>Bohnet, 2010</marker>
<rawString>Bernd Bohnet. 2010. Very high accuracy and fast dependency parsing is not a contradiction. In Coling.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
<author>L´eon Bottou</author>
<author>Michael Karlen</author>
<author>Koray Kavukcuoglu</author>
<author>Pavel Kuksa</author>
</authors>
<title>Natural language processing (almost) from scratch.</title>
<date>2011</date>
<journal>Journal of Machine Learning Research.</journal>
<contexts>
<context position="2619" citStr="Collobert et al., 2011" startWordPosition="404" endWordPosition="407"> use of many feature templates cause a less studied problem: in modern dependency parsers, most of the runtime is consumed not by the core parsing algorithm but in the feature extraction step (He et al., 2013). For instance, Bohnet (2010) reports that his baseline parser spends 99% of its time doing feature extraction, despite that being done in standard efficient ways. In this work, we address all of these problems by using dense features in place of the sparse indicator features. This is inspired by the recent success of distributed word representations in many NLP tasks, e.g., POS tagging (Collobert et al., 2011), machine translation (Devlin et al., 2014), and constituency parsing (Socher et al., 2013). Low-dimensional, dense word embeddings can effectively alleviate sparsity by sharing statistical strength between similar words, and can provide us a good starting point to construct features of words and their interactions. Nevertheless, there remain challenging problems of how to encode all the available information from the configuration and how to model higher-order features based on the dense representations. In this paper, we train a neural network classifier to make parsing decisions within a tr</context>
<context position="15964" citStr="Collobert et al., 2011" startWordPosition="2709" endWordPosition="2712"> LEFT-ARCl over SHIFT, where ci is a configuration, ti E T is the oracle transition. The final training objective is to minimize the cross-entropy loss, plus a l2-regularization term: λ logpti + 2 11θ112 where θ is the set of all parameters {W1w , Wt1, Wl1, b1, W2, Ew, Et, Ell. A slight variation is that we compute the softmax probabilities only among the feasible transitions in practice. For initialization of parameters, we use pretrained word embeddings to initialize Ew and use random initialization within (−0.01, 0.01) for Et and El. Concretely, we use the pre-trained word embeddings from (Collobert et al., 2011) for English (#dictionary = 130,000, coverage = 72.7%), and our trained 50-dimensional word2vec embeddings (Mikolov et al., 2013) on Wikipedia and Gigaword corpus for Chinese (#dictionary = 285,791, coverage = 79.0%). We will also compare with random initialization of Ew in Section 4. The training error derivatives will be backpropagated to these embeddings during the training process. We use mini-batched AdaGrad (Duchi et al., 2011) for optimization and also apply a dropout (Hinton et al., 2012) with 0.5 rate. The parameters which achieve the best unlabeled attachment score on the development</context>
</contexts>
<marker>Collobert, Weston, Bottou, Karlen, Kavukcuoglu, Kuksa, 2011</marker>
<rawString>Ronan Collobert, Jason Weston, L´eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural language processing (almost) from scratch. Journal of Machine Learning Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
</authors>
<title>Deep learning for efficient discriminative parsing.</title>
<date>2011</date>
<booktitle>In AISTATS.</booktitle>
<contexts>
<context position="29282" citStr="Collobert, 2011" startWordPosition="4879" endWordPosition="4880">bank parser, using a simple synchrony network to predict parse decisions in a constituency parser. More recently, (Titov and Henderson, 2007) applied Incremental Sigmoid Belief Networks to constituency parsing and then (Garg and Henderson, 2011) extended this work to transition-based dependency parsers using a Temporal Restricted Boltzman Machine. These are very different neural network architectures, and are much less scalable and in practice a restricted vocabulary was used to make the architecture practical. There have been a number of recent uses of deep learning for constituency parsing (Collobert, 2011; Socher et al., 2013). (Socher et al., 2014) has also built models over dependency representations but this work has not attempted to learn neural networks for dependency parsing. Most recently, (Stenetorp, 2013) attempted to build recursive neural networks for transitionbased dependency parsing, however the empirical performance of his model is still unsatisfactory. 6 Conclusion We have presented a novel dependency parser using neural networks. Experimental evaluations show that our parser outperforms other greedy parsers using sparse indicator features in both accuracy and speed. This is ac</context>
</contexts>
<marker>Collobert, 2011</marker>
<rawString>Ronan Collobert. 2011. Deep learning for efficient discriminative parsing. In AISTATS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Bill MacCartney</author>
<author>Christopher D Manning</author>
</authors>
<title>Generating typed dependency parses from phrase structure parses.</title>
<date>2006</date>
<booktitle>In LREC.</booktitle>
<marker>de Marneffe, MacCartney, Manning, 2006</marker>
<rawString>Marie-Catherine de Marneffe, Bill MacCartney, and Christopher D. Manning. 2006. Generating typed dependency parses from phrase structure parses. In LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Devlin</author>
<author>Rabih Zbib</author>
<author>Zhongqiang Huang</author>
<author>Thomas Lamar</author>
<author>Richard Schwartz</author>
<author>John Makhoul</author>
</authors>
<title>Fast and robust neural network joint models for statistical machine translation.</title>
<date>2014</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="2662" citStr="Devlin et al., 2014" startWordPosition="410" endWordPosition="413">udied problem: in modern dependency parsers, most of the runtime is consumed not by the core parsing algorithm but in the feature extraction step (He et al., 2013). For instance, Bohnet (2010) reports that his baseline parser spends 99% of its time doing feature extraction, despite that being done in standard efficient ways. In this work, we address all of these problems by using dense features in place of the sparse indicator features. This is inspired by the recent success of distributed word representations in many NLP tasks, e.g., POS tagging (Collobert et al., 2011), machine translation (Devlin et al., 2014), and constituency parsing (Socher et al., 2013). Low-dimensional, dense word embeddings can effectively alleviate sparsity by sharing statistical strength between similar words, and can provide us a good starting point to construct features of words and their interactions. Nevertheless, there remain challenging problems of how to encode all the available information from the configuration and how to model higher-order features based on the dense representations. In this paper, we train a neural network classifier to make parsing decisions within a transition-based dependency parser. The neura</context>
<context position="17282" citStr="Devlin et al., 2014" startWordPosition="2927" endWordPosition="2930">ch step, we extract all the corresponding word, POS and label embeddings from the current configuration c, compute the hidden layer h(c) E Rdh, and pick the transition with the highest score: t = arg maxt is feasible W2(t, ·)h(c), and then execute c —* t(c). Comparing with indicator features, our parser does not need to compute conjunction features and look them up in a huge feature table, and thus greatly reduces feature generation time. Instead, it involves many matrix addition and multiplication operations. To further speed up the parsing time, we apply a pre-computation trick, similar to (Devlin et al., 2014). For each position chosen from Sw, we pre-compute matrix multiplications for most top frequent 10, 000 words. Thus, computing the hidden layer only requires looking up the table for these frequent words, and adding the dh-dimensional vector. Similarly, we also precompute matrix computations for all positions and all POS tags and arc labels. We only use this optimization in the neural network parser, but it is only feasible for a parser like the neural network parser which uses a small number of features. In practice, this pre-computation step increases the speed of our parser 8 — 10 times. 4 </context>
</contexts>
<marker>Devlin, Zbib, Huang, Lamar, Schwartz, Makhoul, 2014</marker>
<rawString>Jacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas Lamar, Richard Schwartz, and John Makhoul. 2014. Fast and robust neural network joint models for statistical machine translation. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Duchi</author>
<author>Elad Hazan</author>
<author>Yoram Singer</author>
</authors>
<title>Adaptive subgradient methods for online learning and stochastic optimization.</title>
<date>2011</date>
<journal>The Journal of Machine Learning Research.</journal>
<contexts>
<context position="16401" citStr="Duchi et al., 2011" startWordPosition="2779" endWordPosition="2782">ed word embeddings to initialize Ew and use random initialization within (−0.01, 0.01) for Et and El. Concretely, we use the pre-trained word embeddings from (Collobert et al., 2011) for English (#dictionary = 130,000, coverage = 72.7%), and our trained 50-dimensional word2vec embeddings (Mikolov et al., 2013) on Wikipedia and Gigaword corpus for Chinese (#dictionary = 285,791, coverage = 79.0%). We will also compare with random initialization of Ew in Section 4. The training error derivatives will be backpropagated to these embeddings during the training process. We use mini-batched AdaGrad (Duchi et al., 2011) for optimization and also apply a dropout (Hinton et al., 2012) with 0.5 rate. The parameters which achieve the best unlabeled attachment score on the development set will be chosen for final evaluation. 3.3 Parsing We perform greedy decoding in parsing. At each step, we extract all the corresponding word, POS and label embeddings from the current configuration c, compute the hidden layer h(c) E Rdh, and pick the transition with the highest score: t = arg maxt is feasible W2(t, ·)h(c), and then execute c —* t(c). Comparing with indicator features, our parser does not need to compute conjuncti</context>
</contexts>
<marker>Duchi, Hazan, Singer, 2011</marker>
<rawString>John Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive subgradient methods for online learning and stochastic optimization. The Journal of Machine Learning Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rong-En Fan</author>
<author>Kai-Wei Chang</author>
<author>Cho-Jui Hsieh</author>
<author>XiangRui Wang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>Liblinear: A library for large linear classification.</title>
<date>2008</date>
<journal>The Journal of Machine Learning Research.</journal>
<contexts>
<context position="20810" citStr="Fan et al., 2008" startWordPosition="3473" endWordPosition="3476">se/ nivre/research/Penn2Malt.html 6Pennconverter and Stanford dependencies generate slightly different tokenization, e.g., Pennconverter splits the token WCRS\/Boston NNP into three tokens WCRS NNP / CC Boston NNP. 7Since arc-standard is bottom-up, we remove all features using the head of stack elements, and also add the right child features of the first stack element. 8http://www.maltparser.org/ a first-order graph-based parser (McDonald and Pereira, 2006).9 In this comparison, for MaltParser, we select stackproj (arc-standard) and nivreeager (arc-eager) as parsing algorithms, and liblinear (Fan et al., 2008) for optimization.10 For MSTParser, we use default options. On all datasets, we report unlabeled attachment scores (UAS) and labeled attachment scores (LAS) and punctuation is excluded in all evaluation metrics.11 Our parser and the baseline arcstandard and arc-eager parsers are all implemented in Java. The parsing speeds are measured on an Intel Core i7 2.7GHz CPU with 16GB RAM and the runtime does not include pre-computation or parameter loading time. Table 4, Table 5 and Table 6 show the comparison of accuracy and parsing speed on PTB (CoNLL dependencies), PTB (Stanford dependencies) and CT</context>
</contexts>
<marker>Fan, Chang, Hsieh, Wang, Lin, 2008</marker>
<rawString>Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, XiangRui Wang, and Chih-Jen Lin. 2008. Liblinear: A library for large linear classification. The Journal of Machine Learning Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nikhil Garg</author>
<author>James Henderson</author>
</authors>
<title>Temporal restricted boltzmann machines for dependency parsing.</title>
<date>2011</date>
<booktitle>In ACL-HLT.</booktitle>
<contexts>
<context position="28912" citStr="Garg and Henderson, 2011" startWordPosition="4820" endWordPosition="4824">alist one-hot word representations rather than the distributed representations of modern work. (Mayberry III and Miikkulainen, 1999) explored a shift reduce constituency parser with one-hot word representations and did subsequent parsing work in (Mayberry III and Miikkulainen, 2005). (Henderson, 2004) was the first to attempt to use neural networks in a broad-coverage Penn Treebank parser, using a simple synchrony network to predict parse decisions in a constituency parser. More recently, (Titov and Henderson, 2007) applied Incremental Sigmoid Belief Networks to constituency parsing and then (Garg and Henderson, 2011) extended this work to transition-based dependency parsers using a Temporal Restricted Boltzman Machine. These are very different neural network architectures, and are much less scalable and in practice a restricted vocabulary was used to make the architecture practical. There have been a number of recent uses of deep learning for constituency parsing (Collobert, 2011; Socher et al., 2013). (Socher et al., 2014) has also built models over dependency representations but this work has not attempted to learn neural networks for dependency parsing. Most recently, (Stenetorp, 2013) attempted to bui</context>
</contexts>
<marker>Garg, Henderson, 2011</marker>
<rawString>Nikhil Garg and James Henderson. 2011. Temporal restricted boltzmann machines for dependency parsing. In ACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>He He</author>
<author>Hal Daum´e</author>
<author>Jason Eisner</author>
</authors>
<title>Dynamic feature selection for dependency parsing.</title>
<date>2013</date>
<booktitle>In EMNLP.</booktitle>
<marker>He, Daum´e, Eisner, 2013</marker>
<rawString>He He, Hal Daum´e III, and Jason Eisner. 2013. Dynamic feature selection for dependency parsing. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Henderson</author>
</authors>
<title>Discriminative training of a neural network statistical parser.</title>
<date>2004</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="28589" citStr="Henderson, 2004" startWordPosition="4772" endWordPosition="4773">information is not encoded in the original feature templates of (Zhang and Nivre, 2011). 5 Related Work There have been several lines of earlier work in using neural networks for parsing which have points of overlap but also major differences from our work here. One big difference is that much early work uses localist one-hot word representations rather than the distributed representations of modern work. (Mayberry III and Miikkulainen, 1999) explored a shift reduce constituency parser with one-hot word representations and did subsequent parsing work in (Mayberry III and Miikkulainen, 2005). (Henderson, 2004) was the first to attempt to use neural networks in a broad-coverage Penn Treebank parser, using a simple synchrony network to predict parse decisions in a constituency parser. More recently, (Titov and Henderson, 2007) applied Incremental Sigmoid Belief Networks to constituency parsing and then (Garg and Henderson, 2011) extended this work to transition-based dependency parsers using a Temporal Restricted Boltzman Machine. These are very different neural network architectures, and are much less scalable and in practice a restricted vocabulary was used to make the architecture practical. There</context>
</contexts>
<marker>Henderson, 2004</marker>
<rawString>James Henderson. 2004. Discriminative training of a neural network statistical parser. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey E Hinton</author>
<author>Nitish Srivastava</author>
</authors>
<title>Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.</title>
<date>2012</date>
<location>CoRR, abs/1207.0580.</location>
<marker>Hinton, Srivastava, 2012</marker>
<rawString>Geoffrey E. Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2012. Improving neural networks by preventing co-adaptation of feature detectors. CoRR, abs/1207.0580.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>Wenbin Jiang</author>
<author>Qun Liu</author>
</authors>
<title>Bilingually-constrained (monolingual) shift-reduce parsing.</title>
<date>2009</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="7379" citStr="Huang et al., 2009" startWordPosition="1220" endWordPosition="1223">, w denotes word, t denotes POS tag. given configuration. Information that can be obtained from one configuration includes: (1) all the words and their corresponding POS tags (e.g., has / VBZ); (2) the head of a word and its label (e.g., nsubj, dobj) if applicable; (3) the position of a word on the stack/buffer or whether it has already been removed from the stack. Conventional approaches extract indicator features such as the conjunction of 1 - 3 elements from the stack/buffer using their words, POS tags or arc labels. Table 1 lists a typical set of feature templates chosen from the ones of (Huang et al., 2009; Zhang and Nivre, 2011).2 These features suffer from the following problems: • Sparsity. The features, especially lexicalized features are highly sparse, and this is a common problem in many NLP tasks. The situation is severe in dependency parsing, because it depends critically on word-to-word interactions and thus the high-order features. To give a better understanding, we perform a feature analysis using the features in Table 1 on the English Penn Treebank (CoNLL representations). The results given in Table 2 demonstrate that: (1) lexicalized features are indispensable; (2) Not only are the</context>
</contexts>
<marker>Huang, Jiang, Liu, 2009</marker>
<rawString>Liang Huang, Wenbin Jiang, and Qun Liu. 2009. Bilingually-constrained (monolingual) shift-reduce parsing. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Johansson</author>
<author>Pierre Nugues</author>
</authors>
<title>Extended constituent-to-dependency conversion for english.</title>
<date>2007</date>
<booktitle>In Proceedings of NODALIDA,</booktitle>
<location>Tartu, Estonia.</location>
<marker>Johansson, Nugues, 2007</marker>
<rawString>Richard Johansson and Pierre Nugues. 2007. Extended constituent-to-dependency conversion for english. In Proceedings of NODALIDA, Tartu, Estonia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lingpeng Kong</author>
<author>Noah A Smith</author>
</authors>
<title>An empirical comparison of parsing methods for Stanford dependencies.</title>
<date>2014</date>
<location>CoRR, abs/1404.4314.</location>
<contexts>
<context position="22235" citStr="Kong and Smith, 2014" startWordPosition="3707" endWordPosition="3710">1 90.8 92.0 90.5 12 Our parser 92.2 91.0 92.0 90.7 1013 Table 4: Accuracy and parsing speed on PTB + CoNLL dependencies. Clearly, our parser is superior in terms of both accuracy and speed. Comparing with the baselines of arc-eager and arc-standard parsers, our parser achieves around 2% improvement in UAS and LAS on all datasets, while running about 20 times faster. It is worth noting that the efficiency of our 9http://www.seas.upenn.edu/ strctlrn/MSTParser/ MSTParser.html 10We do not compare with libsvm optimization, which is known to be sightly more accurate, but orders of magnitude slower (Kong and Smith, 2014). 11A token is a punctuation if its gold POS tag is {“ ” : , .} for English and PU for Chinese. 745 Parser Dev Test Speed UAS LAS UAS LAS (sent/s) standard 90.2 87.8 89.4 87.3 26 eager 89.8 87.4 89.6 87.4 34 Malt:sp 89.8 87.2 89.3 86.9 469 Malt:eager 89.6 86.9 89.4 86.8 448 MSTParser 91.4 88.1 90.7 87.6 10 Our parser 92.0 89.7 91.8 89.6 654 Table 5: Accuracy and parsing speed on PTB + Stanford dependencies. Parser Dev Test Speed UAS LAS UAS LAS (sent/s) standard 82.4 80.9 82.7 81.2 72 eager 81.1 79.7 80.3 78.7 80 Malt:sp 82.4 80.5 82.4 80.6 420 Malt:eager 81.2 79.3 80.2 78.4 393 MSTParser 84.0</context>
</contexts>
<marker>Kong, Smith, 2014</marker>
<rawString>Lingpeng Kong and Noah A. Smith. 2014. An empirical comparison of parsing methods for Stanford dependencies. CoRR, abs/1404.4314.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Koo</author>
<author>Xavier Carreras</author>
<author>Michael Collins</author>
</authors>
<title>Simple semi-supervised dependency parsing.</title>
<date>2008</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="1833" citStr="Koo et al., 2008" startWordPosition="269" endWordPosition="272">y parsers has been very appealing. However, these parsers are not perfect. First, from a statistical perspective, these parsers suffer from the use of millions of mainly poorly estimated feature weights. While in aggregate both lexicalized features and higher-order interaction term features are very important in improving the performance of these systems, nevertheless, there is insufficient data to correctly weight most such features. For this reason, techniques for introducing higher-support features such as word class features have also been very successful in improving parsing performance (Koo et al., 2008). Second, almost all existing parsers rely on a manually designed set of feature templates, which require a lot of expertise and are usually incomplete. Third, the use of many feature templates cause a less studied problem: in modern dependency parsers, most of the runtime is consumed not by the core parsing algorithm but in the feature extraction step (He et al., 2013). For instance, Bohnet (2010) reports that his baseline parser spends 99% of its time doing feature extraction, despite that being done in standard efficient ways. In this work, we address all of these problems by using dense fe</context>
</contexts>
<marker>Koo, Carreras, Collins, 2008</marker>
<rawString>Terry Koo, Xavier Carreras, and Michael Collins. 2008. Simple semi-supervised dependency parsing. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sandra K¨ubler</author>
<author>Ryan McDonald</author>
<author>Joakim Nivre</author>
</authors>
<title>Dependency Parsing. Synthesis Lectures on Human Language Technologies.</title>
<date>2009</date>
<publisher>Morgan &amp; Claypool.</publisher>
<marker>K¨ubler, McDonald, Nivre, 2009</marker>
<rawString>Sandra K¨ubler, Ryan McDonald, and Joakim Nivre. 2009. Dependency Parsing. Synthesis Lectures on Human Language Technologies. Morgan &amp; Claypool.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marshall R Mayberry</author>
<author>Risto Miikkulainen</author>
</authors>
<title>Sardsrn: A neural network shift-reduce parser.</title>
<date>1999</date>
<booktitle>In IJCAI.</booktitle>
<marker>Mayberry, Miikkulainen, 1999</marker>
<rawString>Marshall R. Mayberry III and Risto Miikkulainen. 1999. Sardsrn: A neural network shift-reduce parser. In IJCAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marshall R Mayberry</author>
<author>Risto Miikkulainen</author>
</authors>
<title>Broad-coverage parsing with neural networks. Neural Processing Letters.</title>
<date>2005</date>
<marker>Mayberry, Miikkulainen, 2005</marker>
<rawString>Marshall R. Mayberry III and Risto Miikkulainen. 2005. Broad-coverage parsing with neural networks. Neural Processing Letters.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
</authors>
<title>Online learning of approximate dependency parsing algorithms.</title>
<date>2006</date>
<booktitle>In EACL.</booktitle>
<contexts>
<context position="20654" citStr="McDonald and Pereira, 2006" startWordPosition="3451" endWordPosition="3454">r (Nivre et al., 2006),8 and MSTParser — 3http://nlp.cs.lth.se/software/treebank converter/ 4http://nlp.stanford.edu/software/lex-parser.shtml 5http://stp.lingfil.uu.se/ nivre/research/Penn2Malt.html 6Pennconverter and Stanford dependencies generate slightly different tokenization, e.g., Pennconverter splits the token WCRS\/Boston NNP into three tokens WCRS NNP / CC Boston NNP. 7Since arc-standard is bottom-up, we remove all features using the head of stack elements, and also add the right child features of the first stack element. 8http://www.maltparser.org/ a first-order graph-based parser (McDonald and Pereira, 2006).9 In this comparison, for MaltParser, we select stackproj (arc-standard) and nivreeager (arc-eager) as parsing algorithms, and liblinear (Fan et al., 2008) for optimization.10 For MSTParser, we use default options. On all datasets, we report unlabeled attachment scores (UAS) and labeled attachment scores (LAS) and punctuation is excluded in all evaluation metrics.11 Our parser and the baseline arcstandard and arc-eager parsers are all implemented in Java. The parsing speeds are measured on an Intel Core i7 2.7GHz CPU with 16GB RAM and the runtime does not include pre-computation or parameter </context>
</contexts>
<marker>McDonald, Pereira, 2006</marker>
<rawString>Ryan McDonald and Fernando Pereira. 2006. Online learning of approximate dependency parsing algorithms. In EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Greg S Corrado</author>
<author>Jeff Dean</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality.</title>
<date>2013</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="16093" citStr="Mikolov et al., 2013" startWordPosition="2729" endWordPosition="2732">e cross-entropy loss, plus a l2-regularization term: λ logpti + 2 11θ112 where θ is the set of all parameters {W1w , Wt1, Wl1, b1, W2, Ew, Et, Ell. A slight variation is that we compute the softmax probabilities only among the feasible transitions in practice. For initialization of parameters, we use pretrained word embeddings to initialize Ew and use random initialization within (−0.01, 0.01) for Et and El. Concretely, we use the pre-trained word embeddings from (Collobert et al., 2011) for English (#dictionary = 130,000, coverage = 72.7%), and our trained 50-dimensional word2vec embeddings (Mikolov et al., 2013) on Wikipedia and Gigaword corpus for Chinese (#dictionary = 285,791, coverage = 79.0%). We will also compare with random initialization of Ew in Section 4. The training error derivatives will be backpropagated to these embeddings during the training process. We use mini-batched AdaGrad (Duchi et al., 2011) for optimization and also apply a dropout (Hinton et al., 2012) with 0.5 rate. The parameters which achieve the best unlabeled attachment score on the development set will be chosen for final evaluation. 3.3 Parsing We perform greedy decoding in parsing. At each step, we extract all the cor</context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013. Distributed representations of words and phrases and their compositionality. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Jens Nilsson</author>
</authors>
<title>Maltparser: A data-driven parser-generator for dependency parsing.</title>
<date>2006</date>
<booktitle>In LREC.</booktitle>
<contexts>
<context position="20049" citStr="Nivre et al., 2006" startWordPosition="3384" endWordPosition="3387">00, regularization parameter A = 10−8, initial learning rate of Adagrad α = 0.01. To situate the performance of our parser, we first make a comparison with our own implementation of greedy arc-eager and arc-standard parsers. These parsers are trained with structured averaged perceptron using the “early-update” strategy. The feature templates of (Zhang and Nivre, 2011) are used for the arc-eager system, and they are also adapted to the arc-standard system.7 Furthermore, we also compare our parser with two popular, off-the-shelf parsers: MaltParser — a greedy transition-based dependency parser (Nivre et al., 2006),8 and MSTParser — 3http://nlp.cs.lth.se/software/treebank converter/ 4http://nlp.stanford.edu/software/lex-parser.shtml 5http://stp.lingfil.uu.se/ nivre/research/Penn2Malt.html 6Pennconverter and Stanford dependencies generate slightly different tokenization, e.g., Pennconverter splits the token WCRS\/Boston NNP into three tokens WCRS NNP / CC Boston NNP. 7Since arc-standard is bottom-up, we remove all features using the head of stack elements, and also add the right child features of the first stack element. 8http://www.maltparser.org/ a first-order graph-based parser (McDonald and Pereira, </context>
</contexts>
<marker>Nivre, Hall, Nilsson, 2006</marker>
<rawString>Joakim Nivre, Johan Hall, and Jens Nilsson. 2006. Maltparser: A data-driven parser-generator for dependency parsing. In LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>John Bauer</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Parsing with compositional vector grammars.</title>
<date>2013</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="2710" citStr="Socher et al., 2013" startWordPosition="417" endWordPosition="420">t of the runtime is consumed not by the core parsing algorithm but in the feature extraction step (He et al., 2013). For instance, Bohnet (2010) reports that his baseline parser spends 99% of its time doing feature extraction, despite that being done in standard efficient ways. In this work, we address all of these problems by using dense features in place of the sparse indicator features. This is inspired by the recent success of distributed word representations in many NLP tasks, e.g., POS tagging (Collobert et al., 2011), machine translation (Devlin et al., 2014), and constituency parsing (Socher et al., 2013). Low-dimensional, dense word embeddings can effectively alleviate sparsity by sharing statistical strength between similar words, and can provide us a good starting point to construct features of words and their interactions. Nevertheless, there remain challenging problems of how to encode all the available information from the configuration and how to model higher-order features based on the dense representations. In this paper, we train a neural network classifier to make parsing decisions within a transition-based dependency parser. The neural network learns compact dense vector representa</context>
<context position="29304" citStr="Socher et al., 2013" startWordPosition="4881" endWordPosition="4884">g a simple synchrony network to predict parse decisions in a constituency parser. More recently, (Titov and Henderson, 2007) applied Incremental Sigmoid Belief Networks to constituency parsing and then (Garg and Henderson, 2011) extended this work to transition-based dependency parsers using a Temporal Restricted Boltzman Machine. These are very different neural network architectures, and are much less scalable and in practice a restricted vocabulary was used to make the architecture practical. There have been a number of recent uses of deep learning for constituency parsing (Collobert, 2011; Socher et al., 2013). (Socher et al., 2014) has also built models over dependency representations but this work has not attempted to learn neural networks for dependency parsing. Most recently, (Stenetorp, 2013) attempted to build recursive neural networks for transitionbased dependency parsing, however the empirical performance of his model is still unsatisfactory. 6 Conclusion We have presented a novel dependency parser using neural networks. Experimental evaluations show that our parser outperforms other greedy parsers using sparse indicator features in both accuracy and speed. This is achieved by representing</context>
</contexts>
<marker>Socher, Bauer, Manning, Ng, 2013</marker>
<rawString>Richard Socher, John Bauer, Christopher D Manning, and Andrew Y Ng. 2013. Parsing with compositional vector grammars. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Andrej Karpathy</author>
<author>Quoc V Le</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Grounded compositional semantics for finding and describing images with sentences.</title>
<date>2014</date>
<publisher>TACL.</publisher>
<contexts>
<context position="29327" citStr="Socher et al., 2014" startWordPosition="4885" endWordPosition="4888">twork to predict parse decisions in a constituency parser. More recently, (Titov and Henderson, 2007) applied Incremental Sigmoid Belief Networks to constituency parsing and then (Garg and Henderson, 2011) extended this work to transition-based dependency parsers using a Temporal Restricted Boltzman Machine. These are very different neural network architectures, and are much less scalable and in practice a restricted vocabulary was used to make the architecture practical. There have been a number of recent uses of deep learning for constituency parsing (Collobert, 2011; Socher et al., 2013). (Socher et al., 2014) has also built models over dependency representations but this work has not attempted to learn neural networks for dependency parsing. Most recently, (Stenetorp, 2013) attempted to build recursive neural networks for transitionbased dependency parsing, however the empirical performance of his model is still unsatisfactory. 6 Conclusion We have presented a novel dependency parser using neural networks. Experimental evaluations show that our parser outperforms other greedy parsers using sparse indicator features in both accuracy and speed. This is achieved by representing all words, POS tags an</context>
</contexts>
<marker>Socher, Karpathy, Le, Manning, Ng, 2014</marker>
<rawString>Richard Socher, Andrej Karpathy, Quoc V. Le, Christopher D. Manning, and Andrew Y. Ng. 2014. Grounded compositional semantics for finding and describing images with sentences. TACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pontus Stenetorp</author>
</authors>
<title>Transition-based dependency parsing using recursive neural networks.</title>
<date>2013</date>
<booktitle>In NIPS Workshop on Deep Learning.</booktitle>
<contexts>
<context position="29495" citStr="Stenetorp, 2013" startWordPosition="4913" endWordPosition="4914">d then (Garg and Henderson, 2011) extended this work to transition-based dependency parsers using a Temporal Restricted Boltzman Machine. These are very different neural network architectures, and are much less scalable and in practice a restricted vocabulary was used to make the architecture practical. There have been a number of recent uses of deep learning for constituency parsing (Collobert, 2011; Socher et al., 2013). (Socher et al., 2014) has also built models over dependency representations but this work has not attempted to learn neural networks for dependency parsing. Most recently, (Stenetorp, 2013) attempted to build recursive neural networks for transitionbased dependency parsing, however the empirical performance of his model is still unsatisfactory. 6 Conclusion We have presented a novel dependency parser using neural networks. Experimental evaluations show that our parser outperforms other greedy parsers using sparse indicator features in both accuracy and speed. This is achieved by representing all words, POS tags and arc labels as dense vectors, and modeling their interactions through a novel cube activation function. Our model only relies on dense features, and is able to automat</context>
</contexts>
<marker>Stenetorp, 2013</marker>
<rawString>Pontus Stenetorp. 2013. Transition-based dependency parsing using recursive neural networks. In NIPS Workshop on Deep Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan Titov</author>
<author>James Henderson</author>
</authors>
<title>Fast and robust multilingual dependency parsing with a generative latent variable model.</title>
<date>2007</date>
<booktitle>In EMNLP-CoNLL.</booktitle>
<contexts>
<context position="28808" citStr="Titov and Henderson, 2007" startWordPosition="4805" endWordPosition="4808">verlap but also major differences from our work here. One big difference is that much early work uses localist one-hot word representations rather than the distributed representations of modern work. (Mayberry III and Miikkulainen, 1999) explored a shift reduce constituency parser with one-hot word representations and did subsequent parsing work in (Mayberry III and Miikkulainen, 2005). (Henderson, 2004) was the first to attempt to use neural networks in a broad-coverage Penn Treebank parser, using a simple synchrony network to predict parse decisions in a constituency parser. More recently, (Titov and Henderson, 2007) applied Incremental Sigmoid Belief Networks to constituency parsing and then (Garg and Henderson, 2011) extended this work to transition-based dependency parsers using a Temporal Restricted Boltzman Machine. These are very different neural network architectures, and are much less scalable and in practice a restricted vocabulary was used to make the architecture practical. There have been a number of recent uses of deep learning for constituency parsing (Collobert, 2011; Socher et al., 2013). (Socher et al., 2014) has also built models over dependency representations but this work has not atte</context>
</contexts>
<marker>Titov, Henderson, 2007</marker>
<rawString>Ivan Titov and James Henderson. 2007. Fast and robust multilingual dependency parsing with a generative latent variable model. In EMNLP-CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
<author>Yoram Singer</author>
</authors>
<title>Feature-rich part-ofspeech tagging with a cyclic dependency network.</title>
<date>2003</date>
<booktitle>In NAACL.</booktitle>
<contexts>
<context position="18817" citStr="Toutanova et al., 2003" startWordPosition="3184" endWordPosition="3187">ations: CoNLL Syntactic Dependencies (CD) (Johansson � L(θ) = − i 744 Dataset #Train #Dev #Test #words (Nw) #POS (Nt) #labels (Nl) projective (%) PTB: CD 39,832 1,700 2,416 44,352 45 17 99.4 PTB: SD 39,832 1,700 2,416 44,389 45 45 99.9 CTB 16,091 803 1,910 34,577 35 12 100.0 Table 3: Data Statistics. “Projective” is the percentage of projective trees on the training set. and Nugues, 2007) using the LTH Constituent-toDependency Conversion Tool3 and Stanford Basic Dependencies (SD) (de Marneffe et al., 2006) using the Stanford parser v3.3.0.4 The POS tags are assigned using Stanford POS tagger (Toutanova et al., 2003) with ten-way jackknifing of the training data (accuracy ≈ 97.3%). For Chinese, we adopt the same split of CTB5 as described in (Zhang and Clark, 2008). Dependencies are converted using the Penn2Malt tool5 with the head-finding rules of (Zhang and Clark, 2008). And following (Zhang and Clark, 2008; Zhang and Nivre, 2011), we use gold segmentation and POS tags for the input. Table 3 gives statistics of the three datasets.6 In particular, over 99% of the trees are projective in all datasets. 4.2 Results The following hyper-parameters are used in all experiments: embedding size d = 50, hidden lay</context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>Kristina Toutanova, Dan Klein, Christopher D. Manning, and Yoram Singer. 2003. Feature-rich part-ofspeech tagging with a cyclic dependency network. In NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laurens van der Maaten</author>
<author>Geoffrey Hinton</author>
</authors>
<title>Visualizing data using t-SNE.</title>
<date>2008</date>
<journal>The Journal of Machine Learning Research.</journal>
<marker>van der Maaten, Hinton, 2008</marker>
<rawString>Laurens van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-SNE. The Journal of Machine Learning Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>A tale of two parsers: Investigating and combining graphbased and transition-based dependency parsing using beam-search.</title>
<date>2008</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="18968" citStr="Zhang and Clark, 2008" startWordPosition="3210" endWordPosition="3213">D 39,832 1,700 2,416 44,352 45 17 99.4 PTB: SD 39,832 1,700 2,416 44,389 45 45 99.9 CTB 16,091 803 1,910 34,577 35 12 100.0 Table 3: Data Statistics. “Projective” is the percentage of projective trees on the training set. and Nugues, 2007) using the LTH Constituent-toDependency Conversion Tool3 and Stanford Basic Dependencies (SD) (de Marneffe et al., 2006) using the Stanford parser v3.3.0.4 The POS tags are assigned using Stanford POS tagger (Toutanova et al., 2003) with ten-way jackknifing of the training data (accuracy ≈ 97.3%). For Chinese, we adopt the same split of CTB5 as described in (Zhang and Clark, 2008). Dependencies are converted using the Penn2Malt tool5 with the head-finding rules of (Zhang and Clark, 2008). And following (Zhang and Clark, 2008; Zhang and Nivre, 2011), we use gold segmentation and POS tags for the input. Table 3 gives statistics of the three datasets.6 In particular, over 99% of the trees are projective in all datasets. 4.2 Results The following hyper-parameters are used in all experiments: embedding size d = 50, hidden layer size h = 200, regularization parameter A = 10−8, initial learning rate of Adagrad α = 0.01. To situate the performance of our parser, we first make </context>
</contexts>
<marker>Zhang, Clark, 2008</marker>
<rawString>Yue Zhang and Stephen Clark. 2008. A tale of two parsers: Investigating and combining graphbased and transition-based dependency parsing using beam-search. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Joakim Nivre</author>
</authors>
<title>Transition-based dependency parsing with rich non-local features.</title>
<date>2011</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="7403" citStr="Zhang and Nivre, 2011" startWordPosition="1224" endWordPosition="1227">denotes POS tag. given configuration. Information that can be obtained from one configuration includes: (1) all the words and their corresponding POS tags (e.g., has / VBZ); (2) the head of a word and its label (e.g., nsubj, dobj) if applicable; (3) the position of a word on the stack/buffer or whether it has already been removed from the stack. Conventional approaches extract indicator features such as the conjunction of 1 - 3 elements from the stack/buffer using their words, POS tags or arc labels. Table 1 lists a typical set of feature templates chosen from the ones of (Huang et al., 2009; Zhang and Nivre, 2011).2 These features suffer from the following problems: • Sparsity. The features, especially lexicalized features are highly sparse, and this is a common problem in many NLP tasks. The situation is severe in dependency parsing, because it depends critically on word-to-word interactions and thus the high-order features. To give a better understanding, we perform a feature analysis using the features in Table 1 on the English Penn Treebank (CoNLL representations). The results given in Table 2 demonstrate that: (1) lexicalized features are indispensable; (2) Not only are the word-pair features (esp</context>
<context position="14440" citStr="Zhang and Nivre, 2011" startWordPosition="2437" endWordPosition="2440"> the input layer directly: g(w1x1 + ... + wmxm + b) = � �(wiwjwk)xixjxk + b(wiwj)xixj... i,j,k i,j In our case, xi, xj, xk could come from different dimensions of three embeddings. We believe that this better captures the interaction of three ele−0.5 0.2 0.4 0.6 0.8 1 743 ments, which is a very desired property of dependency parsing. Experimental results also verify the success of the cube activation function empirically (see more comparisons in Section 4). However, the expressive power of this activation function is still open to investigate theoretically. The choice of Sw, St, Sl Following (Zhang and Nivre, 2011), we pick a rich set of elements for our final parser. In detail, Sw contains nw = 18 elements: (1) The top 3 words on the stack and buffer: s1, s2, s3, b1, b2, b3; (2) The first and second leftmost / rightmost children of the top two words on the stack: lc1(si), rc1(si), lc2(si), rc2(si), i = 1, 2. (3) The leftmost of leftmost / rightmost of rightmost children of the top two words on the stack: lc1(lc1(si)), rc1(rc1(si)), i = 1, 2. We use the corresponding POS tags for St (nt = 18), and the corresponding arc labels of words excluding those 6 words on the stack/buffer for Sl (nl = 12). A good </context>
<context position="19139" citStr="Zhang and Nivre, 2011" startWordPosition="3237" endWordPosition="3240">ercentage of projective trees on the training set. and Nugues, 2007) using the LTH Constituent-toDependency Conversion Tool3 and Stanford Basic Dependencies (SD) (de Marneffe et al., 2006) using the Stanford parser v3.3.0.4 The POS tags are assigned using Stanford POS tagger (Toutanova et al., 2003) with ten-way jackknifing of the training data (accuracy ≈ 97.3%). For Chinese, we adopt the same split of CTB5 as described in (Zhang and Clark, 2008). Dependencies are converted using the Penn2Malt tool5 with the head-finding rules of (Zhang and Clark, 2008). And following (Zhang and Clark, 2008; Zhang and Nivre, 2011), we use gold segmentation and POS tags for the input. Table 3 gives statistics of the three datasets.6 In particular, over 99% of the trees are projective in all datasets. 4.2 Results The following hyper-parameters are used in all experiments: embedding size d = 50, hidden layer size h = 200, regularization parameter A = 10−8, initial learning rate of Adagrad α = 0.01. To situate the performance of our parser, we first make a comparison with our own implementation of greedy arc-eager and arc-standard parsers. These parsers are trained with structured averaged perceptron using the “early-updat</context>
<context position="28060" citStr="Zhang and Nivre, 2011" startWordPosition="4689" endWordPosition="4692">observed in the experiments on indicator feature systems. Thus our model is able to automatically identify the most useful information for predictions, instead of hand-crafting them as indicator features. • More importantly, we can extract features regarding the conjunctions of more than 3 elements easily, and also those not presented in the indicator feature systems. For example, the 3rd feature above captures the conjunction of words and POS tags of s1, the tag of its leftmost child, and also the label between them, while this information is not encoded in the original feature templates of (Zhang and Nivre, 2011). 5 Related Work There have been several lines of earlier work in using neural networks for parsing which have points of overlap but also major differences from our work here. One big difference is that much early work uses localist one-hot word representations rather than the distributed representations of modern work. (Mayberry III and Miikkulainen, 1999) explored a shift reduce constituency parser with one-hot word representations and did subsequent parsing work in (Mayberry III and Miikkulainen, 2005). (Henderson, 2004) was the first to attempt to use neural networks in a broad-coverage Pe</context>
</contexts>
<marker>Zhang, Nivre, 2011</marker>
<rawString>Yue Zhang and Joakim Nivre. 2011. Transition-based dependency parsing with rich non-local features. In ACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
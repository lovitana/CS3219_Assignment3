<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001872">
<title confidence="0.9990255">
Online Topic Model for Twitter Considering Dynamics of User Interests
and Topic Trends
</title>
<author confidence="0.993409">
Kentaro Sasaki, Tomohiro Yoshikawa, Takeshi Furuhashi
</author>
<affiliation confidence="0.986609">
Graduate School of Engineering Nagoya University
</affiliation>
<email confidence="0.9796495">
sasaki@cmplx.cse.nagoya-u.ac.jp
yoshikawa, furuhashi@cse.nagoya-u.ac.jp
</email>
<sectionHeader confidence="0.993572" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999633095238095">
Latent Dirichlet allocation (LDA) is a
topic model that has been applied to var-
ious fields, including user profiling and
event summarization on Twitter. When
LDA is applied to tweet collections, it gen-
erally treats all aggregated tweets of a user
as a single document. Twitter-LDA, which
assumes a single tweet consists of a single
topic, has been proposed and has shown
that it is superior in topic semantic coher-
ence. However, Twitter-LDA is not capa-
ble of online inference. In this study, we
extend Twitter-LDA in the following two
ways. First, we model the generation pro-
cess of tweets more accurately by estimat-
ing the ratio between topic words and gen-
eral words for each user. Second, we en-
able it to estimate the dynamics of user in-
terests and topic trends online based on the
topic tracking model (TTM), which mod-
els consumer purchase behaviors.
</bodyText>
<sectionHeader confidence="0.99899" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999944543859649">
Microblogs such as Twitter, have prevailed rapidly
in our society recently. Twitter users post a mes-
sage using 140 characters, which is called a tweet.
The characters limit allows users to post tweets
easily about not only personal interest or real life
but also public events such as traffic accidents
or earthquakes. There have been many studies
on how to extract and utilize such information
on tweets (Diao et al., 2012; Pennacchiotti and
Popescu, 2011; Sakaki et al., 2010; Weng et al.,
2010).
Topic models, such as latent Dirichlet alloca-
tion (LDA) (Blei et al., 2003) are widely used to
identify latent topic structure in large collections
of documents. Recently, some studies have ap-
plied LDA to Twitter for user classification (Pen-
nacchiotti and Popescu, 2011), detection of influ-
ential users (Weng et al., 2010), and so on. LDA
is a generative document model, which assumes
that each document is represented as a probabil-
ity distribution over some topics, and that each
word has a latent topic. When we apply LDA
to tweets, each tweet is treated as a single docu-
ment. This direct application does not work well
because a tweet is very short compared with tradi-
tional media such as newspapers. To deal with the
shortness of a tweet, some studies aggregated all
the tweets of a user as a single document (Hong
and Davison, 2010; Pennacchiotti and Popescu,
2011; Weng et al., 2010). On the other hand, Zhao
et al. (2011) proposed “Twitter-LDA,” which is
a model that considers the shortness of a tweet.
Twitter-LDA assumes that a single tweet consists
of a single topic, and that tweets consist of topic
and background words. Zhao et al. (2011) show
that it works well at the point of semantic coher-
ence of topics compared with LDA. However, as
with the case of LDA, Twitter-LDA cannot con-
sider a sequence of tweets because it assumes that
samples are exchangeable. In Twitter, user inter-
ests and topic trends are dynamically changing.
In addition, when new data comes along, a new
model must be generated again with all the data
in Twitter-LDA because it does not assume online
inference. Therefore, it cannot efficiently analyze
the large number of tweets generated everyday. To
overcome these difficulties, a model that considers
the time sequence and has the capability of online
inference is required.
In this study, we first propose an improved
model based on Twitter-LDA, which assumes that
the ratio between topic and background words dif-
fers for each user. This study evaluates the pro-
posed method based on perplexity and shows the
efficacy of the new assumption in the improved
model. Second, we propose a new topic model
called “Twitter-TTM” by extending the improved
</bodyText>
<page confidence="0.959738">
1977
</page>
<note confidence="0.9005325">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1977–1985,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.997891166666667">
model based on the topic tracking model (TTM)
(Iwata et al., 2009), which models the purchase
behavior of consumers and is capable of online
inference. Finally, we demonstrate that Twitter-
TTM can effectively capture the dynamics of user
interests and topic trends in Twitter.
</bodyText>
<sectionHeader confidence="0.671426" genericHeader="method">
2 Improvement of Twitter-LDA
</sectionHeader>
<subsectionHeader confidence="0.957731">
2.1 Improved-Model
</subsectionHeader>
<bodyText confidence="0.999967233333333">
Figure 1(a) shows the graphical representation of
Twitter-LDA based on the following assumptions.
There are K topics in Twitter and each topic is rep-
resented by a topic word distribution. Each user
has his/her topic interests ou represented by a dis-
tribution over K topics. Topic k is assigned to
each tweet of user u depending on the topic inter-
ests ou. Each word in the tweet assigned by topic
k is generated from a background word distribu-
tion BB or a topic word distribution Bk. Whether
the word is a background word or a topic word
is determined by a latent value y. When y = 0,
the word is generated from the background word
distribution BB, and from the topic word distribu-
tion Bk when y = 1. The latent value y is chosen
according to a distribution 7r. In other words, the
ratio between background and topic words is de-
termined by 7r.
In Twitter-LDA, 7r is common for all users,
meaning that the rate between background and
topic words is the same for each user. However,
this assumption could be incorrect, and the rate
could differ for each user. Thus, we develop an
improved model based on Twitter-LDA, which as-
sumes that 7r is different for each user, as shown
in Figure 1(b). In the improved model, the rate
between background and topic words for user u is
determined by a user-specific distribution 7ru. The
improved model is expected to infer the generative
process of tweets more efficiently.
</bodyText>
<subsectionHeader confidence="0.975313">
2.2 Experiment for Improved Model
</subsectionHeader>
<bodyText confidence="0.999584">
We performed an experiment to compare the pre-
dictive performances of LDA, TTM, and the im-
proved model shown in Section 2.1. In this ex-
periment, LDA was applied as the method to ag-
gregate all tweets of a user as a single document.
The original Twitter data set contains 14,305 users
and 292,105 tweets collected on October 18, 2013.
We then removed words that occurred less than
20 times and stop words. Retweetsl were treated
</bodyText>
<subsectionHeader confidence="0.718846">
1Republishing a tweet written by another Twitter user.
</subsectionHeader>
<bodyText confidence="0.999782933333333">
as the same as other general tweets because they
reflected the user’s interests. After the above
preprocessing, we obtained the final dataset with
14,139 users, 252,842 tweets, and 7,763 vocab-
ularies. Each model was inferred with collapsed
Gibbs sampling (Griffiths and Steyvers, 2004) and
the iteration was set at 500. For a fair comparison,
the hyper parameters in these models were opti-
mized in each Gibbs sampling iteration by max-
imizing likelihood using fixed iterations (Minka,
2000).
This study employs perplexity as the evaluation
index, which is the standard metric in information
retrieval literature. The perplexity of a held-out
test set is defined as
</bodyText>
<equation confidence="0.97958575">
� )log p(w„) (1)
perplexity = exp (− 1
N
u
</equation>
<bodyText confidence="0.99994337037037">
where w„ represents words are contained in the
tweets of user u and N is the number of words in
the test set. A lower perplexity means higher pre-
dictive performance. We set the number of topics
K at 50, 100, 150, 200, and 250 and evaluated the
perplexity for each model in each K via a 10-fold
cross-validation.
The results are shown in Table 1, which shows
that the improved model performs better than the
other models for any K. Therefore, the new as-
sumption of the improved model, that the rate be-
tween background and topic words is different for
each user, could be more appropriate. LDA per-
formance worsens with an increase in K because
the aggregated tweets of a single user neglect the
topic of each tweet.
Table 2 shows examples of the tweets of users
with high and low rates of background words. The
users with a high background words rate tend to
use basic words that are often used in any top-
ics, such as “like,” “about,” and “people,” and they
tend to tweet about their personal lives. On the
other hand, for users with a low background words
rate, topical words are often used such as “Arse-
nal,” “Justin,” and “Google”. They tend to tweet
about their interests, including music, sports, and
movies.
</bodyText>
<sectionHeader confidence="0.998456" genericHeader="method">
3 Twitter-TTM
</sectionHeader>
<subsectionHeader confidence="0.999521">
3.1 Model Extension based on Topic
Tracking Model
</subsectionHeader>
<bodyText confidence="0.971259">
We extend the improved model shown in Section
2.1 considering the time sequence and capabil-
</bodyText>
<page confidence="0.961036">
1978
</page>
<figure confidence="0.999375866666666">
s y
Y
fl
0k
K
�
w
Q
z
Nr,s
Nr
U
0B
A
Y
Y
y
7C
�
0k
K
w
Q
z
Nr,s
Nr
U
0B
A
(a) Twitter-LDA (b) Improved-model
</figure>
<figureCaption confidence="0.996284">
Figure 1: Graphical representation of Twitter-LDA and Improved-model
</figureCaption>
<tableCaption confidence="0.413434">
Table 1: Perplexity of each model in 10 runs
</tableCaption>
<table confidence="0.783455666666667">
Number of topic K LDA Twitter-LDA Improved-model
50 1586.7 (14.4) 2191.0 (28.4) 1555.3 (36.7)
100 1612.7 (11.9) 1933.9 (23.6) 1471.7 (22.3)
150 1635.3 (11.2) 1760.1 (15.7) 1372.3 (20.0)
200 1655.2 (13.0) 1635.4 (22.1) 1289.5 (13.3)
250 1672.7 (17.2) 1542.8 (12.5) 1231.1 (11.9)
</table>
<tableCaption confidence="0.985735">
Table 2: Example of tweets of users with high and low rate of background words
</tableCaption>
<bodyText confidence="0.999534419354839">
High rate of background words Low rate of background words
I hope today goes quickly Team Arsenal v will Ozil be
I want to work in a cake Making Justin smile and laugh as he is working on music
All need your support please Google nexus briefly appears in Google play store
ity of online inference based on TTM (Iwata et
al., 2009). TTM is a probabilistic consumer pur-
chase behavior model based on LDA for track-
ing the interests of each user and the trends in
each topic. Other topic models considering the dy-
namics of topics include the dynamic topic model
(DTM) (Blei and Lafferty, 2006) and topic over
time (ToT) (Wang and McCallum, 2006). DTM is
a model for analyzing the time evolution of top-
ics in time-ordered document collections. It does
not track the interests of each user as shown in
Figure 2(a) because it assumes that a user (doc-
ument) has only one time stamp. ToT requires all
the data over time for inference, thus, it is not ap-
propriate for application to continuously generated
data such as Twitter. We consider a model must be
capable of online inference and track the dynam-
ics of user interests and topic trends for modeling
tweets. Since TTM has these abilities, we adapt it
to the improved model described in Section 2.
Figure 2(b) shows the graphical representation
of TTM. TTM assumes that the mean of user in-
terests at the current time is the same as that at the
previous time, unless new data is observed. For-
mally, the current interest ϕt,u are drawn from the
following Dirichlet distribution in which the mean
is the previous interest �ϕt_1,u and the precision is
</bodyText>
<page confidence="0.970925">
1979
</page>
<figure confidence="0.999972333333333">
(a) DTM
Bk K Bk K
ft ft
(b) TTM
t-1 t
Bk
0
w
α α
z
Nu
g
U
Bk
0
w
z
g
Nu
U
r-1 r
0
w
α
z
Nu
U
0
w
α
z
Nu
U
</figure>
<figureCaption confidence="0.996786">
Figure 2: Graphical representation of DTM and TTM
</figureCaption>
<bodyText confidence="0.587652">
αt,u
</bodyText>
<equation confidence="0.983919">
αt,uˆϕt−1,u,k−1 2
Ot,u,k ( )
</equation>
<bodyText confidence="0.999987666666667">
where Ot,u,k represents the probability that user u
is interested in topic k at time t. t is a discrete
variable and can be arbitrarily set as the unit time
interval, e.g., at one day or one week. The preci-
sion αt,u represents the interest persistence of how
consistently user u maintains his/her interests at
time t compared with the previous time t −1. αt,u
is estimated for each time period and each user
because interest persistence depends on both time
and users. As mentioned above, the current topic
trend Bt,k is drawn from the following Dirichlet
distribution with the previous trend �Bt−1,k
</bodyText>
<equation confidence="0.949993333333333">
�p(Bt,k|�Bt−1,k, ,3t,k) ∝ Bβt,kˆθt−1,k,v−1 (3)
t,k,v
v
</equation>
<bodyText confidence="0.999867588235294">
where Bt,k,v represents the probability that word v
is chosen in topic k at time t.
Here our proposed Twitter-TTM adapts the
above TTM assumptions to the improved model.
That is, we extend the improved model whereby
user interest Ot,u and topic trend Bt,k depend on
previous states. Time dependency is not consid-
ered on BB and πu because they can be regarded
as being independent of time.
Figures 3 and 4 show the generative process
and a graphical representation of Twitter-TTM, re-
spectively. Twitter-TTM can capture the dynam-
ics of user interests and topic trends in Twitter
considering the features of tweets online. More-
over, Twitter-TTM can be extended to capture
long-term dependences, as described in Iwata et
al. (2009).
</bodyText>
<subsectionHeader confidence="0.998946">
3.2 Model Inference
</subsectionHeader>
<bodyText confidence="0.998804764705883">
We use a stochastic expectation-maximization al-
gorithm for Twitter-TTM inference, as described
in Wallach (2006) in which Gibbs sampling of la-
tent values and maximum joint likelihood estima-
tion of parameters are alternately iterated. At time
t, we estimate user interests 4)t = { O U
t,u }U=1
topic trends Ot = {�Bt,k}Kk=1, background word
distribution Bt,B, word usage rate distribution πt,u,
interest persistence parameters αt = {αt,u}U u=1,
and trend persistence parameters )3t = {,3t,k}Kk=1
using the previous time interests 4)t−1 and trends
�Ot−1.
We employ collapsed Gibbs sampling to infer
the latent variables. Let Dt be a set of tweets and
Zt, Yt be a set of latent variables z, y at time t. We
can integrate the parameters in the joint distribu-
</bodyText>
<equation confidence="0.969923461538462">
p(Ot,u |��Ot−1,u, αt,u) ∝
k
1980
tion as follows:
p(Dt, Yt, Zt |ˆ4t−1, ˆOt−1, αt, βt, λ,γ)
(Γ(2γ) )U ∏ Γ(γ + nt,u,B)Γ(γ + nt,u,K)
= Γ(γ)2 Γ(2γ + nt,u)
u
∏ vΓ(nt,B,v + λ)
Γ(nt,B + V λ)
Γ(βt,k) ∏ Γ(nt,k,v + βt,k
Γ(nt,k + βt,k) Γ(βt,k ˆθt−1
v
</equation>
<bodyText confidence="0.997857">
of user u at time t, and \j represents a count ex-
cluding the j-th word.
The persistence parameters αt and βt are esti-
mated by maximizing the joint likelihood eq.(4),
using a fixed point iteration (Minka, 2000). The
update formulas are as follows:
</bodyText>
<equation confidence="0.967130428571429">
∑ ˆϕt−1,u,kAt,u,k
αnew
t,u = αt,u Ψ(ct,u + αt,u) − Ψ(αt,u),
k
(8)
Γ(V λ)
XΓ(λ)V
∏X
k
∏X
u
ˆθt−1,k,v)
,k,v)
ˆϕt−1,u,k)
</equation>
<bodyText confidence="0.997657384615385">
where nt,u,B and nt,u,K are the number of back-
ground and topic words of user u at time t, nt,B,v
is the number of times that word v is assigned as
a background word at time t, nt,k,v is the num-
ber of times that word v is assigned to topic k
at time t, ct,u,k is the number of tweets assigned
to topic k for user u at time t. In addition,
nt,u = nt,u,B + nt,uK, nt,B = ∑v nt,B,v, nt,K =
∑k nt,k = ∑k ∑v nt,k,v, nt,u = ∑k nt,u,k, and
ct,u = ∑k ct,u,k.
Given the assignment of all other latent vari-
ables, we derive the following formula calculated
from eq.(4) to infer a latent topic,
</bodyText>
<equation confidence="0.9986205">
p(zi = k|Dt, Yt, Zt\i, ˆ4t−1, ˆOt−1, αt, βt)
Γ(nt,k,v + βt,kˆθt−1,k,v)
,
Γ(nt,k,v\i + βt,kˆθt−1,k,v)
</equation>
<bodyText confidence="0.9999448">
where i = (t, u, s), thus zi represents a topic as-
signed to the s-th tweet of user u at time t, and \i
represents a count excluding the i-th tweet.
Then, when zi = k is given, we derive the fol-
lowing formula to infer a latent variable yj,
</bodyText>
<equation confidence="0.941708">
a nt,k,v\j + βt,kˆθt−1,k,v nt,u,K\j + γ(7)
nt,k\j + βt,k nt,u\j + 2γ
</equation>
<bodyText confidence="0.999765">
where j = (t, u, s, n), thus yj represents a latent
variable assigned to the n-th word in the s-th tweet
</bodyText>
<equation confidence="0.9560168">
ˆϕt−1,u,k), and
βnew v
t,k = βt,k Ψ(nt,k + βt,k) − Ψ(βt,k),
∑ ˆθt−1,k,vBt,k,v
(9)
</equation>
<bodyText confidence="0.999823142857143">
where Bt,k,v = Ψ(nt,k,v + βt,kˆθt−1,k,v) −
Ψ(βt,kˆθt−1,k,v). We can estimate latent variables
Zt, Yt, and parameters αt and βt by iterating
Gibbs sampling with eq.(5), eq.(6), and eq.(7) and
maximum joint likelihood with eq.(8) and eq.(9).
After the iterations, the means of ϕt,u,k and θt,k,v
are obtained as follows.
</bodyText>
<equation confidence="0.9964545">
ct,u,k + αt,uˆϕt−1,u,k (10)
ϕt,u,k =
ct,u + αt,u
nt,k,v + βt,kˆθt−1,k,v (11)
θt,k,v =
nt,k + βt,k
</equation>
<bodyText confidence="0.997670666666667">
These estimates are used as the hyper parameters
of the prior distributions at the next time period
t + 1.
</bodyText>
<sectionHeader confidence="0.99996" genericHeader="method">
4 Related Work
</sectionHeader>
<bodyText confidence="0.999808466666667">
Recently, topic models for Twitter have been pro-
posed. Diao et al. (2012) proposed a topic
model that considers both the temporal informa-
tion of tweets and user’s personal interests. They
applied their model to find bursty topics from
Twitter. Yan et al. (2013) proposed a biterm
topic model (BTM), which assumes that a word-
pair is independently drawn from a specific topic.
They demonstrated that BTM can effectively cap-
ture the topics within short texts such as tweets
compared with LDA. Chua and Asur (2013) pro-
posed two topic models considering time order
and tweet intervals to extract the tweets summa-
rizing a given event. The models mentioned above
do not consider the dynamics of user interests, nor
</bodyText>
<equation confidence="0.927266764705883">
ct,u,k\i + αt,u
a
ct,u\i + αt,u
Γ(nt,k\i + βt,k)
Γ(nt,k + βt,k)
ˆϕt−1,u,k
Γ(αt,u) ∏ Γ(ct,u,k + αt,u
Γ(ct,u + αt,u) k Γ(αt,uˆϕt−
∏X
v
(5)
p(yj = 0|Dt, Yt\j, Zt, λ, γ)
nt,B,v\j + λ
a nt,B\j + V λ
nt,u,B\j + γ (6)
nt,u\j + 2γ ,
p(yj = 1|Dt, Yt\j, Zt, ˆOt−1, βt, γ)
</equation>
<figure confidence="0.935914105263158">
where At,u,k = Ψ(ct,u,k + αt,uˆϕt−1,u,k) −
,
1,u,k) Ψ(αt,u
(4)
1981
1. Draw Bt,B ∼Dirichlet(λ)
2. For each topic k = 1, ..., K,
(a) draw Bt,k ∼Dirichlet(Qt,k �Bt−1,k)
3. For each user u = 1, ..., U,
(a) draw ot,u ∼Dirichlet(αt,u�ot−1,u)
(b) draw 7rt,u ∼Beta(γ)
(c) for each tweet s = 1, ..., Nu
i. draw zt,u,s ∼Multinomial(ot,u)
ii. for each word n = 1, ..., Nu,s
A. draw yt,u,s,n ∼Bernoulli(7rt,u)
B. draw wt,u,s,n ∼
Multinomial(Bt,B) if yt,u,s,n = 0
or Multinomial(Bt,zt,u,B)
if yt,u,s,n = 1
</figure>
<figureCaption confidence="0.9459155">
Figure 3: Generative process of tweets in Twitter-
TTM
</figureCaption>
<bodyText confidence="0.997110066666667">
do they have the capability of online inference;
thus, they cannot efficiently model the large num-
ber of tweets generated everyday, whereas Twitter-
TTM can capture the dynamics of user interests
and topic trends and has the capability of online
inference.
Some online topic models have also been pro-
posed. TM-LDA was proposed by Wang et al.
(2012), which can efficiently model online the top-
ics and topic transitions that naturally arise in a
tweet stream. Their model learns the transition
parameters among topics by minimizing the pre-
diction error on topic distribution in subsequent
tweets. However, the TM-LDA does not con-
sider dynamic word distributions. In other words,
their model can not capture the dynamics of topic
trends. Lau et al. (2012) proposed a topic model
implementing a dynamic vocabulary based on on-
line LDA (OLDA) (AlSumait et al., 2008) and ap-
plied it to track emerging events on Twitter. An
online variational Bayes algorithm for LDA is also
proposed (Hoffman et al., 2010). However, these
methods are based on LDA and do not consider
the shortness of a tweet. Twitter-TTM tackles
the shortness of a tweet by assuming that a single
tweet consists of a single topic. This assumption
is based on the following observation: a tweet is
much shorter than a normal document, so a single
tweet rarely contains multiple topics but rather a
single one.
</bodyText>
<figureCaption confidence="0.833045">
Figure 4: Graphical model of Twitter-TTM
</figureCaption>
<sectionHeader confidence="0.992729" genericHeader="evaluation">
5 Experiment
</sectionHeader>
<subsectionHeader confidence="0.993172">
5.1 Setting
</subsectionHeader>
<bodyText confidence="0.99999652173913">
We evaluated the effectiveness of the proposed
Twitter-TTM using an actual Twitter data set. The
original Twitter data set contains 15,962 users and
4,146,672 tweets collected from October 18 to 31,
2013. We then removed words that occurred less
than 30 times and stop words. After this prepro-
cessing, we obtained the final data set with 15,944
users, 3,679,481 tweets, and 30,096 vocabularies.
We compared the predictive performance of
Twitter-TTM with LDA, TTM, Twitter-LDA,
Twitter-LDA+TTM, and the improved model
based on the perplexity for the next time tweets.
Twitter-LDA+TTM is a combination of Twitter-
LDA and TTM. It is equivalent to Twitter-TTM,
except that the rate between background and topic
words is different for each user. We set the num-
ber of topics K at 100, the iteration of each model
at 500, and the unit time interval at one day. The
hyper parameters in these models were optimized
in each Gibbs sampling iteration by maximizing
likelihood using fixed iterations (Minka, 2000).
The inferences of LDA, Twitter-LDA, and the im-
proved model were made for current time tweets.
</bodyText>
<subsectionHeader confidence="0.988539">
5.2 Result
</subsectionHeader>
<bodyText confidence="0.999897666666667">
Figure 5 shows the perplexity of each model for
each time, where t = 1 in the horizontal axis rep-
resents October 18, t = 2 represents October 19,
..., and t = 13 represents October 31. The perplex-
ity at time t represents the predictive performance
of each model inferred by previous time tweets to
</bodyText>
<figure confidence="0.999131258064516">
r-1 r
a
αa
0
0
Y
Ir
Y
Ir
Z
Nu
x
Z
Nu
r
w
Bk
Nu,s
U
BB
r
Bk
w
Nu,s
U
g
a
g
a
BB
x
</figure>
<page confidence="0.992117">
1982
</page>
<bodyText confidence="0.999854928571429">
the current time tweets. Note that at t = 1, the per-
formance of LDA and TTM, that of Twitter-LDA
and Twitter-LDA+TTM, and that of Twitter-TTM
and the improved model were found to be equiva-
lent.
As shown in Figure 5(a), the proposed Twitter-
TTM shows lower perplexity compared with con-
ventional models, such as LDA, Twitter-LDA, and
TTM at any time, which implies that Twitter-TTM
can appropriately model the dynamics of user in-
terests and topic trends in Twitter. TTM could
not have perplexity lower than LDA although it
considers the dynamics. If LDA could not ap-
propriately model the tweets, then the user inter-
</bodyText>
<figure confidence="0.995425533333333">
t
perplexity
4500
4000
3500
3000
2500
2000
1500
1000
LDA
Twitter-LDA
TTM
Twitter-TTM
1 2 3 4 5 6 7 8 9 10 11 12 13
</figure>
<bodyText confidence="0.984135689655172">
ests �`)t_1 and topic trends Ot_1 in the previous
time are not estimated well in TTM. Figure 5(b)
shows the perplexities of the improved model and
Twitter-TTM. From t = 2, Twitter-TTM shows
lower perplexity than the improved model for each
time. The reason for the high perplexity of the im-
proved model is that it does not consider the dy-
namics. Twitter-TTM also shows lower perplexity
than Twitter-LDA+TTM for each time, as shown
in Figure 5(c), because Twitter-TTM’s assumption
that the rate between background and topic words
is different for each user is more appropriate, as
demonstrated in Section 2.2. These results imply
that Twitter-TTM also outperforms other conven-
tional methods, such as DTM, OLDA, and TM-
LDA, which do not consider the shortness of a
tweet or the dynamics of user interests or topic
trends .
Table 3 shows two topic examples of the topic
evolution analyzed by Twitter-TTM, and Figure 6
shows the trend persistence parameters β of each
topic at each time. The persistence parameters of
the topic “Football” are lower than those of “Birth-
day” because it is strongly affected by trends in the
real world. In fact, the top words in “Football”
change more dynamically than those of “Birth-
day.” For example, in the “Football” topic, though
‘Arsenal’ is usually popular, ‘Madrid’ becomes
more popular on October 24.
</bodyText>
<sectionHeader confidence="0.999486" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9999445">
We first proposed an improved model based
on Twitter-LDA, which estimates the rate be-
tween background and topic words for each user.
We demonstrated that the improved model could
model tweets more efficiently than LDA and
Twitter-LDA. Next we proposed a novel proba-
</bodyText>
<figure confidence="0.697666">
(c) Comparison with Twitter-LDA+TTM
</figure>
<figureCaption confidence="0.996635">
Figure 5: Perplexity for each time
</figureCaption>
<bodyText confidence="0.999759666666667">
bilistic topic model for Twitter, called Twitter-
TTM, which can capture the dynamics of user in-
terests and topic trends and is capable of online
inference. We evaluated Twitter-TTM using an ac-
tual Twitter data set and demonstrated that it could
model more accurately tweets than conventional
</bodyText>
<figure confidence="0.998351772727273">
(a) Comparison with LDA, Twitter-LDA, and TTM
perplexity 4500 Improved-Model
4000 Twitter-TTM
3500
3000
2500
2000
1500
1000
1 2 3 4 5 6 7 8 9 10 11 12 13
t
(b) Comparison with Improved-model
perplexity 4500 Twitter-LDA+TTM
4000 Twitter-TTM
3500
3000
2500
2000
1500
1000
1 2 3 4 5 6 7 8 9 10 11 12 13
t
</figure>
<page confidence="0.95449">
1983
</page>
<bodyText confidence="0.998409363636364">
methods.
The proposed method currently needs to prede-
termine the number of topics each time, and it is
fixed. In future work, we plan to extend the pro-
posed method to capture the birth and death of
topics along the timeline with a variable number
of topics, such as the model proposed by Ahmed
(Ahmed and Xing, 2010). We also plan to ap-
ply the proposed method to content recommenda-
tions and trend analysis in Twitter to investigate
this method further.
</bodyText>
<sectionHeader confidence="0.998275" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999457193181818">
Amr Ahmed and Eric P. Xing. 2010. Timeline: A
dynamic hierarchical Dirichlet process model for re-
covering birth/death and evolution of topics in text
stream. In Proceedings of the 26th Conference on
Uncertainty in Artificial Intelligence (UAI), 20–29.
Loulwah AlSumait, Daniel Barbar´a and Carlotta
Domeniconi. 2008. On-line LDA: Adaptive topic
models for mining text streams with applications
to topic detection and tracking. In Proceedings of
the IEEE International Conference on Data Mining
(ICDM), 3-12.
David M. Blei., and John D. Lafferty. 2006. Dynamic
topic models. In Proceedings of the 23rd Inter-
national Conference on Machine learning (ICML),
113-120.
David M. Blei, Andrew Y. Ng and Michael I. Jordan.
2003. Latent dirichlet allocation. Journal of Ma-
chine Learning Research, 3: 993-1022.
Freddy C. T. Chua and Sitaram Asur. 2013. Automatic
summarization of events from social media. In Pro-
ceedings of the International AAAI Conference on
Weblogs and Social Media (ICWSM).
Qiming Diao, Jing Jiang, Feida Zhu and Ee-Peng Lim
2012. Finding bursty topics from microblogs. In
Proceedings of the 50th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), 536–
544.
Thomas L. Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. In Proceedings of the National
Academy of Sciences of the United States of Amer-
ica,101(1):5228-5235.
Matthew D. Hoffman, Francis Bach and David M. Blei.
2010. Online learning for latent dirichlet allocation.
In Proceedings of the Advances in Neural Informa-
tion Processing Systems (NIPS), 856–864.
Liangjie Hong and Brian D. Davison. 2010. Empiri-
cal study of topic modeling in twitter. In Proceed-
ings of the First Workshop on Social Media Analyt-
ics (SOMA), 80–88.
Tomoharu Iwata, Shinji Watanabe, Takeshi Yamada.
and Naonori Ueda. 2009. Topic tracking model for
analyzing consumer purchase behavior. In Proceed-
ings of the International Joint Conferences on Arti-
ficial Intelligence (IJCAI),1427–1432.
JeyHan Lau, Nigel Collier and Timothy Baldwin.
2012. On-line trend analysis with topic models:#
twitter trends detection topic model online. In Pro-
ceedings of the 23th International Conference on
Computational Linguistics (COLING), 1519–1534.
Thomas P. Minka 2000. Estimating a Dirichlet distri-
bution Technical report, MIT.
Marco Pennacchiotti and Ana-Maria Popescu. 2011.
A machine learning approach to Twitter user clas-
sification. In Proceedings of the International AAAI
Conference on Weblogs and Social Media (ICWSM),
281–288.
Takeshi Sakaki, Makoto Okazaki and Yutaka Matsuo.
2010. Earthquake shakes Twitter users: realtime
event detection by social sensors. In Proceedings of
the World Wide Web Conference (WWW), 851–860.
Hanna M. Wallach 2006. Topic modeling: beyond
bag-of-words. In Proceedings of the 23rd Inter-
national Conference on Machine Learning (ICML),
977–984.
Xuerui Wang and Andrew McCallum. 2006. Topics
over time: a non-Markov continuous-time model of
topical trends. In Proceedings of the International
Conference on Knowledge Discovery and Data Min-
ing (KDD), 424–433.
Yu Wang, Eugene Agichtein and Michele Benz. 2012.
TM-LDA: efficient online modeling of the latent
topic transitions in social media. In Proceedings of
the International Conference on Knowledge Discov-
ery and Data Mining (KDD), 123–131.
Jianshu Weng, Ee Peng Lim, Jing Jiang and Qi He.
2010. Twitterrank: finding topic-sensitive influen-
tial twitterers. In Proceedings of the 3rd ACMInter-
national Conference on Web Search and Data Min-
ing (WSDM), 261–270.
Xiaohui Yan, Jiafeng Guo, Yanyan Lan and Xueqi
Cheng 2013. A biterm topic model for short texts.
In Proceedings of the World Wide Web Conference
(WWW), 1445–1456.
Wayne Xin Zhao, Jing Jiang, Jianshu Weng, Jing He,
Ee-Peng Lim, Hongfei Yan and Xiaoming Li. 2011.
Comparing twitter and traditional media using topic
models. In Advances in Information Retrieval, 338–
349.
</reference>
<page confidence="0.986125">
1984
</page>
<figure confidence="0.997958916666667">
β
600
500
400
300
200
100
0
10/19 10/20 10/21 10/22 10/23 10/24
Birthday
Football
date
</figure>
<figureCaption confidence="0.999186">
Figure 6: Trend persistence parameters β of each topic at each time estimated by Twitter-TTM
</figureCaption>
<tableCaption confidence="0.990965">
Table 3: Two examples of topic evolution analyzed by Twitter-TTM
</tableCaption>
<table confidence="0.999428">
Label Date Top words
Birthday 10/18 birthday,happy,maria,hope,good,love,thanks,bday,lovely,enjoy
10/19 happy,birthday,good,hope,thank,enjoy,love,bday,lovely,great
10/20 birthday,happy,hope,good,love,lovely,great,enjoy,thank,beautiful
10/21 birthday,happy,hope,good,beautiful,love,lovely,bday,great,thank
10/22 birthday,happy,hope,good,beautiful,love,bless,thank,today,bday
10/23 birthday,happy,thank,good,love,hope,beautiful,enjoy,channing,wish
10/24 birthday,happy,thank,love,hope,good,beautiful,fresh,thanks,jamz
Football 10/18 arsenal,ozil,game,team,cazorla,league,wenger,play,season,good
10/19 goal,liverpool,gerrard,arsenal,ozil,league,newcastle,suarez,goals,team
10/20 arsenal,ozil,goal,ramsey,norwich,goals,league,wilshere,mesut,premier
10/21 arsenal,goal,goals,league,townsend,spurs,player,season,wenger,ozil
10/22 arsenal,goal,wenger,ozil,league,arsene,goals,birthday,happy,team
10/23 arsenal,dortmund,ozil,fans,wilshere,borussia,ramsey,lewandowski,giroud,league
10/24 madrid,goals,ronaldo,cska,real,league,city,moscow,champions,yaya
</table>
<page confidence="0.969359">
1985
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.732629">
<title confidence="0.995307">Online Topic Model for Twitter Considering Dynamics of User and Topic Trends</title>
<author confidence="0.987236">Kentaro Sasaki</author>
<author confidence="0.987236">Tomohiro Yoshikawa</author>
<author confidence="0.987236">Takeshi</author>
<affiliation confidence="0.983781">Graduate School of Engineering Nagoya</affiliation>
<email confidence="0.819138">yoshikawa,furuhashi@cse.nagoya-u.ac.jp</email>
<abstract confidence="0.996833772727273">Latent Dirichlet allocation (LDA) is a topic model that has been applied to various fields, including user profiling and event summarization on Twitter. When LDA is applied to tweet collections, it generally treats all aggregated tweets of a user as a single document. Twitter-LDA, which assumes a single tweet consists of a single topic, has been proposed and has shown that it is superior in topic semantic coherence. However, Twitter-LDA is not capable of online inference. In this study, we extend Twitter-LDA in the following two ways. First, we model the generation process of tweets more accurately by estimating the ratio between topic words and general words for each user. Second, we enable it to estimate the dynamics of user interests and topic trends online based on the topic tracking model (TTM), which models consumer purchase behaviors.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Amr Ahmed</author>
<author>Eric P Xing</author>
</authors>
<title>Timeline: A dynamic hierarchical Dirichlet process model for recovering birth/death and evolution of topics in text stream.</title>
<date>2010</date>
<booktitle>In Proceedings of the 26th Conference on Uncertainty in Artificial Intelligence (UAI),</booktitle>
<marker>Ahmed, Xing, 2010</marker>
<rawString>Amr Ahmed and Eric P. Xing. 2010. Timeline: A dynamic hierarchical Dirichlet process model for recovering birth/death and evolution of topics in text stream. In Proceedings of the 26th Conference on Uncertainty in Artificial Intelligence (UAI), 20–29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Loulwah AlSumait</author>
<author>Daniel Barbar´a</author>
<author>Carlotta Domeniconi</author>
</authors>
<title>On-line LDA: Adaptive topic models for mining text streams with applications to topic detection and tracking.</title>
<date>2008</date>
<booktitle>In Proceedings of the IEEE International Conference on Data Mining (ICDM),</booktitle>
<pages>3--12</pages>
<marker>AlSumait, Barbar´a, Domeniconi, 2008</marker>
<rawString>Loulwah AlSumait, Daniel Barbar´a and Carlotta Domeniconi. 2008. On-line LDA: Adaptive topic models for mining text streams with applications to topic detection and tracking. In Proceedings of the IEEE International Conference on Data Mining (ICDM), 3-12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>John D Lafferty</author>
</authors>
<title>Dynamic topic models.</title>
<date>2006</date>
<booktitle>In Proceedings of the 23rd International Conference on Machine learning (ICML),</booktitle>
<pages>113--120</pages>
<contexts>
<context position="9522" citStr="Blei and Lafferty, 2006" startWordPosition="1620" endWordPosition="1623">gh and low rate of background words High rate of background words Low rate of background words I hope today goes quickly Team Arsenal v will Ozil be I want to work in a cake Making Justin smile and laugh as he is working on music All need your support please Google nexus briefly appears in Google play store ity of online inference based on TTM (Iwata et al., 2009). TTM is a probabilistic consumer purchase behavior model based on LDA for tracking the interests of each user and the trends in each topic. Other topic models considering the dynamics of topics include the dynamic topic model (DTM) (Blei and Lafferty, 2006) and topic over time (ToT) (Wang and McCallum, 2006). DTM is a model for analyzing the time evolution of topics in time-ordered document collections. It does not track the interests of each user as shown in Figure 2(a) because it assumes that a user (document) has only one time stamp. ToT requires all the data over time for inference, thus, it is not appropriate for application to continuously generated data such as Twitter. We consider a model must be capable of online inference and track the dynamics of user interests and topic trends for modeling tweets. Since TTM has these abilities, we ad</context>
</contexts>
<marker>Blei, Lafferty, 2006</marker>
<rawString>David M. Blei., and John D. Lafferty. 2006. Dynamic topic models. In Proceedings of the 23rd International Conference on Machine learning (ICML), 113-120.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent dirichlet allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>3</volume>
<pages>993--1022</pages>
<contexts>
<context position="1714" citStr="Blei et al., 2003" startWordPosition="270" endWordPosition="273">consumer purchase behaviors. 1 Introduction Microblogs such as Twitter, have prevailed rapidly in our society recently. Twitter users post a message using 140 characters, which is called a tweet. The characters limit allows users to post tweets easily about not only personal interest or real life but also public events such as traffic accidents or earthquakes. There have been many studies on how to extract and utilize such information on tweets (Diao et al., 2012; Pennacchiotti and Popescu, 2011; Sakaki et al., 2010; Weng et al., 2010). Topic models, such as latent Dirichlet allocation (LDA) (Blei et al., 2003) are widely used to identify latent topic structure in large collections of documents. Recently, some studies have applied LDA to Twitter for user classification (Pennacchiotti and Popescu, 2011), detection of influential users (Weng et al., 2010), and so on. LDA is a generative document model, which assumes that each document is represented as a probability distribution over some topics, and that each word has a latent topic. When we apply LDA to tweets, each tweet is treated as a single document. This direct application does not work well because a tweet is very short compared with tradition</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M. Blei, Andrew Y. Ng and Michael I. Jordan. 2003. Latent dirichlet allocation. Journal of Machine Learning Research, 3: 993-1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Freddy C T Chua</author>
<author>Sitaram Asur</author>
</authors>
<title>Automatic summarization of events from social media.</title>
<date>2013</date>
<booktitle>In Proceedings of the International AAAI Conference on Weblogs and Social Media (ICWSM).</booktitle>
<contexts>
<context position="15710" citStr="Chua and Asur (2013)" startWordPosition="2745" endWordPosition="2748">tes are used as the hyper parameters of the prior distributions at the next time period t + 1. 4 Related Work Recently, topic models for Twitter have been proposed. Diao et al. (2012) proposed a topic model that considers both the temporal information of tweets and user’s personal interests. They applied their model to find bursty topics from Twitter. Yan et al. (2013) proposed a biterm topic model (BTM), which assumes that a wordpair is independently drawn from a specific topic. They demonstrated that BTM can effectively capture the topics within short texts such as tweets compared with LDA. Chua and Asur (2013) proposed two topic models considering time order and tweet intervals to extract the tweets summarizing a given event. The models mentioned above do not consider the dynamics of user interests, nor ct,u,k\i + αt,u a ct,u\i + αt,u Γ(nt,k\i + βt,k) Γ(nt,k + βt,k) ˆϕt−1,u,k Γ(αt,u) ∏ Γ(ct,u,k + αt,u Γ(ct,u + αt,u) k Γ(αt,uˆϕt− ∏X v (5) p(yj = 0|Dt, Yt\j, Zt, λ, γ) nt,B,v\j + λ a nt,B\j + V λ nt,u,B\j + γ (6) nt,u\j + 2γ , p(yj = 1|Dt, Yt\j, Zt, ˆOt−1, βt, γ) where At,u,k = Ψ(ct,u,k + αt,uˆϕt−1,u,k) − , 1,u,k) Ψ(αt,u (4) 1981 1. Draw Bt,B ∼Dirichlet(λ) 2. For each topic k = 1, ..., K, (a) draw Bt,</context>
</contexts>
<marker>Chua, Asur, 2013</marker>
<rawString>Freddy C. T. Chua and Sitaram Asur. 2013. Automatic summarization of events from social media. In Proceedings of the International AAAI Conference on Weblogs and Social Media (ICWSM).</rawString>
</citation>
<citation valid="false">
<authors>
<author>Qiming Diao</author>
<author>Jing Jiang</author>
</authors>
<title>Feida Zhu and Ee-Peng Lim 2012. Finding bursty topics from microblogs.</title>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL), 536–</booktitle>
<pages>544</pages>
<marker>Diao, Jiang, </marker>
<rawString>Qiming Diao, Jing Jiang, Feida Zhu and Ee-Peng Lim 2012. Finding bursty topics from microblogs. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL), 536– 544.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas L Griffiths</author>
<author>Mark Steyvers</author>
</authors>
<title>Finding scientific topics.</title>
<date>2004</date>
<booktitle>In Proceedings of the National Academy of Sciences of the United States of America,101(1):5228-5235.</booktitle>
<contexts>
<context position="6560" citStr="Griffiths and Steyvers, 2004" startWordPosition="1086" endWordPosition="1089">iment, LDA was applied as the method to aggregate all tweets of a user as a single document. The original Twitter data set contains 14,305 users and 292,105 tweets collected on October 18, 2013. We then removed words that occurred less than 20 times and stop words. Retweetsl were treated 1Republishing a tweet written by another Twitter user. as the same as other general tweets because they reflected the user’s interests. After the above preprocessing, we obtained the final dataset with 14,139 users, 252,842 tweets, and 7,763 vocabularies. Each model was inferred with collapsed Gibbs sampling (Griffiths and Steyvers, 2004) and the iteration was set at 500. For a fair comparison, the hyper parameters in these models were optimized in each Gibbs sampling iteration by maximizing likelihood using fixed iterations (Minka, 2000). This study employs perplexity as the evaluation index, which is the standard metric in information retrieval literature. The perplexity of a held-out test set is defined as � )log p(w„) (1) perplexity = exp (− 1 N u where w„ represents words are contained in the tweets of user u and N is the number of words in the test set. A lower perplexity means higher predictive performance. We set the n</context>
</contexts>
<marker>Griffiths, Steyvers, 2004</marker>
<rawString>Thomas L. Griffiths and Mark Steyvers. 2004. Finding scientific topics. In Proceedings of the National Academy of Sciences of the United States of America,101(1):5228-5235.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew D Hoffman</author>
<author>Francis Bach</author>
<author>David M Blei</author>
</authors>
<title>Online learning for latent dirichlet allocation.</title>
<date>2010</date>
<booktitle>In Proceedings of the Advances in Neural Information Processing Systems (NIPS),</booktitle>
<pages>856--864</pages>
<contexts>
<context position="17713" citStr="Hoffman et al., 2010" startWordPosition="3098" endWordPosition="3101"> online the topics and topic transitions that naturally arise in a tweet stream. Their model learns the transition parameters among topics by minimizing the prediction error on topic distribution in subsequent tweets. However, the TM-LDA does not consider dynamic word distributions. In other words, their model can not capture the dynamics of topic trends. Lau et al. (2012) proposed a topic model implementing a dynamic vocabulary based on online LDA (OLDA) (AlSumait et al., 2008) and applied it to track emerging events on Twitter. An online variational Bayes algorithm for LDA is also proposed (Hoffman et al., 2010). However, these methods are based on LDA and do not consider the shortness of a tweet. Twitter-TTM tackles the shortness of a tweet by assuming that a single tweet consists of a single topic. This assumption is based on the following observation: a tweet is much shorter than a normal document, so a single tweet rarely contains multiple topics but rather a single one. Figure 4: Graphical model of Twitter-TTM 5 Experiment 5.1 Setting We evaluated the effectiveness of the proposed Twitter-TTM using an actual Twitter data set. The original Twitter data set contains 15,962 users and 4,146,672 twee</context>
</contexts>
<marker>Hoffman, Bach, Blei, 2010</marker>
<rawString>Matthew D. Hoffman, Francis Bach and David M. Blei. 2010. Online learning for latent dirichlet allocation. In Proceedings of the Advances in Neural Information Processing Systems (NIPS), 856–864.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liangjie Hong</author>
<author>Brian D Davison</author>
</authors>
<title>Empirical study of topic modeling in twitter.</title>
<date>2010</date>
<booktitle>In Proceedings of the First Workshop on Social Media Analytics (SOMA),</booktitle>
<pages>80--88</pages>
<contexts>
<context position="2475" citStr="Hong and Davison, 2010" startWordPosition="402" endWordPosition="405">or user classification (Pennacchiotti and Popescu, 2011), detection of influential users (Weng et al., 2010), and so on. LDA is a generative document model, which assumes that each document is represented as a probability distribution over some topics, and that each word has a latent topic. When we apply LDA to tweets, each tweet is treated as a single document. This direct application does not work well because a tweet is very short compared with traditional media such as newspapers. To deal with the shortness of a tweet, some studies aggregated all the tweets of a user as a single document (Hong and Davison, 2010; Pennacchiotti and Popescu, 2011; Weng et al., 2010). On the other hand, Zhao et al. (2011) proposed “Twitter-LDA,” which is a model that considers the shortness of a tweet. Twitter-LDA assumes that a single tweet consists of a single topic, and that tweets consist of topic and background words. Zhao et al. (2011) show that it works well at the point of semantic coherence of topics compared with LDA. However, as with the case of LDA, Twitter-LDA cannot consider a sequence of tweets because it assumes that samples are exchangeable. In Twitter, user interests and topic trends are dynamically ch</context>
</contexts>
<marker>Hong, Davison, 2010</marker>
<rawString>Liangjie Hong and Brian D. Davison. 2010. Empirical study of topic modeling in twitter. In Proceedings of the First Workshop on Social Media Analytics (SOMA), 80–88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Naonori Ueda</author>
</authors>
<title>Topic tracking model for analyzing consumer purchase behavior.</title>
<date>2009</date>
<booktitle>In Proceedings of the International Joint Conferences on Artificial Intelligence (IJCAI),1427–1432.</booktitle>
<marker>Ueda, 2009</marker>
<rawString>Tomoharu Iwata, Shinji Watanabe, Takeshi Yamada. and Naonori Ueda. 2009. Topic tracking model for analyzing consumer purchase behavior. In Proceedings of the International Joint Conferences on Artificial Intelligence (IJCAI),1427–1432.</rawString>
</citation>
<citation valid="true">
<authors>
<author>JeyHan Lau</author>
<author>Nigel Collier</author>
<author>Timothy Baldwin</author>
</authors>
<title>On-line trend analysis with topic models:# twitter trends detection topic model online.</title>
<date>2012</date>
<booktitle>In Proceedings of the 23th International Conference on Computational Linguistics (COLING),</booktitle>
<pages>1519--1534</pages>
<contexts>
<context position="17467" citStr="Lau et al. (2012)" startWordPosition="3056" endWordPosition="3059">hereas TwitterTTM can capture the dynamics of user interests and topic trends and has the capability of online inference. Some online topic models have also been proposed. TM-LDA was proposed by Wang et al. (2012), which can efficiently model online the topics and topic transitions that naturally arise in a tweet stream. Their model learns the transition parameters among topics by minimizing the prediction error on topic distribution in subsequent tweets. However, the TM-LDA does not consider dynamic word distributions. In other words, their model can not capture the dynamics of topic trends. Lau et al. (2012) proposed a topic model implementing a dynamic vocabulary based on online LDA (OLDA) (AlSumait et al., 2008) and applied it to track emerging events on Twitter. An online variational Bayes algorithm for LDA is also proposed (Hoffman et al., 2010). However, these methods are based on LDA and do not consider the shortness of a tweet. Twitter-TTM tackles the shortness of a tweet by assuming that a single tweet consists of a single topic. This assumption is based on the following observation: a tweet is much shorter than a normal document, so a single tweet rarely contains multiple topics but rath</context>
</contexts>
<marker>Lau, Collier, Baldwin, 2012</marker>
<rawString>JeyHan Lau, Nigel Collier and Timothy Baldwin. 2012. On-line trend analysis with topic models:# twitter trends detection topic model online. In Proceedings of the 23th International Conference on Computational Linguistics (COLING), 1519–1534.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas P Minka</author>
</authors>
<title>Estimating a Dirichlet distribution</title>
<date>2000</date>
<tech>Technical report, MIT.</tech>
<contexts>
<context position="6764" citStr="Minka, 2000" startWordPosition="1122" endWordPosition="1123"> occurred less than 20 times and stop words. Retweetsl were treated 1Republishing a tweet written by another Twitter user. as the same as other general tweets because they reflected the user’s interests. After the above preprocessing, we obtained the final dataset with 14,139 users, 252,842 tweets, and 7,763 vocabularies. Each model was inferred with collapsed Gibbs sampling (Griffiths and Steyvers, 2004) and the iteration was set at 500. For a fair comparison, the hyper parameters in these models were optimized in each Gibbs sampling iteration by maximizing likelihood using fixed iterations (Minka, 2000). This study employs perplexity as the evaluation index, which is the standard metric in information retrieval literature. The perplexity of a held-out test set is defined as � )log p(w„) (1) perplexity = exp (− 1 N u where w„ represents words are contained in the tweets of user u and N is the number of words in the test set. A lower perplexity means higher predictive performance. We set the number of topics K at 50, 100, 150, 200, and 250 and evaluated the perplexity for each model in each K via a 10-fold cross-validation. The results are shown in Table 1, which shows that the improved model </context>
<context position="13328" citStr="Minka, 2000" startWordPosition="2303" endWordPosition="2304">infer the latent variables. Let Dt be a set of tweets and Zt, Yt be a set of latent variables z, y at time t. We can integrate the parameters in the joint distribup(Ot,u |��Ot−1,u, αt,u) ∝ k 1980 tion as follows: p(Dt, Yt, Zt |ˆ4t−1, ˆOt−1, αt, βt, λ,γ) (Γ(2γ) )U ∏ Γ(γ + nt,u,B)Γ(γ + nt,u,K) = Γ(γ)2 Γ(2γ + nt,u) u ∏ vΓ(nt,B,v + λ) Γ(nt,B + V λ) Γ(βt,k) ∏ Γ(nt,k,v + βt,k Γ(nt,k + βt,k) Γ(βt,k ˆθt−1 v of user u at time t, and \j represents a count excluding the j-th word. The persistence parameters αt and βt are estimated by maximizing the joint likelihood eq.(4), using a fixed point iteration (Minka, 2000). The update formulas are as follows: ∑ ˆϕt−1,u,kAt,u,k αnew t,u = αt,u Ψ(ct,u + αt,u) − Ψ(αt,u), k (8) Γ(V λ) XΓ(λ)V ∏X k ∏X u ˆθt−1,k,v) ,k,v) ˆϕt−1,u,k) where nt,u,B and nt,u,K are the number of background and topic words of user u at time t, nt,B,v is the number of times that word v is assigned as a background word at time t, nt,k,v is the number of times that word v is assigned to topic k at time t, ct,u,k is the number of tweets assigned to topic k for user u at time t. In addition, nt,u = nt,u,B + nt,uK, nt,B = ∑v nt,B,v, nt,K = ∑k nt,k = ∑k ∑v nt,k,v, nt,u = ∑k nt,u,k, and ct,u = ∑k ct</context>
<context position="19149" citStr="Minka, 2000" startWordPosition="3336" endWordPosition="3337">laries. We compared the predictive performance of Twitter-TTM with LDA, TTM, Twitter-LDA, Twitter-LDA+TTM, and the improved model based on the perplexity for the next time tweets. Twitter-LDA+TTM is a combination of TwitterLDA and TTM. It is equivalent to Twitter-TTM, except that the rate between background and topic words is different for each user. We set the number of topics K at 100, the iteration of each model at 500, and the unit time interval at one day. The hyper parameters in these models were optimized in each Gibbs sampling iteration by maximizing likelihood using fixed iterations (Minka, 2000). The inferences of LDA, Twitter-LDA, and the improved model were made for current time tweets. 5.2 Result Figure 5 shows the perplexity of each model for each time, where t = 1 in the horizontal axis represents October 18, t = 2 represents October 19, ..., and t = 13 represents October 31. The perplexity at time t represents the predictive performance of each model inferred by previous time tweets to r-1 r a αa 0 0 Y Ir Y Ir Z Nu x Z Nu r w Bk Nu,s U BB r Bk w Nu,s U g a g a BB x 1982 the current time tweets. Note that at t = 1, the performance of LDA and TTM, that of Twitter-LDA and Twitter-</context>
</contexts>
<marker>Minka, 2000</marker>
<rawString>Thomas P. Minka 2000. Estimating a Dirichlet distribution Technical report, MIT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Pennacchiotti</author>
<author>Ana-Maria Popescu</author>
</authors>
<title>A machine learning approach to Twitter user classification.</title>
<date>2011</date>
<booktitle>In Proceedings of the International AAAI Conference on Weblogs and Social Media (ICWSM),</booktitle>
<pages>281--288</pages>
<contexts>
<context position="1596" citStr="Pennacchiotti and Popescu, 2011" startWordPosition="249" endWordPosition="252">enable it to estimate the dynamics of user interests and topic trends online based on the topic tracking model (TTM), which models consumer purchase behaviors. 1 Introduction Microblogs such as Twitter, have prevailed rapidly in our society recently. Twitter users post a message using 140 characters, which is called a tweet. The characters limit allows users to post tweets easily about not only personal interest or real life but also public events such as traffic accidents or earthquakes. There have been many studies on how to extract and utilize such information on tweets (Diao et al., 2012; Pennacchiotti and Popescu, 2011; Sakaki et al., 2010; Weng et al., 2010). Topic models, such as latent Dirichlet allocation (LDA) (Blei et al., 2003) are widely used to identify latent topic structure in large collections of documents. Recently, some studies have applied LDA to Twitter for user classification (Pennacchiotti and Popescu, 2011), detection of influential users (Weng et al., 2010), and so on. LDA is a generative document model, which assumes that each document is represented as a probability distribution over some topics, and that each word has a latent topic. When we apply LDA to tweets, each tweet is treated </context>
</contexts>
<marker>Pennacchiotti, Popescu, 2011</marker>
<rawString>Marco Pennacchiotti and Ana-Maria Popescu. 2011. A machine learning approach to Twitter user classification. In Proceedings of the International AAAI Conference on Weblogs and Social Media (ICWSM), 281–288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takeshi Sakaki</author>
<author>Makoto Okazaki</author>
<author>Yutaka Matsuo</author>
</authors>
<title>Earthquake shakes Twitter users: realtime event detection by social sensors.</title>
<date>2010</date>
<booktitle>In Proceedings of the World Wide Web Conference (WWW),</booktitle>
<pages>851--860</pages>
<contexts>
<context position="1617" citStr="Sakaki et al., 2010" startWordPosition="253" endWordPosition="256">s of user interests and topic trends online based on the topic tracking model (TTM), which models consumer purchase behaviors. 1 Introduction Microblogs such as Twitter, have prevailed rapidly in our society recently. Twitter users post a message using 140 characters, which is called a tweet. The characters limit allows users to post tweets easily about not only personal interest or real life but also public events such as traffic accidents or earthquakes. There have been many studies on how to extract and utilize such information on tweets (Diao et al., 2012; Pennacchiotti and Popescu, 2011; Sakaki et al., 2010; Weng et al., 2010). Topic models, such as latent Dirichlet allocation (LDA) (Blei et al., 2003) are widely used to identify latent topic structure in large collections of documents. Recently, some studies have applied LDA to Twitter for user classification (Pennacchiotti and Popescu, 2011), detection of influential users (Weng et al., 2010), and so on. LDA is a generative document model, which assumes that each document is represented as a probability distribution over some topics, and that each word has a latent topic. When we apply LDA to tweets, each tweet is treated as a single document.</context>
</contexts>
<marker>Sakaki, Okazaki, Matsuo, 2010</marker>
<rawString>Takeshi Sakaki, Makoto Okazaki and Yutaka Matsuo. 2010. Earthquake shakes Twitter users: realtime event detection by social sensors. In Proceedings of the World Wide Web Conference (WWW), 851–860.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hanna M Wallach</author>
</authors>
<title>Topic modeling: beyond bag-of-words.</title>
<date>2006</date>
<booktitle>In Proceedings of the 23rd International Conference on Machine Learning (ICML),</booktitle>
<pages>977--984</pages>
<contexts>
<context position="12237" citStr="Wallach (2006)" startWordPosition="2105" endWordPosition="2106">end Bt,k depend on previous states. Time dependency is not considered on BB and πu because they can be regarded as being independent of time. Figures 3 and 4 show the generative process and a graphical representation of Twitter-TTM, respectively. Twitter-TTM can capture the dynamics of user interests and topic trends in Twitter considering the features of tweets online. Moreover, Twitter-TTM can be extended to capture long-term dependences, as described in Iwata et al. (2009). 3.2 Model Inference We use a stochastic expectation-maximization algorithm for Twitter-TTM inference, as described in Wallach (2006) in which Gibbs sampling of latent values and maximum joint likelihood estimation of parameters are alternately iterated. At time t, we estimate user interests 4)t = { O U t,u }U=1 topic trends Ot = {�Bt,k}Kk=1, background word distribution Bt,B, word usage rate distribution πt,u, interest persistence parameters αt = {αt,u}U u=1, and trend persistence parameters )3t = {,3t,k}Kk=1 using the previous time interests 4)t−1 and trends �Ot−1. We employ collapsed Gibbs sampling to infer the latent variables. Let Dt be a set of tweets and Zt, Yt be a set of latent variables z, y at time t. We can inte</context>
</contexts>
<marker>Wallach, 2006</marker>
<rawString>Hanna M. Wallach 2006. Topic modeling: beyond bag-of-words. In Proceedings of the 23rd International Conference on Machine Learning (ICML), 977–984.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xuerui Wang</author>
<author>Andrew McCallum</author>
</authors>
<title>Topics over time: a non-Markov continuous-time model of topical trends.</title>
<date>2006</date>
<booktitle>In Proceedings of the International Conference on Knowledge Discovery and Data Mining (KDD),</booktitle>
<pages>424--433</pages>
<contexts>
<context position="9574" citStr="Wang and McCallum, 2006" startWordPosition="1629" endWordPosition="1632">kground words Low rate of background words I hope today goes quickly Team Arsenal v will Ozil be I want to work in a cake Making Justin smile and laugh as he is working on music All need your support please Google nexus briefly appears in Google play store ity of online inference based on TTM (Iwata et al., 2009). TTM is a probabilistic consumer purchase behavior model based on LDA for tracking the interests of each user and the trends in each topic. Other topic models considering the dynamics of topics include the dynamic topic model (DTM) (Blei and Lafferty, 2006) and topic over time (ToT) (Wang and McCallum, 2006). DTM is a model for analyzing the time evolution of topics in time-ordered document collections. It does not track the interests of each user as shown in Figure 2(a) because it assumes that a user (document) has only one time stamp. ToT requires all the data over time for inference, thus, it is not appropriate for application to continuously generated data such as Twitter. We consider a model must be capable of online inference and track the dynamics of user interests and topic trends for modeling tweets. Since TTM has these abilities, we adapt it to the improved model described in Section 2.</context>
</contexts>
<marker>Wang, McCallum, 2006</marker>
<rawString>Xuerui Wang and Andrew McCallum. 2006. Topics over time: a non-Markov continuous-time model of topical trends. In Proceedings of the International Conference on Knowledge Discovery and Data Mining (KDD), 424–433.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yu Wang</author>
<author>Eugene Agichtein</author>
<author>Michele Benz</author>
</authors>
<title>TM-LDA: efficient online modeling of the latent topic transitions in social media.</title>
<date>2012</date>
<booktitle>In Proceedings of the International Conference on Knowledge Discovery and Data Mining (KDD),</booktitle>
<pages>123--131</pages>
<contexts>
<context position="17063" citStr="Wang et al. (2012)" startWordPosition="2991" endWordPosition="2994">weet s = 1, ..., Nu i. draw zt,u,s ∼Multinomial(ot,u) ii. for each word n = 1, ..., Nu,s A. draw yt,u,s,n ∼Bernoulli(7rt,u) B. draw wt,u,s,n ∼ Multinomial(Bt,B) if yt,u,s,n = 0 or Multinomial(Bt,zt,u,B) if yt,u,s,n = 1 Figure 3: Generative process of tweets in TwitterTTM do they have the capability of online inference; thus, they cannot efficiently model the large number of tweets generated everyday, whereas TwitterTTM can capture the dynamics of user interests and topic trends and has the capability of online inference. Some online topic models have also been proposed. TM-LDA was proposed by Wang et al. (2012), which can efficiently model online the topics and topic transitions that naturally arise in a tweet stream. Their model learns the transition parameters among topics by minimizing the prediction error on topic distribution in subsequent tweets. However, the TM-LDA does not consider dynamic word distributions. In other words, their model can not capture the dynamics of topic trends. Lau et al. (2012) proposed a topic model implementing a dynamic vocabulary based on online LDA (OLDA) (AlSumait et al., 2008) and applied it to track emerging events on Twitter. An online variational Bayes algorit</context>
</contexts>
<marker>Wang, Agichtein, Benz, 2012</marker>
<rawString>Yu Wang, Eugene Agichtein and Michele Benz. 2012. TM-LDA: efficient online modeling of the latent topic transitions in social media. In Proceedings of the International Conference on Knowledge Discovery and Data Mining (KDD), 123–131.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianshu Weng</author>
<author>Ee Peng Lim</author>
<author>Jing Jiang</author>
<author>Qi He</author>
</authors>
<title>Twitterrank: finding topic-sensitive influential twitterers.</title>
<date>2010</date>
<booktitle>In Proceedings of the 3rd ACMInternational Conference on Web Search and Data Mining (WSDM),</booktitle>
<pages>261--270</pages>
<contexts>
<context position="1637" citStr="Weng et al., 2010" startWordPosition="257" endWordPosition="260">nd topic trends online based on the topic tracking model (TTM), which models consumer purchase behaviors. 1 Introduction Microblogs such as Twitter, have prevailed rapidly in our society recently. Twitter users post a message using 140 characters, which is called a tweet. The characters limit allows users to post tweets easily about not only personal interest or real life but also public events such as traffic accidents or earthquakes. There have been many studies on how to extract and utilize such information on tweets (Diao et al., 2012; Pennacchiotti and Popescu, 2011; Sakaki et al., 2010; Weng et al., 2010). Topic models, such as latent Dirichlet allocation (LDA) (Blei et al., 2003) are widely used to identify latent topic structure in large collections of documents. Recently, some studies have applied LDA to Twitter for user classification (Pennacchiotti and Popescu, 2011), detection of influential users (Weng et al., 2010), and so on. LDA is a generative document model, which assumes that each document is represented as a probability distribution over some topics, and that each word has a latent topic. When we apply LDA to tweets, each tweet is treated as a single document. This direct applica</context>
</contexts>
<marker>Weng, Lim, Jiang, He, 2010</marker>
<rawString>Jianshu Weng, Ee Peng Lim, Jing Jiang and Qi He. 2010. Twitterrank: finding topic-sensitive influential twitterers. In Proceedings of the 3rd ACMInternational Conference on Web Search and Data Mining (WSDM), 261–270.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Xiaohui Yan</author>
<author>Jiafeng Guo</author>
</authors>
<title>Yanyan Lan and Xueqi Cheng 2013. A biterm topic model for short texts.</title>
<booktitle>In Proceedings of the World Wide Web Conference (WWW),</booktitle>
<pages>1445--1456</pages>
<marker>Yan, Guo, </marker>
<rawString>Xiaohui Yan, Jiafeng Guo, Yanyan Lan and Xueqi Cheng 2013. A biterm topic model for short texts. In Proceedings of the World Wide Web Conference (WWW), 1445–1456.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wayne Xin Zhao</author>
<author>Jing Jiang</author>
<author>Jianshu Weng</author>
</authors>
<title>Jing He, Ee-Peng Lim, Hongfei Yan and Xiaoming Li.</title>
<date>2011</date>
<booktitle>In Advances in Information Retrieval,</booktitle>
<volume>338</volume>
<pages>349</pages>
<contexts>
<context position="2567" citStr="Zhao et al. (2011)" startWordPosition="418" endWordPosition="421">t al., 2010), and so on. LDA is a generative document model, which assumes that each document is represented as a probability distribution over some topics, and that each word has a latent topic. When we apply LDA to tweets, each tweet is treated as a single document. This direct application does not work well because a tweet is very short compared with traditional media such as newspapers. To deal with the shortness of a tweet, some studies aggregated all the tweets of a user as a single document (Hong and Davison, 2010; Pennacchiotti and Popescu, 2011; Weng et al., 2010). On the other hand, Zhao et al. (2011) proposed “Twitter-LDA,” which is a model that considers the shortness of a tweet. Twitter-LDA assumes that a single tweet consists of a single topic, and that tweets consist of topic and background words. Zhao et al. (2011) show that it works well at the point of semantic coherence of topics compared with LDA. However, as with the case of LDA, Twitter-LDA cannot consider a sequence of tweets because it assumes that samples are exchangeable. In Twitter, user interests and topic trends are dynamically changing. In addition, when new data comes along, a new model must be generated again with all</context>
</contexts>
<marker>Zhao, Jiang, Weng, 2011</marker>
<rawString>Wayne Xin Zhao, Jing Jiang, Jianshu Weng, Jing He, Ee-Peng Lim, Hongfei Yan and Xiaoming Li. 2011. Comparing twitter and traditional media using topic models. In Advances in Information Retrieval, 338– 349.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
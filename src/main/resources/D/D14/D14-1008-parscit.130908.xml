<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000029">
<title confidence="0.9992045">
A Constituent-Based Approach to Argument Labeling
with Joint Inference in Discourse Parsing
</title>
<author confidence="0.998534">
Fang Kong1∗ Hwee Tou Ng2 Guodong Zhou1
</author>
<affiliation confidence="0.984595">
1 School of Computer Science and Technology, Soochow University, China
2 Department of Computer Science, National University of Singapore
</affiliation>
<email confidence="0.995053">
kongfang@suda.edu.cn nght@comp.nus.edu.sg gdzhou@suda.edu.cn
</email>
<sectionHeader confidence="0.9938" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999850653846154">
Discourse parsing is a challenging task
and plays a critical role in discourse anal-
ysis. In this paper, we focus on label-
ing full argument spans of discourse con-
nectives in the Penn Discourse Treebank
(PDTB). Previous studies cast this task
as a linear tagging or subtree extraction
problem. In this paper, we propose a
novel constituent-based approach to argu-
ment labeling, which integrates the ad-
vantages of both linear tagging and sub-
tree extraction. In particular, the pro-
posed approach unifies intra- and inter-
sentence cases by treating the immediate-
ly preceding sentence as a special con-
stituent. Besides, a joint inference mech-
anism is introduced to incorporate glob-
al information across arguments into our
constituent-based approach via integer lin-
ear programming. Evaluation on PDT-
B shows significant performance improve-
ments of our constituent-based approach
over the best state-of-the-art system. It al-
so shows the effectiveness of our joint in-
ference mechanism in modeling global in-
formation across arguments.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.981045980769231">
Discourse parsing determines the internal struc-
ture of a text and identifies the discourse rela-
tions between its text units. It has attracted in-
creasing attention in recent years due to its impor-
tance in text understanding, especially since the
release of the Penn Discourse Treebank (PDTB)
corpus (Prasad et al., 2008), which adds a layer of
discourse annotations on top of the Penn Treebank
∗The research reported in this paper was carried out while
Fang Kong was a research fellow at the National University
of Singapore.
(PTB) corpus (Marcus et al., 1993). As the largest
available discourse corpus, the PDTB corpus has
become the defacto benchmark in recent studies
on discourse parsing.
Compared to connective identification and dis-
course relation classification in discourse parsing,
the task of labeling full argument spans of dis-
course connectives is much harder and thus more
challenging. For connective identification, Lin et
al. (2014) achieved the performance of 95.76%
and 93.62% in F-measure using gold-standard and
automatic parse trees, respectively. For discourse
relation classification, Lin et al. (2014) achieved
the performance of 86.77% in F-measure on clas-
sifying discourse relations into 16 level 2 types.
However, for argument labeling, Lin et al. (2014)
only achieved the performance of 53.85% in F-
measure using gold-standard parse trees and con-
nectives, much lower than the inter-annotation a-
greement of 90.20% (Miltsakaki et al., 2004).
In this paper, we focus on argument labeling in
the PDTB corpus. In particular, we propose a nov-
el constituent-based approach to argument label-
ing which views constituents as candidate argu-
ments. Besides, our approach unifies intra- and
inter-sentence cases by treating the immediately
preceding sentence as a special constituent. Final-
ly, a joint inference mechanism is introduced to
incorporate global information across arguments
via integer linear programming. Evaluation on the
PDTB corpus shows the effectiveness of our ap-
proach.
The rest of this paper is organized as follows.
Section 2 briefly introduces the PDTB corpus.
Related work on argument labeling is reviewed
in Section 3. In Section 4, we describe our
constituent-based approach to argument labeling.
In Section 5, we present our joint inference mech-
anism via integer linear programming (ILP). Sec-
tion 6 gives the experimental results and analysis.
Finally, we conclude in Section 7.
</bodyText>
<page confidence="0.991773">
68
</page>
<note confidence="0.9466155">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 68–77,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.733958" genericHeader="introduction">
2 Penn Discourse Treebank
</sectionHeader>
<bodyText confidence="0.993600235294118">
As the first large-scale annotated corpus that fol-
lows the lexically grounded, predicate-argument
approach in D-LTAG (Lexicalized Tree Adjoin-
ing Grammar for Discourse) (Webber, 2004), the
PDTB regards a connective as the predicate of a
discourse relation which takes exactly two text s-
pans as its arguments. In particular, the text span
that the connective is syntactically attached to is
called Arg2, and the other is called Arg1.
Although discourse relations can be either ex-
plicitly or implicitly expressed in PDTB, this pa-
per focuses only on explicit discourse relations
that are explicitly signaled by discourse connec-
tives. Example (1) shows an explicit discourse re-
lation from the article wsj 2314 with connective
so underlined, Arg1 span italicized, and Arg2 s-
pan bolded.
</bodyText>
<listItem confidence="0.954922666666667">
(1) But its competitors have much broader busi-
ness interests and so are better cushioned
against price swings.
</listItem>
<bodyText confidence="0.9993855">
Note that a connective and its arguments can ap-
pear in any relative order, and an argument can be
arbitrarily far away from its corresponding con-
nective. Although the position of Arg2 is fixed
once the connective is located, Arg1 can occur in
the same sentence as the connective (SS), in a sen-
tence preceding that of the connective (PS), or in
a sentence following that of the connective (FS),
with proportions of 60.9%, 39.1%, and less than
0.1% respectively for explicit relations in the PDT-
B corpus (Prasad et al., 2008). Besides, out of
all PS cases where Arg1 occurs in some preced-
ing sentence, 79.9% of them are the exact imme-
diately preceding sentence. As such, in this paper,
we only consider the current sentence containing
the connective and its immediately preceding sen-
tence as the text span where Arg1 occurs, similar
to what was done in (Lin et al., 2014).
</bodyText>
<sectionHeader confidence="0.999964" genericHeader="related work">
3 Related Work
</sectionHeader>
<bodyText confidence="0.999885101694916">
For argument labeling in discourse parsing on the
PDTB corpus, the related work can be classified
into two categories: locating parts of arguments,
and labeling full argument spans.
As a representative on locating parts of argu-
ments, Wellner and Pustejovsky (2007) proposed
several machine learning approaches to identify
the head words of the two arguments for discourse
connectives. Following this work, Elwell and
Baldridge (2008) combined general and connec-
tive specific rankers to improve the performance
of labeling the head words of the two arguments.
Prasad et al. (2010) proposed a set of heuristics to
locate the position of the Arg1 sentences for inter-
sentence cases. The limitation of locating parts of
arguments, such as the positions and head word-
s, is that it is only a partial solution to argument
labeling in discourse parsing.
In comparison, labeling full argument spans can
provide a complete solution to argument labeling
in discourse parsing and has thus attracted increas-
ing attention recently, adopting either a subtree
extraction approach (Dinesh et al. (2005), Lin et
al. (2014)) or a linear tagging approach (Ghosh et
al. (2011)).
As a representative subtree extraction approach,
Dinesh et al. (2005) proposed an automatic tree
subtraction algorithm to locate argument spans for
intra-sentential subordinating connectives. How-
ever, only dealing with intra-sentential subordinat-
ing connectives is not sufficient since they con-
stitute only 40.93% of all cases. Instead, Lin et
al. (2014) proposed a two-step approach. First, an
argument position identifier was employed to lo-
cate the position of Arg1. For the PS case, it di-
rectly selects the immediately preceding sentence
as Arg1. For other cases, an argument node iden-
tifier was employed to locate the Arg1- and Arg2-
nodes. Next, a tree subtraction algorithm was used
to extract the arguments. However, as pointed out
in Dinesh et al. (2005), it is not necessarily the
case that a connective, Arg1, or Arg2 is dominated
by a single node in the parse tree (that is, it can be
dominated by a set of nodes). Figure 1 shows the
gold-standard parse tree corresponding to Exam-
ple (1). It shows that Arg1 includes three nodes:
[CC But], [NP its competitors], [u P have much
broader business interests], and Arg2 includes t-
wo nodes: [CC and], [u P are better cushioned a-
gainst price swings]. Therefore, such an argumen-
t node identifier has inherent shortcomings in la-
beling arguments. Besides, the errors propagat-
ed from the upstream argument position classifier
may adversely affect the performance of the down-
stream argument node identifier.
As a representative linear tagging approach,
Ghosh et al. (2011) cast argument labeling as a lin-
ear tagging task using conditional random fields.
Ghosh et al. (2012) further improved the perfor-
</bodyText>
<page confidence="0.991697">
69
</page>
<figure confidence="0.9364564">
S
VP
VP
VP
PP
</figure>
<page confidence="0.953798">
70
</page>
<bodyText confidence="0.947420190476191">
Feature Description Example
CON-Str The string of the given connective (case-sensitive) so
CON-LStr The lowercase string of the given connective so
CON-Cat Subordinating
The syntactic category of the given connective: sub-
ordinating, coordinating, or discourse adverbial
CON-iLSib Number of left siblings of the connective 2
CON-iRSib Number of right siblings of the connective 1
NT-Ctx The context of the constituent. We use POS combi- VP-VP-NULL-CC
nation of the constituent, its parent, left sibling and
right sibling to represent the context. When there is
no parent or siblings, it is marked NULL.
CON-NT-Path RB T V P ↓ V P
The path from the parent node of the connective to
the node of the constituent
CON-NT-Position left
The position of the constituent relative to the connec-
tive: left, right, or previous
CON-NT-Path-iLsib The path from the parent node of the connective to RB T V P ↓ V P:&gt;1
the node of the constituent and whether the number
of left siblings of the connective is greater than one
</bodyText>
<tableCaption confidence="0.996473">
Table 1: Features employed in argument classification.
</tableCaption>
<bodyText confidence="0.845615">
out of 19816) of the nodes in the parse trees.
</bodyText>
<subsectionHeader confidence="0.989323">
4.2 Argument Classification
</subsectionHeader>
<bodyText confidence="0.999943697674419">
In this paper, a multi-category classifier is em-
ployed to determine the role of an argument can-
didate (i.e., Arg1, Arg2, or NULL). Table 1 lists
the features employed in argument classification,
which reflect the properties of the connective and
the candidate constituent, and the relationship be-
tween them. The third column of Table 1 shows
the features corresponding to Figure 1, consider-
ing [RB so] as the given connective and [u P have
much broader business interests] as the constituent
in question.
Similar to Lin et al. (2014), we obtained the syn-
tactic category of the connectives from the list pro-
vided in Knott (1996). However, different from
Lin et al. (2014), only the siblings of the root path
nodes (i.e., the nodes occurring in the path of the
connective to root) are collected as the candidate
constituents in the pruning stage, and the value of
the relative position can be left or right, indicat-
ing that the constituent is located on the left- or
right-hand of the root path respectively. Besides,
we view the root of the previous sentence as a spe-
cial candidate constituent. For example, the value
of the feature CON-NT-Position is previous when
the current constituent is the root of the previous
sentence. Finally, we use the part-of-speech (POS)
combination of the constituent itself, its parent n-
ode, left sibling node and right sibling node to rep-
resent the context of the candidate constituent. In-
tuitively, this information can help determine the
role of the constituent.
For the example shown in Figure 1, we first em-
ploy the pruning algorithm to get the candidate
constituents, and then employ our argument clas-
sifier to determine the role for every candidate.
For example, if the five candidates are labeled as
Arg1, Arg2, Arg2, Arg1, and Arg1, respectively,
we merge all the Arg1 constituents to obtain the
Arg1 text span (i.e., But its competitors have much
broader business interests). Similarly, we merge
the two Arg2 constituents to obtain the Arg2 text s-
pan (i.e., and are better cushioned against price
swings).
</bodyText>
<sectionHeader confidence="0.6936915" genericHeader="method">
5 Joint Inference via Integer Linear
Programming
</sectionHeader>
<bodyText confidence="0.9999475">
In the above approach, decisions are always made
for each candidate independently, ignoring global
information across candidates in the final output.
For example, although an argument span can be
split into multiple discontinuous segments (e.g.,
the Arg2 span of Example (1) contains two dis-
continuous segments, and, are better cushioned
against price swings), the number of discontinu-
ous segments is always limited. Statistics on the
PDTB corpus shows that the number of discontin-
</bodyText>
<page confidence="0.992231">
71
</page>
<bodyText confidence="0.999961533333333">
uous segments for both Arg1 and Arg2 is generally
(&gt;= 99%) at most 2. For Example (1), from left
to right, we can obtain the list of constituent can-
didates: [CC But], [NP its competitors], [V P have
much broader business interests], [CC and], [V P
are better cushioned against price swings]. If our
argument classifier wrongly determines the roles
as Arg1, Arg2, Arg1, Arg2, and Arg1 respectively,
we can find that the achieved Arg1 span contains
three discontinuous segments. Such errors may be
corrected from a global perspective.
In this paper, a joint inference mechanism is in-
troduced to incorporate various kinds of knowl-
edge to resolve the inconsistencies in argumen-
t classification to ensure global legitimate predic-
tions. In particular, the joint inference mechanism
is formalized as a constrained optimization prob-
lem, represented as an integer linear programming
(ILP) task. It takes as input the argument classi-
fiers’ confidence scores for each constituent can-
didate along with a list of constraints, and outputs
the optimal solution that maximizes the objective
function incorporating the confidence scores, sub-
ject to the constraints that encode various kinds of
knowledge.
In this section, we meet the requirement of ILP
with focus on the definition of variables, the objec-
tive function, and the problem-specific constraints,
along with ILP-based joint inference integrating
multiple systems.
</bodyText>
<subsectionHeader confidence="0.998539">
5.1 Definition of Variables
</subsectionHeader>
<bodyText confidence="0.9800041">
Given an input sentence, the task of argumen-
t labeling is to determine what labels should be
assigned to which constituents corresponding to
which connective. It is therefore natural that en-
coding the output space of argument labeling re-
quires various kinds of information about the con-
nectives, the argument candidates corresponding
to a connective, and their argument labels.
Given an input sentence s, we define following
variables:
</bodyText>
<listItem confidence="0.999471555555556">
(1) P: the set of connectives in a sentence.
(2) p E P: a connective in P.
(3) C(p): the set of argument candidates corre-
sponding to connective p. (i.e., the parse tree
nodes obtained in the pruning stage).
(4) c E C(p): an argument candidate.
(5) L: the set of argument labels {Arg1, Arg2,
NULL 1.
(6) l E L: an argument label in L.
</listItem>
<bodyText confidence="0.990385">
In addition, we define the integer variables as
follows:
</bodyText>
<equation confidence="0.907531">
Zlc,p E {0, 11 (1)
</equation>
<bodyText confidence="0.9993845">
If Zlc,p = 1, the argument candidate c, which
corresponds to connective p, should be assigned
the label l. Otherwise, the argument candidate c is
not assigned this label.
</bodyText>
<subsectionHeader confidence="0.997176">
5.2 The Objective Function
</subsectionHeader>
<bodyText confidence="0.9996886">
The objective of joint inference is to find the best
arguments for all the connectives in one sentence.
For every connective, the pruning algorithm is first
employed to determine the set of corresponding
argument candidates. Then, the argument classifi-
er is used to assign a label to every candidate. For
an individual labeling Zlc,p, we measure the quality
based on the confidence scores, fl,c,p, returned by
the argument classifier. Thus, the objective func-
tion can be defined as
</bodyText>
<figure confidence="0.422762">
�max fl,c,pZl (2)
l,c,p c,p
</figure>
<subsectionHeader confidence="0.937437">
5.3 Constraints
</subsectionHeader>
<bodyText confidence="0.995963222222222">
As the key to the success of ILP-based joint infer-
ence, the following constraints are employed:
Constraint 1: The arguments corresponding
to a connective cannot overlap with the connec-
tive. Let c1, c2..., ck be the argument candidates
that correspond to the same connective and over-
lap with the connective in a sentence.1 Then this
constraint ensures that none of them will be as-
signed as Arg1 or Arg2.
</bodyText>
<equation confidence="0.653818">
= k (3)
</equation>
<bodyText confidence="0.9201885">
Constraint 2: There are no overlapping or em-
bedding arguments. Let c1, c2..., ck be the argu-
ment candidates that correspond to the same con-
nective and cover the same word in a sentence.2
</bodyText>
<footnote confidence="0.968392714285714">
1Only when the target connective node does not cover the
connective exactly and our pruning algorithm collects all the
children of the target connective node as part of constituent
candidates, such overlap can be introduced.
2This constraint only works in system combination of
Section 5.4, where additional phantom candidates may intro-
duce such overlap.
</footnote>
<equation confidence="0.9440335">
k
i=1
ZNULL
ci,p
</equation>
<page confidence="0.934846">
72
</page>
<bodyText confidence="0.99972825">
Then this constraint ensures that at most one of
the constituents can be assigned as Arg1 or Arg2.
That is, at least k − 1 constituents should be as-
signed the special label NULL.
</bodyText>
<figure confidence="0.833931875">
al
its competitors have much broader business interests
a2 a3
k ZNULL bl b2
i=1 ci,p &gt; k − 1 (4)
its competitors have much broader business interests
Constraint 3: For a connective, there is at least b3
one constituent candidate assigned as Arg2.
</figure>
<figureCaption confidence="0.988174">
Figure 2: An example on unifying different candi-
</figureCaption>
<equation confidence="0.8922905">
E ZArg2 c,p&gt; 1 (5) dates.
c
</equation>
<bodyText confidence="0.981908">
Constraint 4: Since we view the previous com-
plete sentence as a special Arg1 constituent candi-
date, denoted as m, there is at least one candidate
assigned as Arg1 for every connective.
</bodyText>
<equation confidence="0.988869333333333">
ZArg1
c,p + ZArg1
m,p &gt; 1 (6)
</equation>
<bodyText confidence="0.971938833333333">
Constraint 5: The number of discontinuous
constituents assigned as Arg1 or Arg2 should be at
most 2. That is, if argument candidates c1, c2..., ck
corresponding to the same connective are discon-
tinuous, this constraint ensures that at most two
of the constituents can be assigned the same label
</bodyText>
<equation confidence="0.952440666666667">
Arg1 or Arg2.
ZArg2
ci,p &lt; 2 (7)
</equation>
<subsectionHeader confidence="0.963406">
5.4 System Combination
</subsectionHeader>
<bodyText confidence="0.9487484">
Previous work shows that the performance of ar-
gument labeling heavily depends on the quality of
the syntactic parser. It is natural that combining
different argument labeling systems on differen-
t parse trees can potentially improve the overall
performance of argument labeling.
To explore this potential, we build two argu-
ment labeling systems — one using the Berke-
ley parser (Petrov et al., 2006) and the other the
Charniak parser (Charniak, 2000). Previous s-
tudies show that these two syntactic parsers tend
to produce different parse trees for the same sen-
tence (Zhang et al., 2009). For example, our pre-
liminary experiment shows that applying the prun-
ing algorithm on the output of the Charniak parser
produces a list of candidates with recall of 80.59%
and 89.87% for Arg1 and Arg2 respectively, while
achieving recall of 78.6% and 91.1% for Arg1 and
Arg2 respectively on the output of the Berkeley
parser. It also shows that combining these two can-
didate lists significantly improves recall to 85.7%
and 93.0% for Arg1 and Arg2, respectively.
In subsection 5.2, we only consider the con-
fidence scores returned by an argument classifier.
Here, we proceed to combine the probabilities pro-
duced by two argument classifiers. There are two
remaining problems to resolve:
• How do we unify the two candidate lists?
In principle, constituents spanning the same
sequence of words should be viewed as the
same candidate. That is, for different can-
didates, we can unify them by adding phan-
tom candidates. This is similar to the ap-
proach proposed by Punyakanok et al. (2008)
for the semantic role labeling task. For exam-
ple, Figure 2 shows the candidate lists gen-
erated by our pruning algorithm based on t-
wo different parse trees given the segment
“its competitors have much broader business
interests”. Dashed lines are used for phan-
tom candidates and solid lines for true can-
didates. Here, system A produces one can-
didate a1, with two phantom candidates a2
and a3 added. Analogously, phantom can-
didate b3 is added to the candidate list out-
put by System B. In this way, we can get the
unified candidate list: “its competitors have
much broader business interests”, “its com-
petitors”, “have much broader business inter-
ests”.
</bodyText>
<listItem confidence="0.991675833333333">
• How do we compute the confidence score for
every decision? For every candidate in the
unified list, we first determine whether it is
a true candidate based on the specific parse
tree. Then, for a true candidate, we extrac-
t the features from the corresponding parse
</listItem>
<equation confidence="0.99706775">
E
c
ZA P1 &lt; 2, and
ci
k
i=1
k
i=1
</equation>
<page confidence="0.982599">
73
</page>
<bodyText confidence="0.999380714285714">
tree. On this basis, we can determine the
confidence score using our argument classi-
fier. For a phantom candidate, we set the
same prior distribution as the confidence s-
core. In particular, the probability of the
”NULL” class is set to 0.55, following (Pun-
yakanok et al., 2008), and the probabilities of
Arg1 and Arg2 are set to their occurrence fre-
quencies in the training data. For the example
shown in Figure 2, since System A return-
s “its competitors have much broader busi-
ness interests” as a true candidate, we can ob-
tain its confidence score using our argumen-
t classifier. For the two phantom candidates
— “its competitors” and “have much broader
business interests” — we use the prior dis-
tributions directly. This applies to the candi-
dates for System B. Finally, we simply aver-
age the estimated probabilities to determine
the final probability estimate for every candi-
date in the unified list.
</bodyText>
<sectionHeader confidence="0.99951" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<bodyText confidence="0.999857">
In this section, we systematically evaluate our
constituent-based approach with a joint inference
mechanism to argument labeling on the PDTB
corpus.
</bodyText>
<subsectionHeader confidence="0.992093">
6.1 Experimental settings
</subsectionHeader>
<bodyText confidence="0.999987235294118">
All our classifiers are trained using the OpenNLP
maximum entropy package3 with the default pa-
rameters (i.e. without smoothing and with 100
iterations). As the PDTB corpus is aligned with
the PTB corpus, the gold parse trees and sentence
boundaries are obtained from PTB. Under the au-
tomatic setting, the NIST sentence segmenter4 and
the Charniak parser5 are used to segment and parse
the sentences, respectively. lp solve6 is used for
our joint inference.
This paper focuses on automatically labeling
the full argument spans of discourse connec-
tives. For a fair comparison with start-of-the-
art systems, we use the NUS PDTB-style end-
to-end discourse parser7 to perform other sub-
tasks of discourse parsing except argument label-
ing, which includes connective identification, non-
</bodyText>
<footnote confidence="0.999469">
3http://maxent.sourceforge.net/
4http://duc.nist.gov/duc2004/software/duc2003
.breakSent.tar.gz
5ftp://ftp.cs.brown.edu/pub/nlparser/
6http://lpsolve.sourceforge.net/
7http://wing.comp.nus.edu.sg/ linzihen/parser/
</footnote>
<bodyText confidence="0.9996378">
explicit discourse relation identification and clas-
sification.
Finally, we evaluate our system on two aspects:
(1) the dependence on the parse trees (GS/Auto,
using gold standard or automatic parse trees and
sentence boundaries); and (2) the impact of errors
propagated from previous components (noEP/EP,
using gold annotation or automatic results from
previous components). In combination, we have
four different settings: GS+noEP, GS+EP, Au-
to+noEP and Auto+EP. Same as Lin et al. (2014),
we report exact match results under these four set-
tings. Here, exact match means two spans match
identically, except beginning or ending punctua-
tion symbols.
</bodyText>
<subsectionHeader confidence="0.998577">
6.2 Experimental results
</subsectionHeader>
<bodyText confidence="0.985818307692308">
We first evaluate the effectiveness of our
constituent-based approach by comparing our sys-
tem with the state-of-the-art systems, ignoring
the joint inference mechanism. Then, the con-
tribution of the joint inference mechanism to our
constituent-based approach, and finally the contri-
bution of our argument labeling system to the end-
to-end discourse parser are presented.
Effectiveness of our constituent-based ap-
proach
By comparing with two state-of-the-art argu-
ment labeling approaches, we determine the effec-
tiveness of our constituent-based approach.
</bodyText>
<subsectionHeader confidence="0.829712">
Comparison with the linear tagging approach
</subsectionHeader>
<bodyText confidence="0.999961833333334">
As a representative linear tagging approach,
Ghosh et al. (2011; 2012; 2012) only reported the
exact match results for Arg1 and Arg2 using the
evaluation script for chunking evaluation8 under
GS+noEP setting with Section 02–22 of the PDTB
corpus for training, Section 23–24 for testing, and
Section 00–01 for development. It is also worth
mentioning that an argument span can contain
multiple discontinuous segments (i.e., chunks), so
chunking evaluation only shows the exact match
of every argument segment but not the exact match
of every argument span. In order to fairly compare
our system with theirs, we evaluate our system us-
ing both the exact metric and the chunking eval-
uation. Table 2 compares the results of our sys-
tem without joint inference and the results report-
ed by Ghosh et al. (2012) on the same data split.
We can find that our system performs much bet-
</bodyText>
<footnote confidence="0.9867795">
8http://www.cnts.ua.ac.be/conll2000/chunking/
conlleval.txt
</footnote>
<page confidence="0.997275">
74
</page>
<bodyText confidence="0.564581">
ter than Ghosh’s on both Arg1 and Arg2, even on
much stricter metrics.
</bodyText>
<table confidence="0.9165215">
Systems Arg1 Arg2
ours using exact match 65.68 84.50
ours using chunking evaluation 67.48 88.08
reported by Ghosh et al. (2012) 59.39 79.48
</table>
<tableCaption confidence="0.895387666666667">
Table 2: Performance (F1) comparison of our ar-
gument labeling approach with the linear tagging
approach as adopted in Ghosh et al. (2012)
</tableCaption>
<bodyText confidence="0.9926960625">
Comparison with the subtree extracting ap-
proach
For a fair comparison, we also conduct our
experiments on the same data split of Lin et
al. (2014) with Section 02 to 21 for training, Sec-
tion 22 for development, and Section 23 for test-
ing. Table 3 compares our labeling system without
joint inference with Lin et al. (2014), a representa-
tive subtree extracting approach. From the results,
we find that the performance of our argument la-
beling system significantly improves under all set-
tings. This is because Lin et al. (2014) considered
all the internal nodes of the parse trees, whereas
the pruning algorithm in our approach can effec-
tively filter out those unlikely constituents when
determining Arg1 and Arg2.
</bodyText>
<table confidence="0.999595714285714">
Setting Arg1 Arg2 Arg1&amp;2
GS+noEP 62.84 84.07 55.69
ours GS+EP 61.46 81.30 54.31
Auto+EP 56.04 76.53 48.89
GS+noEP 59.15 82.23 53.85
Lin’s GS+EP 57.64 79.80 52.29
Auto+EP 47.68 70.27 40.37
</table>
<tableCaption confidence="0.997452">
Table 3: Performance (F1) comparison of our ar-
</tableCaption>
<bodyText confidence="0.992921727272727">
gument labeling approach with the subtree extrac-
tion approach as adopted in Lin et al. (2014)
As justified above, by integrating the advan-
tages of both linear tagging and subtree extraction,
our constituent-based approach can capture both
rich syntactic information from parse trees and
local sequential dependency between tokens. The
results show that our constituent-based approach
indeed significantly improves the performance
of argument labeling, compared to both linear
tagging and subtree extracting approaches.
</bodyText>
<subsectionHeader confidence="0.783826">
Contribution of Joint Inference
</subsectionHeader>
<bodyText confidence="0.9838063125">
Same as Lin et al. (2014), we conduct our ex-
periments using Section 02 to 21 for training, Sec-
tion 22 for development, and Section 23 for test-
ing. Table 4 lists the performance of our argumen-
t labeling system without and with ILP inference
under four different settings, while Table 5 reports
the contribution of system combination. It shows
the following:
• On the performance comparison of Arg1 and
Arg2, the performance on Arg2 is much bet-
ter than that on Arg1 with the performance
gap up to 8% under different settings. This is
due to the fact that the relationship between
Arg2 and the connective is much closer. This
result is also consistent with previous studies
on argument labeling.
</bodyText>
<listItem confidence="0.720207571428571">
• On the impact of error propagation from con-
nective identification, the errors propagated
from connective identification reduce the per-
formance of argument labeling by less than
2% in both Arg1 and Arg2 F-measure under
different settings.
• On the impact of parse trees, using automat-
</listItem>
<bodyText confidence="0.738115166666667">
ic parse trees reduces the performance of ar-
gument labeling by about 5.5% in both Arg1
and Arg2 F-measure under different settings.
In comparison with the impact of error prop-
agation, parse trees have much more impact
on argument labeling.
</bodyText>
<listItem confidence="0.982869">
• On the impact of joint inference, it improves
the performance of argument labeling, espe-
cially on automatic parse trees by about 2%.9
• On the impact of system combination, the
performance is improved by about 1.5%.
</listItem>
<table confidence="0.999402333333333">
Setting Arg1 Arg2 Arg1&amp;2
without GS+noEP 62.84 84.07 55.69
Joint GS+EP 61.46 81.30 54.31
Inference Auto+noEP 57.75 79.85 50.27
Auto+EP 56.04 76.53 48.89
with GS+noEP 65.76 83.86 58.18
Joint GS+EP 63.96 81.19 56.37
Inference Auto+noEP 60.24 79.74 52.55
Auto+EP 58.10 76.53 50.73
</table>
<tableCaption confidence="0.9855495">
Table 4: Performance (F1) of our argument label-
ing approach.
</tableCaption>
<bodyText confidence="0.6795075">
Contribution to the end-to-end discourse pars-
er
</bodyText>
<footnote confidence="0.734506">
9Unless otherwise specified, all the improvements in this
paper are significant with P &lt; 0.001.
</footnote>
<page confidence="0.991385">
75
</page>
<table confidence="0.999810714285714">
Systems Setting Arg1 Arg2 Arg1&amp;2
Charniak noEP 60.24 79.74 52.55
EP 58.10 76.53 50.73
Berkeley noEP 60.78 80.07 52.98
EP 58.80 77.21 51.43
Combined noEP 61.97 80.61 54.50
EP 59.72 77.55 52.52
</table>
<tableCaption confidence="0.978241">
Table 5: Contribution of System Combination in
Joint Inference.
</tableCaption>
<bodyText confidence="0.999421142857143">
Lastly, we focus on the contribution of our ar-
gument labeling approach to the overall perfor-
mance of the end-to-end discourse parser. This
is done by replacing the argument labeling mod-
el of the NUS PDTB-style end-to-end discourse
parser with our argument labeling model. Table 6
shows the results using gold parse trees and auto-
matic parse trees, respectively.10 From the results,
we find that using gold parse trees, our argument
labeling approach significantly improves the per-
formance of the end-to-end system by about 1.8%
in F-measure, while using automatic parse trees,
the improvement significantly enlarges to 6.7% in
F-measure.
</bodyText>
<table confidence="0.843223666666667">
Setting New d-parser Lin et al.’s (2014)
GS 34.80 33.00
Auto 27.39 20.64
</table>
<tableCaption confidence="0.991076">
Table 6: Performance (F1) of the end-to-end dis-
course parser.
</tableCaption>
<sectionHeader confidence="0.995487" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999694772727273">
In this paper, we focus on the problem of auto-
matically labeling the full argument spans of dis-
course connectives. In particular, we propose a
constituent-based approach to integrate the advan-
tages of both subtree extraction and linear tagging
approaches. Moreover, our proposed approach in-
tegrates inter- and intra-sentence argument label-
ing by viewing the immediately preceding sen-
tence as a special constituent. Finally, a join-
t inference mechanism is introduced to incorpo-
rate global information across arguments into our
10Further analysis found that the error propagated from
sentence segmentation can reduce the performance of the
end-to-end discourse parser. Retraining the NIST sentence
segmenter using Section 02 to 21 of the PDTB corpus, the
original NUS PDTB-style end-to-end discourse parser can
achieve the performance of 25.25% in F-measure, while the
new version (i.e. replace the argument labeling model with
our argument labeling model) can achieve the performance
of 30.06% in F-measure.
constituent-based approach via integer linear pro-
gramming.
</bodyText>
<sectionHeader confidence="0.996556" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999206142857143">
This research is supported by the Singapore Na-
tional Research Foundation under its International
Research Centre @ Singapore Funding Initiative
and administered by the IDM Programme Office.
This research is also partially supported by Key
project 61333018 and 61331011 under the Nation-
al Natural Science Foundation of China.
</bodyText>
<sectionHeader confidence="0.999173" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999952864864865">
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of the First Meeting
of the North American Chapter of the Association
for Computational Linguistics, pages 132–139.
Nikhil Dinesh, Alan Lee, Eleni Miltsakaki, Rashmi
Prasad, Aravind Joshi, and Bonnie Webber. 2005.
Attribution and the (non-)alignment of syntactic and
discourse arguments of connectives. In Proceedings
of the Workshop on Frontiers in Corpus Annotation
II: Pie in the Sky, pages 29–36.
Robert Elwell and Jason Baldridge. 2008. Discourse
connective argument identification with connective
specific rankers. In Second IEEE International Con-
ference on Semantic Computing, pages 198–205.
Sucheta Ghosh, Richard Johansson, Giuseppe Riccar-
di, and Sara Tonelli. 2011. Shallow discourse pars-
ing with conditional random fields. In Proceedings
of the 5th International Joint Conference on Natural
Language Processing, pages 1071–1079.
Sucheta Ghosh, Giuseppe Riccardi, and Richard Jo-
hansson. 2012. Global features for shallow dis-
course parsing. In Proceedings of the 13th Annual
Meeting of the Special Interest Group on Discourse
and Dialogue, pages 150–159.
Sucheta Ghosh. 2012. End-to-End Discourse Parsing
using Cascaded Structured Prediction. Ph.D. thesis,
University of Trento.
Alistair Knott. 1996. A Data-Driven Methodology for
Motivating a Set of Coherence Relations. Ph.D. the-
sis, University of Edinburgh.
Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2014. A
PDTB-styled end-to-end discourse parser. Natural
Language Engineering, 20(2):151–184.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19(2):313–330.
</reference>
<page confidence="0.841245">
76
</page>
<reference confidence="0.999811418604651">
Eleni Miltsakaki, Rashmi Prasad, Aravind Joshi, and
Bonnie Webber. 2004. The Penn Discourse Tree-
bank. In Proceedings of the Fourth International
Conference on Language Resources and Evaluation,
pages 2237–2240.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and
interpretable tree annotation. In Proceedings of
the 21st International Conference on Computational
Linguistics and the 44th Annual Meeting of the As-
sociation for Computational Linguistics, pages 433–
440.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind Joshi, and Bonnie
Webber. 2008. The Penn Discourse TreeBank 2.0.
In Proceedings of the LREC 2008 Conference, pages
2961–2968.
Rashmi Prasad, Aravind Joshi, and Bonnie Webber.
2010. Exploiting scope for shallow discourse pars-
ing. In Proceedings of the Seventh International
Conference on Language Resources and Evaluation.
Vasin Punyakanok, Dan Roth, and Wen-tau Yih. 2008.
The important of syntactic parsing and inference in
semantic role labeling. Computational Linguistics,
34(2):257–287.
Bonnie Webber. 2004. D-LTAG: extending lexicalized
TAG to discourse. Cognitive Science, 28(5):751–
779.
Ben Wellner and James Pustejovsky. 2007. Automat-
ically identifying the arguments of discourse con-
nectives. In Proceedings of the 2007 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, pages 92–101.
Nianwen Xue and Martha Palmer. 2004. Calibrating
features for semantic role labeling. In Proceedings
of 2004 Conference on Empirical Methods in Natu-
ral Language Processing, pages 88–94.
Hui Zhang, Min Zhang, Chew Lim Tan, and Haizhou
Li. 2009. K-best combination of syntactic parsers.
In Proceedings of the 2009 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1552–1560.
</reference>
<page confidence="0.999121">
77
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.528653">
<title confidence="0.997779">A Constituent-Based Approach to Argument with Joint Inference in Discourse Parsing</title>
<author confidence="0.898409">Hwee Tou Guodong</author>
<affiliation confidence="0.9674955">1School of Computer Science and Technology, Soochow University, 2Department of Computer Science, National University of</affiliation>
<abstract confidence="0.986697214285715">kongfang@suda.edu.cn nght@comp.nus.edu.sg gdzhou@suda.edu.cn Abstract Discourse parsing is a challenging task and plays a critical role in discourse analysis. In this paper, we focus on labeling full argument spans of discourse connectives in the Penn Discourse Treebank (PDTB). Previous studies cast this task as a linear tagging or subtree extraction problem. In this paper, we propose a novel constituent-based approach to argument labeling, which integrates the advantages of both linear tagging and subtree extraction. In particular, the proposed approach unifies intraand intersentence cases by treating the immediately preceding sentence as a special constituent. Besides, a joint inference mechanism is introduced to incorporate global information across arguments into our constituent-based approach via integer linear programming. Evaluation on PDT- B shows significant performance improvements of our constituent-based approach over the best state-of-the-art system. It also shows the effectiveness of our joint inference mechanism in modeling global information across arguments.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A maximum-entropyinspired parser.</title>
<date>2000</date>
<booktitle>In Proceedings of the First Meeting of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>132--139</pages>
<contexts>
<context position="17832" citStr="Charniak, 2000" startWordPosition="2893" endWordPosition="2894">are discontinuous, this constraint ensures that at most two of the constituents can be assigned the same label Arg1 or Arg2. ZArg2 ci,p &lt; 2 (7) 5.4 System Combination Previous work shows that the performance of argument labeling heavily depends on the quality of the syntactic parser. It is natural that combining different argument labeling systems on different parse trees can potentially improve the overall performance of argument labeling. To explore this potential, we build two argument labeling systems — one using the Berkeley parser (Petrov et al., 2006) and the other the Charniak parser (Charniak, 2000). Previous studies show that these two syntactic parsers tend to produce different parse trees for the same sentence (Zhang et al., 2009). For example, our preliminary experiment shows that applying the pruning algorithm on the output of the Charniak parser produces a list of candidates with recall of 80.59% and 89.87% for Arg1 and Arg2 respectively, while achieving recall of 78.6% and 91.1% for Arg1 and Arg2 respectively on the output of the Berkeley parser. It also shows that combining these two candidate lists significantly improves recall to 85.7% and 93.0% for Arg1 and Arg2, respectively.</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Eugene Charniak. 2000. A maximum-entropyinspired parser. In Proceedings of the First Meeting of the North American Chapter of the Association for Computational Linguistics, pages 132–139.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nikhil Dinesh</author>
<author>Alan Lee</author>
<author>Eleni Miltsakaki</author>
<author>Rashmi Prasad</author>
<author>Aravind Joshi</author>
<author>Bonnie Webber</author>
</authors>
<title>Attribution and the (non-)alignment of syntactic and discourse arguments of connectives.</title>
<date>2005</date>
<booktitle>In Proceedings of the Workshop on Frontiers in Corpus Annotation II: Pie in the Sky,</booktitle>
<pages>29--36</pages>
<contexts>
<context position="6876" citStr="Dinesh et al. (2005)" startWordPosition="1072" endWordPosition="1075">e specific rankers to improve the performance of labeling the head words of the two arguments. Prasad et al. (2010) proposed a set of heuristics to locate the position of the Arg1 sentences for intersentence cases. The limitation of locating parts of arguments, such as the positions and head words, is that it is only a partial solution to argument labeling in discourse parsing. In comparison, labeling full argument spans can provide a complete solution to argument labeling in discourse parsing and has thus attracted increasing attention recently, adopting either a subtree extraction approach (Dinesh et al. (2005), Lin et al. (2014)) or a linear tagging approach (Ghosh et al. (2011)). As a representative subtree extraction approach, Dinesh et al. (2005) proposed an automatic tree subtraction algorithm to locate argument spans for intra-sentential subordinating connectives. However, only dealing with intra-sentential subordinating connectives is not sufficient since they constitute only 40.93% of all cases. Instead, Lin et al. (2014) proposed a two-step approach. First, an argument position identifier was employed to locate the position of Arg1. For the PS case, it directly selects the immediately prece</context>
</contexts>
<marker>Dinesh, Lee, Miltsakaki, Prasad, Joshi, Webber, 2005</marker>
<rawString>Nikhil Dinesh, Alan Lee, Eleni Miltsakaki, Rashmi Prasad, Aravind Joshi, and Bonnie Webber. 2005. Attribution and the (non-)alignment of syntactic and discourse arguments of connectives. In Proceedings of the Workshop on Frontiers in Corpus Annotation II: Pie in the Sky, pages 29–36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Elwell</author>
<author>Jason Baldridge</author>
</authors>
<title>Discourse connective argument identification with connective specific rankers.</title>
<date>2008</date>
<booktitle>In Second IEEE International Conference on Semantic Computing,</booktitle>
<pages>198--205</pages>
<contexts>
<context position="6225" citStr="Elwell and Baldridge (2008)" startWordPosition="967" endWordPosition="970">consider the current sentence containing the connective and its immediately preceding sentence as the text span where Arg1 occurs, similar to what was done in (Lin et al., 2014). 3 Related Work For argument labeling in discourse parsing on the PDTB corpus, the related work can be classified into two categories: locating parts of arguments, and labeling full argument spans. As a representative on locating parts of arguments, Wellner and Pustejovsky (2007) proposed several machine learning approaches to identify the head words of the two arguments for discourse connectives. Following this work, Elwell and Baldridge (2008) combined general and connective specific rankers to improve the performance of labeling the head words of the two arguments. Prasad et al. (2010) proposed a set of heuristics to locate the position of the Arg1 sentences for intersentence cases. The limitation of locating parts of arguments, such as the positions and head words, is that it is only a partial solution to argument labeling in discourse parsing. In comparison, labeling full argument spans can provide a complete solution to argument labeling in discourse parsing and has thus attracted increasing attention recently, adopting either </context>
</contexts>
<marker>Elwell, Baldridge, 2008</marker>
<rawString>Robert Elwell and Jason Baldridge. 2008. Discourse connective argument identification with connective specific rankers. In Second IEEE International Conference on Semantic Computing, pages 198–205.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sucheta Ghosh</author>
<author>Richard Johansson</author>
<author>Giuseppe Riccardi</author>
<author>Sara Tonelli</author>
</authors>
<title>Shallow discourse parsing with conditional random fields.</title>
<date>2011</date>
<booktitle>In Proceedings of the 5th International Joint Conference on Natural Language Processing,</booktitle>
<pages>1071--1079</pages>
<contexts>
<context position="6946" citStr="Ghosh et al. (2011)" startWordPosition="1085" endWordPosition="1088">s of the two arguments. Prasad et al. (2010) proposed a set of heuristics to locate the position of the Arg1 sentences for intersentence cases. The limitation of locating parts of arguments, such as the positions and head words, is that it is only a partial solution to argument labeling in discourse parsing. In comparison, labeling full argument spans can provide a complete solution to argument labeling in discourse parsing and has thus attracted increasing attention recently, adopting either a subtree extraction approach (Dinesh et al. (2005), Lin et al. (2014)) or a linear tagging approach (Ghosh et al. (2011)). As a representative subtree extraction approach, Dinesh et al. (2005) proposed an automatic tree subtraction algorithm to locate argument spans for intra-sentential subordinating connectives. However, only dealing with intra-sentential subordinating connectives is not sufficient since they constitute only 40.93% of all cases. Instead, Lin et al. (2014) proposed a two-step approach. First, an argument position identifier was employed to locate the position of Arg1. For the PS case, it directly selects the immediately preceding sentence as Arg1. For other cases, an argument node identifier wa</context>
<context position="8470" citStr="Ghosh et al. (2011)" startWordPosition="1332" endWordPosition="1335">ted by a set of nodes). Figure 1 shows the gold-standard parse tree corresponding to Example (1). It shows that Arg1 includes three nodes: [CC But], [NP its competitors], [u P have much broader business interests], and Arg2 includes two nodes: [CC and], [u P are better cushioned against price swings]. Therefore, such an argument node identifier has inherent shortcomings in labeling arguments. Besides, the errors propagated from the upstream argument position classifier may adversely affect the performance of the downstream argument node identifier. As a representative linear tagging approach, Ghosh et al. (2011) cast argument labeling as a linear tagging task using conditional random fields. Ghosh et al. (2012) further improved the perfor69 S VP VP VP PP 70 Feature Description Example CON-Str The string of the given connective (case-sensitive) so CON-LStr The lowercase string of the given connective so CON-Cat Subordinating The syntactic category of the given connective: subordinating, coordinating, or discourse adverbial CON-iLSib Number of left siblings of the connective 2 CON-iRSib Number of right siblings of the connective 1 NT-Ctx The context of the constituent. We use POS combi- VP-VP-NULL-CC n</context>
<context position="23322" citStr="Ghosh et al. (2011" startWordPosition="3755" endWordPosition="3758"> constituent-based approach by comparing our system with the state-of-the-art systems, ignoring the joint inference mechanism. Then, the contribution of the joint inference mechanism to our constituent-based approach, and finally the contribution of our argument labeling system to the endto-end discourse parser are presented. Effectiveness of our constituent-based approach By comparing with two state-of-the-art argument labeling approaches, we determine the effectiveness of our constituent-based approach. Comparison with the linear tagging approach As a representative linear tagging approach, Ghosh et al. (2011; 2012; 2012) only reported the exact match results for Arg1 and Arg2 using the evaluation script for chunking evaluation8 under GS+noEP setting with Section 02–22 of the PDTB corpus for training, Section 23–24 for testing, and Section 00–01 for development. It is also worth mentioning that an argument span can contain multiple discontinuous segments (i.e., chunks), so chunking evaluation only shows the exact match of every argument segment but not the exact match of every argument span. In order to fairly compare our system with theirs, we evaluate our system using both the exact metric and t</context>
</contexts>
<marker>Ghosh, Johansson, Riccardi, Tonelli, 2011</marker>
<rawString>Sucheta Ghosh, Richard Johansson, Giuseppe Riccardi, and Sara Tonelli. 2011. Shallow discourse parsing with conditional random fields. In Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 1071–1079.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sucheta Ghosh</author>
<author>Giuseppe Riccardi</author>
<author>Richard Johansson</author>
</authors>
<title>Global features for shallow discourse parsing.</title>
<date>2012</date>
<booktitle>In Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue,</booktitle>
<pages>150--159</pages>
<contexts>
<context position="8571" citStr="Ghosh et al. (2012)" startWordPosition="1349" endWordPosition="1352">shows that Arg1 includes three nodes: [CC But], [NP its competitors], [u P have much broader business interests], and Arg2 includes two nodes: [CC and], [u P are better cushioned against price swings]. Therefore, such an argument node identifier has inherent shortcomings in labeling arguments. Besides, the errors propagated from the upstream argument position classifier may adversely affect the performance of the downstream argument node identifier. As a representative linear tagging approach, Ghosh et al. (2011) cast argument labeling as a linear tagging task using conditional random fields. Ghosh et al. (2012) further improved the perfor69 S VP VP VP PP 70 Feature Description Example CON-Str The string of the given connective (case-sensitive) so CON-LStr The lowercase string of the given connective so CON-Cat Subordinating The syntactic category of the given connective: subordinating, coordinating, or discourse adverbial CON-iLSib Number of left siblings of the connective 2 CON-iRSib Number of right siblings of the connective 1 NT-Ctx The context of the constituent. We use POS combi- VP-VP-NULL-CC nation of the constituent, its parent, left sibling and right sibling to represent the context. When t</context>
<context position="24060" citStr="Ghosh et al. (2012)" startWordPosition="3878" endWordPosition="3881">on8 under GS+noEP setting with Section 02–22 of the PDTB corpus for training, Section 23–24 for testing, and Section 00–01 for development. It is also worth mentioning that an argument span can contain multiple discontinuous segments (i.e., chunks), so chunking evaluation only shows the exact match of every argument segment but not the exact match of every argument span. In order to fairly compare our system with theirs, we evaluate our system using both the exact metric and the chunking evaluation. Table 2 compares the results of our system without joint inference and the results reported by Ghosh et al. (2012) on the same data split. We can find that our system performs much bet8http://www.cnts.ua.ac.be/conll2000/chunking/ conlleval.txt 74 ter than Ghosh’s on both Arg1 and Arg2, even on much stricter metrics. Systems Arg1 Arg2 ours using exact match 65.68 84.50 ours using chunking evaluation 67.48 88.08 reported by Ghosh et al. (2012) 59.39 79.48 Table 2: Performance (F1) comparison of our argument labeling approach with the linear tagging approach as adopted in Ghosh et al. (2012) Comparison with the subtree extracting approach For a fair comparison, we also conduct our experiments on the same dat</context>
</contexts>
<marker>Ghosh, Riccardi, Johansson, 2012</marker>
<rawString>Sucheta Ghosh, Giuseppe Riccardi, and Richard Johansson. 2012. Global features for shallow discourse parsing. In Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 150–159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sucheta Ghosh</author>
</authors>
<title>End-to-End Discourse Parsing using Cascaded Structured Prediction.</title>
<date>2012</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Trento.</institution>
<marker>Ghosh, 2012</marker>
<rawString>Sucheta Ghosh. 2012. End-to-End Discourse Parsing using Cascaded Structured Prediction. Ph.D. thesis, University of Trento.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alistair Knott</author>
</authors>
<title>A Data-Driven Methodology for Motivating a Set of Coherence Relations.</title>
<date>1996</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Edinburgh.</institution>
<contexts>
<context position="10383" citStr="Knott (1996)" startWordPosition="1655" endWordPosition="1656">a multi-category classifier is employed to determine the role of an argument candidate (i.e., Arg1, Arg2, or NULL). Table 1 lists the features employed in argument classification, which reflect the properties of the connective and the candidate constituent, and the relationship between them. The third column of Table 1 shows the features corresponding to Figure 1, considering [RB so] as the given connective and [u P have much broader business interests] as the constituent in question. Similar to Lin et al. (2014), we obtained the syntactic category of the connectives from the list provided in Knott (1996). However, different from Lin et al. (2014), only the siblings of the root path nodes (i.e., the nodes occurring in the path of the connective to root) are collected as the candidate constituents in the pruning stage, and the value of the relative position can be left or right, indicating that the constituent is located on the left- or right-hand of the root path respectively. Besides, we view the root of the previous sentence as a special candidate constituent. For example, the value of the feature CON-NT-Position is previous when the current constituent is the root of the previous sentence. </context>
</contexts>
<marker>Knott, 1996</marker>
<rawString>Alistair Knott. 1996. A Data-Driven Methodology for Motivating a Set of Coherence Relations. Ph.D. thesis, University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ziheng Lin</author>
<author>Hwee Tou Ng</author>
<author>Min-Yen Kan</author>
</authors>
<title>A PDTB-styled end-to-end discourse parser.</title>
<date>2014</date>
<journal>Natural Language Engineering,</journal>
<volume>20</volume>
<issue>2</issue>
<contexts>
<context position="2323" citStr="Lin et al. (2014)" startWordPosition="350" endWordPosition="353">s a layer of discourse annotations on top of the Penn Treebank ∗The research reported in this paper was carried out while Fang Kong was a research fellow at the National University of Singapore. (PTB) corpus (Marcus et al., 1993). As the largest available discourse corpus, the PDTB corpus has become the defacto benchmark in recent studies on discourse parsing. Compared to connective identification and discourse relation classification in discourse parsing, the task of labeling full argument spans of discourse connectives is much harder and thus more challenging. For connective identification, Lin et al. (2014) achieved the performance of 95.76% and 93.62% in F-measure using gold-standard and automatic parse trees, respectively. For discourse relation classification, Lin et al. (2014) achieved the performance of 86.77% in F-measure on classifying discourse relations into 16 level 2 types. However, for argument labeling, Lin et al. (2014) only achieved the performance of 53.85% in Fmeasure using gold-standard parse trees and connectives, much lower than the inter-annotation agreement of 90.20% (Miltsakaki et al., 2004). In this paper, we focus on argument labeling in the PDTB corpus. In particular, w</context>
<context position="5775" citStr="Lin et al., 2014" startWordPosition="899" endWordPosition="902">sentence as the connective (SS), in a sentence preceding that of the connective (PS), or in a sentence following that of the connective (FS), with proportions of 60.9%, 39.1%, and less than 0.1% respectively for explicit relations in the PDTB corpus (Prasad et al., 2008). Besides, out of all PS cases where Arg1 occurs in some preceding sentence, 79.9% of them are the exact immediately preceding sentence. As such, in this paper, we only consider the current sentence containing the connective and its immediately preceding sentence as the text span where Arg1 occurs, similar to what was done in (Lin et al., 2014). 3 Related Work For argument labeling in discourse parsing on the PDTB corpus, the related work can be classified into two categories: locating parts of arguments, and labeling full argument spans. As a representative on locating parts of arguments, Wellner and Pustejovsky (2007) proposed several machine learning approaches to identify the head words of the two arguments for discourse connectives. Following this work, Elwell and Baldridge (2008) combined general and connective specific rankers to improve the performance of labeling the head words of the two arguments. Prasad et al. (2010) pro</context>
<context position="7303" citStr="Lin et al. (2014)" startWordPosition="1135" endWordPosition="1138">rovide a complete solution to argument labeling in discourse parsing and has thus attracted increasing attention recently, adopting either a subtree extraction approach (Dinesh et al. (2005), Lin et al. (2014)) or a linear tagging approach (Ghosh et al. (2011)). As a representative subtree extraction approach, Dinesh et al. (2005) proposed an automatic tree subtraction algorithm to locate argument spans for intra-sentential subordinating connectives. However, only dealing with intra-sentential subordinating connectives is not sufficient since they constitute only 40.93% of all cases. Instead, Lin et al. (2014) proposed a two-step approach. First, an argument position identifier was employed to locate the position of Arg1. For the PS case, it directly selects the immediately preceding sentence as Arg1. For other cases, an argument node identifier was employed to locate the Arg1- and Arg2- nodes. Next, a tree subtraction algorithm was used to extract the arguments. However, as pointed out in Dinesh et al. (2005), it is not necessarily the case that a connective, Arg1, or Arg2 is dominated by a single node in the parse tree (that is, it can be dominated by a set of nodes). Figure 1 shows the gold-stan</context>
<context position="10289" citStr="Lin et al. (2014)" startWordPosition="1636" endWordPosition="1639">ication. out of 19816) of the nodes in the parse trees. 4.2 Argument Classification In this paper, a multi-category classifier is employed to determine the role of an argument candidate (i.e., Arg1, Arg2, or NULL). Table 1 lists the features employed in argument classification, which reflect the properties of the connective and the candidate constituent, and the relationship between them. The third column of Table 1 shows the features corresponding to Figure 1, considering [RB so] as the given connective and [u P have much broader business interests] as the constituent in question. Similar to Lin et al. (2014), we obtained the syntactic category of the connectives from the list provided in Knott (1996). However, different from Lin et al. (2014), only the siblings of the root path nodes (i.e., the nodes occurring in the path of the connective to root) are collected as the candidate constituents in the pruning stage, and the value of the relative position can be left or right, indicating that the constituent is located on the left- or right-hand of the root path respectively. Besides, we view the root of the previous sentence as a special candidate constituent. For example, the value of the feature C</context>
<context position="22477" citStr="Lin et al. (2014)" startWordPosition="3633" endWordPosition="3636">ent.tar.gz 5ftp://ftp.cs.brown.edu/pub/nlparser/ 6http://lpsolve.sourceforge.net/ 7http://wing.comp.nus.edu.sg/ linzihen/parser/ explicit discourse relation identification and classification. Finally, we evaluate our system on two aspects: (1) the dependence on the parse trees (GS/Auto, using gold standard or automatic parse trees and sentence boundaries); and (2) the impact of errors propagated from previous components (noEP/EP, using gold annotation or automatic results from previous components). In combination, we have four different settings: GS+noEP, GS+EP, Auto+noEP and Auto+EP. Same as Lin et al. (2014), we report exact match results under these four settings. Here, exact match means two spans match identically, except beginning or ending punctuation symbols. 6.2 Experimental results We first evaluate the effectiveness of our constituent-based approach by comparing our system with the state-of-the-art systems, ignoring the joint inference mechanism. Then, the contribution of the joint inference mechanism to our constituent-based approach, and finally the contribution of our argument labeling system to the endto-end discourse parser are presented. Effectiveness of our constituent-based approa</context>
<context position="24688" citStr="Lin et al. (2014)" startWordPosition="3980" endWordPosition="3983">e data split. We can find that our system performs much bet8http://www.cnts.ua.ac.be/conll2000/chunking/ conlleval.txt 74 ter than Ghosh’s on both Arg1 and Arg2, even on much stricter metrics. Systems Arg1 Arg2 ours using exact match 65.68 84.50 ours using chunking evaluation 67.48 88.08 reported by Ghosh et al. (2012) 59.39 79.48 Table 2: Performance (F1) comparison of our argument labeling approach with the linear tagging approach as adopted in Ghosh et al. (2012) Comparison with the subtree extracting approach For a fair comparison, we also conduct our experiments on the same data split of Lin et al. (2014) with Section 02 to 21 for training, Section 22 for development, and Section 23 for testing. Table 3 compares our labeling system without joint inference with Lin et al. (2014), a representative subtree extracting approach. From the results, we find that the performance of our argument labeling system significantly improves under all settings. This is because Lin et al. (2014) considered all the internal nodes of the parse trees, whereas the pruning algorithm in our approach can effectively filter out those unlikely constituents when determining Arg1 and Arg2. Setting Arg1 Arg2 Arg1&amp;2 GS+noEP </context>
<context position="26065" citStr="Lin et al. (2014)" startWordPosition="4195" endWordPosition="4198"> 3: Performance (F1) comparison of our argument labeling approach with the subtree extraction approach as adopted in Lin et al. (2014) As justified above, by integrating the advantages of both linear tagging and subtree extraction, our constituent-based approach can capture both rich syntactic information from parse trees and local sequential dependency between tokens. The results show that our constituent-based approach indeed significantly improves the performance of argument labeling, compared to both linear tagging and subtree extracting approaches. Contribution of Joint Inference Same as Lin et al. (2014), we conduct our experiments using Section 02 to 21 for training, Section 22 for development, and Section 23 for testing. Table 4 lists the performance of our argument labeling system without and with ILP inference under four different settings, while Table 5 reports the contribution of system combination. It shows the following: • On the performance comparison of Arg1 and Arg2, the performance on Arg2 is much better than that on Arg1 with the performance gap up to 8% under different settings. This is due to the fact that the relationship between Arg2 and the connective is much closer. This re</context>
</contexts>
<marker>Lin, Ng, Kan, 2014</marker>
<rawString>Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2014. A PDTB-styled end-to-end discourse parser. Natural Language Engineering, 20(2):151–184.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="1935" citStr="Marcus et al., 1993" startWordPosition="293" endWordPosition="296">modeling global information across arguments. 1 Introduction Discourse parsing determines the internal structure of a text and identifies the discourse relations between its text units. It has attracted increasing attention in recent years due to its importance in text understanding, especially since the release of the Penn Discourse Treebank (PDTB) corpus (Prasad et al., 2008), which adds a layer of discourse annotations on top of the Penn Treebank ∗The research reported in this paper was carried out while Fang Kong was a research fellow at the National University of Singapore. (PTB) corpus (Marcus et al., 1993). As the largest available discourse corpus, the PDTB corpus has become the defacto benchmark in recent studies on discourse parsing. Compared to connective identification and discourse relation classification in discourse parsing, the task of labeling full argument spans of discourse connectives is much harder and thus more challenging. For connective identification, Lin et al. (2014) achieved the performance of 95.76% and 93.62% in F-measure using gold-standard and automatic parse trees, respectively. For discourse relation classification, Lin et al. (2014) achieved the performance of 86.77%</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eleni Miltsakaki</author>
<author>Rashmi Prasad</author>
<author>Aravind Joshi</author>
<author>Bonnie Webber</author>
</authors>
<title>The Penn Discourse Treebank.</title>
<date>2004</date>
<booktitle>In Proceedings of the Fourth International Conference on Language Resources and Evaluation,</booktitle>
<pages>2237--2240</pages>
<contexts>
<context position="2840" citStr="Miltsakaki et al., 2004" startWordPosition="428" endWordPosition="431">urse connectives is much harder and thus more challenging. For connective identification, Lin et al. (2014) achieved the performance of 95.76% and 93.62% in F-measure using gold-standard and automatic parse trees, respectively. For discourse relation classification, Lin et al. (2014) achieved the performance of 86.77% in F-measure on classifying discourse relations into 16 level 2 types. However, for argument labeling, Lin et al. (2014) only achieved the performance of 53.85% in Fmeasure using gold-standard parse trees and connectives, much lower than the inter-annotation agreement of 90.20% (Miltsakaki et al., 2004). In this paper, we focus on argument labeling in the PDTB corpus. In particular, we propose a novel constituent-based approach to argument labeling which views constituents as candidate arguments. Besides, our approach unifies intra- and inter-sentence cases by treating the immediately preceding sentence as a special constituent. Finally, a joint inference mechanism is introduced to incorporate global information across arguments via integer linear programming. Evaluation on the PDTB corpus shows the effectiveness of our approach. The rest of this paper is organized as follows. Section 2 brie</context>
</contexts>
<marker>Miltsakaki, Prasad, Joshi, Webber, 2004</marker>
<rawString>Eleni Miltsakaki, Rashmi Prasad, Aravind Joshi, and Bonnie Webber. 2004. The Penn Discourse Treebank. In Proceedings of the Fourth International Conference on Language Resources and Evaluation, pages 2237–2240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Leon Barrett</author>
<author>Romain Thibaux</author>
<author>Dan Klein</author>
</authors>
<title>Learning accurate, compact, and interpretable tree annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>433--440</pages>
<contexts>
<context position="17781" citStr="Petrov et al., 2006" startWordPosition="2883" endWordPosition="2886">ates c1, c2..., ck corresponding to the same connective are discontinuous, this constraint ensures that at most two of the constituents can be assigned the same label Arg1 or Arg2. ZArg2 ci,p &lt; 2 (7) 5.4 System Combination Previous work shows that the performance of argument labeling heavily depends on the quality of the syntactic parser. It is natural that combining different argument labeling systems on different parse trees can potentially improve the overall performance of argument labeling. To explore this potential, we build two argument labeling systems — one using the Berkeley parser (Petrov et al., 2006) and the other the Charniak parser (Charniak, 2000). Previous studies show that these two syntactic parsers tend to produce different parse trees for the same sentence (Zhang et al., 2009). For example, our preliminary experiment shows that applying the pruning algorithm on the output of the Charniak parser produces a list of candidates with recall of 80.59% and 89.87% for Arg1 and Arg2 respectively, while achieving recall of 78.6% and 91.1% for Arg1 and Arg2 respectively on the output of the Berkeley parser. It also shows that combining these two candidate lists significantly improves recall </context>
</contexts>
<marker>Petrov, Barrett, Thibaux, Klein, 2006</marker>
<rawString>Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. 2006. Learning accurate, compact, and interpretable tree annotation. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics, pages 433– 440.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rashmi Prasad</author>
<author>Nikhil Dinesh</author>
<author>Alan Lee</author>
<author>Eleni Miltsakaki</author>
<author>Livio Robaldo</author>
<author>Aravind Joshi</author>
<author>Bonnie Webber</author>
</authors>
<title>The Penn Discourse TreeBank 2.0.</title>
<date>2008</date>
<booktitle>In Proceedings of the LREC 2008 Conference,</booktitle>
<pages>2961--2968</pages>
<contexts>
<context position="1695" citStr="Prasad et al., 2008" startWordPosition="252" endWordPosition="255">oach via integer linear programming. Evaluation on PDTB shows significant performance improvements of our constituent-based approach over the best state-of-the-art system. It also shows the effectiveness of our joint inference mechanism in modeling global information across arguments. 1 Introduction Discourse parsing determines the internal structure of a text and identifies the discourse relations between its text units. It has attracted increasing attention in recent years due to its importance in text understanding, especially since the release of the Penn Discourse Treebank (PDTB) corpus (Prasad et al., 2008), which adds a layer of discourse annotations on top of the Penn Treebank ∗The research reported in this paper was carried out while Fang Kong was a research fellow at the National University of Singapore. (PTB) corpus (Marcus et al., 1993). As the largest available discourse corpus, the PDTB corpus has become the defacto benchmark in recent studies on discourse parsing. Compared to connective identification and discourse relation classification in discourse parsing, the task of labeling full argument spans of discourse connectives is much harder and thus more challenging. For connective ident</context>
<context position="5429" citStr="Prasad et al., 2008" startWordPosition="838" endWordPosition="841">mpetitors have much broader business interests and so are better cushioned against price swings. Note that a connective and its arguments can appear in any relative order, and an argument can be arbitrarily far away from its corresponding connective. Although the position of Arg2 is fixed once the connective is located, Arg1 can occur in the same sentence as the connective (SS), in a sentence preceding that of the connective (PS), or in a sentence following that of the connective (FS), with proportions of 60.9%, 39.1%, and less than 0.1% respectively for explicit relations in the PDTB corpus (Prasad et al., 2008). Besides, out of all PS cases where Arg1 occurs in some preceding sentence, 79.9% of them are the exact immediately preceding sentence. As such, in this paper, we only consider the current sentence containing the connective and its immediately preceding sentence as the text span where Arg1 occurs, similar to what was done in (Lin et al., 2014). 3 Related Work For argument labeling in discourse parsing on the PDTB corpus, the related work can be classified into two categories: locating parts of arguments, and labeling full argument spans. As a representative on locating parts of arguments, Wel</context>
</contexts>
<marker>Prasad, Dinesh, Lee, Miltsakaki, Robaldo, Joshi, Webber, 2008</marker>
<rawString>Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Miltsakaki, Livio Robaldo, Aravind Joshi, and Bonnie Webber. 2008. The Penn Discourse TreeBank 2.0. In Proceedings of the LREC 2008 Conference, pages 2961–2968.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rashmi Prasad</author>
<author>Aravind Joshi</author>
<author>Bonnie Webber</author>
</authors>
<title>Exploiting scope for shallow discourse parsing.</title>
<date>2010</date>
<booktitle>In Proceedings of the Seventh International Conference on Language Resources and Evaluation.</booktitle>
<contexts>
<context position="6371" citStr="Prasad et al. (2010)" startWordPosition="991" endWordPosition="994">ne in (Lin et al., 2014). 3 Related Work For argument labeling in discourse parsing on the PDTB corpus, the related work can be classified into two categories: locating parts of arguments, and labeling full argument spans. As a representative on locating parts of arguments, Wellner and Pustejovsky (2007) proposed several machine learning approaches to identify the head words of the two arguments for discourse connectives. Following this work, Elwell and Baldridge (2008) combined general and connective specific rankers to improve the performance of labeling the head words of the two arguments. Prasad et al. (2010) proposed a set of heuristics to locate the position of the Arg1 sentences for intersentence cases. The limitation of locating parts of arguments, such as the positions and head words, is that it is only a partial solution to argument labeling in discourse parsing. In comparison, labeling full argument spans can provide a complete solution to argument labeling in discourse parsing and has thus attracted increasing attention recently, adopting either a subtree extraction approach (Dinesh et al. (2005), Lin et al. (2014)) or a linear tagging approach (Ghosh et al. (2011)). As a representative su</context>
</contexts>
<marker>Prasad, Joshi, Webber, 2010</marker>
<rawString>Rashmi Prasad, Aravind Joshi, and Bonnie Webber. 2010. Exploiting scope for shallow discourse parsing. In Proceedings of the Seventh International Conference on Language Resources and Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasin Punyakanok</author>
<author>Dan Roth</author>
<author>Wen-tau Yih</author>
</authors>
<title>The important of syntactic parsing and inference in semantic role labeling.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>2</issue>
<contexts>
<context position="18953" citStr="Punyakanok et al. (2008)" startWordPosition="3080" endWordPosition="3083">e two candidate lists significantly improves recall to 85.7% and 93.0% for Arg1 and Arg2, respectively. In subsection 5.2, we only consider the confidence scores returned by an argument classifier. Here, we proceed to combine the probabilities produced by two argument classifiers. There are two remaining problems to resolve: • How do we unify the two candidate lists? In principle, constituents spanning the same sequence of words should be viewed as the same candidate. That is, for different candidates, we can unify them by adding phantom candidates. This is similar to the approach proposed by Punyakanok et al. (2008) for the semantic role labeling task. For example, Figure 2 shows the candidate lists generated by our pruning algorithm based on two different parse trees given the segment “its competitors have much broader business interests”. Dashed lines are used for phantom candidates and solid lines for true candidates. Here, system A produces one candidate a1, with two phantom candidates a2 and a3 added. Analogously, phantom candidate b3 is added to the candidate list output by System B. In this way, we can get the unified candidate list: “its competitors have much broader business interests”, “its com</context>
<context position="20185" citStr="Punyakanok et al., 2008" startWordPosition="3298" endWordPosition="3302">”, “have much broader business interests”. • How do we compute the confidence score for every decision? For every candidate in the unified list, we first determine whether it is a true candidate based on the specific parse tree. Then, for a true candidate, we extract the features from the corresponding parse E c ZA P1 &lt; 2, and ci k i=1 k i=1 73 tree. On this basis, we can determine the confidence score using our argument classifier. For a phantom candidate, we set the same prior distribution as the confidence score. In particular, the probability of the ”NULL” class is set to 0.55, following (Punyakanok et al., 2008), and the probabilities of Arg1 and Arg2 are set to their occurrence frequencies in the training data. For the example shown in Figure 2, since System A returns “its competitors have much broader business interests” as a true candidate, we can obtain its confidence score using our argument classifier. For the two phantom candidates — “its competitors” and “have much broader business interests” — we use the prior distributions directly. This applies to the candidates for System B. Finally, we simply average the estimated probabilities to determine the final probability estimate for every candid</context>
</contexts>
<marker>Punyakanok, Roth, Yih, 2008</marker>
<rawString>Vasin Punyakanok, Dan Roth, and Wen-tau Yih. 2008. The important of syntactic parsing and inference in semantic role labeling. Computational Linguistics, 34(2):257–287.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bonnie Webber</author>
</authors>
<title>D-LTAG: extending lexicalized TAG to discourse.</title>
<date>2004</date>
<journal>Cognitive Science,</journal>
<volume>28</volume>
<issue>5</issue>
<pages>779</pages>
<contexts>
<context position="4197" citStr="Webber, 2004" startWordPosition="632" endWordPosition="633">oach to argument labeling. In Section 5, we present our joint inference mechanism via integer linear programming (ILP). Section 6 gives the experimental results and analysis. Finally, we conclude in Section 7. 68 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 68–77, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics 2 Penn Discourse Treebank As the first large-scale annotated corpus that follows the lexically grounded, predicate-argument approach in D-LTAG (Lexicalized Tree Adjoining Grammar for Discourse) (Webber, 2004), the PDTB regards a connective as the predicate of a discourse relation which takes exactly two text spans as its arguments. In particular, the text span that the connective is syntactically attached to is called Arg2, and the other is called Arg1. Although discourse relations can be either explicitly or implicitly expressed in PDTB, this paper focuses only on explicit discourse relations that are explicitly signaled by discourse connectives. Example (1) shows an explicit discourse relation from the article wsj 2314 with connective so underlined, Arg1 span italicized, and Arg2 span bolded. (1</context>
</contexts>
<marker>Webber, 2004</marker>
<rawString>Bonnie Webber. 2004. D-LTAG: extending lexicalized TAG to discourse. Cognitive Science, 28(5):751– 779.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ben Wellner</author>
<author>James Pustejovsky</author>
</authors>
<title>Automatically identifying the arguments of discourse connectives.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>92--101</pages>
<contexts>
<context position="6056" citStr="Wellner and Pustejovsky (2007)" startWordPosition="943" endWordPosition="946">08). Besides, out of all PS cases where Arg1 occurs in some preceding sentence, 79.9% of them are the exact immediately preceding sentence. As such, in this paper, we only consider the current sentence containing the connective and its immediately preceding sentence as the text span where Arg1 occurs, similar to what was done in (Lin et al., 2014). 3 Related Work For argument labeling in discourse parsing on the PDTB corpus, the related work can be classified into two categories: locating parts of arguments, and labeling full argument spans. As a representative on locating parts of arguments, Wellner and Pustejovsky (2007) proposed several machine learning approaches to identify the head words of the two arguments for discourse connectives. Following this work, Elwell and Baldridge (2008) combined general and connective specific rankers to improve the performance of labeling the head words of the two arguments. Prasad et al. (2010) proposed a set of heuristics to locate the position of the Arg1 sentences for intersentence cases. The limitation of locating parts of arguments, such as the positions and head words, is that it is only a partial solution to argument labeling in discourse parsing. In comparison, labe</context>
</contexts>
<marker>Wellner, Pustejovsky, 2007</marker>
<rawString>Ben Wellner and James Pustejovsky. 2007. Automatically identifying the arguments of discourse connectives. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 92–101.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
<author>Martha Palmer</author>
</authors>
<title>Calibrating features for semantic role labeling.</title>
<date>2004</date>
<booktitle>In Proceedings of 2004 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>88--94</pages>
<marker>Xue, Palmer, 2004</marker>
<rawString>Nianwen Xue and Martha Palmer. 2004. Calibrating features for semantic role labeling. In Proceedings of 2004 Conference on Empirical Methods in Natural Language Processing, pages 88–94.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hui Zhang</author>
<author>Min Zhang</author>
<author>Chew Lim Tan</author>
<author>Haizhou Li</author>
</authors>
<title>K-best combination of syntactic parsers.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1552--1560</pages>
<contexts>
<context position="17969" citStr="Zhang et al., 2009" startWordPosition="2915" endWordPosition="2918">,p &lt; 2 (7) 5.4 System Combination Previous work shows that the performance of argument labeling heavily depends on the quality of the syntactic parser. It is natural that combining different argument labeling systems on different parse trees can potentially improve the overall performance of argument labeling. To explore this potential, we build two argument labeling systems — one using the Berkeley parser (Petrov et al., 2006) and the other the Charniak parser (Charniak, 2000). Previous studies show that these two syntactic parsers tend to produce different parse trees for the same sentence (Zhang et al., 2009). For example, our preliminary experiment shows that applying the pruning algorithm on the output of the Charniak parser produces a list of candidates with recall of 80.59% and 89.87% for Arg1 and Arg2 respectively, while achieving recall of 78.6% and 91.1% for Arg1 and Arg2 respectively on the output of the Berkeley parser. It also shows that combining these two candidate lists significantly improves recall to 85.7% and 93.0% for Arg1 and Arg2, respectively. In subsection 5.2, we only consider the confidence scores returned by an argument classifier. Here, we proceed to combine the probabilit</context>
</contexts>
<marker>Zhang, Zhang, Tan, Li, 2009</marker>
<rawString>Hui Zhang, Min Zhang, Chew Lim Tan, and Haizhou Li. 2009. K-best combination of syntactic parsers. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1552–1560.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
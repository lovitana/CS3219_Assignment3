<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.017052">
<title confidence="0.960628">
Modeling Term Translation for Document-informed Machine Translation
</title>
<author confidence="0.996385">
Fandong Meng1, 2 Deyi Xiong3 Wenbin Jiang1, 2 Qun Liu4, 1
</author>
<affiliation confidence="0.997464">
1Key Laboratory of Intelligent Information Processing
Institute of Computing Technology, Chinese Academy of Sciences
2University of Chinese Academy of Sciences
</affiliation>
<email confidence="0.945216">
{mengfandong,jiangwenbin,liuqun}@ict.ac.cn
</email>
<affiliation confidence="0.987086">
3School of Computer Science and Technology, Soochow University
</affiliation>
<email confidence="0.974757">
dyxiong@suda.edu.cn
</email>
<affiliation confidence="0.419576">
4Centre for Next Generation Localisation, Dublin City University
</affiliation>
<sectionHeader confidence="0.983146" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998795">
Term translation is of great importance for
statistical machine translation (SMT), es-
pecially document-informed SMT. In this
paper, we investigate three issues of term
translation in the context of document-
informed SMT and propose three cor-
responding models: (a) a term trans-
lation disambiguation model which se-
lects desirable translations for terms in the
source language with domain information,
</bodyText>
<listItem confidence="0.988325">
(b) a term translation consistency model
that encourages consistent translations for
terms with a high strength of translation
consistency throughout a document, and
(c) a term bracketing model that rewards
</listItem>
<bodyText confidence="0.908504916666667">
translation hypotheses where bracketable
source terms are translated as a whole
unit. We integrate the three models into
hierarchical phrase-based SMT and eval-
uate their effectiveness on NIST Chinese-
English translation tasks with large-scale
training data. Experiment results show
that all three models can achieve sig-
nificant improvements over the baseline.
Additionally, we can obtain a further
improvement when combining the three
models.
</bodyText>
<sectionHeader confidence="0.999465" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99992396969697">
A term is a linguistic expression that is used as
the designation of a defined concept in a language
(ISO 1087). As terms convey concepts of a text,
term translation becomes crucial when the text is
translated from its original language to another
language. The translations of terms are often af-
fected by the domain in which terms are used and
the context that surrounds terms (Vasconcellos et
al., 2001). In this paper, we study domain-specific
and context-sensitive term translation for SMT.
In order to achieve this goal, we focus on three
issues of term translation: 1) translation ambigu-
ity, 2) translation consistency and 3) bracketing.
First, term translation ambiguity is related to trans-
lations of the same term in different domains. A
source language term may have different transla-
tions when it occurs in different domains. Second,
translation consistency is about consistent trans-
lations for terms that occur in the same document.
Usually, it is undesirable to translate the same term
in different ways as it occurs in different parts of
a document. Finally, bracketing concerns whether
a multi-word term is bracketable during transla-
tion. Normally, a multi-word term is translated as
a whole unit into a contiguous target string.
We study these three issues in the context
of document-informed SMT. We use document-
informed information to disambiguate term trans-
lations in different documents and maintain con-
sistent translations for terms that occur in the same
document. We propose three different models for
term translation that attempt to address the three
issues mentioned above. In particular,
</bodyText>
<listItem confidence="0.982946235294118">
• Term Translation Disambiguation Model: In
this model, we condition the translations of
terms in different documents on correspond-
ing per-document topic distributions. In do-
ing so, we enable the decoder to favor trans-
lation hypotheses with domain-specific term
translations.
• Term Translation Consistency Model: This
model encourages the same terms with a high
strength of translation consistency that occur
in different parts of a document to be trans-
lated in a consistent fashion. We calculate
the translation consistency strength of a term
based on the topic distribution of the docu-
ments where the term occurs in this model.
• Term Bracketing Model: We use the brack-
eting model to reward translation hypothe-
</listItem>
<page confidence="0.975621">
546
</page>
<note confidence="0.91057">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 546–556,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999823111111111">
ses where bracketable multi-word terms are
translated as a whole unit.
We integrate the three models into hierarchical
phrase-based SMT (Chiang, 2007). Large-scale
experiment results show that they are all able to
achieve significant improvements of up to 0.89
BLEU points over the baseline. When simulta-
neously integrating the three models into SMT,
we can gain a further improvement, which outper-
forms the baseline by up to 1.16 BLEU points.
In the remainder of this paper, we begin with
a brief overview of related work in Section 2,
and bilingual term extraction in Section 3. We
then elaborate the proposed three models for term
translation in Section 4. Next, we conduct experi-
ments to validate the effectiveness of the proposed
models in Section 5. Finally, we conclude and pro-
vide directions for future work in Section 6.
</bodyText>
<sectionHeader confidence="0.999944" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999969705128205">
In this section, we briefly introduce related work
and highlight the differences between our work
and previous studies.
As we approach term translation disambigua-
tion and consistency via topic modeling, our mod-
els are related to previous work that explores the
topic model (Blei et al., 2003) for machine trans-
lation (Zhao and Xing, 2006; Su et al., 2012;
Xiao et al., 2012; Eidelman et al., 2012). Zhao
and Xing (2006) employ three models that enable
word alignment process to leverage topical con-
tents of document-pairs with topic model. Su et al.
(2012) establish the relationship between out-of-
domain bilingual corpus and in-domain monolin-
gual corpora via topic mapping and phrase-topic
distribution probability estimation for translation
model adaptation. Xiao et al. (2012) propose a
topic similarity model for rule selection. Eidel-
man et al. (2012) use topic models to adapt lexical
weighting probabilities dynamically during trans-
lation. In these studies, the topic model is not used
to address the issues of term translation mentioned
in Section 1.
Our work is also related to document-level
SMT in that we use document-informed informa-
tion for term translation. Tiedemann (2010) pro-
pose cache-based language and translation mod-
els, which are built on recently translated sen-
tences. Gong et al. (2011) extend this by further
introducing two additional caches. They employ
a static cache to store bilingual phrases extracted
from documents in training data that are similar to
the document being translated and a topic cache
with target language topic words. Recently we
have also witnessed efforts that model lexical co-
hesion (Hardmeier et al., 2012; Wong and Kit,
2012; Xiong et al., 2013a; Xiong et al., 2013b)
as well as coherence (Xiong and Zhang, 2013)
for document-level SMT. Hasler et al. (2014a)
use topic models to learn document-level transla-
tion probabilities. Hasler et al. (2014b) use topic-
adapted model to improve lexical selection. The
significant difference between our work and these
studies is that term translation has not been inves-
tigated in these document-level SMT models.
Itagaki and Aikawa (2008) employ bilingual
term bank as a dictionary for machine-aided trans-
lation. Ren et al. (2009) propose a binary feature
to indicate whether a bilingual phrase contains a
term pair. Pinis and Skadins (2012) investigate that
bilingual terms are important for domain adapta-
tion of machine translation. These studies do not
focus on the three issues of term translation as
discussed in Section 1. Furthermore, domain and
document-informed information is not used to as-
sist term translation.
Itagaki et al. (2007) propose a statistical method
to calculate translation consistency for terms with
explicit domain information. Partially inspired
by their study, we introduce a term translation
consistency metric with document-informed infor-
mation. Furthermore, we integrate the proposed
term translation consistency model into an actual
SMT system, which has not been done by Itagaki
et al. (2007). Ture et al. (2012) use IR-inspired
tf-idf scores to encourage consistent translation
choice. Guillou (2013) investigates what kind of
words should be translated consistently. Term
translation consistency has not been investigated
in these studies.
Our term bracketing model is also related
to Xiong et al. (2009)’s syntax-driven bracket-
ing model for phrase-based translation, which pre-
dicts whether a phrase is bracketable or not using
rich syntactic constraints. The difference is that
we construct the model with automatically created
bilingual term bank and do not depend on any syn-
tactic knowledge.
</bodyText>
<sectionHeader confidence="0.982528" genericHeader="method">
3 Bilingual Term Extraction
</sectionHeader>
<bodyText confidence="0.9962875">
Bilingual term extraction is to extract terms from
two languages with the purpose of creating or ex-
</bodyText>
<page confidence="0.995372">
547
</page>
<bodyText confidence="0.999761894736842">
tending a bilingual term bank, which in turn can
be used to improve other tasks such as information
retrieval and machine translation. In this paper, we
want to automatically build a bilingual term bank
so that we can model term translation to improve
translation quality of SMT. Our interest is to ex-
tract multi-word terms.
Currently, there are mainly two strategies to
conduct bilingual term extraction from parallel
corpora. One of them is to extract term candi-
dates separately for each language according to
monolingual term metrics, such as C-value/NC-
value (Frantzi et al., 1998; Vu et al., 2008), or
other common cooccurrence measures such as
Log-Likelihood Ratio, Dice coefficient and Point-
wise Mutual Information (Daille, 1996; Piao et
al., 2006). The extracted monolingual terms are
then paired together (Hjelm, 2007; Fan et al.,
2009; Ren et al., 2009). The other strategy is to
align words and word sequences that are transla-
tion equivalents in parallel corpora and then clas-
sify them into terms and non-terms (Merkel and
Foo, 2007; Lefever et al., 2009; Bouamor et al.,
2012). In this paper, we adopt the first strategy.
In particular, for each sentence pair, we collect all
source phrases which are terms and find aligned
target phrases for them via word alignments. If
the target side is also a term, we store the source
and target term as a term pair.
We conduct monolingual term extraction using
the C-value/NC-value metric and Log-Likelihood
Ratio (LLR) measure respectively. We then com-
bine terms extracted according to the two metrics
mentioned above. For the C-value/NC-value met-
ric based term extraction, we implement it in the
same way as described in Frantzi et al. (1998).
This extraction method recognizes linguistic pat-
terns (mainly noun phrases) listed as follows.
</bodyText>
<equation confidence="0.846698">
((Adj|Noun)&apos;|((Adj|Noun)*
(NounPrep)&apos;)(Adj|Noun)*)Noun
</equation>
<bodyText confidence="0.999983428571429">
It captures the linguistic structures of terms. For
the LLR metric based term extraction, we imple-
ment it according to Daille (1996), who estimate
the propensity of two words to appear together as a
multi-word expression. We then adopt LLR-based
hierarchical reducing algorithm proposed by Ren
et al. (2009) to extract terms with arbitrary lengths.
Since the C-value/NC-value metric based extrac-
tion method can obtain terms in strict linguistic
patterns while the LLR measure based method ex-
tracts more flexible terms, these two methods are
complementary to each other. Therefore, we use
these two methods to extract monolingual multi-
word terms and then combine the extracted terms.
</bodyText>
<sectionHeader confidence="0.993228" genericHeader="method">
4 Models
</sectionHeader>
<bodyText confidence="0.99994725">
This section presents the three models of term
translation. They are the term translation dis-
ambiguation model, term translation consistency
model and term bracketing model respectively.
</bodyText>
<subsectionHeader confidence="0.993781">
4.1 Term Translation Disambiguation Model
</subsectionHeader>
<bodyText confidence="0.999490243243243">
The most straightforward way to disambiguate
term translations in different domains is to cal-
culate the conditional translation probability of
a term given domain information. We use the
topic distribution of a document obtained by a
topic model to represent the domain information
of the document. Since Latent Dirichlet Alloca-
tion (LDA) (Blei et al., 2003) is the most widely-
used topic model, we exploit it for inferring topic
distributions of documents. Xiao et al. (2012)
proposed a topic similarity model for rule selec-
tion. Different from their work, we take an eas-
ier strategy that estimates topic-conditioned term
translation probabilities rather than rule-topic dis-
tributions. This makes our model easily scalable
on large training data.
With the bilingual term bank created from the
training data, we calculate the source-to-target
term translation probability for each term pair con-
ditioned on the topic distribution of the source
document where the source term occurs. We main-
tain a K-dimension (K is the number of topics)
vector for each term pair. The k-th component
p(te|tf, z = k) measures the conditional transla-
tion probability from source term tf to target term
te given the topic k.
We calculate p(te|tf, z = k) via maximum
likelihood estimation with counts from training
data. When the source part of a bilingual term
pair occurs in a document D with topic distribu-
tion p(z|D) estimated via LDA tool, we collect
an instance (tf, te, p(z|D), c), where c is the frac-
tion count of the instance as described in Chiang
(2007). After collection, we get a set of instances
I = {(tf, te, p(z|D), c)} with different document-
topic distributions for each bilingual term pair. Us-
ing these instances, we calculate the probability
</bodyText>
<page confidence="0.862693">
548
</page>
<equation confidence="0.990985666666667">
p(te|tf, z = k) as follows:
p(te|tf, z = k)
E
i∈I,i.tf=tf,i.te=te i.c * p(z = k|D)
=
E
</equation>
<bodyText confidence="0.9793624">
i∈I,i.tf=tf i.c * p(z = k|D)
We associate each extracted term pair in our
bilingual term bank with its corresponding topic-
conditioned translation probabilities estimated in
the Eq. (1). When translating sentences of docu-
ment D0, we first get the topic distribution of D0
using LDA tool. Given a sentence which contains
T terms {tfi}T1 in D0, our term translation disam-
biguation model TermDis can be denoted as
T
</bodyText>
<equation confidence="0.998754">
TermDis = Pd(tei|tfi, D0) (2)
i=1
</equation>
<bodyText confidence="0.989916">
where the conditional source-to-target term trans-
lation probability Pd(tei|tfi, D0) given the docu-
ment D0 is formulated as follows:
</bodyText>
<equation confidence="0.9993495">
Pd(tei|tfi, D0)
K
= p(tei|tfi, z = k) * p(z = k|D0) (3)
k=1
</equation>
<bodyText confidence="0.999861071428572">
Whenever a source term tfi is translated into tei,
we check whether the pair of tfi and its translation
tei can be found in our bilingual term bank. If it
can be found, we calculate the conditional transla-
tion probability from tfi to tei given the document
D0 according to Eq. (3).
The term translation disambiguation model is
integrated into the log-linear model of SMT as a
feature. Its weight is tuned via minimum error rate
training (MERT) (Och, 2003). Through the fea-
ture, we can enable the decoder to favor translation
hypotheses that contain target term translations ap-
propriate for the domain represented by the topic
distribution of the corresponding document.
</bodyText>
<subsectionHeader confidence="0.906453">
4.2 Term Translation Consistency Model
</subsectionHeader>
<bodyText confidence="0.999948279069767">
The term translation disambiguation model helps
the decoder select appropriate translations for
terms that are in accord with their domains. Yet
another translation issue related to the domain-
specific term translation is to what extent a term
should be translated consistently given the domain
where it occurs. Term translation consistency in-
dicates the translation stability that a source term
is translated into the same target term (Itagaki et
al., 2007). When translating a source term, if the
translation consistency strength of the source term
is high, we should take the corresponding target
term as the translation for it. Otherwise, we may
need to create a new translation for it according to
its context. In particular, we want to enable the
decoder to choose between: 1) translating a given
source term into the extracted corresponding tar-
get term or 2) translating it in another way accord-
ing to the strength of its translation consistency.
In doing so, we can encourage consistent transla-
tions for terms with a high translation consistency
strength throughout a document.
Our term translation consistency model can ex-
actly measure the strength of term translation con-
sistency in a document. Since the essential com-
ponent of our term translation consistency model
is the translation consistency strength of the source
term estimated under the topic distribution, we de-
scribe how to calculate it before introducing the
whole model.
With the bilingual term bank created from
training data, we first group each source term
and all its corresponding target terms into a 2-
tuple G(tf, Set(te)), where tf is the source term
and Set(te) is the set of tf’s corresponding tar-
get terms. We maintain a K-dimension (K is
the number of topics) vector for each 2-tuple
G(tf, Set(te)). The k-th component measures the
translation consistency strength cons(tf, k) of the
source term tf given the topic k.
We calculate cons(tf, k) for each
G(tf, Set(te)) with counts from training data as
follows:
</bodyText>
<equation confidence="0.998546">
(qmn * p(k|m) )2(4)
Qk
qmn * p(k|m) (5)
</equation>
<bodyText confidence="0.99991">
where M is the number of documents in which
the source term tf occurs, Nm is the number of
unique corresponding term translations of tf in the
mth document, qmn is the frequency of the nth
translation of tf in the mth document, p(k|m) is
the conditional probability of the mth document
over topic k, and Qk is the normalization factor.
All translations of tf are from Set(te). We adapt
Itagaki et al. (2007)’s translation consistency met-
ric for terms to our topic-based translation consis-
tency measure in the Eq. (4). This equation cal-
culates the translation consistency strength of the
source term tf given the topic k according to the
distribution of tf’s translations in each document
</bodyText>
<equation confidence="0.99799975">
(1)
cons(tf, k) =
Qk = M Nm
m=1 n=1
Nm
n=1
M
m=1
</equation>
<page confidence="0.987988">
549
</page>
<bodyText confidence="0.999917">
where they occur. According to Eq. (4), the trans-
lation consistency strength is a score between 0
and 1. If a source term only occurs in a document
and all its translations are the same, the translation
consistency strength of this term is 1.
We reorganize our bilingual term bank into a
list of 2-tuples G(tf, Set(te))s, each of which is
associated with a K-dimension vector storing the
topic-conditioned translation consistency strength
calculated in the Eq. (4). When translating sen-
tences of document D, we first get the topic dis-
tribution of D via LDA tool. Given a sentence
which contains T terms {tfi}T1 in D, our term
translation consistency model TermCons can be
denoted as
</bodyText>
<equation confidence="0.999368333333333">
T
TermCons = exp(S,(tfi|D)) (6)
i=1
</equation>
<bodyText confidence="0.998048">
where the strength of translation consistency for
tfi given the document D is formulated as fol-
lows:
</bodyText>
<equation confidence="0.999529666666667">
K
S,(tfi|D) = log( cons(tfi, k) * p(k|D)) (7)
k=1
</equation>
<bodyText confidence="0.999930695652174">
During decoding, whenever a hypothesis just
translates a source term tfi into te, we check
whether the translation te can be found in Set(te)
of tfi from the reorganized bilingual term bank. If
it can be found, we calculate the strength of trans-
lation consistency for tfi given the document D
according to Eq. (7) and take it as a soft con-
straint. If the S,(tfi|D) of tfi is high, the decoder
should translate tfi into the extracted correspond-
ing target terms. Otherwise, the decoder will se-
lect translations from outside of Set(te) for tfi. In
doing so, we encourage terms to be translated in
a topic-dependent consistency pattern in the test
data similar to that in the training data so that we
can control the translation consistency of terms in
the test data.
The term translation consistency model is also
integrated into the log-linear model of SMT as a
feature. Through the feature, we can enable the
decoder to translate terms with a high translation
consistency in a document into corresponding tar-
get terms from our bilingual term bank rather than
other translations in a consistent fashion.
</bodyText>
<subsectionHeader confidence="0.989287">
4.3 Term Bracketing Model
</subsectionHeader>
<bodyText confidence="0.999963966666667">
The term translation disambiguation model and
consistency model concern the term translation ac-
curacy with domain information. We further pro-
pose a term bracketing model to guarantee the in-
tegrality of term translation. Xiong et al. (2009)
proposed a syntax-driven bracketing model for
phrase-based translation, which predicts whether
a phrase is bracketable or not using rich syntac-
tic constraints. If a source phrase remains con-
tiguous after translation, they refer to this type of
phrase as bracketable phrase, otherwise unbrack-
etable phrase. For multi-word terms, it is also
desirable to be bracketable since a source term
should be translated as a whole unit and its trans-
lation should be contiguous.
In this paper, we adapt Xiong et al. (2009)’s
bracketing approach to term translation and build
a classifier to measure the probability that a source
term should be translated in a bracketable man-
ner. For all source parts of the extracted bilingual
term bank, we find their target counterparts in the
word-aligned training data. If the corresponding
target counterpart remains contiguous, we take the
source term as a bracketable instance, otherwise
an unbracketable instance. With these bracketable
and unbracketable instances, we train a maximum
entropy binary classifier to predict bracketable (b)
probability of a given source term tf within par-
ticular contexts c(tf). The binary classifier is for-
mulated as follows:
</bodyText>
<equation confidence="0.9910045">
exp(Ej θjhj(b, c(tf)))
Eb&apos; exp(Ej θjhj(b�,c(tf))) (8)
</equation>
<bodyText confidence="0.999973454545454">
where hj E {0,1} is a binary feature function and
θj is the weight of hj. We use the following fea-
tures: 1) the word sequence of the source term, 2)
the first word of the source term, 3) the last word
of the source term, 4) the preceding word of the
first word of the source term, 5) the succeeding
word of the last word of the source term, and 6)
the number of words in the source term.
Given a source sentence which contains T terms
{tfi}T1 , our term bracketing model TermBrack
can be denoted as
</bodyText>
<equation confidence="0.999105333333333">
T
TermBrack = Pb(b|c(tfi)) (9)
i=1
</equation>
<bodyText confidence="0.99978225">
Whenever a hypothesis just covers a source term
tfi, we calculate the bracketable probability of tfi
according to Eq. (8).
The term bracketing model is integrated into the
log-linear model of SMT as a feature. Through the
feature, we want the decoder to translate source
terms with a high bracketable probability as a
whole unit.
</bodyText>
<equation confidence="0.986836">
Pb(b|c(tf)) =
</equation>
<page confidence="0.976833">
550
</page>
<table confidence="0.999916">
Source Target D M
Fangy`u X`ıtˇong defence mechanisms 470 56
Fangy`u X`ıtˇong defence systems
Fangy`u X`ıtˇong defense programmes
Fangy`u X`ıtˇong prevention systems
... ...
Zh`anlu`e Dˇaod`an Fangy`u X`ıtˇong strategic missile defense system 7 0
</table>
<tableCaption confidence="0.999087">
Table 1: Examples of bilingual terms extracted from the training data. “D” means the total number of
</tableCaption>
<bodyText confidence="0.705086">
documents in which the corresponding source term occurs and “M” denotes the number of documents in
which the corresponding source term is translated into different target terms. The source side is Chinese
Pinyin. To save space, we do not list all the 23 different translations of the source term “Fangy`u Xitˇong”.
</bodyText>
<sectionHeader confidence="0.999497" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.9989985">
In this section, we conducted experiments to an-
swer the following three questions.
</bodyText>
<listItem confidence="0.999631857142857">
1. Are our term translation disambiguation,
consistency and bracketing models able to
improve translation quality in BLEU?
2. Does the combination of the three models
provide further improvements?
3. To what extent do the proposed models affect
the translations of test sets?
</listItem>
<subsectionHeader confidence="0.979933">
5.1 Setup
</subsectionHeader>
<bodyText confidence="0.999990789473684">
Our training data consist of 4.28M sentence pairs
extracted from LDC1 data with document bound-
aries explicitly provided. The bilingual training
data contain 67,752 documents, 124.8M Chinese
words and 140.3M English words. We chose
NIST MT05 as the MERT (Och, 2003) tuning set,
NIST MT06 as the development test set, and NIST
MT08 as the final test set. The numbers of docu-
ments/sentences in NIST MT05, MT06 and MT08
are 100/1082, 79/1664 and 109/1357 respectively.
The word alignments were obtained by running
GIZA++ (Och and Ney, 2003) on the corpora in
both directions and using the “grow-diag-final-
and” balance strategy (Koehn et al., 2003). We
adopted SRI Language Modeling Toolkit (Stol-
cke and others, 2002) to train a 4-gram language
model with modified Kneser-Ney smoothing on
the Xinhua portion of the English Gigaword cor-
pus. For the topic model, we used the open source
</bodyText>
<footnote confidence="0.865275833333333">
1The corpora include LDC2003E07, LDC2003E14,
LDC2004T07, LDC2004E12, LDC2005E83, LDC2005T06,
LDC2005T10, LDC2006E24, LDC2006E34, LDC2006E85,
LDC2006E92, LDC2007E87, LDC2007E101,
LDC2008E40, LDC2008E56, LDC2009E16 and
LDC2009E95.
</footnote>
<bodyText confidence="0.9999138125">
LDA tool GibbsLDA++2 with the default setting
for training and inference. We performed 100 it-
erations of the L-BFGS algorithm implemented in
the MaxEnt toolkit3 with both Gaussian prior and
event cutoff set to 1 to train the term bracketing
prediction model (Section 4.3).
We performed part-of-speech tagging for mono-
lingual term extraction (C-value/NC-vaule method
in Section 3) of the source and target languages
with the Stanford NLP toolkit4. The bilingual term
bank was extracted based on the following param-
eter settings of term extraction methods. Empiri-
cally, we set the maximum length of a term to 6
words5. For both the C-value/NC-value and LLR-
based extraction methods, we set the context win-
dow size to 5 words, which is a widely-used set-
ting in previous work. And we set C-value/NC-
value score threshold to 0 and LLR score threshold
to 10 according to the training corpora.
We used the case-insensitive 4-gram BLEU6 as
our evaluation metric. In order to alleviate the im-
pact of the instability of MERT (Och, 2003), we
ran it three times for all our experiments and pre-
sented the average BLEU scores on the three runs
following the suggestion by Clark et al. (2011).
We used an in-house hierarchical phrase-based
decoder to verify our proposed models. Although
the decoder translates a document in a sentence-
by-sentence fashion, it incorporates document-
informed information for sentence translation via
the proposed term translation models trained on
documents.
</bodyText>
<footnote confidence="0.975307375">
2http://sourceforge.net/projects/gibbslda/
3http://homepages.inf.ed.ac.uk/lzhang10/maxent toolkit.html
4http://nlp.stanford.edu/software/tagger.shtml
5We determine the maximum length of a term by testing
{5, 6, 7, 8} in our preliminary experiments. We find that
length 6 produces a slightly better performance than other
values.
6ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v11b.pl
</footnote>
<page confidence="0.995668">
551
</page>
<figureCaption confidence="0.960211333333333">
Figure 1: An example of unbracketable source term in the training data. In (a), “Wˇeiyu´anhuiShˇenyi” is
bracketable while in (b) it is unbracketable. The solid lines connect bilingual phrases. The source side is
Chinese Pinyin.
</figureCaption>
<figure confidence="0.9982304">
shall
members of the commission
the
commission deliberations
Zhǐyǒu
Wěiyuánhuì Chéngyuán
Cái Kě
Cānjiā
Wěiyuánhuì Shěnyì
DŽ
Only
take part in
.
Tā
Jiāng
Zhèxiē Jiànyì
Jiāo Yóu
Shěnyi
Yī Gè
DŽ
Bùzhǎngjí Wbiyuánhuì
submit
these proposals
for
approval
He
to
a
.
committee of ministers
</figure>
<subsectionHeader confidence="0.99636">
5.2 Bilingual Term Bank
</subsectionHeader>
<bodyText confidence="0.999985019230769">
Before reporting the results of the proposed mod-
els, we provide some statistics of the bilingual
term bank extracted from the training data.
According to our statistics, about 1.29M bilin-
gual terms are extracted from the training data.
65.07% of the sentence pairs contain bilingual
terms in the training data. And on average, a
source term has about 1.70 different translations.
These statistics indicate that terms are frequently
used in real-world data and that a source term can
be translated into different target terms.
We also present some examples of bilingual
terms extracted from the training data in Table 1.
Accordingly, we show the total number of doc-
uments in which the corresponding source term
occurs and the number of documents in which
the corresponding source term is translated into
different target terms. The source term “F´angy`u
Xitˇong” has 23 different translations in total. They
are distributed in 470 documents in the training
data. In 414 documents, “F´angy`u X`ıtˇong” has
only one single translation. However, in the other
56 documents it has different translations. This
indicates that “F´angy`u X`ıtˇong” is not consistently
translated in these 56 documents. Different from
this, the source term “Zh`anlu`e Dˇaod`an F´angy`u
X`ıtˇong” only has one translation. And it is trans-
lated consistently in all 7 documents where it oc-
curs. In fact, according to our statistics, there are
about 5.19% source terms whose translations are
not consistent even in the same document.
These examples and statistics suggest 1) that
source terms have domain-specific translations
and 2) that source terms are not necessarily trans-
lated in a consistent manner even in the same doc-
ument. These are exactly the reasons why we pro-
pose the term translation disambiguation and con-
sistency model based on domain information rep-
resented by topic distributions.
Actually, 36.13% of the source terms are not
necessarily translated into target strings as a whole
unit. We show an example of such terms in Fig-
ure 1. In Figure 1-(a), “Wˇeiyu´anhuiShˇenyi” is a
term, and is translated into “commission deliber-
ations” as a whole unit. Therefore “Wˇeiyu´anhu`ı
Shˇenyi” is bracketable in this sentence. How-
ever, in Figure 1-(b), “Wˇeiyu´anhui” and “Shˇenyi”
are translated separately. Therefore “Wˇeiyu´anhu`ı
Shˇenyi” is an unbracketable term in this sentence.
This is the reason why we propose a bracketing
model to predict whether a source term is brack-
etable or not.
</bodyText>
<subsectionHeader confidence="0.995939">
5.3 Effect of the Proposed Models
</subsectionHeader>
<bodyText confidence="0.9999667">
In this section, we validate the effectiveness of the
proposed term translation disambiguation model,
consistency model and bracketing model respec-
tively. In addition to the traditional hiero (Chi-
ang, 2007) system, we also compare against the
“CountFeat” method in Ren et al. (2009) who use
a binary feature to indicate whether a bilingual
phrase contains a term pair. Although Ren et al.
(2009)’s experiments are conducted in a phrase-
based system, the idea can be easily applied to a
hierarchical phrase-based system.
We carried out experiments to investigate the ef-
fect of the term translation disambiguation model
(Dis-Model) and report the results in Table 2. In
order to find the topic number setting with which
our model has the best performance, we ran exper-
iments using the MT06 as the development test set.
From Table 2, we observe that the Dis-Model ob-
tains steady improvements over the baseline and
“CountFeat” method with the topic number K
</bodyText>
<page confidence="0.992081">
552
</page>
<table confidence="0.999197307692308">
Models MT06 MT08 Avg
Baseline 32.43 24.14 28.29
CountFeat 32.77 24.29 28.53
K = 50 32.94* 24.53 28.74
Dis-Model K = 100 33.10* 24.57 28.84
K = 150 33.16* 24.67* 28.92
K = 200 33.08* 24.55 28.81
K = 50 33.09* 24.59 28.84
Cons-Model K = 100 33.13* 24.74* 28.94
K = 150 33.32*+ 24.84*+ 29.08
K = 200 33.02* 24.73* 28.88
Brack-Model 33.09* 24.66* 28.88
Combined-Model 33.59*+ 24.99*+ 29.29
</table>
<tableCaption confidence="0.985737">
Table 2: BLEU-4 scores (%) of the term translation disambiguation model (Dis-Model), the term transla-
</tableCaption>
<bodyText confidence="0.99824272">
tion consistency model (Cons-Model), the term bracketing model (Brack-Model), and the combination of
the three models, on the development test set MT06 and the final test set MT08. K ∈ {50, 100, 150, 200}
which is the number of topics for the Dis-Model and the Cons-Model. “Combined-Model” is the combi-
nation of the three single modes with topic number 150 for the Dis-Model and the Cons-Model. “Base-
line” is the traditional hierarchical phrase-based system. “CountFeat” is the method that adds a counting
feature to reward translation hypotheses containing bilingual term pairs. The “*” and “+” denote that the
results are significantly (Clark et al., 2011) better than those of the baseline system and the CountFeat
method respectively (p&lt;0.01).
ranging from 50 to 150. However, when we set K
to 200, the performance drops. The highest BLEU
scores 33.16 and 24.67 are obtained at the topic
setting K = 150. In fact, our Dis-Model gains
higher performance in BLEU than both the tradi-
tional hiero baseline and the “CountFeat” method
with all topic settings. The “CountFeat” method
rewards translation hypotheses containing bilin-
gual term pairs. However it does not explore any
domain information. Our Dis-Model incorporates
domain information to conduct translation disam-
biguation and achieves higher performance. When
the topic number is set to 150, we gain the high-
est BLEU score, which is higher than that of the
baseline by 0.73 and 0.53 BLEU points on MT06
and MT08, respectively. The final gain over the
baseline is on average 0.63 BLEU points.
We conducted the second group of experiments
to study whether the term translation consistency
model (Cons-Model) is able to improve the per-
formance in BLEU, as well as to investigate the
impact of different topic numbers on the Cons-
Model. Results are shown in Table 2, from which
we observe the similar phenomena to what we
have found in the Dis-Model. Our Cons-Model
gains higher BLEU scores than the baseline sys-
tem and the “CountFeat” method with all topic
settings. Setting topic number to 150 achieves the
highest BLEU score, which is higher than base-
line by 0.89 BLEU points and 0.70 BLEU points
on MT06 and MT08 respectively, and on average
0.79 BLEU points.
We also conducted experiments to verify the ef-
fectiveness of the term bracketing model (Brack-
Model), which conducts bracketing prediction for
source terms. Results in Table 2 show that
our Brack-Model gains higher BLEU scores than
those of the baseline system and the “CountFeat”
method. The final gain of Brack-Model over the
baseline is 0.66 BLEU points and 0.52 points on
MT06 and MT08 respectively, and on average
0.59 BLEU points.
</bodyText>
<subsectionHeader confidence="0.998708">
5.4 Combination of the Three Models
</subsectionHeader>
<bodyText confidence="0.999973181818182">
As shown in the previous subsection, the term
translation disambiguation model, consistency
model and bracketing model substantially outper-
form the baseline. Now, we investigate whether
using these three models simultaneously can lead
to further improvements. The last row in Table 2
shows that the combination of the three models
(Combined-Model) achieves higher BLEU score
than all single models, when we set the topic num-
ber to 150 for the term translation disambigua-
tion model and consistency model. The final gain
</bodyText>
<page confidence="0.99667">
553
</page>
<table confidence="0.9994594">
Models MT06 MT08
Best-Dis-Model 30.89 30.14
Best-Cons-Model 38.04 36.70
Brack-Model 60.46 55.78
Combined-Model 54.39 50.85
</table>
<tableCaption confidence="0.996888">
Table 3: Percentage (%) of 1-best translations
</tableCaption>
<bodyText confidence="0.966055">
which are generated by the Combined-Model and
the three single models with best settings on the
development test set MT06 and the final test set
MT08. The topic number is 150 for Best-Dis-
Model and Best-Cons-Model.
of the Combined-Model over the baseline is 1.16
BLEU points and 0.85 points on MT06 and MT08
respectively, and on average 1.00 BLEU points.
</bodyText>
<subsectionHeader confidence="0.990362">
5.5 Analysis
</subsectionHeader>
<bodyText confidence="0.999993729729729">
In this section, we investigate to what extent the
proposed models affect the translations of test sets.
In Table 3, we show the percentage of 1-best trans-
lations affected by the Combined-Model and the
three single models with best settings on test sets
MT06 and MT08. For single models, if the corre-
sponding feature (disambiguation, consistency or
bracketing) is activated in the 1-best derivation,
the corresponding model has impact on the 1-best
translation. For the Combined-Model, if any of
the corresponding features is activated in the 1-
best derivation, the Combined-Model affects the
1-best translation.
From Table 3, we can see that 1-best transla-
tions of source sentences affected by any of the
proposed models account for a high proportion
(30%∼60%) on both MT06 and MT08. This in-
dicates that all proposed models play an important
role in the translation of both test sets. Among
the three proposed models, the Brack-Model is the
one that affects the largest number of 1-best trans-
lations in both test sets. And the percentage is
60.46% and 55.78% on MT06 and MT08 respec-
tively. The Brack-Model only considers source
terms during decoding, while the Dis-Model and
Cons-Model need to match both source and target
terms. The Brack-Model is more likely to be acti-
vated. Hence the percentage of 1-best translations
affected by this model is higher than those of the
other two models. Since we only investigate the
1-best translations generated by the Combined-
Model and single models, the translations gener-
ated by some single models (e.g., Brack-Model)
may not be generated by the Combined-Model.
Therefore it is hard to say that the numbers of 1-
best translations affected by the Combined-Model
must be greater than those of single models.
</bodyText>
<sectionHeader confidence="0.995639" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999992217391304">
We have studied the three issues of term trans-
lation and proposed three different term trans-
lation models for document-informed SMT. The
term translation disambiguation model enables
the decoder to favor the most suitable domain-
specific translations with domain information for
source terms. The term translation consistency
model encourages the decoder to translate source
terms with a high domain translation consistency
strength into target terms rather than other new
strings. Finally, the term bracketing model re-
wards hypotheses that translate bracketable terms
into continuous target strings as a whole unit.
We integrate the three models into a hierarchical
phrase-based SMT system7 and evaluate their ef-
fectiveness on the NIST Chinese-English transla-
tion task with large-scale training data. Experi-
ment results show that all three models achieve
significant improvements over the baseline. Ad-
ditionally, combining the three models achieves a
further improvement. For future work, we would
like to evaluate our models on term translation
across a range of different domains.
</bodyText>
<sectionHeader confidence="0.998634" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.998454533333333">
This work was supported by National Key Tech-
nology R&amp;D Program (No. 2012BAH39B03) and
CAS Action Plan for the Development of Western
China (No. KGZD-EW-501). Deyi Xiong’s work
was supported by Natural Science Foundation of
Jiangsu Province (Grant No. BK20140355). Qun
Liu’s work was partially supported by Science
Foundation Ireland (Grant No. 07/CE/I1142) as
part of the CNGL at Dublin City University. Sin-
cere thanks to the anonymous reviewers for their
thorough reviewing and valuable suggestions. The
corresponding author of this paper, according to
the meaning given to this role by University of
Chinese Academy of Sciences and Soochow Uni-
versity, is Deyi Xiong.
</bodyText>
<footnote confidence="0.968056333333333">
7Our models are not limited to hierarchical phrase-based
SMT. They can be easily applied to other SMT formalisms,
such as phrase- and syntax-based SMT.
</footnote>
<page confidence="0.997167">
554
</page>
<sectionHeader confidence="0.996311" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999665537037037">
David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent dirichlet allocation. the Journal of ma-
chine Learning research, 3:993–1022.
Houda Bouamor, Aur´elien Max, and Anne Vilnat.
2012. Validation of sub-sentential paraphrases ac-
quired from parallel monolingual corpora. In Pro-
ceedings of the 13th Conference of the European
Chapter of the Association for Computational Lin-
guistics, pages 716–725. Association for Computa-
tional Linguistics.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201–228.
Jonathan H Clark, Chris Dyer, Alon Lavie, and Noah A
Smith. 2011. Better hypothesis testing for statistical
machine translation: Controlling for optimizer insta-
bility. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies: short papers-Volume
2, pages 176–181.
B´eatrice Daille. 1996. Study and implementation of
combined techniques for automatic extraction of ter-
minology. Journal of The balancing act: Combin-
ing symbolic and statistical approaches to language,
1:49–66.
Vladimir Eidelman, Jordan Boyd-Graber, and Philip
Resnik. 2012. Topic models for dynamic translation
model adaptation. In Proceedings of the 50th An-
nual Meeting of the Association for Computational
Linguistics: Short Papers-Volume 2, pages 115–119.
Association for Computational Linguistics.
Xiaorong Fan, Nobuyuki Shimizu, and Hiroshi Nak-
agawa. 2009. Automatic extraction of bilin-
gual terms from a chinese-japanese parallel corpus.
In Proceedings of the 3rd International Universal
Communication Symposium, pages 41–45. ACM.
Katerina T Frantzi, Sophia Ananiadou, and Junichi
Tsujii. 1998. The c-value/nc-value method of au-
tomatic recognition for multi-word terms. In Re-
search and Advanced Technology for Digital Li-
braries, pages 585–604. Springer.
Zhengxian Gong, Min Zhang, and Guodong Zhou.
2011. Cache-based document-level statistical ma-
chine translation. In Proceedings of the Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 909–919.
Liane Guillou. 2013. Analysing lexical consistency
in translation. In Proceedings of the Workshop on
Discourse in Machine Translation, pages 10–18.
Christian Hardmeier, Joakim Nivre, and J¨org Tiede-
mann. 2012. Document-wide decoding for phrase-
based statistical machine translation. In Proceed-
ings of the 2012 Joint Conference on Empiri-
cal Methods in Natural Language Processing and
Computational Natural Language Learning, pages
1179–1190.
Eva Hasler, Phil Blunsom, Philipp Koehn, and Barry
Haddow. 2014a. Dynamic topic adaptation for
phrase-based mt. In Proceedings of the 14th Confer-
ence of the European Chapter of the Association for
Computational Linguistics, Gothenburg, Sweden.
Eva Hasler, Barry Haddow, and Philipp Koehn. 2014b.
Dynamic topic adaptation for smt using distribu-
tional profiles. In Proceedings of the Ninth Work-
shop on Statistical Machine Translation, pages 445–
456, Baltimore, Maryland, USA, June. Association
for Computational Linguistics.
Hans Hjelm. 2007. Identifying cross language term
equivalents using statistical machine translation and
distributional association measures. In Proceedings
of 16th Nordic Conference of Computational Lin-
guistics Nodalida, pages 97–104.
Masaki Itagaki and Takako Aikawa. 2008. Post-mt
term swapper: Supplementing a statistical machine
translation system with a user dictionary. In LREC.
Masaki Itagaki, Takako Aikawa, and Xiaodong He.
2007. Automatic validation of terminology trans-
lation consistency with statistical method. Proceed-
ings of MT summit XI, pages 269–274.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology-
Volume 1, pages 48–54.
Els Lefever, Lieve Macken, and Veronique Hoste.
2009. Language-independent bilingual terminology
extraction from a multilingual parallel corpus. In
Proceedings of the 12th Conference of the European
Chapter of the Association for Computational Lin-
guistics, pages 496–504.
Magnus Merkel and Jody Foo. 2007. Terminology
extraction and term ranking for standardizing term
banks. In Proceedings of 16th Nordic Conference
of Computational Linguistics Nodalida, pages 349–
354.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational linguistics, 29(1):19–51.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 41st Annual Meeting on Association for Compu-
tational Linguistics-Volume 1, pages 160–167.
Scott SL Piao, Guangfan Sun, Paul Rayson, and
Qi Yuan. 2006. Automatic extraction of chi-
nese multiword expressions with a statistical tool.
In Workshop on Multi-word-expressions in a Mul-
tilingual Context held in conjunction with the 11th
EACL, Trento, Italy, pages 17–24.
</reference>
<page confidence="0.98205">
555
</page>
<reference confidence="0.999611228915662">
Pinis and Skadins. 2012. Mt adaptation for under-
resourced domains–what works and what not. In
Human Language Technologies–The Baltic Perspec-
tive: Proceedings of the Fifth International Confer-
ence Baltic HLT 2012, volume 247, page 176. IOS
Press.
Zhixiang Ren, Yajuan L¨u, Jie Cao, Qun Liu, and Yun
Huang. 2009. Improving statistical machine trans-
lation using domain bilingual multiword expres-
sions. In Proceedings of the Workshop on Multiword
Expressions: Identification, Interpretation, Disam-
biguation and Applications, pages 47–54.
Andreas Stolcke et al. 2002. Srilm-an extensible lan-
guage modeling toolkit. In Proceedings of the inter-
national conference on spoken language processing,
volume 2, pages 901–904.
Jinsong Su, Hua Wu, Haifeng Wang, Yidong Chen,
Xiaodong Shi, Huailin Dong, and Qun Liu. 2012.
Translation model adaptation for statistical machine
translation with monolingual topic information. In
Proceedings of the 50th Annual Meeting of the
Association for Computational Linguistics: Long
Papers-Volume 1, pages 459–468.
J¨org Tiedemann. 2010. Context adaptation in statisti-
cal machine translation using models with exponen-
tially decaying cache. In Proceedings of the 2010
Workshop on Domain Adaptation for Natural Lan-
guage Processing, pages 8–15.
Ferhan Ture, Douglas W Oard, and Philip Resnik.
2012. Encouraging consistent translation choices.
In Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 417–426. Association for Computational Lin-
guistics.
Muriel Vasconcellos, Brian Avey, Claudia Gdaniec,
Laurie Gerber, Marjorie Le´on, and Teruko Mita-
mura. 2001. Terminology and machine translation.
Handbook of Terminology Management, 2:697–723.
Thuy Vu, Ai Ti Aw, and Min Zhang. 2008. Term ex-
traction through unithood and termhood unification.
In Proceedings of the third international joint con-
ference on natural language processing.
Billy Wong and Chunyu Kit. 2012. Extending ma-
chine translation evaluation metrics with lexical co-
hesion to document level. In Proceedings of the
2012 Joint Conference on Empirical Methods in
Natural Language Processing and Computational
Natural Language Learning, pages 1060–1068.
Xinyan Xiao, Deyi Xiong, Min Zhang, Qun Liu, and
Shouxun Lin. 2012. A topic similarity model for hi-
erarchical phrase-based translation. In Proceedings
of the 50th Annual Meeting of the Association for
Computational Linguistics: Long Papers-Volume 1,
pages 750–758.
Deyi Xiong and Min Zhang. 2013. A topic-based
coherence model for statistical machine translation.
In Proceedings of the Twenty-Seventh AAAI Confer-
ence on Artificial Intelligence (AAAI-13), Bellevue,
Washington, USA, July.
Deyi Xiong, Min Zhang, Aiti Aw, and Haizhou Li.
2009. A syntax-driven bracketing model for phrase-
based translation. In Proceedings of the Joint Con-
ference of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on Natural
Language Processing of the AFNLP, pages 315–
323.
Deyi Xiong, Guosheng Ben, Min Zhang, Yajuan L¨u,
and Qun Liu. 2013a. Modeling lexical cohesion for
document-level machine translation. In Proceedings
of the Twenty-Third international joint conference
on Artificial Intelligence, pages 2183–2189. AAAI
Press.
Deyi Xiong, Yang Ding, Min Zhang, and Chew Lim
Tan. 2013b. Lexical chain based cohesion mod-
els for document-level statistical machine transla-
tion. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing, pages
1563––1573.
Bing Zhao and Eric P Xing. 2006. Bitam: Bilingual
topic admixture models for word alignment. In Pro-
ceedings of the COLING/ACL on Main conference
poster sessions, pages 969–976.
</reference>
<page confidence="0.998549">
556
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.230153">
<title confidence="0.999935">Modeling Term Translation for Document-informed Machine Translation</title>
<author confidence="0.96168">Deyi Wenbin Qun</author>
<affiliation confidence="0.5810602">Laboratory of Intelligent Information Institute of Computing Technology, Chinese Academy of of Chinese Academy of of Computer Science and Technology, Soochow for Next Generation Localisation, Dublin City University</affiliation>
<abstract confidence="0.99896725">Term translation is of great importance for statistical machine translation (SMT), especially document-informed SMT. In this paper, we investigate three issues of term translation in the context of documentinformed SMT and propose three corresponding models: (a) a term translation disambiguation model which selects desirable translations for terms in the source language with domain information, (b) a term translation consistency model that encourages consistent translations for terms with a high strength of translation consistency throughout a document, and (c) a term bracketing model that rewards translation hypotheses where bracketable source terms are translated as a whole unit. We integrate the three models into hierarchical phrase-based SMT and evaluate their effectiveness on NIST Chinese- English translation tasks with large-scale training data. Experiment results show that all three models can achieve significant improvements over the baseline. Additionally, we can obtain a further improvement when combining the three models.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent dirichlet allocation.</title>
<date>2003</date>
<journal>the Journal of machine Learning research,</journal>
<pages>3--993</pages>
<contexts>
<context position="5210" citStr="Blei et al., 2003" startWordPosition="790" endWordPosition="793">ork in Section 2, and bilingual term extraction in Section 3. We then elaborate the proposed three models for term translation in Section 4. Next, we conduct experiments to validate the effectiveness of the proposed models in Section 5. Finally, we conclude and provide directions for future work in Section 6. 2 Related Work In this section, we briefly introduce related work and highlight the differences between our work and previous studies. As we approach term translation disambiguation and consistency via topic modeling, our models are related to previous work that explores the topic model (Blei et al., 2003) for machine translation (Zhao and Xing, 2006; Su et al., 2012; Xiao et al., 2012; Eidelman et al., 2012). Zhao and Xing (2006) employ three models that enable word alignment process to leverage topical contents of document-pairs with topic model. Su et al. (2012) establish the relationship between out-ofdomain bilingual corpus and in-domain monolingual corpora via topic mapping and phrase-topic distribution probability estimation for translation model adaptation. Xiao et al. (2012) propose a topic similarity model for rule selection. Eidelman et al. (2012) use topic models to adapt lexical we</context>
<context position="11798" citStr="Blei et al., 2003" startWordPosition="1816" endWordPosition="1819"> the extracted terms. 4 Models This section presents the three models of term translation. They are the term translation disambiguation model, term translation consistency model and term bracketing model respectively. 4.1 Term Translation Disambiguation Model The most straightforward way to disambiguate term translations in different domains is to calculate the conditional translation probability of a term given domain information. We use the topic distribution of a document obtained by a topic model to represent the domain information of the document. Since Latent Dirichlet Allocation (LDA) (Blei et al., 2003) is the most widelyused topic model, we exploit it for inferring topic distributions of documents. Xiao et al. (2012) proposed a topic similarity model for rule selection. Different from their work, we take an easier strategy that estimates topic-conditioned term translation probabilities rather than rule-topic distributions. This makes our model easily scalable on large training data. With the bilingual term bank created from the training data, we calculate the source-to-target term translation probability for each term pair conditioned on the topic distribution of the source document where t</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M Blei, Andrew Y Ng, and Michael I Jordan. 2003. Latent dirichlet allocation. the Journal of machine Learning research, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Houda Bouamor</author>
<author>Aur´elien Max</author>
<author>Anne Vilnat</author>
</authors>
<title>Validation of sub-sentential paraphrases acquired from parallel monolingual corpora.</title>
<date>2012</date>
<booktitle>In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>716--725</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="9759" citStr="Bouamor et al., 2012" startWordPosition="1504" endWordPosition="1507">tes separately for each language according to monolingual term metrics, such as C-value/NCvalue (Frantzi et al., 1998; Vu et al., 2008), or other common cooccurrence measures such as Log-Likelihood Ratio, Dice coefficient and Pointwise Mutual Information (Daille, 1996; Piao et al., 2006). The extracted monolingual terms are then paired together (Hjelm, 2007; Fan et al., 2009; Ren et al., 2009). The other strategy is to align words and word sequences that are translation equivalents in parallel corpora and then classify them into terms and non-terms (Merkel and Foo, 2007; Lefever et al., 2009; Bouamor et al., 2012). In this paper, we adopt the first strategy. In particular, for each sentence pair, we collect all source phrases which are terms and find aligned target phrases for them via word alignments. If the target side is also a term, we store the source and target term as a term pair. We conduct monolingual term extraction using the C-value/NC-value metric and Log-Likelihood Ratio (LLR) measure respectively. We then combine terms extracted according to the two metrics mentioned above. For the C-value/NC-value metric based term extraction, we implement it in the same way as described in Frantzi et al</context>
</contexts>
<marker>Bouamor, Max, Vilnat, 2012</marker>
<rawString>Houda Bouamor, Aur´elien Max, and Anne Vilnat. 2012. Validation of sub-sentential paraphrases acquired from parallel monolingual corpora. In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 716–725. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="4223" citStr="Chiang, 2007" startWordPosition="627" endWordPosition="628"> translated in a consistent fashion. We calculate the translation consistency strength of a term based on the topic distribution of the documents where the term occurs in this model. • Term Bracketing Model: We use the bracketing model to reward translation hypothe546 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 546–556, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics ses where bracketable multi-word terms are translated as a whole unit. We integrate the three models into hierarchical phrase-based SMT (Chiang, 2007). Large-scale experiment results show that they are all able to achieve significant improvements of up to 0.89 BLEU points over the baseline. When simultaneously integrating the three models into SMT, we can gain a further improvement, which outperforms the baseline by up to 1.16 BLEU points. In the remainder of this paper, we begin with a brief overview of related work in Section 2, and bilingual term extraction in Section 3. We then elaborate the proposed three models for term translation in Section 4. Next, we conduct experiments to validate the effectiveness of the proposed models in Secti</context>
<context position="12981" citStr="Chiang (2007)" startWordPosition="2016" endWordPosition="2017">of the source document where the source term occurs. We maintain a K-dimension (K is the number of topics) vector for each term pair. The k-th component p(te|tf, z = k) measures the conditional translation probability from source term tf to target term te given the topic k. We calculate p(te|tf, z = k) via maximum likelihood estimation with counts from training data. When the source part of a bilingual term pair occurs in a document D with topic distribution p(z|D) estimated via LDA tool, we collect an instance (tf, te, p(z|D), c), where c is the fraction count of the instance as described in Chiang (2007). After collection, we get a set of instances I = {(tf, te, p(z|D), c)} with different documenttopic distributions for each bilingual term pair. Using these instances, we calculate the probability 548 p(te|tf, z = k) as follows: p(te|tf, z = k) E i∈I,i.tf=tf,i.te=te i.c * p(z = k|D) = E i∈I,i.tf=tf i.c * p(z = k|D) We associate each extracted term pair in our bilingual term bank with its corresponding topicconditioned translation probabilities estimated in the Eq. (1). When translating sentences of document D0, we first get the topic distribution of D0 using LDA tool. Given a sentence which co</context>
<context position="29015" citStr="Chiang, 2007" startWordPosition="4603" endWordPosition="4605">ssion deliberations” as a whole unit. Therefore “Wˇeiyu´anhu`ı Shˇenyi” is bracketable in this sentence. However, in Figure 1-(b), “Wˇeiyu´anhui” and “Shˇenyi” are translated separately. Therefore “Wˇeiyu´anhu`ı Shˇenyi” is an unbracketable term in this sentence. This is the reason why we propose a bracketing model to predict whether a source term is bracketable or not. 5.3 Effect of the Proposed Models In this section, we validate the effectiveness of the proposed term translation disambiguation model, consistency model and bracketing model respectively. In addition to the traditional hiero (Chiang, 2007) system, we also compare against the “CountFeat” method in Ren et al. (2009) who use a binary feature to indicate whether a bilingual phrase contains a term pair. Although Ren et al. (2009)’s experiments are conducted in a phrasebased system, the idea can be easily applied to a hierarchical phrase-based system. We carried out experiments to investigate the effect of the term translation disambiguation model (Dis-Model) and report the results in Table 2. In order to find the topic number setting with which our model has the best performance, we ran experiments using the MT06 as the development </context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2):201–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan H Clark</author>
<author>Chris Dyer</author>
<author>Alon Lavie</author>
<author>Noah A Smith</author>
</authors>
<title>Better hypothesis testing for statistical machine translation: Controlling for optimizer instability.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association</booktitle>
<pages>176--181</pages>
<contexts>
<context position="25082" citStr="Clark et al. (2011)" startWordPosition="4013" endWordPosition="4016">mpirically, we set the maximum length of a term to 6 words5. For both the C-value/NC-value and LLRbased extraction methods, we set the context window size to 5 words, which is a widely-used setting in previous work. And we set C-value/NCvalue score threshold to 0 and LLR score threshold to 10 according to the training corpora. We used the case-insensitive 4-gram BLEU6 as our evaluation metric. In order to alleviate the impact of the instability of MERT (Och, 2003), we ran it three times for all our experiments and presented the average BLEU scores on the three runs following the suggestion by Clark et al. (2011). We used an in-house hierarchical phrase-based decoder to verify our proposed models. Although the decoder translates a document in a sentenceby-sentence fashion, it incorporates documentinformed information for sentence translation via the proposed term translation models trained on documents. 2http://sourceforge.net/projects/gibbslda/ 3http://homepages.inf.ed.ac.uk/lzhang10/maxent toolkit.html 4http://nlp.stanford.edu/software/tagger.shtml 5We determine the maximum length of a term by testing {5, 6, 7, 8} in our preliminary experiments. We find that length 6 produces a slightly better perfo</context>
<context position="30911" citStr="Clark et al., 2011" startWordPosition="4919" endWordPosition="4922">e term bracketing model (Brack-Model), and the combination of the three models, on the development test set MT06 and the final test set MT08. K ∈ {50, 100, 150, 200} which is the number of topics for the Dis-Model and the Cons-Model. “Combined-Model” is the combination of the three single modes with topic number 150 for the Dis-Model and the Cons-Model. “Baseline” is the traditional hierarchical phrase-based system. “CountFeat” is the method that adds a counting feature to reward translation hypotheses containing bilingual term pairs. The “*” and “+” denote that the results are significantly (Clark et al., 2011) better than those of the baseline system and the CountFeat method respectively (p&lt;0.01). ranging from 50 to 150. However, when we set K to 200, the performance drops. The highest BLEU scores 33.16 and 24.67 are obtained at the topic setting K = 150. In fact, our Dis-Model gains higher performance in BLEU than both the traditional hiero baseline and the “CountFeat” method with all topic settings. The “CountFeat” method rewards translation hypotheses containing bilingual term pairs. However it does not explore any domain information. Our Dis-Model incorporates domain information to conduct tran</context>
</contexts>
<marker>Clark, Dyer, Lavie, Smith, 2011</marker>
<rawString>Jonathan H Clark, Chris Dyer, Alon Lavie, and Noah A Smith. 2011. Better hypothesis testing for statistical machine translation: Controlling for optimizer instability. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers-Volume 2, pages 176–181.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B´eatrice Daille</author>
</authors>
<title>Study and implementation of combined techniques for automatic extraction of terminology.</title>
<date>1996</date>
<journal>Journal of The</journal>
<pages>1--49</pages>
<contexts>
<context position="9406" citStr="Daille, 1996" startWordPosition="1445" endWordPosition="1446">translation. In this paper, we want to automatically build a bilingual term bank so that we can model term translation to improve translation quality of SMT. Our interest is to extract multi-word terms. Currently, there are mainly two strategies to conduct bilingual term extraction from parallel corpora. One of them is to extract term candidates separately for each language according to monolingual term metrics, such as C-value/NCvalue (Frantzi et al., 1998; Vu et al., 2008), or other common cooccurrence measures such as Log-Likelihood Ratio, Dice coefficient and Pointwise Mutual Information (Daille, 1996; Piao et al., 2006). The extracted monolingual terms are then paired together (Hjelm, 2007; Fan et al., 2009; Ren et al., 2009). The other strategy is to align words and word sequences that are translation equivalents in parallel corpora and then classify them into terms and non-terms (Merkel and Foo, 2007; Lefever et al., 2009; Bouamor et al., 2012). In this paper, we adopt the first strategy. In particular, for each sentence pair, we collect all source phrases which are terms and find aligned target phrases for them via word alignments. If the target side is also a term, we store the source</context>
<context position="10651" citStr="Daille (1996)" startWordPosition="1644" endWordPosition="1645"> We conduct monolingual term extraction using the C-value/NC-value metric and Log-Likelihood Ratio (LLR) measure respectively. We then combine terms extracted according to the two metrics mentioned above. For the C-value/NC-value metric based term extraction, we implement it in the same way as described in Frantzi et al. (1998). This extraction method recognizes linguistic patterns (mainly noun phrases) listed as follows. ((Adj|Noun)&apos;|((Adj|Noun)* (NounPrep)&apos;)(Adj|Noun)*)Noun It captures the linguistic structures of terms. For the LLR metric based term extraction, we implement it according to Daille (1996), who estimate the propensity of two words to appear together as a multi-word expression. We then adopt LLR-based hierarchical reducing algorithm proposed by Ren et al. (2009) to extract terms with arbitrary lengths. Since the C-value/NC-value metric based extraction method can obtain terms in strict linguistic patterns while the LLR measure based method extracts more flexible terms, these two methods are complementary to each other. Therefore, we use these two methods to extract monolingual multiword terms and then combine the extracted terms. 4 Models This section presents the three models o</context>
</contexts>
<marker>Daille, 1996</marker>
<rawString>B´eatrice Daille. 1996. Study and implementation of combined techniques for automatic extraction of terminology. Journal of The balancing act: Combining symbolic and statistical approaches to language, 1:49–66.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir Eidelman</author>
<author>Jordan Boyd-Graber</author>
<author>Philip Resnik</author>
</authors>
<title>Topic models for dynamic translation model adaptation.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers-Volume 2,</booktitle>
<pages>115--119</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="5315" citStr="Eidelman et al., 2012" startWordPosition="810" endWordPosition="813">dels for term translation in Section 4. Next, we conduct experiments to validate the effectiveness of the proposed models in Section 5. Finally, we conclude and provide directions for future work in Section 6. 2 Related Work In this section, we briefly introduce related work and highlight the differences between our work and previous studies. As we approach term translation disambiguation and consistency via topic modeling, our models are related to previous work that explores the topic model (Blei et al., 2003) for machine translation (Zhao and Xing, 2006; Su et al., 2012; Xiao et al., 2012; Eidelman et al., 2012). Zhao and Xing (2006) employ three models that enable word alignment process to leverage topical contents of document-pairs with topic model. Su et al. (2012) establish the relationship between out-ofdomain bilingual corpus and in-domain monolingual corpora via topic mapping and phrase-topic distribution probability estimation for translation model adaptation. Xiao et al. (2012) propose a topic similarity model for rule selection. Eidelman et al. (2012) use topic models to adapt lexical weighting probabilities dynamically during translation. In these studies, the topic model is not used to ad</context>
</contexts>
<marker>Eidelman, Boyd-Graber, Resnik, 2012</marker>
<rawString>Vladimir Eidelman, Jordan Boyd-Graber, and Philip Resnik. 2012. Topic models for dynamic translation model adaptation. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers-Volume 2, pages 115–119. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaorong Fan</author>
<author>Nobuyuki Shimizu</author>
<author>Hiroshi Nakagawa</author>
</authors>
<title>Automatic extraction of bilingual terms from a chinese-japanese parallel corpus.</title>
<date>2009</date>
<booktitle>In Proceedings of the 3rd International Universal Communication Symposium,</booktitle>
<pages>41--45</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="9515" citStr="Fan et al., 2009" startWordPosition="1461" endWordPosition="1464">m translation to improve translation quality of SMT. Our interest is to extract multi-word terms. Currently, there are mainly two strategies to conduct bilingual term extraction from parallel corpora. One of them is to extract term candidates separately for each language according to monolingual term metrics, such as C-value/NCvalue (Frantzi et al., 1998; Vu et al., 2008), or other common cooccurrence measures such as Log-Likelihood Ratio, Dice coefficient and Pointwise Mutual Information (Daille, 1996; Piao et al., 2006). The extracted monolingual terms are then paired together (Hjelm, 2007; Fan et al., 2009; Ren et al., 2009). The other strategy is to align words and word sequences that are translation equivalents in parallel corpora and then classify them into terms and non-terms (Merkel and Foo, 2007; Lefever et al., 2009; Bouamor et al., 2012). In this paper, we adopt the first strategy. In particular, for each sentence pair, we collect all source phrases which are terms and find aligned target phrases for them via word alignments. If the target side is also a term, we store the source and target term as a term pair. We conduct monolingual term extraction using the C-value/NC-value metric and</context>
</contexts>
<marker>Fan, Shimizu, Nakagawa, 2009</marker>
<rawString>Xiaorong Fan, Nobuyuki Shimizu, and Hiroshi Nakagawa. 2009. Automatic extraction of bilingual terms from a chinese-japanese parallel corpus. In Proceedings of the 3rd International Universal Communication Symposium, pages 41–45. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katerina T Frantzi</author>
<author>Sophia Ananiadou</author>
<author>Junichi Tsujii</author>
</authors>
<title>The c-value/nc-value method of automatic recognition for multi-word terms.</title>
<date>1998</date>
<booktitle>In Research and Advanced Technology for Digital Libraries,</booktitle>
<pages>585--604</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="9255" citStr="Frantzi et al., 1998" startWordPosition="1421" endWordPosition="1424">ith the purpose of creating or ex547 tending a bilingual term bank, which in turn can be used to improve other tasks such as information retrieval and machine translation. In this paper, we want to automatically build a bilingual term bank so that we can model term translation to improve translation quality of SMT. Our interest is to extract multi-word terms. Currently, there are mainly two strategies to conduct bilingual term extraction from parallel corpora. One of them is to extract term candidates separately for each language according to monolingual term metrics, such as C-value/NCvalue (Frantzi et al., 1998; Vu et al., 2008), or other common cooccurrence measures such as Log-Likelihood Ratio, Dice coefficient and Pointwise Mutual Information (Daille, 1996; Piao et al., 2006). The extracted monolingual terms are then paired together (Hjelm, 2007; Fan et al., 2009; Ren et al., 2009). The other strategy is to align words and word sequences that are translation equivalents in parallel corpora and then classify them into terms and non-terms (Merkel and Foo, 2007; Lefever et al., 2009; Bouamor et al., 2012). In this paper, we adopt the first strategy. In particular, for each sentence pair, we collect </context>
</contexts>
<marker>Frantzi, Ananiadou, Tsujii, 1998</marker>
<rawString>Katerina T Frantzi, Sophia Ananiadou, and Junichi Tsujii. 1998. The c-value/nc-value method of automatic recognition for multi-word terms. In Research and Advanced Technology for Digital Libraries, pages 585–604. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhengxian Gong</author>
<author>Min Zhang</author>
<author>Guodong Zhou</author>
</authors>
<title>Cache-based document-level statistical machine translation.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>909--919</pages>
<contexts>
<context position="6228" citStr="Gong et al. (2011)" startWordPosition="950" endWordPosition="953">istribution probability estimation for translation model adaptation. Xiao et al. (2012) propose a topic similarity model for rule selection. Eidelman et al. (2012) use topic models to adapt lexical weighting probabilities dynamically during translation. In these studies, the topic model is not used to address the issues of term translation mentioned in Section 1. Our work is also related to document-level SMT in that we use document-informed information for term translation. Tiedemann (2010) propose cache-based language and translation models, which are built on recently translated sentences. Gong et al. (2011) extend this by further introducing two additional caches. They employ a static cache to store bilingual phrases extracted from documents in training data that are similar to the document being translated and a topic cache with target language topic words. Recently we have also witnessed efforts that model lexical cohesion (Hardmeier et al., 2012; Wong and Kit, 2012; Xiong et al., 2013a; Xiong et al., 2013b) as well as coherence (Xiong and Zhang, 2013) for document-level SMT. Hasler et al. (2014a) use topic models to learn document-level translation probabilities. Hasler et al. (2014b) use top</context>
</contexts>
<marker>Gong, Zhang, Zhou, 2011</marker>
<rawString>Zhengxian Gong, Min Zhang, and Guodong Zhou. 2011. Cache-based document-level statistical machine translation. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 909–919.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liane Guillou</author>
</authors>
<title>Analysing lexical consistency in translation.</title>
<date>2013</date>
<booktitle>In Proceedings of the Workshop on Discourse in Machine Translation,</booktitle>
<pages>10--18</pages>
<contexts>
<context position="8045" citStr="Guillou (2013)" startWordPosition="1231" endWordPosition="1232">tion 1. Furthermore, domain and document-informed information is not used to assist term translation. Itagaki et al. (2007) propose a statistical method to calculate translation consistency for terms with explicit domain information. Partially inspired by their study, we introduce a term translation consistency metric with document-informed information. Furthermore, we integrate the proposed term translation consistency model into an actual SMT system, which has not been done by Itagaki et al. (2007). Ture et al. (2012) use IR-inspired tf-idf scores to encourage consistent translation choice. Guillou (2013) investigates what kind of words should be translated consistently. Term translation consistency has not been investigated in these studies. Our term bracketing model is also related to Xiong et al. (2009)’s syntax-driven bracketing model for phrase-based translation, which predicts whether a phrase is bracketable or not using rich syntactic constraints. The difference is that we construct the model with automatically created bilingual term bank and do not depend on any syntactic knowledge. 3 Bilingual Term Extraction Bilingual term extraction is to extract terms from two languages with the pu</context>
</contexts>
<marker>Guillou, 2013</marker>
<rawString>Liane Guillou. 2013. Analysing lexical consistency in translation. In Proceedings of the Workshop on Discourse in Machine Translation, pages 10–18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian Hardmeier</author>
<author>Joakim Nivre</author>
<author>J¨org Tiedemann</author>
</authors>
<title>Document-wide decoding for phrasebased statistical machine translation.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>1179--1190</pages>
<contexts>
<context position="6576" citStr="Hardmeier et al., 2012" startWordPosition="1005" endWordPosition="1008">ntioned in Section 1. Our work is also related to document-level SMT in that we use document-informed information for term translation. Tiedemann (2010) propose cache-based language and translation models, which are built on recently translated sentences. Gong et al. (2011) extend this by further introducing two additional caches. They employ a static cache to store bilingual phrases extracted from documents in training data that are similar to the document being translated and a topic cache with target language topic words. Recently we have also witnessed efforts that model lexical cohesion (Hardmeier et al., 2012; Wong and Kit, 2012; Xiong et al., 2013a; Xiong et al., 2013b) as well as coherence (Xiong and Zhang, 2013) for document-level SMT. Hasler et al. (2014a) use topic models to learn document-level translation probabilities. Hasler et al. (2014b) use topicadapted model to improve lexical selection. The significant difference between our work and these studies is that term translation has not been investigated in these document-level SMT models. Itagaki and Aikawa (2008) employ bilingual term bank as a dictionary for machine-aided translation. Ren et al. (2009) propose a binary feature to indicat</context>
</contexts>
<marker>Hardmeier, Nivre, Tiedemann, 2012</marker>
<rawString>Christian Hardmeier, Joakim Nivre, and J¨org Tiedemann. 2012. Document-wide decoding for phrasebased statistical machine translation. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1179–1190.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eva Hasler</author>
<author>Phil Blunsom</author>
<author>Philipp Koehn</author>
<author>Barry Haddow</author>
</authors>
<title>Dynamic topic adaptation for phrase-based mt.</title>
<date>2014</date>
<booktitle>In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<location>Gothenburg,</location>
<contexts>
<context position="6728" citStr="Hasler et al. (2014" startWordPosition="1032" endWordPosition="1035">opose cache-based language and translation models, which are built on recently translated sentences. Gong et al. (2011) extend this by further introducing two additional caches. They employ a static cache to store bilingual phrases extracted from documents in training data that are similar to the document being translated and a topic cache with target language topic words. Recently we have also witnessed efforts that model lexical cohesion (Hardmeier et al., 2012; Wong and Kit, 2012; Xiong et al., 2013a; Xiong et al., 2013b) as well as coherence (Xiong and Zhang, 2013) for document-level SMT. Hasler et al. (2014a) use topic models to learn document-level translation probabilities. Hasler et al. (2014b) use topicadapted model to improve lexical selection. The significant difference between our work and these studies is that term translation has not been investigated in these document-level SMT models. Itagaki and Aikawa (2008) employ bilingual term bank as a dictionary for machine-aided translation. Ren et al. (2009) propose a binary feature to indicate whether a bilingual phrase contains a term pair. Pinis and Skadins (2012) investigate that bilingual terms are important for domain adaptation of mach</context>
</contexts>
<marker>Hasler, Blunsom, Koehn, Haddow, 2014</marker>
<rawString>Eva Hasler, Phil Blunsom, Philipp Koehn, and Barry Haddow. 2014a. Dynamic topic adaptation for phrase-based mt. In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, Gothenburg, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eva Hasler</author>
<author>Barry Haddow</author>
<author>Philipp Koehn</author>
</authors>
<title>Dynamic topic adaptation for smt using distributional profiles.</title>
<date>2014</date>
<booktitle>In Proceedings of the Ninth Workshop on Statistical Machine Translation,</booktitle>
<pages>445--456</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, Maryland, USA,</location>
<contexts>
<context position="6728" citStr="Hasler et al. (2014" startWordPosition="1032" endWordPosition="1035">opose cache-based language and translation models, which are built on recently translated sentences. Gong et al. (2011) extend this by further introducing two additional caches. They employ a static cache to store bilingual phrases extracted from documents in training data that are similar to the document being translated and a topic cache with target language topic words. Recently we have also witnessed efforts that model lexical cohesion (Hardmeier et al., 2012; Wong and Kit, 2012; Xiong et al., 2013a; Xiong et al., 2013b) as well as coherence (Xiong and Zhang, 2013) for document-level SMT. Hasler et al. (2014a) use topic models to learn document-level translation probabilities. Hasler et al. (2014b) use topicadapted model to improve lexical selection. The significant difference between our work and these studies is that term translation has not been investigated in these document-level SMT models. Itagaki and Aikawa (2008) employ bilingual term bank as a dictionary for machine-aided translation. Ren et al. (2009) propose a binary feature to indicate whether a bilingual phrase contains a term pair. Pinis and Skadins (2012) investigate that bilingual terms are important for domain adaptation of mach</context>
</contexts>
<marker>Hasler, Haddow, Koehn, 2014</marker>
<rawString>Eva Hasler, Barry Haddow, and Philipp Koehn. 2014b. Dynamic topic adaptation for smt using distributional profiles. In Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 445– 456, Baltimore, Maryland, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hans Hjelm</author>
</authors>
<title>Identifying cross language term equivalents using statistical machine translation and distributional association measures.</title>
<date>2007</date>
<booktitle>In Proceedings of 16th Nordic Conference of Computational Linguistics Nodalida,</booktitle>
<pages>97--104</pages>
<contexts>
<context position="9497" citStr="Hjelm, 2007" startWordPosition="1459" endWordPosition="1460">can model term translation to improve translation quality of SMT. Our interest is to extract multi-word terms. Currently, there are mainly two strategies to conduct bilingual term extraction from parallel corpora. One of them is to extract term candidates separately for each language according to monolingual term metrics, such as C-value/NCvalue (Frantzi et al., 1998; Vu et al., 2008), or other common cooccurrence measures such as Log-Likelihood Ratio, Dice coefficient and Pointwise Mutual Information (Daille, 1996; Piao et al., 2006). The extracted monolingual terms are then paired together (Hjelm, 2007; Fan et al., 2009; Ren et al., 2009). The other strategy is to align words and word sequences that are translation equivalents in parallel corpora and then classify them into terms and non-terms (Merkel and Foo, 2007; Lefever et al., 2009; Bouamor et al., 2012). In this paper, we adopt the first strategy. In particular, for each sentence pair, we collect all source phrases which are terms and find aligned target phrases for them via word alignments. If the target side is also a term, we store the source and target term as a term pair. We conduct monolingual term extraction using the C-value/N</context>
</contexts>
<marker>Hjelm, 2007</marker>
<rawString>Hans Hjelm. 2007. Identifying cross language term equivalents using statistical machine translation and distributional association measures. In Proceedings of 16th Nordic Conference of Computational Linguistics Nodalida, pages 97–104.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masaki Itagaki</author>
<author>Takako Aikawa</author>
</authors>
<title>Post-mt term swapper: Supplementing a statistical machine translation system with a user dictionary.</title>
<date>2008</date>
<booktitle>In LREC.</booktitle>
<contexts>
<context position="7048" citStr="Itagaki and Aikawa (2008)" startWordPosition="1080" endWordPosition="1083">ranslated and a topic cache with target language topic words. Recently we have also witnessed efforts that model lexical cohesion (Hardmeier et al., 2012; Wong and Kit, 2012; Xiong et al., 2013a; Xiong et al., 2013b) as well as coherence (Xiong and Zhang, 2013) for document-level SMT. Hasler et al. (2014a) use topic models to learn document-level translation probabilities. Hasler et al. (2014b) use topicadapted model to improve lexical selection. The significant difference between our work and these studies is that term translation has not been investigated in these document-level SMT models. Itagaki and Aikawa (2008) employ bilingual term bank as a dictionary for machine-aided translation. Ren et al. (2009) propose a binary feature to indicate whether a bilingual phrase contains a term pair. Pinis and Skadins (2012) investigate that bilingual terms are important for domain adaptation of machine translation. These studies do not focus on the three issues of term translation as discussed in Section 1. Furthermore, domain and document-informed information is not used to assist term translation. Itagaki et al. (2007) propose a statistical method to calculate translation consistency for terms with explicit dom</context>
</contexts>
<marker>Itagaki, Aikawa, 2008</marker>
<rawString>Masaki Itagaki and Takako Aikawa. 2008. Post-mt term swapper: Supplementing a statistical machine translation system with a user dictionary. In LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masaki Itagaki</author>
<author>Takako Aikawa</author>
<author>Xiaodong He</author>
</authors>
<title>Automatic validation of terminology translation consistency with statistical method.</title>
<date>2007</date>
<booktitle>Proceedings of MT summit XI,</booktitle>
<pages>269--274</pages>
<contexts>
<context position="7554" citStr="Itagaki et al. (2007)" startWordPosition="1160" endWordPosition="1163">es is that term translation has not been investigated in these document-level SMT models. Itagaki and Aikawa (2008) employ bilingual term bank as a dictionary for machine-aided translation. Ren et al. (2009) propose a binary feature to indicate whether a bilingual phrase contains a term pair. Pinis and Skadins (2012) investigate that bilingual terms are important for domain adaptation of machine translation. These studies do not focus on the three issues of term translation as discussed in Section 1. Furthermore, domain and document-informed information is not used to assist term translation. Itagaki et al. (2007) propose a statistical method to calculate translation consistency for terms with explicit domain information. Partially inspired by their study, we introduce a term translation consistency metric with document-informed information. Furthermore, we integrate the proposed term translation consistency model into an actual SMT system, which has not been done by Itagaki et al. (2007). Ture et al. (2012) use IR-inspired tf-idf scores to encourage consistent translation choice. Guillou (2013) investigates what kind of words should be translated consistently. Term translation consistency has not been</context>
<context position="15073" citStr="Itagaki et al., 2007" startWordPosition="2361" endWordPosition="2364">ain target term translations appropriate for the domain represented by the topic distribution of the corresponding document. 4.2 Term Translation Consistency Model The term translation disambiguation model helps the decoder select appropriate translations for terms that are in accord with their domains. Yet another translation issue related to the domainspecific term translation is to what extent a term should be translated consistently given the domain where it occurs. Term translation consistency indicates the translation stability that a source term is translated into the same target term (Itagaki et al., 2007). When translating a source term, if the translation consistency strength of the source term is high, we should take the corresponding target term as the translation for it. Otherwise, we may need to create a new translation for it according to its context. In particular, we want to enable the decoder to choose between: 1) translating a given source term into the extracted corresponding target term or 2) translating it in another way according to the strength of its translation consistency. In doing so, we can encourage consistent translations for terms with a high translation consistency stre</context>
<context position="17057" citStr="Itagaki et al. (2007)" startWordPosition="2697" endWordPosition="2700">slation consistency strength cons(tf, k) of the source term tf given the topic k. We calculate cons(tf, k) for each G(tf, Set(te)) with counts from training data as follows: (qmn * p(k|m) )2(4) Qk qmn * p(k|m) (5) where M is the number of documents in which the source term tf occurs, Nm is the number of unique corresponding term translations of tf in the mth document, qmn is the frequency of the nth translation of tf in the mth document, p(k|m) is the conditional probability of the mth document over topic k, and Qk is the normalization factor. All translations of tf are from Set(te). We adapt Itagaki et al. (2007)’s translation consistency metric for terms to our topic-based translation consistency measure in the Eq. (4). This equation calculates the translation consistency strength of the source term tf given the topic k according to the distribution of tf’s translations in each document (1) cons(tf, k) = Qk = M Nm m=1 n=1 Nm n=1 M m=1 549 where they occur. According to Eq. (4), the translation consistency strength is a score between 0 and 1. If a source term only occurs in a document and all its translations are the same, the translation consistency strength of this term is 1. We reorganize our bilin</context>
</contexts>
<marker>Itagaki, Aikawa, He, 2007</marker>
<rawString>Masaki Itagaki, Takako Aikawa, and Xiaodong He. 2007. Automatic validation of terminology translation consistency with statistical method. Proceedings of MT summit XI, pages 269–274.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language TechnologyVolume</booktitle>
<volume>1</volume>
<pages>48--54</pages>
<contexts>
<context position="23441" citStr="Koehn et al., 2003" startWordPosition="3753" endWordPosition="3756">t of 4.28M sentence pairs extracted from LDC1 data with document boundaries explicitly provided. The bilingual training data contain 67,752 documents, 124.8M Chinese words and 140.3M English words. We chose NIST MT05 as the MERT (Och, 2003) tuning set, NIST MT06 as the development test set, and NIST MT08 as the final test set. The numbers of documents/sentences in NIST MT05, MT06 and MT08 are 100/1082, 79/1664 and 109/1357 respectively. The word alignments were obtained by running GIZA++ (Och and Ney, 2003) on the corpora in both directions and using the “grow-diag-finaland” balance strategy (Koehn et al., 2003). We adopted SRI Language Modeling Toolkit (Stolcke and others, 2002) to train a 4-gram language model with modified Kneser-Ney smoothing on the Xinhua portion of the English Gigaword corpus. For the topic model, we used the open source 1The corpora include LDC2003E07, LDC2003E14, LDC2004T07, LDC2004E12, LDC2005E83, LDC2005T06, LDC2005T10, LDC2006E24, LDC2006E34, LDC2006E85, LDC2006E92, LDC2007E87, LDC2007E101, LDC2008E40, LDC2008E56, LDC2009E16 and LDC2009E95. LDA tool GibbsLDA++2 with the default setting for training and inference. We performed 100 iterations of the L-BFGS algorithm implemen</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language TechnologyVolume 1, pages 48–54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Els Lefever</author>
<author>Lieve Macken</author>
<author>Veronique Hoste</author>
</authors>
<title>Language-independent bilingual terminology extraction from a multilingual parallel corpus.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>496--504</pages>
<contexts>
<context position="9736" citStr="Lefever et al., 2009" startWordPosition="1500" endWordPosition="1503">o extract term candidates separately for each language according to monolingual term metrics, such as C-value/NCvalue (Frantzi et al., 1998; Vu et al., 2008), or other common cooccurrence measures such as Log-Likelihood Ratio, Dice coefficient and Pointwise Mutual Information (Daille, 1996; Piao et al., 2006). The extracted monolingual terms are then paired together (Hjelm, 2007; Fan et al., 2009; Ren et al., 2009). The other strategy is to align words and word sequences that are translation equivalents in parallel corpora and then classify them into terms and non-terms (Merkel and Foo, 2007; Lefever et al., 2009; Bouamor et al., 2012). In this paper, we adopt the first strategy. In particular, for each sentence pair, we collect all source phrases which are terms and find aligned target phrases for them via word alignments. If the target side is also a term, we store the source and target term as a term pair. We conduct monolingual term extraction using the C-value/NC-value metric and Log-Likelihood Ratio (LLR) measure respectively. We then combine terms extracted according to the two metrics mentioned above. For the C-value/NC-value metric based term extraction, we implement it in the same way as des</context>
</contexts>
<marker>Lefever, Macken, Hoste, 2009</marker>
<rawString>Els Lefever, Lieve Macken, and Veronique Hoste. 2009. Language-independent bilingual terminology extraction from a multilingual parallel corpus. In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics, pages 496–504.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Magnus Merkel</author>
<author>Jody Foo</author>
</authors>
<title>Terminology extraction and term ranking for standardizing term banks.</title>
<date>2007</date>
<booktitle>In Proceedings of 16th Nordic Conference of Computational Linguistics Nodalida,</booktitle>
<pages>349--354</pages>
<contexts>
<context position="9714" citStr="Merkel and Foo, 2007" startWordPosition="1496" endWordPosition="1499">pora. One of them is to extract term candidates separately for each language according to monolingual term metrics, such as C-value/NCvalue (Frantzi et al., 1998; Vu et al., 2008), or other common cooccurrence measures such as Log-Likelihood Ratio, Dice coefficient and Pointwise Mutual Information (Daille, 1996; Piao et al., 2006). The extracted monolingual terms are then paired together (Hjelm, 2007; Fan et al., 2009; Ren et al., 2009). The other strategy is to align words and word sequences that are translation equivalents in parallel corpora and then classify them into terms and non-terms (Merkel and Foo, 2007; Lefever et al., 2009; Bouamor et al., 2012). In this paper, we adopt the first strategy. In particular, for each sentence pair, we collect all source phrases which are terms and find aligned target phrases for them via word alignments. If the target side is also a term, we store the source and target term as a term pair. We conduct monolingual term extraction using the C-value/NC-value metric and Log-Likelihood Ratio (LLR) measure respectively. We then combine terms extracted according to the two metrics mentioned above. For the C-value/NC-value metric based term extraction, we implement it </context>
</contexts>
<marker>Merkel, Foo, 2007</marker>
<rawString>Magnus Merkel and Jody Foo. 2007. Terminology extraction and term ranking for standardizing term banks. In Proceedings of 16th Nordic Conference of Computational Linguistics Nodalida, pages 349– 354.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational linguistics,</journal>
<pages>29--1</pages>
<contexts>
<context position="23334" citStr="Och and Ney, 2003" startWordPosition="3736" endWordPosition="3739">hat extent do the proposed models affect the translations of test sets? 5.1 Setup Our training data consist of 4.28M sentence pairs extracted from LDC1 data with document boundaries explicitly provided. The bilingual training data contain 67,752 documents, 124.8M Chinese words and 140.3M English words. We chose NIST MT05 as the MERT (Och, 2003) tuning set, NIST MT06 as the development test set, and NIST MT08 as the final test set. The numbers of documents/sentences in NIST MT05, MT06 and MT08 are 100/1082, 79/1664 and 109/1357 respectively. The word alignments were obtained by running GIZA++ (Och and Ney, 2003) on the corpora in both directions and using the “grow-diag-finaland” balance strategy (Koehn et al., 2003). We adopted SRI Language Modeling Toolkit (Stolcke and others, 2002) to train a 4-gram language model with modified Kneser-Ney smoothing on the Xinhua portion of the English Gigaword corpus. For the topic model, we used the open source 1The corpora include LDC2003E07, LDC2003E14, LDC2004T07, LDC2004E12, LDC2005E83, LDC2005T06, LDC2005T10, LDC2006E24, LDC2006E34, LDC2006E85, LDC2006E92, LDC2007E87, LDC2007E101, LDC2008E40, LDC2008E56, LDC2009E16 and LDC2009E95. LDA tool GibbsLDA++2 with t</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1,</booktitle>
<pages>160--167</pages>
<contexts>
<context position="14362" citStr="Och, 2003" startWordPosition="2256" endWordPosition="2257">t term translation probability Pd(tei|tfi, D0) given the document D0 is formulated as follows: Pd(tei|tfi, D0) K = p(tei|tfi, z = k) * p(z = k|D0) (3) k=1 Whenever a source term tfi is translated into tei, we check whether the pair of tfi and its translation tei can be found in our bilingual term bank. If it can be found, we calculate the conditional translation probability from tfi to tei given the document D0 according to Eq. (3). The term translation disambiguation model is integrated into the log-linear model of SMT as a feature. Its weight is tuned via minimum error rate training (MERT) (Och, 2003). Through the feature, we can enable the decoder to favor translation hypotheses that contain target term translations appropriate for the domain represented by the topic distribution of the corresponding document. 4.2 Term Translation Consistency Model The term translation disambiguation model helps the decoder select appropriate translations for terms that are in accord with their domains. Yet another translation issue related to the domainspecific term translation is to what extent a term should be translated consistently given the domain where it occurs. Term translation consistency indica</context>
<context position="23062" citStr="Och, 2003" startWordPosition="3692" endWordPosition="3693">ducted experiments to answer the following three questions. 1. Are our term translation disambiguation, consistency and bracketing models able to improve translation quality in BLEU? 2. Does the combination of the three models provide further improvements? 3. To what extent do the proposed models affect the translations of test sets? 5.1 Setup Our training data consist of 4.28M sentence pairs extracted from LDC1 data with document boundaries explicitly provided. The bilingual training data contain 67,752 documents, 124.8M Chinese words and 140.3M English words. We chose NIST MT05 as the MERT (Och, 2003) tuning set, NIST MT06 as the development test set, and NIST MT08 as the final test set. The numbers of documents/sentences in NIST MT05, MT06 and MT08 are 100/1082, 79/1664 and 109/1357 respectively. The word alignments were obtained by running GIZA++ (Och and Ney, 2003) on the corpora in both directions and using the “grow-diag-finaland” balance strategy (Koehn et al., 2003). We adopted SRI Language Modeling Toolkit (Stolcke and others, 2002) to train a 4-gram language model with modified Kneser-Ney smoothing on the Xinhua portion of the English Gigaword corpus. For the topic model, we used </context>
<context position="24931" citStr="Och, 2003" startWordPosition="3987" endWordPosition="3988"> with the Stanford NLP toolkit4. The bilingual term bank was extracted based on the following parameter settings of term extraction methods. Empirically, we set the maximum length of a term to 6 words5. For both the C-value/NC-value and LLRbased extraction methods, we set the context window size to 5 words, which is a widely-used setting in previous work. And we set C-value/NCvalue score threshold to 0 and LLR score threshold to 10 according to the training corpora. We used the case-insensitive 4-gram BLEU6 as our evaluation metric. In order to alleviate the impact of the instability of MERT (Och, 2003), we ran it three times for all our experiments and presented the average BLEU scores on the three runs following the suggestion by Clark et al. (2011). We used an in-house hierarchical phrase-based decoder to verify our proposed models. Although the decoder translates a document in a sentenceby-sentence fashion, it incorporates documentinformed information for sentence translation via the proposed term translation models trained on documents. 2http://sourceforge.net/projects/gibbslda/ 3http://homepages.inf.ed.ac.uk/lzhang10/maxent toolkit.html 4http://nlp.stanford.edu/software/tagger.shtml 5W</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1, pages 160–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott SL Piao</author>
<author>Guangfan Sun</author>
<author>Paul Rayson</author>
<author>Qi Yuan</author>
</authors>
<title>Automatic extraction of chinese multiword expressions with a statistical tool.</title>
<date>2006</date>
<booktitle>In Workshop on Multi-word-expressions in a Multilingual Context held in conjunction with the 11th EACL,</booktitle>
<pages>17--24</pages>
<location>Trento, Italy,</location>
<contexts>
<context position="9426" citStr="Piao et al., 2006" startWordPosition="1447" endWordPosition="1450">n this paper, we want to automatically build a bilingual term bank so that we can model term translation to improve translation quality of SMT. Our interest is to extract multi-word terms. Currently, there are mainly two strategies to conduct bilingual term extraction from parallel corpora. One of them is to extract term candidates separately for each language according to monolingual term metrics, such as C-value/NCvalue (Frantzi et al., 1998; Vu et al., 2008), or other common cooccurrence measures such as Log-Likelihood Ratio, Dice coefficient and Pointwise Mutual Information (Daille, 1996; Piao et al., 2006). The extracted monolingual terms are then paired together (Hjelm, 2007; Fan et al., 2009; Ren et al., 2009). The other strategy is to align words and word sequences that are translation equivalents in parallel corpora and then classify them into terms and non-terms (Merkel and Foo, 2007; Lefever et al., 2009; Bouamor et al., 2012). In this paper, we adopt the first strategy. In particular, for each sentence pair, we collect all source phrases which are terms and find aligned target phrases for them via word alignments. If the target side is also a term, we store the source and target term as </context>
</contexts>
<marker>Piao, Sun, Rayson, Yuan, 2006</marker>
<rawString>Scott SL Piao, Guangfan Sun, Paul Rayson, and Qi Yuan. 2006. Automatic extraction of chinese multiword expressions with a statistical tool. In Workshop on Multi-word-expressions in a Multilingual Context held in conjunction with the 11th EACL, Trento, Italy, pages 17–24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pinis</author>
<author>Skadins</author>
</authors>
<title>Mt adaptation for underresourced domains–what works and what not. In Human Language Technologies–The Baltic Perspective:</title>
<date>2012</date>
<booktitle>Proceedings of the Fifth International Conference Baltic HLT 2012,</booktitle>
<volume>247</volume>
<pages>176</pages>
<publisher>IOS Press.</publisher>
<contexts>
<context position="7251" citStr="Pinis and Skadins (2012)" startWordPosition="1113" endWordPosition="1116"> al., 2013b) as well as coherence (Xiong and Zhang, 2013) for document-level SMT. Hasler et al. (2014a) use topic models to learn document-level translation probabilities. Hasler et al. (2014b) use topicadapted model to improve lexical selection. The significant difference between our work and these studies is that term translation has not been investigated in these document-level SMT models. Itagaki and Aikawa (2008) employ bilingual term bank as a dictionary for machine-aided translation. Ren et al. (2009) propose a binary feature to indicate whether a bilingual phrase contains a term pair. Pinis and Skadins (2012) investigate that bilingual terms are important for domain adaptation of machine translation. These studies do not focus on the three issues of term translation as discussed in Section 1. Furthermore, domain and document-informed information is not used to assist term translation. Itagaki et al. (2007) propose a statistical method to calculate translation consistency for terms with explicit domain information. Partially inspired by their study, we introduce a term translation consistency metric with document-informed information. Furthermore, we integrate the proposed term translation consiste</context>
</contexts>
<marker>Pinis, Skadins, 2012</marker>
<rawString>Pinis and Skadins. 2012. Mt adaptation for underresourced domains–what works and what not. In Human Language Technologies–The Baltic Perspective: Proceedings of the Fifth International Conference Baltic HLT 2012, volume 247, page 176. IOS Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhixiang Ren</author>
<author>Yajuan L¨u</author>
<author>Jie Cao</author>
<author>Qun Liu</author>
<author>Yun Huang</author>
</authors>
<title>Improving statistical machine translation using domain bilingual multiword expressions.</title>
<date>2009</date>
<booktitle>In Proceedings of the Workshop on Multiword Expressions: Identification, Interpretation, Disambiguation and Applications,</booktitle>
<pages>47--54</pages>
<marker>Ren, L¨u, Cao, Liu, Huang, 2009</marker>
<rawString>Zhixiang Ren, Yajuan L¨u, Jie Cao, Qun Liu, and Yun Huang. 2009. Improving statistical machine translation using domain bilingual multiword expressions. In Proceedings of the Workshop on Multiword Expressions: Identification, Interpretation, Disambiguation and Applications, pages 47–54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>Srilm-an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of the international conference on spoken language processing,</booktitle>
<volume>2</volume>
<pages>901--904</pages>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke et al. 2002. Srilm-an extensible language modeling toolkit. In Proceedings of the international conference on spoken language processing, volume 2, pages 901–904.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jinsong Su</author>
<author>Hua Wu</author>
<author>Haifeng Wang</author>
<author>Yidong Chen</author>
<author>Xiaodong Shi</author>
<author>Huailin Dong</author>
<author>Qun Liu</author>
</authors>
<title>Translation model adaptation for statistical machine translation with monolingual topic information.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume</booktitle>
<volume>1</volume>
<pages>459--468</pages>
<contexts>
<context position="5272" citStr="Su et al., 2012" startWordPosition="802" endWordPosition="805">then elaborate the proposed three models for term translation in Section 4. Next, we conduct experiments to validate the effectiveness of the proposed models in Section 5. Finally, we conclude and provide directions for future work in Section 6. 2 Related Work In this section, we briefly introduce related work and highlight the differences between our work and previous studies. As we approach term translation disambiguation and consistency via topic modeling, our models are related to previous work that explores the topic model (Blei et al., 2003) for machine translation (Zhao and Xing, 2006; Su et al., 2012; Xiao et al., 2012; Eidelman et al., 2012). Zhao and Xing (2006) employ three models that enable word alignment process to leverage topical contents of document-pairs with topic model. Su et al. (2012) establish the relationship between out-ofdomain bilingual corpus and in-domain monolingual corpora via topic mapping and phrase-topic distribution probability estimation for translation model adaptation. Xiao et al. (2012) propose a topic similarity model for rule selection. Eidelman et al. (2012) use topic models to adapt lexical weighting probabilities dynamically during translation. In these</context>
</contexts>
<marker>Su, Wu, Wang, Chen, Shi, Dong, Liu, 2012</marker>
<rawString>Jinsong Su, Hua Wu, Haifeng Wang, Yidong Chen, Xiaodong Shi, Huailin Dong, and Qun Liu. 2012. Translation model adaptation for statistical machine translation with monolingual topic information. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1, pages 459–468.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J¨org Tiedemann</author>
</authors>
<title>Context adaptation in statistical machine translation using models with exponentially decaying cache.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Workshop on Domain Adaptation for Natural Language Processing,</booktitle>
<pages>8--15</pages>
<contexts>
<context position="6106" citStr="Tiedemann (2010)" startWordPosition="932" endWordPosition="933">elationship between out-ofdomain bilingual corpus and in-domain monolingual corpora via topic mapping and phrase-topic distribution probability estimation for translation model adaptation. Xiao et al. (2012) propose a topic similarity model for rule selection. Eidelman et al. (2012) use topic models to adapt lexical weighting probabilities dynamically during translation. In these studies, the topic model is not used to address the issues of term translation mentioned in Section 1. Our work is also related to document-level SMT in that we use document-informed information for term translation. Tiedemann (2010) propose cache-based language and translation models, which are built on recently translated sentences. Gong et al. (2011) extend this by further introducing two additional caches. They employ a static cache to store bilingual phrases extracted from documents in training data that are similar to the document being translated and a topic cache with target language topic words. Recently we have also witnessed efforts that model lexical cohesion (Hardmeier et al., 2012; Wong and Kit, 2012; Xiong et al., 2013a; Xiong et al., 2013b) as well as coherence (Xiong and Zhang, 2013) for document-level SM</context>
</contexts>
<marker>Tiedemann, 2010</marker>
<rawString>J¨org Tiedemann. 2010. Context adaptation in statistical machine translation using models with exponentially decaying cache. In Proceedings of the 2010 Workshop on Domain Adaptation for Natural Language Processing, pages 8–15.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ferhan Ture</author>
<author>Douglas W Oard</author>
<author>Philip Resnik</author>
</authors>
<title>Encouraging consistent translation choices.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>417--426</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="7956" citStr="Ture et al. (2012)" startWordPosition="1218" endWordPosition="1221">ation. These studies do not focus on the three issues of term translation as discussed in Section 1. Furthermore, domain and document-informed information is not used to assist term translation. Itagaki et al. (2007) propose a statistical method to calculate translation consistency for terms with explicit domain information. Partially inspired by their study, we introduce a term translation consistency metric with document-informed information. Furthermore, we integrate the proposed term translation consistency model into an actual SMT system, which has not been done by Itagaki et al. (2007). Ture et al. (2012) use IR-inspired tf-idf scores to encourage consistent translation choice. Guillou (2013) investigates what kind of words should be translated consistently. Term translation consistency has not been investigated in these studies. Our term bracketing model is also related to Xiong et al. (2009)’s syntax-driven bracketing model for phrase-based translation, which predicts whether a phrase is bracketable or not using rich syntactic constraints. The difference is that we construct the model with automatically created bilingual term bank and do not depend on any syntactic knowledge. 3 Bilingual Ter</context>
</contexts>
<marker>Ture, Oard, Resnik, 2012</marker>
<rawString>Ferhan Ture, Douglas W Oard, and Philip Resnik. 2012. Encouraging consistent translation choices. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 417–426. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Muriel Vasconcellos</author>
<author>Brian Avey</author>
<author>Claudia Gdaniec</author>
<author>Laurie Gerber</author>
<author>Marjorie Le´on</author>
<author>Teruko Mitamura</author>
</authors>
<date>2001</date>
<booktitle>Terminology and machine translation. Handbook of Terminology Management,</booktitle>
<pages>2--697</pages>
<marker>Vasconcellos, Avey, Gdaniec, Gerber, Le´on, Mitamura, 2001</marker>
<rawString>Muriel Vasconcellos, Brian Avey, Claudia Gdaniec, Laurie Gerber, Marjorie Le´on, and Teruko Mitamura. 2001. Terminology and machine translation. Handbook of Terminology Management, 2:697–723.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thuy Vu</author>
<author>Ai Ti Aw</author>
<author>Min Zhang</author>
</authors>
<title>Term extraction through unithood and termhood unification.</title>
<date>2008</date>
<booktitle>In Proceedings of the third international joint conference on natural language processing.</booktitle>
<contexts>
<context position="9273" citStr="Vu et al., 2008" startWordPosition="1425" endWordPosition="1428">ating or ex547 tending a bilingual term bank, which in turn can be used to improve other tasks such as information retrieval and machine translation. In this paper, we want to automatically build a bilingual term bank so that we can model term translation to improve translation quality of SMT. Our interest is to extract multi-word terms. Currently, there are mainly two strategies to conduct bilingual term extraction from parallel corpora. One of them is to extract term candidates separately for each language according to monolingual term metrics, such as C-value/NCvalue (Frantzi et al., 1998; Vu et al., 2008), or other common cooccurrence measures such as Log-Likelihood Ratio, Dice coefficient and Pointwise Mutual Information (Daille, 1996; Piao et al., 2006). The extracted monolingual terms are then paired together (Hjelm, 2007; Fan et al., 2009; Ren et al., 2009). The other strategy is to align words and word sequences that are translation equivalents in parallel corpora and then classify them into terms and non-terms (Merkel and Foo, 2007; Lefever et al., 2009; Bouamor et al., 2012). In this paper, we adopt the first strategy. In particular, for each sentence pair, we collect all source phrases</context>
</contexts>
<marker>Vu, Aw, Zhang, 2008</marker>
<rawString>Thuy Vu, Ai Ti Aw, and Min Zhang. 2008. Term extraction through unithood and termhood unification. In Proceedings of the third international joint conference on natural language processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Billy Wong</author>
<author>Chunyu Kit</author>
</authors>
<title>Extending machine translation evaluation metrics with lexical cohesion to document level.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>1060--1068</pages>
<contexts>
<context position="6596" citStr="Wong and Kit, 2012" startWordPosition="1009" endWordPosition="1012">r work is also related to document-level SMT in that we use document-informed information for term translation. Tiedemann (2010) propose cache-based language and translation models, which are built on recently translated sentences. Gong et al. (2011) extend this by further introducing two additional caches. They employ a static cache to store bilingual phrases extracted from documents in training data that are similar to the document being translated and a topic cache with target language topic words. Recently we have also witnessed efforts that model lexical cohesion (Hardmeier et al., 2012; Wong and Kit, 2012; Xiong et al., 2013a; Xiong et al., 2013b) as well as coherence (Xiong and Zhang, 2013) for document-level SMT. Hasler et al. (2014a) use topic models to learn document-level translation probabilities. Hasler et al. (2014b) use topicadapted model to improve lexical selection. The significant difference between our work and these studies is that term translation has not been investigated in these document-level SMT models. Itagaki and Aikawa (2008) employ bilingual term bank as a dictionary for machine-aided translation. Ren et al. (2009) propose a binary feature to indicate whether a bilingua</context>
</contexts>
<marker>Wong, Kit, 2012</marker>
<rawString>Billy Wong and Chunyu Kit. 2012. Extending machine translation evaluation metrics with lexical cohesion to document level. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1060–1068.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xinyan Xiao</author>
<author>Deyi Xiong</author>
<author>Min Zhang</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>A topic similarity model for hierarchical phrase-based translation.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume</booktitle>
<volume>1</volume>
<pages>750--758</pages>
<contexts>
<context position="5291" citStr="Xiao et al., 2012" startWordPosition="806" endWordPosition="809">e proposed three models for term translation in Section 4. Next, we conduct experiments to validate the effectiveness of the proposed models in Section 5. Finally, we conclude and provide directions for future work in Section 6. 2 Related Work In this section, we briefly introduce related work and highlight the differences between our work and previous studies. As we approach term translation disambiguation and consistency via topic modeling, our models are related to previous work that explores the topic model (Blei et al., 2003) for machine translation (Zhao and Xing, 2006; Su et al., 2012; Xiao et al., 2012; Eidelman et al., 2012). Zhao and Xing (2006) employ three models that enable word alignment process to leverage topical contents of document-pairs with topic model. Su et al. (2012) establish the relationship between out-ofdomain bilingual corpus and in-domain monolingual corpora via topic mapping and phrase-topic distribution probability estimation for translation model adaptation. Xiao et al. (2012) propose a topic similarity model for rule selection. Eidelman et al. (2012) use topic models to adapt lexical weighting probabilities dynamically during translation. In these studies, the topic</context>
<context position="11915" citStr="Xiao et al. (2012)" startWordPosition="1836" endWordPosition="1839">ion disambiguation model, term translation consistency model and term bracketing model respectively. 4.1 Term Translation Disambiguation Model The most straightforward way to disambiguate term translations in different domains is to calculate the conditional translation probability of a term given domain information. We use the topic distribution of a document obtained by a topic model to represent the domain information of the document. Since Latent Dirichlet Allocation (LDA) (Blei et al., 2003) is the most widelyused topic model, we exploit it for inferring topic distributions of documents. Xiao et al. (2012) proposed a topic similarity model for rule selection. Different from their work, we take an easier strategy that estimates topic-conditioned term translation probabilities rather than rule-topic distributions. This makes our model easily scalable on large training data. With the bilingual term bank created from the training data, we calculate the source-to-target term translation probability for each term pair conditioned on the topic distribution of the source document where the source term occurs. We maintain a K-dimension (K is the number of topics) vector for each term pair. The k-th comp</context>
</contexts>
<marker>Xiao, Xiong, Zhang, Liu, Lin, 2012</marker>
<rawString>Xinyan Xiao, Deyi Xiong, Min Zhang, Qun Liu, and Shouxun Lin. 2012. A topic similarity model for hierarchical phrase-based translation. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1, pages 750–758.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deyi Xiong</author>
<author>Min Zhang</author>
</authors>
<title>A topic-based coherence model for statistical machine translation.</title>
<date>2013</date>
<booktitle>In Proceedings of the Twenty-Seventh AAAI Conference on Artificial Intelligence (AAAI-13),</booktitle>
<location>Bellevue, Washington, USA,</location>
<contexts>
<context position="6684" citStr="Xiong and Zhang, 2013" startWordPosition="1025" endWordPosition="1028">ation for term translation. Tiedemann (2010) propose cache-based language and translation models, which are built on recently translated sentences. Gong et al. (2011) extend this by further introducing two additional caches. They employ a static cache to store bilingual phrases extracted from documents in training data that are similar to the document being translated and a topic cache with target language topic words. Recently we have also witnessed efforts that model lexical cohesion (Hardmeier et al., 2012; Wong and Kit, 2012; Xiong et al., 2013a; Xiong et al., 2013b) as well as coherence (Xiong and Zhang, 2013) for document-level SMT. Hasler et al. (2014a) use topic models to learn document-level translation probabilities. Hasler et al. (2014b) use topicadapted model to improve lexical selection. The significant difference between our work and these studies is that term translation has not been investigated in these document-level SMT models. Itagaki and Aikawa (2008) employ bilingual term bank as a dictionary for machine-aided translation. Ren et al. (2009) propose a binary feature to indicate whether a bilingual phrase contains a term pair. Pinis and Skadins (2012) investigate that bilingual terms</context>
</contexts>
<marker>Xiong, Zhang, 2013</marker>
<rawString>Deyi Xiong and Min Zhang. 2013. A topic-based coherence model for statistical machine translation. In Proceedings of the Twenty-Seventh AAAI Conference on Artificial Intelligence (AAAI-13), Bellevue, Washington, USA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deyi Xiong</author>
<author>Min Zhang</author>
<author>Aiti Aw</author>
<author>Haizhou Li</author>
</authors>
<title>A syntax-driven bracketing model for phrasebased translation.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>315--323</pages>
<contexts>
<context position="8250" citStr="Xiong et al. (2009)" startWordPosition="1260" endWordPosition="1263">with explicit domain information. Partially inspired by their study, we introduce a term translation consistency metric with document-informed information. Furthermore, we integrate the proposed term translation consistency model into an actual SMT system, which has not been done by Itagaki et al. (2007). Ture et al. (2012) use IR-inspired tf-idf scores to encourage consistent translation choice. Guillou (2013) investigates what kind of words should be translated consistently. Term translation consistency has not been investigated in these studies. Our term bracketing model is also related to Xiong et al. (2009)’s syntax-driven bracketing model for phrase-based translation, which predicts whether a phrase is bracketable or not using rich syntactic constraints. The difference is that we construct the model with automatically created bilingual term bank and do not depend on any syntactic knowledge. 3 Bilingual Term Extraction Bilingual term extraction is to extract terms from two languages with the purpose of creating or ex547 tending a bilingual term bank, which in turn can be used to improve other tasks such as information retrieval and machine translation. In this paper, we want to automatically bui</context>
<context position="19628" citStr="Xiong et al. (2009)" startWordPosition="3134" endWordPosition="3137"> the test data. The term translation consistency model is also integrated into the log-linear model of SMT as a feature. Through the feature, we can enable the decoder to translate terms with a high translation consistency in a document into corresponding target terms from our bilingual term bank rather than other translations in a consistent fashion. 4.3 Term Bracketing Model The term translation disambiguation model and consistency model concern the term translation accuracy with domain information. We further propose a term bracketing model to guarantee the integrality of term translation. Xiong et al. (2009) proposed a syntax-driven bracketing model for phrase-based translation, which predicts whether a phrase is bracketable or not using rich syntactic constraints. If a source phrase remains contiguous after translation, they refer to this type of phrase as bracketable phrase, otherwise unbracketable phrase. For multi-word terms, it is also desirable to be bracketable since a source term should be translated as a whole unit and its translation should be contiguous. In this paper, we adapt Xiong et al. (2009)’s bracketing approach to term translation and build a classifier to measure the probabili</context>
</contexts>
<marker>Xiong, Zhang, Aw, Li, 2009</marker>
<rawString>Deyi Xiong, Min Zhang, Aiti Aw, and Haizhou Li. 2009. A syntax-driven bracketing model for phrasebased translation. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 315– 323.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deyi Xiong</author>
<author>Guosheng Ben</author>
<author>Min Zhang</author>
<author>Yajuan L¨u</author>
<author>Qun Liu</author>
</authors>
<title>Modeling lexical cohesion for document-level machine translation.</title>
<date>2013</date>
<booktitle>In Proceedings of the Twenty-Third international joint conference on Artificial Intelligence,</booktitle>
<pages>2183--2189</pages>
<publisher>AAAI Press.</publisher>
<marker>Xiong, Ben, Zhang, L¨u, Liu, 2013</marker>
<rawString>Deyi Xiong, Guosheng Ben, Min Zhang, Yajuan L¨u, and Qun Liu. 2013a. Modeling lexical cohesion for document-level machine translation. In Proceedings of the Twenty-Third international joint conference on Artificial Intelligence, pages 2183–2189. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deyi Xiong</author>
<author>Yang Ding</author>
<author>Min Zhang</author>
<author>Chew Lim Tan</author>
</authors>
<title>Lexical chain based cohesion models for document-level statistical machine translation.</title>
<date>2013</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1563--1573</pages>
<contexts>
<context position="6616" citStr="Xiong et al., 2013" startWordPosition="1013" endWordPosition="1016">ed to document-level SMT in that we use document-informed information for term translation. Tiedemann (2010) propose cache-based language and translation models, which are built on recently translated sentences. Gong et al. (2011) extend this by further introducing two additional caches. They employ a static cache to store bilingual phrases extracted from documents in training data that are similar to the document being translated and a topic cache with target language topic words. Recently we have also witnessed efforts that model lexical cohesion (Hardmeier et al., 2012; Wong and Kit, 2012; Xiong et al., 2013a; Xiong et al., 2013b) as well as coherence (Xiong and Zhang, 2013) for document-level SMT. Hasler et al. (2014a) use topic models to learn document-level translation probabilities. Hasler et al. (2014b) use topicadapted model to improve lexical selection. The significant difference between our work and these studies is that term translation has not been investigated in these document-level SMT models. Itagaki and Aikawa (2008) employ bilingual term bank as a dictionary for machine-aided translation. Ren et al. (2009) propose a binary feature to indicate whether a bilingual phrase contains a </context>
</contexts>
<marker>Xiong, Ding, Zhang, Tan, 2013</marker>
<rawString>Deyi Xiong, Yang Ding, Min Zhang, and Chew Lim Tan. 2013b. Lexical chain based cohesion models for document-level statistical machine translation. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1563––1573.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing Zhao</author>
<author>Eric P Xing</author>
</authors>
<title>Bitam: Bilingual topic admixture models for word alignment.</title>
<date>2006</date>
<booktitle>In Proceedings of the COLING/ACL on Main conference poster sessions,</booktitle>
<pages>969--976</pages>
<contexts>
<context position="5255" citStr="Zhao and Xing, 2006" startWordPosition="798" endWordPosition="801">ion in Section 3. We then elaborate the proposed three models for term translation in Section 4. Next, we conduct experiments to validate the effectiveness of the proposed models in Section 5. Finally, we conclude and provide directions for future work in Section 6. 2 Related Work In this section, we briefly introduce related work and highlight the differences between our work and previous studies. As we approach term translation disambiguation and consistency via topic modeling, our models are related to previous work that explores the topic model (Blei et al., 2003) for machine translation (Zhao and Xing, 2006; Su et al., 2012; Xiao et al., 2012; Eidelman et al., 2012). Zhao and Xing (2006) employ three models that enable word alignment process to leverage topical contents of document-pairs with topic model. Su et al. (2012) establish the relationship between out-ofdomain bilingual corpus and in-domain monolingual corpora via topic mapping and phrase-topic distribution probability estimation for translation model adaptation. Xiao et al. (2012) propose a topic similarity model for rule selection. Eidelman et al. (2012) use topic models to adapt lexical weighting probabilities dynamically during tran</context>
</contexts>
<marker>Zhao, Xing, 2006</marker>
<rawString>Bing Zhao and Eric P Xing. 2006. Bitam: Bilingual topic admixture models for word alignment. In Proceedings of the COLING/ACL on Main conference poster sessions, pages 969–976.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
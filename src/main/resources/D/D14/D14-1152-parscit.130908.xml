<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001525">
<title confidence="0.9980145">
Financial Keyword Expansion via
Continuous Word Vector Representations
</title>
<author confidence="0.975762">
Chuan-Ju Wang
</author>
<affiliation confidence="0.9967105">
Department of Computer Science
University of Taipei
</affiliation>
<address confidence="0.477065">
Taipei 100, Taiwan
</address>
<email confidence="0.976427">
cjwang@utaipei.edu.tw
</email>
<author confidence="0.985709">
Ming-Feng Tsai
</author>
<affiliation confidence="0.968952666666667">
Department of Computer Science &amp;
Program in Digital Content and Technology
National Chengchi University
</affiliation>
<address confidence="0.647765">
Taipei 116, Taiwan
</address>
<email confidence="0.998171">
mftsai@nccu.edu.tw
</email>
<sectionHeader confidence="0.99738" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999854272727273">
This paper proposes to apply the contin-
uous vector representations of words for
discovering keywords from a financial sen-
timent lexicon. In order to capture more
keywords, we also incorporate syntactic
information into the Continuous Bag-of-
Words (CBOW) model. Experimental re-
sults on a task of financial risk prediction
using the discovered keywords demonstrate
that the proposed approach is good at pre-
dicting financial risk.
</bodyText>
<sectionHeader confidence="0.999394" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999902375">
In the present environment with a great deal of
information, how to discover useful insights for
decision making is becoming increasingly impor-
tant. In finance, there are typically two kinds of
information (Petersen, 2004): soft information usu-
ally refers to text, including opinions, ideas, and
market commentary, whereas hard information is
recorded as numbers, such as financial measures
and historical prices. Most financial studies related
to risk analysis are based on hard information, es-
pecially on time series modeling (Christoffersen
and Diebold, 2000; Lee and Tong, 2011; Wu et al.,
2014; Y¨uml¨u et al., 2005). Despite of using only
hard information, some literature incorporates soft
textual information to predict financial risk (Ko-
gan et al., 2009; Leidner and Schilder, 2010; Tsai
and Wang, 2013). Moreover, sentiment analysis, a
technique to make an assessment of the sentiments
expressed in various information, has also been
applied to analyze the soft textual information in
financial news, reports, and social media data (De-
vitt and Ahmad, 2007; Loughran and McDonald,
2011; Wang et al., 2013).
Continuous vector space models (Bengio et
al., 2003; Schwenk, 2007; Mikolov et al., 2010)
are neural network language models, in which
words are represented as high dimensional real val-
ued vectors. These representations have recently
demonstrated promising results across variety of
tasks (Schwenk, 2007; Collobert and Weston, 2008;
Glorot et al., 2011; Socher et al., 2011; Weston et
al., 2011), because of their superiority of capturing
syntactic and semantic regularities in language. In
this paper, we apply the Continuous Bag-of-Words
(CBOW) model (Mikolov et al., 2013) on the soft
textual information in financial reports for discov-
ering keywords via financial sentiments. In spe-
cific, we use the continuous vector representations
of words to find out similar terms based on their
contexts. Additionally, we propose a straightfor-
ward approach to incorporate syntactic information
into the CBOW model for better locating similarly
meaningful or highly correlated words. To the best
of our knowledge, this is the first work to incorpo-
rate more syntactic information by adding Part-Of-
Speech (POS) tags to the words before training the
CBOW model.
In our experiments, the corpora are the annual
SEC1-mandated financial reports, and there are
3,911 financial sentiment keywords for expansion.
In order to verify the effectiveness of the expanded
keywords, we then conduct two prediction tasks,
including regression and ranking. Observed from
our experimental results, for the regression and
ranking tasks, the models trained on the expanded
keywords are consistently better than those trained
the original sentiment keywords only. In addition,
for comparison, we conduct experiments with ran-
dom keyword expansion as baselines. According
to the experimental results, the expansion either
with or without syntactic information outperforms
the baselines. The results suggest that the CBOW
model is effective at expanding keywords for finan-
cial risk prediction.
</bodyText>
<footnote confidence="0.5260735">
1Securities and Exchange Commission
1453
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1453–1458,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</footnote>
<sectionHeader confidence="0.915931" genericHeader="method">
2 Keyword Expansion via Financial
</sectionHeader>
<subsectionHeader confidence="0.8043345">
Sentiment Lexicon
2.1 Financial Sentiment Lexicon
</subsectionHeader>
<bodyText confidence="0.999987583333333">
A sentiment lexicon is the most important resource
for sentiment analysis. Loughran and McDon-
ald (2011) states that a general purpose sentiment
lexicon (e.g., the Harvard Psychosociological Dic-
tionary) might misclassify common words in fi-
nancial texts. Therefore, in this paper, we use a
finance-specific lexicon that consists of the 6 word
lists provided by (Loughran and McDonald, 2011)
as seeds to expand keywords. The six lists are nega-
tive (Fin-Neg), positive (Fin-Pos), uncertainty (Fin-
Unc), litigious (Fin-Lit), strong modal words (MW-
Strong), and weak modal words (MW-Weak).2
</bodyText>
<subsectionHeader confidence="0.998876">
2.2 Simple Keyword Expansion
</subsectionHeader>
<bodyText confidence="0.999954222222222">
With the financial sentiment lexicon, we first use a
collection of financial reports as the training texts
to learn continuous vector representations of words.
Then, each word in the sentiment lexicon is used as
a seed to obtain the words with the highest n cosine
distances (called the top-n words for the word) via
the learned word vector representations. Finally,
we construct an expanded keyword list from the
top-n words for each word.
</bodyText>
<subsectionHeader confidence="0.8645275">
2.3 Keyword Expansion with Syntactic
Information
</subsectionHeader>
<bodyText confidence="0.999985833333333">
For the expansion considering syntactic informa-
tion, we attach the POS tag to each word in the
training texts first. Then, the words in the senti-
ment lexicon with 4 major POS tags (i.e., JJ, NN,
VB, RB) are used as seeds to expand. The rest of
steps is similar to that in Section 2.2.
The reason of considering POS tags for expan-
sion is that, in general, a word with different POS
tags may result in different lists of top-n words. Ta-
ble 1 shows the top-5 words for the word “default”
with different POS tags (noun and adjective). Note
that none of the words in the two lists overlaps.
</bodyText>
<sectionHeader confidence="0.996173" genericHeader="method">
3 Financial Risk Prediction
</sectionHeader>
<subsectionHeader confidence="0.996232">
3.1 The Risk Measure: Volatility
</subsectionHeader>
<bodyText confidence="0.999977">
Volatility is a measure for variation of prices of a
stock over a period of time. Let St be the price
of a stock at time t. Holding the stock from time
t − 1 to time t would lead to a simple return: Rt =
</bodyText>
<footnote confidence="0.722757">
2http://www.nd.edu/˜mcdonald/Word_
Lists.html.
</footnote>
<table confidence="0.997804125">
default (NN) default (JJ)
Cosine Cosine
Word Distance Word Distance
default (v.) 0.63665 nonconform (v.) 0.63462
unwaiv (v.) 0.63466 subprim (v.) 0.62404
uncur (v.) 0.62285 chattel (n.) 0.61510
trigger (n.) 0.60080 foreclos (adj.) 0.61397
unmatur (v.) 0.58208 unguarante (v.) 0.60559
</table>
<tableCaption confidence="0.999566">
Table 1: Top-5 Words for the word “default.”
</tableCaption>
<bodyText confidence="0.823563">
St/St−1 − 1(Tsay, 2005). The volatility of returns
for a stock from time t − n to t can thus be defined
as follows:
Et
i=t−n (Ri − R)2
n
where R¯ = Eti=t−n Ri/(n + 1).
</bodyText>
<subsectionHeader confidence="0.996704">
3.2 Regression Task
</subsectionHeader>
<bodyText confidence="0.999944333333333">
Given a collection of financial reports D =
{d1, d2, ... , dnJ, in which each di E Rp and is
associated with a company ci, we aim to predict the
future risk of each company ci (which is character-
ized by its volatility vi). This prediction problem
can be defined as follows:
</bodyText>
<equation confidence="0.957248">
ˆvi = f(di; w). (2)
</equation>
<bodyText confidence="0.999851833333333">
The goal is to learn a p-dimensional vector w from
the training data T = {(di, vi)Edi E Rp, vi E RJ.
In this paper, we adopt the Support Vector Regres-
sion (SVR) (Drucker et al., 1997) for training such
a regression model. More details about SVR can
be found in (Sch¨olkopf and Smola, 2001).
</bodyText>
<subsectionHeader confidence="0.999815">
3.3 Ranking Task
</subsectionHeader>
<bodyText confidence="0.9997638">
Instead of predicting the volatility of each company
in the regression task, the ranking task aims to rank
companies according to their risk via the textual
information in their financial reports. We first split
the volatilities of company stock returns within a
year into different risk levels by the mechanism
provided in (Tsai and Wang, 2013). The risk levels
can be considered as the relative difference of risk
among the companies.
After obtaining the relative risk levels of the
companies, the ranking task can be defined as fol-
lows: Given a collection of financial reports D,
we aim to rank the companies via a ranking model
f : Rp —* R such that the rank order of the set of
companies is specified by the real value that the
</bodyText>
<equation confidence="0.925983">
=
v[t−n,t]
, (1)
</equation>
<page confidence="0.940418">
1454
</page>
<bodyText confidence="0.999795">
model f takes. Specifically, f(di) &gt; f(dj) means
that the model asserts that ci &gt;- cj, where ci &gt;- cj
means that ci is ranked higher than cj; that is, the
company ci is more risky than cj. For this task, this
paper adopts Ranking SVM (Joachims, 2006).
</bodyText>
<sectionHeader confidence="0.999883" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.998011">
4.1 Dataset and Preprocessings
</subsectionHeader>
<bodyText confidence="0.9997855625">
In the experiments, we use the 10-K corpus (Ko-
gan et al., 2009) to conduct our financial risk pre-
diction tasks. All documents and the 6 financial
sentiment word lists are stemmed by the Porter
stemmer (Porter, 1980), and some stop words are
also removed.
For financial risk prediction, the ground truth,
the twelve months after the report volatility for
each company, v+(12), (which measures the future
risk for each company) can be calculated by Equa-
tion (1), where the stock prices can be obtained
from the Center for Research in Security Prices
(CRSP) US Stocks Database. In addition, to ob-
tain the relative risks among companies used in the
ranking task, we follow (Tsai and Wang, 2013) to
split the companies of each year into 5 risk levels.
</bodyText>
<subsectionHeader confidence="0.962437">
4.2 Keyword Expansion
</subsectionHeader>
<bodyText confidence="0.999988523809524">
In our experiments, Section 7 (Management Dis-
cussion and Analysis) in 10-K corpus is used as
training texts for the tool (word2vec3) to learn the
continuous vector representations of words.
For the simple expansion (denoted as EXP-SIM
hereafter), we use the total 1,667 stemmed senti-
ment words as seeds to obtain the expanded key-
words via the learned word vector representations.
For the expansion considering syntactic informa-
tion (denoted as EXP-SYN), NLTK4 is applied to
attach the POS tag5 to each word in the training
texts; we attach the POS tag to a word with an un-
derscore notation (e.g., default VB). For simplicity,
we combine some POS tags to one tag via the tag
replacement; for example, the tags JJR (adjective,
comparative) and JJS (adjective, superlative) are
replaced to JJ (adjective). The detailed replace-
ment rules are tabulated in Table 2. Words from
the sentiment lexicon with the four types of POS
tags (i.e., JJ, NN, VB, RB) are consider as the seeds
to expand the keywords. For both EXP-SIM and
</bodyText>
<footnote confidence="0.99030425">
3https://code.google.com/p/word2vec/
4http://www.nltk.org/
5The most common POS tag scheme, the Penn Treebank
POS Tags, is adopt in the paper.
</footnote>
<table confidence="0.997875285714286">
After Replacement Before Replacement
JJ JJ, JJR, JJS
NN NN, NNS, NNP, NNPS
PRP PRP,PRP$
RB RB, RBR, RBS
VB VB, VBD, VBG, VBN, VBP, VBZ
WP WP, WP$
</table>
<tableCaption confidence="0.898361">
Table 2: Tag Replacement Rules.
</tableCaption>
<table confidence="0.999974545454545">
Word Cosine Distance Word Cosine Distance
uncur 0.569498 event 0.466834
indentur 0.565450 lender 0.459995
waiv 0.563656 forbear 0.456556
trigger 0.559936 represent 0.450631
cure 0.539999 breach 0.446851
nonpay 0.538445 noncompli 0.431490
unmatur 0.525251 gecc 0.430712
unwaiv 0.510359 customari 0.424447
insolv 0.488534 waiver 0.419338
occurr 0.471123 prepay 0.418969
</table>
<tableCaption confidence="0.8717655">
Table 3: Top-20 (Stemmed) Words for the Word
“default.”
</tableCaption>
<bodyText confidence="0.9999808">
EXP-SYN, we use the top-20 expanded words for
each word (e.g., Table 3) to construct expanded key-
word lists. In total, for EXP-SIM, the expanded
list contains 9,282 unique words and for EXP-SYN,
the list has 13,534 unique words.
</bodyText>
<subsectionHeader confidence="0.999044">
4.3 Word Features
</subsectionHeader>
<bodyText confidence="0.9999558">
In the experiments, the bag-of-words model is
adopted and three word features are used to repre-
sent the 10-K reports in the experiments. Given a
document d, three word features (i.e., TF, TFIDF
and LOG1P) are calculated as follows:
</bodyText>
<listItem confidence="0.998817">
• TF(t, d) = TC(t, d)/|d|,
• TFIDF(t, d) = TF(t, d) × IDF(t, d) =
TC(t, d)/|d |× log(|D|/|d ∈ D : t ∈ d|),
• LOG1P = log(1 + TC(t, d)),
</listItem>
<bodyText confidence="0.994692333333333">
where TC(t, d) denotes the term count of t in d,
|d |is the length of document d, and D denotes the
set of all documents in each year.
</bodyText>
<subsectionHeader confidence="0.991436">
4.4 Experimental Results
</subsectionHeader>
<bodyText confidence="0.999938428571429">
Tables 4 and 5 tabulate the experimental results of
regression and ranking, respectively, in which the
training data is composed of the financial reports
in a five-year period, and the following year is the
test data. For example, the reports from year 1996
to 2000 constitute a training data, and the learned
model is tested on the reports of year 2001.
</bodyText>
<page confidence="0.985698">
1455
</page>
<table confidence="0.924362636363636">
[TFIDF] (Baseline)
Year SEN EXP-RAN EXP-SIM EXP-SYN
(Baseline)
SEN EXP-RAN EXP-SIM EXP-SYN
Kendall’s Tau (Kendall, 1938). Spearman’s Rho (Myers et al., 2003)
2001 0.4384 0.4574 0.4952 0.5049 0.4701 0.4889 0.5266 0.5375
2002 0.4421 0.4706 0.4881 0.4944 0.4719 0.5007 0.5187 0.5256
2003 0.4414 0.4706 0.5105 0.5006 0.4716 0.5015 0.5418 0.5318
2004 0.4051 0.4551 0.4750 0.4961 0.4335 0.4842 0.5043 0.5255
2005 0.3856 0.4482 0.5126 0.5294 0.4117 0.4757 0.5418 0.5579
2006 0.3784 0.4385 0.4588 0.4867 0.4029 0.4641 0.4847 0.5129
</table>
<tableCaption confidence="0.973598">
Table 5: Performance of Ranking.
</tableCaption>
<table confidence="0.998524222222222">
[LOGP] (Baseline) ing the regression and ranking models is even less
Year SEN EXP-RAN EXP-SIM EXP-SYN than that of EXP-RAN. The results suggest that the
Mean Squared Error
2001 0.2526 0.2360 0.2195 0.2148
2002 0.2858 0.2649 0.2433 0.2381
2003 0.2667 0.2512 0.2320 0.2350
2004 0.2345 0.2140 0.1902 0.1872
2005 0.2241 0.2014 0.1754 0.1682
2006 0.2256 0.2072 0.1889 0.1825
</table>
<tableCaption confidence="0.999695">
Table 4: Performance of Regression
</tableCaption>
<bodyText confidence="0.99993684">
In the tables, SEN denotes the experiments
trained on the words from the original financial sen-
timent lexicon. Despite of the experiments trained
on EXP-SIM and EXP-SYN, we also conduct ex-
periments with random keyword expansion (called
EXP-RAN); for the comparison purpose, we keep
the number of words in the randomly expanded
word list the same as that in EXP-SYN. Note that
the randomly expanded list contains all sentiment
words and the rest of words are randomly chosen
from the vocabulary of the dataset. The columns
with label EXP-RAN denote the results averaged
from 20 randomly expanded word lists. The bold
face numbers denote the best performance among
the four word lists.
As shown in Tables 4 and 5, for both regression
and ranking tasks, the models trained on expanded
keywords (i.e., EXP-*) are consistently better than
those trained on the original sentiment keywords
only.6 Additionally, we treat the experiments with
randomly expanded word list (EXP-RAN) as the
baselines.7 From the two tables, we observe that
the expansion either with or without syntactic in-
formation outperforms the baselines. Note that, for
the EXP-SIM, the number of words used for train-
</bodyText>
<footnote confidence="0.995980833333333">
6Due to the page limits, only the results trained on features
LOGP for regression and TFIDF for ranking are reported, but
the performance for models trained on features TF, TFIFG,
and LOGP is very consistent.
7The results for EXP-SYN are all significant better than
the baseline with P &lt; 0.05.
</footnote>
<bodyText confidence="0.9965962">
CBOW model is effective at expanding keywords
for financial risk prediction. Furthermore, incorpo-
rating syntactic information into the CBOW model
can even enhance the performance for the tasks of
financial risk prediction.
</bodyText>
<subsectionHeader confidence="0.997583">
4.5 Discussions
</subsectionHeader>
<bodyText confidence="0.998996">
Below we provide the original texts from 10-K re-
ports that contain the top 1 expanded word, “uncur”
(stemmed), for “default” in Table 3. Two pieces
of sentences are listed as follows (the company
Investment Technology Group, 1997):
</bodyText>
<listItem confidence="0.8712905">
· · · terminate the agreement upon cer-
tain events of bankruptcy or insolvency
or upon an uncured breach by the Com-
pany of certain covenants · · ·
· · · any termination of the license agree-
ment resulting from an uncured default
would have a material adverse effect on
the Company’s results of operations.
</listItem>
<bodyText confidence="0.99990875">
From the above examples, the expanded word “un-
cur” has similar meaning to “default,” which con-
firms the capability of our method of capturing
similarly meaningful or highly correlated words.
</bodyText>
<sectionHeader confidence="0.994626" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999916454545455">
This paper applies the continuous bag-of-words
model on the textual information in financial re-
ports for expanding keywords from a financial sen-
timent lexicon. Additionally, we propose a simple
but novel approach to incorporate syntactic infor-
mation into the continuous bag-of-words model for
capturing more similarly meaningful or highly cor-
related keywords. The experimental results for the
risk prediction problem show that the expansion
either with or without syntactic information out-
performs the baselines. As a direction for further
</bodyText>
<page confidence="0.97557">
1456
</page>
<bodyText confidence="0.999697">
research, it is interesting and important to provide
more analysis on the expanded words via the con-
tinuous vector representations of words.
</bodyText>
<sectionHeader confidence="0.997714" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.998598">
This research was partially supported by the Na-
tional Science Council of Taiwan under the grants
NSC 102-2420-H-004-052-MY2, 102-2221-E-004-
006, and 102-2221-E-845-002-MY3.
</bodyText>
<sectionHeader confidence="0.998789" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99519459139785">
Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Re-
search, 3:1137–1155.
Peter F Christoffersen and Francis X Diebold. 2000.
How relevant is volatility forecasting for financial
risk management? Review of Economics and Statis-
tics, 82(1):12–22.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In Pro-
ceedings of the 25th international conference on Ma-
chine learning, IMCL ’08, pages 160–167.
Ann Devitt and Khurshid Ahmad. 2007. Sentiment
polarity identification in financial news: A cohesion-
based approach. In Proceedings of the 45th Annual
Meeting of the Association of Computational Lin-
guistics, ACL ’07, pages 984–991.
Harris Drucker, Chris JC Burges, Linda Kaufman, Alex
Smola, and Vladimir Vapnik. 1997. Support vector
regression machines. Advances in Neural Informa-
tion Processing Systems, 9:155–161.
Xavier Glorot, Antoine Bordes, and Yoshua Bengio.
2011. Domain adaptation for large-scale sentiment
classification: A deep learning approach. In Pro-
ceedings of the 28th International Conference on
Machine Learning, ICML ’11, pages 513–520.
Thorsten Joachims. 2006. Training linear svms in lin-
ear time. In Proceedings of the 12th ACM SIGKDD
international conference on Knowledge discovery
and data mining, KDD ’06, pages 217–226.
Maurice G Kendall. 1938. A new measure of rank
correlation. Biometrika, 30:81–93.
Shimon Kogan, Dimitry Levin, Bryan R Routledge,
Jacob S Sagi, and Noah A Smith. 2009. Pre-
dicting risk from financial reports with regression.
In Proceedings of Human Language Technologies:
The 2009 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, NAACL ’09, pages 272–280.
Yi-Shian Lee and Lee-Ing Tong. 2011. Forecasting
time series using a methodology based on autore-
gressive integrated moving average and genetic pro-
gramming. Knowledge-Based Systems, 24(1):66–
72.
Jochen L. Leidner and Frank Schilder. 2010. Hunting
for the black swan: risk mining from text. In Pro-
ceedings of the ACL 2010 System Demonstrations,
ACLDemos ’10, pages 54–59.
Tim Loughran and Bill McDonald. 2011. When is a
liability not a liability? textual analysis, dictionaries,
and 10-ks. The Journal of Finance, 66(1):35–65.
Tomas Mikolov, Martin Karafi´at, Lukas Burget, Jan
Cernock`y, and Sanjeev Khudanpur. 2010. Recur-
rent neural network based language model. In Pro-
ceedings of Interspeech, pages 1045–1048.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient estimation of word represen-
tations in vector space. CoRR, abs/1301.3781.
Jerome L Myers, Arnold D Well, and Robert F Lorch Jr.
2003. Research design and statistical analysis.
Routledge.
Mitchell A Petersen. 2004. Information: Hard and soft.
Technical report, Northwestern University.
Martin F Porter. 1980. An algorithm for suffix strip-
ping. Program: electronic library and information
systems, 14(3):130–137.
Bernhard Sch¨olkopf and Alexander J Smola. 2001.
Learning with kernels: Support vector machines,
regularization, optimization, and beyond. MIT
press.
Holger Schwenk. 2007. Continuous space language
models. Computer Speech &amp; Language, 21(3):492–
518.
Richard Socher, Cliff C Lin, Chris Manning, and An-
drew Y Ng. 2011. Parsing natural scenes and nat-
ural language with recursive neural networks. In
Proceedings of the 28th International Conference on
Machine Learning, ICML ’11, pages 129–136.
Ming-Feng Tsai and Chuan-Ju Wang. 2013. Risk rank-
ing from financial reports. In Advances in Informa-
tion Retrieval, pages 804–807. Springer.
Ruey S Tsay. 2005. Analysis of financial time series.
Wiley.
Chuan-Ju Wang, Ming-Feng Tsai, Tse Liu, and Chin-
Ting Chang. 2013. Financial sentiment analysis for
risk prediction. In Proceedings of the Sixth Interna-
tional Joint Conference on Natural Language Pro-
cessing, IJCNLP ’13, pages 802–808.
Jason Weston, Samy Bengio, and Nicolas Usunier.
2011. Wsabie: Scaling up to large vocabulary
image annotation. In Proceedings of the Twenty-
Second international joint conference on Artificial
</reference>
<page confidence="0.846814">
1457
</page>
<reference confidence="0.999346181818182">
Intelligence-Volume Volume Three, pages 2764–
2770.
Desheng Dash Wu, Shu-Heng Chen, and David L Ol-
son. 2014. Business intelligence in risk manage-
ment: Some recent progresses. Information Sci-
ences, 256:1–7.
Serdar Y¨uml¨u, Fikret S G¨urgen, and Nesrin Okay.
2005. A comparison of global, recurrent and
smoothed-piecewise neural models for istanbul
stock exchange (ise) prediction. Pattern Recogni-
tion Letters, 26(13):2093–2103.
</reference>
<page confidence="0.9941">
1458
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.449589">
<title confidence="0.9952335">Financial Keyword Expansion Continuous Word Vector Representations</title>
<author confidence="0.907419">Chuan-Ju</author>
<affiliation confidence="0.999739">Department of Computer University of</affiliation>
<address confidence="0.939431">Taipei 100,</address>
<email confidence="0.952617">cjwang@utaipei.edu.tw</email>
<author confidence="0.703555">Ming-Feng</author>
<affiliation confidence="0.98189">Department of Computer Science Program in Digital Content and National Chengchi</affiliation>
<address confidence="0.900211">Taipei 116,</address>
<email confidence="0.979944">mftsai@nccu.edu.tw</email>
<abstract confidence="0.991819666666667">This paper proposes to apply the continuous vector representations of words for discovering keywords from a financial sentiment lexicon. In order to capture more keywords, we also incorporate syntactic information into the Continuous Bag-of- Words (CBOW) model. Experimental results on a task of financial risk prediction using the discovered keywords demonstrate that the proposed approach is good at predicting financial risk.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>R´ejean Ducharme</author>
<author>Pascal Vincent</author>
<author>Christian Janvin</author>
</authors>
<title>A neural probabilistic language model.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--1137</pages>
<contexts>
<context position="1953" citStr="Bengio et al., 2003" startWordPosition="284" endWordPosition="287">Diebold, 2000; Lee and Tong, 2011; Wu et al., 2014; Y¨uml¨u et al., 2005). Despite of using only hard information, some literature incorporates soft textual information to predict financial risk (Kogan et al., 2009; Leidner and Schilder, 2010; Tsai and Wang, 2013). Moreover, sentiment analysis, a technique to make an assessment of the sentiments expressed in various information, has also been applied to analyze the soft textual information in financial news, reports, and social media data (Devitt and Ahmad, 2007; Loughran and McDonald, 2011; Wang et al., 2013). Continuous vector space models (Bengio et al., 2003; Schwenk, 2007; Mikolov et al., 2010) are neural network language models, in which words are represented as high dimensional real valued vectors. These representations have recently demonstrated promising results across variety of tasks (Schwenk, 2007; Collobert and Weston, 2008; Glorot et al., 2011; Socher et al., 2011; Weston et al., 2011), because of their superiority of capturing syntactic and semantic regularities in language. In this paper, we apply the Continuous Bag-of-Words (CBOW) model (Mikolov et al., 2013) on the soft textual information in financial reports for discovering keywor</context>
</contexts>
<marker>Bengio, Ducharme, Vincent, Janvin, 2003</marker>
<rawString>Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and Christian Janvin. 2003. A neural probabilistic language model. Journal of Machine Learning Research, 3:1137–1155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Christoffersen</author>
<author>Francis X Diebold</author>
</authors>
<title>How relevant is volatility forecasting for financial risk management?</title>
<date>2000</date>
<journal>Review of Economics and Statistics,</journal>
<volume>82</volume>
<issue>1</issue>
<contexts>
<context position="1347" citStr="Christoffersen and Diebold, 2000" startWordPosition="188" endWordPosition="191">proposed approach is good at predicting financial risk. 1 Introduction In the present environment with a great deal of information, how to discover useful insights for decision making is becoming increasingly important. In finance, there are typically two kinds of information (Petersen, 2004): soft information usually refers to text, including opinions, ideas, and market commentary, whereas hard information is recorded as numbers, such as financial measures and historical prices. Most financial studies related to risk analysis are based on hard information, especially on time series modeling (Christoffersen and Diebold, 2000; Lee and Tong, 2011; Wu et al., 2014; Y¨uml¨u et al., 2005). Despite of using only hard information, some literature incorporates soft textual information to predict financial risk (Kogan et al., 2009; Leidner and Schilder, 2010; Tsai and Wang, 2013). Moreover, sentiment analysis, a technique to make an assessment of the sentiments expressed in various information, has also been applied to analyze the soft textual information in financial news, reports, and social media data (Devitt and Ahmad, 2007; Loughran and McDonald, 2011; Wang et al., 2013). Continuous vector space models (Bengio et al.</context>
</contexts>
<marker>Christoffersen, Diebold, 2000</marker>
<rawString>Peter F Christoffersen and Francis X Diebold. 2000. How relevant is volatility forecasting for financial risk management? Review of Economics and Statistics, 82(1):12–22.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
</authors>
<title>A unified architecture for natural language processing: Deep neural networks with multitask learning.</title>
<date>2008</date>
<booktitle>In Proceedings of the 25th international conference on Machine learning, IMCL ’08,</booktitle>
<pages>160--167</pages>
<contexts>
<context position="2233" citStr="Collobert and Weston, 2008" startWordPosition="324" endWordPosition="327">r, sentiment analysis, a technique to make an assessment of the sentiments expressed in various information, has also been applied to analyze the soft textual information in financial news, reports, and social media data (Devitt and Ahmad, 2007; Loughran and McDonald, 2011; Wang et al., 2013). Continuous vector space models (Bengio et al., 2003; Schwenk, 2007; Mikolov et al., 2010) are neural network language models, in which words are represented as high dimensional real valued vectors. These representations have recently demonstrated promising results across variety of tasks (Schwenk, 2007; Collobert and Weston, 2008; Glorot et al., 2011; Socher et al., 2011; Weston et al., 2011), because of their superiority of capturing syntactic and semantic regularities in language. In this paper, we apply the Continuous Bag-of-Words (CBOW) model (Mikolov et al., 2013) on the soft textual information in financial reports for discovering keywords via financial sentiments. In specific, we use the continuous vector representations of words to find out similar terms based on their contexts. Additionally, we propose a straightforward approach to incorporate syntactic information into the CBOW model for better locating simi</context>
</contexts>
<marker>Collobert, Weston, 2008</marker>
<rawString>Ronan Collobert and Jason Weston. 2008. A unified architecture for natural language processing: Deep neural networks with multitask learning. In Proceedings of the 25th international conference on Machine learning, IMCL ’08, pages 160–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ann Devitt</author>
<author>Khurshid Ahmad</author>
</authors>
<title>Sentiment polarity identification in financial news: A cohesionbased approach.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, ACL ’07,</booktitle>
<pages>984--991</pages>
<contexts>
<context position="1851" citStr="Devitt and Ahmad, 2007" startWordPosition="267" endWordPosition="271">d to risk analysis are based on hard information, especially on time series modeling (Christoffersen and Diebold, 2000; Lee and Tong, 2011; Wu et al., 2014; Y¨uml¨u et al., 2005). Despite of using only hard information, some literature incorporates soft textual information to predict financial risk (Kogan et al., 2009; Leidner and Schilder, 2010; Tsai and Wang, 2013). Moreover, sentiment analysis, a technique to make an assessment of the sentiments expressed in various information, has also been applied to analyze the soft textual information in financial news, reports, and social media data (Devitt and Ahmad, 2007; Loughran and McDonald, 2011; Wang et al., 2013). Continuous vector space models (Bengio et al., 2003; Schwenk, 2007; Mikolov et al., 2010) are neural network language models, in which words are represented as high dimensional real valued vectors. These representations have recently demonstrated promising results across variety of tasks (Schwenk, 2007; Collobert and Weston, 2008; Glorot et al., 2011; Socher et al., 2011; Weston et al., 2011), because of their superiority of capturing syntactic and semantic regularities in language. In this paper, we apply the Continuous Bag-of-Words (CBOW) mo</context>
</contexts>
<marker>Devitt, Ahmad, 2007</marker>
<rawString>Ann Devitt and Khurshid Ahmad. 2007. Sentiment polarity identification in financial news: A cohesionbased approach. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, ACL ’07, pages 984–991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harris Drucker</author>
<author>Chris JC Burges</author>
<author>Linda Kaufman</author>
<author>Alex Smola</author>
<author>Vladimir Vapnik</author>
</authors>
<title>Support vector regression machines.</title>
<date>1997</date>
<booktitle>Advances in Neural Information Processing Systems,</booktitle>
<pages>9--155</pages>
<contexts>
<context position="7177" citStr="Drucker et al., 1997" startWordPosition="1131" endWordPosition="1134">eturns for a stock from time t − n to t can thus be defined as follows: Et i=t−n (Ri − R)2 n where R¯ = Eti=t−n Ri/(n + 1). 3.2 Regression Task Given a collection of financial reports D = {d1, d2, ... , dnJ, in which each di E Rp and is associated with a company ci, we aim to predict the future risk of each company ci (which is characterized by its volatility vi). This prediction problem can be defined as follows: ˆvi = f(di; w). (2) The goal is to learn a p-dimensional vector w from the training data T = {(di, vi)Edi E Rp, vi E RJ. In this paper, we adopt the Support Vector Regression (SVR) (Drucker et al., 1997) for training such a regression model. More details about SVR can be found in (Sch¨olkopf and Smola, 2001). 3.3 Ranking Task Instead of predicting the volatility of each company in the regression task, the ranking task aims to rank companies according to their risk via the textual information in their financial reports. We first split the volatilities of company stock returns within a year into different risk levels by the mechanism provided in (Tsai and Wang, 2013). The risk levels can be considered as the relative difference of risk among the companies. After obtaining the relative risk leve</context>
</contexts>
<marker>Drucker, Burges, Kaufman, Smola, Vapnik, 1997</marker>
<rawString>Harris Drucker, Chris JC Burges, Linda Kaufman, Alex Smola, and Vladimir Vapnik. 1997. Support vector regression machines. Advances in Neural Information Processing Systems, 9:155–161.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Glorot</author>
<author>Antoine Bordes</author>
<author>Yoshua Bengio</author>
</authors>
<title>Domain adaptation for large-scale sentiment classification: A deep learning approach.</title>
<date>2011</date>
<booktitle>In Proceedings of the 28th International Conference on Machine Learning, ICML ’11,</booktitle>
<pages>513--520</pages>
<contexts>
<context position="2254" citStr="Glorot et al., 2011" startWordPosition="328" endWordPosition="331">hnique to make an assessment of the sentiments expressed in various information, has also been applied to analyze the soft textual information in financial news, reports, and social media data (Devitt and Ahmad, 2007; Loughran and McDonald, 2011; Wang et al., 2013). Continuous vector space models (Bengio et al., 2003; Schwenk, 2007; Mikolov et al., 2010) are neural network language models, in which words are represented as high dimensional real valued vectors. These representations have recently demonstrated promising results across variety of tasks (Schwenk, 2007; Collobert and Weston, 2008; Glorot et al., 2011; Socher et al., 2011; Weston et al., 2011), because of their superiority of capturing syntactic and semantic regularities in language. In this paper, we apply the Continuous Bag-of-Words (CBOW) model (Mikolov et al., 2013) on the soft textual information in financial reports for discovering keywords via financial sentiments. In specific, we use the continuous vector representations of words to find out similar terms based on their contexts. Additionally, we propose a straightforward approach to incorporate syntactic information into the CBOW model for better locating similarly meaningful or h</context>
</contexts>
<marker>Glorot, Bordes, Bengio, 2011</marker>
<rawString>Xavier Glorot, Antoine Bordes, and Yoshua Bengio. 2011. Domain adaptation for large-scale sentiment classification: A deep learning approach. In Proceedings of the 28th International Conference on Machine Learning, ICML ’11, pages 513–520.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Training linear svms in linear time.</title>
<date>2006</date>
<booktitle>In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, KDD ’06,</booktitle>
<pages>217--226</pages>
<contexts>
<context position="8307" citStr="Joachims, 2006" startWordPosition="1336" endWordPosition="1337">lative difference of risk among the companies. After obtaining the relative risk levels of the companies, the ranking task can be defined as follows: Given a collection of financial reports D, we aim to rank the companies via a ranking model f : Rp —* R such that the rank order of the set of companies is specified by the real value that the = v[t−n,t] , (1) 1454 model f takes. Specifically, f(di) &gt; f(dj) means that the model asserts that ci &gt;- cj, where ci &gt;- cj means that ci is ranked higher than cj; that is, the company ci is more risky than cj. For this task, this paper adopts Ranking SVM (Joachims, 2006). 4 Experiments 4.1 Dataset and Preprocessings In the experiments, we use the 10-K corpus (Kogan et al., 2009) to conduct our financial risk prediction tasks. All documents and the 6 financial sentiment word lists are stemmed by the Porter stemmer (Porter, 1980), and some stop words are also removed. For financial risk prediction, the ground truth, the twelve months after the report volatility for each company, v+(12), (which measures the future risk for each company) can be calculated by Equation (1), where the stock prices can be obtained from the Center for Research in Security Prices (CRSP</context>
</contexts>
<marker>Joachims, 2006</marker>
<rawString>Thorsten Joachims. 2006. Training linear svms in linear time. In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, KDD ’06, pages 217–226.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maurice G Kendall</author>
</authors>
<title>A new measure of rank correlation.</title>
<date>1938</date>
<journal>Biometrika,</journal>
<pages>30--81</pages>
<contexts>
<context position="12142" citStr="Kendall, 1938" startWordPosition="1977" endWordPosition="1978"> term count of t in d, |d |is the length of document d, and D denotes the set of all documents in each year. 4.4 Experimental Results Tables 4 and 5 tabulate the experimental results of regression and ranking, respectively, in which the training data is composed of the financial reports in a five-year period, and the following year is the test data. For example, the reports from year 1996 to 2000 constitute a training data, and the learned model is tested on the reports of year 2001. 1455 [TFIDF] (Baseline) Year SEN EXP-RAN EXP-SIM EXP-SYN (Baseline) SEN EXP-RAN EXP-SIM EXP-SYN Kendall’s Tau (Kendall, 1938). Spearman’s Rho (Myers et al., 2003) 2001 0.4384 0.4574 0.4952 0.5049 0.4701 0.4889 0.5266 0.5375 2002 0.4421 0.4706 0.4881 0.4944 0.4719 0.5007 0.5187 0.5256 2003 0.4414 0.4706 0.5105 0.5006 0.4716 0.5015 0.5418 0.5318 2004 0.4051 0.4551 0.4750 0.4961 0.4335 0.4842 0.5043 0.5255 2005 0.3856 0.4482 0.5126 0.5294 0.4117 0.4757 0.5418 0.5579 2006 0.3784 0.4385 0.4588 0.4867 0.4029 0.4641 0.4847 0.5129 Table 5: Performance of Ranking. [LOGP] (Baseline) ing the regression and ranking models is even less Year SEN EXP-RAN EXP-SIM EXP-SYN than that of EXP-RAN. The results suggest that the Mean Squar</context>
</contexts>
<marker>Kendall, 1938</marker>
<rawString>Maurice G Kendall. 1938. A new measure of rank correlation. Biometrika, 30:81–93.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shimon Kogan</author>
<author>Dimitry Levin</author>
<author>Bryan R Routledge</author>
<author>Jacob S Sagi</author>
<author>Noah A Smith</author>
</authors>
<title>Predicting risk from financial reports with regression.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, NAACL ’09,</booktitle>
<pages>272--280</pages>
<contexts>
<context position="1548" citStr="Kogan et al., 2009" startWordPosition="220" endWordPosition="224">ant. In finance, there are typically two kinds of information (Petersen, 2004): soft information usually refers to text, including opinions, ideas, and market commentary, whereas hard information is recorded as numbers, such as financial measures and historical prices. Most financial studies related to risk analysis are based on hard information, especially on time series modeling (Christoffersen and Diebold, 2000; Lee and Tong, 2011; Wu et al., 2014; Y¨uml¨u et al., 2005). Despite of using only hard information, some literature incorporates soft textual information to predict financial risk (Kogan et al., 2009; Leidner and Schilder, 2010; Tsai and Wang, 2013). Moreover, sentiment analysis, a technique to make an assessment of the sentiments expressed in various information, has also been applied to analyze the soft textual information in financial news, reports, and social media data (Devitt and Ahmad, 2007; Loughran and McDonald, 2011; Wang et al., 2013). Continuous vector space models (Bengio et al., 2003; Schwenk, 2007; Mikolov et al., 2010) are neural network language models, in which words are represented as high dimensional real valued vectors. These representations have recently demonstrated</context>
<context position="8417" citStr="Kogan et al., 2009" startWordPosition="1352" endWordPosition="1356">the ranking task can be defined as follows: Given a collection of financial reports D, we aim to rank the companies via a ranking model f : Rp —* R such that the rank order of the set of companies is specified by the real value that the = v[t−n,t] , (1) 1454 model f takes. Specifically, f(di) &gt; f(dj) means that the model asserts that ci &gt;- cj, where ci &gt;- cj means that ci is ranked higher than cj; that is, the company ci is more risky than cj. For this task, this paper adopts Ranking SVM (Joachims, 2006). 4 Experiments 4.1 Dataset and Preprocessings In the experiments, we use the 10-K corpus (Kogan et al., 2009) to conduct our financial risk prediction tasks. All documents and the 6 financial sentiment word lists are stemmed by the Porter stemmer (Porter, 1980), and some stop words are also removed. For financial risk prediction, the ground truth, the twelve months after the report volatility for each company, v+(12), (which measures the future risk for each company) can be calculated by Equation (1), where the stock prices can be obtained from the Center for Research in Security Prices (CRSP) US Stocks Database. In addition, to obtain the relative risks among companies used in the ranking task, we f</context>
</contexts>
<marker>Kogan, Levin, Routledge, Sagi, Smith, 2009</marker>
<rawString>Shimon Kogan, Dimitry Levin, Bryan R Routledge, Jacob S Sagi, and Noah A Smith. 2009. Predicting risk from financial reports with regression. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, NAACL ’09, pages 272–280.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yi-Shian Lee</author>
<author>Lee-Ing Tong</author>
</authors>
<title>Forecasting time series using a methodology based on autoregressive integrated moving average and genetic programming.</title>
<date>2011</date>
<journal>Knowledge-Based Systems,</journal>
<volume>24</volume>
<issue>1</issue>
<pages>72</pages>
<contexts>
<context position="1367" citStr="Lee and Tong, 2011" startWordPosition="192" endWordPosition="195">cting financial risk. 1 Introduction In the present environment with a great deal of information, how to discover useful insights for decision making is becoming increasingly important. In finance, there are typically two kinds of information (Petersen, 2004): soft information usually refers to text, including opinions, ideas, and market commentary, whereas hard information is recorded as numbers, such as financial measures and historical prices. Most financial studies related to risk analysis are based on hard information, especially on time series modeling (Christoffersen and Diebold, 2000; Lee and Tong, 2011; Wu et al., 2014; Y¨uml¨u et al., 2005). Despite of using only hard information, some literature incorporates soft textual information to predict financial risk (Kogan et al., 2009; Leidner and Schilder, 2010; Tsai and Wang, 2013). Moreover, sentiment analysis, a technique to make an assessment of the sentiments expressed in various information, has also been applied to analyze the soft textual information in financial news, reports, and social media data (Devitt and Ahmad, 2007; Loughran and McDonald, 2011; Wang et al., 2013). Continuous vector space models (Bengio et al., 2003; Schwenk, 200</context>
</contexts>
<marker>Lee, Tong, 2011</marker>
<rawString>Yi-Shian Lee and Lee-Ing Tong. 2011. Forecasting time series using a methodology based on autoregressive integrated moving average and genetic programming. Knowledge-Based Systems, 24(1):66– 72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jochen L Leidner</author>
<author>Frank Schilder</author>
</authors>
<title>Hunting for the black swan: risk mining from text.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACL 2010 System Demonstrations, ACLDemos ’10,</booktitle>
<pages>54--59</pages>
<contexts>
<context position="1576" citStr="Leidner and Schilder, 2010" startWordPosition="225" endWordPosition="228">re are typically two kinds of information (Petersen, 2004): soft information usually refers to text, including opinions, ideas, and market commentary, whereas hard information is recorded as numbers, such as financial measures and historical prices. Most financial studies related to risk analysis are based on hard information, especially on time series modeling (Christoffersen and Diebold, 2000; Lee and Tong, 2011; Wu et al., 2014; Y¨uml¨u et al., 2005). Despite of using only hard information, some literature incorporates soft textual information to predict financial risk (Kogan et al., 2009; Leidner and Schilder, 2010; Tsai and Wang, 2013). Moreover, sentiment analysis, a technique to make an assessment of the sentiments expressed in various information, has also been applied to analyze the soft textual information in financial news, reports, and social media data (Devitt and Ahmad, 2007; Loughran and McDonald, 2011; Wang et al., 2013). Continuous vector space models (Bengio et al., 2003; Schwenk, 2007; Mikolov et al., 2010) are neural network language models, in which words are represented as high dimensional real valued vectors. These representations have recently demonstrated promising results across va</context>
</contexts>
<marker>Leidner, Schilder, 2010</marker>
<rawString>Jochen L. Leidner and Frank Schilder. 2010. Hunting for the black swan: risk mining from text. In Proceedings of the ACL 2010 System Demonstrations, ACLDemos ’10, pages 54–59.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tim Loughran</author>
<author>Bill McDonald</author>
</authors>
<title>When is a liability not a liability? textual analysis, dictionaries, and 10-ks.</title>
<date>2011</date>
<journal>The Journal of Finance,</journal>
<volume>66</volume>
<issue>1</issue>
<contexts>
<context position="1880" citStr="Loughran and McDonald, 2011" startWordPosition="272" endWordPosition="275">ased on hard information, especially on time series modeling (Christoffersen and Diebold, 2000; Lee and Tong, 2011; Wu et al., 2014; Y¨uml¨u et al., 2005). Despite of using only hard information, some literature incorporates soft textual information to predict financial risk (Kogan et al., 2009; Leidner and Schilder, 2010; Tsai and Wang, 2013). Moreover, sentiment analysis, a technique to make an assessment of the sentiments expressed in various information, has also been applied to analyze the soft textual information in financial news, reports, and social media data (Devitt and Ahmad, 2007; Loughran and McDonald, 2011; Wang et al., 2013). Continuous vector space models (Bengio et al., 2003; Schwenk, 2007; Mikolov et al., 2010) are neural network language models, in which words are represented as high dimensional real valued vectors. These representations have recently demonstrated promising results across variety of tasks (Schwenk, 2007; Collobert and Weston, 2008; Glorot et al., 2011; Socher et al., 2011; Weston et al., 2011), because of their superiority of capturing syntactic and semantic regularities in language. In this paper, we apply the Continuous Bag-of-Words (CBOW) model (Mikolov et al., 2013) on</context>
<context position="4286" citStr="Loughran and McDonald (2011)" startWordPosition="624" endWordPosition="628">al results, the expansion either with or without syntactic information outperforms the baselines. The results suggest that the CBOW model is effective at expanding keywords for financial risk prediction. 1Securities and Exchange Commission 1453 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1453–1458, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics 2 Keyword Expansion via Financial Sentiment Lexicon 2.1 Financial Sentiment Lexicon A sentiment lexicon is the most important resource for sentiment analysis. Loughran and McDonald (2011) states that a general purpose sentiment lexicon (e.g., the Harvard Psychosociological Dictionary) might misclassify common words in financial texts. Therefore, in this paper, we use a finance-specific lexicon that consists of the 6 word lists provided by (Loughran and McDonald, 2011) as seeds to expand keywords. The six lists are negative (Fin-Neg), positive (Fin-Pos), uncertainty (FinUnc), litigious (Fin-Lit), strong modal words (MWStrong), and weak modal words (MW-Weak).2 2.2 Simple Keyword Expansion With the financial sentiment lexicon, we first use a collection of financial reports as the</context>
</contexts>
<marker>Loughran, McDonald, 2011</marker>
<rawString>Tim Loughran and Bill McDonald. 2011. When is a liability not a liability? textual analysis, dictionaries, and 10-ks. The Journal of Finance, 66(1):35–65.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Martin Karafi´at</author>
<author>Lukas Burget</author>
<author>Jan Cernock`y</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>Recurrent neural network based language model.</title>
<date>2010</date>
<booktitle>In Proceedings of Interspeech,</booktitle>
<pages>1045--1048</pages>
<marker>Mikolov, Karafi´at, Burget, Cernock`y, Khudanpur, 2010</marker>
<rawString>Tomas Mikolov, Martin Karafi´at, Lukas Burget, Jan Cernock`y, and Sanjeev Khudanpur. 2010. Recurrent neural network based language model. In Proceedings of Interspeech, pages 1045–1048.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space.</title>
<date>2013</date>
<location>CoRR, abs/1301.3781.</location>
<contexts>
<context position="2477" citStr="Mikolov et al., 2013" startWordPosition="362" endWordPosition="365">ghran and McDonald, 2011; Wang et al., 2013). Continuous vector space models (Bengio et al., 2003; Schwenk, 2007; Mikolov et al., 2010) are neural network language models, in which words are represented as high dimensional real valued vectors. These representations have recently demonstrated promising results across variety of tasks (Schwenk, 2007; Collobert and Weston, 2008; Glorot et al., 2011; Socher et al., 2011; Weston et al., 2011), because of their superiority of capturing syntactic and semantic regularities in language. In this paper, we apply the Continuous Bag-of-Words (CBOW) model (Mikolov et al., 2013) on the soft textual information in financial reports for discovering keywords via financial sentiments. In specific, we use the continuous vector representations of words to find out similar terms based on their contexts. Additionally, we propose a straightforward approach to incorporate syntactic information into the CBOW model for better locating similarly meaningful or highly correlated words. To the best of our knowledge, this is the first work to incorporate more syntactic information by adding Part-OfSpeech (POS) tags to the words before training the CBOW model. In our experiments, the </context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient estimation of word representations in vector space. CoRR, abs/1301.3781.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerome L Myers</author>
<author>Arnold D Well</author>
<author>Robert F Lorch Jr</author>
</authors>
<title>Research design and statistical analysis.</title>
<date>2003</date>
<publisher>Routledge.</publisher>
<contexts>
<context position="12179" citStr="Myers et al., 2003" startWordPosition="1981" endWordPosition="1984">e length of document d, and D denotes the set of all documents in each year. 4.4 Experimental Results Tables 4 and 5 tabulate the experimental results of regression and ranking, respectively, in which the training data is composed of the financial reports in a five-year period, and the following year is the test data. For example, the reports from year 1996 to 2000 constitute a training data, and the learned model is tested on the reports of year 2001. 1455 [TFIDF] (Baseline) Year SEN EXP-RAN EXP-SIM EXP-SYN (Baseline) SEN EXP-RAN EXP-SIM EXP-SYN Kendall’s Tau (Kendall, 1938). Spearman’s Rho (Myers et al., 2003) 2001 0.4384 0.4574 0.4952 0.5049 0.4701 0.4889 0.5266 0.5375 2002 0.4421 0.4706 0.4881 0.4944 0.4719 0.5007 0.5187 0.5256 2003 0.4414 0.4706 0.5105 0.5006 0.4716 0.5015 0.5418 0.5318 2004 0.4051 0.4551 0.4750 0.4961 0.4335 0.4842 0.5043 0.5255 2005 0.3856 0.4482 0.5126 0.5294 0.4117 0.4757 0.5418 0.5579 2006 0.3784 0.4385 0.4588 0.4867 0.4029 0.4641 0.4847 0.5129 Table 5: Performance of Ranking. [LOGP] (Baseline) ing the regression and ranking models is even less Year SEN EXP-RAN EXP-SIM EXP-SYN than that of EXP-RAN. The results suggest that the Mean Squared Error 2001 0.2526 0.2360 0.2195 0.</context>
</contexts>
<marker>Myers, Well, Jr, 2003</marker>
<rawString>Jerome L Myers, Arnold D Well, and Robert F Lorch Jr. 2003. Research design and statistical analysis. Routledge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell A Petersen</author>
</authors>
<title>Information: Hard and soft.</title>
<date>2004</date>
<tech>Technical report,</tech>
<institution>Northwestern University.</institution>
<contexts>
<context position="1008" citStr="Petersen, 2004" startWordPosition="141" endWordPosition="142">r representations of words for discovering keywords from a financial sentiment lexicon. In order to capture more keywords, we also incorporate syntactic information into the Continuous Bag-ofWords (CBOW) model. Experimental results on a task of financial risk prediction using the discovered keywords demonstrate that the proposed approach is good at predicting financial risk. 1 Introduction In the present environment with a great deal of information, how to discover useful insights for decision making is becoming increasingly important. In finance, there are typically two kinds of information (Petersen, 2004): soft information usually refers to text, including opinions, ideas, and market commentary, whereas hard information is recorded as numbers, such as financial measures and historical prices. Most financial studies related to risk analysis are based on hard information, especially on time series modeling (Christoffersen and Diebold, 2000; Lee and Tong, 2011; Wu et al., 2014; Y¨uml¨u et al., 2005). Despite of using only hard information, some literature incorporates soft textual information to predict financial risk (Kogan et al., 2009; Leidner and Schilder, 2010; Tsai and Wang, 2013). Moreover</context>
</contexts>
<marker>Petersen, 2004</marker>
<rawString>Mitchell A Petersen. 2004. Information: Hard and soft. Technical report, Northwestern University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin F Porter</author>
</authors>
<title>An algorithm for suffix stripping. Program: electronic library and information systems,</title>
<date>1980</date>
<pages>14--3</pages>
<contexts>
<context position="8569" citStr="Porter, 1980" startWordPosition="1380" endWordPosition="1381">the rank order of the set of companies is specified by the real value that the = v[t−n,t] , (1) 1454 model f takes. Specifically, f(di) &gt; f(dj) means that the model asserts that ci &gt;- cj, where ci &gt;- cj means that ci is ranked higher than cj; that is, the company ci is more risky than cj. For this task, this paper adopts Ranking SVM (Joachims, 2006). 4 Experiments 4.1 Dataset and Preprocessings In the experiments, we use the 10-K corpus (Kogan et al., 2009) to conduct our financial risk prediction tasks. All documents and the 6 financial sentiment word lists are stemmed by the Porter stemmer (Porter, 1980), and some stop words are also removed. For financial risk prediction, the ground truth, the twelve months after the report volatility for each company, v+(12), (which measures the future risk for each company) can be calculated by Equation (1), where the stock prices can be obtained from the Center for Research in Security Prices (CRSP) US Stocks Database. In addition, to obtain the relative risks among companies used in the ranking task, we follow (Tsai and Wang, 2013) to split the companies of each year into 5 risk levels. 4.2 Keyword Expansion In our experiments, Section 7 (Management Disc</context>
</contexts>
<marker>Porter, 1980</marker>
<rawString>Martin F Porter. 1980. An algorithm for suffix stripping. Program: electronic library and information systems, 14(3):130–137.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernhard Sch¨olkopf</author>
<author>Alexander J Smola</author>
</authors>
<title>Learning with kernels: Support vector machines, regularization, optimization, and beyond.</title>
<date>2001</date>
<publisher>MIT press.</publisher>
<marker>Sch¨olkopf, Smola, 2001</marker>
<rawString>Bernhard Sch¨olkopf and Alexander J Smola. 2001. Learning with kernels: Support vector machines, regularization, optimization, and beyond. MIT press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Holger Schwenk</author>
</authors>
<title>Continuous space language models.</title>
<date>2007</date>
<journal>Computer Speech &amp; Language,</journal>
<volume>21</volume>
<issue>3</issue>
<pages>518</pages>
<contexts>
<context position="1968" citStr="Schwenk, 2007" startWordPosition="288" endWordPosition="289">d Tong, 2011; Wu et al., 2014; Y¨uml¨u et al., 2005). Despite of using only hard information, some literature incorporates soft textual information to predict financial risk (Kogan et al., 2009; Leidner and Schilder, 2010; Tsai and Wang, 2013). Moreover, sentiment analysis, a technique to make an assessment of the sentiments expressed in various information, has also been applied to analyze the soft textual information in financial news, reports, and social media data (Devitt and Ahmad, 2007; Loughran and McDonald, 2011; Wang et al., 2013). Continuous vector space models (Bengio et al., 2003; Schwenk, 2007; Mikolov et al., 2010) are neural network language models, in which words are represented as high dimensional real valued vectors. These representations have recently demonstrated promising results across variety of tasks (Schwenk, 2007; Collobert and Weston, 2008; Glorot et al., 2011; Socher et al., 2011; Weston et al., 2011), because of their superiority of capturing syntactic and semantic regularities in language. In this paper, we apply the Continuous Bag-of-Words (CBOW) model (Mikolov et al., 2013) on the soft textual information in financial reports for discovering keywords via financia</context>
</contexts>
<marker>Schwenk, 2007</marker>
<rawString>Holger Schwenk. 2007. Continuous space language models. Computer Speech &amp; Language, 21(3):492– 518.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Cliff C Lin</author>
<author>Chris Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Parsing natural scenes and natural language with recursive neural networks.</title>
<date>2011</date>
<booktitle>In Proceedings of the 28th International Conference on Machine Learning, ICML ’11,</booktitle>
<pages>129--136</pages>
<contexts>
<context position="2275" citStr="Socher et al., 2011" startWordPosition="332" endWordPosition="335">essment of the sentiments expressed in various information, has also been applied to analyze the soft textual information in financial news, reports, and social media data (Devitt and Ahmad, 2007; Loughran and McDonald, 2011; Wang et al., 2013). Continuous vector space models (Bengio et al., 2003; Schwenk, 2007; Mikolov et al., 2010) are neural network language models, in which words are represented as high dimensional real valued vectors. These representations have recently demonstrated promising results across variety of tasks (Schwenk, 2007; Collobert and Weston, 2008; Glorot et al., 2011; Socher et al., 2011; Weston et al., 2011), because of their superiority of capturing syntactic and semantic regularities in language. In this paper, we apply the Continuous Bag-of-Words (CBOW) model (Mikolov et al., 2013) on the soft textual information in financial reports for discovering keywords via financial sentiments. In specific, we use the continuous vector representations of words to find out similar terms based on their contexts. Additionally, we propose a straightforward approach to incorporate syntactic information into the CBOW model for better locating similarly meaningful or highly correlated word</context>
</contexts>
<marker>Socher, Lin, Manning, Ng, 2011</marker>
<rawString>Richard Socher, Cliff C Lin, Chris Manning, and Andrew Y Ng. 2011. Parsing natural scenes and natural language with recursive neural networks. In Proceedings of the 28th International Conference on Machine Learning, ICML ’11, pages 129–136.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ming-Feng Tsai</author>
<author>Chuan-Ju Wang</author>
</authors>
<title>Risk ranking from financial reports.</title>
<date>2013</date>
<booktitle>In Advances in Information Retrieval,</booktitle>
<pages>804--807</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="1598" citStr="Tsai and Wang, 2013" startWordPosition="229" endWordPosition="232">f information (Petersen, 2004): soft information usually refers to text, including opinions, ideas, and market commentary, whereas hard information is recorded as numbers, such as financial measures and historical prices. Most financial studies related to risk analysis are based on hard information, especially on time series modeling (Christoffersen and Diebold, 2000; Lee and Tong, 2011; Wu et al., 2014; Y¨uml¨u et al., 2005). Despite of using only hard information, some literature incorporates soft textual information to predict financial risk (Kogan et al., 2009; Leidner and Schilder, 2010; Tsai and Wang, 2013). Moreover, sentiment analysis, a technique to make an assessment of the sentiments expressed in various information, has also been applied to analyze the soft textual information in financial news, reports, and social media data (Devitt and Ahmad, 2007; Loughran and McDonald, 2011; Wang et al., 2013). Continuous vector space models (Bengio et al., 2003; Schwenk, 2007; Mikolov et al., 2010) are neural network language models, in which words are represented as high dimensional real valued vectors. These representations have recently demonstrated promising results across variety of tasks (Schwen</context>
<context position="7647" citStr="Tsai and Wang, 2013" startWordPosition="1208" endWordPosition="1211">nal vector w from the training data T = {(di, vi)Edi E Rp, vi E RJ. In this paper, we adopt the Support Vector Regression (SVR) (Drucker et al., 1997) for training such a regression model. More details about SVR can be found in (Sch¨olkopf and Smola, 2001). 3.3 Ranking Task Instead of predicting the volatility of each company in the regression task, the ranking task aims to rank companies according to their risk via the textual information in their financial reports. We first split the volatilities of company stock returns within a year into different risk levels by the mechanism provided in (Tsai and Wang, 2013). The risk levels can be considered as the relative difference of risk among the companies. After obtaining the relative risk levels of the companies, the ranking task can be defined as follows: Given a collection of financial reports D, we aim to rank the companies via a ranking model f : Rp —* R such that the rank order of the set of companies is specified by the real value that the = v[t−n,t] , (1) 1454 model f takes. Specifically, f(di) &gt; f(dj) means that the model asserts that ci &gt;- cj, where ci &gt;- cj means that ci is ranked higher than cj; that is, the company ci is more risky than cj. F</context>
<context position="9044" citStr="Tsai and Wang, 2013" startWordPosition="1458" endWordPosition="1461">duct our financial risk prediction tasks. All documents and the 6 financial sentiment word lists are stemmed by the Porter stemmer (Porter, 1980), and some stop words are also removed. For financial risk prediction, the ground truth, the twelve months after the report volatility for each company, v+(12), (which measures the future risk for each company) can be calculated by Equation (1), where the stock prices can be obtained from the Center for Research in Security Prices (CRSP) US Stocks Database. In addition, to obtain the relative risks among companies used in the ranking task, we follow (Tsai and Wang, 2013) to split the companies of each year into 5 risk levels. 4.2 Keyword Expansion In our experiments, Section 7 (Management Discussion and Analysis) in 10-K corpus is used as training texts for the tool (word2vec3) to learn the continuous vector representations of words. For the simple expansion (denoted as EXP-SIM hereafter), we use the total 1,667 stemmed sentiment words as seeds to obtain the expanded keywords via the learned word vector representations. For the expansion considering syntactic information (denoted as EXP-SYN), NLTK4 is applied to attach the POS tag5 to each word in the trainin</context>
</contexts>
<marker>Tsai, Wang, 2013</marker>
<rawString>Ming-Feng Tsai and Chuan-Ju Wang. 2013. Risk ranking from financial reports. In Advances in Information Retrieval, pages 804–807. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruey S Tsay</author>
</authors>
<title>Analysis of financial time series.</title>
<date>2005</date>
<publisher>Wiley.</publisher>
<contexts>
<context position="6535" citStr="Tsay, 2005" startWordPosition="1002" endWordPosition="1003">atility Volatility is a measure for variation of prices of a stock over a period of time. Let St be the price of a stock at time t. Holding the stock from time t − 1 to time t would lead to a simple return: Rt = 2http://www.nd.edu/˜mcdonald/Word_ Lists.html. default (NN) default (JJ) Cosine Cosine Word Distance Word Distance default (v.) 0.63665 nonconform (v.) 0.63462 unwaiv (v.) 0.63466 subprim (v.) 0.62404 uncur (v.) 0.62285 chattel (n.) 0.61510 trigger (n.) 0.60080 foreclos (adj.) 0.61397 unmatur (v.) 0.58208 unguarante (v.) 0.60559 Table 1: Top-5 Words for the word “default.” St/St−1 − 1(Tsay, 2005). The volatility of returns for a stock from time t − n to t can thus be defined as follows: Et i=t−n (Ri − R)2 n where R¯ = Eti=t−n Ri/(n + 1). 3.2 Regression Task Given a collection of financial reports D = {d1, d2, ... , dnJ, in which each di E Rp and is associated with a company ci, we aim to predict the future risk of each company ci (which is characterized by its volatility vi). This prediction problem can be defined as follows: ˆvi = f(di; w). (2) The goal is to learn a p-dimensional vector w from the training data T = {(di, vi)Edi E Rp, vi E RJ. In this paper, we adopt the Support Vect</context>
</contexts>
<marker>Tsay, 2005</marker>
<rawString>Ruey S Tsay. 2005. Analysis of financial time series. Wiley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chuan-Ju Wang</author>
<author>Ming-Feng Tsai</author>
<author>Tse Liu</author>
<author>ChinTing Chang</author>
</authors>
<title>Financial sentiment analysis for risk prediction.</title>
<date>2013</date>
<booktitle>In Proceedings of the Sixth International Joint Conference on Natural Language Processing, IJCNLP ’13,</booktitle>
<pages>802--808</pages>
<contexts>
<context position="1900" citStr="Wang et al., 2013" startWordPosition="276" endWordPosition="279">ecially on time series modeling (Christoffersen and Diebold, 2000; Lee and Tong, 2011; Wu et al., 2014; Y¨uml¨u et al., 2005). Despite of using only hard information, some literature incorporates soft textual information to predict financial risk (Kogan et al., 2009; Leidner and Schilder, 2010; Tsai and Wang, 2013). Moreover, sentiment analysis, a technique to make an assessment of the sentiments expressed in various information, has also been applied to analyze the soft textual information in financial news, reports, and social media data (Devitt and Ahmad, 2007; Loughran and McDonald, 2011; Wang et al., 2013). Continuous vector space models (Bengio et al., 2003; Schwenk, 2007; Mikolov et al., 2010) are neural network language models, in which words are represented as high dimensional real valued vectors. These representations have recently demonstrated promising results across variety of tasks (Schwenk, 2007; Collobert and Weston, 2008; Glorot et al., 2011; Socher et al., 2011; Weston et al., 2011), because of their superiority of capturing syntactic and semantic regularities in language. In this paper, we apply the Continuous Bag-of-Words (CBOW) model (Mikolov et al., 2013) on the soft textual in</context>
</contexts>
<marker>Wang, Tsai, Liu, Chang, 2013</marker>
<rawString>Chuan-Ju Wang, Ming-Feng Tsai, Tse Liu, and ChinTing Chang. 2013. Financial sentiment analysis for risk prediction. In Proceedings of the Sixth International Joint Conference on Natural Language Processing, IJCNLP ’13, pages 802–808.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Weston</author>
<author>Samy Bengio</author>
<author>Nicolas Usunier</author>
</authors>
<title>Wsabie: Scaling up to large vocabulary image annotation.</title>
<date>2011</date>
<booktitle>In Proceedings of the TwentySecond international joint conference on Artificial Intelligence-Volume Volume Three,</booktitle>
<pages>2764--2770</pages>
<contexts>
<context position="2297" citStr="Weston et al., 2011" startWordPosition="336" endWordPosition="339">ents expressed in various information, has also been applied to analyze the soft textual information in financial news, reports, and social media data (Devitt and Ahmad, 2007; Loughran and McDonald, 2011; Wang et al., 2013). Continuous vector space models (Bengio et al., 2003; Schwenk, 2007; Mikolov et al., 2010) are neural network language models, in which words are represented as high dimensional real valued vectors. These representations have recently demonstrated promising results across variety of tasks (Schwenk, 2007; Collobert and Weston, 2008; Glorot et al., 2011; Socher et al., 2011; Weston et al., 2011), because of their superiority of capturing syntactic and semantic regularities in language. In this paper, we apply the Continuous Bag-of-Words (CBOW) model (Mikolov et al., 2013) on the soft textual information in financial reports for discovering keywords via financial sentiments. In specific, we use the continuous vector representations of words to find out similar terms based on their contexts. Additionally, we propose a straightforward approach to incorporate syntactic information into the CBOW model for better locating similarly meaningful or highly correlated words. To the best of our </context>
</contexts>
<marker>Weston, Bengio, Usunier, 2011</marker>
<rawString>Jason Weston, Samy Bengio, and Nicolas Usunier. 2011. Wsabie: Scaling up to large vocabulary image annotation. In Proceedings of the TwentySecond international joint conference on Artificial Intelligence-Volume Volume Three, pages 2764– 2770.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Desheng Dash Wu</author>
<author>Shu-Heng Chen</author>
<author>David L Olson</author>
</authors>
<title>Business intelligence in risk management: Some recent progresses. Information Sciences,</title>
<date>2014</date>
<pages>256--1</pages>
<contexts>
<context position="1384" citStr="Wu et al., 2014" startWordPosition="196" endWordPosition="199">. 1 Introduction In the present environment with a great deal of information, how to discover useful insights for decision making is becoming increasingly important. In finance, there are typically two kinds of information (Petersen, 2004): soft information usually refers to text, including opinions, ideas, and market commentary, whereas hard information is recorded as numbers, such as financial measures and historical prices. Most financial studies related to risk analysis are based on hard information, especially on time series modeling (Christoffersen and Diebold, 2000; Lee and Tong, 2011; Wu et al., 2014; Y¨uml¨u et al., 2005). Despite of using only hard information, some literature incorporates soft textual information to predict financial risk (Kogan et al., 2009; Leidner and Schilder, 2010; Tsai and Wang, 2013). Moreover, sentiment analysis, a technique to make an assessment of the sentiments expressed in various information, has also been applied to analyze the soft textual information in financial news, reports, and social media data (Devitt and Ahmad, 2007; Loughran and McDonald, 2011; Wang et al., 2013). Continuous vector space models (Bengio et al., 2003; Schwenk, 2007; Mikolov et al.</context>
</contexts>
<marker>Wu, Chen, Olson, 2014</marker>
<rawString>Desheng Dash Wu, Shu-Heng Chen, and David L Olson. 2014. Business intelligence in risk management: Some recent progresses. Information Sciences, 256:1–7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Serdar Y¨uml¨u</author>
<author>Fikret S G¨urgen</author>
<author>Nesrin Okay</author>
</authors>
<title>A comparison of global, recurrent and smoothed-piecewise neural models for istanbul stock exchange (ise) prediction.</title>
<date>2005</date>
<journal>Pattern Recognition Letters,</journal>
<volume>26</volume>
<issue>13</issue>
<marker>Y¨uml¨u, G¨urgen, Okay, 2005</marker>
<rawString>Serdar Y¨uml¨u, Fikret S G¨urgen, and Nesrin Okay. 2005. A comparison of global, recurrent and smoothed-piecewise neural models for istanbul stock exchange (ise) prediction. Pattern Recognition Letters, 26(13):2093–2103.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
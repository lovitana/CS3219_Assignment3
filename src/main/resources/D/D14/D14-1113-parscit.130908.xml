<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000003">
<title confidence="0.9986945">
Efficient Non-parametric Estimation of
Multiple Embeddings per Word in Vector Space
</title>
<author confidence="0.995842">
Arvind Neelakantan&apos;, Jeevan Shankar&apos;, Alexandre Passos, Andrew McCallum
</author>
<affiliation confidence="0.9988035">
Department of Computer Science
University of Massachusetts, Amherst
</affiliation>
<address confidence="0.931284">
Amherst, MA, 01003
</address>
<email confidence="0.999439">
{arvind,jshankar,apassos,mccallum}@cs.umass.edu
</email>
<sectionHeader confidence="0.993911" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999867285714286">
There is rising interest in vector-space
word embeddings and their use in NLP,
especially given recent methods for their
fast estimation at very large scale. Nearly
all this work, however, assumes a sin-
gle vector per word type—ignoring poly-
semy and thus jeopardizing their useful-
ness for downstream tasks. We present
an extension to the Skip-gram model that
efficiently learns multiple embeddings per
word type. It differs from recent related
work by jointly performing word sense
discrimination and embedding learning,
by non-parametrically estimating the num-
ber of senses per word type, and by its ef-
ficiency and scalability. We present new
state-of-the-art results in the word similar-
ity in context task and demonstrate its scal-
ability by training with one machine on a
corpus of nearly 1 billion tokens in less
than 6 hours.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.997759381818182">
Representing words by dense, real-valued vector
embeddings, also commonly called “distributed
representations,” helps address the curse of di-
mensionality and improve generalization because
they can place near each other words having sim-
ilar semantic and syntactic roles. This has been
shown dramatically in state-of-the-art results on
language modeling (Bengio et al, 2003; Mnih and
Hinton, 2007) as well as improvements in other
natural language processing tasks (Collobert and
Weston, 2008; Turian et al, 2010). Substantial
benefit arises when embeddings can be trained on
large volumes of data. Hence the recent consider-
able interest in the CBOW and Skip-gram models
*The first two authors contributed equally to this paper.
of Mikolov et al (2013a); Mikolov et al (2013b)—
relatively simple log-linear models that can be
trained to produce high-quality word embeddings
on the entirety of English Wikipedia text in less
than half a day on one machine.
There is rising enthusiasm for applying these
models to improve accuracy in natural language
processing, much like Brown clusters (Brown et
al, 1992) have become common input features
for many tasks, such as named entity extraction
(Miller et al, 2004; Ratinov and Roth, 2009) and
parsing (Koo et al, 2008; T¨ackstr¨om et al, 2012).
In comparison to Brown clusters, the vector em-
beddings have the advantages of substantially bet-
ter scalability in their training, and intriguing po-
tential for their continuous and multi-dimensional
interrelations. In fact, Passos et al (2014) present
new state-of-the-art results in CoNLL 2003 named
entity extraction by directly inputting continuous
vector embeddings obtained by a version of Skip-
gram that injects supervision with lexicons. Sim-
ilarly Bansal et al (2014) show results in depen-
dency parsing using Skip-gram embeddings. They
have also recently been applied to machine trans-
lation (Zou et al, 2013; Mikolov et al, 2013c).
A notable deficiency in this prior work is that
each word type (e.g. the word string plant) has
only one vector representation—polysemy and
hononymy are ignored. This results in the word
plant having an embedding that is approximately
the average of its different contextual seman-
tics relating to biology, placement, manufactur-
ing and power generation. In moderately high-
dimensional spaces a vector can be relatively
“close” to multiple regions at a time, but this does
not negate the unfortunate influence of the triangle
inequality2 here: words that are not synonyms but
are synonymous with different senses of the same
word will be pulled together. For example, pollen
and refinery will be inappropriately pulled to a dis-
</bodyText>
<footnote confidence="0.970773">
2For distance d, d(a, c) ≤ d(a, b) + d(b, c).
</footnote>
<page confidence="0.939223">
1059
</page>
<note confidence="0.905653">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1059–1069,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.99995212244898">
tance not more than the sum of the distances plant-
pollen and plant–refinery. Fitting the constraints of
legitimate continuous gradations of semantics are
challenge enough without the additional encum-
brance of these illegitimate triangle inequalities.
Discovering embeddings for multiple senses per
word type is the focus of work by Reisinger and
Mooney (2010a) and Huang et al (2012). They
both pre-cluster the contexts of a word type’s to-
kens into discriminated senses, use the clusters to
re-label the corpus’ tokens according to sense, and
then learn embeddings for these re-labeled words.
The second paper improves upon the first by em-
ploying an earlier pass of non-discriminated em-
bedding learning to obtain vectors used to rep-
resent the contexts. Note that by pre-clustering,
these methods lose the opportunity to jointly learn
the sense-discriminated vectors and the cluster-
ing. Other weaknesses include their fixed num-
ber of sense per word type, and the computational
expense of the two-step process—the Huang et
al (2012) method took one week of computation
to learn multiple embeddings for a 6,000 subset
of the 30,000 vocabulary on a corpus containing
close to billion tokens.3
This paper presents a new method for learn-
ing vector-space embeddings for multiple senses
per word type, designed to provide several ad-
vantages over previous approaches. (1) Sense-
discriminated vectors are learned jointly with the
assignment of token contexts to senses; thus we
can use the emerging sense representation to more
accurately perform the clustering. (2) A non-
parametric variant of our method automatically
discovers a varying number of senses per word
type. (3) Efficient online joint training makes
it fast and scalable. We refer to our method as
Multiple-sense Skip-gram, or MSSG, and its non-
parametric counterpart as NP-MSSG.
Our method builds on the Skip-gram model
(Mikolov et al, 2013a), but maintains multiple
vectors per word type. During online training
with a particular token, we use the average of its
context words’ vectors to select the token’s sense
that is closest, and perform a gradient update on
that sense. In the non-parametric version of our
method, we build on facility location (Meyerson,
2001): a new cluster is created with probability
proportional to the distance from the context to the
</bodyText>
<footnote confidence="0.582937">
3Personal communication with authors Eric H. Huang and
Richard Socher.
</footnote>
<bodyText confidence="0.998344764705882">
nearest sense.
We present experimental results demonstrating
the benefits of our approach. We show quali-
tative improvements over single-sense Skip-gram
and Huang et al (2012), comparing against word
neighbors from our parametric and non-parametric
methods. We present quantitative results in three
tasks. On both the SCWS and WordSim353 data
sets our methods surpass the previous state-of-
the-art. The Google Analogy task is not espe-
cially well-suited for word-sense evaluation since
its lack of context makes selecting the sense dif-
ficult; however our method dramatically outper-
forms Huang et al (2012) on this task. Finally
we also demonstrate scalabilty, learning multiple
senses, training on nearly a billion tokens in less
than 6 hours—a 27x improvement on Huang et al.
</bodyText>
<sectionHeader confidence="0.999782" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.99993671875">
Much prior work has focused on learning vector
representations of words; here we will describe
only those most relevant to understanding this pa-
per. Our work is based on neural language mod-
els, proposed by Bengio et al (2003), which extend
the traditional idea of n-gram language models by
replacing the conditional probability table with a
neural network, representing each word token by
a small vector instead of an indicator variable, and
estimating the parameters of the neural network
and these vectors jointly. Since the Bengio et al
(2003) model is quite expensive to train, much re-
search has focused on optimizing it. Collobert and
Weston (2008) replaces the max-likelihood char-
acter of the model with a max-margin approach,
where the network is encouraged to score the cor-
rect n-grams higher than randomly chosen incor-
rect n-grams. Mnih and Hinton (2007) replaces
the global normalization of the Bengio model with
a tree-structured probability distribution, and also
considers multiple positions for each word in the
tree.
More relevantly, Mikolov et al (2013a) and
Mikolov et al (2013b) propose extremely com-
putationally efficient log-linear neural language
models by removing the hidden layers of the neu-
ral networks and training from larger context win-
dows with very aggressive subsampling. The
goal of the models in Mikolov et al (2013a) and
Mikolov et al (2013b) is not so much obtain-
ing a low-perplexity language model as learn-
ing word representations which will be useful in
</bodyText>
<page confidence="0.973837">
1060
</page>
<bodyText confidence="0.999884051282051">
downstream tasks. Neural networks or log-linear
models also do not appear to be necessary to
learn high-quality word embeddings, as Dhillon
and Ungar (2011) estimate word vector repre-
sentations using Canonical Correlation Analysis
(CCA).
Word vector representations or embeddings
have been used in various NLP tasks such
as named entity recognition (Neelakantan and
Collins, 2014; Passos et al, 2014; Turian et al,
2010), dependency parsing (Bansal et al, 2014),
chunking (Turian et al, 2010; Dhillon and Ungar,
2011), sentiment analysis (Maas et al, 2011), para-
phrase detection (Socher et al, 2011) and learning
representations of paragraphs and documents (Le
and Mikolov, 2014). The word clusters obtained
from Brown clustering (Brown et al, 1992) have
similarly been used as features in named entity
recognition (Miller et al, 2004; Ratinov and Roth,
2009) and dependency parsing (Koo et al, 2008),
among other tasks.
There is considerably less prior work on learn-
ing multiple vector representations for the same
word type. Reisinger and Mooney (2010a) intro-
duce a method for constructing multiple sparse,
high-dimensional vector representations of words.
Huang et al (2012) extends this approach incor-
porating global document context to learn mul-
tiple dense, low-dimensional embeddings by us-
ing recursive neural networks. Both the meth-
ods perform word sense discrimination as a pre-
processing step by clustering contexts for each
word type, making training more expensive.
While methods such as those described in Dhillon
and Ungar (2011) and Reddy et al (2011) use
token-specific representations of words as part
of the learning algorithm, the final outputs are
still one-to-one mappings between word types and
word embeddings.
</bodyText>
<sectionHeader confidence="0.98415" genericHeader="method">
3 Background: Skip-gram model
</sectionHeader>
<bodyText confidence="0.99966175">
The Skip-gram model learns word embeddings
such that they are useful in predicting the sur-
rounding words in a sentence. In the Skip-gram
model, v(w) ∈ Rd is the vector representation of
the word w ∈ W, where W is the words vocabu-
lary and d is the embedding dimensionality.
Given a pair of words (wt, c), the probability
that the word c is observed in the context of word
</bodyText>
<equation confidence="0.915461333333333">
wt is given by,
1
P(D = 1|v(wt), v(c)) = 1 + e−v(wt)T v(c) (1)
</equation>
<bodyText confidence="0.996">
The probability of not observing word c in the con-
text of wt is given by,
</bodyText>
<equation confidence="0.9997925">
P(D = 0|v(wt), v(c)) =
1 − P(D = 1|v(wt), v(c))
</equation>
<bodyText confidence="0.9990845">
Given a training set containing the sequence of
word types w1, w2, ... , wT, the word embeddings
are learned by maximizing the following objective
function:
</bodyText>
<equation confidence="0.999479">
J(θ) = 1: 1: log P(D = 1|v(wt), v(c))
(wt,ct)∈D+ c∈ct
+ 1: 1: log P(D = 0|v(wt), v(c0))
(wt,ct)∈D− c&apos;∈ct
</equation>
<bodyText confidence="0.99989865">
where wt is the tth word in the training set, ct
is the set of observed context words of word wt
and c0t is the set of randomly sampled, noisy con-
text words for the word wt. D+ consists of
the set of all observed word-context pairs (wt, ct)
(t = 1, 2 ..., T). D− consists of pairs (wt, c0t)
(t = 1, 2 ..., T) where c0t is the set of randomly
sampled, noisy context words for the word wt.
For each training word wt, the set of context
words ct = {wt−Rt, ... , wt−1, wt+1, ... , wt+Rt}
includes Rt words to the left and right of the given
word as shown in Figure 1. Rt is the window size
considered for the word wt uniformly randomly
sampled from the set {1, 2, ... , N}, where N is
the maximum context window size.
The set of noisy context words c0t for the word
wt is constructed by randomly sampling S noisy
context words for each word in the context ct. The
noisy context words are randomly sampled from
the following distribution,
</bodyText>
<equation confidence="0.998038">
P(w) = punigram(w)3/4 (2)
Z
</equation>
<bodyText confidence="0.9999585">
where punigram(w) is the unigram distribution of
the words and Z is the normalization constant.
</bodyText>
<sectionHeader confidence="0.97698" genericHeader="method">
4 Multi-Sense Skip-gram (MSSG) model
</sectionHeader>
<bodyText confidence="0.999083">
To extend the Skip-gram model to learn multiple
embeddings per word we follow previous work
(Huang et al, 2012; Reisinger and Mooney, 2010a)
</bodyText>
<page confidence="0.987607">
1061
</page>
<figureCaption confidence="0.984537333333333">
Figure 1: Architecture of the Skip-gram model
with window size Rt = 2. Context ct of word
wt consists of wt−1, wt−2, wt+1, wt+2.
</figureCaption>
<bodyText confidence="0.999685666666667">
and let each sense of word have its own embed-
ding, and induce the senses by clustering the em-
beddings of the context words around each token.
The vector representation of the context is the av-
erage of its context words’ vectors. For every word
type, we maintain clusters of its contexts and the
sense of a word token is predicted as the cluster
that is closest to its context representation. After
predicting the sense of a word token, we perform
a gradient update on the embedding of that sense.
The crucial difference from previous approaches
is that word sense discrimination and learning em-
beddings are performed jointly by predicting the
sense of the word using the current parameter es-
timates.
In the MSSG model, each word w E W is
associated with a global vector vg(w) and each
sense of the word has an embedding (sense vec-
tor) vs(w, k) (k = 1, 2, ... , K) and a context clus-
ter with center µ(w, k) (k = 1, 2, ... , K). The K
sense vectors and the global vectors are of dimen-
sion d and K is a hyperparameter.
Consider the word wt and let ct =
{wt−Rt, . . . , wt−1, wt+1, ... , wt+Rt} be the
set of observed context words. The vector repre-
sentation of the context is defined as the average
of the global vector representation of the words in
the context. Let vcontext(ct) = 2*1Rt EcEct vg(c)
be the vector representation of the context ct. We
use the global vectors of the context words instead
of its sense vectors to avoid the computational
complexity associated with predicting the sense
of the context words. We predict st, the sense
</bodyText>
<figure confidence="0.803918">
Context Context Cluster Word Sense Context
Vectors Centers Vectors Vectors
</figure>
<figureCaption confidence="0.978506">
Figure 2: Architecture of Multi-Sense Skip-gram
</figureCaption>
<bodyText confidence="0.853776">
(MSSG) model with window size Rt = 2 and
K = 3. Context ct of word wt consists of
wt−1, wt−2, wt+1, wt+2. The sense is predicted by
finding the cluster center of the context that is clos-
est to the average of the context vectors.
of word wt when observed with context ct as
the context cluster membership of the vector
vcontext(ct) as shown in Figure 2. More formally,
</bodyText>
<equation confidence="0.9987795">
st = arg max sim(µ(wt, k), vcontext(ct)) (3)
k=1,2,...,K
</equation>
<bodyText confidence="0.996996625">
The hard cluster assignment is similar to the k-
means algorithm. The cluster center is the aver-
age of the vector representations of all the contexts
which belong to that cluster. For sim we use co-
sine similarity in our experiments.
Here, the probability that the word c is observed
in the context of word wt given the sense of the
word wt is,
</bodyText>
<equation confidence="0.9999705">
P(D = 1|st,vs(wt,1), ... , vs(wt, K), vg(c))
= P(D = 1|vs(wt, st), vg(c))
1
1 + e−vs(wt,st)T vg(c)
</equation>
<bodyText confidence="0.9987285">
The probability of not observing word c in the con-
text of wt given the sense of the word wt is,
</bodyText>
<equation confidence="0.982613">
P(D = 0|st,vs(wt,1), ... , vs(wt, K), vg(c))
= P(D = 0|vs(wt, st), vg(c))
= 1 − P(D = 1|vs(wt, st), vg(c))
</equation>
<bodyText confidence="0.999905666666667">
Given a training set containing the sequence of
word types w1, w2, ..., wT, the word embeddings
are learned by maximizing the following objective
</bodyText>
<figure confidence="0.965108129032258">
Word
Vector
Context
Vectors
v(wt+2)
v(wt)
v(wt+1)
word wt
v(wt-1)
v(wt-2)
Average Context
Vector
vcontext(ct)
vg(wt+2)
vg(wt+1)
v(wt,1)
v(wt,2)
μ(wt,3)
Predicted
Sense st
(wt,3)
μ(wt,1)
μ(wt,2)
vg(wt-1)
vg(wt-2)
vg(wt+2)
vg(wt+1)
vg(wt-1)
vg(wt-2)
1062
Algorithm 1 Training Algorithm of MSSG model
</figure>
<listItem confidence="0.952817">
1: Input: w1, w2, ..., wT, d, K, N.
2: Initialize vs(w, k) and vg(w), bw E W, k E
{1, . . . ,K} randomly, µ(w, k) bw E W, k E
{1,...,K} to 0.
3: for t = 1, 2, ... , T do
4: Rt ∼ {1,...,N}
5: ct = {wt−Rt, ... , wt−1, wt+1, ... , wt+Rt}
6: vcontext(ct) = 2∗Rt Pc∈ct vg(c)
7: st = arg maxk=1,2,...,K {
sim(µ(wt, k), vcontext(ct))}
8: Update context cluster center µ(wt, st)
since context ct is added to context cluster st
of word wt.
9: c0t = Noisy Samples(ct)
10: Gradient update on vs(wt, st), global vec-
tors of words in ct and c0t.
11: end for
12: Output: vs(w, k), vg(w) and context cluster
centers µ(w, k), bw E W, k E {1, ... , K}
</listItem>
<equation confidence="0.91035475">
function:
log P(D = 1|vs(wt, st), vg(c))+
X X log P(D = 0|vs(wt, st), vg(c0))
(wt,ct)∈D− c&apos;∈ct
</equation>
<bodyText confidence="0.999976384615385">
where wt is the tth word in the sequence, ct is the
set of observed context words and c0t is the set of
noisy context words for the word wt. D+ and D−
are constructed in the same way as in the Skip-
gram model.
After predicting the sense of word wt, we up-
date the embedding of the predicted sense for
the word wt (vs(wt, st)), the global vector of the
words in the context and the global vector of the
randomly sampled, noisy context words. The con-
text cluster center of cluster st for the word wt
(µ(wt, st)) is updated since context ct is added to
the cluster st.
</bodyText>
<sectionHeader confidence="0.964661" genericHeader="method">
5 Non-Parametric MSSG model
(NP-MSSG)
</sectionHeader>
<bodyText confidence="0.9994751">
The MSSG model learns a fixed number of senses
per word type. In this section, we describe a
non-parametric version of MSSG, the NP-MSSG
model, which learns varying number of senses per
word type. Our approach is closely related to
the online non-parametric clustering procedure de-
scribed in Meyerson (2001). We create a new clus-
ter (sense) for a word type with probability propor-
tional to the distance of its context to the nearest
cluster (sense).
Each word w E W is associated with sense vec-
tors, context clusters and a global vector vg(w) as
in the MSSG model. The number of senses for a
word is unknown and is learned during training.
Initially, the words do not have sense vectors and
context clusters. We create the first sense vector
and context cluster for each word on its first occur-
rence in the training data. After creating the first
context cluster for a word, a new context cluster
and a sense vector are created online during train-
ing when the word is observed with a context were
the similarity between the vector representation of
the context with every existing cluster center of the
word is less than A, where A is a hyperparameter
of the model.
Consider the word wt and let ct =
{wt−Rt, ... , wt−1, wt+1, . . . , wt+Rt} be the
set of observed context words. The vector repre-
sentation of the context is defined as the average
of the global vector representation of the words in
</bodyText>
<equation confidence="0.547870666666667">
P
the context. Let vcontext(ct) = 1 c∈ct vg(c)
2∗Rt
</equation>
<bodyText confidence="0.9973248">
be the vector representation of the context ct. Let
k(wt) be the number of context clusters or the
number of senses currently associated with word
wt. st, the sense of word wt when k(wt) &gt; 0 is
given by
</bodyText>
<equation confidence="0.843395">
k(wt) + 1, ifmaxk=1,2,...,k(wt){sim
(µ(wt, k), vcontext(ct))} &lt; A
kmax, otherwise
(4)
</equation>
<bodyText confidence="0.999409142857143">
where µ(wt, k) is the cluster center of
the kth cluster of word wt and kmax =
arg maxk=1,2,...,k(wt) sim(µ(wt, k), vcontext(ct)).
The cluster center is the average of the vector
representations of all the contexts which belong to
that cluster. If st = k(wt) + 1, a new context
cluster and a new sense vector are created for the
word wt.
The NP-MSSG model and the MSSG model
described previously differ only in the way word
sense discrimination is performed. The objec-
tive function and the probabilistic model associ-
ated with observing a (word, context) pair given
the sense of the word remain the same.
</bodyText>
<equation confidence="0.904727888888889">
J(θ) =
X
(wt,ct)∈D+
X
c∈ct
⎧
⎨⎪
⎪⎩
st =
</equation>
<page confidence="0.809897">
1063
</page>
<table confidence="0.99859275">
Model Time (in hours)
Huang et al 168
MSSG 50d 1
MSSG-300d 6
NP-MSSG-50d 1.83
NP-MSSG-300d 5
Skip-gram-50d 0.33
Skip-gram-300d 1.5
</table>
<tableCaption confidence="0.999532">
Table 1: Training Time Results. First five model
</tableCaption>
<bodyText confidence="0.51549475">
reported in the table are capable of learning mul-
tiple embeddings for each word and Skip-gram
is capable of learning only single embedding for
each word.
</bodyText>
<sectionHeader confidence="0.999033" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<bodyText confidence="0.999886705882353">
To evaluate our algorithms we train embeddings
using the same corpus and vocabulary as used in
Huang et al (2012), which is the April 2010 snap-
shot of the Wikipedia corpus (Shaoul and West-
bury, 2010). It contains approximately 2 million
articles and 990 million tokens. In all our experi-
ments we remove all the words with less than 20
occurrences and use a maximum context window
(N) of length 5 (5 words before and after the word
occurrence). We fix the number of senses (K) to
be 3 for the MSSG model unless otherwise speci-
fied. Our hyperparameter values were selected by
a small amount of manual exploration on a vali-
dation set. In NP-MSSG we set A to -0.5. The
Skip-gram model, MSSG and NP-MSSG models
sample one noisy context word (S) for each of the
observed context words. We train our models us-
ing AdaGrad stochastic gradient decent (Duchi et
al, 2011) with initial learning rate set to 0.025.
Similarly to Huang et al (2012), we don’t use a
regularization penalty.
Below we describe qualitative results, display-
ing the embeddings and the nearest neighbors of
each word sense, and quantitative experiments in
two benchmark word similarity tasks.
Table 1 shows time to train our models, com-
pared with other models from previous work. All
these times are from single-machine implementa-
tions running on similar-sized corpora. We see
that our model shows significant improvement in
the training time over the model in Huang et
al (2012), being within well within an order-of-
magnitude of the training time for Skip-gram mod-
els.
</bodyText>
<table confidence="0.986270525">
APPLE
Skip-gram blackberry, macintosh, acorn, pear, plum
MSSG pear, honey, pumpkin, potato, nut
microsoft, activision, sony, retail, gamestop
macintosh, pc, ibm, iigs, chipsets
NP-MSSG apricot, blackberry, cabbage, blackberries, pear
microsoft, ibm, wordperfect, amiga, trs-80
FOX
Skip-gram abc, nbc, soapnet, espn, kttv
MSSG beaver, wolf, moose, otter, swan
nbc, espn, cbs, ctv, pbs
dexter, myers, sawyer, kelly, griffith
NP-MSSG rabbit, squirrel, wolf, badger, stoat
cbs,abc, nbc, wnyw, abc-tv
NET
Skip-gram profit, dividends, pegged, profits, nets
MSSG snap, sideline, ball, game-trying, scoring
negative, offset, constant, hence, potential
pre-tax, billion, revenue, annualized, us$
NP-MSSG negative, total, transfer, minimizes, loop
pre-tax, taxable, per, billion, us$, income
ball, yard, fouled, bounced, 50-yard
wnet, tvontorio, cable, tv, tv-5
ROCK
Skip-gram glam, indie, punk, band, pop
MSSG rocks, basalt, boulders, sand, quartzite
alternative, progressive, roll, indie, blues-rock
rocks, pine, rocky, butte, deer
NP-MSSG granite, basalt, outcropping, rocks, quartzite
alternative, indie, pop/rock, rock/metal, blues-rock
RUN
Skip-gram running, ran, runs, afoul, amok
MSSG running, stretch, ran, pinch-hit, runs
operated, running, runs, operate, managed
running, runs, operate, drivers, configure
NP-MSSG two-run, walk-off, runs, three-runs, starts
operated, runs, serviced, links, walk
running, operating, ran, go, configure
re-election, reelection, re-elect, unseat, term-limited
helmed, longest-running, mtv, promoted, produced
</table>
<tableCaption confidence="0.988581">
Table 2: Nearest neighbors of each sense of each
</tableCaption>
<bodyText confidence="0.916299">
word, by cosine similarity, for different algo-
rithms. Note that the different senses closely cor-
respond to intuitions regarding the senses of the
given word types.
</bodyText>
<subsectionHeader confidence="0.996075">
6.1 Nearest Neighbors
</subsectionHeader>
<bodyText confidence="0.999956692307692">
Table 2 shows qualitatively the results of dis-
covering multiple senses by presenting the near-
est neighbors associated with various embeddings.
The nearest neighbors of a word are computed by
comparing the cosine similarity between the em-
bedding for each sense of the word and the context
embeddings of all other words in the vocabulary.
Note that each of the discovered senses are indeed
semantically coherent, and that a reasonable num-
ber of senses are created by the non-parametric
method. Table 3 shows the nearest neighbors of
the word plant for Skip-gram, MSSG , NP-MSSG
and Haung’s model (Huang et al, 2012).
</bodyText>
<page confidence="0.961087">
1064
</page>
<table confidence="0.900389238095238">
Skip- plants, flowering, weed, fungus, biomass
gram
MS plants, tubers, soil, seed, biomass
-SG refinery, reactor, coal-fired, factory, smelter
asteraceae, fabaceae, arecaceae, lamiaceae, eri-
caceae
NP plants, seeds, pollen, fungal, fungus
MS factory, manufacturing, refinery, bottling, steel
-SG fabaceae, legume, asteraceae, apiaceae, flowering
power, coal-fired, hydro-power, hydroelectric, re-
finery
Hua insect, capable, food, solanaceous, subsurface
-ng robust, belong, pitcher, comprises, eagles
et al food, animal, catching, catch, ecology, fly
seafood, equipment, oil, dairy, manufacturer
facility, expansion, corporation, camp, co.
treatment, skin, mechanism, sugar, drug
facility, theater, platform, structure, storage
natural, blast, energy, hurl, power
matter, physical, certain, expression, agents
vine, mute, chalcedony, quandong, excrete
</table>
<tableCaption confidence="0.921282">
Table 3: Nearest Neighbors of the word plant
</tableCaption>
<bodyText confidence="0.97260925">
for different models. We see that the discovered
senses in both our models are more semantically
coherent than Huang et al (2012) and NP-MSSG
is able to learn reasonable number of senses.
</bodyText>
<subsectionHeader confidence="0.999865">
6.2 Word Similarity
</subsectionHeader>
<bodyText confidence="0.976543351351351">
We evaluate our embeddings on two related
datasets: the WordSim-353 (Finkelstein et al,
2001) dataset and the Contextual Word Similari-
ties (SCWS) dataset Huang et al (2012).
WordSim-353 is a standard dataset for evaluat-
ing word vector representations. It consists of a
list of pairs of word types, the similarity of which
is rated in an integral scale from 1 to 10. Pairs
include both monosemic and polysemic words.
These scores to each word pairs are given with-
out any contextual information, which makes them
tricky to interpret.
To overcome this issue, Stanford’s Contextual
Word Similarities (SCWS) dataset was developed
by Huang et al (2012). The dataset consists of
2003 word pairs and their sentential contexts. It
consists of 1328 noun-noun pairs, 399 verb-verb
pairs, 140 verb-noun, 97 adjective-adjective, 30
noun-adjective, 9 verb-adjective, and 241 same-
word pairs. We evaluate and compare our embed-
dings on both WordSim-353 and SCWS word sim-
ilarity corpus.
Since it is not trivial to deal with multiple em-
beddings per word, we consider the following sim-
ilarity measures between words w and w&apos; given
their respective contexts c and c&apos;, where P(w, c, k)
is the probability that w takes the kth sense given
the context c, and d(vs(w, i), vs(w&apos;, j)) is the sim-
ilarity measure between the given embeddings
vs(w, i) and vs(w&apos;, j).
The avgSim metric,
avgSim(w, w&apos;)
d (vs(w, i), vs(w&apos;, j)) ,
computes the average similarity over all embed-
dings for each word, ignoring information from
the context.
To address this, the avgSimC metric,
</bodyText>
<equation confidence="0.891956">
K K
avgSimC(w, w&apos;) = P(w, c, i)P(w&apos;, c&apos;, j)
j=1 i=1
× d (vs(w, i), vs(w&apos;, j))
</equation>
<bodyText confidence="0.983914096774194">
weighs the similarity between each pair of senses
by how well does each sense fit the context at
hand.
The globalSim metric uses each word’s global
context vector, ignoring the many senses:
globalSim(w, w&apos;) = d (vy(w), vy(w&apos;)) .
Finally, localSim metric selects a single sense
for each word based independently on its context
and computes the similarity by
localSim(w, w&apos;) = d (vs(w, k), vs(w&apos;, k&apos;)) ,
where k = arg maxi P(w, c, i) and k&apos; =
arg maxj P(w&apos;, c&apos;, j) and P(w, c, i) is the prob-
ability that w takes the ith sense given context c.
The probability of being in a cluster is calculated
as the inverse of the cosine distance to the cluster
center (Huang et al, 2012).
We report the Spearman correlation between a
model’s similarity scores and the human judge-
ments in the datasets.
Table 5 shows the results on WordSim-353
task. C&amp;W refers to the language model by Col-
lobert and Weston (2008) and HLBL model is the
method described in Mnih and Hinton (2007). On
WordSim-353 task, we see that our model per-
forms significantly better than the previous neural
network model for learning multi-representations
per word (Huang et al, 2012). Among the meth-
ods that learn low-dimensional and dense repre-
sentations, our model performs slightly better than
Skip-gram. Table 4 shows the results for the
SCWS task. In this task, when the words are
</bodyText>
<equation confidence="0.7173105">
1 K K
K2 i=1 j=1
</equation>
<page confidence="0.701663">
1065
</page>
<table confidence="0.999258818181818">
Model globalSim avgSim avgSimC localSim
TF-IDF 26.3 - - -
Collobort &amp; Weston-50d 57.0 - - -
Skip-gram-50d 63.4 - - -
Skip-gram-300d 65.2 - - -
Pruned TF-IDF 62.5 60.4 60.5 -
Huang et al-50d 58.6 62.8 65.7 26.1
MSSG-50d 62.1 64.2 66.9 49.17
MSSG-300d 65.3 67.2 69.3 57.26
NP-MSSG-50d 62.3 64.0 66.1 50.27
NP-MSSG-300d 65.5 67.3 69.1 59.80
</table>
<tableCaption confidence="0.991952">
Table 4: Experimental results in the SCWS task. The numbers are Spearmans correlation p × 100
</tableCaption>
<bodyText confidence="0.941152333333333">
between each model’s similarity judgments and the human judgments, in context. First three models
learn only a single embedding per model and hence, avgSim, avgSimC and localSim are not reported
for these models, as they’d be identical to globalSim. Both our parametric and non-parametric models
outperform the baseline models, and our best model achieves a score of 69.3 in this task. NP-MSSG
achieves the best results when globalSim, avgSim and localSim similarity measures are used. The best
results according to each metric are in bold face.
</bodyText>
<table confidence="0.999796705882353">
Model p × 100
HLBL 33.2
C&amp;W 55.3
Skip-gram-300d 70.4
Huang et al-G 22.8
Huang et al-M 64.2
MSSG 50d-G 60.6
MSSG 50d-M 63.2
MSSG 300d-G 69.2
MSSG 300d-M 70.9
NP-MSSG 50d-G 61.5
NP-MSSG 50d-M 62.4
NP-MSSG 300d-G 69.1
NP-MSSG 300d-M 68.6
Pruned TF-IDF 73.4
ESA 75
Tiered TF-IDF 76.9
</table>
<tableCaption confidence="0.990582">
Table 5: Results on the WordSim-353 dataset.
</tableCaption>
<bodyText confidence="0.9994815">
The table shows the Spearmans correlation p be-
tween the model’s similarities and human judg-
ments. G indicates the globalSim similarity mea-
sure and M indicates avgSim measure.The best
results among models that learn low-dimensional
and dense representations are in bold face. Pruned
TF-IDF (Reisinger and Mooney, 2010a), ESA
(Gabrilovich and Markovitch, 2007) and Tiered
TF-IDF (Reisinger and Mooney, 2010b) construct
spare, high-dimensional representations.
</bodyText>
<figureCaption confidence="0.67051">
Figure 3: The plot shows the distribution of num-
</figureCaption>
<bodyText confidence="0.973414555555555">
ber of senses learned per word type in NP-MSSG
model
given with their context, our model achieves new
state-of-the-art results on SCWS as shown in the
Table-4. The previous state-of-art model (Huang
et al, 2012) on this task achieves 65.7% using
the avgSimC measure, while the MSSG model
achieves the best score of 69.3% on this task. The
results on the other metrics are similar. For a
fixed embedding dimension, the model by Huang
et al (2012) has more parameters than our model
since it uses a hidden layer. The results show
that our model performs better than Huang et al
(2012) even when both the models use 50 dimen-
sional vectors and the performance of our model
improves as we increase the number of dimensions
to 300.
We evaluate the models in a word analogy task
</bodyText>
<page confidence="0.965187">
1066
</page>
<figure confidence="0.999072">
(a) (b)
</figure>
<figureCaption confidence="0.992426">
Figure 4: Figures (a) and (b) show the effect of varying embedding dimensionality and number of senses
respectively of the MSSG Model on the SCWS task.
</figureCaption>
<table confidence="0.999693285714286">
Model Task Sim p × 100
Skip-gram WS-353 globalSim 70.4
MSSG WS-353 globalSim 68.4
MSSG WS-353 avgSim 71.2
NP MSSG WS-353 globalSim 68.3
NP MSSG WS-353 avgSim 69.66
MSSG SCWS localSim 59.3
MSSG SCWS globalSim 64.7
MSSG SCWS avgSim 67.2
MSSG SCWS avgSimC 69.2
NP MSSG SCWS localSim 60.11
NP MSSG SCWS globalSim 65.3
NP MSSG SCWS avgSim 67
NP MSSG SCWS avgSimC 68.6
</table>
<tableCaption confidence="0.953516">
Table 6: Experiment results on WordSim-353 and
</tableCaption>
<bodyText confidence="0.992450827586207">
SCWS Task. Multiple Embeddings are learned for
top 30,000 most frequent words in the vocabulary.
The embedding dimension size is 300 for all the
models for this task. The number of senses for
MSSG model is 3.
introduced by Mikolov et al (2013a) where both
MSSG and NP-MSSG models achieve 64% accu-
racy compared to 12% accuracy by Huang et al
(2012). Skip-gram which is the state-of-art model
for this task achieves 67% accuracy.
Figure 3 shows the distribution of number of
senses learned per word type in the NP-MSSG
model. We learn the multiple embeddings for the
same set of approximately 6000 words that were
used in Huang et al (2012) for all our experiments
to ensure fair comparision. These approximately
6000 words were choosen by Huang et al. mainly
from the top 30,00 frequent words in the vocab-
ulary. This selection was likely made to avoid
the noise of learning multiple senses for infre-
quent words. However, our method is robust to
noise, which can be seen by the good performance
of our model that learns multiple embeddings for
the top 30,000 most frequent words. We found
that even by learning multiple embeddings for the
top 30,000 most frequent words in the vocubu-
lary, MSSG model still achieves state-of-art result
on SCWS task with an avgSimC score of 69.2 as
shown in Table 6.
</bodyText>
<sectionHeader confidence="0.998509" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999724333333333">
We present an extension to the Skip-gram model
that efficiently learns multiple embeddings per
word type. The model jointly performs word
sense discrimination and embedding learning, and
non-parametrically estimates the number of senses
per word type. Our method achieves new state-
of-the-art results in the word similarity in con-
text task and learns multiple senses, training on
close to billion tokens in less than 6 hours. The
global vectors, sense vectors and cluster centers of
our model and code for learning them are avail-
able at https://people.cs.umass.edu/
˜arvind/emnlp2014wordvectors. In fu-
ture work we plan to use the multiple embeddings
per word type in downstream NLP tasks.
</bodyText>
<page confidence="0.990458">
1067
</page>
<sectionHeader confidence="0.998303" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9999455">
This work was supported in part by the Center
for Intelligent Information Retrieval and in part by
DARPA under agreement number FA8750-13-2-
0020. The U.S. Government is authorized to re-
produce and distribute reprints for Governmental
purposes notwithstanding any copyright notation
thereon. Any opinions, findings and conclusions
or recommendations expressed in this material are
those of the authors and do not necessarily reflect
those of the sponsor.
</bodyText>
<sectionHeader confidence="0.998471" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.995845983471074">
Mohit Bansal, Kevin Gimpel, and Karen Livescu.
2014. Tailoring Continuous Word Representations
for Dependency Parsing. Association for Computa-
tional Linguistics (ACL).
Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Re-
search (JMLR).
Peter F. Brown, Peter V. Desouza, Robert L. Mercer,
Vincent J. Della Pietra, and Jenifer C. Lai. 1992.
Class-based N-gram models of natural language
Computational Linguistics.
Ronan Collobert and Jason Weston. 2008. A Uni-
fied Architecture for Natural Language Process-
ing: Deep Neural Networks with Multitask Learn-
ing. International Conference on Machine learning
(ICML).
Paramveer S. Dhillon, Dean Foster, and Lyle Ungar.
2011. Multi-View Learning of Word Embeddings via
CCA. Advances in Neural Information Processing
Systems (NIPS).
John Duchi, Elad Hazan, and Yoram Singer 2011.
Adaptive sub- gradient methods for online learn-
ing and stochastic optimization. Journal of Machine
Learning Research (JMLR).
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan
Ruppin. 2001. Placing search in context: the con-
cept revisited. International Conference on World
Wide Web (WWW).
Evgeniy Gabrilovich and Shaul Markovitch. 2007.
Computing semantic relatedness using wikipedia-
based explicit semantic analysis. International Joint
Conference on Artificial Intelligence (IJCAI).
Eric H. Huang, Richard Socher, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Improving Word
Representations via Global Context and Multiple
Word Prototypes. Association of Computational
Linguistics (ACL).
Terry Koo, Xavier Carreras, and Michael Collins.
2008. Simple Semi-supervised Dependency Parsing.
Association for Computational Linguistics (ACL).
Quoc V. Le and Tomas Mikolov. 2014 Distributed
Representations of Sentences and Documents. Inter-
national Conference on Machine Learning (ICML)
Andrew L. Maas, Raymond E. Daly, Peter T. Pham,
Dan Huang, Andrew Y. Ng, and Christopher Potts.
2011 Learning Word Vectors for Sentiment Analysis
Association for Computational Linguistics (ACL)
Adam Meyerson. 2001 IEEE Symposium on Foun-
dations of Computer Science. International Confer-
ence on Machine Learning (ICML)
Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013a. Efficient Estimation of Word
Representations in Vector Space. Workshop at In-
ternational Conference on Learning Representations
(ICLR).
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-
rado, and Jeffrey Dean. 2013b. Distributed Repre-
sentations of Words and Phrases and their Composi-
tionality. Advances in Neural Information Process-
ing Systems (NIPS).
Tomas Mikolov, Quoc V. Le, and Ilya Sutskever.
2013c. Exploiting Similarities among Languages
for Machine Translation. arXiv.
Scott Miller, Jethran Guinness, and Alex Zamanian.
2004. Name tagging with word clusters and dis-
criminative training. North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies (NAACL-HLT).
Andriy Mnih and Geoffrey Hinton. 2007. Three
new graphical models for statistical language mod-
elling. International Conference on Machine learn-
ing (ICML).
Arvind Neelakantan and Michael Collins. 2014.
Learning Dictionaries for Named Entity Recogni-
tion using Minimal Supervision. European Chap-
ter of the Association for Computational Linguistics
(EACL).
Alexandre Passos, Vineet Kumar, and Andrew McCal-
lum. 2014. Lexicon Infused Phrase Embeddings for
Named Entity Resolution. Conference on Natural
Language Learning (CoNLL).
Lev Ratinov and Dan Roth. 2009. Design Chal-
lenges and Misconceptions in Named Entity Recog-
nition. Conference on Natural Language Learning
(CoNLL).
Siva Reddy, Ioannis P. Klapaftis, and Diana McCarthy.
2011. Dynamic and Static Prototype Vectors for Se-
mantic Composition. International Joint Conference
on Artificial Intelligence (IJCNLP).
1068
Joseph Reisinger and Raymond J. Mooney. 2010a.
Multi-prototype vector-space models of word mean-
ing. North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies (NAACL-HLT)
Joseph Reisinger and Raymond Mooney. 2010b. A
mixture model with sharing for lexical semantics.
Empirical Methods in Natural Language Processing
(EMNLP).
Cyrus Shaoul and Chris Westbury. 2010. The Westbury
lab wikipedia corpus.
Richard Socher, Eric H. Huang, Jeffrey Pennington,
Andrew Y. Ng, and Christopher D. Manning. 2011
Dynamic Pooling and Unfolding Recursive Autoen-
coders for Paraphrase Detection. Advances in Neu-
ral Information Processing Systems (NIPS).
Oscar T¨ackstr¨om, Ryan McDonald, and Jakob Uszkor-
eit. 2012. Cross-lingual Word Clusters for Direct
Transfer of Linguistic Structure. North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies.
Joseph Turian, Lev Ratinov, and Yoshua Bengio.
2010. Word Representations: A Simple and General
Method for Semi-Supervised Learning. Association
for Computational Linguistics (ACL).
Will Y. Zou, Richard Socher, Daniel Cer, and Christo-
pher D. Manning. 2013. Bilingual Word Embed-
dings for Phrase-Based Machine Translation. Em-
pirical Methods in Natural Language Processing.
</reference>
<page confidence="0.985305">
1069
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.937709">
<title confidence="0.9884345">Efficient Non-parametric Estimation Multiple Embeddings per Word in Vector Space</title>
<author confidence="0.991544">Jeevan Alexandre Passos</author>
<author confidence="0.991544">Andrew</author>
<affiliation confidence="0.999984">Department of Computer University of Massachusetts,</affiliation>
<address confidence="0.996732">Amherst, MA,</address>
<abstract confidence="0.998640045454546">There is rising interest in vector-space word embeddings and their use in NLP, especially given recent methods for their fast estimation at very large scale. Nearly all this work, however, assumes a single vector per word type—ignoring polysemy and thus jeopardizing their usefulness for downstream tasks. We present an extension to the Skip-gram model that efficiently learns multiple embeddings per word type. It differs from recent related work by jointly performing word sense discrimination and embedding learning, by non-parametrically estimating the number of senses per word type, and by its efficiency and scalability. We present new state-of-the-art results in the word similarity in context task and demonstrate its scalability by training with one machine on a corpus of nearly 1 billion tokens in less than 6 hours.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Mohit Bansal</author>
<author>Kevin Gimpel</author>
<author>Karen Livescu</author>
</authors>
<title>Tailoring Continuous Word Representations for Dependency Parsing. Association for Computational Linguistics (ACL).</title>
<date>2014</date>
<contexts>
<context position="2904" citStr="Bansal et al (2014)" startWordPosition="434" endWordPosition="437">r many tasks, such as named entity extraction (Miller et al, 2004; Ratinov and Roth, 2009) and parsing (Koo et al, 2008; T¨ackstr¨om et al, 2012). In comparison to Brown clusters, the vector embeddings have the advantages of substantially better scalability in their training, and intriguing potential for their continuous and multi-dimensional interrelations. In fact, Passos et al (2014) present new state-of-the-art results in CoNLL 2003 named entity extraction by directly inputting continuous vector embeddings obtained by a version of Skipgram that injects supervision with lexicons. Similarly Bansal et al (2014) show results in dependency parsing using Skip-gram embeddings. They have also recently been applied to machine translation (Zou et al, 2013; Mikolov et al, 2013c). A notable deficiency in this prior work is that each word type (e.g. the word string plant) has only one vector representation—polysemy and hononymy are ignored. This results in the word plant having an embedding that is approximately the average of its different contextual semantics relating to biology, placement, manufacturing and power generation. In moderately highdimensional spaces a vector can be relatively “close” to multipl</context>
<context position="9174" citStr="Bansal et al, 2014" startWordPosition="1423" endWordPosition="1426">t al (2013a) and Mikolov et al (2013b) is not so much obtaining a low-perplexity language model as learning word representations which will be useful in 1060 downstream tasks. Neural networks or log-linear models also do not appear to be necessary to learn high-quality word embeddings, as Dhillon and Ungar (2011) estimate word vector representations using Canonical Correlation Analysis (CCA). Word vector representations or embeddings have been used in various NLP tasks such as named entity recognition (Neelakantan and Collins, 2014; Passos et al, 2014; Turian et al, 2010), dependency parsing (Bansal et al, 2014), chunking (Turian et al, 2010; Dhillon and Ungar, 2011), sentiment analysis (Maas et al, 2011), paraphrase detection (Socher et al, 2011) and learning representations of paragraphs and documents (Le and Mikolov, 2014). The word clusters obtained from Brown clustering (Brown et al, 1992) have similarly been used as features in named entity recognition (Miller et al, 2004; Ratinov and Roth, 2009) and dependency parsing (Koo et al, 2008), among other tasks. There is considerably less prior work on learning multiple vector representations for the same word type. Reisinger and Mooney (2010a) intro</context>
</contexts>
<marker>Bansal, Gimpel, Livescu, 2014</marker>
<rawString>Mohit Bansal, Kevin Gimpel, and Karen Livescu. 2014. Tailoring Continuous Word Representations for Dependency Parsing. Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>R´ejean Ducharme</author>
<author>Pascal Vincent</author>
<author>Christian Jauvin</author>
</authors>
<title>A neural probabilistic language model.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research (JMLR).</journal>
<contexts>
<context position="1516" citStr="Bengio et al, 2003" startWordPosition="216" endWordPosition="219">s efficiency and scalability. We present new state-of-the-art results in the word similarity in context task and demonstrate its scalability by training with one machine on a corpus of nearly 1 billion tokens in less than 6 hours. 1 Introduction Representing words by dense, real-valued vector embeddings, also commonly called “distributed representations,” helps address the curse of dimensionality and improve generalization because they can place near each other words having similar semantic and syntactic roles. This has been shown dramatically in state-of-the-art results on language modeling (Bengio et al, 2003; Mnih and Hinton, 2007) as well as improvements in other natural language processing tasks (Collobert and Weston, 2008; Turian et al, 2010). Substantial benefit arises when embeddings can be trained on large volumes of data. Hence the recent considerable interest in the CBOW and Skip-gram models *The first two authors contributed equally to this paper. of Mikolov et al (2013a); Mikolov et al (2013b)— relatively simple log-linear models that can be trained to produce high-quality word embeddings on the entirety of English Wikipedia text in less than half a day on one machine. There is rising e</context>
<context position="7442" citStr="Bengio et al (2003)" startWordPosition="1151" endWordPosition="1154">. The Google Analogy task is not especially well-suited for word-sense evaluation since its lack of context makes selecting the sense difficult; however our method dramatically outperforms Huang et al (2012) on this task. Finally we also demonstrate scalabilty, learning multiple senses, training on nearly a billion tokens in less than 6 hours—a 27x improvement on Huang et al. 2 Related Work Much prior work has focused on learning vector representations of words; here we will describe only those most relevant to understanding this paper. Our work is based on neural language models, proposed by Bengio et al (2003), which extend the traditional idea of n-gram language models by replacing the conditional probability table with a neural network, representing each word token by a small vector instead of an indicator variable, and estimating the parameters of the neural network and these vectors jointly. Since the Bengio et al (2003) model is quite expensive to train, much research has focused on optimizing it. Collobert and Weston (2008) replaces the max-likelihood character of the model with a max-margin approach, where the network is encouraged to score the correct n-grams higher than randomly chosen inc</context>
</contexts>
<marker>Bengio, Ducharme, Vincent, Jauvin, 2003</marker>
<rawString>Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and Christian Jauvin. 2003. A neural probabilistic language model. Journal of Machine Learning Research (JMLR).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Peter V Desouza</author>
<author>Robert L Mercer</author>
<author>Vincent J Della Pietra</author>
<author>Jenifer C Lai</author>
</authors>
<title>Class-based N-gram models of natural language Computational Linguistics.</title>
<date>1992</date>
<contexts>
<context position="2248" citStr="Brown et al, 1992" startWordPosition="333" endWordPosition="336"> 2008; Turian et al, 2010). Substantial benefit arises when embeddings can be trained on large volumes of data. Hence the recent considerable interest in the CBOW and Skip-gram models *The first two authors contributed equally to this paper. of Mikolov et al (2013a); Mikolov et al (2013b)— relatively simple log-linear models that can be trained to produce high-quality word embeddings on the entirety of English Wikipedia text in less than half a day on one machine. There is rising enthusiasm for applying these models to improve accuracy in natural language processing, much like Brown clusters (Brown et al, 1992) have become common input features for many tasks, such as named entity extraction (Miller et al, 2004; Ratinov and Roth, 2009) and parsing (Koo et al, 2008; T¨ackstr¨om et al, 2012). In comparison to Brown clusters, the vector embeddings have the advantages of substantially better scalability in their training, and intriguing potential for their continuous and multi-dimensional interrelations. In fact, Passos et al (2014) present new state-of-the-art results in CoNLL 2003 named entity extraction by directly inputting continuous vector embeddings obtained by a version of Skipgram that injects </context>
<context position="9462" citStr="Brown et al, 1992" startWordPosition="1467" endWordPosition="1470"> Dhillon and Ungar (2011) estimate word vector representations using Canonical Correlation Analysis (CCA). Word vector representations or embeddings have been used in various NLP tasks such as named entity recognition (Neelakantan and Collins, 2014; Passos et al, 2014; Turian et al, 2010), dependency parsing (Bansal et al, 2014), chunking (Turian et al, 2010; Dhillon and Ungar, 2011), sentiment analysis (Maas et al, 2011), paraphrase detection (Socher et al, 2011) and learning representations of paragraphs and documents (Le and Mikolov, 2014). The word clusters obtained from Brown clustering (Brown et al, 1992) have similarly been used as features in named entity recognition (Miller et al, 2004; Ratinov and Roth, 2009) and dependency parsing (Koo et al, 2008), among other tasks. There is considerably less prior work on learning multiple vector representations for the same word type. Reisinger and Mooney (2010a) introduce a method for constructing multiple sparse, high-dimensional vector representations of words. Huang et al (2012) extends this approach incorporating global document context to learn multiple dense, low-dimensional embeddings by using recursive neural networks. Both the methods perfor</context>
</contexts>
<marker>Brown, Desouza, Mercer, Pietra, Lai, 1992</marker>
<rawString>Peter F. Brown, Peter V. Desouza, Robert L. Mercer, Vincent J. Della Pietra, and Jenifer C. Lai. 1992. Class-based N-gram models of natural language Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
</authors>
<title>A Unified Architecture for Natural Language Processing:</title>
<date>2008</date>
<booktitle>Deep Neural Networks with Multitask Learning. International Conference on Machine learning (ICML).</booktitle>
<contexts>
<context position="1635" citStr="Collobert and Weston, 2008" startWordPosition="234" endWordPosition="237">d demonstrate its scalability by training with one machine on a corpus of nearly 1 billion tokens in less than 6 hours. 1 Introduction Representing words by dense, real-valued vector embeddings, also commonly called “distributed representations,” helps address the curse of dimensionality and improve generalization because they can place near each other words having similar semantic and syntactic roles. This has been shown dramatically in state-of-the-art results on language modeling (Bengio et al, 2003; Mnih and Hinton, 2007) as well as improvements in other natural language processing tasks (Collobert and Weston, 2008; Turian et al, 2010). Substantial benefit arises when embeddings can be trained on large volumes of data. Hence the recent considerable interest in the CBOW and Skip-gram models *The first two authors contributed equally to this paper. of Mikolov et al (2013a); Mikolov et al (2013b)— relatively simple log-linear models that can be trained to produce high-quality word embeddings on the entirety of English Wikipedia text in less than half a day on one machine. There is rising enthusiasm for applying these models to improve accuracy in natural language processing, much like Brown clusters (Brown</context>
<context position="7870" citStr="Collobert and Weston (2008)" startWordPosition="1219" endWordPosition="1222">d on learning vector representations of words; here we will describe only those most relevant to understanding this paper. Our work is based on neural language models, proposed by Bengio et al (2003), which extend the traditional idea of n-gram language models by replacing the conditional probability table with a neural network, representing each word token by a small vector instead of an indicator variable, and estimating the parameters of the neural network and these vectors jointly. Since the Bengio et al (2003) model is quite expensive to train, much research has focused on optimizing it. Collobert and Weston (2008) replaces the max-likelihood character of the model with a max-margin approach, where the network is encouraged to score the correct n-grams higher than randomly chosen incorrect n-grams. Mnih and Hinton (2007) replaces the global normalization of the Bengio model with a tree-structured probability distribution, and also considers multiple positions for each word in the tree. More relevantly, Mikolov et al (2013a) and Mikolov et al (2013b) propose extremely computationally efficient log-linear neural language models by removing the hidden layers of the neural networks and training from larger </context>
<context position="27453" citStr="Collobert and Weston (2008)" startWordPosition="4553" endWordPosition="4557"> sense for each word based independently on its context and computes the similarity by localSim(w, w&apos;) = d (vs(w, k), vs(w&apos;, k&apos;)) , where k = arg maxi P(w, c, i) and k&apos; = arg maxj P(w&apos;, c&apos;, j) and P(w, c, i) is the probability that w takes the ith sense given context c. The probability of being in a cluster is calculated as the inverse of the cosine distance to the cluster center (Huang et al, 2012). We report the Spearman correlation between a model’s similarity scores and the human judgements in the datasets. Table 5 shows the results on WordSim-353 task. C&amp;W refers to the language model by Collobert and Weston (2008) and HLBL model is the method described in Mnih and Hinton (2007). On WordSim-353 task, we see that our model performs significantly better than the previous neural network model for learning multi-representations per word (Huang et al, 2012). Among the methods that learn low-dimensional and dense representations, our model performs slightly better than Skip-gram. Table 4 shows the results for the SCWS task. In this task, when the words are 1 K K K2 i=1 j=1 1065 Model globalSim avgSim avgSimC localSim TF-IDF 26.3 - - - Collobort &amp; Weston-50d 57.0 - - - Skip-gram-50d 63.4 - - - Skip-gram-300d 6</context>
</contexts>
<marker>Collobert, Weston, 2008</marker>
<rawString>Ronan Collobert and Jason Weston. 2008. A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning. International Conference on Machine learning (ICML).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paramveer S Dhillon</author>
<author>Dean Foster</author>
<author>Lyle Ungar</author>
</authors>
<date>2011</date>
<booktitle>Multi-View Learning of Word Embeddings via CCA. Advances in Neural Information Processing Systems (NIPS).</booktitle>
<marker>Dhillon, Foster, Ungar, 2011</marker>
<rawString>Paramveer S. Dhillon, Dean Foster, and Lyle Ungar. 2011. Multi-View Learning of Word Embeddings via CCA. Advances in Neural Information Processing Systems (NIPS).</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Duchi</author>
<author>Elad Hazan</author>
<author>Yoram Singer</author>
</authors>
<title>Adaptive sub- gradient methods for online learning and stochastic optimization.</title>
<date>2011</date>
<journal>Journal of Machine Learning Research (JMLR).</journal>
<contexts>
<context position="20769" citStr="Duchi et al, 2011" startWordPosition="3527" endWordPosition="3530">les and 990 million tokens. In all our experiments we remove all the words with less than 20 occurrences and use a maximum context window (N) of length 5 (5 words before and after the word occurrence). We fix the number of senses (K) to be 3 for the MSSG model unless otherwise specified. Our hyperparameter values were selected by a small amount of manual exploration on a validation set. In NP-MSSG we set A to -0.5. The Skip-gram model, MSSG and NP-MSSG models sample one noisy context word (S) for each of the observed context words. We train our models using AdaGrad stochastic gradient decent (Duchi et al, 2011) with initial learning rate set to 0.025. Similarly to Huang et al (2012), we don’t use a regularization penalty. Below we describe qualitative results, displaying the embeddings and the nearest neighbors of each word sense, and quantitative experiments in two benchmark word similarity tasks. Table 1 shows time to train our models, compared with other models from previous work. All these times are from single-machine implementations running on similar-sized corpora. We see that our model shows significant improvement in the training time over the model in Huang et al (2012), being within well </context>
</contexts>
<marker>Duchi, Hazan, Singer, 2011</marker>
<rawString>John Duchi, Elad Hazan, and Yoram Singer 2011. Adaptive sub- gradient methods for online learning and stochastic optimization. Journal of Machine Learning Research (JMLR).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lev Finkelstein</author>
<author>Evgeniy Gabrilovich</author>
<author>Yossi Matias</author>
<author>Ehud Rivlin</author>
<author>Zach Solan</author>
<author>Gadi Wolfman</author>
<author>Eytan Ruppin</author>
</authors>
<title>Placing search in context: the concept revisited.</title>
<date>2001</date>
<booktitle>International Conference on World Wide Web (WWW).</booktitle>
<contexts>
<context position="25032" citStr="Finkelstein et al, 2001" startWordPosition="4132" endWordPosition="4135">, dairy, manufacturer facility, expansion, corporation, camp, co. treatment, skin, mechanism, sugar, drug facility, theater, platform, structure, storage natural, blast, energy, hurl, power matter, physical, certain, expression, agents vine, mute, chalcedony, quandong, excrete Table 3: Nearest Neighbors of the word plant for different models. We see that the discovered senses in both our models are more semantically coherent than Huang et al (2012) and NP-MSSG is able to learn reasonable number of senses. 6.2 Word Similarity We evaluate our embeddings on two related datasets: the WordSim-353 (Finkelstein et al, 2001) dataset and the Contextual Word Similarities (SCWS) dataset Huang et al (2012). WordSim-353 is a standard dataset for evaluating word vector representations. It consists of a list of pairs of word types, the similarity of which is rated in an integral scale from 1 to 10. Pairs include both monosemic and polysemic words. These scores to each word pairs are given without any contextual information, which makes them tricky to interpret. To overcome this issue, Stanford’s Contextual Word Similarities (SCWS) dataset was developed by Huang et al (2012). The dataset consists of 2003 word pairs and t</context>
</contexts>
<marker>Finkelstein, Gabrilovich, Matias, Rivlin, Solan, Wolfman, Ruppin, 2001</marker>
<rawString>Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan Ruppin. 2001. Placing search in context: the concept revisited. International Conference on World Wide Web (WWW).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Evgeniy Gabrilovich</author>
<author>Shaul Markovitch</author>
</authors>
<title>Computing semantic relatedness using wikipediabased explicit semantic analysis.</title>
<date>2007</date>
<booktitle>International Joint Conference on Artificial Intelligence (IJCAI).</booktitle>
<contexts>
<context position="29581" citStr="Gabrilovich and Markovitch, 2007" startWordPosition="4906" endWordPosition="4909">.4 Huang et al-G 22.8 Huang et al-M 64.2 MSSG 50d-G 60.6 MSSG 50d-M 63.2 MSSG 300d-G 69.2 MSSG 300d-M 70.9 NP-MSSG 50d-G 61.5 NP-MSSG 50d-M 62.4 NP-MSSG 300d-G 69.1 NP-MSSG 300d-M 68.6 Pruned TF-IDF 73.4 ESA 75 Tiered TF-IDF 76.9 Table 5: Results on the WordSim-353 dataset. The table shows the Spearmans correlation p between the model’s similarities and human judgments. G indicates the globalSim similarity measure and M indicates avgSim measure.The best results among models that learn low-dimensional and dense representations are in bold face. Pruned TF-IDF (Reisinger and Mooney, 2010a), ESA (Gabrilovich and Markovitch, 2007) and Tiered TF-IDF (Reisinger and Mooney, 2010b) construct spare, high-dimensional representations. Figure 3: The plot shows the distribution of number of senses learned per word type in NP-MSSG model given with their context, our model achieves new state-of-the-art results on SCWS as shown in the Table-4. The previous state-of-art model (Huang et al, 2012) on this task achieves 65.7% using the avgSimC measure, while the MSSG model achieves the best score of 69.3% on this task. The results on the other metrics are similar. For a fixed embedding dimension, the model by Huang et al (2012) has mo</context>
</contexts>
<marker>Gabrilovich, Markovitch, 2007</marker>
<rawString>Evgeniy Gabrilovich and Shaul Markovitch. 2007. Computing semantic relatedness using wikipediabased explicit semantic analysis. International Joint Conference on Artificial Intelligence (IJCAI).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric H Huang</author>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Improving Word Representations via Global Context and Multiple Word Prototypes.</title>
<date>2012</date>
<journal>Association of Computational Linguistics (ACL).</journal>
<contexts>
<context position="4423" citStr="Huang et al (2012)" startWordPosition="672" endWordPosition="675">e d, d(a, c) ≤ d(a, b) + d(b, c). 1059 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1059–1069, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics tance not more than the sum of the distances plantpollen and plant–refinery. Fitting the constraints of legitimate continuous gradations of semantics are challenge enough without the additional encumbrance of these illegitimate triangle inequalities. Discovering embeddings for multiple senses per word type is the focus of work by Reisinger and Mooney (2010a) and Huang et al (2012). They both pre-cluster the contexts of a word type’s tokens into discriminated senses, use the clusters to re-label the corpus’ tokens according to sense, and then learn embeddings for these re-labeled words. The second paper improves upon the first by employing an earlier pass of non-discriminated embedding learning to obtain vectors used to represent the contexts. Note that by pre-clustering, these methods lose the opportunity to jointly learn the sense-discriminated vectors and the clustering. Other weaknesses include their fixed number of sense per word type, and the computational expense</context>
<context position="6602" citStr="Huang et al (2012)" startWordPosition="1016" endWordPosition="1019">ring online training with a particular token, we use the average of its context words’ vectors to select the token’s sense that is closest, and perform a gradient update on that sense. In the non-parametric version of our method, we build on facility location (Meyerson, 2001): a new cluster is created with probability proportional to the distance from the context to the 3Personal communication with authors Eric H. Huang and Richard Socher. nearest sense. We present experimental results demonstrating the benefits of our approach. We show qualitative improvements over single-sense Skip-gram and Huang et al (2012), comparing against word neighbors from our parametric and non-parametric methods. We present quantitative results in three tasks. On both the SCWS and WordSim353 data sets our methods surpass the previous state-ofthe-art. The Google Analogy task is not especially well-suited for word-sense evaluation since its lack of context makes selecting the sense difficult; however our method dramatically outperforms Huang et al (2012) on this task. Finally we also demonstrate scalabilty, learning multiple senses, training on nearly a billion tokens in less than 6 hours—a 27x improvement on Huang et al. </context>
<context position="9890" citStr="Huang et al (2012)" startWordPosition="1533" endWordPosition="1536">araphrase detection (Socher et al, 2011) and learning representations of paragraphs and documents (Le and Mikolov, 2014). The word clusters obtained from Brown clustering (Brown et al, 1992) have similarly been used as features in named entity recognition (Miller et al, 2004; Ratinov and Roth, 2009) and dependency parsing (Koo et al, 2008), among other tasks. There is considerably less prior work on learning multiple vector representations for the same word type. Reisinger and Mooney (2010a) introduce a method for constructing multiple sparse, high-dimensional vector representations of words. Huang et al (2012) extends this approach incorporating global document context to learn multiple dense, low-dimensional embeddings by using recursive neural networks. Both the methods perform word sense discrimination as a preprocessing step by clustering contexts for each word type, making training more expensive. While methods such as those described in Dhillon and Ungar (2011) and Reddy et al (2011) use token-specific representations of words as part of the learning algorithm, the final outputs are still one-to-one mappings between word types and word embeddings. 3 Background: Skip-gram model The Skip-gram m</context>
<context position="12498" citStr="Huang et al, 2012" startWordPosition="2011" endWordPosition="2014">nsidered for the word wt uniformly randomly sampled from the set {1, 2, ... , N}, where N is the maximum context window size. The set of noisy context words c0t for the word wt is constructed by randomly sampling S noisy context words for each word in the context ct. The noisy context words are randomly sampled from the following distribution, P(w) = punigram(w)3/4 (2) Z where punigram(w) is the unigram distribution of the words and Z is the normalization constant. 4 Multi-Sense Skip-gram (MSSG) model To extend the Skip-gram model to learn multiple embeddings per word we follow previous work (Huang et al, 2012; Reisinger and Mooney, 2010a) 1061 Figure 1: Architecture of the Skip-gram model with window size Rt = 2. Context ct of word wt consists of wt−1, wt−2, wt+1, wt+2. and let each sense of word have its own embedding, and induce the senses by clustering the embeddings of the context words around each token. The vector representation of the context is the average of its context words’ vectors. For every word type, we maintain clusters of its contexts and the sense of a word token is predicted as the cluster that is closest to its context representation. After predicting the sense of a word token,</context>
<context position="20022" citStr="Huang et al (2012)" startWordPosition="3392" endWordPosition="3395">obabilistic model associated with observing a (word, context) pair given the sense of the word remain the same. J(θ) = X (wt,ct)∈D+ X c∈ct ⎧ ⎨⎪ ⎪⎩ st = 1063 Model Time (in hours) Huang et al 168 MSSG 50d 1 MSSG-300d 6 NP-MSSG-50d 1.83 NP-MSSG-300d 5 Skip-gram-50d 0.33 Skip-gram-300d 1.5 Table 1: Training Time Results. First five model reported in the table are capable of learning multiple embeddings for each word and Skip-gram is capable of learning only single embedding for each word. 6 Experiments To evaluate our algorithms we train embeddings using the same corpus and vocabulary as used in Huang et al (2012), which is the April 2010 snapshot of the Wikipedia corpus (Shaoul and Westbury, 2010). It contains approximately 2 million articles and 990 million tokens. In all our experiments we remove all the words with less than 20 occurrences and use a maximum context window (N) of length 5 (5 words before and after the word occurrence). We fix the number of senses (K) to be 3 for the MSSG model unless otherwise specified. Our hyperparameter values were selected by a small amount of manual exploration on a validation set. In NP-MSSG we set A to -0.5. The Skip-gram model, MSSG and NP-MSSG models sample </context>
<context position="21349" citStr="Huang et al (2012)" startWordPosition="3620" endWordPosition="3623">ic gradient decent (Duchi et al, 2011) with initial learning rate set to 0.025. Similarly to Huang et al (2012), we don’t use a regularization penalty. Below we describe qualitative results, displaying the embeddings and the nearest neighbors of each word sense, and quantitative experiments in two benchmark word similarity tasks. Table 1 shows time to train our models, compared with other models from previous work. All these times are from single-machine implementations running on similar-sized corpora. We see that our model shows significant improvement in the training time over the model in Huang et al (2012), being within well within an order-ofmagnitude of the training time for Skip-gram models. APPLE Skip-gram blackberry, macintosh, acorn, pear, plum MSSG pear, honey, pumpkin, potato, nut microsoft, activision, sony, retail, gamestop macintosh, pc, ibm, iigs, chipsets NP-MSSG apricot, blackberry, cabbage, blackberries, pear microsoft, ibm, wordperfect, amiga, trs-80 FOX Skip-gram abc, nbc, soapnet, espn, kttv MSSG beaver, wolf, moose, otter, swan nbc, espn, cbs, ctv, pbs dexter, myers, sawyer, kelly, griffith NP-MSSG rabbit, squirrel, wolf, badger, stoat cbs,abc, nbc, wnyw, abc-tv NET Skip-gram</context>
<context position="23829" citStr="Huang et al, 2012" startWordPosition="3970" endWordPosition="3973">bors Table 2 shows qualitatively the results of discovering multiple senses by presenting the nearest neighbors associated with various embeddings. The nearest neighbors of a word are computed by comparing the cosine similarity between the embedding for each sense of the word and the context embeddings of all other words in the vocabulary. Note that each of the discovered senses are indeed semantically coherent, and that a reasonable number of senses are created by the non-parametric method. Table 3 shows the nearest neighbors of the word plant for Skip-gram, MSSG , NP-MSSG and Haung’s model (Huang et al, 2012). 1064 Skip- plants, flowering, weed, fungus, biomass gram MS plants, tubers, soil, seed, biomass -SG refinery, reactor, coal-fired, factory, smelter asteraceae, fabaceae, arecaceae, lamiaceae, ericaceae NP plants, seeds, pollen, fungal, fungus MS factory, manufacturing, refinery, bottling, steel -SG fabaceae, legume, asteraceae, apiaceae, flowering power, coal-fired, hydro-power, hydroelectric, refinery Hua insect, capable, food, solanaceous, subsurface -ng robust, belong, pitcher, comprises, eagles et al food, animal, catching, catch, ecology, fly seafood, equipment, oil, dairy, manufacturer</context>
<context position="25111" citStr="Huang et al (2012)" startWordPosition="4145" endWordPosition="4148">chanism, sugar, drug facility, theater, platform, structure, storage natural, blast, energy, hurl, power matter, physical, certain, expression, agents vine, mute, chalcedony, quandong, excrete Table 3: Nearest Neighbors of the word plant for different models. We see that the discovered senses in both our models are more semantically coherent than Huang et al (2012) and NP-MSSG is able to learn reasonable number of senses. 6.2 Word Similarity We evaluate our embeddings on two related datasets: the WordSim-353 (Finkelstein et al, 2001) dataset and the Contextual Word Similarities (SCWS) dataset Huang et al (2012). WordSim-353 is a standard dataset for evaluating word vector representations. It consists of a list of pairs of word types, the similarity of which is rated in an integral scale from 1 to 10. Pairs include both monosemic and polysemic words. These scores to each word pairs are given without any contextual information, which makes them tricky to interpret. To overcome this issue, Stanford’s Contextual Word Similarities (SCWS) dataset was developed by Huang et al (2012). The dataset consists of 2003 word pairs and their sentential contexts. It consists of 1328 noun-noun pairs, 399 verb-verb pa</context>
<context position="27228" citStr="Huang et al, 2012" startWordPosition="4516" endWordPosition="4519">ow well does each sense fit the context at hand. The globalSim metric uses each word’s global context vector, ignoring the many senses: globalSim(w, w&apos;) = d (vy(w), vy(w&apos;)) . Finally, localSim metric selects a single sense for each word based independently on its context and computes the similarity by localSim(w, w&apos;) = d (vs(w, k), vs(w&apos;, k&apos;)) , where k = arg maxi P(w, c, i) and k&apos; = arg maxj P(w&apos;, c&apos;, j) and P(w, c, i) is the probability that w takes the ith sense given context c. The probability of being in a cluster is calculated as the inverse of the cosine distance to the cluster center (Huang et al, 2012). We report the Spearman correlation between a model’s similarity scores and the human judgements in the datasets. Table 5 shows the results on WordSim-353 task. C&amp;W refers to the language model by Collobert and Weston (2008) and HLBL model is the method described in Mnih and Hinton (2007). On WordSim-353 task, we see that our model performs significantly better than the previous neural network model for learning multi-representations per word (Huang et al, 2012). Among the methods that learn low-dimensional and dense representations, our model performs slightly better than Skip-gram. Table 4 </context>
<context position="29940" citStr="Huang et al, 2012" startWordPosition="4961" endWordPosition="4964"> indicates the globalSim similarity measure and M indicates avgSim measure.The best results among models that learn low-dimensional and dense representations are in bold face. Pruned TF-IDF (Reisinger and Mooney, 2010a), ESA (Gabrilovich and Markovitch, 2007) and Tiered TF-IDF (Reisinger and Mooney, 2010b) construct spare, high-dimensional representations. Figure 3: The plot shows the distribution of number of senses learned per word type in NP-MSSG model given with their context, our model achieves new state-of-the-art results on SCWS as shown in the Table-4. The previous state-of-art model (Huang et al, 2012) on this task achieves 65.7% using the avgSimC measure, while the MSSG model achieves the best score of 69.3% on this task. The results on the other metrics are similar. For a fixed embedding dimension, the model by Huang et al (2012) has more parameters than our model since it uses a hidden layer. The results show that our model performs better than Huang et al (2012) even when both the models use 50 dimensional vectors and the performance of our model improves as we increase the number of dimensions to 300. We evaluate the models in a word analogy task 1066 (a) (b) Figure 4: Figures (a) and </context>
</contexts>
<marker>Huang, Socher, Manning, Ng, 2012</marker>
<rawString>Eric H. Huang, Richard Socher, Christopher D. Manning, and Andrew Y. Ng. 2012. Improving Word Representations via Global Context and Multiple Word Prototypes. Association of Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Koo</author>
<author>Xavier Carreras</author>
<author>Michael Collins</author>
</authors>
<title>Simple Semi-supervised Dependency Parsing. Association for Computational Linguistics (ACL).</title>
<date>2008</date>
<contexts>
<context position="2404" citStr="Koo et al, 2008" startWordPosition="360" endWordPosition="363">BOW and Skip-gram models *The first two authors contributed equally to this paper. of Mikolov et al (2013a); Mikolov et al (2013b)— relatively simple log-linear models that can be trained to produce high-quality word embeddings on the entirety of English Wikipedia text in less than half a day on one machine. There is rising enthusiasm for applying these models to improve accuracy in natural language processing, much like Brown clusters (Brown et al, 1992) have become common input features for many tasks, such as named entity extraction (Miller et al, 2004; Ratinov and Roth, 2009) and parsing (Koo et al, 2008; T¨ackstr¨om et al, 2012). In comparison to Brown clusters, the vector embeddings have the advantages of substantially better scalability in their training, and intriguing potential for their continuous and multi-dimensional interrelations. In fact, Passos et al (2014) present new state-of-the-art results in CoNLL 2003 named entity extraction by directly inputting continuous vector embeddings obtained by a version of Skipgram that injects supervision with lexicons. Similarly Bansal et al (2014) show results in dependency parsing using Skip-gram embeddings. They have also recently been applied</context>
<context position="9613" citStr="Koo et al, 2008" startWordPosition="1492" endWordPosition="1495"> been used in various NLP tasks such as named entity recognition (Neelakantan and Collins, 2014; Passos et al, 2014; Turian et al, 2010), dependency parsing (Bansal et al, 2014), chunking (Turian et al, 2010; Dhillon and Ungar, 2011), sentiment analysis (Maas et al, 2011), paraphrase detection (Socher et al, 2011) and learning representations of paragraphs and documents (Le and Mikolov, 2014). The word clusters obtained from Brown clustering (Brown et al, 1992) have similarly been used as features in named entity recognition (Miller et al, 2004; Ratinov and Roth, 2009) and dependency parsing (Koo et al, 2008), among other tasks. There is considerably less prior work on learning multiple vector representations for the same word type. Reisinger and Mooney (2010a) introduce a method for constructing multiple sparse, high-dimensional vector representations of words. Huang et al (2012) extends this approach incorporating global document context to learn multiple dense, low-dimensional embeddings by using recursive neural networks. Both the methods perform word sense discrimination as a preprocessing step by clustering contexts for each word type, making training more expensive. While methods such as th</context>
</contexts>
<marker>Koo, Carreras, Collins, 2008</marker>
<rawString>Terry Koo, Xavier Carreras, and Michael Collins. 2008. Simple Semi-supervised Dependency Parsing. Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Quoc V Le and Tomas Mikolov</author>
</authors>
<date>2014</date>
<booktitle>Distributed Representations of Sentences and Documents. International Conference on Machine Learning (ICML)</booktitle>
<contexts>
<context position="9392" citStr="Mikolov, 2014" startWordPosition="1458" endWordPosition="1459">t appear to be necessary to learn high-quality word embeddings, as Dhillon and Ungar (2011) estimate word vector representations using Canonical Correlation Analysis (CCA). Word vector representations or embeddings have been used in various NLP tasks such as named entity recognition (Neelakantan and Collins, 2014; Passos et al, 2014; Turian et al, 2010), dependency parsing (Bansal et al, 2014), chunking (Turian et al, 2010; Dhillon and Ungar, 2011), sentiment analysis (Maas et al, 2011), paraphrase detection (Socher et al, 2011) and learning representations of paragraphs and documents (Le and Mikolov, 2014). The word clusters obtained from Brown clustering (Brown et al, 1992) have similarly been used as features in named entity recognition (Miller et al, 2004; Ratinov and Roth, 2009) and dependency parsing (Koo et al, 2008), among other tasks. There is considerably less prior work on learning multiple vector representations for the same word type. Reisinger and Mooney (2010a) introduce a method for constructing multiple sparse, high-dimensional vector representations of words. Huang et al (2012) extends this approach incorporating global document context to learn multiple dense, low-dimensional </context>
</contexts>
<marker>Mikolov, 2014</marker>
<rawString>Quoc V. Le and Tomas Mikolov. 2014 Distributed Representations of Sentences and Documents. International Conference on Machine Learning (ICML)</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew L Maas</author>
<author>Raymond E Daly</author>
<author>Peter T Pham</author>
<author>Dan Huang</author>
<author>Andrew Y Ng</author>
<author>Christopher Potts</author>
</authors>
<title>Learning Word Vectors for Sentiment Analysis Association for Computational Linguistics (ACL)</title>
<date>2011</date>
<contexts>
<context position="9269" citStr="Maas et al, 2011" startWordPosition="1438" endWordPosition="1441">s learning word representations which will be useful in 1060 downstream tasks. Neural networks or log-linear models also do not appear to be necessary to learn high-quality word embeddings, as Dhillon and Ungar (2011) estimate word vector representations using Canonical Correlation Analysis (CCA). Word vector representations or embeddings have been used in various NLP tasks such as named entity recognition (Neelakantan and Collins, 2014; Passos et al, 2014; Turian et al, 2010), dependency parsing (Bansal et al, 2014), chunking (Turian et al, 2010; Dhillon and Ungar, 2011), sentiment analysis (Maas et al, 2011), paraphrase detection (Socher et al, 2011) and learning representations of paragraphs and documents (Le and Mikolov, 2014). The word clusters obtained from Brown clustering (Brown et al, 1992) have similarly been used as features in named entity recognition (Miller et al, 2004; Ratinov and Roth, 2009) and dependency parsing (Koo et al, 2008), among other tasks. There is considerably less prior work on learning multiple vector representations for the same word type. Reisinger and Mooney (2010a) introduce a method for constructing multiple sparse, high-dimensional vector representations of word</context>
</contexts>
<marker>Maas, Daly, Pham, Huang, Ng, Potts, 2011</marker>
<rawString>Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. 2011 Learning Word Vectors for Sentiment Analysis Association for Computational Linguistics (ACL)</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Meyerson</author>
</authors>
<date>2001</date>
<booktitle>IEEE Symposium on Foundations of Computer Science. International Conference on Machine Learning (ICML)</booktitle>
<contexts>
<context position="6260" citStr="Meyerson, 2001" startWordPosition="967" endWordPosition="968">ly discovers a varying number of senses per word type. (3) Efficient online joint training makes it fast and scalable. We refer to our method as Multiple-sense Skip-gram, or MSSG, and its nonparametric counterpart as NP-MSSG. Our method builds on the Skip-gram model (Mikolov et al, 2013a), but maintains multiple vectors per word type. During online training with a particular token, we use the average of its context words’ vectors to select the token’s sense that is closest, and perform a gradient update on that sense. In the non-parametric version of our method, we build on facility location (Meyerson, 2001): a new cluster is created with probability proportional to the distance from the context to the 3Personal communication with authors Eric H. Huang and Richard Socher. nearest sense. We present experimental results demonstrating the benefits of our approach. We show qualitative improvements over single-sense Skip-gram and Huang et al (2012), comparing against word neighbors from our parametric and non-parametric methods. We present quantitative results in three tasks. On both the SCWS and WordSim353 data sets our methods surpass the previous state-ofthe-art. The Google Analogy task is not espe</context>
<context position="17481" citStr="Meyerson (2001)" startWordPosition="2936" endWordPosition="2937"> for the word wt (vs(wt, st)), the global vector of the words in the context and the global vector of the randomly sampled, noisy context words. The context cluster center of cluster st for the word wt (µ(wt, st)) is updated since context ct is added to the cluster st. 5 Non-Parametric MSSG model (NP-MSSG) The MSSG model learns a fixed number of senses per word type. In this section, we describe a non-parametric version of MSSG, the NP-MSSG model, which learns varying number of senses per word type. Our approach is closely related to the online non-parametric clustering procedure described in Meyerson (2001). We create a new cluster (sense) for a word type with probability proportional to the distance of its context to the nearest cluster (sense). Each word w E W is associated with sense vectors, context clusters and a global vector vg(w) as in the MSSG model. The number of senses for a word is unknown and is learned during training. Initially, the words do not have sense vectors and context clusters. We create the first sense vector and context cluster for each word on its first occurrence in the training data. After creating the first context cluster for a word, a new context cluster and a sens</context>
</contexts>
<marker>Meyerson, 2001</marker>
<rawString>Adam Meyerson. 2001 IEEE Symposium on Foundations of Computer Science. International Conference on Machine Learning (ICML)</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient Estimation of Word Representations in Vector Space.</title>
<date>2013</date>
<booktitle>Workshop at International Conference on Learning Representations (ICLR).</booktitle>
<contexts>
<context position="1894" citStr="Mikolov et al (2013" startWordPosition="277" endWordPosition="280">urse of dimensionality and improve generalization because they can place near each other words having similar semantic and syntactic roles. This has been shown dramatically in state-of-the-art results on language modeling (Bengio et al, 2003; Mnih and Hinton, 2007) as well as improvements in other natural language processing tasks (Collobert and Weston, 2008; Turian et al, 2010). Substantial benefit arises when embeddings can be trained on large volumes of data. Hence the recent considerable interest in the CBOW and Skip-gram models *The first two authors contributed equally to this paper. of Mikolov et al (2013a); Mikolov et al (2013b)— relatively simple log-linear models that can be trained to produce high-quality word embeddings on the entirety of English Wikipedia text in less than half a day on one machine. There is rising enthusiasm for applying these models to improve accuracy in natural language processing, much like Brown clusters (Brown et al, 1992) have become common input features for many tasks, such as named entity extraction (Miller et al, 2004; Ratinov and Roth, 2009) and parsing (Koo et al, 2008; T¨ackstr¨om et al, 2012). In comparison to Brown clusters, the vector embeddings have th</context>
<context position="5932" citStr="Mikolov et al, 2013" startWordPosition="912" endWordPosition="915">enses per word type, designed to provide several advantages over previous approaches. (1) Sensediscriminated vectors are learned jointly with the assignment of token contexts to senses; thus we can use the emerging sense representation to more accurately perform the clustering. (2) A nonparametric variant of our method automatically discovers a varying number of senses per word type. (3) Efficient online joint training makes it fast and scalable. We refer to our method as Multiple-sense Skip-gram, or MSSG, and its nonparametric counterpart as NP-MSSG. Our method builds on the Skip-gram model (Mikolov et al, 2013a), but maintains multiple vectors per word type. During online training with a particular token, we use the average of its context words’ vectors to select the token’s sense that is closest, and perform a gradient update on that sense. In the non-parametric version of our method, we build on facility location (Meyerson, 2001): a new cluster is created with probability proportional to the distance from the context to the 3Personal communication with authors Eric H. Huang and Richard Socher. nearest sense. We present experimental results demonstrating the benefits of our approach. We show quali</context>
<context position="8285" citStr="Mikolov et al (2013" startWordPosition="1283" endWordPosition="1286">ing the parameters of the neural network and these vectors jointly. Since the Bengio et al (2003) model is quite expensive to train, much research has focused on optimizing it. Collobert and Weston (2008) replaces the max-likelihood character of the model with a max-margin approach, where the network is encouraged to score the correct n-grams higher than randomly chosen incorrect n-grams. Mnih and Hinton (2007) replaces the global normalization of the Bengio model with a tree-structured probability distribution, and also considers multiple positions for each word in the tree. More relevantly, Mikolov et al (2013a) and Mikolov et al (2013b) propose extremely computationally efficient log-linear neural language models by removing the hidden layers of the neural networks and training from larger context windows with very aggressive subsampling. The goal of the models in Mikolov et al (2013a) and Mikolov et al (2013b) is not so much obtaining a low-perplexity language model as learning word representations which will be useful in 1060 downstream tasks. Neural networks or log-linear models also do not appear to be necessary to learn high-quality word embeddings, as Dhillon and Ungar (2011) estimate word v</context>
<context position="31318" citStr="Mikolov et al (2013" startWordPosition="5206" endWordPosition="5209">WS-353 globalSim 70.4 MSSG WS-353 globalSim 68.4 MSSG WS-353 avgSim 71.2 NP MSSG WS-353 globalSim 68.3 NP MSSG WS-353 avgSim 69.66 MSSG SCWS localSim 59.3 MSSG SCWS globalSim 64.7 MSSG SCWS avgSim 67.2 MSSG SCWS avgSimC 69.2 NP MSSG SCWS localSim 60.11 NP MSSG SCWS globalSim 65.3 NP MSSG SCWS avgSim 67 NP MSSG SCWS avgSimC 68.6 Table 6: Experiment results on WordSim-353 and SCWS Task. Multiple Embeddings are learned for top 30,000 most frequent words in the vocabulary. The embedding dimension size is 300 for all the models for this task. The number of senses for MSSG model is 3. introduced by Mikolov et al (2013a) where both MSSG and NP-MSSG models achieve 64% accuracy compared to 12% accuracy by Huang et al (2012). Skip-gram which is the state-of-art model for this task achieves 67% accuracy. Figure 3 shows the distribution of number of senses learned per word type in the NP-MSSG model. We learn the multiple embeddings for the same set of approximately 6000 words that were used in Huang et al (2012) for all our experiments to ensure fair comparision. These approximately 6000 words were choosen by Huang et al. mainly from the top 30,00 frequent words in the vocabulary. This selection was likely made </context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013a. Efficient Estimation of Word Representations in Vector Space. Workshop at International Conference on Learning Representations (ICLR).</rawString>
</citation>
<citation valid="false">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<booktitle>2013b. Distributed Representations of Words and Phrases and their Compositionality. Advances in Neural Information Processing Systems (NIPS).</booktitle>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, </marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013b. Distributed Representations of Words and Phrases and their Compositionality. Advances in Neural Information Processing Systems (NIPS).</rawString>
</citation>
<citation valid="false">
<authors>
<author>Tomas Mikolov</author>
<author>Quoc V Le</author>
<author>Ilya Sutskever</author>
</authors>
<booktitle>2013c. Exploiting Similarities among Languages for Machine Translation. arXiv.</booktitle>
<marker>Mikolov, Le, Sutskever, </marker>
<rawString>Tomas Mikolov, Quoc V. Le, and Ilya Sutskever. 2013c. Exploiting Similarities among Languages for Machine Translation. arXiv.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Miller</author>
<author>Jethran Guinness</author>
<author>Alex Zamanian</author>
</authors>
<title>Name tagging with word clusters and discriminative training. North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT).</title>
<date>2004</date>
<contexts>
<context position="2350" citStr="Miller et al, 2004" startWordPosition="350" endWordPosition="353"> of data. Hence the recent considerable interest in the CBOW and Skip-gram models *The first two authors contributed equally to this paper. of Mikolov et al (2013a); Mikolov et al (2013b)— relatively simple log-linear models that can be trained to produce high-quality word embeddings on the entirety of English Wikipedia text in less than half a day on one machine. There is rising enthusiasm for applying these models to improve accuracy in natural language processing, much like Brown clusters (Brown et al, 1992) have become common input features for many tasks, such as named entity extraction (Miller et al, 2004; Ratinov and Roth, 2009) and parsing (Koo et al, 2008; T¨ackstr¨om et al, 2012). In comparison to Brown clusters, the vector embeddings have the advantages of substantially better scalability in their training, and intriguing potential for their continuous and multi-dimensional interrelations. In fact, Passos et al (2014) present new state-of-the-art results in CoNLL 2003 named entity extraction by directly inputting continuous vector embeddings obtained by a version of Skipgram that injects supervision with lexicons. Similarly Bansal et al (2014) show results in dependency parsing using Skip</context>
<context position="9547" citStr="Miller et al, 2004" startWordPosition="1481" endWordPosition="1484">ation Analysis (CCA). Word vector representations or embeddings have been used in various NLP tasks such as named entity recognition (Neelakantan and Collins, 2014; Passos et al, 2014; Turian et al, 2010), dependency parsing (Bansal et al, 2014), chunking (Turian et al, 2010; Dhillon and Ungar, 2011), sentiment analysis (Maas et al, 2011), paraphrase detection (Socher et al, 2011) and learning representations of paragraphs and documents (Le and Mikolov, 2014). The word clusters obtained from Brown clustering (Brown et al, 1992) have similarly been used as features in named entity recognition (Miller et al, 2004; Ratinov and Roth, 2009) and dependency parsing (Koo et al, 2008), among other tasks. There is considerably less prior work on learning multiple vector representations for the same word type. Reisinger and Mooney (2010a) introduce a method for constructing multiple sparse, high-dimensional vector representations of words. Huang et al (2012) extends this approach incorporating global document context to learn multiple dense, low-dimensional embeddings by using recursive neural networks. Both the methods perform word sense discrimination as a preprocessing step by clustering contexts for each w</context>
</contexts>
<marker>Miller, Guinness, Zamanian, 2004</marker>
<rawString>Scott Miller, Jethran Guinness, and Alex Zamanian. 2004. Name tagging with word clusters and discriminative training. North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andriy Mnih</author>
<author>Geoffrey Hinton</author>
</authors>
<title>Three new graphical models for statistical language modelling.</title>
<date>2007</date>
<booktitle>International Conference on Machine learning (ICML).</booktitle>
<contexts>
<context position="1540" citStr="Mnih and Hinton, 2007" startWordPosition="220" endWordPosition="223">lability. We present new state-of-the-art results in the word similarity in context task and demonstrate its scalability by training with one machine on a corpus of nearly 1 billion tokens in less than 6 hours. 1 Introduction Representing words by dense, real-valued vector embeddings, also commonly called “distributed representations,” helps address the curse of dimensionality and improve generalization because they can place near each other words having similar semantic and syntactic roles. This has been shown dramatically in state-of-the-art results on language modeling (Bengio et al, 2003; Mnih and Hinton, 2007) as well as improvements in other natural language processing tasks (Collobert and Weston, 2008; Turian et al, 2010). Substantial benefit arises when embeddings can be trained on large volumes of data. Hence the recent considerable interest in the CBOW and Skip-gram models *The first two authors contributed equally to this paper. of Mikolov et al (2013a); Mikolov et al (2013b)— relatively simple log-linear models that can be trained to produce high-quality word embeddings on the entirety of English Wikipedia text in less than half a day on one machine. There is rising enthusiasm for applying t</context>
<context position="8080" citStr="Mnih and Hinton (2007)" startWordPosition="1253" endWordPosition="1256">he traditional idea of n-gram language models by replacing the conditional probability table with a neural network, representing each word token by a small vector instead of an indicator variable, and estimating the parameters of the neural network and these vectors jointly. Since the Bengio et al (2003) model is quite expensive to train, much research has focused on optimizing it. Collobert and Weston (2008) replaces the max-likelihood character of the model with a max-margin approach, where the network is encouraged to score the correct n-grams higher than randomly chosen incorrect n-grams. Mnih and Hinton (2007) replaces the global normalization of the Bengio model with a tree-structured probability distribution, and also considers multiple positions for each word in the tree. More relevantly, Mikolov et al (2013a) and Mikolov et al (2013b) propose extremely computationally efficient log-linear neural language models by removing the hidden layers of the neural networks and training from larger context windows with very aggressive subsampling. The goal of the models in Mikolov et al (2013a) and Mikolov et al (2013b) is not so much obtaining a low-perplexity language model as learning word representati</context>
<context position="27518" citStr="Mnih and Hinton (2007)" startWordPosition="4566" endWordPosition="4569">he similarity by localSim(w, w&apos;) = d (vs(w, k), vs(w&apos;, k&apos;)) , where k = arg maxi P(w, c, i) and k&apos; = arg maxj P(w&apos;, c&apos;, j) and P(w, c, i) is the probability that w takes the ith sense given context c. The probability of being in a cluster is calculated as the inverse of the cosine distance to the cluster center (Huang et al, 2012). We report the Spearman correlation between a model’s similarity scores and the human judgements in the datasets. Table 5 shows the results on WordSim-353 task. C&amp;W refers to the language model by Collobert and Weston (2008) and HLBL model is the method described in Mnih and Hinton (2007). On WordSim-353 task, we see that our model performs significantly better than the previous neural network model for learning multi-representations per word (Huang et al, 2012). Among the methods that learn low-dimensional and dense representations, our model performs slightly better than Skip-gram. Table 4 shows the results for the SCWS task. In this task, when the words are 1 K K K2 i=1 j=1 1065 Model globalSim avgSim avgSimC localSim TF-IDF 26.3 - - - Collobort &amp; Weston-50d 57.0 - - - Skip-gram-50d 63.4 - - - Skip-gram-300d 65.2 - - - Pruned TF-IDF 62.5 60.4 60.5 - Huang et al-50d 58.6 62.</context>
</contexts>
<marker>Mnih, Hinton, 2007</marker>
<rawString>Andriy Mnih and Geoffrey Hinton. 2007. Three new graphical models for statistical language modelling. International Conference on Machine learning (ICML).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arvind Neelakantan</author>
<author>Michael Collins</author>
</authors>
<title>Learning Dictionaries for Named Entity Recognition using Minimal Supervision.</title>
<date>2014</date>
<booktitle>European Chapter of the Association for Computational Linguistics (EACL).</booktitle>
<contexts>
<context position="9092" citStr="Neelakantan and Collins, 2014" startWordPosition="1409" endWordPosition="1412">larger context windows with very aggressive subsampling. The goal of the models in Mikolov et al (2013a) and Mikolov et al (2013b) is not so much obtaining a low-perplexity language model as learning word representations which will be useful in 1060 downstream tasks. Neural networks or log-linear models also do not appear to be necessary to learn high-quality word embeddings, as Dhillon and Ungar (2011) estimate word vector representations using Canonical Correlation Analysis (CCA). Word vector representations or embeddings have been used in various NLP tasks such as named entity recognition (Neelakantan and Collins, 2014; Passos et al, 2014; Turian et al, 2010), dependency parsing (Bansal et al, 2014), chunking (Turian et al, 2010; Dhillon and Ungar, 2011), sentiment analysis (Maas et al, 2011), paraphrase detection (Socher et al, 2011) and learning representations of paragraphs and documents (Le and Mikolov, 2014). The word clusters obtained from Brown clustering (Brown et al, 1992) have similarly been used as features in named entity recognition (Miller et al, 2004; Ratinov and Roth, 2009) and dependency parsing (Koo et al, 2008), among other tasks. There is considerably less prior work on learning multiple</context>
</contexts>
<marker>Neelakantan, Collins, 2014</marker>
<rawString>Arvind Neelakantan and Michael Collins. 2014. Learning Dictionaries for Named Entity Recognition using Minimal Supervision. European Chapter of the Association for Computational Linguistics (EACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexandre Passos</author>
<author>Vineet Kumar</author>
<author>Andrew McCallum</author>
</authors>
<date>2014</date>
<booktitle>Lexicon Infused Phrase Embeddings for Named Entity Resolution. Conference on Natural Language Learning (CoNLL).</booktitle>
<contexts>
<context position="2674" citStr="Passos et al (2014)" startWordPosition="400" endWordPosition="403">ext in less than half a day on one machine. There is rising enthusiasm for applying these models to improve accuracy in natural language processing, much like Brown clusters (Brown et al, 1992) have become common input features for many tasks, such as named entity extraction (Miller et al, 2004; Ratinov and Roth, 2009) and parsing (Koo et al, 2008; T¨ackstr¨om et al, 2012). In comparison to Brown clusters, the vector embeddings have the advantages of substantially better scalability in their training, and intriguing potential for their continuous and multi-dimensional interrelations. In fact, Passos et al (2014) present new state-of-the-art results in CoNLL 2003 named entity extraction by directly inputting continuous vector embeddings obtained by a version of Skipgram that injects supervision with lexicons. Similarly Bansal et al (2014) show results in dependency parsing using Skip-gram embeddings. They have also recently been applied to machine translation (Zou et al, 2013; Mikolov et al, 2013c). A notable deficiency in this prior work is that each word type (e.g. the word string plant) has only one vector representation—polysemy and hononymy are ignored. This results in the word plant having an em</context>
<context position="9112" citStr="Passos et al, 2014" startWordPosition="1413" endWordPosition="1416">y aggressive subsampling. The goal of the models in Mikolov et al (2013a) and Mikolov et al (2013b) is not so much obtaining a low-perplexity language model as learning word representations which will be useful in 1060 downstream tasks. Neural networks or log-linear models also do not appear to be necessary to learn high-quality word embeddings, as Dhillon and Ungar (2011) estimate word vector representations using Canonical Correlation Analysis (CCA). Word vector representations or embeddings have been used in various NLP tasks such as named entity recognition (Neelakantan and Collins, 2014; Passos et al, 2014; Turian et al, 2010), dependency parsing (Bansal et al, 2014), chunking (Turian et al, 2010; Dhillon and Ungar, 2011), sentiment analysis (Maas et al, 2011), paraphrase detection (Socher et al, 2011) and learning representations of paragraphs and documents (Le and Mikolov, 2014). The word clusters obtained from Brown clustering (Brown et al, 1992) have similarly been used as features in named entity recognition (Miller et al, 2004; Ratinov and Roth, 2009) and dependency parsing (Koo et al, 2008), among other tasks. There is considerably less prior work on learning multiple vector representati</context>
</contexts>
<marker>Passos, Kumar, McCallum, 2014</marker>
<rawString>Alexandre Passos, Vineet Kumar, and Andrew McCallum. 2014. Lexicon Infused Phrase Embeddings for Named Entity Resolution. Conference on Natural Language Learning (CoNLL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lev Ratinov</author>
<author>Dan Roth</author>
</authors>
<date>2009</date>
<booktitle>Design Challenges and Misconceptions in Named Entity Recognition. Conference on Natural Language Learning (CoNLL).</booktitle>
<contexts>
<context position="2375" citStr="Ratinov and Roth, 2009" startWordPosition="354" endWordPosition="357">recent considerable interest in the CBOW and Skip-gram models *The first two authors contributed equally to this paper. of Mikolov et al (2013a); Mikolov et al (2013b)— relatively simple log-linear models that can be trained to produce high-quality word embeddings on the entirety of English Wikipedia text in less than half a day on one machine. There is rising enthusiasm for applying these models to improve accuracy in natural language processing, much like Brown clusters (Brown et al, 1992) have become common input features for many tasks, such as named entity extraction (Miller et al, 2004; Ratinov and Roth, 2009) and parsing (Koo et al, 2008; T¨ackstr¨om et al, 2012). In comparison to Brown clusters, the vector embeddings have the advantages of substantially better scalability in their training, and intriguing potential for their continuous and multi-dimensional interrelations. In fact, Passos et al (2014) present new state-of-the-art results in CoNLL 2003 named entity extraction by directly inputting continuous vector embeddings obtained by a version of Skipgram that injects supervision with lexicons. Similarly Bansal et al (2014) show results in dependency parsing using Skip-gram embeddings. They ha</context>
<context position="9572" citStr="Ratinov and Roth, 2009" startWordPosition="1485" endWordPosition="1488">. Word vector representations or embeddings have been used in various NLP tasks such as named entity recognition (Neelakantan and Collins, 2014; Passos et al, 2014; Turian et al, 2010), dependency parsing (Bansal et al, 2014), chunking (Turian et al, 2010; Dhillon and Ungar, 2011), sentiment analysis (Maas et al, 2011), paraphrase detection (Socher et al, 2011) and learning representations of paragraphs and documents (Le and Mikolov, 2014). The word clusters obtained from Brown clustering (Brown et al, 1992) have similarly been used as features in named entity recognition (Miller et al, 2004; Ratinov and Roth, 2009) and dependency parsing (Koo et al, 2008), among other tasks. There is considerably less prior work on learning multiple vector representations for the same word type. Reisinger and Mooney (2010a) introduce a method for constructing multiple sparse, high-dimensional vector representations of words. Huang et al (2012) extends this approach incorporating global document context to learn multiple dense, low-dimensional embeddings by using recursive neural networks. Both the methods perform word sense discrimination as a preprocessing step by clustering contexts for each word type, making training</context>
</contexts>
<marker>Ratinov, Roth, 2009</marker>
<rawString>Lev Ratinov and Dan Roth. 2009. Design Challenges and Misconceptions in Named Entity Recognition. Conference on Natural Language Learning (CoNLL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Siva Reddy</author>
<author>Ioannis P Klapaftis</author>
<author>Diana McCarthy</author>
</authors>
<date>2011</date>
<booktitle>Dynamic and Static Prototype Vectors for Semantic Composition. International Joint Conference on Artificial Intelligence (IJCNLP).</booktitle>
<contexts>
<context position="10277" citStr="Reddy et al (2011)" startWordPosition="1594" endWordPosition="1597">s prior work on learning multiple vector representations for the same word type. Reisinger and Mooney (2010a) introduce a method for constructing multiple sparse, high-dimensional vector representations of words. Huang et al (2012) extends this approach incorporating global document context to learn multiple dense, low-dimensional embeddings by using recursive neural networks. Both the methods perform word sense discrimination as a preprocessing step by clustering contexts for each word type, making training more expensive. While methods such as those described in Dhillon and Ungar (2011) and Reddy et al (2011) use token-specific representations of words as part of the learning algorithm, the final outputs are still one-to-one mappings between word types and word embeddings. 3 Background: Skip-gram model The Skip-gram model learns word embeddings such that they are useful in predicting the surrounding words in a sentence. In the Skip-gram model, v(w) ∈ Rd is the vector representation of the word w ∈ W, where W is the words vocabulary and d is the embedding dimensionality. Given a pair of words (wt, c), the probability that the word c is observed in the context of word wt is given by, 1 P(D = 1|v(wt)</context>
</contexts>
<marker>Reddy, Klapaftis, McCarthy, 2011</marker>
<rawString>Siva Reddy, Ioannis P. Klapaftis, and Diana McCarthy. 2011. Dynamic and Static Prototype Vectors for Semantic Composition. International Joint Conference on Artificial Intelligence (IJCNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Reisinger</author>
<author>Raymond J Mooney</author>
</authors>
<title>Multi-prototype vector-space models of word meaning. North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)</title>
<date>2010</date>
<contexts>
<context position="4398" citStr="Reisinger and Mooney (2010" startWordPosition="667" endWordPosition="670">ately pulled to a dis2For distance d, d(a, c) ≤ d(a, b) + d(b, c). 1059 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1059–1069, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics tance not more than the sum of the distances plantpollen and plant–refinery. Fitting the constraints of legitimate continuous gradations of semantics are challenge enough without the additional encumbrance of these illegitimate triangle inequalities. Discovering embeddings for multiple senses per word type is the focus of work by Reisinger and Mooney (2010a) and Huang et al (2012). They both pre-cluster the contexts of a word type’s tokens into discriminated senses, use the clusters to re-label the corpus’ tokens according to sense, and then learn embeddings for these re-labeled words. The second paper improves upon the first by employing an earlier pass of non-discriminated embedding learning to obtain vectors used to represent the contexts. Note that by pre-clustering, these methods lose the opportunity to jointly learn the sense-discriminated vectors and the clustering. Other weaknesses include their fixed number of sense per word type, and </context>
<context position="9766" citStr="Reisinger and Mooney (2010" startWordPosition="1516" endWordPosition="1519">dency parsing (Bansal et al, 2014), chunking (Turian et al, 2010; Dhillon and Ungar, 2011), sentiment analysis (Maas et al, 2011), paraphrase detection (Socher et al, 2011) and learning representations of paragraphs and documents (Le and Mikolov, 2014). The word clusters obtained from Brown clustering (Brown et al, 1992) have similarly been used as features in named entity recognition (Miller et al, 2004; Ratinov and Roth, 2009) and dependency parsing (Koo et al, 2008), among other tasks. There is considerably less prior work on learning multiple vector representations for the same word type. Reisinger and Mooney (2010a) introduce a method for constructing multiple sparse, high-dimensional vector representations of words. Huang et al (2012) extends this approach incorporating global document context to learn multiple dense, low-dimensional embeddings by using recursive neural networks. Both the methods perform word sense discrimination as a preprocessing step by clustering contexts for each word type, making training more expensive. While methods such as those described in Dhillon and Ungar (2011) and Reddy et al (2011) use token-specific representations of words as part of the learning algorithm, the final</context>
<context position="12526" citStr="Reisinger and Mooney, 2010" startWordPosition="2015" endWordPosition="2018">rd wt uniformly randomly sampled from the set {1, 2, ... , N}, where N is the maximum context window size. The set of noisy context words c0t for the word wt is constructed by randomly sampling S noisy context words for each word in the context ct. The noisy context words are randomly sampled from the following distribution, P(w) = punigram(w)3/4 (2) Z where punigram(w) is the unigram distribution of the words and Z is the normalization constant. 4 Multi-Sense Skip-gram (MSSG) model To extend the Skip-gram model to learn multiple embeddings per word we follow previous work (Huang et al, 2012; Reisinger and Mooney, 2010a) 1061 Figure 1: Architecture of the Skip-gram model with window size Rt = 2. Context ct of word wt consists of wt−1, wt−2, wt+1, wt+2. and let each sense of word have its own embedding, and induce the senses by clustering the embeddings of the context words around each token. The vector representation of the context is the average of its context words’ vectors. For every word type, we maintain clusters of its contexts and the sense of a word token is predicted as the cluster that is closest to its context representation. After predicting the sense of a word token, we perform a gradient updat</context>
<context position="29539" citStr="Reisinger and Mooney, 2010" startWordPosition="4901" endWordPosition="4904">LBL 33.2 C&amp;W 55.3 Skip-gram-300d 70.4 Huang et al-G 22.8 Huang et al-M 64.2 MSSG 50d-G 60.6 MSSG 50d-M 63.2 MSSG 300d-G 69.2 MSSG 300d-M 70.9 NP-MSSG 50d-G 61.5 NP-MSSG 50d-M 62.4 NP-MSSG 300d-G 69.1 NP-MSSG 300d-M 68.6 Pruned TF-IDF 73.4 ESA 75 Tiered TF-IDF 76.9 Table 5: Results on the WordSim-353 dataset. The table shows the Spearmans correlation p between the model’s similarities and human judgments. G indicates the globalSim similarity measure and M indicates avgSim measure.The best results among models that learn low-dimensional and dense representations are in bold face. Pruned TF-IDF (Reisinger and Mooney, 2010a), ESA (Gabrilovich and Markovitch, 2007) and Tiered TF-IDF (Reisinger and Mooney, 2010b) construct spare, high-dimensional representations. Figure 3: The plot shows the distribution of number of senses learned per word type in NP-MSSG model given with their context, our model achieves new state-of-the-art results on SCWS as shown in the Table-4. The previous state-of-art model (Huang et al, 2012) on this task achieves 65.7% using the avgSimC measure, while the MSSG model achieves the best score of 69.3% on this task. The results on the other metrics are similar. For a fixed embedding dimensi</context>
</contexts>
<marker>Reisinger, Mooney, 2010</marker>
<rawString>Joseph Reisinger and Raymond J. Mooney. 2010a. Multi-prototype vector-space models of word meaning. North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Reisinger</author>
<author>Raymond Mooney</author>
</authors>
<title>A mixture model with sharing for lexical semantics.</title>
<date>2010</date>
<booktitle>Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="4398" citStr="Reisinger and Mooney (2010" startWordPosition="667" endWordPosition="670">ately pulled to a dis2For distance d, d(a, c) ≤ d(a, b) + d(b, c). 1059 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1059–1069, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics tance not more than the sum of the distances plantpollen and plant–refinery. Fitting the constraints of legitimate continuous gradations of semantics are challenge enough without the additional encumbrance of these illegitimate triangle inequalities. Discovering embeddings for multiple senses per word type is the focus of work by Reisinger and Mooney (2010a) and Huang et al (2012). They both pre-cluster the contexts of a word type’s tokens into discriminated senses, use the clusters to re-label the corpus’ tokens according to sense, and then learn embeddings for these re-labeled words. The second paper improves upon the first by employing an earlier pass of non-discriminated embedding learning to obtain vectors used to represent the contexts. Note that by pre-clustering, these methods lose the opportunity to jointly learn the sense-discriminated vectors and the clustering. Other weaknesses include their fixed number of sense per word type, and </context>
<context position="9766" citStr="Reisinger and Mooney (2010" startWordPosition="1516" endWordPosition="1519">dency parsing (Bansal et al, 2014), chunking (Turian et al, 2010; Dhillon and Ungar, 2011), sentiment analysis (Maas et al, 2011), paraphrase detection (Socher et al, 2011) and learning representations of paragraphs and documents (Le and Mikolov, 2014). The word clusters obtained from Brown clustering (Brown et al, 1992) have similarly been used as features in named entity recognition (Miller et al, 2004; Ratinov and Roth, 2009) and dependency parsing (Koo et al, 2008), among other tasks. There is considerably less prior work on learning multiple vector representations for the same word type. Reisinger and Mooney (2010a) introduce a method for constructing multiple sparse, high-dimensional vector representations of words. Huang et al (2012) extends this approach incorporating global document context to learn multiple dense, low-dimensional embeddings by using recursive neural networks. Both the methods perform word sense discrimination as a preprocessing step by clustering contexts for each word type, making training more expensive. While methods such as those described in Dhillon and Ungar (2011) and Reddy et al (2011) use token-specific representations of words as part of the learning algorithm, the final</context>
<context position="12526" citStr="Reisinger and Mooney, 2010" startWordPosition="2015" endWordPosition="2018">rd wt uniformly randomly sampled from the set {1, 2, ... , N}, where N is the maximum context window size. The set of noisy context words c0t for the word wt is constructed by randomly sampling S noisy context words for each word in the context ct. The noisy context words are randomly sampled from the following distribution, P(w) = punigram(w)3/4 (2) Z where punigram(w) is the unigram distribution of the words and Z is the normalization constant. 4 Multi-Sense Skip-gram (MSSG) model To extend the Skip-gram model to learn multiple embeddings per word we follow previous work (Huang et al, 2012; Reisinger and Mooney, 2010a) 1061 Figure 1: Architecture of the Skip-gram model with window size Rt = 2. Context ct of word wt consists of wt−1, wt−2, wt+1, wt+2. and let each sense of word have its own embedding, and induce the senses by clustering the embeddings of the context words around each token. The vector representation of the context is the average of its context words’ vectors. For every word type, we maintain clusters of its contexts and the sense of a word token is predicted as the cluster that is closest to its context representation. After predicting the sense of a word token, we perform a gradient updat</context>
<context position="29539" citStr="Reisinger and Mooney, 2010" startWordPosition="4901" endWordPosition="4904">LBL 33.2 C&amp;W 55.3 Skip-gram-300d 70.4 Huang et al-G 22.8 Huang et al-M 64.2 MSSG 50d-G 60.6 MSSG 50d-M 63.2 MSSG 300d-G 69.2 MSSG 300d-M 70.9 NP-MSSG 50d-G 61.5 NP-MSSG 50d-M 62.4 NP-MSSG 300d-G 69.1 NP-MSSG 300d-M 68.6 Pruned TF-IDF 73.4 ESA 75 Tiered TF-IDF 76.9 Table 5: Results on the WordSim-353 dataset. The table shows the Spearmans correlation p between the model’s similarities and human judgments. G indicates the globalSim similarity measure and M indicates avgSim measure.The best results among models that learn low-dimensional and dense representations are in bold face. Pruned TF-IDF (Reisinger and Mooney, 2010a), ESA (Gabrilovich and Markovitch, 2007) and Tiered TF-IDF (Reisinger and Mooney, 2010b) construct spare, high-dimensional representations. Figure 3: The plot shows the distribution of number of senses learned per word type in NP-MSSG model given with their context, our model achieves new state-of-the-art results on SCWS as shown in the Table-4. The previous state-of-art model (Huang et al, 2012) on this task achieves 65.7% using the avgSimC measure, while the MSSG model achieves the best score of 69.3% on this task. The results on the other metrics are similar. For a fixed embedding dimensi</context>
</contexts>
<marker>Reisinger, Mooney, 2010</marker>
<rawString>Joseph Reisinger and Raymond Mooney. 2010b. A mixture model with sharing for lexical semantics. Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cyrus Shaoul</author>
<author>Chris Westbury</author>
</authors>
<title>The Westbury lab wikipedia corpus.</title>
<date>2010</date>
<contexts>
<context position="20108" citStr="Shaoul and Westbury, 2010" startWordPosition="3407" endWordPosition="3411">ense of the word remain the same. J(θ) = X (wt,ct)∈D+ X c∈ct ⎧ ⎨⎪ ⎪⎩ st = 1063 Model Time (in hours) Huang et al 168 MSSG 50d 1 MSSG-300d 6 NP-MSSG-50d 1.83 NP-MSSG-300d 5 Skip-gram-50d 0.33 Skip-gram-300d 1.5 Table 1: Training Time Results. First five model reported in the table are capable of learning multiple embeddings for each word and Skip-gram is capable of learning only single embedding for each word. 6 Experiments To evaluate our algorithms we train embeddings using the same corpus and vocabulary as used in Huang et al (2012), which is the April 2010 snapshot of the Wikipedia corpus (Shaoul and Westbury, 2010). It contains approximately 2 million articles and 990 million tokens. In all our experiments we remove all the words with less than 20 occurrences and use a maximum context window (N) of length 5 (5 words before and after the word occurrence). We fix the number of senses (K) to be 3 for the MSSG model unless otherwise specified. Our hyperparameter values were selected by a small amount of manual exploration on a validation set. In NP-MSSG we set A to -0.5. The Skip-gram model, MSSG and NP-MSSG models sample one noisy context word (S) for each of the observed context words. We train our models</context>
</contexts>
<marker>Shaoul, Westbury, 2010</marker>
<rawString>Cyrus Shaoul and Chris Westbury. 2010. The Westbury lab wikipedia corpus.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Eric H Huang</author>
<author>Jeffrey Pennington</author>
<author>Andrew Y Ng</author>
<author>Christopher D Manning</author>
</authors>
<title>Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection.</title>
<date>2011</date>
<booktitle>Advances in Neural Information Processing Systems (NIPS).</booktitle>
<contexts>
<context position="9312" citStr="Socher et al, 2011" startWordPosition="1445" endWordPosition="1448">l be useful in 1060 downstream tasks. Neural networks or log-linear models also do not appear to be necessary to learn high-quality word embeddings, as Dhillon and Ungar (2011) estimate word vector representations using Canonical Correlation Analysis (CCA). Word vector representations or embeddings have been used in various NLP tasks such as named entity recognition (Neelakantan and Collins, 2014; Passos et al, 2014; Turian et al, 2010), dependency parsing (Bansal et al, 2014), chunking (Turian et al, 2010; Dhillon and Ungar, 2011), sentiment analysis (Maas et al, 2011), paraphrase detection (Socher et al, 2011) and learning representations of paragraphs and documents (Le and Mikolov, 2014). The word clusters obtained from Brown clustering (Brown et al, 1992) have similarly been used as features in named entity recognition (Miller et al, 2004; Ratinov and Roth, 2009) and dependency parsing (Koo et al, 2008), among other tasks. There is considerably less prior work on learning multiple vector representations for the same word type. Reisinger and Mooney (2010a) introduce a method for constructing multiple sparse, high-dimensional vector representations of words. Huang et al (2012) extends this approach</context>
</contexts>
<marker>Socher, Huang, Pennington, Ng, Manning, 2011</marker>
<rawString>Richard Socher, Eric H. Huang, Jeffrey Pennington, Andrew Y. Ng, and Christopher D. Manning. 2011 Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection. Advances in Neural Information Processing Systems (NIPS).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oscar T¨ackstr¨om</author>
<author>Ryan McDonald</author>
<author>Jakob Uszkoreit</author>
</authors>
<title>Cross-lingual Word Clusters for Direct Transfer of Linguistic Structure. North American Chapter of the Association for Computational Linguistics: Human Language Technologies.</title>
<date>2012</date>
<marker>T¨ackstr¨om, McDonald, Uszkoreit, 2012</marker>
<rawString>Oscar T¨ackstr¨om, Ryan McDonald, and Jakob Uszkoreit. 2012. Cross-lingual Word Clusters for Direct Transfer of Linguistic Structure. North American Chapter of the Association for Computational Linguistics: Human Language Technologies.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>Lev Ratinov</author>
<author>Yoshua Bengio</author>
</authors>
<title>Word Representations: A Simple and General Method for Semi-Supervised Learning. Association for Computational Linguistics (ACL).</title>
<date>2010</date>
<contexts>
<context position="1656" citStr="Turian et al, 2010" startWordPosition="238" endWordPosition="241">y by training with one machine on a corpus of nearly 1 billion tokens in less than 6 hours. 1 Introduction Representing words by dense, real-valued vector embeddings, also commonly called “distributed representations,” helps address the curse of dimensionality and improve generalization because they can place near each other words having similar semantic and syntactic roles. This has been shown dramatically in state-of-the-art results on language modeling (Bengio et al, 2003; Mnih and Hinton, 2007) as well as improvements in other natural language processing tasks (Collobert and Weston, 2008; Turian et al, 2010). Substantial benefit arises when embeddings can be trained on large volumes of data. Hence the recent considerable interest in the CBOW and Skip-gram models *The first two authors contributed equally to this paper. of Mikolov et al (2013a); Mikolov et al (2013b)— relatively simple log-linear models that can be trained to produce high-quality word embeddings on the entirety of English Wikipedia text in less than half a day on one machine. There is rising enthusiasm for applying these models to improve accuracy in natural language processing, much like Brown clusters (Brown et al, 1992) have be</context>
<context position="9133" citStr="Turian et al, 2010" startWordPosition="1417" endWordPosition="1420">ling. The goal of the models in Mikolov et al (2013a) and Mikolov et al (2013b) is not so much obtaining a low-perplexity language model as learning word representations which will be useful in 1060 downstream tasks. Neural networks or log-linear models also do not appear to be necessary to learn high-quality word embeddings, as Dhillon and Ungar (2011) estimate word vector representations using Canonical Correlation Analysis (CCA). Word vector representations or embeddings have been used in various NLP tasks such as named entity recognition (Neelakantan and Collins, 2014; Passos et al, 2014; Turian et al, 2010), dependency parsing (Bansal et al, 2014), chunking (Turian et al, 2010; Dhillon and Ungar, 2011), sentiment analysis (Maas et al, 2011), paraphrase detection (Socher et al, 2011) and learning representations of paragraphs and documents (Le and Mikolov, 2014). The word clusters obtained from Brown clustering (Brown et al, 1992) have similarly been used as features in named entity recognition (Miller et al, 2004; Ratinov and Roth, 2009) and dependency parsing (Koo et al, 2008), among other tasks. There is considerably less prior work on learning multiple vector representations for the same word</context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word Representations: A Simple and General Method for Semi-Supervised Learning. Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Will Y Zou</author>
<author>Richard Socher</author>
<author>Daniel Cer</author>
<author>Christopher D Manning</author>
</authors>
<date>2013</date>
<booktitle>Bilingual Word Embeddings for Phrase-Based Machine Translation. Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="3044" citStr="Zou et al, 2013" startWordPosition="457" endWordPosition="460">). In comparison to Brown clusters, the vector embeddings have the advantages of substantially better scalability in their training, and intriguing potential for their continuous and multi-dimensional interrelations. In fact, Passos et al (2014) present new state-of-the-art results in CoNLL 2003 named entity extraction by directly inputting continuous vector embeddings obtained by a version of Skipgram that injects supervision with lexicons. Similarly Bansal et al (2014) show results in dependency parsing using Skip-gram embeddings. They have also recently been applied to machine translation (Zou et al, 2013; Mikolov et al, 2013c). A notable deficiency in this prior work is that each word type (e.g. the word string plant) has only one vector representation—polysemy and hononymy are ignored. This results in the word plant having an embedding that is approximately the average of its different contextual semantics relating to biology, placement, manufacturing and power generation. In moderately highdimensional spaces a vector can be relatively “close” to multiple regions at a time, but this does not negate the unfortunate influence of the triangle inequality2 here: words that are not synonyms but ar</context>
</contexts>
<marker>Zou, Socher, Cer, Manning, 2013</marker>
<rawString>Will Y. Zou, Richard Socher, Daniel Cer, and Christopher D. Manning. 2013. Bilingual Word Embeddings for Phrase-Based Machine Translation. Empirical Methods in Natural Language Processing.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>